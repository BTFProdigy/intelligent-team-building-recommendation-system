2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 152?161,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Segmentation Similarity and Agreement
Chris Fournier
University of Ottawa
Ottawa, ON, Canada
cfour037@eecs.uottawa.ca
Diana Inkpen
University of Ottawa
Ottawa, ON, Canada
diana@eecs.uottawa.ca
Abstract
We propose a new segmentation evaluation
metric, called segmentation similarity (S), that
quantifies the similarity between two segmen-
tations as the proportion of boundaries that
are not transformed when comparing them us-
ing edit distance, essentially using edit dis-
tance as a penalty function and scaling penal-
ties by segmentation size. We propose several
adapted inter-annotator agreement coefficients
which use S that are suitable for segmenta-
tion. We show that S is configurable enough
to suit a wide variety of segmentation evalua-
tions, and is an improvement upon the state of
the art. We also propose using inter-annotator
agreement coefficients to evaluate automatic
segmenters in terms of human performance.
1 Introduction
Segmentation is the task of splitting up an item, such
as a document, into a sequence of segments by plac-
ing boundaries within. The purpose of segmenting
can vary greatly, but one common objective is to
denote shifts in the topic of a text, where multiple
boundary types can also be present (e.g., major ver-
sus minor topic shifts). Human-competitive auto-
matic segmentation methods can help a wide range
of computational linguistic tasks which depend upon
the identification of segment boundaries in text.
To evaluate automatic segmentation methods, a
method of comparing an automatic segmenter?s per-
formance against the segmentations produced by hu-
man judges (coders) is required. Current meth-
ods of performing this comparison designate only
one coder?s segmentation as a reference to com-
pare against. A single ?true? reference segmentation
from a coder should not be trusted, given that inter-
annotator agreement is often reported to be rather
poor (Hearst, 1997, p. 54). Additionally, to en-
sure that an automatic segmenter does not over-fit
to the preference and bias of one particular coder,
an automatic segmenter should be compared directly
against multiple coders.
The state of the art segmentation evaluation met-
rics (Pk and WindowDiff) slide a window across a
designated reference and hypothesis segmentation,
and count the number of windows where the number
of boundaries differ. Window-based methods suffer
from a variety of problems, including: i) unequal
penalization of error types; ii) an arbitrarily defined
window size parameter (whose choice greatly af-
fects outcomes); iii) lack of clear intuition; iv) in-
applicability to multiply-coded corpora; and v) re-
liance upon a ?true? reference segmentation.
In this paper, we propose a new method of
comparing two segmentations, called segmentation
similarity 1 (S), that: i) equally penalizes all er-
ror types (unless explicitly configured otherwise);
ii) appropriately responds to scenarios tested; iii) de-
fines no arbitrary parameters; iv) is intuitive; and
v) is adapted for use in a variety of popular inter-
annotator agreement coefficients to handle multiply-
coded corpora; and vi) does not rely upon a ?true?
reference segmentation (it is symmetric). Capitaliz-
ing on the adapted inter-annotator agreement coeffi-
cients, the relative difficulty that human segmenters
have with various segmentation tasks can now be
quantified. We also propose that these coefficients
can be used to evaluate and compare automatic seg-
mentation methods in terms of human agreement.
This paper is organized as follows. In Sec-
tion 2, we review segmentation evaluation and inter-
annotator agreement. In Section 3, we present S and
1A software implementation of segmentation similarity (S)
is available at http://nlp.chrisfournier.ca/
152
inter-annotator agreement coefficient adaptations. In
Section 4, we evaluate S and WindowDiff in vari-
ous scenarios and simulations, and upon a multiply-
coded corpus.
2 Related Work
2.1 Segmentation Evaluation
Precision, recall, and their mean (F?-measure) have
been previously applied to segmentation evaluation.
Precision is the proportion of boundaries chosen that
agree with a reference segmentation, and recall is
the proportion of boundaries chosen that agree with
a reference segmentation out of all boundaries in the
reference and hypothesis (Pevzner and Hearst, 2002,
p. 3). For segmentation, these metrics are unsuitable
because they penalize near-misses of boundaries as
full-misses, causing them to drastically overestimate
the error. Near-misses are prevalent in segmentation
and can account for a large proportion of the errors
produced by a coder, and as inter-annotator agree-
ment often shows, they do not reflect coder error,
but the difficulty of the task.
Pk (Beeferman and Berger, 1999, pp. 198?200)2
is a window-based metric which attempts to solve
the harsh near-miss penalization of precision, recall,
and F?-measure. In Pk, a window of size k, where
k is defined as half of the mean reference segment
size, is slid across the text to compute penalties.
A penalty of 1 is assigned for each window whose
boundaries are detected to be in different segments
of the reference and hypothesis segmentations, and
this count is normalized by the number of windows.
Pevzner and Hearst (2002, pp. 5?10) highlighted
a number of issues with Pk, specifically that: i) False
negatives (FNs) are penalized more than false pos-
itives (FPs); ii) It does not penalize FPs that fall
within k units of a reference boundary; iii) Its sen-
sitivity to variations in segment size can cause it to
linearly decrease the penalty for FPs if the size of
any segments fall below k; and iv) Near-miss errors
are too harshly penalized.
To attempt to mitigate the shortcomings of Pk,
Pevzner and Hearst (2002, p. 10) proposed a
modified metric which changed how penalties were
2Pk is a modification of P? (Beeferman et al, 1997, p. 43).
Other modifications such as TDT Cseg (Doddington, 1998, pp.
5?6) have been proposed, but Pk has seen greater usage.
counted, named WindowDiff (WD). A window of
size k is still slid across the text, but now penal-
ties are attributed to windows where the number of
boundaries in each segmentation differs (see Equa-
tion 1, where b(Rij) and b(Hij) represents the num-
ber of boundaries within the segments in a window
of size k from position i to j, and N the number of
sentences plus one), with the same normalization.
WD(R,H) =
1
N ? k
N?k?
i=1,j=i+k
(|b(Rij)?b(Hij)| > 0) (1)
WindowDiff is able to reduce, but not eliminate,
sensitivity to segment size, gives more equal weights
to both FPs and FNs (FNs are, in effect, penalized
less3), and is able to catch mistakes in both small
and large segments. It is not without issues though;
Lamprier et al (2007) demonstrated that WindowD-
iff penalizes errors less at the beginning and end of
a segmentation (this is corrected by padding the seg-
mentation at each end by size k). Additionally, vari-
ations in the window size k lead to difficulties in in-
terpreting and comparing WindowDiff?s values, and
the intuition of the method remains vague.
Franz et al (2007) proposed measuring perfor-
mance in terms of the number of words that are FNs
and FPs, normalized by the number of word posi-
tions present (see Equation 2).
RFN =
1
N
?
w
FN(w), RFP =
1
N
?
w
FP (w) (2)
RFN and RFP have the advantage that they take
into account the severity of an error in terms of seg-
ment size, allowing them to reflect the effects of er-
roneously missing, or added, words in a segment
better than window based metrics. Unfortunately,
RFN and RFP suffer from the same flaw as preci-
sion, recall, and F?-measure in that they do not ac-
count for near misses.
2.2 Inter-Annotator Agreement
The need to ascertain the agreement and reliabil-
ity between coders for segmentation was recognized
3Georgescul et al (2006, p. 48) note that both FPs and FNs
are weighted by 1/N?k, and although there are ?equiprobable
possibilities to have a [FP] in an interval of k units?, ?the total
number of equiprobable possibilities to have a [FN] in an inter-
val of k units is smaller than (N?k)?, making the interpretation
of a full miss as a FN less probable than as a FP.
153
by Passonneau and Litman (1993), who adapted the
percentage agreement metric by Gale et al (1992,
p. 254) for usage in segmentation. This percentage
agreement metric (Passonneau and Litman, 1993, p.
150) is the ratio of the total observed agreement of a
coder with the majority opinion for each boundary
over the total possible agreements. This measure
failed to take into account chance agreement, or to
less harshly penalize near-misses.
Hearst (1997) collected segmentations from 7
coders while developing the automatic segmenter
TextTiling, and reported mean ? (Siegel and Castel-
lan, 1988) values for coders and automatic seg-
menters (Hearst, 1997, p. 56). Pairwise mean ?
scores were calculated by comparing a coder?s seg-
mentation against a reference segmentation formu-
lated by the majority opinion strategy used in Pas-
sonneau and Litman (1993, p. 150) (Hearst, 1997,
pp. 53?54). Although mean ? scores attempt to
take into account chance agreement, near misses are
still unaccounted for, and use of Siegel and Castel-
lan?s (1988) ? has declined in favour of other coeffi-
cients (Artstein and Poesio, 2008, pp. 555?556).
Artstein and Poesio (2008) briefly touch upon
recommendations for coefficients for segmentation
evaluation, and though they do not propose a mea-
sure, they do conjecture that a modification of a
weighted form of ? (Krippendorff, 1980; Krippen-
dorff, 2004) using unification and WindowDiff may
suffice (Artstein and Poesio, 2008, pp. 580?582).
3 Segmentation Similarity
For discussing segmentation, a segment?s size (or
mass) is measured in units, the error is quantified
in potential boundaries (PBs), and we have adopted
a modified form of the notation used by Artstein and
Poesio (2008), where the set of:
? Items is {i|i ? I} with cardinality i;
? Categories is {k|k ? K} with cardinality k;
? Coders is {c|c ? C} with cardinality c;
? Segmentations of an item i by a coder c is {s|s ?
S}, where when sic is specified with only one sub-
script, it denotes sc, for all relevant items (i); and
? Types of segmentation boundaries is {t|t ? T} with
cardinality t.
3.1 Sources of Dissimilarity
Linear segmentation has three main types of errors:
1. s1 contains a boundary that is off by n PBs in s2;
2. s1 contains a boundary that s2 does not; or
3. s2 contains a boundary that s1 does not.
These types of errors can be seen in Figure 1, and
are conceptualized as a pairwise transposition of a
boundary for error 1, and the insertion or deletion
(depending upon your perspective) of a boundary for
errors 2 and 3. Since we do not designate either seg-
mentation as a reference or hypothesis, we refer to
insertions and deletions both as substitutions.
s1
s2
1 3 2
Figure 1: Types of segmentations errors
It is important to not penalize near misses as full
misses in many segmentation tasks because coders
often agree upon the existence of a boundary, but
disagree upon its exact location. In the previous sce-
nario, assigning a full miss would mean that even a
boundary loosely agreed-upon, as in Figure 1, error
1, would be regarded as completely disagreed-upon.
3.2 Edit Distance
In S, concepts from Damereau-Levenshtein edit dis-
tance (Damereau, 1964; Levenshtein, 1966) are ap-
plied to model segmentation edit distance as two op-
erations: substitutions and transpositions.4 These
two operations represent full misses and near misses,
respectively. Using these two operations, a new
globally-optimal minimum edit distance is applied
to a pair of sequences of sets of boundaries to model
the sources of dissimilarity identified earlier.5
Near misses that are remedied by transposition are
penalized as b PBs of error (where b is the number
of boundaries transposed), as opposed to the 2b PBs
of errors by which they would be penalized if they
were considered to be two separate substitution op-
erations. Transpositions can also be considered over
n > 2 PBs (n-wise transpositions). This is useful
if, for a specific task, near misses of up to n PBs are
not to be penalized as full misses (default n = 2).
The error represented by the two operations can
also be scaled (i.e., weighted) from 1 PB each to a
4Beeferman et al (1997, p. 42) briefly mention using an edit
distance without transpositions, but discard it in favour of P?.
5For multiple boundaries, an add/del operation is added, and
transpositions are considered only within boundary types.
154
fraction. The distance over which an n-wise trans-
position occurred can also be used in conjunction
with the scalar operation weighting so that a transpo-
sition is weighted using the function in Equation 3.
te(n, b) = b? (1/b)
n?2 where n ? 2 and b > 0 (3)
This transposition error function was chosen so
that, in an n-wise transposition where n = 2 PBs
and the number of boundaries transposed b = 2, the
penalty would be 1 PB, and the maximum penalty as
limn?? te(n) would be b PBs, or in this case 2 PBs
(demonstrated later in Figure 5b).
3.3 Method
In S, we conceptualize the entire segmentation, and
individual segments, as having mass (i.e., unit mag-
nitude/length), and quantify similarity between two
segmentations as the proportion of boundaries that
are not transformed when comparing segmentations
using edit distance, essentially using edit distance as
a penalty function and scaling penalties by segmen-
tation size. S is a symmetric function that quantifies
the similarity between two segmentations as a per-
centage, and applies to any granularity or segmenta-
tion unit (e.g., paragraphs, sentences, clauses, etc.).
Consider a somewhat contrived example
containing?for simplicity and brevity?only one
boundary type (t = 1). First, a segmentation must
be converted into a sequence of segment mass
values (see Figure 2).
0 1 2 3 4 5 6
? 1 3 2
Figure 2: Annotation of segmentation mass
Then, a pair of segmentations are converted into
parallel sequences of boundary sets, where each set
contains the types of boundaries present at that po-
tential boundary location (if there is no boundary
present, then the set is empty), as in Figure 3.
s1
s2
1 2 2 3 3 1 2
{1} {} {1} {} {1} {} {} {1} {} {} {1} {1} {}
1 2 1 2 6 2
{1} {} {1} {1} {} {1} {} {} {} {} {} {1} {}
Figure 3: Segmentations annotated with mass and their
corresponding boundary set sequences
The edit distance is calculated by first identify-
ing all potential substitution operations that could
occur (in this case 5). A search for all potential n-
wise transpositions that can be made over n adja-
cent sets between the sequences is then performed,
searching from the beginning of the sequence to the
end, keeping only those transpositions which do not
overlap and which result in transposing the most
boundaries between the sequences (to minimize the
edit distance). In this case, we have only one non-
overlapping 2-wise transposition. We then subtract
the number of boundaries involved in transpositions
between the sequences (2 boundaries) from the num-
ber of substitutions, giving us an edit distance of 4
PBs: 1 transposition PB and 3 substitution PBs.
s1
s2
1 2 2 3 3 1 2
{1} {} {1} {} {1} {} {} {1} {} {} {1} {1} {}
1 2 1 2 6 2
{1} {} {1} {1} {} {1} {} {} {} {} {} {1} {}
Transposition
Sub. Sub. Sub.
Figure 4: Edit operations performed on boundary sets
Edit distance, and especially the number of oper-
ations of each type performed, is useful in identi-
fying the number of full and near misses that have
occurred?which indicates whether one?s choice of
transposition window size n is either too generous
or too harsh. Edit distance as a penalty does not
incorporate information on the severity of an error
with respect to the size of a segment, and is not an
easily comparable value without some form of nor-
malization. To account for these issues, we define
S so that boundary edit distance is used to subtract
penalties for each edit operation that occurs, from
the number of potential boundaries in a segmenta-
tion, normalizing this value by the total number of
potential boundaries in a segmentation.
S(si1, si2) =
t ?mass(i)? t? d(si1, si1, T )
t ?mass(i)? t
(4)
S, as shown in Equation 4, scales the mass of the
item by the cardinality of the set of boundary types
(t) because the edit distance function d(si1, si1, T )
will return a value of [0, t ? mass(i)] PBs, where
t ? Z+?while subtracting the edit distance and t.6
6The number of potential boundaries in a segmentation si
155
The numerator is normalized by the total number
of potential boundaries per boundary type. This re-
sults in a function with a range of [0, 1]. It returns 0
when one segmentation contains no boundaries, and
the other contains the maximum number of possible
boundaries. It returns 1 when both segmentations
are identical.
Using the default configuration of this equation,
S = 9/13 = 0.6923, a very low similarity, which
WindowDiff also agrees upon (1?WD = 0.6154).
The edit-distance function d(si1, si1, T ) can also be
assigned values of the range [0, 1] as scalar weights
(wsub, wtrp) to reduce the penalty attributed to par-
ticular edit operations, and configured to use a trans-
position error function (Equation 3, used by default).
3.4 Evaluating Automatic Segmenters
Coders often disagree in segmentation tasks (Hearst,
1997, p. 56), making it improbable that a single,
correct, reference segmentation could be identified
from human codings. This improbability is the re-
sult of individual coders adopting slightly differ-
ent segmentation strategies (i.e., different granular-
ity). In light of this, we propose that the best avail-
able evaluation strategy for automatic segmentation
methods is to compare performance against multiple
coders directly, so that performance can be quanti-
fied relative to human reliability and agreement.
To evaluate whether an automatic segmenter
performs on par with human performance, inter-
annotator agreement can be calculated with and
without the inclusion of an automatic segmenter,
where an observed drop in the coefficients would
signify that the automatic segmenter does not per-
form as reliably as the group of human coders.7 This
can be performed independently for multiple auto-
matic segmenters to compare them to each other?
assuming that the coefficients model chance agree-
ment appropriately?because agreement is calculated
(and quantifies reliability) over all segmentations.
3.5 Inter-Annotator Agreement
Similarity alone is not a sufficiently insightful mea-
sure of reliability, or agreement, between coders.
with t boundary types is t ?mass(i)? t.
7Similar to how human competitiveness is ascertained by
Medelyan et al (2009, pp. 1324?1325) and Medelyan (2009,
pp. 143?145) by comparing drops in inter-indexer consistency.
Chance agreement occurs in segmentation when
coders operating at slightly different granularities
agree due to their codings, and not their own in-
nate segmentation heuristics. Inter-annotator agree-
ment coefficients have been developed that assume a
variety of prior distributions to characterize chance
agreement, and to attempt to offer a way to iden-
tify whether agreement is primarily due to chance,
or not, and to quantify reliability.
Artstein and Poesio (2008) note that most of a
coder?s judgements are non-boundaries. The class
imbalance caused by segmentations often contain-
ing few boundaries, paired with no handling of near
misses, causes most inter-annotator agreement co-
efficients to drastically underestimate agreement on
segmentations. To allow for agreement coefficients
to account for near misses, we have adapted S for use
with Cohen?s ?, Scott?s pi, Fleiss?s multi-pi (pi?), and
Fleiss?s multi-? (??), which are all coefficients that
range from [Ae/1?Ae , 1], where 0 indicates chance
agreement, and 1 perfect agreement. All four coeffi-
cients have the general form:
?, pi, ??, and pi? =
Aa ? Ae
1? Ae
(5)
For each agreement coefficient, the set of cate-
gories is defined as solely the presence of a bound-
ary (K = {segt|t ? T}), per boundary type (t).
This category choice is similar to those chosen by
Hearst (1997, p. 53), who computed chance agree-
ment in terms of the probability that coders would
say that a segment boundary exists (segt), and the
probability that they would not (unsegt). We have
chosen to model chance agreement only in terms of
the presence of a boundary, and not the absence,
because coders have only two choices when seg-
menting: to place a boundary, or not. Coders do
not place non-boundaries. If they do not make a
choice, then the default choice is used: no boundary.
This default option makes it impossible to determine
whether a segmenter is making a choice by not plac-
ing a boundary, or whether they are not sure whether
a boundary is to be placed.8 For this reason, we
only characterize chance agreement between coders
in terms of one boundary presence category per type.
8This could be modelled as another boundary type, which
would be modelled in S by the set of boundary types T .
156
3.5.1 Scott?s pi
Proposed by Scott (1955), pi assumes that chance
agreement between coders can be characterized as
the proportion of items that have been assigned to
category k by both coders (Equation 7). We cal-
culate agreement (Apia ) as pairwise mean S (scaled
by each item?s size) to enable agreement to quantify
near misses leniently, and chance agreement (Apie )
can be calculated as in Artstein and Poesio (2008).
Apia =
?
i?I mass(i) ? S(si1, si2)?
i?I mass(i)
(6)
Apie =
?
k?K
(
Ppie (k)
)2
(7)
We calculate chance agreement per category as
the proportion of boundaries (segt) assigned by all
coders over the total number of potential boundaries
for segmentations, as shown in Equation 8.
Ppie (segt) =
?
c?C
?
i?I |boundaries(t, sic)|
c ?
?
i?I
(
mass(i)? 1
) (8)
This adapted coefficient appropriately estimates
chance agreement in situations where there no in-
dividual coder bias.
3.5.2 Cohen?s ?
Proposed by Cohen (1960), ? characterizes
chance agreement as individual distributions per
coder, calculated as shown in Equations 9-10 using
our definition of agreement (Apia ) as shown earlier.
A?a = A
pi
a (9)
A?e =
?
k?K
P?e (k|c1) ? P
?
e (k|c2) (10)
We calculate category probabilities as in Scott?s
pi, but per coder, as shown in Equation 11.
P?e (segt|c) =
?
i?I |boundaries(t, sic)|
?
i?I
(
mass(i)? 1
) (11)
This adapted coefficient appropriately estimates
chance agreement for segmentation evaluations
where coder bias is present.
3.5.3 Fleiss?s Multi-pi
Proposed by Fleiss (1971), multi-pi (pi?) adapts
Scott?s pi for multiple annotators. We use Artstein
and Poesio?s (2008, p. 564) proposal for calculat-
ing actual and expected agreement, and because all
coders rate all items, we express agreement as pair-
wise mean S between all coders as shown in Equa-
tions 12-13, adapting only Equation 12.
Api
?
a =
1
(c
2
)
c?1?
m=1
c?
n=m+1
?
i?I mass(i) ? S(sim, sin)
?
i?I
(
mass(i)? 1
) (12)
Api
?
e =
?
k?K
(
Ppie (k)
)2
(13)
3.5.4 Fleiss?s Multi-?
Proposed by Davies and Fleiss (1982), multi-?
(??) adapts Cohen?s ? for multiple annotators. We
use Artstein and Poesio?s (2008, extended version)
proposal for calculating agreement just as in pi?, but
with separate distributions per coder as shown in
Equations 14-15.
A?
?
a = A
pi?
a (14)
A?
?
e =
?
k?K
(
1
(c
2
)
c?1?
m=1
c?
n=m+1
P?e (k|cm) ? P
?
e (k|cn)
)
(15)
3.6 Annotator Bias
To identify the degree of bias in a group of coders?
segmentations, we can use a measure of variance
proposed by Artstein and Poesio (2008, p. 572) that
is quantified in terms of the difference between ex-
pected agreement when chance is assumed to vary
between coders, and when it is assumed to not.
B = Api
?
e ?A
??
e (16)
4 Experiments
To demonstrate the advantages of using S, as op-
posed to WindowDiff (WD), we compare both met-
rics using a variety of contrived scenarios, and then
compare our adapted agreement coefficients against
pairwise meanWD9 for the segmentations collected
by Kazantseva and Szpakowicz (2012).
In this section, because WD is a penalty-based
metric, it is reported as 1?WD so that it is easier
to compare against S values. When reported in this
way, 1?WD and S both range from [0, 1], where 1
represents no errors and 0 represents maximal error.
9Permuted, and with window size recalculated for each pair.
157
0 20 40 60 80 100
0
0.2
0.4
0.6
0.8
1
Number of full misses / false positives
1?WD
S
(a) Increasing the number of full misses,
or FPs, where k = 25 for WD
0 2 4 6 8 10
0.7
0.8
0.9
1
Distance between boundaries in each seg. (units)
1?WD
S(n = 3)
S(n = 5,scale)
S(n = 5,wtrp = 0)
(b) Increasing the distance between two
boundaries considered to be a near miss
until metrics consider them a full miss
0 20 40 60 80 100
0.2
0.4
0.6
0.8
1
Segmentation mass (m)
1?WD
S
k/m
(c) Increasing the mass m of segmenta-
tions configured as shown in Figure 10
showing the effect of k on 1?WD
Figure 5: Responses of 1?WD and S to various segmentation scenarios
4.1 Segmentation Cases
Maximal versus minimal segmentation When
proposing a new metric, its reactions to extrema
must be illustrated, for example when a maximal
segmentation is compared to a minimal segmenta-
tion, as shown in Figure 6. In this scenario, both
1?WD and S appropriately identify that this case
represents maximal error, or 0. Though not shown
here, both metrics also report a similarity of 1.0
when identical segmentations are compared.
s1
s2
14
1 1 1 1 1 1 1 1 1 1 1 1 1 1
Figure 6: Maximal versus minimal seg. masses
Full misses For the most serious source of error,
full misses (i.e., FPs and FNs), both metrics appro-
priately report a reduction in similarity for cases
such as Figure 7 that is very similar (1?WD =
0.8462, S= 0.8461). Where the two metrics differ
is when this type of error is increased.
s1
s2
1 2 2 2 4 2 1
1 2 8 2 1
Figure 7: Full misses in seg. masses
S reacts to increasing full misses linearly, whereas
WindowDiff can prematurely report a maximal
number of errors. Figure 5a demonstrates this ef-
fect, where for each iteration we have taken seg-
mentations of 100 units of mass with one matching
boundary at the first hypothesis boundary position,
and uniformly increased the number of internal hy-
pothesis segments, giving us 1 matching boundary,
and [0, 98] FPs. This premature report of maximal
error (at 7 FP) by WD is caused by the window size
(k = 25) being greater than all of the internal hy-
pothesis segment sizes, making all windows penal-
ized for containing errors.
Near misses When dealing with near misses, the
values of both metrics drop (1?WD = 0.8182,
S = 0.9231), but to greatly varying degrees. In
comparison to full misses, WindowDiff penalizes a
near miss, like that in Figure 8, far more than S.
This difference is due to the distance between the
two boundaries involved in a near miss; S shows,
in this case, 1 PB of error until it is outside of the
n-wise transposition window (where n = 2 PBs),
at which point it is considered an error of not one
transposition, but two substitutions (2 PBs).
s1
s2
6 8
7 7
Figure 8: Near misses in seg. masses
If we wanted to completely forgive near misses
up to n PBs, we could set the weighting of trans-
positions in S to wtrp = 0. This is useful if a spe-
cific segmentation task accepts that near misses are
very probable, and that there is little cost associated
with a near miss in a window of n PBs. We can
also set n to a high number, i.e., 5 PBs, and use the
scaled transposition error (te) function (Equation 3)
to slowly increase the error from b = 1 PB to b = 2
PBs, as shown in Figure 5b, which shows how both
158
Scenario 1: FN, p = 0.5 Scenario 2: FP, p = 0.5 Scenario 3: FP and FN, p = 0.5
(20,30) (15,35) (20,30) (15,35) (20,30) (15,35)
WD 0.2340? 0.0113 0.2292? 0.0104 0.2265? 0.0114 0.2265? 0.0111 0.3635? 0.0126 0.3599? 0.0117
S 0.9801? 0.0006 0.9801? 0.0006 0.9800? 0.0006 0.9800? 0.0006 0.9605? 0.0009 0.9603? 0.0009
(10,40) (5,45) (10,40) (5,45) (10,40) (5,45)
WD 0.2297? 0.0105 0.2206? 0.0079 0.2256? 0.0102 0.2184? 0.0069 0.3516? 0.0110 0.3254? 0.0087
S 0.9799? 0.0007 0.9796? 0.0007 0.9800? 0.0006 0.9796? 0.0007 0.9606? 0.0010 0.9598? 0.0011
Table 1: Stability of mean (with standard deviation) values of WD and S in three different scenarios, each defining
the: probability of a false positive (FP), false negative (FN), or both. Each scenario varies the range of internal
segment sizes (e.g., (20, 30)). Low standard deviation and similar within-scenario means demonstrates low sensitivity
to variations in internal segment size.
metrics react to increases in the distance between a
near miss in a segment of 25 units. These configura-
tions are all preferable to the drop of 1?WD.
4.2 Segmentation Mass Scale Effects
It is important for a segmentation evaluation met-
ric to take into account the severity of an error in
terms of segment size. An error in a 100 unit seg-
ment should be considered less severe than an er-
ror in a 2 unit segment, because an extra boundary
placed within a 100 unit segment (e.g., Figure 9 with
m = 100) could probably indicate a weak boundary,
whereas in a 4 unit segment the probability that an
extra boundary exists right next to two agreed-upon
boundaries should be small for most tasks, meaning
that it is probable that the extra boundary is an error,
and not a weak boundary.
s1
s2
m/4
m/2
m/4
m/4
m/4
m/4
m/4
Figure 9: Two segmentations of mass m with a full miss
To demonstrate that S is sensitive to segment size,
Figure 5c shows how S and 1?WD respond when
comparing segmentations configured as shown in
Figure 10 (containing one match and one full miss)
with linearly increasing mass (4 ? m ? 100).
1?WD will eventually indicate 0.68, whereas S ap-
propriately discounts the error as mass is increased,
approaching 1 as limm??. 1?WD behaves in this
way because of how it calculates its window size pa-
rameter, k, which is plotted as k/m to show how its
value influences 1?WD.
s1
s2
m/4 m? (
m/4)
m/4
m/4
m/2
Figure 10: Two segmentations of mass m compared with
increasing m in Figure 5c (s1 as reference)
4.3 Variation in Segment Sizes
When Pevzner and Hearst (2002) proposed WD,
they demonstrated that it was not as sensitive as
Pk to variations in the size of segments inside a
segmentation. To show this, they simulated how
WD performs upon a segmentation comprised of
1000 segments with four different uniformly dis-
tributed ranges of internal segment sizes (keeping
the mean at approximately 25 units) in compari-
son to a hypothesis segmentation with errors (false
positives, false negatives, and both) uniformly dis-
tributed within segments (Pevzner and Hearst, 2002,
pp. 11?12). 10 trials were performed for each seg-
ment size range and error probability, with 100 hy-
potheses generated per trial. Recreating this simu-
lation, we compare the stability of S in comparison
to WD, as shown in Table 1. We can see that WD
values show substantial within-scenario variation for
each segment size range, and larger standard devia-
tions, than S.
4.4 Inter-Annotator Agreement Coefficients
Here, we demonstrate the adapted inter-annotator
agreement coefficients upon topical paragraph-level
segmentations produced by 27 coders of 20 chapters
from the novel The Moonstone by Wilkie Collins
collected by Kazantseva and Szpakowicz (2012).
Figure 11 shows a heat map of each chapter where
the percentage of coders who agreed upon each po-
tential boundary is represented. Comparing this heat
map to the inter-annotator agreement coefficients in
Table 2 allows us to better understand why certain
chapters have lower reliability.
Chapter 1 has the lowest pi?S score in the table, and
also the highest bias (BS). One of the reasons for
this low reliability can be attributed to the chapter?s
small mass (m) and few coders (|c|), which makes
it more sensitive to chance agreement. Visually, the
159
1
2
3
4
5
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
1.0
0.0
Ch
ap
ter
s
Potential boundary positions (between paragraphs)
Coderagreementperpotentialboundary(%
)
Figure 11: Heat maps for the segmentations of each chap-
ter showing the percentage of coders who agree upon
boundary positions (darker shows higher agreement)
predominance of grey indicates that, although there
are probably two boundaries, their exact location is
not very well agreed upon. In this case, 1?WD
incorrectly indicates the opposite, that this chapter
may have relatively moderate reliability, because it
is not corrected for chance agreement.
1?WD indicates that the lowest reliability is
found in Chapter 19. pi?S indicates that this is one
of the higher agreement chapters, and looking at the
heat map, we can see that it does not contain any
strongly agreed upon boundaries. In this chapter,
there is little opportunity to agree by chance due to
the low number of boundaries (|b|) placed, and be-
cause the judgements are tightly clustered in a fair
amount of mass, the S component of pi?S appropri-
ately takes into account the near misses observed
and gives it a high reliability score.
Chapter 17 received the highest pi?S in the table,
which is another example of how tight clustering of
boundary choices in a large mass leads pi?S to appro-
priately indicate high reliability despite that there are
not as many individual highly-agreed-upon bound-
aries, whereas 1?WD indicates that there is low re-
liability. 1?WD and pi?S both agree, however, that
chapter 16 has high reliability.
Despite WindowDiff?s sensitivity to near misses,
it is evident that its pairwise mean cannot be used
to consistently judge inter-annotator agreement, or
reliability. S demonstrates better versatility when
accounting for near misses, and when used as part
of inter-annotator agreement coefficients, it prop-
erly takes into account chance agreement. Follow-
ing Artstein and Poesio?s (2008, pp. 590?591) rec-
Ch. pi?S ?
?
S BS 1?WD |c| |b| m
1 0.7452 0.7463 0.0039 0.6641? 0.1307 4 13 13
2 0.8839 0.8840 0.0009 0.7619? 0.1743 6 20 15
3 0.8338 0.8340 0.0013 0.6732? 0.1559 4 23 38
4 0.8414 0.8417 0.0019 0.6019? 0.2245 4 25 46
5 0.8773 0.8774 0.0003 0.6965? 0.1106 6 34 42
7 0.8132 0.8133 0.0002 0.6945? 0.1822 6 20 15
8 0.8495 0.8496 0.0006 0.7505? 0.0911 6 48 39
9 0.8104 0.8105 0.0009 0.6502? 0.1319 6 35 33
10 0.9077 0.9078 0.0002 0.7729? 0.0770 6 56 83
11 0.8130 0.8135 0.0022 0.6189? 0.1294 4 73 111
12 0.9178 0.9178 0.0001 0.6504? 0.1277 6 40 102
13 0.9354 0.9354 0.0002 0.5660? 0.2187 6 21 58
14 0.9367 0.9367 0.0001 0.7128? 0.1744 6 35 70
15 0.9344 0.9344 0.0001 0.7291? 0.0856 6 40 97
16 0.9356 0.9356 0.0000 0.8016? 0.0648 6 41 69
17 0.9447 0.9447 0.0002 0.6717? 0.2044 5 23 70
18 0.8921 0.8922 0.0005 0.5998? 0.1614 5 28 59
19 0.9021 0.9022 0.0009 0.4796? 0.2666 5 15 36
20 0.8590 0.8591 0.0003 0.6657? 0.1221 6 21 21
21 0.9286 0.9286 0.0004 0.6255? 0.2003 5 17 60
Table 2: S-based inter-annotator agreements and pairwise
mean 1?WD and standard deviation with the number of
coders, boundaries, and mass per chapter
ommendation, and given the low bias (mean coder
groupBS = 0.0061?0.0035), we propose reporting
reliability using pi? for this corpus, where the mean
coder group pi?S for the corpus is 0.8904 ? 0.0392
(counting 1039 full and 212 near misses).
5 Conclusion and Future Work
We have proposed a segmentation evaluation met-
ric which solves the key problems facing segmenta-
tion analysis today, including an inability to: appro-
priately quantify near misses when evaluating auto-
matic segmenters and human performance; penalize
errors equally (or, with configuration, in a manner
that suits a specific segmentation task); compare an
automatic segmenter directly against human perfor-
mance; require a ?true? reference; and handle mul-
tiple boundary types. Using S, task-specific eval-
uation of automatic and human segmenters can be
performed using multiple human judgements unhin-
dered by the quirks of window-based metrics.
In current and future work, we will show how S
can be used to analyze hierarchical segmentations,
and illustrate how to apply S to linear segmentations
containing multiple boundary types.
Acknowledgments
We thank Anna Kazantseva for her invaluable feed-
back and corpora, and Stan Szpakowicz, Martin Sca-
iano, and James Cracknell for their feedback.
160
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596. MIT Press, Cam-
bridge, MA, USA.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text Segmentation Using Exponential Models.
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, 2:35?46.
Association for Computational Linguistics, Strouds-
burg, PA, USA.
Doug Beeferman and Adam Berger. 1999. Statisti-
cal models for text segmentation. Machine learning,
34(1?3):177?210. Springer Netherlands, NL.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46. Sage, Beverly Hills, CA, USA.
Frederick J. Damerau. 1964. A technique for computer
detection and correction of spelling errors. Commu-
nications of the ACM, 7(3):171?176. Association for
Computing Machinery, Stroudsburg, PA, USA.
Mark Davies and Joseph L. Fleiss. 1982. Measur-
ing agreement for multinomial data. Biometrics,
38(4):1047?1051. Blackwell Publishing Inc, Oxford,
UK.
George R. Doddington. 1998. The topic detection and
tracking phase 2 (TDT2) evaluation plan. DARPA
Broadcast News Transcription and Understanding
Workshop, pp. 223?229. Morgan Kaufmann, Waltham,
MA, USA.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382. American Psychological Association,
Washington, DC, USA.
Martin, Franz, J. Scott McCarley, and Jian-Ming Xu.
2007. User-oriented text segmentation evaluation
measure. Proceedings of the 30th annual international
ACM SIGIR conference on Research and development
in information retrieval, pp. 701?702. Association for
Computing Machinery, Stroudsburg, PA, USA.
William Gale, Kenneth Ward Church, and David
Yarowsky. 1992. Estimating upper and lower bounds
on the performance of word-sense disambiguation pro-
grams. Proceedings of the 30th annual meeting of
the Association for Computational Linguistics, pp.
249?256. Association for Computational Linguistics,
Stroudsburg, PA, USA.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. An analysis of quantitative aspects in
the evaluation of thematic segmentation algorithms.
Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, pp. 144?151. Association for
Computational Linguistics, Stroudsburg, PA, USA.
Marti A. Hearst. 1997. TextTiling: Segmenting Text into
Multi-paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64. MIT Press, Cambridge, MA,
USA.
Anna Kazantseva and Stan Szpakowicz. 2012. Topical
Segmentation: a Study of Human Performance. Pro-
ceedings of Human Language Technologies: The 2012
Annual Conference of the North American Chapter
of the Association for Computational Linguistics. As-
sociation for Computational Linguistics, Stroudsburg,
PA, USA.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology, Chapter 12. Sage, Beverly
Hills, CA, USA.
Klaus Krippendorff. 2004. Content Analysis: An Intro-
duction to Its Methodology, Chapter 11. Sage, Beverly
Hills, CA, USA.
Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and
Frederic Saubion 2007. On evaluation methodologies
for text segmentation algorithms. Proceedings of the
19th IEEE International Conference on Tools with Ar-
tificial Intelligence, 2:19?26. IEEE Computer Society,
Washington, DC, USA.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710. American Institute
of Physics, College Park, MD, USA.
Olena Medelyan. 2009. Human-competitive automatic
topic indexing. PhD Thesis. University of Waikato,
Waikato, NZ.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pp. 1318?1327. Association for Compu-
tational Linguistics, Stroudsburg, PA, USA.
Rebecca J. Passonneau and Diane J. Litman. 1993.
Intention-based segmentation: human reliability and
correlation with linguistic cues. Proceedings of the
31st annual meeting of the Association for Computa-
tional Linguistics, pp. 148?155). Association for Com-
putational Linguistics, Stroudsburg, PA, USA.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36. MIT
Press, Cambridge, MA, USA.
William A. Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public Opinion
Quarterly, 19(3):321?325. American Association for
Public Opinion Research, Deerfield, IL, USA.
Sidney Siegel and N. John Castellan, Jr. 1988. Non-
parametric Statistics for the Behavioral Sciences. 2nd
Edition, Chapter 9.8. McGraw-Hill, New York, USA.
161
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1702?1712,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evaluating Text Segmentation using Boundary Edit Distance
Chris Fournier
University of Ottawa
Ottawa, ON, Canada
cfour037@eecs.uottawa.ca
Abstract
This work proposes a new segmentation
evaluation metric, named boundary simi-
larity (B), an inter-coder agreement coef-
ficient adaptation, and a confusion-matrix
for segmentation that are all based upon an
adaptation of the boundary edit distance in
Fournier and Inkpen (2012). Existing seg-
mentation metrics such as Pk, WindowD-
iff, and Segmentation Similarity (S) are
all able to award partial credit for near
misses between boundaries, but are biased
towards segmentations containing few or
tightly clustered boundaries. Despite S?s
improvements, its normalization also pro-
duces cosmetically high values that over-
estimate agreement & performance, lead-
ing this work to propose a solution.
1 Introduction
Text segmentation is the task of splitting text into
segments by placing boundaries within it. Seg-
mentation is performed for a variety of purposes
and is often a pre-processing step in a larger task.
E.g., text can be topically segmented to aid video
and audio retrieval (Franz et al, 2007), question
answering (Oh et al, 2007), subjectivity analysis
(Stoyanov and Cardie, 2008), and even summa-
rization (Haghighi and Vanderwende, 2009).
A variety of segmentation granularities, or
atomic units, exist, including segmentations at the
morpheme (e.g., Sirts and Aluma?e 2012), word
(e.g., Chang et al 2008), sentence (e.g., Rey-
nar and Ratnaparkhi 1997), and paragraph (e.g.,
Hearst 1997) levels. Between each atomic unit lies
the potential to place a boundary. Segmentations
can also represent the structure of text as being
organized linearly (e.g., Hearst 1997), hierarchi-
cally (e.g., Eisenstein 2009), etc. Theoretically,
segmentations could also contain varying bound-
ary types, e.g., two boundary types could differen-
tiate between act and scene breaks in a play.
Because of its value to natural language pro-
cessing, various text segmentation tasks have
been automated such as topical segmentation?
for which a variety of automatic segmenters exist
(e.g., Hearst 1997, Malioutov and Barzilay 2006,
Eisenstein and Barzilay 2008, and Kazantseva and
Szpakowicz 2011). This work addresses how to
best select an automatic segmenter and which seg-
mentation metrics are most appropriate to do so.
To select an automatic segmenter for a particu-
lar task, a variety of segmentation evaluation met-
rics have been proposed, including Pk (Beefer-
man and Berger, 1999, pp. 198?200), WindowDiff
(WD; Pevzner and Hearst 2002, p. 10), and most
recently Segmentation Similarity (S; Fournier and
Inkpen 2012, p. 154?156). Each of these met-
rics have a variety of flaws: Pk and WindowD-
iff both under-penalize errors at the beginning of
segmentations (Lamprier et al, 2007) and have a
bias towards favouring segmentations with few or
tightly-clustered boundaries (Niekrasz and Moore,
2010), while S produces overly optimistic values
due to its normalization (shown later).
To overcome the flaws of existing text segmen-
tation metrics, this work proposes a new series of
metrics derived from an adaptation of boundary
edit distance (Fournier and Inkpen, 2012, p. 154?
156). This new metric is named boundary similar-
ity (B). A confusion matrix to interpret segmenta-
tion as a classification problem is also proposed,
allowing for the computation of information re-
trieval (IR) metrics such as precision and recall.1
In this work: ?2 reviews existing segmentation
metrics; ?3 proposes an adaptation of boundary
edit distance, a new normalization of it, a new
confusion matrix for segmentation, and an inter-
1An implementation of boundary edit distance, bound-
ary similarity, B-precision, and B-recall, etc. is provided at
http://nlp.chrisfournier.ca/
1702
coder agreement coefficient adaptation; ?4 com-
pares existing segmentation metrics to those pro-
posed herein; ?5 evaluates S and B based inter-
coder agreement; and ?6 compares B, S, and WD
while evaluating automatic segmenters.
2 Related Work
2.1 Segmentation Evaluation
Many early studies evaluated automatic seg-
menters using information retrieval (IR) metrics
such as precision, recall, etc. These metrics looked
at segmentation as a binary classification prob-
lem and were very harsh in their comparisons?no
credit was awarded for nearly missing a boundary.
Near misses occur frequently in segmentation?
although manual coders often agree upon the bulk
of where segment lie, they frequently disagree
upon the exact position of boundaries (Artstein
and Poesio, 2008, p. 40). To attempt to overcome
this issue, both Passonneau and Litman (1993) and
Hearst (1993) conflated multiple manual segmen-
tations into one that contained only those bound-
aries which the majority of coders agreed upon. IR
metrics were then used to compare automatic seg-
menters to this majority solution. Such a major-
ity solution is unsuitable, however, because it does
not contain actual subtopic breaks, but instead the
conflation of a collection of potentially disagree-
ing solutions. Additionally, the definition of what
constitutes a majority is subjective (e.g., Passon-
neau and Litman (1993, p. 150), Litman and Pas-
sonneau (1995), Hearst (1993, p. 6) each used 4/7,
3/7, and > 50%, respectively).
To address the issue of awarding partial
credit for an automatic segmenter nearly missing
a boundary?without conflating segmentations,
Beeferman and Berger (1999, pp. 198?200) pro-
posed a new metric named Pk. Pevzner and Hearst
(2002, pp. 3?4) explain Pk well: a window of size
k?where k is half of the mean manual segmen-
tation length?is slid across both automatic and
manual segmentations. A penalty is awarded if
the window?s edges are found to be in differing
or the same segments within the manual segmen-
tation and the automatic segmentation disagrees.
Pk is the sum of these penalties over all windows.
Measuring the proportion of windows in error al-
lows Pk to penalize a fully missed boundary by
k windows, whereas a nearly missed boundary is
penalized by the distance that it is offset.
Pk was not without issue, however. Pevzner
and Hearst (2002, pp. 5?10) identified that Pk:
i) penalizes false negatives (FNs)2 more than false
positives (FPs); ii) does not penalize full misses
within k units of a reference boundary; iii) penal-
ize near misses too harshly in some situations; and
iv) is sensitive to internal segment size variance.
To solve Pk?s issues, Pevzner and Hearst (2002,
pp. 10) proposed a modification referred to as
WindowDiff (WD). Its major difference is in how
it decides to penalized windows: within a window,
if the number of boundaries in the manual segmen-
tation (Mij) differs from the number of bound-
aries in the automatic segmentation (Aij), then a
penalty is given. The ratio of penalties over win-
dows then represents the degree of error between
the segmentations, as in Equation 1. This change
better allowed WD to: i) penalize FPs and FNs
more equally;3 ii) Not skip full misses; iii) Less
harshly penalize near misses; and iv) Reduce its
sensitivity to internal segment size variance.
WD(M,A) = 1N ? k
N?k?
i=1,j=i+k
(|Mij ?Aij | > 0) (1)
WD did not, however, solve all of the issues
related to window-based segmentation compari-
son. WD, and inherently Pk: i) Penalize er-
rors less at the beginning and end of segmenta-
tions (Lamprier et al, 2007); ii) Are biased to-
wards favouring automatic segmentations with ei-
ther few or tightly-clustered boundaries (Niekrasz
and Moore, 2010); iii) Calculate window size k
inconsistently;4 iv) Are not symmetric5 (meaning
that they cannot be used to produce a pairwise
mean of multiple manual segmentations6).
Segmentation Similarity (S; Fournier and
Inkpen 2012, pp. 154?156) took a different ap-
proach to comparing segmentations. Instead of us-
ing windows, the work proposes a new restricted
edit distance called boundary edit distance which
differentiates between full and near misses. S then
2I.e., a boundary present in the manual but not the auto-
matic segmentation, and the reverse for a false positive.
3Georgescul et al (2006, p. 48) noted that WD interprets
a near miss as a FP probabilistically more than as a FN.
4k must be an integer, but half of a mean may be a frac-
tion, thus rounding must be used, but no rounding method
is specified. It is also not specified whether k should be set
once during a study or recalculated for each comparison?
this work assumes the latter.
5Window size is calculated only upon the manual segmen-
tation, meaning that one must be a manual and other an auto-
matic segmentation.
6This also means that WD and Pk cannot be adapted to
compute inter-coder agreement coefficients.
1703
normalizes the counts of full and near misses iden-
tified by boundary edit distance, as shown in Equa-
tion 2, where sa and sb are the segmentations, nt
is the maximum distance that boundaries may span
to be considered a near miss, edits(sa, sb, nt) is the
edit distance, and pb(D) is the number of potential
boundaries in a document D (pb(D) = |D| ? 1).
S(sa, sb, nt) = 1? |edits(sa, sb, nt)|pb(D) (2)
Boundary edit distance models full misses as
the addition/deletion of a boundary, and near
misses as n-wise transpositions. An n-wise trans-
position is the act of swapping the position of
a boundary with an empty position such that it
matches a boundary in the segmentation compared
against (up to a spanning distance of nt). S also
scales the severity of a near miss by the distance
over which it is transposed, allowing it to scale
the penalty of a near misses much like WD. S is
also symmetric, allowing it to be used in pairwise
means and inter-coder agreement coefficients.
The usage of an edit distance that supported
transpositions to compare segmentations was an
advancement over window-based methods, but
boundary edit distance and its normalization S are
not without problems, specifically: i) This edit dis-
tance uses string reversals (ABCD =? DCBA)
to perform transpositions, making it cumbersome
to analyse individual pairs of boundaries between
segmentations; ii) S is sensitive to variations in the
total size of a segmentation, leading it to favour
very sparse segmentations with few boundaries;
iii) S produces cosmetically high values, making
it difficult to interpret and causing over-estimation
of inter-coder agreement. In this work, these defi-
ciencies are demonstrated and a new set of metrics
are proposed as replacements.
2.2 Inter-Coder Agreement
Inter-coder agreement coefficients are used to
measure whether a group of human judges (i.e.
coders) agree with each other greater than chance.
Such coefficients are used to determine the relia-
bility and replicability of the coding scheme and
instructions used to collect manual codings (Car-
letta, 1996). Although direct interpretation of such
coefficients is difficult, they are an invaluable tool
when comparing segmentation data that has been
collected with differing labels and when estimat-
ing the replicability of a study. A variety of inter-
coder agreement coefficients exist, but this work
focuses upon a selection of those discussed by Art-
stein and Poesio (2008), specifically: Scott?s pi
(Scott, 1955) Fleiss? multi-pi (pi?, Fleiss 1971)7,
Cohen?s ? (Cohen, 1960), and multi-? (??, Davies
and Fleiss 1982). Their general forms are shown in
Equation 3, where Aa represents actual agreement,
and Ae expected (i.e., chance) agreement between
coders.
?, pi, ??, and pi? = Aa ? Ae1? Ae (3)
When calculating agreement between manual
segmenters, boundaries are considered labels and
their positions the decisions. Unfortunately, be-
cause of the frequency of near misses that oc-
cur in segmentation, using such labels and de-
cisions causes inter-coder agreement coefficients
to drastically underestimate actual agreement?
much like how automatic segmenter performance
is underestimated when segmentation is treated
as a binary classification problem. Hearst (1997,
pp. 53?54) attempted to adapt pi? to award par-
tial credit for near misses by using the percentage
agreement metric of Gale et al (1992, p. 254) to
compute actual agreement?which conflates mul-
tiple manual segmentations together according to
whether a majority of coders agree upon a bound-
ary or not. Unfortunately, such a method of com-
puting agreement grossly inflates results, and ?the
statistic itself guarantees at least 50% agreement
by only pairing off coders against the majority
opinion? (Isard and Carletta, 1995, p. 63).
Fournier and Inkpen (2012, pp. 154?156) pro-
posed using pairwise mean S for actual agree-
ment to allow inter-coder agreement coefficients
to award partial credit for near misses. Unfor-
tunately, because S produces cosmetically high
values, it also causes inter-coder agreement coef-
ficients to drastically overestimates actual agree-
ment. This work demonstrates this deficiency and
proposes and evaluates a solution.
3 A New Proposal for Edit-Based Text
Segmentation Evaluation
In this section, a new boundary edit distance based
segmentation metric and confusion matrix is pro-
posed to solve the deficiencies of S for both seg-
mentation comparison and inter-coder agreement.
7Sometimes referred to as K (Siegel and Castellan, 1988).
1704
3.1 Boundary Edit Distance
In this section, Boundary Edit Distance (BED; as
proposed in Fournier and Inkpen 2012, pp. 154?
156) is introduced in more detail, and a few termi-
nological and conceptual changes are made.
Boundary Edit Distance uses three main edit op-
erations to model segmentation differences:
? Additions/deletions (AD; referred to origi-
nally as substitutions) for full misses;
? Substitutions (S; not shown for brevity) for
confusing one boundary type with another;
? n-wise transpositions (T) for near misses.
These edit operations are symmetric and oper-
ate upon the set of boundaries that occur at each
potential boundary position in a pair of segmenta-
tions. An example of how these edit operations are
applied8 is shown in Figure 1, where a near miss
(T), a matching pair of boundaries (M), and two
full misses (ADs) are shown with the maximum
distance that a transposition can span (nt) set to 2
potential boundaries (i.e., only adjacent positions
can be transposed).
s1 2 4 4 4
s2 3 3 6 2
T
M
AD
AD
Figure 1: Boundary edit operations
In Figure 1, the location of the errors is clearly
shown. Importantly, however, pairs of boundaries
between the segmentations can be seen that rep-
resent the decisions made, and the correctness of
these decisions. Imagine that s1 is a manual seg-
mentation, and s2 is an automatic segmenter?s hy-
pothesis. The transposition is a partially correct
decision, or boundary pair. The match is a correct
boundary pair. The additions/deletions, however,
could be one of two erroneous decisions: to not
place an expected boundary (FN), or to place a su-
perfluous boundary (FP).9
This work proposes assigning a correctness
score for each boundary pair/decision (shown in
Table 1) and then using the mean of this score as
a normalization of boundary edit distance. This
interpretation intuitively relates boundary edit dis-
tance to coder judgements, making it ideal for
8A complete explanation of Boundary Edit Distance is de-
tailed in Fournier (2013, Section 4.1.2).
9Also note that the ADs are close together, and if nt > 2,
then they would be considered a T, and not two ADs?this is
one way to award partial credit for near misses.
calculating actual agreement in inter-coder agree-
ment coefficients and comparing segmentations.
Pair Correctness
Match 1
Addition/deletion 0
Transposition 1? wt span(Te, nt)
Substitution 1? ws ord(Se,Tb)
Table 1: Correctness of boundary pair
3.2 Boundary Similarity
The new boundary edit distance normalization
proposed herein is referred to as boundary similar-
ity (B). Assuming that boundary edit distance pro-
duces sets of edit operations where Ae is the set of
additions/deletions, Te the set of n-wise transpo-
sitions, Se the set of substitutions, and BM the set
of matching boundary pairs, boundary similarity
similarity can be defined as shown in Equation 4?
one minus the incorrectness of each boundary pair
over the total number of boundary pairs.
B(s1, s2, nt) = 1?|Ae|+ wt span(Te, nt) + ws ord(Se,Tb)|Ae|+ |Te|+ |Se|+ |BM |
(4)
This form, one minus a penalty function, was
chosen so that it was easier to compare against
other penalty functions considered (not shown
here for brevity). This normalization was also cho-
sen because it is equivalent to mean boundary pair
correctness and so that it ranges in value from 0 to
1. In the worst case, a segmentation comparison
will result in no matches, no near misses, no sub-
stitutions, andX full misses, i.e., |Ae| = X and all
other terms in Equation 4 are zero, meaning that:
B = 1? X + 0 + 0X + 0 + 0 + 0
= 1? X/X = 1? 1 = 0
In the best case, a segmentation comparison will
result in X matches, no near misses, no substitu-
tions, and no full misses, i.e., |BM | = X and all
other terms in Equation 4 are zero, meaning that:
B = 1? 0 + 0 + 00 + 0 + 0 +X
= 1? 0/X = 1? 0 = 1
For all other scenarios, varying numbers of
matches, near misses, substitutions and full misses
will result in values of B between 0 and 1.
Equation 4 takes two segmentations (in any or-
der), and the maximum transposition spanning
distance (nt). This distance represents the great-
est offset between boundary positions that could
be considered a near miss and can be used to scale
1705
the severity of a near miss. A variety of scaling
functions could be used, and this work arbitrarily
chooses a simple fraction to represent each trans-
position?s severity in terms of its distance from its
paired boundary over nt plus a constant wt (0 by
default), as shown in Equation 5.
wt span(Te, nt) =
|Te|?
j=1
(
wt +
abs(Te[j][1]? Te[j][2])
nt ? 1
)
(5)
If multiple boundary types are used, then sub-
stitution edit operations would occur when one
boundary type was confused with another. As-
signing each boundary type tb ? Tb a number on
an ordinal scale, substitutions can be weighted by
their distance on this scale over the maximum dis-
tance plus a constant ws (0 by default), as shown
in Equation 6.
ws ord(Se,Tb) =
|Se|?
j=1
(
ws +
abs(Se[j][1]? Se[j][2])
max(Tb)?min(Tb)
)
(6)
These scaling functions allow for edit penalties
to range from 0 to ws/t plus some linear distance.
3.3 A Confusion Matrix for Segmentation
The mean correctness of each pair (i.e., B) gives an
indication of just how similar one segmentation is
to another, but what if one wants to identify some
specific attributes of the performance of an auto-
matic segmenter? Is the segmenter confusing one
boundary type with another, or is it very precise
but has poor recall? The answers to these ques-
tions can be obtained by looking at text segmenta-
tion as a multi-class classification problem.
This work proposes using a task?s set of bound-
ary types (Tb) and the lack of a boundary (?)
to represent the set of segmentation classes in
a boundary classification problem. Using these
classes, a confusion matrix (defined in Equation 7)
can be created which sums boundary pair correct-
ness so that information-retrieval metrics can be
calculated that award partial credit to near misses
by scaling edits operations.
CM(a, p) =
?
??????
??????
|BM,a| + ws ord(Sa,pe , Tb)
+wt span(Ta,pe , nt) if a = p
ws ord(Sa,pe , Tb)
+wt span(Ta,pe , nt) if a 6= p
|Ae,a| if p = ?
|Ae,p| if a = ?
(7)
An example confusion matrix is shown in Fig-
ure 2 from which IR metrics such as precision, re-
call, and F?-measure can be computed (referred to
as B-precision, B-recall, etc.).
Actual
Pr
ed
ict
ed B Non-BB CM(1, 1) CM(?, 1)
Non-B CM(1,?) CM(?,?)
Figure 2: Example confusion matrix (Tb = {1})
3.4 B-Based Inter-coder Agreement
Fournier and Inkpen (2012, p. 156?157) adapted
four inter-coder agreement formulations provided
by Artstein and Poesio (2008) to use S to award
partial credit for near misses, but because S pro-
duces cosmetically high agreement values they
grossly overestimate agreement. To solve this
issue, this work instead proposes using micro-
average B (i.e., mean boundary pair correctness
over all documents and codings compared) to
solve this issue (demonstrated in ?5) because it
does not over-estimate actual agreement (demon-
strated in ?4 and 5).
4 Discussion of Segmentation Metrics
Before analysing how each metric compares to
each other upon a large data set, it would be useful
to investigate how they act on a smaller scale. To
that end, this section discusses how each metric in-
terprets a set of hypothetical segmentations of an
excerpt of a poem by Coleridge (1816, pp. 55?58)
titled Kubla Khan (shown in Figure 3)?chosen ar-
bitrarily for its brevity (and beauty). These seg-
mentations are topical and at the line-level.
1. In Xanadu did Kubla Khan
2. A stately pleasure-dome decree:
3. Where Alph, the sacred river, ran
4. Through caverns measureless to man
5. Down to a sunless sea.
6. So twice five miles of fertile ground
7. With walls and towers were girdled round:
8. And here were gardens bright with sinuous rills,
9. Where blossomed many an incense-bearing tree;
10. And here were forests ancient as the hills,
11. Enfolding sunny spots of greenery.
Figure 3: Excerpt from the poem Kubla Khan (Co-
leridge, 1816, pp. 55?58) with line numbers
Topical segmentations of this poem are difficult
to produce because there is still some structural
form (i.e., punctuation) which may dictate where
a boundary lies, but the imagery, places, times, and
subjects of the poem appear to twist and wind like
a vision in a dream. Thus, placing a topical bound-
ary in this text is a highly subjective task. One
hypothetical topical segmentation of the excerpt is
shown in Figure 4. In this section, a variety of
1706
contrived automatic segmentations are compared
to this manual segmentation to illustrate how each
metric reacts to different mistakes.
Lines Description
1?2 Kubla Khan and his decree
3?5 Waterways
6?11 Fertile ground and greenery
Figure 4: A hypothetical manual segmentation
Assuming that Figure 4 represents an accept-
able manual segmentation (m), how would each
metric react to an automatic segmentation (a) that
combines the segments 1?2 and 3?5 together?
This would represent a full miss, or a false neg-
ative, as shown in Figure 5. S interprets these seg-
mentations as being quite similar, yet, the auto-
matic segmentation is missing a boundary. B and
1?WD,10 in this case, better reflect this error.
m
a
S B 1?WD
0.9 0.5 0.777?
k = 2
Figure 5: False negative
How would each metric react to an automatic
segmentation that is very close to placing the
boundaries correctly, but makes the slight mis-
take of thinking that the segment on waterways
(3?5) ends a bit too early? This would repre-
sent a near miss, as shown in Figure 6. S and
1?WD incorrectly interpret this error as being
equivalent to the previous false negative?a trou-
bling result. Segmentation comparison metrics
should be able to discern between the full and a
near miss shown in these two figures, and an au-
tomatic segmenter that nearly misses a boundary
should be awarded a better score than one which
fully misses a boundary?B recognizes this and
awards the near miss a higher score.
m
a
S B 1?WD
0.9 0.75 0.777?
k = 2
Figure 6: Near miss
How would each metric react to an automatic
segmentation that adds an additional boundary be-
tween line 8 and 9? This would not be ideal
because such a boundary falls in the middle of
a cohesive description of a garden, representing
10WD is reported as 1?WD because WD is normally a
penalty metric where a value of 0 is ideal, unlike S and B. Ad-
ditionally, k = 2 for all examples in this section because WD
computes k from the manual segmentationm, which does not
change in these examples.
a full miss, or false positive, as in Figure 7. S
and 1?WD incorrectly interpret this error as be-
ing equivalent to the previous two errors?an even
more troubling result. In this case, there are two
matching boundaries and a pair that do not match,
which is arguably preferable to the full miss and
one match in Figure 5, but not to the match and
near miss in Figure 6. B recognizes this, and
awards a higher score to this automatic segmenter
than that in Figure 5, but below Figure 6.
m
a
S B 1?WD0.9 0.666? 0.777?
k = 2
Figure 7: False positive
How would each metric react to an automatic
segmentation that compensates for its lack of pre-
cision by spuriously adding boundaries in clusters
around where it thinks that segments should begin
or end? This is shown in Figure 8. This kind of
behaviour is finally penalized differently by S and
1?WD (unlike the other errors shown in this sec-
tion), but it only barely results in a dip in their val-
ues. B also penalizes this behaviour, but does so
much more harshly?in B?s interpretation, this is
as egregious as committing a false negative (e.g.,
Figure 5)?an arguably correct interpretation, if
the evaluation desires to maximize similarity with
a manual segmentation.
m
a
S B 1?WD
0.8 0.5 0.666?
k = 2
Figure 8: Cluster of false positives
These short demonstrations of how S, B, and
1?WD interpret error should lead one to con-
clude that: i) WD can penalize near misses to
the same degree as full misses?overly harshly;
ii) Both S and WD are not very discriminating
when small segments are analysed; and iii) B is
the only one of the three metrics that is able to
often discriminate between these situations. B, if
used to rank these automatic segmenters, would
rank them from best to worst performing as: the
near miss, false positive, and then a tie between
the false negative and cluster of false positives?a
reasonable ranking in the context of an evaluation
seeking similarity with a manual segmentation.
5 Segmentation Agreement
Having a bit more confidence in B compared to S
and WD on a small scale from the previous sec-
1707
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8P(miss) while P(near) = 0.0921
0.75
0.80
0.85
0.90
0.95
1.00
pi?v
alue
usin
gS
234
567
8910
(a) S-based pi? showing increasing full
misses with constant near misses
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8P(miss) while P(near) = 0.0921
0.2
0.4
0.6
0.8
1.0
pi?v
alue
usin
gB b
234
567
8910
(b) B-based pi? showing increasing full
misses with constant near misses
2 3 4 5 6 7 8 9 10Coders (quantity)
0.0
0.2
0.4
0.6
0.8
1.0
pi?v
alue
usin
gB b
S B
(c) S and B based pi? with fully random
segmentations
Figure 9: Artificial data sets illustrating how pi adapted to use either S or B reacts to increasing full
misses and random segmentations and varying numbers of coders
tion, it makes sense to analyse some larger data
sets. Two such data sets are The Stargazer data
set collected by Hearst (1997) and The Moonstone
data set collected by Kazantseva and Szpakowicz
(2012). Both are linear topical segmentations at
the paragraph level with only one boundary type,
but that is where their similarities end.
The Stargazer text is a science magazine article
titled ?Stargazers look for life? (Baker, 1990) seg-
mented by 7 coders and was one of twelve articles
chosen for its length (between 1,800 and 2,500
words) and for having little structural demarca-
tion. ?The Moonstone? is a 19th century romance
novel by Collins (1868) segmented by 4?6 coders
per chapter; of its 23 chapters, 2 were coded in a
pilot study and another 20 were coded individually
by 27 undergraduate English students in 5 groups.
For the Stargazer data set, using S-based pi?,
an inter-coder agreement coefficient of 0.7562 is
obtained?a reasonable level by content analysis
standards. Unfortunately, this value is highly in-
flated, and B-based pi? gives a much more conser-
vative coefficient at 0.4405. For the Moonstone
data set, the agreement coefficients for each group
of 4?6 coders using S-based pi? is again over-
inflated at 0.91, 0.92, 0.90, 0.94, 0.83. B-based
pi? instead reports that the coefficients should be
0.20, 0.18, 0.40, 0.38, 0.23.
Which of these coefficients should be trusted?
Is agreement in these data sets high or low? To
help answer that, this work looks at how the coders
in the data sets behaved. If the segmenters in the
Moonstone data set truly agreed with each other,
then they should have all behaved similarly. One
measure of coder behaviour is the frequency that
they placed boundaries (normalized by their op-
portunity to place boundaries, i.e. the sum of the
potential boundaries in the chapters that each seg-
mented). This normalized frequency is shown per
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27
Coder
0.00
0.05
0.10
0.15
0.20
0.25
Bou
nda
ries
per
pot
ent
ial
bou
nda
ry
Figure 11: Normalized boundaries placed by each
coder in the Moonstone data set (with mean?SD)
coder in Figure 11 for The Moonstone data set,
along with bars indicating the mean and one stan-
dard deviation above and below. As can be seen,
the coders fluctuated wildly in the frequency with
which they placed boundaries?some (e.g., coder
7) to degrees exceeding 2 standard deviations. The
Moonstone data set as a whole does not exhibit
coders who behaved similarly, supporting the as-
sertion by B-based pi? that these coders do not
agree well (though pockets of agreement exist).
How can it be demonstrated that S-based
agreement over-estimates agreement, and B-based
agreement does not? One way to demonstrate this
is through simulation. By estimating parameters
from the large Moonstone data set such as the dis-
tribution of internal segment sizes produced by all
coders, a random segmentation of the novel with
similar characteristics can be created. From this
single random segmentation, other segmentation
can be created with a probability of either placing
an offset boundary (i.e., a near miss) or placing
an extra/omitting a boundary (i.e., a full miss)?
a pseudo-coding. Manipulating these probabilities
and keeping the probability of a near miss at a con-
stant natural level should produce a slowly declin-
1708
Random Human BayesSeg APS MinCutAutomatic segmenter
0.80
0.82
0.84
0.86
0.88
0.90
0.92
0.94
S?
valu
e
n = 90 n = 90 n = 90 n = 90 n = 90
(a) S
Random Human BayesSeg APS MinCutArtificial Segmenter
0.2
0.3
0.4
0.5
0.6
B bm
ean
and
95%
CIs
n = 1057 n = 841 n = 964 n = 738 n = 871
(b) B
Random Human BayesSeg APS MinCutAutomatic segmenter
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
1?W
D?v
alue
n = 90 n = 90 n = 90 n = 90 n = 90
(c) 1?WD
Figure 10: Mean performance of 5 segmenters using varying metrics with 95% confidence intervals
ing amount of agreement which is unaffected by
the number of pseudo-coders. This is not appar-
ent, however, for S-based pi? in Figure 9a; as the
probability of a full miss increases, agreement ap-
pears to rise and varies depending upon the num-
ber of pseudo-coders. B-based pi? however shows
declining agreement and little to no variation de-
pending upon the number of pseudo-coders, as
shown in Figure 9b.
If instead of creating pseudo-coders from a ran-
dom segmentation a series of random segmenta-
tions with the same parameters were generated, a
properly functioning inter-coder agreement coef-
ficient should report some agreement (due to the
similar parameters used to create the segmenta-
tions) but it should be quite low. Figure 9c shows
this, and that S-based pi? drastically over-estimates
what should be very low agreement whereas B-
based pi? properly reports low agreement.
From these demonstrations, it is evident that
S-based inter-coder agreement coefficients dras-
tically over-estimate agreement, as does S itself
in pairwise mean form. B-based coefficients,
however, properly discriminate between levels of
agreement regardless of the number of coders and
do not over-estimate.
6 Evaluation of Automatic Segmenters
Having looked at how S, WD, and B perform at
a small scale in ?4 and on larger data set in ?5,
this section demonstrates the use of these met-
rics to evaluate some automatic segmenters. Three
automatic segmenters were trained?or had their
parameters estimated upon?The Moonstone data
set, including MinCut; (Malioutov and Barzilay,
2006), BayesSeg; (Eisenstein and Barzilay, 2008),
and APS (Kazantseva and Szpakowicz, 2011).
To put this evaluation into context, an upper and
lower bound were also created comprised of a ran-
dom coder from the manual data (Human) and a
random segmenter (Random), respectively. These
automatic segmenters, and the upper and lower
bounds, were created, trained, and run by another
researcher (Anna Kazantseva) with their labels re-
moved during the development of the metrics de-
tailed herein (to improve the impartiality of these
analyses). An ideal segmentation evaluation met-
ric should, in theory, place the three automatic seg-
menters between the upper and lower bounds in
terms of performance if the metrics, and the seg-
menters, function properly.
The mean performance of the upper and lower
bounds upon the test set of the Moonstone data
set using S, B, and WD are shown in Figure 10a?
10c along with 95% confidence intervals. Despite
the difference in the scale of their values, both S
and WD performed almost identically, placing the
three automatic segmenters between the upper and
lower bounds as expected. For S, statistically sig-
nificant differences11 (? = 0.05) were found be-
tween all segmenters except between APS?human
and MinCut?BayesSeg, and WD could only find
significant differences between the automatic seg-
menters and the upper and lower bounds. B, how-
ever, shows a marked deviation, and places Min-
Cut and APS statistically significantly below the
random baseline with only BayesSeg between the
upper and lower bounds?to a significant degree.
Why would pairwise mean B act in such an
unexpected manner? The answer lies in a fur-
ther analysis using the confusion matrix proposed
earlier to calculate B-precision and B-recall (as
shown in Table 2). From the values in Table 2,
all three automatic segmenters appear to have B-
precision above the baseline and below the upper
bound, but the B-recall of both APS and MinCut
is below that of the random baseline (illustrated
11Using Kruskal-Wallis rank sum multiple comparison
tests (Siegel and Castellan, 1988, pp. 213-214) for S and
WD and the Wilcoxon-Nemenyi-McDonald-Thompson test
(Hollander and Wolfe, 1999, p. 295) for B.
1709
B n B-P B-R B-F1 TP FP FN TN
Random 0.2640? 0.0129 1057 0.3991 0.4673 0.4306 279.0 420 318 4236.0
Human 0.5285? 0.0164 841 0.6854 0.7439 0.7135 444.5 204 153 4451.5
BayesSeg 0.3745? 0.0146 964 0.5247 0.6224 0.5694 361.0 327 219 4346.0
APS 0.2873? 0.0163 738 0.6773 0.3403 0.4530 212.0 101 411 4529.0
MinCut 0.2468? 0.0141 871 0.4788 0.3496 0.4041 215.0 234 400 4404.0
Table 2: Mean performance of 5 segmenters using micro-average B, B-precision (B-P), B-recall (B-R),
and B-F?-measure (B-F1) along with the associated confusion matrix values for 5 segmenters
0.0 0.2 0.4 0.6 0.8 1.0
B? recall
0.0
0.2
0.4
0.6
0.8
1.0
B?
pre
cis
ion
Random
Human
BayesSeg
APS
MinCut
Figure 12: Mean B-precision versus B-recall of 5
automatic segmenters
in Figure 12). These automatic segmenters were
developed and performance tuned using WD, thus
it would be expected that they would perform as
they did according to WD, but the evaluation using
B highlights WD?s bias towards sparse segmenta-
tions (i.e., those with low B-recall)?a failing that
S also appears to share. Mean B shows an un-
biased ranking of these automatic segmenters in
terms of the upper and lower bounds. B, then,
should be preferred over S and WD for an un-
biased segmentation evaluation that assumes that
similarity to a human solution is the best measure
of performance for a task.
7 Conclusions
In this work, a new segmentation evaluation met-
ric, referred to as boundary similarity (B) is
proposed as an unbiased metric, along with a
boundary-edit-distance-based (BED-based) con-
fusion matrix to compute predictably biased IR
metrics such as precision and recall. Additionally,
a method of adapting inter-coder agreement coef-
ficients to award partial credit for near misses is
proposed that uses B as opposed to S for actual
agreement so as to not over-estimate agreement.
B overcomes the cosmetically high values of S
and, the bias towards segmentations with few or
tightly-clustered boundaries of WD?manifesting
in this work as a bias towards precision over recall
for both WD and S. When such precision is desir-
able, however, B-precision can be computed from
a BED-based confusion matrix, along with other
IR metrics. WD and Pk should not be preferred
because their biases do not occur consistently in
all scenarios, whereas BED-based IR metrics offer
expected biases built upon a consistent, edit-based,
interpretation of segmentation error.
B also allows for an intuitive comparison of
boundary pairs between segmentations, as op-
posed to the window counts of WD or the sim-
plistic edit count normalization of S. When an un-
biased segmentation evaluation metric is desired,
this work recommends the usage of B and the use
of an upper and lower bound to provide context.
Otherwise, if the evaluation of a segmentation task
requires some biased measure, the predictable bias
of IR metrics computed from a BED-based con-
fusion matrix is recommended. For all evalua-
tions, however, a justification for the biased/un-
biased metrics used should be given, and more
than one metric should be reported so as to allow
a reader to ascertain for themselves whether a par-
ticular automatic segmenter?s bias in some manner
is cause for concern or not.
8 Future Work
Future work includes adapting this work to anal-
yse hierarchical segmentations and using it to at-
tempt to explain the low inter-coder agreement co-
efficients reported in topical segmentation tasks.
Acknowledgements
I would like to thank Anna Kazantseva for her in-
valuable feedback and data. Additionally, I would
like to thank my thesis committee members?Stan
Szpakowicz, James Green, and Xiaodan Zhu?for
their feedback along with my supervisor Diana
Inkpen and colleague Martin Scaiano.
1710
References
Artstein, Ron and Massimo Poesio. 2008. Inter-
coder agreement for computational linguistics.
Computational Linguistics 34(4):555?596.
Baker, David. 1990. Stargazers look for life. South
Magazine 117:76?77.
Beeferman, Doug and Adam Berger. 1999. Sta-
tistical models for text segmentation. Machine
Learning 34:177?210.
Carletta, Jean. 1996. Assessing Agreement on
Classification Tasks: The Kappa Statistic. Com-
putational Linguistics 22(2):249?254.
Chang, Pi-Chuan, Michel Galley, and Christo-
pher D. Manning. 2008. Optimizing Chinese
word segmentation for machine translation per-
formance. In Proceedings of the Third Work-
shop on Statistical Machine Translation. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, pages 224?232.
Cohen, Jacob. 1960. A Coefficient of Agreement
for Nominal Scales. Educational and Psycho-
logical Measurement 20:37?46.
Coleridge, Samuel Taylor. 1816. Christabel,
Kubla Khan, and the Pains of Sleep. John Mur-
ray.
Collins, Wilkie. 1868. The Moonstone. Tinsley
Brothers.
Davies, Mark and Joseph L. Fleiss. 1982. Measur-
ing agreement for multinomial data. Biometrics
38:1047?1051.
Eisenstein, Jacob. 2009. Hierarchical text seg-
mentation from multi-scale lexical cohesion.
In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
pages 353?361.
Eisenstein, Jacob and Regina Barzilay. 2008.
Bayesian unsupervised topic segmentation. In
Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing. Association for Computational Linguistics,
Morristown, NJ, USA, pages 334?343.
Fleiss, Joseph L. 1971. Measuring nominal scale
agreement among many raters. Psychological
Bulletin 76:378?382.
Fournier, Chris and Diana Inkpen. 2012. Segmen-
tation Similarity and Agreement. In Proceed-
ings of Human Language Technologies: The
2012 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, pages 152?
161.
Fournier, Christopher. 2013. Evaluating Text Seg-
mentation. Master?s thesis, University of Ot-
tawa.
Franz, Martin, J. Scott McCarley, and Jian-Ming
Xu. 2007. User-oriented text segmentation eval-
uation measure. In Proceedings of the 30th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval. Association for Computing Machinery,
Stroudsburg, PA, USA, pages 701?702.
Gale, William, Kenneth Ward Church, and David
Yarowsky. 1992. Estimating upper and lower
bounds on the performance of word-sense dis-
ambiguation programs. In Proceedings of
the 30th Annual Meeting of the Association
for Computational Linguistics. Association for
Computational Linguistics, Stroudsburg, PA,
USA, pages 249?256.
Georgescul, Maria, Alexander Clark, and Susan
Armstrong. 2006. An analysis of quantita-
tive aspects in the evaluation of thematic seg-
mentation algorithms. In Proceedings of the
7th SIGdial Workshop on Discourse and Dia-
logue. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, pages 144?151.
Haghighi, Aria and Lucy Vanderwende. 2009.
Exploring content models for multi-document
summarization. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, NAACL ?09, pages 362?370.
Hearst, Marti A. 1993. TextTiling: A Quantitative
Approach to Discourse. Technical report, Uni-
versity of California at Berkeley, Berkeley, CA,
USA.
Hearst, Marti A. 1997. TextTiling: Segmenting
Text into Multi-paragraph Subtopic Passages.
Computational Linguistics 23:33?64.
Hollander, Myles and Douglas A. Wolfe. 1999.
1711
Nonparametric Statistical Methods. John Wi-
ley & Sons, 2nd edition.
Isard, Amy and Jean Carletta. 1995. Replicability
of transaction and action coding in the map task
corpus. In AAAI Spring Symposium: Empirical
Methods in Discourse Interpretation and Gen-
eration. pages 60?66.
Kazantseva, Anna and Stan Szpakowicz. 2011.
Linear Text Segmentation Using Affinity Prop-
agation. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language
Processing. Association for Computational Lin-
guistics, Edinburgh, Scotland, UK., pages 284?
293.
Kazantseva, Anna and Stan Szpakowicz. 2012.
Topical Segmentation: a Study of Human Per-
formance. In Proceedings of Human Language
Technologies: The 2012 Annual Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics. Associ-
ation for Computational Linguistics, Strouds-
burg, PA, USA, pages 211?220.
Lamprier, Sylvain, Tassadit Amghar, Bernard
Levrat, and Frederic Saubion. 2007. On eval-
uation methodologies for text segmentation al-
gorithms. In Proceedings of the 19th IEEE In-
ternational Conference on Tools with Artificial
Intelligence. IEEE Computer Society, Washing-
ton, DC, USA, volume 2, pages 19?26.
Litman, Diane J. and Rebecca J. Passonneau.
1995. Combining multiple knowledge sources
for discourse segmentation. In Proceedings
of the 33rd Annual Meeting of the Association
for Computational Linguistics. Association for
Computational Linguistics, Stroudsburg, PA,
USA, pages 108?115.
Malioutov, Igor and Regina Barzilay. 2006. Min-
imum cut model for spoken lecture segmen-
tation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguis-
tics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics. Associ-
ation for Computational Linguistics, Strouds-
burg, PA, USA, pages 25?32.
Niekrasz, John and Johanna D. Moore. 2010. Un-
biased discourse segmentation evaluation. In
Proceedings of the IEEE Spoken Language
Technology Workshop, SLT 2010. IEEE 2010,
pages 43?48.
Oh, Hyo-Jung, Sung Hyon Myaeng, and Myung-
Gil Jang. 2007. Semantic passage segmentation
based on sentence topics for question answer-
ing. Information Sciences 177(18):3696?3717.
Passonneau, Rebecca J. and Diane J. Litman.
1993. Intention-based segmentation: human
reliability and correlation with linguistic cues.
In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Stroudsburg, PA, USA, pages 148?155.
Pevzner, Lev and Marti A. Hearst. 2002. A cri-
tique and improvement of an evaluation metric
for text segmentation. Computational Linguis-
tics 28:19?36.
Reynar, Jeffrey C. and Adwait Ratnaparkhi. 1997.
A maximum entropy approach to identifying
sentence boundaries. In Proceedings of the
5th Conference on Applied Natural Language
Processing. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, pages 16?19.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale coding.
Public Opinion Quarterly 19:321?325.
Siegel, Sidney and N. J. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sci-
ences, McGraw-Hill, New York, USA, chapter
9.8. 2nd edition.
Sirts, Kairit and Tanel Aluma?e. 2012. A Hierar-
chical Dirichlet Process Model for Joint Part-of-
Speech and Morphology Induction. In Proceed-
ings of Human Language Technologies: The
2012 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, pages 407?
416.
Stoyanov, Veselin and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis.
In Proceedings of the 22nd International Con-
ference on Computational Linguistics. Associ-
ation for Computational Linguistics, Strouds-
burg, PA, USA, pages 817?824.
1712
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 47?51,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
An initial study of topical poetry segmentation
Chris Fournier
University of Ottawa
Ottawa, ON, Canada
cfour037@eecs.uottawa.ca
Abstract
This work performs some basic research upon
topical poetry segmentation in a pilot study
designed to test some initial assumptions and
methodologies. Nine segmentations of the
poem titled Kubla Khan (Coleridge, 1816,
pp. 55-58) are collected and analysed, pro-
ducing low but comparable inter-coder agree-
ment. Analyses and discussions of these cod-
ings focus upon how to improve agreement
and outline some initial results on the nature
of topics in this poem.
1 Introduction
Topical segmentation is the division of a text by
placing boundaries between segments. Within a seg-
mentation, each segment should represent a coherent
and cohesive topic. The decision to place a boundary
between two segments of text is subjective and must
often be determined manually. The factors involved
in performing this subjective task are poorly under-
stood, which motivates this work to begin the basic
research required to understand this phenomenon.
For literature, topical segmentations have been
produced for a short story (Kozima, 1993) and a
novel (Kazantseva and Szpakowicz, 2012). Poetry,
however, has had little attention in terms of topical
segmentation. Brooke et al (2012) collected seg-
mentations of poetry that sought to delineate which
voices communicate various segments of The Waste-
land by T.S. Elliot (1888-1965), but a voice seg-
ment does not necessarily correlate with a topical
segment. Because The Wasteland?s defining feature
is its voice-shifts, more data is required to under-
stand the variety of topical segments that could exist
within poetry besides those delineated by changing
voice ? which this work aims to provide.1
1Available at http://nlp.chrisfournier.ca/
This work?s goal is to begin to provide some
initial information about what constitutes a topic
in poetry by analysing the Romantic-era poem ti-
tled Kubla Khan (Coleridge, 1816, pp. 55-58) by
Samuel Taylor Coleridge (1772?1834). Chosen for
its beauty, variety, short length (54 lines), and lack of
strict adherence to a prescribed structure (e.g., son-
nets, odes, etc.), it is assumed that this purported
fragment of a dream will contain a wide variety of
different topics (as judged by manual coders).
This work aims to discover from reader?s interpre-
tations of topical segmentation in poetry the:
? Structure of these topics (e.g., are they linear,
hierarchical, or something else?);
? Types and variety of topics (e.g., do topics shift
when there are changes in time, place, descrip-
tion, exposition, etc.); and
? Relationship between poetic features and topi-
cal boundaries (e.g., do stanzas correlate with
topical boundaries?).
Unfortunately, this work is simply a pilot study
and it cannot make any generalizations about poetry
overall, but inferences can be made about this single
poem and its topical structure.
2 Related Work
Topical Segmentation Topical segmentation of
expository texts such as popular science magazine
articles have been well studied by Hearst (1993,
1994, 1997) while developing the automatic topi-
cal segmenter named TextTiling. On a parallel track,
Kozima (1993) segmented a simplified version of O.
Henry?s (William Sydney Porter; 1862?1910) short
story titled Springtime a` la Carte (Thornley, 1816).
Both bodies of work focused upon using lexical co-
hesion to model where topic boundaries occur and
collected manual segmentations to study. This data,
47
however, was never analysed for the types of seg-
ments contained, but only for the presence or ab-
sence of topic boundaries at specific positions.
Kazantseva and Szpakowicz (2012) delved deeper
into topical segmentation of literature by collecting
segmentations of Wilkie Collins? (1824?1883) ro-
mantic novel The Moonstone (Collins, 1868). In
the novel, 20 of its chapters were segmented indi-
vidually by 27 annotators (in groups of 4?6) into
episodes. Episodes were defined as ?topically con-
tinuous spans of text demarcated by the most percep-
tible shifts of topic in the chapter? (Kazantseva and
Szpakowicz, 2012, p. 213). This work also analysed
the boundaries placed by the coders themselves, but
not the types of segments that they produced.
Brooke et al (2012) collected voice-switch seg-
mentations of The Wasteland by T.S. Elliot (1888-
1965). Although voices are not topics, voice switch-
ing could constitute topical boundaries. Segmenta-
tions from 140 English literature undergraduate stu-
dents and 6 expert readings were collected and used
to compose one authoritative reference segmentation
to test a large number automatic segmenters upon.
Agreement and Comparison Inter-coder agree-
ment coefficients measure the agreement between a
group of human judges (i.e. coders) and whether
their agreement is greater than chance. Low coeffi-
cient values indicate that a task may have restricted
coders such that their responses do not represent an
empirical model of the task, or the task instructions
did not sufficiently define the task. High coefficient
values indicate the degree of reliability and repli-
cability of a coding scheme and the coding collec-
tion methodology (Carletta, 1996). Although there
is much debate about what coefficient value repre-
sents adequate agreement, any coefficient value can
be used to compare studies of the same task that use
different coding schemes or methodologies.
Many inter-coder agreement coefficients exist, but
this work uses Fleiss? multi-pi (pi?, Fleiss 1971; oc-
casionally referred to as K by Siegel and Castellan
1988) to measure agreement because it generalizes
individual coder performance to give a better pic-
ture of the replicability of a study. Specifically, an
adaptation of the proposal by Fournier and Inkpen
(2012, pp. 154?156) for computing pi? is used that
is detailed by Fournier (2013).
Fournier (2013) modifies the work of Fournier
and Inkpen (2012) to provide a more discriminative
measure of similarity between segmentations called
boundary similarity (B) ? an edit distance based
measure which is unbiased, more consistent, and
more intuitive than traditional segmentation compar-
ison methods such as Pk (Beeferman and Berger,
1999, pp. 198?200) and WindowDiff (Pevzner
and Hearst, 2002, p. 10). Using the inter-coder
agreement formulations provided in Fournier and
Inkpen (2012), Fournier (2013) provides B-based
inter-coder agreement coefficients including Fleiss?
multi-pi (referred to as pi?B) which can discern be-
tween low/high agreement while still awarding par-
tial credit for near misses.
3 Study Design
This work is a small study meant to inform future
larger studies on topical poetry segmentation. To
that end, a single 54 line poem, Kubla Khan (Co-
leridge, 1816, pp. 55-58), is segmented. Written
in four stanzas (originally published in two) com-
posed of tetra and penta-meter iambs, this well stud-
ied work appears to show a large variety of topical
segment breaks, including time, place, scenery, nar-
ration, exposition, etc. Stripped of its indentation
and with its stanzas compressed into one long se-
quence of numbered lines, this poem was presented
to segmenters to divide into topics.
Objectives The objective of this study is to iden-
tify whether topics in poems fit well into a linear
topic structure (i.e., boundaries cannot overlap) and
to test the annotation instructions used. Addition-
ally, a survey of the types and variety of topics is de-
sirable to inform whether more than one boundary
type might be needed to model segment boundaries
(and to inspire statistical features for training an au-
tomatic topical poetry segmenter). Finally, the re-
lationship between poem features and topic bound-
aries is of interest; specifically, for this initial work,
do stanzas correlate with topical boundaries?
Subjects Nine subjects were recruited using Ama-
zon?s Mechanical Turk from the United States who
had an exemplary work record (i.e., were ?Mas-
ter Tukers?). Segment text summaries were anal-
ysed for correct language use to ensure that coders
48
demonstrated English language proficiency.
Granularity Segmentations were solicited at the
line level (arbitrarily assuming that a topic will not
change within a line, but may between lines). This
level is assumed to be fine enough to partition seg-
ments accurately while still being coarse enough to
make the task short (only 54 lines can be divided into
segments). Because there may be a great number of
topics found in the poem by readers, it is assumed
that a nearly missed boundary would only be those
that are adjacent to another (i.e., nt for B is set to 2).
Collection procedure Segmenters were asked to
read the poem and to divide it into topical segments
where a topic boundary could represent a change
in time, scenery, or any other detail that the reader
deems important. A short example coding was also
provided to augment the instructions. Along with
line number spans, a single sentence description of
the segment was requested (for segment type analy-
sis and to verify coder diligence and thoughtfulness)
and overall comments on the task were solicited.
4 Study Results and Analysis
Time The 9 subjects took 35.1556?18.6796 min-
utes to read and segment the poem.2 Each was remu-
nerated $8 USD, or $18.91? 11.03 USD per hour.
Segmentations The 9 coders placed 17.6667 ?
6.2716 boundaries within the 54 lines of the poem.
The number of segmentations produced by each
coder is shown in Figure 1a, along with the mean
and standard deviation (SD).
Agreement The segmentations provided by the 9
coders in this study have an inter-coder agreement
coefficient value of pi?B = 0.3789. This value is low,
but it is only slightly below that of Hearst (1997)
(0.4405) and Kazantseva and Szpakowicz (2012)
(0.20, 0.18, 0.40, 0.38, 0.23 for each of the 5 groups)
as reported in Fournier (2013). This value is also not
unexpected given the different coding behaviours
(e.g., boundary placement frequency) in Figure 1a.
Similarity Using Boundary Similarity (B), taking
1 ? B can yield a simple distance function between
2One coder took far less time because they submitted part of
their answers via email and time was not accurately recorded.
segmentations. Because of the low agreement of
this study, it is assumed that there must be subsets
of coders who agree more with each other than with
others (i.e., clusters). Using 1?B as a distance func-
tion between segmentations, hierarchical agglom-
erative clustering was used to obtain the clusters
shown in Figure 1b. Computing inter-coder agree-
ment for these clusters produces subsets with signif-
icantly higher than overall agreement (Table 1).
Labels Taking the single-sentence descriptions of
each topic, an attempt was made to label them as
belonging to one or more of these categories:
1. Exposition (e.g, story/plot development);
2. Event (e.g., an action or event occurred);
3. Place (Location is stated or changed);
4. Description (of an entity; can be specific):
a) Scenery b) Person c) Sound d) Comparison
(simile or metaphor)
5. Statement (to the reader).
These labels were decided by the author while read-
ing the segmentations and were iteratively con-
structed until they suitably described the one-line
segment topic summaries. Using Jaccard similarity,
the labels placed on each position were compared
to those of each other coder to obtain mean similar-
ity of each line, as plotted in Figure 1c. This shows
that in terms of topic types, actual agreement varies
by position. The portions with the highest agree-
ment are at the beginning of the poem and contain
scenery description which appear to have been easy
to agree upon (type-wise). Overall, mean label sim-
ilarity between all coders was 0.5330 ? 0.4567, but
some of the identified clusters exhibited even higher
similarity (Table 1).
Feature correlations There is some evidence to
suggest that boundaries between the four stanzas at
lines 11?12, 30?31, and 36?37 correlate with top-
ical shifts because 6/9, 9/9, and 9/9 (respectively)
coders placed boundaries at these locations. There is
little evidence to suggest that the indentation of line
5 and lines 31?34 (not shown) correlate with topical
shifts because only 1/9 and 5/9 (respectively) coders
placed boundaries between these segments.
Topical structure One of the coders commented
that they felt that the segments should overlap and
49
0 1 2 3 4 5 6 7 8Coder
0
5
10
15
20
25
30
Boun
darie
spla
ced(
quan
tity) 20
26
18
10
26
16
9
23
11
(a) Coder boundaries placed with
mean and SD
1 0 2 4 7 3 5 6 8Coder
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Mea
nDi
stan
ce(1
?B
)
(b) Hierarchical agglomerative clusters
using 1? B as a distance metric
0 10 20 30 40 50Line0.0
0.2
0.4
0.6
0.8
1.0
Mean
Labe
lJac
card
Simil
arity
(c) Mean Jaccard similarity of topic
label types per line
Figure 1: Various analyses of the 9 manual segmentations of Kubla Khan
Coders {4, 7} {0, 2} {6, 8} {1, 0, 2} {1, 0, 2, 4, 7} {3, 5, 6, 8} {5, 6, 8}
pi?B 0.3704 0.6946 0.7625 0.5520 0.4474 0.4764 0.5389
E(J) 0.491? 0.495 0.460? 0.439 0.685? 0.464 0.508? 0.452 0.512? 0.467 0.593? 0.425 0.580? 0.432
Table 1: Inter-coder agreement (pi?B) and mean Jaccard topic label similarity (with WD) for coder clusters
coded so. These codings were adjusted by the au-
thor to not overlap for analysis, but the coder?s com-
ment highlights that perhaps these segments should
be able to overlap, or that linear segmentation may
not be an adequate model for topics in poetry.
5 Discussion
Given the low (but comparable) inter-coder agree-
ment values of this study, it is evident that some vari-
ables are not properly being controlled by the proce-
dure used herein. Before a larger study is performed,
the issue of low agreement must be explained; some
hypotheses for this are that:
1. Coders may have been of varying levels of ed-
ucation, English proficiency, or motivation;
2. Instructions may have not been clear or exhaus-
tive in terms of the potential topics types;
3. A linear segmentation not allowing for overlap
may artificially constrain coders; and
4. The poem selected may simply be inherently
difficult to interpret and thus segment.
This study has, however, catalogued a number
of topic labels which can be used to better educate
coders about the types of topical segments that ex-
ist, which could lead to obtaining higher inter-coder
agreement. Pockets of agreement do exist, as shown
in the clusters and their agreement and topic label
similarity values (Table 1). If more data is collected,
but inter-coder agreement stays steady, perhaps in-
stead these clusters will remain and become more
populated. Maybe these clusters will reveal that the
problem was modelled correctly, but that there is
simply a difference between the coders that was not
previously known. Such a difference could be spot-
ted using clustering, but what the actual difference is
may remain a mystery unless more biographical de-
tails are available (e.g., sex, age, education, English
proficiency, reading preferences, etc.).
6 Conclusions and Future Work
Although Kubla Khan is a beautiful poem, its topical
segmentation is vexing. Low inter-coder agreement
exemplified by this study indicates that the method-
ology used to investigate topical poetry segmenta-
tion may require some modifications, or more bio-
graphical details must be sought to identify the cause
of the low agreement. Clustering was able to iden-
tify pockets of high agreement and similarity, but
the nature of these clusters is largely unknown ?
what biographical details or subjective opinions of
the task separate these groups?
Future work will continue with subsequent pilot
studies to attempt to raise the level of inter-coder
agreement or to explain the low agreement by look-
ing for clusters of coders who agree (and attempting
to explain the relationships between coders in these
clusters). Also, more poems need to be analysed to
make generalisations about poetry overall. The re-
lationships between topical segments in poetry and
other poetic features such as rhyme, meter, and ex-
pert opinions are also worth investigation.
50
References
Beeferman, Doug and Adam Berger. 1999. Statisti-
cal models for text segmentation. Machine Learn-
ing 34:177?210.
Brooke, Julian, Adam Hammond, and Graeme Hirst.
2012. Unsupervised Stylistic Segmentation of Po-
etry with Change Curves and Extrinsic Features.
In Proceedings of the 1st NAACL-HLT Workshop
on Computational Linguistics for Literature. As-
sociation for Computational Linguistics, Strouds-
burg, PA, USA, pages 26?35.
Carletta, Jean. 1996. Assessing Agreement on Clas-
sification Tasks: The Kappa Statistic. Computa-
tional Linguistics 22(2):249?254.
Coleridge, Samuel Taylor. 1816. Christabel, Kubla
Khan, and the Pains of Sleep. John Murray.
Collins, Wilkie. 1868. The Moonstone. Tinsley
Brothers.
Fleiss, Joseph L. 1971. Measuring nominal scale
agreement among many raters. Psychological
Bulletin 76:378?382.
Fournier, Chris. 2013. Evaluating Text Segmenta-
tion using Boundary Edit Distance. In Proceed-
ings of 51st Annual Meeting of the Association for
Computational Linguistics. Association for Com-
putational Linguistics, Stroudsburg, PA, USA.
Fournier, Chris and Diana Inkpen. 2012. Segmen-
tation Similarity and Agreement. In Proceedings
of Human Language Technologies: The 2012 An-
nual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Stroudsburg, PA, USA, pages 152?161.
Hearst, Marti A. 1993. TextTiling: A Quantitative
Approach to Discourse. Technical report, Uni-
versity of California at Berkeley, Berkeley, CA,
USA.
Hearst, Marti A. 1994. Context and Structured in
Automated Full-Text Information Access Context
and Structure in Automated Full-Text Information
Access. Ph.D. thesis, University of California
Berkeley.
Hearst, Marti A. 1997. TextTiling: Segmenting Text
into Multi-paragraph Subtopic Passages. Compu-
tational Linguistics 23:33?64.
Kazantseva, Anna and Stan Szpakowicz. 2012. Top-
ical Segmentation: a Study of Human Perfor-
mance. In Proceedings of Human Language Tech-
nologies: The 2012 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
pages 211?220.
Kozima, Hideki. 1993. Text segmentation based on
similarity between words. In Proceedings of the
31st Annual Meeting of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Stroudsburg, PA, USA, ACL
?93, pages 286?288.
Pevzner, Lev and Marti A. Hearst. 2002. A critique
and improvement of an evaluation metric for text
segmentation. Computational Linguistics 28:19?
36.
Siegel, Sidney and N. J. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences,
McGraw-Hill, New York, USA, chapter 9.8. 2
edition.
Thornley, G. C., editor. 1816. British and Ameri-
can Short Stories. Longman Simplified English
Series. Longman.
51
