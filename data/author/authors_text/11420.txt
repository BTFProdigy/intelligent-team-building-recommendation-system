Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 43?48,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multiple Word Alignment with Profile Hidden Markov Models
Aditya Bhargava and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{abhargava,kondrak}@cs.ualberta.ca
Abstract
Profile hidden Markov models (Profile
HMMs) are specific types of hidden Markov
models used in biological sequence analysis.
We propose the use of Profile HMMs for
word-related tasks. We test their applicability
to the tasks of multiple cognate alignment and
cognate set matching, and find that they work
well in general for both tasks. On the latter
task, the Profile HMM method outperforms
average and minimum edit distance. Given
the success for these two tasks, we further
discuss the potential applications of Profile
HMMs to any task where consideration of a
set of words is necessary.
1 Introduction
In linguistics, it is often necessary to align words or
phonetic sequences. Covington (1996) uses align-
ments of cognate pairs for the historical linguis-
tics task of comparative reconstruction and Ner-
bonne and Heeringa (1997) use alignments to com-
pute relative distances between words from various
Dutch dialects. Algorithms for aligning pairs of
words have been proposed by Covington (1996) and
Kondrak (2000). However, it is often necessary to
align multiple words. Covington (1998) proposed
a method to align multiple words based on a hand-
crafted scale of similarity between various classes
of phonemes, again for the purpose of comparative
reconstruction of languages.
Profile hidden Markov models (Profile HMMs)
are specific types of hidden Markov models used
in biological sequence analysis, where they have
yielded success for the matching of given sequences
to sequence families as well as to multiple sequence
alignment (Durbin et al, 1998). In this paper, we
show that Profile HMMs can be adapted to the task
of aligning multiple words. We apply them to sets
of multilingual cognates and show that they pro-
duce good alignments. We also use them for the re-
lated task of matching words to established cognate
sets, useful for a situation where it is not immedi-
ately obvious to which cognate set a word should be
matched. The accuracy on the latter task exceeds the
accuracy of a method based on edit distance.
Profile HMMs could also potentially be used for
the computation of word similarity when a word
must be compared not to another word but to an-
other set of words, taking into account properties
of all constituent words. The use of Profile HMMs
for multiple sequence alignment also presents ap-
plications to the acquisition of mapping dictionaries
(Barzilay and Lee, 2002) and sentence-level para-
phrasing (Barzilay and Lee, 2003).
This paper is organized as follows: we first de-
scribe the uses of Profile HMMs in computational
biology, their structure, and then discuss their appli-
cations to word-related tasks. We then discuss our
data set and describe the tasks that we test and their
experimental setups and results. We conclude with
a summary of the results and a brief discussion of
potential future work.
2 Profile hidden Markov models
In computational biology, it is often necessary to
deal with multiple sequences, including DNA and
protein sequences. For such biological sequence
analysis, Profile HMMs are applied to the common
tasks of simultaneously aligning multiple related se-
quences to each other, aligning a new sequence to
43
Begin End
DL
IL
MLM1
I1
D1
I0
Figure 1: A prototypical Profile HMM of length L. Mi is
the ith match state, Ii is the ith insert state, and Di is the
ith delete state. Delete states are silent and are used to
indicate gaps in a sequence.
an already-aligned family of sequences, and evalu-
ating a new sequence for membership in a family of
sequences.
Profile HMMs consist of several types of states:
match states, insert states, delete states, as well as
a begin and end state. For each position in a Pro-
file HMM, there is one match state, one insert state,
and one delete state. A Profile HMM can thus be vi-
sualized as a series of columns, where each column
represents a position in the sequence (see Figure 1).
Any arbitrary sequence can then be represented as a
traversal of states from column to column.
Match states form the core of the model; each
match state is represented by a set of emission prob-
abilities for each symbol in the output alphabet.
These probabilities indicate the distribution of val-
ues for a given position in a sequence. Each match
state can probabilistically transition to the next (i.e.
next-column) match and delete states as well as the
current (i.e. current-column) insert state.
Insert states represent possible values that can be
inserted at a given position in a sequence (before a
match emission or deletion). They are represented
in the same manner as match states, with each out-
put symbol having an associated probability. Insert
states are used to account for symbols that have been
inserted to a given position that might not other-
wise have occurred ?naturally? via a match state. In-
sert states can probabilistically transition to the next
match and delete states as well as the current insert
state (i.e. itself). Allowing insert states to transition
to themselves enables the consideration of multiple-
symbol inserts.
MMIIIM
AG...C
A-AG.C
AG.AA-
--AAAC
AG...C
Figure 2: A small DNA multiple alignment from (Durbin
et al, 1998, p. 123).
Similarly, delete states represent symbols that
have been removed from a given position. For a se-
quence to use a delete state for a given position indi-
cates that a given character position in the model has
no corresponding characters in the given sequence.
Hence, delete states are by nature silent and thus
have no emission probabilities for the output sym-
bols. This is an important distinction from match
states and insert states. Each delete state can prob-
abilistically transition to the next match and delete
states as well as the current insert state.
Figure 2 shows a small example of a set of DNA
sequences. The match columns and insert columns
are marked with the letters M and I respectively in
the first line. Where a word has a character in a
match column, it is a match state emission; when
there is instead a gap, it is a delete state occur-
rence. Any characters in insert columns are insert
state emissions, and gaps in insert columns repre-
sent simply that the particular insert state was not
used for the sequence in question.
Durbin et al (1998) describe the uses of Pro-
file HMMs for tasks in biological sequence analy-
sis. Firstly, a Profile HMM must be constructed. If
a Profile HMM is to be constructed from a set of
aligned sequences, it is necessary to designate cer-
tain columns as match columns and others as insert
column. The simple heuristic that we adopt is to
label those columns match states for which half or
more of the sequences have a symbol present (rather
than a gap). Other columns are labelled insert states.
Then the probability akl of state k transitioning to
state l can be estimated by counting the number of
timesAkl that the transition is used in the alignment:
akl = Akl?
l? Akl?
Similarly, the probability ek(a) of state k emitting
symbol a is estimated by counting the number of
44
times Ek(a) that the emission is used in the align-
ment:
ek(a) = Ek(a)?
a? Ek(a?)
There is the danger that some probabilities may be
set to zero, so it is essential to add pseudocounts.
The pseudocount methods that we explore are de-
scribed in section 3.
If a Profile HMM is to be constructed from a set
of unaligned sequences, an initial model is gener-
ated after which it can be trained to the sequences
using the Baum-Welch algorithm. The length of the
model must be chosen, and is usually set to the av-
erage length of the unaligned sequences. To gener-
ate the initial model, which amounts to setting the
transition and emission probabilities to some initial
values, the probabilities are sampled from Dirichlet
distributions.
Once a Profile HMM has been constructed, it can
be used to evaluate a given sequence for member-
ship in the family. This is done via a straightforward
application of the forward algorithm (to get the full
probability of the given sequence) or the Viterbi al-
gorithm (to get the alignment of the sequence to the
family). For the alignment of multiple unaligned se-
quences, a Profile HMM is constructed and trained
as described above and then each sequence can be
aligned using the Viterbi algorithm.
It should also be noted that Profile HMMs are
generalizations of Pair HMMs, which have been
used for cognate identification and word similar-
ity (Mackay and Kondrak, 2005) between pairs of
words. Unlike Pair HMMs, Profile HMMs are
position-specific; this is what allows their applica-
tion to multiple sequences but also means that each
Profile HMM must be trained to a given set of se-
quences, whereas Pair HMMs can be trained over a
very large data set of pairs of words.
3 Adapting Profile HMMs to words
Using Profile HMMs for biological sequences in-
volves defining an alphabet and working with related
sequences consisting of symbols from that alphabet.
One could perform tasks with cognates sets in a sim-
ilar manner; cognates are, after all, related words,
and words are nothing more than sequences of sym-
bols from an alphabet. Thus Profile HMMs present
potential applications to similar tasks for cognate
sets. We apply Profile HMMs to the multiple align-
ment of cognate sets, which is done in the same
manner as multiple sequence alignment for biolog-
ical sequences described above. We also test Pro-
file HMMs for determining the correct cognate set
to which a word belongs when given a variety of
cognate sets for the same meaning; this is done in a
similar manner to the sequence membership evalua-
tion task described above.
Although there are a number of Profile HMM
packages available (e.g. HMMER), we decided to
develop an implementation from scratch in order to
achieve greater control over various adjustable pa-
rameters.1 We investigated the following parame-
ters:
Favouring match states When constructing a Pro-
file HMM from unaligned sequences, the
choice of initial model probabilities can have a
significant effect on results. It may be sensible
to favour match states compared to other states
when constructing the initial model; since the
transition probabilities are sampled from a
Dirichlet distribution, the option of favouring
match states assigns the largest returned proba-
bility to the transition to a match state.
Pseudocount method We implemented three pseu-
docount methods from (Durbin et al, 1998). In
the following equations, ej(a) is the probability
of state j emitting character a. cja represents
the observed counts of state j emitting symbol
a. A is the weight given to the pseudocounts.
Constant value A constant value AC is added
to each count. This is a generalization of
Laplace?s rule, where C = 1A .
ej(a) = cja +AC?
a? cja? +A
Background frequency Pseudocounts are
added in proportion to the background
frequency qa, which is the frequency of
occurrence of character a.
ej(a) = cja +Aqa?
a? cja? +A
1Our implementation is available online at http://www.
cs.ualberta.ca/?ab31/profilehmm.
45
Substitution matrix (Durbin et al, 1998)
Given a matrix s(a, b) that gives the log-
odds similarity of characters a and b, we
can determine the conditional probability
of a character b given character a:
P (b|a) = qbes(a,b)
Then we define fja to be the probability
derived from the counts:
fja = cja?
a? cja?
Then the pseudocount values are set to:
?ja = A
?
b
fjbP (a|b)
Finally, the pseudocount values are added
to the real counts as above:
ej(a) = cja + ?ja?
a? cja? + ?ja?
Pseudocount weight The weight that the pseudo-
counts are given (A in the above equations).
Smoothing during Baum-Welch The problem has
many local optima and it is therefore easy for
the Baum-Welch algorithm to get stuck around
one of these. In order to avoid local optima,
we tested the option of adding pseudocounts
during Baum-Welch (i.e. between iterations)
rather than after it. This serves as a form
of noise injection, effectively bumping Baum-
Welch away from local optima.
4 Data for experiments
Our data come from the Comparative Indoeuropean
Data Corpus (Dyen et al, 1992). The data consist
of words in 95 languages in the Indoeuropean fam-
ily organized into word lists corresponding to one
of 200 meanings. Each word is represented in the
English alphabet. Figure 3 shows a sample from
the original corpus data. We manually converted the
data into disjoint sets of cognate words, where each
cognate set contains only one word from each lan-
guage. We also removed words that were not cog-
nate with any other words.
On average, there were 4.37 words per cognate
set. The smallest cognate set had two words (since
a 026 DAY
...
b 003
026 53 Bulgarian DEN
026 47 Czech E DENY
026 45 Czech DEN
026 43 Lusatian L ZEN
026 44 Lusatian U DZEN
026 50 Polish DZIEN
026 51 Russian DEN
026 54 Serbocroatian DAN
026 42 Slovenian DAN
026 41 Latvian DIENA
026 05 Breton List DEIZ, DE(Z)
026 04 Welsh C DYDD
026 20 Spanish DIA
026 17 Sardinian N DIE
026 11 Ladin DI
026 08 Rumanian List ZI
026 09 Vlach ZUE
026 15 French Creole C ZU
026 13 French JOUR
026 14 Walloon DJOU
026 10 Italian GIORNO
...
Figure 3: An excerpt from the original corpus data. The
first two numbers denote the meaning and the language,
respectively.
we excluded those words that were not cognate with
any other words), and the largest had 84 words.
There were on average 10.92 cognate sets in a mean-
ing. The lowest number of cognate sets in a meaning
was 1, and the largest number was 22.
5 Multiple cognate alignment
Similar to their use for multiple sequence alignment
of sequences in a family, we test Profile HMMs for
the task of aligning cognates. As described above,
an initial model is generated. We use the aforemen-
tioned heuristic of setting the initial model length to
the average length of the sequences. The transition
probabilities are sampled from a uniform-parameter
Dirichlet distribution, with each parameter having
a value of 5.0. The insert-state emission probabil-
ities are set to the background frequencies and the
match-state emission probabilities are sampled from
a Dirichlet distribution with parameters set in pro-
portion to the background frequency. The model is
46
MIIMIIMI MIIMIIMI
D--E--N- D--E--NY
Z--E--N- DZ-E--N-
DZIE--N- D--A--N-
DI-E--NA D--E--IZ
D--I--A- D--Y--DD
D--I--E- Z-----U-
Z--U--E- Z-----I-
J--O--UR D-----I-
DJ-O--U- G--IORNO
Figure 4: The alignment generated via the Profile HMM
method for some cognates. These were aligned together,
but we show them in two columns to preserve space.
trained to the cognate set via the Baum-Welch algo-
rithm, and then each word in the set is aligned to
the model using the Viterbi algorithm. The words
are added to the training via a summation; therefore,
the order in which the words are considered has no
effect, in contrast to iterative pairwise methods.
The setting of the parameter values is discussed in
section 6.
5.1 Results
To evaluate Profile HMMs for multiple cognate
alignment, we analyzed the alignments generated for
a number of cognate sets. We found that increasing
the pseudocount weight to 100 improved the quality
of the alignments by effectively biasing the model
towards similar characters according to the substitu-
tion matrix.
Figure 4 shows the Profile HMM alignment for a
cognate set of words with the meaning ?day.? As
with Figure 2, the alignment?s first line is a guide
label used to indicate which columns are match
columns and which are insert columns; note that
consecutive insert columns represent the same insert
state and so are not aligned by the Profile HMM.
While there were some duplicate words (i.e. words
that had identical English orthographic representa-
tions but came from different languages), we do not
show them here for brevity.
In this example, we see that the Profile HMM
manages to identify those columns that are more
highly conserved as match states. The ability to
identify characters that are similar and align them
correctly can be attributed to the provided substitu-
tion matrix.
Note that the characters in the insert columns
should not be treated as aligned even though they
represent emissions from the same insert state (this
highlights the difference between match and insert
states). For example, Y, A, Z, D, R, and O are all
placed in a single insert column even though they
cannot be traced to a single phoneme in a protoform
of the cognate set. Particularly infrequent charac-
ters are more likely to be put together than separated
even if they are phonetically dissimilar.
There is some difficulty, also evident from other
alignments we generated, in isolating phonemes rep-
resented by pairs of characters (digraphs) as singular
entities. In the given example, this means that the dz
in dzien was modelled as a match state and then an
insert state. This is, however, an inherent difficulty
in using data represented only with the English al-
phabet, which could potentially be addressed if the
data were instead represented in a standard phonetic
notation such as IPA.
6 Cognate set matching
Evaluating alignments in a principled way is diffi-
cult because of the lack of a gold standard. To adjust
for this, we also evaluate Profile HMMs for the task
of matching a word to the correct cognate set from
a list of cognate sets with the same meaning as the
given word, similar to the evaluation of a biologi-
cal sequence for membership in a family. This is
realized by removing one word at a time from each
word list and then using the resulting cognate sets
within the meaning as possible targets. A model is
generated from each possible target and a log-odds
score is computed for the word using the forward
algorithm. The scores are then sorted and the high-
est score is taken to be the cognate set to which the
given word belongs. The accuracy is then the frac-
tion of times the correct cognate set is identified.
To determine the best parameter values, we used
a development set of 10 meanings (roughly 5%
of the data). For the substitution matrix pseudo-
count method, we used a log-odds similarity ma-
trix derived from Pair HMM training (Mackay and
Kondrak, 2005). The best results were achieved
with favouring of match states enabled, substitution-
matrix-based pseudocount, pseudocount weight of
0.5, and pseudocounts added during Baum-Welch.
47
6.1 Results
We employed two baselines to generate scores be-
tween a given word and cognate set. The first base-
line uses the average edit distance of the test word
and the words in the given cognate set as the score
of the word against the set. The second baseline is
similar but uses the minimum edit distance between
the test word and any word in the given cognate set
as the score of the word against the entire set. For ex-
ample, in the example set given in Figure 4, the aver-
age edit distance between zen and all other words in
the set is 2.58 (including the hidden duplicate words)
and the minimum edit distance is 1. All other can-
didate sets are similarly scored and the one with the
lowest score is considered to be the correct cluster
with ties broken randomly.
With the parameter settings described in the pre-
vious section, the Profile HMM method correctly
identifies the corresponding cognate set with an ac-
curacy of 93.2%, a substantial improvement over the
average edit distance baseline, which obtains an ac-
curacy of 77.0%.
Although the minimum edit distance baseline also
yields an impressive accuracy of 91.0%, its score is
based on a single word in the candidate set, and so
would not be appropriate for cases where consider-
ation of the entire set is necessary. Furthermore, the
baseline benefits from the frequent presence of du-
plicate words in the cognate sets. Profile HMMs are
more robust, thanks to the presence of identical or
similar characters in corresponding positions.
7 Conclusions
Profile HMMs present an approach for working with
sets of words. We tested their use for two cognate-
related tasks. The method produced good-quality
multiple cognate alignments, and we believe that
they could be further improved with phonetically
transcribed data. For the task of matching words to
correct cognate sets, we achieved an improvement
over the average edit distance and minimum edit dis-
tance baselines.
Since Profile HMM training is highly sensitive to
the choice of initial model, we would like to ex-
plore more informed methods of constructing the
initial model. Similarly, for building models from
unaligned sequences, the addition of domain knowl-
edge would likely prove beneficial. We also plan to
investigate better pseudocount methods, as well as
the possibility of using n-grams as output symbols.
By simultaneously considering an entire set of re-
lated words, Profile HMMs provide a distinct ad-
vantage over iterative pairwise methods. The suc-
cess on our tasks of multiple alignment and cognate
set matching suggests applicability to similar tasks
involving words, such as named entity recognition
across potentially multi-lingual corpora.
Acknowledgements
We thank Qing Dou for organizing the cognate sets
from the original data. We are also grateful to the
anonymous reviewers for their valuable comments.
This research was funded in part by the Natural Sci-
ences and Engineering Research Council of Canada.
References
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proc. of EMNLP, pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of NAACL-HLT, pages
16?23.
Michael A. Covington. 1996. An algorithm to align
words for historical comparison. Computational Lin-
guistics, 22(4):481?496.
Michael A. Covington. 1998. Alignment of multi-
ple languages for historical comparison. In Proc. of
COLING-ACL, pages 275?279.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological sequence analy-
sis: probabilistic models of proteins and nucleic acids.
Cambridge University Press.
Isidore Dyen, Joseph B. Kruskal, and Paul Black. 1992.
An Indoeuropean classification: A lexicostatistical ex-
periment. Transactions of the American Philosophical
Society, 82(5).
Grzegorz Kondrak. 2000. A new algorithm for the align-
ment of phonetic sequences. In Proc. of NAACL, pages
288?295.
Wesley Mackay and Grzegorz Kondrak. 2005. Comput-
ing word similarity and identifying cognates with pair
hidden Markov models. In Proc. of CoNLL, pages 40?
47.
John Nerbonne and Wilbert Heeringa. 1997. Measur-
ing dialect distance phonetically. In Proc. of the Third
Meeting of ACL SIGPHON.
48
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 28?31,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
DIRECTL: a Language-Independent Approach to Transliteration
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Kenneth Dwyer, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,abhargava,qdou,dwyer,kondrak}@cs.ualberta.ca
Abstract
We present DIRECTL: an online discrimi-
native sequence prediction model that em-
ploys a many-to-many alignment between
target and source. Our system incorpo-
rates input segmentation, target charac-
ter prediction, and sequence modeling in
a unified dynamic programming frame-
work. Experimental results suggest that
DIRECTL is able to independently dis-
cover many of the language-specific reg-
ularities in the training data.
1 Introduction
In the transliteration task, it seems intuitively im-
portant to take into consideration the specifics of
the languages in question. Of particular impor-
tance is the relative character length of the source
and target names, which vary widely depending on
whether languages employ alphabetic, syllabic, or
ideographic scripts. On the other hand, faced with
the reality of thousands of potential language pairs
that involve transliteration, the idea of a language-
independent approach is highly attractive.
In this paper, we present DIRECTL: a translit-
eration system that, in principle, can be applied to
any language pair. DIRECTL treats the transliter-
ation task as a sequence prediction problem: given
an input sequence of characters in the source lan-
guage, it produces the most likely sequence of
characters in the target language. In Section 2,
we discuss the alignment of character substrings
in the source and target languages. Our transcrip-
tion model, described in Section 3, is based on
an online discriminative training algorithm that
makes it possible to efficiently learn the weights
of a large number of features. In Section 4, we
provide details of alternative approaches that in-
corporate language-specific information. Finally,
in Section 5 and 6, we compare the experimental
results of DIRECTL with its variants that incor-
porate language-specific pre-processing, phonetic
alignment, and manual data correction.
2 Transliteration alignment
In the transliteration task, training data consist of
word pairs that map source language words to
words in the target language. The matching be-
tween character substrings in the source word and
target word is not explicitly provided. These hid-
den relationships are generally known as align-
ments. In this section, we describe an EM-based
many-to-many alignment algorithm employed by
DIRECTL. In Section 4, we discuss an alternative
phonetic alignment method.
We apply an unsupervised many-to-many align-
ment algorithm (Jiampojamarn et al, 2007) to the
transliteration task. The algorithm follows the ex-
pectation maximization (EM) paradigm. In the
expectation step shown in Algorithm 1, partial
counts ? of the possible substring alignments are
collected from each word pair (xT , yV ) in the
training data; T and V represent the lengths of
words x and y, respectively. The forward prob-
ability ? is estimated by summing the probabili-
ties of all possible sequences of substring pairings
from left to right. The FORWARD-M2M procedure
is similar to lines 5 through 12 of Algorithm 1, ex-
cept that it uses Equation 1 on line 8, Equation 2
on line 12, and initializes ?0,0 := 1. Likewise, the
backward probability ? is estimated by summing
the probabilities from right to left.
?t,v += ?(xtt?i+1, ?)?t?i,v (1)
?t,v += ?(xtt?i+1, yvv?j+1)?t?i,v?j (2)
The maxX and maxY variables specify the
maximum length of substrings that are permitted
when creating alignments. Also, for flexibility, we
allow a substring in the source word to be aligned
with a ?null? letter (?) in the target word.
28
Algorithm 1: Expectation-M2M alignment
Input: xT , yV ,maxX,maxY, ?
Output: ?
? := FORWARD-M2M (xT , yV ,maxX,maxY )1
? := BACKWARD-M2M (xT , yV ,maxX,maxY )2
if (?T,V = 0) then3
return4
for t = 0 . . . T , v = 0 . . . V do5
if (t > 0) then6
for i = 1 . . .maxX st t? i ? 0 do7
?(xtt?i+1, ?) +=
?t?i,v?(xtt?i+1,?)?t,v
?T,V8
if (v > 0 ? t > 0) then9
for i = 1 . . .maxX st t? i ? 0 do10
for j = 1 . . . maxY st v ? j ? 0 do11
?(xtt?i+1, yvv?j+1) +=
?t?i,v?j?(xtt?i+1,y
v
v?j+1)?t,v
?T,V12
In the maximization step, we normalize the par-
tial counts ? to the alignment probability ? using
the conditional probability distribution. The EM
steps are repeated until the alignment probability
? converges. Finally, the most likely alignment for
each word pair in the training data is computed
with the standard Viterbi algorithm.
3 Discriminative training
We adapt the online discriminative training frame-
work described in (Jiampojamarn et al, 2008) to
the transliteration task. Once the training data has
been aligned, we can hypothesize that the ith let-
ter substring xi ? x in a source language word
is transliterated into the ith substring yi ? y in
the target language word. Each word pair is rep-
resented as a feature vector ?(x,y). Our feature
vector consists of (1) n-gram context features, (2)
HMM-like transition features, and (3) linear-chain
features. The n-gram context features relate the
letter evidence that surrounds each letter xi to its
output yi. We include all n-grams that fit within
a context window of size c. The c value is deter-
mined using a development set. The HMM-like
transition features express the cohesion of the out-
put y in the target language. We make a first order
Markov assumption, so that these features are bi-
grams of the form (yi?1, yi). The linear-chain fea-
tures are identical to the context features, except
that yi is replaced with a bi-gram (yi?1, yi).
Algorithm 2 trains a linear model in this fea-
ture space. The procedure makes k passes over
the aligned training data. During each iteration,
the model produces the nmost likely output words
Y?j in the target language for each input word xj
in the source language, based on the current pa-
Algorithm 2: Online discriminative training
Input: Data {(x1,y1), (x2,y2), . . . , (xm,ym)},
number of iterations k, size of n-best list n
Output: Learned weights ?
? := ~01
for k iterations do2
for j = 1 . . .m do3
Y?j = {y?j1, . . . , y?jn} = argmaxy[? ? ?(xj ,y)]4
update ? according to Y?j and yj5
return ?6
rameters ?. The values of k and n are deter-
mined using a development set. The model param-
eters are updated according to the correct output
yj and the predicted n-best outputs Y?j , to make
the model prefer the correct output over the in-
correct ones. Specifically, the feature weight vec-
tor ? is updated by using MIRA, the Margin In-
fused Relaxed Algorithm (Crammer and Singer,
2003). MIRA modifies the current weight vector
?o by finding the smallest changes such that the
new weight vector ?n separates the correct and in-
correct outputs by a margin of at least ?(y, y?), the
loss for a wrong prediction. We define this loss to
be 0 if y? = y; otherwise it is 1 + d, where d is
the Levenshtein distance between y and y?. The
update operation is stated as a quadratic program-
ming problem in Equation 3. We utilize a function
from the SVMlight package (Joachims, 1999) to
solve this optimization problem.
min?n ? ?n ? ?o ?
subject to ?y? ? Y? :
?n ? (?(x,y) ? ?(x, y?)) ? ?(y, y?)
(3)
The argmax operation is performed by an exact
search algorithm based on a phrasal decoder (Zens
and Ney, 2004). This decoder simultaneously
finds the l most likely substrings of letters x that
generate the most probable output y, given the
feature weight vector ? and the input word xT .
The search algorithm is based on the following dy-
namic programming recurrence:
Q(0, $) = 0
Q(t, p) = max
p?,p,
t?maxX?t?<t
{? ? ?(xtt?+1, p?, p) +Q(t?, p?)}
Q(T+1, $) = max
p?
{? ? ?($, p?, $) +Q(T, p?)}
To find the n-best predicted outputs, the table
Q records the top n scores for each output sub-
string that has the suffix p substring and is gen-
erated by the input letter substring xt1; here, p? is
29
a sub-output generated during the previous step.
The notation ?(xtt?+1, p?, p) is a convenient way
to describe the components of our feature vector
?(x,y). The n-best predicted outputs Y? can be
discovered by backtracking from the end of the ta-
ble, which is denoted by Q(T + 1, $).
4 Beyond DIRECTL
4.1 Intermediate phonetic representation
We experimented with converting the original Chi-
nese characters to Pinyin as an intermediate repre-
sentation. Pinyin is the most commonly known
Romanization system for Standard Mandarin. Its
alphabet contains the same 26 letters as English.
Each Chinese character can be transcribed pho-
netically into Pinyin. Many resources for Pinyin
conversion are available online.1 A small percent-
age of Chinese characters have multiple pronunci-
ations represented by different Pinyin representa-
tions. For those characters (about 30 characters in
the transliteration data), we manually selected the
pronunciations that are normally used for names.
This preprocessing step significantly reduces the
size of target symbols from 370 distinct Chinese
characters to 26 Pinyin symbols which enables our
system to produce better alignments.
In order to verify whether the addition of
language-specific knowledge can improve the
overall accuracy, we also designed intermediate
representations for Russian and Japanese. We
focused on symbols that modify the neighbor-
ing characters without producing phonetic output
themselves: the two yer characters in Russian,
and the long vowel and sokuon signs in Japanese.
Those were combined with the neighboring char-
acters, creating new ?super-characters.?
4.2 Phonetic alignment with ALINE
ALINE (Kondrak, 2000) is an algorithm that
performs phonetically-informed alignment of two
strings of phonemes. Since our task requires
the alignment of characters representing different
writing scripts, we need to first replace every char-
acter with a phoneme that is the most likely to be
produced by that character.
We applied slightly different methods to the
test languages. In converting the Cyrillic script
into phonemes, we take advantage of the fact
that the Russian orthography is largely phonemic,
which makes it a relatively straightforward task.
1For example, http://www.chinesetopinyin.com/
In Japanese, we replace each Katakana character
with one or two phonemes using standard tran-
scription tables. For the Latin script, we simply
treat every letter as an IPA symbol (International
Phonetic Association, 1999). The IPA contains a
subset of 26 letter symbols that tend to correspond
to the usual phonetic value that the letter repre-
sents in the Latin script. The Chinese characters
are first converted to Pinyin, which is then handled
in the same way as the Latin script.
Similar solutions could be engineered for other
scripts. We observed that the transcriptions do not
need to be very precise in order for ALINE to pro-
duce high quality alignments.
4.3 System combination
The combination of predictions produced by sys-
tems based on different principles may lead to im-
proved prediction accuracy. We adopt the follow-
ing combination algorithm. First, we rank the in-
dividual systems according to their top-1 accuracy
on the development set. To obtain the top-1 pre-
diction for each input word, we use simple voting,
with ties broken according to the ranking of the
systems. We generalize this approach to handle n-
best lists by first ordering the candidate translitera-
tions according to the highest rank assigned by any
of the systems, and then similarly breaking ties by
voting and system ranking.
5 Evaluation
In the context of the NEWS 2009 Machine
Transliteration Shared Task (Li et al, 2009), we
tested our system on six data sets: from English to
Chinese (EnCh) (Li et al, 2004), Hindi (EnHi),
Russian (EnRu) (Kumaran and Kellner, 2007),
Japanese Katakana (EnJa), and Korean Hangul
(EnKo); and from Japanese Name to Japanese
Kanji (JnJk)2. We optimized the models? param-
eters by training on the training portion of the
provided data and measuring performance on the
development portion. For the final testing, we
trained the models on all the available labeled data
(training plus development data). For each data
set, we converted any uppercase letters to lower-
case. Our system outputs the top 10 candidate an-
swers for each input word.
Table 1 reports the performance of our system
on the development and final test sets, measured
in terms of top-1 word accuracy (ACC). For cer-
tain language pairs, we tested variants of the base
2http://www.cjk.org/
30
Task Model Dev Test
EnCh DIRECTL 72.4 71.7
INT(M2M) 73.9 73.4
INT(ALINE) 73.8 73.2
COMBINED 74.8 74.6
EnHi DIRECTL 41.4 49.8
DIRECTL+MC 42.3 50.9
EnJa DIRECTL 49.9 50.0
INT(M2M)? 49.6 49.2
INT(ALINE) 48.3 51.0
COMBINED? 50.6 50.5
EnKo DIRECTL 36.7 38.7
EnRu DIRECTL 80.2 61.3
INT(M2M) 80.3 60.8
INT(ALINE) 80.0 60.7
COMBINED? 80.3 60.8
JnJk DIRECTL 53.5 56.0
Table 1: Top-1 word accuracy on the development
and test sets. The asterisk denotes the results ob-
tained after the test reference sets were released.
system described in Section 4. DIRECTL refers
to our language-independent model, which uses
many-to-many alignments. The INT abbreviation
denotes the models operating on the language-
specific intermediate representations described in
Section 4.1. The alignment algorithm (ALINE or
M2M) is given in brackets.
In the EnHi set, many names consisted of mul-
tiple words: we assumed a one-to-one correspon-
dence between consecutive English words and
consecutive Hindi words. In Table 1, the results in
the first row (DIRECTL) were obtained with an au-
tomatic cleanup script that replaced hyphens with
spaces, deleted the remaining punctuation and nu-
merical symbols, and removed 43 transliteration
pairs with a disagreement between the number of
source and target words. The results in the sec-
ond row (DIRECTL+MC) were obtained when the
cases with a disagreement were individually ex-
amined and corrected by a Hindi speaker.
We did not incorporate any external resources
into the models presented in Table 1. In order
to emphasize the performance of our language-
independent approach, we consistently used the
DIRECTL model for generating our ?standard?
runs on all six language pairs, regardless of its rel-
ative performance on the development sets.
6 Discussion
DIRECTL, our language-independent approach to
transliteration achieves excellent results, espe-
cially on the EnCh, EnRu, and EnHi data sets,
which represent a wide range of language pairs
and writing scripts. Both the many-to-many
and phonetic alignment algorithms produce high-
quality alignments. The former can be applied di-
rectly to the training data without the need for an
intermediate representation, while the latter does
not require any training. Surprisingly, incorpo-
ration of language-specific intermediate represen-
tations does not consistently improve the perfor-
mance of our system, which indicates that DI-
RECTL may be able to discover the structures im-
plicit in the training data without additional guid-
ance. The EnHi results suggest that manual clean-
ing of noisy data can yield noticeable gains in ac-
curacy. On the other hand, a simple method of
combining predictions from different systems pro-
duced clear improvement on the EnCh set, but
mixed results on two other sets. More research on
this issue is warranted.
Acknowledgments
This research was supported by the Alberta Inge-
nuity, Informatics Circle of Research Excellence
(iCORE), and Natural Sciences of Engineering
Research Council of Canada (NSERC).
References
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
International Phonetic Association. 1999. Handbook
of the International Phonetic Association. Cam-
bridge University Press.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL, pages 905?913.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in kernel methods:
support vector learning, pages 169?184. MIT Press.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288?295.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. SI-
GIR, pages 721?722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source channel model for machine transliteration. In
Proc. ACL, pages 159?166.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. ACL-
IJCNLP Named Entities Workshop.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proc. HLT-NAACL, pages 257?264.
31
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 293?303,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Predicting the Semantic Compositionality of Prefix Verbs
Shane Bergsma, Aditya Bhargava, Hua He, Grzegorz Kondrak
Department of Computing Science
University of Alberta
{bergsma,abhargava,hhe,kondrak}@cs.ualberta.ca
Abstract
In many applications, replacing a complex
word form by its stem can reduce sparsity, re-
vealing connections in the data that would not
otherwise be apparent. In this paper, we focus
on prefix verbs: verbs formed by adding a pre-
fix to an existing verb stem. A prefix verb is
considered compositional if it can be decom-
posed into a semantically equivalent expres-
sion involving its stem. We develop a clas-
sifier to predict compositionality via a range
of lexical and distributional features, includ-
ing novel features derived from web-scale N-
gram data. Results on a new annotated cor-
pus show that prefix verb compositionality can
be predicted with high accuracy. Our system
also performs well when trained and tested on
conventional morphological segmentations of
prefix verbs.
1 Introduction
Many verbs are formed by adding prefixes to exist-
ing verbs. For example, remarry is composed of a
prefix, re-, and a stem, marry. We present an ap-
proach to predicting the compositionality of prefix
verbs. The verb remarry is compositional; it means
to marry again. On the other hand, retire is gener-
ally non-compositional; it rarely means to tire again.
There is a continuum of compositionality in prefix
verbs, as in other complex word forms and multi-
word expressions (Bannard et al, 2003; Creutz and
Lagus, 2005; Fazly et al, 2009; Xu et al, 2009).
We adopt a definition of compositionality specifi-
cally designed to support downstream applications
that might benefit from knowledge of verb stems.
For example, suppose our corpus contains the fol-
lowing sentence: ?Pope Clement VII denied Henry
VIII permission to marry again before a decision
was given in Rome.? A user might submit the ques-
tion, ?Which pope refused Henry VIII permission to
remarry?? If we can determine that the meaning of
remarry could also be provided via the stem marry,
we could add marry to our search terms. This is
known as morphological query expansion (Bilotti et
al., 2004). Here, such an expansion leads to a better
match between question and answer.
Previous work has shown that ?full morpholog-
ical analysis provides at most very modest bene-
fits for retrieval? (Manning et al, 2008). Stem-
ming, lemmatization, and compound-splitting often
increase recall at the expense of precision, but the
results depend on the morphological complexity of
the text?s language (Hollink et al, 2004).
The lack of success in applying morphological
analysis in IR is unsurprising given that most pre-
vious systems are not designed with applications
in mind. For example, the objective of the influ-
ential Linguistica program is ?to produce an out-
put that matches as closely as possible the analy-
sis that would be given by a human morphologist?
(Goldsmith, 2001). Unsupervised systems achieve
this aim by exploiting learning biases such as min-
imum description length for lexicons (Goldsmith,
2001; Creutz and Lagus, 2007) and high entropy
across morpheme boundaries (Keshava and Pitler,
2006). Supervised approaches learn directly from
words annotated by morphologists (Van den Bosch
and Daelemans, 1999; Toutanova and Cherry, 2009),
often using CELEX, a lexical database that includes
293
morphological information (Baayen et al, 1996).
The conventional approach in morphology is to
segment words into separate morphemes even when
the words are not entirely compositional combina-
tions of their parts (Creutz and Lagus, 2005). For
example, while co- is considered a separate mor-
pheme in the verb cooperate, the meaning of coop-
erate is not simply to operate jointly. These forms
are sometimes viewed as perturbations of compo-
sition (de Marken, 1996). In practice, a user may
query, ?Which nations do not cooperate with the In-
ternational Criminal Court?? An expansion of the
query to include operate may have undesirable con-
sequences.
Rather than relying on conventional standards, we
present an algorithm whose objective is to find only
those prefix verbs that exhibit semantic composi-
tionality; i.e., prefix verbs that are fully meaning-
preserving, sums-of-their-parts. We produce a new
corpus, annotated according to this definition. We
use these annotated examples to learn a discrimina-
tive model of semantic compositionality.
Our classifier relies on a variety of features that
exploit the distributional patterns of verbs and stems.
We build on previous work that applies semantics
to morphology (Yarowsky and Wicentowski, 2000;
Schone and Jurafsky, 2001; Baroni et al, 2002), and
also on work that exploits web-scale data for seman-
tic analysis (Turney, 2001; Nakov, 2007; Kummer-
feld and Curran, 2008). For example, we measure
how often a prefix verb appears with a hyphen be-
tween the prefix and stem. We also look at the dis-
tribution of the stem as a separate word: we calculate
the probability of the prefix verb and the separated
stem?s co-occurrence in a segment of discourse; we
also calculate the distributional similarity between
the verb and the separated stem. High scores for
these measures indicate compositionality. We ex-
tract counts from a web-scale N-gram corpus, allow-
ing us to efficiently leverage huge volumes of unla-
beled text.
Our system achieves 93.6% accuracy on held-out
data, well above several baselines and comparison
systems. We also train and test our system on con-
ventional morphological segmentations. Our clas-
sifier remains reliable in this setting, making half
as many errors as the state-of-the-art unsupervised
Morfessor system (Creutz and Lagus, 2007).
2 Problem Definition and Setting
A prefix verb is a derived word with a bound mor-
pheme as prefix. While derivation can change both
the meaning and part-of-speech of a word (as op-
posed to inflection, which does not change ?referen-
tial or cognitive meaning? (Katamba, 1993)), here
the derived form remains a verb.
We define prefix-verb compositionality as a se-
mantic equivalence between a verb and a paraphrase
involving the verb?s stem. The stem must be used as
a verb in the paraphrase. Words can be introduced,
if needed, to account for the meaning contributed by
the prefix, e.g., outbuild?build more/better/faster
than. A bidirectional entailment between the prefix
verb and the paraphrase is required.
Words can have different meanings in different
contexts. For example, a nation might ?resort to
force,? (non-compositional) while a computer pro-
gram can ?resort a linked list? (compositional). We
therefore define prefix-verb compositionality as a
context-specific property of verb tokens rather than
a global property of verb types. However, it is worth
noting that we ultimately found the compositionality
of types to be very consistent across contexts (Sec-
tion 5.1.2), and we were unable to leverage contex-
tual information to improve classification accuracy;
our final system is essentially type-based. Other re-
cent morphological analyzers have also been type-
based (Keshava and Pitler, 2006; Poon et al, 2009).
Our system takes as input a verb token in unin-
flected form along with its sentence as context. The
verb must be divisible into an initial string and a fol-
lowing remainder such that the initial string is on
our list of prefixes and the remainder is on our list of
stems. Hyphenation is allowed, e.g., both re-enter
and reenter are acceptable inputs. The system deter-
mines whether the prefix/stem combination is com-
positional in the current context. For example, the
verb unionize in, ?The workers must unionize,? can
be divided into a prefix un- and a stem ionize. The
system should determine that here unionize is not a
compositional combination of these parts.
The algorithm requires a list of prefixes and stems
in a given language. For our experiments, we use
both dictionary and corpus-based methods to con-
struct these lists (Section 4).
294
3 Supervised Compositionality Detection
We use a variety of lexical and statistical informa-
tion when deciding whether a prefix verb is compo-
sitional. We adopt a discriminative approach. We
assume some labeled examples are available to train
a classifier. Relevant information is encoded in a
feature vector, and a learning algorithm determines
a set of weights for the features using the training
data. As compositionality is a binary decision, we
can adopt any standard package for binary classifi-
cation. In our experiments we use support vector
machines.
Our features include both local information that
depends only on the verb string (sometimes referred
to as lexical features) and also global information
that depends on the verb and the stem?s distribution
in text. Our approach can therefore be regarded as a
simple form of semi-supervised learning; we lever-
age both a small number of labeled examples and a
large volume of unlabeled text.
If a frequency or similarity is undefined in our cor-
pus, we indicate this with a separate feature; weights
on these features act as a kind of smoothing.
3.1 Features based on Web-Scale N-gram Data
We use web-scale N-gram data to extract distribu-
tional features. The most widely-used N-gram cor-
pus is the Google 5-gram Corpus (Brants and Franz,
2006). We use Google V2: a new N-gram corpus
(also with N-grams of length one-to-five) created
from the same one-trillion-word snapshot of the web
as the Google 5-gram Corpus, but with enhanced fil-
tering and processing of the source text (Lin et al,
2010). For Google V2, the source text was also part-
of-speech tagged, and the resulting part-of-speech
tag distribution is included for each N-gram. There
are 4.1 billion N-grams in the corpus.
The part-of-speech tag distributions are particu-
larly useful, as they allow us to collect verb-specific
counts. For example, while a string like reuse oc-
curs 1.1 million times in the web corpus, it is only
tagged as a verb 270 thousand times. Conflating the
noun/verb senses can lead to misleading scores for
certain features. E.g., the hyphenation frequency
of re-use would appear relatively low, even though
reuse is semantically compositional.
Lin et al (2010) also provide a high-coverage,
10-million-phrase set of clusters extracted from the
N-grams; we use these for our similarity features
(Section 3.1.3). There are 1000 clusters in total.
The data does not provide the context vectors for
each phrase; rather, each phrase is listed with its 20
most similar clusters, measured by cosine similar-
ity with the cluster centroid. We use these centroid
similarities as values in a 1000-dimensional cluster-
membership feature space. To calculate the similar-
ity between two verbs, we calculate the cosine simi-
larity between their cluster-membership vectors.
The feature classes in the following four subsec-
tions each make use of web-scale N-gram data.
3.1.1 HYPH features
Hyphenated verbs are usually compositional (e.g.,
re-elect). Of course, a particular instance of a com-
positional verb may or may not occur in hyphenated
form. However, across a large corpus, compositional
prefix verbs tend to occur in a hyphenated form more
often than do non-compositional prefix verbs. We
therefore provide real-valued features for how often
the verb was hyphenated and unhyphenated on the
web. For example, we collect counts for the fre-
quencies of re-elect (33K) and reelect (9K) in our
web corpus, and we convert the frequencies to log-
counts. We also give real-valued features for the hy-
phenated/unhyphenated log-counts using only those
occurrences of the verb that were tagged as a verb,
exploiting the tag distributions in our web corpus as
described above.
Nakov and Hearst (2005) previously used hy-
phenation counts as an indication of a syntactic re-
lationship between nouns. In contrast, we leverage
hyphenation counts as an indication of a semantic
property of verbs.
3.1.2 COOC features
COOC features, and also the SIM (Section 3.1.3)
and YAH (Section 3.2.2) features, concern the asso-
ciation in text between the prefix verb and its stem,
where the stem occurs as a separate word. We call
this the separated stem.
If a prefix verb is compositional, it is more likely
to occur near its separated stem in text. We often
see agree and disagree, read and reread, etc. occur-
ring in the same segment of discourse. We create
features for the association of the prefix verb and its
295
separated stem in a discourse. We include the log-
count of how often the verb and stem occur in the
same N-gram (of length 2-to-5) in our N-gram cor-
pus. Note that the 2-to-4-gram counts are not strictly
a subset of the 5-gram counts, since fewer 5-grams
pass the data?s minimum frequency threshold.
We also include a real-valued pointwise mutual
information (PMI) feature for the verb and separated
stem?s co-occurrence in an N-gram. For the PMI, we
regard occurrence in an N-gram as an event, and cal-
culate the probability that a verb and separated stem
jointly occur in an N-gram, divided by the probabil-
ity of their occurring in an N-gram independently.
3.1.3 SIM features
If a prefix verb is compositional, it should oc-
cur in similar contexts to its stem. The idea that
a stem and stem+affix should be semantically sim-
ilar has been exploited previously for morphological
analysis (Schone and Jurafsky, 2000). We include
a real-valued feature for the distributional similar-
ity of the verb and stem using Lin?s thesaurus (Lin,
1998). The coverage of this measure was low: it
was non-zero for only 93 of the 1000 prefix verbs in
our training set. We therefore also include distribu-
tional similarity calculated using the web-scale 10-
million-phrase clustering as described above. Us-
ing this data, similarity is defined for 615 of the
1000 training verbs. We also explored a variety of
WordNet-based similarity measures, but these ulti-
mately did not prove helpful on development data.
3.1.4 FRQ features
We include real-valued features for the raw fre-
quencies of the verb and the stem on the web. If
these frequencies are widely different, it may in-
dicate a non-compositional usage. Yarowsky and
Wicentowski (2000) use similar statistics to iden-
tify words related by inflection, but they gather their
counts from a much smaller corpus. In addition,
higher-frequency prefix verbs may be a priori more
likely to be non-compositional. A certain frequency
is required for an irregular usage to become famil-
iar to language speakers. The potential correlation
between frequency and non-compositionality could
thus also be exploited by the classifier via the FRQ
features.
3.2 Other Features
3.2.1 LEX features
We provide lexical features for various aspects
of a prefix verb. Binary features indicate the oc-
currence of particular verbs, prefixes, and stems,
and whether the prefix verb is hyphenated. While
hyphenated prefix verbs are usually compositional,
even non-compositional prefix verbs may be hy-
phenated if the prefix and stem terminate and be-
gin with a vowel, respectively. For example, non-
compositional uses of co-operate are often hyphen-
ated, whereas the compositional remarry is rarely
hyphenated. We therefore have indicator features
for the conjunction of the prefix and the first letter
of the stem (e.g., co-o), and also for the prefix con-
joined with a flag indicating whether the stem begins
with a vowel (e.g., co+vowel).
3.2.2 YAH features
While the COOC features capture many cases
where the verb and separated stem occur in close
proximity (especially, but not limited to, conjunc-
tions), there are many other cases where a longer
distance might separate a compositional verb and
its separated stem. For example, consider the sen-
tence, ?Brush the varnish on, but do not overbrush.?
Here, the verb and separated stem do not co-occur
within a 5-gram window, and their co-occurrence
will therefore not be recorded in our N-gram cor-
pus. As an approximation for co-occurrence counts
within a longer segment of discourse, we count the
number of pages on the web where the verb and sep-
arated stem co-occur. We use hit-counts returned
by the Yahoo search engine API.1 Similar to our
COOC features, we include a real-valued feature for
the pointwise mutual information of the prefix verb
and separated stem?s co-occurrence on a web page,
i.e., we use Turney?s PMI-IR (Turney, 2001).
Baroni et al (2002) use similar statistics to help
discover morphologically-related words. In contrast
to our features, however, their counts are derived
from source text that is several orders of magnitude
smaller in size.
1http://developer.yahoo.com/search/boss/
296
3.2.3 DIC features
One potentially useful resource, when available,
is a dictionary of the conventional morphological
segmentations of words in the language. Although
these segmentations have been created for a differ-
ent objective than that of our annotations, we hy-
pothesize that knowledge of morphology can help
inform our system?s predictions. For each prefix
verb, we include features for whether or not the pre-
fix and stem are conventionally segmented into sep-
arate morphemes, according to a morphological dic-
tionary. Similar to the count-based features, we in-
clude a DIC-undefined feature for the verbs that are
not in the dictionary; any precompiled dictionary
will have imperfect coverage of actual test examples.
Interestingly, DIC features are found to be among
our least useful features in the final evaluation.
4 Experiments
4.1 Resources
We use CELEX (Baayen et al, 1996) as our dictio-
nary for the DIC features. We also use CELEX to help
extract our lists of prefixes and stems. We take ev-
ery prefix that is marked in CELEX as forming a new
verb by attaching to an existing verb. For stems, we
use every verb that occurs in CELEX, but we also
extend this list by automatically collecting a large
number of words that were automatically tagged as
verbs in the NYT section of Gigaword (Graff, 2003).
To be included in the extra-verb list, a verb must oc-
cur more than ten times and be tagged as a verb more
than 70% of the time by a part-of-speech tagger. We
thereby obtain 43 prefixes and 6613 stems.2 We
aimed for an automatic, high-precision list for our
initial experiments. This procedure is also amenable
to human intervention; one could alternatively cast a
wider net for possible stems and then manually filter
false positives.
4.2 Annotated Data
We carried out a medium-scale annotation to provide
training and evaluation data for our experiments.3
2The 43 prefixes are: a- ab- ac- ad- as- be- circum- co- col-
com- con- cor- counter- cross- de- dis- e- em- en- ex- fore- im-
in- inter- ir- mis- out- over- per- photo- post- pre- pro- psycho-
re- sub- super- sur- tele- trans- un- under- with-
3Our annotated data is publicly available at:
http://www.cs.ualberta.ca/?ab31/verbcomp/
The data for our annotations also comes from the
NYT section of Gigaword. We first build a list of
possible prefix verbs. We include any verb that a) is
composed of a valid prefix and stem; and b) occurs
at least twice in the corpus.4 If the verb occurs less
than 50 times in the corpus, we also require that it
was tagged as a verb in at least 70% of cases. This
results in 2077 possible prefix verbs for annotation.
For each verb type in our list of possible prefix
verbs, we randomly select for annotation sentences
from Gigaword containing the verb. We take at most
three sentences for each verb type so that a few very
common types (such as become, understand, and im-
prove) do not comprise the majority of annotated ex-
amples. The resulting set of sentences includes a
small number of sentences with incorrectly-tagged
non-verbs; these are simply marked as non-verbs
by our annotators and excluded from our final data
sets. A graphical program was created for the an-
notation; the program automatically links to the on-
line Merriam-Webster dictionary entries for the pre-
fix verb and separated stem. When in doubt about
a verb?s meaning, our annotators adhere to the dic-
tionary definitions. A single annotator labeled 1718
examples, indicating for each sentence whether the
prefix verb was compositional. A second annota-
tor then labeled a random subset of 150 of these ex-
amples, and agreement was calculated. The annota-
tors agreed on 137 of the 150 examples. The Kappa
statistic (Jurafsky and Martin, 2000, page 315), with
P(E) computed from the confusion matrices, is 0.82,
above the 0.80 level considered to indicate good re-
liability.
For our experiments, the 1718 annotated exam-
ples are randomly divided into 1000 training, 359
development, and 359 held-out test examples.
4.3 Classifier Settings
We train a linear support vector machine classifier
using the efficient LIBLINEAR package (Fan et al,
2008). We use L2-loss and L2-regularization. We
4We found that the majority of single-occurrence verbs in
the Gigaword data were typos. We would expect true hapax
legomena to be largely compositional, and we could potentially
derive better statistics if we include them (Baayen and Sproat,
1996). One possible option, employed in previous work, is to
ensure words of interest are ?manually corrected for typing er-
rors before further analysis? (Baayen and Renouf, 1996).
297
optimize the choice of features and regularization
hyperparameter on development data, attaining a
maximum when C = 0.1.
4.4 Evaluation
We compare the following systems:
1. Base1: always choose compositional (the ma-
jority class).
2. Base2: for each prefix, choose the majority
class over the verbs having that prefix in train-
ing data.
3. Morf: the unsupervised Morfessor sys-
tem (Creutz and Lagus, 2007) (Categories-
ML, from 110K-word corpus). If Morfessor
splits the prefix and stem into separate mor-
phemes, we take the prediction as composi-
tional. If it does anything else, we take it as
non-compositional.
4. SCD: Supervised Compositionality Detection:
the system proposed in this paper.
We evaluate using accuracy: the percentage of ex-
amples classified correctly in held-out test data.
5 Results
We first analyze our annotations, gaining insight into
the relation between our definition and conventional
segmentations. We also note the consistency of our
annotations across contexts. We then provide the
main results of our system. Finally, we provide the
results of our system when trained and tested on con-
ventional morphological segmentations.
5.1 Analysis of Annotations
5.1.1 Annotation consistency with dictionaries
The majority of our examples are not present in
a morphological dictionary, even in one as compre-
hensive as CELEX. The prefix verbs are in CELEX
for only 670 of the 1718 total annotated instances.
For those that are in CELEX, Table 1 provides
the confusion matrix that relates the CELEX seg-
mentations to our annotations. The table shows
that the major difference between our annotations
and CELEX is that our definition of compositionality
is more strict than conventional morphological seg-
mentations. When CELEX does not segment the pre-
fix from the stem (case 0), our annotations agree in
CELEX segmentation
1 0
Compositionality 1 227 10
annotation 0 250 183
Table 1: Confusion matrix on the subset of prefix verb
annotations that are also in CELEX. 1 indicates that the
prefix and stem are segmented into separate morphemes,
0 indicates otherwise.
183 of 193 cases. When CELEX does split the prefix
from the stem (case 1), the meaning is semantically
compositional in less than half the cases. This is
a key difference between conventional morphology
and our semantic definition.
It is also instructive to analyze the 10 cases that
are semantically compositional but which CELEX
did not segment. Most of these are verbs that are
conventionally viewed as single morphemes because
they entered English as complete words. For exam-
ple, await comes from the Old North French await-
ier, itself from waitier. In practice, it is useful to
know that await is compositional, i.e. that it can be
rephrased as wait for. Downstream applications can
exploit the compositionality of await, but miss the
opportunity if using the conventional lack of seg-
mentation.
5.1.2 Annotation consistency across contexts
We next analyze our annotated data to determine
the consistency of compositionality across different
occurrences of the same prefix-verb type. There are
1248 unique prefix verbs in our 1718 labeled exam-
ples: 45 verbs occur three times, 380 occur twice
and 823 occur only once. Of the 425 verbs that oc-
cur multiple times, only 6 had different annotations
in different examples (i.e., six verbs occur in both
compositional and non-compositional usages in our
dataset). These six instances are subtle, debatable,
and largely uninteresting, depending on distinctions
like whether the proclaim sense of blazon can sub-
stitute for the celebrate sense of emblazon, etc.
It is easy to find clearer ambiguities online,
such as compositional examples of typically non-
compositional verbs (how to recover a couch, when
to redress a wound, etc.). However, in our data verbs
like recover and redress always occur in their more
dominant non-compositional sense. People may
298
Set # Base1 Base2 Morf SCD
Test 359 65.7 87.2 73.8 93.6
? CELEX 128 30.5 73.4 50.8 89.8
/? CELEX 231 85.3 94.8 86.6 95.7
? train 107 69.2 93.5 74.8 97.2
/? train 252 64.3 84.5 73.4 92.1
Table 2: Number of examples (#) and accuracy (%) on
test data, and on in-CELEX vs. not-in-CELEX, and in-
training-data vs. not-in-training splits.
consciously or unconsciously recognize the possi-
bility for confusion and systematically hyphenate
prefixes from the stem if a less-common composi-
tional usage is employed. For example, our data has
?repress your feelings? for the non-compositional
case but the hyphenated ?re-press the center? for the
compositional usage.5
Due to the consistency of compositionality across
contexts, context-based features may simply not be
very useful for classification. All the features we de-
scribe in Section 3 depend only on the prefix verb
itself and not the verb context. Various context-
dependent features did not improve accuracy on our
development data and were thus excluded from the
final system.
5.2 Main Results
The first row of Table 2 gives the results of all
systems on test data. SCD achieves 93.6% ac-
curacy, making one fifth as many errors as the
majority-class baseline (Base1) and half as many er-
rors as the more competitive prefix-based predictor
(Base2). The substantial difference between SCD
and Base2 shows that SCD is exploiting much infor-
mation beyond the trivial memorization of a deci-
sion for each prefix. Morfessor performs better than
Base1 but significantly worse than Base2. This indi-
cates that state-of-the-art unsupervised morpholog-
ical segmentation is not yet practical for semantic
preprocessing. Of course, Morfessor was also de-
signed with a different objective; in Section 5.3 we
compare Morfessor and SCD on conventional mor-
5Note that many examples like recover, repress and redress
are only ambiguous in text, not in speech. Pronunciation re-
duces ambiguity in the same way that hyphens do in text. Con-
versely, observe that knowledge of compositionality could po-
tentially help speech synthesis.
Prefix # Tot # Comp SCD
re- 166 147 95.8
over- 26 25 96.2
out- 23 18 91.3
de- 21 0 100.0
pre- 19 16 94.7
un- 17 1 94.1
dis- 10 0 90.0
under- 9 7 77.8
co- 7 6 100.0
en- 5 2 60.0
Table 3: Total number of examples (# Tot), number of
examples that are compositional (# Comp), and accuracy
(%) of SCD on test data, by prefix.
phological segmentations.
We further analyzed the systems by splitting the
test data two ways.
First, we separate verbs that occur in our mor-
phological dictionary (? CELEX) from those that
do not (/? CELEX). Despite using the dictionary
segmentation itself as a feature, the performance
of SCD is worse on the ? CELEX verbs (89.8%).
The comparison systems drop even more dramati-
cally on this subset. The ? CELEX verbs comprise
the more frequent, irregular verbs in English. Non-
compositionality is the majority class on the exam-
ples that are in the dictionary.
On the other hand, one would expect verbs that
are not in a comprehensive dictionary to be largely
compositional, and indeed most of the /? CELEX
verbs are compositional. However, there is still
much to be gained from applying SCD, which makes
a third as many errors as the system which always
assigns compositional (95.7% for SCD vs. 85.3%
for Base1).
Our second way of splitting the data is to divide
our test set into prefix verbs that also occurred in
training sentences (? train) and those that did not (/?
train). Over 70% did not occur in training. SCD
scores 97.2% accuracy on those that did. The clas-
sifier is thus able to exploit the consistency of anno-
tations across different contexts (Section 5.1.2). The
92.1% accuracy on the /?-train portion also shows
the features allow the system to generalize well to
new, previously-unseen verbs.
Table 3 gives the results of our system on sets of
299
-LEX -HYPH -COOC -SIM -YAH -FRQ -DIC
85.0 92.8 92.5 93.6 93.6 93.6 93.6
85.5 93.6 92.8 93.0 93.3 93.9
86.9 90.5 93.3 93.6 93.6
84.1 90.3 93.3 93.6
87.5 90.5 93.0
85.5 89.4
Table 4: Accuracy (%) of SCD as different feature classes
are removed. Performance with all features is 93.6%.
verbs divided according to their prefix. The table in-
cludes those prefixes that occurred at least 5 times
in the test set. Note that the prefixes have a long
tail: these ten prefixes cover only 303 of the 359
test examples. Accuracy is fairly high across all the
different prefixes. Note also that the three prefixes
de-, un-, and dis- almost always correspond to non-
compositional verbs. Each of these prefixes corre-
sponds to a subtle form of negation, and it is usually
difficult to paraphrase the negation using the stem.
For example, to demilitarize does not mean to not
militarize (or any other simple re-phrasing using the
stem as a verb), and so our annotation marks it as
non-compositional. Whether such a strict strategy is
ultimately best may depend on the target application.
Feature Analysis
We perform experiments to evaluate which features
are most useful for this task. Table 4 gives the ac-
curacy of our system as different feature classes are
removed. A similar table was previously used for
feature analysis in Daume? III and Marcu (2005).
Each row corresponds to performance with a group
of features; each entry is performance with a par-
ticular feature class individually removed the group.
We remove the least helpful feature class from each
group in succession moving group-to-group down
the rows.
We first remove the DIC features. These do not
impact performance on test data. The last row gives
the performance with only HYPH features (85.5, re-
moving LEX), and only LEX features (89.4, remov-
ing HYPH). These are found to be the two most ef-
fective features for this task, followed by the COOC
statistics. The other features, while marginally help-
ful on development data, are relatively ineffective on
the test set. In all cases, removing LEX features hurts
Base1 Base2 Morf SCD
76.0 79.6 72.4 86.4
Table 5: Accuracy (%) on CELEX.
the most. Removing LEX not only removes useful
stem, prefix, and hyphen information, but it also im-
pairs the ability of the classifier to use the other fea-
tures to separate the examples.
5.3 CELEX Experiments and Results
Finally, we train and test our system on prefix verbs
where the segmentation decisions are provided by
a morphological dictionary. We are interested in
whether the strong results of our system could trans-
fer to conventional morphological segmentations.
We extract all verbs in CELEX that are valid verbs
for our system (divisible into a prefix and verb stem),
and take the CELEX segmentation as the label; i.e.,
whether the prefix and stem are separated into dis-
tinct morphemes. We extract 1006 total verbs.
We take 506 verbs for training, 250 verbs as a
development set (to tune our classifier?s regulariza-
tion parameter) and 250 verbs as a final held-out test
set. We use the same features and classifier as in
our main results, except we remove the DIC features
which are now the instance labels.
Table 5 shows the performance of our two base-
line systems along with Morfessor and SCD. While
the majority-class baseline is much higher, the
prefix-based baseline is 7% lower, indicating that
knowledge of prefixes, and lexical features in gen-
eral, are less helpful for conventional segmentations.
In fact, performance only drops 2% when we re-
move the LEX features, showing that web-scale in-
formation alone can enable solid performance on
this task. Surprisingly, Morfessor performs worse
here, below both baselines and substantially below
the supervised system. We confirmed our Morfessor
program was generating the same segmentations as
the online demo. We also experimented with Lin-
guistica (Goldsmith, 2001), training on a large cor-
pus, but results were worse than with Morfessor.
Accurate segmentation of prefix verbs is clearly
part of the mandate of these systems; prefix verb
segmentation is simply a very challenging task. Un-
like other, less-ambiguous tasks in morphology, a
prefix/stem segmentation is plausible for all of our
300
input verbs, since the putative morphemes are by
definition valid morphemes in the language.
Overall, the results confirm and extend previous
studies that show semantic information is helpful in
morphology (Schone and Jurafsky, 2000; Yarowsky
and Wicentowski, 2000). However, we reiterate that
optimizing systems according to conventional mor-
phology may not be optimal for downstream ap-
plications. Furthermore, accuracy is substantially
lower in this setting than in our main results. Target-
ing conventional segmentations may be both more
challenging and less useful than focusing on seman-
tic compositionality.
6 Related Work
There is a large body of work on morphological
analysis of English, but most of this work does not
handle prefixes. Porter?s stemmer is a well-known
suffix-stripping algorithm (Porter, 1980), while
publicly-available lemmatizers like morpha (Min-
nen et al, 2001) and PC-KIMMO (Karp et al, 1992)
only process inflectional morphology. FreeLing (At-
serias et al, 2006) comes with a few simple rules
for deterministically stripping prefixes in some lan-
guages, but not English (e.g., only semi- and re- can
be stripped when analyzing OOV Spanish verbs).
A number of modern morphological analyzers use
supervised machine learning. These systems could
all potentially benefit from the novel distributional
features used in our model. Van den Bosch and
Daelemans (1999) use memory-based learning to
analyze Dutch. Wicentowski (2004)?s supervised
WordFrame model includes a prefixation compo-
nent. Results are presented on over 30 languages.
Erjavec and Dz?eroski (2004) present a supervised
lemmatizer for Slovene. Dreyer et al (2008) per-
form supervised lemmatization on Basque, English,
Irish and Tagalog; like us they include results when
the set of lemmas is given. Toutanova and Cherry
(2009) present a discriminative lemmatizer for En-
glish, Bulgarian, Czech and Slovene, but only han-
dle suffix morphology. Poon et al (2009) present an
unsupervised segmenter, but one that is based on a
log-linear model that can include arbitrary and in-
terdependent features of the type proposed in our
work. We see potential in combining the best el-
ements of both approaches to obtain a system that
does not need annotated training data, but can make
use of powerful web-scale features.
Our approach follows previous systems for mor-
phological analysis that leverage semantic as well
as orthographic information (Yarowsky and Wicen-
towski, 2000; Schone and Jurafsky, 2001; Baroni et
al., 2002). Similar problems also arise in core se-
mantics, such as how to detect the compositionality
of multi-word expressions (Lin, 1999; Baldwin et
al., 2003; Fazly et al, 2009). Our problem is sim-
ilar to the analysis of verb-particle constructions or
VPCs (e.g., round up, sell off, etc.) (Bannard et al,
2003). Web-scale data can be used for a variety of
problems in semantics (Lin et al, 2010), including
classifying VPCs (Kummerfeld and Curran, 2008).
We motivated our work by describing applications
in information retrieval, and here Google is clearly
the elephant in the room. It is widely reported that
Google has been using stemming since 2003; for ex-
ample, a search today for Porter stemming returns
pages describing the Porter stemmer, and the re-
turned snippets have words like stemming, stem-
mer, and stem in bold text. Google can of course
develop high-quality lists of morphological variants
by paying attention to how users reformulate their
queries. User query sessions have previously been
used to expand queries using similar terms, such as
substituting feline for cat (Jones et al, 2006). We
show that high-quality, IR-friendly stemming is pos-
sible even without query data. Furthermore, query
data could be combined with our other features for
highly discriminative word stemming in context.
Beyond information retrieval, suffix-based stem-
ming and lemmatization have been used in a range
of NLP applications, including text categorization,
textual entailment, and statistical machine transla-
tion. We believe accurate prefix-stripping can also
have an impact in these areas.
7 Conclusions and Future Work
We presented a system for predicting the semantic
compositionality of prefix verbs. We proposed a
new, well-defined and practical definition of compo-
sitionality, and we annotated a corpus of sentences
according to this definition. We trained a discrimina-
tive model to predict compositionality using a range
of lexical and web-scale statistical features. Novel
301
features include measures of the frequency of prefix-
stem hyphenation, and statistics for the likelihood of
the verb and stem co-occurring as separate words in
an N-gram. The classifier is highly accurate across a
range of prefixes, correctly predicting composition-
ality for 93.6% of examples.
Our preliminary results provide strong motiva-
tion for investigating and applying new distribu-
tional features in the prediction of both conventional
morphology and in task-directed semantic composi-
tionality. Our techniques could be used on a variety
of other complex word forms. In particular, many
of our features extend naturally to identifying stem-
stem compounds (like panfry or healthcare). Also, it
would be possible for our system to handle inflected
forms by first converting them to their lemmas us-
ing a morphological analyzer. We could also jointly
learn the compositionality of words across their in-
flections, along the lines of Yarowsky and Wicen-
towski (2000).
There are also other N-gram-derived features that
warrant further investigation. One source of in-
formation that has not previously been exploited is
the ?lexical fixedness? (Fazly et al, 2009) of non-
compositional prefix verbs. If prefix verbs are rarely
rephrased in another form, they are likely to be non-
compositional. For example, in our N-gram data,
the count of quest again is relatively low compared
to the count of request, indicating request is non-
compositional. On the other hand, marry again is
relatively frequent, indicating that remarry is com-
positional. Incorporation of these and other N-gram
counts could further improve classification accuracy.
References
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonza?lez, Llu??s Padro?, and Muntsa Padro?.
2006. FreeLing 1.3: Syntactic and semantic services
in an open-source NLP library. In LREC.
R. Harald Baayen and Antoinette Renouf. 1996. Chron-
icling the Times: Productive lexical innovations in an
English newspaper. Language, 72(1).
Harald Baayen and Richard Sproat. 1996. Estimating
lexical priors for low-frequency morphologically am-
biguous forms. Comput. Linguist., 22(2):155?166.
R. Harald Baayen, Richard Piepenbrock, and Leon
Gulikers. 1996. The CELEX2 lexical database.
LDC96L14.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In ACL 2003
Workshop on Multiword Expressions.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In ACL 2003 Workshop on Multiword Ex-
pressions.
Marco Baroni, Johannes Matiasek, and Harald Trost.
2002. Unsupervised discovery of morphologically re-
lated words based on orthographic and semantic sim-
ilarity. In ACL-02 Workshop on Morphological and
Phonological Learning (SIGPHON), pages 48?57.
Matthew W. Bilotti, Boris Katz, and Jimmy Lin. 2004.
What works better for question answering: Stemming
or morphological query expansion? In Information
Retrieval for Question Answering (IR4QA) Workshop
at SIGIR 2004.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In International and Interdisci-
plinary Conference on Adaptive Knowledge Represen-
tation and Reasoning.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4(1):1?
34.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT-EMNLP.
Carl de Marken. 1996. Linguistic structure as composi-
tion and perturbation. In ACL.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In EMNLP.
Tomaz? Erjavec and Sas?o Dz?eroski. 2004. Machine learn-
ing of morphosyntactic structure: Lemmatising un-
known Slovene words. Applied Artificial Intelligence,
18:17?41.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Comput. Linguist., 35(1):61?
103.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Comput. Linguist.,
27(2):153?198.
David Graff. 2003. English Gigaword. LDC2003T05.
302
Vera Hollink, Jaap Kamps, Christof Monz, and Maarten
de Rijke. 2004. Monolingual document retrieval for
European languages. IR, 7(1):33?52.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
WWW.
Daniel Jurafsky and James H. Martin. 2000. Speech and
language processing. Prentice Hall.
Daniel Karp, Yves Schabes, Martin Zaidel, and Dania
Egedi. 1992. A freely available wide coverage mor-
phological analyzer for English. In COLING.
Francis Katamba. 1993. Morphology. MacMillan Press.
Samarth Keshava and Emily Pitler. 2006. A simpler, in-
tuitive approach to morpheme induction. In 2nd Pas-
cal Challenges Workshop.
Jonathan K. Kummerfeld and James R. Curran. 2008.
Classification of verb particle constructions with the
Google Web1T Corpus. In Australasian Language
Technology Association Workshop.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
N-grams. In LREC.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat.
Lang. Eng., 7(3):207?223.
Preslav Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In CoNLL.
Preslav Ivanov Nakov. 2007. Using the Web as an Im-
plicit Training Set: Application to Noun Compound
Syntax and Semantics. Ph.D. thesis, University of Cal-
ifornia, Berkeley.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In HLT-NAACL.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
Patrick Schone and Daniel Jurafsky. 2000. Knowledge-
free induction of morphology using latent semantic
analysis. In LLL/CoNLL.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
free induction of inflectional morphologies. In
NAACL.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In ACL-IJCNLP.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning.
Antal Van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In ACL.
Richard Wicentowski. 2004. Multilingual noise-robust
supervised morphological analysis using the word-
frame model. In ACL SIGPHON.
Ying Xu, Christoph Ringlstetter, and Randy Goebel.
2009. A continuum-based approach for tightness anal-
ysis of Chinese semantic units. In PACLIC.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL.
303
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 693?696,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Language identification of names with SVMs
Aditya Bhargava and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{abhargava,kondrak}@cs.ualberta.ca
Abstract
The task of identifying the language of text
or utterances has a number of applications in
natural language processing. Language iden-
tification has traditionally been approached
with character-level language models. How-
ever, the language model approach crucially
depends on the length of the text in ques-
tion. In this paper, we consider the problem
of language identification of names. We show
that an approach based on SVMs with n-gram
counts as features performs much better than
language models. We also experiment with
applying the method to pre-process transliter-
ation data for the training of separate models.
1 Introduction
The task of identifying the language of text or utter-
ances has a number of applications in natural lan-
guage processing. Font Llitjo?s and Black (2001)
show that language identification can improve the
accuracy of letter-to-phoneme conversion. Li et
al. (2007) use language identification in a translit-
eration system to account for different semantic
transliteration rules between languages when the tar-
get language is Chinese. Huang (2005) improves the
accuracy of machine transliteration by clustering his
training data according to the source language.
Language identification has traditionally been
approached using character-level n-gram language
models. In this paper, we propose the use of sup-
port vector machines (SVMs) for the language iden-
tification of very short texts such as proper nouns.
We show that SVMs outperform language models
on two different data sets consisting of personal
names. Furthermore, we test the hypothesis that lan-
guage identification can improve transliteration by
pre-processing the source data and training separate
models using a state-of-the-art transliteration sys-
tem.
2 Previous work
N -gram approaches have proven very popular for
language identification in general. Cavnar and Tren-
kle (1994) apply n-gram language models to general
text categorization. They construct character-level
language models using n-grams up to a certain max-
imum length from each class in their training cor-
pora. To classify new text, they generate an n-gram
frequency profile from the text and then assign it to
the class having the most similar language model,
which is determined by summing the differences in
n-gram ranks. Given 14 languages, text of 300 char-
acters or more, and retaining the 400 most common
n-grams up to length 5, they achieve an overall accu-
racy of 99.8%. However, the accuracy of the n-gram
approach strongly depends on the length of the texts.
Kruengkrai et al (2005) report that, on a language
identification task of 17 languages with average text
length 50 bytes, the accuracy drops to 90.2%. When
SVMs were used for the same task, they achieved
99.7% accuracy.
Konstantopoulos (2007) looks particularly at the
task of identifying the language of proper nouns. He
focuses on a data set of soccer player names coming
from 13 possible national languages. He finds that
using general n-gram language models yields an av-
erage F1 score of only 27%, but training the models
specifically to these smaller data gives significantly
better results: 50% average F1 score for last names
693
only, and 60% for full names.
On the other hand, Li et al (2007) report some
good results for single-name language identification
using n-gram language models. For the task of sepa-
rating single Chinese, English, and Japanese names,
they achieve an overall accuracy of 94.8%. One rea-
son that they do better is because of the smaller num-
ber of classes. We can further see that the languages
in question are very dissimilar, making the problem
easier; for example, the character ?x? appears only
in the list of Chinese names, and the bigram ?kl? ap-
pears only in the list of English names.
3 Language identification with SVMs
Rather than using language models to determine the
language of a name, we propose to count charac-
ter n-gram occurrences in the given name, for n up
to some maximum length, and use these counts as
the features in an SVM. We choose SVMs because
they can take a large number of features and learn to
weigh them appropriately. When counting n-grams,
we include space characters at the beginning and
end of each word, so that prefixes and suffixes are
counted appropriately. In addition to n-gram counts,
we also include word length as a feature.
In our initial experiments, we tested several dif-
ferent kernels. The kernels that performed the best
were the linear, sigmoid, and radial basis function
(RBF) kernels. We tested various maximum n-gram
lengths; Figure 1 shows the accuracy of the linear
kernel as a function of maximum n-gram length.
Polynomial kernels, a substring match?count string
kernel, and a string kernel based on the edit distance
all performed poorly in comparison. We also exper-
imented with other modifications such as normaliz-
ing the feature vectors, and decreasing the weights
of frequent n-gram counts to avoid larger counts
dominating smaller counts. Since the effects were
negligible, we exclude these results from this paper.
In our experiments, we used the LIBLINEAR
(Fan et al, 2008) package for the linear kernel and
the LIBSVM (Chang and Lin, 2001) package for the
RBF and sigmoid kernels. We discarded any peri-
ods and parentheses, but kept apostrophes and hy-
phens, and we converted all letters to lower case.
We removed very short names of length less than
two. For all data sets, we held out 10% of the data
 40 50 60 70 80
 1  2  3  4  5  6Accuracy (%) Maximum n-gram length
Figure 1: Cross-validation accuracy of the linear kernel
on the Transfermarkt full names corpus.
as the test set. We then found optimal parameters
for each kernel type using 10-fold cross-validation
on the remaining training set. This yielded optimum
maximum n-gram lengths of four for single names
and five for full names. Using the optimal parame-
ters, we constructed models from the entire training
data and then tested the models on the held-out test
set.
4 Intrinsic evaluation
We used two corpora to test our SVM-based ap-
proach: the Transfermarkt corpus of soccer player
names, and the Chinese-English-Japanese (CEJ)
corpus of first names and surnames. These corpora
are described in further detail below.
4.1 Transfermarkt corpus
The Transfermarkt corpus (Konstantopoulos, 2007)
consists of European soccer player names annotated
with one of 13 possible national languages, with sep-
arate lists provided for last names and full names.
Diacritics were removed in order to avoid trivializ-
ing the task. There are 14914 full names, with aver-
age length 14.8, and 12051 last names, with average
length 7.8. It should be noted that these data are
noisy; the fact that a player plays for a certain na-
tion?s team does not necessarily indicate that his or
her name is of that nation?s language. For example,
Dario Dakovic was born in Bosnia but plays for the
Austrian national team; his name is therefore anno-
tated as German.
Table 1 shows our results on the Transfermarkt
corpus. Because Konstantopoulos (2007) provides
only F1 scores, we used his scripts to generate new
results using language models and calculate the ac-
curacy instead, which allows us to be consistent with
our tests on other data sets. Our results show that us-
694
Method Last names Full names
Language models 44.7 54.2
Linear SVM 56.4 79.9
RBF SVM 55.7 78.9
Sigmoid SVM 56.2 78.7
Table 1: Language identification accuracy on the Trans-
fermarkt corpus. Language models have n = 5.
ing SVMs clearly outperforms using language mod-
els on the Transfermarkt corpus; in fact, SVMs yield
better accuracy on last names than language models
on full names. Differences between kernels are not
statistically significant.
4.2 CEJ corpus
The CEJ corpus (Li et al, 2007) provides a com-
bined list of first names and surnames, each classi-
fied as Chinese, English, or Japanese. There are a
total of 97115 names with an average length of 7.6
characters. This corpus was used for the semantic
transliteration of personal names into Chinese.
We found that the RBF and sigmoid kernels were
very slow?presumably due to the large size of the
corpus?so we tested only the linear kernel. Table 2
shows our results in comparison to those of language
models reported in (Li et al, 2007); we reduce the
error rate by over 50%.
5 Application to machine transliteration
Machine transliteration is one of the primary poten-
tial applications of language identification because
the language of a word often determines its pronun-
ciation. We therefore tested language identification
to see if results could indeed be improved by using
language identification as a pre-processing step.
5.1 Data
The English-Hindi corpus of names (Li et al, 2009;
MSRI, 2009) contains a test set of 1000 names rep-
resented in both the Latin and Devanagari scripts.
We manually classified these names as being of ei-
ther Indian or non-Indian origin, occasionally resort-
ing to web searches to help disambiguate them.1 We
discarded those names that fell into both categories
1Our tagged data are available online at http://www.
cs.ualberta.ca/?ab31/langid/.
Method Ch. Eng. Jap. All
Lang. model 96.4 89.9 96.5 94.8
Linear SVM 99.0 94.8 97.6 97.6
Table 2: Language identification accuracy on the CEJ
corpus. Language models have n = 4.
(e.g. ?Maya?) as well as those that we could not
confidently classify. In total, we discarded 95 of
these names, and randomly selected 95 names from
the training set that we could confidently classify to
complete our corpus of 1000 names. Of the 1000
names, 546 were classified as being of Indian origin
and the remaining 454 were classified as being of
non-Indian origin; the names have an average length
of 7.0 characters.
We trained our language identification approach
on 900 names, with the remaining 100 names serv-
ing as the test set. The resulting accuracy was 80%
with the linear kernel, 84% with the RBF kernel,
and 83% with the sigmoid kernel. In this case, the
performance of the RBF kernel was found to be sig-
nificantly better than that of the linear kernel accord-
ing to the McNemar test with p < 0.05.
5.2 Experimental setup
We tested a simple method of combining language
identification with transliteration. We use a lan-
guage identification model to split the training, de-
velopment, and test sets into disjoint classes. We
train a transliteration model on each separate class,
and then combine the results.
Our transliteration system was DIRECTL (Ji-
ampojamarn et al, 2009). We trained the language
identification model over the entire set of 1000
tagged names using the parameters from above. Be-
cause these names comprised most of the test set
and were now being used as the training set for the
language identification model, we swapped various
names between sets such that none of the words used
for training the language identification model were
in the final transliteration test set.
Using this language identification model, we split
the data. After splitting, the ?Indian? training, de-
velopment, and testing sets had 5032, 575, and 483
words respectively while the ?non-Indian? sets had
11081, 993, and 517 words respectively.
695
5.3 Results
Splitting the data and training two separate mod-
els yielded a combined top-1 accuracy of 46.0%, as
compared to 47.0% achieved by a single translitera-
tion model trained over the full data; this difference
is not statistically significant. Somewhat counter-
intuitively, using language identification as a pre-
processing step for machine transliteration yields no
improvement in performance for our particular data
and transliteration system.
While it could be argued that our language identi-
fication accuracy of 84% is too low to be useful here,
we believe that the principal reason for this perfor-
mance decrease is the reduction in the amount of
data available for the training of the separate mod-
els. We performed an experiment to confirm this
hypothesis: we randomly split the full data into two
sets, matching the sizes of the Indian and non-Indian
sets. We then trained two separate models and com-
bined the results; this yielded a top-1 accuracy of
41.5%. The difference between this and the 46.0%
result above is statistically significant with p < 0.01.
From this we conclude that the reduction in data size
was a significant factor in the previously described
null result, and that language identification does pro-
vide useful information to the transliteration system.
In addition, we believe that the transliteration system
may implicitly leverage the language origin infor-
mation. Whether a closer coupling of the two mod-
ules could produce an increase in accuracy remains
an open question.
6 Conclusion
We have proposed a novel approach to the task of
language identification of names. We have shown
that applying SVMs with n-gram counts as fea-
tures outperforms the predominant approach based
on language models. We also tested language identi-
fication in one of its potential applications, machine
transliteration, and found that a simple method of
splitting the data by language yields no significant
change in accuracy, although there is an improve-
ment in comparison to a random split.
In the future, we plan to investigate other methods
of incorporating language identification in machine
transliteration. Options to explore include the use
of language identification probabilities as features in
the transliteration system (Li et al, 2007), as well as
splitting the data into sets that are not necessarily
disjoint, allowing separate transliteration models to
learn from potentially useful common information.
Acknowledgements
We thank Sittichai Jiampojamarn for his assistance
with the DIRECTL transliteration system, Shane
Bergsma for his advice, and Stasinos Konstantopou-
los for providing us with his scripts and data. This
research was supported by the Natural Sciences and
Engineering Research Council of Canada.
References
W. B. Cavnar and J. M. Trenkle. 1994. N-gram-based
text categorization. In Proc. of the Third Annual Sym-
posium on Document Analysis and Information Re-
trieval, pages 161?175.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a li-
brary for support vector machines. Software available
at http://www.csie.ntu.edu.tw/?cjlin/
libsvm.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. Journal of Machine Learning Re-
search, 9:1871?1874.
A. Font Llitjo?s and A. W. Black. 2001. Knowledge of
language origin improves pronunciation accuracy of
proper names. In Proc. of Eurospeech, pages 1919?
1922.
F. Huang. 2005. Cluster-specific named entity transliter-
ation. In Proc. of HLT-EMNLP, pages 435?442.
S. Jiampojamarn, A. Bhargava, Q. Dou, K. Dwyer, and
G. Kondrak. 2009. DirecTL: a language independent
approach to transliteration. In Proc. of ACL-IJCNLP
Named Entities Workshop, pages 28?31.
S. Konstantopoulos. 2007. What?s in a name? In Proc.
of RANLP Computational Phonology Workshop.
C. Kruengkrai, P. Srichaivattana, V. Sornlertlamvanich,
and H. Isahara. 2005. Language identification based
on string kernels. In Proc. of International Symposium
on Communications and Information Technologies.
H. Li, K. C. Sim, J.-S. Kuo, and M. Dong. 2007. Seman-
tic transliteration of personal names. In Proc. of ACL,
pages 120?127.
H. Li, A. Kumaran, V. Pervouchine, and M. Zhang. 2009.
Report of NEWS 2009 machine transliteration shared
task. In Proc. of Named Entities Workshop: Shared
Task on Transliteration, pages 1?18.
MSRI, 2009. Microsoft Research India. http://
research.microsoft.com/india.
696
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 396?406,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Leveraging supplemental representations for sequential transduction
Aditya Bhargava
Department of Computer Science
University of Toronto
Toronto, ON, Canada, M5S 3G4
aditya@cs.toronto.edu
Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
kondrak@cs.ualberta.ca
Abstract
Sequential transduction tasks, such as
grapheme-to-phoneme conversion and ma-
chine transliteration, are usually addressed
by inducing models from sets of input-output
pairs. Supplemental representations offer valu-
able additional information, but incorporating
that information is not straightforward. We
apply a unified reranking approach to both
grapheme-to-phoneme conversion and ma-
chine transliteration demonstrating substantial
accuracy improvements by utilizing heteroge-
neous transliterations and transcriptions of the
input word. We describe several experiments
that involve a variety of supplemental data
and two state-of-the-art transduction systems,
yielding error rate reductions ranging from
12% to 43%. We further apply our approach to
system combination, with error rate reductions
between 4% and 9%.
1 Introduction
Words exist independently of writing, as abstract enti-
ties shared among the speakers of a language. Those
abstract entities have various representations, which
in turn may have different realizations. Orthographic
forms, phonetic transcriptions, alternative transliter-
ations, and even sound-wave spectrograms are all
related by referring to the same abstract word and
they all convey information about its pronunciation.
Figure 1 shows various representations of the word
Dickens. The primary (canonical) orthographic rep-
resentation of the word corresponds to the language
to which the word belongs. The secondary ortho-
graphic representations in different writing scripts
are transliterations of the word, which exhibit phono-
orthography
Dickens
???? ?
?????
???????
???????
?
transliterations /d?k?nz/
dIkInz
D IH K AH N Z
dIk@nz
d I k x n z
?
transcriptions
MTL G2P
Figure 1: Several NLP tasks involve conversion between
various word representations. The tasks on which we focus
are shown in black.
logical adaptation to the target language. The vari-
ous phonetic transcriptions consist of sequences of
phonemes representing the pronunciation of the word.
Transcription schemes may differ in the number and
coverage of various phonemes, as well as the choice
of the underlying speech variety. The spoken pro-
nunciation (represented by the waveform) presents a
common latent influence on the representations.
Several well-known NLP tasks involve matching,
alignment, and conversion between different word
representations. Grapheme-to-phoneme conversion
(G2P) aims at generating a transcription of a word
from its orthographic representation. The reverse
task is phoneme-to-grapheme conversion (P2G). Ma-
chine transliteration (MTL) deals with conversion be-
tween orthographic representations in different writ-
ing scripts; forward transliteration proceeds from the
primary representation to secondary representations,
while the reverse task is called back-transliteration
(BTL). The conversion between a sound and an or-
thography is the goal of text-to-speech synthesis
396
(TTS) and speech recognition (SR), where transcrip-
tions are often used as intermediate forms.
Although both MTL and G2P take orthographic
representations as input, the two tasks are rarely con-
sidered in conjunction. Traditionally, G2P has been
investigated in the context of text-to-speech systems,
while MTL is of interest to the information retrieval
andmachine translation communities. In addition, un-
like phonetic transcription schemes, which are often
specific to a particular pronunciation lexicon, writing
systems are well-standardized, with plenty of translit-
eration examples available in text corpora and on
the Web (Kumaran et al, 2010b). While the goal of
G2P is producing a maximally faithful representa-
tion of the pronunciation, transliterations are often
influenced by other factors, such as the phonological
constraints of the target language, the orthographic
form in the source language, the morphological adap-
tations related to the translation process, and even the
semantic connotations of the output in the case of lo-
gographic scripts. In spite of those differences, both
transcriptions and transliterations contain valuable
information about the pronunciation of the word.
In this paper, we show that it is possible to improve
the accuracy of both G2P and MTL by incorporating
supplemental representations of the word pronuncia-
tion. Our method is based on SVM reranking of the
n-best output lists of a base transduction system, with
features including similarity scores between repre-
sentations and n-grams derived from accurate align-
ments. We describe a series of experiments in both
G2P and MTL contexts, demonstrating substantial
reductions in error rate for these base tasks when aug-
mented with various supplemental representations.
We then further test the effectiveness of the same ap-
proach for combining the results of two independent
base systems.
2 Related work
Because of its crucial role in speech synthe-
sis, grapheme-to-phoneme conversion has been re-
searched extensively. Most out-of-vocabulary words
are names, which often exhibit idiosyncratic pronun-
ciation (Black et al, 1998). Excepting languages
with highly transparent orthographies, the number
of letter-to-sound rules appears to grow geometri-
cally with the lexicon size, with no asymptotic limit
(Kominek and Black, 2006). A number of machine
learning approaches have been proposed for G2P, in-
cluding neural networks (Sejnowski and Rosenberg,
1987), instance-based learning (van den Bosch and
Daelemans, 1998), pronunciation by analogy (Marc-
hand and Damper, 2000), decision trees (Kienappel
and Kneser, 2001), hidden Markov models (Taylor,
2005), joint n-gram models (Bisani and Ney, 2008),
and online discriminative learning (Jiampojamarn et
al., 2008). The current state-of-the-art is represented
by the latter two approaches, which are available as
the SEQUITUR and DIRECTL+ systems, respectively.
Machine transliteration has also received much at-
tention (Knight and Graehl, 1998; Li et al, 2004;
Sproat et al, 2006; Klementiev and Roth, 2006; Ze-
lenko and Aone, 2006). In the last few years, the
Named Entities Workshop (NEWS) Shared Tasks on
Transliteration have been the forum for validating
diverse approaches on common data sets (Li et al,
2009; Li et al, 2010; Zhang et al, 2011). Both SE-
QUITUR and DIRECTL+, originally G2P systems,
have been successfully adapted to MTL (Finch and
Sumita, 2010; Jiampojamarn et al, 2010b).
Most of the research on both G2P and MTL as-
sumes the existence of a homogeneous training set of
input-output pairs. However, following the pivot ap-
proaches developed in other areas of NLP (Utiyama
and Isahara, 2007; Cohn and Lapata, 2007; Wu and
Wang, 2009; Snyder et al, 2009), the idea of tak-
ing advantage of other-language data has recently
been applied to machine transliteration. Khapra et
al. (2010) construct a transliteration system between
languages A and B by composing two transliteration
systems A ? C and C ? B, where C is called
a bridge or pivot language, resulting in a relatively
small drop in accuracy. Zhang et al (2010) and Ku-
maran et al (2010a) report that combinations of pivot
systems A ? C ? B with direct systems A ? B
produce better results than using the direct systems
only. The models, which are composed using a linear
combination of scores, utilize a single pivot language
C, and require training data between all three lan-
guages A, B, and C. However, such a pivot-based
framework makes it difficult to incorporate multiple
pivot languages, and has shown most promising re-
sults for cases in which data for the original A? B
task are limited. Lastly, Finch and Sumita (2010) de-
veloped an MTL approach that combined the output
397
input word n-best outputs re-ranked n-best list
Sudan sud@n
sud{n
?
sud#n
sud#n
sUd#n
?
sud@n
supplemental
representations
sudAn
S UW D AE N
?????????
?????
?
base system re-ranker
Figure 2: An overview of our approach on an example from the G2P task. The correct output is shown in bold.
of two systems using a linear combination of system
scores and a manually-tuned weight.
On the G2P side, Loots and Niesler (2009) investi-
gate the problem of leveraging transcriptions from a
different dialect of English, while Bhargava and Kon-
drak (2011) focus on leveraging transliterations from
multiple writing scripts. Bhargava et al (2011) show
that the reranking method proposed by Bhargava and
Kondrak (2011) can increase the accuracy of MTL as
well. In this paper, we aim to confirm the generality
of the same method by testing it on a broad range of
tasks: a) leveraging transcriptions for both G2P and
MTL; b) utilizing supplemental transcriptions and
transliterations simultaneously; c) improving G2P in
general, rather than just G2P of names; and d) com-
bining different transduction systems.
3 Leveraging supplemental data
Incorporating supplemental information directly into
an existing system is not always feasible. With gener-
ative approaches, one would have to find some way of
modelling the relationship between the system inputs,
outputs, and the supplemental data. Discriminative
approaches are not necessarily easier: DIRECTL+, a
discriminative G2P system, needs to be able to gener-
ate features on-the-fly for partial grapheme-phoneme
sequence pairs during the decoding. Instead, we inte-
grate an existing system as a black box that generates
n-best lists of candidate outputs, resulting in a modu-
lar and general post hoc approach that can be applied
to multiple tasks and settings.
3.1 Task definition
The task is to convert an input string s into a target
string t, where both strings are representations of a
word w. In G2P, s is a string of graphemes while
t consists of phonemes; in MTL, both s and t are
grapheme sequences, although in different scripts.
We assume that we have a base system T (s) that at-
tempts this task and produces an n-best list of outputs
t?1, t?2, . . . , t?n for the input s. T is imperfect, i.e., the
correct output t may appear in a position in the list
other than the first. It is reasonable to expect that such
a system also provides a list of scores corresponding
to the outputs. We further assume that we have access
to supplemental representations of w; both transliter-
ations and transcriptions may serve this purpose. Our
objective is to improve the accuracy on the task in
question with respect to the base system T (s).
3.2 Reranking
For the purpose of exposition, we reiterate here the
particulars of the reranking approach of Bhargava and
Kondrak (2011) that we apply to the various tasks and
supplemental data sources. The method uses SVM
reranking of the n-best lists produced by the base sys-
tem in order to to move the correct output to the top
of the list using supplemental data. SVM reranking
(Joachims, 2002) facilitates the exploitation of multi-
ple sources of supplemental data, as shown in Figure
2. The feature construction process is performed for
each candidate output in the n-best list, as well as
each pairing of a candidate output with a supplemen-
tal representation. The features used for reranking
may or may not overlap with the features used by the
398
base system. While we focus on the G2P and MTL
tasks in this paper, this method is general enough as
to potentially be applied to other sequence prediction
tasks.
3.3 Alignment
In order to construct the feature vectors, we need
the alignments between the alternative representa-
tions of the same word. For the alignment of supple-
mental data with candidate outputs, we apply M2M-
ALIGNER (Jiampojamarn et al, 2007). We use the
same method for the alignment between the input and
the candidate outputs, unless the base system already
provides this information.
M2M-ALIGNER is a generalization of the learned
edit distance algorithm of Ristad and Yianilos (1998).
It iteratively refines the alignment of a set of
string pairs in an unsupervised manner using the
expectation-maximization (EM) approach. In addi-
tion to the alignment, M2M-ALIGNER produces an
alignment probability, which reflects the similarity
between two strings. Intuitively, if two strings contain
symbols or n-grams that often co-occur in the train-
ing data, their alignment score will be higher. The
strings in question are often of completely different
scripts, which precludes the application of standard
similarity measures such as Levenshtein distance.
3.4 Score features
The similarity of candidate outputs to alternative rep-
resentations of a word is probably the most intuitive
feature for reranking. We include a real-valued simi-
larity feature for each pairing between a supplemental
representation and a candidate output, which is set
according to the M2M-ALIGNER alignment score.
Another important set of features are the confi-
dence scores assigned to each candidate output by
the base system. In addition to the original scores, we
also include a set of features that indicate the differ-
ences between scores for all pairs of outputs in the
n-best list. This allows the reranker to incorporate a
notion of relative confidence with respect to the other
candidate outputs. Similarly, we compute differences
between the similarity scores of candidate outputs
and supplemental representations.
3.5 N -gram features
Following (Jiampojamarn et al, 2010a), we include
several types of n-gram features. The features are
defined on substrings occurring in pairs of aligned
strings. Each feature is binary, indicating the presence
or absence of the particular feature type in the given
aligned pair, which could be either the original base
system?s input and output, or else a candidate output
and a supplemental representation.
We can divide the n-gram features into four cate-
gories. Context features bind an output symbol with
input n-grams in a focus window centred around the
input-output alignment; the input n-grams represent
the context in which the output character is gener-
ated. Markov features are n-grams of output symbols,
which allow previously generated output characters
to influence the current output character. Linear chain
features associate the context and Markov features.
Joint n-gram features combine aligned input and out-
put n-grams of the same length on both sides.
In the standard string transduction task, the output
string t is generated incrementally from the input
s. In contrast, in the reranking setting, both strings
are complete and available. This allows us to reverse
the direction of the context and linear chain features,
allowing us to associate output n-grams with single
input symbols. In addition, we can apply those fea-
tures in both directions across candidate outputs and
supplemental representations, further increasing the
amount of information provided to the reranker.
4 Experiments
Our experiments aim at comprehensive evaluation
of the reranking approach on both MTL and G2P
tasks, employing various supplemental representa-
tions. Relevant code and scripts associated with our
experimental results are available online1.
4.1 Data
We extract transcriptions from two lexica: Combilex
(Richmond et al, 2009), which includes both Re-
ceived Pronunciation (RP) and General American
(GA) pronunciation variants, and CELEX (Baayen et
al., 1996), which includes RP only. After discarding
duplicates and letter diacritics, the total number of
1http://www.cs.toronto.edu/?aditya/
g2p-tl-rr
399
Language Corpus size Japanese Overlap
Bengali 13,624 2,152
Chinese 40,214 14,056
Hebrew 10,501 3,997
Hindi 13,427 2,507
Japanese 28,013 ?
Kannada 11,540 2,170
Korean 7,778 7,733
Persian 12,386 4,047
Tamil 11,642 2,205
Thai 28,932 10,378
Table 1: The number of unique entries in each translit-
eration corpus, and the number of common single-word
entries (overlap) with the Japanese corpus.
word-transcription pairs are 114,094 for Combilex,
and 66,859 for CELEX. We use 10% of the data for
development, 10% for testing, and the remaining 80%
for training. The development set is merged with the
training set for final testing.
Our transliteration data come from the shared tasks
of the 2011 NEWS workshop (Zhang et al, 2011).
The number of entries in each transliteration corpus
is shown in the middle column of Table 1.
4.2 Base systems
In order to verify the generality of our approach,
we perform all experiments using two different base
transduction systems described in Section 2: SE-
QUITUR and DIRECTL+. Both systems are set to
provide 10-best output lists along with scores for
each output.2 SEQUITUR is modified to provide
log-probabilities instead of regular probabilities. DI-
RECTL+ is run with the complete set of features
described by Jiampojamarn et al (2010a). System
parameters, such as maximum number of iterations,
are determined during development.
M2M-ALIGNER is used throughout for the align-
ment of various representations. The aligner is trained
on an intersection of a relevant pair of data sets. For
example, the intersection of the English-to-Japanese
and English-to-Hindi corpora on the basis of common
2While running times prevented us from extensively ana-
lyzing reranking performance vs. n-best list size, our initial
tests produced almost identical results for n = 5, n = 10, and
n = 20.
entries on the English side yields a corpus matching
Japanese transliterations with Hindi transliterations.
M2M-ALIGNER, after having been trained on this
corpus, is able to produce a similarity score for an
arbitrary Japanese-Hindi pair. We set a lower limit
of ?100 on the M2M-ALIGNER log-probabilities,
and use the default of 2-2 alignments; deletions are
enabled for the supplemental data side of the align-
ment.
4.3 MTL experiments
When faced with the task of transliterating a word
from the original script to a secondary script, we
would like to leverage the information encoded in
transliterations of the same word that are available
in other scripts. For example, consider the problem
of automatically generating a Wikipedia stub article3
in Hindi about guitarist John Petrucci. We assume
that we have access to an MTL system trained on the
English-Hindi transliterations, but we also want to
take advantage of the existing transliterations of the
name that are easy to extract from the corresponding
articles on the topic in Japanese and other languages.
In this case, the orthography of the last name reflects
its Italian origins, but the pronunciation depends on
its degree of assimilation to English phonology. This
type of information is often difficult to determine
even for humans, and we posit that it may be inferred
from other transliterations.
Similarly, phonetic transcriptions more directly en-
code the pronunciation and thus present an important
resource for exploitation. In fact, some transliteration
systems use a phonetic transcription as an interme-
diate representation (Knight and Graehl, 1998), al-
though these methods do not generally fare as well
as those that perform the transliteration process di-
rectly (Al-Onaizan and Knight, 2002; Li et al, 2009).
Transcriptions are often available; larger pronuncia-
tion dictionaries contain tens of thousands of entries,
including some proper names (for which machine
transliteration is most relevant), and many names in
Wikipedia are accompanied by an IPA transcription.
Our first experiment aims at improving the translit-
eration accuracy from English to Japanese Katakana.
The English-Japanese corpus has one of the largest
overlaps (number of entries with a common input)
3A stub article is a skeleton article with little content.
400
SEQUITUR DIRECTL+
Acc. ERR Acc. ERR
BASE 49.6 51.1
RERANKED 56.2 13.5 57.3 12.7
ORACLE 85.0 70.3 80.4 60.0
Table 2: Word accuracies and error rate reductions (ERR)
in percentages for English-to-Japanese MTL augmented
by corresponding transliterations from other languages.
BASE is the base system while RERANKED represents the
same system with its output reranked using supplemental
transliterations. ORACLE represents an oracle reranker.
with the other transliteration and transcription cor-
pora, the former of which is shown in Table 1. In
total, there are 18,505 entries for which at least one
transliteration from a non-Japanese language is avail-
able and 6,288 for which at least one transcription is
available. The reranker is trained on an intersection
of the English-Japanese training set and the supple-
mental data; similarly, the reranking test set is an
intersection of the English-Japanese test set and the
supplemental data. Note that we compute word ac-
curacy on these intersected sets, so the results of the
base systems that we report here may not represent
their performance on the full data set.
Table 2 shows the results4 on the test set of 1,891
entries, including the performance of an oracle (per-
fect) reranker for comparison. This same approach
applied to the English-to-Hindi transliteration task
yields an error rate reduction of 9% over the base
performance of DIRECTL+ (Bhargava et al, 2011)5,
which confirms that our reranking method?s applica-
bility is not limited to a particular language.
In the second experiment, instead of supplemental
transliterations, we use supplemental transcriptions
from the RP and GA Combilex corpora as well as
CELEX. The number of common elements with the
English-Japanese transliteration corpus was 6,288
for Combilex and 2,351 for CELEX; in total, there
were 6,384 transliteration entries for which at least
4Unless otherwise noted, all improvements reported in this
paper are statistically significant with p < 0.01 using the McNe-
mar test.
5Note that this result is computed over the full English-Hindi
data set, so is in fact slightly diluted compared to the results we
present here.
SEQUITUR DIRECTL+
Acc. ERR Acc. ERR
BASE 57.9 58.6
RERANKED 65.6 18.4 63.9 12.8
ORACLE 89.9 51.5 84.6 62.6
Table 3: Word accuracies and error rate reductions (ERR)
in percentages for English-to-Japanese MTL augmented
by corresponding transcriptions.
one transcription was available. Table 3 shows the
results, giving a similar error rate reduction as for
using supplemental transliterations.
Surprisingly, if we proceed to the next logical step
and use both transcriptions and transliterations as
supplemental representations simultaneously, the er-
ror rate reduction is slightly lower than in the above
two experiments. This difference is so small as to
be statistically insignificant. We have no convincing
explanation for this phenomenon, although we note
that, in general, significant heterogeneity in data can
increase the difficulty of a given task.
4.4 G2P experiments
Consider the example of an automatic speech synthe-
sis system tasked with generating an audio version of
a news article that contains foreign names. Often, for-
eign versions of the same news article already exist;
in these, the namewill have been transliterated. These
transliterations could then be leveraged to guide the
system?s pronunciation of the name. The same is con-
ceivable of other types of words, although translitera-
tions are generally mostly available for names only.
On the other hand, transcription schemes are not
consistent across different pronunciation lexica. Their
phonemic inventories often differ, and it is not always
possible to construct a consistent mapping between
them. In addition, because of pronunciation variation
and dialectal differences, a substantial fraction of
transcriptions fail to match across dictionaries. Nev-
ertheless, if a phonetic transcription is already avail-
able, even in an alternative format, it could facilitate
the task of generating a new pronunciation.
The first G2P experiment concerns the application
of supplemental transcriptions. The goal is to quan-
tify the improvements achieved using our reranking
401
approach, and to compare reranking to two other
methods of utilizing supplemental transcriptions, to
which we refer as MERGE and P2P, respectively.
MERGE implements the most intuitive approach
of merging different lexica into a single training set.
In order to make this work, we first need to make
sure that all data is converted to a single transcription
scheme. Combilex and CELEX make different dis-
tinctions among phonemes, making it unclear how
some phonemes might be mapped from CELEX into
Combilex; fortuitously, the opposite conversion is
more agreeable.6 This allows us to convert Combilex
to CELEX?s format via a simple rule-based script
and then merge the two corpora together. This pro-
vides an alternative method against which we can
compare our reranking-based approach which would
treat Combilex as a source of supplemental represen-
tations.
P2P is a phoneme-to-phoneme conversion ap-
proach inspired by the work of Loots and Niesler
(2009). In that approach, a phoneme-to-phoneme
model is derived from a training set of phonetic tran-
scription pairs representing two different pronuncia-
tion lexicons. We use such model to convert the Com-
bilex transcriptions to the scheme used by CELEX
for the words that are missing from CELEX. Where
Loots and Niesler (2009) use decision trees for both
the base system and the corpus converter, we use the
much higher-performing state-of-the-art SEQUITUR
and DIRECTL+ systems.
The two transcription corpora have 15,028 en-
tries in common. As with the MTL experiments, the
reranker is trained on an intersection of the Combilex
G2P data and the supplemental data.
The results on the intersected set of 1,498 words
are shown in Table 4. We can see that merging the
corpora provides a clear detriment in performance
for data where an alternative transcription is avail-
able from another corpus. Even if we look at the full
CELEX test set (as opposed to the intersected subset
used in Table 4), DIRECTL+ trained only on CELEX
achieves 93.0% word accuracy on the CELEX test
set where DIRECTL+ trained on CELEX merged
with Combilex achieves 87.3%. Evidently, the dis-
6In particular, Combilex distinguishes between [l] and the
velarized (?dark?) [ l&]. These can be collapsed into the single
/l/ phoneme for CELEX, but it is not clear how to handle the
conversion in the reverse direction.
SEQUITUR DIRECTL+
Acc. ERR Acc. ERR
BASE 87.3 88.1
MERGE 74.2 ? 71.6 ?
P2P 85.7 ? 87.0 ?
RERANKED 92.7 42.9 92.0 32.6
ORACLE 97.6 81.2 96.7 72.5
Table 4: Word accuracies and error rate reductions (ERR)
in percentages for CELEX G2P augmented by Combilex
transcriptions.
parate conventions of the two corpora ?confuse? the
base G2P systems. In contrast, our reranker performs
well, yielding spectacular error reductions of 32%
and 42%.
The differences between the two corpora account
for the inadequate performance of the P2P approach.
Inducing a full transduction model requires much
more training data that simply reranking the exist-
ing outputs, but in this case models for these two
approaches (P2P and reranking) are trained on the
same amount of data. Furthermore, when the supple-
mental transcription is radically different from the
n-best outputs, the alignment simply fails, and the
reranking approach gracefully falls back to the origi-
nal G2P model. In contrast, the P2P approach has no
such option.
It may be interesting to note what happens when
the P2P model is replaced with our rule-based
Combilex-to-CELEX converter. Such an approach
has the advantage of being fast and not dependent on
the training of any base system. However, it achieves
only 64.8% word accuracy, which is lower than any
of the results in Table 4. Clearly, a simple mapping
script fails to capture the differences between the
corpora.
Turning to supplemental transliterations, Bhargava
and Kondrak (2011) have already shown that supple-
mental transliterations can improve G2P accuracy on
names. It is interesting to verify whether this conclu-
sion also applies to other types of words that occur
in the G2P data set. Performing this test with DI-
RECTL+ as the base system shows good error rate
reduction on names (about 12%) as reported, but a
much smaller statistically insignificant error rate re-
402
duction on core vocabulary words (around 2%). In
other words, the supplemental transliterations are
able to help only for names.
This discrepancy is attributable to the fact that
names (and, more generally, named entities) are the
raison d?e?tre of transliterations. Because the pro-
cess of transliteration occurs primarily for names
that must be ?translated? phonetically, we expect
transliterations? utility as supplemental representa-
tions to apply mostly for names. The smaller num-
ber of transliterations for core vocabulary words also
makes it difficult for any system to learn how to apply
transliterations of such words.
4.5 Base system matters
While our SVM reranking approach demonstrates
significant improvements for all tasks and all tested
base systems, the magnitude of the performance in-
crease is dependent on the base system. In particular,
we see a common thread recurring throughout all
experiments: SEQUITUR sees higher improvements
than does DIRECTL+. Although reranking treats the
base system as a black box, we are limited by the
amount of room for improvement available in the
base system?s outputs. Our results above show that
the performance of an oracle reranker (a reranker
that automatically selects the correct output from
the n-best list) is consistently higher for SEQUITUR
than for DIRECTL+. Higher oracle reranker scores
indicate greater reranking potential, and we observe
a corresponding higher error reduction, sometimes
leading SEQUITUR to outperform DIRECTL+ after
reranking despite having been the lower performer
prior to reranking.
We hypothesize that another reason for the greater
influence of reranking on SEQUITUR is the fact that
the reranker?s features are related to those used in
DIRECTL+. Because SEQUITUR implements a dia-
metrically different, generative approach to transduc-
tion, it benefits more from reranking. However, DI-
RECTL+ still sees significant performance increases
despite the feature similarity, which demonstrates
that the supplemental representations do provide use-
ful additional information.
4.6 System combination
Although the reranking approach was developed for
the purpose of leveraging supplemental data, it can
SEQUITUR DIRECTL+
Acc. ERR Acc. ERR
BASE 45.5 47.3
LINCOMB 49.4 7.2 49.4 4.0
RERANKED 50.2 8.7 49.2 3.7
ORACLE 82.4 67.7 77.3 56.9
Table 5: Word accuracies and error rate reductions (ERR)
in percentages for English-to-Japanese MTL augmented
by predicted transliterations from the other base system.
also increase the accuracy when no genuine supple-
mental data is available. The idea is to perform sys-
tem combination by treating the output of one of
the systems as the supplemental data for the other
system, effectively casting the system combination
problem into our reranking framework. In our last
experiment, we test the combination of DIRECTL+
and SEQUITUR for English-to-Japanese MTL by des-
ignating either of them as the base system. Since the
supplemental data are generated, we are not limited
to a particular subset, and can conduct the experiment
on the entire English-to-Japanese set, with the test set
having 2,801 entries. For comparison, we also test a
linear combination of the (normalized) system scores
with a manually tuned weight parameter (LINCOMB).
This baseline is similar to the system combination
method of Finch and Sumita (2010).
Table 5 contains the results for English-to-
Japanese transliterations, which indicate a significant
increase in accuracy in both cases, thereby demon-
strating the viability of our approach for system com-
bination. This experiment extends the system com-
bination result on English-to-Hindi transliteration
reported by Bhargava et al (2011), in which DI-
RECTL+ served as the base system while SEQUITUR
provided the supplemental data. The system in ques-
tion yielded nearly a 4% error rate reduction, which
made it the top-ranking submission at the NEWS
2011 Shared Task on Transliteration.
On the other hand, LINCOMB turns out to be a
strong baseline, which is evidenced by the fact that
the differences between our reranking approach and
LINCOMB are statistically insignificant. This is likely
because LINCOMB can take advantage of the full
n-best lists provided by both systems, whereas the
403
reranking approach uses only the top-1 result from
the ?supplemental? system. Combining the two n-
best lists in this way also gives a higher oracle score
of 86.4%, suggesting that this may be a good and
computationally cheap first step prior to reranking
using proper supplemental data as described above.
5 Future work
We plan on investigating a more parsimonious
method of incorporating supplemental data. There
are two aspects to this. First, while our experiments in
this paper treated base systems as black boxes for the
purposes of examining the effect of the supplemen-
tal data in isolation, reranking is limited by its post
hoc nature. After all, if the correct output does not
appear in the base systems? n-best list, even a perfect
reranker would be unable to find it. Incorporating the
supplemental data earlier in the process would allow
us to overcome this limitation at the expense of being
a solution specific to the base system.
Second, we would like to be able to incorporate
general supplemental information rather than being
limited by the existence of relevant data. In partic-
ular, a good transliteration model should encode a
general version of the information provided by a sin-
gle transliteration, so being able to apply that infor-
mation would allow us to overcome our dependence
on existing data as well as provide more potentially
useful information even when a transliteration or tran-
scription already exists.
Finally, we plan on examining other potential sup-
plemental resources. Given the success of our ap-
proach in the face of sometimes-noisy transliteration
data7, other noisy data may be applicable as well.
For example, IPA transcriptions could be mined from
Wikipedia despite the fact that different transcrip-
tions may have been written by different people. Sim-
ilarly, difficult-to-pronounce names or words are of-
ten accompanied by ad hoc approximately-phonetic
re-spellings, which may also prove useful.
6 Conclusion
In this paper, we examined the relevance of alter-
native, supplemental representations for the tasks
7Jiampojamarn et al (2009) found a significant increase in
English-to-Hindi transliteration performance after applying a
simple rule-based cleaning script.
of grapheme-to-phoneme conversion and machine
transliteration, both of which have pronunciation
as an important underlying influence. We applied
an SVM reranking approach that leverages the sup-
plemental data using features constructed from n-
grams as well as from similarity and system scores.
The approach yielded excellent improvements when
used with both the SEQUITUR and DIRECTL+ base
systems. Over the state-of-the-art DIRECTL+, we
achieved significant error rate reductions of 13%
for English-to-Japanese MTL using supplemental
transliterations, 13% using supplemental transcrip-
tions, and 33% for English G2P using supplemental
transcriptions. For system combination, we found a
smaller but still significant error rate reduction of 4%.
The fact that the improvements vary systematically
by base system help confirm that the supplemental
data do provide inherently useful information.
We can also take a step back to take a broader
look at our approach. It applies similar features as
those used in the standard generation task in a new,
orthogonal direction (supplemental data) with suc-
cessful results. This notion is general enough that it
may potentially be applicable to other tasks, such as
part-of-speech tagging or machine translation.
Acknowledgements
We thank Sittichai Jiampojamarn and Shane Bergsma
for helpful discussions. This research was supported
by the Natural Sciences and Engineering Research
Council of Canada.
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in Arabic texts. In Proceed-
ings of the ACL-02 Workshop on Computational Ap-
proaches to Semitic Languages, Philadelphia, Penn-
sylvania, USA, July. Association for Computational
Linguistics.
R. Harald Baayen, Richard Piepenbrock, and Leon Gulik-
ers. 1996. The CELEX2 lexical database. LDC96L14.
Aditya Bhargava and Grzegorz Kondrak. 2011. How
do you pronounce your name? Improving G2P with
transliterations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 399?408,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
404
Aditya Bhargava, Bradley Hauer, and Grzegorz Kondrak.
2011. Leveraging transliterations from multiple lan-
guages. In Proceedings of the 3rd Named Entities
Workshop (NEWS 2011), pages 36?40, Chiang Mai,
Thailand, November. Asian Federation of Natural Lan-
guage Processing.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conversion.
Speech Communication, 50(5):434?451, May.
Alan W. Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In The
Third ESCA/COCOSDA Workshop (ETRW) on Speech
Synthesis, Jenolan Caves House, Blue Mountains, New
South Wales, Australia, November.
Trevor Cohn and Mirella Lapata. 2007. Machine trans-
lation by triangulation: Making effective use of multi-
parallel corpora. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 728?735, Prague, Czech Republic, June.
Association for Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2010. Transliteration
using a phrase-based statistical machine translation sys-
tem to re-score the output of a joint multigram model.
In Proceedings of the 2010 Named Entities Workshop,
pages 48?52, Uppsala, Sweden, July. Association for
Computational Linguistics.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden Markov models to letter-to-phoneme con-
version. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Proceedings
of the Main Conference, pages 372?379, Rochester,
New York, USA, April. Association for Computational
Linguistics.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kon-
drak. 2008. Joint processing and discriminative train-
ing for letter-to-phoneme conversion. In Proceedings of
ACL-08: HLT, pages 905?913, Columbus, Ohio, USA,
June. Association for Computational Linguistics.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Ken-
neth Dwyer, and Grzegorz Kondrak. 2009. DirecTL:
a language independent approach to transliteration. In
Proceedings of the 2009 Named Entities Workshop:
Shared Task on Transliteration (NEWS 2009), pages 28?
31, Suntec, Singapore, August. Association for Com-
putational Linguistics.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kon-
drak. 2010a. Integrating joint n-gram features into
a discriminative training framework. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 697?700, Los Ange-
les, California, USA, June. Association for Computa-
tional Linguistics.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,
Aditya Bhargava, Qing Dou, Mi-Young Kim, and Grze-
gorz Kondrak. 2010b. Transliteration generation and
mining with limited training resources. In Proceedings
of the 2010 Named Entities Workshop, pages 39?47,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 133?142, Edmon-
ton, Alberta, Canada. Association for Computing Ma-
chinery.
Mitesh M. Khapra, A Kumaran, and Pushpak Bhat-
tacharyya. 2010. Everybody loves a rich cousin: An
empirical study of transliteration through bridge lan-
guages. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
420?428, Los Angeles, California, June. Association
for Computational Linguistics.
Anne K. Kienappel and Reinhard Kneser. 2001. De-
signing very compact decision trees for grapheme-to-
phoneme transcription. In EUROSPEECH-2001, pages
1911?1914, Aalborg, Denmark, September.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilingual
comparable corpora. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 82?88, New York City, USA, June.
Association for Computational Linguistics.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612, December.
John Kominek and Alan W. Black. 2006. Learning pro-
nunciation dictionaries: Language complexity and word
selection strategies. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 232?239, New York City, New York,
USA, June. Association for Computational Linguistics.
A. Kumaran, Mitesh M. Khapra, and Pushpak Bhat-
tacharyya. 2010a. Compositional machine translit-
eration. 9(4):13:1?13:29, December.
A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010b.
Report of news 2010 transliteration mining shared task.
In Proceedings of the 2010 Named Entities Workshop,
pages 21?28, Uppsala, Sweden, July. Association for
Computational Linguistics.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of the 42nd Meeting of the Association
405
for Computational Linguistics (ACL?04), Main Volume,
pages 159?166, Barcelona, Spain, July.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min
Zhang. 2009. Report of NEWS 2009 machine translit-
eration shared task. In Proceedings of the 2009 Named
Entities Workshop: Shared Task on Transliteration
(NEWS 2009), pages 1?18, Suntec, Singapore, August.
Association for Computational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Report of NEWS 2010 transliteration
generation shared task. In Proceedings of the 2010
Named Entities Workshop, pages 1?11, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Linsen Loots and Thomas R. Niesler. 2009. Data-driven
phonetic comparison and conversion between south
african, british and american english pronunciations. In
Proceedings of Interspeech, Brighton, UK, September.
Yannick Marchand and Robert I. Damper. 2000. A multi-
strategy approach to improving pronunciation by anal-
ogy. Computational Linguistics, 26(2):195?219, June.
Korin Richmond, Robert Clark, and Sue Fitt. 2009. Ro-
bust LTS rules with the Combilex speech technology
lexicon. In Proceedings of Interspeech, pages 1295?
1298, Brighton, UK, September.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning
string edit distance. IEEE Transactions on Pattern
Recognition and Machine Intelligence, 20(5):522?532,
May.
Terrence J. Sejnowski and Charles R. Rosenberg. 1987.
Parallel networks that learn to pronounce english text.
Complex Systems, 1(1):145?168.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2009. Adding more languages im-
proves unsupervised multilingual part-of-speech tag-
ging: a bayesian non-parametric approach. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
83?91, Boulder, Colorado, USA, June. Association for
Computational Linguistics.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable corpora.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 73?80, Sydney, Australia, July. Association for
Computational Linguistics.
Paul Taylor. 2005. Hidden Markov models for grapheme
to phoneme conversion. In Proceedings of Interspeech,
pages 1973?1976, Lisbon, Portugal, September.
Masao Utiyama and Hitoshi Isahara. 2007. A comparison
of pivot methods for phrase-based statistical machine
translation. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceed-
ings of theMain Conference, pages 484?491, Rochester,
New York, USA, April. Association for Computational
Linguistics.
van den Bosch and Walter Daelemans. 1998. Do not
forget: Full memory in memory-based learning of
word pronunciation. In D.M.W. Powers, editor, NeM-
LaP3/CoNLL98: NewMethods in Language Processing
and Computational Natural Language Learning, pages
195?204, Sydney, Australia. Association for Computa-
tional Linguistics.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot lan-
guage approach for machine translation. In Proceed-
ings of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
154?162, Suntec, Singapore, August. Association for
Computational Linguistics.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimina-
tive methods for transliteration. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 612?617, Sydney, Australia,
July. Association for Computational Linguistics.
Min Zhang, Xiangyu Duan, Vladimir Pervouchine, and
Haizhou Li. 2010. Machine transliteration: Leveraging
on third languages. In Coling 2010: Posters, pages
1444?1452, Beijing, China, August. Coling 2010 Orga-
nizing Committee.
Min Zhang, Haizhou Li, A Kumaran, and Ming Liu. 2011.
Report of NEWS 2011 machine transliteration shared
task. In Proceedings of the 3rd Named Entities Work-
shop (NEWS 2011), pages 1?13, Chiang Mai, Thailand,
November. Asian Federation of Natural Language Pro-
cessing.
406
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 399?408,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
How do you pronounce your name? Improving G2P with transliterations
Aditya Bhargava and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{abhargava,kondrak}@cs.ualberta.ca
Abstract
Grapheme-to-phoneme conversion (G2P) of
names is an important and challenging prob-
lem. The correct pronunciation of a name is
often reflected in its transliterations, which are
expressed within a different phonological in-
ventory. We investigate the problem of us-
ing transliterations to correct errors produced
by state-of-the-art G2P systems. We present a
novel re-ranking approach that incorporates a
variety of score and n-gram features, in order
to leverage transliterations from multiple lan-
guages. Our experiments demonstrate signifi-
cant accuracy improvements when re-ranking
is applied to n-best lists generated by three
different G2P programs.
1 Introduction
Grapheme-to-phoneme conversion (G2P), in which
the aim is to convert the orthography of a word to its
pronunciation (phonetic transcription), plays an im-
portant role in speech synthesis and understanding.
Names, which comprise over 75% of unseen words
(Black et al, 1998), present a particular challenge
to G2P systems because of their high pronunciation
variability. Guessing the correct pronunciation of a
name is often difficult, especially if they are of for-
eign origin; this is attested by the ad hoc transcrip-
tions which sometimes accompany new names intro-
duced in news articles, especially for international
stories with many foreign names.
Transliterations provide a way of disambiguating
the pronunciation of names. They are more abun-
dant than phonetic transcriptions, for example when
news items of international or global significance are
reported in multiple languages. In addition, writing
scripts such as Arabic, Korean, or Hindi are more
consistent and easier to identify than various pho-
netic transcription schemes. The process of translit-
eration, also called phonetic translation (Li et al,
2009b), involves ?sounding out? a name and then
finding the closest possible representation of the
sounds in another writing script. Thus, the correct
pronunciation of a name is partially encoded in the
form of the transliteration. For example, given the
ambiguous letter-to-phoneme mapping of the En-
glish letter g, the initial phoneme of the name Gersh-
win may be predicted by a G2P system to be ei-
ther /g/ (as in Gertrude) or /?/ (as in Gerald). The
transliterations of the name in other scripts provide
support for the former (correct) alternative.
Although it seems evident that transliterations
should be helpful in determining the correct pronun-
ciation of a name, designing a system that takes ad-
vantage of this insight is not trivial. The main source
of the difficulty stems from the differences between
the phonologies of distinct languages. The mappings
between phonemic inventories are often complex
and context-dependent. For example, because Hindi
has no /w/ sound, the transliteration of Gershwin
instead uses a symbol that represents the phoneme
/V/, similar to the /v/ phoneme in English. In ad-
dition, converting transliterations into phonemes is
often non-trivial; although few orthographies are as
inconsistent as that of English, this is effectively the
G2P task for the particular language in question.
In this paper, we demonstrate that leveraging
transliterations can, in fact, improve the grapheme-
to-phoneme conversion of names. We propose a
novel system based on discriminative re-ranking that
is capable of incorporating multiple transliterations.
We show that simplistic approaches to the problem
399
fail to achieve the same goal, and that translitera-
tions from multiple languages are more helpful than
from a single language. Our approach can be com-
bined with any G2P system that produces n-best lists
instead of single outputs. The experiments that we
perform demonstrate significant error reduction for
three very different G2P base systems.
2 Improving G2P with transliterations
2.1 Problem definition
In both G2P and machine transliteration, we are in-
terested in learning a function that, given an input
sequence x, produces an output sequence y. In the
G2P task, x is composed of graphemes and y is
composed of phonemes; in transliteration, both se-
quences consist of graphemes but they represent dif-
ferent writing scripts. Unlike in machine translation,
the monotonicity constraint is enforced; i.e., we as-
sume that x and y can be aligned without the align-
ment links crossing each other (Jiampojamarn and
Kondrak, 2010). We assume that we have available a
base G2P system that produces an n-best list of out-
puts with a corresponding list of confidence scores.
The goal is to improve the base system?s perfor-
mance by applying existing transliterations of the in-
put x to re-rank the system?s n-best output list.
2.2 Similarity-based methods
A simple and intuitive approach to improving G2P
with transliterations is to select from the n-best list
the output sequence that is most similar to the cor-
responding transliteration. For example, the Hindi
transliteration in Figure 1 is arguably closest in per-
ceptual terms to the phonetic transcription of the
second output in the n-best list, as compared to
the other outputs. One obvious problem with this
method is that it ignores the relative ordering of the
n-best lists and their corresponding scores produced
by the base system.
A better approach is to combine the similarity
score with the output score from the base system, al-
lowing it to contribute an estimate of confidence in
its output. For this purpose, we apply a linear combi-
nation of the two scores, where a single parameter ?,
ranging between zero and one, determines the rela-
tive weight of the scores. The exact value of ? can be
optimized on a training set. This approach is similar
to the method used by Finch and Sumita (2010) to
combine the scores of two different machine translit-
eration systems.
2.3 Measuring similarity
The approaches presented in the previous section
crucially depend on a method for computing the
similarity between various symbol sequences that
represent the same word. If we have a method
of converting transliterations to phonetic represen-
tations, the similarity between two sequences of
phonemes can be computed with a simple method
such as normalized edit distance or the longest com-
mon subsequence ratio, which take into account the
number and position of identical phonemes. Alter-
natively, we could apply a more complex approach,
such as ALINE (Kondrak, 2000), which computes
the distance between pairs of phonemes. However,
the implementation of a conversion program would
require ample training data or language-specific ex-
pertise.
A more general approach is to skip the tran-
scription step and compute the similarity between
phonemes and graphemes directly. For example, the
edit distance function can be learned from a training
set of transliterations and their phonetic transcrip-
tions (Ristad and Yianilos, 1998). In this paper, we
apply M2M-ALIGNER (Jiampojamarn et al, 2007),
an unsupervised aligner, which is a many-to-many
generalization of the learned edit distance algorithm.
M2M-ALIGNER was originally designed to align
graphemes and phonemes, but can be applied to dis-
cover the alignment between any sets of symbols
(given training data). The logarithm of the probabil-
ity assigned to the optimal alignment can then be
interpreted as a similarity measure between the two
sequences.
2.4 Discriminative re-ranking
The methods described in Section 2.2, which are
based on the similarity between outputs and translit-
erations, are difficult to generalize when multiple
transliterations of a single name are available. A lin-
ear combination is still possible but in this case opti-
mizing the parameters would no longer be straight-
forward. Also, we are interested in utilizing other
features besides sequence similarity.
The SVM re-ranking paradigm offers a solution
400
Gershwininput
/????w?n//d?????w?n/ /d?????w?n/n-best outputs
?????? ??????? ???????transliterations (/??r???n/) (/?a??uwi?/) (/?er?vin/)
Figure 1: An example name showing the data used for feature construction. Each arrow links a pair used to generate
features, including n-gram and score features. The score features use similarity scores for transliteration-transcription
pairs and system output scores for input-output pairs. One feature vector is constructed for each system output.
to the problem. Our re-ranking system is informed
by a large number of features, which are based on
scores and n-grams. The scores are of three types:
1. The scores produced by the base system for
each output in the n-best list.
2. The similarity scores between the outputs and
each available transliteration.
3. The differences between scores in the n-best
lists for both (1) and (2).
Our set of binary n-gram features includes those
used for DIRECTL+ (Jiampojamarn et al, 2010).
They can be divided into four types:
1. The context features combine output symbols
(phonemes) with n-grams of varying sizes in a
window of size c centred around a correspond-
ing position on the input side.
2. The transition features are bigrams on the out-
put (phoneme) side.
3. The linear chain features combine the context
features with the bigram transition features.
4. The joint n-gram features are n-grams contain-
ing both input and output symbols.
We apply the features in a new way: instead of be-
ing applied strictly to a given input-output set, we
expand their use across many languages and use all
of them simultaneously. We apply the n-gram fea-
tures across all transliteration-transcription pairs in
addition to the usual input-output pairs correspond-
ing to the n-best lists. Figure 1 illustrates the set of
pairs used for feature generation.
In this paper, we augment the n-gram features by
a set of reverse features. Unlike a traditional G2P
generator, our re-ranker has access to the outputs
produced by the base system. By swapping the input
and the output side, we can add reverse context and
linear-chain features. Since the n-gram features are
also applied to transliteration-transcription pairs, the
reverse features enable us to include features which
bind a variety of n-grams in the transliteration string
with a single corresponding phoneme.
The construction of n-gram features presupposes
a fixed alignment between the input and output se-
quences. If the base G2P system does not provide
input-output alignments, we use M2M-ALIGNER
for this purpose. The transliteration-transcription
pairs are also aligned by M2M-ALIGNER, which at
the same time produces the corresponding similarity
scores. (We set a lower limit of -100 on the M2M-
ALIGNER scores.) If M2M-ALIGNER is unable to
produce an alignment, we indicate this with a binary
feature that is included with the n-gram features.
3 Experiments
We perform several experiments to evaluate our
transliteration-informed approaches. We test simple
401
similarity-based approaches on single-transliteration
data, and evaluate our SVM re-ranking approach
against this as well. We then test our approach us-
ing all available transliterations. Relevant code and
scripts required to reproduce our experimental re-
sults are available online1.
3.1 Data & setup
For pronunciation data, we extracted all names from
the Combilex corpus (Richmond et al, 2009). We
discarded all diacritics, duplicates and multi-word
names, which yielded 10,084 unique names. Both
the similarity and SVM methods require transliter-
ations for identifying the best candidates in the n-
best lists. They are therefore trained and evaluated
on the subset of the G2P corpus for which transliter-
ations available. Naturally, allowing transliterations
from all languages results in a larger corpus than the
one obtained by the intersection with transliterations
from a single language.
For our experiments, we split the data into 10%
for testing, 10% for development, and 80% for
training. The development set was used for initial
tests and experiments, and then for our final results
the training and development sets were combined
into one set for final system training. For SVM re-
ranking, during both development and testing we
split the training set into 10 folds; this is necessary
when training the re-ranker as it must have system
output scores that are representative of the scores on
unseen data. We ensured that there was never any
overlap between the training and testing data for all
trained systems.
Our transliteration data come from the shared
tasks on transliteration at the 2009 and 2010 Named
Entities Workshops (Li et al, 2009a; Li et al, 2010).
We use all of the 2010 English-source data plus the
English-to-Russian data from 2009, which makes
nine languages in total. In cases where the data
provide alternative transliterations for a given in-
put, we keep only one; our preliminary experiments
indicated that including alternative transliterations
did not improve performance. It should be noted
that these transliteration corpora are noisy: Jiampo-
jamarn et al (2009) note a significant increase in
1http://www.cs.ualberta.ca/?ab31/
g2p-tl-rr
Language Corpus size Overlap
Bengali 12,785 1,840
Chinese 37,753 4,713
Hindi 12,383 2,179
Japanese 26,206 4,773
Kannada 10,543 1,918
Korean 6,761 3,015
Russian 6,447 487
Tamil 10,646 1,922
Thai 27,023 5,436
Table 1: The number of unique single-word entries in the
transliteration corpora for each language and the amount
of common data (overlap) with the pronunciation data.
English-to-Hindi transliteration performance with a
simple cleaning of the data.
Our tests involving transliterations from multiple
languages are performed on the set of names for
which we have both the pronunciation and translit-
eration data. There are 7,423 names in the G2P cor-
pus for which at least one transliteration is available.
Table 1 lists the total size of the transliteration cor-
pora as well as the amount of overlap with the G2P
data. Note that the base G2P systems are trained us-
ing all 10,084 names in the corpus as opposed to
only the 7,423 names for which there are transliter-
ations available. This ensures that the G2P systems
have more training data to provide the best possible
base performance.
For our single-language experiments, we normal-
ize the various scores when tuning the linear com-
bination parameter ? so that we can compare values
across different experimental conditions. For SVM
re-ranking, we directly implement the method of
Joachims (2002) to convert the re-ranking problem
into a classification problem, and then use the very
fast LIBLINEAR (Fan et al, 2008) to build the SVM
models. Optimal hyperparameter values were deter-
mined during development.
We evaluate using word accuracy, the percentage
of words for which the pronunciations are correctly
predicted. This measure marks pronunciations that
are even slightly different from the correct one as in-
correct, so even a small change in pronunciation that
might be acceptable or even unnoticeable to humans
would count against the system?s performance.
402
3.2 Base systems
It is important to test multiple base systems in order
to ensure that any gain in performance applies to the
task in general and not just to a particular system.
We use three G2P systems in our tests:
1. FESTIVAL (FEST), a popular speech synthe-
sis package, which implements G2P conver-
sion with CARTs (decision trees) (Black et al,
1998).
2. SEQUITUR (SEQ), a generative system based
on the joint n-gram approach (Bisani and Ney,
2008).
3. DIRECTL+ (DTL), the discriminative system
on which our n-gram features are based (Ji-
ampojamarn et al, 2010).
All systems are capable of providing n-best output
lists along with scores for each output, although for
FESTIVAL they had to be constructed from the list
of output probabilities for each input character.
We run DIRECTL+ with all of the features de-
scribed in (Jiampojamarn et al, 2010) (i.e., context
features, transition features, linear chain features,
and joint n-gram features). System parameters, such
as maximum number of iterations, were determined
during development. For SEQUITUR, we keep de-
fault options except for the enabling of the 10 best
outputs and we convert the probabilities assigned to
the outputs to log-probabilities. We set SEQUITUR?s
joint n-gram order to 6 (this was also determined
during development).
Note that the three base systems differ slightly in
terms of the alignment information that they pro-
vide in their outputs. FESTIVAL operates letter-by-
letter, so we use the single-letter inputs with the
phoneme outputs as the aligned units. DIRECTL+
specifies many-to-many alignments in its output. For
SEQUITUR, however, since it provides no informa-
tion regarding the output structure, we use M2M-
ALIGNER to induce alignments for n-gram feature
generation.
3.3 Transliterations from a single language
The goal of the first experiment is to compare sev-
eral similarity-based methods, and to determine how
they compare to our re-ranking approach. In order to
find the similarity between phonetic transcriptions,
we use the two different methods described in Sec-
tion 2.2: ALINE and M2M-ALIGNER. We further
test the use of a linear combination of the similar-
ity scores with the base system?s score so that its
confidence information can be taken into account;
the linear combination weight is determined from
the training set. These methods are referred to as
ALINE+BASE and M2M+BASE. For these experi-
ments, our training and testing sets are obtained by
intersecting our G2P training and testing sets respec-
tively with the Hindi transliteration corpus, yielding
1,950 names for training and 229 names for testing.
Since the similarity-based methods are designed
to incorporate homogeneous same-script translitera-
tions, we can only run this experiment on one lan-
guage at a time. Furthermore, ALINE operates on
phoneme sequences, so we first need to convert the
transliterations to phonemes. An alternative would
be to train a proper G2P system, but this would re-
quire a large set of word-pronunciation pairs. For
this experiment, we choose Hindi, for which we
constructed a rule-based G2P converter. Aside from
simple one-to-one mapping (romanization) rules,
the converter has about ten rules to adjust for con-
text.
For these experiments, we apply our SVM re-
ranking method in two ways:
1. Using only Hindi transliterations (referred to as
SVM-HINDI).
2. Using all available languages (referred to as
SVM-ALL).
In both cases, the test set is restricted to the same
229 names, in order to provide a valid comparison.
Table 2 presents the results. Regardless of the
choice of the similarity function, the simplest ap-
proaches fail in a spectacular manner, significantly
reducing the accuracy with respect to the base sys-
tem. The linear combination methods give mixed re-
sults, improving the accuracy for FESTIVAL but not
for SEQUITUR or DIRECTL+ (although the differ-
ences are not statistically significant). However, they
perform much better than the methods based on sim-
ilarity scores alone as they are able to take advan-
tage of the base system?s output scores. If we look
at the values of ? that provide the best performance
403
Base system
FEST SEQ DTL
Base 58.1 67.3 71.6
ALINE 28.0 26.6 27.5
M2M 39.3 36.2 36.2
ALINE+BASE 58.5 65.9 71.2
M2M+BASE 58.5 66.4 70.3
SVM-HINDI 63.3 69.0 69.9
SVM-ALL 68.6 72.5 75.6
Table 2: Word accuracy (in percentages) of various meth-
ods when only Hindi transliterations are used.
on the training set, we find that they are higher for
the stronger base systems, indicating more reliance
on the base system output scores. For example,
for ALINE+BASE the FESTIVAL-based system has
? = 0.58 whereas the DIRECTL+-based system has
? = 0.81. Counter-intuitively, the ALINE+BASE
and M2M+BASE methods are unable to improve
upon SEQUITUR or DIRECTL+. We would expect
to achieve at least the base system?s performance,
but disparities between the training and testing sets
prevent this.
The two SVM-based methods achieve much bet-
ter results. SVM-ALL produces impressive accu-
racy gains for all three base systems, while SVM-
HINDI yields smaller (but still statistically signifi-
cant) improvements for FESTIVAL and SEQUITUR.
These results suggest that our re-ranking method
provides a bigger boost to systems built with dif-
ferent design principles than to DIRECTL+ which
utilizes a similar set of features. On the other hand,
the results also show that the information obtained
by consulting a single transliteration may be insuf-
ficient to improve an already high-performing G2P
converter.
3.4 Transliterations from multiple languages
Our second experiment expands upon the first; we
use all available transliterations instead of being re-
stricted to one language. This rules out the sim-
ple similarity-based approaches, but allows us to
test our re-ranking approach in a way that fully uti-
lizes the available data. We test three variants of our
transliteration-informed SVM re-ranking approach,
Base system
FEST SEQ DTL
Base 55.3 66.5 70.8
SVM-SCORE 62.1 68.4 71.0
SVM-N-GRAM 66.2 72.5 73.8
SVM-ALL 67.2 73.4 74.3
Table 3: Word accuracy of the base system versus the re-
ranking variants with transliterations from multiple lan-
guages.
which differ with respect to the set of included fea-
tures:
1. SVM-SCORE includes only the three types of
score features described in Section 2.4.
2. SVM-N-GRAM uses only the n-gram features.
3. SVM-ALL is the full system that combines the
score and n-gram features.
The objective is to determine the degree to which
each of the feature classes contributes to the overall
results. Because we are using all available transliter-
ations, we achieve much greater coverage over our
G2P data than in the previous experiment; in this
case, our training set consists of 6,660 names while
the test set has 763 names.
Table 3 presents the results. Note that the base-
line accuracies are somewhat lower than in Table 2
because of the different test set. We find that, when
using all features, the SVM re-ranker can provide
a very impressive error reduction over FESTIVAL
(26.7%) and SEQUITUR (20.7%) and a smaller but
still significant (p < 0.01 with the McNemar test)
error reduction over DIRECTL+ (12.1%).
When we consider our results using only the score
and n-gram features, we can see that, interestingly,
the n-gram features are most important. We draw
a further conclusion from our results: consider the
large disparity in improvements over the base sys-
tems. This indicates that FESTIVAL and SEQUITUR
are benefiting from the DIRECTL+-style features
used in the re-ranking. Without the n-gram fea-
tures, however, there is still a significant improve-
ment over FESTIVAL, demonstrating that the scores
do provide useful information. In this case there is
404
no way for DIRECTL+-style information to make
its way into the re-ranking; the process is based
purely on the transliterations and their similarities
with the transcriptions in the output lists, indicat-
ing that the system is capable of extracting use-
ful information directly from transliterations. In the
case of DIRECTL+, the transliterations help through
the n-gram features rather than the score features;
this is probably because the crucial feature that
signals the inability of M2M-ALIGNER to align a
given transliteration-transcription pair belongs to the
set of the n-gram features. Both the n-gram fea-
tures and score features are dependent on the align-
ments, but they differ in that the n-gram features
allow weights to be learned for local n-gram pairs
whereas the score features are based on global infor-
mation, providing only a single feature for a given
transliteration-transcription pair. The two therefore
overlap to some degree, although the score fea-
tures still provide useful information via probabili-
ties learned during the alignment training process.
A closer look at the results provides additional
insight into the operation of our re-ranking system.
For example, consider the name Bacchus, which DI-
RECTL+ incorrectly converts into /b?k?@s/. The
most likely reason why our re-ranker selects instead
the correct pronunciation /b?k@s/ is that M2M-
ALIGNER fails to align three of the five available
transliterations with /b?k?@s/. Such alignment fail-
ures are caused by a lack of evidence for the map-
ping of the grapheme representing the sound /k/
in the transliteration training data with the phoneme
/?/. In addition, the lack of alignments prevents any
n-gram features from being enabled.
Considering the difficulty of the task, the top ac-
curacy of almost 75% is quite impressive. In fact,
many instances of human transliterations in our cor-
pora are clearly incorrect. For example, the Hindi
transliteration of Bacchus contains the /?/ conso-
nant instead of the correct /k/. Moreover, our strict
evaluation based on word accuracy counts all sys-
tem outputs that fail to exactly match the dictio-
nary data as errors. The differences are often very
minor and may reflect an alternative pronunciation.
The phoneme accuracy2 of our best result is 93.1%,
2The phoneme accuracy is calculated from the minimum
edit distance between the predicted and correct pronunciations.
# TL # Entries Improvement
? 1 111 0.9
? 2 266 3.0
? 3 398 3.8
? 4 536 3.2
? 5 619 2.8
? 6 685 3.4
? 7 732 3.7
? 8 762 3.5
? 9 763 3.5
Table 4: Absolute improvement in word accuracy (%)
over the base system (DIRECTL+) of the SVM re-ranker
for various numbers of available transliterations.
which provides some idea of how similar the pre-
dicted pronunciation is to the correct one.
3.5 Effect of multiple transliterations
One motivating factor for the use of SVM re-ranking
was the ability to incorporate multiple transliteration
languages. But how important is it to use more than
one language? To examine this question, we look
particularly at the sets of names having at most k
transliterations available. Table 4 shows the results
with DIRECTL+ as the base system. Note that the
number of names with more than five transliterations
was small. Importantly, we see that the increase in
performance when only one transliteration is avail-
able is so small as to be insignificant. From this, we
can conclude that obtaining improvement on the ba-
sis of a single transliteration is difficult in general.
This corroborates the results of the experiment de-
scribed in Section 3.3, where we used only Hindi
transliterations.
4 Previous work
There are three lines of research that are relevant to
our work: (1) G2P in general; (2) G2P on names; and
(3) combining diverse data sources and/or systems.
The two leading approaches to G2P are repre-
sented by SEQUITUR (Bisani and Ney, 2008) and
DIRECTL+ (Jiampojamarn et al, 2010). Recent
comparisons suggests that the former obtains some-
what higher accuracy, especially when it includes
joint n-gram features (Jiampojamarn et al, 2010).
Systems based on decision trees are far behind. Our
405
results confirm this ranking.
Names can present a particular challenge to G2P
systems. Kienappel and Kneser (2001) reported a
higher error rate for German names than for general
words, while on the other hand Black et al (1998)
report similar accuracy on names as for other types
of English words. Yang et al (2006) and van den
Heuvel et al (2007) post-process the output of a
general G2P system with name-specific phoneme-
to-phoneme (P2P) systems. They find significant im-
provement using this method on data sets consisting
of Dutch first names, family names, and geograph-
ical names. However, it is unclear whether such an
approach would be able to improve the performance
of the current state-of-the-art G2P systems. In addi-
tion, the P2P approach works only on single outputs,
whereas our re-ranking approach is designed to han-
dle n-best output lists.
Although our approach is (to the best of our
knowledge) the first to use different tasks (G2P and
transliteration) to inform each other, this is concep-
tually similar to model and system combination ap-
proaches. In statistical machine translation (SMT),
methods that incorporate translations from other lan-
guages (Cohn and Lapata, 2007) have proven effec-
tive in low-resource situations: when phrase trans-
lations are unavailable for a certain language, one
can look at other languages where the translation
is available and then translate from that language.
A similar pivoting approach has also been applied
to machine transliteration (Zhang et al, 2010). No-
tably, the focus of these works have been on cases in
which there are less data available; they also modify
the generation process directly, rather than operating
on existing outputs as we do. Ultimately, a combina-
tion of the two approaches is likely to give the best
results.
Finch and Sumita (2010) combine two very dif-
ferent approaches to transliteration using simple lin-
ear interpolation: they use SEQUITUR?s n-best out-
puts and re-rank them using a linear combination
of the original SEQUITUR score and the score for
that output of a phrased-based SMT system. The lin-
ear weights are hand-tuned. We similarly use linear
combinations, but with many more scores and other
features, necessitating the use of SVMs to determine
the weights. Importantly, we combine different data
types where they combine different systems.
5 Conclusions & future work
In this paper, we explored the application of translit-
erations to G2P. We demonstrated that transliter-
ations have the potential for helping choose be-
tween n-best output lists provided by standard G2P
systems. Simple approaches based solely on sim-
ilarity do not work when tested using a single
transliteration language (Hindi), necessitating the
use of smarter methods that can incorporate mul-
tiple transliteration languages. We apply SVM re-
ranking to this task, enabling us to use a variety
of features based not only on similarity scores but
on n-grams as well. Our method shows impressive
error reductions over the popular FESTIVAL sys-
tem and the generative joint n-gram SEQUITUR sys-
tem. We also find significant error reduction using
the state-of-the-art DIRECTL+ system. Our analy-
sis demonstrated that it is essential to provide the
re-ranking system with transliterations from multi-
ple languages in order to mitigate the differences
between phonological inventories and smooth out
noise in the transliterations.
In the future, we plan to generalize our approach
so that it can be applied to the task of generating
transliterations, and to combine data from distinct
G2P dictionaries. The latter task is related to the no-
tion of domain adaptation. We would also like to ap-
ply our approach to web data; we have shown that it
is possible to use noisy transliteration data, so it may
be possible to leverage the noisy ad hoc pronuncia-
tion data as well. Finally, we plan to investigate ear-
lier integration of such external information into the
G2P process for single systems; while we noted that
re-ranking provides a general approach applicable to
any system that can generate n-best lists, there is a
limit as to what re-ranking can do, as it relies on the
correct output existing in the n-best list. Modifying
existing systems would provide greater potential for
improving results even though the changes would be
necessarily system-specific.
Acknowledgements
We are grateful to Sittichai Jiampojamarn and Shane
Bergsma for the very helpful discussions. This re-
search was supported by the Natural Sciences and
Engineering Research Council of Canada.
406
References
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451, May.
Alan W. Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In The
Third ESCA/COCOSDA Workshop (ETRW) on Speech
Synthesis, Jenolan Caves House, Blue Mountains, New
South Wales, Australia, November.
Trevor Cohn and Mirella Lapata. 2007. Machine trans-
lation by triangulation: Making effective use of multi-
parallel corpora. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 728?735, Prague, Czech Republic, June.
Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Andrew Finch and Eiichiro Sumita. 2010. Translitera-
tion using a phrase-based statistical machine transla-
tion system to re-score the output of a joint multigram
model. In Proceedings of the 2010 Named Entities
Workshop (NEWS 2010), pages 48?52, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Sittichai Jiampojamarn and Grzegorz Kondrak. 2010.
Letter-phoneme alignment: An exploration. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 780?788,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden Markov models to letter-to-phoneme con-
version. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Proceedings
of the Main Conference, pages 372?379, Rochester,
New York, USA, April. Association for Computational
Linguistics.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language independent approach to translitera-
tion. In Proceedings of the 2009 Named Entities Work-
shop: Shared Task on Transliteration (NEWS 2009),
pages 28?31, Suntec, Singapore, August. Association
for Computational Linguistics.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kon-
drak. 2010. Integrating joint n-gram features into a
discriminative training framework. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 697?700, Los An-
geles, California, USA, June. Association for Compu-
tational Linguistics.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 133?142, Ed-
monton, Alberta, Canada. Association for Computing
Machinery.
Anne K. Kienappel and Reinhard Kneser. 2001. De-
signing very compact decision trees for grapheme-
to-phoneme transcription. In EUROSPEECH-2001,
pages 1911?1914, Aalborg, Denmark, September.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proceedings of
the First Meeting of the North American Chapter of
the Association for Computational Linguistics, pages
288?295, Seattle, Washington, USA, April.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min
Zhang. 2009a. Report of NEWS 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Transliter-
ation (NEWS 2009), pages 1?18, Suntec, Singapore,
August. Association for Computational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2009b. Whitepaper of NEWS 2009 ma-
chine transliteration shared task. In Proceedings
of the 2009 Named Entities Workshop: Shared Task
on Transliteration (NEWS 2009), pages 19?26, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Report of NEWS 2010 transliteration
generation shared task. In Proceedings of the 2010
Named Entities Workshop (NEWS 2010), pages 1?11,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Korin Richmond, Robert Clark, and Sue Fitt. 2009. Ro-
bust LTS rules with the Combilex speech technology
lexicon. In Proceedings of Interspeech, pages 1295?
1298, Brighton, UK, September.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string edit distance. IEEE Transactions on Pattern
Recognition and Machine Intelligence, 20(5):522?
532, May.
Henk van den Heuvel, Jean-Pierre Martens, and Nanneke
Konings. 2007. G2P conversion of names. what can
we do (better)? In Proceedings of Interspeech, pages
1773?1776, Antwerp, Belgium, August.
Qian Yang, Jean-Pierre Martens, Nanneke Konings, and
Henk van den Heuvel. 2006. Development of a
phoneme-to-phoneme (p2p) converter to improve the
grapheme-to-phoneme (g2p) conversion of names. In
407
Proceedings of the 2006 International Conference on
Language Resources and Evaluation, pages 2570?
2573, Genoa, Italy, May.
Min Zhang, Xiangyu Duan, Vladimir Pervouchine, and
Haizhou Li. 2010. Machine transliteration: Leverag-
ing on third languages. In Coling 2010: Posters, pages
1444?1452, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
408
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 39?47,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Transliteration Generation and Mining with Limited Training Resources
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma, Aditya Bhargava,
Qing Dou, Mi-Young Kim, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,dwyer,bergsma,abhargava,qdou,miyoung2,kondrak}@cs.ualberta.ca
Abstract
We present DIRECTL+: an online dis-
criminative sequence prediction model
based on many-to-many alignments,
which is further augmented by the in-
corporation of joint n-gram features.
Experimental results show improvement
over the results achieved by DIRECTL in
2009. We also explore a number of diverse
resource-free and language-independent
approaches to transliteration mining,
which range from simple to sophisticated.
1 Introduction
Many out-of-vocabulary words in statistical ma-
chine translation and cross-language information
retrieval are named entities. If the languages in
question use different writing scripts, such names
must be transliterated. Transliteration can be de-
fined as the conversion of a word from one writ-
ing script to another, which is usually based on the
phonetics of the original word.
DIRECTL+ is our current approach to name
transliteration which is an extension of the DI-
RECTL system (Jiampojamarn et al, 2009). We
augmented the feature set with joint n-gram fea-
tures which allow the discriminative model to uti-
lize long dependencies of joint information of
source and target substrings (Jiampojamarn et al,
2010). Experimental results suggest an improve-
ment over the results achieved by DIRECTL in
2009.
Transliteration mining aims at automatically
obtaining bilingual lists of names written in differ-
ent scripts. We explore a number of different ap-
proaches to transliteration mining in the context of
the NEWS 2010 Shared Task.1 The sole resource
that is provided for each language pair is a ?seed?
1http://translit.i2r.a-star.edu.sg/
news2010
dataset that contains 1K transliteration word pairs.
The objective is then to mine transliteration pairs
from a collection of Wikipedia titles/topics that are
given in both languages.
We explore a number of diverse resource-free
and language-independent approaches to translit-
eration mining. One approach is to bootstrap the
seed data by generating pseudo-negative exam-
ples, which are combined with the positives to
form a dataset that can be used to train a clas-
sifier. We are particularly interested in achiev-
ing good performance without utilizing language-
specific resources, so that the same approach can
be applied with minimal or no modifications to an
array of diverse language pairs.
This paper is divided in two main parts that cor-
respond to the two tasks of transliteration genera-
tion and transliteration mining.
2 Transliteration generation
The structure of this section is as follows. In Sec-
tion 2.1, we describe the pre-processing steps that
were applied to all datasets. Section 2.2 reviews
two methods for aligning the source and target
symbols in the training data. We provide details
on the DIRECTL+ systems in Section 2.3. In Sec-
tion 2.4, we discuss extensions of DIRECTL+ that
incorporate language-specific information. Sec-
tion 2.5 summarizes our results.
2.1 Pre-processing
For all generation tasks, we pre-process the pro-
vided data as follows. First, we convert all char-
acters in the source word to lower case. Then,
we remove non-alphabetic characters unless they
appear in both the source and target words. We
normalize whitespace that surrounds a comma, so
that there are no spaces before the comma and ex-
actly one space following the comma. Finally, we
separate multi-word titles into single words, using
whitespace as the separator. We assume a mono-
39
tonic matching and ignore the titles that have a dif-
ferent number of words on both sides.
We observed that in the ArAe task there are
cases where an extra space is added to the target
when transliterating from Arabic names to their
English equivalents; e.g., ?Al Riyad?, ?El Sayed?,
etc. In order to prevent the pre-processing from
removing too many title pairs, we allow non-equal
matching if the source title is a single word.
For the English-Chinese (EnCh) task, we con-
vert the English letter ?x? to ?ks? to facilitate bet-
ter matching with its Chinese targets.
During testing, we pre-process test data in the
same manner, except that we do not remove non-
alphabetic characters. After the pre-processing
steps, our system proposes 10-best lists for single
word titles in the test data. For multi-word titles,
we construct 10-best lists by ranking the combina-
tion scores of single words that make up the test
titles.
2.2 Alignment
In the transliteration tasks, training data consist
of pairs of names written in source and target
scripts without explicit character-level alignment.
In our experiments, we applied two different algo-
rithms to automatically generate alignments in the
training data. The generated alignments provide
hypotheses of substring mappings in the training
data. Given aligned training data, a transliteration
model is trained to generate names in the target
language given names in the source language.
The M2M-aligner (Jiampojamarn et al, 2007)
is based on the expectation maximization (EM)
algorithm. It allows us to create alignments be-
tween substrings of various lengths. We opti-
mized the maximum substring sizes for the source
and target based on the performance of the end
task on the development sets. We allowed empty
strings (nulls) only on the target side. We used the
M2M-aligner for all alignment tasks, except for
English-Pinyin alignment. The source code of the
M2M-aligner is publicly available.2
An alternative alignment algorithm is based on
the phonetic similarity of graphemes. The key idea
of this approach is to represent each grapheme by a
phoneme or a sequence of phonemes that is likely
to be represented by the grapheme. The sequences
of phonemes on the source side and the target
side can then be aligned on the basis of phonetic
2http://code.google.com/p/m2m-aligner/
b a r c - l a y
| | | | | | | |
b a - k u r - i
Figure 1: An alignment example.
similarity between phonemes. The main advan-
tage of the phonetic alignment is that it requires
no training data. We use the ALINE phonetic
aligner (Kondrak, 2000), which aligns two strings
of phonemes. The example in Figure 1 shows
the alignment of the word Barclay to its Katakana
transliteration ba-ku-ri. The one-to-one alignment
can then be converted to a many-to-many align-
ment by grouping the Japanese phonemes that cor-
respond to individual Katakana symbols.
2.3 DIRECTL+
We refer to our present approach to transliteration
as DIRECTL+. It is an extension of our DIRECTL
system (Jiampojamarn et al, 2009). It includes ad-
ditional ?joint n-gram? features that allow the dis-
criminative model to correlate longer source and
target substrings. The additional features allow
our discriminative model to train on information
that is present in generative joint n-gram models,
and additionally train on rich source-side context,
transition, and linear-chain features that have been
demonstrated to be important in the transliteration
task (Jiampojamarn et al, 2010).
Our model is based on an online discriminative
framework. At each training iteration, the model
generates an m-best list for each given source
name based on the current feature weights. The
feature weights are updated according to the gold-
standard answers and the generated m-best an-
swer lists using the Margin Infused Relaxed Algo-
rithm (MIRA) (Crammer and Singer, 2003). This
training process iterates over the training examples
until the model converges. For m-best and n-gram
parameters, we set m = 10 and n = 6 for all lan-
guage pairs. These parameters as well as others
were optimized on the development sets.
We trained our models directly on the data
that were provided by the organizers, with three
exceptions. In order to improve performance,
we gave special treatment to English-Korean
(EnKo), English-Chinese (EnCh), and English-
Hindi (EnHi). These special cases are described
in the next section.
40
2.4 Beyond DIRECTL+
2.4.1 Korean Jaso
A Korean syllable can be decomposed into two
or three components called Jaso: an initial con-
sonant, a middle vowel, and optionally a final con-
sonant. The Korean generation for EnKo involves
the following three steps: (1) English-to-Jaso gen-
eration, (2) correction of illegal Jaso sequences,
and (3) Jaso-to-Korean conversion.
In order to correct illegal Jaso sequences that
cannot be combined into Korean syllables in step
2, we consider both vowel and consonant rules.
A Korean vowel can be either a simple vowel or
a complex vowel that combines two simple vow-
els. We can use this information in order to replace
double vowels with one complex vowel. We also
use the silent consonant o (i-eung) when we need
to insert a consonant between double vowels. A
Korean vowel - (eu) is most commonly inserted
between two English consonants in transliteration.
In order to resolve three consecutive consonants, it
can be placed into the most probable position ac-
cording to the probability distribution of the train-
ing data.
2.4.2 Japanese Katakana
In the Japanese Katakana generation task, we re-
place each Katakana symbol with one or two let-
ters using standard romanization tables. This has
the effect of expressing the target side in Latin let-
ters, which facilitates the alignment. DIRECTL+
is trained on the converted data to generate the tar-
get from the source. A post-processing program
then attempts to convert the generated letters back
into Katakana symbols. Sequences of letters that
cannot be converted into Katakana are removed
from the output m-best lists and replaced by lower
scoring sequences that pass the back-conversion
filter. Otherwise, there is usually a single valid
mapping because most Katakana symbols are rep-
resented by single vowels or a consonant-vowel
pair. The only apparent ambiguity involves the
letter n, which can either stand by itself or clus-
ter with the following vowel letter. We resolve the
ambiguity by always assuming the latter case un-
less the letter n occurs at the end of the word.
2.4.3 Chinese Pinyin
Following (Jiampojamarn et al, 2009), we experi-
mented with converting the original Chinese char-
acters to Pinyin as an intermediate representation.
Pinyin is the most commonly known romanization
system for Standard Mandarin and many free tools
are available for converting Chinese characters to
Pinyin. Its alphabet contains the same 26 letters
as English. Each Chinese character can be tran-
scribed phonetically into Pinyin. A small percent-
age of Chinese characters have multiple pronunci-
ations, and are thus represented by different Pinyin
sequences. For those characters, we manually se-
lected the pronunciations that are normally used
for names. This pre-processing step significantly
reduces the size of the target symbols: from 370
distinct Chinese characters to 26 Pinyin symbols.
This allows our system to produce better align-
ments.
We developed three models: (1) trained on the
original Chinese characters, (2) trained on Pinyin,
and (3) the model that incorporates the phonetic
alignment described in Section 2.2. The combi-
nation of the predictions of the different systems
was performed using the following simple algo-
rithm (Jiampojamarn et al, 2009). First, we rank
the individual systems according to their top-1 ac-
curacy on the development set. To obtain the top-
1 prediction for each input word, we use simple
voting, with ties broken according to the ranking
of the systems. We generalize this approach to
handle n-best lists by first ordering the candidate
transliterations according to the rank assigned by
each individual system, and then similarly break-
ing ties by voting and using the ranking of the sys-
tems.
2.4.4 Language identification for Hindi
Bhargava and Kondrak (2010) apply support vec-
tor machines (SVMs) to the task of identifying
the language of names. The intuition here is that
language information can inform transliteration.
Bhargava and Kondrak (2010) test this hypothe-
sis on the NEWS 2009 English-Hindi transliter-
ation data by training language identification on
data manually tagged as being of either Indian or
non-Indian origin. It was found that splitting the
data disjointly into two sets and training separate
transliteration models yields no performance in-
crease due to the decreased size of the data for the
models.
We adopt this approach for the NEWS 2010
task, but here we do not use disjoint splits. In-
stead, we use the SVMs to generate probabilities,
and then we apply a threshold to these probabili-
ties to generate two datasets. For example, if we
set the threshold to be 0.05, then we determine the
41
probabilities of a given name being of Indian ori-
gin (phi) and of being of non-Indian origin (pen).
If phi < 0.05 then the name is excluded from the
Indian set, and if pen < 0.05 then the name is
excluded from the non-Indian set. Using the two
obtained non-disjoint sets, we then train a translit-
eration model for each set using DIRECTL+.
Since the two sets are not disjoint, we must de-
cide how to combine the two results. Given that a
name occurs in both sets, and both models provide
a ranked list of possible targets for that name, we
obtain a combined ranking using a linear combi-
nation over the mean reciprocal ranks (MRRs) of
the two lists. The weights used are phi and pen so
that the more likely a name is considered to be of
Indian origin, the more strongly the result from the
Indian set is considered relative to the result from
the non-Indian set.
2.5 Evaluation
In the context of the NEWS 2010 Machine
Transliteration Shared Task we tested our sys-
tem on all twelve datasets: from English to Chi-
nese (EnCh), Thai (EnTh), Hindi (EnHi), Tamil
(EnTa), Bangla (EnBa), Kannada (EnKa), Ko-
rean Hangul (EnKo), Japanese Katakana (EnJa),
Japanese Kanji (JnJk); and, in the opposite di-
rection, to English from Arabic (ArAe), Chi-
nese (ChEn), and Thai (ThEn). For all datasets,
we trained transliteration models on the provided
training and development sets without additional
resources.
Table 1 shows our best results obtained on the
datasets in terms of top-1 accuracy and mean F-
score. We also include the rank in standard runs
ordered by top-1 word accuracy. The EnCh re-
sult presented in the table refers to the output of
the three-system combination, using the combi-
nation algorithm described in Section 2.4.3. The
respective results for the three component EnCh
systems were: 0.357, 0.360, and 0.363. The
EnJa result in the table refers the system described
in Section 2.4.2 that applied specific treatment
to Japanese Katakana. Based on our develop-
ment results, this specific treatment improves as
much as 2% top-1 accuracy over the language-
independent model. The EnHi system that in-
corporates language identification obtained ex-
actly the same top-1 accuracy as the language-
independent model. However, the EnKo system
with Jaso correction produced the top-1 accu-
Task top-1 F-score Rank
EnCh 0.363 0.707 2
ChEn 0.137 0.740 1
EnTh 0.378 0.866 2
ThEn 0.352 0.861 2
EnHi 0.456 0.884 1
EnTa 0.390 0.891 2
EnKa 0.341 0.867 2
EnJa 0.398 0.791 1
EnKo 0.554 0.770 1
JnJk 0.126 0.426 1
ArAe 0.464 0.924 1
EnBa 0.395 0.877 2
Table 1: Transliteration generation results
racy of 0.554, which is a significant improvement
over 0.387 achieved by the language-independent
model.
3 Transliteration mining
This section is structured as follows. In Sec-
tion 3.1, we describe the method of extracting
transliteration candidates that serves as the input
to the subsequently presented mining approaches.
Two techniques for generating negative exam-
ples are discussed in Section 3.2. Our language-
independent approaches to transliteration mining
are described in Section 3.3, and a technique for
mining English-Chinese pairs is proposed in Sec-
tion 3.4. In Section 3.5, we address the issue of
overlapping predictions. Finally, Section 3.6 and
Section 3.7 summarize our results.
3.1 Extracting transliteration candidates
We cast the transliteration mining task as a bi-
nary classification problem. That is, given a word
in the source language and a word in the target
language, a classifier predicts whether or not the
pair constitutes a valid transliteration. As a pre-
processing step, we extract candidate translitera-
tions from the pairs of Wikipedia titles. Word seg-
mentation is performed based on sequences of one
or more spaces and/or punctuation symbols, which
include hyphens, underscores, brackets, and sev-
eral other non-alphanumeric characters. Apostro-
phes and single quotes are not used for segmenta-
tion (and therefore remain in a given word); how-
ever, all single quote-like characters are converted
into a generic apostrophe. Once an English ti-
tle and its target language counterpart have been
42
segmented into words, we form the candidate set
for this title as the cross product of the two sets
of words after discarding any words that contain
fewer than two characters.
After the candidates have been extracted, indi-
vidual words are flagged for certain attributes that
may be used by our supervised learner as addi-
tional features. Alternatively, the flags may serve
as criteria for filtering the list of candidate pairs
prior to classification. We identify words that are
capitalized, consist of all lowercase (or all capital)
letters, and/or contain one or more digits. We also
attempt to encode each word in the target language
as an ASCII string, and flag that word if the opera-
tion succeeds. This can be used to filter out words
that are written in English on both the source and
target side, which are not transliterations by defi-
nition.
3.2 Generating negative training examples
The main issue with applying a supervised learn-
ing approach to the NEWS 2010 Shared Task is
that annotated task-specific data is not available
to train the system. However, the seed pairs do
provide example transliterations, and these can be
used as positive training examples. The remaining
issue is how to select the negative examples.
We adopt two approaches for selecting nega-
tives. First, we generate all possible source-target
pairs in the seed data, and take as negatives those
pairs which are not transliterations but have a
longest common subsequence ratio (LCSR) above
0.58; this mirrors the approach used by Bergsma
and Kondrak (2007). The method assumes that
the source and target words are written in the same
script (e.g., the foreign word has been romanized).
A second possibility is to generate all seed pair-
ings as above, but then randomly select negative
examples, thus mirroring the approach in Klemen-
tiev and Roth (2006). In this case, the source and
target scripts do not need to be the same. Com-
pared with the LCSR technique, random sampling
in this manner has the potential to produce nega-
tive examples that are very ?easy? (i.e., clearly not
transliterations), and which may be of limited util-
ity when training a classifier. On the other hand, at
test time, the set of candidates extracted from the
Wikipedia data will include pairs that have very
low LCSR scores; hence, it can be argued that dis-
similar pairs should also appear as negative exam-
ples in the training set.
3.3 Language-independent approaches
In this section, we describe methods for transliter-
ation mining that can, in principle, be applied to a
wide variety of language pairs without additional
modification. For the purposes of the Shared Task,
however, we convert all source (English) words to
ASCII by removing diacritics and making appro-
priate substitutions for foreign letters. This is done
to mitigate sparsity in the relatively small seed sets
when training our classifiers.
3.3.1 Alignment-derived romanization
We developed a simple method of performing ro-
manization of foreign scripts. Initially, the seed set
of transliterations is aligned using the one-to-one
option of the M2M-aligner approach (Jiampoja-
marn et al, 2007). We allow nulls on both the
source and target sides. The resulting alignment
model contains pairs of Latin letters and foreign
script symbols (graphemes) sorted by their con-
ditional probability. Then, for each grapheme,
we select a letter (or a null symbol) that has the
highest conditional probability. The process pro-
duces an approximate romanization table that can
be obtained without any knowledge of the target
script. This method of romanization was used by
all methods described in the remainder of Sec-
tion 3.3.
3.3.2 Normalized edit distance
Normalized edit distance (NED) is a measure of
the similarity of two strings. We define a uniform
edit cost for each of the three operations: substitu-
tion, insertion, and deletion. NED is computed by
dividing the minimum edit distance by the length
of the longer string, and subtracting the resulting
fraction from 1. Thus, the extreme values of NED
are 1 for identical strings, and 0 for strings that
have no characters in common.
Our baseline method, NED+ is simply the NED
measure augmented with filtering of the candidate
pairs described in Section 3.1. In order to address
the issue of morphological variants, we also fil-
ter out the pairs in which the English word ends
in a consonant and the foreign word ends with a
vowel. With no development set provided, we set
the similarity thresholds for individual languages
on the basis of the average word length in the seed
sets. The values were 0.38, 0.48, 0.52, and 0.58
for Hindi, Arabic, Tamil, and Russian, respec-
tively, with the last number taken from Bergsma
and Kondrak (2007).
43
3.3.3 Alignment-based string similarity
NED selects transliteration candidates when the
romanized foreign strings have high character
overlap with their English counterparts. The mea-
sure is independent of the language pair. This
is suboptimal for several reasons. First of all,
phonetically unrelated words can share many in-
cidental character matches. For example, the
French word ?recettes? and the English word
?proceeds? share the letters r,c,e,e,s as a com-
mon subsequence, but the words are phonetically
unrelated. Secondly, many reliable, recurrent,
language-specific substring matches are prevalent
in true transliterations. These pairings may or may
not involve matching characters. NED can not
learn or adapt to these language-specific patterns.
In light of these drawbacks, researchers have
proposed string similarity measures that can learn
from provided example pairs and adapt the simi-
larity function to a specific task (Ristad and Yiani-
los, 1998; Bilenko and Mooney, 2003; McCallum
et al, 2005; Klementiev and Roth, 2006).
One particularly successful approach is by
Bergsma and Kondrak (2007), who use discrim-
inative learning with an improved feature repre-
sentation. The features are substring pairs that are
consistent with a character-level alignment of the
two strings. This approach strongly improved per-
formance on cognate identification, while varia-
tions of it have also proven successful in transliter-
ation discovery (Goldwasser and Roth, 2008). We
therefore adopted this approach for the translitera-
tion mining task.
We produce negative training examples using
the LCSR threshold approach described in Sec-
tion 3.2. For features, we extract from the aligned
word pairs all substring pairs up to a maximum
length of three. We also append characters mark-
ing the beginning and end of words, as described
in Bergsma and Kondrak (2007). For our clas-
sifier, we use a Support Vector Machine (SVM)
training with the very efficient LIBLINEAR pack-
age (Fan et al, 2008). We optimize the SVM?s
regularization parameter using 10-fold cross vali-
dation on the generated training data. At test time,
we apply our classifier to all the transliteration
candidates extracted from the Wikipedia titles,
generating transliteration pairs whenever there is
a positive classification.
3.3.4 String kernel classifier
The alignment-based classifier described in the
preceding section is limited to using substring fea-
tures that are up to (roughly) three or four letters
in length, due to the combinatorial explosion in the
number of unique features as the substring length
increases. It is natural to ask whether longer sub-
strings can be utilized to learn a more accurate pre-
dictor.
This question inspired the development of a sec-
ond SVM-based learner that uses a string kernel,
and therefore does not have to explicitly repre-
sent feature vectors. Our kernel is a standard n-
gram (or spectrum) kernel that implicitly embeds
a string in a feature space that has one co-ordinate
for each unique n-gram (see, e.g., (Shawe-Taylor
and Cristianini, 2004)). Let us denote the alphabet
over input strings as A. Given two input strings x
and x?, this kernel function computes:
k(x, x?) =
?
s?An
#(s, x)#(s, x?)
where s is an n-gram and #(a, b) counts the num-
ber of times a appears as a substring of b.
An extension of the n-gram kernel that we em-
ploy here is to consider all n-grams of length
1 ? n ? k, and weight each n-gram as a func-
tion of its length. In particular, we specify a value
? and weight each n-gram by a factor of ?n. We
implemented this kernel in the LIBSVM software
package (Chang and Lin, 2001). Optimal values
for k, ?, and the SVM?s regularization parame-
ter were estimated for each dataset using 5-fold
cross-validation. The values of (k, ?) that we ul-
timately used were: EnAr (3, 0.8), EnHi (8, 0.8),
EnRu (5, 1.2), and EnTa (5, 1.0).
Our input string representation for a candidate
pair is formed by first aligning the source and tar-
get words using M2M-aligner (Jiampojamarn et
al., 2007). Specifically, an alignment model is
trained on the seed examples, which are subse-
quently aligned and used as positive training ex-
amples. We then generate 20K negative examples
by random sampling (cf. Section 3.2) and apply
the alignment model to this set. Not all of these
20K word pairs will necessarily be aligned; we
randomly select 10K of the successfully aligned
pairs to use as negative examples in the training
set.
Each aligned pair is converted into an ?align-
ment string? by placing the letters that appear in
44
Word pair zubtsov z u b ov
Aligned pair z|u|b|t|s|o|v| z|u|b|| |o|v|
Align?t string zz|uu|bb|t|s |oo|vv
Table 2: An example showing how an alignment
string (the input representation for the string ker-
nel) is created from a word pair.
the same position in the source and target next to
one another, while retaining the separator charac-
ters (see Table 2). We also appended beginning
and end of word markers. Note that no romaniza-
tion of the target words is necessary for this pro-
cedure.
At test time, we apply the alignment model to
the candidate word pairs that have been extracted
from the train data, and retain all the successfully
aligned pairs. Here, M2M-aligner also acts as a
filter, since we cannot form alignment strings from
unaligned pairs ? these yield negative predictions
by default. We also filter out pairs that met any of
the following conditions: 1) the English word con-
sists of all all capital or lowercase letters, 2) the
target word can be converted to ASCII (cf. Sec-
tion 3.1), or 3) either word contains a digit.
3.3.5 Generation-based approach
In the mining tasks, we are interested in whether a
candidate pair (x, y) is a transliteration pair. One
approach is to determine if the generated translit-
erations of a source word y? = ?(x) and a target
word x? = ?(y) are similar to the given candi-
date pair. We applied DIRECTL+ to the mining
tasks by training transliteration generation models
on the provided seed data in forward and back-
ward transliteration directions, creating ?(x) and
?(y) models. We now define a transliteration
score function in Eq. 1. N(x?, x) is the normal-
ized edit distance between string x? and x, and w1
and w2 are combination weights to favor forward
and backward transliteration models.
S(x, y) = w1 ? N(y?, y) + w2 ? N(x?, x)w1 + w2
(1)
A candidate pair is considered a transliteration
pair if its S(x, y) > ? . Ideally, we would like
to optimize these parameters, ?, w1, w2 based on
a development set for each language pair. Unfor-
tunately, no development sets were provided for
the Shared Task. Therefore, following Bergsma
and Kondrak (2007), we adopt the threshold of
? = 0.58. We experimented with three sets of val-
ues for w1 and w2: (1, 0), (0.5, 0.5), and (0, 1).
Our final predictions were made using w0 = 0
and w1 = 1, which appeared to produce the best
results. Thus, only the backward transliteration
model was ultimately employed.
3.4 English-Chinese string matching
Due to the fact that names transliterated into Chi-
nese consist of multiple Chinese characters and
that the Chinese text provided in this shared task
is not segmented, we have to adopt a different ap-
proach to the English-Chinese mining task (Unlike
many other languages, there are no clear bound-
aries between Chinese words). We first train a
generation model using the seed data and then ap-
ply a greedy string matching algorithm to extract
transliteration pairs.
The generation model is built using the discrim-
inative training framework described in (Jiampoja-
marn et al, 2008). Two models are learned: one
is trained using English and Chinese characters,
while the other is trained on English and Pinyin (a
standard phonetic representation of Chinese char-
acters). In order to mine transliteration pairs from
Wikipedia titles, we first use the generation model
to produce transliterations for each English token
on the source side as both Chinese characters and
Pinyin. The generated Chinese characters are ul-
timately converted to Pinyin during string match-
ing. We also convert all the Chinese characters on
the target side to their Pinyin representations when
performing string matching.
The transliteration pairs are then mined by com-
bining two different strategies. First of all, we ob-
serve that most of the titles that contain a separa-
tion symbol ? ? ? on the target side are translit-
erations. In this case, the number of tokens on
both sides is often equal. Therefore, the mining
task can be formulated as a matching problem.
We use a competitive linking approach (Melamed,
2000) to find the best match. First, we select
links between all possible pairs if similarity of
strings on both sides is above a threshold (0.6 ?
length(Pinyin)). We then greedily extract the
pairs with highest similarity until the number of
unextracted segments on either side becomes zero.
The problem becomes harder when there is no
indication of word segmentation for Chinese. In-
stead of trying to segment the Chinese characters
first, we use an incremental string matching strat-
45
egy. For each token on the source side, the algo-
rithm calculates its similarity with all possible n-
grams (2 ? n ? L) on the target side, where L
is the length of the Chinese title (i.e., the number
of characters). If the similarity score of n-gram
with the highest similarity surpasses a threshold
(0.5 ? length(Pinyin)), the n-gram sequence is
proposed as a possible transliteration for the cur-
rent source token.
3.5 Resolving overlapping predictions
Given a set of candidate word pairs that have been
extracted from a given Wikipedia title according to
the procedure described in Section 3.1, our clas-
sifiers predict a class label for each pair inde-
pendently of the others. Pairs that receive neg-
ative predictions are discarded immediately and
are never reported as mined pairs. However, it
is sometimes necessary to arbitrate between pos-
itive predictions, since it is possible for a classifier
to mark as transliterations two or more pairs that
involve the same English word or the same target
word in the title. Clearly, mining multiple overlap-
ping pairs will lower the system?s precision, since
there is (presumably) at most one correct translit-
eration in the target language version of the title
for each English word.3
Our solution is to apply a greedy algorithm that
sorts the word pair predictions for a given title
in descending order according to the scores that
were assigned by the classifier. We make one pass
through the sorted list and report a pair of words as
a mined pair unless the English word or the target
language word has already been reported (for this
particular title).4
3.6 Results
In the context of the NEWS 2010 Shared Task
on Transliteration Generation we tested our sys-
tem on all five data sets: from English to Rus-
sian (EnRu), Hindi (EnHi), Tamil (EnTa), Arabic
(EnAr), and Chinese (EnCh). The EnCh set dif-
fers from the remaining sets in the lack of transpar-
ent word segmentation on the Chinese side. There
were no development sets provided for any of the
language pairs.
3On the other hand, mining all such pairs might improve
recall.
4A bug was later discovered in our implementation of this
algorithm, which had failed to add the words in a title?s first
mined pair to the ?already reported? list. This sometimes
caused up to two additional mined pairs per title to be re-
ported in the prediction files that were submitted.
Task System F P R
EnRu NED+ .875 .880 .869
BK-2007 .778 .684 .902
StringKernel* .811 .746 .889
DIRECTL+ .786 .778 .795
EnHi NED+ .907 .875 .941
BK-2007 .882 .883 .880
StringKernel .924 .954 .895
DIRECTL+ .904 .945 .866
EnTa NED+ .791 .916 .696
BK-2007 .829 .808 .852
StringKernel .914 .923 .906
DIRECTL+ .801 .919 .710
EnAr NED+ .800 .818 .783
BK-2007 .816 .834 .798
StringKernel* .827 .917 .753
DIRECTL+ .742 .861 .652
EnCh GreedyMatch .530 .698 .427
DIRECTL+ .009 .045 .005
Table 3: Transliteration mining results. An aster-
isk (*) indicates an unofficial result.
Table 3 shows the results obtained by our var-
ious systems on the final test sets, measured in
terms of F-score (F), precision (P), and recall
(R). The systems referred to as NED+, BK-2007,
StringKernel, DIRECTL+, and GreedyMatch are
described in Section 3.3.2, Section 3.3.3, Sec-
tion 3.3.4, Section 3.3.5, and Section 3.4 respec-
tively. The runs marked with an asterisk (*)
were produced after the Shared Task deadline, and
therefore are not included in the official results.
3.7 Discussion
No fixed ranking of the four approaches emerges
across the four alphabetic language pairs (all ex-
cept EnCh). However, StringKernel appears to be
the most robust, achieving the highest F-score on
three language pairs. This suggests that longer
substring features are indeed useful for classifying
candidate transliteration pairs. The simple NED+
method is a clear winner on EnRu, and obtains de-
cent scores on the remaining alphabetic language
pairs. The generation-based DIRECTL+ approach
ranks no higher than third on any language pair,
and it fails spectacularly on EnCh because of the
word segmentation ambiguity.
Finally, we observe that there are a number of
cases where the results for our discriminatively
trained classifiers, BK-2007 and StringKernel, are
46
not significantly better than those of the simple
NED+ approach. We conjecture that automatically
generating training examples is suboptimal for this
task. A more effective strategy may be to filter all
possible word pairs in the seed data to only those
with NED above a fixed threshold. We would then
apply the same threshold to the Wikipedia candi-
dates, only passing to the classifier those pairs that
surpass the threshold. This would enable a better
match between the training and test operation of
the system.
4 Conclusion
The results obtained in the context of the NEWS
2010 Machine Transliteration Shared Task con-
firm the effectiveness of our discriminative ap-
proaches to transliteration generation and mining.
Acknowledgments
This research was supported by the Alberta Inge-
nuity Fund, Informatics Circle of Research Excel-
lence (iCORE), and the Natural Sciences and En-
gineering Research Council of Canada (NSERC).
References
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proc. ACL.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
NAACL-HLT.
Mikhail Bilenko and Raymond J. Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proc. KDD.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. JMLR,
9:1871?1874.
Dan Goldwasser and Dan Roth. 2008. Transliteration
as constrained optimization. In Proc. EMNLP.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In NEWS ?09: Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration, pages 28?31.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010. Integrating joint n-gram features
into a discriminative training framework. In Proc.
NAACL-HLT.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In Proc. HLT-NAACL.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288?295.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proc. UAI.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Trans. Pattern Analy-
sis and Machine Intelligence, 20(5).
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
47
