Machine Learning Approach To Augmenting News Headline 
 Generation 
Ruichao Wang 
Dept. of Computer Science 
University College Dublin 
Ireland 
rachel@ucd.ie 
John Dunnion 
Dept. of Computer   Science 
University College Dublin 
Ireland 
John.Dunnion@ucd.ie 
Joe Carthy 
Dept. of Computer Science 
University College Dublin 
Ireland 
Joe.Carthy@ucd.ie 
 
 
Abstract 
In this paper, we present the 
HybridTrim system which uses a 
machine learning technique to combine 
linguistic, statistical and positional 
information to identify topic labels for 
headlines in a text. We compare our 
system with the Topiary system which, 
in contrast, uses a statistical learning 
approach to finding topic descriptors 
for headlines. The Topiary system, 
developed at the University of 
Maryland with BBN, was the top 
performing headline generation system 
at DUC 2004. Topiary-style headlines 
consist of a number of general topic 
labels followed by a compressed 
version of the lead sentence of a news 
story. The Topiary system uses a 
statistical learning approach to finding 
topic labels. The performance of these 
systems is evaluated using the ROUGE 
evaluation suite on the DUC 2004 news 
stories collection. 
1 Introduction 
In this paper we present an approach to headline 
generation for a single document. This headline 
generation task was added to the annual 
summ5arisation evaluation in the Document 
Understanding Conference (DUC) 2003. It was 
also included in the DUC 2004 evaluation plan 
where summary quality was automatically 
judged using a set of n-gram word overlap 
metrics called ROUGE (Lin and Hovy, 2003).  
Eighteen research groups participated in the 
headline generation task at DUC 2004, i.e. Task 
1: very short summary generation. The Topiary 
system was the top performing headline system 
at DUC 2004. It generated headlines by 
combining a set of topic descriptors with a 
compressed version of the lead sentence, e.g. 
KURDISH TURKISH SYRIA: Turkey sent 
10,000 troops to southeastern border. These 
topic descriptors were automatically identified 
using a statistical approach called Unsupervised 
Topic Discovery (UTD) (Zajic et al, 2004). The 
disadvantage of this technique is that meaningful 
topic descriptors will only be identified if this 
technique is trained on the corpus containing the 
news stories that are to be summarised. In 
addition, the corpus must contain clusters of 
related news stories to ensure that reliable 
cooccurrence statistics are generated.  
In this paper we compare the UTD method 
with an alternative topic label identifier that can 
be trained on an auxiliary news corpus, and 
observe the effect of these labels on summary 
quality when combined with compressed lead 
sentences. Our topic labeling technique works 
by combining linguistic and statistical 
information about terms using the C5.0 
(Quinlan, 1998) machine learning algorithm, to 
predict which words in the source text should be 
included in the resultant gist with the 
compressed lead sentence. In this paper, we 
compare the performance of this system, 
HybridTrim, with the Topiary system and a 
number of other baseline gisting systems on a 
collection of news documents from the DUC 
2004 corpus (DUC, 2003). 
155
2 Topiary System 
In this section, we describe the Topiary system 
developed at the University of Maryland with 
BBN Technologies. As already stated, this 
system was the top performing headline 
generation system at DUC 2004. A Topiary-
style headline consists of a set of topic labels 
followed by a compressed version of the lead 
sentence. Hence, the Topiary system views 
headline generation as a two-step process: first, 
create a compressed version of the lead sentence 
of the source text, and second, find a set of topic 
descriptors that adequately describe the general 
topic of the news story. We will now look at 
each of these steps in more detail. 
Dorr et al (2003) stated that when human 
subjects were asked to write titles by selecting 
words in order of occurrence in the source text, 
86.8% of these headline words occurred in the 
first sentence of the news story. Based on this 
result Dorr, Zajic and Schwartz, concluded that 
compressing the lead sentence was sufficient 
when generating titles for news stories. 
Consequently, their DUC 2003 system 
HedgeTrimmer used linguistically-motivated 
heuristics to remove constituents that could be 
eliminated from a parse tree representation of the 
lead sentence without affecting the factual 
correctness or grammaticality of the sentence. 
These linguistically-motivated trimming rules 
(Dorr et al, 2003; Zajic et al, 2004) iteratively 
remove constituents until a desired sentence 
compression rate is reached.  
The compression algorithm begins by 
removing determiners, time expressions and 
other low content words. More drastic 
compression rules are then applied to remove 
larger constituents of the parse tree until the 
required headline length is achieved. For the 
DUC 2004 headline generation task systems 
were required to produce headlines no longer 
than 75 bytes, i.e. about 10 words. The following 
worked example helps to illustrate the sentence 
compression process.1 
 
                                                          
1   The part of speech tags in the example are explained as 
follows: S represents a simple declarative clause; SBAR 
represents a clause introduced by a (possibly empty) 
subordinating conjunction; NP is a noun phrase; VP is a 
verb phrase; ADVP is an adverbial phrase. 
Lead Sentence: The U.S. space shuttle 
Discovery returned home this morning after 
astronauts successfully ended their 10-day 
Hubble Space telescope service mission. 
 
Parse: (S (NP (NP The U.S. space shuttle) 
Discovery) (VP returned (NP home) (NP this 
morning)) (SBAR after (S (NP astronauts) (VP 
(ADVP successfully) ended (NP their 10-day 
Hubble Space telescope service mission))))) 
 
1. Choose leftmost S of parse tree and 
remove all determiners, time expressions and 
low content units such as quantifiers (e.g. 
each, many, some), possessive pronouns (e.g. 
their, ours, hers) and deictics (e.g. this, tese, 
those):  
 
Before: (S (NP (NP The U.S. space shuttle) 
Discovery) (VP returned (NP home) (NP this 
morning)) (SBAR after (S (NP astronauts) (VP 
(ADVP successfully) ended (NP their 10-day 
Hubble Space telescope service mission))))) 
 
After: (S (NP (NP U.S. space shuttle) 
Discovery) (VP returned (NP home))  (SBAR 
after (S (NP astronauts) (VP (ADVP 
successfully) ended (NP 10-day Hubble Space 
telescope service mission))))) 
  
2. The next step iteratively removes 
constituents until the desired length is 
reached. In this instance the algorithm will 
remove the trailing SBAR.  
 
Before: (S (NP (NP U.S. space shuttle) 
Discovery) (VP returned (NP home))  (SBAR 
after (S (NP astronauts) (VP (ADVP 
successfully) ended (NP 10-day Hubble Space 
telescope service mission))))) 
 
After: U.S. space shuttle Discovery returned 
home. 
 
Like the ?trailing SBAR? rule, the other 
iterative rules identify and remove non-essential 
relative clauses and subordinate clauses from the 
lead sentence. A more detailed description of 
these rules can be found in Dorr et al (2003) and  
Zajic et al (2004) In this example, we can see 
that after compression the lead sentence reads 
156
more like a headline. The readability of the 
sentence in this case could be further improved 
by replacing the past tense verb ?returned? with 
its present tense form; however, this refinement 
is not currently implemented by the Topiary 
system or by our implementation of this 
compression algorithm.  
As stated earlier, a list of relevant topic 
words is also concatenated with this compressed 
sentence resulting in the final headline. The 
topic labels are generated by the UTD 
(Unsupervised Topic Discovery) algorithm 
(Zajic et al, 2004). This unsupervised 
information extraction algorithm creates a short 
list of useful topic labels by identifying 
commonly occurring words and phrases in the 
DUC corpus. So for each document in the 
corpus it identifies an initial set of important 
topic names for the document using a modified 
version of the tf.idf metric. Topic models are 
then created from these topic names using the 
OnTopic? software package. The list of topic 
labels associated with the topic models closest in 
content to the source document are then added to 
the beginning of the compressed lead sentence 
produced in the previous step, resulting in a 
Topiary-style summary.  
One of the problems with this approach is 
that it will only produce meaningful topic 
models and labels if they are generated from a 
corpus containing additional on-topic documents 
on the news story being summarised. In the next 
section, we explore two alternative techniques 
for identifying topic labels, where useful 
summary words are identified ?locally? by 
analysing the source document rather than 
?globally? using the entire DUC corpus, i.e. the 
UTD method. 
3 C5.0 
C5.0 (Quinlan, 1998) is a commercial machine 
learning program developed by RuleQuest 
Research and is the successor of the widely used 
ID3 (Quinlan, 1983) and C4.5 (Quinlan, 1993) 
algorithms developed by Ross Quinlan. C5.0 is a 
tool for detecting patterns that delineate 
categories. It subsequently generates decision 
trees based on these patterns. A decision tree is a 
classifier represented as a tree structure, where 
each node is either a leaf node, a classification 
that applies to all instances that reach the leaf 
(Witten, 2000), or a non-leaf node, some test is 
carried out on a single attribute-value, with one 
branch and sub-tree for each possible outcome 
of the test. A decision tree is a powerful and 
popular tool for classification and prediction and 
can be used to classify an instance by starting at 
the root of the tree and moving down the tree 
branch until reaching a leaf node. However, a 
decision tree may not be very easy to 
understand. An important feature of C5.0 is that 
it can convert trees into collections of rules 
called rulesets. C5.0 rulesets consist of 
unordered collections of simple if-then rules. It 
is easy to read a set of rules directly from a 
decision tree.  One rule is generated for each 
leaf. The antecedent of the rule includes a 
condition for every node on the path from the 
root to that leaf, and the consequent of the rule is 
the class assigned by the leaf. This process 
produces rules that are unambiguous in that the 
order in which they are executed is irrelevant 
(Witten, 2000). 
C5.0 has been used for text classification in a 
number of research projects. For example, 
Akhtar et al (2001) used C5.0 for automatically 
marking up XML documents, Newman et al 
(2005) used it for generating multi-document 
summary, while Zhang et al (2004) applied this 
approach to World Wide Web site 
summarisation. 
4 HybridTrim System 
The HybridTrim system uses our 
implementation of the Hedge Trimmer algorithm 
and the C5.0 (Quinlan, 1998) machine learning 
algorithm to create a decision tree capable of 
predicting which words in the source text should 
be included in the resultant gist.  
To identify pertinent topic labels the 
algorithm follows a two-step process: the first 
step involves creating an intermediate 
representation of a source text, and the second 
involves transforming this representation into a 
summary text.  The intermediate representation 
we have chosen is a set of features, that we feel 
are good indicators of possible ?summary 
words?. We focus our efforts on the content 
words of a document, i.e. the nouns, verbs and 
adjectives that occur within the document.  For 
each occurrence of a term in a document, we 
calculate several features: the tf, or term 
157
frequency of the word in the document; the idf, 
or inverse document frequency of the term taken 
from an auxiliary corpus (TDT, 2004); and the 
relative position of a word with respect to the 
start of the document in terms of word distance. 
We also include binary features indicating 
whether a word is a noun, verb or adjective and 
whether it occurs in a noun or proper noun 
phrase.  The final feature is a lexical cohesion 
score calculated with the aid of a linguistic 
technique called lexical chaining.  Lexical 
chaining is a method of clustering words in a 
document that are semantically similar with the 
aid of a thesaurus, in our case WordNet.  Our 
chaining method identifies the following word 
relationship (in order of strength):  repetition, 
synonymy, specialisation and generalisation, and 
part/whole relationships.  Once all lexical chains 
have been created for a text then a score is 
assigned to each chained word based on the 
strength of the chain in which it occurs.  More 
specifically, as shown in Equation (1), the chain 
strength score is the sum of each strength score 
assigned to each word pair in the chain.  
where repsi is the frequency of word i in the 
text, and rel(i,j) is a score assigned based on the 
strength of the relationship between word i and j.  
More information on the chaining process and 
cohesion score can be found in Doran et al 
(2004a) and Stokes (2004).  
Using the DUC 2003 corpus as the training 
data for our classifier, we then assigned each 
word a set of values for each of these features, 
which are then used with a set of gold standard 
human-generated summaries to train a decision 
tree summarisation model using the C5.0 
machine learning algorithm. The DUC 2003 
evaluation provides four human summaries for 
each document, where words in the source text 
that occur in these model summaries are 
considered to be positive training examples, 
while document words that do not occur in these 
summaries are considered to be negative 
examples. Further use is made of these four 
summaries, where the model is trained to 
classify a word based on its summarisation 
potential. More specifically, the appropriateness 
of a word as a summary term is determined 
based on the class assigned to it by the decision 
tree. These classes are ordered from strongest to 
weakest as follows: ?occurs in 4 summaries?, 
?occurs in 3 summaries?, ?occurs in 2 
summaries?, ?occurs in 1 summary?, ?occurs in 
none of the summaries?. If the classifier predicts 
that a word will occur in all four of the human 
generated summaries, then it is considered to be 
a more appropriate summary word than a word 
predicted to occur in only three of the model 
summaries. This resulted in a total of 103267 
training cases, where 5762 instances occurred in 
one summary, 1791 in two, 1111 in three, 726 in 
four, and finally 93877 instances were negative. 
A decision tree classifier was then produced by 
the C5.0 algorithm based on this training data.  
To gauge the accuracy of our decision tree 
topic label classifier, we used a training/test data 
split of 90%/10%, and found that on this test set 
the classifier had a precision (true positives 
divided by true positives and false positives) of 
63% and recall (true positives divided by true 
positives and false negatives) of 20%. 
5 Evaluation and Results 
In this section we present the results of our 
headline generation experiments on the DUC 
2004 corpus. 2  We use the ROUGE (Recall-
Oriented Understudy for Gisting Evaluation) 
metrics to evaluate the quality of our 
automatically generated headlines. In DUC 2004 
task 1, participants were asked to generate very 
short (<=75 bytes) single-document summaries 
for documents on TDT-defined events.  
The DUC 2004 corpus consists of 500 
Associated Press and New York Times 
newswire documents. The headline-style 
summaries created by each system were 
evaluated against a set of human generated (or 
model) summaries using the ROUGE metrics. 
The format of the evaluation was based on six 
scoring metrics: ROUGE-1, ROUGE-2, 
ROUGE-3, ROUGE-4, ROUGE-LCS and 
ROUGE-W. The first four metrics are based on 
the average n-gram match between a set of 
model summaries and the system-generated 
summary for each document in the corpus. 
ROUGE-LCS calculated the longest common 
                                                          
2 Details of our official DUC 2004 headline generation 
system can be found in Doran et al (2004b). This system 
returned a list of keywords rather than ?a sentence + 
keywords? as a headline. It used a decision tree classifier to 
identify appropriate summary terms in the news story based 
on a number of linguistic and statistical word features. 
? += )),(*)(()( jirelrepsrepschainScore ji (1)
158
sub-string between the system summaries and 
the models, and ROUGE-W is a weighted 
version of the LCS measure. So for all ROUGE 
metrics, the higher the ROUGE value the better 
the performance of the summarisation system, 
since high ROUGE scores indicate greater 
overlap between the system summaries and their 
respective models. Lin and Hovy (2003) have 
shown that these metrics correlated well with 
human judgments of summary quality, and the 
summarisation community is now accepting 
these metrics as a credible and less time-
consuming alternative to manual summary 
evaluation. In the official DUC 2004 evaluation 
all summary words were stemmed before the 
ROUGE metrics were calculated; however, 
stopwords were not removed. No manual 
evaluation of headlines was performed. 
5.1 ROUGE Evaluation Results 
Table 1 shows the results of our headline 
generation experiments on the DUC 2004 
collection. Seven systems in total took part in 
this evaluation, three Topiary-style headline 
generation systems and four baselines: the goal 
of our experiments was to evaluate 
linguistically-motivated heuristic approaches to 
title generation, and establish which of our 
alternative techniques for padding Topiary-style 
headlines with topic labels works best.  
Since the DUC 2004 evaluation, Lin (2004) 
has concluded that certain ROUGE metrics 
correlate better with human judgments than 
others, depending on the summarisation task 
being evaluated, i.e. single document, headline, 
or multi-document summarisation. In the case of 
headline generation, Lin found that ROUGE-1, 
ROUGE-L and ROUGE-W scores worked best 
and so only these scores are included in Table 1.  
 
Table 1. ROUGE scores for headline generation 
systems 
As the results show the best performing topic 
labeling techniques are the TF and Hybrid 
systems. TF system is a baseline system that 
chooses high frequency content words as topic 
descriptors. Hybrid system is our decision tree 
classifier described in the previous section.  
Both of these systems outperform the 
Topiary's UTD method. The top three 
performing systems in this table combine topic 
labels with a compressed version of the lead 
sentence. Comparing these results to the Trim 
system (that returns the reduced lead sentence 
only), it is clear that the addition of topic 
descriptors greatly improves summary quality. 
The performance of the baseline TFTrim system 
and the HybridTrim system are very similar for 
all Rouge metrics; however, both systems 
outperform the Topiary headline generator. 
6 Conclusions and Future work 
The results of our experiment have shown the 
TFTrim system (the simplest of the three 
Topiary-style headline generators examined in 
this paper) is the most appropriate headline 
approach because it yields high quality short 
summaries and, unlike the Topiary and 
HybridTrim systems, it requires no prior 
training. This is an interesting result as it shows 
that a simple tf weighting scheme can produce as 
good, if not better, topic descriptors than the 
statistical UTD method employed by the 
University of Maryland and our own 
statistical/linguistic approach to topic label 
identification.      
  In future work, we intend to proceed by 
improving the sentence compression procedure 
described in this paper. We are currently 
working on the use of term frequency 
information as a means of improving the 
performance of the Hedge Trimmer algorithm by 
limiting the elimination of important parse tree 
components during sentence compression. 
References 
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge 
Trimmer: A Parse-and-Trim Approach to 
Headline Generation. In the Proceedings of the 
Document Understanding Conference (DUC). 
C-Y Lin and E. Hovy. 2003. Automatic Evaluation of 
Summaries using n-gram Co-occurrence Statistics, 
Proceedings of HLT/NACCL.  
C-Y Lin. 2004. ROUGE: A Package for Automatic 
Evaluation of Summaries, In the Proceedings of 
 Systems R-1 R-L R-W 
TFTrim 0.279 0.213 0.126 
HybridTrim 0.274 0.214 0.127 
Combinat
ion 
Systems Topiary 0.249 0.20 0.119 
TF 0.244 0.171 0.098 
Hybrid 0.219 0.176 0.102 
Trim 0.201 0.183 0.101 
Baseline 
Systems 
UTD 0.159 0.130 0.078 
159
the ACL workshop, Text Summarization Branches 
Out, Barcelona, Spain, pp. 56-60. 
DUC. 2003. http://www-nlpir.nist.gov/projects/duc/, 
Accessed March 2005. 
D. Zajic and B. Dorr and R. Schwartz. 2004.  
BBN/UMD at DUC-2004: Topiary, Proceedings of 
the Document Understanding Conference (DUC). 
I. H. Witten and E. Frank. 2000. Data Mining, 
Practical Machine Learning Tools and Techniques 
with Java Implementations. Morgan Kaufann 
Publishers.  ISBN 1-55860-552-5. 
N. Stokes. 2004. Applications of Lexical Cohesion 
Analysis in the Topic Detection and Tracking 
domain.  Ph.D.  Thesis, Dept. of Computer 
Science, University College Dublin. 
R. Quinlan. 1983. Learning efficient classification 
procedures and their application to chess end 
games, in Machine Learning. An Artificial 
Intelligence Approach edited by R.S. Michalski, 
J.G. Carbonell and T.M. Mitchell, Tioga, Palo 
Alto, CA, 1983, pp.463-482. 
R. Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann Publishers, San 
Mateo, California ISBN 1-55860-238-0.  
R. Quinlan. 1998. C5.0: An Informal Tutorial, 
http://www.rulequest.com/see5-unix.html, 
Accessed March 2005. 
S. Akhtar, J. Dunnion and R. Reilly. 2001. 
Automating XML mark-up, Joint International 
Conference of the Association for Computers and 
the Humanities (ACH) and the Association for 
Literary and Linguistic Computing (ALLC), New 
York. 
TDT Pilot Study Corpus. 2004.  
http://www.nist.gov/speech/tests/tdt. 
W. Doran, N. Stokes, J. Dunnion, and J. Carthy. 
2004a. Assessing the Impact of Lexical Chain 
Scoring Methods and Sentence Extraction 
Schemes on Summarization. In the Proceedings of 
CICLing, Seoul. 
W. Doran, N. Stokes, E. Newman, J. Dunnion, J. 
Carthy, and F. Toolan. 2004b. News Story Gisting 
at University College Dublin. In the Proceedings 
of the Document Understanding Conference 
(DUC). 
Y. Zhang, N. Zincir-Heywood, E. Milios. 2004.  
World Wide Web Site Summaisation. To Appear in 
Web Intelligence and Agent Systems: An 
International Journal (The Web intelligence 
Consortium), 2(1), pages 39-53. 
 
 
 
 
 
160
Representing semantics of texts - a non-statistical approach
Svetlana Hensman
School of Computing
Dublin Institute of Technology
Kevin Street
Dublin 8, Ireland
Svetlana.Hensman@comp.dit.ie
John Dunnion
Intelligent Information Retrieval Group
Department of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
John.Dunnion@ucd.ie
Abstract
This paper describes a non-statistical
approach for semantic annotation of
documents by analysing their syntax
and by using semantic/syntactic behav-
iour patterns described in VerbNet. We
use a two-stage approach, firstly iden-
tifying the semantic roles in a sen-
tence, and then using these roles to rep-
resent some of the relations between
the concepts in the sentence and a list
of noun behaviour patterns to resolve
some of the unknown (generic) rela-
tions between concepts. All outlined
algorithms were tested on two corpora
which differs in size, type, style and
genre, and the performance does not
vary significantly.
1 Introduction
This paper describes a system for semi-automatic
conceptual graph acquisition using a combina-
tion of linguistic resources, such as VerbNet and
WordNet, together with semi-automatically com-
piled domain-specific knowledge. Such seman-
tic information has a number of possible applica-
tions, for example in the area of information re-
trieval/extraction for enhancing the search meth-
ods or in question-answering systems, allowing
users to communicate with the system in natural
language (English).
We use conceptual graphs (CGs) (Sowa, 1984),
a knowledge-representation formalism based on
semantic networks and the existential graphs of
C. S. Pierce, to represent the semantics of doc-
uments. There are number of systems for gen-
erating conceptual graphs representation of sen-
tences: Sowa and Way (Sowa and Way, 1986)
use a lexicon of canonical graphs which are com-
bined to build a conceptual graph representation
of a sentence, while Veraldi at al. (Velardi et al,
1988) describe a prototype of a semantic proces-
sor for Italian sentences, which uses a manually
acquired lexicon of about 850 word-sense defin-
itions, each including 10 ? 20 surface semantic
patterns (SSPs) representing both usage informa-
tion and semantic constraints.
There are also systems aimed at extracting par-
tial knowledge from texts, by either filling seman-
tic templates (Hobbs et al, 1996) or by generating
a set of linguistic patterns for information extrac-
tion (Harabagiu and Maiorano, 2000), to name
but a few.
The following sections describe in more detail
the various aspects of our system, the experiments
that we carried out to test the proposed algorithms
and finally draw some conclusions.
2 System overview
We use a two-step approach for constructing con-
ceptual graph representations of texts: firstly,
by using VerbNet and WordNet, we identify
the semantic roles in a sentence, and secondly,
using these semantic roles and a set of syn-
tactic/semantic rules we construct a conceptual
graph.
To evaluate our algorithms we use test docu-
ments from two corpora in different domains ?
the Reuters-21578 text categorization test collec-
209
tion (Reuters, 1987) and the collection of aviation
incident reports provided by the Irish Air Acci-
dent Investigation Unit (AAIU) (Air Accident In-
vestigation Unit, 2004). All documents are parsed
using Eugene Charniak?s maximum entropy in-
spired parser (Charniak, 2000).
3 Semantic role identification
There are number of different existing ap-
proaches for identifying semantic roles, varying
from traditional parsing approaches, for exam-
ple using HPSG grammars and Lexical Func-
tional Grammars, that strongly rely on manually-
developed grammars and lexicons, to data-driven
approaches, for example AutoSlog (Riloff and
Schmelzenbach, 1998). In the domain of the Air
Traveler Information System (Miller et al, 1996)
the authors apply statistical methods to compute
the probability that a constituent can fill in a se-
mantic slot within a semantic frame. Gildea and
Jurafsky (Gildea and Jurafsky, 2002) describe a
statistical approach for semantic role labelling us-
ing data collected from FrameNet by analysing a
number of features such as phrase type, grammat-
ical function, position in the sentence, etc.
Shi and Mihalcea (Shi and Mihalcea, 2004)
propose a rule-based approach for semantic pars-
ing using FrameNet and WordNet. They extract
rules from the tagged data provided by FrameNet,
which specify the realisation (order and different
syntactic features) for the present semantic roles.
They also create a feature set representation of
the sentence and match it to each of the extracted
rules. The result is the rule providing the most
feature matches. The authors do not provide any
information on how they select between different
matches with the same score, or if there is any
semantic check on suitability of a phrase to re-
alise a semantic role (FrameNet does not provide
any restrictions on the semantic roles similar to
the selectional restrictions present in VerbNet).
The approach we propose for semantic role
identification uses information about each verb?s
behaviour, provided in VerbNet, and the Word-
Net taxonomy to decide whether a phrase can be
a suitable match for a semantic role.
VerbNet (Kipper et al, 2000) is a computa-
tional verb lexicon, based on Levin?s verb classes,
that contains syntactic and semantic information
for English verbs. Each VerbNet class defines a
list of members, a list of possible thematic roles,
and a list of frames (patterns) of how these se-
mantic roles can be realized in a sentence.
WordNet (Fellbaum, 1998) is an English lex-
ical database containing about 120 000 entries
of nouns, verbs, adjectives and adverbs, hier-
archically organized in synonym groups (called
synsets), and linked with relations such as hyper-
nym, hyponym, holonym and others.
To identify the semantic roles for a clause in a
sentence we identify and match the clause pattern
to each of the possible semantic frames for the
clause verb (from VerbNet). The result is a list
of all possible semantic role assignments, from
which we must identify the correct one.
3.1 Constructing sentence patterns for the
verbs in a sentence
For each sentence clause we construct a syntac-
tical pattern, which is a flat parse representation
that identifies the main verb and the other main
categories of the clause. As a sentence can have
subordinate clauses, we usually have more than
one syntactic pattern per sentence. Each such pat-
tern is processed individually.
Using a constituency parser (such as Char-
niak?s) is suitable in the majority of cases, but
there are some sentences where the correct set of
role fillers cannot be identified by using the parse
tree. For example, for sentences such as
The price of oil will rise by 5 cents by
the end of the year.
the phrase the price of oil will be identified as
a possible role filler by our system, while the cor-
rect result would have the price identified as the
Attribute and oil as the Patient. For such cases the
use of a dependency parser (such as a Link Gram-
mar parser or a Functional Dependency Grammar
parser) would be required.
We also address some simple cases of pronoun
anaphoric reference. For example, for patterns
such as
Iomega Corp said it has laid off over a
quarter of its professional and manage-
ment staff.
210
we identify the pronoun it as referring to the
subject of the verb in the main clause (which here
is Iomega Corp) if they agree by gender and num-
ber. In cases where the type of the concept repre-
sented by the phrase is known, an agreement by
type is also required.
Some cases of intersentential pronoun
anaphoric references are also resolved by
analysing the previous sentence context for
suitable candidates, that agree by gender, number
and type. Agreement by type is present if the
type of the phrase is compatible (or the same) as
the type of the phrase it references. For example,
if the company refers to Iomega Corp, which is
listed as an instance of the type organization, then
the types of the two phrases are compatible, as
company is defined as sub-type of organization.
If agreement by type cannot be assured, the
reference is not resolved. The reference is only
resolved if there is a single possibility for its
resolution.
3.2 Extracting VerbNet semantic role frames
Each verb can be described in VerbNet as a mem-
ber of more than one class, and therefore the list
of its possible semantic frames is a combination
of the semantic frames defined in each of the
classes in which it participates.
We extract all the semantic frames in a class
and consider them to be possible semantic frames
for each of the verbs that are members of this
class. Each verb class also defines a list of se-
lectional constraints for the semantic roles. For
example, for all the verbs that are members of the
VerbNet class get-13.5.1 one of the possible se-
mantic role frames is:
Agent[+animate OR +organization] V Theme
Prep(from) Source[+concrete].
The selectional constraints check is imple-
mented using one or a combination of the fol-
lowing techniques: hypernym relations defined in
WordNet, pattern matching techniques, syntactic
rules and some heuristics.
3.3 Matching algorithm
The matching algorithm matches the sentence
pattern against each of the possible semantic role
frames extracted from VerbNet. We match the
constituents before and after the verb in the sen-
tence pattern to the semantic roles before and after
the verb in the semantic role frame.
If the number of the available constituents in
the sentence pattern is less than the number of
the required slots in the frame, the match fails.
If there is more than one constituent available to
fill a slot in a semantic frame, each of them is
considered a different match. If, for a seman-
tic frame, we find a constituent for each of the
semantic role slots that complies with the selec-
tional constraints, the algorithm considers this a
possible match.
Multiple results are identified when there are
two or more phrases in a sentence that are possi-
ble semantic role realisations, or if there are two
or more semantic frames for which matches were
found. To select the correct role assignment we
use a weighting function that assigns scores to
each result and returns the one with the highest
score. For each identified role the weighting func-
tion adds one point if the role does not have any
selectional restrictions, and two points if there are
restrictions (including prepositional restrictions).
The total score for a solution is the sum of the
scores for each identified roles. The solution with
the highest score is selected.
For example, for the sentence
USAir bought Piedmont for 69 dlrs
cash per share.
the algorithm identifies two possible role as-
signments:
Agent[+animate OR +organization]
matching NP(The company)
Theme[] matching NP(the shares)
Asset[+currency] matching PP(for 69
dlrs cash per share)
with weightframe1 = 2 + 1 + 2 = 5 and the
second solution
Agent[+animate OR +organization]
matching NP(The company)
Theme[] matching the NP(the shares)
with weightframe5 = 2 + 1 = 3
Therefore, the algorithm returns the first set of
role assignments as a result.
211
4 Building conceptual graphs
The conceptual graph representation of the sen-
tence is built through the following steps: firstly,
for each of the constituents of the sentence we re-
cursively build a conceptual graph representation;
then we link all the conceptual graphs represent-
ing the constituents into a single graph; and fi-
nally, we resolve the unknown (generic) relations.
Each of these steps is described in more detail in
the following sub-sections.
4.1 Building a conceptual graph
representation of a phrase
The first step involves building a conceptual graph
for a phrase. Our general assumption is that
each lexeme in the sentence is represented us-
ing a separate concept, therefore all nouns, adjec-
tives, adverbs and pronouns are represented using
concepts, while the determiners and numbers are
used to specify the referent of the relevant concept
(thus further specifying the concept).
Below we illustrate the procedure for building
a conceptual graph for some of the most common
types of phrases.
 NP -> DT JJ NN
For phrases following this pattern we create
two concepts - one for the NNwith a concept
referent corresponding to the type of the de-
terminer DT, and another concept represent-
ing the adjective, and link both of them by
an Attribute relation. If the phrase contains
more than one adjective, each of them is rep-
resented by a separate concept and they are
all linked with Attribute relations to the con-
cept representing the noun.
 NP -> NP , SBAR ,
This pattern represents phrases where the
noun is further specified by the SBAR (for
example, The co-pilot, who was acting as
a main pilot, landed the plane.) For these
patterns a conceptual graph is built for the
SBAR and the head concept, if a WHNP
phrase (e.g. which or who), is replaced by
the concept created for the NP.
 PP -> IN NP
For such prepositional phrases we construct
a conceptual graph representing the noun
phrase. We also keep track of the preposition
heading the prepositional phrase, as it is used
to mark the relation between this phrase and
the other relevant phrases in the sentence.
4.2 Attaching all constituents to the verb
Once the graphs for each of the constituents are
constructed they are linked together in a single
conceptual graph. As each of them describes
some aspect of the concept represented by the
verb, we link them to that concept.
If the constituent already has an identified se-
mantic role during the previous phase, the same
relation is used when constructing the concep-
tual graph between the CG representing the con-
stituent and the verb. If the constituent does not
have any semantic roles identified, a relation with
a generic label is used, which allows us to build
the structure of the CG concentrating on the con-
cepts involved, and to resolve the generic labels
at a later stage. The generic labels used are ei-
ther REL, or in the case of prepositional phrases
headed with a proposition prep, REL prep (e.g.
REL on).
4.3 Resolving unknown relations
Finally we resolve some of the unknown (generic)
relations in the conceptual graph. We keep a data-
base of the most common syntactic realisation of
relations between concepts with specific types.
An example of a relation correction rule is:
Flight REL from City -> Flight Source City
where the left part of the rule represents the two
concepts linked by a generic relation and the right
side represents the graph after the modification.
All generic relations present after this step must
be manually resolved by the user. The system of-
fers help by suggesting possible relations intro-
duced by a preposition. For example, the preposi-
tion for can indicate Beneficiary (e.g. a book for
Mary), Duration (e.g. for three hours), etc.
5 Query representation
Representation of questions differs than represen-
tation of declarative sentences and deserves spe-
cial attention. For sentences representing ques-
tions we try to identify the statement that will
212
correspond to the question and then construct the
conceptual graph in a similar way as for declara-
tive sentences.
5.1 Yes/No questions
We process simple yes/no questions (questions
that require a yes/no answer) that are constructed
by a subject-verb inversion by applying a trans-
formation to reverse the question to a declarative
sentence.
5.2 Wh question
The parse tree of a sentence expressing a
wh question has the following general struc-
ture: SBARQ ->WH phrase SQ ? where the
WH phrase is either WHNP, WHADVP or WHPP
and represents the concept that triggers the query.
The SQ represents the rest of the sentence.
Similarly to yes/no questions, these type of
questions are also transformed to declarative sen-
tences. The wh word (e.g. who, what, where,
when) is represented by a generic concept. The
relation that attaches this concept to the verb de-
pends on the type of the wh phrase and can be one
of the following:
WHNP
These phrases are headed by the wh ques-
tion words who, what or which. The rela-
tion between the wh phrase and the verb is
either identified from applying a suitable se-
mantic frame for this verb, or it is a generic
one, REL.
WHADVP
These phrases represent an adverbial modi-
fier for time, place or location. If the phrase
marked as WHADVP is where the relation is
locative; if it is when, the relation is tempo-
ral; and if it is how, the relation is manner.
WHPP
Such phrases are not processed by our sys-
tem.
6 Experimental results
Each module of the system was evaluated sepa-
rately.
The first experiment we carried out was to es-
timate the accuracy of the sentence frame con-
structed by the role labelling module and it was
performed on randomly selected 2% of the verbs
in Reuters and 7% of the verbs in AAIU corpora.
The parse trees produced by Charniak?s parser
were manually edited to avoid any errors due to
incorrect parses. The results showed that the sys-
tem identified the correct set of possible candi-
dates for semantic roles for 90% and 89% of the
verbs in the Reuters and in the AAIU documents
respectively.
Further experiments were carried out to eval-
uate the performance of the role assigning mod-
ule. As a testbed we randomly selected 2% of
the verbs in Reuters and 15% of the verbs in
the AAIU documents. From these, we analysed
only those cases where the verb is a member of
at least one VerbNet frame and the possible role
candidates were correctly identified. For 60% and
70% of the remaining verbs, respectively, the al-
gorithm identifies a single correct solution. In
3% and 4% of the cases respectively a partially
correct result is found (in the majority of such
cases it is Agent, Patient and Theme roles that are
correctly identified, together with some incorrect
ones).
In 11% and 9% of the cases for Reuters and
AAIU, respectively, the algorithm identifies a set
of possible solutions, containing the correct and
several incorrect ones. For these cases the weight-
ing function identifies the correct solution in 38%
of the of the cases for AAIU documents and 59%
of the cases for the Reuters documents, while in
40% and 21% of the cases, respectively, it identi-
fies the correct and one or more incorrect results.
We also evaluated the percentage of the syntac-
tic patterns that the graph builder recognises: for
AAUI and Reuters documents, respectively, we
can build a graph for 76% and 67% of the noun
phrases, for 95% and 94% of the prepositional
phrases and for 91% and 97% of the subordinate
clauses.
7 Conclusions
In this paper we have described an approach
for constructing conceptual graphs for English
sentences, using VerbNet, WordNet and some
domain-specific knowledge. The achieved accu-
213
racy is strongly influenced by the lack of VerbNet
descriptions of many verbs present in both cor-
pora, as well as the lack of semantic frames for
the present verb sense. Also, as the approach is
not statistical, it does not require large amount of
training data.
There are several other lexical resources cur-
rently available that seem suitable for semantic
role identification, among them FrameNet and
PropBank. Our choice of VerbNet as a lexical re-
source is based on our belief that a set of domain-
independent descriptive role labels (such as those
defined in VerbNet) is more suitable as it allows
for generalisations.
A drawback of both FrameNet and PropBank
is that the roles do not include any selectional
restrictions, which makes it hard for a non-
statistical method to identify the correct filler for
each role. As shown earlier, the selectional re-
strictions defined for the semantic roles prove to
be a valuable asset when deciding if a phrase can
be a role filler. While we can resolve the majority
of them by analysing the syntactic structure or by
using the WordNet hierarchy, some are more dif-
ficult to resolve. For example, the restriction solid
describes an attribute or a state of an object, rela-
tions which cannot be checked by using WordNet.
FrameNet on the other hand defines usages not
only for verbs, but also for nouns. As one of the
causes for the relatively poor performance of the
conceptual graph building module is the lack of a
sufficient number of relation-correction rules, our
current approach to increasing their number is try-
ing to extract the rules from FrameNet.
Work on the system is ongoing and efforts
are continuing to implement a verb sense disam-
biguation component.
References
Air Accident Investigation Unit. 2004. Irish Air Acci-
dent Investigation Unit Reports. Available online:
(http://www.aaiu.ie/).
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of NAACL-2000,
pages 132?139.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, May.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Sanda Harabagiu and Steven Maiorano. 2000. Ac-
quisition of linguistic patterns for knowledge-based
information extraction. In Proceedings of LREC-
2000, Athens, June.
Jerry Hobbs, Douglas Appelt, John Bear, David Is-
rael, Megumi Kameyama, Mark Stickel, and Mabry
Tyson. 1996. FASTUS: A cascaded finite-state
transducer for extracting information from natural-
language text. In In Finite State Devices for Natural
Language Processing, Cambridge, MA. MIT Press.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-Based Construction of a Verb Lexicon.
In Proceedings of Seventeenth National Conference
on Artificial Intelligence (AAAI-2000), pages 691 ?
696, Austin, TX, July 30 - August 3.
Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical ap-
proach to natural language interfaces. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 55?61, Santa
Cruz, June. Morgan Kaufmann Publishers, Inc.
Reuters. 1987. Reuters-21578 Text Cate-
gorization Collection. Available online:
(http://kdd.ics.uci.edu/databases/reuters21578/-
reuters21578.html).
Ellen Riloff and Mark Schmelzenbach. 1998. An Em-
pirical Approach to Conceptual Case Frame Acqui-
sition. In Proceedings of the Sixth Workshop on
Very Large Corpora.
Lei Shi and Rada Mihalcea. 2004. Open Text Parsing
Using FrameNet and WordNet. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, Proceed-
ings of HLT-NAACL 2004: Demonstration Papers,
pages 247?250, Boston, Massachusetts, USA, May
2 ? May 7. Association for Computational Linguis-
tics.
John F. Sowa and Eileen C. Way. 1986. Imple-
menting a semantic interpreter using conceptual
graphs. IBM Journal of Research and Develop-
ment, 30(1):57?69, January.
John F. Sowa. 1984. Conceptual Structures: Infor-
mation Processing in Mind and Machine. Addison-
Wesley, Reading, M.
Paola Velardi, Maria Teresa Pazienza, and Mario De-
Giovanetti. 1988. Conceptual graphs for the analy-
sis and generation of sentences. IBM Journal of Re-
search and Development, 32(2):251?267,March.
214
  
Automating XML markup of text documents 
Shazia Akhtar 
Department of Computer 
Science, University College 
Dublin, Belfield, Dublin 4, 
Ireland 
Shazia.Akhtar@ucd.i
e
Ronan G. Reilly 
Department of Computer 
Science, National University 
of Ireland, Maynooth, Ireland 
Ronan.Reilly@may.ie
John Dunnion 
Department of Computer 
Science, University College 
Dublin, Belfield, Dublin 4, 
Ireland 
John.Dunnion@ucd.ie
 
Abstract 
We present a novel system for automatically 
marking up text documents into XML and 
discuss the benefits of XML markup for intel-
ligent information retrieval. The system uses 
the Self-Organizing Map (SOM) algorithm to 
arrange XML marked-up documents on a two-
dimensional map so that similar documents 
appear closer to each other.  It then employs 
an inductive learning algorithm C5 to auto-
matically extract and apply markup rules from 
the nearest SOM neighbours of an unmarked 
document.  The system is designed to be adap-
tive, so that once a document is marked-up; its 
behaviour is modified to improve accuracy.  
The automatically marked-up documents are 
again categorized on the Self-Organizing 
Map.   
1 Introduction 
Vast amounts of information are now available in elec-
tronic form to which accurate and speedy access is get-
ting more difficult.  The increasing quantity of 
information has created a need for intelligent manage-
ment and retrieval techniques. Many of the existing in-
formation retrieval systems, which deal with large 
volumes of documents, have poor retrieval performance 
because these systems can use a little knowledge in the 
documents.  By adopting XML as a standard document 
format, content-based queries can be performed by ex-
ploiting the XML structure of the documents. In addi-
tion, specifically tagged sections of the documents can 
be searched rather than the entire document, thus pro-
viding fast and effective retrieval.  Furthermore, using 
the logical structure of a document created by  
 
 
 
 
 
 
 
 
XML markup, different types of operations can be per-
formed, for example, the same content can be reused in 
a variety of formats, specific elements can be extracted 
from the XML documents and full documents satisfying 
certain structural conditions can be retrieved from the 
database. These and other advantages of using XML 
make it a complete solution for content management 
and intelligent information retrieval. However, despite 
the advantages and the popularity of XML, we still do 
not have large repositories of XML because automatic 
XML markup is still a challenge and the process of 
manually marking up XML documents is complex, te-
dious and expensive. Most of the existing automatic 
markup systems are limited to certain domains and do 
not perform general automatic markup. In addressing 
the need for more general automatic markup of text 
documents, we present a system with a novel hybrid 
architecture.  The system uses the techniques of Self-
Organizing Map (SOM) algorithm (Kohonen, 1997) and 
an inductive learning algorithm, C5 (Quinlan, 1993, 
2000).   
 
2 System overview 
The system has two phases.  The first phase of the sys-
tem deals with the formation of a map of valid XML (a 
valid XML document is one which is well-formed and 
which has been validated against a DTD) marked-up 
documents using the SOM algorithm. The second phase 
deals with the automatic markup of new (unmarked) 
document according to the markup of existing docu-
ments.  Once a document is marked-up, the system?s 
behaviour is modified to improve accuracy.  These two 
phases of the system are currently implemented inde-
pendently but will be combined to form an integrated 
  
hybrid system.  This paper focuses on phase 2 of the 
system. 
 
 Phase 2 of the system is implemented as an independ-
ent automatic XML markup system, which is Figure 1. 
It comprises two main modules ? a Rule extraction 
module and a Markup module.  The rule extraction 
module deals with the extraction of rules using an in-
ductive learning approach (Mitchell, 1997).  Firstly, 
during a preliminary phase, training examples are col-
lected from the valid XML marked-up documents.  
These documents should be from a specific domain and 
their markup should be valid and conformant to the 
rules of a single Document Type Definition (DTD).  An 
XML document consists of a strictly nested hierarchy of 
elements with a root element.  Only elements having 
text nodes are considered as markup elements for our 
system.  
  
 
  
 
Figure 3: Automatic XML markup system 
 
The markup of elements nested within other elements 
can be accomplished by using the DTD.  Each training 
instance corresponds to an element containing a text 
node from the collection of marked-up documents.  The 
text enclosed between the start and end tags of all occur-
rences of each element is encoded using a fixed-width 
feature vector.  We have used 31 features in our ex-
periments.  The set of feature vectors is used by the sys-
tem to learn classifiers.  An inductive learning algorithm 
processes these encoded instances to develop classifiers 
for elements having specific tag names. These classifi-
ers segment the text of an unmarked document into dif-
ferent elements of the resulting XML marked-up 
document.  In our system, the C5 program is used to 
learn classifiers. These classifiers are later used to 
markup the segments of text as XML elements. 
 
The second module deals with the creation of XML 
markup.  The unmarked document to be used for this 
process should be from the same domain and should 
have a similar structure to the documents, which were 
used for learning the rules.  To accomplish the markup, 
the unmarked document is segmented into pieces of text 
using a variety of heuristics.  These heuristics are de-
rived from the set of training examples.  By using the 
DTD conformant to the document set used for learning 
the rules and by using the text segments stored for each 
element, a hierarchical structure of the document is en-
coded and the marked-up document is produced. 
 
The markup produced by the system can be validated 
according to a DTD.  However, in order to check the 
accuracy of the markup, we have to examine it manually 
and compare it with the original source (if available) as 
XML processors can only validate the syntax of the 
markup, and not its semantics. 
 
3 Performance 
We used documents from a number of different domains 
for our experiments, including letters from the Mac-
Greevy archive (Schreibman, 1998, 2000), a database of 
employee records, Shakespearean plays (Bosak, 1998), 
poems from an early American encoding project, and 
scientific journal articles (Openly Informatics, Inc., 
1999-200). Figure 2 shows a part of a scene from ?A 
Midsummer Night's Dream? as an example of XML 
markup automatically produced by our system. The 
underlined text was not marked up by our system. 
 
We have also evaluated our system with some of the 
document sets. For evaluation, we considered the ele-
ments representing the content of the document, and a 
human expert is required to evaluate this. We have used 
three performance measures in evaluating the automatic 
markup process.  These measures are 
? The percentage of markup elements determined 
correctly by the system  
? The percentage of markup elements determined 
incorrectly by the system  
? The percentage of markup elements not deter-
mined by the system (i.e. text nodes for these 
markup elements are not present in the marked-
up document produced by the system) 
 
The elements of 10 valid XML marked-up letters from 
the MacGreevy archive were used to learn C5 rules and 
text segmentation heuristics. By applying these rules 
and heuristics, 55 elements of five unmarked letters 
  
from the MacGreevy archive were automatically 
marked up by the system with 96% accuracy (we use 
the term  ?accuracy? here to mean the number of 
marked-up elements correctly determined by the sys-
tem).  Similarly, elements of 5 valid XML marked-up 
Shakespeare plays were used as training examples and 
13882 elements of four Shakespearean plays were 
automatically marked-up by the system. In this case the 
accuracy rate was 92%.  
 
? 
<SCENE>
<TITLE> SCENE I. Athens. The
palace of THESEUS. </TITLE>
<STAGEDIR> Enter THESEUS,
HIPPOLYTA, PHILOSTRATE, and
Attendants</STAGEDIR>
<SPEECH>
<SPEAKER>THESEUS</SPEAKER>
<LINE>Now, fair Hippolyta, our
nuptial hour</LINE>
<LINE>Draws on a pace; four
happy days bring in</LINE>
<LINE>Another moon: but, O, me
thinks, how slow</LINE>
<LINE>This old moon wanes!
she lingers my desires,</LINE>
<LINE>Like to a step-dame or
a dowager</LINE>
<LINE>Long withering out a
young man revenue. </LINE>
</SPEECH>
<SPEECH>
<SPEAKER>HIPPOLYTA</SPEAKER>
<LINE>Four days will quickly
steep themselves in night;
</LINE>
<LINE>Four nights will quickly
dream away the time; </LINE>
<LINE>And then the moon, like
to a silver bow</LINE>
<LINE>New-bent in heaven,
shall behold the night</LINE>
<LINE>Of our solemnities
</LINE>
</SPEECH>
      ? 
                
   
Figure 2.  A section taken from ?A Midsummer Night?s 
Dream? marked up by our system 
 
4 Conclusion 
We have described a system with a novel hybrid archi-
tecture that uses an inductive learning approach to per-
form automatic markup of text documents.  The system 
automatically marks up documents by capturing markup 
information from the neighbouring documents on a 
Self-Organizing Map.  Such marked-up documents can 
be used for management and retrieval purposes accord-
ing to the structural information they contain.  The re-
sults from our experiments demonstrate that our 
approach is practical and that our system provides a 
novel approach for automatically marking up text 
documents in XML.  The functionality of our system 
makes it a useful tool for electronic information ex-
change.   
Acknowledgements 
 
The support of the Informatics Research Initiative of 
Enterprise Ireland is gratefully acknowledged. The work 
was funded under grant PRP/00/INF/06. 
References 
 
Bosak, J. (1998).  Shakespeare 2.00.  
[http://metalab.unc.edu/bosak/xml/eg/shaks200.zip] 
Kohonen, T. (1997).  Self-Organizing Maps.  Springer 
Series in Information Science, Berlin, Heidelberg, New 
York. 
Mitchell, T. (1997).  Machine Learning.  McGraw Hill. 
Quinlan, J. R. (1993).  C4.5: Programs For Machine 
Learning.  Morgan Kauffman Publishers, San Mateo, 
Calif. 
Quinlan, J. R. (2000).  Data Mining Tools See5 and C5.0.  
[http://www.rulequest.com/see5-info.html] 
Openly Informatics, Inc. (1999-2000). 
[http://www.openly.com/efirst] 
Schreibman, S. (1998).  The MacGreevy Archive.  
[http://www.ucd.ie/~cosei/archive.htm] 
Schreibman, S. (2000). The MacGreevy Archive. 
[http://jafferson.village.Virginia.edu/macgreevy] 
 
 
 
  
 
 
  
  
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 103?107, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
IIRG: A Na??ve Approach to Evaluating Phrasal Semantics
Lorna Byrne, Caroline Fenlon, John Dunnion
School of Computer Science and Informatics
University College Dublin
Ireland
{lorna.byrne@ucd.ie,caroline.fenlon@ucdconnect.ie,john.dunnion@ucd.ie}
Abstract
This paper describes the IIRG 1 system en-
tered in SemEval-2013, the 7th International
Workshop on Semantic Evaluation. We partic-
ipated in Task 5 Evaluating Phrasal Semantics.
We have adopted a token-based approach to
solve this task using 1) Na??ve Bayes methods
and 2) Word Overlap methods, both of which
rely on the extraction of syntactic features. We
found that the word overlap method signifi-
cantly out-performs the Na??ve Bayes methods,
achieving our highest overall score with an ac-
curacy of approximately 78%.
1 Introduction
The Phrasal Semantics task consists of two related
subtasks. Task 5A requires systems to evaluate
the semantic similarity of words and compositional
phrases. Task 5B requires systems to evaluate the
compositionality of phrases in context. We partici-
pated in Task 5B and submitted three runs for eval-
uation, two runs using the Na??ve Bayes Machine
Learning Algorithm and a Word Overlap run using
a simple bag-of-words approach.
Identifying non-literal expressions poses a major
challenge in NLP because they occur frequently and
often exhibit irregular behavior by not adhering to
grammatical constraints. Previous research in the
area of identifying literal/non-literal use of expres-
sions includes generating a wide range of different
features for use with a machine learning prediction
algorithm. (Li and Sporleder, 2010) present a system
1Intelligent Information Retrieval Group
involving identifying the global and local contexts of
a phrase. Global context was determined by look-
ing for occurrences of semantically related words
in a given passage, while local context focuses on
the words immediately preceding and following the
phrase. Windows of five words at each side of the
target were taken as features. More syntactic fea-
tures were also used, including details of nodes from
the dependency tree of each example. The system
produced approximately 90% accuracy when tested,
for both idiom-specific and generic models. It was
found that the statistical features (global and local
contexts) performed well, even on unseen phrases.
(Katz and Giesbrecht, 2006) found that similarities
between words in the expression and its context indi-
cate literal usage. This is comparable to (Sporleder
and Li, 2009), which used cohesion-based classi-
fiers based on lexical chains and graphs. Unsu-
pervised approaches to classifying idiomatic use in-
clude clustering (Fazly et al, 2009), which classified
data based on semantic analyzability (whether the
meaning of the expression is similar to the mean-
ings of its parts) and lexical and syntactic flexibility
(measurements of how much variation exists within
the expression).
2 Task 5B
In Task 5B, participants were required to make a
binary decision as to whether a target phrase is used
figuratively or literally within a given context. The
phrase ?drop the ball? can be used figuratively, for
example in the sentence
We get paid for completing work, so we?ve designed
a detailed workflow process to make sure we don?t
103
drop the ball.
and literally, for example in the sentence
In the end, the Referee drops the ball with the
attacking player nearby.
In order to train systems, participants were given
training data consisting of approximately 1400 text
snippets (one or more sentences) containing 10 tar-
get phrases, together with real usage examples sam-
pled from the WaCky (Baroni et al, 2009) corpora.
The number of examples and distribution of figura-
tive and literal instances varied for each phrase.
Participants were allowed to submit three runs for
evaluation purposes.
2.1 Approach
The main assumption for our approach is that tokens
preceding and succeeding the target phrase might in-
dicate the usage of the target phrase, i.e. whether the
target phrase is being used in a literal or figurative
context. Firstly, each text snippet was processed us-
ing the Stanford Suite of Core NLP Tools2 to to-
kenise the snippet and produce part-of-speech tags
and lemmas for each token.
During the training phase, we identified and ex-
tracted a target phrase boundary for each of the tar-
get phrases. A target phrase boundary consists of a
window of tokens immediately before and after the
target phrase. The phrase boundaries identified for
the first two runs were restricted to windows of one,
i.e. the token immediately before and after the target
phrase were extracted, tokens were also restricted to
the canonical form.
For example, the target phrase boundary iden-
tified for the snippet: ?The returning team will
drop the ball and give you a chance to recover .? is
as follows:
before:will
after:and
and the target phrase boundary identified for
the snippet: ?Meanwhile , costs are going
through the roof .? is as follows:
before:go
after:.
2http://nlp.stanford.edu/software
IIRG Training Runs
RunID Accuracy (%)
Run0 85.29
Run1 81.84
Run2 95.92
Table 1: Results of IIRG Training Runs
We then trained multiple Na??ve Bayes classifiers
on these extracted phrase boundaries. The first clas-
sifier was trained on the set of target phrase bound-
aries extracted from the entire training set of tar-
get phrases and usage examples (Run0); the sec-
ond classifier was trained on the set of target phrase
boundaries extracted from the entire training set of
target phrases and usage examples including the
phrase itself as a predictor variable (Run1); and a
set of target-phrase classifiers, one per target phrase,
were trained on the set of target phrase boundaries
extracted from each individual target phrase (Run2).
The results of the initial training runs can be
seen in Table 1. Although Run0 yielded very high
accuracy scores on the training data, outperforming
Run1, in practice this approach performed poorly
on unseen data and was biased towards a figurative
classification. We thus opted not to implement this
run in the testing phase and instead concentrated on
Run1 and Run2.
For our third submitted run, we adopted a word
overlap method which implemented a simple bag-
of-words approach. For each target phrase we cre-
ated a bag-of-words by selecting the canonical form
of all of the noun tokens in each corresponding train-
ing usage example. The frequency of occurrence
of each token within a given context was recorded
and each token was labeled as figurative or literal
depending on its frequency of occurrence within a
given context. The frequency of occurrence of each
token was also recorded in order to adjust the thresh-
old of token occurrences for subsequent runs. For
this run, Run3, the token frequency threshold was
set to 2, so that a given token must occur two or more
times in a given context to be added to the bag-of-
words.
104
3 Results
System performance is measured in terms of accu-
racy. The results of the submitted runs can be seen
in Table 2.
Of the submitted runs, the Word Overlap method
(Run3) performed best overall. This approach was
also consistently good across all phrases, with scores
ranging from 70% to 80%, as seen in Table 3.
The classifiers trained on the canonical phrase
boundaries (Run1 and Run2) performed poorly on
unseen data. They were also biased towards a figura-
tive prediction. For several phrases they incorrectly
classified all literal expressions as figurative. They
were not effective at processing all of the phrases:
in Run1, some phrases had very high scores rela-
tive to the overall score (e.g. ?break a leg?), while
others scored very poorly (e.g. ?through the roof?).
In Run2, a similar effect was found. Interestingly,
even though separate classifiers were trained for
each phrase, the accuracy was lower than that of
Run1 in several cases (e.g. ?through the roof?). This
may be a relic of the small, literally-skewed, train-
ing data for some of the phrases, or may suggest that
this approach is not suitable for those expressions.
The very high accuracy of the classifiers tested on a
subset of the training data may be attributed to over-
fitting. The approach used in Run1 and Run2 is un-
likely to yield very accurate results for the classifi-
cation of general data, due to the potential for many
unseen canonical forms of word boundaries.
3.1 Additional Runs
After the submission deadline, we completed some
additional runs, the results of which can be seen in
Table 4.
These runs were similar to Run1 and Run2, where
we used Na??ve Bayes Classifiers to train on ex-
tracted target phrase boundaries. However, for Run4
and Run5 we restricted the phrase boundaries to the
canonical form of the nearest verb (Run4) or nearest
noun (Run5) that was present in a bag-of-words.
We used the same bag-of-words created for Run3
for the noun-based bag-of-words, and this same ap-
proach was used to create the (canonical form) verb-
based bag-of-words. If there were no such verbs or
nouns present then the label NULL was applied. If
a phrase occurred at the start or end of a text snip-
pet this information was also captured. The Na??ve
Bayes classifiers were then trained using labels from
the following set of input labels: FIGURATIVE,
LITERAL, START, END or NULL, which indicate
the target phrase boundaries of the target phrases.
For example, the target phrase boundaries identi-
fied for the snippet: ?Meanwhile , costs are going
through the roof .? for Run4 and Run5, respectively,
are as follows:
before:FIGURATIVE
after:END
where the FIGURATIVE label is the classification
of the token ?going? as indicated in the verb-based
bag-of-words, and
before:FIGURATIVE
after:END
where the FIGURATIVE label is the classification
of the token ?costs? as indicated in the noun-based
bag-of-words.
As in Run1 and Run2, an entire-set classifier and
individual target-phrase classifiers were trained for
both runs. These additional runs performed well,
yielding high accuracy results and significantly out-
performing Run1 and Run2.
The Run4 classifiers did not perform compara-
tively well across all phrases. In particular, the target
phrase ?break a leg?, had very low accuracy scores,
possibly because the training data for the phrase was
small and contained mostly literal examples. The
ranges of phrase scores for the noun classification
runs (Run5) were similar to those of the Word Over-
lap runs. The results across each phrase were also
consistent, with no scores significantly lower than
the overall accuracy. Using target phrase boundaries
based on noun classifications may prove to yield rea-
sonable results when extended to more phrases, as
opposed to the erratic results found when using verb
classifications.
In both Run4 and Run5, very similar overall re-
sults were produced from both the entire-set and
target-phrase classifiers. In most cases, the run per-
formed poorly on the same phrases in both instances,
indicating that the approach may not be appropriate
for the particular phrase. For example, the verb clas-
sifications runs scored low accuracy for ?drop the
ball?, while the noun classifications run was approx-
imately 80% accurate for the same phrase using both
105
IIRG Submitted Runs (%)
RunID Overall
Accuracy
Precision
(Figurative)
Recall
(Figurative)
Precision
(Literal)
Recall
(Literal)
Run1 53.03 52.03 89.97 60.25 15.65
Run2 50.17 50.81 41.81 54.06 58.84
Run3 77.95 79.65 75.92 76.62 80.27
Table 2: Results of Runs Submitted to Sem-Eval 2013
IIRG Submitted Runs - Per Phrase Accuracy (%)
RunID At the
end
of the
day
Bread
and
butter
Break
a leg
Drop
the
ball
In the
bag
In the
fast
lane
Play
ball
Rub
it in
Through
the roof
Under
the
micro-
scope
Run1 68.92 57.89 40.00 40.82 43.42 67.86 52.63 66.67 64.94 33.33
Run2 45.95 38.16 83.33 57.14 48.68 75.00 46.05 56.67 29.87 62.82
Run3 75.68 82.89 73.33 83.67 72.37 75.00 78.95 60.00 80.52 83.33
Table 3: Results of Runs Submitted to Sem-Eval 2013 (per phrase)
IIRG Additional Runs - Accuracy (%)
RunID Entire-Set
Classifier
Target-Phrase
Classifier
Run4 64.81 65.99
Run5 75.25 76.60
Table 4: Accuracy of Additional Unsubmitted Runs
an entire-set and target-phrase classifier.
4 Conclusion
This is the first year we have taken part in the Se-
mantic Evaluation Exercises, participating in Task
5b, Evaluating Phrasal Semantics. Task 5B requires
systems to evaluate the compositionality of phrases
in context. We have adopted a token-based approach
to solve this task using 1) Na??ve Bayes methods
whereby target phrase boundaries were identified
and extracted in order to train multiple classifiers;
and 2) Word Overlap methods, whereby a simple
bag-of-words was created for each target phrase. We
submitted three runs for evaluation purposes, two
runs using Na??ve Bayes methods (Run1 and Run2)
and one run based on a Word Overlap approach
(Run3). The Word Overlap approach, which limited
each bag-of-words to using the canonical form of the
nouns in the text snippets, yielded the highest accu-
racy scores of all submitted runs, at approximately
78% accurate. An additional run (Run5), also us-
ing the canonical form of the nouns in the usage ex-
amples but implementing a Na??ve Bayes approach,
yielded similar results, almost 77% accuracy. The
approaches which were restricted to using the nouns
in the text snippets yielded the highest accuracy re-
sults, thus indicating that nouns provide important
contextual information for distinguishing literal and
figurative usage.
In future work, we will explore whether we can
improve the performance of the target phrase bound-
aries by experimenting with the local context win-
dow sizes. Another potential improvement might
be to examine whether implementing more sophisti-
cated strategies for selecting tokens for the bags-of-
words improves the effectiveness of the Word Over-
lap methods.
References
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky Wide Web: A Collection of
Very Large Linguistically Processed Web-Crawled
Corpora. Language Resources and Evaluation 43,
3(3):209?226.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised Type and Token Identification
of Idiomatic Expressions. Computational Linguistics,
35(1):61?103, March.
106
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
Identification of Non-Compositional Multi-Word Ex-
pressions using Latent Semantic Analysis. In Pro-
ceedings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Under-
lying Properties, pages 12?19.
Linlin Li and Caroline Sporleder. 2010. Linguistic Cues
for Distinguishing Literal and Non-Literal Usages. In
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING ?10), pages 683?
691.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
Recognition of Literal and Non-Literal Use of Id-
iomatic expressions. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009).
107
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 38?46,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Analyzing Patient Records to Establish If and When a Patient Suffered from
a Medical Condition
James Cogley, Nicola Stokes, Joe Carthy and John Dunnion
School of Computer Science and Informatics
University College Dublin
Dublin, Ireland
James.Cogley@ucdconnect.ie {Nicola.Stokes, Joe.Carthy, John.Dunnion}@ucd.ie
Abstract
The growth of digital clinical data has raised
questions as to how best to leverage this data
to aid the world of healthcare. Promising ap-
plication areas include Information Retrieval
and Question-Answering systems. Such sys-
tems require an in-depth understanding of the
texts that are processed. One aspect of this
understanding is knowing if a medical con-
dition outlined in a patient record is recent,
or if it occurred in the past. As well as this,
patient records often discuss other individu-
als such as family members. This presents
a second problem - determining if a medi-
cal condition is experienced by the patient de-
scribed in the report or some other individ-
ual. In this paper, we investigate the suitabil-
ity of a machine learning (ML) based system
for resolving these tasks on a previously unex-
plored collection of Patient History and Phys-
ical Examination reports. Our results show
that our novel Score-based feature approach
outperforms the standard Linguistic and Con-
textual features described in the related litera-
ture. Specifically, near-perfect performance is
achieved in resolving if a patient experienced
a condition. While for the task of establish-
ing when a patient experienced a condition,
our ML system significantly outperforms the
ConText system (87% versus 69% f-score, re-
spectively).
1 Introduction
The growth of the digitization of clinical docu-
ments has fostered interest in how to best lever-
age this data in providing assistance in the world
of healthcare, including novel information re-
trieval (Voorhees and Tong, 2010), question an-
swering (Demner-Fushman and Lin, 2007; Patrick
and Li, 2011) and clinical summarization sys-
tems (Feblowitz et al, 2011).
Given the richness of the language found in clin-
ical reports, novel systems require a deeper under-
standing of this textual data. One aspect of this un-
derstanding is the assertion status of medical condi-
tions (Demner-Fushman et al, 2011). The assertion
status of a medical condition may refer to Negation
Resolution, Temporal Grounding (deciding if a con-
dition is currently or historical, and Condition Attri-
bution (deciding if a condition is experienced by the
patient described in the report or some other individ-
ual). The focus of this paper rests on the latter two
tasks of Temporal Grounding and Condition Attribu-
tion as Negation has been satisfactorily addressed in
Chapman et al (2007).
Several approaches, ranging in complexity, have
been proposed for resolving temporal information.
Hripcsak et al (2005) modeled the task as a con-
straint satisfaction problem. Another rule-based ap-
proach that achieved moderate results uses regular
expressions matching occurrences of trigger terms
(Chapman et al 2007). A trigger term in this context
refers to a term or phrase that provides strong evi-
dence supporting the attribution (e.g., patient, fam-
ily member) or temporality (e.g., current, past) of
a condition. Given the limitations of solely us-
ing pre-composed trigger term lists, recent focus
has been placed on the use of rule-based learning
systems with different feature sets (Mowery et al,
2009). Section headers, tense and aspect are investi-
gated as features, with promising results for the tem-
porality task achieving an accuracy score of 89%.
However, the authors? acknowledge that conclusions
drawn must be tentative as a majority class classifier
achieved an accuracy of 86.9% (only 13% of condi-
tions in the dataset are historical).
38
This paper extends current work in the domain in
the following ways. The dataset used in these exper-
iments is generated from a collection of previously
unannotated History & Physical (H&P) Examina-
tion reports. Prior work has focused on other report
types such as discharge summaries and emergency
department reports. In these cases the distribution
of historical and recent conditions is often heavily
skewed in favour of descriptions of recent conditions
experienced by the patient. As H&P reports aim to
provide a comprehensive picture of a patient?s past
and present state, a more uniform distribution of his-
torical and recent conditions is present in this report
type. This work extends previous work by exploring
the use of machine learning (ML) as an alternative to
hand-crafted rule based systems and rule-based ML
approaches to resolving these tasks.
In this work, a comparative analysis of several
ML algorithms from different paradigms are eval-
uated, in order to determine the best approach for
our tasks. Building on this, the performance of four
automatically extracted feature sets are evaluated,
identifying their contributions and also their interac-
tions. This work also extends existing work by au-
tomatically extracting features that were previously
extracted manually as well as the proposal of a set
of novel score-based features. The performance of
the ML algorithms are compared to the rule-based
system - ConText. Our results show that the ML
approaches significantly outperform this rule-based
system on the Temporal Grounding task.
2 Related Work
Natural Language Processing techniques have
been shown to have many different uses in Clinical
Text Analysis, such as in the representation (Sager
et al, 1994) and understanding (Christensen, 2002)
of clinical narratives, and frequently now in the con-
text of more elaborate large-scale systems, such as a
clinical decision support system (Demner-Fushman
et al, 2009).
Given the sensitive nature of clinical narratives
and the difficulty in obtaining data collections for
experimental purposes, evaluation and comparison
of NLP systems in this domain is difficult. However,
recently anonymised data provided by the Biomedi-
cal Language Understanding (BLU) Lab at the Uni-
versity of Pittsburgh as well as datasets provided
as part of the i2b2/VA 2010 challenge (Uzuner et
al., 2011), has greatly aided NLP research into the
processing of clinical narratives. The dataset pro-
vided by BLU Lab and used in this work con-
sists of 101,711 reports of several different report
types ranging from discharge summaries to surgical
pathology reports. The report types differ in con-
tent, technical language and structure. For example,
surgical pathology reports are very technical and ex-
plicit in the information that they convey, e.g. results
of a biopsy, blood cell counts etc. In contrast, dis-
charge summaries and consultation reports are more
expressive, and aim to create a more complete pa-
tient profile, e.g. including work and personal cir-
cumstances. The BLU Lab have published a num-
ber of papers on the topic of resolving the assertion
status of medical conditions (Chapman et al, 2007;
Harkema et al, 2009; Mowery et al, 2009). Their
ConText algorithm (Chapman et al, 2007) uses
handcrafted regular expressions, along with trigger
terms and termination terms to determine character-
istics of a condition mention in a text. The condition
characteristics investigated included negation, tem-
porality (recent, historical, hypothetical) and experi-
encer (patient, other). Their approach worked very
well on the negation and hypothetical temporality,
achieving an f-score of 97% in determining nega-
tion and an f-score of 88% in resolving hypothetical
conditions. However, the approach was less success-
ful when determining historical conditions and their
experiencer, with f-scores of 71% and 67%, respec-
tively. These results were generated on emergency
room reports only.
In more recent work, their algorithm was ap-
plied to 5 other types of clinical document, namely:
surgical pathology, operative procedure, radiol-
ogy, echocardiogram and discharge summaries
(Harkema et al, 2009). Results achieved on these
new datasets were largely the same, with f-scores
for negation ranging between 75% and 95%, and for
hypothetical conditions ranging between 76% and
96%. Again, a marked drop-off was seen for histor-
ical conditions, with few occurrences of conditions
for other experiencers annotated in the datasets (i.e.
relatives) making it difficult to draw definitive con-
clusions from this work.
Although this manual rule based approach has
performed well and is advocated due to its ease of
implementation (Meystre et al, 2008), Harkema et
al. (2009) note its limitations in resolving historical
conditions, and encourage the possibility of statisti-
cal classifiers in which information other than lexi-
cal items, are considered as features. Further work
investigating the use of Machine Learning (Uzuner
et al, 2009; Mowery et al, 2009) has seen posi-
39
tive breakthroughs in resolving the assertion status
of medical conditions.
The 2010 i2b2 challenge (Uzuner et al, 2011)
provided a rigid and standardized platform for eval-
uating systems in resolving the assertion status of
medical conditions found in Discharge Summaries.
The challenge consisted of three subtasks:Concept
Extraction, Assertion and Relation Identification.
The second subtask of Assertion involved the devel-
opment of systems that resolved the assertion sta-
tus of medical conditions. As part of the asser-
tion task there were six possible assertion statuses:
present, absent, uncertain, conditional, or not associ-
ated with the patient. Systems submitted to this chal-
lenge ranged from more simplistic pattern matching
techniques to more complex supervised and semi-
supervised approaches (de Bruijn et al, 2011; Clark
et al, 2011). The datasets used in the 2010 i2b2
challenge were not available to non-participants at
the time the experiments presented in this work were
conducted. We plan to explore these datasets in
future work. This research investigates patient vs.
non-patient conditions as well as past vs. present
conditions in order to fine tune feature-sets that may
be generalized to further assertion statuses.
In the context of this paper, while the BLU Lab
clinical report collection is available, the medical
condition annotations are not. As already stated, our
intention is to explore a machine learning approach
to these tasks. For this purpose we annotated a por-
tion of the consultation report section of the collec-
tion. There were two reasons for this - firstly, the
BLU Lab have not reported results on this report
type so there is no duplication of annotation effort
and secondly, it turns out that the consultation re-
ports are a much richer source of historical condi-
tions and condition attribution than any of the report
types annotated previously.
3 Method
3.1 Corpus
For this task, 120 H&P reports were randomly
extracted from the BluLab?s NLP repository. As
already stated, this report type?s fuller descriptions
make it richer than previous datasets in instances
of condition attribution and temporal grounding. A
breakdown in the distributions of these annotations
can be seen in Tables 1 and 2.
H&P reports may vary in the organization of con-
tent, but the content is mostly uniform, expressing
the same information about patients (Sager et al,
1987). As well as this, many reports feature head-
ings for different sections of the report (past medical
history, impression), information which can be used
as features in a classification task. Before annotat-
ing conditions found in the text, preprocessing was
required in order to retain such information.
Table 1: Annotated Condition Attribution Occurrences
Class Count
Patient 872
Other 93
Total 965
Table 2: Annotated Temporal Grounding Occurrences
Class Count
Historical 448
Recent 424
Total 872
3.1.1 Preprocessing
Preprocessing of the data consisted of a simple
Java program that extended Lingpipe1 tools in or-
der to correctly split sentences on this dataset, and
extract the heading for the section in which the sen-
tence is contained.
The preprocessing outputs the sentence number,
followed by a separator, the sentence?s contents and
the heading under which the sentence features. Sen-
tences were split for ease of annotation and also
to allow parsing and part-of-speech tagging by the
C&C2 parsing tools. C&C was chosen for its scala-
bilty, speed and the accuracy of its biomedical lan-
guage models. A cursory analysis of its output in-
dicates that its performance is acceptable. As C&C
does not provide a sentence splitter, Lingpipe?s split-
ter was availed of for this task.
3.1.2 Annotation
Annotation of the dataset was performed by two
annotators over a 60 hour period. The inter-
annotator agreement was measured using the kappa
statistic (Carletta, 1996). A kappa statistic of 0.78
was achieved. The annotators were presented with
the collection, to annotate with an XML like tag
?CONDITION?. This tag must have two attributes,
?EXP? representing condition attribution and ?HIST?
1http://alias-i.com/lingpipe/
2http://svn.ask.it.usyd.edu.au/trac/
candc
40
representing the temporal grounding of the condi-
tion.
? HIST: A value of 1 indicates the occurrence of a
historical condition, where 0 describes a current
or recent condition. e.g. ?The patient presented
with <CONDITION NUM=?1? HIST=?0?> re-
nal failure </CONDITION>? would indicate the
condition ?renal failure? is current.
? EXP: A value of 1 implies the expe-
riencer is the patient with 0 signifying
?other?. e.g. ?The patient has a fam-
ily history of <CONDITION NUM=?1?
EXP=?0?>hypertension </CONDITION>?
signifies the condition ?hypertension? is not
experienced by the patient.
3.2 Machine Learning Algorithms
Early work in the resolution of assertion status
primarily focused on the use of manually created
rule-based systems, with more recent work focusing
on statistical and ML methods. However, the do-
main of ML contains many sub-paradigms and vary-
ing approaches to classification. In this paper, three
ML methods that have not been previously applied
to this task are explored. These three classifiers,
namely Naive Bayes, k-Nearest Neighbour and Ran-
dom Forest represent the paradigms of probabilistic,
lazy and ensemble learning, respectively.
Naive Bayes is a probabilistic classifier imple-
menting Bayes Theorem. As a result, features im-
plemented using this classifier are deemed to be in-
dependent. Despite this strong assumption it has
been shown to be more than successful in text classi-
fication tasks such as spam filtering (Provost, 1999).
k-Nearest Neighbour (kNN) (Cover and Hart,
1967) is a simple pattern recognition algorithm that
classifies an instance according to its distance to the
k closest training instances. This algorithm has been
chosen to represent the paradigm of lazy learning,
i.e. there is no training phase as all computation
is performed at the classification stage. Despite its
simplicity, k-NN has often produce high accuracy
results in comparison to other approaches (Caruana,
2006).
The final classifier chosen for this task represents
the state-of-the-art in machine learning, namely the
Random Forest algorithm (Breiman, 2001). A Ran-
dom Forest consists of many different decision trees,
combining bagging (Breiman, 1996), and random
feature selection.
3.3 Features
In this section, a list of features contributing to
this task are presented. All features are automati-
cally extracted using a set of tools developed by the
authors. Section 3.3.1 presents score-based features
that are unique to this work. Section 3.3.2 describes
the implementation of features outlined in Chapman
et al(2007). Section 3.3.3 and Section 3.3.4 present
features similar to those investigated in Mowery et
al. (2009).
3.3.1 Score based features
Scored based features used in this system extend
and reinforce Trigger List features by computing a
normalized score for the number of occurrences of
Trigger List terms3. This feature aims to add fur-
ther granularity to the decision making of the ML al-
gorithms, presenting a floating point number rather
than a binary one.
The equation for computing these scores is de-
fined as follows.
s =
Nt
(Nw ? Sw)
(1)
Nt represents the number of trigger terms found in
the sentence that contains the condition, Nw is the
total number of words in the sentence, with Sw being
the number of stopwords4. These scores are calcu-
lated for each type of trigger term. For example, for
trigger type relative mention, a score is calculated
using mentions of relatives in the sentence.
3.3.2 Trigger List Features
? precededByHistTerm: This feature performs
a look-up for trigger terms from the historical
word list, checking if it directly precedes the
condition. An example historical trigger term
would be ?history of? as in ?a history of dia-
betes?. If a condition, such as diabetes, is mod-
ified by a historical trigger term, it will return 1,
otherwise 0.
? containsHistMention: This is a weaker
form of precededByHistTerm, checking sim-
ply if a trigger term from the historical list oc-
curs in the same sentence as the condition. If
one does, it will return 1 otherwise 0.
? hasRelativeMention: If the sentence which
contains the condition also contains a trigger
3These trigger lists may be downloaded at http:
//csserver.ucd.ie/?jcogley/downloads/
wordlists.tar.gz
4The list of stopwords may be downloaded at
http://csserver.ucd.ie/?jcogley/downloads/
stopwords.txt
41
term from the experiencer list such as ?mother?,
?father? or ?uncle? it will return 1, otherwise 0.
? hasPatientMention: 1 if the sentence men-
tions the patient, otherwise 0.
? containsDeath: 1 if it contains the terms ?de-
ceased?, ?died? from the death trigger terms list
otherwise 0. A sentence describing death is more
likely to refer to a relative, rather than the pa-
tient.
? mentionsCommunity: 1 if one of ?area?,
?community? from the geographical trigger list
is mentioned, otherwise 0. If a sentence de-
scribes a community, as in ?there has been a re-
cent outbreak of flu in the area?, it is not refer-
ring to the patient, therefore the condition should
not be attributed to the patient.
? precededByWith: 1 if the condition is directly
preceded by ?with?, otherwise 0. ?with? was
found to have higher frequency when describ-
ing patients rather than individuals other than the
patient. e.g. ?Patient presented with high blood
pressure and fever.?
? containsPseudoTerms: Pseudo-historical
terms or phrases may mention a term that is
found in the Historical list, but do not indicate
that a condition mention in the same sentence is
being used in a historical context. For example,
?poor history? is a pseudo-historical trigger
term. It uses a historical trigger term (?history?);
however ?poor history? refers to the incomplete
nature of the patient?s medical history, not the
historical nature of their condition. This feature
returns 1 on the occurrence of a pseudo trigger
term, otherwise 0.
3.3.3 Contextual features
In resolving the textual context of conditions, it
is important to look at what surrounds the condition
beyond the lexical items. With these contextual fea-
tures, we can capture that section in which a sen-
tence occurs, and how many conditions occur in the
sentence.
? isInFamHist: The importance of header infor-
mation is motivated by the assumption that con-
ditions that fall under explicit headings, are more
than likely to have a greater affinity to the head-
ing. This feature returns 1 if it is under Family
History, 0 otherwise.
? isInList: A binary feature denoting whether
a condition occurs as part of a list of conditions,
with one condition per line. Returns 1 if it is a
member of such a list, otherwise 0.
? numOfConditions: This feature represents the
number of conditions present in a given sen-
tence. A higher number of conditions indicates
that the condition may be part of a list. Sentences
that contain a list of conditions tend to list past
conditions rather than recently suffered illnesses.
3.3.4 Linguistically motivated features
Three features were designed to monitor the ef-
fect of the verb tense on a condition. This feature
has already been shown to aid the classification pro-
cess (Mowery et al, 2009). For this task, linguistic
features were extracted from the output of the C&C
parsing tool, using both part-of-speech tags along
with dependency information.
? hasPastTense: A binary feature with 1 indi-
cating the sentence contains a past tense verb, 0
otherwise. e.g. ?The patient previously suffered
renal failure? would return 1. If a condition is
modified by a past tense verb, it has occurred in
the past.
? hasPresentTense: A binary feature with 1
indicating the sentence contains a present tense
verb, 0 otherwise. If a condition is modified by a
present tense verb, the condition is current. e.g.
?the patient presents coughing?.
? containsModalVerb: A binary feature with 1
indicating the sentence contains a modal verb,
0 otherwise. e.g. ?palpitations may have been
caused by anxiety?. The presence of the modal
?may? following the condition indicates the con-
dition is currently being examined and is there-
fore recent.
? tenseInClause: Analyzes the tense found in
the same syntactic clause as the condition being
examined. For example, in ?abdominal pain has
ceased, but patient now complains of lower ex-
tremity pain?, ?abdominal pain? has a past tense
within its clausal boundary, where the clause
which contains ?lower extremity pain? has a
present tense verb.
? tenseChange: Determines whether the verb
tense used in the clause that contains the con-
dition differs with the verb in another clause in
the sentence. e.g. ?Migraines persist yet palpi-
tations resolved?. This feature allows finer gran-
ularity in resolving the tense surrounding condi-
tions, such as the description of current condi-
tions in the context of the patient?s history.
42
4 Experiment Setup & Evaluation
There are two aims of the experiments reported
in this section: firstly, to evaluate the performance
of ML algorithms in resolving the assertion status of
medical conditions. Secondly, to assess the perfor-
mance of individual feature sets in order to discover
the most contributory and informatory features or
sets of features. To evaluate the latter, classifications
using all possible combinations of feature sets (listed
in Table 3) were performed.
Table 3: Feature-set Combinations
ID Feature-Sets
TrigLingConScore trigger, linguistic, score-based, contextual
TrigLingScore trigger, linguistic, score-based
TrigLingCon trigger, linguistic, contextual
TrigConScore trigger, score-based, contextual
LingConScore linguistic, score-based, contextual
TrigLing trigger, linguistic
TrigScore trigger, score-based
TrigCon trigger, contextual
LingScore linguistic, score-based
LingCon linguistic, contextual
ConScore score-based, contextual
Trigger trigger
Ling linguistic
Score score-based
Con contextual
4.1 Evaluation
The systems are evaluated by the metrics preci-
sion, recall and f-score:
precision =
TP
TP + FP
recall =
TP
TP + FN
f =
2? Precision?Recall
Precision + Recall
where TP is the number of true positives, FP is the num-
ber of false positives, FN is the number of false negatives.
Systems are evaluated using both cross-validation
and hold-out methods. In the hold-out method there
are two data sets, one used for training the classifier
and a second for testing it on a blind sub-set of test
material. 10-fold cross-validation is performed on
the training sets and final results are reported in this
paper on the held-out blind test set. Three hold-out
classification splits were experimented with (i.e.,
train/test splits: 30%/70%; 50%/50%; 70%/30%).
We found that results for each of the data splits and
cross-validation experiments were largely uniform.
To avoid repetition of results, Section 5 focuses on
experiments using a held-out method training/test
split of 70%/30%. All hold-out experiments were
implemented using Weka?s (Hall et al, 2009) Ex-
perimenter interface. Cross-Validation experiments
were performed using a script developed by the au-
thors in conjunction with Weka?s API. This allowed
the ML approaches and the ConText algorithm to be
evaluated against the same test-folds.
4.1.1 Comparison with a rule-based system
ConText (Chapman et al, 2007) is a simple yet
effective rule-based system designed to resolve the
assertion status of medical conditions. Comparative
analysis is performed between an implementation of
ConText5 and the ML approaches described in 3.2.
The ML systems were trained on 70% of the dataset
(610 conditions). The remaining 30% (262 condi-
tions) was used as a test set for both ConText and
the Machine Learning systems. For cross-validation
experiments, ConText was evaluated against each
test set fold. For the Condition Attribution exper-
iments training was performed on 675 conditions
with testing performed on 290 conditions. In eval-
uating Temporal Grounding the training set com-
prised of 610 conditions with the test-set containing
262 conditions.
5 Experimental Results
This section reports results of the experiments
outlined in Section 4.
5.1 Condition Attribution
In a system that resolves the assertion status of
medical conditions, it is of benefit to know who is
experiencing the medical condition before resolving
more complex information such as temporality. In
this section, preliminary results on Condition Attri-
bution are presented. The dataset used in evaluat-
ing the effectiveness of Condition Attribution was
highly skewed, as shown in Table 1. This is a natural
skew caused simply by the fact that reports discuss
the patient more than other individuals (e.g., blood
relatives). As a result the baseline score using a Ma-
jority Class classifier achieved an f-score of 95%
(Table 4). Given these results, the contextual fea-
ture set contributes most, as shown by the removal
of the contextual feature set in TrigLingScore coin-
ciding with a drop in performance. However, the
skewed dataset resulted in no statistical significance
5http://code.google.com/p/negex/
downloads/detail?name=GeneralConText.Java.
v.1.0_10272010.zip
43
between classifiers and feature-sets.
Table 4: Selected feature-sets (f-score) using Cross-
Validation for the Condition Attribution task
ID RFor kNN NB Maj.
TrigLingConScore 100% 100% 100% 95%
TrigLingScore 96% 96% 96% 95%
TrigConScore 100% 100% 100% 95%
Con 100% 100% 100% 95%
In this task, ConText achieved an f-score of 99%.
As there is little difference in scores achieved be-
tween ConText and the approaches in Table 4 - a
manual rule-based system can be considered ade-
quate for this task.
Taking a closer look at the performance of the fea-
ture sets, we see that the contextual feature set con-
tributes most to the task with the removal of contex-
tual features coinciding with a drop in performance
e.g., TrigLingScore in Table 4. The strength of this
feature set lies with the feature isInFamHist. This
feature simply checks if the condition occurs under
the heading ?Family History?. Its highly influen-
tial performance would indicate that its quite rare
for the mention of another individual anywhere else
in a clinical report. The Con run, which is solely
composed of contextual features achieves near per-
fect performance, an indication that the contribution
of other features to the task of Condition Attribu-
tion is minimal. Although this work used only H&P
reports, this finding may be generalized to other re-
port types such as Discharge Summaries which also
explicitly mark sections pertaining to other individ-
uals.
5.2 Temporal Grounding
The distribution of past and recent medical con-
ditions is not skewed (as in the Condition Attribu-
tion task), and hence it presents a more challeng-
ing classification task. Despite the varying per-
formance of individual classifiers and feature sets
the results obtained are again largely similar across
cross-validation and hold-out methods, with the per-
formance of each training set fitting the distribu-
tion in Figure 1. Initial experiments investigated the
use of another state-of-the-art classifier, the Support
Vector Machine using a polykernel, however due to
its relatively poor performance (70% f-score, with
its precision soundly beaten by other approaches) it
will not be discussed in further detail.
Random Forest proved to be the most effective
classifier across almost all feature sets. However,
kNN was a very near second place - Random Forest
only significantly6 outperformed kNN on two occa-
sions (TrigLingConScore, LingConScore). In con-
strast, Naive Bayes performed poorly - despite hav-
ing outperformed all other systems on the precision
metric, it failed to outperform the baseline majority
classifier on the recall.
Although the precision of ConText matches that
of the Random Forest and kNN (Table 5), it is also
let down by its recall performance. As a result, there
is a statistical significant difference between its f-
score and that of the Random Forest and kNN.
Table 5: Temporal Grounding ConText Comparison
System Precision Recall F-score
kNN 80% 80% 80%
RandomForest 82% 84% 83%
NaiveBayes 91% 61% 72%
ConText 80% 61% 69%
Majority 55% 100% 71%
6 Discussion
The distribution of recent and historical condi-
tions for the task of Temporal Grounding is more
evenly distributed than that used in Condition Attri-
bution, allowing for a more interesting comparison
of the approaches and features employed.
Figure 1 shows the performance of each ML for
each feature-set combination. Random Forest was
expectedly the best performing algorithm. However,
more surprising was the comparative performance
of the often overlooked kNN algorithm. Both ap-
proaches statistically significantly outperformed the
rule-based system ConText. Though the rule based
system matched the high performing ML systems in
terms of precision, it was significantly outperformed
with respect to recall.
The most contributory feature set in the ML runs
was the novel score-based feature set. This feature
creates a normalized score for the occurrence of trig-
ger terms in the same sentence as the medical con-
dition in question. It was designed to reinforce the
importance of trigger terms, by providing a numeric
score to support the binary Trigger List feature. The
addition of score-based features to any of the fea-
ture combinations coincided with a statistical signif-
icant boost in performance, with Score (composed
solely of score-based features) outperforming half of
all other feature combinations as seen in Figure 1,.
On the contrary, the effect of contextual features
on the performance of the algorithms for Temporal
6Significance calculated by Paired T-Test with 95% confi-
dence.
44
30%	 ?
40%	 ?
50%	 ?
60%	 ?
70%	 ?
80%	 ?
90%	 ?
100%	 ?
TrigLin
gConSc
ore 
TrigLin
gScore
 
TrigLin
gCon 
TrigCo
nScore
 
LingCo
nScore
 
TrigLin
g 
TrigSco
re TrigCo
n 
LingSco
re LingCo
n 
ConSco
re Trig Ling Score Con 
Ran.	 ?Forest	 ? kNN	 ? Naive	 ?Bayes	 ? Maj.	 ?Class	 ?
Figure 1: Temporal Grounding f-score performance with 70% Training Data
Grounding is minimal, or even detrimental to the
task. For example, in Figure 1, the performance
of the kNN algorithm increases from TrigLingCon-
Score to TrigLingScore with the removal of contex-
tual features. The performance of the Random For-
est is unaffected by such detrimental features as it
performs its own feature selection prior to classifi-
cation. Though there are several feature set com-
binations reaching a high level of performance, the
most effective approach combines trigger list terms,
linguistic and score based features with the Random
Forest algorithm.
These experiments extend previous work by pro-
viding a systematic, automated approach to feature
extraction for the purpose of ML approaches to Tem-
poral Grounding. They also indicate the high per-
formance and contribution of our novel score-based
features. These features are not designed to solely
classify instances found in H&P reports and can
be applied to other clinical reports such as Dis-
charge Summaries and Emergency Department re-
ports. Previous work has involved the use of the
latter mentioned report types. Unfortunately, given
the terms-of-use of these datasets they could not be
made available to authors to facilitate comparative
experiments.
7 Conclusion
In this paper, we proposed the use of machine
learning (ML) in resolving if and when a patient
experienced a medical condition. The implemented
ML algorithms made use of features comprising of
trigger terms, linguistic and contextual information,
and novel score-based features. In an evaluation of
these feature sets it was found that score-based fea-
tures contributed to the task of resolving when a pa-
tient experienced a medical condition.
The ML approaches were also evaluated against
the rule-based system ConText on a new annotated
dataset of History & Physical (H&P) Examination
Reports. In this evaluation it was discovered that the
task of resolving if a condition was experienced by
the patient was adequately solved by the ConText
system, achieving an f-score of 99%. Although, the
ML approaches proposed achieved perfect perfor-
mance, there is no statistical significance between
the result sets. However, the more challenging task
of deciding when a patient experienced a medical
condition is deemed to be best suited to a ML ap-
proach, with the top performing classifier Random
Forest achieving an f-score of 87%, significantly
outperforming ConText which achieved 69% on the
same dataset .
The results achieved in these tasks have paved the
way for several avenues of future work. We be-
lieve that the performance of these tasks is now suffi-
ciently accurate to justify their inclusion in an Infor-
mation Retrieval (IR) application. It is our intention
to use our medical condition analysis techniques to
annotate clinical documents and build an advanced
IR system capable of taking advantage of this mark
up in the context of the TREC Medical Records
Track 20127. With the availability of datasets such
as that of the i2b2 Shared Task 2010 data, further
work will include experimentation on these datasets
as well as an investigation into further assertion sta-
tuses.
8 Acknowledgments
We are grateful to Dr Martina Naughton for her
advice on many aspects of this paper. We also wish
to acknowledge the support of Science Foundation
Ireland, who fund this research under grant number
10/RFP/CMS2836.
7http://groups.google.com/group/trec-med
45
References
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24:123?140.
L. Breiman. 2001. Random forests. Machine Learning,
45:5?32.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational Linguistics,
22(2):249 ? 254.
R. Caruana. 2006. An empirical comparison of super-
vised learning algorithms. In Proceedings of 23rd In-
ternational Conference on Machine Learning.
W. W. Chapman, D. Chu, and J. N. Dowling. 2007. Con-
text: An algorithm for identifying contextual features
from clinical text. In BioNLP 2007: Biological, trans-
lational, and clinical language processing, pages 81?
88, June.
L. M. Christensen. 2002. Mplus: A probabilistic med-
ical language understanding system. In Proceedings
of Workshop on Natural Language Processing in the
Biomedical Domain, pages 29?36.
C. Clark, J. Aberdeen, M. Coarr, D. Tresner-Kirsch,
B. Wellner, A. Yeh, and L. Hirschman. 2011. Mitre
system for clinical assertion status classification. Jour-
nal of the American Medical Informatics Association.
T. Cover and P. Hart. 1967. Nearest neighbor pattern
classification. Transactions on Information Theory.
B. de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, and
X. Zhu. 2011. Machine-learned solutions for three
stages of clinical information extraction: the state of
the art at i2b2 2010. Journal of the American Medical
Informatics Association.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statisti-
cal techniques. In Computational Linguistics, pages
63?103.
D Demner-Fushman, W W. Chapman, and C J. McDon-
ald. 2009. What can natural language processing do
for clinical decision support? Journal of Biomedical
Informatics, 42:760?772.
D. Demner-Fushman, S. Abhyankar, A. Jimeno-Yepes,
R. Loane, B. Rance, F. Lang, N. Ide, E. Apostolova,
and A. R. Aronson. 2011. A knowledge-based ap-
proach to medical records retrieval. In TREC 2011
Working Notes.
J. Feblowitz, A. Wright, H. Singh, L. Samal, and D. Sit-
tig. 2011. Summarization of clinical information: A
conceptual model. Biomedical Informatics.
M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The weka data mining
software: An update. SIGKDD Explorations.
H. Harkema, J. N. Dowling, T. Thornblade, and W. W.
Chapman. 2009. Context: An algorithm for identify-
ing contextual features from clinical text. Journal of
Biomedical Informatics, 42(5):839?851.
G. Hripcsak, L. Zhou, S. Parsons, A. K. Das, and S. B.
Johnson. 2005. Modeling electronic discharge sum-
maries as a simple temporal constraint satisfaction
problem. Journal of the American Medical Informat-
ics Association, 12(1):55?63, January.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler, and
J. F. Hurdle. 2008. Extracting information from tex-
tual documents in the electronic health record: a re-
view of recent research. IMIA Yearbook of Medical
Informatics, pages 128?144.
D. L. Mowery, H Harkema, J. N. Dowling, J. L. Lust-
garten, and W. W. Chapman. 2009. Distinguishing
historical from current problems in clinical reports-
which textual features help? In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processin, pages 10?18. Association for
Computational Linguistics.
J. Patrick and M. Li. 2011. An ontolotgy for clinical
questions about the contents of patients notes. Journal
of Biomedical Informatics.
J. Provost. 1999. Naive-bayes vs. rule-learning in classi-
fication of email. Technical report, The University of
Texas at Austin.
N. Sager, C. Friedman, and M.S. Lyman. 1987. Medi-
cal Language Processing: Computer Management of
Narrative Data. Addison-Wesley.
N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. J.
Tick. 1994. Natural language processing and the rep-
resentation of clinical data. Journal of the American
Medical Informatics Association, 1:142?160.
O. Uzuner, X. Zhang, and T. Sibanda. 2009. Machine
learning and rule-based approaches to assertion classi-
fication. Journal of the American Medical Informatics
Association, 16(1):109?115.
O?. Uzuner, BR. South, S. Shen, and SL. DuVall. 2011.
2010 i2b2/va challenge on concepts, assertions, and re-
lations in clinical text. Journal of the American Medi-
cal Informatics Association.
E. Voorhees and R. Tong. 2010. Overview of the trec
2011 medical records track. preprint.
46
