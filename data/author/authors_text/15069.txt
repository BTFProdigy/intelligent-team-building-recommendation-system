Cross-lingual Slot Filling from Comparable Corpora
Matthew Snover, Xiang Li, Wen-Pin Lin, Zheng Chen, Suzanne Tamang,
Mingmin Ge, Adam Lee, Qi Li, Hao Li, Sam Anzaroot, Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
New York, NY 11367, USA
msnover@qc.cuny.edu, hengji@cs.qc.cuny.edu
Abstract
This paper introduces a new task of
crosslingual slot filling which aims to dis-
cover attributes for entity queries from
crosslingual comparable corpora and then
present answers in a desired language. It is
a very challenging task which suffers from
both information extraction and machine
translation errors. In this paper we ana-
lyze the types of errors produced by five
different baseline approaches, and present
a novel supervised rescoring based valida-
tion approach to incorporate global evi-
dence from very large bilingual compara-
ble corpora. Without using any additional
labeled data this new approach obtained
38.5% relative improvement in Precision
and 86.7% relative improvement in Recall
over several state-of-the-art approaches.
The ultimate system outperformed mono-
lingual slot filling pipelines built on much
larger monolingual corpora.
1 Introduction
The slot filling task at NIST TAC Knowledge
Base Population (KBP) track (Ji et al, 2010)
is a relatively new and popular task with the
goal of automatically building profiles of enti-
ties from large amounts of unstructured data,
and using these profiles to populate an existing
knowledge base. These profiles consist of nu-
merous slots such as ?title?, ?parents? for per-
sons and ?top-employees? for organizations. A
variety of approaches have been proposed to ad-
dress both tasks with considerable success; nev-
ertheless, all of the KBP tasks so far have been
limited to monolingual processing. However, as
the shrinking fraction of the world?s Web pages
are written in English, many slot fills can only
be discovered from comparable documents in
foreign languages. By comparable corpora we
mean texts that are about similar topics, but
are not in general translations of each other.
These corpora are naturally available, for ex-
ample, many news agencies release multi-lingual
news articles on the same day. In this paper we
propose a new and more challenging crosslin-
gual slot filling task, to find information for any
English query from crosslingual comparable cor-
pora, and then present its profile in English.
We developed complementary baseline ap-
proaches which combine two difficult problems:
information extraction (IE) and machine trans-
lation (MT). In this paper we conduct detailed
error analysis to understand how we can exploit
comparable corpora to construct more complete
and accurate profiles.
Many correct answers extracted from our
baselines will be reported multiple times in any
external large collection of comparable docu-
ments. We can thus take advantage of such in-
formation redundancy to rescore candidate an-
swers. To choose the best answers we consult
large comparable corpora and corresponding IE
results. We prefer those answers which fre-
quently appear together with the query in cer-
tain IE contexts, including co-occurring names,
coreference links, relations and events. For ex-
ample, we prefer ?South Korea? instead of ?New
York Stock Exchange? as the ?per:employee of ?
answer for ?Roh Moo-hyun? using global ev-
idence from employment relation extraction.
Such global knowledge from comparable corpora
110
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 110?119,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
provides substantial improvement over each in-
dividual baseline system and even state-of-the-
art monolingual slot filling systems. Compared
to previous methods of exploiting comparable
corpora, our approach is novel in multiple as-
pects because it exploits knowledge from: (1)
both local and global statistics; (2) both lan-
guages; and (3) both shallow and deep analysis.
2 Related Work
Sudo et al (2004) found that for a crosslin-
gual single-document IE task, source language
extraction and fact translation performed no-
tably better than machine translation and tar-
get language extraction. We observed the same
results. In addition we also demonstrate that
these two approaches are complementary and
can be used to boost each other?s results in a
statistical rescoring model with global evidence
from large comparable corpora.
Hakkani-Tur et al (2007) described a filtering
mechanism using two crosslingual IE systems
for improving crosslingual document retrieval.
Many previous validation methods for crosslin-
gual QA, such as those organized by Cross Lan-
guage Evaluation Forum (Vallin et al, 2005), fo-
cused on local information which involves only
the query and answer (e.g. (Kwork and Deng,
2006)), keyword translation (e.g. (Mitamura et
al., 2006)) and surface patterns (e.g. (Soubbotin
and Soubbotin, 2001)). Some global valida-
tion approaches considered information redun-
dancy based on shallow statistics including co-
occurrence, density score and mutual informa-
tion (Clarke et al, 2001; Magnini et al, 2001;
Lee et al, 2008), deeper knowledge from depen-
dency parsing (e.g. (Shen et al, 2006)) or logic
reasoning (e.g. (Harabagiu et al, 2005)). How-
ever, all of these approaches made limited efforts
at disambiguating entities in queries and limited
use of fact extraction in answer search and vali-
dation.
Several recent IE studies have stressed the
benefits of using information redundancy on
estimating the correctness of the IE out-
put (Downey et al, 2005; Yangarber, 2006;
Patwardhan and Riloff, 2009; Ji and Grish-
man, 2008). Some recent research used com-
parable corpora to re-score name translitera-
tions (Sproat et al, 2006; Klementiev and Roth,
2006) or mine new word translations (Fung and
Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao
and Zhai, 2005; Hassan et al, 2007; Udupa et
al., 2009; Ji, 2009). To the best of our knowl-
edge, this is the first work on mining facts from
comparable corpora for answer validation in a
new crosslingual entity profiling task.
3 Experimental Setup
3.1 Task Definition
The goal of the KBP slot filling task is to extract
facts from a large source corpus regarding cer-
tain attributes (?slots?) of an entity, which may
be a person or organization, and use these facts
to augment an existing knowledge base (KB).
Along with each slot answer, the system must
provide the ID of a document which supports
the correctness of this answer. KBP 2010 (Ji et
al., 2010) defines 26 types of attributes for per-
sons (such as the age, birthplace, spouse, chil-
dren, job title, and employing organization) and
16 types of attributes for organizations (such
as the top employees, the founder, the year
founded, the headquarters location, and the sub-
sidiaries).
The new problem we define in this paper is an
extension of this task to a crosslingual paradigm.
Given a query in a target language t and a col-
lection of documents in a source language s,
a system must extract slot answers about the
query and present the answers in t. In this pa-
per we examine a specific setting of s=Chinese
and t=English.
To score crosslingual slot filling, we pool all
the system responses and group equivalent an-
swers into equivalence classes. Each system re-
sponse is rated as correct, wrong, inexact or re-
dundant. Given these judgments, we calculate
the precision, recall and F-measure of each sys-
tem, crediting only correct answers.
3.2 Data and Query Selection
We use the comparable corpora of English
TDT5 (278,358 documents) and Chinese TDT5
111
(56,424 documents) as our source collection.
For query selection, we collected all the en-
tities from the entire source collection and
counted their frequencies. We then selected 50
informative entities (25 persons and 25 organiza-
tions) which were located in the middle range of
frequency counts. Among the 25 person queries,
half are Chinese-specific names, and half are
non-Chinese names. The 25 organizations fol-
low a representative distribution according to
the entity subtypes defined in NIST Automatic
Content Extraction (ACE) program1.
3.3 Baseline Pipelines
3.3.1 Overview
We employ the following two types of base-
line crosslingual slot filling pipelines to process
Chinese documents. Figure 1 and Table 1 shows
the five system pipelines we have used to con-
duct our experiments.
Type A Translate Chinese texts into English,
and apply English slot filling systems to the
translations.
Type B Translate English queries into Chinese,
apply Chinese slot filling systems to Chinese
texts, and translate answers back to English. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Machine 
Translation 
English 
Texts 
Chinese 
Texts 
English Candidate Answers 
English
Query
English Slot Filling 
 Answer 
Translation  Pattern Matching 
 Supervised
Classification
Chinese Slot Filling 
 Supervised 
Classification 
Chinese 
Query 
 Query 
Translation
Figure 1: Overview of Baseline Crosslingual Slot Fill-
ing Pipelines
1http://www.itl.nist.gov/iad/mig/tests/ace/
 
Pipeline Label Components Data 
(1) English Supervised Classification Mono-
lingual (2) English Pattern Matching 
English 
TDT5 
 
(3) 
MT+English 
Supervised 
Classification Type A  
(4) 
MT+English 
Pattern Matching Cross-lingual 
Type 
B 
 
 
(5) 
Query Translation 
+Chinese Supervised 
Classification 
+Answer Translation 
Chinese 
TDT5 
 
 
 
Table 1: Monolingual and Crosslingual Baseline Slot
Filling Pipelines
3.3.2 Monolingual Slot Filling
We applied a state-of-the-art bilingual slot
filling system (Chen et al, 2010) to process
bilingual comparable corpora. This baseline
system includes a supervised ACE IE pipeline
and a bottom-up pattern matching pipeline.
The IE pipeline includes relation extraction and
event extraction based on maximum entropy
models that incorporate diverse lexical, syntac-
tic, semantic and ontological knowledge. The
extracted ACE relations and events are then
mapped to KBP slot fills. In pattern matching,
we extract and rank patterns based on a dis-
tant supervision approach (Mintz et al, 2009)
that uses entity-attribute pairs from Wikipedia
Infoboxes and Freebase (Bollacker et al, 2008).
We set a low threshold to include more answer
candidates, and then a series of filtering steps
to refine and improve the overall pipeline re-
sults. The filtering steps include removing an-
swers which have inappropriate entity types or
have inappropriate dependency paths to the en-
tities.
3.3.3 Document and Name Translation
We use a statistical, phrase-based MT sys-
tem (Zens and Ney, 2004) to translate Chinese
documents into English for Type A Approaches.
The best translation is computed by using a
weighted log-linear combination of various sta-
tistical models: an n-gram language model, a
phrase translation model and a word-based lex-
112
icon model. The latter two models are used in
source-to-target and target-to-source directions.
The model scaling factors are optimized with re-
spect to the BLEU score similar to (Och, 2003).
The training data includes 200 million running
words in each language. The total language
model training data consists of about 600 mil-
lion running words.
We applied various name mining approaches
from comparable corpora and parallel corpora,
as described in (Ji et al, 2009) to extract and
translate names in queries and answers in Type
B approaches. The accuracy of name translation
is about 88%. For those names not covered by
these pairs, we relied on Google Translate 2 to
obtain results.
4 Analysis of Baseline Pipelines
In this section we analyze the coverage (Sec-
tion 4.1) and precision (Section 4.2) results of
the baseline pipelines. We then illustrate the
potential for global validation from comparable
corpora through a series of examples.
4.1 Coverage Analysis: Toward
Information Fusion
Table 2 summarizes the Precision (P), Recall
(R) and F-measure (F) of baseline pipelines and
the union of their individual results.
Table 2: Baseline Pipeline Results 
System P R F 
(1) 0.08 0.54 0.15 
(2) 0.02 0.35 0.03 Mono- 
lingual Union of 
(1)+(2) 
0.03 0.69 0.05 
(3) 0.04 0.04 0.04 
(4) 0.03 0.25 0.05 
Union of 
(3)+(4) 0.03 0.26 0.05 
(5) 0.04 0.46 0.08 
Cross- 
lingual 
Union of 
(3)+(4)+(5) 0.03 0.56 0.05 
Compara
ble 
Corpora 
Union of 
(1)+(2)+(3)+
(4)+(5) 
0.02 1 0.04 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2http://translate.google.com/
Although crosslingual pipelines used a much
smaller corpus than monolingual pipelines, they
extracted comparable number of correct answers
(66 vs. 81) with a slightly better precision.
In fact, the crosslingual pipeline (5) performs
even better than monolingual pipeline (2), es-
pecially on the employment slots. In particu-
lar, 96.35% of the correct answers for Chinese-
specific person queries (e.g. ?Tang Jiaxuan?)
were extracted from Chinese data. Even for
those facts discovered from English data, they
are about quite general slots such as ?title? and
?employee of ?. In contrast, Chinese data covers
more diverse biographical slots such as ?family
members? and ?schools attended?.
Compared to the union of Type A approaches
(pipelines (3)+(4)), Pipeline (5) returned many
more correct answers with higher precision. The
main reason is that Type A approaches suffer
from MT errors. For example, MT mistakenly
translated the query name ?Celine Dion? into
?Clinton? and thus English slot filling compo-
nents failed to identify any answers. One can
hypothesize that slot filling on MT output can
be improved by re-training extraction compo-
nents directly from MT output. However, our
experiments of learning patterns from MT out-
put showed negative impact, mainly because
MT errors were too diverse to generalize. In
other cases even though slot filling produced cor-
rect results, MT still failed to translate the an-
swer names correctly. For example, English slot
filling successfully found a potential answer for
?org:founded by? of the query ?Microsoft? from
the following MT output: ?The third largest of
the Microsoft common founder Alan Doss , aged
50, and net assets of US 22 billion.?; however,
the answer string ?Paul Allen? was mistakenly
translated into ?Alan Doss?. MT is not so cru-
cial for ?per:title? slot because it does not require
translation of contexts.
To summarize, 59% of the missing errors were
due to text, query or answer translation errors
and 20% were due to slot filling errors. Never-
theless, the union of (3)+(4)+(5) still contain
more correct answers. These baseline pipelines
were developed from a diverse set of algorithms,
and typically showed strengths in specific slots.
113
In general we can conclude that monolin-
gual and crosslingual pipelines are complemen-
tary. Combining the responses from all baseline
pipelines, we can get similar number of correct
answers compared to one single human annota-
tor.
4.2 Precision Analysis: Toward Global
Validation
The spurious errors from baseline crosslingual
slot filling pipelines reveal both the shortcom-
ings of the MT system and extraction across
languages. Table 3 shows the distribution of
spurious errors.
Pipeline Spurious Errors Distribution
Content Translation 
+ Extraction 
85% 
Query Translation 13% 
Type A 
Answer Translation 2% 
Word Segmentation 34% 
Relation Extraction 33% 
Coreference 17% 
Semantic Type 13% 
Type B 
Slot Type 3% 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3: Distribution of Spurious Errors
Table 3 indicates a majority (85%) of spurious
errors from Type A pipelines were due to ap-
plying monolingual slot filling methods to MT
output which preserves Chinese structure.
As demonstrated in previous work (e.g. (Par-
ton and McKeown, 2010; Ji et al, 2009)),
we also found that many (14.6%) errors were
caused by the low quality of name translation
for queries and answers.
For example, ?????/McGinty? was mis-
takenly translated into the query name ?Kim
Jong-il?, which led to many incorrect answers
such as ?The British Royal joint military re-
search institute? for ?per:employee of ?.
In contrast, the spurious errors from Type B
pipelines were more diverse. Chinese IE com-
ponents severely suffered from word segmen-
tation errors (34%), which were then directly
propagated into Chinese document retrieval and
slot filling. Many segmentation errors occurred
with out-of-vocabulary names, especially per-
son names and nested organization names. For
example, the name ????/Yao Mingbao? was
mistakenly segmented into two words ???/Yao
Ming? and ??/bao?, and thus the document was
mistakenly retrieved for the query ?Yao Ming?.
In many cases (33%) Chinese relation and
event extraction components failed to cap-
ture Chinese-specific structures due to the lim-
ited size of training corpora. For example,
from the context ???????????????
?/Xiao Wan-chang, who were invited to be-
come the economics consultant for Chen Shui-
bian?, Chinese slot filling system mistakenly ex-
tracted ?consultant? as a ?per:title? answer for
the query ?Chen Shui-bian? using a common
pattern ?<query><title>?.
13% of errors were caused due to invalid se-
mantic types for certain slots. For example,
many metaphoric titles such as ?tough guy?
don?t match the definition of ?per:title? in the
annotation guideline ?employment or member-
ship position?.
5 Global Validation
Based on the above motivations we propose to
incorporate global evidence from a very large
collection of comparable documents to refine
local decisions. The central idea is to over-
generate candidate answers from multiple weak
baselines to ensure high upper-bound of recall,
and then conduct effective global validation to
filter spurious errors while keeping good answers
in order to enhance precision.
5.1 Supervised Rescoring
Ideally, we want to choose a validation model
which can pick out important features in a con-
text wider than that used by baseline pipelines.
Merging individual systems to form the union of
answers can be effective, but Table 2 shows that
simple union of all pipelines produced worse F-
measure than the best pipeline.
In this paper we exploit the reranking
paradigm, commonly used in information re-
trieval, to conduct global validation. By model-
ing the empirical distribution of labeled training
data, statistical models are used to identify the
114
strengths and weaknesses (e.g. high and low pre-
cision slots) of individual systems, and rescore
answers accordingly. Specially, we develop a
supervised Maximum Entropy (MaxEnt) based
model to rescore the answers from the pipelines,
selecting only the highest-scoring answers.
The rescorer was trained (using cross-
validation) on varying subsets of the features.
The threshold at which an answer is deemed to
be true is chosen to maximize the F-Measure on
the training set.
5.2 Validation Features
Table 4 describes the validation features used for
rescoring, where q is the query, q? the Chinese
translation of q, t the slot type, a the candidate
answer, a? the Chinese form of a, s the context
sentence and d is the context document support-
ing a.
The feature set benefits from multiple dimen-
sions of crosslingual slot filling. These features
were applied to both languages wherever anno-
tation resources were available.
In the KBP slot filling task, slots are of-
ten dependent on each other, so we can im-
prove the results by improving the ?coherence?
of the story (i.e. consistency among all gener-
ated answers - query profiles). We use feature
f2 to check whether the same answer was gen-
erated for conflicting slots, such as per:parents
and per:children.
Compared to traditional QA tasks, slot fill-
ing is a more fine-grained task in which differ-
ent slots are expected to obtain semantically
different answers. Therefore, we explored se-
mantic constraints in both local and global con-
texts. For example, we utilized bilingual name
gazetteers from ACE training corpora, Google
n-grams (Ji and Lin, 2009) and the geonames
website 3 to encode features f6, f8 and f9; The
org:top members/employees slot requires a sys-
tem to distinguish whether a person member/
employee is in the top position, thus we encoded
f10 for this purpose.
The knowledge used in our baseline pipelines
is relatively static ? it is not updated during the
3http://www.geonames.org/statistics/
extraction process. Achieving high performance
for cross-lingual slot filling requires that we take
a broader view, one that looks outside a sin-
gle document or a single language in order to
exploit global knowledge. Fortunately, as more
and more large crosslingual comparable corpora
are available, we can take advantage of informa-
tion redundancy to validate answers. The basic
intuition is that if a candidate answer a is cor-
rect, it should appear together with the query
q repeatedly, in different documents, or even in
certain coreference links, relations and events.
For example, ?David Kelly - scientist?, and
??????/Shintaro Ishihara - ??/governor?
pairs appear frequently in ?title? coreference
links in both English and Chinese corpora;
?Elizabeth II? is very often involved in an ?em-
ployment? relation with ?United Kingdom? in
English corpora. On the other hand, some in-
correct answers with high global statistics can be
filtered out using these constraints. For exam-
ple, although the query ????/Tang Jiaxuan?
appears frequently together with the candidate
per:title answer ???/personnel?, it is linked by
few coreference links; in contrast, it?s coreferen-
tial with the correct title answer ?????/State
Council member? much more frequently.
We processed cross-lingual comparable cor-
pora to extract coreference links, relations and
events among mentions (names, nominals and
time expressions etc.) and stored them in an
external knowledge base. Any pair of <q, a>
is then compared to the entries in this knowl-
edge base. We used 157,708 documents from
Chinese TDT5 and Gigaword to count Chinese
global statistics, and 7,148,446 documents from
DARPA GALE MT training corpora to count
English global statistics, as shown in features
f12 and f13. Fact based global features f14, f15,
f16 and f17, were calculated from 49,359 Chi-
nese and 280,513 English documents (annotated
by the bilingual IE system in Section 3.3.2.
6 Experiments
In this section, we examine the overall perfor-
mance of this method. We then discuss the
usefulness of the individual sets of features. In
115
Characteristics 
Scope Depth Language 
Description 
f1: frequency of <q, a, t> that appears in all baseline outputs Global 
(Cross-
system) 
Shallow 
 English f2: number of conflicting slot types in which answer a appears in all baseline 
outputs 
f3: conjunction of t and whether a is a year answer Shallow English 
f4: conjunction of t and whether a includes numbers or letters 
f5: conjunction of place t and whether a is a country name 
f6: conjunction of per:origin t and whether a is a nationality 
f7: if t=per:title, whether a is an acceptable title 
f8: if t requires a name answer, whether a is a name 
Local 
Deep 
 
English 
 
f9: whether a has appropriate semantic type 
f10: conjunction of org:top_members/employees and whether there is a high-level 
title in s 
Global 
(Within-
Document) 
Deep English 
f11: conjunction of alternative name and whether a is an acronym of q 
Chinese f12: conditional probability of q/q' and a/a' appear in the same document Shallow 
(Statistics) English f13: conditional probability  of q/q' and a/a' appear in the same sentence 
Both f14:  co-occurrence of q/q' and a/a'  appear in coreference links 
English f15: co-occurrence of q/q' and a/a'  appear in relation/event links 
English f16: conditional probability of q/q' and a/a' appear in relation/event links 
Global 
(Cross-
document 
in 
comparable 
corpora) 
Deep 
(Fact-
based) 
English f17: mutual information of q/q' and a/a' appear in relation/event links 
 
Table 4: Validation Features for Crosslingual Slot Filling
the following results, the baseline features are
always used in addition to any other features.
6.1 Overall Performance
Because of the data scarcity, ten-fold cross-
validation, across queries, was used to train
and test the system. Quantitative results after
combining answers from multiple pipelines are
shown in Table 5. We used two basic features,
one is the slot type and the other is the entity
type of the query (i.e. person or organization).
This basic feature set is already successful in im-
proving the precision of the pipelines, although
this results in a number of correct answers be-
ing discarded as well. By adding the additional
validation features described previously, both
the f-score and precision of the models are im-
proved. In the case of the cross-lingual pipelines
(3+4+5) the number of correct answers chosen
is almost doubled while increasing the precision
of the output.
6.2 Impact of Global Validation
A comparison of the benefits of global versus lo-
cal features are shown in Table 6, both of which
dramatically improve scores over the baseline
features. The global features are universally
Pipelines F P R
Basic Features
1+2 0.31 0.31 0.30
3+4+5 0.26 0.39 0.20
1+2+3+4+5 0.27 0.29 0.25
Full Features
1+2 0.37 0.30 0.46
3+4+5 0.36 0.35 0.37
1+2+3+4+5 0.31 0.28 0.35
Table 5: Using Basic Features to Filter Answers
more beneficial than the local features, although
the local features generate results with higher
precision at the expense of the number of correct
answers returned. The global features are espe-
cially useful for pipelines 3+4+5, where the per-
formance using just these features reaches those
of using all other features ? this does not hold
true for the monolingual pipelines however.
6.3 Impact of Fact-driven Deep
Knowledge
The varying benefit of fact-driven cross-
document features and statistical cross-
document features are shown in Table 7.
116
Pipelines F P R
Local Features
1+2 0.34 0.35 0.33
3+4+5 0.29 0.40 0.22
1+2+3+4+5 0.27 0.32 0.24
Global Features
1+2 0.35 0.30 0.42
3+4+5 0.37 0.36 0.38
1+2+3+4+5 0.33 0.29 0.38
Table 6: The Benefit of Global versus Local Features
While both feature sets are beneficial, the
monolingual pipelines (1+2) benefit more
from statistical features while the cross-lingual
pipelines (3+4+7) benefit slightly more from
the fact-based features. Despite this bias, the
overall results when the features are used in
all pipelines are very close with the fact-based
features being slightly more useful overall.
Pipelines F P R
Fact-Based Features
1+2 0.33 0.27 0.42
3+4+5 0.35 0.43 0.29
1+2+3+4+5 0.30 0.27 0.34
Statistical Features
1+2 0.37 0.34 0.40
3+4+5 0.34 0.35 0.33
1+2+3+4+5 0.29 0.25 0.34
Table 7: Fact vs. Statistical Cross-Doc Features
Translation features were only beneficial to
pipelines 3, 4, and 5, and provided a slight in-
crease in precision from 0.39 to 0.42, but pro-
vided no noticeable benefit when used in con-
junction with results from pipelines 1 and 2.
This is because the answers where translation
features would be most useful were already be-
ing selected by pipelines 1 and 2 using the base-
line features.
6.4 Discussion
The use of any re-scoring, even with baseline
features, provides large gains over the union of
the baseline pipelines, removing large number
of incorrect answers. The use of more sophis-
ticated features provided substantial gains over
the baseline features. In particular, global fea-
tures proved very effective. Further feature en-
gineering to address the remaining errors and
the dropped correct answer would likely provide
increasing gains in performance.
In addition, two human annotators, indepen-
dently, conducted the same task on the same
data, with a second pass of adjudication. The F-
scores of inter-annotator agreement were 52.0%
for the first pass and 73.2% for the second pass.
This indicates that slot filling remains a chal-
lenging task for both systems and human anno-
tators?only one monolingual system exceeded
30% F-score in the KBP2010 evaluation.
7 Conclusion and Future Work
Crosslingual slot filling is a challenging task
due to limited performance in two separate ar-
eas: information extraction and machine trans-
lation. Various methods of combining tech-
niques from these two areas provided weak yet
complementary baseline pipelines. We proposed
an effective approach to integrate these base-
lines and enhance their performance using wider
and deeper knowledge from comparable cor-
pora. The final system based on cross-lingual
comparable corpora outperformed monolingual
pipelines on much larger monolingual corpora.
The intuition behind our approach is that
over-generation of candidate answers from weak
baselines provides a potentially strong recall
upper-bound. The remaining enhancement be-
comes simpler: filtering errors. Our experiments
also suggest that our rescoring models tend to
over-fit due to small amount of training data.
Manual annotation and assessment are quite
costly, motivating future work in active learning
and semi-supervised learning methods. In addi-
tion, we plan to apply our results as feedback to
improve MT performance on facts using query
and answer-driven language model adaptation.
We have demonstrated our approach on English-
Chinese pair, but the framework is language-
independent; ultimately we would like to extend
the task to extracting information from more
languages.
117
Acknowledgments
This work was supported by the U.S. NSF CAREER
Award under Grant IIS-0953149 and PSC-CUNY
Research Program. Any opinions, findings, and con-
clusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily
reflect the views of the National Science Foundation.
References
K. Bollacker, R. Cook, and P. Tufts. 2008. Free-
base: A shared database of structured general hu-
man knowledge. In Proc. National Conference on
Artificial Intelligence.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Marissa Passantino, and Heng Ji. 2010. Top-
down and bottom-up: A combined approach to
slot filling. Lecture Notes in Computer Science,
6458:300?309, December.
C. L. A. Clarke, G. V. Cormack, and T.R. Lynam.
2001. Exploiting redundancy in question answer-
ing. In Proc. SIGIR2001.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A Probabilistic Model of Redundancy in
Information Extraction. In Proc. IJCAI 2005.
Pascale Fung and Lo Yuen Yee. 1998. An ir ap-
proach for translating new words from nonparallel
and comparable texts. In COLING-ACL.
Dilek Hakkani-Tur, Heng Ji, and Ralph Grishman.
2007. Using information extraction to improve
cross-lingual document retrieval. In Proc. RANLP
workshop on Multi-source, Multilingual Informa-
tion Extraction and Summarization.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
A. Hickl, and P. Wang. 2005. Employing two
question answering systems in trec 2005. In Proc.
TREC2005.
Ahmed Hassan, Haytham Fahmy, and Hany Has-
san. 2007. Improving named entity translation
by exploiting comparable and parallel corpora. In
RANLP.
Heng Ji and Ralph Grishman. 2008. Refining Event
Extraction through Cross-Document Inference. In
Proc. of ACL-08: HLT, pages 254?262.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In Proc.
PACLIC2009.
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard
Zens, and Hermann Ney. 2009. Name translation
for distillation. Handbook of Natural Language
Processing and Machine Translation: DARPA
Global Autonomous Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, and
Kira Griffitt. 2010. An overview of the tac2010
knowledge base population track. In Proc.
TAC2010.
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information
networks. In ACL-IJCNLP 2009 workshop on
Building and Using Comparable Corpora (BUCC
2009): from Parallel to Non-parallel Corpora.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL 2006.
K.-L. Kwork and P. P. Deng. 2006. Chinese
question-answering: Comparing monolingual with
english-chinese cross-lingual results. In Asia In-
formation Retrieval Symposium.
Cheng-Wei Lee, Yi-Hsun Lee, and Wen-Lian Hsu.
2008. Exploring shallow answer ranking features
in cross-lingual and monolingual factoid question
answering. Computational Linguistics and Chi-
nese Language Processing, 13:1?26, March.
B. Magnini, M. Negri, R. Prevete, and H. Tanev.
2001. Is it the right answer?: Exploiting web
redundancy for answer validation. In Proc.
ACL2001.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In ACL-IJCNLP
2009.
Teruko Mitamura, Mengqiu Wang, Hideki Shima,
and Frank Lin. 2006. Keyword translation accu-
racy and cross-lingual question answering in chi-
nese and japanese. In EACL 2006 Workshop on
MLQA.
F. J. Och. 2003. Minimum error rate training in
statistical machine translaton. In Proc.ACL2003.
Kristen Parton and Kathleen McKeown. 2010. Mt
error detection for cross-lingual question answer-
ing. Proc. COLING2010.
Siddharth Patwardhan and Ellen Riloff. 2009. A
Unified Model of Phrasal and Sentential Evidence
for Information Extraction. In Proc. EMNLP
2009.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and ger-
man corpora. In ACL 1999.
Li Shao and Hwee Tou Ng. 2004. Mining new word
translations from comparable corpora. In COL-
ING2004.
D. Shen, G. Saarbruechen, and D. Klakow. 2006.
Exploring correlation of dependency relation
paths for answer extraction. In Proc. ACL2006.
118
M. M. Soubbotin and S. M. Soubbotin. 2001. Pat-
terns of potential answer expressions as clues to
the right answers. In Proc. TREC2001.
Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006. Named entity transliteration with compa-
rable corpora. In ACL 2006.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2004. Cross-lingual information extraction evalu-
ation. In Proc. COLING2004.
Tao Tao and Chengxiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-language
information integration. In Proc. KDD2005.
Raghavendra Udupa, K. Saravanan, A. Kumaran,
and Jagadeesh Jagarlamudi. 2009. Mint: A
method for effective and scalable mining of named
entity transliterations from large comparable cor-
pora. In EACL2009.
Alessandro Vallin, Bernardo Magnini, Danilo Gi-
ampiccolo, Lili Aunimo, Christelle Ayache, Petya
Osenova, Anselmo Peas, Maaren de Rijke, Bogdan
Sacaleanu, Diana Santos, and Richard Sutcliffe.
2005. Overview of the clef 2005 multilingual ques-
tion answer track. In Proc. CLEF2005.
Roman Yangarber. 2006. Verification of Facts across
Document Boundaries. In Proc. International
Workshop on Intelligent Information Access.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proc. HLT/NAACL 2004.
119
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 43?52,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Language-Independent Name Translation Mining from
Wikipedia Infoboxes
Wen-Pin Lin, Matthew Snover, Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
New York, NY 11367, USA
danniellin@gmail.com, msnover@qc.cuny.edu, hengji@cs.qc.cuny.edu
Abstract
The automatic generation of entity profiles
from unstructured text, such as Knowledge
Base Population, if applied in a multi-lingual
setting, generates the need to align such pro-
files from multiple languages in an unsuper-
vised manner. This paper describes an unsu-
pervised and language-independent approach
to mine name translation pairs from entity pro-
files, using Wikipedia Infoboxes as a stand-in
for high quality entity profile extraction. Pairs
are initially found using expressions that are
written in language-independent forms (such
as dates and numbers), and new translations
are then mined from these pairs. The algo-
rithm then iteratively bootstraps from these
translations to learn more pairs and more
translations. The algorithm maintains a high
precision, over 95%, for the majority of its
iterations, with a slightly lower precision of
85.9% and an f-score of 76%. A side effect
of the name mining algorithm is the unsuper-
vised creation of a translation lexicon between
the two languages, with an accuracy of 64%.
We also duplicate three state-of-the-art name
translation mining methods and use two ex-
isting name translation gazetteers to compare
with our approach. Comparisons show our
approach can effectively augment the results
from each of these alternative methods and re-
sources.
1 Introduction
A shrinking fraction of the world?s web pages are
written in English, while about 3,000 languages are
endangered (Krauss, 2007). Therefore the ability
to access information across a range of languages,
especially low-density languages, is becoming in-
creasingly important for many applications. In this
paper we hypothesize that in order to extend cross-
lingual information access to all the language pairs
on the earth, or at least to some low-density lan-
guages which are lacking fundamental linguistic re-
sources, we can start from the much more scalable
task of ?information? translation, or more specifi-
cally, new name translation.
Wikipedia, as a remarkable and rich online ency-
clopedia with a wealth of general knowledge about
varied concepts, entities, events and facts in the
world, may be utilized to address this need. As
of March 2011 Wikipedia contains pages from 275
languages1, but statistical machine translation (MT)
techniques can only process a small portion of them
(e.g. Google translate can only translate between
59 languages). Wikipedia infoboxes are a highly
structured form of data and are composed of a set
of subject-attribute-value triples that summarize or
highlight the key features of the concept or sub-
ject of each article. A large number of instance-
centered knowledge-bases that have harvested this
structured data are available. The most well-known
are probably DBpedia (Auer et al, 2007), Free-
base (Bollacker et al, 2007) and YAGO (Suchanek
et al, 2007). However, almost all of these ex-
isting knowledge bases contain only one language.
Even for high-density languages, more than 70% of
Wikipedia pages and their infobox entries do not
contain cross-lingual links.
1http://meta.wikimedia.org/wiki/List_of_
Wikipedias
43
Recent research into Knowledge Base Population,
the automatic generation of profiles for named enti-
ties from unstructured text has raised the possibility
of automatic infobox generation in many languages.
Cross-lingual links between entities in this setting
would require either expensive multilingual human
annotation or automatic name pairing. We hypoth-
esize that overlaps in information across languages
might allow automatic pairing of profiles, without
any preexisting translational capabilities. Wikipedia
infoboxes provide a proxy for these high quality
cross lingual automatically generated profiles upon
which we can explore this hypothesis.
In this paper we propose a simple and general un-
supervised approach to discover name translations
from knowledge bases in any language pair, using
Wikipedia infoboxes as a case study. Although dif-
ferent languages have different writing systems, a
vast majority of the world?s countries and languages
use similar forms for representing information such
as time/calendar date, number, website URL and
currency (IBM, 2010). In fact most languages com-
monly follow the ISO 8601 standard2 so the formats
of time/date are the same or very similar. Therefore,
we take advantage of this language-independent for-
matting to design a new and simple bootstrapping
based name pair mining approach. We start from
language-independent expressions in any two lan-
guages, and then extract those infobox entries which
share the same slot values. The algorithm itera-
tively mines more name pairs by utilizing these pairs
and comparing other slot values. In this unsuper-
vised manner we don?t need to start from any name
transliteration module or document-wise temporal
distributions as in previous work.
We conduct experiments on English and Chinese
as we have bi-lingual annotators available for eval-
uating results. However, our approach does not re-
quire any language-specific knowledge so it?s gen-
erally applicable to any other language pairs. We
also compare our approach to state-of-the-art name
translation mining approaches.
1.1 Wikipedia Statistics
A standard Wikipedia entry includes a title, a docu-
ment describing the entry, and an ?infobox? which
2http://en.wikipedia.org/wiki/ISO_8601
is a fixed-format table designed to be added to
the top right-hand corner of the article to con-
sistently present a summary of some unifying at-
tributes (or ?slots?) about the entry. For example,
in the Wikipedia entry about the singer ?Beyonce
Knowles?, the infobox includes information about
her birth date, origin, song genres, occupation, etc.
As of November 2010, there were 10,355,225 En-
glish Wikipedia entries, and 772,826 entries. Only
27.2% of English Wikipedia entries have cross-
lingual hyperlinks referring to their corresponding
Chinese entries.
Wikipedia entries are created and updated expo-
nentially (Almeida et al, 2007) because of the in-
creasing number of contributors, many of whom are
not multi-lingual speakers. Therefore it is valuable
to align the cross-lingual entries by effective name
mining.
1.2 Motivating Example
Figure 1: A Motivating Example
Figure 1 depicts a motivating example for our ap-
proach. Based on the assumption that if two per-
son entries had the same birth date and death date,
44
they are likely to be the same person, we can find
the entity pair of (Michael Jackson /???.???).
We can get many name pairs using similar language-
independent clues. Then starting from these name
pairs, we can iteratively get new pairs with a large
portion of overlapped slots. For example, since
??????? and ?The Jackson 5? share many slot
values such as ?member? and ?years active?, they
are likely to be a translation pair. Next we can use
the new pair of (The Jackson 5 / ?????) to
mine more pairs such as ?????? and ?Steeltown
Records.?
2 Data and Pre-Processing
Because not all Wikipedia contributors follow the
standard naming conventions and date/number for-
mats for all languages, infoboxes include some
noisy instances. Fortunately the NIST TAC Knowl-
edge Base Population (KBP) task (Ji et al, 2010) de-
fined mapping tables which can be directly used to
normalize different forms of slot types3. For exam-
ple, we can group ?birthdate?, ?date of birth?, ?date-
birth? and ?born? to ?birth date.? In addition, we also
normalized all date slot values into one standard for-
mat as ?YYYY MM DD.? For example, both ?1461-
8-5? and ?5 August, 1461? are normalized as ?1461
08 05.? Only those Wikipedia entries that have at
least one slot corresponding to the Knowledge Base
Population task are used for name mining. Entries
with multiple infoboxes are also discarded as these
are typically ?List of ? entries and do not corre-
spond to a particular named entity. The number of
entries in the resulting data set are shown in Table 1.
The set of slots were finally augmented to include
the entry?s name as a new slot. The cross-lingual
links between Chinese and English Wikipedia pages
were used as the gold standard that the unsupervised
algorithm attempted to learn.
Language Entries Slot Values E-Z Pairs
English (E) 634,340 2,783,882 11,109Chinese (Z) 21,152 110,466
Table 1: Processed Data Statistics
3It is important to note that the vast majority of Chinese
Wikipedia pages store slot types in English in the underlying
wiki source, removing the problem of aligning slot types be-
tween languages.
3 Unsupervised Name Pair Mining
The name pair mining algorithm takes as input a set
of English infoboxes E and Chinese infoboxes Z.
Each infobox consists of a set of slot-value pairs,
where each slot or value may occur multiple times in
a single infobox. The output of the algorithm is a set
of pairs of English and Chinese infoboxes, match-
ing an infobox in one language to the corresponding
infobox in the other language. There is nothing in-
herently designed in the algorithm for English and
Chinese, and this method could be applied to any
language pair.
Because the algorithm is unsupervised, it begins
with no initial pairs, nor is there any initial trans-
lation lexicon between the two languages. As the
new pairs are learned, both the entries titles and the
values of their infoboxes are used to generate new
translations which can be used to learn more cross-
lingual name pairs.
3.1 Search Algorithm
The name pair mining algorithm considers all pairs
of English and Chinese infoboxes4, assigns a score,
described in Section 3.2, to each pair and then greed-
ily selects the highest scoring pairs, with the follow-
ing constraints:
1. Each infobox can only be paired to a single in-
fobox in the other language, with the highest
scoring infobox being selected. While there are
some instances of two entries in one language
for one entity which both have translation links
to the same page in another language, these are
rare occurrences and did not occur for the KBP
mapped data used in these experiments.
2. An pair (e, z) can only be added if the score
for the pair is at least 95%5 percent higher than
the score for the second best pair for both e and
z. This eliminates the problem of ties in the
data, and follows the intuition that if there are
4The algorithm does not need to compare all pairs of in-
foboxes as the vast majority will have a score of 0. Only those
pairs with some equivalent slot-value pairs need to be scored.
The set of non-zero scoring pairs can thus be quickly found by
indexing the slot-value pairs.
5The value of 95% was arbitrarily chosen; variations in this
threshold produce only small changes in performance.
45
multiple pairs with very similar scores it is ben-
eficial to postpone the decision until more evi-
dence becomes available.
To improve the speed of the algorithm, the top 500
scoring pairs, that do not violate these constraints,
are added at each iteration. The translation lexicon
is then updated. The translation lexicon is updated
each iteration from the total set of pairs learned us-
ing the following procedure. For each pair (e, z) in
the learned pairs, new translations are added for each
of the following conditions:
1. A translation of the name of e to the name z is
added.
2. If a slot s in e has one value, ve, and that slot
in z has one value, vz , a translation ve ? vz is
added.
3. If a slot s has multiple values in e and z, but all
but one of these values, for both e and z, have
translations to values in the other entry, then a
translation is learned for the resulting untrans-
lated value.
These new translations are all given equal weight
and are added to the translation lexicon even if the
evidence for this translation occurs in only a sin-
gle name pair6. These translations can be used to
align more name pairs in subsequent iterations by
providing more evidence that a given pair should be
aligned. After a translation is learned, we consider
the English side to be equivalent to the Chinese side
when scoring future infobox pairs.
The algorithm halts when there are no longer any
new name pairs with non-zero score which also sat-
isfy the search constraints described above.
3.2 Scoring Function
A score can be calculated for the pairing of an En-
glish infobox, e and a Chinese infobox, z according
to the following formula:
?
s?slots
{
IZ(s) + IE(s) ?v1, v2 : z.s.v1 ? e.s.v2
0 otherwise
(1)
6Assigning a probability to each translation learned based
upon the number of entries providing evidence for the transla-
tion could be used to further refine the predictions of the model,
but was not explored in this work.
A slot-value pair in Chinese, z.s.v1, is considered
equivalent to a slot-value pair in English, e.s.v2, if
the values are the same (typically only the case with
numerical values) or if there is a known translation
from v1 to v2. These translations are automatically
learned during the name-mining process. Initially
there are no known translations between the two lan-
guages.
The term IL(s) in equation 1 reflects how infor-
mative the slot s is in either English (E) or Chinese
(Z), and is calculated as the number of unique val-
ues for that slot for that language divided by the to-
tal number of slot-value pairs for that language, as
shown in equation 2.
IL(slot s) =
|{v|i ? L ? ?i.s.v}|
|{i.s.v|i ? L}|
(2)
If a slot s contains unique values such that a slot
and value pair is never repeated then IL(s) is 1.0
and indicates that the slot distinguishes entities very
well. Slots such as ?date of birth? are less infor-
mative since many individuals share the same birth-
date, and slots such as ?origin? are the least informa-
tive since so many people are from the same coun-
tries. A sampling of the IL(s) scores is shown in
Table 2. The slots ?origin? and ?religion? are the two
lowest scoring slots in both languages, while ?in-
fobox name? (the name of wikipedia page in ques-
tion), ?website?, ?founded? are the highest scoring
slot types.
Slot IZ IE
origin 0.21 0.03
religion 0.24 0.08
parents 0.57 0.60
date of birth 0.84 0.33
spouse 0.97 0.86
founded by 0.97 0.94
website 0.99 0.96
infobox name 1.00 1.00
Table 2: Sample I(s) Values
4 Evaluation
In this section we present the evaluation results of
our approach.
46
4.1 Evaluation Method
Human evaluation of mined name pairs can be dif-
ficult as a human assessor may frequently need to
consult the infoboxes of the entries along with con-
textual documents to determine if a Chinese entry
and an English entry correspond to the same en-
tity. This is especially true when the translations are
based on meanings instead of pronunciations. An al-
ternative way of mining name pairs from Wikipedia
is to extract titles from a Chinese Wikipedia page
and its corresponding linked English page if the link
exists (Ji et al, 2009). This method results in a
very high precision but can miss pairs if no such
link between the pages exists. We utilized these
cross-lingual page links as an answer key and then
only performed manual evaluation, using a bilingual
speaker, on those pairs generated by our algorithm
that were not in the answer key.
4.2 Results
Figure 2 shows the precision, recall and f-score of
the algorithm as it learns more pairs. The final
output of the mining learned 8799 name pairs, of
which 7562 were correct according to the cross-
lingual Wikipedia links. This results in a precision
of 85.94%, a recall of 68.07% and a F1 score of
75.9%. The precision remains above 95% for the
first 7,000 name pairs learned. If highly precise an-
swers are desired, at the expense of recall, the algo-
rithm could be halted earlier. The translation lexicon
contained 18,941 entries, not including translations
learned from the entry names themselves.
Assessment Number
Link Missing From Wikipedia 35 2.8%
Same Name, Different Entity 17 1.4%
Partially Correct 98 7.9%
Incorrect 1,087 87.9%
Table 3: Human Assessment of Errors
Because the answer key for name mining is au-
tomatically extracted from the cross-lingual links
in Wikipedia, it is possible that correct name pairs
could be missing from the answer key if no cross-
lingual link exists. To examine if any such pairs
were learned, a manual assessment of the name pairs
that were not in the answer key was performed, as
shown in Table 4.2. This assessment was performed
by bilingual speakers with an inter-annotator agree-
ment rate of 93.75%.
The vast majority, 87.9%, of the presumably er-
roneous name pairs assessed that were missing from
the answer-key were actually incorrect pairs. How-
ever, 35, or 2.8%, of the name pairs were actually
correct with their corresponding Wikipedia pages
lacking cross-lingual links (these corrections are
not reflected in the previous results reported above,
which were based solely on the pairs in the an-
swer key). For a small portion, 1.4%, of the errors,
the name translation is correct but the entries actu-
ally refer to different entities with the same name.
One such example is (Martin Rowlands / ???).
The English entity, ?Martin Rowlands? is an ath-
lete (an English football player), while the Chinese
entity is a former Hong Kong government official,
whose name translates to English as ?Martin Row-
lands?, as revealed on his Wikipedia page. Neither
entity has an entry in the other language. The fi-
nal category are partially correct answers, such as
the pair (Harrow, London / ???), where the En-
glish entry refers to an area within the London Bor-
ough of Harrow, while the Chinese entry refers to
the London Borough of Harrow as a whole. The
English entry ?Harrow, London? does not have a
corresponding entry in Chinese, although there is
an entry in both language for the larger Borough it-
self. All of these cases represent less 15% of the
learned name pairs though as 85.94% of the name
pairs were already determined to be correct based
on cross-lingual Wikipedia links.
Judgement Percent
Correct 64.4%
Partial 18.4%
Incorrect 15.1%
Not Translations 2.1%
Table 4: Slot Value Translation Assessment from Ran-
dom Sample of 1000
The name mining algorithm bootstraps many
name pairs by using possible translations between
the slot values in previously learned pairs. The fi-
nal translation lexicon learned had 18,941 entries.
A random sample of 1,000 entries from the trans-
47
Figure 2: Performance of Unsupervised Name Mining
lation lexicon was assessed by a human annotator,
and judged as correct, partial, incorrect or not trans-
lations, as shown in Table 4.2. Partial translations
were usually cases where a city was written with
its country name in language and as just the city
name in the other languages, such as ?Taipei Taiwan
Republic of China? and ????? (Taipei). Cases
are marked as ?not translations? if both sides are in
the same language, typically English, such as ?Eric
Heiden? in English being considered a translation of
?Eric Arthur Heiden? from a Chinese entry (not in
Chinese characters though). This normally occurs if
the Chinese page contained English words that were
not translated or transliterated.
An example7 of the name mining is shown in Fig-
ure 3, where the correct name pair for (George W.
Bush / ????????) is learned in iteration i,
is mined for additional translations and then pro-
vides evidence in iteration i+1 for the correct name
pair (Laura Bush / ?????????). When
learning the name pair for ?George W. Bush?, ev-
idence is first found from the slots marked as equiv-
alent (approx). Translations for ?Harvard Busi-
ness School? and ? Republican Party? were learned
in previous iterations from other name pairs and
now provide evidence, along with the identical val-
ues in the ?date of birth? slot for the pair (George
W. Bush / ????????). After learning this
7Many slot value pairs that were not relevant for the calcu-
lation are not shown to save space. Otherwise, this example is
as learned in the unsupervised name mining.
pair, new translations are extracted from the pair
for ?George W. Bush?, ?George Walker Bush?,
?President of the United States?, ?Laura Bush?,
and ?Yale University?. The translations for ?Laura
Bush? and ?George W. Bush? provide crucial in-
formation in the next iteration that the pair (Laura
Bush / ?????????) is correct. From this,
more translations are learned, although not all of
these translations are fully correct, such as ?Author
Teacher Librarian First Lady? which is now pos-
tulated to be a translation of ????? (Librar-
ian), which is only partially true, as the other pro-
fessions are not represented in the translation. While
such translations may not be fully correct, they still
could prove useful for learning future name pairs (al-
though this is unlikely in this case since there are
very few entries with ?first lady? as part of their ti-
tle.
5 Discussion
Besides retaining high accuracy, the final list of
name pairs revealed several advantages of our ap-
proach.
Most previous name translation methods are lim-
ited to names which are phonetically transliterated
(e.g. translate Chinese name ???? (You shen
ke)? to ?Yushchenko? in English). But many other
types of names such as organizations are often ren-
dered semantically, for example, the Chinese name
????? (jie fang zhi hu)? is translated into ?Lib-
eration Tiger? in English. Some other names in-
48
Iteration i
George W. Bush ???????? (George Walker Bush)
alt names George Walker Bush alt names ?????? (George Bush)
title President of the United States title ???? (President of the
USA)
date of birth 1946-7-6 ? date of birth 1946-7-6
member of Republican Party ? member of ??? (Republican Party)
spouse Laura Bush spouse ????????? (Laura
Welch Bush)
schools attended Yale University schools attended ???? (Yale University)
schools attended Harvard Business School ? schools attended ????? (Harvard Business
School)
Iteration i + 1
Laura Bush ????????? (Laura Welch Bush)
alt names Laura Bush ? alt names ????????? (Laura
Welch Bush)
alt names ????????? (Laura
Lane Welch)
date of birth 1946-11-4 ? date of birth 1946-11-4
place of birth Midland Texas place of birth ???????? (Texas
Midland)
title Author Teacher Librarian First
Lady
title ????? (Librarian)
title First Lady of the United States ? title ??????(First Lady of
USA)
spouse George W. Bush ? spouse ???????? (George
Walker Bush)
Figure 3: Example of Learned Name Pairs with Gloss Translations in Parentheses
volve both semantic and phonetic translations, or
none of them. Our approach is able to discover all
these different types, regardless of their translation
sources. For example, our approach successfully
mined a pair (Tarrytown / ???) where ?Tarry-
town? is translated into ????? neither by its pro-
nunciation ?bai you cun? nor its meaning ?tar vil-
lage.?
Name abbreviations are very challenging to trans-
late because they need expansions based on con-
texts. However our approach mined many abbrevia-
tions using slot value comparison. For example, the
pair of (Yctc /????) was successfully mined al-
though its English full name ?Yeh-Chiang Technol-
ogy Corp.? did not appear in the infoboxes.
Huang (2005) also pointed out that name transla-
tion benefited from origin-specific features. In con-
trast, our approach is able to discover name pairs
from any origins. For example, we discovered the
person name pair (Seishi Yokomizo / ????) in
which ?Seishi Yokomizo? was transliterated based
on Japanese pronunciation.
Furthermore, many name translations are context
dependent. For example, a person name in Chinese
?????????? could be translated into ?Yasser
Arafat? (PLO Chairman) or ?Yasir Arafat? (Crick-
eter) based on different contexts. Our method can
naturally disambiguate such entities based on slot
comparison at the same time as translation mining.
More importantly, our final list includes a large
portion of uncommon names, which can be valu-
able to address the out-of-vocabulary problem in
both MT and cross-lingual information processing.
Especially we found many of them are not in the
name pairs mined from the cross-lingual Wikipedia
title links, such as (Axis Communications / ???),
(Rowan Atkinson / ??????), (ELSA Technol-
ogy /?????) and (Nelson Ikon Wu /???).
49
6 Comparison with Previous Methods and
Resources
There have been some previous methods focusing on
mining name translations using weakly-supervised
learning. In addition there are some existing name
translation gazetteers which were manually con-
structed. We duplicated a variety of alternative
state-of-the-art name translation mining methods
and mined some corresponding name pair sets for
comparison. In fact we were able to implement the
techniques in previous approaches but could not du-
plicate the same number of results because we could
not access the same data sets. Therefore the main
purpose of this experiment is not to claim our ap-
proach outperforms these existing methods, rather
to investigate whether we can mine any new infor-
mation on top of these methods from reasonable
amounts of data.
1. Name Pair Mining from Bitexts
Within each sentence pair in a parallel cor-
pus, we ran an HMM based bilingual name
tagger (references omitted for anonymous re-
view). If the types of the name tags on both
sides are identical, we extract the name pairs
from this sentence. Then at the corpus-wide
level, we count the frequency for each name
pair, and only keep the name pairs that are fre-
quent enough. The corpora used for this ap-
proach were all DARPA GALE MT training
corpora.
2. Comparable Corpora
We implemented an information extraction
driven approach as described in Ji (2009) to
extract name pairs from comparable corpora.
This approach is based on extracting infor-
mation graphs from each language and align
names by a graph traverse algorithm. The cor-
pora used for this approach were 2000 English
documents and 2000 Chinese documents from
the Gigaword corpora.
3. Using patterns for Web mining
We constructed heuristic patterns such as par-
enthetical structure ?Chinese name (English
name)? (Lin et al, 2008) to extract name pairs
from web data with mixed Chinese and En-
glish. We used about 1,000 web pages for this
experiment.
4. Bilingual Gazetteer
We exploited an LDC bilingual name dictio-
nary (LDC2005T34) and a Japanese-English
person name dictionary including 20126
Japanese names written in Chinese charac-
ters (Kurohashi et al, 1994).
5. ACE2007 Entity Translation Training Data
We also used ACE 2007 entity translation train-
ing corpus which includes 119 Chinese-English
document pairs.
Table 5 shows the number of correct and unique
pairs mined pairs from each of the above ap-
proaches, as well as how these name mining meth-
ods can be augmented using the infobox name min-
ing described in this paper. The names mined from
our approach greatly extend the total number of cor-
rect translations with only a small number of con-
flicting name translations.
7 Related Work
Most of the previous name translation work com-
bined supervised transliteration approaches with
Language Model based re-scoring (Al-Onaizan and
Knight, 2002; Huang et al, 2004; Huang, 2005).
Our goal of addressing name translation for a large
number of languages is similar to the panlingual lex-
ical translation project (Etzioni et al, 2007). Some
recent research used comparable corpora to re-score
name transliterations (Sproat et al, 2006; Klemen-
tiev and Roth, 2006) or mine new word transla-
tions (Udupa et al, 2009; Ji, 2009; Fung and Yee,
1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al,
2007). However, most of these approaches needed
large amount of seeds and suffered from informa-
tion extraction errors, and thus relied on phonetic
similarity or document similarity to re-score candi-
date name translation pairs.
Some recent cross-lingual information access
work explored attribute mining from Wikipedia
pages. For example, Bouma et al (2009) aligned at-
tributes in Wikipedia infoboxes based on cross-page
links. Navigli and Ponzetto (2010) built a multi-
lingual semantic network by integrating the cross-
lingual Wikipedia page links and WordNet. Ji et
50
# Name Infobox Mining
Method Pairs # New # Conflicting
Automatic
(1) Bitexts 2,451 8,673 78
(2) Comparable Corpora 288 8,780 13
(3) Patterns for Web Mining 194 8799 0
Manual (4) Bilingual Gazetteer 59,886 8,689 74(5) ACE2007 Training Data 1,541 8,718 52
Table 5: Name Pairs Mined Using Previous Methods
al. (2009) described various approaches to auto-
matically mine name translation pairs from aligned
phrases (e.g. cross-lingual Wikipedia title links)
or aligned sentences (bi-texts). G et al (2009)
mined candidate words from Wikipedia and vali-
dated translations based on parallecl corpora. Some
other work mined name translations from mono-
lingual documents that include foreign language
texts. For example, Lin et al (2008) described a
parenthesis translation mining method; You et al
(2010) applied graph alignment algorithm to ob-
tain name translation pairs based on co-occurrence
statistics. This kind of data does not commonly exist
for low-density languages. Sorg and Cimiano (2008)
discovered cross-lingual links between English and
German using supervised classification based on
support vector machines. Adar et al (2009) aligned
cross-lingual infoboxes using a boolean classifier
based on self-supervised training with various lin-
guistic features. In contrast, our approach described
in this paper is entirely based on unsupervised learn-
ing without using any linguistic features. de Melo
and Weikum (2010) described an approach to detect
imprecise or wrong cross-lingual Wikipedia links
based on graph repair operations. Our algorithm can
help recover those missing cross-lingual links.
8 Conclusion and Future Work
In this paper we described a simple, cheap and ef-
fective self-boosting approach to mine name trans-
lation pairs from Wikipedia infoboxes. This method
is implemented in a completely unsupervised fash-
ion, without using any manually created seed set,
training data, transliteration or pre-knowledge about
the language pair. The underlying motivation is
that some certain expressions, such as numbers and
dates, are written in language-independent forms
among a large majority of languages. Therefore our
approach can be applied to any language pairs in-
cluding low-density languages as long as they share
a small set of such expressions. Experiments on
English-Chinese pair showed that this approach is
able to mine thousands of name pairs with more
than 85% accuracy. In addition the resulting name
pairs can be used to significantly augment the results
from existing approaches. The mined name pairs are
made publicly available.
In the future we will apply our method to mine
other entity types from more language pairs. We
will also extend our name discovery method to all
infobox pairs, not just those that can be mapped
into KBP-like slots. As a bi-product, our method
can be used for automatic cross-lingual Wikipedia
page linking, as well as unsupervised translation lex-
icon extraction, although this might require confi-
dence estimates on the translations learned. Once
our approach is applied to a panlingual setting (most
languages on the Wikipedia), we can also utilize
the voting results across multiple languages to au-
tomatically validate information or correct poten-
tial errors in Wikipedia infoboxes. Finally, as au-
tomatic name profile generation systems are gener-
ated cross-lingually, our method could be attempted
to automatic cross-lingual mappings between enti-
ties.
Acknowledgement
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
Number W911NF-09-2-0053, the U.S. NSF CA-
REER Award under Grant IIS-0953149 and PSC-
CUNY Research Program. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as repre-
51
senting the official policies, either expressed or im-
plied, of the Army Research Laboratory or the U.S.
Government. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
ment purposes notwithstanding any copyright nota-
tion hereon.
References
Eytan Adar, Michael Skinner, and Daniel S. Weld. 2009.
Information arbitrage across multi-lingual wikipedia.
In Second ACM International Conference on Web
Search and Data Mining (WSDM?09), Barcelona,
Spain, February 2009, February.
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In ACL 2002.
Rodrigo B. Almeida, BarzanMosafari, and Junghoo Cho.
2007. On the evolution of wikipedia. In Int. Conf. on
Weblogs and Social Media.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
Dbpedia: A nucleus for a web of open data. In The 6th
International Semantic Web Conference.
Kurt Bollacker, Robert Cook, and Patrick Tufts. 2007.
Freebase: A shared database of structured general hu-
man knowledge. In The National Conference on Arti-
ficial Intelligence (Volume 2).
Gosse Bouma, Sergio Duarte, and Zahurul Islam. 2009.
Cross-lingual alignment and complettion of wikipedia
templates. In The Third International Workshop on
Cross Lingual Information Access: Addressing the In-
formation Need of Multilingual Societies.
Gerard de Melo and Gerhard Weikum. 2010. Untangling
the cross-lingual link structure of wikipedia. In 48th
Annual Meeting of the Association for Computational
Linguistics (ACL 2010), Uppsala, Sweden.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach for
translating new words from nonparallel and compara-
ble texts. In COLING-ACL.
Rohit Bharadwaj G, Niket Tandon, and Vasudeva Varma.
2009. An iterative approach to extract dictionaries
from wikipedia for under-resourced languages. In
Proc. ICON2010, February.
Ahmed Hassan, Haytham Fahmy, and Hany Hassan.
2007. Improving named entity translation by exploit-
ing comparable and parallel corpora. In RANLP.
Fei Huang, Stephan Vogel, and Alex Waibel. 2004. Im-
proving named entity translation combining phonetic
and semantic similarities. In HLT/NAACL2004.
Fei Huang. 2005. Cluster-specific name transliteration.
In HLT-EMNLP 2005.
IBM. 2010. Ibm globalization library.
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard Zens,
and Hermann Ney. 2009. Name translation for distil-
lation. Handbook of Natural Language Processing and
Machine Translation: DARPA Global Autonomous
Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, and Kira
Griffitt. 2010. An overview of the tac2010 knowledge
base population track. In Text Analytics Conference
(TAC2010).
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information net-
works. In ACL-IJCNLP 2009 workshop on Building
and Using Comparable Corpora (BUCC 2009): from
Parallel to Non-parallel Corpora.
Michael E. Krauss. 2007. Keynote-mass Language Ex-
tinction and Documentation: The Race Over Time. The
Vanishing Languages of the Pacific Rim. Oxford Uni-
versity Press.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of japanese
morphological analyzer juman. In The International
Workshop on Sharable Natural Language Resources
and pp.22-28.
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca. 2008. Mining parenthetical translations
from the web by word alignment. In ACL2008.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belnet: Building a very large multilingual semantic
network. In 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), Uppsala,
Sweden.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In ACL 1999.
Li Shao and Hwee Tou Ng. 2004. Mining new
word translations from comparable corpora. In COL-
ING2004.
Philipp Sorg and Philipp Cimiano. 2008. Enrich-
ing the crosslingual link structure of wikipedia - a
classification-based approach. In AAAI 2008 Work-
shop on Wikipedia and Artifical Intelligence, June.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowledge.
In The 16th International World Wide Web conference.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-
gadeesh Jagarlamudi. 2009. Mint: A method for ef-
fective and scalable mining of named entity transliter-
ations from large comparable corpora. In EACL2009.
Gae-won You, Seung won Hwang, Young-In Song, Long
Jiang, and Zaiqing Nie. 2010. Mining name transla-
tions from entity graph mapping. In EMNLP2010.
52
