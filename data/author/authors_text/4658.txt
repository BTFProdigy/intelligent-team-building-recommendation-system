Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1175?1181,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multilingual Deterministic Dependency Parsing Framework using Modi-
fied Finite Newton Method Support Vector Machines 
 
Yu-Chieh Wu Jie-Chi Yang Yue-Shi Lee 
Dept. of Computer Science and In-
formation Engineering 
Graduate Institute of Network 
Learning Technology 
Dept. of Computer Science and 
Information Engineering 
National Central University National Central University Ming Chuan University 
Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan 
bcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw lees@mcu.edu.tw 
  
Abstract 
In this paper, we present a three-step mul-
tilingual dependency parser based on a 
deterministic shift-reduce parsing algo-
rithm. Different from last year, we sepa-
rate the root-parsing strategy as sequential 
labeling task and try to link the neighbor 
word dependences via a near neighbor 
parsing. The outputs of the root and 
neighbor parsers were encoded as features 
for the shift-reduce parser. In addition, the 
learners we used for the two parsers and 
the shift-reduce parser are quite different 
(conditional random fields and the modi-
fied finite-Newton method support vector 
machines). We found that our method 
could benefit from the two-preprocessing 
stages. To speed up training, in this year, 
we employ the MFN-SVM (modified fi-
nite-Newton method support vector ma-
chines) which can be learned in linear 
time. The experimental results show that 
our method achieved the middle rank over 
the 23 teams. We expect that our method 
could be further improved via well-tuned 
parameter validations for different lan-
guages. 
1 Introduction 
The target of dependency parsing is to 
automatically recognize the head-modifier 
relationships between words in natural language 
sentences. Usually, a dependency parser can 
construct a similar grammar tree with the 
dependency graph. In this year, CoNLL-2007 
shared task (Nivre et al, 2007) focuses on 
multilingual dependency parsing based on ten 
different languages (Hajic et al, 2004; Aduriz et 
al., 2003; Mart? et al, 2007; Chen et al, 2003; 
B?hmova et al, 2003; Marcus et al, 1993; 
Johansson and Nugues, 2007; Prokopidis et al, 
2005; Czendes et al, 2005; Montemagni et al, 
2003; Oflazer et al, 2003) and domain adaptation 
for English (Marcus et al, 1993; Johansson and 
Nugues, 2007; Kulick et al, 2004; MacWhinney, 
2000; Brown, 1973) without taking the language-
specific knowledge into consideration. The 
ultimate goal of them is to design ideal 
multilingual and domain portable dependency 
parsing systems. 
To accomplish the multilingual and domain ad-
aptation tasks, we present a three-pass parsing 
model based on a shift-reducing algorithm (Ya-
mada and Matsumoto, 2003; Chang et al, 2006), 
namely, neighbor parsing, root relation parsing, 
and shift-reduce parsing. Our method favors exam-
ining the ?un-parsed? tokens, which incrementally 
shrink. At the beginning, the parsing direction is 
mainly determined by the amount of un-parsed 
tokens in the sentence with either forward or 
backward parse. In this step, the projective parsing 
method can be used to evaluate most of the non-
projective Treebank datasets. Once the direction is 
determined, the pseudo-projectivize transformation 
algorithm (Nivre and Nilsson, 2005) converts most 
non-projective training data into projective and 
decodes the parsed text into non-projective. Here-
after, both neighbor-parser and root-parser were 
trained to discovery additional features for the 
downstream shift-reduce parse model. We found 
that the two additional features could improve the 
performance. Subsequently, the modified shift-
reduce parsing algorithm starts to parse the final 
dependencies with two-pass processing, i.e., pre-
dict parse action and label the relations.  
1175
In the remainder of this paper, Section 2 de-
scribes the proposed parsing model, and Section 3 
lists the experimental settings and results. Section 
4 presents the discussion and analysis of our parser. 
In Section 5, we draw the future direction and con-
clusion. 
2 System Description 
Over the past decades, many state-of-the-art pars-
ing algorithm were proposed, such as head-word 
lexicalized PCFG (Collins, 1998), Maximum En-
tropy (Charniak, 2000), Maximum/Minimum 
spanning tree (MST) (McDonald et al, 2005), 
shift-reduce-based deterministic parsing (Yamada 
and Matsumoto, 2003; Chang et al, 2006; Nivre, 
2003). Among them, the shift-reduce methods 
were shown to be the most efficient method, which 
only costs at most 2n~3n actions to parse a sen-
tence (Chang et al, 2006; Nivre, 2003). Chang et 
al. (2006) further added the ?wait-right? action to 
the words that had children and could not be re-
duced in current state. This could avoid the so-
called ?too early reduce? problems.  
The overall parsing model can be found in Fig-
ure 1. Figure 2 illustrates the detail system spec of 
our parsing model. 
 
 
Figure 1: System architecture 
 
2.1 Neighbor Parser 
As shown in Figure 1, the first step is to identify 
the neighbor head-modifier relations between two 
consecutive words. Cheng et al (2006) also re-
ported that the use of neighboring dependency at-
tachment tagger enhance the unlabeled attachment 
scores from 84.38 to 84.6 for 13 languages. Usu-
ally, it is the case that the select features are fixed 
and could not be tuned to capture the second order 
features (McDonald et al, 2006). At each location, 
there the focus and next words are always com-
pared. It may fail to link the next and next+1 word 
pair since the next word might be reduced due to 
an earlier wrong decision.   
 
?. Parsing Algorithm:
  
1. Neighbor Parser 
2. Root Parser 
3. Shift-Reduce Algorithm (Yamada 
and Matsumoto, 2003) 
?. Parser Characteris-tics: 
 
1. Deterministic 
2. two-pass (Labeling separated) 
3. Pseudo-Projective en(de)-coding 
(Nivre and Nilsson, 2005) 
?. Learner: MFN-SVM 
(1) One-versus-All 
(2) Linear Kernel 
?. Feature Set: 
  
1. Lexical (Unigram/Bigram) 
2. Fine-grained POS (and BiPOS) 
3. Lemma/FEAT used 
?. Post-Processing: Non-Used 
?. Additional/External Resources: Non-Used 
Figure 2: System spec  
 
However, starting parsing based on the result of 
neighbor parsing is not a good idea since it could 
produce error propagation problems. Rather, we 
include the result of our neighbor parsing as fea-
tures to increase the original feature set. In the pre-
liminary study, we found that the derived features 
are very useful for most languages. 
As conventional sequential tagging problems, 
such part-of-speech tagging and phrase chunking, 
we employ the conditional random fields (CRF) as 
learners (Kudo et al, 2004). The basic idea of the 
neighbor parsing can be shown in Figure 3.  
The first and second colums in Figure 3 repre-
sents the basic word and fine-grained POS froms, 
while the third column indicates if this word has 
the LH (left-head) or RH (right-head) with associ-
ated relations or O (no neighbor head in either left 
or right neighbor word). The used features are:  
Word, fine-grained POS, bigram, and bi-POS with 
context window = 2(left) and 4(right) 
1176
 
Figure 3: Sequential tagging model for neighbor 
parse 
 
Unfortunately, for some languages, like Chi-
nese and Czech, training with CRF is because of 
the large number of features and the head relations. 
To make it practical, we focus on just three types: 
left head, right head, and out-of-neighbor. This 
effectively reduces most of the feature space for 
the CRF. The training time for the neighbor parser 
with only three categories is less than 5 minutes 
while it takes three days with taking all the relation 
tag into account. 
2.2 Root Parser 
After the neighbor parse, the tagged labels are 
good features for the root parse. In the second 
stage, the root parser identifies the root words in 
the sentence. Nevertheless, for some languages, 
such as Arabic and Czech, the roots might be sev-
eral types as against to Chinese and English in 
which the number of labels of roots is merely one. 
Similar to the neighbor parser, we also take the 
root label into account. As noted, for Chinese and 
English, the goal of the root parser can be reduced 
to determine whether the current word is root or 
not.  
 
 
Figure 4: Sequential tagging model for neighbor 
parse 
 
Similar to the neighbor parse, the root parsing 
task can also be treated as a sequential tagging 
problem. Figure 4 shows the basic concept of the 
root parser. The third column is mainly derived 
from the neighbor parser, while the fourth column 
represents whether the current word is a root with 
relation or not. 
2.3 Parsing Algorithm 
After adding the neighbor and root parser output as 
features, in the final stage, the modified Yamada?s 
shift-reduce parsing algorithm (Yamada and Ma-
tsumoto, 2003) is then run. This method is deter-
ministic and can deal with projective data only. 
There are three basic operation (action) types: Shift 
(S), Left (L), and Right (R). The operation is 
mainly determined via the classifier according to 
the selected features (see 2.4). Each time, the op-
eration is applied to two unparsed words, namely, 
focus and next. If there exists an arc between the 
two words (either left or right), then the head of 
focus or next word is found; otherwise (i.e., shift), 
next two words are considered at next stage. This 
method could be economically performed via 
maintaining two pointers, focus, and next without 
an explicit stack. The parse operation is iteratively 
run until no more relation can be found in the sen-
tence.  
In 2006, Chang et al (2006) further reported 
that the use of ?step-back? in comparison to the 
original ?stay?. Furthermore, they also add the 
?wait-left? operations to prevent the ?too early re-
duce? problems. In this way, the parse actions can 
be reduced to be bound in 3n where n is the num-
ber of words in a sentence. 
Now we compare the adopted parsing algorithm 
in this year to the one we employed last year (Wu 
et al, 2006a). The common characteristics are: 
 
1. the same number of parse operations (4) 
2. shift-reduce 
3. linearly scaled 
4. deterministic and projective 
 
On the contrary, their parse actions are quite dif-
ferent. Therefore these two methods have different 
run time. This gives the two methods rise to differ-
ent iterative times. The main reason is that the 
step-back might trace back to previous words, 
which can be viewed as pop the top words on the 
stack back to the unparsed strings, while the 
Nivre?s method does not trace-back any two words 
1177
in the stack. In other words, if a word is pushed 
into the stack, it will no longer be compared with 
the other deeper words inside the stack. Hence 
some of the non-root words in the stack remain to 
be parsed. A simple solution is to adopt an exhaus-
tive post-processing step for the unparsed words in 
the stack (details in (Wu et al, 2006a, 2006b)). 
A good advantage of the step-back is that it can 
trace back to the unparsed words in the stack. But 
theoretically, the required parse actions still more 
than the Nivre?s algorithm (2n vs. 3n). 
By adopting the projectivized en/de-coding over 
the modified Yamada?s algorithm, we can treat the 
words that do not have a parent as roots. Thus, for 
some languages (e.g. Czech and Arabic), the mul-
tiple root problem can be easily solved. In this year 
we separate the parse action and the relation label 
into two stages as opposed to having one pass last 
year. In this way, we can simply adopt a sequential 
tagger to auto-assign the relation labels after the 
whole sentence is parsed. 
2.4 Features and Learners 
Unlike last year, we did separate the action predic-
tion and the label recognition into two stages 
where the one of the learners could provide more 
information to another. The used features of the 
two learners are quite similar and listed as follows: 
 
Basic feature type (for previous 2 and next 3 words): 
Word, POS (fine-grained), Lemma, FEAT, NParse, 
RParse 
 
Enhanced feature type: 
Bigram, BiPOS for focus and next words 
previous two parse actions 
 
For label recognition: 
Label tag to its head, label tags for previous two 
words 
 
In this paper, we replicate and modify the modi-
fied finite Newton support vector machines (MFN-
SVM) (Keerthi and DeCoste, 2005) as the learner.  
The MFN-SVM is a very efficient SVM opti-
mization method which linearly scales with the 
number of training examples. Usually, the trained 
models from MFN-SVM are quite large that could 
not be processed in practice. We therefore defined 
the positive lower bound (10-10) and the negative 
upper bound (-10-10) to eliminate values that tend 
to be zero.  
However, the SVM is a binary classifier which 
only recognizes true or false. For multiclass prob-
lem, we use the so-called one-versus-all (OVA) 
method with linear kernel to combine the results of 
each individual classifier. The final class in testing 
phase is mainly determined by selecting the maxi-
mum similarity. 
For all languages, our parser uses the same set-
tings and features. For all the languages (except for 
Basque and Turkish), we use backward parsing 
direction to keep the un-parsed token rate low. 
3 Experimental Result 
3.1 Dataset and Evaluation Metrics 
The testing data is provided by the (Nivre et al, 
2007) which consists of 10 language treebanks. 
More detailed descriptions of the dataset can be 
found at the web site1. The experimental results are 
mainly evaluated by the unlabeled and labeled at-
tachment scores. CoNLL also provided a perl 
script to automatic compute these rates. 
3.2 Results 
Table 1 presents the overall parsing performance 
of the 10 languages. As shown in Table 1, we list 
two parsing results at column B and column C 
(new and old). It is worth to note that the result B 
is produced by training the neighbor parser with 
full labels instead of the three categories, 
left/right/out-of-neighbor. A is the official pro-
vided parse results. Some of the parsing results in 
A did not include the enhanced feature type and 
neighbor/root parses due to the time limitation. For 
the domain adaptation task, we directly use the 
trained English model to classify the PChemtb and 
CHILDES corpora without further adjustment. 
In addition, we also apply the Maltparser 0.4, 
which is implemented with the Nivre?s algorithm 
(Nivre et al, 2006) to be compared. The Maltpaser 
also includes the SVM and memory-based learner 
(MBL). Nevertheless, the training time complexity 
of the SVM in Maltparser is not linear time as 
MFN-SVM. Therefore we use the default MBL 
and feature model 3 (M3) in this experiment. To 
make a fair comparison, the input training data was 
also projectivized through the same pseudo-
projective encoding/decoding methods.  
                                                          
1 http://nextens.uvt.nl/depparse-wiki/SharedTaskWebsite 
1178
To perform the significant test, we evaluate the 
statistical difference among the three results. If the 
answer is ?Yes?, it means the two systems are sig-
nificant difference under at least 95% confidence 
score (p < 0.05). 
The final column of the Table 1 lists the non-
root words unparsed rate of the modified Ya-
mada?s method and the Nivre?s parsing model 
which we employed last year. Among 10 lan-
guages, we can find that the modified Yamada?s 
method outperform our old method in five lan-
guages, while fail to win in three languages. We 
did not report the comparative study between the 
forward parsing and backward parsing directions 
here since only the two languages (Basque and 
Turkish) were better in performing forward direc-
tion. 
4 Discussion 
Now we turn to discuss the improvement of the use 
of the neighbor parse and root parse. All of the ex-
periments were conducted by additional runs 
where we removed the neighbor and root parse 
outputs from the feature set. In this experiment, we 
report four representative languages that tend to 
achieve the best and worst improvements. Table 2 
lists the comparative study of the four languages. 
As listed in Table 2, both English and Chinese 
got substantial benefit from the use of the two 
parsers. As observed by (Isozaki et al, 2004), in-
corporating both top-down (root find) and bottom-
up (base-NP) can yield better improvement over 
the Yamada?s parsing algorithm. Thus, instead of 
pre-determining the root and base-phrase structures, 
the tagging results of the neighbor and root parsers 
were included as new features to add wider infor-
mation for the shift-reduce parser. It is also inter-
esting to link neighbors and determine the root 
before parsing. We plan to compare it with out 
method in the future.  
 
Table 2: The effective of the used Neighbor/Root 
Parser in the selected four languages 
 With N/R Parser Without 
Chinese 79.29 75.51 
English 84.27 79.49 
Basque 72.26 72.32 
Turkish 75.65 76.60 
 
On the other hand, we also found that 2 out of 
the 10 languages had been negatively affected by 
the neighbor and root parsers. In Basque they made 
a marginally negative improvement, and in the 
Turkish the two parsers did decrease the original 
parsing models. We further observed that the main 
cause is that the weak performance of the neighbor 
parser. In Turkish, the recall/precision rates of the 
neighbor dependence are 92.61/93.12 with include 
neighbor parse outputs, while it achieved 
93.71/93.51 with purely run the modified Ya-
mada?s method. We can expect that the result 
could achieve higher LAS score when the neighbor 
parser is improved. As mentioned in section 2.1, 
2.2, the selected features for the two parsers are 
unified for the 10 languages. It is not surprising 
Table 1: A general statistical table of labeled attachment score, test and un-parsed rate (percentage) 
Statistic test  Un-Parsed Rate Language A (Official) 
B 
(Corrected)
C 
(Malt-Parser 0.4) A vs B A vs C B vs C Old New 
Arabic 66.16 70.71 56.67 Yes No Yes 1.08% 0.69%
Basque 70.71 72.26 57.79 Yes Yes Yes 3.04% 3.72%
Catalan 81.44 81.44 76.36 Yes No No 0.45% 0.27%
Chinese 74.69 79.29 68.15 Yes Yes Yes 0.00% 0.00%
Czech 66.72 70.24 56.96 Yes No Yes 4.17% 3.87%
English 79.49 84.27 75.53 Yes Yes Yes 1.66% 0.84%
Greek 70.63 77.64 58.81 No Yes Yes 2.26% 2.12%
Hungarian 69.08 71.98 59.41 Yes Yes Yes 3.88% 5.38%
Italian 78.79 78.38 74.08 Yes No Yes 0.63% 0.63%
Turkish 72.52 75.65 64.41 Yes Yes Yes 4.93% 5.54%
pchemtb_closed 55.31** 73.35 - - - - - -
*CHILDES_closed 52.89 58.29 - - - - - -
* The CHILDES data does not contain the relation tag, instead, the unlabeled attachment score is listed 
** The original submission of the pchemtb_closed task can not pass through the evaluator and hence is not the official score. After correcting 
the format problems, the actual LAS score should be 55.31. 
1179
that for certain data the fixed feature set might per-
form even worse than the original shift-reduce 
parser. A better way is to validate the features with 
variant settings for different languages. We left the 
feature engine task as future work. 
5 Conclusion and Future Remarks 
Multilingual dependency parsing investigates on 
proposing a general framework of dependence 
parsing algorithms. This paper presents and ana-
lyzes the impact of two preprocessing components, 
namely, neighbor parsing and root-parsing. Those 
two parsers provide very useful additional features 
for downstream shift-reduce parser. The experi-
mental results also demonstrated that the use of the 
two components did improve results for the se-
lected languages. In the error-analysis, we also ob-
served that for some languages, parameter tuning 
and feature selection is very important for system 
performance.  
In the future, we plan to report the actual per-
formance with replacing the MFN-SVM by the 
polynomial kernel SVM. In our pilot study, the use 
of approximate-polynomial kernel (Wu et al, 2007) 
outperforms the linear kernel SVM in Chinese and 
Arabic. Also, we are investigating how to convert 
the shift-reduce parser into approximate N-best 
parser efficiently. In this way, the parse reranking 
algorithm can be adopted to further improve the 
performance.  
References  
A. Abeill?, editor. 2003. Treebanks: Building and Using 
Parsed Corpora. Kluwer.  
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 2003. 
Construction of a Basque Dependency Treebank. In 
Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 2003. 
The PDT: a 3-level annotation scenario. In Abeill? 
(2003), chapter 7, 103?127. 
R. Brown. 1973. A First Language: The Early Stages. 
Harvard University Press. 
M. W. Chang, Q. Do, and D. Roth. 2006. Multilingual 
Dependency Parsing: A Pipeline Approach. In Recent 
Advances in Natural Language Processing, pages 
195-204. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementation. 
In Abeill? (2003), chapter 13, pages 231?248. 
Y. Cheng, M. Asahara and Y. Matsumoto. 2006. Multi-
lingual Dependency Parsing at NAIST. In Proc. of 
the 10th Conference on Natural Language Learning, 
pages 191-195. 
D. Czendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. Beska. 
2004. Prague Arabic Dependency Treebank: Devel-
opment in Data and Tools. In Proc. of the NEMLAR 
Intern. Conf. on Arabic Language Resources and 
Tools, pages 110?117. 
H. Isozaki; H. Kazawa; T. Hirao. 2004. A Deterministic 
Word Dependency Analyzer Enhanced With Prefer-
ence Learning. In Proc. of the 20th International 
Conference on Computational Linguistics, pages 
275-281. 
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. In 
Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
S. Keerthi and D. DeCoste. 2005. A modified finite 
Newton method for fast solution of large scale linear 
SVMs. Journal of Machine Learning Research. 6: 
341-361. 
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc- 
Donald, M. Palmer, A. Schein, and L. Ungar. 2004. 
Integrated annotation for biomedical information ex- 
traction. In Proc. of the Human Language 
Technology Conference and the Annual Meeting of 
the North American Chapter of the Association for 
Computational Linguistics (HLT/NAACL).  
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
R. McDonald, K. Lerman and F. Pereira. 2006. 
Multilingual Dependency Analysis with a Two-Stage 
Discriminative. In Proc. of the 10th Conference on 
Natural Language Learning, pages 216-220. 
1180
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, O. 
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
pages 189?210.  
J. Nivre. 2003. An efficient algorithm for projective 
dependency parsing. In Proc. of the International 
Workshop on Parsing Technology, pages 149-160. 
J. Nivre, and J. Nilsson. 2005. Pseudo-projective 
dependency Parsing. In Proc. of the 43rd Annual 
Meeting of the Association for Computational 
Linguistics (ACL), pages 99-106.  
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 
2006. Labeled pseudo-projective dependency parsing 
with support vector machines. In Proc. of the 10th 
Conference on Natural Language Learning, pages 
221-225. 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 shared 
task on dependency parsing. In Proc. of the Joint 
Conf. on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning (EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r. 
2003. Building a Turkish treebank. In Abeill? (2003), 
chapter 15, pages 261?277.  
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th Workshop 
on Treebanks and Linguistic Theories (TLT), pages 
149?160. 
T. Kudo, K, Yamamoto, and Y. Matsumoto. 2004. 
Appliying conditional random fields to Japanese 
morphological analysis, In Proc. of the 2004 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2004), pages 230-
237. 
Y. C. Wu, Y. S. Lee, and J. C. Yang. 2006a. The 
exploration of deterministic and efficient dependency 
parsing. In Proc. of the 10th Conference on 
Computational Natural Language Learning, pages 
241-245. 
Y. C. Wu, J. C. Yang, and Q. X. Lin. 2006b. 
Description of the NCU Chinese word segmentation 
and named entity recognition system for SIGHAN 
bakeoff 2006. In Proc. of the 5th SIGHAN Workshop 
on Chinese Language Processing, pages 209-212. 
Y. C. Wu, J. C. Yang, and Y. S. Lee. 2007. An Ap-
proximate Approach for Training Polynomial Kernel 
SVMs in Linear Time. In Proc. of the 45th Annual 
Meeting of the Association for Computational 
Linguistics (ACL), in press. 
H. Yamada and Y. Matsumoto. 2003. Statistical 
dependency analysis with support vector machines. 
In Proc. of the 8th International Workshop on 
Parsing Technologies, pages 195?206. 
 
1181
Coling 2008: Companion volume ? Posters and Demonstrations, pages 135?138
Manchester, August 2008
Robust and Efficient Chinese Word Dependency Analysis with     
Linear Kernel Support Vector Machines 
 
Yu-Chieh Wu 
Dept. of Computer Science and In-
formation Engineering 
National Central University 
Taoyuan, Taiwan 
bcbb@db.csie.ncu.edu.tw 
Jie-Chi Yang 
Graduate Institute of Network 
Learning Technology 
National Central University 
Taoyuan, Taiwan 
yang@cl.ncu.edu.tw 
Yue-Shi Lee 
Dept. of Computer Science and Infor-
mation Engineering 
Ming Chuan University 
Taoyuan, Taiwan 
{leeys}@mcu.edu.tw  
 
Abstract 
Data-driven learning based on shift reduce pars-
ing algorithms has emerged dependency parsing 
and shown excellent performance to many Tree-
banks. In this paper, we investigate the extension 
of those methods while considerably improved 
the runtime and training time efficiency via L2-
SVMs. We also present several properties and 
constraints to enhance the parser completeness in 
runtime. We further integrate root-level and bot-
tom-level syntactic information by using sequen-
tial taggers. The experimental results show the 
positive effect of the root-level and bottom-level 
features that improve our parser from 81.17% to 
81.41% and 81.16% to 81.57% labeled attach-
ment scores with modified Yamada?s and Nivre?s 
method, respectively on the Chinese Treebank. In 
comparison to well-known parsers, such as Malt-
Parser (80.74%) and MSTParser (78.08%), our 
methods produce not only better accuracy, but 
also drastically reduced testing time in 0.07 and 
0.11, respectively. 
1 Introduction 
With the late development of Chinese Treebank 
(Xue et al 2005), parsing Chinese is still an ongo-
ing research issue. The goal of dependency parsing 
is to find the head-modifier (labeled) relations in 
texts. Though some of the parsing algorithms are 
language independent and show state-of-the-art per-
formance on multilingual dependency Treebanks 
(Nivre et al, 2007; Buchholz and Marsi, 2006), they 
are often too slow for online purpose. Therefore, to 
develop an efficient and effective dependency 
parser is indispensable. 
Over the past few years, several research studies 
had addressed the use of shift-reduce and edge-
factored-based approaches attend fairly accurate 
performance in Chinese (Cheng et al, 2005; Hall, 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights 
reserved. 
2005; Wang et al, 2006). The former (shift-reduce) 
is a linear time algorithm, while the latter involves 
n3 for decoding where n is the length of sentence. 
Even the shift-reduce approaches seems to be very 
efficient, most studies (Hall et al, 2007; Nivre et al, 
2006) yet employ nonlinear kernel methods such as 
polynomial kernel support vector machines (SVMs). 
Furthermore, there is no research work directly 
compare with the two methods with Chinese Tree-
bank. Nevertheless, the empirical training and test-
ing time comparisons of those methods has not been 
reported yet. 
In this paper, we present an efficient and robust 
parser for Chinese based on linear classifiers and 
shift-reduce parsing algorithms. We propose several 
useful properties to enhance the completeness of the 
two well-known shift-reduce algorithms, namely 
Nivre?s shift reduce (NSR) (Nivre, 2003) and 
Yamada?s shift reduce (YSR) (Yamada and 
Matsumoto, 2003) algorithms. To enhance the 
performance, we add root and bottom (neighbor) 
information by adopting sequential taggers. We also 
perform experiments on the Chinese Treebank and 
compare with two of the state-of-the-art parsers. 
2 Parsing Algorithms 
At the beginning, we briefly review the selected two 
parsing algorithm as follows. The NSR makes use 
of four parse actions to incrementally construct the 
dependency graph. By following the same notations 
as (Nivre, 2003), NSR initializes with (S, I, A) = (? , 
W, ? ) where S is the stack (represented as a list), I 
is the queue initiated with all words, and A is the set 
of directed and labeled edges for the dependency 
graph. The stack is a list of words whose parent or 
child has not been found entirely. NSR incremen-
tally parses a pair of words (one is the top of the 
stack and the other is the first word of the queue) 
and uses four parse actions to construct the depend-
ency graph. The four parse actions are: {Left-Arc 
(LA), Right-Arc (RA), Shift, Reduce}. Both LA and 
RA could be parameterized with a dependence rela-
135
tion type. By parsing a pair of words step-by-step, 
the parser terminates when the queue is empty. 
Similar to NSR, YSR constructs the dependency 
graph by incrementally parse a pair of no_head 
words. The original YSR algorithm (Yamada and 
Matsumoto, 2003) makes use of three parse actions 
to parse a sentence left-to-right and involves n2 
parser transitions. Recently, Chang et al (2006) 
showed that by adding an extra parse action Wait-
Left and performing the ?step-back? operation can 
accomplish parse in linear time. The step-back 
means that after an action determined, the parse pair 
moves back with except for Shift action. Wait-Left is 
mainly proposed to wait the next word until all its 
right children having attached to heads. In this paper, 
we employ such modification to form our basic 
YSR algorithm. 
2.1 Useful Properties 
We give more formal definitions of the dependency 
graph as follows.  
Let R = {r1, r2, r3,?, rN} be the finite set of de-
pendency arc labels with N types. A dependency 
graph G =(W, A) where W is the string of words W = 
w1, w2, w3, etc. and A is the set of directed labeled 
arcs (wx, r, wy) where r?R, and wx, wy?W. For a 
parse pair wx and wy in a sentence, we introduce the 
following notation: 
1. wx?wy: wx is the head of wy, and wx?wy: wy is 
the head of wx. 
2. wx < wy: word wx is on the left hand side of 
word wy in the sentence. 
3. (wx, r, wy): denotes the word wy is the head of 
wx with relation r. 
Definition 1: Valid dependency graph 
A dependency graph G is well formed and valid iff 
the following conditions are true. 
  1. G is connected 
  2. G is acyclic (cycle free) 
  3. G is projective 
  4. For each node in G, there is only one parent ex-
cept the root word 
  5. G is a single rooted graph 
Definition 2: Parse pair 
When the parsing algorithm considers a pair of 
word (wx, wy), we name the pair ?parse pair?. 
Definition 1 gives the formal definition of a valid 
dependency graph. Condition 3 and condition 5 are 
not always true for all languages. For example, 
there are multiple roots in Arabic dependency Tree-
bank, while the dependency graph is usually non-
projective according to the linguistic characteristics. 
Fortunately the dependency graphs in Chinese are 
fully projective and single rooted and thus compati-
ble with Definition 1. 
However, we can not always assume the classi-
fier is perfect. During run-time, the classifier might 
make incorrect decision which leads to incorrect 
parse graph and even constructs an incomplete and 
invalid parse graph.  For example, for NSR it is usu-
ally retain more than two words that are not at-
tached to their heads in the stack. To solve it, we 
propose the following properties to enhance the 
completeness of the original NSR/YSR parsers. 
Definition 3: One word sentence 
If there is only one unparsed word, then it must be 
the root. 
Proposition 4: Constrained parsing I 
For a parse pair (wx, wy), if the head of wx is not 
found previously, then the parse action Reduce is 
invalid. 
Proof. The parse action Reduce will remove wx 
from the stack and leads to an unconnected and 
multiple roots dependency graph (violates definition 
1).  
Proposition 5: Unique pair parsing 
If there are only two unparsed words in G, then the 
parse action of this pair of words is limited to be 
{LA, RA}. 
Proof. Clearly if the parse action is Reduce, then it 
violates Proposition 4 (unconnected graph). Simi-
larly when applying Shift, the state does not change, 
i.e., there are still two unparsed words. Nonetheless, 
by applying either LA or RA, the two isolated words 
will be linked and gives a connected graph. 
Proposition 6: Constrained parsing II 
For a parse pair (wx, wy), if the head of wx is found, 
then the parse action RA is invalid. 
Proof. Assume the head of wx is wm. If the parser 
predicts RA, then it regards wy as the head of wx. 
Therefore it violates Definition 1 (for each word 
there should be at most one head in the sentence). 
On the other hand, actions Reduce and Shift do not 
change the structure of G and is intuitively valid 
parsing actions. In the case of LA, by adding the 
edge from wx to wy, the dependency graph does not 
violate definition 1. Thus, LA is also a valid action. 
Definition 3 is very common and intuitively seen 
in the case of one word parsing at the final stage. 
The Proposition 4 limits the parser actions that 
bring about a single-rooted dependency graph. 
Proposition 5 is particularly useful when there are 
only two unparsed words in the stack for the NSR. 
On the basis of the original NSR algorithm, the 
parse work is done when the input queue is empty. 
However, words will be shift and put onto the stack 
if their heads are not found currently. Finally if the 
queue is empty and these words are still retained in 
the stack, then it will produce multiple roots and 
lead to an unconnected graph. Proposition 6 is pro-
posed to avoid the case of multiple heads in the sen-
tence when there are two no-head words. To handle 
more than two words on the stack, the ?step-back? 
operation is used.  
136
Some of the above properties can also be applied 
to YSR with slightly modifications. We skip the 
details here owing to the space constraints. 
?
(chiuan)
?
(ace)
?
(class)
??
(heros)
,
(,)
??
(game)
????
(cliffhanger)
?
(.)
??
(league cup)
??
(struggle)
P
P
Sub
VMod
VMod
SubNMod AMod NMod
NMod
?
(guo)  
Figure 1: An example of Chinese dependency graph 
3 Root and Neighbor Information 
In general, the shift-reduce parsing algorithm in-
crementally parse a pair of words until the final 
parse graph has built. However, it is usually the case 
that when an error decision made at earlier stage, 
the real heads of the following words will be mis-
attached. In particular the head is nearby the current 
pair of words. Similarly if the root word is misclas-
sified as a child of other word, then the all nodes 
immediately modified by the root will attached to 
the wrong root.  
One solution to improve this problem is to en-
hance the root and bottom (neighbor) information 
during parse. To obtain such information, we em-
ploy the sequential taggers to predict. That is one 
sequential tagger learns to determine whether the 
current word is the child of its left/right word or 
none of them while the other is to recognize the root 
word. For example, if the word is labeled as ?left-
Mod?, then it means its left word is the head of it 
and the relation tag is ?Mod?. 
Finding root in Chinese is even simpler, since 
there is only one root word in the same sentence in 
Chinese Treebank. Here, we adopt the same tech-
nology to label the root by using sequential taggers. 
Such solution had also been applied to English 
Treebank where a polynomial kernel SVM was used 
(Isozaki et al, 2004). However, there are two differ-
ences to our method. First, we enable our root tag-
ger to incorporate bottom-level features. More pre-
cisely, the two taggers are cascaded combined. Sec-
ond, to enhance the top-level syntactic information, 
our root tagger does not only recognize the root 
word, but also the words which belong to the im-
mediate child of it. We give the following property 
to prove that attaching all root children to the root 
still leads to a valid dependency graph.  
Proposition 7: Cycle-free for root tagger 
The dependency graph is a cycle-free graph by link-
ing root child to the root words. 
Proof. The minimum cycle length in a valid de-
pendency graph is two (two edges for two words by 
linked each other). Assume there are K children for 
a root. By attaching all children to the root, it leads 
to the out-degree of each child is 1, while the in-
degree of the root is K. According to the Definition 
1, the root word does not have any parent (out-
degree of the root is exactly zero) and will never 
attach to any word in the sentence (include its chil-
dren).  
 
Figure 2: Attaching neighbor relations with sequental 
taggers 
 
Figure 3: Attaching root words with sequential taggers 
 
The sequential tagger used in this paper is 
CRF++ (Kudo et al, 2004). One advantage of con-
ditional random fields (CRF) is that it is a structural 
learning method and can search optimal tag se-
quence with efficient Viterbi search algorithm. 
Features used for the two taggers include word, 
part-of-speech tag, and prefix/suffix Chinese char-
acters with context window = 3. Features that oc-
curs less than twice in the training data is removed. 
Figure 1 shows an example of Chinese dependency 
graph. Figure 2 illustrates the sample of attaching 
neighbors with CRF++ by using the same sentence 
as in Figure 1. Figure 3 shows the example of iden-
tifying root and its children with CRF++. 
 
 Table 1: Feature set used for NSR and YSR 
Feature type Stack position Queue position 
Word
POS
BiWord
BiPOS
Neighbor (NSR)
Root (NSR)
Neighbor (YSR)
Root (YSR)
History
Child (Word)
Child (POS)
-1,0 
-1,0 
(-2,-1),(-1,0),(-2,0),(-1,+1) 
(-2,-1),(-1,0),(-2,0),(-1,+1) 
-2,-1,0 
-1,0 
0 
0 
-2,-1 
0 
0 
0,+1,+2,+3
0,+1,+2,+3
(0,1),(1,2),(2,3),(0,2),(1,3)
(0,1),(1,2),(2,3),(0,2),(1,3)
0,+1,+2
0
0
0
4 Experiments 
We randomly select 90% of the Chinese Treebank 
5.1 corpus for training and the remaining 10% is 
used for testing. Totally there are 0.45 million 
words in the training data and 50144 words for test-
ing. By following (Hall et al, 2006), we use the 
same headword table to convert the CTB into de-
pendency structure. The gold-tagged POS tags are 
137
used in the experiments. All experimental results 
are evaluated by LAS (label attachment score), UAS 
(unlabeled attachment score), and root accuracy. 
4.1 Settings 
In this paper, we employ the MSTParser (McDon-
ald et al, 2006) and MaltParser (Nivre, 2003) for 
comparison. We adopt the best settings for Malt-
Parser with SVM and MBL learners as reported by 
(Hall et al, 2006)2. For MSTParser, the Eisner?s 
decoding algorithm is used. 
The learner we used in this paper is L2-SVM with 
linear kernel (Keerthi and DeCoste, 2005). The one-
versus-all (OVA) strategy is applied to handle mul-
ticlass problems. Features that appear less than 
twice are removed from the feature set. Table 4 lists 
the feature set for the NSR and YSR. 
4.2 Results 
Table 2 summarizes overall experimental results. 
The final two rows list the entire training and testing 
time of the corresponding methods. From this table, 
we can see that our method (both NSR and YSR) 
achieve the best and second best parsing accuracy in 
terms of LAS, UAS, and root accuracy. For testing 
time efficiency, both our NSR and YSR also outper-
form the other methods. Meanwhile there is no sig-
nificant difference between NSR and YSR from the 
aspect of run time efficiency view. In comparison to 
MaltParser, NSR yields 14 times faster in parsing 
speed.  
Next, we analyze the effect of the two sequential 
taggers. The pure system performance of the 
neighbor tagger is 88.54 in F(?) rate, while the root 
tagger only achieves 61.67 F(?) score. The entire 
training time of the two taggers takes about 10 
hours. Table 3 shows the compared results. It is 
clear that adding the two taggers leads better pars-
ing accuracy than pure NSR and YSR. For example, 
it enhances the LAS score from 81.17 to 81.41 for 
NSR. Furthermore, the pure NSR and YSR still 
produce better parsing accuracy than MaltParser and 
MSTParser.  
5 Conclusion 
This paper presents an efficient and robust Chinese 
dependency parsing based on shift reduce parsing 
algorithms. We employ two sequential taggers to 
label the root and neighbor information as features. 
Experimental results show that our methods outper-
form two top-performed parsers, MaltParser and 
MSTParser in both accuracy and run-time efficiency. 
In the future, we will to investigate the effect of full 
parsing Chinese by applying shift-reduce-like ap-
proaches. 
                                                 
2 http://w3.msi.vxu.se/~nivre/research/chiMaltSVM.html 
 
Table 2: Parsing accuracy of each parsing algorithm 
this paper Evaluation
Metrics 
MaltParser
(SVM) 
MaltParser 
(MBT) MST NSR YSR
LAS 80.74 73.53 78.08 81.41 81.57 
UAS 81.98 75.40 79.53 82.60 82.76 
LAC 91.28 86.26 89.21 92.26 92.37 
Root  
A
65.88 69.36 73.71 74.93 77.61 
Sentence  
A
33.12 25.67 24.07 32.85 33.44 
TrainTime 6.74hr 3.42min 7.51hr 2.76hr 2.24hr
TestTime 15.92min 3.22min 10.15min 1.12min 1.15min
Table 3: Effective of the additional root and neighbor 
information 
Improvement rate NSR YSR 
LAS 81.17?81.41 81.16?81.57 
UAS 82.33?82.60 82.37?82.76 
LAC 92.07?92.26 92.15?92.37 
Root_ Accuracy 74.14?74.93 76.24?77.61 
References 
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on 
Multilingual Dependency Parsing. In Proc. of CoNLL, pp. 149-164. 
Ming-Wei Chang, Quang Do, and Dan Roth. 2006. Multilingual depend-
ency parsing: a pipeline approach. Recent Advances in Natural Lan-
guage Processing, pp. 195-204. 
Yuchang Cheng, Masayuki Asahara, and Yuji Matsumoto. 2005. Chinese 
deterministic dependency analyzer: examining effects of global fea-
tures and root node finder. In Proc. of SIGHAN, pp. 17-24. 
Yuchang Cheng, Masayuki  Asahara, and Yuji Matsumoto. 2006. Multi-
lingual Dependency Parsing at NAIST. In Proc. of CoNLL, pp. 191-
195. 
Jason Eisner. 1996. Three New Probabilistic Models for Dependency 
Parsing: An Exploration. In Proc. of COLING, pp. 340-345. 
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006. Discriminative Classi-
fiers for Deterministic Dependency Parsing. In Proc. of COLING-
ACL Main Conference Poster Sessions, pp. 316-323. 
Hideki Isozaki, Hideto Kazawa, and Tsutomu Hirao. 2004. A determinis-
tic word dependency analyzer enhanced with preference learning. In 
Proc. of COLING, pp. 275-281. 
Sathiya Keerthi and Dennis DeCoste. 2005. A modified finite Newton 
method for fast solution of large scale linear SVMs, JMLR, 6: 341-
361. 
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based 
text analysis. In Proc. of ACL, pp. 24-31. 
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying 
conditional random fields to Japanese morphological analysis, In 
Proc. of EMNLP, pp. 230-237. 
Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilin-
gual dependency analysis with a two-stage discriminative. In Proc. of 
CoNLL, pp. 216-220. 
Joakim Nivre. 2003. An efficient algorithm for projective dependency 
parsing. In Proc. of IWPT, pp. 149-160. 
Joakim Nivre, Johan Hall, Jens Nilsson, G?lsen Eryigit, and Svetoslav 
Marinov. 2006. Labeled pseudo-projective dependency parsing with 
support vector machines. In Proc. of CoNLL, pp. 221-226. 
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDonald, Jens Nilsson, 
Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared 
task on dependency parsing. In Proc. of EMNLP-CoNLL, pp. 915-
932. 
Qin Iris Wang, Dekang Lin, and Dale Schuurmans. 2007. Simple training 
of dependency parsers via structured boosting. In Proc. of IJCAI, pp. 
1756-1762. 
Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007. Multilingual de-
terministic dependency parsing framework using modified finite 
Newton method support vector machines. In Proc. of EMNLP-
CoNLL, pp. 1175-1181. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer. 2005. The 
Penn Chinese Treebank: phrase structure annotation of a large corpus. 
Natural Language Engineering, 11(2):207-238. 
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency 
analysis with support vector machines. In Proc. of IWPT, pp. 195-
206. 
 
138
Description of the NCU Chinese Word Segmentation and Part-of-Speech 
Tagging for SIGHAN Bakeoff 2007 
 
 
Yu-Chieh Wu Jie-Chi Yang Yue-Shi Lee 
Dept. of Computer Science and 
Information Engineering 
Graduate Institute of Net-
work Learning Technology 
Dept. of Computer Science and In-
formation Engineering 
National Central University National Central University Ming Chuan University 
Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan 
bcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw leeys@mcu.edu.tw 
 
 
 
 
Abstract 
In Chinese, most of the language process-
ing starts from word segmentation and 
part-of-speech (POS) tagging. These two 
steps tokenize the word from a sequence 
of characters and predict the syntactic la-
bels for each segmented word. In this pa-
per, we present two distinct sequential 
tagging models for the above two tasks. 
The first word segmentation model was 
basically similar to previous work which 
made use of conditional random fields 
(CRF) and set of predefined dictionaries 
to recognize word boundaries. Second, we 
revise and modify support vector ma-
chine-based chunking model to label the 
POS tag in the tagging task. Our method 
in the WS task achieves moderately rank 
among all participants, while in the POS 
tagging task, it reaches very competitive 
results. 
 
1 Introduction 
With the rapid expansion of online text articles 
such as blog, web news, and research/technical 
reports, there is an increasing demand for text min-
ing and management. Different from western-like 
languages, handling oriented languages is far more 
difficult since there is no explicit boundary symbol 
to indicate what a word is in the text. However the 
most important preliminary step for natural lan-
guage processing is to tokenize words and separate 
them from the word sequence. In Chinese, the 
word tokenization is also known as word segmen-
tation or Chinese word tokenization. The problem 
of the Chinese word segmentation is very critical 
for most Chinese linguistics because the error seg-
mented words deeply affects the downstream pur-
pose, like POS tagging and parsing. In addition 
tokenizing the unknown words is also an unavoid-
able problem. 
To support the above targets, it is necessary to 
detect the boundaries between words in a given 
sentence. In tradition, the Chinese word segmenta-
tion technologies can be categorized into three 
types, (heuristic) rule-based, machine learning, and 
hybrid. Among them, the machine learning-based 
techniques showed excellent performance in many 
recent research studies (Peng et al, 2004; Zhou et 
al., 2005; Gao et al, 2004). This method treats the 
word segmentation problem as a sequence of word 
classification. The classifier online assigns either 
?boundary? or ?non-boundary? label to each word 
by learning from the large annotated corpora. Ma-
chine learning-based word segmentation method is 
quite similar to the word sequence inference tech-
niques, such as part-of-speech (POS) tagging 
(Clark et al, 2003; Gimenez and Marquez, 2003), 
phrase chunking (Lee and Wu, 2007) and word 
dependency parsing (Wu et al, 2006, 2007). 
In this paper, we present two prototype systems 
for Chinese word segmentation and POS tagging 
161
Sixth SIGHAN Workshop on Chinese Language Processing
tasks. The former was basically an extension of 
previous literatures (Ng and Low, 2004; Zhou et al, 
2006), while the latter incorporates the unknown 
word and known word tagging into one step. The 
two frameworks were designed based on two vari-
ant machine learning algorithms, namely CRF and 
SVM. In our pilot study, the SVM showed better 
performance than CRF in the POS tagging task. To 
identify unknown words, we also encode the suffix 
and prefix features to represent the training exam-
ple. The strategy was showed very effective for 
improving both known and unknown word chunk-
ing on both Chinese and English phrase chunking 
(Lee and Wu, 2007). In this year, the presented 
word segmentation method achieved moderate 
rank among all participants. Meanwhile, the pro-
posed SVM-based POS tagging model reached 
very competitive accuracy in most POS tasks. For 
example, our method yields second best result on 
the CTB POS tagging track.  
The rest of this paper is organized as follows. 
Section 2 describes employed machine learning 
algorithms, CRF and SVM. In section 3, we pre-
sent the proposed word segmentation and POS 
tagging framework which used for the SIGHAN-
bake-off this year. Experimental result and evalua-
tions are reported in section 4. Finally, in section 5, 
we draw conclusion and future remarks. 
2 Classification Algorithms 
2.1 Conditional Random Fields 
Conditional random field (CRF) was an extension 
of both Maximum Entropy Model (MEMs) and 
Hidden Markov Models (HMMs) that was firstly 
introduced by (Lafferty et al, 2001). CRF defined 
conditional probability distribution P(Y|X) of given 
sequence given input sentence where Y is the 
?class label? sequence and X denotes as the obser-
vation word sequence.  
A CRF on (X,Y) is specified by a feature vector 
F of local context and the corresponding feature 
weight ?. The F can be treated as the combination 
of state transition and observation value in conven-
tional HMM. To determine the optimal label se-
quence, the CRF uses the following equation to 
estimate the most probability. 
 
),(maxarg),|(maxarg xyFxyPy
yy
?? ==  
 
The most probable label sequence y can be effi-
ciently extracted via the Viterbi algorithm. How-
ever, training a CRF is equivalent to estimate the 
parameter set?for the feature set. In this paper, we 
directly use CRF++ (Kudo and Matsumoto, 2003) 
which included the quasi-Newton L-BFGS 1 
method (Nocedal and Wright, 1999) to iterative 
update the parameters. 
 
2.2 Support Vector Machines 
Assume we have a set of training examples,  
}1 ,1{ ,  ),,(),...,,(),,( 2211 ?+??? iDinn yxyxyxyx  
where xi is a feature vector in D-dimension space 
of the i-th example, and yi is the label of xi either 
positive or negative. The training of SVMs in-
volves minimizing the following object function 
(primal form, soft-margin (Vapnik, 1995)): 
 
?
=
?+?=
n
i
ii yxWLossCWWW
1
),(
2
1
)( :minimize ?    (1) 
 
The loss function indicates the loss of misclassi-
fication risk. Usually, the hinge-loss is used (Vap-
nik, 1995; Keerthi and DeCoste, 2005). The factor 
C in (1) is a parameter that allows one to trade off 
training error and margin size. To classify a given 
testing example X, the decision rule takes the fol-
lowing form: 
 
))),((()( ?
?
+=
SVsx
iii
i
bxXKysignXy ?                         (2) 
 
?i represents the weight of training example xi 
which lies on the hyperplane, and b denotes as a 
bias threshold. SVs means the support vectors and 
obviously has the non-zero weights of ?i. 
)()(),( ii xXxXK ?? ?=  is a pre-defined kernel func-
tion that might transform the original feature space 
from D?  to 'D?  (usually D<<D?). In the linear 
kernel form, the ),( ixXK  simply compute the dot 
products of the two variables. By introducing of 
the polynomial kernel, we re-write the decision 
function of (1) as: 
 
                                                          
1 http://www-unix.mcs.anl.gov/tao/ 
162
Sixth SIGHAN Workshop on Chinese Language Processing
))),(1(((
))),((()(
?
?
?
?
++=
+=
SVsx
d
iii
SVsx
iii
i
i
bxXdotysign
bxXKysignXy
?
?
                          (3) 
 
where  
 
d
ii xXdotxXK )),(1(),( +=                                     (4) 
 
and d is the polynomial kernel degree.  
In many NLP problems, the training and testing 
examples are represented as bits of binary vectors. 
In this section, we focus on this case. Later, we 
present a general form without considering this 
constraint. 
 
3 System Description 
In this section, we first describe the problem set-
tings for the word segmentation problems. In sec-
tion 3.2, the proposed POS tagging framework is 
then presented. 
 
3.1 Word Sequence Classification 
Similar to English text chunking (Ramshaw and 
Marcus, 1995; Lee and Wu, 2007), the word se-
quence classification model aims to classify each 
word via encoding its context features.  
  By encoding with BIES (LMR tagging scheme) 
or IOB2 style, both WS and NER problems can be 
viewed as a sequence of word classification. Dur-
ing testing, we seek to find the optimal word type 
for each Chinese character. These types strongly 
reflect the actual word boundaries for Chinese 
words or named entity phrases.   
As reported by (Zhou et al, 2006), the use of 
richer tag set can effectively enhance the perform-
ance. They extend the tag of ?Begin of word? into 
?second-begin? and ?third-begin? to capture more 
character types. However, there are some ambigu-
ous problem to the 3-character Chinese words and 
4-character Chinese words. For example, to encode 
????? with his extended tag set, the first char-
acter can be encoded as ?B? tag. But for the second 
character, we can use ?second-begin? or ?I? tag to 
represent the middle of word.  
In order to make the extension clearer, in this 
paper, we explicitly extend the B tag and E tag 
with ?after begin? (BI), and ?before end? (IE) tags. 
Table 1 lists the difference between the traditional 
BIES and the proposed E-BIES encodings methods. 
Table 2 illustrates an example of how the BIES 
and E-BIES encode with different number of char-
acters. 
 
Table 1: BIES and E-BIES encoding strategies 
 BIES E-BIES 
Begin of a word B B
After begin of a word - BI
Middle of a word I I
Before end of a word - IE
End of a word E E
Single word S S
 
Table 2: An example of the BIES and E-BIES 
encoding strategies 
N-character word BIES E-BIES 
? S S
?? B,E B,E
??? B,I,E B,BI,E
???? B,I,I,E B,BI,IE,E
????? B,I,I,I,E B,BI,I,IE,E
 
To effect classify each character, in this paper, 
we adopted most feature types to train the CRF 
(Kudo and Matsumoto, 2004). Table 3 lists the 
adopted feature templates. The dictionary flag is 
very similar to previous literature (Ng and Low, 
2004) while we adding up English full-character 
into our dictionary. 
 
Table 3: Feature template used for Chinese 
word segmentation task 
Feature Type Context Position Description 
Unigram C-2,C-1,C0,C1,C2 
Chinese character fe
ature
Nearing Bi-
gram 
(C-2,C-1)(C-1,C0) 
(C1,C0)(C1,C2) 
Bi-character feature
Jump Bigram (C-1,C1) 
Non-continuous char
acter feature
Dictionary 
Flag C0 
Date, Digital, Englis
h letter or punctuatio
n
Dictionary 
Flag N-gram (C-1,C0,C1) 
N-gram of the dictio
nary flags
 
3.2 Feature Codification for Chinese POS 
Tagging 
As reported by (Ng, and Low, 2004; Clark et al, 
2003), the pure POS tagging performance is no 
more than 92% in the CTB data and no more than 
163
Sixth SIGHAN Workshop on Chinese Language Processing
96.8% in English WSJ. The learner used in his lit-
erature is maximum entropy model. However the 
main limitation of his POS tagging strategy is that 
the unknown word classification problem was not 
resolved.  
  To circumvent this vita, we simply extend the 
idea of SVM-based chunker (Lee and Wu, 2007) 
and develop our own SVM-based POS tagger. Al-
though CRF showed excellent performance in 
word segmentation task, in English POS tagging, 
the SVM is more effective than CRF. Also in our 
closed experiment, we had tried transformation-
based error-driven learner (TBL), CRF, and SVM 
classifiers. The pilot experiment showed that the 
SVM outperformed the other two learners and 
achieved almost 94% accuracy in the CTB data. 
Meanwhile TBL reached the worst result than the 
other two classifiers (~88%).  
  Handling unknown word is very important to 
POS tagging problem. As pointed out by (Lee and 
Wu, 2007; Gimenez, and Marquez, 2003), the in-
troduction of suffix features can effectively help to 
guess the unknown words for tagging and chunk-
ing. Different from (Gimenez and Marquez, 2003), 
we did not derive data for unknown word guessing. 
Instead, we directly encode all suffix- and prefix- 
features for each training instance. In training 
phase, the rich feature types are able to disambigu-
ate not only the unknown word guessing, but also 
improve the known word classification. As re-
ported by (Lee and Wu, 2007), the strategy did 
improve the English and Chinese chunking per-
formance for both known and unknown words. 
 
Table 4: Feature patterns used for Chinese POS 
tagging task 
Feature 
Type 
Context Position Description 
Unigram W-2,W-1,W0,W1,W2 
Chinese word feat
ure
Nearing 
Bigram 
(W-2,W-1)(W-1,W0) 
(W1,W0)(W1,W2) 
Bi-word feature
Jump Bi-
gram 
(W-2,W0)(W-1,W1) 
(W2,W0)(W1,W3)  
Non-continuous c
haracter feature
Possible 
tags W0 
Possible POS tag i
n the training data
Prefix 3/2/1 
characters W-1,W0,W1 
Pre-characters of 
word
Suffix 3/2/1 
characters W-1,W0,W1 
Post-characters of 
word
 
  The used feature set of our POS tagger is listed in 
Table 4. In this paper, we did not conduct the fea-
ture selection experiment for each tagging corpus, 
instead a unified feature set was used due to the 
time line. We trust our POS tagger could be further 
improved by removing or adding new feature set. 
 The learner used in this paper (SVM) is mainly 
developed by our own (Wu et al, 2007). The cost 
factor C is simply set as 0.15 for all languages. 
Furthermore, to remove rare words, we eliminate 
the words which appear no more than twice in the 
training data.  
 
4 Evaluations and Experimental Result 
4.1 Dataset and Evaluations 
In this year, we mainly focus on the close track for 
WS and POS tagging tracks. The CTB, SXU, and 
NCC corpora were used for evaluated the pre-
sented word segmentation method, while all the 
released POS tagging data were tested by our 
SVM-based tagger, included CityU, CKIP, CTB, 
NCC, and PKU. Both settings of the two models 
were set as previously noted. The evaluation of the 
two tasks was mainly measured by the three met-
rics, namely, recall, precision, and f-measure. 
However, the evaluation process for the POS tag-
ging track is somewhat different from WS. In WS, 
participant should reform the testing data into sen-
tence level whereas in the POS tagging track the 
word had been correctly segmented. Thus the 
measurement of the POS tagging track is mainly 
accuracy-based (correct or incorrect).  
 
4.2 Experimental Result on Word Segmenta-
tion Task 
In this year, we only select the following three data 
to perform our method for the word segmentation 
task. They are CTB, NCC, and SXU where the 
NCC and SXU are fresh in this year. Table5 shows 
the experimental results of our model in the close 
WS track with except for CKIP and CityU corpora.  
 
Table 5: Official results on the word segmenta-
tion task (closed-task) 
 Recall Precision F-measure
CTB 0.9471 0.9500 0.9486
NCC 0.9236 0.9269 0.9252
SXU 0.9505 0.9515 0.9510
 
164
Sixth SIGHAN Workshop on Chinese Language Processing
As shown above, our method in the CTB data 
showed 10th best out of 26 submissions. In the 
NCC and SXU datasets, our method achieved 
19/26 and 18/30 rank. In overall, the presented ex-
tend-BIES scheme seems to work well on the CTB 
data and results in middle rank in comparison to 
the other participants. 
 
4.3 Experimental Result on Part-of-Speech 
Tagging Task 
In the second experiment, we focus on the de-
signed POS tagging model. To measure the effec-
tiveness, we apply our method to all the released 
dataset, i.e., CityU, CKIP, CTB, NCC, and PKU.   
Table 6 lists the experimental result of our method 
in this task.  
  Similar to WS task, our method is still very effec-
tive to CTB dataset. It turns out our method 
achieved second best in the CTB, while for the 
other corpora, it achieved 4th best among all the 
participants. We also found that our method was 
very close to the top 1 score about 1.3% (CKIP) to 
0.09%. For the NCC, and PKU, our method was 
worse than the best system in 0.8% in overall accu-
racy. We conclude that by selecting suitable fea-
tures and cost factor C to SVM, our method can be 
further improved. We left the work as future direc-
tion. 
 
Table 6: Official results on the part-of-speech 
tagging task (closed-task) 
 Riv Roov Rmt Accuracy
CityU 0.9326 0.4322 0.8707 0.8865
CKIP 0.9504 0.5631 0.9065 0.9160
CTB 0.9554 0.7135 0.9183 0.9401
NCC 0.9658 0.5822 0.9116 0.9456
PKU 0.9591 0.5832 0.9173 0.9368
 
5 Conclusions and Future Work 
Chinese word segmentation is the most important 
infrastructure for many Chinese linguistic tech-
nologies such as text categorization and informa-
tion retrieval. In this paper, we present simple 
Chinese word segmentation and part-of-speech 
tagging models based on the conventional se-
quence classification technique. We treat the two 
tasks as two different learning framework and ap-
plying CRF and SVM as separated learners. With-
out any prior knowledge and rules, such a simple 
technique shows satisfactory results on both word 
segmentation and part-of-speech tagging tasks. In 
POS tagging task, our model shows very competi-
tive results which merely spend few hours to train. 
To reach state-of-the-art, our method still needs to 
further select features and parameter tunings. In the 
future, one of the main directions is to extend this 
model toward full unsupervised learning from 
large un-annotated text. Mining from large unla-
beled data have been showed benefits to improve 
the original accuracy. Thus, not only the stochastic 
feature analysis, but also adjust the learner from 
unlabeled data are important future remarks. 
 
References  
Clark, S., Curran, J. R., and Osborne, M. 2003. 
Bootstrapping POS Taggers Using Unlabeled 
data. In Proceedings of the 7th Conference on 
Natural Language Learning (CoNLL), pages 49-
55. 
Gao, J., Wu, A., Li, M., Huang, C. N., Li, H., Xia, 
X., and Qin, H. 2004. Adaptive Chinese word 
segmentation. In Proceedings the 41st Annual 
Meeting of the Association for Computational 
Linguistics, pp. 21-26. 
Gim?nez, J., and M?rquez, L. 2003. Fast and accu-
rate Part-of-Speech tagging: the SVM approach 
revisited. In Proceedings of the International 
Conference on Recent Advances in Natural 
Language Processing, pages 158-165. 
Keerthi, S., and DeCoste, D. 2005. A Modified 
Finite Newton Method for Fast Solution of 
Large Scale Linear SVMs. Journal of Machine 
Learning Research, 6: 341-361. 
Kudo, T., and Matsumoto, Y. 2004. Appliying 
Conditional Random Fields to Japanese Mor-
phological Analysis. In Proceedings of the Em-
pirical. Methods in Natural Language 
Processing (EMNLP), pages 230-237. 
Lafferty, J., McCallum, A., and Pereira, F. 2001. 
Conditional Random Field: Probabilistic models 
for segmenting and labeling sequence data. In 
Proceedings of the International Conference on 
Machine Learning. 
Ramshaw, L. A., and Marcus, M. P. 1995. Text 
Chunking Using Transformation-based Learning. 
165
Sixth SIGHAN Workshop on Chinese Language Processing
In Proceedings of the 3rd Workshop on Very 
Large Corpora, pages 82-94. 
Lee, Y. S. and Wu, Y. C. 2007. A Robust Multi-
lingual Portable Phrase Chunking System. Ex-
pert Systems with Applications, 33(3): 1-26. 
Ng, H. T., and Low, J. K. 2004. Chinese Part-of-
Speech Tagging: One-at-a-time or All-at-once? 
Word-based or Character-based? In Proceedings 
of the Empirical. Methods in Natural Language 
Processing (EMNLP). 
Nocedal, J., and Wright, S. 1999. Numerical opti-
mization. Springer. 
Peng, F., Feng, F., and McCallum, A. 2004. Chi-
nese segmentation and new word detection us-
ing conditional random fields. In Porceedings of 
the Computational Linguistics, pp. 562-568. 
Shi, W. 2005. Chinese Word Segmentation Based 
On Direct Maximum Entropy Model. In Pro-
ceedings of the Fourth SIGHAN Workshop on 
Chinese Language Processing. 
Vapnik, V. N. 1995. The Nature of Statistical 
Learning Theory. Springer. 
Wu, Y. C., Yang, J. C., and Lee, Y. S. 2007. An 
Approximate Approach for Training Polynomial 
Kernel SVMs in Linear Time. In Proceedings of 
the Annual Meeting of the Association for 
Computational Linguistics, pages 65-68. 
Wu, Y. C., Lee, Y. S., and Yang, J. C. 2007. Multi-
lingual Deterministic Dependency Parsing 
Framework using Modified Finite Newton 
Method Support Vector Machines. In Proceed-
ings of the Joint Conferences on Empirical 
Methods on Natural Language Processing and 
Conference on Natural Language Learning 
(EMNLP-CoNLL), pages 1175-1181. 
Wu, Y. C., Lee, Y. S., and Yang, J. C. 2006. The 
Exploration of Deterministic and Efficient De-
pendency Parsing. In Proceedings of the 10th 
Conference on Natural Language Learning 
(CoNLL). 
Zhou, H., Huang, C. N., and Li, M. 2006. An Im-
proved Word Segmentation System with Condi-
tional Random Fields. In Proceedings of the 
SIGHAN Workshop on Chinese Language 
Processing Workshop, pages 162-165. 
Zhou, J., Dai, X., Ni, R., Chen, J. 2005. .A Hybrid 
Approach to Chinese Word Segmentation 
around CRFs. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language 
Processing. 
 
166
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 65?68,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Approximate Approach for Training Polynomial Kernel SVMs in 
Linear Time 
Yu-Chieh Wu Jie-Chi Yang Yue-Shi Lee 
Dept. of Computer Science and 
Information Engineering 
Graduate Institute of Net-
work Learning Technology
Dept. of Computer Science and 
Information Engineering 
National Central University National Central University Ming Chuan University 
Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan 
bcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw lees@mcu.edu.tw 
 
Abstract 
Kernel methods such as support vector ma-
chines (SVMs) have attracted a great deal 
of popularity in the machine learning and 
natural language processing (NLP) com-
munities. Polynomial kernel SVMs showed 
very competitive accuracy in many NLP 
problems, like part-of-speech tagging and 
chunking. However, these methods are 
usually too inefficient to be applied to large 
dataset and real time purpose. In this paper, 
we propose an approximate method to 
analogy polynomial kernel with efficient 
data mining approaches. To prevent expo-
nential-scaled testing time complexity, we 
also present a new method for speeding up 
SVM classifying which does independent 
to the polynomial degree d. The experi-
mental results showed that our method is 
16.94 and 450 times faster than traditional 
polynomial kernel in terms of training and 
testing respectively. 
1 Introduction 
Kernel methods, for example support vector 
machines (SVM) (Vapnik, 1995) are successfully 
applied to many natural language processing (NLP) 
problems. They yielded very competitive and 
satisfactory performance in many classification 
tasks, such as part-of-speech (POS) tagging 
(Gimenez and Marquez, 2003), shallow parsing 
(Kudo and Matsumoto, 2001, 2004; Lee and Wu, 
2007), named entity recognition (Isozaki and 
Kazawa, 2002), and parsing (Nivre et al, 2006). 
In particular, the use of polynomial kernel SVM 
implicitly takes the feature combinations into ac-
count instead of explicitly combines features. By 
setting with polynomial kernel degree (i.e., d), dif-
ferent number of feature conjunctions can be im-
plicitly computed. In this way, polynomial kernel 
SVM is often better than linear kernel which did 
not use feature conjunctions. However, the training 
and testing time costs for polynomial kernel SVM 
is far slow than the linear kernel. For example, it 
took one day to train the CoNLL-2000 task with 
polynomial kernel SVM, while the testing speed is 
merely 20-30 words per second (Kudo and Ma-
tsumoto, 2001). Although the author provided the 
solution for fast classifying with polynomial kernel 
(Kudo and Matsumoto, 2004), the training time is 
still inefficient. Nevertheless, the testing time of 
their method exponentially scales with polynomial 
kernel degree d, i.e., O(|X|d) where |X| denotes as 
the length of example X.  
On the contrary, even the linear kernel SVM 
simply disregards the effect of feature combina-
tions during training and testing, it performs not 
only more efficient than polynomial kernel, but 
also can be improved through directly appending 
features derived from the set of feature combina-
tions. Examples include bigram, trigram, etc. Nev-
ertheless, selecting the feature conjunctions was 
manually and heuristically encoded and should 
perform amount of validation trials to discover 
which is useful or not. In recent years, several 
studies had reported that the training time of linear 
kernel SVM can be reduced to linear time 
(Joachims, 2006; Keerthi and DeCoste, 2005). But 
they did not and difficult to be extent to polyno-
mial kernels.  
In this paper, we propose an approximate ap-
proach to extend the linear kernel SVM toward 
polynomial. By introducing the well-known se-
quential pattern mining approach (Pei et al, 2004), 
65
frequent feature conjunctions, namely patterns 
could be discovered and also kept as expand fea-
ture space. We then adopt the mined patterns to re-
represent the training/testing examples. Subse-
quently, we use the off-the-shelf linear kernel 
SVM algorithm to perform training and testing. 
Besides, to exponential-scaled testing time com-
plexity, we propose a new classification method 
for speeding up the SVM testing. Rather than 
enumerating all patterns for each example, our 
method requires O(Favg*Navg) which is independent 
to the polynomial kernel degree. Favg is the average 
number of frequent features per example, while the 
Navg is the average number of patterns per feature.  
2 SVM and Kernel Methods 
Suppose we have the training instance set for bi-
nary classification problem: 
}1 ,1{ ,  ),,(),...,,(),,( 2211 ?+??? iDinn yxyxyxyx  
where xi is a feature vector in D-dimension 
space of the i-th example, and yi is the label of xi 
either positive or negative. The training of SVMs 
involves in minimize the following object (primal 
form, soft-margin) (Vapnik, 1995): 
?
=
+?=
n
i
ii yxWLossCWWW
1
),(
2
1)(  :minimize ?             (1) 
The loss function indicates the loss of training 
error. Usually, the hinge-loss is used (Keerthi and 
DeCoste, 2005). The factor C in (1) is a parameter 
that allows one to trade off training error and mar-
gin. A small value for C will increase the number 
of training errors. 
To determine the class (+1 or -1) of an example 
x can be judged by computing the following equa-
tion. 
))),(((sign)( ?
?
+=
SVsx
iii
i
bxxKyxy ?
                              (2) 
?i is the weight of training example xi (?i>0), 
and b denotes as a threshold. Here the xi should be 
the support vectors (SVs), and are representative of 
training examples. The kernel function K is the 
kernel mapping function, which might map from 
D?  to 'D?  (usually D<<D?). The natural linear ker-
nel simply uses the dot-product as (3). 
),(),( ii xxdotxxK =                                             (3) 
A polynomial kernel of degree d is given by (4). 
d
ii xxdotxxK )),(1(),( +=                                      (4) 
One can design or employ off-the-shelf kernel 
types for particular applications. In particular to the 
use of polynomial kernel-based SVM, it was 
shown to be the most successful kernels for many 
natural language processing (NLP) problems 
(Kudo and Matsumoto, 2001; Isozaki and Kazawa, 
2002; Nivre et al, 2006).  
It is known that the dot-product (linear form) 
represents the most efficient kernel computing 
which can produce the output value by linearly 
combining all support vectors such as 
?
?
=+=
SVsx
iii
i
xywbwxdotxy ?  ere        wh)),((sign)(
     (5) 
By combining (2) and (4), the determination of 
an example of x using the polynomial kernel can 
be shown as follows. 
)))1),((((sign)( bxxdotyxy di
SVsx
ii
i
++= ?
?
?
                   (6) 
Usually, degree d is set more than 1. When d is 
set as 1, the polynomial kernel backs-off to linear 
kernel. Although the effectiveness of polynomial 
kernel, it can not be shown to linearly combine all 
support vectors into one weight vector whereas it 
requires computing the kernel function (4) for each 
support vector xi. The situation is even worse when 
the number of support vectors become huge (Kudo 
and Matsumoto, 2004). Therefore, whether in 
training or testing phrase, the cost of kernel com-
putations is far more expensive than linear kernel. 
3 Approximate Polynomial Kernel 
In 2004, Kudo and Matsumoto (2004) derived both 
implicitly (6) and explicitly form of polynomial 
kernel. They indicated that the use of explicitly 
enumerate the feature combinations is equivalent 
to the polynomial kernel (see Lemma 1 and Exam-
ple 1, Kudo and Matsumoto, 2004) which shared 
the same view of (Cumby and Roth, 2003).  
We follow the similar idea of the above studies 
that requires explicitly enumerated all feature com-
binations. To meet with our problem, we employ 
the well-known sequential pattern mining algo-
rithm, namely PrefixSpan (Pei et al, 2004) to effi-
cient mine the frequent patterns. However, directly 
adopt the algorithm is not a good idea. To fit with 
SVM, we modify the original PrefixSpan algo-
rithm according to the following constraints.  
Given a set features, the PrefixSpan mines the 
frequent patterns which occurs more than prede-
fined minimum support in the training set and lim-
ited in the length of predefined d, which is equiva-
lent to the polynomial kernel degree d. For exam-
66
ple, if the minimum support is 5, and d=2, then a 
feature combination (fi, fj) must appear more than 5 
times in set of x.  
Definition 1 (Frequent single-item sequence):  
Given a set of feature vectors x, minimum support, 
and d, mining the frequent patterns (feature combi-
nations) is to mine the patterns in the single-item 
sequence database. 
Lemma 2 (Ordered feature vector):  
For each example, the feature vector could be 
transformed into an ordered item (feature) list, i.e., 
f1<f2<?<fmax where fmax is the highest dimension of 
the example. 
Proof. It is very easy to sort an unordered feature 
vector into the ordered list with conventional sort-
ing algorithm. 
Definition 3 (Uniqueness of the features per ex-
ample):  
Given the set of mined patterns, for any feature fi, 
it is impossible to appear more than once in the 
same pattern. 
Different from conventional sequential pattern 
mining method, in feature combination mining for 
SVM only contains a set of feature vectors each of 
which is independently treated. In other words, no 
compound features in the vector. If it exists, one 
can simply expand the compound features as an-
other new feature. 
By means of the above constraints, mining the 
frequent patterns can be reduced to mining the lim-
ited length of frequent patterns in the single-item 
database (set of ordered vectors). Furthermore, 
during each phase, we need only focus on finding 
the ?frequent single features? to expand previous 
phase. More detail implementation issues can refer 
(Pei et al, 2004). 
3.1 Speed-up Testing 
To efficiently expand new features for the original 
feature vectors, we propose a new method to fast 
discovery patterns. Essentially, the PrefixSpan al-
gorithm gradually expands one item from previous 
result which can be viewed as a tree growing. An 
example can be found in Figure 1.  
Each node in Figure 1 is the associate feature of 
root. The whole patterns expanded by fj can be rep-
resented as the path from root to each node. For 
example, pattern (fj, fk, fm, fr) can be found via trav-
ersing the tree starting from fj. In this way, we can 
re-expand the original feature vector via visiting 
corresponding trees for each feature.  
 
Figure 1: The tree representation of feature fj  
 
Table 1: Encoding frequent patterns with DFS array 
representation 
Level 0 1 2 3 2 1 2 1 2 2
Label Root k m r p m p o p q
Item fj fk fm fr fp fm fp fo fp fq
 
However, traversing arrays is much more effi-
cient than visiting trees. Therefore, we adopt the l2-
sequences encoding method based on the DFS 
(depth-first-search) sequence as (Wang et al, 2004) 
to represent the trees. An l2-sequence does not only 
store the label information but also take the node 
level into account. Examples can be found in Table 
1.  
 
Theorem 4 (Uniqueness of l2-sequence): Given 
trees T1, and T2, their l2-sequences are identical if 
and only if T1 and T2 are isomorphic, i.e., there 
exists a one-to-one mapping for set of nodes, node 
labels, edges, and root nodes. 
Proof. see theorem 1 in (Wang et al, 2004). 
Definition 5 (Ascend-descend relation):  
Given a node k of feature fk in l2-sequence, all of 
the descendant of k that rooted by k have the 
greater feature numbers than fk.  
Definition 6 (Limited visiting space):  
Given the highest feature fmax of vector X, and fk 
rooted l2-sequence, if fmax<fk, then we can not find 
any pattern that prefix by fk. 
 
Both definitions 5 and 6 strictly follow lemma 2 
that kept the ordered relations among features. For 
example, once node k could be found in X, it is 
unnecessary to visit its children. More specifically, 
to determine whether a frequent pattern is in X, we 
need to compare feature vector of X and l2-
sequence database. It is clearly that the time com-
plexity of our method is O(Favg*Navg) where Favg is 
the average number of frequent features per exam-
ple, while the Navg is the average length of l2-
sequence. In other words, our method does not de-
pendent on the polynomial kernel degree.  
67
4 Experiments 
To evaluate our method, we examine the well-
known shallow parsing task which is the task of 
CoNLL-20001. We also adopted the released perl-
evaluator to measure the recall/precision/f1 rates. 
The used feature consists of word, POS, ortho-
graphic, affix(2-4 prefix/suffix letters), and previ-
ous chunk tags in the two words context window 
size (the same as (Lee and Wu, 2007)). We limited 
the features should at least appear more than twice 
in the training set.  
For the learning algorithm, we replicate the 
modified finite Newton SVM as learner which can 
be trained in linear time (Keerthi and DeCoste, 
2005). We also compare our method with the stan-
dard linear and polynomial kernels with SVMlight 2.  
4.1 Results 
Table 2 lists the experimental results on the 
CoNLL-2000 shallow parsing task. Table 3 com-
pares the testing speed of different feature expan-
sion techniques, namely, array visiting (our method) 
and enumeration. 
Table 2: Experimental results for CoNLL-2000 shal-
low parsing task 
CoNLL-2000 F1 Mining Time 
Training 
Time 
Testing 
Time 
Linear Kernel 93.15 N/A 0.53hr 2.57s
Polynomial(d=2) 94.19 N/A 11.52hr 3189.62s
Polynomial(d=3) 93.95 N/A 19.43hr 6539.75s
Our Method 
(d=2,sup=0.01) 
93.71 <10s 0.68hr 6.54s
Our Method 
(d=3,sup=0.01) 
93.46 <15s 0.79hr 9.95s
Table 3: Classification time performance of enu-
meration and array visiting techniques 
Array visiting Enumeration CoNLL-2000 d=2 d=3 d=2 d=3 
Testing time 6.54s 9.95s 4.79s 11.73s
Chunking speed 
(words/sec) 7244.19 4761.50 9890.81 4038.95
It is not surprising that the best performance was 
obtained by the classical polynomial kernel. But 
the limitation is that the slow in training and test-
ing time costs. The most efficient method is linear 
kernel SVM but it does not as accurate as polyno-
mial kernel. However, our method stands for both 
efficiency and accuracy in this experiment. In 
terms of training time, it slightly slower than the 
linear kernel, while it is 16.94 and ~450 times 
faster than polynomial kernel in training and test-
                                                 
1 http://www.cnts.ua.ac.be/conll2000/chunking/ 2 http://svmlight.joachims.org/ 
ing. Besides, the pattern mining time is far smaller 
than SVM training.  
As listed in Table 3, we can see that our method 
provide a more efficient solution to feature expan-
sion when d is set more than two. Also it demon-
strates that when d is small, the enumerate-based 
method is a better choice (see PKE in (Kudo and 
Matsumoto, 2004)).  
5 Conclusion 
This paper presents an approximate method for 
extending linear kernel SVM to analogy polyno-
mial-like computing. The advantage of this method 
is that it does not require maintaining the cost of 
support vectors in training, while achieves satisfac-
tory result. On the other hand, we also propose a 
new method for speeding up classification which is 
independent to the polynomial kernel degree. The 
experimental results showed that our method close 
to the performance of polynomial kernel SVM and 
better than the linear kernel. In terms of efficiency, 
our method did not only improve 16.94 times 
faster in training and 450 times in testing, but also 
faster than previous similar studies.  
References 
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. International Conference on Machine 
Learning, pages 104-114. 
Hideki Isozaki and Hideto Kazawa. 2002. Efficient support 
vector classifiers for named entity recognition. Interna-
tional Conference on Computational Linguistics, pages 1-7. 
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong Wang, 
Helen Pinto, Qiming Chen, Umeshwar Dayal and Mei-
Chun Hsu. 2004. Mining Sequential Patterns by Pattern-
Growth: The Prefix Span Approach. IEEE Trans. on 
Knowledge and Data Engineering, 16(11): 1424-1440. 
Sathiya Keerthi and Dennis DeCoste. 2005. A modified finite 
Newton method for fast solution of large scale linear SVMs. 
Journal of Machine Learning Research. 6: 341-361. 
Taku Kudo and Yuji Matsumoto. 2001. Fast methods for 
kernel-based text analysis. Annual Meeting of the Associa-
tion for Computational Linguistics, pages 24-31. 
Taku Kudo and Yuji Matsumoto. 2001. Chunking with sup-
port vector machines. Annual Meetings of the North 
American Chapter and the Association for the Computa-
tional Linguistics. 
Yue-Shi Lee and Yu-Chieh Wu. 2007. A Robust Multilingual 
Portable Phrase Chunking System. Expert Systems with 
Applications, 33(3): 1-26. 
Vladimir N. Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer. 
Chen Wang, Mingsheng Hong, Jian Pei, Haofeng Zhou, Wei 
Wang and Baile Shi. 2004. Efficient Pattern-Growth 
Methods for Frequent Tree Pattern Mining. Pacific knowl-
edge discovery in database (PAKDD). 
68
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 241?245, New York City, June 2006. c?2006 Association for Computational Linguistics
The Exploration of Deterministic and Efficient Dependency Parsing  
Yu-Chieh Wu Yue-Shi Lee Jie-Chi Yang
Dept. of Computer Science and 
Information Engineering 
Dept. of Computer Science and 
Information Engineering 
Graduate Institute of Net-
work Learning Technology 
National Central University Ming Chuan University National Central University
Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan 
bcbb@db.csie.ncu.edu.tw lees@mcu.edu.tw yang@cl.ncu.edu.tw 
Abstract
In this paper, we propose a three-step 
multilingual dependency parser, which 
generalizes an efficient parsing algorithm 
at first phase, a root parser and post-
processor at the second and third stages. 
The main focus of our work is to provide 
an efficient parser that is practical to use 
with combining only lexical and part-of-
speech features toward language inde-
pendent parsing. The experimental results 
show that our method outperforms Malt-
parser in 13 languages. We expect that 
such an efficient model is applicable for 
most languages. 
1 Introduction 
The target of dependency parsing is to automati-
cally recognize the head-modifier relationships 
between words in natural language sentences. Usu-
ally, a dependency parser can construct a similar 
grammar tree with the dependency graph. In this 
year, CoNLL-X shared task (Buchholz et al, 2006) 
focuses on multilingual dependency parsing with-
out taking the language-specific knowledge into 
account. The ultimate goal of this task is to design 
an ideal multilingual portable dependency parsing 
system. 
  To accomplish the shared task, we present a very 
light-weight and efficient parsing model to the 13 
distinct treebanks (Haji? et al, 2004; Simov et al, 
2005; Simov and Osenova, 2003; Chen et al, 2003; 
B?hmov? et al, 2003; Kromann 2003; van der 
Beek et al, 2002; Brants et al, 2002; Kawata and 
Bartels, 2000; Afonso et al, 2002; D?eroski et al, 
2006; Civit and Mart? 2002; Nivre et al, 2005; 
Oflazer et al, 2003; Atalay et al, 2003) with a 
three-step process, Nivre?s algorithm (Nivre, 2003), 
root parser, and post-processing. Our method is 
quite different from the conventional three-pass 
processing, which usually exhaustively processes 
the whole dataset three times, while our method 
favors examining the ?un-parsed? tokens, which 
incrementally shrink. At the beginning, we slightly 
modify the original parsing algorithm (proposed by 
(Nivre, 2003)) to construct the initial dependency 
graph. A root parser is then used to recognize root 
words, which were not parsed during the previous 
step. At the third phase, the post-processor (which 
is another learner) recognizes the still un-parsed 
words. However, in this paper, we aim to build a 
multilingual portable parsing model without em-
ploying deep language-specific knowledge, such as 
lemmatization, morphologic analyzer etc. Instead, 
we only make use of surface lexical and part-of-
speech (POS) information. Combining these shal-
low features, our parser achieves a satisfactory re-
sult for most languages, especially Japanese. 
  In the remainder of this paper, Section 2 describes 
the proposed parsing model, and Section 3 lists the 
experimental settings and results. Section 4 pre-
sents the discussion and analysis of our parser with 
three selected languages. In Section 5, we draw the 
future direction and conclusion. 
2 System Description 
241
Over the past decades, many state-of-the-art pars-
ing algorithm were proposed, such as head-word 
lexicalized PCFG (Collins, 1998), Maximum En-
tropy (Charniak, 2000), Maximum/Minimum 
spanning tree (MST) (McDonald et al, 2005), Bot-
tom-up deterministic parsing (Yamada and Ma-
tsumoto, 2003), and Constant-time deterministic 
parsing (Nivre, 2003). Among them, the Nivre?s 
algorithm (Nivre, 2003) was shown to be most ef-
ficient method, which only costs at most 2n transi-
tion actions to parse a sentence (O(n3) for the 
bottom-up or MST approaches). Nivre?s method is 
mainly consists of four transition actions, 
Left/Right/Reduce/Shift. We further extend these 
four actions by dividing the ?reduce? into ?reduce? 
and ?sleep (reduce-but-shift)? two actions. Because 
the too early reduce action makes the following 
words difficult to find the parents. Thus, during 
training, if a word which is the child of the top of 
the stack, it is then assigned to the ?sleep? category 
and pushed into stack, otherwise, the conventional 
reduce action is applied. Besides, we do not ar-
range these transition actions with priority order, 
instead, the decision is made by the classifier. The 
overall parsing model can be found in Figure 1. 
Table 1 lists the detail system spec of our model. 
Figure 1: System architecture 
Table 1: Overall parsing system summary  
?. Parsing Algorithm: 1. Nivre's Algorithm (Nivre, 2003) 
2. Root Parser 
3. Exhaustive-based Post-processing 
?. Parser Characteris-
tics:
1. Top-down + Bottom-up 
2. Deterministic + Exhaustive 
3. Labeling integrated 
4. Non-Projective 
?. Learner: SVMLight (Joachims, 1998) 
  (1) One-versus-One 
(2) Linear Kernel 
?. Feature Set: 1. Lexical (Unigram/Bigram) 
2. Fine-grained POS and Coarse grained 
BiCPOS
.?  Post-Processing: Another learner is used to re-recognize 
heads in stacks 
.? Additional/External Resources: Non-Used 
2.1 Constant-time Parser and Analysis 
The Nivre?s algorithm makes use of a stack and an 
input list to model the word dependency relations 
via identifying the transition action of the top token 
on the stack (Top) and the next token of the input 
list (Next). Typically a learning algorithm can be 
used to recognize these actions via encoding fea-
tures of the two terms (Top and Next). The ?Left? 
and ?Reduce? pops the Top from stack whereas the 
?Right?, ?Reduce-But-Shift?, and ?Shift? push to-
ken Next into the top of stack. Nivre (Nivre, 2003) 
had proved that this algorithm can accomplish de-
pendency parsing at most 2n transition actions.  
Although, the Nivre?s algorithm is much more 
efficient than the others, it produces three problems. 
1. It does not explicitly indicate which words are 
the roots. 
2. Some of the terms in the stack do not belong 
to the root but still should be parsed. 
3. It always only compares the Top and Next
words.
The problem (2) and (3) are complement with each 
other. A straightforward way resolution is to adopt 
the exhaustive parsing strategy (Covington, 2001). 
Unfortunately, such a brute-force way may cause 
exponential training and testing spaces, which is 
impractical to apply to the large-scale corpus, for 
example, the Czech Treebank (1.3 million words). 
To overcome this and keep the efficiency, we de-
sign a post-processor that re-cycles the residuum in 
the stack and re-identify the heads of them. Since 
most of the terms (90-95%) of the terms had be 
processed in previous stages, the post-processor 
just exhaustively parses a small part. In addition, 
for problem (1), we propose a root parser based on 
the parsed result of the Nivre?s algorithm. We dis-
cuss the root-parser and post-processor in the next 
two subsections.
2.2 Root Parser 
After the first stage, the stack may contain root and 
un-parsed words. The root parser identifies the root 
word in the stack. The main advantage of this 
strategy could avoid sequential classification proc-
ess, which only focuses on terms in the stack.  
We build a classifier, which learns to find root 
word based on encoding context and children fea-
tures. However, most of the dependency relations 
were constructed at the first stage. Thus, we have 
more sufficient head-modifier information rather 
242
than only taking the contexts into account. The 
used features are listed as follows. 
Neighbor terms,bigrams,POS,BiCPOS (+/-2 window) 
Left most child term, POS, Bigram, BiCPOS 
Right most child term, POS, Bigram, BiCPOS 
2.3 Post-Processing 
Before post-processing, we remove the root words 
from stack, which were identified by root-parser. 
The remaining un-parsed words in stack were used 
to construct the actual dependency graph via ex-
haustive comparing with parsed-words. It is neces-
sary to build a post-processor since there are about 
10% un-parsed words in each training set. We pro-
vide the un-parsed rate of each language in Table 2 
(the r.h.s. part).
By applying previous two steps (constant-time 
parser and root parser) to the training data, the re-
maining un-parsed tokens were recorded. Not only 
using the forward parsing direction, the backward 
direction is also taken into account in this statistics. 
Averagely, the un-parsed rates of the forward and 
backward directions are 13% and 4% respectively. 
The back ward parsing often achieves lower un-
parsed rate among all languages (except for Japa-
nese and Turkish). 
To find the heads of the un-parsed words, we 
copy the whole sentence into the word list again, 
and re-compare the un-parsed tokens (in stack) and 
all of the words in the input list. Comparing with 
the same words is disallowed. The comparing 
process is going on until the actual head is found. 
Acquiescently, we use the nearest root words as its 
head. Although such a brute force way is time-
consuming. However, it only parses a small part of 
un-parsed tokens (usually, 2 or 3 words per sen-
tence).
2.4 Features and Learners 
For the constant-time parser of the first stage, we 
employ the features as follows. 
Basic features:  
Top.word,Top.pos,Top.lchild.pos,Top.lchild.relation,
Top.rchild.pos, Top.rchild.relation,Top.head.pos, 
Top.head.relation,
Next.word, Next.pos, Next.lchild.pos, 
Next.lchild.relation, Next+1.pos, Next+2.pos, Next+3.pos
Enhanced features: 
Top.bigram,Top.bicpos,Next.bigram,Next.bicpos,
Next+1.word,Next+2.word,Next+3.word
In this paper, we use the support vector machines 
(SVM) (Joachims, 1998) as the learner. SVM is 
widely used in many natural language processing 
(NLP) areas, for example, POS tagging (Wu et al, 
2006). However, the SVM is a binary classifier 
which only recognizes true or false. For multiclass 
problem, we use the so-called one-versus-one 
(OVO) method with linear kernel to combine the 
results of each pairwise subclassifier. The final 
class in testing phase is mainly determined by ma-
jority voting. 
  For all languages, our parser uses the same set-
tings and features. For all the languages (except 
Japanese and Turkish), we use backward parsing 
direction to keep the un-parsed token rate low. 
3 Experimental Result 
3.1 Dataset and Evaluation Metrics 
The testing data is provided by the (Buchholz et al, 
2006) which consists of 13 language treebanks. 
The experimental results are mainly evaluated by 
the unlabeled and labeled attachment scores. The 
CoNLL also provided a perl-scripter to automatic 
compute these rates. 
3.2 System Results 
Table 2 presents the overall parsing performance 
of the 13 languages. As shown in Table 2, we list 
two parsing results at the second and third columns 
(new and old). It is worth to note that the result B 
is produced by removing the enhanced features and 
the post-processing step from our parser, while the 
result A is the complete use of the enhanced fea-
tures and the overall three-step parsing. In this year, 
we submit result B to the CoNLL shared task due 
to the time limitation.  
  In addition, we also apply the Maltparser, which 
is implemented with the Nivre?s algorithm (Nivre, 
2003) to be compared. The Maltpaser also includes 
the SVM and memory-based learner (MBL). Nev-
ertheless, it does not optimize the SVM where the 
training and testing times are too long to be com-
pared even the linear kernel is used. Therefore we 
use the default MBL and feature model 3 (M3) in 
this experiment. We also perform the significant 
test to evaluate the statistical difference among the 
three results. If the answer is ?Yes?, it means the 
two systems are significant difference under at 
least 95% confidence score (p < 0.05). 
243
Table 2: A general statistical table of labeled attachment score, test and un-parsed rate (percentage) 
Statistic test Un-Parsed Rate A
(New result) 
B
(Old result)
C
(Maltparser) A vs. B B vs. C A vs. C Forward Backward 
Arabic 63.75 63.81 54.11 No Yes Yes 10.3 1.4
Chinese 81.25 74.81 73.92 Yes No Yes 4.01 2.3
Czech 71.24 59.36 59.36 Yes No Yes 16.1 5.6
Danish 79.52 78.38 77.31 No No No 12.8 2.5
Dutch 68.45 68.45 63.61 No Yes Yes 18.4 9.8
German 79.57 76.52 76.52 Yes No Yes 12.7 9.2
Japanese 91.43 90.11 89.07 Yes No Yes 1.1 4.4
Portugese 81.33 81.47 75.38 No Yes Yes 24.3 3.17
Slovene 68.41 67.83 55.04 No Yes Yes 14.9 5.5
Spanish 74.65 72.99 72.81 Yes No Yes 20 0.5
Swedish 79.53 71.72 76.28 Yes Yes Yes 19.1 2.8
Turkish 55.33 55.09 52.18 No Yes Yes 2.5 4
Bulgarian 81.23 79.73 79.73 No No No 15.7 1.2
AVG 75.05 72.32 69.64 13.22 4.02
4 Discussion 
4.1 Analysis of Overview Aspect 
Although our method is efficient for parsing that 
achieves satisfactory result, it is still away from the 
state-of-the-art performance. Many problems give 
rise to not only the language-specific characteris-
tics, but also the parsing strategy. We found that 
our method is weak to the large-scale training size 
and large dependency class datasets, for example, 
German (Brants et al, 2002) and Czech. For Dutch, 
we observe that the large non-projective tokens 
and relations in this set. Overall, we conclude the 
four main limitations of our parsing model. 
1.Unbalanced and large dependency relation 
classes 
2.Too fine or coarse POS tag 
3.Long sentences and non-projective token rates 
4.Feature engineering and root accuracy 
The main reason of the first problem is still caused 
by the unbalanced distribution of the training data. 
Usually, the right-action categories obtain much 
fewer training examples. For example, in the Turk-
ish data, 50 % of the categories receive less than 
0.1% of the training examples, 2/3 are the right 
dependency group. For the Czech, 74.6% of the 
categories receive less than 0.1% of the training 
examples.  
Second, the too fine grained size of POS tag  set 
often cause the features too specific that is difficult 
to be generalized by the learner. Although we 
found the grained size is not the critical factor of 
our parser, it is closely related to the fourth prob-
lem, feature engineering. For example, in Chinese 
(Chen et al, 2003), there are 303 fine grained POS 
types which achieves better result on the labeled 
attachment score is higher than the coarse grained 
(81.25 vs. 81.17). Intuitively, the feature combina-
tions deeply affect the system performance (see A 
vs. C where we extend more features than the 
original Nivre?s algorithm). 
Problem 3 exposes the disadvantage of our 
method, which is weak to identify the long dis-
tance dependency. The main reason is resulted 
from the Nivre?s algorithm in step 1. This method 
is quite sensitive and non error-recovered since it is 
a deterministic parsing strategy. Abnormal or 
wrong push or pop actions usually cause the error 
propagation to the remaining words in the list. For 
example, there are large parts of errors are caused 
by too early reduce or missed left arc makes some 
words could not find the actual heads. On the con-
trary, one can use an N-best selection to choose the 
optimal dependency graph or applying MST or 
exhaustive parsing schema. Usually, these ap-
proaches are quite inefficient which requires at 
least O(n3).
Finally, in this paper, we only take the surface 
lexical word and POS tag into account without 
employing the language-specific features, such as 
Lemma, Morph?etc. Actually, it is an open ques-
tion to compile and investigate the feature engi-
neering. On the other hand, we also find the 
performance of the root parser in some languages 
is poor. For example, for Dutch the root precision 
rate is only 38.52, while the recall rate is 76.07. It 
indicates most of the words in stack were wrongly 
recognized as root. This is because there are sub-
stantially un-parsed rate that left many un-parsed 
words remain in stack. One way to remedy the 
problem can adjust the root parser to independently 
identify root word by sequential word classifica-
tion at first step and then apply the Nivre?s algo-
rithm. We left the comparison of the issue as future 
work.
244
4.2 Analysis of Specific View 
We select three languages, Arabic, Japanese, and 
Turkish to be more detail analysis. Figure 2 illus-
trates the learning curve of the three languages and 
Table 3 summarizes the comparisons of ?fine vs. 
coarse? POS types and ?forward vs. backward? 
parsing directions.
  For the three languages, we found that most of the 
errors frequently appear to the noun POS tags 
which often denominate half of the training set. In 
Turkish, the lower performance on the noun POS 
attachment rate deeply influents the overall parsing. 
For example, the error rate of Noun in Turkish is 
39% which is the highest error rate. On the con-
trary, the head error rates fall in the middle rank 
for the other two languages.  
??
??
??
??
??
??
???
?? ?? ?? ?? ?? ?? ?? ?? ?? ???
??????????????????????????
???
???
???
???
??
?
???
???
??
??
???????? ?????? ???????
Figure 2: Learning curve of the three datasets 
Table 3: Parsing performance of different grained 
POS tags and forward/backward parsing directions
Parsing 
direction LA-Score  
POS
grained LA-Score
Ja Forward 91.35 Fine 91.35 
 Backward 85.75 
Forward
Coarse 91.25 
Ar Forward 60.62 Fine 63.55 
Backward 63.55 
Backward
Coarse 63.63 
Tu Forward 55.47 Fine 55.47 
Backward 55.59 
Forward
Coarse 55.59 
  In Turkish, we also find an interesting result 
where the recall rate of the distance=2 parsing 
(56.87) is lower than distance=3-6, and >7 (62.65, 
57.83). In other words, for Turkish, our parser 
failed to recognize the distance=2 dependency rela-
tions. For the other languages, usually the identifi-
cation rate of the longer distance parsing should be 
lower than the smaller distance. Thus, a future 
work to parsing Turkish, should put more emphasis 
on improving not only the noun POS type, but also 
the distance=2 parsing.  
  Besides, the root parsing accuracy is also an im-
portant factor to most languages. In Japanese, al-
though our parser achieves more than 97% 
left/right arc rates. However, for the root word pre-
cision rate is quite lower (85.97). Among all de-
pendency relation classification rates, the root class 
usually locates in the lowest rank for the three lan-
guages.
5 Conclusion and Future Remarks 
Dependency parsing is one of the most important 
issues in NLP community. This paper presents and 
analyzes the impact of the efficient parsing model 
that only combines with lexical and part-of-speech 
information. To go language-independent, we did 
not tune any parameter settings in our model and 
exclude most of the language-dependent feature set, 
which provided by the CoNLL (Buchholz et al, 
2006). The main focus of our work coincides with 
the target goal of the CoNLL shared task, i.e., go 
multilingual dependency parsing without taking 
the language-specific knowledge into account. A 
future work on the deterministic parsing strategy is 
to convert the existing model toward N-best pars-
ing.
References  
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006. 
CoNLL-X Shared Task on Multilingual Dependency Pars-
ing, In Proceedings of the Tenth Conf. on Computational 
Natural Language Learning CoNLL-X.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. 
In NAACL, pages 132-139. 
Michael Collins. 1998. Head-driven statistical models for 
natural language processing. Ph.D. thesis. University of 
Pennsylvania. 
Michael A. Covington. 2001. A fundamental Algorithm for 
Dependency Parsing. In Proceedings of the Annual ACM 
Southeast Conference, pages 95-102. 
Jason M. Eisner. 1996. Three new probabilistic models for 
dependency parsing: An exploration. In COLING, pages 
340-345.
Thornsten Joachims. 1998. Text categorization with support 
vector machines: learning with many relevant features. In 
ECML, pages 137-142. 
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. 
Online Large-Margin Training of Dependency Parsers, In
ACL, pages 91-98. 
Joakim Nivre. 2003. An Efficient Algorithm for Projective 
Dependency Parsing. In Proceedings of the International 
Workshop on Parsing Technology, pages 149-160. 
Yu C. Wu, Chia H. Chang, and Yue S. Lee. 2006. A General 
and Multi-lingual Phrase Chunking Model based on Mask-
ing Method. In CICLING, pages 144-155. 
Hiroyasu Yamada, and Yuji Matsumoto. 2003. Statistical 
Dependency Analysis with Support Vector Machines. In 
Proceedings of the International Workshop on Parsing 
Technology, pages 195-206. 
245
A Chinese Word Segmentation System Based on Structured 
Support Vector Machine Utilization of Unlabeled Text Corpus 
Chongyang Zhang 
Anhui Province 
Engineering Laboratory 
of Speech and Language,  
University of Science and 
Technology of China 
cyzhang9 
@mail.ustc.edu.cn 
Zhigang Chen 
Anhui Province 
Engineering Laboratory 
of Speech and Language,  
University of Science and 
Technology of China 
Chenzhigang 
@ustc.edu 
Guoping Hu 
Anhui Province 
Engineering Laboratory 
of Speech and Language, 
University of Science and 
Technology of China 
Applecore 
@ustc.edu 
?
?
Abstract 
Character-based tagging method has 
achieved great success in Chinese Word 
Segmentation (CWS). This paper 
proposes a new approach to improve the 
CWS tagging accuracy by structured 
support vector machine (SVM) 
utilization of unlabeled text corpus. First, 
character N-grams in unlabeled text 
corpus are mapped into low-dimensional 
space by adopting SOM algorithm. Then 
new features extracted from these maps 
and another kind of feature based on 
entropy for each N-gram are integrated 
into the structured SVM methods for 
CWS. We took part in two tracks of the 
Word Segmentation for Simplified 
Chinese Text in bakeoff-2010: Closed 
track and Open track. The test corpora 
cover four domains: Literature, 
Computer Science, Medicine and 
Finance. Our system achieved good 
performance, especially in the open 
track on the domain of medicine, our 
system got the highest score among 18 
systems. 
1 Introduction 
In the last decade, many statistics-based 
methods for automatic Chinese word 
segmentation (CWS) have been proposed with 
development of machine learning and statistical 
method (Huang and Zhao, 2007). Especially, 
character-based tagging method which was 
proposed by Nianwen Xue (2003) achieves 
great success in the second International 
Chinese word segmentation Bakeoff in 2005 
(Low et al, 2005). The character-based tagging 
method formulates the CWS problem as a task 
of predicting a tag for each character in the 
sentence, i.e. every character is considered as 
one of four different types in 4-tag set: B (begin 
of word), M (middle of word), E (end of word), 
and S (single-character word).  
Most of these works train tagging models 
only on limited labeled training sets, without 
using any unsupervised learning outcomes from 
unlabeled text. But in recent years, researchers 
begin to exploit the value of enormous 
unlabeled corpus for CWS, such as some 
statistics information on co-occurrence of sub-
sequences in the whole text has been extracted 
from unlabeled data and been employed as input 
features for tagging model training (Zhao and  
Kit , 2007).  
Word clustering is a common method to 
utilize unlabeled corpus in language processing 
research to enhance the generalization ability, 
such as part-of-speech clustering and semantic 
clustering (Lee et al, 1999 and B Wang and H 
Wang 2006). Character-based tagging method 
usually employs N-gram features, where an N-
gram is an N-character segment of a string. We 
believe that there are also semantic or 
grammatical relationships between most of N-
grams and these relationships will be useful in 
CWS. Intuitively, assuming the training data 
contains the bigram ?? /? ?(The last two 
characters of the word ?Israel? in Chinese), not 
contain the bigram ?? /? ?(The last two 
characters of the word ?Turkey? in Chinese), if 
we could cluster the two bigrams together 
according to unlabeled corpus and employ it as 
a feature for supervised training of tagging 
model, then maybe we will know that there 
should be a word boundary after ??/?? though 
we only find the existence of word boundary 
after ?? /? ? in the training data. So we 
investigate how to apply clustering method onto 
unlabeled data for the purpose of improving 
CWS accuracy in this paper. 
This paper proposes a novel method of 
using unlabeled data for CWS, which employs 
Self-Organizing Map (SOM) (Kohonen 1982) 
to organize Chinese character N-grams on a 
two-dimensional array, named as ?N-gram 
cluster map? (NGCM), in which the character 
N-grams similar in grammatical structure and 
semantic meaning are organized in the same or 
adjacent position. Two different arrays are built 
based the N-gram?s preceding context and 
succeeding context respectively because 
normally N-gram is just part of Chinese word 
and doesn?t share similar preceding and 
succeeding context in the same time. Then 
NGCM-based features are extracted and applied 
to tagging model of CWS. Another kind of 
feature based on entropy for each N-gram is 
also introduced for improving the performance 
of CWS. 
The rest of this paper is organized as 
follows: Section 2 describes our system; Section 
3 describes structured SVM and the features 
which are obtained from labeled corpus and also 
unlabeled corpus; Section 4 shows experimental 
results on Bakeoff-2010 and Section 5 gives our 
conclusion. 
2 System description 
2.1 Open track: 
The architecture of our system for open track is 
shown in Figure 1. For improving the cross-
domain performance, we train and test with 
dictionary-based word segmentation outputs. 
On large-scale unlabeled corpus we use Self-
Organizing Map (SOM) (Kohonen 1982) to 
organize Chinese character N-grams on a two-
dimensional array, named as ?N-gram cluster 
map? (NGCM), in which the character N-grams 
similar in grammatical structure and semantic 
meaning are organized in the same or adjacent 
position. Then new features are extracted from 
these maps and integrated into the structured 
SVM methods for CWS. 
The large-scale 
unlabeled corpus 
test text
SomDictionary Based 
CWS
NGCM
Structured SVM Model
Results
Training text
Labeled 
data
Dictionary Based 
CWS
Labeled 
data
?
Figure 1: Open track system?
2.2 Closed track: 
Training text test text
Som Statistic
NGCM Entropy
Structured SVM Model
Results
Figure 2: closed track system 
Because the large-scale unlabeled corpus is 
forbidden to be used on closed track. We trained 
the SOM only on the data provided by 
organizers. To make up for the deficiency of the 
sparse data on SOM, we add entropy-based 
features (ETF) for every N-gram to structured 
SVM model. The architecture of our system for 
close track is shown in Figure 2. 
3 Learning algorithm 
3.1 Structured support vector machine 
The structured support vector machine can learn 
to predict structured y , such as trees sequences 
or sets, from x  based on large-margin approach. 
We employ a structured SVM that can predict a 
sequence of labels 1( ,..., )Ty y y=  for a given 
observation sequences? 1( ,..., )Tx x x= , where 
ty ?? ,?  is the label set for y. 
There are two types of features in the 
structured SVM: transition features (interactions 
between neighboring labels along the chain), 
emission features (interactions between 
attributes of the observation vectors and a 
specific label).we can represent the input-output 
pairs via joint feature map (JFM) 
1
1
1
1
( ) ( )
( , )
( ) ( )
T
t c t
t
T
c t c t
t
x y
x y
y y
?
?
?
=
? +
=
? ???? ?? ?= ? ?? ??? ?? ?
?
?
 
where
{
1 2
1 2
,
1,
0,
( ) ( ( , ), ( , ),..., ( , )) '
{0,1} ,  { , ,..., }
Kronecker delta  ,
c
K
K
K
i j
i j
i j
y y y y y y y
y y y y
? ? ?
? ? =?
?
? ?
? ? =
=
 
( )x?  denotes an arbitrary feature representation 
of the inputs. The sign " "?  expresses tensor 
product defined as ? : d kR R? dkR? , 
[ ] ( 1)i j da b + ?? [ ] [ ]i ja b= . T  is the length of an 
observation sequence. 0? ?  is a scaling factor 
which balances the two types of contributions.?
Note that both transition features and 
emission features can be extended by including 
higher-order interdependencies of labels (e.g. 
1 2( ) ( ) ( )c t c t c ty y y+ +? ?? ?? ),by including 
input features from a window centered at the 
current position (e.g. replacing ( )tx?  with 
( ,..., ,... )t r t t rx x x? ? + )or by combining higher-
order output features with input features (e.g. 
1( ) ( ) ( )t c t c t
t
x y y? +?? ??? ) 
The w-parametrized discriminant function 
:F X Y R? ? interpreted as measuring the 
compatibility of x and y is defined as: 
( , ; ) , ( , )F x y w w x y?=
 
So we can maximize this function over the 
response variable to make a prediction 
( ) arg max ( , , )
y Y
f x F x y w
?
=
 
Training the parameters can be formulated 
as the following optimization problem. 
,
1
1
min ,
2
. . , :
, ( , ) ( , ) ( , )
n
iw
i
i i i i i i i
C
w w
n
s t i y Y
w x y x y y y
? ?
? ? ?
=
+
? ? ?
? ? ? ?
?
where n  is the number of the training set, i?  is 
a slack variable , 0C ?  is a constant 
controlling the tradeoff between training error 
minimization and margin maximization, 
1( , )y y?  is the loss function ,usually the 
number of misclassified tags in the sentence. 
3.2 Features set for tagging model 
For a training sample denoted as 
1( ,..., )Tx x x=  and 1( ,..., )Ty y y= . We chose 
first-order interdependencies of labels to be 
transition features, and dependencies between 
labels and N-grams (n=1, 2, 3, 4) at current 
position in observed input sequence to be 
emission features.  
So our JFM is the concatenation of the 
follow vectors 
1
1
1
( ) ( )
T
c t c t
t
y y
? +
=
? ???
 
1
( ) ( ), { 1,0,1}
T
t m c t
t
x y m? +
=
?? ? ??
  
1
1
( ) ( ), { 2, 1,0,1}
T
t m t m c t
t
x x y m? + + +
=
?? ? ? ??
 
1 1
1
( ) ( ),
{ 2, 1,0,1,2}
T
t m t m t m c t
t
x x x y
m
? + ? + + +
=
??
? ? ?
?
?
1 1 2
1
( ) ( ),
{ 3, 2, 1,0,1,2}
T
t m t m t m t m c t
t
x x x x y
m
? + ? + + + + +
=
??
? ? ? ?
?
 
?
Figure 3 shows the transition features and 
the emission features of N-grams (n=1, 2) at 3y . 
The emission features of 3-grams and 4-grams 
are not shown here because of the large number 
of the dependencies. 
1y 2y 3y 4y
1x 2x 3x 4x
5y
5x
 
Figure 3: the transition features and the 
emission features at 3y  for structured SVM 
?
3.3 SOM-based N-gram cluster maps 
and the NGCM mapping feature 
The Self-Organizing Map (SOM) (Kohonen 
1982), sometimes called Kohonen map, was 
developed by Teuvo Kohonen in the early 
1980s. 
Self-organizing semantic maps (Ritter and 
Kohonen 1989, 1990) are SOMs that have been 
organized according to word similarities, 
measured by the similarity of the short contexts 
of the words. Our algorithm of building N-gram 
cluster maps is similar to self-organizing 
semantic maps. Because normally N-gram is 
just part of Chinese word and do not share 
similar preceding and succeeding context in the 
same time, so we build two different maps 
according to the preceding context and the 
succeeding context of N-gram individually. In 
the end we build two NGCMs: NGCMP 
(NGCM according to preceding context) and 
NGCMS (NGCM according to succeeding 
context).  
Due to the limitation of our computer and 
time we only get two 15 15?  size 2GCMs for 
open track system from large-scale unlabeled 
corpus which was obtained easily from websites 
like Sohu, Netease, Sina and People Daily. 
The 2GCMP and 2GCMS we got for the 
open track task are shown in Figure 4 and 
Figure 5 respectively. 
0,0
0,1 1,1 2,1
0,2 1,2 2,2
1,0 2,0
14,14
14,2
14,0
0,14 1,14 2,14
14,1
?/?
?/?
?/?
?/?
?
?/?
?/?
?/?
?/?
?/?
?
?/?
?/?
?/?
?/?
?
?/?
?/?
?/?
?/?
?
 
Figure 4: 2GCMP 
0,0
0,1 1,1 2,1
0,2 1,2 2,2
1,0 2,0
14,14
14,2
14,0
0,14 1,14 2,14
14,1
?/?
?/?
?/?
?/?
?/?
?
?/?
?/?
?/?
?/?
?
?/?
?/?
?/?
?/?
...
?/?
?/?
?/?
?/?
...
 
Figure 5: 2GCMS 
After checking the results, we find that the 
2GCMS have following characters:1) most of 
the meaningless bigrams that contain characters 
from more than one word, such as the bigram "
??" in "...????..." , are organized into the 
same neurons in the map, 2) most of the first or 
last bigrams of the country names are organized 
into a few adjacent neurons, such as ??/??, 
??/??, ??/?? and ??/??in 2GCMS , ??/
??, ??/??, ??/??, ??/?? , and ??/?? in 
2GCMP. 
Two 20 1?  size 2GCMs are trained for the 
closed track system only on the data provided 
by organizers. The results are not as good as the 
results of the 15 15?  size 2GCMs because of 
the less training data. The second character 
described above is no longer apparent as well as 
the 15 15?  size 2GCMs, but it still kept the first 
character. 
Then we adopt the position of the neurons 
which current N-gram mapped in the NGCM as 
a new feature. So every feature has D 
dimensions (D equals to the dimension of the 
NGCM, every dimension is corresponding to 
the coordinate value in the NGCM). In this way, 
N-gram which is originally represented as a 
high dimensional vector based on its context is 
mapped into a very low-dimensional space. We 
call it NGCM mapping feature. So our previous 
JFM in section 3.2 is concatenated with the 
following features: 
2GCMS 1
1
( ) ( ), { 2, 1}
T
t m t m c t
t
x x y m? + + +
=
?? ? ? ??
 
2GCMP 1
1
( ) ( ), {0,1}
T
t m t m c t
t
x x y m? + + +
=
?? ??
2GCMS 1
1
( ) ( ), { 2, 1}
T
t m t m c t
t
x x y m? + + +
=
?? ? ? ??
2GCMP 1
1
( ) ( ), {0,1}
T
t m t m c t
t
x x y m? + + +
=
?? ??
 
where 2GCMS( )x?   and 2GCMP ( )x?  
2{0,1,...,14}? denote the NGCM mapping 
feature from 2GCMS and 2GCMP respectively. 
NGCM ( )x?  denotes the quantization error  of  
current N-gram x on its NGCM. 
As an example, the process of import features 
from NGCMs at 3y  is presented in Figure 6. 
1y 2y 3y 4y
1x 2x 3x 4x
5y
5x
2GCMS 2GCMP
 
Figure 6: Using 2GCMS and 2GCMP as input 
to structured SVM 
 
3.4 Entropy-based features 
On closed track, the entropy of the preceding 
and succeeding characters conditional on the N-
gram and also the self-information of the N-
gram are used as features for the structured 
SVM methods. Then our previous JFM in 
section 3.2 is concatenated with the following 
features: 
1
1 2 1 2 3 1 2 3 4
( | ) ( ),
{ , , }
T
Ngram c t
t
Ngram t t t t t t t t t
H P N x y
x x x x x x x x x x
=
+ + + + + + + + +
= ??
?
?
?
1
2 1 3 2 1 4 3 2 1
1
( | ) ( ),
{ , , }
( ) ( )
T
Ngram c t
t
Ngram t t t t t t t t t
T
Ngram c t
t
H S N x y
x x x x x x x x x x
I N x y
=
? ? ? ? ? ? ? ? ?
=
= ??
?
= ??
?
?
 
all the ngrams used in section 3.2Ngramx ?  
Where P and S  denote the set of the preceding 
and succeeding characters respectively. The 
entropy: ( | )NgramH X N x= =  
( | ) log ( | )
t
t Ngram t Ngram
X x
p x x p x x
?
??  
The self-information of the N-gram NgramN x= : 
( ) log ( )Ngram NgramI x p x= ?  
4 Applications and Experiments 
4.1 Text Preprocessing 
Text is usually mixed up with numerical or 
alphabetic characters in Chinese natural 
language, such as ??? office ????? 9
??. These numerical or alphabetic characters 
are barely segmented in CWS. Hence, we treat 
these symbols as a whole ?character? according 
to the following two preprocessing steps. First 
replace one alphabetic character to four 
continuous alphabetic characters with E1 to E4 
respectively, five or more alphabetic characters 
with E5. Then replace one numerical number to 
four numerical numbers with N1 to N4 and five 
or more numerical numbers with N5. After text 
preprocessing, the above examples will be ??
? E5????? N1??. 
4.2 Character-based tagging method 
for CWS 
Previous works show that 6-tag set achieved 
a better CWS performance (Zhao et al, 
2006). Thus, we opt for this tag set. This 6-
tag set adds ?B2? and ?B3? to 4-tag set 
which stand for the type of the second and 
the third character in a Chinese word 
respectively. For example, the tag sequence 
for the sentence ??????/?/??/?
?(Shanghai World Expo / will / last / six 
months)? will be ?B B2 B3 M E S B E B E?. 
4.3 Results in the bakeoff-2010 
We use hmmsvm  version 3.1 to build our 
structured SVM models. The cut-off threshold is 
set to 2. The precision parameter is set to 0.1. 
The tradeoff between training error 
minimization and margin maximization is set to 
1000. 
We took part in two tracks of the Word 
Segmentation for Simplified Chinese Text in 
bakeoff-2010: c (Closed track), o (Open track). 
The test corpora cover four domains: A 
(Literature), B (Computer Science), C 
(Medicine), D (Finance). 
Precision(P),Recall(R),F-measure(F),Out-
Of-Vocabulary Word Recall(OOV RR) and In- 
Vocabulary Word Recall(IV RR) are adopted to 
measure the performance of word segmentation 
system. 
Table 1 shows the results of our system on 
the word segmentation task for simplified 
Chinese text in bakeoff-2010. Table 2 shows the 
comparision between our system results and 
best results in bakeoff-2010. 
Table 1: The results of our systems 
Tabel 2: The comparision between our system 
results and best results in bakeoff-2010 
 
It is obvious that our systems are stable and 
reliable even in the domain of medicine when 
the F-measure of the best results was decreased. 
Our open track system performs better than 
closed track system, demonstrating the benefit 
of the dictionary-based word segmentation 
outputs and the NGCMs which are training on 
large-scale unlabeled corpus. 
5 Conclusion 
This paper proposes a new approach to improve 
the CWS tagging accuracy by structured support 
vector machine (SVM) utilization of unlabeled 
text corpus. We use SOM to organize Chinese 
character N-grams on a two-dimensional array, 
so that the N-grams similar in grammatical 
structure and semantic meaning are organized in 
the same or adjacent position. Then new 
features extracted from these maps and another 
kind of feature based on entropy for each N-
gram are integrated into the structured SVM 
methods for CWS. Our system achieved good 
performance, especially in the open track on the 
domain of medicine, our system got the 
highest score among 18 systems. 
In future work, we will try to organizing all 
the N-grams on a much larger array, so that 
every neuron will be labeled by a single N-gram. 
The ultimate objective is to reduce the 
dimension of input features for supervised CWS 
learning by replacing N-gram features with two-
dimensional NGCM mapping features. 
References 
B.Wang, H.Wang 2006.A Comparative Study on 
Chinese Word Clustering. Computer Processing 
of Oriental Languages. Beyond the Orient: The 
Research Challenges Ahead, pages 157-164 
Chang-Ning Huang and Hai Zhao. 2007. Chinese 
word segmentation: A decade review. Journal of 
Chinese Information Processing, 21(3):8?20. 
Chung-Hong Lee & Hsin-Chang Yang.1999, A Web 
Text Mining Approach Based on Self-Organizing 
Map, ACM-library 
G.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B. 
Taskar, and S. V. N. Vishwanathan, editors. 2007 
Predicting Structured Data. MIT Press, 
Cambridge, Massachusetts. 
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-
Liang Lu. 2006. Effective tag set selection 
  R P F1 OOV RR IV RR 
A c 0.932 0.935 0.933 0.654 0.953
o 0.942 0.943 0.942 0.702 0.959
B c 0.935 0.934 0.935 0.792 0.961
o 0.948 0.946 0.947 0.812 0.973
C c 0.937 0.934 0.936 0.761 0.959
o 0.941 0.935 0.938 0.787 0.96
D c 0.955 0.956 0.955 0.848 0.965
o 0.948 0.955 0.951 0.853 0.957
  F1(Bakeoff-2010) F1(Our system) 
A c 0.946 0.933 
o 0.955 0.942 
B c 0.951 0.935 
o 0.95 0.947 
C c 0.939 0.936 
o 0.938 0.938 
D c 0.959 0.955 
o 0.96 0.951 
inChinese word segmentation via conditional 
random field modeling. In Proceedings of 
PACLIC-20, pages 87?94. Wuhan, China. 
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006.An 
improved Chinese word segmentation system with 
conditional random field. In SIGHAN-5, pages 
162?165, Sydney, Australia, July 22-23. 
Hai Zhao and Chunyu Kit. 2007. Incorporating 
global information into supervised learning for 
Chinese word segmentation. In PACLING-2007, 
pages 66?74, Melbourne,Australia, September 19-
21. 
H.Ritter, and T.Kohonen, 1989. Self-organizing 
semantic maps. Biological Cybernetics, vol. 61, 
no. 4, pp. 241-254. 
I.Tsochantaridis,T.Joachims,T.Hofmann,and Y.Altun. 
2005. Large Margin Methods for Structured and 
Interdependent Output Variables, Journal of 
Machine Learning Research (JMLR), 
6(Sep):1453-1484. 
Jin Kiat Low, Hwee Tou Ng, and Wenyuan 
Guo.2005. A maximum entropy approach to 
Chinese word segmentation. In Proceedings of the 
Fourth SIGHAN Workshop on Chinese Language 
Processing, pages 161?164. Jeju Island,Korea. 
J.Lafferty,A.McCallum, F.Pereira. 2001. Conditional 
random fields: Probabilistic models for 
segmenting and labeling sequence data. In 
Proceedings of the International Conference on 
Machine Learning (ICML). San Francisco: 
Morgan Kaufmann Publishers, 282?289. 
Nianwen Xue and Susan P. Converse., 2002, 
Combining Classifiers for Chinese Word 
Segmentation, In Proceedings of First SIGHAN 
Workshop on Chinese Language Processing. 
Nianwen Xue. 2003. Chinese word segmentation as 
character tagging. Computational Linguistics and 
Chinese Language Processing, 8(1):29?48. 
R.Sproat and T.Emerson. 2003.The first 
international Chinese word segmentation bakeoff. 
In The Second SIGHAN Workshop on Chinese 
Language Processing, pages 133?143.Sapporo, 
Japan. 
S.Haykin, 1994. Neural Networks: A Comprehensive 
Foundation. NewYork: MacMillan. 
T.Joachims, T.Finley, Chun-Nam Yu. 2009, Cutting-
Plane Training of Structural SVMs, Machine 
Learning Journal,77(1):27-59. 
T.Joachims. 2008 . 
hmmsvm  Sequence Tagging with 
Structural Support Vector Machines, 
http://www.cs.cornell.edu/People/tj/svm_light/sv
m_hmm.html 
T.Honkela, 1997. Self-Organizing Maps in Natural 
Language Processing. PhD thesis, Helsinki 
University of Technology, Department of 
Computer Science and Engineering, Laboratory of 
Computer and Information Science. 
T.Kohonen. 1982.Self-organized formation of 
topologically correct feature maps. Biological 
Cybernetics, 43, pp. 59-69. 
T.Kohonen., J.Hynninen, J.Kangas, J.Laaksonen, 
1996 ,SOM_PAK: The Self-Organizing Map 
Program Package,Technical Report A31, 
Helsinki University of Technology , 
http://www.cis.hut.fi/nnrc/nnrc-programs.html 
Y.Altun, I.Tsochantaridis, T.Hofmann. 2003. Hidden 
Markov Support Vector Machines. In Proceedings 
of International Conference on Machine Learning 
(ICML). 
 
