Two levels of evaluation in a complex NL system
Jean-Baptiste Berthelin
LIMSI-CNRS
B?t.508 Universit? Paris XI
91403 Orsay, France
jbb@limsi.fr
Brigitte Grau
LIMSI-CNRS
B?t.508 Universit? Paris XI
91403 Orsay, France
bg@limsi.fr
Martine Hurault-Plantet
LIMSI-CNRS
B?t.508 Universit? Paris XI
91403 Orsay, France
mhp@limsi.fr
Abstract
The QALC question-answering system,
developed at LIMSI, has been a
participant for two years in the QA
track of the TREC conference. In this
paper, we present a quantitative
evaluation of various modules in our
system, based on two criteria: first, the
numbers of documents containing the
correct answer and selected by the
system; secondly, the number of
answers found. The first criterion is
used for evaluating locally the modules
in the system, which contribute in
selecting documents that are likely to
contain the answer. The second one
provides a global evaluation of the
system. As such, it also serves for an
indirect evaluation of various modules.
1 Introduction
For two years, the TREC Evaluation
Conference, (Text REtrieval Conference) has
been featuring a Question Answering track, in
addition to those already existing. This track
involves searching for answers to a list of
questions, within a collection of documents
provided by NIST, the conference organizer.
Questions are factual or encyclopaedic, while
documents are newspaper articles. The TREC9-
QA track, for instance, proposed 700 questions
whose answers should be retrieved in a corpus
of about one million documents.
In addition to the evaluation, by human
judges, of their systems? results (Voorhees and
Tice, 2000), TREC participants are also
provided with an automated evaluation tool,
along with a database. These data consist of a
list of judgements of all results sent in by all
participants. The evaluation tool automatically
delivers a score to a set of answers given by a
system to a set of questions. This score is
derived from the mean reciprocal rank of the
first five answers. For each question, the first
correct answers get a mark in reverse proportion
to their rank. Those evaluation tool and data are
quite useful, since it gives us a way of
appreciating what happens when modifying our
system to improve it.
We have been taking part to TREC for two
years, with the QALC question-answering
system (Ferret et al 2000), currently developed
at LIMSI. This system has following
architecture: parsing of the question to find the
expected type of the answer, selection of a
subset of documents among the approximately
one million TREC-provided items, tagging of
named entities within the documents, and,
finally, search for possible answers. Some of the
components serve to enrich both questions and
documents, by adding system-readable data into
them. Such is the case for the modules that parse
questions and tag documents. Other components
operate a selection among documents, using
added data. One example of such modules are
those which select relevant documents, another
is the one which extracts the answer from the
documents.
A global evaluation of the system is based on
judgement about its answers. This criterion
provides only indirect evaluation of each
component, via the evolution of the final score
when this component is modified. To get a
closer evaluation of our modules, we need other
criteria. In particular, concerning the evaluation
of components for document selection, we
adopted an additional criterion about selected
relevant documents, that is, those that yield the
correct answer.
This paper describes a quantitative
evaluation of various modules in our system,
based on two criteria: first, the number of
selected relevant documents, and secondly, the
number of found answers. The first criterion is
used for evaluating locally the modules in the
system, which contribute in selecting documents
that are likely to contain the answer. The second
one provides a global evaluation of the system.
It also serves for an indirect evaluation of
various modules.
2 System architecture
Figure 1 shows the architecture of the QALC
system, made of five separate modules:
Question analysis, Search engine, Re-indexing
and selection of documents, Named entity
recognition, and Question/sentence pairing.
Tagged Questions:
Named entity tags
Vocabulary &
  frequencies
Named entity
 recognition
Candidate
terms
Retrieved
documents
Tagged sentences: named entity
    tags and term indexation
Ordered sequences of 250 and
           50 characters
Question analysis Search engine
Questions
Subset of ranked documents
Corpus
Re-indexing and selection of
      documents (FASTR)
Question/Sentence pairing
Figure 1. The QALC system
2.1 Question analysis
Question analysis is performed in order to assign
features to questions and use these features for
the matching measurement between a question
and potential answer sentences. It relies on a
shallow parser which spots discriminating
patterns and assigns categories to a question.
The categories correspond to the types of named
entities that are likely to constitute the answer to
this question. Named entities receive one of the
following types: person, organisation, location
(city or place), number (a time expression or a
number expression). For example the pattern
how far yields to the answer type length:
Question: How far away is the moon?
Answer type: LENGTH
Answer  within the document :
With a <b_numex_TYPE="NUMBER"> 28
<e_numex> -power telescope you can see it on
the  moon <b_numex_TYPE="LENGTH">
250,000 miles <e_numex> away.
2.2 Selection of relevant documents
The second module is a classic search
engine, giving, for each question, a ranked list of
documents, each of which could contain the
answer.
This set of documents is then processed by a
third module, made of FASTR (Jacquemin,
1999), a shallow transformational natural
language analyser and of a ranker. This module
can select, among documents found by the
search engine, a subset that satisfies more
refined criteria. FASTR improves things
because it indexes documents with a set of
terms, including not only the (simple or
compound) words of the initial question, but
also their morphological, syntactic and semantic
variants. Each index is given a weight all the
higher as it is close to the original word in the
question, or as it is significant. For instance,
original terms are considered more reliable than
semantic variants, and proper names are
considered more significant than nouns. Then,
documents are ordered according to the number
and the quality of the terms they contain. An
analysis of the weight graph of the indexed
documents enables the system to select a
relevant subpart of those documents, whose size
varies along the questions. Thus, when the curve
presents a high negative slope, the system only
select documents before the fall, otherwise a
fixed threshold is used.
2.3 Named entity recognition
The fourth module tags named entities in
documents selected by the third one. Named
entities are recognized through a combination of
lexico-syntactic patterns and significantly large
lexical data. The three lists used for lexical
lookup are CELEX (1998), a lexicon of 160,595
inflected words with associated lemma and
syntactic category, a list of 8,070 first names
(6,763 of which are from the CLR (1998)
archive) and a list of 211,587 family names also
from the CLR archive.
2.4 Question-sentence pairing
The fifth module evaluates each sentence in
the ranker-selected documents, using a
similarity measure between, on one side, terms
and named entities in the sentence, and on the
other side, words in the questions and expected
answer type. To do so, it uses the results of the
question parser, and the named entity tagger,
along with a frequency-weighted vocabulary of
the TREC corpus.
The QALC system proposes long and short
answers. Concerning the short ones, the system
focuses on parts of sentences that contain the
expected named entity tags, when they are
available, or on the larger subpart without any
terms.
3 Search engine evaluation
The second module of the QALC system deals
with the selection, through a search engine, of
documents that may contain an answer to a
given question from the whole TREC corpus
(whose size is about 3 gigabytes).
We tested three search engines with the 200
questions that were proposed at the TREC8 QA
track. The first one is Zprise, a vectorial search
engine developed by NIST. The second is
Indexal (de Loupy et al1998), a pseudo-boolean
search engine developed by Bertin
Technologies1. The third search engine is ATT
whose results to the TREC questions are
provided by NIST in the form of ranked lists of
the top 1000 documents retrieved for each
question. We based our search engine tests on
                                                           
1 We are grateful to Bertin Technologies for providing us
with the outputs of Indexal on the TREC collection for the
TREC8-QA and TREC9-QA question set.
the list of relevant documents extracted from the
list of correct answers provided by TREC
organizers.
Since a search engine produces a large
ranked list of relevant documents, we had to
define the number of documents to retain for
further processing. Indeed, having too many
documents leads to a question processing time
that is too long, but conversely, having too few
documents reduces the possibility of obtaining
the correct answer. The other goal of the tests
obviously was to determine the best search
engine, that is to say the one that gives the
highest number of relevant documents.
3.1 Document selection threshold
In order to determine the best selection
threshold, we carried out four different tests
with the Zprise search engine. We ran Zprise for
the 200 questions and then compared the
number of relevant documents respectively in
the top 50, 100, 200, and 500 retrieved
documents. Table 1 shows the test results.
Selection
Threshold
Questions with
relevant
documents
Questions with
no relevant
documents
50 181 19
100 184 16
200 193 7
500 194 6
Table 1. Number of questions with and
without relevant documents retrieved for
different thresholds
According to Table 1, the improvement of
the search engine results tends to decrease
beyond the threshold of 200 documents. The top
200 ranked documents thus seem to offer the
best trade-off between the number of documents
in which the answer may be found and the
question processing time.
3.2 Evaluation
We compared the results given by the three
search engines for a threshold of 200
documents. Table 2 gives the tests results.
Search Engine Indexal Zprise ATT
Number of questions
with relevant
documents retrieved
182 193 194
Number of questions
without relevant
documents retrieved
18 7 6
Total number of
relevant documents
that were retrieved
814 931 1021
Table 2. Compared performances of the
Indexal, Zprise and ATT search engines
All three search engines perform quite well.
Nevertheless, the ATT search engine revealed
itself the most efficient according to the
following two criteria: the lowest number of
questions for which no relevant document was
retrieved, and the most relevant documents
retrieved for all the 200 questions. Both criteria
are important. First, it is most essential to obtain
relevant documents for as many questions as
possible. But the number of relevant documents
for each question also counts, since having more
sentences containing the answer implies a
greater probability to actually find it.
4 Document ranking evaluation
As the processing of 200 documents by the
following Natural Language Processing (NLP)
modules still was too time-consuming, we
needed an additional stronger selection. The
selection of relevant documents performed by
the re-indexing and selection module relies on
an NLP-based indexing composed of both
single-word and phrase indices, and linguistic
links between the occurrences and the original
terms. The original terms are extracted from the
questions. The tool used for extracting text
sequences that correspond to occurrences or
variants of these terms is FASTR (Jacquemin,
1999). The ranking of the documents relies on a
weighted combination of the terms and variants
extracted from the documents. The use of multi-
words and variants for document weighting
makes a finer ranking possible.
The principle of the selection is the
following: when there is a sharp drop of the
documents weight curve after a given rank, we
keep only those documents which occur before
the drop. Otherwise, we arbitrarily keep the first
100.
In order to evaluate the efficiency of the
ranking process, we proceeded to several
measures. First, we apply our system on the
material given for the TREC8 evaluation, one
time with the ranking process, and another time
without this process. 200 documents were
retained for each of the 200 questions. The
system was scored by 0.463 in the first case, and
by 0.452 in the second case. These results show
that document selection slightly improves the
final score while much reducing the amount of
text to process.
However, a second measurement gave us
more details about how things are improved.
Indeed, when we compare the list of relevant
documents selected by the search engine with
the list of ranker-selected ones, we find that the
ranker loses relevant documents. For thirteen
questions among the 200 in the test, the ranker
did not consider relevant documents selected by
the search engine. What happens is: the global
score improves, because found answers rank
higher, but the number of found answers
remains the same.
The interest to perform such a selection is
also illustrated by the results given Table 3,
computed on the TREC9 results.
Number of documents
selected by ranking
100 <<100
Distribution among the
questions
342
(50%)
340
(50%)
Number of correct
answers
175
(51%)
200
(59%)
Number of correct answer
at rank 1
88
(50%)
128
(64%)
Table 3. Evaluation of the ranking process
We see that the selection process discards
documents for 50% of the questions: 340
questions are processed from less than 100
documents. For those 340 questions, the average
number of selected documents is 37. The
document set retrieved for those questions has a
weight curve with a sharp drop. QALC finds
more often the correct answer and in a better
position for these 340 questions than for the 342
remaining ones. These results are very
interesting when applying such time-consuming
processes as named-entities recognition and
question/sentence matching. Document selection
will also enable us to apply further sentence
syntactic analysis.
5 Question-sentence pairing evaluation
We sent to TREC9 two runs which gave
answers of 250 characters length, and one run
which gave answers of 50 characters length. The
first and the last runs used ATT as search
engine, and the second one, Indexal. Results are
consistent with our previous analysis (see
Section 3.2). Indeed, the run with ATT search
engine gives slightly better results (0.407 strict)2
than those obtained with the Indexal search
engine (0.375 strict).  Table 4 sums up the
number of answers found by our two runs.
Rank of the correct
answer retrieved
Run using
ATT
Run using
Indexal
1 216 187
2 to 5 159 185
Total of correct
answers retrieved
375 372
No correct answer
retrieved
307 310
Table 4. Number of correct answers retrieved,
by rank, for the two runs at 250 characters
The score of the run with answers of 50
characters length was not encouraging,
amounting only 0.178, with 183 correct answers
retrieved3.
5.1 Long answers
From results of the evaluation concerning
document ranking, we see that the performance
level of the question-sentence matcher depends
partly on the set of sentences it has parsed, and
not only on the presence, or absence, of the
answer within these sentences. In other words,
we do not find the answer each time it is in the
set of selected sentences, but we find it easily if
there are few documents (and then few
sentences) selected. That is because similarity
                                                           
2 With this score, the QALC system was ranked 6th among
25 participants at TREC9 QA task for answers with 250
characters length.
3 With this score, the QALC system was ranked 19th
among 24 participants at TREC9 QA task for answers with
50 characters length.
assessment relies upon a small number of
criteria, which are found to be insufficiently
discriminant. Therefore, several sentences
obtain the same mark, in which case, the rank of
the correct answer depends on the order in
which sentences are encountered.
This is something we cannot yet manage, so
we evaluated the matcher?s performance,
without any regard to the side effect induced by
document processing order. As remarked in 3.2,
search engines perform well. In particular, ATT
retains relevant documents, namely, those that
yield good answers, for 97 percent of the
questions. The ranker, while improving the final
score, loses some questions. After it stepped in,
the system retains relevant documents for 90%
of the questions. The matcher finds a relevant
document in the first five answers for 74% of
the questions, but answers only 62% of them
correctly.  Finding the right document is but one
step, knowing where to look inside it is no
obvious task.
5.2 Short answers
A short answer is selectively extracted from
a long one. We submitted this short answer
selector (under 50 characters) to evaluation
looking for the impact of the expected answer
type. Among TREC questions, some expect an
answer consisting of a named entity: for instance
a date, a personal or business name. In such
cases, assigning a type to the answer is rather
simple, although it implies the need of a good
named entity recognizer. Answers to other
questions (why questions for instance, or some
sort of what questions), however, will consist of
a noun or sentence. Finding its type is more
complex, and is not done very often.
Some systems, like FALCON (Harabagiu et
al 2000) use Wordnet word class hierarchies to
assign types to answers. Among 682 answers in
TREC9, 57.5% were analysed by our system as
named-entity questions, while others received
no type assignment. Among answers from our
best 250-character run, 62.7% were about
named entities. However, our run for shorter
answers, yielding a more modest score, gives
84% of named-entities answers. In our system
answer type assignment is of surprisingly small
import, where longer answers are concerned.
However, it does modify the selecting process,
when the answer is extracted from a longer
sentence.
Such evaluations help us to see more clearly
where our next efforts should be directed.
Having more criteria in the similarity
measurement would, in particular, be a source of
improvement.
6 Discussion
We presented quantitative evaluations. But since
we feel that evaluations should contribute to
improvements of the system, more qualitative
and local ones also appear interesting.
TREC organizers send us, along with run
results, statistics about how many runs found the
correct answer, and at which rank. Such
statistics are useful in many ways. Particularly,
they provide a characterisation of a posteriori
difficult questions. Knowing that a question is a
difficult one is certainly relevant when trying to
answer it. Concerning this problem, de Loupy
and Bellot (2000) proposed an interesting set of
criteria to recognize a priori difficult questions.
They use word frequency, multi-words,
polysemy (a source of noise) and synonymy (a
source of silence). They argue that an
?intelligent? system could even insist that a
question be rephrased when it is too difficult.
While their approach is indeed quite promising,
we consider that their notion of a priori
difficulty should be complemented by the notion
of a posteriori difficulty we mentioned: the two
upcoming examples of queries show that a
question may seem harmless at first sight, even
using de Loupy and Bellot?s criteria, and still
create problems for most systems.
From these statistics, we also found
disparities between our system and others for
certain questions. At times, it finds a good
answer where most others fail and obviously the
reverse also happens. This is the case in the two
following examples. The first one concerns an
interesting issue in a QA system that is the
determination of which terms from the question
are to be selected for the question-answer
pairing. This is particularly important when the
question has few words. For instance, to the
question  How far away is the moon?, our term
extractor kept not only moon?(NN), but also
away?(RB) . Moreover, our question parser
knows that how far is an interrogative phrase
yielding a LENGTH type for the answer. This
leads our system to retrieve the correct answer:
With a 28-power telescope, you can see it on the
moon 250,000 miles away4.
The second example concerns the relative
weight of the terms within the question. When a
proper noun is present, it must be found in the
answer, hence an important weight for it. Look
at the question Who manufactures the software,
? ?PhotoShop??? . The term extractor kept
s o f t w a r e ( N N ) , PhotoShop(NP),  a n d
manufacture(VBZ) as terms to be matched, but
the matcher assigns equal weights to them, so
we could not find the answer5. Later, we
modified these weights, and the problem was
solved.
Indeed, evaluation corpus seems to be
difficult to build. Apart from the problem of the
question difficulty level, question type
distribution may also vary from a corpus to
another. For instance, we note that TREC8
proposed much more questions with named
entity answer type (about 80%) than TREC9
(about 60%). Thus, some participants who trains
their systems on the TREC8 corpus were
somehow disapointed by their results at TREC9
with regards with their training results (Scott
and Gaizauskas, 2000).
However, it is generally hard to predict what
will happen if we modify the system. A local
improvement can result in a loss of performance
for other contexts. Although the system?s
complexity cannot be reduced to just two levels
(a local one and a global one), this can be an
efficient step in the design of improvements to
the whole system via local adjustments. But this
is a very frequent situation in engineering tasks.
7 Conclusion and perspectives
Each evaluation reflects a viewpoint, underlying
the criterion we use. In our case, the choice of
criteria was guided by the existence of two main
stages in the QA process, namely the selection
of relevant documents and the selection of the
answer among the selected documents
sentences. Sometimes, such criteria concur in
                                                           
4 Among the 42 runs using 250 byte limit, submitted at
TREC9-QA, only seven found the correct answer at rank 1,
and 27 do not found it.
5 22 runs, out of 42 found the right answer at rank 1. Only
9 were unable to find it.
revealing the same positive or negative feature
of the system. They can also yield a more
precise assessment of the reasons behind these
features, as was the case in our evaluation of the
ranker. Moreover, when a system consists of
several modules, their specific evaluations
should imply different criteria.
This is particularly true in dialogue systems,
where different kinds of processes are co-
operating. Since information retrieval is an
interactive task, it seems natural to associate a
dialogue component to it. Indeed, users tend to
ask a question, evaluate the answer, and
reformulate their question to make it more
specific (or, contrariwise, more general, or quite
different). A QA system is, therefore, a good
applicative setting for a dialogue module.
Quantitative assessment of the QA system
would be useful in assessing the dialogue system
in this particular context. Such a global
assessment would provide an objective
judgement about whether the task (finding the
answer) was achieved, or not. Successfulness in
a task is a necessary component of the
evaluation, nevertheless it is just a part of it.
Obviously, dialogue evaluation is also a matter
of cost (time, number of exchanges) and of user-
friendliness (cognitive ergonomy).
However, objectivity is almost impossible to
attain in these domains. In a recent debate
(LREC 2000), serious objections about natural
language tools evaluation and validation were
developed e.g. by Sabah (2000). The main issue
he raises is about the great complexity of such
systems. However, we consider that by going as
far as possible in the experimental search for
evaluation criteria, we also make a meaningful
contribution to this debate. While it is true that
complexity should never be ignored, we
consider that, by successive approximate
modelisation and evaluation cycles, we can
capture some of it at each step of our system?s
developement.
References
CELEX. 1998.
http://www.ldc.upenn.edu/readme_files/celex.read
me.html. Consortium for Lexical Resources,
UPenns, Eds.
CLR. 1998. http://crl.nmsu.edu/cgi-
bin/Tools/CLR/clrcat#D3. Consortium for Lexical
Resources, NMSUs, Eds., New Mexico.
Fabre C., Jacquemin C, 2000. Boosting variant
recognition with light semantics. Proceedings
COLING?2000, pp. 264-270, Luxemburg.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA, MIT Press.
Ferret O., Grau B., Hurault-Plantet M., Illouz G.,
Jacquemin C. (2000), QALC ? the Question-
Answering system of LIMSI-CNRS. Pre-
proceedings of TREC9, NIST, Gaithersburg, CA.
Harabagiu S., Pasca M., Maiorano J. 2000.
Experiments with Open-Domain Textual Question
Answering. Proceedings of Coling'2000,
Saarbrucken, Germany.
Jacquemin C. 1999. Syntagmatic and paradigmatic
representations of term variation. Proceedings of
ACL'99. 341-348.
de Loupy C., Bellot P., El-B?ze M., Marteau P.-F..
Query Expansion and Classification of Retrieved
Documents, TREC7 (1998), 382-389.
de Loupy C., Bellot P. 2000. Evaluation of
Document Retrieval Systems and Query
Difficulty. Proceedings of the Second
International Conference on Language Resources
and Evaluation (LREC 2000) Workshop, Athens,
Greece. 32-39.
Sabah G. 2000 To validate or not to validate? Some
theoretical difficulties for a scientific evaluation of
natural language processing systems. Proceedings
of the Second International Conference on
Language Resources and Evaluation (LREC 2000)
Workshop, Athens, Greece. 58-61.
Scott S., Gaizauskas R. 2000. University of Sheffield
TREC-9 Q & A System. Pre-proceedings of
TREC9, NIST, Gaithersburg, CA. 548-557.
Voorhees E., Tice D. 2000. Implementing a Question
Answering Evaluation. Proceedings of the Second
International Conference on Language Resources
and Evaluation (LREC 2000). Athens, Greece. 40-
45.
Terminological variants for document selection and
question/answer matching
Olivier Ferret Brigitte Grau Martine Hurault-Plantet
Gabriel Illouz Christian Jacquemin
LIMSI-CNRS
Bat.508 Universit? ParisXI
91403 Orsay, France
{ferret, grau, mhp, gabrieli, jacquemin}@limsi.fr
Abstract
Answering precise questions requires
applying Natural Language techniques
in order to locate the answers inside
retrieved documents. The QALC
system, presented in this paper,
participated to the Question Answering
track of the TREC8 and TREC9
evaluations. QALC exploits an analysis
of documents based on the search for
multi-word terms and their variations.
These indexes are used to select a
minimal number of documents to be
processed and to give indices when
comparing question and sentence
representations. This comparison also
takes advantage of a question analysis
module and recognition of numeric and
named entities in the documents.
1 Introduction
The Question Answering (QA) track at TREC8
and TREC9 is due to the recent need for more
sophisticated paradigms in Information
Retrieval (IR). Question answering generally
refers to encyclopedic or factual questions that
require concise answers. But current IR
techniques do not yet enable a system to give
precise answers to precise questions. Question
answering is thus an area of IR that calls for
Natural Language Processing (NLP) techniques
that can provide rich linguistic features as
output. Such NLP modules should be deeply
integrated in search and matching components
so that answer selection can be performed on
such linguistic features and take advantage of
them. In addition, IR and NLP techniques have
to collaborate in the resulting system in order to
cope with large-scale and broad coverage text
databases while deriving benefit from added
knowledge.
We developed a system for question
answering, QALC, evaluated in the framework
of the QA tracks at TREC8 and TREC9. The
QALC system comprises NLP modules for
multi-word term and named entity extraction
with a specific concern for term conflation
through variant recognition. Since named entity
recognition has already been described
extensively in other publications (Baluja 1999),
we present the contribution of terminological
variants to adding knowledge to our system.
The two main activities involving
terminology in NLP are term acquisition and
term recognition. Basically, terms can be viewed
as a particular type of lexical data. Term
variation may involve structural, morphological,
and semantic transformations of single or multi-
words terms (Fabre and Jacquemin, 2000).
In this paper, we describe how QALC uses
high level indexes, made of terms and variants,
to select among documents the most relevant
ones with regard to a question, and then to
match candidate answers with this question. In
the selection process, the documents first
retrieved by a search engine, are then
postfiltered and ranked through a weighting
scheme based on high level indexes, in order to
retain the top ranked ones. Similarly, all systems
that participated in TREC9 have a search engine
component that firstly selects a subset of the
provided database of about one million
documents. Since a search engine produces a
ranked list of relevant documents, systems then
have to define the highest number of documents
to retain. Indeed, having too many documents
leads to a question processing time that is too
long, but conversely, having too few documents
reduces the possibility of obtaining the correct
answer. For reducing the amount of text to
process, one approach consists of keeping one or
more relevant text paragraphs from each
document retrieved. Kwok et al(2000), for
instance use an IR engine that retrieves the top
300 sub-documents of about 300-550 words and,
on the other hand, the FALCON system
(Harabagiu et al 2000) performs a paragraph
retrieval stage after the application of a boolean
retrieval engine. These systems work on the
whole database and apply a bag-of-words
technique to select passages whereas QALC first
retains a large subset of documents, among
which it then selects relevant documents by
applying richer criteria based on the use of the
linguistic structures of the words.
QALC indexes, used for document selection,
are made of single and multi-word terms
retrieved by a 2-step procedure: (1)?automatic
term extraction from questions through part-of-
speech tagging and pattern matching and
(2)?automatic document indexing through term
recognition and variant conflation. As a result,
linguistic variation is explicitly addressed
through the exploitation of word paradigms,
contrarily to other approaches like the one taken
in COPSY (Schwarz 1988) where an
approximate matching technique between the
query and the documents implicitly takes it into
account. Finally, terms acquired at step?(1) and
indexes from step?(2) are also used by the
matching procedure between a question and the
relevant document sentences.
In the next section, we describe the
architecture of the QALC system. Then, we
present the question processing for term
extraction. We continue with the description of
FASTR, a transformational shallow parser that
recognizes and marks the extracted terms as well
as their linguistic variants within the documents.
The two following sections present the modules
of the QALC system where terms and variants
are used, namely the document selection and
question/answer matching modules. Finally, we
present the results obtained by the QALC
system as well as an evaluation of the
contribution of this NLP technique to the QA
task through the use of the reference collections
for the QA track. In conclusion, suggestions for
more ambitious, but still realistic, developments
using NLP are outlined.
2 System Overview
Natural Language Processing components in the
QALC system (see Figure 1) enrich the selected
documents with terminological indexes in order
to go beyond reasoning about single words. Rich
linguistic features are also used to deduce what a
question is about.
Tagged Questions:
Named entity tags
Vocabulary &
  frequencies
Named entity
 recognition
Candidate
terms
Retrieved
documents
Tagged sentences: named entity
    tags and term indexation
Ordered sequences of 250 and
           50 characters
Question analysis Search engine
Questions
Subset of ranked documents
Corpus
Re-indexing and selection of
      documents (FASTR)
Question/Sentence pairing
Figure 1. The QALC system
The analysis of a question relies on a shallow
parser which spots discriminating patterns and
assigns categories to the question. The
categories correspond to the types of entities that
are likely to constitute the answer to the
question.
In order to select the best documents from
the results given by the search engine and to
locate the answers inside them, we work with
terms and their variants, i.e. morphologic,
syntactic and semantic equivalent expressions.
A term extractor has been developed, based on
syntactic patterns which describe complex
nominal phrases and their subparts. These terms
are used by FASTR (Jacquemin 1999), a
shallow transformational natural language
analyzer that recognizes their occurrences and
their variants. Each occurrence or variant
constitutes an index that is subsequently used in
the processes of document ranking and
question/document matching.
Documents are ordered according to a weight
computed thanks to the number and the quality
of the terms and variants they contain. For
example, original terms with proper names are
considered more reliable than semantic variants.
An analysis of the weight graph enables the
system to select a relevant subpart of the
documents, whose size varies along the
questions. This selection takes all its importance
when applying the last processes which consist
of recognizing named-entities and analyzing
each sentence to decide whether it is a possible
answer or not. As such processes are time
consuming we attempt to limit their application
to a minimal number of documents.
Named entities are recognized in the
documents and used to measure the similarity
between the document sentences and a question.
Named entities receive one of the following
types: person, organization, location (city or
place), number (a time expression or a number
expression). They are defined in a way similar to
the MUC task and recognized through a
combination of lexico-syntactic patterns and
significantly large lexical data.
Finally, the question/answer matching
module uses all the data extracted from the
questions and the documents by the preceding
modules. We developed a similarity measure
that attributes weights to each characteristic, i.e.
named entity tags and terms and variants, and
makes a combination of them. The QALC
system proposes long and short answers.
Concerning the short ones, the system focuses
on parts of sentences that contain the expected
named entity tags, when they are known, or on
the largest subpart without any terms of the
question.
3 Terms and Variants
3.1 Term extraction
For automatic acquisition of terms from
questions, we use a simple technique of filtering
through patterns of part-of-speech categories.
No statistical ranking is possible because of the
small size of the questions from which terms are
extracted. First, questions are tagged with the
help of the TreeTagger (Schmid 1999). Patterns
of syntactic categories are then used to extract
terms from the tagged questions. They are very
close to those described by Justeson and
Katz?(1995), but we do not include post-posed
prepositional phrases. The pattern used for
extracting terms is:
(((((JJ | NN | NP | VBG)) ? (JJ | NN | NP | VBG) (NP
| NN))) | (VBD) | (NN) | (NP) | (CD))
where NN are common nouns, NP proper nouns,
JJ adjectives, VBG gerunds, VBD past
participles and CD numeral determiners.
The longest string is acquired first and
substrings can only be acquired if they do not
begin at the same word as the superstring. For
instance, from the sequence nameNN ofIN theDT
USNP helicopterNN pilotNN shotVBD downRP,
the following four terms are acquired: U S
helicopter pilot, helicopter pilot, pilot, and
shoot.
The mode of acquisition chosen for terms
amounts to considering only the substructures
that correspond to an attachment of modifiers to
the leftmost constituents (the closest one). For
instance, the decomposition of US helicopter
pilot into helicopter pilot and pilot is equivalent
to extracting the subconstituents of the structure
[US [helicopter [pilot]]].
3.2 Variant recognition through FASTR
The automatic indexing of documents is
performed by FASTR (Jacquemin 1999), a
transformational shallow parser for the
recognition of term occurrences and variants.
Terms are transformed into grammar rules and
the single words building these terms are
extracted and linked to their morphological and
semantic families.
The morphological family of a single word w
is the set M(w) of terms in the CELEX database
(CELEX 1998) which have the same root
morpheme as w. For instance, the morphological
family of the noun maker is made of the nouns
maker, make and remake, and the verbs to make
and to remake.
The semantic family of a single word w is the
union S (w ) of the synsets  of WordNet1.6
(Fellbaum 1998) to which w belongs. A synset is
a set of words that are synonymous for at least
one of their meanings. Thus, the semantic family
of a word w is the set of the words w' such that
w' is considered as a synonym of one of the
meanings of w. The semantic family of maker,
obtained from WordNet1.6, is composed of
three nouns: maker, manufacturer, shaper and
the semantic family of c a r is car, auto,
automobile, machine, motorcar.
Variant patterns that rely on morphological
and semantic families are generated through
metarules. They are used to extract terms and
variants from the document sentences in the
TREC corpus. For instance, the following
pattern, named NtoSemArg, extracts the
occurrence making many automobiles as a
variant of the term car maker:
VM('maker') RP? PREP? (ART (NN|NP)? PREP)?
ART? (JJ?|?NN?|?NP |?VBD?|?VBG)[0-3] NS('car')
where RP are particles, PREP prepositions, ART
articles, and VBD, VBG verbs. VM('maker') is
any verb in the morphological family of the
noun maker and NS('car') is any noun in the
semantic family of car.
Relying on the above morphological and
semantic families, auto maker, auto parts
maker , car manufacturer, make autos, and
making many automobiles are extracted as
correct variants of the original term car maker
through the set of metarules used for the QA
track experiment. Unfortunately, some incorrect
variants are extracted as well, such as make
those cuts in auto produced by the preceding
metarule.
3.3 Document selection
The output of NLP-based indexing is a list of
term occurrences composed of a document
identifier d, a term identifier?a pair t(q,i)
composed of a question number q and a unique
index i?, a text sequence, and a variation
identifier v (a metarule). For instance, the
following index :
LA092690-0038 t(131,1)
making many automobiles NtoVSemArg
means that the occurrence making many
automobiles from document d=LA092690-0038
is obtained as a variant of term i=1 in question
q=131 (car maker) through the variation
NtoVSemArg given in Section 3.2.
Each document d selected for a question q is
associated with a weight. The weighting scheme
relies on a measure of quality of the different
families of variations described by
Jacquemin?(1999): non-variant occurrences are
weighted 3.0, morphological and morpho-
syntactic variants are weighted 2.0, and
semantic and morpho-syntactico-semantic
variants are weighted 1.0.
Since proper names are more reliable indices
than common names, each term t(q,i) receives a
weight P(t(q , i )) between 0 and 1.0
corresponding to its proportion of proper names.
For instance, President Cleveland's wife is
weighted 2/3=0.66. Since another factor of
reliability is the length of terms, a factor |t(q,i)|
in the weighting formula denotes the number of
words in term t(q,i). The weight Wq(d) of a
query q  in a document d  is given by the
following formula (1). The products of the
weightings of each term extracted by the indexer
are summed over the indices I(d) extracted from
document d and normalized according to the
number of terms |T(q)| in query q.
  
W (d)
( ) ( ( ( , ))) ( , )
( )
q
( ( , ), ) ( )
=
? + ?
?
? w v P t q i t q i
T q
t q i v I d
1 2
         (1)
Mainly two types of weighting curves are
observed for the retrieved documents: curves
with a plateau and a sharp slope at a given
threshold (Figure 2.a) and curves with a slightly
decreasing weight (Figure 2.b).
The edge of a plateau is detected by examining
simultaneously the relative decrease of the slope
with respect to the preceding one, and the
relative decrease of the value with respect to the
preceding one. When a threshold is detected, we
only select documents before this threshold,
otherwise a fixed cutoff threshold is used. In our
experiments, for each query q, the 200 best
ranked documents retrieved by the search
engine1 were subsequently processed by the re-
indexing module. Our studies (Ferret et al 2000)
show that 200 is a minimum number such as
almost all the relevant documents are kept.
When no threshold was detected, we fixed the
value of the threshold to 100.
0
0
10
10
20
20
30
30
40
40
50
50
60
60
70
70
80
80
90
90
100
100
0
0
1
1
2
2
3
3
4
4
5
5
6
6
7
8
9
10
rank of the document
w
ei
gh
t
Question #87
rank of the document
Truncation of the ranked list
Question #86
w
ei
gh
t
(a)
(b)
Figure 2. Two types of weighting curve.
Through this method, the cutoff threshold is
8 for question #87 (Who followed Willy Brandt
as chancellor of the Federal Republic of
Germany?, Figure 2(a))2 and 100 for question
#86 (Who won two gold medals in skiing in the
Olympic Games in Calgary?, Figure 2(b)). As
indicated by Figure??2(a), there is an important
difference of weight between documents #8 and
#9. The weight of document #8 is 9.57 while the
                                                           
1 We used in particular Indexal (Loupy et al1998), a search
engine provided by Bertin Technologie.
2 Questions come from the TREC8 data.
weight of document #9 is 7.29 because the term
Federal Republic only exists in document #8.
This term has a high weight because it is
composed of two proper names.
4 Question-Answer Matching
4.1 Question type categorization
Question type categorization is performed in
order to assign features to questions and use
these features for the similarity measurement
between a question and potential answer
sentences. Basically, question categorization
allows the prediction of the kind(s) of answer,
called target (for instance, NUMBER).
Sentences inside the retrieved documents are
labeled with the same tags as questions. During
the similarity measurement, the more the
question and a sentence share the same tags, the
more they are considered as involved in a
question-answer relation. For example:
Question:
How many people live in the Falklands?
?> target = NUMBER
Answer:
F a l k l a n d s  p o p u l a t i o n  o f  <bnumex
TYPE=NUMBER> 2,100 <enumex> is
concentrated.
We established 17 types of answer. Some
systems define more categories. For instance
Prager et al (2000) identify about 50 types of
answer.
4.2 Answer Selection
In the QALC system, we have taken the
sentence as a basic unit because it is large
enough to contain the answer to questions about
simple facts and to give a context that permits
the user to judge if the suggested answer is
actually correct. The module associates each
question with the Na most similar sentences (Na
is equal to 5 for the QA task at TREC).
The overall principle of the selection process
is the following: each sentence from the
documents selected for a question is compared
with this question. To perform this comparison,
sentences and questions are turned into vectors
that contain three kinds of elements: content
words, term identifiers and named entity tags. A
specific weight (between 0 and 1.0) is associated
with each of these elements in order to express
their relative importance.
The content words are the lemmatized forms
of mainly adjectives, verbs and nouns such as
they are given by the TreeTagger. Each content
word in a vector is weighted according to its
degree of specificity in relation to the corpus in
which answers are searched through the tf.idf
weighting scheme. For questions, the term
identifiers refer to the terms extracted by the
term extractor described in Section?3.1 and
receive a fixed weight. In sentence vectors, term
identifiers are associated with the normalized
score from the ranking module (see Section 3.3).
The named entity tags correspond to the possible
types of answers, provided by the question
analysis module. In each sentence these tags
delimit the named entities that were recognized
by the corresponding module of the QALC
system and specify their type. Unlike term
identifiers, named entity tags are given the same
fixed weight in both sentence and question
vectors because the matching module uses the
types of the named entities and not their values.
In our experiments, the linguistic features
(terms and named entities) are used to favor
appropriate sentences when they have not
enough content words in common with the
question or when the question only contains a
few content words. Thus, the weights of term
identifiers or named entity tags are reduced by
applying a coefficient in order to be globally
lower than the weights of the content words.
Finally, the comparison between a sentence
vector Vd and a question vector Vq is achieved
by computing the following similarity measure:
?
?
=
j j
i i
dq
wq
wd
VVsim ),( (2)
where wqj is the weight of an element in the
question vector and wdi is the weight of an
element in a sentence vector that is also in the
question vector. This measure evaluates the
proportion and the importance of the elements in
the question vector that are found in the
sentence vector with regards to all the elements
of the question vector. Moreover, when the
similarity value is nearly the same for two
sentences, we favor the one in which the content
words of the question are the least scattered.
The next part gives an example of the
matching operations for the TREC8 question
Q16 What two US biochemists won the Nobel
Prize in medicine in 1992? This question is
turned into the following vector:
two (1.0) US (1.0) biochemist (0.9)
nobel (1.0) prize (0,6) medicine (0,5)
win (0,3) 1992 (1.0) <PERSON> (0.5)
16.01 (0.5) 16.04 (0.5)
where <PERSON> is the expected type of the
answer, 16.01 is the identifier of the U S
biochemist term and 16.04 is the identifier of the
Nobel Prize term.
The same kind of vector is built for the
sentence <NUMBER> Two </NUMBER> US
biochemists, <PERSON> Edwin Krebs
</PERSON> and <CITY> Edmond </CITY>
Fischer, jointly won the <NUMBER> 1992
</NUMBER> Nobel Medicine Prize for work
that could advance the search for an anti-cancer
drug, coming from the document FT924-14045
that was selected for the question Q163 :
two (1.0) US (1.0) biochemist (0.9)
nobel (1.0) prize (0,6) medicine (0,5)
win (0,3) 1992 (1.0) Edwin (0.0)
Krebs (0.0) Edmond (0.0) Fischer (0.0)
work (0.0) advance (0.0) search (0.0)
anti-cancer (0.0) jointly (0.0) drug (0.0)
<PERSON> (0.5) <NUMBER> (0.0) <CITY>(0.0)
16.01 (0.5) 16.04 (0.3)
where the weight 0.0 is given to the elements
that are not part of the question vector. The term
US biochemist is found with no variation and
Nobel Prize appears as a syntactic variant.
Finally, according to (2), the similarity measure
between theses two vectors is equal to 0.974.
5 Results and Evaluation
We sent to TREC9 three runs whose variations
concern the searched engine used and the length
of the answer (250 or 50 characters). Among
those runs, the best one obtained a score of
0.407 with 375 correct answers among 682
questions, for answers of 250 characters length.
The score computed by NIST is the reciprocal
mean of the rank, from 1 to 5, of the correct
                                                           
3 This sentence is taken from the output of the named entity
recognizer.
answer. With this score, the QALC system was
ranked 6th among 25 participants at TREC 9
QA task.
Document selection relies on a quantitative
measure, i.e. the document weight, whose
computation is based on syntactic and semantic
indices, i.e. the terms and the terminological
variants. Those indices allow the system to take
into account words as well as group of words
and their internal relations within the
documents. Following examples, that we have
got from selected documents for TREC9 QA
task, show what kind of indices are added to the
question words.
For the question 252 When was the first flush
toilet invented? , one multi-word extracted term
is flush toilet. This term is marked by FASTR
when recognized in a document, but it is also
marked when a variant is found, as for instance
low-flush toilet in the following document
sentence where low-flush is recognized as
equivalent to flush:
Santa Barbara , Calif. , is giving $ 80 to
anyone who converts to a low-flush toilet.
252.01   flush toilet[JJ][NN]
             low-flush[flush][JJ] toilet[toilet][NN]
             1.00
In the given examples, after the identification
number of the term, appears the reference term,
made of the lemmatized form of the words and
their syntactic category, followed by the variant
found in the sentence, with each word, its
lemmatized form and its category, and finally its
weight.
In the example above, the term found in the
sentence is equivalent to the reference term, and
thus its weight is 1.00.
The second example shows a semantic
variant. Salary and average salary are terms
extracted from the question 337, What's the
average salary of a professional baseball player
?. The semantic variant pay, got from WordNet,
was recognized in the following sentence?:
Did the NBA union opt for the courtroom
because its members, whose average pay tops
$500000 a year, wouldn't stand still for a
strike over free agency ?
337.01    salary[NN] pay[pay][NN] 0.25
337.00    average [JJ]salary[NN]
               average[average][JJ] pay[pay][NN]
               0.40
In order to evaluate the efficiency of the
selection process, we proceeded to several
measures. We apply our system on the material
given for the TREC8 evaluation, one time with
the selection process, and another time without
this process. At each time, 200 documents were
returned by the search engine for each of the 200
questions. When selection was applied, at most
100 documents were selected and subsequently
processed by the matching module. Otherwise,
the 200 documents were processed. The system
was scored by 0.463 in the first case, and by
0.452 in the second case. These results show
that the score increases when processing less
documents above all because it is just the
relevant documents that are selected.
The benefit from performing such a selection
is also illustrated by the results given in Table 1,
computed on the TREC9 results.
Number of documents selected
by ranking
100 <<100
Distribution among the
questions
342
(50%)
340
(50%)
Number of correct answers 175
(51%)
200
(59%)
Number of correct answer at
rank 1
88
(50%)
128
(64%)
Table 1. Evaluation of the ranking process
We see that the selection process discards a
lot of documents for 50% of the questions (340
questions are processed from less than 100
documents). The document set retrieved for
those questions had a weighting curve with a
sharp slope and a plateau as in Figure 2(a).
QALC finds more often the correct answer and
in a better position for these 340 questions than
for the 342 remaining ones. The average number
of documents selected, when there are less than
100, is 37. These results are very interesting
when applying such time-consuming processes
as  named ent i ty  recogni t ion and
question/sentence matching. Document selection
will also enable us to apply later on syntactic
and semantic sentence analysis.
6 Conclusion
The goal of a question-answering system is to
find an answer to a precise question, with a
response time short enough to satisfy the user.
As the answer is searched within a great amount
of documents, it seems relevant to apply mainly
numerical methods because they are fast. But, as
we said in the introduction, precise answers
cannot be obtained without adding NLP tools to
IR techniques. In this paper, we proposed a
question answering system which uses
terminological variants first to reduce the
number of documents to process while
increasing the system performance, and then to
improve the matching between a question and its
potential answers. Furthermore, reducing the
amount of text to process will afterwards allow
us to apply more complex methods such as
semantic analysis. Indeed, TREC organizers
foresee a number of possible improvements for
the future?: real-time answering, evaluation and
justification of the answer, completeness of the
answer which could result from answers
distributed along multiple documents, and
finally interactive question answering so that the
user could specify her/his intention. All those
improvements require more data sources as well
as advanced reasoning about pragmatic and
semantic knowledge.
Thus, the improvements that we now want to
bring to our system will essentially pertain to a
semantic and pragmatic approach. For instance,
WordNet that we already use to get the semantic
variants of a word, will be exploited to refine
our set of question types. We also plan to use a
shallow syntactico-semantic parser in order to
construct a semantic representation of both the
potential answer and the question. This
representation will allow QALC to select the
answer not only from the terms and variants but
also from the syntactic and semantic links that
terms share with each other.
References
Baluja, S., Vibhu O. M., Sukthankar, R. 1999
Applying machine learning for high performance
named-entity extraction. P r o c e e d i n g s
PACLING'99 Waterloo, CA. 365-378.
CELEX. 1998.
http://www.ldc.upenn.edu/readme_files/celex.read
me.html. Consortium for Lexical Resources,
UPenns, Eds.
Fabre C., Jacquemin C, 2000. Boosting variant
recognition with light semantics. Proceedings
COLING?2000, pp. 264-270, Luxemburg.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA, MIT Press.
Ferret O., Grau B., Hurault-Plantet M., Illouz G.,
Jacquemin C. (2000), QALC ? the Question-
Answering system of LIMSI-CNRS, pre-
proceedings of TREC9, NIST, Gaithersburg, CA.
Harabagiu S., Pasca M., Maiorano J. 2000.
Experiments with Open-Domain Textual Question
Answering. Proceedings of  Coling'2000,
Saarbrucken, Germany.
Jacquemin C. 1999. Syntagmatic and paradigmatic
representations of term variation. Proceedings of
ACL'99. 341-348.
Justeson J., Katz S. 1995. Technical terminology:
some linguistic properties and an algorithm for
identification in texte. Natural Language
Engineering. 1: 9-27.
Kwok K.L., Grunfeld L., Dinstl N., Chan M. 2000.
TREC9 Cross Language, Web and Question-
Answering Track experiments using PIRCS. Pre-
proceedings of TREC9, Gaithersburg, MD, NIST
Eds. 26-35.
Loupy C. , Bellot P., El-B?ze M., Marteau P.-F..
Query Expansion and Classification of Retrieved
Documents, TREC (1998), 382-389.
Prager J., Brown, E., Radev, D., Czuba, K. (2000),
One Search Engine or two for Question-
Answering, NISTs, Eds., Proceedings of TREC9,
Gaithersburg, MD. 250-254.
Schmid H. 1999. Improvments in Part-of-Speech
Tagging with an Application To German.
Natural?Language Processing Using Very Large
Corpora, Dordrecht, S. Armstrong, K. W. Chuch,
P. Isabelle, E. Tzoukermann,  D. Yarowski, Eds.,
Kluwer Academic Publisher.
Schwarz C. 1988. The TINA Project: text content
analysis at the Corporate Research Laboratories at
Siemens. Proceedings of Intelligent Multimedia
Information Retrieval Systems and Management
(RIAO?88) Cambridge, MA. 361-368.
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 17?23
Manchester, August 2008
Human judgment as a parameter in evaluation campaigns
Jean-Baptiste Berthelin and Cyril Grouin and Martine Hurault-Plantet and Patrick Paroubek
LIMSI-CNRS
BP 133
F-91403 Orsay Cedex
firstname.lastname@limsi.fr
Abstract
The relevance of human judgment in an
evaluation campaign is illustrated here
through the DEFT text mining campaigns.
In a first step, testing a topic for a cam-
paign among a limited number of human
evaluators informs us about the feasibility
of a task. This information comes from the
results obtained by the judges, as well as
from their personal impressions after pass-
ing the test.
In a second step, results from individual
judges, as well as their pairwise matching,
are used in order to adjust the task (choice
of a marking scale for DEFT?07 and selec-
tion of topical categories for DEFT?08).
Finally, the mutual comparison of com-
petitors? results, at the end of the evalu-
ation campaign, confirms the choices we
made at its starting point, and provides
means to redefine the task when we shall
launch a future campaign based on the
same topic.
1 Introduction
For the past four years, the DEFT1 (De?fi Fouille
de Texte) campaigns have been aiming to evalu-
ate methods and software developed by several re-
search teams in French text mining, on a variety of
topics.
The different editions concerned, in this or-
der, the identification of speakers in political
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1See http://deft.limsi.fr/ for a presentation in
French.
speeches (2005), the topical segmentation of po-
litical, scientific and juridical corpora (2006), the
automatic affectation of opinion values to texts de-
veloping an argumented judgment (2007), and the
identification of the genre and topic of a docu-
ment (2008).
Human judgment was used during the prepara-
tion of the last two campaigns, to assess the dif-
ficulty of the task, and to see which parameters
could be modified. To do this, before the partic-
ipants start competing via their software, we put
human judges in front of versions of the task with
various sets of parameters. This allows us to adjust
the definition of the task according to which diffi-
culties were encountered, and how judges agree to-
gether. These human judges are in small number,
and belong to our team. However, results of the
campaign are automatically evaluated with refer-
ence to results attached to the corpus from the start.
This is because the evaluation of a campaign?s
results by human judges is expensive. For in-
stance, TREC2 international evaluation campaigns
are supported by the NIST institute and funded by
state agencies. In Europe, on the same domains,
the CLEF3 campaigns are funded by the Euro-
pean Commission, and in France, evaluation cam-
paigns are also funded by projects, such as Tech-
nolangue4. DEFT campaigns, however, are con-
ducted with small budgets. That means for us to
have selected corpora that contain the desired re-
sults. For instance, in a campaign for topical cat-
egorization, we must start with a topically tagged
corpus. By so doing, we also can, at the end of
a campaign, compare results from human judges
with results from competitors, using an identical
2http://trec.nist.gov
3http://www.clef-campaign.org
4http://www.technolangue.net
17
common reference.
In this paper, we describe experiments we per-
formed with human judgments when preparing
DEFT campaigns. We survey the various steps
in the preparation of the last two campaigns, and
we go through the detail of how human evalua-
tion, performed during these steps, led us to the
parametrization of these two campaigns. We also
present a comparative analysis of results found by
human judges and results submitted by competi-
tors in the challenge. We conclude about the rel-
evance of the human evaluation of a task, prior to
evaluating software dedicated to this task.
2 Parametrization of the campaign
We were competitors in the 2005 and 2006 edi-
tions, and became organisators for the 2007 and
2008 campaigns. For both challenges that we or-
ganized, we went through the classical steps of the
evaluation paradigm (Adda et al, 1999), to which
we systematically added a step of human test of the
task, in order to adjust those parameters that could
be modified. The steps, therefore, are following:
1. thinking about potential topics;
2. choice of a task and collection of corpora;
3. choice of measurements;
4. test of the task by human judges on an extract
of the corpus in order to precisely define its
parameters;
5. launching the task, recruiting participants;
6. testing period;
7. adjudication: possibility of complaints about
the results;
8. workshop that closes the campaign.
Whenever human judges have to evaluate the
results of participants in a campaign, the main
problems are about correctly defining the judging
criteria to be applied by judges, and that judges
be in sufficient number to vote on judging each
document. Hovy et al (2002) describe work to-
ward formalization of software evaluation method-
ology in NLP, developed in the EAGLES5 and
5http://www.ilc.cnr.it/EAGLES96/home.
html
ISLE6 projects. For cost-efficiency reasons, au-
tomatic evaluation is relevant, and its results have
sometimes been compared to results from human
judges. For instance, Eck and Hori (2005) com-
pare results of evaluation measurements used in
automatic translation with human judgments on
the same corpus. In (Burstein and Wolska, 2003),
the authors describe an experiment in the evalua-
tion of writing style and find a better agreement
between the automatic evaluation system and one
human judge, than between two human judges.
Returning to the DEFT campaign, once the task
is chosen, the corpora are collected, and evaluation
measurements are defined, there can remain some
necessity of adjusting parameters, according to the
expected difficulty of the task. This could be, for
instance, the level of granularity in a task of top-
ical segmentation, or which categories should be
relevant in a task of categorization. To get this ad-
justing done, we submit the task to human judges.
In 2007, the challenge was about the automatic
affectation of opinion values to texts developing an
argumented judgment (Grouin et al, 2007). We
collected opinion texts already tagged by an opin-
ion value, such as film reviews that, in addition
to a text giving the judgment of the critic on the
film, also feature a mark in the shape of a variable
number of stars. The adjustable parameter of the
task, therefore, is the scale of opinion values. The
task will be more or less difficult, according to the
range of this scale.
The 2008 campaign was about classifying a set
of documents by genre and topic (Hurault-Plantet
et al, 2008). The choice of genres and topics is
a crucial one. Some pairs of topics or genres are
more difficult to separate than other ones. We also
had to find different genres sharing a set of topical
categories, while corpora in French are not so very
abundant. So we selected two genres, encyclo-
pedia and daily newspaper, and about ten general
topical categories. The parameter we had to ad-
just was the set of categories to be matched against
each other.
3 Assessing the difficulty of a task
3.1 Calibration of an opinion value scale
In 2007, the challenge was about the automatic af-
fectation of opinion values to texts developing an
argumented judgment. In view of that, we col-
lected four corpora that covered various domains:
6http://www.ilc.cnr.it/EAGLES96/isle/
18
reviews of films and books, of video games and of
scientific papers, as well as parliamentary debates
about a draft law.
Each corpus had the interesting feature of com-
bining a mark or opinion with a descriptive text, as
the mark was used to sum up the judment in the
argumentative part of this text. Due to the diver-
sity of sources, we found as many marking scales
as involved copora:
? 2 values for parliamentary debates7 (the rep-
resentative who took part in the debate was
either in favour or in disfavour of the draft
law) ;
? 4 values for scientific paper reviews (accepted
as it stands ? accepted with minor changes
? accepted with major changes and second
overall reviewing ?rejected), based on a set of
criteria including interestingness, relevance
and originality of the paper?s content ;
? 5 values for film and book reviews8 (a mark
between 0 and 4, from bad to excellent) ;
? 20 values for video game reviews9 (a global
mark calculated from a set of advices about
various aspects of the game: graphics, playa-
bility, life span, sound track and scenario).
In order to, first, assess the feasibility of the task,
and to, secondly, define the scale of values to be
used in the evaluation campaign, we submitted hu-
man judges to several tests (Paek, 2001): they were
instructed to assign a mark on two kinds of scale, a
wide one with the original values, and a restricted
one with 2 or 3 values, depending on the corpus it
was applying to. The results from various judges
were evaluated in terms of precision and recall, and
matched to each other by way of the Kappa coeffi-
cient (Carletta, 1996) (Cohen, 1960).
We present hereunder the values of the ? coef-
ficient between pairs of human judges, and with
the reference, on the video game corpus. The wide
scale (Table 1) uses the original values (marks be-
tween 0 and 20), while the restricted scale (Ta-
ble 2) relies upon 3 values with following defini-
tions: class 0 for original marks between 0 and 10,
class 1 for marks between 11 and 14, and class 2
for marks between 15 and 20.
7http://www.assemblee-nationale.fr/12/
debats/
8http://www.avoir-alire.com
9http://www.jeuxvideo.com/etajvbis.htm
Judge Ref. 1 2 3
Ref. 0.17 0.12 0.07
1 0.17 0.03 0.05
2 0.12 0.03 0.07
3 0.07 0.05 0.07
Table 1: Video game corpus: wide scale, marks
from 0 to 20.
Judge Ref. 1 2 3
Ref. 0.74 0.79 0.69
1 0.74 0.74 0.54
2 0.79 0.74 0.69
3 0.69 0.54 0.69
Table 2: Video game corpus: restricted scale,
marks from 0 to 2.
Table 1 and 2 show that agreement between
judges varies widely when marking scales are
modified. Table 1 shows that there is an insuffi-
cient agreement among judges on the wide scale,
with ? coefficients lower than 0.20, while the
agreement between these same judges can be con-
sidered as good on the restricted scale, with ? co-
efficients between 0.54 and 0.79 (Table 2), the me-
dian being at 0.74.
In order to confirm the validity of the change
in scales, we used the ? to test how each judge
agreed with himself, between his two sets of re-
sults (Table 3). Therefore, we compared judg-
ments made by each judge using the initial value
scale and converted towards the restricted scale,
with judgments made by the same judge directly
using the restricted value scale. This measurement
shows the degree of correspondence between both
scales for each judge. Among the three judges who
took part in the test, the first and third one agree
well with themselves, while for the second one, the
agreement is only moderate.
Judge 1 2 3
1 0.74
2 0.46
3 0.70
Table 3: Video game corpus: agreement of each
judge with himself when scales change.
We did the same for a second corpus, of film re-
views. The test involved five judges, and the scale
19
change was smaller, since it was from five values
to three, and not from twenty to three. For this
scale change, we merged the two lowest values (0
and 1) into one (0), and the two highest ones (3
and 4) into one (2), and the middle value in the
wide scale (2) remained the intermediate one in
the restricted scale (1). This scale change was the
most relevant one, since, with 29.7% of the docu-
ments, the class of the middle mark (2) accounted
for almost one third of the corpus. However, the
two other groups of documents are less well bal-
anced. Indeed, the lowest mark concerns less doc-
uments than the highest one: 4.6% and 10.3% re-
spectively for the initial marks 0 and 1, while one
finds 39.8% and 15.6% of documents for the marks
3 and 4. Grouping the documents in only two
classes, by joining the middle class with the two
lowest ones, would have yielded a better balance
between classes, with 44.6% of documents for the
lower mark and 55.4% for the higher one, but that
would have been less meaningful.
Results from human judges are shown in the Ta-
bles 4 and 5 for both scales.
Judge Ref. 1 2 3 4 5
Ref. 0.10 0.29 0.39 0.46 0.47
1 0.10 0.37 0.49 0.48 0.35
2 0.29 0.37 0.36 0.30 0.43
3 0.39 0.49 0.36 0.49 0.54
4 0.46 0.48 0.30 0.49 0.60
5 0.47 0.35 0.43 0.54 0.60
Table 4: Film review corpus: wide scale, marks
from 0 to 4
Judge Ref. 1 2 3 4 5
Ref. 0.27 0.62 0.53 0.56 0.67
1 0.27 0.45 0.43 0.57 0.37
2 0.62 0.45 0.73 0.48 0.54
3 0.53 0.43 0.73 0.62 0.62
4 0.56 0.57 0.48 0.62 0.76
5 0.67 0.37 0.54 0.62 0.76
Table 5: Film review corpus: restricted scale,
marks from 0 to 2.
Agreements between human judges ranked from
bad to moderate for the wide scale (the five origi-
nal values in this corpus), while these agreements
rank from insufficient to good in the case of the
restricted scale with three values. We can see that
differences induced by the scale change are much
less important than with the video game corpus.
This agrees well with the scales being much closer
to each other.
By first performing a hand-made evaluation, and
secondly, matching between themselves the results
from the judges, we found a way to assess with
greater precision the difficulty of the evaluation
task we were about to launch. Concerning the
first two review corpora (films and books, video
games), we attached values good, average and bad
to the three selected classes. The scale for sci-
entific paper reviews was also restricted to three
classes for which following values were selected:
paper accepted as it stands or with minor edits, pa-
per accepted after major edits, paper rejected. Fi-
nally, since its original scale had only two values,
the corpus of parliamentary debates underwent no
change of scale.
3.2 Choice of a topical category set
In order to determine which topical categories
should be recognized in the 2008 task of classify-
ing documents by genre and topic, we performed a
manual evaluation of a sample of the corpus with 4
human judges. The sample included 30 Le Monde
papers for the journalistic genre, and 30 Wikipedia
entries for the encyclopedic genre. Only the title
and body of each article was kept in the sample,
and the tables were deleted. All marks of inclu-
sion in either corpus were also deleted (references
to Le Monde and Wikipedia tags).
The test ran this way: each article was put in a
separate file, and the evaluators had to identify the
genre and the topical category under which it was
published. All articles were included in one set,
which means evaluators had to choose, between all
categories and genres, which ones to match with
each document. This test was made with a first
selection of 8 categories, shared by both genres,
listed in Table 6.
Table 7 shows that results from human judges
in terms of precision and recall were excellent on
the identification of genre (F-scores between 0.94
and 1.00) and quite good on the identification of
categories (F-scores between 0.66 and 0.82).
We also proceeded to the pairwise matching of
results from human judges via the ? coefficient.
Results show an excellent agreement of judges
among themselves and with the reference for genre
identification (Table 8). The agreement is mod-
20
Le Monde Wikipedia
Notebook People
Economy Economy
France French Politics
International International Politics,
minus category
French Politic
Science Science
Society Society,
minus subcategories
Politics, People,
Sport, Media
Sport Sport
Television Television
Table 6: Correspondence between categories from
Le Monde and Wikipedia for the 8 categories in
the test.
Judge 1 2 3 4
Genres 1.00 0.98 0.97 0.94
Categories 0.79 0.77 0.82 0.66
Table 7: F-scores obtained by human judges on the
identification of genre and categories.
erate to good for categoy identification (Table 9).
These good results led us to keep the corpora as
they stood, since they appeared to constitute a
good reference for the defined task. However, we
made an exception for category Notebook (biogra-
phies of celebrities) which we discarded for two
reasons. First, it is more of a genre, namely, ?bi-
ography?, rather than a topical category. Secondly,
we found it rather difficult to assign a single cate-
gory to articles which could belong in two different
ones, as would be the case for the biography of a
sportsman, which would fall under both categories
Notebook et Sport.
Judge Re?f. 1 2 3 4
Re?f. 1.00 0.97 0.93 0.87
1 1.00 0.97 0.93 0.87
2 0.97 0.97 0.90 0.83
3 0.93 0.93 0.90 0.87
4 0.87 0.87 0.83 0.87
Table 8: ? coefficient between human judges and
the reference: Identification of genre.
Our task of genre and topic classification in-
Judge Re?f. 1 2 3 4
Re?f. 0.56 0.52 0.60 0.39
1 0.56 0.69 0.75 0.55
2 0.52 0.69 0.71 0.61
3 0.60 0.75 0.71 0.52
4 0.39 0.55 0.61 0.52
Table 9: ? coefficient between human judges and
the reference: Identification of categories.
cluded two subtasks, one being genre and topic
recognition for a first set of categories, the other
one being only topic recognition for a second set
of categories. Therefore, the corpus had to be di-
vided in two parts. In order to find which cate-
gories had to go into which subcorpus, we decided
to estimate, for each category, the difficulty of rec-
ognizing it. To do so, we calculated the precision
and recall of each evaluator for each category. This
measurement was obtained via a second evaluation
of human judges, with a wider set of categories (by
adding categories Art and Literature).
The ordering of categories by decreasing pre-
cision is following: Sport (1.00), International
(0.80), France (0.76), Literature (0.76), Art (0.74),
Television (0.71), Economy (0.58), Science (0.33),
Society (0.26). This means no document in the
Sport category was misclassified, and, contrari-
wise, categories Science and Society were the most
problematic ones.
The ordering by decreasing recall is slightly
different: International (0.87), Economy (0.80),
Sport (0.75), France (0.70), Art (0.62), Literature
(0.49), Television (0.46), Society (0.42), Science
(0.33). Hence, articles in the International cate-
gory were best identified. This ordering also con-
firms the difficulty felt by human judges concern-
ing the categories Society and Science.
We decided to distribute the categories for each
subtask according to a balance between easy and
diffucult ones in terms of human evaluation:
? Art, Economy, Sport, Television for the sub-
task with both genre and category recogni-
tion;
? France, International, Literature, Science,
Society for the subtask with only category
recognition. For this second subset, we put
together three categories which are topically
close (France, International and Society).
21
4 Human judgments and software
4.1 Confirming the difficulty of a task
The 2007 edition of DEFT highlighted two main
phenomena concerning the corpora involved in the
task.
First, each corpus yielded a different level of dif-
ficulty, and this gradation of difficulty among cor-
pora appeared both for human evaluators and com-
petitors in the challenge (Paroubek et al, 2007).
Judges Competitors
Debates 0.77/1.00 0.54/0.72
Game reviews 0.73/0.90 0.46/0.78
Film reviews 0.52/0.79 0.38/0.60
Paper reviews 0.41/0.58 0.40/0.57
Table 10: Minimal and maximal strict F-scores
between human evaluators and competitors in the
challenge, 2007 edition.
During human tests, judges mentioned the great
facility of finding about opinions expressed in the
corpus of parliamentary debate. Next came cor-
pora of video game reviews, and then of film and
book reviews, whose difficulty was considered av-
erage, and last, the corpus of scientific paper re-
views, which the judges perceived as particularly
difficult. This gradation of difficulty among cor-
pora was also found among competitors, following
the same ordering of three levels of difficulty.
Secondly, the difficulties met by human eval-
uators are also found in the case of competitors.
Upon finishing human tests, judges felt difficulties
in evaluating the corpus of scientific paper reviews,
yielding poor results. Now, the results of competi-
tors on the same corpus are quite as poor, occupy-
ing exactly the same value interval as for human
judges. Most competitors, by the way, obtained
their worst results on this corpus.
The alikeness of results between judges and
competitors reflects the complexity of the corpus:
when preparing the campaign, we observed that
reviews were quite short. Therefore, assigning a
value had to rely upon a small amount of data.
From that, we can derive a minimal size for docu-
ments to be used in this kind of evaluation. More-
over, a paper review can be seen as an aid for the
author, to be expressed as positively as possible,
even if it is also addressed to the Program Commit-
tee which has to accept or reject the paper. There-
fore, the mark could prove more negative than the
text of the review.
The case of comments about videogames is a
different one. Indeed, giving a global mark on a
scale of 20 is a difficult task. Therefore, this mark
comes most often from a sum of smaller marks
which rate either the whole document according
to various criteria, or parts of this document. In
our corpus, each reviewer rates the game accord-
ing to several criteria, namely, graphics, playa-
bility, life span, sound track and scenario, from
which a rather long text is produced, making the
judgment an easier task to perform. However, the
global mark differs from the sum of the smaller
ones from various criteria, hence the difficulty for
human judges to reckon this global mark on a scale
of 20.
4.2 Confirmation of the expected success of
competitors
Contrary to the 2007 edition, in which competi-
tors obtained results that confirmed those of human
judges, the 2008 edition gave them the opportunity
to reach a higher level than human evaluators.
While genre identification yielded no special
problem, either for human evaluators or for com-
petitors, and the results obtained by both groups
are similar, competitors reached better results than
human judges in topical categorization.
Concerning genre identification, strict F-scores
are situated between 0.94 and 1.00 for human
judges, and between 0.95 and 0.98 for the best
runs of competitors (each competitor was allowed
to submit up to three collections of results, only
the best one being used for the final ranking). As
for topical categorization, strict F-scores go from
0.66 to 0.82 for human evaluators, and from 0.84
to 0.89 for best runs from competitors.
The equivalence of results on genre identifica-
tion between judges and competitors can be ex-
plained by the fact that it was a simple, binary
choice (the newspaper Le Monde vs. Wikipedia).
Contrariwise, competitors obtained better re-
sults in topical categorization, since machines have
a stronger abstraction capacity than humans in
presence of the 9 topical categories we defined
(Art, Economy, France, International, Literature,
Science, Society, Sport and Television). However,
conditions were not quite similar, since human
judges had to pick a category among eight, and
not, like the automatic systems, a category within
two subsets of four and five categories. Indeed,
22
we dispatched the categories into two sets, by bal-
ancing categories that are easy or difficult for hu-
man evaluators. For the second set of categories,
we carefully put together three semantically close
ones, (France, International and Society, all three
of them being about political and societal con-
tents), to make the task more difficult. Although
the second set of categories seems more compli-
cated for human judges, half of the competitors ob-
tained better results in topical categorization of the
second set than of the first one.
5 Conclusion
The relevance of human judgment in an evaluation
campaign is present from the beginning to the end
of a campaign.
In a first step, testing a topic for a campaign
among a limited number of human evaluators al-
lows us to check the feasibility of a task. This
checking relies both on the results obtained by
judges (recall, precision, F-scores) and on their
personal impressions after passing the test.
In a second step, the study of both the results ob-
tained by the judges, and their pairwise matching
involving such a comparator as the ? coefficient
allows us to adjust the task (choice of a marking
scale for DEFT?07 and selection of topical cate-
gories for DEFT?08).
Finally, the mutual comparison of competitors?
results, at the end of the evaluation campaign, al-
lows us to validate the choices we made at its start-
ing point, and even to reposition the task when we
shall launch a future campaign based on the same
topic.
References
Adda, Gilles, Joseph Mariani, Patrick Paroubek, Mar-
tin Rajman, and Josette Lecomte. 1999. L?action
GRACE d?e?valuation de l?assignation des parties du
discours pour le franc?ais. Langues, 2(2):119?129,
juin.
Burstein, Jill and Magdalena Wolska. 2003. Toward
evaluation of writing style: Finding overly repetitive
word use in student essays. In 10th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL?03, pages 35?42, Budapest,
Hungary, april.
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistics. Computational Lin-
guistics, 2(22):249?254.
Cohen, Jacob. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, (20):37?46.
Eck, Matthias and Chiori Hori. 2005. Overview of
the iwslt 2005 evaluation campaign. In International
Workshop on Spoken Language Translation, pages
5?14, Pittsburg, PA.
Grouin, Cyril, Jean-Baptiste Berthelin, Sarra El Ayari,
Thomas Heitz, Martine Hurault-Plantet, Miche`le
Jardino, Zohra Khalis, and Michel Lastes. 2007.
Pre?sentation de DEFT?07 (D ?Efi Fouille de Textes).
In Actes de l?atelier de clo?ture du 3e`me D ?Efi
Fouille de Textes, pages 1?8, Grenoble. Association
Franc?aise d?Intelligence Artificielle.
Hovy, Eduard, Margaret King, and Andrei Popescu-
Belis. 2002. Principles of context-based machine
translation evaluation. Machine Translation.
Hurault-Plantet, Martine, Jean-Baptiste Berthelin,
Sarra El Ayari, Cyril Grouin, Patrick Paroubek, and
Sylvain Loiseau. 2008. Re?sultats de l?e?dition 2008
du D ?Efi Fouille de Textes. In Actes TALN?08, Avi-
gnon. Association pour le Traitement Automatique
des Langues.
Paek, Tim. 2001. Empirical Methods for Evaluat-
ing Dialog Systems. In Proceedings of the ACL
2001 Workshop on Evaluation Methodologies for
Language and Dialogue Systems, pages 3?10.
Paroubek, Patrick, Jean-Baptiste Berthelin, Sarra El
Ayari, Cyril Grouin, Thomas Heitz, Martine Hurault-
Plantet, Miche`le Jardino, Zohra Khalis, and Michel
Lastes. 2007. Re?sultats de l?e?dition 2007 du D ?Efi
Fouille de Textes. In Actes de l?atelier de clo?ture du
3e`me D ?Efi Fouille de Textes, pages 9?17, Grenoble.
Association Franc?aise d?Intelligence Artificielle.
23
