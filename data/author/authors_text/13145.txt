Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1119?1127,
Beijing, August 2010
Syntax Based Reordering with Automatically Derived Rules for
Improved Statistical Machine Translation
Karthik Visweswariah
IBM Research
v-karthik@in.ibm.com
Jiri Navratil
IBM Research
jiri@us.ibm.com
Jeffrey Sorensen
Google, Inc.
sorenj@google.com
Vijil Chenthamarakshan
IBM Research
vijil.e.c@in.ibm.com
Nanda Kambhatla
IBM Research
kambhatla@in.ibm.com
Abstract
Syntax based reordering has been shown
to be an effective way of handling word
order differences between source and
target languages in Statistical Machine
Translation (SMT) systems. We present
a simple, automatic method to learn rules
that reorder source sentences to more
closely match the target language word or-
der using only a source side parse tree and
automatically generated alignments. The
resulting rules are applied to source lan-
guage inputs as a pre-processing step and
demonstrate significant improvements in
SMT systems across a variety of lan-
guages pairs including English to Hindi,
English to Spanish and English to French
as measured on a variety of internal test
sets as well as a public test set.
1 Introduction
Different languages arrange words in different or-
ders, whether due to grammatical constraints or
other conventions. Dealing with these word order
permutations is one of the fundamental challenges
of machine translation. Given an exceptionally
large training corpus, a phrase-based system can
learn these reordering on a case by case basis.
But, if our systems are to generalize to phrases not
seen in the training data, they must explicitly cap-
ture and model these reorderings. However, per-
mutations are difficult to model and impractical to
search.
Presently, approaches that handle reorderings
typically model word and phrase movements via
a distortion model and rely on the target language
model to produce words in the right order. Early
distortion models simply penalized longer jumps
more than shorter jumps (Koehn et al, 2003)
independent of the source or target phrases
in question. Other models (Tillman, 2004),
(Al-Onaizan and Papineni, 2006) generalize this
to include lexical dependencies on the source.
Another approach is to incorporate features,
based on the target syntax, during modeling and
decoding, and this is shown to be effective for var-
ious language pairs (Yamada and Knight, 2001),
(Zollmann and Venugopal, 2006). Hierarchical
phrase-based decoding (Chiang, 2005) also al-
lows for long range reordering without explic-
itly modeling syntax. While these approaches
have been shown to improve machine translation
performance (Zollmann et al, 2008) they usually
combine chart parsing with the decoding process,
and are significantly more computationally inten-
sive than phrase-based systems.
A third approach, one that has proved to be
useful for phrase-based SMT systems, is to re-
order each source-side sentence using a set of
rules applied to a parse tree of the source sen-
tence. The goal of these rules is to make the
word order of the source sentence more sim-
ilar to the expected target sentence word or-
der. With this approach, the reordering rules
are applied before training and testing with an
SMT system. The efficacy of these methods has
been shown on various language pairs including:
French to English (Xia and McCord, 2004), Ger-
man to English (Collins et al, 2005), English to
1119
Chinese, (Wang et al, 2007) and Hindi to English
(Ramanathan et al, 2008).
In this paper, we propose a simple model for re-
ordering conditioned on the source side parse tree.
The model is learned using a parallel corpus of
source-target sentence pairs, machine generated
word alignments, and source side parses. We ap-
ply the reordering model to both training and test
data, for four different language pairs: English
? Spanish, English ? French, English ? Hindi,
and English ? German. We show improvements
in machine translation performance for all of the
language pairs we consider except for English ?
German. We use this negative result to propose
extensions to our reordering model. We note that
the syntax based reordering we propose can be
combined with other approaches to handling re-
ordering and does not have to be followed by an
assumption of monotonicity. In fact, our phrase-
based model, trained upon reordered data, retains
its reordering models and search, but we expect
that these facilities are employed much more spar-
ingly with reordered inputs.
2 Related work
There is a significant quantity of work in syntax
based reordering employed to improve machine
translation systems. We summarize our contribu-
tions to be:
? Learning the reordering rules based on train-
ing data (without relying on linguistic knowl-
edge of the language pair)
? Requiring only source side parse trees
? Experimental results showing the efficacy for
multiple language pairs
? Using a lexicalized distortion model for our
baseline decoder
There have been several studies that have
demonstrated improvements with syntax
based reordering based upon hand-written
rules. There have also been studies inves-
tigating the sources of these improvements
(Zwarts and Dras, 2007). Hand-written rules
depend upon expert knowledge of the linguis-
tic properties of the particular language pair.
Initial efforts (Niessen and Ney, 2001) were
made at improving German-English translation
by handling two phenomena: question inver-
sion and detachable verb prefixes in German.
In (Collins et al, 2005), (Wang et al, 2007),
(Ramanathan et al, 2008), (Badr et al, 2009)
rules are developed for translation from Ger-
man to English, Chinese to English, English
to Hindi, and English to Arabic respectively.
(Xu et al, 2009) develop reordering rules based
upon a linguistic analysis of English and Korean
sentences and then apply those rules to trans-
lation from English into Korean and four other
languages: Japanese, Hindi, Urdu and Turkish.
Unlike this body of work, we automatically learn
the rules from the training data and show efficacy
on multiple language pairs.
There have been some studies that try to learn
rules from the data. (Habash, 2007) learns re-
ordering rules based on a dependency parse and
they report a negative result for Arabic to En-
glish translation. (Zhang et al, 2007) learn re-
ordering rules on chunks and part of speech
tags, but the rules they learn are not hierarchi-
cal and would require large amounts of training
data to learn rules for long sentences. Addition-
ally, we only keep a single best reordering (in-
stead of a lattice with possible reorderings) which
makes the decoding significantly more efficient.
(Xia and McCord, 2004) uses source and target
side parse trees to automatically learn rules to re-
order French sentences to match English order.
The requirement to have both source and target
side parse trees makes this method inapplicable
to any language that does not have adequate tree
bank resources. In addition, this work reports re-
sults using monotone decoding, since their exper-
iments using non-monotone decoding without a
distortion model were actually worse.
3 Reordering issues in specific languages
In this section we discuss the reordering issues
typical of translating between English and Hindi,
French, Spanish and German which are the four
language pairs we experiment on in this paper.
3.1 Spanish and French
Typical word ordering patterns common to these
two European languages relate to noun phrases in-
cluding groups of nouns and adjectives. In con-
1120
trast to English, French and Spanish adjectives
and adjunct nouns follow the main noun, i.e. we
typically observe a reversal of word order in noun
phrases, e.g., ?A beautiful red car? translates
into French as ?Une voiture rouge beau?, and as
?Un coche rojo bonito? into Spanish. Phrase-
based MT systems are capable of capturing these
patterns provided they occur with sufficient fre-
quency for each example in the training data. For
rare noun phrases, however, the MT may pro-
duce erroneous word order that can lead to seri-
ous distortions in the meaning. Particularly dif-
ficult are nominal phrases from specialized do-
mains that involve challenging terminology, for
example: ?group reference attribute? and ?valida-
tion checking code?. In both instances, the base-
line MT system generated translations with an in-
correct word order and, consequently, possibly a
different meaning. We will return to these two ex-
amples in Section 5.1 to compare the output of a
MT system with and without reordering.
3.2 German
Unlike French and Spanish, German poses a con-
siderably different challenge with respect to word
ordering. The most frequent reordering in German
relates to verbs, particularly verb groups consist-
ing of auxiliary and main verbs, as well as verbs
in relative clauses. Moreover, reordering patterns
between German and English tend to span large
portions of the sentence. We included German in
our investigations to determine whether our auto-
mated rule extraction procedure can capture such
long distance patterns.
3.3 Hindi
Hindi word order is significantly different than
English word order; the typical order followed
is Subject Object Verb (although Object Subject
Verb order can be used if nouns are followed by
appropriate case markers). This is in contrast to
English which has a Subject Verb Object order.
This can result in words that are close in English
moving arbitrarily far apart in Hindi depending on
the length of the noun phrase representing the ob-
ject and the length of the verb phrase. These long
range reorderings are generally hard for a phrase
based system to capture. Another way Hindi and
English differ is that prepositions in English be-
come postpositions in Hindi and appear after the
noun phrase. Again, this reordering can lead to
long distance movements of words. We include
Hindi in our investigation since it has significantly
different structure as compared to English.
4 Learning reordering rules
In this section we describe how we learn rules that
transform source parse trees so the leaf word order
is more like the target language. We restrict our-
selves to reorderings that can be obtained by per-
muting child nodes at various interior nodes in a
parse tree. With many reordering phenomena dis-
cussed in Section 3 this is a fairly strong assump-
tion about pairs of languages, and there are exam-
ples in English?Hindi where such an assumption
will not allow us to generate the right reordering.
As an example consider the English sentence ?I
do not want to play?. The sentence has a parse:
S
NP
PRP
I
VP
VBP
do
RB
not
VP
VB
want
S
VP
TO
to
VP
VB
play
The correct word order of the translation in Hindi
is ?I to play not want? In this case, the word not
breaks up the verb phrase want to play and hence
the right Hindi word order cannot be obtained by
the reordering allowed by our model. We found
such examples to be rare in English?Hindi, and
we impose this restriction for the simplicity of the
model. Experimental results on several languages
show benefits of reordering in spite of this simpli-
fying assumption.
Consider a source sentence s and its corre-
sponding constituency parse tree S1. We set up
the problem in a probabilistic framework, i.e. we
would like to build a probabilistic model P (T |S)
that assigns probabilities to trees such that the
1In this paper we work with constituency parse trees. Ini-
tial experiments, applying similar techniques to dependency
parse trees did not yield improvements.
1121
word order in trees T which are assigned higher
probability match the order of words in the target
language. A parse tree, S is a set of nodes. Inte-
rior nodes have an ordered list of children. Leaf
nodes in the tree are the words in the sentence
s, and interior nodes are labeled by the linguis-
tic constituent that they represent. Each word has
a parent node (with only one child) labeled by the
part-of-speech tag of the word.
Our model assigns non-zero probabilities to
trees that can be obtained by permuting the child
nodes at various interior nodes of the tree S. We
assume that children of a node are ordered inde-
pendently of all other nodes in the tree. Thus
P (T |S) =
?
n?I(S)
P (?(cn)|S, n, cn),
where I(S) is the set of interior nodes in the tree
S, cn is the list of children of node n and ? is a
permutation. We further assume that the reorder-
ing at a particular node is dependent only on the
labels of its children:
P (T |S) =
?
n?I(S)
P (?(cn)|cn).
We parameterize our model using a log-linear
model:
P (?(cn)|cn) =
1
Z(cn)
exp(?T f(?, cn)). (1)
We choose the simplest possible set of feature
functions: for each observed sequence of non-
terminals we have one boolean feature per per-
mutation of the sequence of non-terminals, with
the feature firing iff that particular sequence is ob-
served. Assuming, we have a training corpus C of
(T, S) tree pairs, we could optimize the parame-
ters of our model to maximize :
?
S?C P (T |S).
With the simple choice of feature functions de-
scribed above, this amounts to:
P (?(cn)|cn) =
count(?(cn))
count(cn)
,
where count(cn) is the number of times the se-
quences of nodes cn is observed in the training
data and count(?(cn)) is the number of times
that cn in S is permuted to ?(cn) in T . In Sec-
tion 6, we show considering more general fea-
ture functions and relaxing some of the indepen-
dence might yield improvements on certain lan-
guage pairs.
For each source sentence s with parse S we find
the tree T that makes the given alignment for that
sentence pair most monotone. For each node n in
the source tree S let Dn be the set of words that
are descendants of n. Let us denote by tpos(n) the
average position of words in the target sentence
that are aligned to words in Dn. Then
tpos(n) = 1|Dn|
?
w?Dn
a(w),
where a(w) is the index of the word on the target
side that w is aligned with. If a word w is not
aligned to any target word, we leave it out from
the mean position calculation above. If a word w
is aligned to many words we let a(w) be the mean
position of the words that w is aligned to. For each
node n in the tree we transform the tree by sorting
the list of children of n according to tpos. The
pairs of parse trees that we obtain (S, T ) in this
manner form our training corpus to estimate our
parameters.
In using our model, we once again go for the
simplest choice, we simply reorder the source side
sentences by choosing arg maxT P (T |S) both in
training and in testing; this amounts to reordering
each interior node based on the most frequent re-
ordering of the constituents seen in training. To
reduce the effect of noise in training alignments
we apply the reordering, only if we have seen the
constituent sequence often enough in our training
data (a count threshold parameter) and if the most
frequent reordering is sufficiently more frequent
than the next most frequent reordering (a signifi-
cance threshold).
5 Experiments
5.1 Results for French, Spanish, and German
In each language, the rule extraction was
performed using approximately 1.2M sen-
tence pairs aligned using a maxent aligner
(Ittycheriah and Roukos, 2005) trained using a
variety of domains (Europarl, computer manuals)
1122
and a maximum entropy parser for English
(Ratnaparkhi, 1999). With a significance thresh-
old of 1.2, we obtain about 1000 rules in the
eventual reordering process.
Phrase-based systems were trained for each lan-
guage pair using 11M sentence pairs spanning a
variety of publicly available (e.g. Europarl, UN
speeches) and internal corpora (IT technical and
news domains). The system phrase blocks were
extracted based on a union of HMM and max-
ent alignments with corpus-selective count prun-
ing. The lexicalized distortion model was used
as described in (Al-Onaizan and Papineni, 2006)
with a window width of up to 5 and a maximum
number of skipped (not covered) words during de-
coding of 2. The distortion model assigns a prob-
ability to a particular word to be observed with
a specific jump. The decoder uses a 5-gram in-
terpolated language model spanning the various
domains mentioned above. The baseline system
without reordering and a system with reordering
was trained and evaluated in contrastive experi-
ments. The evaluation was performed utilizing the
following (single-reference) test sets:
? News: 541 sentences from the news domain.
? TechA: 600 sentences from a computer-
related technical domain, this has been used
as a dev set.
? TechB: 1038 sentences from a similar do-
main as TechA used as a blind test.
? Dev09: 1026 sentences defined as the news-
dev2009b development set of the Workshop
on Statistical Machine Translation 2009 2.
This set provides a reference measurement
using a public data set. Previously published
results on this set can be found, for example,
in (Popovic et al, 2009).
In order to assess changes in word ordering pat-
terns prior to and after an application of the re-
ordering, we created histograms of word jumps
in the alignments obtained in the baseline as well
as in the reordered system. Given a source word
si at index i and the target word tj it is aligned
to at index j, a jump of 1 would correspond to
si+1 aligning to target word tj+1, while an align-
ment to tj?1 corresponds to a jump of -1, etc. A
2http://statmt.org/wmt09/
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1
?0.5
0
0.5
1
1.5
2
x 105
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?ES)
?8 ?6 ?4 ?2 0 2 4 6 8 10
?5000
0
5000
10000
15000
20000
Distance to next position
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?FR)
Figure 1: Difference-histogram of word order
distortions for English?Spanish (upper), and
English?French (lower).
histogram over the jump values gives us a sum-
mary of word order distortion. If all of the jumps
were one, then there is no reordering between the
two languages. To gain insight into changes in-
troduced by our reordering we look at differences
of the two histograms i.e., counts after reordering
minus counts before reordering. We would hope
that after reordering most of the jumps are small
and concentrated around one. Figure 1 shows
such difference-histograms for the language pairs
English?Spanish and English?French, respec-
tively, on a sample of about 15k sentence pairs
held out of the system training data. Here, a pos-
itive difference value indicates an increased num-
ber after reordering. In both cases a consistent
trend toward monotonicity is observed, i.e more
jumps of size one and two, and fewer large jumps.
This confirms the intended reordering effect and
indicates that the reordering rules extracted gen-
eralize well.
Table 1 shows the resulting uncased BLEU
scores for English-Spanish and English-French.
In both cases the reordering has a consistent
positive effect on the BLEU scores across test sets.
In examining the sources of improvement, we no-
ticed that word order in several noun phrases that
1123
System News TechA TechB Dev09
Baseline 0.3849 0.3371 0.3483 0.2244
Sp
an
ish
Reordered 0.4031 0.3582 0.3605 0.2320
Baseline 0.5140 0.2971 0.3035 0.2014
Fr
en
ch
Reordered 0.5242 0.3152 0.3154 0.2092
Baseline 0.2580 0.1582 0.1697 0.1281
G
er
m
an
Reordered 0.2544 0.1606 0.1682 0.1271
Baseline 20.0
H
in
di
Reordered 21.7
Table 1: Uncased BLEU scores for phrase-based
machine translation.
were not common in the training data were fixed
by use of the reordering rules.
Table 1 shows the BLEU scores for the
English?German language pair, for which a
mixed result is observed. The difference-
histogram for English?German, shown in Figure
2, differs from those of the other languages with
several increases in jumps of large magnitude, in-
dicating failure of the extracted rules to general-
ize.
The failure of our simple method to gain con-
sistent improvements comparable to Spanish and
French, along with our preliminary finding that a
relatively few manually crafted reordering rules
(we describe these in Section 6.4) tend to outper-
form our method, leads us to believe that a more
refined approach is needed in this case and will be
subject of further discussion below.
5.2 Results for Hindi
Our Hindi-English experiments were run with
an internal parallel corpus of roughly 250k sen-
tence pairs (5.5M words) consisting of various
domains (including news). To learn reordering
rules we used HMM alignments and a maxent
parser (Ratnaparkhi, 1999), with a count thresh-
old of 100, and a significance threshold of 1.7
(these settings gave us roughly 200 rules). We also
experimented with other values of these thresh-
olds and found that the performance of our sys-
tems were not very sensitive to these thresholds.
We trained Direct Translation Model 2 (DTM)
?10 ?5 0 5 10
?600
?400
?200
0
200
400
600
800
1000
1200
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?DE)
Figure 2: Difference-histogram of word order dis-
tortions for English?German.
systems (Ittycheriah and Roukos, 2007) with and
without source reordering and evaluated on a test
set of 357 sentences from the News domain.
We note that the DTM baseline includes features
(functions of target words and jump size) that al-
low it to model lexicalized reordering phenomena.
The reordering window size was set to +/- 8 words
for the baseline and system with reordered in-
puts. Table 1 shows the uncased BLEU scores for
English-Hindi, showing a gain from using the re-
ordering rules. For the reordered case, the HMM
alignments are rederived, but the accuracy of these
were no better than those of the unreordered in-
put and experiments showed that the gains in per-
formance were not due to the effect on the align-
ments.
Figure 3 shows difference-histograms for the
language pair English?Hindi, on a sample of
about 10k sentence pairs held out of the system
training data. The histogram indicates that our
reordering rules generalize and that the reordered
English is far more monotonic with respect to the
Hindi.
6 Analysis of errors and future
directions
In this section, we analyze some of the sources of
errors in reordering rules learned via our model, to
better understand directions for further improve-
ment.
1124
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1.5
?1
?0.5
0
0.5
1
1.5
x 104
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?HI)
Figure 3: Difference-histogram of word order dis-
tortions for English?Hindi.
6.1 Model weakness
In our initial experiments, we noticed that for the
most frequent reordering rules in English?Hindi
(e.g that IN NP or NP PP flips in Hindi) the prob-
ability of a reordering was roughly 65%. This
was concerning since it meant that on 35% of the
data we would be making wrong reordering deci-
sions by choosing the most likely reordering. To
get a better feel for whether we needed a stronger
model (e.g by lexicalization or by looking at larger
context in the tree rather than just the children),
we analyzed some of the cases in our training data
where (IN,NP), (NP, PP) pairs were left unaltered
in Hindi. In doing that analysis, we noticed exam-
ples involving negatives that our model does not
currently handle. The first issue was mentioned
in Section 4, where the assumption that we can
achieve the right word order by reordering con-
stituent phrases, is incorrect. The second issue
is illustrated by the following sentences: I have
some/no books, which have similar parse struc-
tures, the only difference being the determiner
some vs the determiner no. In Hindi, the order
of the fragments some books and the fragment
no books are different (in the first case the words
stay in order, in the second the flip). Handling
this example would need our model to be lexical-
ized. These issue of negatives requiring special
handling also came up in our analysis of German
(Section 6.4). Other than the negatives (which re-
quire a lexicalized model), the major reason for
the lack of sharpness of the reordering rule proba-
bility was alignment errors and parser issues. We
Aligner
Number of
Sentences fMeasure BLEU score
HMM 250k 62.4 21.7
MaxEnt 250k 76.6 21.4
Manual 5k - 21.3
Table 2: Using different alignments
look at these topics next.
6.2 Alignment accuracy
Since we rely on automatically generated align-
ments to learn the rules, low accuracy of
the alignments could impact the quality of
the rules learned. This is especially a con-
cern for English?Hindi since the quality of
HMM alignments are fairly low. To quan-
tify this effect, we learn reordering rules us-
ing three sets of alignments: HMM alignments,
alignments from a supervised MaxEnt aligner
(Ittycheriah and Roukos, 2005), and hand align-
ments. Table 2 summarizes our results using
aligners with differing alignment qualities for our
English?Hindi task and shows that quality of
alignments in learning the rules is not the driving
factor in affecting rule quality.
6.3 Parser accuracy
Accuracy of the parser in the source language is
a key requirement for our reordering method, be-
cause we choose the single best reordering based
on the most likely parse of the source sentence.
This would especially be an issue in translat-
ing from languages other than English, where the
parser would not be of quality comparable to the
English parser.
In examining some of the errors in reordering
we did observe a fair fraction attributable to
issues in parsing, as seen in the example sentence:
The rich of this country , corner almost 90% of
the wealth .
The second half of the sentence is parsed by the
Berkeley parser (Petrov et al, 2006) as:
FRAG
NP-SBJ
NN
corner
ADVP
RB
almost
NP-SBJ
NP
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
1125
and by IBM?s maximum entropy
parser parser (Ratnaparkhi, 1999) as:
VP
VB
corner
NP
NP
QP
RB
almost
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
With the first parse, we get the right Hindi order
for the second part of the sentence which is: the
wealth of almost 90% corner . To investigate the
effect of choice of parser we compared using the
Berkeley parser and the IBM parser for reorder-
ing, and we found the BLEU score essentially
unchanged: 21.6 for the Berkeley parser and
21.7 for the IBM parser. A potential source of
improvements might be to use alternative parses
(via different parsers or n-best parses) to generate
n-best reorderings both in training and at test.
6.4 Remarks on German reordering
Despite a common heritage, German word order is
distinct from English, particularly regarding verb
placement. This difference can be dramatic, if an
auxiliary (e.g. modal) verb is used in conjunction
with a full verb, or the sentence contains a subor-
dinate clause. In addition to our experiments with
automatically learned rules, a small set of hand-
crafted reordering rules was created and evalu-
ated. Our preliminary results indicate that the lat-
ter rules tend to outperform the automatically de-
rived ones by 0.5-1.0 BLEU points on average.
These rules are summarized as follows:
1. In a VP immediately following an NP, move
the negation particle to main verb.
2. Move a verb group away from a modal verb;
to the end the of a VP. Negation also moves
along with verb.
3. Move verb group to end of an embed-
ded/relative clause.
4. In a VP following a subject, move negation
to the end of VP (handling residual cases)
The above hand written rules show several weak-
nesses of our automatically learned rules for re-
ordering. Since our model is not lexicalized, nega-
tions are not handled properly as they are tagged
RB (along with other adverbs). Another limitation
apparent from the first rule above (the movement
of verbs in a verb phrase depends on the previous
phrase being a noun phrase) is that the automatic
reordering rule for a node?s children depends only
on the children of that node and not a larger con-
text. For instance, a full verb following a modal
verb is typically parsed as a VP child node of the
modal VP node, hence the automatic rule, as cur-
rently considered, will not take the modal verb
(being a sibling of the full-verb VP node) into ac-
count. We are currently investigating extensions
of the automatic rule extraction alorithm to ad-
dress these shortcomings.
6.5 Future directions
Based on our analysis of the errors and on the
hand designed German rules we would like to ex-
tend our model with more general feature func-
tions in Equation 1 by allowing features: that
are dependent on the constituent words (or head-
words), that examine a large context than just a
nodes children (see the first German rule above)
and that fire for all permutations when the con-
stituent X is moved to the end (or start). This
would allow us to generalize more easily to learn
rules of the type ?move X to the end of the
phrase?. Another direction that we feel should be
explored, is the use of multiple parses to obtain
multiple reorderings and combine these at a later
stage.
7 Conclusions
In this paper we presented a simple method to
automatically derive rules for reordering source
sentences to make it look more like target
language sentences. Experiments (on inter-
nal and public test sets) indicate performance
gains for English?French, English?Spanish,
and English?Hindi. For English?German we
did not see improvements with automatically
learned rules while a few hand designed rules did
give improvements, which motivated a few direc-
tions to explore.
1126
References
[Al-Onaizan and Papineni2006] Al-Onaizan, Yaser and
Kishore Papineni. 2006. Distortion models for sta-
tistical machine translation. In Proceedings of ACL.
[Badr et al2009] Badr, Ibrahim, Rabih Zbib, and
James Glass. 2009. Syntactic phrase reordering for
english-to-arabic statistical machine translation. In
Proceedings of EACL.
[Chiang2005] Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine transla-
tion. In Proceedings of ACL.
[Collins et al2005] Collins, Michael, Philipp Koehn,
and Ivona Kucerova. 2005. Clause restructuring
for statistical machine translation. In Proceedings
of ACL.
[Habash2007] Habash, Nizar. 2007. Syntactic prepro-
cessing for statistical machine translation. In MT
Summit.
[Ittycheriah and Roukos2005] Ittycheriah, Abraham
and Salim Roukos. 2005. A maximum entropy
word aligner for arabic-english machine translation.
In Proceedings of HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abraham
and Salim Roukos. 2007. Direct translation model
2. In Proceedings of HLT-NAACL, pages 57?64.
[Koehn et al2003] Koehn, Philipp, Franz Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of HLT-NAACL.
[Niessen and Ney2001] Niessen, Sonja and Hermann
Ney. 2001. Morpho-syntactic analysis for reorder-
ing in statistical machine translation. In Proc. MT
Summit VIII.
[Petrov et al2006] Petrov, Slav, Leon Barrett, Romain
Thibaux, and Dan Klein. 2006. Learning accu-
rate, compact, and interpretable tree annotation. In
COLING-ACL.
[Popovic et al2009] Popovic, Maja, David Vilar,
Daniel Stein, Evgeny Matusov, and Hermann Ney.
2009. The RWTH machine translation system for
WMT 2009. In Proceedings of WMT 2009.
[Ramanathan et al2008] Ramanathan, A., P. Bhat-
tacharyya, J. Hegde, R. M. Shah, and M. Sasikumar.
2008. Simple syntactic and morphological process-
ing can help english-hindi statistical machine trans-
lation. In Proceedings of International Joint Con-
ference on Natural Language Processing.
[Ratnaparkhi1999] Ratnaparkhi, Adwait. 1999. Learn-
ing to parse natural language with maximum en-
tropy models. Machine Learning, 34(1-3).
[Tillman2004] Tillman, Christoph. 2004. A unigram
orientation model for statistical machine translation.
In Proceedings of HLT-NAACL.
[Wang et al2007] Wang, Chao, Michael Collins, and
Philipp Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In Proceedings
of EMNLP-CoNLL.
[Xia and McCord2004] Xia, Fei and Michael McCord.
2004. Improving a statistical mt system with auto-
matically learned rewrite patterns. In Proceedings
of Coling.
[Xu et al2009] Xu, Peng, Jaeho Kang, Michael Ring-
gaard, and Franz Och. 2009. Using a dependency
parser to improve SMT for Subject-Object-Verb lan-
guages. In Proceedings of NAACL-HLT.
[Yamada and Knight2001] Yamada, Kenji and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of ACL.
[Zhang et al2007] Zhang, Yuqi, Richard Zens, and
Hermann Ney. 2007. Chunk-level reordering
of source language sentences with automatically
learned rules for statistical machine translation. In
NAACL-HLT AMTA Workshop on Syntax and Struc-
ture in Statistical Translation.
[Zollmann and Venugopal2006] Zollmann, Andreas
and Ashish Venugopal. 2006. Syntax augmented
machine translation via chart parsing. In Pro-
ceedings on the Workshop on Statistical Machine
Translation.
[Zollmann et al2008] Zollmann, Andreas, Ashish
Venugopal, Franz Och, and Jay Ponte. 2008. A
systematic comparison of phrase-based, hierar-
chical and syntax-augmented statistical MT. In
Proceedings of COLING.
[Zwarts and Dras2007] Zwarts, Simon and Mark Dras.
2007. Syntax-based word reordering in phrase-
based statistical machine translation: why does it
work? In Proc. MT Summit.
1127
Coling 2010: Poster Volume, pages 1283?1291,
Beijing, August 2010
Urdu and Hindi: Translation and sharing of linguistic resources
Karthik Visweswariah, Vijil Chenthamarakshan, Nandakishore Kambhatla
IBM Research India
{v-karthik,vijil.e.c,kambhatla}@in.ibm.com
Abstract
Hindi and Urdu share a common phonol-
ogy, morphology and grammar but are
written in different scripts. In addition,
the vocabularies have also diverged signif-
icantly especially in the written form. In
this paper we show that we can get rea-
sonable quality translations (we estimated
the Translation Error rate at 18%) between
the two languages even in absence of a
parallel corpus. Linguistic resources such
as treebanks, part of speech tagged data
and parallel corpora with English are lim-
ited for both these languages. We use the
translation system to share linguistic re-
sources between the two languages. We
demonstrate improvements on three tasks
and show: statistical machine translation
from Urdu to English is improved (0.8
in BLEU score) by using a Hindi-English
parallel corpus, Hindi part of speech tag-
ging is improved (upto 6% absolute) by
using an Urdu part of speech corpus and
a Hindi-English word aligner is improved
by using a manually word aligned Urdu-
English corpus (upto 9% absolute in F-
Measure).
1 Introduction
Hindi and Urdu are official languages of India
and Urdu is also the national language of Pak-
istan. Hindi is spoken by around 853 million peo-
ple and Urdu by around 164 million people (Malik
et al, 2008). Although native speakers of Hindi
can comprehend most of spoken Urdu and vice
versa, these languages have diverged a bit since
independence of India and Pakistan ? with Hindi
deriving a lot of words from Sanskrit and Urdu
from Persian. One clear difference between Hindi
and Urdu is the script: Hindi is written in a left-
to-right Devanagari script while Urdu is written
in Nastaliq calligraphy style of the right-to-left
Perso-Arabic script. Hence, despite the similari-
ties, it is impossible for an Urdu speaker to read
Hindi text and vice versa. The first problem we
address is the translation between Hindi and Urdu
in the absence of a Hindi-Urdu parallel corpus.
Though these languages together are spoken by
around a billion people they are not very rich in
linguistic resources. A treebank for Hindi is still
under development1 and part of speech taggers for
Hindi and Urdu are trained on very small amounts
of data. For translation between Hindi/Urdu and
English there are no large corpora, the available
corpora are an order of magnitude smaller than
those available for European languages or Arabic-
English. Given the lack of linguistic resources
in each of the languages and the similarities be-
tween these languages, we explore whether each
language can benefit from resources available in
the other language.
1.1 Urdu-Hindi script conversion/translation
Sharing resources between Hindi and Urdu re-
quires us to be able to convert from one written
form to the other. Given that the languages share a
good fraction of their spoken vocabularies, the ob-
vious approach to convert between the two scripts
would be to transliterate between them. While this
approach has recently been attempted (Malik et
al., 2009), (Malik et al, 2008) there are two main
problems with this approach.
Challenges in Hindi-Urdu transliteration:
Urdu uses diacritical marks that were taken from
the Arabic script which serve various purposes.
Urdu has short and long vowels. Short vowels
are indicated by placing a diacritic with the con-
1https://verbs.colorado.edu/hindi
wiki/index.php/Hindi Treebank Data
1283
Figure 1: An Urdu sentence transliterated and
translated to Hindi
sonant that precedes it in the syllable. The diacrit-
ical marks are also used for gemination (doubling
of a consonant), which in Hindi is handled using a
conjunct form where the consonant is essentially
repeated twice. Yet another function of diacritical
marks is to mark the absence of a vowel follow-
ing a base consonant. Though diacritical marks
are critical for correct pronunciation and some-
times even for disambiguation of certain words,
they are sparingly used in written material in-
tended for native speakers of the language. Miss-
ing diacritical marks create substantial difficulties
for transliteration systems. Another difficulty is
created by the fact that Urdu words cannot have
a short vowel at the end of a word, whereas the
corresponding Hindi word can sometimes have a
short vowel. This cannot be resolved deterministi-
cally and results ambiguity in transliteration from
Urdu to Hindi. A third issue is the presence of
certain sounds (and their corresponding letters)
that have no equivalent in Urdu. These letters
are approximated in Urdu with phonetic equiva-
lents. Transliteration from Urdu to Hindi suffers
in the presence of words with these letters. Re-
cent work on Urdu-Hindi transliteration (Malik et
al., 2009) report transliteration word error rates
of 16.4% and 23.1% for Urdu sentences with and
without diacritical marks respectively. This prob-
lem is illustrated in Figure 1. The figure shows an
Urdu sentence that is transliterated to Hindi using
the Hindi Urdu Machine Transliteration (HUMT)
system 2 and translated using our Statistical Ma-
chine Translation System. The words which are
in red are transliteration errors (mainly because of
missing diacritical marks).
Difference in Word Frequency Distribu-
tions: Even if we could transliterate perfectly be-
tween Urdu and Hindi it might not be desirable to
2http://www.puran.info/HUMT/HUMT.aspx
do so from the point of view of human understand-
ing or for machine consumption. This is because
word frequencies of shared words would be dif-
ferent in Hindi and Urdu. At the extreme, there
are several Urdu words that a fluent Hindi speaker
would not understand and vice versa. More com-
monly, native speakers of Hindi and Urdu would
use different words to refer to the same concept,
even though both these words are technically cor-
rect in either of these languages. In initial experi-
ments to quantify this issue on our corpus, which
is mainly from the news domain, we estimated
that around 28% of the word tokens in Urdu would
not be natural in Hindi. This estimate assumes
perfect transliteration, and we estimated the total
error rate including transliteration at around 55%
for the publicly available HUMT system. In Fig-
ure 1, the words that have been underlined have
been replaced using a different word by our SMT
system, even though the original word might be
technically correct. Our preliminary experiments
exploring this issue convinced us that to be able
to convert from Urdu into natural Hindi (and vice
versa) we would need to go beyond transliteration
to translation to deal with the divergence of the
vocabularies in the written forms of the two lan-
guages.
Importance of Context We would like to point
out that in addition to word for word fidelity,
there are more subtle issues in translating from
Urdu-Hindi. One issue is that words in Hindi are
drawn from different source languages, and with
word to word translations, we might end up with
phrases that are unnatural. For example, consider
different ways of writing the English phrase Na-
tional and News in Hindi. The word National
in Hindi could possibly be written as rashtriya,
kaumi or national which have origins in Sanskrit,
Persian/Arabic and English respectively. Simi-
larly the word News could be written as samachar,
khabaren or news (once again with origins in San-
skrit, Persian/Arabic and English). The natural
ways for writing the phrase national news are:
rashtriya samachar, kaumi khabaren or national
news, any of the other six combinations would be
quite rare.
Another issue is that corresponding words in
Hindi and Urdu might have different genders. An
1284
example from (Sinha, 2009) are the words vajah
(Urdu, feminine) and karan (Hindi, masculine),
which would mean that the phrase because of him
would be written as us ke karan in Hindi and as us
ki vajah se in Urdu. We note that the ke in Hindi
and ki in Urdu are different because of the differ-
ence in genders of the word following them. This
suggests we would need to go beyond word for
word translation and would need to use a higher
order n-gram language model to translate with fi-
delity between Hindi and English.
We have established the need for going beyond
transliteration, but a key challenge is to achieve
good translation accuracy in the absence of a
Hindi-Urdu parallel corpus. In Section 3 we de-
scribe a multi-pronged approach to translate be-
tween Hindi and Urdu in the absence of a parallel
corpus that exploits the similarities between the
languages.
1.2 Applications: sharing linguistic resources
We next outline the three tasks for which we con-
sider sharing resources between Hindi and Urdu
which serve as a test of the efficacy of our sys-
tems.
Statistical machine translation
In recent years, there is a lot of interest in Statis-
tical Machine Translation (SMT) Systems (Brown
et al, 1993). Modern SMT systems (Koehn et al,
2003; Ittycheriah and Roukos, 2007) learn trans-
lation models based on large amounts of paral-
lel data. The quality of an SMT system is de-
pendent on the amount of parallel data on which
the system is trained. Unfortunately, for the pairs
Urdu-English and Hindi-English, parallel data are
not available in large quantities, thereby limiting
the quality of these SMT systems. In this pa-
per we show that we can improve the accuracy of
an Urdu?English SMT system by using a Hindi-
English parallel corpus.
Part of Speech tagging
Part of Speech (POS) tagging involves marking
the part of speech of a word based on its defini-
tion and surrounding context in a sentence. Se-
quential modeling techniques like HiddenMarkov
Models (Rabiner, 1990) and Conditional Random
Fields (Lafferty et al, 2001) are commonly used
to build Part of Speech taggers. These models are
typically trained using a manually tagged part of
speech corpus. Manual tagging of data requires
lot of human effort and hence large corpora are not
readily available for many languages. We improve
a Hindi POS tagger by using a manually tagged
Urdu POS corpus.
Supervised bitext alignment
Machine generated word alignments between
pairs of languages have many applications: build-
ing statistical machine translation systems, build-
ing dictionaries, projection of syntactic informa-
tion to resource poor languages (Yarowsky and
Ngai, 2001). Most of the early work on generat-
ing word alignments has been unsupervised, e.g.
IBM Models 1-5 (Brown et al, 1993), recent im-
provements on the IBM Models (Moore, 2004),
and the HMM algorithm described in (Vogel et al,
1996). Recently, significant improvements in per-
formance of aligners have been achieved by the
use of human annotated word alignments (Itty-
cheriah and Roukos, 2007; Lacoste-Julien et al,
2006). We describe a method to transfer man-
ual word alignments from Urdu-English to Hindi-
English to improve Hindi-English word align-
ments.
1.3 Contributions
Our main contributions are summarized below:
We present a hybrid technique to translate be-
tween Hindi and Urdu in the absence of a Hindi-
Urdu parallel corpus that significantly improves
upon past efforts to convert between Hindi and
Urdu via transliteration. We validate the efficacy
of the translation systems we present, by using it
to share linguistic resources between Hindi and
Urdu for three important tasks:
1. We improve a part of speech tagger for Hindi
using an Urdu part of speech corpus.
2. We use manual Urdu-English word align-
ments to improve the task of Hindi-English
bitext alignments.
3. We use a Hindi-English parallel corpus to
improve translation from Urdu to English.
1285
2 Related work
Converting between the scripts of Hindi and Urdu
is non-trivial and has been a recent focus (Ma-
lik et al, 2008; Malik et al, 2009). (Malik et
al., 2008) uses hand designed rules encoded us-
ing finite state transducers to transliterate between
Hindi and Urdu. As reported in (Malik et al,
2009) these hand designed rules achieve accu-
racies of only about 50% in the absence of di-
acritical marks. (Malik et al, 2009) improves
Urdu?Urdu transliteration performance to 79%
by post processing the output of the transducer
with a statistical language model. In contrast to
(Malik et al, 2009) we use a statistical model
for character transliteration. As discussed in Sec-
tion 1.1, due to the divergence of vocabularies
in written Hindi and Urdu, transliteration is not
sufficient to convert from written Urdu to written
Hindi. We also use a more flexible model that
allows for more natural translations by allowing
Urdu words to translate into Hindi words that do
not sound the same.
(Sinha, 2009) builds an English-Urdu machine
translation system using an English-Hindi ma-
chine translation system and a Hindi-Urdu word
mapping table, suitably adjusted for part of speech
and gender. Their system is not statistical, and
is largely based on manual creation of a large
database of Hindi-Urdu correspondences. Addi-
tionally, as mentioned in the conclusion, their sys-
tem cannot be used for direct translation from
Hindi to Urdu, since a grammatical analysis of
the English provides information necessary for the
Hindi to Urdu mapping. In contrast to this work,
our techniques are largely statistical, require min-
imal manual effort and can directly translate be-
tween Hindi and Urdu without the associated En-
glish.
3 Approach to translating between Hindi
and Urdu
As discussed in Section 1, transliteration between
Hindi and Urdu is not a straightforward task and
current efforts result in fairly high error rates. We
would like to combine the approaches of translit-
eration and translation since our goal is to use the
translation for sharing linguistic resources rather
than for direct consumption.
We use a fairly standard phrase based transla-
tion system to translate between Hindi and Urdu.
The key challenge that we overcome is being able
to develop such a system with acceptable accu-
racy in the absence of Hindi-Urdu resources (we
have neither a parallel corpus nor a dictionary with
sufficient coverage). In spite of the absence of re-
sources, translation between this language pair is
made feasible by the fact that word order is largely
maintained and translation can be done maintain-
ing a word to word correspondence. There are
some exceptions to the monotonicity in the two
languages. Consider the English phrase Govern-
ment of Sindh which in Urdu would be hukumat
e sindh in the same word order as in English,
while in Hindi it would be sindhi sarkar with the
word order flipped (with respect to English and
Urdu). This example also shows that sometimes
we do not have a word for word translation be-
tween Hindi and Urdu, the word sindhi in Hindi
corresponding to the Urdu words e sindh. In spite
of these exceptions, Hindi-Urdu translation can
largely be done with the monotonicity assumption
and with the assumption of word to word corre-
spondences. Thus the central issue in translating
between Hindi and Urdu is the creation of a word
to word conditional probability table. We explain
our technique assuming we are translating from
Urdu to Hindi. We take a hybrid approach to cre-
ating this table, using three different approaches.
The first approach is the pivot language ap-
proach (Wu and Wang, 2007), with English as a
pivot language. We get probabilities of a Urdu
word u being generated by a Hindi word h, con-
sidering intermediate English phrases e as:
Pp(u|h) =
?
e
P (u|e)P (e|h)
The translation probabilities P (u|e) and P (e|h)
are obtained using an Urdu-English and an
English-Hindi parallel corpus respectively.
This approach works reasonably well, but suf-
fers from a couple of drawbacks. There are sev-
eral common Hindi and Urdu words for which the
translation is unsatisfactory. This is because the
alignments for these words are not precise, they
often do not align to any English word, or align to
1286
an English words in combination with other Hindi
words. A common example of this is with verbs,
consider for example the English sentence
He works
which would translate into Hindi/Urdu as:
vah kaam karta hai
with word alignments He? vah, works? kaam
karta hai . Automatic aligners often make mis-
takes on these multi-word alignments, and this
create problems for words like karta and hai
which often do not have direct equivalents in En-
glish. To deal with this issue we manually build a
small phrase table for the most frequent Hindi and
Urdu words by a consulting an online Hindi-Urdu-
English dictionary (Platts, 1884). We also man-
ually handle the frequent examples we observed
of cases where we need to handle differences in
tokenization between Hindi and Urdu (e.g keliye
written as one word in Urdu and as ke liye in
Hindi).
The other issue with the pivot language ap-
proach is that for word pairs which are rare in
one of the languages,?e P (u|e)P (e|h) can eas-
ily work out to zero. This is exacerbated by align-
ment errors for rarer words. Thus, to strengthen
our phrase table especially for infrequent words,
we use a transliteration approach to build a phrase
table. Note that for rare words like names of peo-
ple and places, the words in Hindi and Urdu are
transliterations of each other.
In light of the issues in transliterating between
Hindi and Urdu (Malik et al, 2008; Malik et
al., 2009) we take a statistical approach (Abdul-
Jaleel and Larkey, 2003) to building a translitera-
tion based phrase table.
We assume a generative model for producing
Urdu words from Hindi words based on a charac-
ter transliteration probability table Pc. The prob-
ability Pt(u|h) of generating a Urdu word u from
a Hindi word h is given by:
Pt(u|h) =
?
a
?
i
Pc(ui|ha(i))P (ai|ai?1),
where a represents the alignment between the
Hindi and Urdu characters, a(i) is the the index
of the Hindi character that the ith Urdu charac-
ter is aligned to, Pc(uc|hc) is the probability of
an Urdu character uc being generated by a Hindi
character hc and P (ai|ai?1) represents a distor-
tion probability. Since transliteration is mono-
tonic and we want to encourage small jumps we
set: P (ai|ai?1) = c?(ai?ai?1) for ai > ai?1 and
0 otherwise. To obtain Pc we use the EM algo-
rithm and we can reuse standard machinery that
is used to obtain HMM word alignments in Statis-
tical Machine Translation (with the constraint of
Monotone alignments). To calculate a translitera-
tion based phrase table, for each Hindi word h we
search over a large vocabulary of Urdu words and
retain words u for which Pt(u|h) is sufficiently
high as possible transliterations of h. We set the
probabilities in the transliteration based phrase ta-
ble to be proportional to Pt(u|h). Finding this ta-
ble requires calculating Pt(u|h) for every pair of
words in the Urdu and Hindi vocabulary, we use
the Forward-Backward algorithm for efficiency
and parallelize the calculations over several ma-
chines.
The only remaining issue is how we get train-
ing data to train our transliteration model. To ob-
tain such training data we use a table of consonant
character conversions between Hindi and Urdu as
given in (Malik et al, 2008). We look for words in
our pivot language based translation table, where
there are at least three consonants and at least 50%
of the consonants are shared. We observed that
this yields pairs of words that are transliterations
of one another with high precision. These word
pairs are used as training data to build our charac-
ter transliteration model Pc.
Final word translation table is obtained by com-
bining our three approaches as follows: If the
word is present in our dictionary, we use the trans-
lation given in the dictionary and exclude all oth-
ers, if not we linearly interpolate between the
probability table we get based on using English
as a pivot language and probability table we get
based on transliteration.
4 Experimental results
In this section we report on experiments to eval-
uate the quality of our translation method de-
scribed in Section 3 and report on the application
of Hindi?Urdu translation to the sharing of lin-
guistic resources between the two languages.
1287
Algorithm 1 Create Urdu-Hindi Phrase Table
for all u such that u is very frequent Urdu word
do
h? Hindi word for u from dictionary
Pd(u|h)? 1
end for
U ? Urdu vocabulary
H ? Hindi vocabulary vocabulary
for all u ? U , h ? H do
Pp(u|h) ?
?
e P (u|e)P (e|h) {Create an
Urdu-Hindi translation table using English as
the pivot}
end for
for all u ? U , h ? H such that Pp(u|h) > ?
and ConsonantOverlap(u, h) > ? do
Add (u, h) to training set T
end for
Pc ?
argmax
Q
?
(u,h)?T
?
a
?
i
Q(ui|hai))P (ai|ai?1)
{Maximize using EM}
for all u ? U , h ? H do
Pt(u|h) ? c
?
a
?
i
Pc(ui|ha(i))P (ai|ai?1)
{Use Forward-Backward Algorithm}
end for
for all u ? U , h ? H do
if Pd(u|h)? 1 then
Pfinal(u|h)? 1
else
Pfinal(u|h)? ?pPp(u|h) + ?tPt(u|h)
end if
end for
4.1 Evaluation of Hindi-Urdu translation
We built a Hindi-Urdu transliteration system as
explained in Section 3. For building a pivot
language based translation table we used 70k
sentences from the NIST MT-08 corpus train-
ing corpus for Urdu-English. For Hindi-English
we used an internal corpus of 230k sentences.
We built our statistical transliteration model on
roughly 3k word pairs that we obtained as de-
scribed in Section 3. For Urdu?Hindi translation,
we used a five gram language model built from
a crawl of archives from Hindi news web sites
(the corpus size was about 60 million words). For
Hindi?Urdu translation we use the MT-08 Urdu
corpus (about 1.5 million words) to build a trigram
LM.
We evaluated the translation system in translat-
ing from Urdu to Hindi. We asked an annotator to
evaluate 100 sentences ( 2700 words), by marking
an error on a word if it was a wrong translation or
unnatural in Hindi. We compared our translation
system against the Hindi Urdu Machine Translit-
eration (HUMT) system3. We found an error rate
of 18% for our system as against 46% for the
HUMT system.
4.2 Word alignments
In this section we describe experiments at im-
proving a Hindi-English word aligner using hand
alignments for an Urdu-English corpus. For the
Urdu-English corpus we use a manually word
aligned corpus of roughly 10k sentences, while
for the Hindi-English corpus we had roughly 3k
sentences out of which we set aside 300 sentences
( 5300 words) for a test set. In addition to these
(relatively) small supervised corpora we also use
a sentence parallel Hindi-English corpus (without
manual word alignments) of roughly 250k sen-
tences.
For word alignments we use the Maximum
Entropy aligner described in (Ittycheriah and
Roukos, 2005) that is trained using hand aligned
training data. We first translate the Urdu sentences
in the Urdu-English word aligned corpus to Hindi,
and then transfer the alignments by simply replac-
ing the alignment links to a Urdu word by links
to the corresponding decoded Hindi word. The
above procedure covers bulk of the cases since
Urdu-Hindi translation is largely a word to word
translation. The special case of a phrase of multi-
ple Urdu words decoded to multiple Hindi words
is handled as follows: we align each of the words
in the Hindi phrase to the union of the sets of
English words that each word in the Urdu phrase
aligns to. Once we convert the Urdu-English man-
ual alignments to an additional corpus we build
two Hindi-English alignment models, one on the
original corpus, the other on the (Urdu?Hindi)-
English corpus. The MaxEnt aligner (Ittycheriah
and Roukos, 2005) models the probability of a
3http://www.puran.info/HUMT/HUMT.aspx
1288
nTrain Hindi data + Urdu
5 60.8 69.8
50 64.1 70.5
800 71.4 73.0
2800 75.1 75.7
Table 1: Word alignment F-Measure as a func-
tion of the number of manually aligned Hindi-
English sentences used for training. The third col-
umn shows improvements obtained by adding 10k
Urdu-English word alignments sentences.
particular set of links in the alignment L given the
source sentence S and the target sentence T as:
P (L|S, T ) = ?Mi=1 p(li|tM1 , sK1 , li?11 ). Let us de-
note by Ph and Pu the alignment models trained
on the Hindi-English and the (Urdu?Hindi)-
English corpora respectively. We combine these
models log-linearly to obtain our final model for
alignment:
P (L|S, T ) = P?h (L|S, T )P 1??u (L|S, T ).
To find the most likely alignment we use the same
algorithm as in (Ittycheriah and Roukos, 2005)
since the structure of the model is unchanged.
We report on the performance (Table 1) of a
baseline Hindi-English word aligner built with
varying amounts of Hindi-English manually word
aligned training data compared against an aligner
that combines in a model trained on the 10k
(Urdu?Hindi)-English sentences. We observe
large gains with small amounts of labelled Hindi-
English alignment data, and even when we have
2800 sentences of Hindi-English data we see a
gain in performance adding in the Urdu data.
We note that the MaxEnt aligner we use (Itty-
cheriah and Roukos, 2005) defaults to (roughly)
doing an HMM alignment using a word trans-
lation matrix obtained via unsupervised training.
Thus the aligners reported on in Table 1 use a
large amount of unsupervised data in addition to
the small amounts of labelled data mentioned in
the Table.
4.3 POS tagging
Unlike English for which there is an abundance
of POS training data for Hindi and Urdu data is
quite limited. For our experiments, we use the
num. words f(wi, ti), g(ti?1, ti) + h(tui , ti)
5k 76.5 82.5
10k 81.7 84.7
20k 84.5 86.7
47k 90.6 91.0
Table 2: POS tagging accuracy as a function of
the amount of Hindi POS tagged data used to
build the model. The third column indicates the
use of the Urdu data via a feature type.
CRULP corpus (Hussain, 2008) for Urdu and a
corpus from IITB (Dalal et al, 2007) for Hindi.
The CRULP POS corpus has 150k words and
uses a tagset of size 46 to tag the corpus. The
IITB corpus has 50k words and uses a tagset of
size 26. We set a side a test set of size 5k words
from the IITB corpus. For part of speech tagging
we use CRFs (Lafferty et al, 2001) with two types
of features, f(ti, wi) and g(ti, ti?1). With the
small amounts of training data we have, adding
additional feature templates degraded the perfor-
mance.
In our POS tagging experiments we consider
using the Urdu corpus to help POS tagging in
Hindi. We first translate all of the CRULP Urdu
data to Hindi. We cannot simply add in this data
to the training data because of differences in the
tagsets used in the data sets for the two languages.
In order to make use of the additional Urdu POS
tagged data (translated to Hindi), we build a sep-
arate POS tagger on this data, and use predictions
from this model as a feature in training the Hindi
POS tagger. We use these predictions via a fea-
ture template h(ti, tui ) where tui denotes the tag
assigned to the ith word by the POS tagger built
from the CRULP Urdu data set translated into
Hindi.
We present results in Table 2 with varying
amounts of Hindi data used for training, in each
case we present results with and without use of
the Urdu resources. We see a small gain even
when we use all of the available Hindi training
data and as expected we see larger gains when
smaller amounts of Hindi data are used.
We analyzed the type of errors and the er-
ror reduction when using the Urdu data for the
case where we used only 5k words of Hindi data.
1289
We find that the two frequent error types that
were greatly reduced were noun being tagged
as main verb (reduction of 65% relative) and
main verb tagged as auxiliary verb (reduction of
71%). Reduction in confusion between nouns and
main verbs is expected since these are open word
classes that can most benefit from additional data.
This also causes the reduction in errors of tag-
ging main verbs as auxiliary verbs, since in Hindi,
verbs are multi word groups with a main verb fol-
lowed by one or more auxiliary verbs. Reduction
of error rate in most of the other error types were
close to the overall error rate reduction.
4.4 Sharing parallel corpora for machine
translation
We experimented with using our internal Hindi-
English parallel corpus ( 230k) sentences to obtain
better translation for Urdu-English. The Urdu-
English corpus we use is the NIST MT-08 training
data set ( 70k sentences). We use the Direct Trans-
lation Model 2 (DTM) described in (Ittycheriah
and Roukos, 2007) for all our translation experi-
ments.
We build our baseline Urdu?English system
using the NIST MT-08 training data. In training
our DTM model we use HMM alignments, align-
ments with the MaxEnt aligner, and hand align-
ments for 10k sentences (the hand alignments
were used to train the MaxEnt aligner).
We translated the Hindi in our Hindi-English
corpus to Urdu, creating an additional Urdu-
English corpus. We then use a MaxEnt aligner
to align the Urdu-English words in this corpus.
Since we expect this corpus to be relatively noisy
due to incorrect translation from Urdu to Hindi we
do not include this corpus while generating HMM
alignments. We add the synthetic Urdu-English
data with MaxEnt alignments to our baseline data
and train a DTM model. Results comparing to the
baseline are given Table 3, which shows an im-
provement of 0.8 in BLEU score over the baseline
system by using data from the Hindi-English cor-
pus.
This improvement is not due to unknown
words being covered (the vocabulary covered is
the same). Also note that in the bridge language
approach we cannot get alernative translations
Corpus MT08 Eval
Urdu 23.1
+Hindi 23.9
Table 3: Improvement in Urdu-English machine
translation using Hindi-English data .
for single words that were not already present in
the Urdu-English phrase table. Thus, we believe
that the improvement is due to longer phrases
being seen more often in training. An example
improved translation is shown below:
Ref: just as long as its there they feel safe
Baseline: as long as this they just think there are safe
Improved: just as long as they are there they feel safe
5 Conclusions
In this paper, we showed that we can translate be-
tween Hindi and English without a parallel corpus
and improve upon previous efforts at transliterat-
ing between the two languages. We also showed
that Hindi-Urdu translation can be useful to the
sharing of linguistic resources between the two
languages. We believe this approach to sharing
linguistic resources will be of immense value es-
pecially with resources like treebanks which re-
quire a large effort to develop.
Acknowledgments
We thank Salim Roukos and Abe Ittycheriah for
discussions that helped guide our efforts.
References
[AbdulJaleel and Larkey2003] AbdulJaleel, Nasreen
and Leah S. Larkey. 2003. Statistical transliteration
for english-arabic cross language information
retrieval. In CIKM.
[Brown et al1993] Brown, Peter F., Vincent J.Della
Pietra, Stephen A. Della Pietra, and Robert. L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19:263?311.
[Dalal et al2007] Dalal, Aniket, Kumara Nagaraj, Uma
Sawant, Sandeep Shelke, and Pushpak Bhat-
tacharyya. 2007. Building feature rich pos tagger
for morphologically rich languages. In Proceed-
ings of the Fifth International Conference on Nat-
ural Language Processing, Hyderabad, India, Jan-
uary.
1290
[Hussain2008] Hussain, Sarmad. 2008. Resources for
urdu language processing. In Proceedings of the 6th
workshop on Asian Language Resources.
[Ittycheriah and Roukos2005] Ittycheriah, Abraham
and Salim Roukos. 2005. A maximum entropy
word aligner for arabic-english machine translation.
In HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abraham
and Salim Roukos. 2007. Direct translation model
2. In Sidner, Candace L., Tanja Schultz, Matthew
Stone, and ChengXiang Zhai, editors, HLT-NAACL,
pages 57?64. The Association for Computational
Linguistics.
[Koehn et al2003] Koehn, Philipp, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In NAACL ?03: Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
[Lacoste-Julien et al2006] Lacoste-Julien, Simon,
Benjamin Taskar, Dan Klein, and Michael I. Jordan.
2006. Word alignment via quadratic assignment. In
HLT-NAACL.
[Lafferty et al2001] Lafferty, J., A. McCallum, , and
F. Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning.
[Malik et al2008] Malik, M. G. Abbas, Christian
Boitet, and Pushpak Bhattacharyya. 2008. Hindi
urdu machine transliteration using finite-state trans-
ducers. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 537?544, Manchester, UK, August.
Coling 2008 Organizing Committee.
[Malik et al2009] Malik, Abbas, Laurent Besacier,
Christian Boitet, and Pushpak Bhattacharyya. 2009.
A hybrid model for urdu hindi transliteration. In
Proceedings of the 2009 Named Entities Workshop:
Shared Task on Transliteration (NEWS 2009), pages
177?185, Suntec, Singapore, August. Association
for Computational Linguistics.
[Moore2004] Moore, Robert C. 2004. Improving
ibm word alignment model 1. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
518?525, Barcelona, Spain, July.
[Platts1884] Platts, John T. 1884. A dictionary of
Urdu, classical Hindi and English. W. H. Allen and
Co.
[Rabiner1990] Rabiner, Lawrence R. 1990. A tutorial
on hidden markov models and selected applications
in speech recognition. pages 267?296.
[Sinha2009] Sinha, R. Mahesh K. 2009. Developing
english-urdu machine translation via hindi. In Third
Workshop on Computational Approaches to Arabic-
Script-based Languages.
[Vogel et al1996] Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based word align-
ment in statistical translation. In Proceedings of
the 16th conference on Computational linguistics,
pages 836?841, Morristown, NJ, USA. Association
for Computational Linguistics.
[Wu and Wang2007] Wu, Hua and Haifeng Wang.
2007. Pivot language approach for phrase-based
statistical machine translation. In ACL.
[Yarowsky and Ngai2001] Yarowsky, David and Grace
Ngai. 2001. Inducing multilingual pos taggers and
np bracketers via robust projection across aligned
corpora. In NAACL.
1291
