Identifying Similar and Co-referring Documents Across Languages 
Pattabhi R K Rao T 
AU-KBC Research Centre, 
MIT Campus, Anna University, 
Chennai-44, India. 
pattabhi@au-kbc.org 
Sobha L 
AU-KBC Research Centre, 
MIT Campus, Anna University, 
Chennai-44, India. 
sobha@au-kbc.org 
 
 
Abstract 
This paper presents a methodology for 
finding similarity and co-reference of 
documents across languages. The similarity 
between the documents is identified ac-
cording to the content of the whole docu-
ment and co-referencing of documents is 
found by taking the named entities present 
in the document. Here we use Vector Space 
Model (VSM) for identifying both similar-
ity and co-reference. This can be applied in 
cross-lingual search engines where users 
get documents of very similar content from 
different language documents.  
1 Introduction 
In this age of information technology revolution, 
the growth of technology and easy accessibility has 
contributed to the explosion of text data on the web 
in different media forms such as online news 
magazines, portals, emails, blogs etc in different 
languages. This represents 80% of the unstructured 
text content available on the web. There is an ur-
gent need to process such huge amount of text us-
ing Natural Language Processing (NLP) tech-
niques. One of the significant challenges with the 
explosion of text data is to organize the documents 
into meaningful groups according to their content.  
The work presented in this paper has two parts 
a) finding multilingual cross-document similarity 
and b) multilingual cross-document entity co-
referencing. The present work analyzes the docu-
ments and identifies whether the documents are 
similar and co-referring. Two objects are said to be 
similar, when they have some common properties 
between them. For example, two geometrical fig-
ures are said to be similar if they have the same 
shape. Hence similarity is a measure of degree of 
resemblance between two objects. 
Two documents are said to be similar if their 
contents are same. For example a document D1 
describes about a bomb blast incident in a city and 
document D2 also describes about the same bomb 
blast incident, its cause and investigation details, 
then D1 and D2 are said to be similar. But if 
document D3 talks of terrorism in general and ex-
plains bomb blast as one of the actions in terrorism 
and not a particular incident which D1 describes, 
then documents D1 and D3 are dissimilar. The task 
of finding document similarity differs from the 
task of document clustering. Clustering is a task of 
categorization of documents based on domain/field. 
In the above example, documents D1, D2, D3 can 
be said to be in a cluster of crime domain. When 
documents are similar they share common noun 
phrases, verb phrases and named entities. While in 
document clustering, sharing of named entities and 
noun phrases is not essential but still there can be 
some noun phrases and named entities in common. 
Cross-document co-referencing of entities refers to 
the identification of same entities across the docu-
ments. When the named entities present in the 
documents which are similar and also co-
referencing, then the documents are said to be co-
referring documents. 
The paper is further organized as follows. In 
section 2, the motivation behind this paper is ex-
plained and in 3 the methodology used is described. 
Results and discussions are dealt in section 4 and 
conclusion in section 5. 
2 Motivation 
Dekang Lin (1998) defines similarity from the in-
formation theoretic perspective and is applicable if 
the domain has probabilistic model.  In the past 
decade there has been significant amount of work 
done on finding similarity of documents and orga-
nizing the documents according to their content. 
Similarity of documents are identified using differ-
ent methods such as Self-Organizing Maps (SOMs) 
(Kohonen et al 2000; Rauber, 1999), based on On-
tologies and taxanomy (Gruber, 1993; Resnik, 
1995), Vector Space Model (VSM) with similarity 
measures like Dice similarity, Jaccard?s similarity, 
cosine similarity (Salton, 1989). Bagga (Bagga et 
al., 1998) have used VSM in their work for finding 
co-references across the documents for English 
documents. Chung and Allan (2004) have worked 
on cross-document co-referencing using large scale 
corpus, where they have said ambiguous names 
from the same domain (here for example, politics) 
are harder to disambiguate when compared to 
names from different domains. In their work 
Chung and Allan compare the effectiveness of dif-
ferent statistical methods in cross-document co-
reference resolution task. Harabagiu and Maiorano 
(2000) have worked on multilingual co-reference 
resolution on English and Romanian language 
texts. In their system, ?SWIZZLE? they use a data-
driven methodology which uses aligned bilingual 
corpora, linguistic rules and heuristics of English 
and Romanian documents to find co-references. In 
the Indian context, obtaining aligned bilingual cor-
pora is difficult. Document similarity between In-
dian languages and English is tough since the sen-
tence structure differs and Indian languages are 
agglutinative in nature. In the recent years there 
has been some work done in the Indian languages, 
(Pattabhi et al 2007) have used VSM for multilin-
gual cross-document co-referencing, for English 
and Tamil, where no bilingual aligned corpora is 
used. 
One of the methods used in cross-lingual infor-
mation retrieval (CLIR) is Latent Semantic Analy-
sis (LSA) in conjunction with multilingual parallel 
aligned corpus. This approach works well for in-
formation retrieval task where it has to retrieve 
most similar document in one language to a query 
given in another language. One of the drawbacks 
of using LSA in multilingual space for the tasks of 
document clustering, document similarity is that it 
gives similar documents more based on the lan-
guage than by topic of the documents in different 
languages (Chew et al 2007). Another drawback 
of LSA is that the reduced dimension matrix is dif-
ficult to interpret semantically. The examples in 
Table 1, illustrate this. 
 Before Reduction After Reduction 
1
.
{(car),(truck),(flower)} {(1.2810*car+0.5685*tr
uck),(flower) 
2 {(car),(bottle),(flower)} {(1.2810*car+0.5685*b
ottle),(flower) 
Table 1. LSA Example 
 
In the first example the component 
(1.2810*car+0.5685*truck) can be inferred as 
?Vehicle? but in cases such as in second example, 
the component (1.2810*car+0.5685*bottle) does 
not have any interpretable meaning in natural lan-
guage. In LSA the dimension reduction factor ?k? 
has very important role to play and the value of ?k? 
can be found by doing several experiments. The 
process of doing dimension reduction in LSA is 
computationally expensive. When LSA is used, it 
reduces the dimensions statistically and when there 
is no parallel aligned corpus, this can not be inter-
preted semantically. 
Hence, in the present work, we propose VSM 
which is computationally simple, along with cosine 
similarity measure to find document similarity as 
well as entity co-referencing. We have taken Eng-
lish and three Dravidian languages viz. Tamil, Te-
lugu and Malayalam for analysis. 
3 Methodology 
In VSM, each document is represented by a vector 
which specifies how many times each term occurs 
in the document (the term frequencies). These 
counts are weighted to reflect the importance of 
each term and weighting is the inverse document 
frequency (idf). If a term t occurs in n documents 
in the collection then the ?idf? is the inverse of log 
n. This vector of weighted counts is called a "bag 
of words" representation. Words such as "stop 
words" (or function words) are not included in the 
representation.  
The documents are first pre-processed, to get 
syntactic and semantic information for each word 
in the documents. The preprocessing of documents 
involves sentence splitting, morph analysis, part-
of-speech (POS) tagging, text chunking and named 
entity tagging. The documents in English are pre-
processed using Brill?s Tagger (Brill, 1994) for 
POS tagging and fn-TBL (Ngai and Florian, 2001) 
for text chunking. The documents in Indian lan-
guages are preprocessed, using  a generic engine 
(Arulmozhi et al, 2006) for POS tagging, and text 
chunking based on TBL (Sobha and Vijay, 2006). 
For both English and Indian language documents 
the named entity tagging is done using Named En-
tity Recognizer (NER) which was developed based 
on conditional random field (CRF). The tagset 
used by the NER tagger is a hierarchical tagset, 
consists of mainly i) ENAMEX, ii) NUMEX and 
iii) TIMEX. Inside the ENAMEX there are mainly 
11 subtype?s viz. a) Person b) Organization c) Lo-
cation d) Facilities e) Locomotives f) Artifacts g) 
Entertainment h) Cuisines i) Organisms j) Plants k) 
Disease.  For the task of multilingual cross-
document entities co-referencing, the documents 
are further processed for anaphora resolution 
where the corresponding antecedents for each ana-
phor are tagged in the document. For documents in 
English and Tamil, anaphora resolution is done 
using anaphora resolution system. For documents 
in Malayalam and Telugu anaphora resolution is 
done manually. After the preprocessing of docu-
ments, the language model is built by computing 
the term frequency ? inverse document frequency 
(tf-idf) matrix. For the task of finding multilingual 
cross-document similarity, we have performed four 
different experiments. They are explained below: 
 
E1: The terms are taken from documents after 
removing the stop words. These are raw terms 
where no preprocessing of documents is done; the 
terms are unique words in the document collection. 
E2: The terms taken are the words inside the 
noun phrases, verb phrases and NER expressions 
after removing the stop words. 
E3: The whole noun phrase/verb phrase/NER 
expression is taken to be a single term. 
E4: The noun phrase/NER expression along 
with the POS tag information is taken as a single 
term. 
The first experiment is the standard VSM im-
plementation. The rest three experiments differ in 
the way the terms are taken for building the VSM. 
For building the VSM model which is common for 
all language document texts, it is essential that 
there should be translation/transliteration tool. First 
the terms are collected from individual language 
documents and a unique list is formed. After that, 
using the translation/transliteration tool the equiva-
lent terms in language L2 for language L1 are 
found. The translation is done using a bilingual 
dictionary for the terms present in the dictionary. 
For most of the NERs only transliteration is possi-
ble since those are not present in the dictionary. 
The transliteration tool is developed based on the 
phoneme match it is a rule based one. All the In-
dian language documents are represented in roman 
notation (wx-notation) for the purpose of process-
ing.  
After obtaining equivalent terms in all lan-
guages, the VSM model is built. Let S1 and S2 be 
the term vectors representing the documents D1 
and D2, then their similarity is given by equation 
(1) as shown below. 
 
Sim(S1,S2) = ? (W1j x W2j )                      -- (1) 
  tj 
 Where,  
       tj is a term present in both vectors S1and S2. 
       W1j is the weight of term tj in S1 and  
       W2j is the weight of term tj in S2. 
 
The weight of term tj in the vector S1 is calculated 
by the formula given by equation (2), below. 
 
Wij=(tf*log(N/df))/[sqrt(Si12+Si22+??+Sin2)] --(2) 
Where, 
 tf = term frequency of term tj 
 N=total number of documents in the collection 
df = number of documents in the collection that 
the term tj    occurs in. 
 sqrt represents square root 
The denominator [sqrt(Si12+Si22+??+Sin2)] is the co-
sine normalization factor. This cosine normalization 
factor is the Euclidean length of the vector Si, where ?i? 
is the document number in the collection and Sin2 is the 
square of the product of (tf*log(N/df)) for term tn in the 
vector Si. 
For the task of multilingual cross-document en-
tity co-referencing, the words with-in the anaphor 
tagged sentences are considered as terms for build-
ing the language model.  
4 Results and Discussion 
The corpus used for experiments is collected from 
online news magazines and online news portals. 
The sources in English include ?The Hindu?, 
?Times of India?, ?Yahoo News?, ?New York 
Times?, ?Bangkok Post?, ?CNN?, ?WISC?, ?The 
Independent?. The sources for Tamil include ?Di-
namani?, ?Dinathanthi?, ?Dinamalar?, ?Dina-
karan?, and ?Yahoo Tamil?. The work was primar-
ily done using English and Tamil. Later on this 
was extended for Malayalam and Telugu. The data 
sources for Malayalam are ?Malayala Manorama?, 
?Mathrubhumi?, ?Deshabhimani?, ?Deepika? and 
sources for Telugu include ?Eenadu?, ?Yahoo Te-
lugu? and ?Andhraprabha?. First we discuss about 
English and Tamil and Later Telugu and Malaya-
lam. 
The domains of the news taken include sports, 
business, politics, tourism etc. The news articles 
were collected using a crawler, and hence we find 
in the collection, a few identical news articles be-
cause they appear in different sections of the news 
magazine like in Front page section, in state sec-
tion and national section. 
The dataset totally consists of 1054 English 
news articles, 390 Tamil news articles. Here we 
discuss results in two parts; in the first part results 
pertaining to document similarity are explained. In 
second part we discuss results on multilingual 
cross-document entity co-referencing. 
4.1 Document Similarity 
The data collection was done in four instances, 
spread in a period of two months. At the first in-
stance two days news was crawled from different 
news sources in English as well as Tamil. In the 
first set 1004 English documents and 297 Tamil 
documents were collected. 
In this set when manually observed (human 
judgment) it was found that there are 90 similar 
documents forming 31 groups, rest of the docu-
ments were not similar. This is taken as gold stan-
dard for the evaluation of the system output. 
As explained in the previous section, on this set 
the four experiments were performed. In the first 
experiment (E1), no preprocessing of the docu-
ments was done except that the stop words were 
removed and the language model was built. In this 
it was observed that the number of similar docu-
ments is 175 forming 25 groups. Here it was ob-
served that along with actual similar documents, 
system also gives other not similar documents (ac-
cording to gold standard) as similar ones. This is 
due to the fact there is no linguistic information 
given to the system, hence having words alone 
does not tell the context, or in which sense it is 
used. And apart from that named entities when 
split don?t give exact meaning, for example in 
name of hotels ?Leela Palace? and ?Mysore Pal-
ace?, if split into words yields three words, 
?Leela?, ?Mysore?, and ?Palace?. In a particular 
document, an event at hotel Leela Palace is de-
scribed and the hotel is referred as Leela Palace or 
by Palace alone. Another document describes 
about Dussera festival at Mysore Palace. Now here 
the system identifies both these documents to be 
similar even though both discuss about different 
events. The precision of the system was observed 
to be 51.4%, where as the recall is 100% since all 
the documents which were similar in the gold stan-
dard is identified. Here while calculating the preci-
sion; we are considering the number of documents 
that are given by the system as similar to the num-
ber of documents similar according to the gold 
standard. 
Hence to overcome the above discussed prob-
lem, we did the second experiment (E2) where 
only words which occur inside the noun phrases, 
verb phrases and named entities are considered as 
terms for building the language model. Here it is 
observed that the number of similar documents is 
140 forming 30 groups. This gives a precision of 
64.2% and 100% recall. Even though we find a 
significant increase in the precision but still there 
are large number of false positives given by the 
system. A document consists of noun phrases and 
verb phrases, when the individual tokens inside 
these phrases are taken; it is equivalent to taking 
almost the whole document. This reduces the 
noise. The problem of ?Leela Palace? and ?Mysore 
Palace? as explained in the previous paragraph still 
persists here. 
In the third experiment (E3) the whole noun 
phrase, verb phrase and named entity is considered 
as a single term for building the language model. 
Here the phrases are not split into individual to-
kens; the whole phrase is a single term for lan-
guage model. This significantly reduces the num-
ber of false positives given by the system. The sys-
tem identifies 106 documents as similar documents 
forming 30 groups. Now the precision of the sys-
tem is 84.9%. In this experiment, the problem of 
?Leela Palace? and ?Mysore Palace? is solved. 
Though this problem was solved the precision of 
the system is low, hence we performed the fourth 
(E4) experiment. 
In the fourth experiment (E4), the part-of-speech 
(POS) information is given along with the phrase 
for building the language model. It is observed that 
the precision of the system increases. The number 
of similar documents identified is 100 forming 31 
groups. This gives a precision of 90% and a recall 
of 100%.  
Another important factor which plays a crucial 
role in implementation of language model or VSM 
is the threshold point. What is the threshold point 
that is to be taken? For obtaining an answer for this 
question, few experiments were performed by set-
ting the threshold at various points in the range 
0.75 to 0.95. When the threshold was set at 0.75 
the number of similar documents identified by the 
system was larger, not true positives but instead 
false positives. Hence the recall was high and pre-
cision was low at 50%. When the threshold was 
moved up and set at 0.81, the number of similar 
documents identified was more accurate and the 
number of false positives got reduced. The preci-
sion was found to be 66%.  When the threshold 
was moved up still further and set at 0.90, it was 
found that the system identified similar documents 
which were matching with the human judgment. 
The precision of the system was found to be 90%. 
The threshold was moved up further to 0.95, think-
ing that the precision would further improve, but 
this resulted in documents which were actually 
similar to be filtered out by the system. Hence the 
threshold chosen was 0.9, since the results ob-
tained at this threshold point had matched the hu-
man judgment. For the experiments E1, E2, E3 and 
E4 explained above, the threshold is fixed at 0.9. 
A new set of data consisting of 25 documents 
from 5 days news articles is collected. This is com-
pletely taken from single domain, terrorism. These 
news articles describe specifically the Hyderabad 
bomb blast, which occurred on August 25th 2007. 
All these 25 documents were only English docu-
ments from various news magazines. This data set 
was collected specifically to observe the perform-
ance of the system, when the documents belonging 
to single domain are given. In the new data set, 
from terrorism domain, human judgment for docu-
ment similarity was found to have 13 similar docu-
ments forming 3 groups. While using this data set 
the noun phrases, verb phrases and named entities 
along with POS information were taken as terms to 
build the language model and the threshold was set 
at 0.9, it was observed that the system finds 14 
documents to be similar forming 3 groups. Here, 
out of 14 similar documents, only 12 documents 
match with the human judgment and one document 
which ought to be identified was not identified by 
the system. The document which was not identified 
described about the current event, that is, bomb 
blast on 25th August in the first paragraph and then 
the rest of the document described about the simi-
lar events that occurred in the past. Hence the simi-
larity score obtained for this document with respect 
to other documents in the group was 0.84 which is 
lower than the threshold fixed. Hence the recall of 
the system is 92.3% and the precision of the sys-
tem is 85.7%. 
Another data set consisting of 114 documents 
was taken from tourism domain. The documents 
were both in Tamil and English, 79 documents in 
Tamil and 35 documents in English. This data set 
describes various pilgrim places and temples in 
Southern India. The human annotators have found 
21 similar documents which form a group of three. 
These similar documents describe about Lord 
Siva?s and Lord Murugan?s temples.  The system 
obtained 25 documents as similar and grouped into 
three groups. Out of 25 documents obtained as 
similar, four were dissimilar. These dissimilar 
documents described non-Siva temples in the same 
place. In these dissimilar documents the names of 
offerings, festivals performed were referred by the 
same names as in the rest of the documents of the 
group, hence these documents obtained similarity 
score of 0.96 with respect to other documents in 
the group. Here we get a precision of 84% and a 
recall of 100%. 
A new data set consisting of 46 documents was 
taken from various news magazines. This set con-
sists of 24 English documents, 11 Tamil docu-
ments, 7 Malayalam documents and 4 Telugu 
documents.  This data set describes the earthquake 
in Indonesia on 12th September 2007 and tsunami 
warning in other countries. The news articles were 
collected on two days 13th and 14th September 
2007.  
The documents collected were in different font 
encoding schemes. Hence before doing natural 
language processing such as morph-analysis, POS 
tagging etc, the documents were converted to a 
common roman notation (wx-notation) using the 
font converter for each encoding scheme. 
Here we have used multilingual dictionaries of 
place; person names etc for translation. The lan-
guage model is built by taking noun phrases and 
verb phrases along with POS information were as 
terms. In this set human annotators have found 45 
documents to be similar and have grouped them 
into one group. The document which was identi-
fied as dissimilar describes about a Tamil film 
shooting at Indonesia being done during the quake 
time. The system had identified all the 46 docu-
ments including the film shooting document in the 
collection to be similar and put into one group. The 
?film shooting? document consisted of two para-
graphs about the quake incident, other two para-
graphs consisted of statement by the film producer 
stating that the whole crew is safe and the shooting 
is temporarily suspended for next few days. Since 
this document also contained the content describ-
ing the earthquake found in other documents of the 
group, the system identified this ?film shooting? 
document to be similar. Here one interesting point 
which was found was that all the documents gave a 
very high similarity score greater than 0.95. Hence 
the precision of the system is 97.8% and recall 
100%. 
The summary of all these experiments with dif-
ferent dataset is shown in the table 2 below. 
SNo Dataset Preci-
sion % 
Recall 
% 
1 English 1004 and Tamil 
297 documents 
90.0 100.0 
2 English 25 ? terrorism 
domain documents 
85.7 92.3 
3 35 English Docs and 
Tamil 79 docs - Tour-
ism domain 
84.0 100.0 
4 46 Docs on Earth 
Quake incident ? 24 
English, 11 Tamil, 7 
Malayalam, 4 Telugu 
97.8 100.0 
Average 89.3 % 98.07% 
Table 2. Summary of Results for Document 
similarity for four different data sets 
4.2 Document Co-referencing 
The documents that were identified as similar ones 
are taken for entity co-referencing. In this work the 
identification of co-referencing documents is done 
for English and Tamil. In this section first we dis-
cuss the co-referencing task for English documents 
in terrorism domain, then for documents in English 
and Tamil in Tourism domain. In the end of this 
section we discuss about documents in English and 
Tamil, which are not domain specific. 
  In the first experiment, the document collection in 
terrorism domain is taken for co-referencing task. 
This data set of 25 documents in terrorism domain 
consists of 60 unique person names. In this work 
we consider only person names for entity co-
referencing. In this data set, 14 documents are 
identified as similar ones by the system. These 14 
documents consist of 26 unique person names. .  
The language model is built using only named 
entity terms and the noun, verb phrases occurring 
in the same sentence where the named entity oc-
curs. POS information is also provided with the 
terms. Here we find that out of 26 entities, the sys-
tem co-references correctly for 24 entities, even 
though the last names are same.  The results ob-
tained for these named entities is shown in the be-
low table Table 3. 
E
ntity 
N
am
e 
N
o. of links  
containing 
the entity
C
orrect 
R
esponses 
obtained
T
otal R
e-
sponses ob-
tained 
P
recision 
%
 
R
ecall %
 
Y S Ra-
jasekhar 
Reddy 
7 7 7 100 100 
Indrasena 
Reddy 
1 1 1 100 100 
K Jana 
Reddy 
1 1 1 100 100 
Shivaraj 
Patil 
2 2 2 100 100 
Manmohan 
Singh 
4 4 4 100 100 
Abdul Sha-
hel 
Mohammad 
1 1 2 50 100 
Mohammad 
Abdullah 
1 1 2 50 100 
Mohammad 
Amjad 
1 1 1 100 100 
Mohammad 
Yunus 
1 1 1 100 100 
Ibrahim 1 1 1 100 100 
Dawood 
Ibrahim 
1 1 1 100 100 
Madhukar 
Gupta 
3 3 3 100 100 
N Chandra-
babu Naidu 
2 2 2 100 100 
Tasnim 
Aslam 
2 2 2 100 100 
Mahender 
Agrawal 
1 1 1 100 100 
Somnath 
Chatterjee 
2 2 2 100 100 
Pervez 
Musharaff 
2 2 2 100 100 
Sonia Gan-
dhi 
2 2 2 100 100 
Taslima 1 1 1 100 100 
Nasrin 
Bandaru 
Dattatreya 
1 1 1 100 100 
L K Advani 2 2 2 100 100 
Average 95.2 100 
Table 3. Results for entity co-referencing for Eng-
lish documents in terrorism domain 
 
The system identifies the entity names ending 
with ?Reddy? correctly. These names in the docu-
ments occur along with definite descriptions which 
helps the system in disambiguating these names. 
For example ?Y S Rajasekhar Reddy? in most cases 
is referred to as ?Dr. Reddy? along with the defi-
nite description ?chief minister?. Similarly the 
other name ?K Jana Reddy? occurs with the defi-
nite description ?Home minister?. Since here we 
are taking full noun phrases as terms for building 
language model, this helps obtaining good results. 
For entities such as ?Abdul Shahel Mohammad? 
and ?Mohammad Abdullah?, it is observed that the 
both names are referred in the documents as 
?Mohammad? and surrounding phrases do not 
have any distinguishing phrases such as definite 
descriptions, which differentiate these names. Both 
these entities have been involved in masterminding 
of the Hyderabad bomb blast. Hence the system 
couldn?t disambiguate between these two named 
entities and identifies both to be same, hence it 
fails here.  
In the second experiment, the data set in Tour-
ism domain consisting of 79 Tamil Documents and 
35 English documents is taken for the task of co-
referencing. In this data set 25 documents were 
identified as similar. Now these similar documents 
of 25 are considered for entity co-referencing task. 
There are 35 unique names of Gods. Here in this 
domain, one of the interesting points is that, there 
are different names to refer to a single God. For 
example Lord Murugan, is also referred by other 
names such as ?Subramanyan?, ?Saravana?, ?Kart-
tikeyan?, ?Arumukan? etc. Simialrly for Lord Siva 
is referred by ?Parangirinathar?, ?Dharbaranes-
wara? etc. It is observed that in certain documents 
the alias names are not mentioned along with 
common names. In these instances even human 
annotators found it tough for co-referencing, hence 
the system could not identify the co-references. 
This problem of alias names can be solved by hav-
ing a thesaurus and using it for disambiguation. 
The results obtained for these named entities are 
shown in the table 4, below. 
E
ntity 
N
am
e 
N
o. of 
links  con-
taining the 
entity
C
orrect 
R
esponses 
obtained  
T
otal R
e-
sponses 
obtained 
P
recision 
%
 
R
ecall %
 
Murugan 7 7 8 87.5 100 
Shiva 10 9 9 100 90 
Parvathi 10 9 11 81.8 90 
Nala 5 5 5 100 100 
Damayan-
thi 
2 2 2 100 100 
Narada 3 3 3 100 100 
Sanees-
warar 
6 6 7 85.7 100 
Deivayani 4 4 4 100 100 
Vishnu 2 2 2 100 100 
Vinayaka 3 3 3 100 100 
Indra 2 2 2 100 100 
Thiruna-
vukkarasar 
1 1 1 100 100 
Mayan 2 2 2 100 100 
Average 96.5 98.4 
Table 4. Results for entity co-referencing for 
English and Tamil Documents in Tourism domain 
 
The co-referencing system could disambiguate a 
document which was identified as similar by the 
system and dissimilar by the human annotator. 
 Another experiment is performed where both 
English and Tamil Documents are taken for entity 
co-referencing. In this experiment we have taken 
the data set in which there are 1004 English docu-
ments and 297 Tamil documents.  The documents 
are not domain specific. Here 100 documents are 
identified as similar ones, which contains of 64 
English and 36 Tamil documents. Now we con-
sider these 100 similar documents for entity co-
referencing. In the 100 similar documents, there 
are 520 unique named entities. The table (Table 5) 
below shows results of few interesting named enti-
ties in this set of 100 similar documents. 
E
ntity 
N
am
e 
N
o. of links  
containing 
the entity
C
orrect 
R
esponses 
obtained 
T
otal R
e-
sponses ob-
tained 
P
recision 
%
 
R
ecall  %
 
Karunanidhi 7 7 7 100 100 
Manmohan Singh 15 14 16 87.5 93.3 
Sonia Gandhi 54 54 58 93.1 100 
Shivaraj Patil 8 8 10 80 100 
Prathibha Patil 24 24 26 92.3 100 
Lalu Prasad 5 5 5 100 100 
Atal Bihari Va-
jpayee 
4 4 4 100 100 
Abdul Kalam 22 22 22 100 100 
Sania Mirza 10 10 10 100 100 
Advani 8 8 8 100 100 
Average 95.3 99.3 
Table 5. Results for entity co-referencing for 
English and Tamil Documents not of any specific 
domain 
5 Conclusion 
The VSM method is a well known statistical 
method, but here it has been applied for multilin-
gual cross-document similarity, which is a first of 
its kind. Here we have tried different experiments 
and found that using phrases with its POS informa-
tion as terms for building language model is giving 
good performance. In this we have got an average 
precision of 89.3 and recall of 98.07% for docu-
ment similarity. Here we have also worked on mul-
tilingual cross-document entity co-referencing and 
obtained an average precision of 95.6 % and recall 
of 99.2 %. The documents taken for multilingual 
cross-document co-referencing are similar docu-
ments identified by the similarity system. Consid-
ering similar documents, helps indirectly in getting 
contextual information for co-referencing entities, 
because obtaining similar documents removes 
documents which are not in the same context. 
Hence this helps in getting good precision. Here 
we have worked on four languages viz. English, 
Tamil, Malayalam and Telugu. This can be applied 
for other languages too. Multilingual document 
similarity and co-referencing, helps in retrieving 
similar documents across languages. 
References 
Arulmozhi Palanisamy and Sobha Lalitha Devi. 2006. 
HMM based POS Tagger for a Relatively Free Word 
Order Language, Journal of Research on Computing 
Science, Mexico. 18:37-48. 
Bagga, Amit and Breck Baldwin. 1998. Entity-Based 
Cross-Document Coreferencing Using the Vector 
Space Model, Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics 
and the 17th International Conference on Computa-
tional Linguistics (COLING-ACL'98):79-85. 
Brill, Eric. 1994. Some Advances in transformation 
Based Part of Speech Tagging, Proceedings of the 
Twelfth International Conference on Artificial Intel-
ligence (AAAI-94), Seattle, WA 
Peter A. Chew,  Brett W. Bader, Tamara G. Kolda, Ah-
med Abdelali. 2007. Cross-Language Information 
Retrieval Using PARAFAC2, In the Proceedings 
Thirteenth International Conference on Knowledge 
Discovery and Data Mining (KDD? 07), San Jose, 
California.:143-152. 
Chung Heong Gooi and James Allan. 2004. Cross-
Document Coreference on a Large Scale Corpus, 
Proceedings of HLT-NAACL: 9-16. 
Dekang Lin.  1998. An Information-Theoretic Definition 
of Similarity, Proceedings of International Confer-
ence on Machine Learning, Madison, Wisconsin, 
July.  
T. R. Gruber. 1993. A translation approach to portable 
ontologies, Knowledge Acquisition, 5(2):199?220. 
Harabagiu M Sanda and Steven J Maiorano. 2000. Mul-
tilingual Coreference Resolution, Proceedings of 6th 
Applied Natural Language Processing Conference: 
142?149. 
Kohonen, Teuvo Kaski, Samuel Lagus, Krista Salojarvi, 
Jarkko Honkela, Jukka Paatero,Vesa Saarela, Anti.  
2000. Self organisation of a massive document col-
lection, IEEE Transactions on Neural Networks, 
11(3): 574-585. 
G. Ngai and R. Florian. 2001. Transformation-Based 
Learning in the Fast Lane, Proceedings of the 
NAACL'2001, Pittsburgh, PA: 40-47 
R K Rao Pattabhi, L Sobha, and Amit Bagga. 2007. 
Multilingual cross-document co-referencing, Pro-
ceedings of 6th Discourse Anaphora and Anaphor 
Resolution Colloquium (DAARC), March 29-30, 
2007, Portugal:115-119 
Rauber, Andreas Merkl, Dieter. 1999. The SOMLib 
digital library system,  In the Proceedings of the 3rd 
European Conference on Research and Advanced 
Technology for Digital Libraries (ECDL'99), Paris, 
France. Berlin: 323-341. 
P. Resnik. 1995. Using information content to evaluate 
semantic similarity in taxonomy, Proceedings of 
IJCAI: 448?453. 
Salton, Gerald. 1989. Automatic Text Processing: The 
Transformation, Analysis and Retrieval of Informa-
tion by Computer, Reading, MA: Addison Wesley 
Sobha L, and Vijay Sundar Ram. 2006. Noun Phrase 
Chunker for Tamil, Proceedings of the First National 
Symposium on Modeling and Shallow Parsing of In-
dian Languages (MSPIL), IIT Mumbai, India: 194-
198. 
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11?15,
Beijing, August 2010
How to Get the Same News from Different Language News 
Papers 
T. Pattabhi R. K Rao 
AU-KBC Research Centre 
Anna University Chennai 
 
Sobha Lalitha Devi 
AU-KBC Research Centre 
Anna University Chennai  
sobha@au-kbc.org 
 
Abstract 
This paper presents an ongoing work on 
identifying similarity between documents 
across News papers in different 
languages. Our aim is to identify similar 
documents for a given News or event as 
a query, across languages and make cross 
lingual search more accurate and easy. 
For example given  an event or News in 
English, all the English news documents 
related to the query are retrieved as well 
as in other languages such as Hindi, 
Bengali, Tamil, Telugu, Malayalam, 
Spanish. We use Vector Space Model, a 
known method for similarity calculation, 
but the novelty is in identification of 
terms for VSM calculation. Here a robust 
translation system is not used for 
translating the documents. The system is 
working with good recall and precision. 
1 Introduction 
In this paper we present a novel method for 
identifying similar News documents from 
various language families such as Indo-
European, Indo- Aryan and Dravidian. The 
languages considered from the above language 
families are English, Hindi, Bengali, Tamil, 
Telugu, Malayalam and Spanish. The News 
documents in various languages are obtained 
using a crawler. The documents are represented 
as vector of terms. 
 Given a query in any of the language mentioned 
above, the documents relevant to the query are 
retrieved. The first two document retrieved in the 
language of the query is taken as base for the 
identification of similar documents. The 
documents are converted into terms and the 
terms are translated to other languages using 
bilingual dictionaries. The terms thus obtained is 
used for similarity calculation. The paper is 
further organized as follows.  In the following 
section 2, related work is described. In section 3, 
the algorithm is discussed. Section 4 describes 
experiments and results.  The paper concludes 
with section 5. 
2 Related Work 
In the past decade there has been significant 
amount of work done on finding similarity of 
documents and organizing the documents 
according to their content. Similarity of 
documents are identified using different methods 
such as Self-Organizing Maps (SOMs) 
(Kohonen et al 2000; Rauber, 1999), based on 
Ontologies and taxanomy (Gruber, 1993; Resnik, 
1995), Vector Space Model (VSM) with 
similarity measures like Dice similarity, 
Jaccard?s similarity, cosine similarity (Salton, 
1989). 
    Many similarity measures were developed, 
such as information content (Resnik, 1995) 
mutual information (Hindle, 1990), Dice 
coefficient (Frakes and Baeza-Yates, 1992), 
cosine coefficient (Frakes and Baeza-Yates, 
1992), distance-based measurements (Lee et al, 
1989; Rada et al, 1989), and feature contrast 
model (Tversky, 1977). McGill etc. surveyed 
and compared 67 similarity measures used in 
information retrieval (McGill et al, 1979). 
11
3 Methodology 
Similarity is a fundamental concept. Two 
documents can be said to be similar if both the 
documents have same content, describing a topic 
or an event or an entity. Similarity is a measure 
of degree of resemblance, or commonality 
between the documents. 
    In this work we have used Vector Space 
Model (VSM) for document representation. In 
VSM the documents are represented as vectors 
of unique terms. Here we have performed 
experiments by creating three types of document 
vector space models. In the first case we have 
taken all unique words in the document 
collection for vector of terms. In the second case 
we take the terms after removing all stop words. 
In the third case we have taken a sequence of 
words as terms. After the document model is 
built we use cosine similarity measure to identify 
the degree of similarity between documents.  
    In this work we have taken documents from 
the languages mentioned in the previous section. 
For the purpose of identifying similar documents 
across the languages we use map of term vectors 
of documents from English to other languages. 
Using the term vector map we can identify 
similar documents for various languages. 
3.1 Similarity analyser 
    The main modules are i) Document vector 
creator ii) Translator and iii) Similarity 
identifier.  
a) Document Vector Creator: Each document 
is represented as vector of terms. Here we take 
three types of term vectors. In the first type a 
single word is taken as a term which is the 
standard implementation of VSM. In the second 
type single words are taken but the stop words 
are removed. 
    In the third type each term is a sequence of 
words, where we define the number of words in 
the sequence as 4. This moving window of 4 is 
obtained by performing many experiments using 
different combinations of words. So our term of 
vector is defined as a set of four consecutive 
words, where the last three words in the 
preceding sequence is considered as the first 
three words in the following sequence. For 
example if a sentence has 10 words (w), the 
vector of terms for this sentence is w1w2w3w4, 
w2w3w4w5, w3w4w5w6, w4w5w6w7, 
w5w6w7w8, w6w7w8w9, w7w8w9w10. The 
weights of the terms in the vector are the term 
frequency and inverse document frequency (tf-
idf). While creating document vectors, for Indian 
languages which are highly agglutinative and 
morphologically rich we use morphological 
analyzer to reduce the word into its root and it is 
used for document vector creation.  
    The first two experiments are the standard 
VSM implementation. The third experiment 
differs in the way the terms are taken for 
building the VSM. For building the VSM model 
which is common for all language document 
texts, it is essential that there should be 
translation/transliteration tool. First the terms are 
collected from individual language documents 
and a unique list is formed. The unique list of 
words is then translated using the translator 
module.  
b) Word by Word Translator: In this module, 
the terms from English documents are taken and 
are translated to different languages. The 
translation is done word by word with the use of 
bilingual and multilingual synset dictionaries. 
This translation creates a map of terms from 
English to different languages. We have used 
bilingual dictionaries from English to Spanish, 
Hindi, Tamil, Telugu, and Malayalam 
dictionaries. Also we have used multilingual 
synset dictionaries for English, Tamil, Telugu, 
Hindi, and Malayalam. For each pair of bilingual 
dictionaries there are more than 100K root 
words. Since in this work we do not require 
syntactically and semantically correct translation 
of the sentences we adopted word to word 
translation. Hence we did not use any other 
system such as SMT for English to Indian 
languages. Named entities require transliteration. 
Here we have used a transliteration tool. This 
tool uses rule based approach, based on the 
phoneme match.  The transliteration tool 
produces all possible transliteration outputs. 
Here we take into consideration the top five best 
possible outputs. For example the name ?Lal 
Krishna Advani? would get transliterations in 
Indian languages as ?laala krishna athvaani?, 
?laala krishna advaani?.  
     c) Similarity Identifier: The similarity 
identifier module takes the query in the form 
document as input and identifies all relevant 
12
documents. The similarity identifier uses cosine 
similarity measure over documents vector 
creator. The cosine similarity measure is the dot 
product of two vectors and is between 0 and 1 
value. The more it is closer to 1, the similarity is 
more.  The formula of cosine similarity is as 
follows: 
            Sim(S1,S2)tj = ? (W1j x W2j ) -- (1) 
Where, 
  tj is a term present in both vectors S1and S2. 
  W1j is the weight of term tj in S1 and 
  W2j is the weight of term tj in S2. 
 
The weight of term tj in the vector S1 is 
calculated by the formula given by equation (2), 
below. 
 
Wij=(tf*log(N/df))/[sqrt(Si12+Si22+?+Sin2)]                                                            
                                                           --(2) 
Where, 
  tf = term frequency of term tj  
  N=total number of documents in the collection 
  df = number of documents in the collection that 
          the term tj occurs in. 
  sqrt represents square root 
The denominator 
  [sqrt(Si12+Si22+??+Sin2)] is the cosine 
normalization factor. This cosine normalization 
factor is the Euclidean length of the vector Si, 
where ?i? is the document number in the 
collection and Sin2 is the square of the product 
of (tf*log(N/df)) for term  in the vector Si. 
4 Experiments and Results 
We have performed three experiments with two 
different data sets. The first data set was 
collected by crawling the web for a single day?s 
news articles and obtained 1000 documents from 
various online news magazines in various 
languages. The test set was taken from Times of 
India, The Hindu for English, BBC, Dinamani, 
Dinamalar for Tamil, Yahoo for Telugu, 
Matrubhumi for Malayalam, BBC and Dainik 
Jagran for Hindi and BBC for Spanish. The 
distribution of documents in the first set for 
various languages is as follows: 300 English, 
200 Tamil, 150 Telugu, 125 Hindi, 125 
Malayalam, 50 Spanish. The figure 1 given 
below shows the language distribution in this 
first set.  
The number of similar documents were 600 in 
this set.  
English
Tamil
Telugu
Hindi
Malyalam
Spanish
 
Figure 1. Data Distribution of Set 1 
    In the second data set we have taken news 
documents of one week time duration. This 
consisted of 9750 documents. The language 
distribution for this data set is shown in figure 2. 
This second data set consisted of 5350 similar 
documents.  
        
English
Tamil
Telugu
Hindi
Malayalam
Spanish
      
Figure 2. Data Distribution of Set 2 
In the first experiment we took all the unique 
words (separated by white space) as terms for 
building the document vector. In the second 
experiment the terms taken were same as the 
first experiment, except that all the stop words 
were removed. In the third experiment, the terms 
taken for document vector creation were four 
consecutive words.  The results obtained for 
three experiments for data set 1 is shown in 
Table 1. And results for data set 2 are shown in 
Table 2.  Table 3 shows the similarity 
identification for various languages. 
    Here we take a news story document as a 
query and perform similarity analysis across all 
documents in the document collection to identify 
similarly occurring news stories. In the first data 
set in the gold standard there are 600 similar 
pairs of documents. And in the second data set 
there are 5350 similar pairs of documents in the 
gold standard. 
    It is observed that even though there were 
more similar documents which could have been 
identified, but the system could not identify 
those documents. The cosine measure for those  
13
unidentified documents was found to be lower 
than 0.8. We have taken 0.8 as the threshold for 
documents to be considered similar. In the 
documents which were not identified by the 
system, the content described consisted of less 
number of words. These were mostly two 
paragraph documents; hence the similarity score 
obtained was less than the threshold. In 
experiment three, we find that the number of 
false positives is decreased and also the number 
of documents identified similar is increased. This 
is because, in this case the system sees for terms 
of four words and hence single word matches are 
reduced. This reduces false positives. The other 
advantage of this is the words get the context, in 
a sense that the words in each sequence are not 
independent. The words get an order and are 
sensitive to that order. This solves sense 
disambiguation. Hence we find that it is solving 
the polysemy problem to some extent.  The 
system can be further improved by creating 
robust map files between terms in different 
languages. The bilingual dictionaries also need 
to be improved. 
    In our work, since we are using a sequence of 
words as terms for document vectors, we do not 
require proper, sophisticated translation systems. 
A word by word translation would suffice to get 
the desired results.  
 
 
Table 1. Similarity Results on Data Set 1 
 
Table 2. Similarity Results on Data Set 2  
    Table 3.Similarity Results Data Set with Ex:3 
5 Conclusion 
Here we have shown how we can identify 
similar News document in various languages. 
The results obtained are encouraging; we obtain 
an average precision of 97.8% and recall of 
94.3%. This work differs from previous works in 
two aspects: 1) no language preprocessing of the 
documents is required and 2) terms taken for 
VSM are a sequence of four words.  
References 
Frakes, W. B. and Baeza-Yates, R., editors 1992. 
Information Retrieval, Data Structure and 
Algorithms. Prentice Hall. 
T. R. Gruber. 1993. A translation approach to 
portable ontologies, Knowledge Acquisition, 
5(2):199?220. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of  ACL-90, 
pages 268?275, Pittsburg, Pennsylvania. 
Kohonen, Teuvo Kaski, Samuel Lagus, Krista 
Salojarvi, Jarkko Honkela, Jukka Paatero,Vesa 
Saarela, Anti.  2000. Self organisation of a massive 
document collection, IEEE Transactions on Neural 
Networks, 11(3): 574-585. 
Lee, J. H., Kim, M. H., and Lee, Y. J. 1989. 
Information retrieval based on conceptual distance 
in is-a hierarchies. Journal of Documentation, 
49(2):188?207. 
McGill et al, M. 1979. An evaluation of factors 
affecting document ranking by information 
retrieval systems. Project report, Syracuse 
University School of Information Studies. 
Rauber, Andreas Merkl, Dieter. 1999. The SOMLib 
digital library system,  In the Proceedings of the 
3rd European Conference on Research and 
Advanced Technology for Digital Libraries 
(ECDL'99), Paris, France. Berlin: 323-341. 
Lang Gold 
Std 
similar 
docs 
System 
Identifi
ed 
correct 
System 
Identifi
ed 
wrong 
Prec 
% 
Rec 
% 
Eng 1461 1377 30 97.86 94.25 
Span 732 690 15 97.87 94.26 
Hin 588 554 11 98.05 94.22 
Mal 892 839 19 97.78 94.05 
Tam 932 880 22 97.56 94.42 
Tel 745 703 17 97.63 94.36 
AVG 97.79 94.26 
Exp 
No 
Gold std 
Similari
ty 
System 
Identified 
Correct 
System 
Identified 
Wrong 
Pre
c 
% 
Rec 
% 
1 600 534 50 91.4 89.0 
2 600 547 44 92.5 91.2 
3 600 565 10 98.3 94.2 
Exp 
No 
Gold 
Standard 
Similarity 
System 
Identified 
Correct 
System 
Identifi
ed 
Wrong 
Prec 
% 
Rec 
% 
1 5350 4820 476 91.0 90.0 
2 5350 4903 410 92.3 91.6 
3 5350 5043 114 97.8 94.3 
14
Rada, R., Mili, H., Bicknell, E., and Blettner, M. 
1989. Development and application of a metric on 
semantic nets. IEEE Transaction on Systems, Man, 
and Cybernetics, 19(1):17?30. 
P. Resnik. 1995. Using information content to 
evaluate semantic similarity in taxonomy, 
Proceedings of IJCAI: 448?453. 
Salton, Gerald. 1989. Automatic Text Processing: The 
Transformation, Analysis and Retrieval of 
Information by Computer, Reading, MA: Addison 
Wesley 
Tversky, A. 1977. Features of similarity. 
Pychological Review, 84:327?352. 
 
 
15
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 93?96,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Hybrid Approach for Coreference Resolution 
 
 
First Author: Sobha, Lalitha Devi., Pattabhi, RK Rao., Vijay Sundar Ram, R. 
Second Author: Malarkodi, CS., Akilandeswari, A. 
AU-KBC Research Centre, 
MIT Campus of Anna University, 
Chrompet, Chennai, India. 
sobha@au-kbc.org 
 
 
 
 
 
 
Abstract 
This paper describes our participation in 
the CoNLL-2011 shared task for closed 
task. The approach used combines refined 
salience measure based pronominal 
resolution and CRFs for non-pronominal 
resolution. In this work we also use 
machine learning based approach for 
identifying non-anaphoric pronouns. 
1 Introduction 
In this paper we describe our system, used in the 
CoNLL-2011 shared task ?Modeling Unrestricted 
Coreference in OntoNotes?. The goal of this task is 
to identify coreference chains in a document. The 
coreference chains can include names, nominal 
mentions, pronouns, verbs that are coreferenced 
with a noun phrases.  
The coreferents are classified into two types, 
pronominal and non-pronominal referents. We use 
two different approaches using machine learning 
and salience factor in the resolution of the above 
two types. Pronominal resolution is done using 
salience factors and Non-Pronominals using 
machine learning approach. Pronominal resolution 
refers to identification of a Noun phrase (NP) that 
is referred by a pronominal and Non-Pronominals 
are NP referring to another NP. In the next section 
we describe the system in detail. 
2 System Description 
In this section we give a detailed description of our 
system. The task is divided into two sub-tasks. 
They are 
    i) Pronominal resolution 
   ii) Non-pronominal resolution 
2.1 Pronominal Resolution 
Here we have identified salience factors and 
assigned weights for each factor.  Before resolving 
the pronouns we identify whether a given pronoun 
is anaphoric or not. In example, (1) below, the 
pronoun ?It?, does not refer to any entity, and it is 
a pleonastic ?it?. 
(1) ?It will rain today? 
In identifying the non-anaphoric pronouns such 
as ?it? we use a CRFs engine, a machine learning 
approach. We build a language model using the 
above ML method to identify the non-anaphoric 
pronouns and the features used in training are word 
and it?s POS in a window of five (two preceding 
and two following words to the pronoun). After the 
non-anaphoric pronoun identification, we resolve 
the anaphoric pronouns using a pronominal 
resolution system. Though we use salience factors 
based on the Lappin and Leass (1994), we have 
substantially deviated from the basic algorithm and 
have also used factors from Sobha (2008), where 
named entity and ontology are considered for 
resolution. 
For identifying an antecedent for a pronoun we 
consider all the noun phrases before the pronoun in 
93
the current sentence and in the four sentences 
preceding the current sentence. Those noun 
phrases which agree in PNG with the pronoun are 
considered as the possible candidates. The PNG is 
obtained using the gender data work of Shane 
Bergsma and Dekang Lin (2006). The possible 
candidates are scored based on the salience factors 
and ranked. The salience factors considered here 
are presented in the table 1. 
 
Salience Factors Weights 
Current Sentence 
(sentence in which 
pronoun occurs) 
100 
For the preceding 
sentences up to four 
sentences from the 
current sentence 
Reduce sentence score 
by 10 
Current Clause 
(clause in which 
pronoun occurs) 
100 ? for possessive 
pronoun 
50 ? for non-possessive 
pronouns  
Immediate Clause 
(clause preceding or 
following the current 
clause) 
50 ? for possessive  
pronoun 
100 ? for non-
possessive pronouns 
Non-immediate 
Clause (neither the 
current or immediate 
clause) 
50 
Possessive NP 65 
Existential NP 70 
Subject 80 
Direct Object 50 
Indirect Object 40 
Compliment of PP 30 
  
Table 1: Salience Factors and weights 
 
Improving pronominal resolution Using Name 
Entity (NE) and WordNet: Pronouns such as 
?He?, ?She?, ?I? and ?You? can take antecedents 
which are animate and particularly having the NE 
tag PERSON. Similarly the pronoun ?It? can never 
take an animate as the antecedent. From the 
WordNet we obtain the information of noun 
category such as ?person?, ?object?, ?artifact?, 
?location? etc. Using the NE information provided 
in the document and the category information in 
WordNet, the irrelevant candidates are filtered out 
from the possible candidates. Thus the antecedent 
and pronoun category agrees. 
The highest ranked candidate is considered as 
the antecedent for the particular pronoun. 
In TC and BC genres, the pronouns ?I? and 
?you? refer to the speakers involved in the 
conversation. For these pronouns we identify the 
antecedent using heuristic rules making use of the 
speaker information provided. 
2.2 Non-pronominal Coreference resolution 
In identifying the Non-pronominal as said earlier, 
we have used a CRFs based machine learning 
approach. CRFs are well known for label 
sequencing tasks such as Chunking, Named Entity 
tagging (Lafferty et al 2001; Taku Kudo 2005). 
Here we have CRFs for classification task, by 
using only the current state features and not the 
features related to state transition. The features 
used for training are based on Soon et al(2001). 
We have changed the method of deriving, values 
of the features such as String match, alias, from the 
Soon el al method and found that our method is 
giving more result.  The features used in our work 
are as follows. 
a) Distance feature ? same as in Soon et al
b) Definite NP - same as in Soon et al
c) Demonstrative NP ? same as in Soon et al
d) String match ? (Not as Soon et althe possible 
values are between 0 and 1. This is calculated as 
ratio of the number of words matched between the 
NPs and the total number of words of the anaphor 
NP. Here we consider the NP on the left side as 
antecedent NP and NP on the right side as anaphor 
NP. 
e) Number Agreement ? We use the gender data 
file (Bergsma and Lin, 2006) and also the POS 
information 
f) Gender agreement ? We use the gender data 
file (Bergsma and Lin, 2006) 
g) Alias feature ? (Not as in Soon et al the alias 
feature takes the value 0 or 1. This is obtained 
using three methods, 
     i) Comparing the head of the NPs, if both are 
same then scored as 1 
     ii) If both the NPs start with NNP or NNPS 
POS tags, and if they are same then scored as 1 
     iii) Looks for Acronym match, if one is an 
acronym of other it is scored as 1 
h) Both proper NPs ? same as Soon et al  
i )  NE tag information. 
94
The semantic class information (noun category) 
obtained from the WordNet is used for the filtering 
purpose. The pairs which do not have semantic 
feature match are filtered out. We have not used 
the appositive feature described in Soon et al
(2001), since we are not considering appositives 
for the coreference chains.  
The feature template for CRF is defined in such 
a way that more importance is given to the features 
such as the string match, gender agreement and 
alias feature. The data for training is prepared by 
taking all NPs between an anaphor and antecedent 
as negative NPs and the antecedent and anaphor as 
positive NP. 
The core CRFs engine for Non-pronominal 
resolution system identifies the coreferring pairs of 
NPs. The Coreferring pairs obtained from 
pronominal resolution system and Non-pronominal 
system are merged to generate the complete 
coreference chains. The merging is done as 
follows: A member of a coreference pair is 
compared with all the members of the coreference 
pairs identified and if it occurs in anyone of the 
pair, then the two pairs are grouped.  This process 
is done for all the members of the identified pairs 
and the members in each group are aligned based 
on their position in the document to form the chain. 
3 Evaluation   
In this section we present the evaluation of the 
complete system, which was developed under the 
closed task, along with the independent evaluation 
of the two sub-modules. 
a) Non-anaphoric detection modules 
b) Pronominal resolution module 
The data used for training as well as testing was 
provided CoNLL-2001 shared task (Pradhan et al, 
2011), (Pradhan et al, 2007) organizers. The 
results shown in this paper were obtained for the 
development data. 
The non-anaphoric pronoun detection module is 
trained using the training data. This module was 
evaluated using the 91files development data. The 
training data contained 1326 non-anaphoric 
pronouns. The development data used for 
evaluation had 160 non-anaphoric pronouns. The 
table 2 shows the evaluation, of the non-anaphoric 
pronoun detection module. 
The Pronominal resolution module was also 
evaluated on the development data. The filtering of 
non-anaphoric pronouns helped in the increase in 
precision of the pronoun resolution module. The 
table 3 shows the evaluation of pronoun resolution 
module on the development data. Here we show 
the results without the non-anaphor detection and 
with non-anaphor detection. 
 
Type of 
pronoun 
Actual 
(gold 
standard
) 
System 
identified 
Correctly 
Accuracy 
(%) 
Anaphoric 
Pronouns 
939 908 96.6 
Non-
anaphoric 
pronouns 
160 81 50.6 
Total 1099 989 89.9 
   Table 2: Evaluation of Non-anaphoric pronoun 
 
System 
type 
Total 
Anap
horic 
Pron
ouns 
System 
identifi
ed 
pronou
ns 
System 
correctl
y 
Resolv
ed 
Pronou
ns 
Prec
isio
n 
(%) 
Without 
non-
anaphoric 
pronoun 
detection 
939 1099 693 63.1 
With non-
anaphoric 
pronoun 
detection 
939 987 693 70.2 
  Table 3: Evaluation of Pronominal resolution    
module 
 
The output of the Non-pronominal resolution 
module, merged with the output of the pronominal 
resolution module and it was evaluated using 
scorer program of the CoNLL-2011. The 
evaluation was done on the development data, 
shown in the table 4. 
On analysis of the output we found mainly three 
types of errors. They are 
 
a) Newly invented chains ? The system identifies 
new chains that are not found in the gold standard 
annotation. This reduces the precision of the 
95
system. This is because of the string match as one 
of the features. 
 
Metri
c 
Mention 
Detection 
Coreference 
Resolution 
Rec  Prec F1 Rec Prec F1 
MUC 68.1 61.5 64.6 52.1 49.9 50.9 
BCU
BED 
68.1 61.5 64.6 66.6 67.6 67.1 
CEA
FE 
68.1 61.5 64.6 42.8 44.9 43.8 
Avg 68.1 61.5 64.6 53.8 54.1 53.9 
Table 4: Evaluation of the Complete System 
 
b) Only head nouns in the chain ? We observed 
that system while selecting pair for identifying 
coreference, the pair has only the head noun 
instead of the full phrase. In the phrase ?the letters 
sent in recent days?, the system identifies ?the 
letters? instead of the whole phrase. This affects 
both the precision and recall of the system. 
c) Incorrect merging of chains ? The output 
chains obtained from the pronominal resolution 
system and the non-pronominal resolution system 
are merged to form a complete chain. When the 
antecedents in the pronominal chain are merged 
with the non-pronominal chains, certain chains are 
wrongly merged into single chain. For example 
?the chairman of the committee? is identified as 
coreferring with another similar phrase ?the 
chairman of executive board? by the non-
pronominal resolution task. Both of these are 
actually not referring to the same person. This 
happens because of string similarity feature of the 
non-pronominal resolution. This merging leads to 
building a wrong chain. Hence this affects the 
precision and recall of the system. 
4 Conclusion 
We have presented a coreference resolution system 
which combines the pronominal resolution using 
refined salience based approach with non-
pronominal resolution using CRFs, machine 
learning approach. In the pronominal resolution, 
initially we identify the non-anaphoric pronouns 
using CRFs based technique. This helps in 
improving the precision. In non-pronominal 
resolution algorithm, the string match feature is an 
effective feature in identifying coreference. But, 
this feature is found to introduce errors. We need 
to add additional contextual and semantic feature 
to reduce above said errors.  The results on the 
development set are encouraging.  
References  
Shane Bergsma, and Dekang Lin. 2006. Bootstrapping 
Path-Based Pronoun Resolution. In Proceedings of 
the Conference on Computational Lingustics / 
Association for Computational Linguistics 
(COLING/ACL-06), Sydney, Australia, July 17-21, 
2006. 
John Lafferty, Andrew McCallum, Fernando Pereira.   
2001. Conditional Random Fields: Probabilistic  
Models for Segmenting and Labeling Sequence Data.   
In Proceedings of the Eighteenth International   
Conference on Machine Learning (ICML-2001).  
282-289. 
S. Lappin and H. Leass. 1994. An Algorithm for 
Pronominal Anaphora Resolution. Computational 
Linguistics, 20(4):535?562, 1994. 
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, 
Martha Palmer, Ralph Weischedel, Nianwen Xue. 
2011. CoNLL-2011 Shared Task: Modeling 
Unrestricted Coreference in OntoNotes. In 
Proceedings of the Fifteenth Conference on 
Computational Natural Language Learning (CoNLL 
2011). 
Sameer Pradhan and Lance Ramshaw and Ralph 
Weischedel and Jessica MacBride and Linnea 
Micciulla. 2007. Unrestricted Coreference: 
Identifying Entities and Events in OntoNotes. In 
Proceedings of the IEEE International Conference on 
Semantic Computing (ICSC)". Irvine, CA, 
September 17-19, 2007.  
Sobha, L. 2008. Anaphora Resolution Using Named 
Entity and Ontology. In Proceedings of the Second 
Workshop on Anaphora Resolution (WAR II), Ed 
Christer Johansson, NEALT Proceedings Series, Vol. 
2 (2008) Estonia. 91-96. 
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A 
Machine Learning Approach to Coreference 
Resolution of Noun Phrases. Computational 
Linguistics, 27(4):521?544. 
Taku Kudo. 2005. CRF++, an open source toolkit for   
CRF, http://crfpp.sourceforge.net . 
96
