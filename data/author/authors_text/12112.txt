Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 843?851,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-Supervised Cause Identification from Aviation Safety Reports
Isaac Persing and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,vince}@hlt.utdallas.edu
Abstract
We introduce cause identification, a new
problem involving classification of in-
cident reports in the aviation domain.
Specifically, given a set of pre-defined
causes, a cause identification system seeks
to identify all and only those causes that
can explain why the aviation incident de-
scribed in a given report occurred. The dif-
ficulty of cause identification stems in part
from the fact that it is a multi-class, multi-
label categorization task, and in part from
the skewness of the class distributions and
the scarcity of annotated reports. To im-
prove the performance of a cause identi-
fication system for the minority classes,
we present a bootstrapping algorithm that
automatically augments a training set by
learning from a small amount of labeled
data and a large amount of unlabeled data.
Experimental results show that our algo-
rithm yields a relative error reduction of
6.3% in F-measure for the minority classes
in comparison to a baseline that learns
solely from the labeled data.
1 Introduction
Automatic text classification is one of the most im-
portant applications in natural language process-
ing (NLP). The difficulty of a text classification
task depends on various factors, but typically, the
task can be difficult if (1) the amount of labeled
data available for learning the task is small; (2)
it involves multiple classes; (3) it involves multi-
label categorization, where more than one label
can be assigned to each document; (4) the class
distributions are skewed, with some categories
significantly outnumbering the others; and (5) the
documents belong to the same domain (e.g., movie
review classification). In particular, when the doc-
uments to be classified are from the same domain,
they tend to be more similar to each other with
respect to word usage, thus making the classes
less easily separable. This is one of the reasons
why topic-based classification, even with multiple
classes as in the 20 Newsgroups dataset1, tends to
be easier than review classification, where reviews
from the same domain are to be classified accord-
ing to the sentiment expressed2 .
In this paper, we introduce a new text classifi-
cation problem involving the Aviation Safety Re-
porting System (ASRS) that can be viewed as a
difficult task along each of the five dimensions dis-
cussed above. Established in 1967, ASRS collects
voluntarily submitted reports about aviation safety
incidents written by flight crews, attendants, con-
trollers, and other related parties. These incident
reports are made publicly available to researchers
for automatic analysis, with the ultimate goal of
improving the aviation safety situation. One cen-
tral task in the automatic analysis of these reports
is cause identification, or the identification of why
an incident happened. Aviation safety experts at
NASA have identified 14 causes (or shaping fac-
tors in NASA terminology) that could explain why
an incident occurred. Hence, cause identification
can be naturally recast as a text classification task:
given an incident report, determine which of a set
of 14 shapers contributed to the occurrence of the
incident described in the report.
As mentioned above, cause identification is
considered challenging along each of the five
aforementioned dimensions. First, there is a
scarcity of incident reports labeled with the
shapers. This can be attributed to the fact that
there has been very little work on this task. While
the NASA researchers have applied a heuristic
method for labeling a report with shapers (Posse
1http://kdd.ics.uci.edu/databases/20newsgroups/
2Of course, the fact that sentiment classification requires
a deeper understanding of a text also makes it more difficult
than topic-based text classification (Pang et al, 2002).
843
et al, 2005), the method was evaluated on only
20 manually labeled reports, which are not made
publicly available. Second, the fact that this is
a 14-class classification problem makes it more
challenging than a binary classification problem.
Third, a report can be labeled with more than one
category, as several shapers can contribute to the
occurrence of an aviation incident. Fourth, the
class distribution is very skewed: based on an
analysis of our 1,333 annotated reports, 10 of the
14 categories can be considered minority classes,
which account for only 26% of the total num-
ber of labels associated with the reports. Finally,
our cause identification task is domain-specific,
involving the classification of documents that all
belong to the aviation domain.
This paper focuses on improving the accuracy
of minority class prediction for cause identifica-
tion. Not surprisingly, when trained on a dataset
with a skewed class distribution, most supervised
machine learning algorithms will exhibit good per-
formance on the majority classes, but relatively
poor performance on the minority classes. Unfor-
tunately, achieving good accuracies on the minor-
ity classes is very important in our task of identify-
ing shapers from aviation safety reports, where 10
out of the 14 shapers are minority classes, as men-
tioned above. Minority class prediction has been
tackled extensively in the machine learning liter-
ature, using methods that typically involve sam-
pling and re-weighting of training instances, with
the goal of creating a less skewed class distribution
(e.g., Pazzani et al (1994), Fawcett (1996), Ku-
bat and Matwin (1997)). Such methods, however,
are unlikely to perform equally well for our cause
identification task given our small labeled set, as
the minority class prediction problem is compli-
cated by the scarcity of labeled data. More specif-
ically, given the scarcity of labeled data, many
words that are potentially correlated with a shaper
(especially a minority shaper) may not appear in
the training set, and the lack of such useful indi-
cators could hamper the acquisition of an accurate
classifier via supervised learning techniques.
We propose to address the problem of minority
class prediction in the presence of a small training
set by means of a bootstrapping approach, where
we introduce an iterative algorithm to (1) use a
small set of labeled reports and a large set of unla-
beled reports to automatically identify words that
are most relevant to the minority shaper under con-
sideration, and (2) augment the labeled data by us-
ing the resulting words to annotate those unlabeled
reports that can be confidently labeled. We evalu-
ate our approach using cross-validation on 1,333
manually annotated reports. In comparison to a
supervised baseline approach where a classifier is
acquired solely based on the training set, our boot-
strapping approach yields a relative error reduc-
tion of 6.3% in F-measure for the minority classes.
In sum, the contributions of our work are three-
fold. First, we introduce a new, challenging
text classification problem, cause identification
from aviation safety reports, to the NLP commu-
nity. Second, we created an annotated dataset for
cause identification that is made publicly available
for stimulating further research on this problem3.
Third, we introduce a bootstrapping algorithm for
improving the prediction of minority classes in the
presence of a small training set.
The rest of the paper is organized as follows. In
Section 2, we present the 14 shapers. Section 3 ex-
plains how we preprocess and annotate the reports.
Sections 4 and 5 describe the baseline approaches
and our bootstrapping algorithm, respectively. We
present results in Section 6, discuss related work
in Section 7, and conclude in Section 8.
2 Shaping Factors
As mentioned in the introduction, the task of cause
identification involves labeling an incident report
with all the shaping factors that contributed to the
occurrence of the incident. Table 1 lists the 14
shaping factors, as well as a description of each
shaper taken verbatim from Posse et al (2005).
As we can see, the 14 classes are not mutually ex-
clusive. For instance, a lack of familiarity with
equipment often implies a deficit in proficiency in
its use, so the two shapers frequently co-occur. In
addition, while some classes cover a specific and
well-defined set of issues (e.g., Illusion), some en-
compass a relatively large range of situations. For
instance, resource deficiency can include prob-
lems with equipment, charts, or even aviation per-
sonnel. Furthermore, ten shaping factors can be
considered minority classes, as each of them ac-
count for less than 10% of the labels. Accurately
predicting minority classes is important in this do-
main because, for example, the physical factors
minority shaper is frequently associated with in-
cidents involving near-misses between aircraft.
3http://www.hlt.utdallas.edu/?persingq/ASRSdataset.html
844
Id Shaping Factor Description %
1 Attitude Any indication of unprofessional or antagonistic attitude by a controller or flight crew mem-
ber, e.g., complacency or get-homeitis (in a hurry to get home).
2.4
2 Communication
Environment
Interferences with communications in the cockpit such as noise, auditory interference, radio
frequency congestion, or language barrier.
5.5
3 Duty Cycle A strong indication of an unusual working period, e.g., a long day, flying very late at night,
exceeding duty time regulations, having short and inadequate rest periods.
1.8
4 Familiarity A lack of factual knowledge, such as new to or unfamiliar with company, airport, or aircraft. 3.2
5 Illusion Bright lights that cause something to blend in, black hole, white out, sloping terrain, etc. 0.1
6 Other Anything else that could be a shaper, such as shift change, passenger discomfort, or disori-
entation.
13.3
7 Physical
Environment
Unusual physical conditions that could impair flying or make things difficult. 16.0
8 Physical
Factors
Pilot ailment that could impair flying or make things more difficult, such as being tired,
drugged, incapacitated, suffering from vertigo, illness, dizziness, hypoxia, nausea, loss of
sight or hearing.
2.2
9 Preoccupation A preoccupation, distraction, or division of attention that creates a deficit in performance,
such as being preoccupied, busy (doing something else), or distracted.
6.7
10 Pressure Psychological pressure, such as feeling intimidated, pressured, or being low on fuel. 1.8
11 Proficiency A general deficit in capabilities, such as inexperience, lack of training, not qualified, or not
current.
14.4
12 Resource
Deficiency
Absence, insufficient number, or poor quality of a resource, such as overworked or unavail-
able controller, insufficient or out-of-date chart, malfunctioning or inoperative or missing
equipment.
30.0
13 Taskload Indicators of a heavy workload or many tasks at once, such as short-handed crew. 1.9
14 Unexpected Something sudden and surprising that is not expected. 0.6
Table 1: Descriptions of shaping factor classes. The ?%? column shows the percent of labels the shapers account for.
3 Dataset
We downloaded our corpus from the ASRS web-
site4. The corpus consists of 140,599 incident
reports collected during the period from January
1998 to December 2007. Each report is a free
text narrative that describes not only why an in-
cident happened, but also what happened, where it
happened, how the reporter felt about the incident,
the reporter?s opinions of other people involved in
the incident, and any other comments the reporter
cared to include. In other words, a lot of informa-
tion in the report is irrelevant to (and thus compli-
cates) the task of cause identification.
3.1 Preprocessing
Unlike newswire articles, at which many topic-
based text classification tasks are targeted, the
ASRS reports are informally written using various
domain-specific abbreviations and acronyms, tend
to contain poor grammar, and have capitalization
information removed, as illustrated in the follow-
ing sentence taken from one of the reports.
HAD BEEN CLRED FOR APCH BY
ZOA AND HAD BEEN HANDED OFF
TO SANTA ROSA TWR.
4http://asrs.arc.nasa.gov/
This sentence is grammatically incorrect (due to
the lack of a subject), and contains abbrevia-
tions such as CLRED, APCH, and TWR. This
makes it difficult for a non-aviation expert to un-
derstand. To improve readability (and hence fa-
cilitate the annotation process), we preprocess
each report as follows. First, we expand the ab-
breviations/acronyms with the help of an official
list of acronyms/abbreviations and their expanded
forms5. Second, though not as crucial as the first
step, we heuristically restore the case of the words
by relying on an English lexicon: if a word ap-
pears in the lexicon, we assume that it is not a
proper name, and therefore convert it into lower-
case. After preprocessing, the example sentence
appears as
had been cleared for approach by ZOA
and had been handed off to santa rosa
tower.
Finally, to facilitate automatic analysis, we stem
each word in the narratives.
3.2 Human Annotation
Next, we randomly picked 1,333 preprocessed re-
ports and had two graduate students not affiliated
5See http://akama.arc.nasa.gov/ASRSDBOnline/pdf/
ASRS Decode.pdf. In the very infrequently-occurring case
where the same abbreviation or acronym may have more
than expansion, we arbitrarily chose one of the possibilities.
845
Id Total (%) F1 F2 F3 F4 F5
1 52 (3.9) 11 7 7 17 10
2 119 (8.9) 29 29 22 16 23
3 38 (2.9) 10 5 6 9 8
4 70 (5.3) 11 12 9 14 24
5 3 (0.2) 0 0 0 1 2
6 289 (21.7) 76 44 60 42 67
7 348 (26.1) 73 63 82 59 71
8 48 (3.6) 11 14 8 11 4
9 145 (10.9) 29 25 38 28 25
10 38 (2.9) 12 10 4 7 5
11 313 (23.5) 65 50 74 46 78
12 652 (48.9) 149 144 125 123 111
13 42 (3.2) 7 8 8 6 13
14 14 (1.1) 3 3 3 3 2
Table 2: Number of occurrences of each shaping
factor in the dataset. The ?Total? column shows the num-
ber of narratives labeled with each shaper and the percentage
of narratives tagged with each shaper in the 1,333 labeled
narrative set. The ?F? columns show the number narratives
associated with each shaper in folds F1 ? F5.
x (# Shapers) 1 2 3 4 5 6
Percentage 53.6 33.2 10.3 2.7 0.2 0.1
Table 3: Percentage of documents with x labels.
with this research independently annotate them
with shaping factors, based solely on the defi-
nitions presented in Table 1. To measure inter-
annotator agreement, we compute Cohen?s Kappa
(Carletta, 1996) from the two sets of annotations,
obtaining a Kappa value of only 0.43. This not
only suggests the difficulty of the cause identifica-
tion task, but also reveals the vagueness inherent
in the definition of the 14 shapers. As a result,
we had the two annotators re-examine each report
for which there was a disagreement and reach an
agreement on its final set of labels. Statistics of the
annotated dataset can be found in Table 2, where
the ?Total? column shows the size of each of the
14 classes, expressed both as the number of re-
ports that are labeled with a particular shaper and
as a percent (in parenthesis). Since we will per-
form 5-fold cross validation in our experiments,
we also show the number of reports labeled with
each shaper under the ?F? columns for each fold.
To get a better idea of how many reports have mul-
tiple labels, we categorize the reports according to
the number of labels they contain in Table 3.
4 Baseline Approaches
In this section, we describe two baseline ap-
proaches to cause identification. Since our ulti-
mate goal is to evaluate the effectiveness of our
bootstrapping algorithm, the baseline approaches
only make use of small amounts of labeled data for
acquiring classifiers. More specifically, both base-
lines recast the cause identification problem as a
set of 14 binary classification problems, one for
predicting each shaper. In the binary classification
problem for predicting shaper si, we create one
training instance from each document in the train-
ing set, labeling the instance as positive if the doc-
ument has si as one of its labels, and negative oth-
erwise. After creating training instances, we train
a binary classifier, ci, for predicting si, employing
as features the top 50 unigrams that are selected
according to information gain computed over the
training data (see Yang and Pedersen (1997)). The
SVM learning algorithm as implemented in the
LIBSVM software package (Chang and Lin, 2001)
is used for classifier training, owing to its robust
performance on many text classification tasks.
In our first baseline, we set al the learning pa-
rameters to their default values. As noted before,
we divide the 1,333 annotated reports into five
folds of roughly equal size, training the classifiers
on four folds and applying them separately to the
remaining fold. Results are reported in terms of
precision (P), recall (R), and F-measure (F), which
are computed by aggregating over the 14 shapers
as follows. Let tpi be the number of test reports
correctly labeled as positive by ci; pi be the total
number of test reports labeled as positive by ci;
and ni be the total number of test reports that be-
long to si according to the gold standard. Then,
P =
?
i tpi
?
i pi
,R =
?
i tpi
?
i ni
, and F = 2PRP + R.
Our second baseline is similar to the first, ex-
cept that we tune the classification threshold (CT)
to optimize F-measure. More specifically, recall
that LIBSVM trains a classifier that by default em-
ploys a CT of 0.5, thus classifying an instance as
positive if and only if the probability that it be-
longs to the positive class is at least 0.5. How-
ever, this may not be the optimal threshold to use
as far as performance is concerned, especially for
the minority classes, where the class distribution
is skewed. This is the motivation behind tuning
the CT of each classifier. To ensure a fair compar-
ison with the first baseline, we do not employ ad-
ditional labeled data for parameter tuning; rather,
we reserve 25% of the available training data for
tuning, and use the remaining 75% for classifier
846
acquisition. This amounts to using three folds
for training and one fold for development in each
cross validation experiment. Using the develop-
ment data, we tune the 14 CTs jointly to optimize
overall F-measure. However, an exact solution to
this optimization problem is computationally ex-
pensive. Consequently, we find a local maximum
by employing a local search algorithm, which al-
ters one parameter at a time to optimize F-measure
by holding the remaining parameters fixed.
5 Our Bootstrapping Algorithm
One of the potential weaknesses of the two base-
lines described in the previous section is that the
classifiers are trained on only a small amount of
labeled data. This could have an adverse effect
on the accuracy of the resulting classifiers, espe-
cially those for the minority classes. The situation
is somewhat aggravated by the fact that we are
adopting a one-versus-all scheme for generating
training instances for a particular shaper, which,
together with the small amount of labeled data, im-
plies that only a couple of positive instances may
be available for training the classifier for a minor-
ity class. To alleviate the data scarcity problem
and improve the accuracy of the classifiers, we
propose in this section a bootstrapping algorithm
that automatically augments a training set by ex-
ploiting a large amount of unlabeled data. The ba-
sic idea behind the algorithm is to iteratively iden-
tify words that are high-quality indicators of the
positive or negative examples, and then automati-
cally label unlabeled documents that contain a suf-
ficient number of such indicators.
Our bootstrapping algorithm, shown in Figure
1, aims to augment the set of positive and neg-
ative training instances for a given shaper. The
main function, Train, takes as input four argu-
ments. The first two arguments, P and N , are the
positive and negative instances, respectively, gen-
erated by the one-versus-one scheme from the ini-
tial training set, as described in the previous sec-
tion. The third argument, U , is the unlabeled set
of documents, which consists of all but the doc-
uments in the training set. In particular, U con-
tains the documents in the development and test
sets. Hence, we are essentially assuming access
to the test documents (but not their labels) dur-
ing the training process, as in a transductive learn-
ing setting. The last argument, k, is the number
of bootstrapping iterations. In addition, the algo-
Train(P,N, U, k)
Inputs:
P : positively labeled training examples of shaper x
N : negatively labeled training examples of shaper x
U : set of unlabeled narratives in corpus
k: number of bootstrapping iterations
PW ? ?
NW ? ?
for i = 0 to k ? 1 do
if |P | > |N | then
[P, PW ]? ExpandTrainingSet(P,N, U, PW )
else
[N, NW ]?ExpandTrainingSet(N,P, U, NW )
end if
end for
ExpandTrainingSet(A,B, U, W )
Inputs:
A, B, U : narrative sets
W : unigram feature set
for j = 1 to 4 do
t? arg maxt/?W
(
log( C(t,A)C(t,B)+1 )
)
// C(t, X): number of narratives in X containing t
W ?W ? {t}
end for
return [A ? S(W, U), W ]
// S(W,U): narratives in U containing ? 3 words in W
Figure 1: Our bootstrapping algorithm.
rithm uses two variables, PW and NW , to store
the sets of high-quality indicators for the positive
instances and the negative instances, respectively,
that are found during the bootstrapping process.
Next, we begin our k bootstrapping iterations.
In each iteration, we expand either P or N , de-
pending on their relative sizes. In order to keep
the two sets as close in size as possible, we choose
to expand the smaller of the two sets.6 After that,
we execute the function ExpandTrainingSet to ex-
pand the selected set. Without loss of general-
ity, assume that P is chosen for expansion. To
do this, ExpandTrainingSet selects four words that
seem much more likely to appear in P than in
N from the set of candidate words7. To select
these words, we calculate the log likelihood ratio
log( C(t,P )C(t,N)+1 ) for each candidate word t, where
C(t, P ) is the number of narratives in P that con-
tain t, and C(t,N) similarly is the number of nar-
ratives in N that contain t. If this ratio is large,
6It may seem from the way P and N are constructed that
N is almost always larger than P and therefore is unlikely to
be selected for expansion. However, the ample size of the un-
labeled set means that the algorithm still adds large numbers
of narratives to the training data. Hence, even for minority
classes, P often grows larger than N by iteration 3.
7A candidate word is a word that appears in the training
set (P ?N ) at least four times.
847
we posit that t is a good indicator of P . Note that
incrementing the count in the denominator by one
has a smoothing effect: it avoids selecting words
that appears infrequently in P and not at all in N .
There is a reason for selecting multiple words
(rather than just one word) in each bootstrap-
ping iteration: we want to prevent the algorithm
from selecting words that are too specific to one
subcategory of a shaping factor. For example,
shaping factor 7 (Physical Environment) is com-
posed largely of incidents influenced by weather
phenomena. In one experiment, we tried select-
ing only one word per bootstrapping iteration.
For shaper 7, the first word added to PW was
?snow?. Upon the next iteration, the algorithm
added ?plow? to PW. While ?plow? may itself be
indicative of shaper 7, we believe its selection was
due to the recent addition to P of a large number of
narratives containing ?snow?. Hence, by selecting
four words per iteration, we are forcing the algo-
rithm to ?branch out? among these subcategories.
After adding the selected words to PW , we
augment P with all the unlabeled documents con-
taining at least three words from PW . The rea-
son we impose the ?at least three? requirement
is precision: we want to ensure, with a reason-
able level of confidence, that the unlabeled doc-
uments chosen to augment P should indeed be
labeled with the shaper under consideration, as
incorrectly labeled documents would contaminate
the labeled data, thus accelerating the deterioration
of the quality of the automatically labeled data in
subsequent bootstrapping iterations and adversely
affecting the accuracy of the classifier trained on it
(Pierce and Cardie, 2001).
The above procedure is repeated in each boot-
strapping iteration. As mentioned above, if N
is smaller in size than P , we will expand N in-
stead, adding to NW the four words that are the
strongest indicators of a narrative being a negative
example of the shaper under consideration, and
augmenting N with those unlabeled narratives that
contain at least three words from NW .
The number of bootstrapping iterations is con-
trolled by the input parameter k. As we will see
in the next section, we run the bootstrapping algo-
rithm for up to five iterations only, as the quality
of the bootstrapped data deteriorates fairly rapidly.
The exact value of k will be determined automati-
cally using development data, as discussed below.
After bootstrapping, the augmented training
data can be used in combination with any of the
two baseline approaches to acquire a classifier for
identifying a particular shaper. Whichever base-
line is used, we need to reserve one of the five
folds to tune the parameter k in our cross vali-
dation experiments. In particular, if the second
baseline is used, we will tune CT and k jointly
on the development data using the local search al-
gorithm described previously, where we adjust the
values of both CT and k for one of the 14 classi-
fiers in each step of the search process to optimize
the overall F-measure score.
6 Evaluation
6.1 Baseline Systems
Since our evaluation centers on the question of
how effective our bootstrapping algorithm is in ex-
ploiting unlabeled documents to improve classifier
performance, our two baselines only employ the
available labeled documents to train the classifiers.
Recall that our first baseline, which we call
B0.5 (due to its being a baseline with a CT of
0.5), employs default values for all of the learn-
ing parameters. Micro-averaged 5-fold cross val-
idation results of this baseline for all 14 shapers
and for just 10 minority classes (due to our focus
on improving minority class prediction) are ex-
pressed as percentages in terms of precision (P),
recall (R), and F-measure (F) in the first row of
Table 4. As we can see, the baseline achieves
an F-measure of 45.4 (14 shapers) and 35.4 (10
shapers). Comparing these two results, the higher
F-measure achieved using all 14 shapers can be at-
tributed primarily to improvements in recall. This
should not be surprising: as mentioned above, the
number of positive instances of a minority class
may be small, thus causing the resulting classi-
fier to be biased towards classifying a document
as negative.
Instead of employing a CT value of 0.5, our
second baseline, Bct, tunes CT using one of the
training folds and simply trains a classifier on the
remaining three folds. For parameter tuning, we
tested CTs of 0.0, 0.05, . . ., 1.0. Results of this
baseline are shown in row 2 of Table 4. In com-
parison to the first baseline, we see that F-measure
improves considerably by 7.4% and 4.5% for 14
shapers and 10 shapers respectively8 , which illus-
8It is important to note that the parameters are optimized
separately for each pair of 14-shaper and 10-shaper exper-
iments in this paper, and that the 10-shaper results are not
848
All 14 Classes 10 Minority Classes
System P R F P R F
B0.5 67.0 34.4 45.4 68.3 23.9 35.4
Bct 47.4 59.2 52.7 47.8 34.3 39.9
E0.5 60.9 40.4 48.6 53.2 35.3 42.4
Ect 50.5 54.9 52.6 49.1 39.4 43.7
Table 4: 5-fold cross validation results.
trates the importance of employing the right CT
for the cause identification task.
6.2 Our Approach
Next, we evaluate the effectiveness of our boot-
strapping algorithm in improving classifier per-
formance. More specifically, we apply the two
baselines separately to the augmented training set
produced by our bootstrapping algorithm. When
combining our bootstrapping algorithm with the
first baseline, we produce a system that we call
E0.5 (due to its being trained on the expanded
training set with a CT of 0.5). E0.5 has only one
tunable parameter, k (i.e., the number of boot-
strapping iterations), whose allowable values are
0, 1, . . ., 5. When our algorithm is used in com-
bination with the second baseline, we produce an-
other system, Ect, which has both k and the CT
as its parameters. The allowable values of these
parameters, which are to be tuned jointly, are the
same as those employed by Bct and E0.5.
Results of E0.5 are shown in row 3 of Table
4. In comparison to B0.5, we see that F-measure
increases by 3.2% and 7.0% for 14 shapers and
10 shapers, respectively. Such increases can be
attributed to less imbalanced recall and precision
values, as a result of a large gain in recall accom-
panied by a roughly equal drop in precision. These
results are consistent with our intuition: recall can
be improved with a larger training set, but preci-
sion can be hampered when learning from nois-
ily labeled data. Overall, these results suggest that
learning from the augmented training set is useful,
especially for the minority classes.
Results of Ect are shown in row 4 of Table 4.
In comparison to Bct, we see mixed results: F-
measure increases by 3.8% for 10 shapers (which
represents a relative error reduction of 6.3%, but
drops by 0.1% for 14 shapers. Overall, these re-
sults suggest that when the CT is tunable, train-
ing set expansion helps the minority classes but
hurts the remaining classes. A closer look at the
results reveals that the 0.1% F-measure drop is due
simply extracted from the 14-shaper experiments.
to a large drop in recall accompanied by a smaller
gain in precision. In other words, for the four
non-minority classes, the benefits obtained from
using the bootstrapped documents can also be ob-
tained by simply adjusting the CT. This could be
attributed to the fact that a decent classifier can be
trained using only the hand-labeled training exam-
ples for these four shapers, and as a result, the au-
tomatically labeled examples either provide very
little new knowledge or are too noisy to be useful.
On the other hand, for the 10 minority classes, the
3.8% gain in F-measure can be attributed to a si-
multaneous rise in recall and precision. Note that
such gain cannot possibly be obtained by simply
adjusting the CT, since adjusting the CT always
results in higher recall and lower precision or vice
versa. Overall, the simultaneous rise in recall and
precision implies that the bootstrapped documents
have provided useful knowledge, particularly in
the form of positive examples, for the classifiers.
Even though the bootstrapped documents are nois-
ily labeled, they can still be used to improve the
classifiers, as the set of initially labeled positive
examples for the minority classes is too small.
6.3 Additional Analyses
Quality of the bootstrapped data. Since the
bootstrapped documents are noisily labeled, a nat-
ural question is: How noisy are they? To get a
sense of the accuracy of the bootstrapped docu-
ments without further manual labeling, recall that
our experimental setup resembles a transductive
setting where the test documents are part of the
unlabeled data, and consequently, some of them
may have been automatically labeled by the boot-
strapping algorithm. In fact, 137 documents in the
five test folds were automatically labeled in the
14-shaper Ect experiments, and 69 automatically
labeled documents were similarity obtained from
the 10-shaper Ect experiments. For 14 shapers, the
accuracies of the positively and negatively labeled
documents are 74.6% and 97.1%, respectively,
and the corresponding numbers for 10 shapers are
43.2% and 81.3%. These numbers suggest that
negative examples can be acquired with high ac-
curacies, but the same is not true for positive ex-
amples. Nevertheless, learning the 10 shapers
from the not-so-accurately-labeled positive exam-
ples still allows us to outperform the correspond-
ing baseline.
849
Shaping Factor Positive Expanders Negative Expanders
Familiarity unfamiliar, layout, unfamilarity, rely
Physical Environment cloud, snow, ice, wind
Physical Factors fatigue, tire, night, rest, hotel, awake, sleep, sick declare, emergency, advisory, separation
Preoccupation distract, preoccupied, awareness, situational,
task, interrupt, focus, eye, configure, sleep
declare, ice snow, crash, fire, rescue, anti,
smoke
Pressure bad, decision, extend, fuel, calculate, reserve,
diversion, alternate
Table 5: Example positive and negative expansion words collected by Ect for selected shaping factors.
Analysis of the expanders. To get an idea of
whether the words acquired during the bootstrap-
ping process (henceforth expanders) make intu-
itive sense, we show in Table 5 example positive
and negative expanders obtained for five shaping
factors from the Ect experiments. As we can see,
many of the positive expanders are intuitively ob-
vious. We might, however, wonder about the con-
nection between, for example, the shaper Famil-
iarity and the word ?rely?, or between the shaper
Pressure and the word ?extend?. We suspect that
the bootstrapping algorithm is likely to make poor
word selections particularly in the cases of the mi-
nority classes, where the positively labeled train-
ing data used to select expansion words is more
sparse. As suggested earlier, poor word choice
early in the algorithm is likely to cause even poorer
word choice later on.
On the other hand, while none of the negative
expanders seem directly meaningful in relation to
the shaper for which they were selected, some of
them do appear to be related to other phenomena
that may be negatively correlated with the shaper.
For instance, the words ?snow? and ?ice? were
selected as negative expanders for Preoccupation
and also as positive expanders for Physical Envi-
ronment. While these two shapers are only slightly
negatively correlated, it is possible that Preoccu-
pation may be strongly negatively correlated with
the subset of Physical Environment incidents in-
volving cold weather.
7 Related Work
Since we recast cause identification as a text clas-
sification task and proposed a bootstrapping ap-
proach that targets at improving minority class
prediction, the work most related to ours involves
one or both of these topics.
Guzma?n-Cabrera et al (2007) address the
problem of class skewness in text classification.
Specifically, they first under-sample the majority
classes, and then bootstrap the classifier trained
on the under-sampled data using unlabeled doc-
uments collected from the Web.
Minority classes can be expanded without the
availability of unlabeled data as well. For ex-
ample, Chawla et al (2002) describe a method
by which synthetic training examples of minor-
ity classes can be generated from other labeled
training examples to address the problem of im-
balanced data in a variety of domains.
Nigam et al (2000) propose an iterative semi-
supervised method that employs the EM algorithm
in combination with the naive Bayes generative
model to combine a small set of labeled docu-
ments and a large set of unlabeled documents. Mc-
Callum and Nigam (1999) suggest that the ini-
tial labeled examples can be obtained using a list
of keywords rather than through annotated data,
yielding an unsupervised algorithm.
Similar bootstrapping methods are applicable
outside text classification as well. One of the
most notable examples is Yarowsky?s (1995) boot-
strapping algorithm for word sense disambigua-
tion. Beginning with a list of unlabeled contexts
surrounding a word to be disambiguated and a list
of seed words for each possible sense, the algo-
rithm iteratively uses the seeds to label a training
set from the unlabeled contexts, and then uses the
training set to identify more seed words.
8 Conclusions
We have introduced a new problem, cause identi-
fication from aviation safety reports, to the NLP
community. We recast it as a multi-class, multi-
label text classification task, and presented a boot-
strapping algorithm for improving the prediction
of minority classes in the presence of a small train-
ing set. Experimental results show that our algo-
rithm yields a relative error reduction of 6.3% in
F-measure over a purely supervised baseline when
applied to the minority classes. By making our
annotated dataset publicly available, we hope to
stimulate research in this challenging problem.
850
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the
paper. We are indebted to Muhammad Arshad
Ul Abedin, who provided us with a preprocessed
version of the ASRS corpus and, together with
Marzia Murshed, annotated the 1,333 documents.
This work was supported in part by NASA Grant
NNX08AC35A and NSF Grant IIS-0812261.
References
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The Kappa statistic. Computational
Linguistics, 22(2):249?254.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: A library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. SMOTE:
Synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 16:321?357.
Tom Fawcett. 1996. Learning with skewed class distri-
butions ? summary of responses. Machine Learn-
ing List: Vol. 8, No. 20.
Rafael Guzma?n-Cabrera, Manuel Montes-y-Go?mez,
Paolo Rosso, and Luis Villasen?or Pineda. 2007.
Taking advantage of the Web for text classification
with imbalanced classes. In Proceedings of MICAI,
pages 831?838.
Miroslav Kubat and Stan Matwin. 1997. Addressing
the curse of imbalanced training sets: One-sided se-
lection. In Proceedings of ICML, pages 179?186.
Andrew McCallum and Kamal Nigam. 1999. Text
classification by bootstrapping with keywords, EM
and shrinkage. In Proceedings of the ACL Work-
shop for Unsupervised Learning in Natural Lan-
guage Processing, pages 52?58.
Kamal Nigam, Andrew McCallum, Sebastian Thrun,
and Tom Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Ma-
chine Learning, 39(2/3):103?134.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
Michael Pazzani, Christopher Merz, Patrick Murphy,
Kamal Ali, Timothy Hume, and Clifford Brunk.
1994. Reducing misclassification costs. In Proceed-
ings of ICML, pages 217?225.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of EMNLP, pages 1?9.
Christian Posse, Brett Matzke, Catherine Anderson,
Alan Brothers, Melissa Matzke, and Thomas Ferry-
man. 2005. Extracting information from narratives:
An application to aviation safety reports. In Pro-
ceedings of the Aerospace Conference 2005, pages
3678?3690.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of ICML, pages 412?420.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the ACL, pages 189?196.
851
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229?239,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Modeling Organization in Student Essays
Isaac Persing and Alan Davis and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,alan,vince}@hlt.utdallas.edu
Abstract
Automated essay scoring is one of the most
important educational applications of natural
language processing. Recently, researchers
have begun exploring methods of scoring es-
says with respect to particular dimensions of
quality such as coherence, technical errors,
and relevance to prompt, but there is rela-
tively little work on modeling organization.
We present a new annotated corpus and pro-
pose heuristic-based and learning-based ap-
proaches to scoring essays along the organi-
zation dimension, utilizing techniques that in-
volve sequence alignment, alignment kernels,
and string kernels.
1 Introduction
Automated essay scoring, the task of employing
computer technology to evaluate and score writ-
ten text, is one of the most important educational
applications of natural language processing (NLP)
(see Shermis and Burstein (2003) and Shermis et al
(2010) for an overview of the state of the art in this
task). Recent years have seen a surge of interest in
this and other educational applications in the NLP
community, as evidenced by the panel discussion
on ?Emerging Application Areas in Computational
Linguistics? at NAACL 2009, as well as increased
participation in the series of workshops on ?Innova-
tive Use of NLP for Building Educational Applica-
tions?. Besides its potential commercial value, au-
tomated essay scoring brings about a number of rel-
atively less-studied but arguably rather challenging
discourse-level problems that involve the computa-
tional modeling of different facets of text structure,
such as content, coherence, and organization.
A major weakness of many existing essay scor-
ing engines such as IntelliMetric (Elliot, 2001) and
Intelligent Essay Assessor (Landauer et al, 2003)
is that they adopt a holistic scoring scheme, which
summarizes the quality of an essay with a single
score and thus provides very limited feedback to
the writer. In particular, it is not clear which di-
mension of an essay (e.g., coherence, relevance)
a score should be attributed to. Recent work ad-
dresses this problem by scoring a particular dimen-
sion of essay quality such as coherence (Miltsakaki
and Kukich, 2004), technical errors, and relevance
to prompt (Higgins et al, 2004). Automated sys-
tems that provide instructional feedback along mul-
tiple dimensions of essay quality such as Criterion
(Burstein et al, 2004) have also begun to emerge.
Nevertheless, there is an essay scoring dimension
for which few computational models have been de-
veloped ? organization. Organization refers to the
structure of an essay. A high score on organization
means that writers introduce a topic, state their po-
sition on that topic, support their position, and con-
clude, often by restating their position (Silva, 1993).
A well-organized essay is structured in a way that
logically develops an argument. Note that organi-
zation is a different facet of text structure than co-
herence, which is concerned with the transition of
ideas at both the global (e.g., paragraph) and local
(e.g., sentence) levels. While organization is an im-
portant dimension of essay quality, state-of-the-art
essay scoring software such as e-rater V.2 (Attali
and Burstein, 2006) employs rather simple heuristic-
based methods for computing the score of an essay
along this particular dimension.
Our goal in this paper is to develop a compu-
tational model for the organization of student es-
229
says. While many models of text coherence have
been developed in recent years (e.g., Barzilay and
Lee (2004), Barzilay and Lapata (2005), Soricut and
Marcu (2006), Elsner et al (2007)), the same is not
true for text organization. One reason is the avail-
ability of training and test data for coherence mod-
eling. Coherence models are typically evaluated on
the sentence ordering task, and hence training and
test data can be generated simply by scrambling the
order of the sentences in a text. On the other hand, it
is not particularly easy to find poorly organized texts
for training and evaluating organization models. We
believe that student essays are an ideal source of
well- and poorly-organized texts. We evaluate our
organization model on a data set of 1003 essays an-
notated with organization scores.
In sum, our contributions in this paper are two-
fold. First, we address a less-studied discourse-level
task ? predicting the organization score of an essay
? by developing a computational model of organi-
zation, thus establishing a baseline against which fu-
ture work on this task can be compared. Second, we
annotate a subset of our student essay corpus with
organization scores and make this data set publicly
available. Since progress in organization modeling
is hindered in part by the lack of a publicly anno-
tated corpus, we believe that our data set will be a
valuable resource to the NLP community.
2 Corpus Information
We use as our corpus the 4.5 million word Interna-
tional Corpus of Learner English (ICLE) (Granger
et al, 2009), which consists of more than 6000 es-
says written by university undergraduates from 16
countries and 16 native languages who are learners
of English as a Foreign Language. 91% of the ICLE
texts are argumentative. The essays we used vary
greatly in length, containing an average of 31.1 sen-
tences in 7.5 paragraphs, averaging 4.1 sentences per
paragraph. About one quarter of the essays had five
or fewer paragraphs, and another quarter contained
nine or more paragraphs. Similarly, about one quar-
ter of essays contained 24 or fewer sentences and the
longest quarter contained 36 or more sentences
We selected a subset consisting of 1003 essays
from the ICLE to annotate and use for training and
testing of our model of essay organization. While
Topic Languages Essays
Most university degrees are
theoretical and do not prepare
students for the real world.
They are therefore of very lit-
tle value.
13 147
The prison system is out-
dated. No civilized society
should punish its criminals: it
should rehabilitate them.
11 103
In his novel Animal Farm,
George Orwell wrote ?All
men are equal but some are
more equal than others.? How
true is this today?
10 82
Table 1: Some examples of writing topics.
narrative writing asks students to compose descrip-
tive stories, argumentative (also known as persua-
sive) writing requires students to state their opinion
on a topic and to validate that opinion with convinc-
ing arguments. For this reason, we selected only ar-
gumentative essays rather than narrative pieces, be-
cause they contain the discourse structures and kind
of organization we are interested in modeling.
To ensure representation across native languages
of the authors, we selected mostly essays written
in response to topics which are well-represented in
multiple languages. This avoids many issues that
may arise when certain vocabulary is used in re-
sponse to a particular topic for which essays written
by authors from only a few languages are available.
Table 1 shows three of the twelve topics selected for
annotation. Fifteen native languages are represented
in the set of essays selected for annotation.
3 Corpus Annotation
To develop our essay organization model, human an-
notators scored 1003 essays using guidelines in an
essay annotation rubric. Annotators evaluated the
organization of each essay using a numerical score
from 1 to 4 at half-point increments. This contrasts
with previous work on essay scoring, where the cor-
pus is annotated with a binary decision (i.e., good or
bad) for a given scoring dimension (e.g., Higgins et
al. (2004)). Hence, our annotation scheme not only
provides a finer-grained distinction of organization
quality (which can be important in practice), but also
230
makes the prediction task more challenging.
The meaning of each integer score was described
and discussed in detail. Table 2 shows the descrip-
tion of each score for the organization dimension.
Score Description of Essay Organization
4 essay is well structured and is organized in
a way that logically develops an argument
3 essay is fairly well structured but could
somewhat benefit from reorganization
2 essay is poorly structured and would
greatly benefit from reorganization
1 essay is completely unstructured and re-
quires major reorganization
Table 2: Descriptions of the meaning of each score.
Our annotators were selected from over 30 appli-
cants who were familiarized with the scoring rubric
and given sample essays to score. The six who were
most consistent with the expected scores were given
additional essays to annotate. To ensure consistency
in scoring, we randomly selected a large subset of
our corpus (846 essays) to have graded by two differ-
ent annotators. Analysis of these doubly annotated
essays reveals that, though annotators only exactly
agree on the organization score of an essay 29% of
the time, the scores they apply are within 0.5 points
in 71% of essays and within 1.0 point in 93% of es-
says. Additionally, if we treat one annotator?s scores
as a gold standard and the other annotator?s scores
as predictions, the predicted scores have a mean er-
ror of 0.54 and a mean squared error of 0.50. Table 3
shows the number of essays that received each of the
seven scores for organization.
score 1.0 1.5 2.0 2.5 3.0 3.5 4.0
essays 24 14 35 146 416 289 79
Table 3: Distribution of organization scores.
4 Function Labeling
As mentioned before, a high score on organization
means that writers introduce a topic, support their
position, and conclude. If one or more of these ele-
ments are missing or if they appear out of order (e.g.,
the conclusion appears before the introduction), the
resulting essay will typically be considered poorly
organized. Hence, knowing the discourse function
label of each paragraph in an essay would be help-
ful for predicting its organization score.
Two questions naturally arise. First, how can we
obtain the discourse function label of each para-
graph? One way is to automatically acquire such
labels from a corpus of student essays where each
paragraph is annotated with its discourse function
label. To our knowledge, however, there is no pub-
licly available corpus that is annotated with such in-
formation. As a result, we will resort to labeling a
paragraph with its function label heuristically.
Second, which paragraph function labels would
be most useful for scoring the organization of an es-
say? Based on our linguistic intuition, we identify
four potentially useful paragraph function labels: In-
troduction, Body, Rebuttal, and Conclusion. Table 4
gives the descriptions of these labels.
Label Name Paragraph Type
I Introduction introduces essay topic and
states author?s position and
main ideas
B Body provides reasons, evidence,
and examples to support main
ideas
C Conclusion summarizes and concludes ar-
guments made in body para-
graphs
R Rebuttal considers counter-arguments
to thesis or main ideas
Table 4: Descriptions of paragraph function labels.
Setting aside for the moment the problem of ex-
actly how to predict an essay?s organization score
given its paragraph sequence, the problem of ob-
taining paragraph labels to use for this task still re-
mains. As mentioned above, we adopt a heuristic ap-
proach to paragraph function labeling. The question,
then, is: what kind of knowledge sources should our
heuristics be based on? We have identified two types
of knowledge sources that are potentially useful for
paragraph labeling. The first of these are positional,
dealing with where in the essay a paragraph appears.
So for example, the first paragraph in an essay is
likely to be an Introduction, while the last is likely
to be a Conclusion. A paragraph in any other posi-
tion, on the other hand, is more likely to be a Body
or Rebuttal paragraph.
231
Label Name Sentence Function
P Prompt restates the prompt given to the author and contains no new material or opinions
T Transition shifts the focus to new topics but contains no meaningful information
H Thesis states the author?s position on the topic for which he/she is arguing
M Main Idea asserts reasons and foundational arguments that support the thesis
E Elaboration further explains reasons and ideas but contains no evidence or examples
S Support provides evidence and examples to support the claims made in other statements
C Conclusion summarizes and concludes the entire argument or one of the main ideas
R Rebuttal considers counter-arguments that contrast with the thesis or main ideas
O Solution puts to rest the questions and problems brought up by counter-arguments
U Suggestion proposes solutions the problems brought up by the argument
Table 5: Descriptions of sentence function labels.
A second potentially useful knowledge source in-
volves the types of sentences appearing in a para-
graph. This idea presupposes that, like paragraphs,
sentences too can have discourse function labels in-
dicating the logical role they play in an argument.
The sentence label schema we propose, which is de-
scribed in Table 5, is based on work in discourse
structure by Burstein et al (2003), but features addi-
tional sentence labels.
To illustrate why these sentence function labels
may be useful for paragraph labeling, consider a
paragraph containing a Thesis sentence. The pres-
ence of a Thesis sentence is a strong indicator that
the paragraph containing it is either an Introduction
or Conclusion. Similarly, a paragraph containing
Rebuttal or Solution sentences is more likely to be
a Body or Rebuttal paragraph.
Hence, to obtain a paragraph?s function label,
we need to first label its sentences. However, we
are faced with the same problem: how can we ob-
tain the sentence function labels? One way is to
learn them from a corpus where each sentence is
manually annotated with its sentence function la-
bel, which is the approach adopted by Burstein et
al. (2003). However, this annotated corpus is not
publicly available. In fact, to our knowledge, there
is no publicly-available corpus that is annotated with
sentence function labels. Consequently, we adopt a
heuristic approach to sentence function labeling.
Overall, we created a knowledge-lean set of
heuristic rules labeling paragraphs and sentences.
Because many of the paragraph labeling heuristics
depend on the availability of sentence labels, we will
describe the sentence labeling heuristics first. For
each sentence function label x, we identify several
features whose presence increases our confidence
that a given sentence is an example of x. So for
example, the presence of any of the words ?agree?,
?think?, or ?opinion? increases our confidence that
the sentence they occur in is a Thesis. If the sentence
instead contains words such as ?however?, ?but?,
or ?argue?, these increase our confidence that the
sentence is a Rebuttal. The features we examine
for sentence labeling are not limited to words, how-
ever. Each content word the sentence shares with
the essay prompt gives us evidence that the sentence
is a restatement of the prompt. Having searched a
sentence for all these clues, we finally assign the
sentence the function label having the most support
among the clues found.
The heuristic rules for paragraph labeling are sim-
ilar in nature, though they depend heavily on the
labels of a paragraph?s component sentences. If a
paragraph contains Thesis, Prompt, or Background
sentences, the paragraph is likely to be an Introduc-
tion. However, if a paragraph contains Main Idea,
Support, or Conclusion sentences, it is likely to be
a Body paragraph. Finally, as mentioned previously,
some positional information is used in labeling para-
graphs. For example, a paragraph that is the first
paragraph in an essay is likely to be an Introduction,
but a paragraph that is neither the first nor the last
is likely to be either a Rebuttal or Body paragraph.
After searching a paragraph for all these features,
we gather the pieces of evidence in support of each
paragraph label and assign the paragraph the label
having the most support.1
1Space limitations preclude a complete listing of these para-
232
5 Heuristic-Based Organization Scoring
Having applied labels to each paragraph in an es-
say, how can we use these labels to predict the es-
say?s score? Recall that the importance of each para-
graph label stems not from the label itself, but from
the sequence of labels it appears in. Motivated by
this observation, we exploit a technique that is com-
monly used in bioinformatics ? sequence align-
ment. While sequence alignment has also been used
in text and paraphrase generation (e.g., Barzilay and
Lee (2002; 2003)), it has not been extensively ap-
plied to other areas of language processing, includ-
ing essay scoring. In this section, we will present
two heuristic approaches to organization scoring,
one based on aligning paragraph sequences and the
other on aligning sentence sequences.
5.1 Aligning Paragraph Sequences
As mentioned above, our first approach to heuristic
organization scoring involves aligning paragraph se-
quences. Specifically, this approach operates in two
steps. Given an essay e in the test set, we (1) find the
k essays in the training set that are most similar to e
via paragraph sequence alignment, and then (2) pre-
dict the organization score of e by aggregating the
scores of its k nearest neighbors obtained in the first
step. Below we describe these two steps in detail.
First, to obtain the k nearest neighbors of e,
we employ the Needleman-Wunsch alignment algo-
rithm (Needleman and Wunsch, 1970), which com-
putes a similarity score for any pair of essays by
finding an optimal alignment between their para-
graph sequences. To illustrate why we believe se-
quence alignment can help us determine which es-
says are most similar, consider two example es-
says. One essay, which we will call IBBBC, begins
with an Introductory paragraph, follows it with three
Body paragraphs, and finally ends with a Conclud-
ing paragraph. Another essay CRRRI begins with
a paragraph stating its Conclusion, follows it with
three Rebuttal paragraphs, and ends with a para-
graph Introducing the essay?s topic. We can tell by
a casual glance at the sequences that any reasonable
similarity function should tell us that they are not
graph and sentence labeling heuristics. See our website at
http://www.hlt.utdallas.edu/
?
alan/ICLE/ for
the complete list of heuristics.
very similar. The Needleman-Wunsch alignment al-
gorithm has this effect since the score of the align-
ment it produces would be hurt by the facts that (1)
there is not much overlap in the sets of paragraph
labels each contains, and (2) the paragraph labels
they do share (I and C) do not occur in the same
order. The resulting alignment would therefore con-
tain many mismatches or indels.2
If we now consider a third essay whose para-
graph sequence could be represented as IBRBC, a
good similarity function should tell us that IBBBC
and IBRBC are very similar. The Needleman-
Wunsch alignment score between the two paragraph
sequences has this property, as the alignment al-
gorithm would discover that the two sequences are
identical except for the third paragraph label, which
could be mismatched for a small penalty. We would
therefore conclude that the IBBBC and IBRBC es-
says should receive similar organization scores.
To fully specify how to find the k nearest neigh-
bors of an essay, we need to define a similarity func-
tion between paragraph labels. In sequence align-
ment, similarity function S(i, j) tells us how likely
it is that symbol i (in our case, a paragraph label)
will be substituted with another symbol j. While
we expect that in an alignment between high-scoring
essays, an Introduction paragraph is most likely to
be aligned with another Introduction paragraph, how
much worse should the alignment score be if an In-
troduction paragraph needs to be mismatched with
a Rebuttal paragraph or replaced with an indel? We
solve this problem by heuristically defining the sim-
ilarity function as follows: S(i, j) = 1 when i = j,
S(i, j) = ?1 when i 6= j, and also S(i,?) =
S(?, i) = ?1, where ??? is an indel. In other
words, the similarity function encourages the align-
ment between two identical function labels and dis-
courages the alignment between two different func-
tion labels, regardless of the type of function labels.
After obtaining the k nearest neighbors of e, the
next step is to predict the organization score of e
by aggregating the scores of its k nearest neighbors
into one number. (Note that we know the organiza-
2In pairwise sequence alignment, a mismatch occurs when
one symbol has to be substituted for another to make two se-
quences match. An indel indicates that in order to transform
one sequence to match another, we must either insert a symbol
into one sequence or delete a symbol from the other sequence.
233
tion score of each nearest neighbor, since they are
all taken from the training set.) One natural way to
do this would be to take the mean, median, or mode
of its k nearest neighboring essays from the training
set. Hence, our first heuristic method Hp for scoring
organization has three variants.
5.2 Aligning Sentence Sequences
An essay?s paragraph sequence captures information
about its organization at a high level, but ignores
much of its lower level structure. Since we have also
heuristically labeled sentences, it now makes sense
to examine the sequences of sentence function labels
within an essay?s paragraphs. The intuition is that at
least some portion of an essay?s organization score
can be attributed to the organization of the sentence
sequences of its component paragraphs.
To address this concern, we propose a second
heuristic approach to organization scoring. Given
a test essay e, we first find for each paragraph in
e the k paragraphs in the training set that are most
similar to it. Specifically, each paragraph is repre-
sented by its sequence of sentence function labels.
Given this paragraph representation, we can find the
k nearest neighbors of a paragraph by applying the
Needleman-Wunsch algorithm described in the pre-
vious subsection to align sentence sequences, using
the same similarity function we defined above.
Next, we score each paragraph pi by aggregating
the scores of its k nearest neighbors obtained in the
first step, assuming the score of a nearest neighbor
paragraph is the same as the organization score of
the training set essay containing it. As before, we
can employ the mean, median, or mode to aggregate
the scores of the nearest neighbors of pi.
Finally, we predict the organization score of e by
aggregating the scores of its paragraphs obtained in
the second step. Again, we can employ mean, me-
dian, or mode to aggregate the scores. Since we have
three ways of aggregating the scores of a paragraph?s
nearest neighbors and three ways of aggregating the
resulting paragraph scores, this second method Hs
for scoring organization has nine variants.
6 Learning-Based Organization Scoring
In the previous section, we proposed two heuris-
tic approaches to organization scoring, one based
on aligning paragraph label sequences and the other
based on aligning sentence label sequences. In the
process of constructing these two systems, however,
we created a lot of information about the essays
which might also be useful for organization scoring,
but which the heuristic systems are unable to exploit.
To remedy the problem, we introduce three learning-
based systems which abstract the additional infor-
mation we produced in three different ways. In each
system, we use the SVMlight (Joachims, 1999) im-
plementation of regression support vector machines
(SVMs) (Cortes and Vapnik, 1995) to train a regres-
sor because SVMs have been frequently and suc-
cessfully applied to a variety of NLP problems.
6.1 Linear Kernel
Owing to the different ways we presented of com-
bining the scores of an essay?s nearest neighbors,
the paragraph label sequence alignment approach
has three variants, and its sentence label sequence
alignment counterpart has nine. Unfortunately, these
heuristic approaches suffer from two major weak-
nesses. First, it is not intuitively clear which of
these 12 ways for predicting an essay?s organiza-
tion score is clearly better than the others. Second,
it is not clear that the k nearest neighbors of an es-
say will always be similar to it with respect to or-
ganization score. While we do expect the alignment
scores between good essays with reasonable para-
graph sequences to be high, poorly organized es-
says by their nature have more random paragraph
sequences. Hence, we have no intuition about the k
nearest neighbors of a poor essay, as it may have as
high an alignment score with another poorly orga-
nized essay as with a good essay.
Our solution to these problems is to use the orga-
nization scores obtained by the 12 heuristic variants
as features in a linear kernel SVM learner. We be-
lieve that using the estimates given by all the 12 vari-
ants of the two heuristic approaches rather than only
one of them addresses the first weakness mentioned
above. The second weakness, on the other hand, is
addressed by treating the organization score predic-
tions obtained by the nearest neighbor methods as
features for an SVM learner rather than as estimates
of an essay?s organization score.
The approach we have just described, however,
does not exploit the full power of linear kernel
234
SVMs. One strength of linear kernels is that they
make it easy to incorporate a wide variety of dif-
ferent types of features. In an attempt to further
enhance the prediction capability of the SVM re-
gressor, we will provide it with not only the 12 fea-
tures derived from the heuristic-based approaches,
but also with two additional types of features.
First, to give our learner more direct access to
the information we used to heuristically predict es-
say scores, we can extract paragraph label subse-
quences3 from each essay and use them as features.
To illustrate the intuition behind these features, con-
sider two paragraph subsequences: Introduction?
Body and Rebuttal?Introduction. It is fairly typi-
cal to see the first subsequence, I?B, at the begin-
ning of a good essay, so its occurrence should give
us a small amount of evidence that the essay it oc-
curs in is well-organized. The presence of the sec-
ond subsequence, R?I, however, should indicate that
its essay?s organization is poor because, in general, a
good essay should not give a Rebuttal before an In-
troduction. Because we can envision subsequences
of various lengths being useful, we create a binary
presence or absence feature in the linear kernel for
each paragraph subsequence of length 1, 2, 3, 4, or
5 appearing in the training set.
Second, we employ sentence label subsequences
as features in the linear kernel. Recall that when
describing our alignment-based nearest neighbor
organization score prediction methods, we noted
that an essay?s organization score may be partially
attributable to how well the sentences within its
paragraphs are organized. For example, if one
of an essay?s paragraphs contains the sentence la-
bel subsequence Main Idea?Elaboration?Support?
Conclusion this gives us some evidence that the es-
say is overall well-organized since one of its compo-
nent paragraphs contains this reasonably-organized
subsequence. An essay with a paragraph contain-
ing the subsequence Conclusion?Support?Thesis?
Rebuttal, however, is likely to be poorly orga-
nized because this is a poorly-organized subse-
quence. Since sentence label subsequences of dif-
fering lengths may be useful for score prediction, we
create a binary presence or absence feature for each
sentence label subsequence of length 1, 2, 3, 4, or 5
3Note that a subsequence is not necessarily contiguous.
in the training set.
While the number of nearest neighbor features is
manageable, the presence of a large number of fea-
tures can sometimes confuse a learner. For that rea-
son, we do feature selection on the two types of
subsequence features, selecting only 100 features
for each type that has the highest information gain
(see Yang and Pedersen (1997) for details). We
call the system resulting from the use of these three
types of features Rlnps because it uses Regression
with linear kernel to predict essay scores, and it
uses nearest neighbor, paragraph subsequence, and
sentence subsequence features.
6.2 String Kernel
In a traditional learning setting, the feature set em-
ployed by an off-the-shelf learning algorithm typ-
ically consists of flat features (i.e., features whose
values are discrete- or real-valued, as the ones de-
scribed in the Linear Kernel subsection). Advanced
machine learning algorithms such as SVMs, on the
other hand, have enabled the use of structured fea-
tures (i.e., features whose values are structures such
as parse trees and sequences), owing to their ability
to employ kernels to efficiently compute the similar-
ity between two potentially complex structures.
Perhaps the most obvious advantage of employ-
ing structured features is simplicity. To understand
this advantage, consider learning in a traditional set-
ting. Recall that we can only employ flat features in
this setting, as we did with the linear kernel. Hence,
if we want to use information from a parse tree as
features, we will need to design heuristics to extract
the desired parse-based features from parse trees.
For certain tasks, designing a good set of heuris-
tics can be time-consuming and sometimes difficult.
On the other hand, SVMs enable a parse tree to
be employed directly as a structured feature, obvi-
ating the need to design heuristics to extract infor-
mation from potentially complex structures. How-
ever, structured features have only been applied to a
handful of NLP tasks such as semantic role labeling
(Moschitti, 2004), syntactic parsing and named en-
tity identification (Collins and Duffy, 2002), relation
extraction (Bunescu and Mooney, 2005), and coref-
erence resolution (Versley et al, 2008). Our goal
here is to explore this rarely-exploited capability of
SVMs for the task of essay scoring.
235
While the vast majority of previous NLP work
on using structured features have involved tree ker-
nels, we employ a kernel that is rarely investigated in
NLP: string kernels (Lodhi et al, 2002). Informally,
a string kernel aims to efficiently compute the sim-
ilarity between two strings (or sequences) of sym-
bols based on the similarity of their subsequences.
We apply string kernels to essay scoring as follows:
we represent each essay using its paragraph function
label sequence, and employ a string kernel to com-
pute the similarity between two essays based on this
representation. Typically, a string kernel takes as in-
put two parameters: K (which specifies the length
of the subsequences in the two strings to compare)
and ? (which is a value between 0 and 1 that spec-
ifies whether matches between non-contiguous sub-
sequences in the two strings should be considered
as important as matches between contiguous subse-
quences). In our experiments, we select values for
these parameters in a somewhat arbitrary manner. In
particular, since ? ranges between 0 and 1, we sim-
ply set it to 0.5. For K , since in the flat features we
considered all paragraph label sequences of lengths
from 1 to 5, we again take the middle value, setting
it to 3. We call the system using this kernel Rs be-
cause it uses a Regression SVM with a string kernel
to predict essay scores.
6.3 Alignment Kernel
In general, the purpose of a kernel function is to
measure the similarity between two examples. The
string kernel we described in the previous subsec-
tion is just one way of measuring the similarity of
two essays given their paragraph sequences. While
this may be the most obvious way to use paragraph
sequence information from a machine learning per-
spective, our earlier use of the Needleman-Wunsch
algorithm suggests a more direct way of extracting
structured information from paragraph sequences.
More specifically, recall that the Needleman-
Wunsch algorithm finds an optimal alignment be-
tween two paragraph sequences, where an opti-
mal alignment is defined as an alignment having
the highest possible alignment score. The optimal
alignment score can be viewed as another similar-
ity measure between two essays. As such, with
some slight modifications, the alignment score be-
tween two paragraph sequences can be used as the
kernel value for an Alignment Kernel.4 We call
the system using this kernel Ra because it uses a
Regression SVM with an alignment kernel to pre-
dict essay scores.
6.4 Combining Kernels
Recall that the flat features are computed using a lin-
ear kernel, while the two types of structured features
are computed using string and alignment kernels. If
we want our learner to make use of more than one of
these types of features, we need to employ a compos-
ite kernel to combine them. Specifically, we define
and employ the following composite kernel:
Kc(F1, F2) =
1
n
n
?
i=1
Ki(F1, F2),
where F1 and F2 are the full set of features (contain-
ing both flat and structured features) that represent
the two essays under consideration, Ki is the ith ker-
nel we are combining, and n is the number of kernels
we are combining. To ensure that each kernel under
consideration contributes equally to the composite
kernel, each kernel value Ki(F1, F2) is normalized
so that its value falls between 0 and 1.
7 Evaluation
7.1 Evaluation Metrics
We designed three evaluation metrics to measure the
error of our organization scoring system. The sim-
plest metric, S1, is perhaps the most intuitive. It
measures the frequency at which a system predicts
the wrong score out of the seven possible scores.
Hence, a system that predicts the right score only
25% of the time would receive an S1 score of 0.75.
The S2 metric is slightly less intuitive than S1,
but no less reasonable. It measures the average
distance between the system?s score and the actual
score. This metric reflects the idea that a system
that estimates scores close to the annotator-assigned
scores should be preferred over a system whose esti-
mations are further off, even if both systems estimate
the correct score at the same frequency.
Finally, the S3 evaluation metric measures the
average square of the distance between a system?s
4In particular, we note that for theoretical reasons, a kernel
function must always return a non-negative value. The align-
ment score function does not have this property, so we increase
all alignment scores until their theoretical minimum value is 0.
236
organization score estimations and the annotator-
assigned scores. The intuition behind this system
is that not only should we prefer a system whose es-
timations are close to the annotator scores, but we
should also prefer one whose estimations are not too
frequently very far away from the annotator scores.
These three scores are given by:
1
N
?
Ai 6=Ei
1, 1
N
N
?
i=1
|Ai ? Ei|,
1
N
N
?
i=1
(Ai ? Ei)2,
where Ai and Ei are the annotator assigned and sys-
tem estimated scores respectively for essay i, and N
is the number of essays. Since many of the systems
we have described assign test essays real-valued or-
ganization scores, to obtain Ei for system S1 we
round the outputs of each system to the nearest of
the seven scores the human annotators were permit-
ted to assign (1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0).
To test our system, we performed 5-fold cross val-
idation on our 1003 essay set, micro-averaging our
results into three scores corresponding to the three
scoring metrics described above.
7.2 Results and Discussion
The average baseline. As mentioned before, there
is no standard baseline for organization modeling
against which we can compare our systems. To start
with, we employ a simple ?average? baseline. Avg
computes the average organization score of essays
in the training set and assigns this score to each test
set essay. Results of this baseline are shown in row
1 of Table 6. Though simple, this baseline is by no
means easy-to-beat, since 41% of the essays have a
score of 3, and 96% of the essays have a score that
is within one point of 3.
Heuristic baselines. Recall that we have 12 ver-
sions of the two heuristic approaches to organization
prediction. Space limitations preclude a discussion
of the results of all these versions, so instead, to ob-
tain the strongest baseline results, we show only the
best results achieved by the three versions based on
aligning paragraph label sequences in row 2 (Hp)
and the best results achieved by the nine versions
based on aligning sentence label sequences in row
3 (Hs) of Table 6. It is clear from the results that
the Hp systems yielded the best baseline predictions
under all three scoring metrics, performing signif-
icantly better than both the Avg and Hs systems
System S1 S2 S3
1 Avg .585 .412 .348
2 Hp .548 .339 .198
3 Hs .575 .397 .329
4 Rlnps .520 .331 .186
5 Rs .577 .369 .222
6 Ra .686 .519 .429
7 Rlsnps .534 .332 .187
8 Rlanps .541 .332 .178
9 Rsa .517 .325 .177
10 Rlsanps .517 .323 .175
Table 6: System Performance
(p < 0.01) with respect to the S2 and S3 metrics,
but its S1 performance is less significant with re-
spect to Avg (p < 0.1) and is indistinguishable at
even the p < 0.1 level from Hs.5 In general, how-
ever, it appears to be the case that systems based
on aligning paragraph label sequences achieve better
results than systems that attempt to align sentence
label sequences.
Learning-based approaches. Rows 4?6 of Table
6 show the results we obtained using each of the
three single-kernel systems. When compared to the
best baseline, these results suggest that Hp is a pretty
good heuristic approach to organization scoring. In
fact, only one of these three learning-based sys-
tems (Rlnps) performs better than Hp under the three
scoring metrics, and in each case, the difference be-
tween the two is not significant even at p < 0.1. This
suggests that, even though Rlnps performs slightly
better than Hp, the only major benefit we have ob-
tained by using the linear kernel is that it has made
it unnecessary for us to choose between the 12 pro-
posed heuristic systems.
Considering that the second best one-kernel sys-
tem, Rs, does not have access to any of the near-
est neighbor features, which have already proven
useful, its performance seems reasonably good in
that its performance is at least better than the Avg
system. This suggests that, even though Rs does
not perform exceptionally, it is extracting some use-
ful information for organization scoring from the
heuristically assigned paragraph label sequences.
The best one-kernel system, Rlnps, however, is sig-
5All significance tests are two-tailed paired t-tests.
237
nificantly better than Rs with respect to all three
scoring metrics, with p < 0.1 for S1 and p < 0.05
for S2 and S3. By contrast, it initially appears that
the alignment kernel is not extracting any useful
information from these paragraph sequences at all,
since its S1, S2, and S3 scores are all much worse
than all of the baseline systems. The second best
one-kernel system Rs performs significantly better
than Ra at p < 0.01 for all three scoring metrics.
Next, we explore the impact of composite kernels,
which allow our learners to make use of multiple
types of flat and structured features. Specifically, the
results shown in rows 7?9 are obtained by combin-
ing two kernels at a time. These experiments reveal
the surprising result that the two worst performing
single-kernel systems, Rs and Ra, when combined
into Rsa, yield the best two-kernel system results,
which are significant with respect to the best one-
kernel system results under S3 at p < 0.1. This re-
sult suggests that these two different methods of ex-
tracting information from paragraph sequences pro-
vide us with different kinds of evidence useful for
organization scoring, although neither method by it-
self was exceptionally useful. Though Rsa does
not have any access to nearest neighbor informa-
tion, it still performs significantly better than Hp at
p < 0.05 under S1 and S3.
While we have already pointed out that Rsa is
the best composite two-kernel system, it is not clear
which of Rlsnps and Rlanps is second-best. Neither
system consistently performs better than the other
under all three scoring metrics, and the differences
between them are not significant even at p < 0.1. It
is clear only that Rsa is better than both, as its scores
are statistically significantly better at p < 0.01 with
respect to Rlsnps and Rlanps under at least one of
the three scoring metrics in each case.
Finally, in the last row of Table 6, we combine
all three kernels into one SVM learner. The most
important lesson we learn from this experiment is
that each of the three kernels provides the learner
with a different kind of useful information, so that
a composite kernel using all three sources of in-
formation performs better than any system using
fewer kernels. Although the improvements over the
best two-kernel system (Rsa) and one-kernel sys-
tem (Rlnps) are small, they are still statistically sig-
nificant at p < 0.1 under one of the scoring metrics,
S3. When we compare this combined system to the
best baseline (Hp), we discover the improvements
derived from the three-kernel system are significant
improvements over it at p < 0.05 and p < 0.01 with
respect to S1 and S3 respectively.
Feature analysis. To better understand which of
the three flat features (nearest neighbors, paragraph
label sequences, or sentence label sequences) con-
tributes the most to the linear kernel portion of the
systems? performances, we analyze the three fea-
ture types on Rlnps using the backward elimination
feature selection algorithm. First, we remove each
of the three feature groups independently from the
Rlnps?s feature set and determine which of the three
removals yields the best performance according to
each scoring metric. Next, among the remaining
two feature groups, we repeat the same step, remov-
ing each of the two groups independently from the
feature set to determine which of the two removals
yields the best performance.
While space limitations preclude showing the ac-
tual numbers, the trend is consistent among all three
scoring metrics: the first feature type to remove
is paragraph sequences (meaning that they are the
least important) and the last to remove is the near-
est neighbor features. Nevertheless, performance al-
ways drops when a feature type is removed, indicat-
ing that all three feature types contribute positively
to overall performance. The fact that flat paragraph
sequence features proved to be least useful high-
lights the importance of the structured methods we
presented for using paragraph sequence information.
8 Conclusions
We have investigated the relatively less-studied
problem of modeling the organization in student es-
says. The contributions of our work include the
novel application of two techniques from bioinfor-
matics and machine learning ? sequence align-
ment and string kernels, as well as the introduc-
tion of alignment kernels ? to essay scoring. We
showed that each technique makes a significant con-
tribution to a scoring system, and we hope that this
work will increase awareness of these powerful tech-
niques among NLP researchers. Finally, to stimulate
work on this problem, we make our corpus of anno-
tated essays available to other researchers.
238
Acknowledgments
We thank the three reviewers for their comments.
Our six annotators, Andrew Hubbs, Karin Khoo,
Jayne Koath, Christopher Maier, Andrew Mallon,
and Cory Thornton, all deserve numerous thanks,
because without the countless hours they each spent
annotating hundreds of essays, none of the research
described in this paper would have been possible.
References
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the ACL, pages 141?148.
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of EMNLP, pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In HLT-NAACL 2003: Main Pro-
ceedings, pages 16?23.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Main Proceedings, pages 113?120.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT/EMNLP, pages 724?731.
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2004. Automated essay evaluation: The Criterion on-
line writing evaluation service. AI Magazine, 25(3),
27?36.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the write stuff: Automatic identification of
discourse structure in student essays. IEEE Intelligent
Systems, 18(1):32?39.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of the ACL, pages 263?270.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Scott Elliot. 2001. IntelliMetric: From here to validity.
Paper presented at the annual meeting of the American
Educational Research Association, Seattle, WA.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In NAACL HLT 2007: Proceedings of the
Main Conference, pages 436?443.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses universitaires de
Louvain.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In HLT-NAACL 2004: Main
Proceedings, pages 185?192.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and Alexan-
der Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning, pages 44?56. MIT Press.
Thomas K. Landauer, Darrell Laham, and Peter W.
Foltz. 2003. Automated scoring and annotation of
essays with the Intelligent Essay AssessorTM. In Auto-
mated Essay Scoring: A Cross-Disciplinary Perspec-
tive, pages 87?112.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Christopher J. C. H. Watkins. 2002.
Text classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring systems.
Natural Language Engineering, 10(1):25?55.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of the
ACL, pages 335?342.
Saul Ben Needleman and Christian Dennis Wunsch.
1970. A general method applicable to the search for
similarities in the amino acid sequence of two proteins.
Journal of Molecular Biology, 48(3):443?453, March.
Mark Shermis and Jill Burstein. 2003. Automated Essay
Scoring: A Cross-Disciplinary Perspective. Lawrence
Erlbaum Associates, Inc., Mahwah, NJ.
Mark Shermis, Jill Burstein, Derrick Higgins, and Klaus
Zechner. 2010. Automated essay scoring: Writing
assessment and instruction. In International Encyclo-
pedia of Education (3rd edition), pages 20?26.
Tony Silva. 1993. Toward an understanding of the dis-
tinct nature of L2 writing: The ESL research and its
implications. 27(4):657?677.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 803?810.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference systems
based on kernels methods. In Proceedings of COL-
ING, pages 961?968.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML, pages 412?420.
239
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1127?1138,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Vote Prediction on Comments in Social Polls
Isaac Persing and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,vince}@hlt.utdallas.edu
Abstract
A poll consists of a question and a set of
predefined answers from which voters can
select. We present the new problem of vote
prediction on comments, which involves
determining which of these answers a
voter selected given a comment she wrote
after voting. To address this task, we ex-
ploit not only the information extracted
from the comments but also extra-textual
information such as user demographic in-
formation and inter-comment constraints.
In an evaluation involving nearly one mil-
lion comments collected from the popu-
lar SodaHead social polling website, we
show that a vote prediction system that ex-
ploits only textual information can be im-
proved significantly when extended with
extra-textual information.
1 Introduction
We introduce in this paper a new opinion mining
task, vote prediction on comments in social polls.
Recall that a poll consists of a question accompa-
nied by a set of predefined answers. A user who
votes on the question will choose one of these an-
swers and will be prompted to enter a comment
giving an explanation of why she chose the an-
swer. Given a poll and a user comment written
in response to it, the task of vote prediction seeks
to determine which predefined answer was chosen
by the author of the comment.
A solution to the vote prediction problem would
contribute significantly to our understanding of the
underlying attitudes of individual social polling
website users. This understanding could be ex-
ploited for tasks such as improving user experi-
ence or directed advertising; if we can predict how
a user will vote on a question, we can make more
accurate guesses about what kind of content/ads
related to the question the user would like to see.
Unfortunately, a major difficulty of vote predic-
tion arises from the casual nature of discussion in
social media. A comment often contains insuffi-
cient information for inferring the user?s vote, or
in some cases may even be entirely absent.
In light of this difficulty, we exploit two addi-
tional types of information in the prediction pro-
cess. First, we employ demographic features de-
rived from user profiles. Demographic features
may be broadly useful for other opinion mining
tasks such as stance classification (Somasundaran
and Wiebe, 2010), as many social media web-
sites like CreateDebate1 allow users to create pro-
files with similar demographic information. Previ-
ous work has attempted to predict such latent fea-
tures (e.g., Rao and Yarowsky (2010), Burger et
al. (2011)) rather than employing them for opin-
ion mining tasks.
Second, we exploit inter-comment constraints
to help us perform joint inference over votes on
different questions. Note that previous work on
debate stance recognition has also employed con-
straints to improve the inference process. Specif-
ically, in stance prediction, it is typical to em-
ploy so-called author constraints (e.g., Thomas
et al. (2006), Bansal et al. (2008), Walker et al.
(2012a), Hasan and Ng (2013)), which specify that
two documents written by the same author for the
same topic should have the same stance. However,
in vote prediction, author constraints are not use-
ful because a user is not permitted to cast more
than one vote per question, unlike in stance pre-
diction, where users may engage in a debate and
therefore post more than once per debate topic.
Consequently, we propose two new types of con-
straints for exploiting inter topic user voting pat-
terns. One constraint involves pairs of authors and
the other involves pairs of questions. These con-
straints are also potentially useful for other opin-
1http://www.createdebate.com/
1127
ion mining tasks involving social media, as social
media sites typically allow users to comment on
multiple topics. Note that enforcing constraints in-
volving two questions is by no means trivial, as the
possible class values associated with the two com-
ments may not necessarily be the same.
Another contribution of our work lies in our
adaptation of the label propagation algorithm (Zhu
and Ghahramani, 2002) to enforce constraints for
vote prediction. Recall that existing stance classi-
fication approaches enforce constraints using min-
imum cut (Thomas et al., 2006), integer linear pro-
gramming (Lu et al., 2012), and loopy belief prop-
agation (Burfoot et al., 2011). Our decision to em-
ploy label propagation stems in part from the in-
ability of loopy belief propagation and integer lin-
ear programming to efficiently process the nearly
one million comments we have, and in part from
the inability of the traditional two-way minimum
cut algorithm to handle multiclass classification.
It is worth noting, however, that other variations
of the label propagation algorithm have been pro-
posed for unrelated NLP tasks such as automati-
cally harvesting temporal facts from the web (e.g.,
Wang et al. (2011) and Wang et al. (2012)).
While we are the first to address the vote predic-
tion task, other researchers have previously used
social media to predict the outcomes of various
events, primarily by analyzing Twitter data. For
example, Tumasjan et al. (2010) and Gayo-Avello
et al. (2011) performed the related task of predict-
ing the outcomes of elections. Rather than pre-
dicting election outcomes, O?Connor et al. (2010)
focused on finding correlations between measures
derived from tweets and the outcomes of politi-
cal events like elections and polls. Finally, Asur
and Huberman (2010) predicted movies? box of-
fice success. These tasks contrast with our task of
vote prediction in that they are concerned with ag-
gregate measures such as the fraction of the vote
each candidate or party will win in an election or
how much money a movie will make at the box
office, whereas vote prediction is concerned with
predicting how individual people will vote on a
much wider variety of news/political topics.
2 Corpus
SodaHead2 is a social polling website where users
vote on and ask questions about a wide variety of
topics ranging from the serious (e.g., ?Should the
2http://www.sodahead.com
U.S. raise the minimum wage??) to the silly (e.g
?What is your favorite kind of pie??). Whenever a
user votes on one of these questions, choosing one
of a set of predefined answers, she is prompted to
enter a comment giving an explanation of why she
chose the answer she did. Our corpus3 consists of
all the comments4 users posted under all featured
questions in the News & Politics category of the
SodaHead website between March 12, 2008 and
August 21, 2013.
This dataset consists of a total of 997,379 com-
ments over 4,803 different questions, so an aver-
age of 208 comments are written in response to
each question. The length of an average comment
is 49 words. As Table 1 illustrates, these questions
may have more than two possible answers, with an
average question having 2.4 possible answers.
Each SodaHead user has her own profile that
contains demographic information about her. As
we can see from Table 2, many users choose to
provide only some information about themselves,
leaving many of the demographic fields blank.
108,462 users posted at least one comment in our
corpus, with an average user commenting on 9.2
of our questions.
3 Baseline Systems
To perform our experiments, we first split our
comments into three sets, a test set for evaluating
performance, a training set for training classifiers,
and a development set for tuning parameters. In
order to ensure that the comparisons of our experi-
ments are valid, we construct our test set using the
same 20% of comments in the dataset regardless
of experiment. Since our goal is to plot a learning
curve illustrating how our various vote prediction
systems perform given different amounts of train-
ing and development data, we vary the size of our
training and development sets across experiments
so that in the smallest experiment, together they
comprise 25% of the remaining (non-test) com-
ments, and in the largest experiment, they com-
3http://www.hlt.utdallas.edu/%7epersingq/SocialPolls/ is
the distribution site for our corpus. We preserve user
anonymity by replacing the original id of each user with a
random number in our corpus.
4A ?comment? is the text a user posted when submitting
her vote on a question. It does not include posts not associ-
ated with a vote (such as responses to other posts) or votes
where the user chose not to enter a comment. Thus, there
is a one-to-one relationship between comments in votes in
our dataset. The vote associated with a comment is always
known.
1128
Question Vote Comment
Who Won Round Two of
the Presidential Debate?
Barack Obama Binders full of women. That is all.
Mitt Romney Obama is inept and a liar. We can?t survive 4 more years of his crazy crap.
What?s the Best Way to
Read a Magazine?
in print Upside down like Luna Lovegood.
online Print costs money. It also doesn?t have a Search function.
on a tablet device since sooooo many people have tablet devices why read it as print or online?
on a smartphone Clicked in print!!! Aargh
Table 1: Sample questions and comments. All of the pre-defined answers for these questions are repre-
sented by one comment.
User ID 3479864 3189372
Age 25-34
Smoker No
Drinker No
Income
Sexual Orientation Straight
Relationship Status Single
Political Views Conservative Moderate
Ethnicity
Looking For
Career Industry
Children Undecided
Education High School
Gender Female Male
Religious Views Other Christian
Employment Status
Weight Type
Table 2: Sample user profiles.
prise 100% of the remaining comments. For each
experiment, we maintain a ratio of three training
comments to one development comment.
Recall that each comment in our dataset is writ-
ten in response to a particular question. For each
test comment, our goal is to predict the user?s an-
swer to the question given the text of her comment.
One of the major inherent difficulties of our task
is that it consists not of one, but of 4,803 sep-
arate multiclass classification problems (one for
each question). As a result, our approach to the
problem necessarily has to be somewhat generic,
as it would be too time-consuming to develop an
appropriate feature set for each question.
3.1 Baseline 1
Our first baseline?s (B
1
) approach employs 4,803
multiclass classifiers (one for each question). Each
classifier is trained on one question?s training set,
representing each comment using only a bias fea-
ture. Each of our classifiers is trained using MAL-
LET?s (McCallum, 2002) implementation of max-
imum entropy (ME) classification. This is equiv-
alent to merely counting the number of training
set comments that voted for each possible answer,
selecting the most frequent answer, then applying
this label to all the comments in the test set. This
majority baseline serves primarily to tell us how
well our more sophisticated baseline performs.
3.2 Baseline 2
Our second baseline (B
2
) is constructed in exactly
the same way as B
1
except that each classifier is
trained using both a bias feature and a standard set
of feature types described below.
3.2.1 Features
Since the questions in our dataset come from the
News & Politics category of the SodaHead web-
site, many of the questions? topics are political.
For that reason, it makes sense to use features
which have been shown to work well on other
political classification problems. We therefore
base our feature set on that used by Walker et
al. (2012b) for political debate classification. Our
features are described below.
N-grams. Unigrams have been shown to per-
form well in ideological debates (Somasundaran
and Wiebe, 2010), so we therefore present our
classifiers with lemmatized unigram, bigram, and
trigram features. We normalize the n-gram feature
vector to unit length to avoid giving undue influ-
ence to longer comments.
Cue Words. Based on other work (Fox Tree
and Schrock, 1999; Fox Tree and Schrock, 2002;
Groen et al., 2010; Walker et al., 2012b), we also
present our classifiers with features representing
the first lemmatized unigram, bigram, and trigram
appearing in each comment. These may be useful
in our task when, for example, a user?s comment
begins with or entirely consists of a restatement of
the answer she chose. So if the possible answers
for a given question are ?Yes? and ?No?, a user
might write in her comment ?Yes. Because ...?,
and this would make the ?CueWord:Yes? feature
useful for classifying this comment.
Emotion Frequency. For each word in a com-
ment, we used the NRC Emotion Word Lexicon
1129
(Mohammad and Yang, 2011) to discover if the
word conveys any emotion. Then, for each emo-
tion or sentiment covered by the lexicon (anger,
anticipation, disgust, fear, joy, sadness, surprise,
trust, positive, or negative) e
i
, we construct a fea-
ture e
i
:
C(e
i
)
total
describing how much of the comment
consists of words conveying emotion e
i
, where
C(e
i
) is the count of words in the comment bear-
ing emotion e
i
and total is the number of words
in the comment. To understand why this fea-
ture may be useful, consider the question ?Does
Sarah Palin deserve VP?? We suspect that users
who post comments laden with words associated
with positive emotions like joy are more likely
to vote ?Yes? because the positive emotions im-
ply they are happy about a Sarah Palin vice presi-
dency. Similarly, users who post comments laden
with negative emotions like anger might be more
likely to vote ?No?.
Dependencies. We use the Stanford Parser (de
Marneffe et al., 2006) to extract a set of depen-
dencies from each comment. For an example of
how dependencies might help in our task, con-
sider the second comment in Table 1. From this
comment, we can extract the dependency triple
dependency:(nsubj,inept,obama), which indicates
that the user who wrote it does not like Obama and
is therefore more likely to have voted for Romney
in the question. Dependency feature vectors are
normalized to unit length.
Emotion Dependencies. To form an emo-
tion dependency feature, we take a regular de-
pendency feature and replace each of its words
where possible with the emotion it evokes as deter-
mined by the NRC Emotion Word Lexicon. Thus
from the dependency:(nsubj,inept,obama) exam-
ple above, we would generate three features: emo-
tiondependency:(nsubj,anger,obama), emotionde-
pendency:(nsubj,disgust,obama), and emotionde-
pendency:(nsubj,negative,obama). These features
help generalize dependencies, and this is use-
ful because predictive features like emotiondepen-
dency:(nsubj,negative,obama) appear frequently
in the comments for this question, but depen-
dency:(nsubj,inept,obama) does not. Emotion De-
pendency feature vectors are normalized to unit
length.
Post Information. Features under this category
just calculate some basic statistics about a com-
ment. These features may be useful because, for
example, the question ?Most Scandalous Politi-
cians of 2008? Who deserves the title?? has six
possible answers, each except the last naming a
particular well-known politician. The last choice
is ?The most scandalous politician of 2008 is ...?
and the user is expected to name a politician in her
comment. It would make sense for users choos-
ing this option to have written longer responses
since they have to name and possibly explain their
choice to users who might not necessarily know
who their chosen politician is.
3.2.2 Feature Selection
Because some of the feature types (n-grams, cue
words, dependencies, and emotion dependencies)
described in the previous subsection are expected
to generate a large number of non-predictive fea-
tures, we trim some of the most irrelevant fea-
tures out of the feature set to avoid memory prob-
lems. Therefore, following Yang and Pedersen
(1997), for each question we calculate the infor-
mation gain of each feature of these types on the
training set. We then remove those features having
the lowest information gain as well as those fea-
tures occurring less than ten times in the dataset.
Early experiments showed that 1,000 was a rea-
sonable number of features to keep, so for all ex-
periments we keep only the top 1,000 features of
these types. Note that we do not apply feature se-
lection to emotion frequency or post information
features, as each of these sets consists of a small
number of real-valued features.
4 Demographic Features
As mentioned in the introduction, a major diffi-
culty inherent to our problem is that in many cases
a comment contains insufficient information for
inferring the underlying vote. Aside from being
short, the comments shown in Table 1 are typ-
ical of comments found in the dataset. Some
comments are like the first and third in the table,
requiring some obscure bit of world knowledge
to understand what the writer is saying. Others
like the fourth only explain why the user did not
choose a particular answer, which is always po-
tentially useful, but sufficient only if the comment
excludes every other possible choice.
Because it is difficult to tell how a user voted
given her comment, we exploit the demographic
information users provide in their profiles as an
additional source of information. Since many
of the questions in our dataset deal with poli-
tics, we anticipate that information about things
1130
such as whether a comment was written by a
conservative or progressive user would be use-
ful for predicting the answers of many comments.
For each comment, we encode demographic in-
formation as features in the following way. For
each field in the user?s profile shown in Table 2
(aside from user ID), we construct a feature of the
form F
i
:V
i
if the user filled in field F
i
with value
V
i
. Thus, any comment made by user 3479864
would include the features Age:25?34, Politi-
calViews:Conservative, Gender:Female, and Reli-
gion:Other.
Here is an example of a comment whose
predicted vote gets corrected by adding demo-
graphic features to our system. For the ques-
tion, ?LPGA Decides to Allow Transgender Com-
petitors: Good or Bad Move for Golf??, user
2252750 writes, ?LPGA ...can let monkeys play if
they wish....nobody gives a rip... bark?. Of the
three possible answers for this question, ?Good
move?, ?Bad move?, and ?Undecided?, our base-
line system without demographics believes that
user 2252750 probably voted for the third, as
?nobody gives a rip? makes him sound apathetic
toward the issue. However, our demographic
system notices that his profile contains ?Reli-
gion:Christian?, and users with this demographic
attribute choose ?Bad Move? 64% of the time.
Thus, demographic features allowed our system to
correctly predict his vote for ?Bad Move?.
Since demographics are also expected to gen-
erate a large number of non-predictive features,
we apply feature selection to them as described in
Section 3.2.2.
5 Enforcing Constraints
We mentioned earlier that an average SodaHead
question contains 208 comments. This implies
that there are only about 31?125 comments5 in
the average training set for one of our ME classi-
fiers. It would be difficult to train a good classi-
fier from a training set this small even if we had
feature sets tailored to work well on each of the
4,803 questions. While we have already attempted
to exploit user information (in the form of de-
mographic features) to help improve our system?s
performance, this approach still treats the task as
4,803 separate classification problems. It does not
allow for the possibility that classification on one
5At the low and high end of the learning curve respec-
tively.
question may be improved by exploiting informa-
tion gleaned from votes on other questions.
One way we might exploit such information is
by first noticing that, for any pair of questions,
there may be multiple users who commented on
both. This overlap between questions allows us to
calculate how predictive a user?s vote on one ques-
tion is of how she will vote on the other. For ex-
ample, on the question ?Who Would You Rather
Have Dinner With??, we found that users who
voted for ?Mitt Romney? were much more likely
to choose ?No, I?m still voting for him? on the
question ?Does Mitt Romney?s ?Entitled? Remarks
Change Your Opinion of Him??. Similarly, users
who voted to have dinner with ?Barack Obama?
were much more likely to vote ?Yes, I?m not vot-
ing for him anymore? on the ?entitled? question.
A system that somehow takes into account this in-
formation might correctly classify a difficult com-
ment on the ?entitled? question if it notices that the
comment was written by a user who commented
on both questions and it knows how the user voted
on the ?dinner? question. We call the kind of con-
straint described here a QuestionPair constraint.
We might also exploit information from other
questions by noticing that there are users who
share similar attitudes on a wide variety of top-
ics in our dataset. We can gauge how often a
pair of users agree with each other by compar-
ing their votes on every question on which they
have both voted where their comments appear in
the training set. So for example, if we see that
two users have agreed on questions about George
H.W. Bush, Bill Clinton, and George W. Bush,
we can guess that they will also agree on a ques-
tion about Barack Obama. Similarly, if they dis-
agreed on all those questions, they are likely to
disagree on the last question. A system that takes
into account this kind of information could cor-
rectly classify an otherwise difficult comment if it
knows how another user voted on this question and
also knows how often the two users agree on other
questions. We call the kind of constraint described
here a VoterPair constraint.
In order to enforce both kinds of constraints,
we introduce a variation of the label propagation
algorithm (Zhu and Ghahramani, 2002). In our
version of the label propagation algorithm, each
comment in our dataset is represented by a node
in a graph. Each node is associated with a proba-
bility distribution indicating the likelihood that the
1131
comment belongs to each of its question?s possible
answers. Thus, when we initialize the graph, each
training set node?s probability distribution is set to
reflect its comment?s actual label (with a proba-
bility of 1 for the comment?s actual label and 0
for each other answer), and each development or
test set node?s probability distribution is set to the
value predicted by another classifier such as B
2
or
B
2
+ Dem since the algorithm is not permitted
to see the comment?s actual label. Lines 7?12 in
Figure 1 describe the graph?s initialization.
Now that we have set up the graph?s nodes, we
need to explain how our graph?s edges work. As
we discussed earlier in this section, the edges in
our graph will represent two kinds of soft con-
straints. Each edge allows one of a node?s neigh-
bors to cast a vote (in the form of a probability dis-
tribution over possible answers) for what it thinks
the node?s answer should be. Let us call the com-
ment node whose label we are trying to predict the
target node and the comment node which casts the
vote the source node.
Our graph contains a QuestionPair edge be-
tween any source and target comments written by
the same user. Since a user cannot comment more
than once on any question, the source and target
comments will occur in two different questions. In
order to determine how the source node votes over
a QuestionPair edge, we need to calculate some
probabilities. In particular, we need to determine
the probability that a user will vote for possible
answer k in the target question Q
I
given that she
voted for answer l in the source question Q
J
:
P (Q
I
k
|Q
J
l
) =
C(Q
I
k
,Q
J
l
)+?
?
m?A(Q
I
)
(C(Q
I
m
,Q
J
l
)+?)
where C(Q
I
n
, Q
J
l
) is the number of users who
voted for answer n in Q
I
and answer l in Q
J
, and
A(Q
I
) is the set of possible answers on Q
I
. We
set ?, the smoothing factor, to 10 since this value
worked well in earlier experiments. The source
node S casts its vote on target node T for the prob-
ability distribution given by:
QP
S,T
(Q
I
k
) =
?
m?A(Q
J
)
P
S
(Q
J
m
)P (Q
I
k
|Q
J
m
)
where P
S
(Q
J
m
) is the probability currently asso-
ciated with answer m in S?s question (Q
J
).
The graph contains a VoterPair edge between
any source and target nodes on the same question
if the users who posted these comments have both
voted on at least one other question together and
their comments on the other question(s) occurred
in the training set. To determine how the source
node votes over a VoterPair edge, we need to cal-
culate the probability that the source and target
users will agree on a generic issue:
P
agr
(U
S
, U
T
) =
C
agr
(U
S
,U
T
)+1
C
agr
(U
S
,U
T
)+C
dis
(U
S
,U
T
)+2
where C
agr
(U
S
, U
T
) is the number of questions
on which users U
S
and U
T
voted for the same
answer and both their comments occurred in the
training set, C
dis
(U
S
, U
T
) is the number of ques-
tions on which U
S
and U
T
voted for different
answers where both their comments occurred in
the training set, and the +1 and +2 are used for
smoothing. The probability distribution that the
source node S votes for on target node T is then
given by:
V P
S,T
(Q
I
k
) = P
S
(Q
I
k
)P
agr
(U
S
, U
T
)
+
?
m?A(Q
I
),
m 6=k
(P
S
(Q
I
m
))
1? P
agr
(U
S
, U
T
)
|A(Q
I
)| ? 1
where P
S
(Q
I
n
) is the probability currently asso-
ciated with answer n in the source node?s question
(Q
I
), and |A(Q
I
)| is the number of possible an-
swers on Q
I
. We divide the second term, which
deals with disagreement, by |A(Q
I
)| ? 1 because,
even if we know that the target and source users
disagreed on the answer to a particular question
and that the source user did not vote for answer
k, there is only a 1
|A(Q
I
)|?1
chance that the target
user voted for answer k since there are |A(Q
I
)|?1
non-k answers to choose from.
Now that we have described how edges are
added to the graph and how source comment nodes
vote over the edges, we are ready to begin iterat-
ing over the label propagation algorithm (line 13 in
Figure 1). For each iteration of the algorithm, we
update each development or test set node?s answer
probability distribution by assigning it a weighted
sum of (1) the initial probability distribution as-
signed to the node, (2) the sum of the Question-
Pair edges? votes, and (3) the sum of the VoterPair
edges? votes (line 16 in Figure 1). Upon comple-
tion of the algorithm, if our soft constraints work
as expected, the new labeling of comment nodes
should be more accurate than their initial labeling.
We tune the parameters W
I
, W
V
, W
Q
, and
iterations jointly by an exhaustive search of the
parameter space to maximize classification accu-
racy on the development set. Each of the weight
parameters is allowed to take one of the values 0,
1, or 2, and the iteration parameter is allowed take
one of the values 0, 1, 2, 3, 4, 5.
1132
1: LabelPropagation(Tr,D, Te, iterations,W
i
,W
V
,W
Q
, I)
2: Inputs:
3: Tr,D, Te: Comments in Training, Development, and Test set
4: iterations: The number of iterations to perform
5: W
i
,W
V
,W
Q
: Weights assigned to initial, VoterPair, and QuestionPair constraints
6: I: Initial answer probability distribution for all comments. Should reflect actual labels for training set comments and
classifier predictions for development and test set comments
7: for all C ? Tr ?D ? Te do
8: Create node representing C
9: C
p
? I
C
10: // C
p
: node C?s current probability distribution over possible answers
11: // I
C
: initial answer probability distribution for comment C
12: end for
13: for j = 1 to iterations do
14: for all node C ? D ? Te do
15: Add all edges targeting node C
16: C
p
? Norm(W
I
I
C
+ W
V
?
k
V P
k,C
+ W
Q
?
k
QP
k,C
)
17: // V P
k,C
, QP
k,C
: kth VoterPair, and kth QuestionPair votes for node C
18: Remove all edges targeting node C
19: end for
20: end for
Figure 1: Our label propagation algorithm.
One may be surprised to notice how we add
edges to the graph in the algorithm only to delete
them three lines later (lines 15 and 18 in Figure 1).
Though edges can be added at any point in the al-
gorithm, one benefit of using the label propagation
algorithm is that it is simple enough that it is not
necessary store all the edges in memory at once.
The only time we need to store an edge is when its
target is being voted on. This means that the label
propagation algorithm can handle large datasets
like ours with huge numbers of nodes and edges
without being prohibitively space-expensive.
6 Evaluation
6.1 Experimental Setup
We mentioned in Section 3 that we split our
dataset of 997,379 comments into a test set com-
prising about 20% of the dataset?s comments and
a training and development set comprising some
fraction of the remaining 80% of the comments.
We actually split the data up like this five different
times so that each comment appears in an experi-
ment?s test set exactly once. In this way, through
the use of five fold cross-validation, we can report
our results on the entire dataset.
6.2 Results and Discussion
Figure 2a shows the accuracy of the predictions
made by various systems. First, let us compare
our first and second baselines. Recall that the first
baseline (B
1
) predicts that all test comments will
have the same label as the majority of training
comments, and the second baseline?s (B
2
) predic-
tions are the output of ME classifiers trained with a
generic feature set. As we can see from the graph,
at very small training set sizes, the standard set of
features supplied to B
2
does little more than con-
fuse the ME learner, as it performs slightly but not
significantly worse6 than the first baseline when
the training/development set comprises only 25%
of the available data. This is understandable, as
25% of an average question?s available data is only
42 comments, an extremely small number of ex-
amples to learn from for most NLP tasks. Clearly
a better approach than the one provided by the sec-
ond baseline is needed. Though the average train-
ing set sizes at the 50%, 75%, and 100% levels are
still relatively small, B
2
significantly outperforms
B
1
at all these levels.
The small improvement sizes yielded by B
2
may be attributable to some of the inherent dif-
ficulties of the problem, particularly that (1) it is
composed of so many (4,803) separate subprob-
lems that it is impractical for us to tailor a unique
feature set for each one, (2) the average question is
associated with a very small number of comments
(about 208), making it difficult to train a reason-
ably good classifier for any question, and (3) many
of the comments contain insufficient information
for inferring the underlying votes. Perhaps some
of our proposed extensions to B
2
can help address
6All significance tests are paired t-tests, with p < 0.05.
Because we calculate a large number of significance results,
the p values we report are obtained using Holm-Bonferroni
multiple testing correction (Holm, 1979).
1133
 62
 63
 64
 65
 66
 67
 68
 69
 70
 25  50  75  100
A
cc
ur
ac
y 
(%
)
Training/Development Set Size (%)
B2+Dem+QPair+VPair
B2+Dem+VPair
B2+Dem+QPair
B2+QPair+VPair
B2+VPair
B2+QPair
B2+Dem
B2
B1
(a) Vote prediction.
 62
 63
 64
 65
 66
 67
 68
 69
 70
 25  50  75  100
A
cc
ur
ac
y 
(%
)
Training/Development Set Size (%)
B1+Dem+QPair+VPair
B1+Dem+VPair
B1+Dem+QPair
B1+QPair+VPair
B1+VPair
B1+QPair
B1+Dem
B1
(b) Arbitrary User Vote Prediction.
Figure 2: Five-fold cross-validation vote prediction learning curves.
some of these problems.
The first improvement we proposed involved
exploiting demographic features provided by users
to help with our prediction tasks. When we com-
bine Dem and B
2
?s feature sets, the resulting
system (B
2
+ Dem) performs better than any of
the systems discussed thus far at all four train-
ing/development set size levels, yielding signifi-
cant improvements over B
2
at all four levels. This
demonstrates that our demographic features are a
useful complement to a standard approach like the
one used by B
2
.
The second improvement we proposed involved
using a variation of the label propagation algo-
rithm to enforce QuestionPair constraints. Ques-
tionPair constraints, recall, allowed us to exploit
the observed voting patterns of users who voted
in the training set on any particular pair of ques-
tions. These constraints were expected to improve
our predictions for any user who voted on both
questions when at least one of their votes appeared
in the test set. System B
2
+ QPair corresponds
to following the algorithm in Figure 1, using sys-
tem B
2
?s ME classifiers to initialize a label prop-
agation graph, and then setting the VoterPair edge
weight (W
V
) to 0, thus allowing only Question-
Pair constraints. When we compare this system to
B
2
, we see that the performance boost Question-
Pair constraints give us over the baseline is consis-
tently greater than the boost given by adding de-
mographic features to it (B
2
+ Dem) across all
training/development set sizes. The improvement
over B
2
is even significant at the 75% and 100%
training/development set sizes.
The last improvement we proposed involved
adding VoterPair constraints to the label propaga-
tion graph. Recall that VoterPair constraints al-
lowed us to exploit how frequently we observed
two users agreeing with each other to predict
whether they will agree on any question they both
voted on. System B
2
+V Pair corresponds to fol-
lowing the label propagation algorithm using B
2
?s
ME classifiers to initialize the graph, then setting
the QuestionPair edge weight (W
Q
) to 0, thus al-
lowing only VoterPair constraints. The addition of
VoterPair constraints yields the largest significant
improvements over B
2
at all four levels, indicating
that, in the absence of our other proposed improve-
ments, VoterPair edge constraints are the most im-
portant addition we can make to our baseline.
While we have now shown that each of our pro-
posed extensions yields significant improvements
over B
2
, this does not necessarily mean that each
one is useful in the presence of the others. For
example, it might be the case that QuestionPair
constraints and Demographic features correct the
same kinds of classification errors, and therefore it
may be sufficient to use either one or the other to
obtain good results, but using both is unnecessary.
To test how useful they are in each other?s pres-
1134
ence, we perform the following experiment. First,
we run the algorithm using all three improvements
(B
2
+Dem+QPair+V Pair in Figure 2a). We
then run the same experiment three more times,
each time removing one of the three extensions.
By measuring how much performance decreases
when we remove each of the three improvements,
we can determine whether each improvement pro-
vides unique useful information, or whether the in-
formation it provides is already being provided by
one of the other improvements.
To see what happens when we remove demo-
graphic features from the full system, we need to
compare B
2
+Dem+QPair+V Pair and B
2
+
QPair+V Pair in Figure 2a. While the decrease
in performance after removing demographic fea-
tures was modest, the difference is nevertheless
significant at all four training/development set
sizes, suggesting that demographic features do
provide unique information to the system.
By comparing line B
2
+ Dem + QPair +
V Pair to line B
2
+Dem+V Pair, we can deter-
mine the impact of QuestionPair constraints. Re-
moving QuestionPair constraints also had a mod-
est impact on the full system?s performance, de-
creasing accuracy at all four training/development
set sizes, significantly so at the 50%, 75%, and
100% levels. Interestingly, the impact of Ques-
tionPair constraints appears to grow with the train-
ing set, while the demographic features appear
to have a greater impact when the training set is
small. We can see this by noting that the two lines
cross at around 55%. This suggests that Question-
Pair constraints are especially useful in problems
where it is cheap to obtain a lot of training data,
but in problems where the data has to be manually
annotated, demographic features are more useful.
Finally, we can compare line B
2
+ Dem +
QPair + V Pair to line B
2
+ Dem + QPair to
see what happens when we remove VoterPair con-
straints from our system. This comparison illus-
trates that VoterPair constraints are by far the most
important improvement we removed from the full
system, as removing them yielded large significant
decreases at all four levels.
Though thus far we have only used it to analyze
the the contributions of different individual im-
provements, the full system B
2
+Dem+QPair+
V Pair is interesting in itself. Of all the systems
we have constructed, it performs the best, yield-
ing improvements of up to 5.18% and 3.88% when
compared to B
1
and B
2
respectively. Its improve-
ments over both baselines are statistically signifi-
cant at all four training/development set sizes.
6.3 Arbitrary User Vote Prediction
One interesting question that we have not yet ad-
dressed is, is it possible to predict how a user
would vote on a question she has not yet seen?
This problem is interesting because an average
question receives votes from only 0.2% of the
users in our dataset, and thus a system for predict-
ing an arbitrary user?s vote would be able to pre-
dict the votes of the other 99.8% of users. A solu-
tion to this prediction problem would have practi-
cal applications in areas such as directed advertis-
ing (e.g., if we could predict how a user would vote
on the magazine question in Table 1, we would
have a better idea of what kinds of reading de-
vices/services would interest her).
We can mimic this problem with our dataset
by treating the comment text associated with test
votes as unseen since we cannot expect an arbi-
trary user to have commented on any particular
question we are interested in7. It does, however,
make sense for us to expect our arbitrary user to
have provided some personal demographic infor-
mation, and thus a system for making these types
of predictions could reasonably make use of de-
mographic features. Similarly, in this situation we
would expect to have knowledge of all users? train-
ing set voting histories. Thus, it would also be rea-
sonable for our system to exploit the QuestionPair
and VoterPair constraints described in Section 5.
Thus, to test how well our system performs on this
task, we repeat all experiments from the previous
section while replacing B
2
(which uses a ME clas-
sifier trained on comment-based features) with B
1
(the most frequent baseline, which uses a ME clas-
sifier trained using only a bias feature). The results
of these experiments are shown in Figure 2b.
If we compare the results from B
1
to B
1
+Dem
(which compliments B
1
?s bias feature with the de-
mographic feature set), we notice that B
1
+Dem
is significantly worse than B
1
at all training set
sizes. This confirms our suspicion from the pre-
7Although we are trying to mimic the situation in which
we predict how an arbitrary user would vote on an arbitrary
question, we caution that the vote data we train and evaluate
on was not obtained from a set of arbitrary SodaHead users. It
consists only of votes from users who chose which questions
they wanted to answer. For this reason, the data we train and
evaluate on for any question might not be a representative
sample of SodaHead users as a whole.
1135
vious section that demographic features by them-
selves serve only to confuse the learner, though we
will see in a moment that they are a helpful sup-
plement to more sophisticated systems.
We can evaluate QuestionPair constraints in this
setting by comparing the results from B
1
to B
1
+
QPair. B
1
+QPair consistently outperforms B
1
at all four training set sizes, significantly so at the
75% and 100% levels, and thus QuestionPair con-
straints are also a useful addition to our system.
VoterPair constraints can be evaluated in this
setting by comparing B
1
to B
1
+ V Pair. B
1
+
V Pair significantly outperforms B
1
at all four
training set sizes, and from the graph it appears
to be our most beneficial improvement.
To evaluate whether demographic features are
useful in the presence of the other improve-
ments, we compare the full system, B
1
+Dem+
QPair + V Pair, to its corresponding version
without demographic features, B
1
+ QPair +
V Pair. Though B
1
+ QPair + V Pair signif-
icantly outperforms the full system at the 25%
training set size, the full system significantly out-
performs B
1
+ QPair + V Pair at the 75% and
100% levels, indicating that in this setting, demo-
graphic features are useful in the presence of a
large training set.
We can evaluate the utility of QuestionPair con-
straints in this setting by comparing the full system
to B
1
+Dem+ V Pair. When we remove Ques-
tionPair constraints, accuracy is consistently low-
ered at all four training set sizes, significantly so
at 50%, 75%, and 100%. This tells us that Ques-
tionPair constraints are useful in this setting.
We can evaluate how useful VoterPair con-
straints are by checking how much B
1
+ Dem +
V Pair+QPair?s performance drops when we re-
move VoterPair constraints from it, yielding B
1
+
Dem + QPair. Performance drops considerably
and significantly at all four training set sizes after
removing VoterPair constraints, suggesting that in
this setting, VoterPair constraints are still the most
important of our proposed improvements.
Finally, while we have already established that
all our proposed improvements can improve per-
formance under both settings (comments visible
and comments invisible), it may be worthwhile
to compare the two sets of experiments to deter-
mine whether the comment features used in sys-
tems with B
2
are useful.
A casual inspection of the two figures shows
that, broadly, each system that uses comment-
based features in Figure 2a tends to slightly out-
perform the most comparable system in Figure 2b.
At the low end of the curves, the two systems often
differ by about 1.0% in absolute accuracy, though
at the high end, the difference tends to be much
smaller, with the full system with comment fea-
tures outperforming the full system without com-
ment features by only 0.3%. Since in this setting
it is reasonable to assume a large training set, this
last result is the one we are most interested in, and
it suggests that our full system?s performance does
not suffer much due to the absence of comment
features.
One final observation we can make is that, when
comments are not visible, demographic features
appear to actively harm the performance of sys-
tems trained on a small amount of data, though
at larger training set sizes they are mostly help-
ful. We can tell this by comparing systems with
demographic features to systems without them in
Figure 2b (e.g., by comparing B
1
+Dem+QPair
to B
1
+QPair or B
1
+Dem+ V Pair to B
1
+
V Pair) at the 25% training set size. This is not
the case in the setting where comments are visi-
ble, as we see that demographic features always
appear helpful in Figure 2a. This reinforces the
notion that demographic features provide useful
information in general, but that they are by them-
selves too sparsely available to do more than con-
fuse the learner. They need to be supplemented by
other information sources in order for the learner
to draw correct conclusions.
7 Conclusion
We examined the task of vote prediction on com-
ments from the SodaHead website. To address this
task, we exploited not only information extracted
from the comments but also extra-textual informa-
tion, including demographic information and two
types of inter-comment constraints, QuestionPair
constraints and VoterPair constraints. Our exper-
iments involving 997,379 comments showed that
each of these extensions significantly improved a
baseline that exploited only textual information,
with VoterPair constraints being the most effective
and demographic information being the least ef-
fective. When used in combination, they obtained
up to a 3.88% improvement in absolute accuracy
over the baseline. To stimulate research on this
task, we make our dataset publicly available.
1136
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of this paper. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proceedings
of the 2010 IEEE/WIC/ACM International Confer-
ence on Web Intelligence and Intelligent Agent Tech-
nology, pages 492?499.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In COLING 2008: Companion Volume:
Posters, pages 15?18.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506?1515.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Gender discrimination on
twitter. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1301?1309.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation, pages 449?
454.
Jean E. Fox Tree and Josef C. Schrock. 1999. Dis-
course markers in spontaneous speech: Oh what a
difference an oh makes. Journal of Memory and
Language, 40:280?295.
Jean E. Fox Tree and Josef C. Schrock. 2002. Basic
meanings of you know and i mean. Journal of Prag-
matics, 34:427?447.
Daniel Gayo-Avello, Panagiotis Takis Metaxas, and
Eni Mustafaraj. 2011. Limits of electoral predic-
tions using twitter. In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media, pages 490?493.
Martin Groen, Jan Noyes, and Frans Verstraten. 2010.
The effect of substituting discourse markers on their
role in dialogue. Discourse Processes: A Multidis-
ciplinary Journal, 47:388?420.
Kazi Saidul Hasan and Vincent Ng. 2013. Stance
classification of ideological debates: Data, mod-
els, features, and constraints. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1348?1356.
Sture Holm. 1979. A simple sequentially rejective
multiple test procedure. Scandinavian Journal of
Statistics, 6:65?70.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM International Conference
on Information and Knowledge Management, pages
1642?1646.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit. http://mallet.cs.
umass.edu.
Saif Mohammad and Tony Yang. 2011. Tracking sen-
timent in mail: How genders differ on emotional
axes. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 70?79.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
Fourth International AAAI Conference on Weblogs
and Social Media, pages 122?129.
Delip Rao and David Yarowsky. 2010. Detecting latent
user properties in social media. In Proceedings of
the NIPS workshop on Machine Learning for Social
Networks.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335.
Andranik Tumasjan, Timm Sprenger, Philipp Sandner,
and Isabell Welpe. 2010. Predicting elections with
twitter: What 140 characters reveal about political
sentiment. In Proceedings of the Fourth Interna-
tional AAAI Conference on Weblogs and Social Me-
dia, pages 178?185.
Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012a. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?596.
1137
Marilyn A. Walker, Pranav Anand, Rob Abbott, Jean
E. Fox Tree, Craig Martell, and Joseph King. 2012b.
That is your evidence?: Classifying stance in on-
line political debate. Decision Support Systems,
53(4):719?729.
Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2011. Harvesting facts from
textual web sources by constrained label propaga-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement, pages 837?846.
Yafang Wang, Maximilian Dylla, Marc Spaniol, and
Gerhard Weikum. 2012. Coupling label propaga-
tion and constraints for temporal fact extraction. In
Proceedings of the ACL 2012 Conference Short Pa-
pers, pages 233?237.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412?420.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propaga-
tion. Technical Report CMU-CALD-02-107, CMU
CALD.
1138
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 260?269,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Thesis Clarity in Student Essays
Isaac Persing and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,vince}@hlt.utdallas.edu
Abstract
Recently, researchers have begun explor-
ing methods of scoring student essays with
respect to particular dimensions of qual-
ity such as coherence, technical errors,
and relevance to prompt, but there is rel-
atively little work on modeling thesis clar-
ity. We present a new annotated corpus
and propose a learning-based approach to
scoring essays along the thesis clarity di-
mension. Additionally, in order to pro-
vide more valuable feedback on why an
essay is scored as it is, we propose a sec-
ond learning-based approach to identify-
ing what kinds of errors an essay has that
may lower its thesis clarity score.
1 Introduction
Automated essay scoring, the task of employing
computer technology to evaluate and score writ-
ten text, is one of the most important educational
applications of natural language processing (NLP)
(see Shermis and Burstein (2003) and Shermis et
al. (2010) for an overview of the state of the art
in this task). A major weakness of many ex-
isting scoring engines such as the Intelligent Es-
say AssessorTM(Landauer et al, 2003) is that they
adopt a holistic scoring scheme, which summa-
rizes the quality of an essay with a single score and
thus provides very limited feedback to the writer.
In particular, it is not clear which dimension of
an essay (e.g., style, coherence, relevance) a score
should be attributed to. Recent work addresses this
problem by scoring a particular dimension of es-
say quality such as coherence (Miltsakaki and Ku-
kich, 2004), technical errors, Relevance to Prompt
(Higgins et al, 2004), and organization (Persing
et al, 2010). Essay grading software that provides
feedback along multiple dimensions of essay qual-
ity such as E-rater/Criterion (Attali and Burstein,
2006) has also begun to emerge.
Nevertheless, there is an essay scoring dimen-
sion for which few computational models have
been developed ? thesis clarity. Thesis clarity
refers to how clearly an author explains the thesis
of her essay, i.e., the position she argues for with
respect to the topic on which the essay is written.1
An essay with a high thesis clarity score presents
its thesis in a way that is easy for the reader to
understand, preferably but not necessarily directly,
as in essays with explicit thesis sentences. It addi-
tionally contains no errors such as excessive mis-
spellings that make it more difficult for the reader
to understand the writer?s purpose.
Our goals in this paper are two-fold. First, we
aim to develop a computational model for scoring
the thesis clarity of student essays. Because there
are many reasons why an essay may receive a low
thesis clarity score, our second goal is to build a
system for determining why an essay receives its
score. We believe the feedback provided by this
system will be more informative to a student than
would a thesis clarity score alone, as it will help
her understand which aspects of her writing need
to be improved in order to better convey her the-
sis. To this end, we identify five common errors
that impact thesis clarity, and our system?s pur-
pose is to determine which of these errors occur
in a given essay. We evaluate our thesis clarity
scoring model and error identification system on a
data set of 830 essays annotated with both thesis
clarity scores and errors.
In sum, our contributions in this paper are three-
fold. First, we develop a scoring model and error
identification system for the thesis clarity dimen-
sion on student essays. Second, we use features
explicitly designed for each of the identified error
1An essay?s thesis is the overall message of the entire es-
say. This concept is unbound from the the concept of thesis
sentences, as even an essay that never explicitly states its the-
sis in any of its sentences may still have an overall message
that can be inferred from the arguments it makes.
260
Topic Languages Essays
Most university degrees are the-
oretical and do not prepare stu-
dents for the real world. They are
therefore of very little value.
13 131
The prison system is outdated.
No civilized society should pun-
ish its criminals: it should reha-
bilitate them.
11 80
In his novel Animal Farm,
George Orwell wrote ?All men
are equal but some are more
equal than others.? How true is
this today?
10 64
Table 1: Some examples of writing topics.
types in order to train our scoring model, in con-
trast to many existing systems for other scoring di-
mensions, which use more general features devel-
oped without the concept of error classes. Third,
we make our data set consisting of thesis clarity
annotations of 830 essays publicly available in or-
der to stimulate further research on this task. Since
progress in thesis clarity modeling is hindered in
part by the lack of a publicly annotated corpus, we
believe that our data set will be a valuable resource
to the NLP community.
2 Corpus Information
We use as our corpus the 4.5 million word Interna-
tional Corpus of Learner English (ICLE) (Granger
et al, 2009), which consists of more than 6000 es-
says written by university undergraduates from 16
countries and 16 native languages who are learn-
ers of English as a Foreign Language. 91% of the
ICLE texts are argumentative. We select a sub-
set consisting of 830 argumentative essays from
the ICLE to annotate and use for training and test-
ing of our models of essay thesis clarity. Table 1
shows three of the thirteen topics selected for an-
notation. Fifteen native languages are represented
in the set of essays selected for annotation.
3 Corpus Annotation
For each of the 830 argumentative essays, we ask
two native English speakers to (1) score it along
the thesis clarity dimension and (2) determine the
subset of the five pre-defined errors that detracts
from the clarity of its thesis.
Scoring. Annotators evaluate the clarity of each
essay?s thesis using a numerical score from 1 to
4 at half-point increments (see Table 2 for a de-
scription of each score). This contrasts with pre-
vious work on essay scoring, where the corpus is
Score Description of Thesis Clarity
4 essay presents a very clear thesis and requires
little or no clarification
3 essay presents a moderately clear thesis but
could benefit from some clarification
2 essay presents an unclear thesis and would
greatly benefit from further clarification
1 essay presents no thesis of any kind and it is
difficult to see what the thesis could be
Table 2: Descriptions of the meaning of scores.
annotated with a binary decision (i.e., good or bad)
for a given scoring dimension (e.g., Higgins et al
(2004)). Hence, our annotation scheme not only
provides a finer-grained distinction of thesis clar-
ity (which can be important in practice), but also
makes the prediction task more challenging.
To ensure consistency in annotation, we ran-
domly select 100 essays to have graded by both
annotators. Analysis of these essays reveals that,
though annotators only exactly agree on the the-
sis clarity score of an essay 36% of the time, the
scores they apply are within 0.5 points in 62% of
essays and within 1.0 point in 85% of essays. Ta-
ble 3 shows the number of essays that receive each
of the seven scores for thesis clarity.
score 1.0 1.5 2.0 2.5 3.0 3.5 4.0
essays 4 9 52 78 168 202 317
Table 3: Distribution of thesis clarity scores.
Error identification. To identify what kinds of
errors make an essay?s thesis unclear, we ask one
of our annotators to write 1?4 sentence critiques
of thesis clarity on 527 essays, and obtain our list
of five common error classes by categorizing the
things he found to criticize. We present our anno-
tators with descriptions of these five error classes
(see Table 4), and ask them to assign zero or more
of the error types to each essay.
It is important to note that we ask our anno-
tators to mark an essay with one of these errors
only when the error makes the thesis less clear. So
for example, an essay whose thesis is irrelevant to
the prompt but is explicitly and otherwise clearly
stated would not be marked as having a Relevance
to Prompt error. If the irrelevant thesis is stated
in such a way that its inapplicability to the prompt
causes the reader to be confused about what the
essay?s purpose is, however, then the essay would
be assigned a Relevance to Prompt error.
To measure inter-annotator agreement on error
identification, we ask both annotators to identify
261
Id Error Description
CP Confusing Phrasing The thesis is phrased oddly, making it hard to understand the writer?s point.
IPR Incomplete Prompt Response The thesis seems to leave some part of a multi-part prompt unaddressed.
R Relevance to Prompt The apparent thesis?s weak relation to the prompt causes confusion.
MD Missing Details The thesis leaves out important detail needed to understand the writer?s point.
WP Writer Position The thesis describes a position on the topic without making it clear that this is
the position the writer supports.
Table 4: Descriptions of thesis clarity errors.
the errors in the same 100 essays that were doubly-
annotated with thesis clarity scores. We then com-
pute Cohen?s Kappa (Carletta, 1996) on each er-
ror from the two sets of annotations, obtaining an
average Kappa value of 0.75, which indicates fair
agreement. Table 5 shows the number of essays
assigned to each of the five thesis clarity errors.
As we can see, Confusing Phrasing, Incomplete
Prompt Response, and Relevance to Prompt are
the major error types.
error CP IPR R MD WP
essays 152 123 142 47 39
Table 5: Distribution of thesis clarity errors.
Relationship between clarity scores and error
classes. To determine the relationship between
thesis clarity scores and the five error classes, we
train a linear SVM regressor using the SVMlight
software package (Joachims, 1999) with the five
error types as independent variables and the re-
duction in thesis clarity score due to errors as the
dependent variable. More specifically, each train-
ing example consists of a target, which we set to
the essay?s thesis clarity score minus 4.0, and six
binary features, each of the first five representing
the presence or absence of one of the five errors in
the essay, and the sixth being a bias feature which
we always set to 1. Representing the reduction in
an essay?s thesis clarity score with its thesis clarity
score minus 4.0 allows us to more easily interpret
the error and bias weights of the trained system,
as under this setup, each error?s weight should be a
negative number reflecting how many points an es-
say loses due to the presence of that error. The bias
feature allows for the possibility that an essay may
lose points from its thesis clarity score for prob-
lems not accounted for in our five error classes.
By setting this bias feature to 1, we tell our learner
that an essay?s default score may be less than 4.0
because these other problems may lower the aver-
age score of otherwise perfect essays.
After training, we examined the weight param-
eters of the learned regressor and found that they
were all negative: ?0.6 for CP, ?0.5998 for IPR,
?0.8992 for R, ?0.6 for MD, ?0.8 for WP, and
?0.1 for the bias. These results are consistent
with our intuition that each of the enumerated er-
ror classes has a negative impact on thesis clarity
score. In particular, each has a demonstrable neg-
ative impact, costing essays an average of more
than 0.59 points when it occurs. Moreover, this set
of errors accounts for a large majority of all errors
impacting thesis clarity because unenumerated er-
rors cost essays an average of only one-tenth of
one point on the four-point thesis clarity scale.
4 Error Classification
In this section, we describe in detail our system for
identifying thesis clarity errors.
4.1 Model Training and Application
We recast the problem of identifying which the-
sis clarity errors apply to an essay as a multi-label
classification problem, wherein each essay may be
assigned zero or more of the five pre-defined er-
ror types. To solve this problem, we train five bi-
nary classifiers, one for each error type, using a
one-versus-all scheme. So in the binary classifi-
cation problem for identifying error ei, we create
one training instance from each essay in the train-
ing set, labeling the instance as positive if the es-
say has ei as one of its labels, and negative other-
wise. Each instance is represented by seven types
of features, including two types of baseline fea-
tures (Section 4.2) and five types of features we
introduce for error identification (Section 4.3).
After creating training instances for error ei, we
train a binary classifier, bi, for identifying which
test essays contain error ei. We use SVMlight for
classifier training with the regularization param-
eter, C , set to ci. To improve classifier perfor-
mance, we perform feature selection. While we
employ seven types of features (see Sections 4.2
and 4.3), only the word n-gram features are sub-
ject to feature selection.2 Specifically, we employ
2We do not apply feature selection to the remaining fea-
262
the top ni n-gram features as selected according to
information gain computed over the training data
(see Yang and Pedersen (1997) for details). Fi-
nally, since each classifier assigns a real value to
each test essay presented to it indicating its con-
fidence that the essay should be assigned error ei,
we employ a classification threshold ti to decide
how high this real value must be in order for our
system to conclude that an essay contains error ei.
Using held-out validation data, we jointly tune
the three parameters in the previous paragraph, ci,
ni, and ti, to optimize the F-score achieved by bi
for error ei.3 However, an exact solution to this op-
timization problem is computationally expensive.
Consequently, we find a local maximum by em-
ploying the simulated annealing algorithm (Kirk-
patrick et al, 1983), altering one parameter at a
time to optimize F-score by holding the remaining
parameters fixed.
After training the classifiers, we use them to
classify the test set essays. The test instances are
created in the same way as the training instances.
4.2 Baseline Features
Our Baseline system for error classification em-
ploys two types of features. First, since labeling
essays with thesis clarity errors can be viewed as
a text categorization task, we employ lemmatized
word unigram, bigram, and trigram features that
occur in the essay that have not been removed by
the feature selection parameter ni. Because the
essays vary greatly in length, we normalize each
essay?s set of word features to unit length.
The second type of baseline features is based on
random indexing (Kanerva et al, 2000). Random
indexing is ?an efficient, scalable and incremen-
tal alternative? (Sahlgren, 2005) to Latent Seman-
tic Indexing (Deerwester et al, 1990; Landauer
ture types since each of them includes only a small number
of overall features that are expected to be useful.
3For parameter tuning, we employ the following values.
ci may be assigned any of the values 102, 103, 104, 105, or
106. ni may be assigned any of the values 3000, 4000, 5000,
or ALL, where ALL means all features are used. For ti, we
split the range of classification values bi returns for the test set
into tenths. ti may take the values 0.0, 0.1, 0.2, . . ., 1.0, and
X, where 0.0 classifies all instances as negative, 0.1 classifies
only instances bi assigned values in the top tenth of the range
as positive, and so on, and X is the default threshold, labeling
essays as positive instances of ei only if bi returns for them a
value greater than 0. It was necessary to assign ti in this way
because the range of values classifiers return varies greatly
depending on which error type we are classifying and which
other parameters we use. This method gives us reasonably
fine-grained thresholds without having to try an unreasonably
large number of values for ti.
and Dutnais, 1997) which allows us to automat-
ically generate a semantic similarity measure be-
tween any two words. We train our random in-
dexing model on over 30 million words of the En-
glish Gigaword corpus (Parker et al, 2009) using
the S-Space package (Jurgens and Stevens, 2010).
We expect that features based on random index-
ing may be particularly useful for the Incomplete
Prompt Response and Relevance to Prompt errors
because they may help us find text related to the
prompt even if some of its components have been
rephrased (e.g., an essay may talk about ?jail?
rather than ?prison?, which is mentioned in one
of the prompts). For each essay, we therefore gen-
erate four random indexing features, one encoding
the entire essay?s similarity to the prompt, another
encoding the essay?s highest individual sentence?s
similarity to the prompt, a third encoding the high-
est entire essay similarity to one of the prompt sen-
tences, and finally one encoding the highest indi-
vidual sentence similarity to an individual prompt
sentence. Since random indexing does not pro-
vide a straightforward way to measure similar-
ity between groups of words such as sentences
or essays, we use Higgins and Burstein?s (2007)
method to generate these features.
4.3 Novel Features
Next, we introduce five types of novel features.
Spelling. One problem we note when examining
the information gain top-ranked features for the
Confusing Phrasing error is that there are very few
common confusing phrases that can contribute to
this error. Errors of this type tend to be unique, and
hence are not very useful for error classification
(because we are not likely to see the same error
in the training and test sets). We notice, however,
that there are a few misspelled words at the top of
the list. This makes sense because a thesis sen-
tence containing excessive misspellings may be
less clear to the reader. Even the most common
spelling errors, however, tend to be rare. Further-
more, we ask our annotators to only annotate an
error if it makes the thesis less clear. The mere
presence of an awkward phrase or misspelling is
not enough to justify the Confusing Phrasing label.
Hence, we introduce a misspelling feature whose
value is the number of spelling errors in an essay?s
most-misspelled sentence.4
4We employ SCOWL (http://wordlist.
sourceforge.net/) as our dictionary, assuming that a
263
Keywords. Improving the prediction of major-
ity classes can greatly enhance our system?s over-
all performance. Hence, since we have introduced
the misspelling feature to enhance our system?s
performance on one of the more frequently occur-
ring errors (Confusing Phrasing), it makes sense
to introduce another type of feature to improve
performance on the other two most frequent er-
rors, Incomplete Prompt Response and Relevance
to Prompt. For this reason, we introduce keyword
features. To use this feature, we first examine each
of the 13 essay prompts, splitting it into its com-
ponent pieces. For our purposes, a component of
a prompt is a prompt substring such that, if an es-
say does not address it, it may be assigned the In-
complete Prompt Response label. Then, for each
component, we manually select the most impor-
tant (primary) and second most important (sec-
ondary) words that it would be good for a writer
to use to address the component. To give an ex-
ample, the lemmatized version of the third com-
ponent of the second essay in Table 1 is ?it should
rehabilitate they?. For this component we selected
?rehabilitate? as a primary keyword and ?society?
as a secondary keyword. To compute one of our
keyword features, we compute the random index-
ing similarity between the essay and each group of
primary keywords taken from components of the
essay?s prompt and assign the feature the lowest
of these values. If this feature has a low value, that
suggests that the essay may have an Incomplete
Prompt Response error because the essay proba-
bly did not respond to the part of the prompt from
which this value came. To compute another of the
keyword features, we count the numbers of com-
bined primary and secondary keywords the essay
contains from each component of its prompt, and
divide each number by the total number of primary
and secondary features for that component. If the
greatest of these fractions has a low value, that in-
dicates the essay?s thesis might not be very Rele-
vant to the Prompt.5
Aggregated word n-gram features. Other
ways we could measure our system?s performance
(such as macro F-score) would consider our
system?s performance on the less frequent errors
no less important than its performance on the
word that does not appear in the dictionary is misspelled.
5Space limitations preclude a complete listing of the key-
word features. See our website at http://www.hlt.
utdallas.edu/
?
persingq/ICLE/ for the complete
list.
most frequent errors. For this reason, it now
makes sense for us to introduce a feature tailored
to help our system do better at identifying the
least-frequent error types, Missing Details and
Writer Position, each of which occurs in fewer
than 50 essays. To help with identification of
these error classes, we introduce aggregated
word n-gram features. While we mention in the
previous section one of the reasons regular word
n-gram features can be expected to help with
these error classes, one of the problems with
regular word n-gram features is that it is fairly
infrequent for the exact same useful phrase to
occur too frequently. Additionally, since there are
numerous word n-grams, some infrequent ones
may just by chance only occur in positive training
set instances, causing the learner to think they
indicate the positive class when they do not. To
address these problems, for each of the five error
classes ei, we construct two Aggregated word
features Aw+i and Aw?i. For each essay, Aw+i
counts the number of word n-grams we believe
indicate that an essay is a positive example of ei,
and Aw?i counts the number of word n-grams
we believe indicate an essay is not an example of
ei. Aw+ n-grams for the Missing Details error
tend to include phrases like ?there is something?
or ?this statement?, while Aw? ngrams are often
words taken directly from an essay?s prompt.
N-grams used for Writer Position?s Aw+ tend
to suggest the writer is distancing herself from
whatever statement is being made such as ?every
person?, but n-grams for this error?s Aw? feature
are difficult to find. Since Aw+i and Aw?i are
so error specific, they are only included in an
essay?s feature representation when it is presented
to learner bi. So while aggregated word n-grams
introduce ten new features, each learner bi only
sees two of these (Aw+i and Aw?i).
We construct the lists of word n-grams that are
aggregated for use as the Aw+ and Aw? fea-
ture values in the following way. For each error
class ei, we sort the list of all features occurring
at least ten times in the training set by information
gain. A human annotator then manually inspects
the top thousand features in each of the five lists
and sorts each list?s features into three categories.
The first category for ei?s list consists of features
that indicate an essay may be a positive instance.
Each word n-gram from this list that occurs in an
essay increases the essay?s Aw+i value by one.
264
Similarly, any word n-gram sorted into the second
category, which consists of features the annotator
thinks indicate a negative instance of ei, increases
the essay?s Aw? value by one. The third category
just contains all the features the annotator did not
believe were useful enough to either class, and we
make no further use of those features. For most er-
ror types, only about 12% of the top 1000 features
get sorted into one of the first two categories.
POS n-grams. We might further improve our
system?s performance on the Missing Details er-
ror type by introducing a feature that aggregates
part-of-speech (POS) tag n-grams in the same way
that the Aw features aggregate word n-gram fea-
tures. For this reason, we include POS tag 1, 2,
3, and 4-grams in the set of features we sort in
the previous paragraph. For each error ei, we se-
lect POS tag n-grams from the top thousand fea-
tures of the information gain sorted list to count
toward the Ap+i and Ap?i aggregation features.
We believe this kind of feature may help improve
performance on Missing Details because the list
of features aggregated to generate the Ap+i fea-
ture?s value includes POS n-gram features like CC
? NN ? (scare quotes). This feature type may also
help with Confusing Phrasing because the list of
POS tag n-grams our annotator generated for its
Ap+i contains useful features like DT NNS VBZ
VBN (e.g., ?these signals has been?), which cap-
tures noun-verb disagreement.
Semantic roles. Our last aggregated feature is
generated using FrameNet-style semantic role la-
bels obtained using SEMAFOR (Das et al, 2010).
For each sentence in our data set, SEMAFOR
identifies each semantic frame occurring in the
sentence as well as each frame element that par-
ticipates in it. For example, a semantic frame
may describe an event that occurs in a sentence,
and the event?s frame elements may be the peo-
ple or objects that participate in the event. For
a more concrete example, consider the sentence
?They said they do not believe that the prison sys-
tem is outdated?. This sentence contains a State-
ment frame because a statement is made in it. One
of the frame elements participating in the frame is
the Speaker ?they?. From this frame, we would
extract a feature pairing the frame together with
its frame element to get the feature ?Statement-
Speaker-they?. This feature indicates that the es-
say it occurs in might be a positive instance of the
Writer Position error since it tells us the writer is
attributing some statement being made to someone
else. Hence, this feature along with several oth-
ers like ?Awareness-Cognizer-we all? are useful
when constructing the lists of frame features for
Writer Position?s aggregated frame features Af+i
and Af?i. Like every other aggregated feature,
Af+i and Af?i are generated for every error ei.
5 Score Prediction
Because essays containing thesis clarity errors
tend to have lower thesis clarity scores than essays
with fewer errors, we believe that thesis clarity
scores can be predicted for essays by utilizing the
same features we use for identifying thesis clarity
errors. Because our score prediction system uses
the same feature types we use for thesis error iden-
tification, each essay?s vector space representation
remains unchanged. Only its label changes to one
of the values in Table 2 in order to reflect its thesis
clarity score. To make use of the fact that some
pairs of scores are more similar than others (e.g.,
an essay with a score of 3.5 is more similar to an
essay with a score of 4.0 than it is to one with a
score of 1.0), we cast thesis clarity score predic-
tion as a regression rather than classification task.
Treating thesis clarity score prediction as a re-
gression problem removes our need for a classi-
fication threshold parameter like the one we use
in the error identification problem, but if we use
SVMlight?s regression option, it does not remove
the need for tuning a regularization parameter, C ,
or a feature selection parameter, n.6 We jointly
tune these two parameters to optimize perfor-
mance on held-out validation data by performing
an exhaustive search in the parameter space.7
After we select the features, construct the essay
instances, train a regressor on training set essays,
and tune parameters on validation set essays, we
can use the regressor to obtain thesis clarity scores
on test set essays.
6Before tuning the feature selection parameter, we have to
sort the list of n-gram features occurring the training set. To
enable the use of information gain as the sorting criterion, we
treat each distinct score as its own class.
7The absence of the classification threshold parameter and
the fact that we do not need to train multiple learners, one for
each score, make it feasible for us to do two things. First, we
explore a wider range of values for the two parameters: we
allow C to take any value from 100, 101, 102, 103, 104, 105,
106, or 107, and we allow n to take any value from 1000,
2000, 3000, 4000, 5000, or ALL. Second, we exhaustively
explore the space defined by these parameters in order to ob-
tain an exact solution to the parameter optimization problem.
265
6 Evaluation
In this section, we evaluate our systems for error
identification and scoring. All the results we re-
port are obtained via five-fold cross-validation ex-
periments. In each experiment, we use 3/5 of our
labeled essays for model training, another 1/5 for
parameter tuning, and the final 1/5 for testing.
6.1 Error Identification
Evaluation metrics. To evaluate our thesis clar-
ity error type identification system, we compute
precision, recall, micro F-score, and macro F-
score, which are calculated as follows. Let tpi be
the number of test essays correctly labeled as posi-
tive by error ei?s binary classifier bi; pi be the total
number of test essays labeled as positive by bi; and
gi be the total number of test essays that belong to
ei according to the gold standard. Then, the preci-
sion (Pi), recall (Ri), and F-score (Fi) for bi and
the macro F-score (F?) of the combined system for
one test fold are calculated by
Pi =
tpi
pi
,Ri =
tpi
gi
, Fi =
2PiRi
Pi +Ri
, F? =
?
i Fi
5 .
However, the macro F-score calculation can be
seen as giving too much weight to the less frequent
errors. To avoid this problem, we also calculate
for each system the micro precision, recall, and F-
score (P, R, and F), where
P =
?
i tpi?
i pi
,R =
?
i tpi?
i gi
,F = 2PRP + R .
Since we perform five-fold cross-validation,
each value we report for each of these measures
is an average over its values for the five folds.8
Results and discussion. Results on error iden-
tification, expressed in terms of precision, recall,
micro F-score, and macro F-score are shown in
the first four columns of Table 6. Our Baseline
system, which only uses word n-gram and random
indexing features, seems to perform uniformly
poorly across both micro and macro F-scores (F
and F?; see row 1). The per-class results9 show
that, since micro F-score places more weight on
the correct identification of the most frequent er-
rors, the system?s micro F-score (31.1%) is fairly
close to the average of the scores obtained on the
three most frequent error classes, CP, IPR, and R,
8This averaging explains why the formula for F does not
exactly hold in the Table 6 results.
9Per-class results are not shown due to space limitations.
Error Identification Scoring
System P R F F? S1 S2 S3
1 B 24.8 44.7 31.1 24.0 .658 .517 .403
2 Bm 24.2 44.2 31.2 25.3 .654 .515 .402
3 Bmk 29.2 44.2 34.9 26.7 .663 .490 .369
4 Bmkw 28.5 49.6 35.5 31.4 .651 .484 .374
5 Bmkwp 34.2 49.6 40.4 34.6 .671 .483 .377
6 Bmkwpf 33.6 54.4 41.4 37.3 .672 .486 .382
Table 6: Five-fold cross-validation results for the-
sis clarity error identification and scoring.
and remains unaffected by very low F-scores on
the two remaining infrequent classes.10
When we add the misspelling feature to the
baseline, resulting in the system called Bm
(row 2), the micro F-score sees a very small, in-
significant improvement.11 What is pleasantly sur-
prising, however, is that, even though the mis-
spelling features were developed for the Confus-
ing Phrasing error type, they actually have more
of a positive impact on Missing Details and Writer
Position, bumping their individual error F-scores
up by about 5 and 3 percent respectively. This sug-
gests that spelling difficulties may be correlated
with these other essay-writing difficulties, despite
their apparent unrelatedness. This effect is strong
enough to generate the small, though insignificant,
gain in macro F-score shown in the table.
When we add keyword features to the system,
micro F-score increases significantly by 3.7 points
(row 3). The micro per-class results reveal that,
as intended, keyword features improve Incomplete
Prompt Response and Relevance to Prompt?s F-
scores reveals that they do by 6.4 and 9.2 percent-
age points respectively. The macro F-scores reveal
this too, though the macro F-score gains are 3.2
points and 11.5 points respectively. The macro F-
score of the overall system would likely have im-
proved more than shown in the table if the addition
of keyword features did not simultaneously reduce
Missing Details?s score by several points.
While we hoped that adding aggregated word
n-gram features to the system (row 4) would be
able to improve performance on Confusing Phras-
ing due to the presence of phrases such as ?in uni-
versity be? in the error?s Aw+i list, there turned
out to be few such common phrases in the data set,
10Since parameters for optimizing micro F-score and
macro F-score are selected independently, the per-class F-
scores associated with micro F-score are different than those
used for calculating macro F-score. Hence, when we discuss
per-class changes influencing micro F-score, we refer to the
former set, and otherwise we refer to the latter set.
11All significance tests are paired t-tests, with p < 0.05.
266
so performance on this class remains mostly un-
changed. This feature type does, however, result
in major improvements to micro and macro perfor-
mance on Missing Details and Writer Position, the
other two classes this feature was designed to help.
Indeed, the micro F-score versions of Missing De-
tails and Writer Position improve by 15.3 and 10.8
percentage points respectively. Since these are mi-
nority classes, however, the large improvements
result in only a small, insignificant improvement
in the overall system?s micro F-score. The macro
F-score results for these classes, however, improve
by 6.5% and 17.6% respectively, giving us a nearly
5-point, statistically significant bump in macro F-
score after we add this feature.
Confusing Phrasing has up to now stubbornly
resisted any improvement, even when we added
features explicitly designed to help our system do
better on this error type. When we add aggregated
part of speech n-gram features on top of the pre-
vious system, that changes dramatically. Adding
these features makes both our system?s F-scores
on Confusing Phrasing shoot up almost 8%, re-
sulting in a significant, nearly 4.9% improvement
in overall micro F-score and a more modest but
insignificant 3.2% improvement in macro F-score
(row 5). The micro F-score improvement can
also be partly attributed to a four point improve-
ment in Incomplete Prompt Response?s micro F-
score. The 13.7% macro F-score improvement of
the Missing Details error plays a larger role in the
overall system?s macro F-score improvement than
Confusing Phrasing?s improvement, however.
The improvement we see in micro F-score when
we add aggregated frame features (row 6) can be
attributed almost solely to improvements in classi-
fication of the minority classes. This is surprising
because, as we mentioned before, minority classes
tend to have a much smaller impact on overall
micro F-score. Furthermore, the overall micro
F-score improvement occurrs despite declines in
the performances on two of the majority class er-
rors. Missing Details and Writer Position?s mi-
cro F-score performances increase by 19.1% and
13.4%. The latter is surprising only because of
the magnitude of its improvement, as this feature
type was explicitly intended to improve its perfor-
mance. We did not expect this aggregated feature
type to be especially useful for Missing Details er-
ror identification because very few of these types
of features occur in its Af+i list, and there are
none in its Af?i list. The few that are in the for-
mer list, however, occur fairly often and look like
fairly good indicators of this error (both the exam-
ples ?Event-Event-it? and ?Categorization-Item-
that? occur in the positive list, and both do seem
vague, indicating more details are to be desired).
Overall, this system improves our base-
line?s macro F-score performance significantly by
13.3% and its micro F-score performance signifi-
cantly by 10.3%. As we progressed, adding each
new feature type to the baseline system, there was
no definite and consistent pattern to how the pre-
cisions and recalls changed in order to produce
the universal increases in the F-scores that we ob-
served for each new system. Both just tended to
jerkily progress upward as new feature types were
added. This confirms our intuition about these fea-
tures ? namely that they do not all uniformly im-
prove our performance in the same way. Some aim
to improve precision by telling us when essays are
less likely to be positive instances of an error class,
such as any of the Aw?i, Ap?i, or Af?i features,
and others aim to tell us when an essay is more
likely to be a positive instance of an error.
6.2 Scoring
Scoring metrics. We design three evaluation
metrics to measure the error of our thesis clarity
scoring system. The S1 metric measures the fre-
quency at which a system predicts the wrong score
out of the seven possible scores. Hence, a system
that predicts the right score only 25% of the time
would receive an S1 score of 0.75.
The S2 metric measures the average distance
between the system?s score and the actual score.
This metric reflects the idea that a system that
estimates scores close to the annotator-assigned
scores should be preferred over a system whose
estimations are further off, even if both systems
estimate the correct score at the same frequency.
Finally, the S3 metric measures the average
square of the distance between a system?s the-
sis clarity score estimations and the annotator-
assigned scores. The intuition behind this metric
is that not only should we prefer a system whose
estimations are close to the annotator scores, but
we should also prefer one whose estimations are
not too frequently very far away from the annota-
tor scores. These three scores are given by:
1
N
?
Aj 6=E?j
1, 1N
N?
i=1
|Aj ? Ej|,
1
N
N?
i=1
(Aj ? Ej)2
267
where Aj , Ej , and E?j are the annotator assigned,
system estimated, and rounded system estimated
scores12 respectively for essay j, and N is the
number of essays.
Results and discussion. Results on scoring are
shown in the last three columns of Table 6. We
see that the thesis clarity score predicting variation
of the Baseline system, which employs as features
only word n-grams and random indexing features,
predicts the wrong score 65.8% of the time. Its
predicted score is on average 0.517 points off of
the actual score, and the average squared distance
between the predicted and actual scores is 0.403.
We observed earlier that a high number of mis-
spellings may be positively correlated with one
or more unrelated errors. Adding the misspelling
feature to the scoring systems, however, only
yields minor, insignificant improvements to their
performances under the three scoring metrics.
While adding keyword features on top of this
system does not improve the frequency with which
the right score is predicted, it both tends to move
the predictions closer to the actual thesis clar-
ity score value (as evidenced by the significant
improvement in S2) and ensures that predicted
scores will not too often stray too far from the
actual value (as evidenced by the significant im-
provement in S3). Overall, the scoring model em-
ploying the Bmk feature set performs significantly
better than the Baseline scoring model with re-
spect to two out of three scoring metrics.
The only remaining feature type whose addition
yields a significant performance improvement is
the aggregated word feature type, which improves
system Bmk?s S2 score significantly while having
an insignificant impact on the other S metrics.
Neither of the remaining aggregative features
yields any significant improvements in perfor-
mance. This is a surprising finding since, up un-
til we introduced aggregated part-of-speech tag n-
gram features into our regressor, each additional
feature that helped with error classification made
at least a small but positive contribution to at least
two out of the three S scores. These aggregative
features, which proved to be very powerful when
assigning error labels, are not as useful for thesis
12Since our regressor assigns each essay a real value rather
than an actual valid thesis clarity score, it would be difficult
to obtain a reasonable S1 score without rounding the system
estimated score to one of the possible values. For that rea-
son, we round the estimated score to the nearest of the seven
scores the human annotators were permitted to assign (1.0,
1.5, 2.0, 2.5, 3.0, 3.5, 4.0) only when calculating S1.
S1 (Bmkw) S2 (Bmkwp) S3 (Bmk)
Gold .25 .50 .75 .25 .50 .75 .25 .50 .75
1.0 3.5 3.5 3.5 3.0 3.2 3.5 3.1 3.2 3.3
1.5 2.5 3.0 3.0 2.8 3.1 3.2 2.6 3.0 3.2
2.0 3.0 3.0 3.5 3.0 3.2 3.5 3.0 3.1 3.4
2.5 3.0 3.5 3.5 3.0 3.3 3.6 3.0 3.3 3.5
3.0 3.0 3.5 3.5 3.1 3.4 3.5 3.1 3.3 3.5
3.5 3.5 3.5 4.0 3.2 3.4 3.6 3.2 3.4 3.5
4.0 3.5 3.5 4.0 3.4 3.6 3.8 3.4 3.5 3.7
Table 7: Regressor scores for top three systems.
clarity scoring.
To more closely examine the behavior of the
best scoring systems, in Table 7 we chart the dis-
tributions of scores they predict for each gold stan-
dard score. As an example of how to read this ta-
ble, consider the number 2.8 appearing in row 1.5
in the .25 column of the S2 (Bmkwp) region. This
means that 25% of the time, when system Bmkwp
(which obtains the best S2 score) is presented with
a test essay having a gold standard score of 1.5,
it predicts that the essay has a score less than or
equal to 2.8 for the S2 metric.
From this table, we see that each of the best sys-
tems has a strong bias toward predicting more fre-
quent scores as there are no numbers less than 3.0
in the 50% columns, and about 82.8% of all essays
have gold standard scores of 3.0 or above. Never-
theless, no system relies entirely on bias, as evi-
denced by the fact that each column in the table
has a tendency for its scores to ascend as the gold
standard score increases, implying that the sys-
tems have some success at predicting lower scores
for essays with lower gold standard scores.
Finally, we note that the difference in error
weighting between the S2 and S3 scoring metrics
appears to be having its desired effect, as there is a
strong tendency for each entry in the S3 subtable
to be less than or equal to its corresponding entry
in the S2 subtable due to the greater penalty the
S3 metric imposes for predictions that are very far
away from the gold standard scores.
7 Conclusion
We examined the problem of modeling thesis clar-
ity errors and scoring in student essays. In addition
to developing these models, we proposed novel
features for use in our thesis clarity error model
and employed these features, each of which was
explicitly designed for one or more of the error
types, to train our scoring model. We make our
thesis clarity annotations publicly available in or-
der to stimulate further research on this task.
268
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of the paper. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with E-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The Kappa statistic. Computational
Linguistics, 22(2):249?254.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 948?956.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses universitaires
de Louvain.
Derrick Higgins and Jill Burstein. 2007. Sentence sim-
ilarity measures for essay coherence. In Proceed-
ings of the 7th International Workshop on Computa-
tional Semantics.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects
of coherence in student essays. In Human Lan-
guage Technologies: The 2004 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 185?192.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, Chapter 11, pages 169?
184. MIT Press, Cambridge, MA.
David Jurgens and Keith Stevens. 2010. The S-Space
package: An open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for Latent
Semantic Analysis. In Proceedings the 22nd Annual
Conference of the Cognitive Science Society, pages
103?106.
Scott Kirkpatrick, C. D. Gelatt, and Mario P. Vecchi.
1983. Optimization by simulated annealing. Sci-
ence, 220(4598):671?680.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to Plato?s problem: The Latent Semantic
Analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Thomas K. Landauer, Darrell Laham, and Peter W.
Foltz. 2003. Automated scoring and annotation of
essays with the Intelligent Essay AssessorTM. In Au-
tomated Essay Scoring: A Cross-Disciplinary Per-
spective, pages 87?112. Lawrence Erlbaum Asso-
ciates, Inc., Mahwah, NJ.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition. Linguistic Data Consortium, Philadelphia.
Isaac Persing, Alan Davis, and Vincent Ng. 2010.
Modeling organization in student essays. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 229?
239.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Proceedings of the Methods and Appli-
cations of Semantic Indexing Workshop at the 7th In-
ternational Conference on Terminology and Knowl-
edge Engineering.
Mark D. Shermis and Jill C. Burstein. 2003. Au-
tomated Essay Scoring: A Cross-Disciplinary Per-
spective. Lawrence Erlbaum Associates, Inc., Mah-
wah, NJ.
Mark D. Shermis, Jill Burstein, Derrick Higgins, and
Klaus Zechner. 2010. Automated essay scoring:
Writing assessment and instruction. In International
Encyclopedia of Education (3rd edition). Elsevier,
Oxford, UK.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412?420.
269
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1534?1543,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Modeling Prompt Adherence in Student Essays
Isaac Persing and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{persingq,vince}@hlt.utdallas.edu
Abstract
Recently, researchers have begun explor-
ing methods of scoring student essays with
respect to particular dimensions of qual-
ity such as coherence, technical errors,
and prompt adherence. The work on
modeling prompt adherence, however, has
been focused mainly on whether individ-
ual sentences adhere to the prompt. We
present a new annotated corpus of essay-
level prompt adherence scores and pro-
pose a feature-rich approach to scoring es-
says along the prompt adherence dimen-
sion. Our approach significantly outper-
forms a knowledge-lean baseline prompt
adherence scoring system yielding im-
provements of up to 16.6%.
1 Introduction
Automated essay scoring, the task of employing
computer technology to evaluate and score writ-
ten text, is one of the most important educational
applications of natural language processing (NLP)
(see Shermis and Burstein (2003) and Shermis et
al. (2010) for an overview of the state of the art
in this task). A major weakness of many ex-
isting scoring engines such as the Intelligent Es-
say AssessorTM(Landauer et al, 2003) is that they
adopt a holistic scoring scheme, which summa-
rizes the quality of an essay with a single score and
thus provides very limited feedback to the writer.
In particular, it is not clear which dimension of
an essay (e.g., style, coherence, relevance) a score
should be attributed to. Recent work addresses this
problem by scoring a particular dimension of es-
say quality such as coherence (Miltsakaki and Ku-
kich, 2004), technical errors, organization (Pers-
ing et al, 2010), and thesis clarity (Persing and
Ng, 2013). Essay grading software that provides
feedback along multiple dimensions of essay qual-
ity such as E-rater/Criterion (Attali and Burstein,
2006) has also begun to emerge.
Our goal in this paper is to develop a com-
putational model for scoring an essay along an
under-investigated dimension ? prompt adher-
ence. Prompt adherence refers to how related an
essay?s content is to the prompt for which it was
written. An essay with a high prompt adherence
score consistently remains on the topic introduced
by the prompt and is free of irrelevant digressions.
To our knowledge, little work has been done
on scoring the prompt adherence of student essays
since Higgins et al (2004). Nevertheless, there are
major differences between Higgins et al?s work
and our work with respect to both the way the task
is formulated and the approach. Regarding task
formulation, while Higgins et al focus on classi-
fying each sentence as having either good or bad
adherence to the prompt, we focus on assigning
a prompt adherence score to the entire essay, al-
lowing the score to range from one to four points
at half-point increments. As far as the approach
is concerned, Higgins et al adopt a knowledge-
lean approach to the task, where almost all of
the features they employ are computed based on
a word-based semantic similarity measure known
as Random Indexing (Kanerva et al, 2000). On
the other hand, we employ a large variety of fea-
tures, including lexical and knowledge-based fea-
tures that encode how well the concepts in an es-
say match those in the prompt, LDA-based fea-
tures that provide semantic generalizations of lex-
ical features, and ?error type? features that encode
different types of errors the writer made that are
related to prompt adherence.
In sum, our contributions in this paper are two-
fold. First, we develop a scoring model for the
prompt adherence dimension on student essays us-
ing a feature-rich approach. Second, in order to
stimulate further research on this task, we make
our data set consisting of prompt adherence an-
1534
Topic Languages Essays
Most university degrees are the-
oretical and do not prepare stu-
dents for the real world. They are
therefore of very little value.
13 131
The prison system is outdated.
No civilized society should pun-
ish its criminals: it should reha-
bilitate them.
11 80
In his novel Animal Farm,
George Orwell wrote ?All men
are equal but some are more
equal than others.? How true is
this today?
10 64
Table 1: Some examples of writing topics.
notations of 830 essays publicly available. Since
progress in prompt adherence modeling is hin-
dered in part by the lack of a publicly annotated
corpus, we believe that our data set will be a valu-
able resource to the NLP community.
2 Corpus Information
We use as our corpus the 4.5 million word Interna-
tional Corpus of Learner English (ICLE) (Granger
et al, 2009), which consists of more than 6000 es-
says written by university undergraduates from 16
countries and 16 native languages who are learn-
ers of English as a Foreign Language. 91% of the
ICLE texts are argumentative. We select a subset
consisting of 830 argumentative essays from the
ICLE to annotate for training and testing of our
essay prompt adherence scoring system. Table 1
shows three of the 13 topics selected for annota-
tion. Fifteen native languages are represented in
the set of annotated essays.
3 Corpus Annotation
We ask human annotators to score each of the 830
argumentative essays along the prompt adherence
dimension. Our annotators were selected from
over 30 applicants who were familiarized with the
scoring rubric and given sample essays to score.
The six who were most consistent with the ex-
pected scores were given additional essays to an-
notate. Annotators evaluated how well each es-
say adheres to its prompt using a numerical score
from one to four at half-point increments (see Ta-
ble 2 for a description of each score). This con-
trasts with previous work on prompt adherence es-
say scoring, where the corpus is annotated with a
binary decision (i.e., good or bad) (e.g., Higgins
et al (2004; 2006), Louis and Higgins (2010)).
Hence, our annotation scheme not only provides
Score Description of Prompt Adherence
4 essay fully addresses the prompt and consis-
tently stays on topic
3 essay mostly addresses the prompt or occasion-
ally wanders off topic
2 essay does not fully address the prompt or con-
sistently wanders off topic
1 essay does not address the prompt at all or is
completely off topic
Table 2: Descriptions of the meaning of scores.
a finer-grained distinction of prompt adherence
(which can be important in practice), but also
makes the prediction task more challenging.
To ensure consistency in annotation, we ran-
domly select 707 essays to have graded by mul-
tiple annotators. Analysis reveals that the Pear-
son?s correlation coefficient computed over these
doubly annotated essays is 0.243. Though annota-
tors exactly agree on the prompt adherence score
of an essay only 38% of the time, the scores they
apply fall within 0.5 points in 66% of essays and
within 1.0 point in 89% of essays. For the sake
of our experiments, whenever annotators disagree
on an essay?s prompt adherence score, we assign
the essay the average of all annotations rounded to
the nearest half point. Table 3 shows the number
of essays that receive each of the seven scores for
prompt adherence.
score 1.0 1.5 2.0 2.5 3.0 3.5 4.0
essays 0 0 8 44 105 230 443
Table 3: Distribution of prompt adherence scores.
4 Score Prediction
In this section, we describe in detail our system for
predicting essays? prompt adherence scores.
4.1 Model Training and Application
We cast the problem of predicting an essay?s
prompt adherence score as 13 regression prob-
lems, one for each prompt. Each essay is repre-
sented as an instance whose label is the essay?s
true score (one of the values shown in Table 3)
with up to seven types of features including base-
line (Section 4.2) and six other feature types pro-
posed by us (Section 4.3). Our regressors may as-
sign an essay any score in the range of 1.0?4.0.
Using regression captures the fact that some
pairs of scores are more similar than others (e.g.,
an essay with a prompt adherence score of 3.5 is
more similar to an essay with a score of 4.0 than it
is to one with a score of 1.0). A classification sys-
1535
tem, by contrast, may sometimes believe that the
scores 1.0 and 4.0 are most likely for a particu-
lar essay, even though these scores are at opposite
ends of the score range.
Using a different regressor for each prompt cap-
tures the fact that it may be easier for an essay to
adhere to some prompts than to others, and com-
mon problems students have writing essays for
one prompt may not apply to essays written in re-
sponse to another prompt. For example, in essays
written in response to the prompt ?Marx once said
that religion was the opium of the masses. If he
was alive at the end of the 20th century, he would
replace religion with television,? students some-
times write essays about all the evils of television,
forgetting that their essay is only supposed to be
about whether it is ?the opium of the masses?. Stu-
dents are less likely to make an analogous mistake
when writing for the prompt ?Crime does not pay.?
After creating training instances for prompt p
i
,
we train a linear regressor, r
i
, with regularization
parameter c
i
for scoring test essays written in re-
sponse to p
i
using the linear SVM regressor imple-
mented in the LIBSVM software package (Chang
and Lin, 2001). All SVM-specific learning param-
eters are set to their default values except c
i
, which
we tune to maximize performance on held-out val-
idation data.
After training the classifiers, we use them to
classify the test set essays. The test instances are
created in the same way as the training instances.
4.2 Baseline Features
Our baseline system for score prediction employs
various features based on Random Indexing.
1. Random Indexing Random Indexing (RI) is
?an efficient, scalable and incremental alterna-
tive? (Sahlgren, 2005) to Latent Semantic Index-
ing (Deerwester et al, 1990; Landauer and Dut-
nais, 1997) which allows us to automatically gen-
erate a semantic similarity measure between any
two words. We train our RI model on over 30 mil-
lion words of the English Gigaword corpus (Parker
et al, 2009) using the S-Space package (Jurgens
and Stevens, 2010). We expect that features based
on RI will be useful for prompt adherence scor-
ing because they may help us find text related
to the prompt even if some of its concepts have
have been rephrased (e.g., an essay may talk about
?jail? rather than ?prison?, which is mentioned in
one of the prompts), and because they have al-
ready proven useful for the related task of deter-
mining which sentences in an essay are related to
the prompt (Higgins et al, 2004).
For each essay, we therefore attempt to adapt
the RI features used by Higgins et al (2004) to
our problem of prompt adherence scoring. We do
this by generating one feature encoding the entire
essay?s similarity to the prompt, another encoding
the essay?s highest individual sentence?s similarity
to the prompt, a third encoding the highest entire
essay similarity to one of the prompt sentences,
another encoding the highest individual sentence
similarity to an individual prompt sentence, and fi-
nally one encoding the entire essay?s similarity to
a manually rewritten version of the prompt that ex-
cludes extraneous material (such as ?In his novel
Animal Farm, George Orwell wrote,? which is in-
troductory material from the third prompt in Ta-
ble 1). Our RI feature set necessarily excludes
those features from Higgins et al that are not
easily translatable to our problem since we are
concerned with an entire essay?s adherence to its
prompt rather than with each of its sentences? re-
latedness to the prompt. Since RI does not pro-
vide a straightforward way to measure similar-
ity between groups of words such as sentences
or essays, we use Higgins and Burstein?s (2007)
method to generate these features.
4.3 Novel Features
Next, we introduce six types of novel features.
2. N-grams As our first novel feature, we use
the 10,000 most important lemmatized unigram,
bigram, and trigram features that occur in the es-
say. N-grams can be useful for prompt adherence
scoring because they can capture useful words and
phrases related to a prompt. For example, words
and phrases like ?university degree?, ?student?,
and ?real world? are relevant to the first prompt in
Table 1, so it is more likely that an essay adheres
to the prompt if they appear in the essay.
We determine the ?most important? n-gram fea-
tures using information gain computed over the
training data (Yang and Pedersen, 1997). Since the
essays vary greatly in length, we normalize each
essay?s set of n-gram features to unit length.
3. Thesis Clarity Keywords Our next set of fea-
tures consists of the keyword features we intro-
duced in our previous work on essay thesis clarity
scoring (Persing and Ng, 2013). Below we give an
overview of these keyword features and motivate
1536
why they are potentially useful for prompt adher-
ence scoring.
The keyword features were formed by first ex-
amining the 13 essay prompts, splitting each into
its component pieces. As an example of what is
meant by a ?component piece?, consider the first
prompt in Table 1. The components of this prompt
would be ?Most university degrees are theoreti-
cal?, ?Most university degrees do not prepare stu-
dents for the real world?, and ?Most university de-
grees are of very little value.?
Then the most important (primary) and second
most important (secondary) words were selected
from each prompt component, where a word was
considered ?important? if it would be a good word
for a student to use when stating her thesis about
the prompt. So since the lemmatized version of the
third component of the second prompt in Table 1
is ?it should rehabilitate they?, ?rehabilitate? was
selected as a primary keyword and ?society? as a
secondary keyword.
Features are then computed based on these key-
words. For instance, one thesis clarity keyword
feature is computed as follows. The RI similarity
measure is first taken between the essay and each
group of the prompt?s primary keywords. The fea-
ture then gets assigned the lowest of these values.
If this feature has a low value, that suggests that
the student ignored the prompt component from
which the value came when writing the essay.
To compute another of the thesis clarity key-
word features, the numbers of combined primary
and secondary keywords the essay contains from
each component of its prompt are counted. These
numbers are then divided by the total count of pri-
mary and secondary features in their respective
components. The greatest of the fractions gener-
ated in this way is encoded as a feature because if
it has a low value, that indicates the essay?s thesis
may not be very relevant to the prompt.1
4. Prompt Adherence Keywords The thesis
clarity keyword features described above were in-
tended for the task of determining how clear an
essay?s thesis is, but since our goal is instead to de-
termine how well an essay adheres to its prompt,
it makes sense to adapt keyword features to our
task rather than to adopt keyword features ex-
1Space limitations preclude a complete listing of the the-
sis clarity keyword features. See our website at http:
//www.hlt.utdallas.edu/
?
persingq/ICLE/ for
the complete list.
actly as they have been used before. For this
reason, we construct a new list of keywords for
each prompt component, though since prompt ad-
herence is more concerned with what the student
says about the topics than it is with whether or
not what she says about them is stated clearly,
our keyword lists look a little different than the
ones discussed above. For an example, we ear-
lier alluded to the problem of students merely dis-
cussing all the evils of television for the prompt
?Marx once said that religion was the opium of the
masses. If he was alive at the end of the 20th cen-
tury, he would replace religion with television.?
Since the question suggests that students discuss
whether television is analogous to religion in this
way, our set of prompt adherence keywords for
this prompt contains the word ?religion? while the
previously discussed keyword sets do not. This
is because a thesis like ?Television is bad? can be
stated very clearly without making any reference
to religion at all, and so an essay with a thesis like
this can potentially have a very high thesis clarity
score. It should not, however, have a very high
prompt adherence score, as the prompt asked the
student to discuss whether television is like reli-
gion in a particular way, so religion should be at
least briefly addressed for an essay to be awarded
a high prompt adherence score.
Additionally, our prompt adherence keyword
sets do not adopt the notions of primary and sec-
ondary groups of keywords for each prompt com-
ponent, instead collecting all the keywords for a
component into one set because ?secondary? key-
words tend to be things that are important when we
are concerned with what a student is saying about
the topic rather than just how clearly she said it.
We form two types of features from prompt ad-
herence keywords. While both types of features
measure how much each prompt component was
discussed in an essay, they differ in how they en-
code the information. To obtain feature values of
the first type, we take the RI similarities between
the whole essay and each set of prompt adherence
keywords from the prompt?s components. This
results in one to three features, as some prompts
have one component while others have up to three.
We obtain feature values of the second type as
follows. For each component, we count the num-
ber of prompt adherence keywords the essay con-
tains. We divide this number by the number of
prompt adherence keywords we identified from
1537
the component. This results in one to three fea-
tures since a prompt has one to three components.
5. LDA Topics A problem with the features we
have introduced up to this point is that they have
trouble identifying topics that are not mentioned
in the prompt, but are nevertheless related to the
prompt. These topics should not diminish the es-
say?s prompt adherence score because they are at
least related to prompt concepts. For example,
consider the prompt ?All armies should consist en-
tirely of professional soldiers: there is no value in
a system of military service.? An essay contain-
ing words like ?peace?, ?patriotism?, or ?training?
are probably not digressions from the prompt, and
therefore should not be penalized for discussing
these topics. But the various measures of keyword
similarities described above will at best not notice
that anything related to the prompt is being dis-
cussed, and at worst, this might have effects like
lowering some of the RI similarity scores, thereby
probably lowering the prompt adherence score the
regressor assigns to the essay. While n-gram fea-
tures do not have exactly the same problem, they
would still only notice that these example words
are related to the prompt if multiple essays use the
same words to discuss these concepts. For this
reason, we introduce Latent Dirichlet Allocation
(LDA) (Blei et al, 2003) features.
In order to construct our LDA features, we
first collect all essays written in response to each
prompt into its own set. Note that this feature type
exploits unlabeled data: it includes all essays in
the ICLE responding to our prompts, not just those
in our smaller annotated 830 essay dataset. We
then use the MALLET (McCallum, 2002) imple-
mentation of LDA to build a topic model of 1,000
topics around each of these sets of essays. This
results in what we can think of as a soft clustering
of words into 1,000 sets for each prompt, where
each set of words represents one of the topics LDA
identified being discussed in the essays for that
prompt. So for example, the five most impor-
tant words in the most frequently discussed topic
for the military prompt we mentioned above are
?man?, ?military?, ?service?, ?pay?, and ?war?.
We also use the MALLET-generated topic
model to tell us how much of each essay is spent
discussing each of the 1,000 topics. The model
might tell us, for example, that a particular essay
written on the military prompt spends 35% of the
time discussing the ?man?, ?military?, ?service?,
?pay?, and ?war? topic and 65% of the time dis-
cussing a topic whose most important words are
?fully?, ?count?, ?ordinary?, ?czech?, and ?day?.
Since the latter topic is discussed so much in the
essay and does not appear to have much to do with
the military prompt, this essay should probably
get a bad prompt adherence score. We construct
1,000 features from this topic model, one for each
topic. Each feature?s value is obtained by using
the topic model to tell us how much of the essay
was spent discussing the feature?s corresponding
topic. From these features, our regressor should
be able to learn which topics are important to a
good prompt adherent essay.
6. Manually Annotated LDA Topics A weak-
ness of the LDA topics feature type is that it may
result in a regressor that has trouble distinguishing
between an infrequent topic that is adherent to the
prompt and one that just represents an irrelevant
digression. This is because an infrequent topic
may not appear in the training set often enough for
the regressor to make this judgment. We introduce
the manually annotated LDA topics feature type to
address this problem.
In order to construct manually annotated LDA
topic features, we first build 13 topic models, one
for each prompt, just as described in the section
on LDA topic features. Rather than requesting
models of 1,000 topics, however, we request mod-
els of only 100 topics2. We then go through all
13 lists of 100 topics as represented by their top
ten words, manually annotating each topic with a
number from 0 to 5 representing how likely it is
that the topic is adherent to the prompt. A topic
labeled 5 is very likely to be related to the prompt,
where a topic labeled 0 appears totally unrelated.
Using these annotations alongside the topic dis-
tribution for each essay that the topic models pro-
vide us, we construct ten features. The first five
features encode the sum of the contributions to an
essay of topics annotated with a number ? 1, the
sum of the contributions to an essay of topics an-
notated with a number ? 2, and so on up to 5.
The next five features are similar to the last,
with one feature taking on the sum of the contri-
butions to an essay of topics annotated with the
number 0, another feature taking on the sum of the
2We use 100 topics for each prompt in the manually an-
notated version of LDA features rather than the 1,000 topics
we use in the regular version of LDA features because 1,300
topics are not too costly to annotate, but manually annotating
13,000 topics would take too much time.
1538
contributions to an essay of topics annotated with
the number 1, and so on up to 4. We do not include
a feature for topics annotated with the number 5
because it would always have the same value as
the feature for topics ? 5.
Features like these should give the regressor a
better idea how much of an essay is composed of
prompt-related arguments and discussion and how
much of it is irrelevant to the prompt, even if some
of the topics occurring in it are too infrequent to
judge just from training data.
7. Predicted Thesis Clarity Errors In our pre-
vious work on essay thesis clarity scoring (Persing
and Ng, 2013), we identified five classes of errors
that detract from the clarity of an essay?s thesis:
Confusing Phrasing. The thesis is phrased oddly,
making it hard to understand the writer?s point.
Incomplete Prompt Response. The thesis leaves
some part of a multi-part prompt unaddressed.
Relevance to Prompt. The apparent thesis?s weak
relation to the prompt causes confusion.
Missing Details. The thesis leaves out an impor-
tant detail needed to understand the writer?s point.
Writer Position. The thesis describes a position
on the topic without making it clear that this is the
position the writer supports.
We hypothesize that these errors, though orig-
inally intended for thesis clarity scoring, could
be useful for prompt adherence scoring as well.
For instance, an essay that has a Relevance to
Prompt error or an Incomplete Prompt Response
error should intuitively receive a low prompt ad-
herence score. For this reason, we introduce fea-
tures based on these errors to our feature set for
prompt adherence scoring3.
While each of the essays in our data set was pre-
viously annotated with these thesis clarity errors,
in a realistic setting a prompt adherence scoring
system will not have access to these manual error
labels. As a result, we first need to predict which
of these errors is present in each essay. To do this,
we train five maximum entropy classifiers for each
prompt, one for each of the five thesis clarity er-
rors, using MALLET?s (McCallum, 2002) imple-
mentation of maximum entropy classification. In-
stances are presented to classifier for prompt p for
error e in the following way. If a training essay
is written in response to p, it will be used to gen-
3See our website at http://www.hlt.utdallas.
edu/
?
persingq/ICLE/ for the complete list of error an-
notations.
erate a training instance whose label is 1 if e was
annotated for it or 0 otherwise. Since error pre-
diction and prompt adherence scoring are related
problems, the features we associate with this in-
stance are features 1?6 which we have described
earlier in this section. The classifier is then used
to generate probabilities telling us how likely it is
that each test essay has error e.
Then, when training our regressor for prompt
adherence scoring, we add the following features
to our instances. We add a binary feature indicat-
ing the presence or absence of each error. Or in
the case of test essays, the feature takes on a real
value from 0 to 1 indicating how likely the classi-
fier thought it was that the essay had each of the
errors. This results in five additional features, one
for each error.
5 Evaluation
In this section, we evaluate our system for prompt
adherence scoring. All the results we report
are obtained via five-fold cross-validation exper-
iments. In each experiment, we use 3
5
of our la-
beled essays for model training, another 1
5
for pa-
rameter tuning, and the final 1
5
for testing.
5.1 Experimental Setup
5.1.1 Scoring Metrics
We employ four evaluation metrics. As we will see
below, S1, S2, and S3 are error metrics, so lower
scores imply better performance. In contrast, PC
is a correlation metric, so higher correlation im-
plies better performance.
The simplest metric, S1, measures the fre-
quency at which a system predicts the wrong score
out of the seven possible scores. Hence, a system
that predicts the right score only 25% of the time
would receive an S1 score of 0.75.
The S2 metric measures the average distance
between a system?s score and the actual score.
This metric reflects the idea that a system that pre-
dicts scores close to the annotator-assigned scores
should be preferred over a system whose predic-
tions are further off, even if both systems estimate
the correct score at the same frequency.
The S3 metric measures the average square
of the distance between a system?s score predic-
tions and the annotator-assigned scores. The in-
tuition behind this system is that not only should
we prefer a system whose predictions are close
to the annotator scores, but we should also prefer
1539
one whose predictions are not too frequently very
far away from the annotator scores. These three
scores are given by:
1
N
?
A
j
6=E
?
j
1,
1
N
N
?
i=1
|A
j
? E
j
|,
1
N
N
?
i=1
(A
j
? E
j
)
2
where A
j
, E
j
, and E?
j
are the annotator assigned,
system predicted, and rounded system predicted
scores4 respectively for essay j, and N is the num-
ber of essays.
The last metric, PC , computes Pearson?s cor-
relation coefficient between a system?s predicted
scores and the annotator-assigned scores. PC
ranges from ?1 to 1. A positive (negative) PC
implies that the two sets of predictions are posi-
tively (negatively) correlated.
5.1.2 Parameter Tuning
As mentioned earlier, for each prompt p
i
, we train
a linear regressor r
i
using LIBSVM with regular-
ization parameter c
i
. To optimize our system?s
performance on the three error measures described
previously, we use held-out validation data to in-
dependently tune each of the c
i
values5. Note that
each of the c
i
values can be tuned independently
because a c
i
value that is optimal for predicting
scores for p
i
essays with respect to any of the error
performance measures is necessarily also the opti-
mal c
i
when measuring that error on essays from
all prompts. However, this is not case with Pear-
son?s correlation coefficient, as the PC value for
essays from all 13 prompts cannot be simplified as
a weighted sum of the PC values obtained on each
individual prompt. In order to obtain an optimal
result as measured by PC , we jointly tune the c
i
parameters to optimize the PC value achieved by
our system on the same held-out validation data.
However, an exact solution to this optimization
problem is computationally expensive, as there are
too many (713) possible combinations of c values
to exhaustively search. Consequently, we find a
local maximum by employing the simulated an-
4Since our regressor assigns each essay a real value rather
than an actual valid score, it would be difficult to obtain a
reasonable S1 score without rounding the system estimated
score to one of the possible values. For that reason, we round
the estimated score to the nearest of the seven scores the hu-
man annotators were permitted to assign (1.0, 1.5, 2.0, 2.5,
3.0, 3.5, 4.0) only when calculating S1. For other scoring
metrics, we only round the predictions to 1.0 or 4.0 if they
fall outside the 1.0?4.0 range.
5For parameter tuning, we employ the following values.
c
i
may be assigned any of the values 100 101, 102, 103, 104,
10
5
, or 106.
System S1 S2 S3 PC
Baseline .517 .368 .234 .233
Our System .488 .348 .197 .360
Table 4: Five-fold cross-validation results for
prompt adherence scoring.
nealing algorithm (Kirkpatrick et al, 1983), alter-
ing one c
i
value at a time to optimize PC while
holding the remaining parameters fixed.
5.2 Results and Discussion
Five-fold cross-validation results on prompt ad-
herence score prediction are shown in Table 4. On
the first line, this table shows that our baseline sys-
tem, which recall uses only various RI features,
predicts the wrong score 51.7% of the time. Its
predictions are off by an average of .368 points,
and the average squared distance between its pre-
dicted score and the actual score is .234. In addi-
tion, its predicted scores and the actual scores have
a Pearson correlation coefficient of 0.233.
The results from our system, which uses all
seven feature types described in Section 4, are
shown in row 2 of the table. Our system obtains
S1, S2, S3, and PC scores of .488, .348, .197,
and .360 respectively, yielding a significant im-
provement over the baseline with respect to S2,
S3, and PC with p < 0.05, p < 0.01, and p < 0.06
respectively6 . While our system yields improve-
ments by all four measures, its improvement over
the baseline S1 score is not significant. These re-
sults mean that the greatest improvements our sys-
tem makes are that it ensures that our score pre-
dictions are not too often very far away from an
essay?s actual score, as making such predictions
would tend to drive up S3, yielding a relative er-
ror reduction in S3 of 15.8%, and it also ensures
a better correlation between predicted and actual
scores, thus yielding the 16.6% improvement in
PC .
7 It also gives more modest improvements in
how frequently exactly the right score is predicted
(S1) and is better at predicting scores closer to the
actual scores (S2).
5.3 Feature Ablation
To gain insight into how much impact each of the
feature types has on our system, we perform fea-
6All significance tests are paired t-tests.
7These numbers are calculated B?O
B?P
where B is the base-
line system?s score, O is our system?s score, and P is a per-
fect score. Perfect scores for error measures and PC are 0
and 1 respectively.
1540
ture ablation experiments in which we remove the
feature types from our system one-by-one.
Results of the ablation experiments when per-
formed using the four scoring metrics are shown in
Table 5. The top line of each subtable shows what
our system?s score would be if we removed just
one of the feature types from our system. So to see
how our system performs by the S1 metric if we
remove only predicted thesis clarity error features,
we would look at the first row of results of Ta-
ble 5(a) under the column headed by the number 7
since predicted thesis clarity errors are the seventh
feature type introduced in Section 4. The number
here tells us that our system?s S1 score without
this feature type is .502. Since Table 4 shows that
when our system includes this feature type (along
with all the other feature types), it obtains an S1
score of .488, this feature type?s removal costs our
system .014 S1 points, and thus its inclusion has a
beneficial effect on the S1 score.
From row 1 of Table 5(a), we can see that re-
moving feature 4 yields a system with the best S1
score in the presence of the other feature types in
this row. For this reason, we permanently remove
feature 4 from the system before we generate the
results on line 2. Thus, we can see what happens
when we remove both feature 4 and feature 5 by
looking at the second entry in row 2. And since
removing feature 6 harms performance least in the
presence of row 2?s other feature types, we perma-
nently remove both 4 and 6 from our feature set
when we generate the third row of results. We it-
eratively remove the feature type that yields a sys-
tem with the best performance in this way until we
get to the last line, where only one feature type is
used to generate each result.
Since the feature type whose removal yields the
best system is always the rightmost entry in a line,
the order of column headings indicates the rela-
tive importance of the feature types, with the left-
most feature types being most important to per-
formance and the rightmost feature types being
least important in the presence of the other fea-
ture types. This being the case, it is interesting to
note that while the relative importance of differ-
ent feature types does not remain exactly the same
if we measure performance in different ways, we
can see that some feature types tend to be more im-
portant than others in a majority of the four scor-
ing metrics. Features 2 (n-grams), 3 (thesis clarity
keywords), and 6 (manually annotated LDA top-
(a) Results using the S1 metric
3 5 1 7 2 6 4
.527 .502 .512 .502 .511 .500 .488
.527 .502 .512 .501 .513 .500
.525 .508 .505 .505 .504
.513 .527 .520 .513
.523 .520 .506
.541 .527
(b) Results using the S2 metric
2 6 3 1 4 5 7
.356 .350 .348 .350 .349 .348 .348
.351 .349 .348 .348 .348 .347
.351 .349 .348 .348 .347
.350 .349 .348 .348
.358 .351 .349
.362 .352
(c) Results using the S3 metric
2 6 1 5 4 7 3
.221 .201 .197 .197 .197 .197 .196
.215 .201 .197 .196 .196 .196
.212 .203 .199 .197 .196
.212 .203 .199 .197
.212 .203 .199
.223 .204
(d) Results using the PC metric
6 3 2 1 7 5 4
.326 .332 .303 .344 .348 .348 .361
.326 .332 .304 .343 .348 .348
.324 .337 .292 .345 .352
.322 .337 .297 .346
.316 .321 .323
.218 .325
Table 5: Feature ablation results. In each subtable,
the first row shows how our system would perform if each
feature type was removed. We remove the least important
feature type, and show in the next row how the adjusted sys-
tem would perform without each remaining type. For brevity,
a feature type is referred to by its feature number: (1) RI; (2)
n-grams; (3) thesis clarity keywords; (4) prompt adherence
keywords; (5) LDA topics; (6) manually annotated LDA top-
ics; and (7) predicted thesis clarity errors.
ics) tend to be the most important feature types,
as they tend to be the last feature types removed
in the ablation subtables. Features 1 (RI) and 5
(LDA topics) are of middling importance, with
neither ever being removed first or last, and each
tending to have a moderate effect on performance.
Finally, while features 4 (prompt adherence key-
words) and 7 (predicted thesis clarity errors) may
by themselves provide useful information to our
system, in the presence of the other feature types
they tend to be the least important to performance
as they are often the first feature types removed.
While there is a tendency for some feature types
to always be important (or unimportant) regardless
of which scoring metric is used to measure per-
1541
S1 S2 S3 PC
Gold .25 .50 .75 .25 .50 .75 .25 .50 .75 .25 .50 .75
2.0 3.35 3.56 3.79 3.40 3.52 3.73 3.06 3.37 3.64 3.06 3.37 3.64
2.5 3.43 3.63 3.80 3.25 3.52 3.79 3.24 3.45 3.67 3.24 3.46 3.73
3.0 3.64 3.78 3.85 3.56 3.70 3.90 3.52 3.65 3.74 3.52 3.66 3.79
3.5 3.73 3.81 3.88 3.63 3.78 3.90 3.59 3.70 3.81 3.60 3.74 3.85
4.0 3.76 3.84 3.88 3.70 3.83 3.90 3.63 3.75 3.84 3.66 3.78 3.88
Table 6: Regressor scores for our system.
formance, the relative importance of different fea-
ture types does not always remain consistent if we
measure performance in different ways. For ex-
ample, while we identified feature 3 (thesis clar-
ity keywords) as one of the most important fea-
ture types generally due to its tendency to have a
large beneficial impact on performance, when we
are measuring performance using S3, it is the least
useful feature type. Furthermore, its removal in-
creases the S3 score by a small amount, meaning
that its inclusion actually makes our system per-
form worse with respect to S3. Though feature 3 is
an extreme example, all feature types fluctuate in
importance, as we see when we compare their or-
ders of removal among the four ablation subtables.
Hence, it is important to know how performance
is measured when building a system for scoring
prompt adherence.
Feature 3 is not the only feature type whose re-
moval sometimes has a beneficial impact on per-
formance. As we can see in Table 5(b), the re-
moval of features 4, 5, and 7 improves our sys-
tem?s S2 score by .001 points. The same effect
occurs in Table 5(c) when we remove features 4,
7, and 3. These examples illustrate that under
some scoring metrics, the inclusion of some fea-
ture types is actively harmful to performance. For-
tunately, this effect does not occur in any other
cases than the two listed above, as most feature
types usually have a beneficial or at least neutral
impact on our system?s performance.
For those feature types whose effect on perfor-
mance is neutral in the first lines of ablation results
(feature 4 in S1, features 3, 5, and 7 in S2, and fea-
tures 1, 4, 5, and 7 in S3), it is important to note
that their neutrality does not mean that they are
unimportant. It merely means that they do not im-
prove performance in the presence of other feature
types. We can see this is the case by noting that
they are not all the least important feature types in
their respective subtables as indicated by column
order. For example, by the time feature 1 gets per-
manently removed in Table 5(c), its removal harms
performance by .002 S3 points.
5.4 Analysis of Predicted Scores
To more closely examine the behavior of our sys-
tem, in Table 6 we chart the distributions of scores
it predicts for essays having each gold standard
score. As an example of how to read this table,
consider the number 3.06 appearing in row 2.0 in
the .25 column of the S3 region. This means that
25% of the time, when our system with parameters
tuned for optimizing S3 is presented with a test es-
say having a gold standard score of 2.0, it predicts
that the essay has a score less than or equal to 3.06.
From this table, we see that our system has a
strong bias toward predicting more frequent scores
as there are no numbers less than 3.0 in the table,
and about 93.7% of all essays have gold standard
scores of 3.0 or above. Nevertheless, our system
does not rely entirely on bias, as evidenced by the
fact that each column in the table has a tendency
for its scores to ascend as the gold standard score
increases, implying that our system has some suc-
cess at predicting lower scores for essays with
lower gold standard prompt adherence scores.
Another interesting point to note about this ta-
ble is that the difference in error weighting be-
tween the S2 and S3 scoring metrics appears to be
having its desired effect, as every entry in the S3
subtable is less than its corresponding entry in the
S2 subtable due to the greater penalty the S3 met-
ric imposes for predictions that are very far away
from the gold standard scores.
6 Conclusion
We proposed a feature-rich approach to the under-
investigated problem of predicting essay-level
prompt adherence scores on student essays. In an
evaluation on 830 argumentative essays selected
from the ICLE corpus, our system significantly
outperformed a Random Indexing based baseline
by several evaluation metrics. To stimulate further
research on this task, we make all our annotations,
including our prompt adherence scores, the LDA
topic annotations, and the error annotations pub-
licly available.
1542
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with E-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: A library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/
?
cjlin/libsvm.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent smeantic analysis. Jour-
nal of American Society of Information Science,
41(6):391?407.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses universitaires
de Louvain.
Derrick Higgins and Jill Burstein. 2007. Sentence sim-
ilarity measures for essay coherence. In Proceed-
ings of the 7th International Workshop on Computa-
tional Semantics.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects
of coherence in student essays. In Human Lan-
guage Technologies: The 2004 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 185?192.
Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineer-
ing, 12(2):145?159.
David Jurgens and Keith Stevens. 2010. The S-Space
package: An open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of the 22nd An-
nual Conference of the Cognitive Science Society,
pages 103?106.
Scott Kirkpatrick, C. D. Gelatt, and Mario P. Vecchi.
1983. Optimization by simulated annealing. Sci-
ence, 220(4598):671?680.
Thomas K. Landauer and Susan T. Dutnais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Thomas K. Landauer, Darrell Laham, and Peter W.
Foltz. 2003. Automated scoring and annotation of
essays with the Intelligent Essay AssessorTM? In Au-
tomated Essay Scoring: A Cross-Disciplinary Per-
spective, pages 87?112. Lawrence Erlbaum Asso-
ciates, Inc., Mahwah, NJ.
Annie Louis and Derrick Higgins. 2010. Off-topic
essay detection using short prompt texts. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 92?95.
Andrew Kachites McCallum. 2002. MALLET: A
Machine Learning for Language Toolkit. http:
//mallet.cs.umass.edu.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Robert Parker, David Graf, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition. Linguistic Data Consortium, Philadelphia.
Isaac Persing and Vincent Ng. 2013. Modeling the-
sis clarity in student essays. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
260?269.
Isaac Persing, Alan Davis, and Vincent Ng. 2010.
Modeling organization in student essays. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 229?
239.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Semantic
Indexing Workshop at the 7th International Confer-
ence on Terminology and Knowledge Engineering.
Mark D. Shermis and Jill C. Burstein. 2003. Au-
tomated Essay Scoring: A Cross-Disciplinary Per-
spective. Lawrence Erlbaum Associates, Inc., Mah-
wah, NJ.
Mark D. Shermis, Jill Burstein, Derrick Higgins, and
Klaus Zechner. 2010. Automated essay scoring:
Writing assessment and instruction. In International
Encyclopedia of Education (3rd edition). Elsevier,
Oxford, UK.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412?420.
1543
