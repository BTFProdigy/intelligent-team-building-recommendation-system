Integrated Shallow and Deep Parsing: TopP meets HPSG
Anette Frank, Markus Beckerz, Berthold Crysmann, Bernd Kiefer and Ulrich Scha?fer
DFKI GmbH School of Informaticsz
66123 Saarbru?cken, Germany University of Edinburgh, UK
firstname.lastname@dfki.de M.Becker@ed.ac.uk
Abstract
We present a novel, data-driven method
for integrated shallow and deep parsing.
Mediated by an XML-based multi-layer
annotation architecture, we interleave a
robust, but accurate stochastic topological
field parser of German with a constraint-
based HPSG parser. Our annotation-based
method for dovetailing shallow and deep
phrasal constraints is highly flexible, al-
lowing targeted and fine-grained guidance
of constraint-based parsing. We conduct
systematic experiments that demonstrate
substantial performance gains.1
1 Introduction
One of the strong points of deep processing (DNLP)
technology such as HPSG or LFG parsers certainly
lies with the high degree of precision as well as
detailed linguistic analysis these systems are able
to deliver. Although considerable progress has been
made in the area of processing speed, DNLP systems
still cannot rival shallow and medium depth tech-
nologies in terms of throughput and robustness. As
a net effect, the impact of deep parsing technology
on application-oriented NLP is still fairly limited.
With the advent of XML-based hybrid shallow-
deep architectures as presented in (Grover and Las-
carides, 2001; Crysmann et al, 2002; Uszkoreit,
2002) it has become possible to integrate the added
value of deep processing with the performance and
robustness of shallow processing. So far, integration
has largely focused on the lexical level, to improve
upon the most urgent needs in increasing the robust-
ness and coverage of deep parsing systems, namely
1This work was in part supported by a BMBF grant to the
DFKI project WHITEBOARD (FKZ 01 IW 002).
lexical coverage. While integration in (Grover and
Lascarides, 2001) was still restricted to morphologi-
cal and PoS information, (Crysmann et al, 2002) ex-
tended shallow-deep integration at the lexical level
to lexico-semantic information, and named entity
expressions, including multiword expressions.
(Crysmann et al, 2002) assume a vertical,
?pipeline? scenario where shallow NLP tools provide
XML annotations that are used by the DNLP system
as a preprocessing and lexical interface. The per-
spective opened up by a multi-layered, data-centric
architecture is, however, much broader, in that it en-
courages horizontal cross-fertilisation effects among
complementary and/or competing components.
One of the culprits for the relative inefficiency of
DNLP parsers is the high degree of ambiguity found
in large-scale grammars, which can often only be re-
solved within a larger syntactic domain. Within a hy-
brid shallow-deep platform one can take advantage
of partial knowledge provided by shallow parsers to
pre-structure the search space of the deep parser. In
this paper, we will thus complement the efforts made
on the lexical side by integration at the phrasal level.
We will show that this may lead to considerable per-
formance increase for the DNLP component. More
specifically, we combine a probabilistic topological
field parser for German (Becker and Frank, 2002)
with the HPSG parser of (Callmeier, 2000). The
HPSG grammar used is the one originally developed
by (Mu?ller and Kasper, 2000), with significant per-
formance enhancements by B. Crysmann.
In Section 2 we discuss the mapping problem
involved with syntactic integration of shallow and
deep analyses and motivate our choice to combine
the HPSG system with a topological parser. Sec-
tion 3 outlines our basic approach towards syntactic
shallow-deep integration. Section 4 introduces vari-
ous confidence measures, to be used for fine-tuning
of phrasal integration. Sections 5 and 6 report on
experiments and results of integrated shallow-deep
parsing, measuring the effect of various integra-
tion parameters on performance gains for the DNLP
component. Section 7 concludes and discusses pos-
sible extensions, to address robustness issues.
2 Integrated Shallow and Deep Processing
The prime motivation for integrated shallow-deep
processing is to combine the robustness and effi-
ciency of shallow processing with the accuracy and
fine-grainedness of deep processing. Shallow analy-
ses could be used to pre-structure the search space of
a deep parser, enhancing its efficiency. Even if deep
analysis fails, shallow analysis could act as a guide
to select partial analyses from the deep parser?s chart
? enhancing the robustness of deep analysis, and the
informativeness of the combined system.
In this paper, we concentrate on the usage of shal-
low information to increase the efficiency, and po-
tentially the quality, of HPSG parsing. In particu-
lar, we want to use analyses delivered by an effi-
cient shallow parser to pre-structure the search space
of HPSG parsing, thereby enhancing its efficiency,
and guiding deep parsing towards a best-first analy-
sis suggested by shallow analysis constraints.
The search space of an HPSG chart parser can
be effectively constrained by external knowledge
sources if these deliver compatible partial subtrees,
which would then only need to be checked for com-
patibility with constituents derived in deep pars-
ing. Raw constituent span information can be used
to guide the parsing process by penalizing con-
stituents which are incompatible with the precom-
puted ?shape?. Additional information about pro-
posed constituents, such as categorial or featural
constraints, provide further criteria for prioritis-
ing compatible, and penalising incompatible con-
stituents in the deep parser?s chart.
An obvious challenge for our approach is thus to
identify suitable shallow knowledge sources that can
deliver compatible constraints for HPSG parsing.
2.1 The Shallow-Deep Mapping Problem
However, chunks delivered by state-of-the-art shal-
low parsers are not isomorphic to deep syntactic
analyses that explicitly encode phrasal embedding
structures. As a consequence, the boundaries of
deep grammar constituents in (1.a) cannot be pre-
determined on the basis of a shallow chunk analy-
sis (1.b). Moreover, the prevailing greedy bottom-up
processing strategies applied in chunk parsing do not
take into account the macro-structure of sentences.
They are thus easily trapped in cases such as (2).
(1) a. [
CL
There was [
NP
a rumor [
CL
it was going
to be bought by [
NP
a French company [
CL
that
competes in supercomputers]]]]].
b. [
CL
There was [
NP
a rumor]] [
CL
it was going
to be bought by [
NP
a French company]] [
CL
that competes in supercomputers].
(2) Fred eats [
NP
pizza and Mary] drinks wine.
In sum, state-of-the-art chunk parsing does nei-
ther provide sufficient detail, nor the required accu-
racy to act as a ?guide? for deep syntactic analysis.
2.2 Stochastic Topological Parsing
Recently, there is revived interest in shallow anal-
yses that determine the clausal macro-structure of
sentences. The topological field model of (German)
syntax (Ho?hle, 1983) divides basic clauses into dis-
tinct fields ? pre-, middle-, and post-fields ? delim-
ited by verbal or sentential markers, which consti-
tute the left/right sentence brackets. This model of
clause structure is underspecified, or partial as to
non-sentential constituent structure, but provides a
theory-neutral model of sentence macro-structure.
Due to its linguistic underpinning, the topologi-
cal field model provides a pre-partitioning of com-
plex sentences that is (i) highly compatible with
deep syntactic analysis, and thus (ii) maximally ef-
fective to increase parsing efficiency if interleaved
with deep syntactic analysis; (iii) partiality regarding
the constituency of non-sentential material ensures
robustness, coverage, and processing efficiency.
(Becker and Frank, 2002) explored a corpus-
based stochastic approach to topological field pars-
ing, by training a non-lexicalised PCFG on a topo-
logical corpus derived from the NEGRA treebank of
German. Measured on the basis of hand-corrected
PoS-tagged input as provided by the NEGRA tree-
bank, the parser achieves 100% coverage for length
 40 (99.8% for all). Labelled precision and recall
are around 93%. Perfect match (full tree identity) is
about 80% (cf. Table 1, disamb +).
In this paper, the topological parser was provided
a tagger front-end for free text processing, using the
TnT tagger (Brants, 2000). The grammar was ported
to the efficient LoPar parser of (Schmid, 2000). Tag-
ging inaccuracies lead to a drop of 5.1/4.7 percent-
CL-V2
VF-TOPIC LK-VFIN MF RK-VPART NF
ART NN VAFIN ART ADJA NN VAPP CL-SUBCL
Der,1 Zehnkampf,2 ha?tte,3 eine,4 andere,5 Dimension,6 gehabt,7 ,
The decathlon would have a other dimension had LK-COMPL MF RK-VFIN
KOUS PPER PROAV VAPP VAFIN
wenn,9 er,10 dabei,11 gewesen,12 wa?re,13 .
if he there been had .
<TOPO2HPSG type=?root? id=?5608?>
<MAP CONSTR id=?T1? constr=?v2 cp? conf
ent
=?0.87? left=?W1? right=?W13?/>
<MAP CONSTR id=?T2? constr=?v2 vf? conf
ent
=?0.87? left=?W1? right=?W2?/>
<MAP CONSTR id=?T3? constr=?vfronted vfin+rk? conf
ent
=?0.87? left=?W3? right=?W3?/>
<MAP CONSTR id=?T6? constr=?vfronted rk-complex? conf
ent
=?0.87? left=?W7? right=?W7?/>
<MAP CONSTR id=?T4? constr=?vfronted vfin+vp+rk? conf
ent
=?0.87? left=?W3? right=?W13?/>
<MAP CONSTR id=?T5? constr=?vfronted vp+rk? conf
ent
=?0.87? left=?W4? right=?W13?/>
<MAP CONSTR id=?T10? constr=?extrapos rk+nf? conf
ent
=?0.87? left=?W7? right=?W13?/>
<MAP CONSTR id=?T7? constr=?vl cpfin compl? conf
ent
=?0.87? left=?W9? right=?W13?/>
<MAP CONSTR id=?T8? constr=?vl compl vp? conf
ent
=?0.87? left=?W10? right=?W13?/>
<MAP CONSTR id=?T9? constr=?vl rk fin+complex+finlast? conf
ent
=?0.87? left=?W12? right=?W13?/>
</TOPO2HPSG>
Der
D
Zehnkampf
N?
NP-NOM-SG
haette
V
eine
D
andere
AP-ATT
Dimension
N?
N?
NP-ACC-SG
gehabt
V
EPS
wenn
C
er
NP-NOM-SG
dabei
PP
gewesen
V
waere
V-LE
V
V
S
CP-MOD
EPS
EPS
EPS/NP-NOM-SG
S/NP-NOM-SG
S
Figure 1: Topological tree w/param. cat., TOPO2HPSG map-constraints, tree skeleton of HPSG analysis
dis- cove- perfect LP LR 0CB 2CB
amb rage match in % in % in % in %
+ 100.0 80.4 93.4 92.9 92.1 98.9
  99.8 72.1 88.3 88.2 87.8 97.9
Table 1: Disamb: correct (+) / tagger ( ) PoS input.
Eval. on atomic (vs. parameterised) category labels.
age points in LP/LR, and 8.3 percentage points in
perfect match rate (Table 1, disamb  ).
As seen in Figure 1, the topological trees abstract
away from non-sentential constituency ? phrasal
fields MF (middle-field) and VF (pre-field) directly
expand to PoS tags. By contrast, they perfectly ren-
der the clausal skeleton and embedding structure of
complex sentences. In addition, parameterised cate-
gory labels encode larger syntactic contexts, or ?con-
structions?, such as clause type (CL-V2, -SUBCL,
-REL), or inflectional patterns of verbal clusters (RK-
VFIN,-VPART). These properties, along with their
high accuracy rate, make them perfect candidates for
tight integration with deep syntactic analysis.
Moreover, due to the combination of scrambling
and discontinuous verb clusters in German syntax, a
deep parser is confronted with a high degree of local
ambiguity that can only be resolved at the clausal
level. Highly lexicalised frameworks such as HPSG,
however, do not lend themselves naturally to a top-
down parsing strategy. Using topological analyses to
guide the HPSG will thus provide external top-down
information for bottom-up parsing.
3 TopP meets HPSG
Our work aims at integration of topological and
HPSG parsing in a data-centric architecture, where
each component acts independently2 ? in contrast
to the combination of different syntactic formalisms
within a unified parsing process.3 Data-based inte-
gration not only favours modularity, but facilitates
flexible and targeted dovetailing of structures.
3.1 Mapping Topological to HPSG Structures
While structurally similar, topological trees are not
fully isomorphic to HPSG structures. In Figure 1,
e.g., the span from the verb ?ha?tte? to the end of the
sentence forms a constituent in the HPSG analysis,
while in the topological tree the same span is domi-
nated by a sequence of categories: LK, MF, RK, NF.
Yet, due to its linguistic underpinning, the topo-
logical tree can be used to systematically predict
key constituents in the corresponding ?target? HPSG
2See Section 6 for comparison to recent work on integrated
chunk-based and dependency parsing in (Daum et al, 2003).
3As, for example, in (Duchier and Debusmann, 2001).
analysis. We know, for example, that the span from
the fronted verb (LK-VFIN) till the end of its clause
CL-V2 corresponds to an HPSG phrase. Also, the
first position that follows this verb, here the leftmost
daughter of MF, demarcates the left edge of the tra-
ditional VP. Spans of the vorfeld VF and clause cat-
egories CL exactly match HPSG constituents. Cate-
gory CL-V2 tells us that we need to reckon with a
fronted verb in position of its LK daughter, here 3,
while in CL-SUBCL we expect a complementiser in
the position of LK, and a finite verb within the right
verbal complex RK, which spans positions 12 to 13.
In order to communicate such structural con-
straints to the deep parser, we scan the topological
tree for relevant configurations, and extract the span
information for the target HPSG constituents. The
resulting ?map constraints? (Fig. 1) encode a bracket
type name4 that identifies the target constituent and
its left and right boundary, i.e. the concrete span in
the sentence under consideration. The span is en-
coded by the word position index in the input, which
is identical for the two parsing processes.5
In addition to pure constituency constraints, a
skilled grammar writer will be able to associate spe-
cific HPSG grammar constraints ? positive or neg-
ative ? with these bracket types. These additional
constraints will be globally defined, to permit fine-
grained guidance of the parsing process. This and
further information (cf. Section 4) is communicated
to the deep parser by way of an XML interface.
3.2 Annotation-based Integration
In the annotation-based architecture of (Crysmann
et al, 2002), XML-encoded analysis results of all
components are stored in a multi-layer XML chart.
The architecture employed in this paper improves
on (Crysmann et al, 2002) by providing a central
Whiteboard Annotation Transformer (WHAT) that
supports flexible and powerful access to and trans-
formation of XML annotation based on standard
XSLT engines6 (see (Scha?fer, 2003) for more de-
tails on WHAT). Shallow-deep integration is thus
fully annotation driven. Complex XSLT transforma-
tions are applied to the various analyses, in order to
4We currently extract 34 different bracket types.
5We currently assume identical tokenisation, but could ac-
commodate for distinct tokenisation regimes, using map tables.
6Advantages we see in the XSLT approach are (i) minimised
programming effort in the target implementation language for
XML access, (ii) reuse of transformation rules in multiple mod-
ules, (iii) fast integration of new XML-producing components.
extract or combine independent knowledge sources,
including XPath access to information stored in
shallow annotation, complex XSLT transformations
to the output of the topological parser, and extraction
of bracket constraints.
3.3 Shaping the Deep Parser?s Search Space
The HPSG parser is an active bidirectional chart
parser which allows flexible parsing strategies by us-
ing an agenda for the parsing tasks.7 To compute pri-
orities for the tasks, several information sources can
be consulted, e.g. the estimated quality of the parti-
cipating edges or external resources like PoS tagger
results. Object-oriented implementation of the prior-
ity computation facilitates exchange and, moreover,
combination of different ranking strategies. Extend-
ing our current regime that uses PoS tagging for pri-
oritisation,8 we are now utilising phrasal constraints
(brackets) from topological analysis to enhance the
hand-crafted parsing heuristic employed so far.
Conditions for changing default priorities Ev-
ery bracket pair br
x
computed from the topological
analysis comes with a bracket type x that defines its
behaviour in the priority computation. Each bracket
type can be associated with a set of positive and neg-
ative constraints that state a set of permissible or for-
bidden rules and/or feature structure configurations
for the HPSG analysis.
The bracket types fall into three main categories:
left-, right-, and fully matching brackets. A right-
matching bracket may affect the priority of tasks
whose resulting edge will end at the right bracket
of a pair, like, for example, a task that would
combine edges C and F or C and D in Fig. 2.
Left-matching brackets work analogously. For fully
matching brackets, only tasks that produce an edge
that matches the span of the bracket pair can be af-
fected, like, e.g., a task that combines edges B and C
in Fig. 2. If, in addition, specified rule as well as fea-
ture structure constraints hold, the task is rewarded
if they are positive constraints, and penalised if they
are negative ones. All tasks that produce crossing
edges, i.e. where one endpoint lies strictly inside the
bracket pair and the other lies strictly outside, are
penalised, e.g., a task that combines edges A and B.
This behaviour can be implemented efficiently
when we assume that the computation of a task pri-
7A parsing task encodes the possible combination of a pas-
sive and an active chart edge.
8See e.g. (Prins and van Noord, 2001) for related work.
brxbrx
A
B C
D E
F
Figure 2: An example chart with a bracket pair of
type x. The dashed edges are active.
ority takes into account the priorities of the tasks it
builds upon. This guarantees that the effect of chang-
ing one task in the parsing process will propagate
to all depending tasks without having to check the
bracket conditions repeatedly.
For each task, it is sufficient to examine the start-
and endpoints of the building edges to determine if
its priority is affected by some bracket. Only four
cases can occur:
1. The new edge spans a pair of brackets: a match
2. The new edge starts or ends at one of the brack-
ets, but does not match: left or right hit
3. One bracket of a pair is at the joint of the build-
ing edges and a start- or endpoint lies strictly
inside the brackets: a crossing (edges A and B
in Fig. 2)
4. No bracket at the endpoints of both edges: use
the default priority
For left-/right-matching brackets, a match behaves
exactly like the corresponding left or right hit.
Computing the new priority If the priority of a
task is changed, the change is computed relative to
the default priority. We use two alternative confi-
dence values, and a hand-coded parameter (x), to
adjust the impact on the default priority heuristics.
conf
ent
(br
x
) specifies the confidence for a concrete
bracket pair br
x
of type x in a given sentence, based
on the tree entropy of the topological parse. conf
pr
specifies a measure of ?expected accuracy? for each
bracket type. Sec. 4 will introduce these measures.
The priority p(t) of a task t involving a bracket
br
x
is computed from the default priority ~p(t) by:
p(t) = ~p(t)  (1 conf
ent
(br
x
)  conf
pr
(x) (x))
4 Confidence Measures
This way of calculating priorities allows flexible pa-
rameterisation for the integration of bracket con-
straints. While the topological parser?s accuracy is
high, we need to reckon with (partially) wrong anal-
yses that could counter the expected performance
gains. An important factor is therefore the confi-
dence we can have, for any new sentence, into the
best parse delivered by the topological parser: If
confidence is high, we want it to be fully considered
for prioritisation ? if it is low, we want to lower its
impact, or completely ignore the proposed brackets.
We will experiment with two alternative confi-
dence measures: (i) expected accuracy of particular
bracket types extracted from the best parse deliv-
ered, and (ii) tree entropy based on the probability
distribution encountered in a topological parse, as
a measure of the overall accuracy of the best parse
proposed ? and thus the extracted brackets.9
4.1 Conf
pr
: Accuracy of map-constraints
To determine a measure of ?expected accuracy? for
the map constraints, we computed precision and re-
call for the 34 bracket types by comparing the ex-
tracted brackets from the suite of best delivered
topological parses against the brackets we extracted
from the trees in the manually annotated evalua-
tion corpus in (Becker and Frank, 2002). We obtain
88.3% precision, 87.8% recall for brackets extracted
from the best topological parse, run with TnT front
end. We chose precision of extracted bracket types
as a static confidence weight for prioritisation.
Precision figures are distributed as follows: 26.5%
of the bracket types have precision  90% (93.1%
in avg, 53.5% of bracket mass), 50% have pre-
cision  80% (88.9% avg, 77.7% bracket mass).
20.6% have precision  50% (41.26% in avg, 2.7%
bracket mass). For experiments using a threshold
on conf
pr
(x) for bracket type x, we set a threshold
value of 0.7, which excludes 32.35% of the low-
confidence bracket types (and 22.1% bracket mass),
and includes chunk-based brackets (see Section 5).
4.2 Conf
ent
: Entropy of Parse Distribution
While precision over bracket types is a static mea-
sure that is independent from the structural complex-
ity of a particular sentence, tree entropy is defined as
the entropy over the probability distribution of the
set of parsed trees for a given sentence. It is a use-
ful measure to assess how certain the parser is about
the best analysis, e.g. to measure the training utility
value of a data point in the context of sample selec-
tion (Hwa, 2000). We thus employ tree entropy as a
9Further measures are conceivable: We could extract brack-
ets from some n-best topological parses, associating them with
weights, using methods similar to (Carroll and Briscoe, 2002).
10
20
30
40
50
60
70
80
90
00.20.40.60.81
in
 %
Normalized entropy
precision
recall
coverage
Figure 3: Effect of different thresholds of normal-
ized entropy on precision, recall, and coverage
confidence measure for the quality of the best topo-
logical parse, and the extracted bracket constraints.
We carry out an experiment to assess the effect
of varying entropy thresholds  on precision and re-
call of topological parsing, in terms of perfect match
rate, and show a way to determine an optimal value
for . We compute tree entropy over the full prob-
ability distribution, and normalise the values to be
distributed in a range between 0 and 1. The normali-
sation factor is empirically determined as the highest
entropy over all sentences of the training set.10
Experimental setup We randomly split the man-
ually corrected evaluation corpus of (Becker and
Frank, 2002) (for sentence length  40) into a train-
ing set of 600 sentences and a test set of 408 sen-
tences. This yields the following values for the train-
ing set (test set in brackets): initial perfect match
rate is 73.5% (70.0%), LP 88.8% (87.6%), and LR
88.5% (87.8%).11 Coverage is 99.8% for both.
Evaluation measures For the task of identifying
the perfect matches from a set of parses we give the
following standard definitions: precision is the pro-
portion of selected parses that have a perfect match
? thus being the perfect match rate, and recall is the
proportion of perfect matches that the system se-
lected. Coverage is usually defined as the proportion
of attempted analyses with at least one parse. We ex-
tend this definition to treat successful analyses with
a high tree entropy as being out of coverage. Fig. 3
shows the effect of decreasing entropy thresholds
 on precision, recall and coverage. The unfiltered
set of all sentences is found at =1. Lowering  in-
10Possibly higher values in the test set will be clipped to 1.
11Evaluation figures for this experiment are given disregard-
ing parameterisation (and punctuation), corresponding to the
first row of figures in table 1.
82
84
86
88
90
92
94
96
0.160.180.20.220.240.260.280.3
in
 %
Normalized entropy
precision
recall
f-measure
Figure 4: Maximise f-measure on the training set to
determine best entropy threshold
creases precision, and decreases recall and coverage.
We determine f-measure as composite measure of
precision and recall with equal weighting (=0.5).
Results We use f-measure as a target function on
the training set to determine a plausible . F-measure
is maximal at =0.236 with 88.9%, see Figure 4.
Precision and recall are 83.7% and 94.8% resp.
while coverage goes down to 83.0%. Applying the
same  on the test set, we get the following results:
80.5% precision, 93.0% recall. Coverage goes down
to 80.6%. LP is 93.3%, LR is 91.2%.
Confidence Measure We distribute the comple-
ment of the associated tree entropy of a parse tree tr
as a global confidence measure over all brackets br
extracted from that parse: conf
ent
(br) = 1 ent(tr).
For the thresholded version of conf
ent
(br), we set
the threshold to 1   = 1  0:236 = 0:764.
5 Experiments
Experimental Setup In the experiments we use
the subset of the NEGRA corpus (5060 sents,
24.57%) that is currently parsed by the HPSG gram-
mar.12 Average sentence length is 8.94, ignoring
punctuation; average lexical ambiguity is 3.05 en-
tries/word. As baseline, we performed a run with-
out topological information, yet including PoS pri-
oritisation from tagging.13 A series of tests explores
the effects of alternative parameter settings. We fur-
ther test the impact of chunk information. To this
12This test set is different from the corpus used in Section 4.
13In a comparative run without PoS-priorisation, we estab-
lished a speed-up factor of 1.13 towards the baseline used in
our experiment, with a slight increase in coverage (1%). This
compares to a speed-up factor of 2.26 reported in (Daum et al,
2003), by integration of PoS guidance into a dependency parser.
end, phrasal fields determined by topological pars-
ing were fed to the chunk parser of (Skut and Brants,
1998). Extracted NP and PP bracket constraints are
defined as left-matching bracket types, to compen-
sate for the non-embedding structure of chunks.
Chunk brackets are tested in conjunction with topo-
logical brackets, and in isolation, using the labelled
precision value of 71.1% in (Skut and Brants, 1998)
as a uniform confidence weight.14
Measures For all runs we measure the absolute
time and the number of parsing tasks needed to com-
pute the first reading. The times in the individual
runs were normalised according to the number of
executed tasks per second. We noticed that the cov-
erage of some integrated runs decreased by up to
1% of the 5060 test items, with a typical loss of
around 0.5%. To warrant that we are not just trading
coverage for speed, we derived two measures from
the primary data: an upper bound, where we asso-
ciated every unsuccessful parse with the time and
number of tasks used when the limit of 70000 pas-
sive edges was hit, and a lower bound, where we
removed the most expensive parses from each run,
until we reached the same coverage. Whereas the
upper bound is certainly more realistic in an applica-
tion context, the lower bound gives us a worst case
estimate of expectable speed-up.
Integration Parameters We explored the follow-
ing range of weighting parameters for prioritisation
(see Section 3.3 and Table 2).
We use two global settings for the heuristic pa-
rameter . Setting  to 1
2
without using any confi-
dence measure causes the priority of every affected
parsing task to be in- or decreased by half its value.
Setting  to 1 drastically increases the influence of
topological information, the priority for rewarded
tasks is doubled and set to zero for penalized ones.
The first two runs (rows with  P  E) ignore
both confidence parameters (conf
pr=ent
=1), measur-
ing only the effect of higher or lower influence of
topological information. In the remaining six runs,
the impact of the confidence measures conf
pr=ent
is
tested individually, namely +P  E and  P +E, by
setting the resp. alternative value to 1. For two runs,
we set the resp. confidence values that drop below
a certain threshold to zero (PT, ET) to exclude un-
14The experiments were run on a 700 MHz Pentium III ma-
chine. For all runs, the maximum number of passive edges was
set to the comparatively high value of 70000.
factor msec (1st) tasks
low-b up-b low-b up-b low-b up-b
Baseline     524 675 3813 4749
Integration of topological brackets w/ parameters
 P  E  1
2
2.21 2.17 237 310 1851 2353
 P  E 1 2.04 2.10 257 320 2037 2377
+P  E  1
2
2.15 2.21 243 306 1877 2288
PT  E  1
2
2.20 2.30 238 294 1890 2268
 P +E  1
2
2.27 2.23 230 302 1811 2330
 P ET  1
2
2.10 2.00 250 337 1896 2503
+P  E 1 2.06 2.12 255 318 2021 2360
PT  E 1 2.08 2.10 252 321 1941 2346
PT with chunk and topological brackets
PT  E  1
2
2.13 2.16 246 312 1929 2379
PT with chunk brackets only
PT  E  1
2
0.89 1.10 589 611 4102 4234
Table 2: Priority weight parameters and results
certain candidate brackets or bracket types. For runs
including chunk bracketing constraints, we chose
thresholded precision (PT) as confidence weights
for topological and/or chunk brackets.
6 Discussion of Results
Table 2 summarises the results. A high impact on
bracket constraints (1) results in lower perfor-
mance gains than using a moderate impact ( 1
2
)
(rows 2,4,5 vs. 3,8,9). A possible interpretation is
that for high , wrong topological constraints and
strong negative priorities can mislead the parser.
Use of confidence weights yields the best per-
formance gains (with  1
2
), in particular, thresholded
precision of bracket types PT, and tree entropy
+E, with comparable speed-up of factor 2.2/2.3 and
2.27/2.23 (2.25 if averaged). Thresholded entropy
ET yields slightly lower gains. This could be due to
a non-optimal threshold, or the fact that ? while pre-
cision differentiates bracket types in terms of their
confidence, such that only a small number of brack-
ets are weakened ? tree entropy as a global measure
penalizes all brackets for a sentence on an equal ba-
sis, neutralizing positive effects which ? as seen in
+/ P ? may still contribute useful information.
Additional use of chunk brackets (row 10) leads
to a slight decrease, probably due to lower preci-
sion of chunk brackets. Even more, isolated use of
chunk information (row 11) does not yield signifi-
01000
2000
3000
4000
5000
6000
7000
0 5 10 15 20 25 30 35
baseline
+PT ?(0.5)
12867 12520 11620 9290
0
100
200
300
400
500
600
#sentences
msec
Figure 5: Performance gain/loss per sentence length
cant gains over the baseline (0.89/1.1). Similar re-
sults were reported in (Daum et al, 2003) for inte-
gration of chunk- and dependency parsing.15
For PT -E  1
2
, Figure 5 shows substantial per-
formance gains, with some outliers in the range of
length 25?36. 962 sentences (length >3, avg. 11.09)
took longer parse time as compared to the baseline
(with 5% variance margin). For coverage losses, we
isolated two factors: while erroneous topological in-
formation could lead the parser astray, we also found
cases where topological information prevented spu-
rious HPSG parses to surface. This suggests that
the integrated system bears the potential of cross-
validation of different components.
7 Conclusion
We demonstrated that integration of shallow topo-
logical and deep HPSG processing results in signif-
icant performance gains, of factor 2.25?at a high
level of deep parser efficiency. We show that macro-
structural constraints derived from topological pars-
ing improve significantly over chunk-based con-
straints. Fine-grained prioritisation in terms of con-
fidence weights could further improve the results.
Our annotation-based architecture is now easily
extended to address robustness issues beyond lexical
matters. By extracting spans for clausal fragments
from topological parses, in case of deep parsing fail-
15(Daum et al, 2003) report a gain of factor 2.76 relative to a
non-PoS-guided baseline, which reduces to factor 1.21 relative
to a PoS-prioritised baseline, as in our scenario.
ure the chart can be inspected for spanning anal-
yses for sub-sentential fragments. Further, we can
simplify the input sentence, by pruning adjunct sub-
clauses, and trigger reparsing on the pruned input.
References
M. Becker and A. Frank. 2002. A Stochastic Topological
Parser of German. In Proceedings of COLING 2002,
pages 71?77, Taipei, Taiwan.
T. Brants. 2000. Tnt - A Statistical Part-of-Speech Tag-
ger. In Proceedings of Eurospeech, Rhodes, Greece.
U. Callmeier. 2000. PET ? A platform for experimenta-
tion with efficient HPSG processing techniques. Nat-
ural Language Engineering, 6 (1):99 ? 108.
C. Carroll and E. Briscoe. 2002. High precision extrac-
tion of grammatical relations. In Proceedings of COL-
ING 2002, pages 134?140.
B. Crysmann, A. Frank, B. Kiefer, St. Mu?ller, J. Pisko-
rski, U. Scha?fer, M. Siegel, H. Uszkoreit, F. Xu,
M. Becker, and H.-U. Krieger. 2002. An Integrated
Architecture for Deep and Shallow Processing. In
Proceedings of ACL 2002, Pittsburgh.
M. Daum, K.A. Foth, and W. Menzel. 2003. Constraint
Based Integration of Deep and Shallow Parsing Tech-
niques. In Proceedings of EACL 2003, Budapest.
D. Duchier and R. Debusmann. 2001. Topological De-
pendency Trees: A Constraint-based Account of Lin-
ear Precedence. In Proceedings of ACL 2001.
C. Grover and A. Lascarides. 2001. XML-based data
preparation for robust deep parsing. In Proceedings of
ACL/EACL 2001, pages 252?259, Toulouse, France.
T. Ho?hle. 1983. Topologische Felder. Unpublished
manuscript, University of Cologne.
R. Hwa. 2000. Sample selection for statistical gram-
mar induction. In Proceedings of EMNLP/VLC-2000,
pages 45?52, Hong Kong.
S. Mu?ller and W. Kasper. 2000. HPSG analysis of
German. In W. Wahlster, editor, Verbmobil: Founda-
tions of Speech-to-Speech Translation, Artificial Intel-
ligence, pages 238?253. Springer, Berlin.
R. Prins and G. van Noord. 2001. Unsupervised pos-
tagging improves parsing accuracy and parsing effi-
ciency. In Proceedings of IWPT, Beijing.
U. Scha?fer. 2003. WHAT: An XSLT-based Infrastruc-
ture for the Integration of Natural Language Process-
ing Components. In Proceedings of the SEALTS Work-
shop, HLT-NAACL03, Edmonton, Canada.
H. Schmid, 2000. LoPar: Design and Implementation.
IMS, Stuttgart. Arbeitspapiere des SFB 340, Nr. 149.
W. Skut and T. Brants. 1998. Chunk tagger: statistical
recognition of noun phrases. In ESSLLI-1998 Work-
shop on Automated Acquisition of Syntax and Parsing.
H. Uszkoreit. 2002. New Chances for Deep Linguistic
Processing. In Proceedings of COLING 2002, pages
xiv?xxvii, Taipei, Taiwan.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 144?151, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Investigating the Effects of Selective Sampling on the Annotation Task
Ben Hachey, Beatrice Alex and Markus Becker
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, UK
{bhachey,v1balex,s0235256}@inf.ed.ac.uk
Abstract
We report on an active learning experi-
ment for named entity recognition in the
astronomy domain. Active learning has
been shown to reduce the amount of la-
belled data required to train a supervised
learner by selectively sampling more in-
formative data points for human annota-
tion. We inspect double annotation data
from the same domain and quantify poten-
tial problems concerning annotators? per-
formance. For data selectively sampled
according to different selection metrics,
we find lower inter-annotator agreement
and higher per token annotation times.
However, overall results confirm the util-
ity of active learning.
1 Introduction
Supervised training of named entity recognition
(NER) systems requires large amounts of manually
annotated data. However, human annotation is typ-
ically costly and time-consuming. Active learn-
ing promises to reduce this cost by requesting only
those data points for human annotation which are
highly informative. Example informativity can be
estimated by the degree of uncertainty of a single
learner as to the correct label of a data point (Cohn
et al, 1995) or in terms of the disagreement of a
committee of learners (Seung et al, 1992). Ac-
tive learning has been successfully applied to a va-
riety of tasks such as document classification (Mc-
Callum and Nigam, 1998), part-of-speech tagging
(Argamon-Engelson and Dagan, 1999), and parsing
(Thompson et al, 1999).
We employ a committee-based method where the
degree of deviation of different classifiers with re-
spect to their analysis can tell us if an example is
potentially useful. In a companion paper (Becker et
al., 2005), we present active learning experiments
for NER in radio-astronomical texts following this
approach.1 These experiments prove the utility of
selective sampling and suggest that parameters for a
new domain can be optimised in another domain for
which annotated data is already available.
However there are some provisos for active learn-
ing. An important point to consider is what effect
informative examples have on the annotators. Are
these examples more difficult? Will they affect the
annotators? performance in terms of accuracy? Will
they affect the annotators performance in terms of
time? In this paper, we explore these questions us-
ing doubly annotated data. We find that selective
sampling does have an adverse effect on annotator
accuracy and efficiency.
In section 2, we present standard active learn-
ing results showing that good performance can be
achieved using fewer examples than random sam-
pling. Then, in section 3, we address the questions
above, looking at the relationship between inter-
annotator agreement and annotation time and the ex-
amples that are selected by active learning. Finally,
section 4 presents conclusions and future work.
1Please refer to the companion paper for details of the
selective sampling approach with experimental adaptation re-
sults as well as more information about the corpus of radio-
astronomical abstracts.
144
2 Bootstrapping NER
The work reported here was carried out in order to
assess methods of porting a statistical NER system to
a new domain. We started with a NER system trained
on biomedical literature and built a new system to
identify four novel entities in abstracts from astron-
omy articles. This section introduces the Astronomy
Bootstrapping Corpus (ABC) which was developed
for the task, describes our active learning approach
to bootstrapping, and gives a brief overview of the
experiments.
2.1 The Astronomy Bootstrapping Corpus
The ABC corpus consists of abstracts of radio astro-
nomical papers from the NASA Astrophysics Data
System archive2, a digital library for physics, as-
trophysics, and instrumentation. Abstracts were ex-
tracted from the years 1997-2003 that matched the
query ?quasar AND line?. A set of 50 abstracts
from the year 2002 were annotated as seed mate-
rial and 159 abstracts from 2003 were annotated as
testing material. A further 778 abstracts from the
years 1997-2001 were provided as an unannotated
pool for bootstrapping. On average, these abstracts
contain 10 sentences with a length of 30 tokens. The
annotation marks up four entity types:
Instrument-name (IN) Names of telescopes and
other measurement instruments, e.g. Superconduct-
ing Tunnel Junction (STJ) camera, Plateau de Bure
Interferometer, Chandra, XMM-Newton Reflection
Grating Spectrometer (RGS), Hubble Space Tele-
scope.
Source-name (SN) Names of celestial objects,
e.g. NGC 7603, 3C 273, BRI 1335-0417, SDSSp
J104433.04-012502.2, PC0953+ 4749.
Source-type (ST) Types of objects, e.g. Type II Su-
pernovae (SNe II), radio-loud quasar, type 2 QSO,
starburst galaxies, low-luminosity AGNs.
Spectral-feature (SF) Features that can be
pointed to on a spectrum, e.g. Mg II emission, broad
emission lines, radio continuum emission at 1.47
GHz, CO ladder from (2-1) up to (7-6), non-LTE
line.
2http://adsabs.harvard.edu/preprint_
service.html
The seed and test data sets were annotated by two
astrophysics PhD students. In addition, they anno-
tated 1000 randomly sampled sentences from the
pool to provide a random baseline for active learn-
ing. These sentences were doubly annotated and ad-
judicated and form the basis for our calculations in
section 3.
2.2 Inter-Annotator Agreement
In order to ensure consistency in annotation projects,
corpora are often annotated by more than one an-
notator, e.g. in the annotation of the Penn Treebank
(Marcus et al, 1994). In these cases, inter-annotator
agreement is frequently reported between different
annotated versions of a corpus as an indicator for
the difficulty of the annotation task. For example,
Brants (2000) reports inter-annotator agreement in
terms of accuracy and f-score for the annotation of
the German NEGRA treebank.
Evaluation metrics for named entity recognition
are standardly reported as accuracy on the token
level, and as f-score on the phrasal level, e.g.
Sang (2002), where token level annotation refers to
the B-I-O coding scheme.3 Likewise, we will use
accuracy to report inter-annotator agreement on the
token level, and f-score for the phrase level. We
may arbitrarily assign one annotator?s data as the
gold standard, since both accuracy and f-score are
symmetric with respect to the test and gold set. To
see why this is the case, note that accuracy can sim-
ply be defined as the ratio of the number of tokens
on which the annotators agree over the total number
of tokens. Also the f-score is symmetric, since re-
call(A,B) = precision(B,A) and (balanced) f-score is
the harmonic mean of recall and precision (Brants,
2000). The pairwise f-score for the ABC corpus is
85.52 (accuracy of 97.15) with class information and
86.15 (accuracy of 97.28) without class information.
The results in later sections will be reported using
this pairwise f-score for measuring agreement.
For NER, it is also common to compare an anno-
tator?s tagged document to the final, reconciled ver-
sion of the document, e.g. Robinson et al (1999)
and Strassel et al (2003). The inter-annotator f-
score agreement calculated this way for MUC-7 and
Hub 4 was measured at 97 and 98 respectively. The
3B-X marks the beginning of a phrase of type X, I-X denotes
the continuation of an X phrase, and O a non-phrasal token.
145
doubly annotated data for the ABC corpus was re-
solved by the original annotators in the presence
of an astronomy adjudicator (senior academic staff)
and a computational linguist. This approach gives
an f-score of 91.89 (accuracy of 98.43) with class
information for the ABC corpus. Without class in-
formation, we get an f-score of 92.22 (accuracy of
98.49), indicating that most of our errors are due to
boundary problems. These numbers suggest that our
task is more difficult than the generic NER tasks from
the MUC and HUB evaluations.
Another common agreement metric is the kappa
coefficient which normalises token level accuracy
by chance, e.g. Carletta et al (1997). This met-
ric showed that the human annotators distinguish
the four categories with a reproducibility of K=.925
(N=44775, k=2; where K is the kappa coefficient,
N is the number of tokens and k is the number of
annotators).
2.3 Active Learning
We have already mentioned that there are two main
approaches in the literature to assessing the informa-
tivity of an example: the degree of uncertainty of a
single learner and the disagreement between a com-
mittee of learners. For the current work, we employ
query-by-committee (QBC). We use a conditional
Markov model (CMM) tagger (Klein et al, 2003;
Finkel et al, 2005) to train two different models on
the same data by splitting the feature set. In this sec-
tion we discuss several parameters of this approach
for the current task.
Level of annotation For the manual annotation of
named entity examples, we needed to decide on the
level of granularity. The question arises of what con-
stitutes an example that will be submitted to the an-
notators. Possible levels include the document level,
the sentence level and the token level. The most fine-
grained annotation would certainly be on the token
level. However, it seems unnatural for the annota-
tor to label individual tokens. Furthermore, our ma-
chine learning tool models sequences at the sentence
level and does not allow to mix unannotated tokens
with annotated ones. At the other extreme, one may
submit an entire document for annotation. A possi-
ble disadvantage is that a document with some inter-
esting parts may well contain large portions with re-
dundant, already known structures for which know-
ing the manual annotation may not be very useful.
In the given setting, we decided that the best granu-
larity is the sentence.
Sample Selection Metric There are a variety of
metrics that could be used to quantify the degree
of deviation between classifiers in a committee (e.g.
KL-divergence, information radius, f-measure). The
work reported here uses two sentence-level met-
rics based on KL-divergence and one based on f-
measure.
KL-divergence has been used for active learning
to quantify the disagreement of classifiers over the
probability distribution of output labels (McCallum
and Nigam, 1998; Jones et al, 2003). It measures
the divergence between two probability distributions
p and q over the same event space ?:
D(p||q) =
?
x??
p(x) log
p(x)
q(x)
(1)
KL-divergence is a non-negative metric. It is zero
for identical distributions; the more different the two
distributions, the higher the KL-divergence. Intu-
itively, a high KL-divergence score indicates an in-
formative data point. However, in the current formu-
lation, KL-divergence only relates to individual to-
kens. In order to turn this into a sentence score, we
need to combine the individual KL-divergences for
the tokens within a sentence into one single score.
We employed mean and max.
The f-complement has been suggested for active
learning in the context of NP chunking as a struc-
tural comparison between the different analyses of
a committee (Ngai and Yarowsky, 2000). It is the
pairwise f-measure comparison between the multi-
ple analyses for a given sentence:
fMcomp =
1
2
?
M,M ??M
(1? F1(M(t),M
?(t))) (2)
where F1 is the balanced f-measure of M(t) and
M ?(t), the preferred analyses of data point t accord-
ing to different members M,M ? of ensemble M.
We take the complement so that it is oriented the
same as KL-divergence with high values indicating
high disagreement. This is equivalent to taking the
inter-annotator agreement between |M| classifiers.
146
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 10000  15000  20000  25000  30000  35000  40000  45000
F-
sc
or
e
Number of Tokens in Training Data
Ave KL-divergence
Random sampling
Figure 1: Learning curve of the real AL experiment.
2.4 Experiments
To tune the active learning parameters discussed
in section 2.3, we ran detailed simulated experi-
ments on the named entity data from the BioNLP
shared task of the COLING 2004 International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (Kim et al, 2004).
These results are treated in detail in the companion
paper (Becker et al, 2005).
We used the CMM tagger to train two different
models by splitting the feature set to give multiple
views of the same data. The feature set was hand-
crafted such that it comprises different views while
empirically ensuring that performance is sufficiently
similar. On the basis of the findings of the simulation
experiments we set up the real active learning anno-
tation experiment using: average KL-divergence as
the selection metric and a feature split that divides
the full feature set roughly into features of words
and features derived from external resources. As
smaller batch sizes require more retraining iterations
and larger batch sizes increase the amount of anno-
tation necessary at each round and could lead to un-
necessary strain for the annotators, we settled on a
batch size of 50 sentences for the real AL experi-
ment as a compromise between computational cost
and work load for the annotator.
We developed an active annotation tool and ran
real annotation experiments on the astronomy ab-
stracts described in section 2.1. The tool was given
to the same astronomy PhD students for annotation
who were responsible for the seed and test data. The
learning curve for selective sampling is plotted in
figure 1.4 The randomly sampled data was dou-
bly annotated and the learning curve is averaged be-
tween the two annotators.
Comparing the selective sampling performance to
the baseline, we confirm that active learning pro-
vides a significant reduction in the number of exam-
ples that need annotating. In fact, the random curve
reaches an f-score of 76 after approximately 39000
tokens have been annotated while the selective sam-
pling curve reaches this level of performance after
only ? 24000 tokens. This represents a substantial
reduction in tokens annotated of 38.5%. In addition,
at 39000 tokens, selectively sampling offers an error
reduction of 21.4% with a 3 point improvement in
f-score.
3 Evaluating Selective Sampling
Standardly, the evaluation of active learning meth-
ods and the comparison of sample selection metrics
draws on experiments over gold-standard annotated
corpora, where a set of annotated data is at our dis-
posal, e.g. McCallum and Nigam (1998), Osborne
and Baldridge (2004). This assumes implicitly that
annotators will always produce gold-standard qual-
ity annotations, which is typically not the case, as we
discussed in Section 2.2. What is more, we speculate
that annotators might have an even higher error rate
on the supposedly more informative, but possibly
also more difficult examples. However, this would
not be reflected in the carefully annotated and veri-
fied examples of a gold standard corpus. In the fol-
lowing analysis, we leverage information from dou-
bly annotated data to explore the effects on annota-
tion of selectively sampled examples.
To evaluate the practicality and usefulness of ac-
tive learning as a generally applicable methodology,
it is desirable to be able to observe the behaviour
of the annotators. In this section, we will report on
the evaluation of various subsets of the doubly an-
notated portion of the ABC corpus comprising 1000
sentences, which we sample according to a sample
selection metric. That is, examples are added to the
subsets according to the sample selection metric, se-
lecting those with higher disagreement first. This
allows us to trace changes in inter-annotator agree-
4Learning curves reflect the performance on the test set us-
ing the full feature set.
147
ment between the full corpus and selected subsets
thereof. Also, we will inspect timing information.
This novel methodology allows us to experiment
with different sample selection metrics without hav-
ing to repeat the actual time and resource intensive
annotation.
3.1 Error Analysis
To investigate the types of classification errors, it is
common to set up a confusion matrix. One approach
is to do this at the token level. However, we are deal-
ing with phrases and our analysis should reflect that.
Thus we devised a method for constructing a confu-
sion matrix based on phrasal alignment. These con-
fusion matrices are constructed by giving a double
count for each phrase that has matching boundaries
and a single count for each phrase that does not have
matching boundaries. To illustrate, consider the fol-
lowing sentences?annotated with phrases A, B, and
C for annotator 1 on top and annotator 2 on bottom?
as sentence 1 and sentence 2 respectively:A A
BA CA
BA C
A
Sentence 1 will get a count of 2 for A/A and for
A/B and a count of 1 for O/C, while sentence 2
will get 2 counts of A/O, and 1 count each of O/A,
O/B, and O/C. Table 1 contains confusion matrices
for the first 100 sentences sorted by averaged KL-
divergence and for the full set of 1000 random sen-
tences from the pool data. (Note that these confusion
matrices contain percentages instead of raw counts
so they can be directly compared.)
We can make some interesting observations look-
ing at these phrasal confusion matrices. The main
effect we observed is the same as was suggested by
the f-score inter-annotator agreement errors in sec-
tion 2.1. Specifically, looking at the full random set
of 1000 sentences, almost all errors (Where ? is any
entity phrase type, ?/O + O/? errorsall errors = 95.43%) are
due to problems with phrase boundaries. Compar-
ing the full random set to the 100 sentences with
the highest averaged KL-divergence, we can see that
this is even more the case for the sub-set of 100 sen-
tences (97.43%). Therefore, we can observe that
100: A2
IN SN ST SF O
IN 12.0 0.0 0.0 0.0 0.4
SN 0.0 10.4 0.0 0.0 0.4
A1 ST 0.0 0.4 30.3 0.0 1.0
SF 0.0 0.0 0.0 31.1 3.9
O 0.2 0.4 2.9 6.4 ?
1000: A2
IN SN ST SF O
IN 9.4 0.0 0.0 0.0 0.3
SN 0.0 10.1 0.2 0.1 0.3
A1 ST 0.0 0.1 41.9 0.1 1.6
SF 0.0 0.0 0.1 25.1 3.0
O 0.3 0.2 2.4 4.8 ?
Table 1: Phrasal confusion matrices for document
sub-set of 100 sentences sorted by average KL-
divergence and for full random document sub-set of
1000 sentences (A1: Annotator 1, A2: Annotator 2).
Entity 100 1000
Instrument-name 12.4% 9.7%
Source-name 10.8% 10.7%
Source-type 31.7% 43.7%
Spectral-feature 35.0% 28.2%
O 9.9% 7.7%
Table 2: Normalised distributions of agreed entity
annotations.
there is a tendency for the averaged KL-divergence
selection metric to choose sentences where phrase
boundary identification is difficult.
Furthermore, comparing the confusion matrices
for 100 sentences and for the full set of 1000 shows
that sentences containing less common entity types
tend to be selected first while sentences containing
the most common entity types are dispreferred. Ta-
ble 2 contains the marginal distribution for annotator
1 (A1) from the confusion matrices for the ordered
sub-set of 100 and for the full random set of 1000
sentences. So, for example, the sorted sub-set con-
tains 12.4% Instrument-name annotations (the
least common entity type) while the full set con-
tains 9.7%. And, 31.7% of agreed entity annota-
tions in the first sub-set of 100 are Source-type
(the most common entity type), whereas the propor-
148
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 0.97
 0.98
 0  5000  10000  15000  20000  25000  30000
In
te
r-a
nn
ot
at
or
 A
gr
ee
m
en
t (A
cc
)
Size (Tokens) of KL-sorted Document Subset
KL-divergence
Figure 2: Raw agreement plotted against KL-sorted
document subsets.
tion of agreed Source-type annotations in the
full random set is 43.7%. Looking at the O row, we
also observe that sentences with difficult phrases are
preferred. A similar effect can be observed in the
marginals for annotator 2.
3.2 Annotator Performance
So far, the behaviour we have observed is what you
would expect from selective sampling; there is a
marked improvement in terms of cost and error rate
reduction over random sampling. However, selec-
tive sampling raises questions of cognitive load and
the quality of annotation. In the following section
we investigate the relationship between informativ-
ity, inter-annotator agreement, and annotation time.
While reusability of selective samples for other
learning algorithms has been explored (Baldridge
and Osborne, 2004), no effort has been made to
quantify the effect of selective sampling on anno-
tator performance. We concentrate first on the ques-
tion: Are informative examples more difficult to an-
notate? One way to quantify this effect is to look
at the correlation between human agreement and the
token-level KL-divergence. The Pearson correlation
coefficient indicates the degree to which two vari-
ables are related. It ranges between ?1 and 1, where
1 means perfectly positive correlation, and ?1 per-
fectly negative correlation. A value of 0 indicates no
correlation. The Pearson correlation coefficient on
all tokens gives a very weak correlation coefficient
of ?0.009.5 However, this includes many trivial to-
5In order to make this calculation, we give token-level agree-
 0.76
 0.77
 0.78
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 0.85
 0.86
 100  200  300  400  500  600  700  800  900  1000
In
te
r-a
nn
ot
at
or
 A
gr
ee
m
en
t (F
)
Size (Sents) of Selection Metric-sorted Subset
Ave KL-divergence
Max KL-divergence
F-complement
Figure 3: Human disagreement plotted against se-
lection metric-sorted document subsets.
kens which are easily identified as being outside an
entity phrase. If we look just at tokens that at least
one of the annotators posits as being part of an en-
tity phrase, we observe a larger effect with a Pear-
son correlation coefficient of ?0.120, indicating that
agreement tends to be low when KL-divergence is
high. Figure 2 illustrates this effect even more dra-
matically. Here we plot accuracy against token sub-
sets of size 1000, 2000, .., N where tokens are added
to the subsets according to their KL-divergence, se-
lecting those with the highest values first. This
demonstrates clearly that tokens with higher KL-
divergence have lower inter-annotator agreement.
However, as discussed in sections 2.3 and 2.4,
we decided on sentences as the preferred annota-
tion level. Therefore, it is important to explore these
relationships at the sentence level as well. Again,
we start by looking at the Pearson correlation coeffi-
cient between f-score inter-annotator agreement (as
described in section 2.1) and our active learning se-
lection metrics:
Ave KL Max KL 1-F
All Tokens ?0.090 ?0.145 ?0.143
O Removed ?0.042 ?0.092 ?0.101
Here O Removed means that sentences are removed
for which the annotators agree that there are no en-
tity phrases (i.e. all tokens are labelled as being
outside an entity phrase). This shows a relation-
ment a numeric representation by assigning 1 to tokens on
which the annotators agree and 0 to tokens on which they dis-
agree.
149
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 100  200  300  400  500  600  700  800  900  1000
Av
er
ag
e 
tim
e 
pe
r t
ok
en
Size (Sents) of Selection Metric-sorted Subset
Ave KL-divergence
Max KL-divergence
F-complement
Figure 4: Annotation time plotted against selection
metric-sorted document subsets.
ship very similar to what we observed at the token
level: a negative correlation indicating that agree-
ment is low when KL-divergence is high. Again,
the effect of selecting informative examples is better
illustrated with a plot. Figure 3 plots f-score agree-
ment against sentence subsets sorted by our sentence
level selection metrics. Lower agreement at the left
of these plots indicates that the more informative ex-
amples according to our selection metrics are more
difficult to annotate.
So, active learning makes the annotation more dif-
ficult. But, this raises a further question: What effect
do more difficult examples have on annotation time?
To investigate this, we once again start by looking
at the Pearson correlation coefficient, this time be-
tween the annotation time and our selection metrics.
However, as our sentence-level selection metrics af-
fect the length of sentences selected, we normalise
sentence-level annotation times by sentence length:
Ave KL Max KL 1-F
All Tokens 0.157 ?0.009 0.082
O Removed 0.216 ?0.007 0.106
Here we see a small positive correlations for av-
eraged KL-divergence and f-complement indicating
that sentences that score higher according to our se-
lection metrics do generally take longer to annotate.
Again, we can visualise this effect better by plotting
average time against KL-sorted subsets (Figure 4).
This demonstrates that sentences preferred by our
selection metrics generally take longer to annotate.
4 Conclusions and Future Work
We have presented active learning experiments in
a novel NER domain and investigated negative side
effects. We investigated the relationship between
informativity of an example, as determined by se-
lective sampling metrics, and inter-annotator agree-
ment. This effect has been quantified using the Pear-
son correlation coefficient and visualised using plots
that illustrate the difficulty and time-intensiveness of
examples chosen first by selective sampling. These
measurements clearly demonstrate that selectively
sampled examples are in fact more difficult to anno-
tate. And, while sentence length and entities per sen-
tence are somewhat confounding factors, we have
also shown that selective sampling of informative
examples appears to increase the time spent on in-
dividual examples.
High quality annotation is important for building
accurate models and for reusability. While anno-
tation quality suffers for selectively sampled exam-
ples, selective sampling nevertheless provided a dra-
matic cost reduction of 38.5% in a real annotation
experiment, demonstrating the utility of active learn-
ing for bootstrapping NER in a new domain.
In future work, we will perform further investi-
gations of the cost of resolving annotations for se-
lectively sampled examples. And, in related work,
we will use timing information to assess token, en-
tity and sentence cost metrics for annotation. This
should also lead to a better understanding of the re-
lationship between timing information and sentence
length for different selection metrics.
Acknowledgements
The work reported here, including the related de-
velopment of the astronomy bootstrapping corpus
and the active learning tools, were supported by
Edinburgh-Stanford Link Grant (R36759) as part of
the SEER project. We are very grateful for the time
and resources invested in corpus preparation by our
collaborators in the Institute for Astronomy, Univer-
sity of Edinburgh: Rachel Dowsett, Olivia Johnson
and Bob Mann. We are also grateful to Melissa Kro-
nenthal and Jean Carletta for help collecting data.
150
References
Shlomo Argamon-Engelson and Ido Dagan. 1999.
Committee-based sample selection for probabilistic
classifiers. Journal of Artificial Intelligence Research,
11:335?360.
Jason Baldridge and Miles Osborne. 2004. Ensemble-
based active learning for parse selection. In Pro-
ceedings of the 5th Conference of the North American
Chapter of the Association for Computational Linguis-
tics.
Markus Becker, Ben Hachey, Beatrice Alex, and Claire
Grover. 2005. Optimising selective sampling for boot-
strapping named entity recognition. In ICML-2005
Workshop on Learning with Multiple Views.
Thorsten Brants. 2000. Inter-annotator agreement for a
German newspaper corpus. In Proceedings of the 2nd
International Conference on Language Resources and
Evaluation (LREC-2000).
Jean Carletta, Amy Isard, Stephen Isard, Jacqueline C.
Kowtko, Gwyneth Doherty-Sneddon, and Anne H.
Anderson. 1997. The reliability of a dialogue
structure coding scheme. Computational Linguistics,
23(1):13?31.
David. A. Cohn, Zoubin. Ghahramani, and Michael. I.
Jordan. 1995. Active learning with statistical mod-
els. In G. Tesauro, D. Touretzky, and T. Leen, editors,
Advances in Neural Information Processing Systems,
volume 7, pages 705?712. The MIT Press.
Jenny Finkel, Shipra Dingare, Christopher Manning,
Beatrice Alex Malvina Nissim, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics. In press.
Rosie Jones, Rayid Ghani, Tom Mitchell, and Ellen
Riloff. 2003. Active learning with multiple view fea-
ture sets. In ECML 2003 Workshop on Adaptive Text
Extraction and Mining.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proceedings of the COLING 2004 International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings the Sev-
enth Conference on Natural Language Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330.
Andrew McCallum and Kamal Nigam. 1998. Employing
EM and pool-based active learning for text classifica-
tion. In Proceedings of the 15th International Confer-
ence on Machine Learning.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics.
Patricia Robinson, Erica Brown, John Burger, Nancy
Chinchor, Aaron Douthat, Lisa Ferro, and Lynette
Hirschman. 1999. Overview: Information extraction
from broadcast news. In Proceedings DARPA Broad-
cast News Workshop.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the
2002 Conference on Computational Natural Language
Learning.
H. Sebastian Seung, Manfred Opper, and Haim Som-
polinsky. 1992. Query by committee. In Computa-
tional Learning Theory.
Stephanie Strassel, Alexis Mitchell, and Shudong Huang.
2003. Multilingual resources for entity extraction. In
Proceedings of the ACL 2003 Workshop on Multilin-
gual and Mixed-language Named Entity Recognition.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of the 16th International Conference on Ma-
chine Learning.
151
  	
An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ 