A Hybrid Feature Set based Maximum Entropy Hindi Named Entity
Recognition
Sujan Kumar Saha
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
sujan.kr.saha@gmail.com
Sudeshna Sarkar
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
shudeshna@gmail.com
Pabitra Mitra
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
pabitra@gmail.com
Abstract
We describe our effort in developing a
Named Entity Recognition (NER) system
for Hindi using Maximum Entropy (Max-
Ent) approach. We developed a NER an-
notated corpora for the purpose. We have
tried to identify the most relevant features
for Hindi NER task to enable us to develop
an efficient NER from the limited corpora
developed. Apart from the orthographic and
collocation features, we have experimented
on the efficiency of using gazetteer lists as
features. We also worked on semi-automatic
induction of context patterns and experi-
mented with using these as features of the
MaxEnt method. We have evaluated the per-
formance of the system against a blind test
set having 4 classes - Person, Organization,
Location and Date. Our system achieved a
f-value of 81.52%.
1 Introduction
Named Entity Recognition involves locating and
classifying the names in text. NER is an important
task, having applications in Information Extraction
(IE), question answering, machine translation and in
most other NLP applications.
NER systems have been developed for English
and few other languages with high accuracies. These
systems take advantage of large amount of Named
Entity (NE) annotated corpora and other NER re-
sources. However when we started working on a
NER system for Hindi, we did not have any NER
annotated corpora for Hindi, neither did we have ac-
cess to any comprehensive gazetteer list.
In this work we have identified suitable features
for the Hindi NER task. Orthography features, the
suffix and prefix information, as well as information
about the sorrounding words and their tags are used
to develop a Maximum Entropy (MaxEnt) based
Hindi NER system. Additionally, we have acquired
gazetteer lists for Hindi and used these gazetteers in
the Maximum Entropy (MaxEnt) based Hindi NER
system. We also worked on semi-automatically
learning of context pattern for identifying names.
These context pattern rules have been integrated into
the MaxEnt based NER system, leading to a high ac-
curacy.
The paper is organized as follows. A brief survey
of different techniques used for the NER task in dif-
ferent languages and domains are presented in Sec-
tion 2. The MaxEnt based NER system is described
in Section 3. Various features used in NER are then
discussed. Next we present the experimental results
and related discussions. Finally Section 8 concludes
the paper.
2 Previous Work
A variety of techniques has been used for NER. The
two major approaches to NER are:
1. Linguistic approaches.
2. Machine Learning based approaches.
The linguistic approaches typically use rules man-
ually written by linguists. There are several rule-
based NER systems, containing mainly lexicalized
343
grammar, gazetteer lists, and list of trigger words,
which are capable of providing 88%-92% f-measure
accuracy for English (Grishman, 1995; McDonald,
1996; Wakao et al, 1996).
The main disadvantages of these rule-based tech-
niques are that these require huge experience and
grammatical knowledge of the particular language
or domain and these systems are not transferable to
other languages or domains.
Machine Learning (ML) based techniques for
NER make use of a large amount of NE anno-
tated training data to acquire high level language
knowledge. Several ML techniques have been suc-
cessfully used for the NER task of which Hidden
Markov Model (Bikel et al, 1997), Maximum En-
tropy (Borthwick, 1999), Conditional Random Field
(Li and Mccallum, 2004) are most common. Com-
binations of different ML approaches are also used.
Srihari et al (2000) combines Maximum Entropy,
Hidden Markov Model and handcrafted rules to
build an NER system.
NER systems use gazetteer lists for identifying
names. Both the linguistic approach (Grishman,
1995; Wakao et al, 1996) and the ML based ap-
proach (Borthwick, 1999; Srihari et al, 2000) use
gazetteer lists.
The linguistic approach uses hand-crafted rules
which needs skilled linguistics. Some recent ap-
proaches try to learn context patterns through ML
which reduce amount of manual labour. Talukder et
al.(2006) combined grammatical and statistical tech-
niques to create high precision patterns specific for
NE extraction. An approach to lexical pattern learn-
ing for Indian languages is described by Ekbal and
Bandopadhyay (2007). They used seed data and an-
notated corpus to find the patterns for NER.
The NER task for Hindi has been explored by
Cucerzan and Yarowsky in their language indepen-
dent NER work which used morphological and con-
textual evidences (Cucerzan and Yarowsky, 1999).
They ran their experiment with 5 languages - Roma-
nian, English, Greek, Turkish and Hindi. Among
these the accuracy for Hindi was the worst. For
Hindi the system achieved 41.70% f-value with a
very low recall of 27.84% and about 85% preci-
sion. A more successful Hindi NER system was
developed by Wei Li and Andrew Mccallum (2004)
using Conditional Random Fields (CRFs) with fea-
ture induction. They were able to achieve 71.50%
f-value using a training set of size 340k words. In
Hindi the maximum accuracy is achieved by (Kumar
and Bhattacharyya, 2006). Their Maximum Entropy
Markov Model (MEMM) based model gives 79.7%
f-value.
3 Maximum Entropy Based Model
We have used a Maximum Entropy model to build
the NER in Hindi. MaxEnt is a flexible statistical
model which assigns an outcome for each token
based on its history and features. MaxEnt computes
the probability p(o|h) for any o from the space of
all possible outcomes O, and for every h from the
space of all possible histories H . A history is all
the conditioning data that enables one to assign
probabilities to the space of outcomes. In NER,
history can be viewed as all information derivable
from the training corpus relative to the current
token. The computation of p(o|h) in MaxEnt
depends on a set of features, which are helpful in
making predictions about the outcome. The features
may be binary-valued or multi-valued. For instance,
one of our features is: the current token is a part
of the surname list; how likely is it to be part of
a person name. Formally, we can represent this
feature as follows:
f(h, o) =
{
1 if wi in surname list and o = person
0 otherwise
(1)
Given a set of features and a training corpus,
the MaxEnt estimation process produces a model
in which every feature fi has a weight ?i. We can
compute the conditional probability as (Pietra et al,
1997):
p(o|h) = 1Z(h)
?
i
?ifi(h,o) (2)
Z(h) =
?
o
?
i
?ifi(h,o) (3)
So the conditional probability of the outcome is
the product of the weights of all active features, nor-
malized over the products of all the features. For
our development we have used a Java based open-
nlp MaxEnt toolkit1 to get the probability values of
1www.maxent.sourceforge.net.
344
a word belonging to each class. That is, given a se-
quence of words, the probability of each class is ob-
tained for each word. To find the most probable tag
corresponding to each word of a sequence, we can
choose the tag having the highest class conditional
probability value. But this method is not good as it
might result in an inadmissible output tag.
Some tag sequences should never happen. To
eliminate these inadmissible sequences we have
made some restrictions. Then we used a beam
search algorithm with a beam of length 3 with these
restrictions.
The training data for this task is composed of
about 243K words which is collected from the
popular daily Hindi newspaper ?Dainik Jagaran?.
This corpus has been manually annotated and has
about 16,482 NEs. In this development we have
considered 4 types of NEs, these are Person(P),
Location(L), Organization(O) and Date(D). To
recognize entity boundaries each name class N
is subdivided into 4 sub-classes, i.e., N Begin,
N Continue, N End, and N Unique. Hence,
there are a total of 17 classes including 1 class for
not-name. The corpus contains 6, 298 Person, 4, 696
Location, 3, 652 Organization and 1, 845 Date enti-
ties.
4 Features for Hindi NER
Machine learning approaches like MaxEnt, CRF etc.
make use of different features for identifying the
NEs. Orthographic features (like capitalization, dec-
imal, digits), affixes, left and right context (like pre-
vious and next words), NE specific trigger words,
gazetteer features, POS and morphological features
etc. are generally used for NER. In English and
some other languages, capitalization features play
an important role as NEs are generally capitalized
for these languages. Unfortunately this feature is not
applicable for Hindi. Also Indian person names are
more diverse, lots of common words having other
meanings are also used as person names. These
make difficult to develop a NER system on Hindi.
Li and Mccallum (2004) used the entire word text,
character n-grams (n = 2, 3, 4), word prefix and suf-
fix of lengths 2, 3 and 4, and 24 Hindi gazetteer lists
as atomic features in their Hindi NER. Kumar and
Bhattacharyya (2006) used word features (suffixes,
digits, special characters), context features, dictio-
nary features, NE list features etc. in their MEMM
based Hindi NER system. In the following we have
discussed about the features we have identified and
used to develop the Hindi NER system.
4.1 Feature Description
The features which we have identified for Hindi
Named Entity Recognition are:
Static Word Feature: The previous and next
words of a particular word are used as features. The
previous m words (wi?m...wi?1) to next n words
(wi+1...wi+n) can be treated. During our experi-
ment different combinations of previous 4 to next
4 words are used.
Context Lists: Context words are defined as the
frequent words present in a word window for a par-
ticular class. We compiled a list of the most frequent
words that occur within a window of wi?3...wi+3
of every NE class. For example, location con-
text list contains the words like ?jAkara2? (go-
ing to), ?desha? (country), ?rAjadhAnI? (capital)
etc. and person context list contains ?kahA? (say),
?prdhAnama.ntrI? (prime minister) etc. For a
given word, the value of this feature correspond-
ing to a given NE type is set to 1 if the window
wi?3...wi+3 around the wi contains at last one word
from this list.
Dynamic NE tag: Named Entity tags of the pre-
vious words (ti?m...ti?1) are used as features.
First Word: If the token is the first word of a
sentence, then this feature is set to 1. Otherwise, it
is set to 0.
Contains Digit: If a token ?w? contains digit(s)
then the feature ContainsDigit is set to 1. This
feature is helpful for identifying company product
names (e.g. 06WD1992), house number (e.g. C226)
etc.
Numerical Word: For a token ?w? if the word
is a numerical word i.e. a word denoting a number
(e.g. eka (one), do (two), tina (three) etc.) then the
feature NumWord is set to 1.
Word Suffix: Word suffix information is helpful
to identify the named NEs. Two types of suffix fea-
tures have been used. Firstly a fixed length word
suffix of the current and surrounding words are used
2All Hindi words are written in italics using the ?Itrans?
transliteration.
345
as features. Secondly we compiled lists of common
suffixes of person and place names in Hindi. For ex-
ample, ?pura?, ?bAda?, ?nagara? etc. are location
suffixes. We used two binary features correspond-
ing to the lists - whether a given word has a suffix
from the list.
Word Prefix: Prefix information of a word may
be also helpful in identifying whether it is a NE. A
fixed length word prefix of current and surrounding
words are treated as a features.
Parts-of-Speech (POS) Information: The POS
of the current word and the surrounding words may
be useful feature for NER. We have access to a Hindi
POS pagger developed at IIT Kharagpur which has
an accuracy about 90%. The tagset of the tagger
contains 28 tags. We have used the POS values of
the current and surrounding tokens as features.
We realized that the detailed POS tagging is not
very relevant. Since NEs are noun phrases, the noun
tag is very relevant. Further the postposition follow-
ing a name may give a clue to the NE type. So we de-
cided to use a coarse-grained tagset with only three
tags - nominal (Nom), postposition (PSP) and other
(O).
The POS information is also used by defining sev-
eral binary features. An example is the NomPSP
binary feature. The value of this feature is defined
to be 1 if the current token is nominal and the next
token is a PSP.
5 Enhancement using Gazetteer Feature
Lists of names of various types are helpful in name
identification. We have compiled some specialized
name lists from different web sources. But the
names in these lists are in English, not in Hindi.
So we have transliterated these English name lists
to make them useful for our Hindi NER task.
For the transliteration we have build a 2-phase
transliteration module. We have defined an inter-
mediate alphabet containing 34 characters. English
names are transliterated to this intermediate form us-
ing a map-table. Hindi strings are also transliter-
ated to the intermediate alphabet form using a dif-
ferent map-table. For a English-Hindi string pair,
if transliterations of the both strings are same, then
we conclude that one string is the transliteration of
the other. This transliteration module works with
91.59% accuracy.
Using the transliteration approach we have con-
structed 8 lists. Which are, month name and days of
the week (40)3, organization end words list (92), per-
son prefix words list (123), list of common locations
(80), location names list (17,600), first names list
(9722), middle names list (35), surnames list (1800).
The lists can be used in name identification in var-
ious ways. One way is to check whether a token is
in any list. But this approach is not good as it has
some limitations. Some words may present in two or
more gazetteer lists. For example, ?bangAlora? is in
surnames list and also in location names list. Confu-
sions arise to make decisions for these words. Some
words are in gazetteer lists but sometimes these are
used in text as not-name entity. For example, ?gayA?
is in location list but sometimes the word is used as
verb in text and makes confusion. These limitations
might be reduced if the contexts are considered.
We have used these gazetteer lists as features
of MaxEnt. We have prepared several binary fea-
tures which are defined as whether a given word is
in a particular list. For example, a binary feature
FirstName is 1 for a particular token ?t? if ?t? is in
the first name list.
6 Context Pattern based Features
Context patterns are helpful for identifying NEs. As
manual identification of context patterns takes much
manual labour and linguistic knowledge, we have
developed a module for semi-automatically learning
of context pattern. The summary of the context pat-
tern learning module is given follows:
1. Collect some seed entities (E) for each class.
2. For each seed entity e in E, from the corpus
find context string(C) comprised of n tokens
before e, a placeholder for the class instance
and n tokens after e. [We have used n = 3]
This set of tokens form initial pattern.
3. Search the pattern in the corpus and find the
coverage and precision.
4. Discard the patterns having low precision.
3The italics integers in brackets indicate the size of the lists.
346
5. Generalize the patterns by dropping one or
more tokens to increase coverage.
6. Find best patterns having good precision and
coverage.
The quality of a pattern is measured by precision
and coverage. Precision is the ratio of correct iden-
tification and the total identification, when the par-
ticular pattern is used to identify of NEs of a spe-
cific type from a raw text. Coverage is the amount
of total identification. We have given more impor-
tance to precision and we have marked a pattern as
effective if the precision is more than 95%. The
method is applied on an un-annotated text having
4887011 words collected from ?Dainik Jagaran? and
context patterns are learned. These context patterns
are used as features of MaxEnt in the Hindi NER
system. Some example patterns are:
1. mukhyama.ntrI <PER> Aja
2. <PER> ne kahA ki
3. rAjadhAnI <LOC> me
7 Evaluation
We have evaluated the system using a blind test cor-
pus of 25K words, which is distinct from the training
corpus. The accuracies are measured in terms of the
f-measure, which is the weighted harmonic mean of
precision and recall. Here we can mention that we
have evaluated the performance of the system on ac-
tual NEs. That means the system annotates the test
data using 17 tags, similar to the training data. Dur-
ing evaluation we have merged the sub-tags of a par-
ticular entity to get a complete NEs and calculated
the accuracies. At the end of section 7.1 we have
also mentioned the accuracies if evaluated on the
tags. A number of experiments are conducted con-
sidering various combinations of features to identify
the best feature set for the Hindi NER task.
7.1 Baseline
The baseline performance of the system without us-
ing gazetteer and context patterns are presented in
Table 1. They are summarized below.
While experimenting with static word features,
we have observed that a window of previous two
Feature Class F-value
f1 = Word, NE Tag
PER 63.33
LOC 69.56
ORG 58.58
DAT 91.76
TOTAL 69.64
f2 = Word, NE Tag,
PER 69.75
LOC 75.8
ORG 59.31
Suffix (? 2) DAT 89.09
TOTAL 73.42
f3 = Word, NE Tag,
PER 70.61
LOC 71
ORG 59.31
Suffix (? 2), Prefix DAT 89.09
TOTAL 72.5
f4 = Word, NE Tag,
PER 70.61
LOC 75.8
ORG 60.54
Digit, Suffix (? 2) DAT 93.8
TOTAL 74.26
f5 = Word, NE Tag, POS
PER 64.25
LOC 71
ORG 60.54
DAT 89.09
TOTAL 70.39
Suffix (? 2), Digit,
PER 72.26
f6 = Word, NE Tag, LOC 78.6
ORG 51.36
NomPSP DAT 92.82
TOTAL 75.6
Table 1: F-values for different features
words to next two words (Wi?2...Wi+2) gives best
results. But when several other features are com-
bined then single word window (Wi?1...Wi+1) per-
forms better. Similarly we have experimented with
suffixes of different lengths and observed that the
suffixes of length ? 2 gives the best result for the
Hindi NER task. In using POS information, we
have observed that the coarse-grained POS tagger
information is more effective than the finer-grained
POS values. A feature set, combining finer-grained
POS values, surrounding words and previous NE
tag, gives a f-value of 70.39%. But when the
coarse-grained POS values are used instead of the
347
finer-grained POS values, the f-value is increased
to 74.16%. The most interesting fact we have ob-
served that more complex features do not guaran-
tee to achieve better results. For example, a feature
set combined with current and surrounding words,
previous NE tag and fixed length suffix information,
gives a f-value 73.42%. But when prefix information
are added the f-value decreased to 72.5%. The high-
est accuracy achieved by the system is 75.6% f-value
without using gazetteer information and context pat-
terns.
The results in Table 1 are obtained by evaluating
on the actual NEs. But when the system is evaluated
on the tags the f-value increases. For f6, the accu-
racy achieved on actual NEs is 75.6%, but if eval-
uated on tags, the value increased to 77.36%. Sim-
ilarly, for f2, the accuracy increased to 75.91% if
evaluated on tags. The reason is the NEs contain-
ing 3 or more words, are subdivided to N-begin, N-
continue (1 or more) and N-end. So if there is an
error in any of the subtags, the total NE becomes
an error. We observed many cases where NEs are
partially identified by the system, but these are con-
sidered as error during evaluation.
7.2 Using Gazetteer Lists and Context Patterns
Next we add gazetteer and context patterns as fea-
tures in our MaxEnt based NER system. In Ta-
ble 2 we have compared the results after addition
of gazetteer information and context patterns with
previous results. While experimenting we have ob-
served that gazetteer lists and context patterns are
capable of increasing the performance of our base-
line system. That is tested on all the baseline feature
sets. In Table 2 the comparison is shown for only
two features - f2 and f6 which are defined in Table 1.
It may be observed that the relative advantage of us-
ing both gazetteer and context patterns together over
using them individually is not much. For example,
when gazetteer information are added with f2, the f-
value is increased by 6.38%, when context patterns
are added the f-value is increased by 6.64%., but
when both are added the increment is 7.27%. This
may be due to the fact that both gazetteer and con-
text patterns lead to the same identifications. Using
the comprehensive feature set (using gazetteer infor-
mation and context patterns) the MaxEnt based NER
system achieves the maximum f-value of 81.52%.
F-value
Fea-
ture
Class No
Gaz
or
Pat
With
Gaz
With
Pat
With
Gaz
and
Pat
f2
PER 69.75 74.2 75.61 76.03
LOC 75.8 82.02 79.94 82.02
ORG 59.31 72.61 73.4 74.63
DAT 89.09 94.29 95.32 95.32
TOTAL 73.42 79.8 80.06 80.69
f6
PER 72.26 76.03 75.61 78.41
LOC 78.6 82.02 80.49 83.26
ORG 51.36 72.61 74.1 75.43
DAT 92.82 94.28 95.87 96.5
TOTAL 75.6 80.24 80.37 81.52
Table 2: F-values for different features with
gazetteers and context patterns
8 Conclusion
We have shown that our MaxEnt based NER sys-
tem is able to achieve a f-value of 81.52%, using a
hybrid set of features including traditional NER fea-
tures augmented with gazetteer lists and extracted
context patterns. The system outperforms the exist-
ing NER systems in Hindi.
Feature selection and feature clustering might
lead to further improvement of performance and is
under investigation.
9 Acknowledgement
The work is partially funded by Microsoft Research
India.
References
Bikel Daniel M., Miller Scott, Schwartz Richard and
Weischedel Ralph. 1997. Nymble: A High Perfor-
mance Learning Name-finder. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, pages 194?201.
Borthwick Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
Computer Science Department, New York University.
Cucerzan Silviu and Yarowsky David. 1999. Language
Independent Named Entity Recognition Combining
348
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and
VLC 1999, pages 90?99.
Ekbal A. and Bandyopadhyay S. 2007. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of International Conference on
Natural Language Processing (ICON), 2007.
Grishman Ralph. 1995. The New York University Sys-
tem MUC-6 or Where?s the syntax? In Proceedings of
the Sixth Message Understanding Conference.
Kumar N. and Bhattacharyya Pushpak. 2006. Named
Entity Recognition in Hindi using MEMM. In Techni-
cal Report, IIT Bombay, India..
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction (Short
Paper). ACM Transactions on Computational Logic.
McDonald D. 1996. Internal and external evidence in the
identification and semantic categorization of proper
names. In B. Boguraev and J. Pustejovsky, editors,
Corpus Processing for Lexical Acquisition, pages 21?
39.
Pietra Stephen Della, Pietra Vincent Della and Lafferty
John. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(4):380?393.
Srihari R., Niu C. and Li W. 2000. A Hybrid Approach
for Named Entity and Sub-Type Tagging. In Proceed-
ings of the sixth conference on Applied natural lan-
guage processing.
Talukdar Pratim P., Brants T., Liberman M., and Pereira
F. 2006. A context pattern induction method
for named entity extraction. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Wakao T., Gaizauskas R. and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
349
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 17?24,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
A Hybrid Approach for Named Entity Recognition in Indian Languages
Sujan Kumar Saha Sanjay Chatterji Sandipan Dandapat
Indian Institute of Technology Indian Institute of Technology Indian Institute of Technology
Kharagpur, West Bengal Kharagpur, West Bengal Kharagpur, West Bengal
India - 721302 India - 721302 India - 721302
sujan.kr.saha@gmail.com sanjay chatter@yahoo.com sandipan@cse.iitkgp.ernet.in
Sudeshna Sarkar Pabitra Mitra
Indian Institute of Technology Indian Institute of Technology
Kharagpur, West Bengal Kharagpur, West Bengal
India - 721302 India - 721302
shudeshna@gmail.com pabitra@gmail.com
Abstract
In this paper we describe a hybrid system
that applies Maximum Entropy model (Max-
Ent), language specific rules and gazetteers
to the task of Named Entity Recognition
(NER) in Indian languages designed for the
IJCNLP NERSSEAL shared task. Starting
with Named Entity (NE) annotated corpora
and a set of features we first build a base-
line NER system. Then some language spe-
cific rules are added to the system to recog-
nize some specific NE classes. Also we have
added some gazetteers and context patterns
to the system to increase the performance.
As identification of rules and context pat-
terns requires language knowledge, we were
able to prepare rules and identify context
patterns for Hindi and Bengali only. For the
other languages the system uses the MaxEnt
model only. After preparing the one-level
NER system, we have applied a set of rules
to identify the nested entities. The system
is able to recognize 12 classes of NEs with
65.13% f-value in Hindi, 65.96% f-value in
Bengali and 44.65%, 18.74%, and 35.47%
f-value in Oriya, Telugu and Urdu respec-
tively.
1 Introduction
Named entity recognition involves locating and clas-
sifying the names in text. NER is an important
task, having applications in Information Extraction
(IE), Question Answering (QA), Machine Transla-
tion (MT) and in most other NLP applications.
This paper presents a Hybrid NER system for In-
dian languages which is designed for the IJCNLP
NERSSEAL shared task competition, the goal of
which is to perform NE recognition on 12 types
of NEs - person, designation, title-person, organiza-
tion, abbreviation, brand, title-object, location, time,
number, measure and term.
In this work we have identified suitable features
for the Hindi NER task. Orthography features, suf-
fix and prefix information, morphology informa-
tion, part-of-speech information as well as informa-
tion about the surrounding words and their tags are
used to develop a MaxEnt based Hindi NER sys-
tem. Then we realized that the recognition of some
classes will be better if we apply class specific lan-
guage rules in addition to the MaxEnt model. We
have defined rules for time, measure and number
classes. We made gazetteers based identification for
designation, title-person and some terms. Also we
have used person and location gazetteers as features
of MaxEnt for better identification of these classes.
Finally we have built a module for semi-automatic
extraction of context patterns and extracted context
patterns for person, location, organization and title-
object classes and these are added to the baseline
NER system.
The shared task was defined to build the NER sys-
tems for 5 Indian languages - Hindi, Bengali, Oriya,
Telugu and Urdu for which training data was pro-
17
vided. Among these 5 languages only Bengali and
Hindi are known to us but we have no knowledge for
other 3 languages. So we are unable to build rules
and extract context patterns for these languages. The
NER systems for these 3 languages contain only
the baseline system i.e. the MaxEnt system. Also
our baseline MaxEnt NER system uses morphologi-
cal and parts-of-speech (POS) information as a fea-
ture. Due to unavailability of morphological ana-
lyzer and POS tagger for these 3 languages, these in-
formation are not added to the systems. Among the
3 languages, only for Oriya NER system we have
used small gazetteers for person, location and des-
ignation extracted from the training data. For Ben-
gali and Hindi the developed systems are complete
hybrid systems containing rules, gazetteers, context
patterns and the MaxEnt model.
The paper is organized as follows. A brief sur-
vey of different techniques used for the NER task
in different languages and domains are presented in
Section 2. Also a brief survey on nested NE recog-
nition system is presented here. A discussion on
the training data is given in Section 3. The MaxEnt
based NER system is described in Section 4. Vari-
ous features used in NER are then discussed. Next
we present the experimental results and related dis-
cussions in Section 8. Finally Section 9 concludes
the paper.
2 Previous Work
A variety of techniques has been used for NER. The
two major approaches to NER are:
1. Linguistic approaches.
2. Machine Learning (ML) based approaches.
The linguistic approaches typically use rules man-
ually written by linguists. There are several rule-
based NER systems, containing mainly lexicalized
grammar, gazetteer lists, and list of trigger words,
which are capable of providing 88%-92% f-measure
accuracy for English (Grishman, 1995; McDonald,
1996; Wakao et al, 1996).
The main disadvantages of these rule-based tech-
niques are that these require huge experience and
grammatical knowledge of the particular language
or domain and these systems are not transferable to
other languages or domains.
ML based techniques for NER make use of a
large amount of NE annotated training data to ac-
quire high level language knowledge. Several ML
techniques have been successfully used for the NER
task of which Hidden Markov Model (HMM) (Bikel
et al, 1997), Maximum Entropy (MaxEnt) (Borth-
wick, 1999), Conditional Random Field (CRF) (Li
and Mccallum, 2004) are most common. Combina-
tions of different ML approaches are also used. Sri-
hari et al (2000) combines MaxEnt, Hidden Markov
Model (HMM) and handcrafted rules to build an
NER system.
NER systems use gazetteer lists for identifying
names. Both the linguistic approach (Grishman,
1995; Wakao et al, 1996) and the ML based ap-
proach (Borthwick, 1999; Srihari et al, 2000) use
gazetteer lists.
Linguistic approach uses handcrafted rules which
needs skilled linguistics. Some recent approaches
try to learn context patterns through ML which re-
duce amount of manual labour. Talukder et al(2006)
combined grammatical and statistical techniques to
create high precision patterns specific for NE extrac-
tion. An approach to lexical pattern learning for In-
dian languages is described by Ekbal and Bandopad-
hyay (2007). They used seed data and annotated cor-
pus to find the patterns for NER.
The NER task for Hindi has been explored by
Cucerzan and Yarowsky in their language indepen-
dent NER work which used morphological and con-
textual evidences (Cucerzan and Yarowsky, 1999).
They ran their experiment with 5 languages - Roma-
nian, English, Greek, Turkish and Hindi. Among
these the accuracy for Hindi was the worst. For
Hindi the system achieved 41.70% f-value with a
very low recall of 27.84% and about 85% precision.
A more successful Hindi NER system was devel-
oped by Wei Li and Andrew Mccallum (2004) using
Conditional Random Fields (CRFs) with feature in-
duction. They were able to achieve 71.50% f-value
using a training set of size 340k words. In Hindi
the maximum accuracy is achieved by Kumar and
Bhattacharyya, (2006). Their Maximum Entropy
Markov Model (MEMM) based model gives 79.7%
f-value.
All the NER systems described above are able
to detect one-level NEs. In recent years, the inter-
est in detection of nested NEs has increased. Here
18
we mention few attempts for nested NE detection.
Zhou et al (2004) described an approach to iden-
tify cascaded NEs from biomedical texts. They de-
tected the innermost NEs first and then they derived
rules to find the other NEs containing these as sub-
strings. Another approach, described by McDonald
et al (2005), uses structural multilevel classifica-
tion to deal with overlapping and discontinuous enti-
ties. B. Gu (2006) has treated the task of identifying
the nested NEs a binary classification problem and
solved it using support vector machines. For each
token in nested NEs, they used two schemes to set
its class label: labeling as the outermost entity or the
inner entities.
3 Training Data
The data used for the training of the systems was
provided. The annotated data uses Shakti Standard
Format (SSF). For our development we have con-
verted the SSF format data into the IOB formatted
text in which a B ? XXX tag indicates the first
word of an entity type XXX and I?XXX is used
for subsequent words of an entity. The tag O indi-
cates the word is outside of a NE. The training data
for Hindi contains more than 5 lakh words, for Ben-
gali about 160K words and about 93K, 64K and 36K
words for Oriya, Telugu and Urdu respectively.
In time of development we have observed that
the training data, provided by the organizers of the
shared task, contains several types of errors in NE
tagging. These errors in the training corpora affects
badly to the machine learning (ML) based models.
But we have not made corrections of the errors in
the training corpora in time of our development. All
the results shown in the paper are obtained using the
provided corpora without any modification in NE
annotation.
4 Maximum Entropy Based Model
We have used MaxEnt model to build the baseline
NER system. MaxEnt is a flexible statistical model
which assigns an outcome for each token based on
its history and features. Given a set of features and a
training corpus, the MaxEnt estimation process pro-
duces a model. For our development we have used
a Java based open-nlp MaxEnt toolkit1 to get the
1www.maxent.sourceforge.net
probability values of a word belonging to each class.
That is, given a sequence of words, the probability
of each class is obtained for each word. To find the
most probable tag corresponding to each word of a
sequence, we can choose the tag having the highest
class conditional probability value. But this method
is not good as it might result in an inadmissible as-
signment.
Some tag sequences should never happen. To
eliminate these inadmissible sequences we have
made some restrictions. Then we used a beam
search algorithm with a beam of length 3 with these
restrictions.
4.1 Features
MaxEnt makes use of different features for identify-
ing the NEs. Orthographic features (like capitaliza-
tion, decimal, digits), affixes, left and right context
(like previous and next words), NE specific trigger
words, gazetteer features, POS and morphological
features etc. are generally used for NER. In En-
glish and some other languages, capitalization fea-
tures play an important role as NEs are generally
capitalized for these languages. Unfortunately this
feature is not applicable for the Indian languages.
Also Indian person names are more diverse, lots of
common words having other meanings are also used
as person names. Li and Mccallum (2004) used the
entire word text, character n-grams (n = 2, 3, 4),
word prefix and suffix of lengths 2, 3 and 4, and 24
Hindi gazetteer lists as atomic features in their Hindi
NER. Kumar and Bhattacharyya (2006) used word
features (suffixes, digits, special characters), context
features, dictionary features, NE list features etc. in
their MEMM based Hindi NER system. In the fol-
lowing we have discussed about the features we have
identified and used to develop the Indian language
NER systems.
Static Word Feature: The previous and next
words of a particular word are used as features. The
previous m words (wi?m...wi?1) to next n words
(wi+1...wi+n) can be considered. During our exper-
iment different combinations of previous 4 to next 4
words are used.
Context Lists: Context words are defined as the
frequent words present in a word window for a par-
ticular class. We compiled a list of the most frequent
words that occur within a window of wi?3...wi+3
19
of every NE class. For example, location con-
text list contains the words like ?jAkara2? (go-
ing to), ?desha? (country), ?rAjadhAnI? (capital)
etc. and person context list contains ?kahA? (say),
?pradhAnama.ntrI? (prime minister) etc. For a
given word, the value of this feature correspond-
ing to a given NE type is set to 1 if the window
wi?3...wi+3 around the wi contains at last one word
from this list.
Dynamic NE tag: Named Entity tags of the pre-
vious words (ti?m...ti?1) are used as features.
First Word: If the token is the first word of a
sentence, then this feature is set to 1. Otherwise, it
is set to 0.
Contains Digit: If a token ?w? contains digit(s)
then the feature ContainsDigit is set to 1.
Numerical Word: For a token ?w? if the word
is a numerical word i.e. a word denoting a number
(e.g. eka (one), do (two), tina (three) etc.) then the
feature NumWord is set to 1.
Word Suffix: Word suffix information is helpful
to identify the NEs. Two types of suffix features
have been used. Firstly a fixed length word suffix of
the current and surrounding words are used as fea-
tures. Secondly we compiled lists of common suf-
fixes of person and place names in Hindi. For ex-
ample, ?pura?, ?bAda?, ?nagara? etc. are location
suffixes. We used binary features corresponding to
the lists - whether a given word has a suffix from a
particular list.
Word Prefix: Prefix information of a word may
also be helpful in identifying whether it is a NE. A
fixed length word prefix of current and surrounding
words are treated as features.
Root Information of Word: Indian languages
are morphologically rich. Words are inflected in var-
ious forms depending on its number, tense, person,
case etc. Identification of NEs becomes difficult for
these inflections. The task becomes easier if instead
of the inflected words, corresponding root words are
checked whether these are NE or not. For that task
we have used morphological analyzers for Hindi and
Bengali which are developed at IIT kharagpur.
Parts-of-Speech (POS) Information: The POS
of the current word and the surrounding words may
2All Hindi words are written in italics using the ?Itrans?
transliteration
be useful feature for NER. We have accessed to
Hindi and Bengali POS taggers developed at IIT
Kharagpur which has accuracy about 90%. The
tagset of the tagger contains 28 tags. We have used
the POS values of the current and surrounding to-
kens as features.
We realized that the detailed POS tagging is not
very relevant. Since NEs are noun phrases, the noun
tag is very relevant. Further the postposition follow-
ing a name may give a clue to the NE type for Hindi.
So we decided to use a coarse-grained tagset with
only three tags - nominal (Nom), postposition (PSP)
and other (O).
The POS information is also used by defining sev-
eral binary features. An example is the NomPSP
binary feature. The value of this feature is defined
to be 1 if the current token is nominal and the next
token is a PSP.
5 Language Specific Rules
After building of the MaxEnt model we have ob-
served that only a small set of rules are able to iden-
tify the classes like number, measure, time, more ef-
ficiently than the MaxEnt based model. Then we
have tried to define the rules for these classes. The
rule identification is done manually and requires lan-
guage knowledge. We have defined the required
rules for Bengali and Hindi but we are unable to do
the same for other 3 languages as the languages are
unknown to us. In the following we have mentioned
some example rules which are defined and used in
our system.
? IF ((Wi is a number or numeric word) AND
(Wi+1 is an unit))
THEN (Wi Wi+1) bigram is a measure NE.
? IF ((Wi is a number or numeric word) AND
(Wi+1 is a month-name) AND (Wi+2 is a 4
digit number))
THEN (Wi Wi+1 Wi+2) trigram is a time NE.
? IF ((Wi denotes a day of a week) AND (Wi+1
is a number or numeric word) AND (Wi+2 is a
month name))
THEN (Wi Wi+1 Wi+2) trigram is a time NE.
We have defined 36 rules in total for time, mea-
sure and number classes. These rules use some lists
20
which are built. These lists contain correspond-
ing entries both in the target language and in En-
glish. For example the months names list contains
the names according to the English calender and the
names according to the Indian calender. In the fol-
lowing we have mentioned the lists we have pre-
pared for the rule-based module.
? Names of months.
? Names of seasons.
? Days of a week.
? Names of units.
? Numerical words.
5.1 Semi-automatic Extraction of Context
Patterns
Similar to the rules defined for time, measure and
date classes, if efficient context patterns (CP) can
be extracted for a particular class, these can help
in identification of NEs of the corresponding class.
But extraction of CP requires huge labour if done
manually. We have developed a module for semi-
automatically extraction of context patterns. This
module makes use of the most frequent entities of
a particular class as seed for that class and finds the
surrounding tokens of the seed to extract effective
patterns. We mark a pattern as ?effective? if the pre-
cision of the pattern is very high. Precision of a pat-
tern is defined as the ratio of correct identification
and the total identification when the pattern is used
to identify NEs of a particular type from a text.
For our task we have extracted patterns for per-
son, location, organization and title-object classes.
These patterns are able to identify the NEs of a spe-
cific classes but detection of NE boundary is not
done properly by the patterns. For boundary detec-
tion we have added some heuristics and used POS
information of the surrounding words. The patterns
for a particular class may identify the NEs of other
classes also. For example the patterns for identify-
ing person names may also identify the designation
or title-persons. These need to be handled carefully
at the time of using patterns. In the following some
example patterns are listed which are able to identify
person names for Hindi.
? <PER> ne kahA ki
? <PER> kA kathana he.n
? mukhyama.ntrI <PER> Aja
? <PER> ne apane gra.ntha
? <PER> ke putra <PER>
6 Use of Gazetteer Lists
Lists of names of various types are helpful in name
identification. Firstly we have prepared the lists us-
ing the training corpus. But these are not sufficient.
Then we have compiled some specialized name lists
from different web sources. But the names in these
lists are in English, not in Indian languages. So we
have transliterated these English name lists to make
them useful for our NER task.
Using transliteration we have constructed several
lists. Which are, month name and days of the week,
list of common locations, location names list, first
names list, middle names list, surnames list etc.
The lists can be used in name identification in var-
ious ways. One way is to check whether a token is in
any list. But this approach is not good as it has some
limitations. Some words may present in two or more
gazetteer lists. Confusions arise to make decisions
for these words. Some words are in gazetteer lists
but sometimes these are used in text as not-name en-
tity. We have used these gazetteer lists as features of
MaxEnt. We have prepared several binary features
which are defined as whether a given word is in a
particular list.
7 Detection of Nested Entities
The training corpora used for the models, are not
annotated as nested. The maximal entities are an-
notated in the training corpus. For detection of the
nested NEs, we have derived some rules. For exam-
ple, if a particular word is a number or numeric word
and is a part of a NE type other than ?number?, then
we have made the nesting. Again, if any common lo-
cation identifier word like, jilA (district), shahara
(town) etc. is a part of a ?location? entity then we
have nested there. During one-level NE identifica-
tion, we have generated lists for all the identified lo-
cation and person names. Then we have searched
other NEs containing these as substring to make the
21
nesting. After preparing the one-level NER system,
we have applied the derived rules on it to identify
the nested entities.
8 Evaluation
The accuracies of the system are measured in terms
of the f-measure, which is the weighted harmonic
mean of precision and recall. Nested, maximal and
lexical accuracies are calculated separately. The
test data for all the five languages are provided.
The size of the shared task test files are: Hindi
- 38,704 words, Bengali - 32,796 words, Oriya -
26,988 words, Telugu - 7,076 words and Urdu -
12,805 words.
We have already mentioned that after preparing
a one-level NER system, the rule-based module is
used to modify it to a nested one. A number of ex-
periments are conducted considering various combi-
nations of features to identify the best feature set for
Indian language NER task. It is very difficult and
time consuming to conduct experiments for all the
languages. During the development we have con-
ducted all the experiments on Hindi and Bengali. We
have prepared a development test data composed of
24,265 words for Hindi and 10,902 word for Ben-
gali and accuracies of the system are tested on the
development data. The details of the experiments on
Hindi data for the best feature selection is described
in the following section.
8.1 Best Feature Set Selection
The performance of the system on the Hindi data
using various features are presented in Table 1.
They are summarized below. While experimenting
with static word features, we have observed that a
window of previous two words to next two words
(Wi?2...Wi+2) gives best results. But when sev-
eral other features are combined then smaller win-
dow (Wi?1...Wi+1) performs better. Similarly we
have experimented with suffixes of different lengths
and observed that the suffixes of length ? 2 gives
the best result for the Hindi NER task. In using
POS information, we have observed that the coarse-
grained POS tagger information is more effective
than the finer-grained POS values. The most in-
teresting fact we have observed that more complex
features do not guarantee to achieve better results.
For example, a feature set combined with current
and surrounding words, previous NE tag and fixed
length suffix information, gives a f-value 64.17%.
But when prefix information are added the f-value
decreased to 63.73%. Again when the context lists
are added to the feature set containing words, previ-
ous tags, suffix information, digit information and
the NomPSP binary feature, the accuracy has de-
creased to 67.33% from 68.0%.
Feature Overall
F-value
Word, NE Tag 58.92
Word, NE Tag, Suffix (? 2) 64.17
Word, NE Tag, Suffix (? 2),
Prefix
63.73
Word, NE Tag, Digit, Suffix 66.61
Word, NE Tag, Context List 63.57
Word, NE Tag, POS (full) 61.28
Word, NE Tag, Suffix (? 2),
Digit, NomPSP
68.60
Word, NE Tag, Suffix (? 2),
Digit, Context List, NomPSP
67.33
Word, NE Tag, Suffix (?
2), Digit, NomPSP, Linguis-
tic Rules
73.40
Word, NE Tag, Suffix (? 2),
Digit, NomPSP, Gazetteers
72.08
Word, NE Tag, Suffix (?
2), Digit, NomPSP, Linguis-
tic Rules, Gazetteers
74.53
Table 1: Hindi development set f-values for different
features
The feature set containing words, previous
tags, suffix information, digit information and the
NomPSP binary feature is the identified best feature
set without linguistic rules and gazetteer informa-
tion. Then we have added the linguistic rules, pat-
terns and gazetteer information to the system and the
changes in accuracies are shown in the table.
8.2 Results on the Test Data
The best identified feature set is used for the de-
velopment of the NER systems for all the five lan-
guages. We have already mentioned that for only
for Bengali and Hindi we have added linguistic rules
22
and gazetteer lists in the MaxEnt based NER sys-
tems. The accuracy of the system on the shared task
test data for all the languages are shown in Table 2.
Lan-
guage
Type Preci-
sion
Recall F-
measure
Bengali
Maximal 52.92 68.07 59.54
Nested 55.02 68.43 60.99
Lexical 62.30 70.07 65.96
Hindi
Maximal 75.19 58.94 66.08
Nested 79.58 58.61 67.50
Lexical 82.76 53.69 65.13
Oriya
Maximal 21.17 26.92 23.70
Nested 27.73 28.13 27.93
Lexical 51.51 39.40 44.65
Telugu
Maximal 10.47 9.64 10.04
Nested 22.05 13.16 16.48
Lexical 25.23 14.91 18.74
Urdu
Maximal 26.12 29.69 27.79
Nested 27.99 29.21 28.59
Lexical 37.58 33.58 35.47
Table 2: Accuracy of the system for all languages
The accuracies of Oriya, Telugu and Urdu lan-
guages are poor compared to the other two lan-
guages. The reasons are POS information, mor-
phological information, language specific rules and
gazetteers are not used for these languages. Also the
size of training data for these languages are smaller.
To mention, for Urdu, size of the training data is only
about 36K words which is very small to train a Max-
Ent model.
It is mentioned that we have prepared a set of rules
which are capable of identifying the nested NEs.
Once the one-level NER system has built, we have
applied the rules on it. In Table 3 we have shown
the f-values of each class after addition of the nested
rules. The detailed results for all languages are not
shown. In the table we have shown only the results
of Bengali and Hindi.
For both the languages ?title-person? and ?desig-
nation? classes are suffering from poor accuracies.
The reason is, in the training data and also in the
annotated test data, these classes contains many an-
notation errors. Also the classes being closely re-
lated to each other, the system fails to distinguish
them properly. The detection of the ?term? class is
Hindi Bengali
Class Maximal Nested Maximal Nested
Person 70.87 71.00 77.45 79.09
Desig-
nation
48.98 59.81 26.32 26.32
Organi-
zation
47.22 47.22 41.43 71.43
Abbre-
viation
- 72.73 51.61 51.61
Brand - - - -
Title-
person
- 60.00 5.19 47.61
Title-
object
41.32 40.98 72.97 72.97
Location 86.02 87.02 76.27 76.27
Time 67.42 67.42 56.30 56.30
Number 84.59 85.13 40.65 40.65
Measure 59.26 55.17 62.50 62.50
Term 48.91 50.51 43.67 43.67
Table 3: Comparison of maximal and nested f-
values for different classes of Hindi and Bengali
very difficult. In the test files amount of ?term? en-
tity is large, for Bengali - 434 and for Hindi - 1080,
so the poor accuracy of the class affects badly to the
overall accuracy. We have made rule-based identi-
fication for ?number?, ?measure? and ?time? classes;
the accuracies of these classes proves that the rules
need to be modified to achieve better accuracy for
these classes. Also the accuracy of the ?organiza-
tion? class is not high, because amount of organiza-
tion entities is not sufficient in the training corpus.
We have achieved good results for other two main
classes - ?person? and ?location?.
8.3 Comparison with Other Shared Task
Systems
The comparison of the accuracies of our system
and other shared task systems is given in Table 4.
From the comparison we can see that our system
has achieved the best accuracies for most of the lan-
guages.
9 Conclusion
We have prepared a MaxEnt based system for the
NER task in Indian languages. We have also added
23
Lan-
guage
Our S2 S6 S7
Bengali 65.96 39.77 40.63 59.39
Hindi 65.13 46.84 50.06 33.12
Oriya 44.65 45.84 39.04 28.71
Telugu 18.74 46.58 40.94 4.75
Urdu 35.47 44.73 43.46 35.52
Table 4: Comparison of our lexical f-measure accu-
racies with the systems : S2 - Praveen P.(2008), S6 -
Gali et al(2008) and S7 - Ekbal et al(2008)
rules and gazetteers for Bengali and Hindi. Also our
derived rules need to be modified for improvement
of the system. We have not made use of rules and
gazetteers for Oriya, Telugu and Urdu. As the size
of training data is not much for these 3 languages,
rules and gazetteers would be effective. We have
experimented with MaxEnt model only, other ML
methods like HMM, CRF or MEMM may be able
to give better accuracy. We have not worked much
on the detection of nested NEs. Proper detection of
nested entities may lead to further improvement of
performance and is under investigation.
References
Bikel Daniel M., Miller Scott, Schwartz Richard and
Weischedel Ralph. 1997. Nymble: A High Perfor-
mance Learning Name-finder. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, 194?201.
Borthwick Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
Computer Science Department, New York University.
Cucerzan Silviu and Yarowsky David. 1999. Language
Independent Named Entity Recognition Combining
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and
VLC 1999, 90?99.
Ekbal A. and Bandyopadhyay S. 2007. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of International Conference on
Natural Language Processing (ICON), 2007.
Ekbal A., Haque R., Das A., Poka V. and Bandyopad-
hyay S. 2008. Language Independent Named Entity
Recognition in Indian Languages In Proceedings of
IJCNLP workshop on NERSSEAL. (Accepted)
Gali K., Surana H., Vaidya A., Shishtla P. and Misra
Sharma D. 2008. Named Entity Recognition through
CRF Based Machine Learning and Language Specific
Heuristics In Proceedings of IJCNLP workshop on
NERSSEAL. (Accepted)
Grishman Ralph. 1995. The New York University Sys-
tem MUC-6 or Where?s the syntax? In Proceedings of
the Sixth Message Understanding Conference.
Gu B. 2006. Recognizing Nested Named Entities in GE-
NIA corpus. In Proceedings of the BioNLP Workshop
on Linking Natural Language Processing and Biology
at HLT-NAACL 06, pages 112-113.
Kumar N. and Bhattacharyya Pushpak. 2006. Named
Entity Recognition in Hindi using MEMM. In Techni-
cal Report, IIT Bombay, India..
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Condi-
tional Random Fields and Feature Induction (Short Pa-
per). In ACM Transactions on Computational Logic.
McDonald D. 1996. Internal and external evidence in the
identification and semantic categorization of proper
names. In B. Boguraev and J. Pustejovsky, editors,
Corpus Processing for Lexical Acquisition, 21?39.
McDonald R., Crammer K. and Pereira F. 2005. Flexible
text segmentation with structured multilabel classifica-
tion. In Proceedings of EMNLP05.
Praveen P. 2008. Hybrid Named Entity Recogni-
tion System for South-South East Indian Languages.
InProceedings of IJCNLP workshop on NERSSEAL.
(Accepted)
Srihari R., Niu C. and Li W. 2000. A Hybrid Approach
for Named Entity and Sub-Type Tagging. In Proceed-
ings of the sixth conference on Applied natural lan-
guage processing.
Talukdar Pratim P., Brants T., Liberman M., and Pereira
F. 2006. A context pattern induction method
for named entity extraction. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Wakao T., Gaizauskas R. and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
Zhou G., Zhang J., Su J., Shen D. and Tan C. 2004.
Recognizing Names in Biomedical Texts: a Machine
Learning Approach. Bioinformatics, 20(7):1178-
1190.
24
Gazetteer Preparation for Named Entity Recognition in Indian Languages
Sujan Kumar Saha
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
sujan.kr.saha@gmail.com
Sudeshna Sarkar
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
shudeshna@gmail.com
Pabitra Mitra
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
pabitra@gmail.com
Abstract
This paper describes our approaches for the
preparation of gazetteers for named entity
recognition (NER) in Indian languages. We
have described two methodologies for the
preparation of gazetteers1. Since the rel-
evant gazetteer lists are more easily avail-
able in English we have used a translitera-
tion based approach to convert available En-
glish name lists to Indian languages. The
second approach is a context pattern induc-
tion based domain specific gazetteer prepa-
ration. This approach uses a domain specific
raw corpus and a few seed entities to learn
context patterns and then the corresponding
name lists are generated by using bootstrap-
ping.
1 Introduction
Named entity recognition involves locating and clas-
sifying the names in text. NER is an important task,
having applications in information extraction (IE),
question answering (QA), machine translation and
in most other NLP applications.
NER systems have been developed for resource-
rich languages like English with very high accura-
cies. But constructing an NER for a resource-poor
language is very challenging due to unavailability of
proper resources. Name-dictionaries or gazetteers
are very helpful NER resources and in most Indian
1Specialized list of names for a particular class of Named
Entity (NE). For Example, India is in the location gazetteer,
Sachin is in the person first name gazetteer.
languages there is no reasonable size publicly avail-
able list. The web contains lots of such resources,
which can be used for Indian language NER devel-
opment. But most of the web resources are in En-
glish. Our approach is to transliterate the relevant
English resources and name dictionaries into Indian
languages to make them useful for Indian language
NER task. But direct transliteration from English to
an Indian languages is not easy. Few attempts are
taken to build English to Indian language transliter-
ation systems but the word agreement ratio (WAR)
reached is upto 69.3% (Ekbal et al, 2006).
We have attempted to build a transliteration sys-
tem which uses an intermediate alphabet. Both
the English and the Indian language strings are
transliterated to the intermediate alphabet and for a
English-Indian language pair, if the transliterated in-
termediate alphabet strings are same then we have
concluded that the strings are the transliteration of
one another. We have transliterated the available En-
glish name lists into the intermediate alphabet and
these might be used as gazetteers. The Indian lan-
guage words need to be transliterated to the inter-
mediate format to check whether the word is in a
gazetteer or not. This system does not transliter-
ate the English name lists into Indian languages but
makes them useful in Indian languages NER task.
Transliteration based approaches are useful when
there is availability of English name lists. But when
relevant English name lists are not available then
also we can prepare gazetteers from raw corpus.
We have defined a semi-automatic context pattern
(CP) extraction based gazetteer preparation frame-
work. This framework uses bootstrapping to prepare
The 6th Workshop on Asian Languae Resources, 2008
9
the gazetteers from a large raw corpus starting from
few seed entities. Firstly fixed length patterns are
formed using the surrounding words of the seeds.
Depending on the pattern precision, the patterns are
discarded or generalized by dropping tokens from
the patterns. This set of high precision patterns ex-
tracts other named entities (NEs) which are added to
the seed list for the next iteration of the process. Fi-
nally we are able to prepare the required gazetteers.
To prove the effectiveness of the gazetteer prepa-
ration approach, we have prepared some gazetteers
like names of cricketers, names of tennis players etc.
from a raw Hindi sports domain corpus. The de-
tails of the approaches are given in the following
sections.
The paper is organized as follows. Usefulness
of gazetteers in NER, transliteration approaches in
general and specific for Indian languages and gen-
eral pattern extraction methodologies are discussed
in section 2. Section 3 presents the architecture of
the 2-phase transliteration system and preparation of
gazetteers using that. In section 4 context pattern
extraction based gazetteer preparation is discussed.
Finally section 5 concludes the paper.
2 Previous Work
The main approaches to NER are Linguistic ap-
proaches and Machine Learning (ML) based ap-
proaches. The linguistic approach typically uses
rule-based models manually written by linguists.
ML based techniques make use of a large amount
of annotated training data to acquire high-level lan-
guage knowledge. Several ML techniques like Hid-
den Markov Model (HMM)(Bikel et al, 1997),
Maximum Entropy Model(MaxEnt) (Borthwick,
1999), Conditional Random Field(CRF) (Li and
McCallum, 2004) etc. have been successfully used
for the NER task. Both the approaches may make
use of gazetteer information to build systems. There
are many systems which use gazetteers to improve
the accuracy.
Ralph Grishman has developed a rule-based NER
system which uses some specialized name dictionar-
ies including names of all countries, names of major
cities, names of companies, common first names etc
(Grishman, 1995). Another rule based NER system
is developed by Wakao et al (1996) which has used
several gazetteers like organization names, location
names, person names, human titles etc.
We will now mention some ML based sys-
tems. MENE is a MaxEnt based system de-
veloped by Borthwick. This system has used 8
dictionaries (Borthwick, 1999), which are: First
names (1,245), Corporate names (10,300), Corpo-
rate names without suffix (10,300), Colleges and
Universities (1,225), Corporate suffixes (244), Date
and Time (51) etc. The italics numbers in bracket
indicates the size of the dictionaries. The hy-
brid system developed by Srihari et al(2000) com-
bines several modules built by using MaxEnt, HMM
and handcrafted rules. This system uses the fol-
lowing gazetteers: First name (8,000), Family
name (14,000) and a big gazetteer of Locations
(250,000). There are many other systems which
have used name dictionaries to improve the accu-
racy. Kozareva (2006) described a methodology
to generate gazetteer lists automatically for Spanish
and to build NER system with labeled and unlabeled
data. The location gazetteer is built by finding loca-
tion patterns which looks for specific prepositions.
And the person gazetteer is constructed with graph
exploration algorithm.
Transliteration is also a very important topic
and lots of transliteration systems for different lan-
guages have been developed using different ap-
proaches. The basic approaches for transliteration
are phoneme based or spelling-based. A phoneme-
based statistical transliteration system from Ara-
bic to English was developed by Knight and
Graehl(1998). This system uses a finite state
transducer that implements transformation rules to
do back-transliteration. A spelling-based model
that directly maps English letter sequences into
Arabic letters was developed by Al-Onaizan and
Knight(2002). Several transliteration systems ex-
ist for English-Japanese, English-Chinese, English-
Spanish and many other languages to English. But
very few attempts have been reported on the de-
velopment of transliteration systems between Indian
languages and English. We can mention a transliter-
ation system for Bengali-English transliteration de-
veloped by Ekbal et al(2006). They have proposed
different models modifying the joint source chan-
nel model. In that system a Bengali string is di-
vided into transliteration units containing a vowel
The 6th Workshop on Asian Languae Resources, 2008
10
modifier or matra at the end of each unit. Sim-
ilarly the English string is also divided into units.
Then they defined various unigram, bigram or tri-
gram models depending on the consideration of the
contexts of the units. They have also considered lin-
guistic knowledge in the form of possible conjuncts
and diphthongs in Bengali and their representations
in English. This system is capable of transliterat-
ing mainly person names. The highest transliteration
accuracy achieved by them is 69.3% Word Agree-
ment Ratio (WAR) for Bengali to English and 67.9%
WAR for English to Bengali transliteration.
In the field of IE, patterns play a key role in
identifying relevant pieces of information. Soder-
land et al(1995), Rillof and Jones(1999), Lin et
al.(2003), Downey et al(2004), Etzioni et al(2005)
described different approaches to context pattern in-
duction. Talukder et al(2006) combined grammati-
cal and statistical techniques to create high precision
patterns specific for NE extraction. An approach to
lexical pattern learning for Indian languages is de-
scribed by Ekbal and Bandopadhyay (2007). They
used seed data and annotated corpus to find the pat-
terns for NER.
3 Transliteration based Gazetteer
Preparation
Gazetteers or name dictionaries are helpful in NER.
We have already discussed about some English NER
systems where the usefulness of the gazetteers have
been established. However while developing NER
systems in Indian languages, we tried to find relevant
gazetteers. But we could not obtain openly available
gazetteer lists for these languages. But we found
that there are a lot of resources of names of Indian
persons, Indian places, organizations etc. in English
available in the web. In Table 1 we have mentioned
some of the sources which contains relevant name
lists.
But it is not possible to use the available name
lists directly in the Indian language NER task as
these are in English. We have decided to translit-
erate the English lists into Indian languages to make
them useful in the Indian language NER task.
List Web Sources
http://www.bsnl.co.in/ onlinedi-
rectory.htm
First
Name
http://web1.mtnl.net.in/ direc-
tory/
http://www.eci.gov.in/
http://hiren.info/indian-baby-
names/
http://www.indiaexpress.com/
specials/babynames/
http://surnamedirectory.com/
surname-index.html
Surname http://web1.mtnl.net.in/ direc-
tory/
http://en.wikipedia.org
India Lo-
cation
http://indiavilas.com/ indi-
ainfo/pincodes.asp
http://www.indiapost.gov.in
http://www.eci.gov.in/
World
Location
http://www.maxmind.com/
app/worldcities
http://en.wikipedia.org/wiki
Table 1: Web sources for some relevant name lists
3.1 Transliteration
The transliteration from English to Hindi is quite
difficult. English alphabet contains 26 characters
whereas the Hindi alphabet contains 52 characters.
So the mapping is not trivial. We have already men-
tioned that for Bengali a transliteration system was
developed by Ekbal et al Similar approach can be
used to develop transliteration systems for other In-
dian languages. But this approach uses a bilingual
transliteration corpus, which requires much efforts
to built, is unavailable in proper size in all Indian
languages. Also using this approach the word agree-
ment ratio obtained is below 70%.
To make the transliteration process easier and
more accurate, we have decided to build a 2-phase
transliteration module. Our goal is to make deci-
sion that a particular Indian language string is in an
English gazetteer or not. We need not transliterate
directly from Indian language strings to English or
English name lists into Indian languages. Our idea is
to define an intermediate alphabet and both English
and Indian language strings will be transliterated to
The 6th Workshop on Asian Languae Resources, 2008
11
the intermediate alphabet. For two English-Hindi
string pair, if the intermediate alphabet is same then
we can conclude that one string is the transliteration
of the other.
First of all we need to decide the size of the inter-
mediate alphabet. Preserving the phonetic proper-
ties we have defined our intermediate alphabet con-
sisting of 34 characters. To indicate these 34 charac-
ters, we have given unique character-id to each char-
acter.
3.2 English to Intermediate Alphabet
Transliteration
For transliterating English strings into the interme-
diate state, we have built a phonetic map table. This
phonetic map table maps an English n-gram into an
intermediate character. A part of the map table is
given in Table 2. In the map table, the mapping is
from strings of varying length in the English to one
character in the intermediate alphabet. In our table
the length of the left hand side varies from 1 to 3.
English Intermediate
A a?
EE, I, II ??
OO, U u?
B, W ?b
BH, V v?
CH c?
R, RH r?
SH, S s?
Table 2: A part of the map table
In the following we have described the procedure
of transliteration.
Procedure 1: Transliteration
Source string - English, Output string - Intermediate.
1. Scan the source string (S) from left to right.
2. Extract the first n-gram (G) from the string.
(n = 3)
3. Find if it is in the map-table.
4. If yes, insert its corresponding intermediate
state entity into target string T.
Remove the n-gram from S.
S = S ? G.
Go to step 2.
5. Else set n = n? 1.
Go to step 3.
Here we can take an Indian language name,
?surabhi?, as example to explain the procedure in
details. When the name is written in English, it
may be written in several ways like ?suravi, ?shu-
ravi?, ?surabhee?, ?shurabhi? etc. The English string
?surabhi? is transliterated to ?s?u?r?a?v???? by the translit-
erator. Again if we see the transliteration for ?shu-
ravi?, then also the intermediate transliterated string
is same as the previous one.
3.3 Indian Language to Intermediate Alphabet
Transliteration
This is a 2-phase process. The first phase transliter-
ates the Indian language string into itrans. Itrans is
representation of Indian language alphabets in terms
of ASCII. Since Indian text is composed of syllabic
units rather than individual alphabetic letters, itrans
uses combinations of two or more letters of En-
glish alphabet to represent an Indian language syl-
lable. However, there being multiple sounds in In-
dian languages corresponding to the same English
letter, not all Indian syllables can be represented by
logical combinations of English alphabet. Hence,
itrans uses some non-alphabetic special characters
also in some of the syllables. A map table2, with
some heuristic knowledge, is used for the translit-
eration. For example, the Hindi word ?surabhi? is
converted ?sUrabhI? in itrans.
In the second phase the itrans string is translit-
erated into the intermediate state using the similar
procedure described section 3.2. Here also we use
a map-table containing the mappings from itrans to
intermediate alphabet. This procedure transliterates
the example itrans word ?sUrabhI? to ?s?u?r?a?v????.
3.4 Evaluation
In section 3.2 and 3.3 we have described two phase
transliteration with an example word. We have
shown that our transliteration system transliterates
the Indian language name ?surabhi? and the corre-
sponding English strings into the same intermediate
2The map table is available at www.aczoom.com/itrans.
The 6th Workshop on Asian Languae Resources, 2008
12
string. The system has limitations like sometimes
two different strings can be mapped into a same in-
termediate alphabet string.
For the evaluation of the system we have applied
the transliteration system for two languages - Hindi
and Bengali. For evaluating the system for Hindi
we have created a bi-lingual corpus containing 1070
English-Hindi word pair most of which are names.
980 of them are transliterated correctly by the sys-
tem. The system accuracy is 980 ? 100/1070 =
91.59%. For evaluating the system for Bengali, we
have used a similar bi-lingual corpus and the system
transliterates with 89.3% accuracy.
3.5 Prepared Gazetteer Lists
Previously we have mentioned the web sources
where some name lists are available. Names of
a particular category are collected from different
sources and merged to build a English name list of
that category. Then we have applied our transliter-
ation procedure on the list and transliterated the list
into the intermediate alphabet. This intermediate al-
phabet list acts as a gazetteer in NER task in Indian
languages. When an Indian language NER system
needs to access the gazetteer lists, it transliterates the
Indian language strings into the intermediate alpha-
bet, and searches into the list. In the following we
have described the prepared gazetteer lists which are
useful for a general domain Indian language NER
system.
First Name List: This list contains 10,200 first
names collected from the web. Most of the collected
first names are of Indian origin. Apart from the In-
dian names, we have also collected some non-Indian
names. These non-Indian names are generally the
names of some famous persons, like sports stars,
film stars, scientists, politicians, who are likely to
come in Indian language texts. In our first name list
1500 such names are included.
Surname List: This is a very important list which
contains common surnames. We have prepared the
surname list from different sources containing about
1500 Indian surnames and 400 other surnames.
Indian Locations: This list contains about
14,000 entities. The names of states, cities and
towns, districts, important places in different cities
and even lots of village names are collected in the
list. The list needs to be processed into a list of un-
igrams (e.g., kolakAtA3 (Kolkata), bihAra (Bihar)),
bigrams (e.g., nayI dillI (New Delhi), pashchima
bA.nglA (West Bengal)) and trigrams (e.g. uttaara
chabisha paraganA (North 24 Pargana)). The words
are matched with unigrams, sequences of two con-
secutive words are matched against bigrams and
sequences of three consecutive words are matched
against trigrams.
World Location: The list contains the names of
the countries, different state and city names in world
and also the names of important rivers, mountains
etc. The list contains about 4,000 location names.
Similar to the Indian location list, this list also needs
to be processed as unigram, bigrams and trigrams.
4 Context Pattern Extraction based
Gazetteer Preparation
Gazetteers can also be prepared by extracting con-
text patterns. Transliteration based gazetteer prepa-
ration is applicable while there is availability of En-
glish or parallel language name list. But if such
relevant name lists are not available, but a large
raw corpus is available, then we can use the con-
text pattern extraction based methodology to prepare
the gazetteers. This method seeks some high pre-
cision context patterns by using some seed entities
and hits the patterns to the raw corpus to prepare the
gazetteers.
The overall methodology of extracting context
patterns from a raw corpus is summarized as fol-
lows:
1. Find a large raw corpus and some seed entities
(E) for each class of NEs.
2. For each seed entity e in E, from the corpus
find context string(C) comprised of n tokens
before e, a placeholder for the class instance
and n tokens after e. [We have used n = 3]
This set of words form initial pattern.
3. Search the pattern to the corpus and find the
coverage and precision.
4. Discard the patterns having low precision.
5. Generalize the patterns by dropping one or
more tokens to increase coverage.
3The Indian languages strings are written in italics font and
using itrans transliteration.
The 6th Workshop on Asian Languae Resources, 2008
13
6. Find best patterns having good precision and
coverage.
The details of the context pattern extraction
based gazetteer preparation methodology is de-
scribed in the following subsections. We have taken
a Hindi sports domain raw corpus and prepared
some gazetteers like names of cricketers, names of
tennis players to prove the effectiveness of the pro-
posed methodology.
4.1 Selection of Seed Entity
Context pattern extraction based gazetteer prepa-
ration methodology is applied to a sports domain
corpus which contains about 20 lakhs words col-
lected from the popular Hindi newspaper ?Dainik
Jagaran?. We have worked on preparing the lists
of cricket players, list of tennis players. We have
collected the most frequent names to build the seed
list. For preparing the list of tennis players, we
have taken 5 names as seed entities : Andre Agassi,
Steffi Graf, Serena Williams, Roger Federer and Jus-
tine Henin. Similarly the seed list of cricket players
name list contains only 3 names: Sachin Tendulkar,
Brian Lara and Glenn McGrath.
4.2 Context Extraction
To extract the patterns for a particular category, we
select a part of the corpus where the target seeds will
be available with high frequency. For example to
get the patterns for the names of the cricketers, we
select a part of the corpus where most of the sen-
tences are cricket related. To select the cricket re-
lated sentences, we prepared a list containing the
most frequent words related to cricket like, rAna
(run), ballebAja (batsman), gedabAja (bowler) etc.
Depending upon the presence of such words we have
selected the ?part?. In our development the cricket
?part? contains 120K words. Similar ?part? is devel-
oped for other gazetteers. For a particular seed, we
find the occurrences of the seed entity in the corre-
sponding raw ?part? corpus. Then we extracted three
tokens immediately preceding the seed and three to-
kens immediately following the seed. A placeholder
(CRIC for cricketers, TENS for tennis players)
replaces the seed. The placeholder and the surround-
ing tokens t?3 t?2 t?1 placeholder t+1 t+2 t+3)
form the initial set of patterns.
For the seed sachina tedulkara (Sachin Ten-
dulkar) we extract 92 initial patterns. Some of which
are:
? ki mAsTara blAsTara CRIC ko Tima ke
? dravi.Da aura CRIC 241 nAbAda ke
? bhAratiya ballebAja CRIC ne 100 rana
? mere vichAra se CRIC ko Takkara dene
4.3 Pattern Quality Measure
We measured the quality of a pattern depending on
its precision and coverage. Precision is the ratio of
correct identification and the total identification. If
the precision is high then also we have assumed that
the pattern is a good pattern. In our development
we have marked a pattern as good if the precision is
100%.
We search the initial patterns in the correspond-
ing ?part? corpus to measure the precision and cov-
erage. If the precision is less than 100% for a pat-
tern then we have rejected the pattern. Otherwise we
have tried to make it more generalized to increase
the coverage. To make the generalization we have
dropped the left most and right most tokens one by
one and checked the pattern quality. If for a initial
pattern, several patterns presents with 100% preci-
sion then we have selected those patterns for which
no subset of those is a good pattern. By this way we
have prepared a list of good patterns for a particular
gazetteer type.
In time of ?good? pattern selection we have made
some interesting observations.
? There are some patterns which satisfy the 100%
precision criteria but the coverage is very poor
in terms of new entity extraction. For exam-
ple, ?mAsTara blAsTara CRIC ko? is a pattern
with 100% precision. The pattern has 24 in-
stances in the ?part? corpus, but all the extracted
entities are ?sachina tedulkara?. We have also
examined the pattern in the total raw corpus.
It is capable of extracting ?sachina tedulkara?
only. So in spite of fulfilling all the criteria the
pattern is not a ?good? pattern.
? Another interesting observation is, that there
are some patterns which are ?good? patterns in
The 6th Workshop on Asian Languae Resources, 2008
14
the context of the ?part? corpus, but when used
in the total raw corpus, it extracts non-relevant
entities. For example ?mere vichAra se CRIC
ko Takkara? is a ?good? pattern so it should ex-
tract the names of the cricketers. But when
this is used in the total raw corpus it extracts
non-cricketer entities (e.g. tennis players, chess
players) also. To make such patterns useful we
have extracted all the cricket related sentences
in the similar way which was used for selecting
the ?part? corpus and then the patterns are used
to extract entities from these sentences.
? There present are patterns with very high cover-
age but precision is just below 100%. We have
analyzed these patterns and manually identified
the wrongly extracted entities. If the wrong en-
tities can be grouped together and are having
some specific properties then we have added
these entities in a ?pattern exception list?. Then
the pattern is used as a ?good? pattern and the
exception list is used to detect the wrong iden-
tifications.
In the following we have given some example of
?good? patterns which are useful in identification of
the names of the cricket players.
? ballebAja CRIC ko Tima ke
? ballebAja CRIC ne
? CRIC kA arddhashataka
4.4 Gazetteer Preparation
The extracted ?good? patterns are capable of iden-
tifying NEs from a raw corpus. These patterns are
then used to prepare the gazetteers. The seeds form
the initial gazetteer list for a particular gazetteer
type. The ?good? patterns are used to extract entities
from the total raw corpus. The entities identified by
the patterns are added to the corresponding gazetteer
list. In that way we can add more entities in our first
phase gazetteer list. These new entities are taken as
seeds for the next phase. Then the same procedure
is followed repeatedly to develop a large gazetteer.
We have already mentioned that we have worked
with a sports domain corpus and prepared some
gazetteers. This gazetteers are prepared just to prove
the efficiency of our approach. By using only 3 seed
entities we become able to prepare a gazetteer which
contains 412 names of the cricketers. Even using
this approach only one seed ?Sachin Tendualkar? ex-
tracts 297 names after the second iteration. Similarly
we have collected 245 names of tennis players from
5 seed entities.
5 Conclusion
In this paper we have described our approaches for
the preparation of gazetteers. We have also prepared
some gazetteers using both the approaches to show
their effectiveness. These approaches are very use-
ful for the NER task in resource-poor languages and
also in domain specific NER task.
References
Al-Onaizan Y. and Knight K. 2002. Machine Translit-
eration of Names in Arabic Text. Proceedings of
the ACL Workshop on Computational Approaches to
Semitic Languages.
Bikel Daniel M., Miller Scott, Schwartz Richard and
Weischedel Ralph. 1997. Nymble: A high perfor-
mance learning name-finder. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, peges 194?201.
Borthwick Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
Computer Science Department, New York University.
Downey D., Etzioni O., Soderland S., Weld D.S. 2004.
Learning text patterns for Web information extraction
and assessment. In AAAI-04 Workshop on Adaptive
Text Extraction and Mining, pages 50?55.
Ekbal A. and Bandyopadhyay S. 2007. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of International Conference on
Natural Language Processing (ICON), 2007.
Ekbal A., Naskar S. and Bandyopadhyay S. 2006. A
Modified Joint Source Channel Model for Translitera-
tion. In Proceedings of the COLING/ACL 2006, Aus-
tralia, pages 191?198.
Etzioni Oren, Cafarella Michael, Downey Doug, Popescu
Ana-Maria, Shaked Tal, Soderland Stephen, Weld
Daniel S. and Yates Alexander. 2005. Unsupervised
named-entity extraction from the Web: An experimen-
tal study. In Artificial Intelligence, 165(1): 91-134.
Grishman Ralph. 1995. The New York University Sys-
tem MUC-6 or Where?s the syntax? In Proceedings of
the Sixth Message Understanding Conference.
The 6th Workshop on Asian Languae Resources, 2008
15
Knight K. and Graehl J. 1998. Machine Transliteration.
Computational Linguistics, 24(4): 599?612.
Kozareva Zornitsa. 2006. Bootstrapping Named Entity
Recognition with Automatically Generated Gazetteer
Lists. In Proceedings of EACL student session (EACL
2006).
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Condi-
tional Random Fields and Feature Induction (Short Pa-
per). In ACM Transactions on Computational Logic.
Lin Winston, Yangarber Roman and Grishman Ralph.
2003. Bootstrapped learning of semantic classes from
positive and negative examples. In Proceedings of
ICML-2003 Workshop on The Continuum from La-
beled to Unlabeled Data.
Riloff E. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Articial Intelli-
gence, pages 1044?1049.
Srihari R., Niu C. and Li W. 2000. A Hybrid Approach
for Named Entity and Sub-Type Tagging. In Proceed-
ings of the sixth conference on Applied natural lan-
guage processing.
Soderland Stephen, Fisher David, Aseltine Jonathan,
Lehnert Wendy. 1995. CRYSTAL: Inducing a Con-
ceptual Dictionary. In Proceedings of the Fourteenth
International Joint Conference on Artificial Intelli-
gence.
Talukdar P. Pratim, T. Brants, M. Liberman and F.
Pereira. 2006. A context pattern induction method
for named entity extraction. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X).
Wakao T., Gaizauskas R. and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
The 6th Workshop on Asian Languae Resources, 2008
16
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 221?224,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Automatic Part-of-Speech Tagging for Bengali: An Approach for 
Morphologically Rich Languages in a Poor Resource Scenario 
Sandipan Dandapat, Sudeshna Sarkar, Anupam Basu 
Department of Computer Science and Engineering 
Indian Institute of Technology Kharagpur 
India 721302 
{sandipan,sudeshna,anupam.basu}@cse.iitkgp.ernet.in 
 
Abstract 
This paper describes our work on build-
ing Part-of-Speech (POS) tagger for 
Bengali. We have use Hidden Markov 
Model (HMM) and Maximum Entropy 
(ME) based stochastic taggers. Bengali is 
a morphologically rich language and our 
taggers make use of morphological and 
contextual information of the words.  
Since only a small labeled training set is 
available (45,000 words), simple stochas-
tic approach does not yield very good re-
sults. In this work, we have studied the 
effect of using a morphological analyzer 
to improve the performance of the tagger. 
We find that the use of morphology helps 
improve the accuracy of the tagger espe-
cially when less amount of tagged cor-
pora are available. 
1 Introduction 
Part-of-Speech (POS) taggers for natural lan-
guage texts have been developed using linguistic 
rules, stochastic models as well as a combination 
of both (hybrid taggers). Stochastic models (Cut-
ting et al, 1992; Dermatas et al, 1995; Brants, 
2000) have been widely used in POS tagging for 
simplicity and language independence of the 
models. Among stochastic models, bi-gram and 
tri-gram Hidden Markov Model (HMM) are 
quite popular. Development of a high accuracy 
stochastic tagger requires a large amount of an-
notated text. Stochastic taggers with more than 
95% word-level accuracy have been developed 
for English, German and other European Lan-
guages, for which large labeled data is available. 
Our aim here is to develop a stochastic POS tag-
ger for Bengali but we are limited by lack of a 
large annotated corpus for Bengali. Simple 
HMM models do not achieve high accuracy 
when the training set is small. In such cases, ad-
ditional information may be coded into the 
HMM model to achieve higher accuracy (Cutting 
et al, 1992). The semi-supervised model de-
scribed in Cutting et al (1992), makes use of 
both labeled training text and some amount of 
unlabeled text. Incorporating a diverse set of 
overlapping features in a HMM-based tagger is 
difficult and complicates the smoothing typically 
used for such taggers. In contrast, methods based 
on Maximum Entropy (Ratnaparkhi, 1996), 
Conditional Random Field (Shrivastav, 2006) 
etc. can deal with diverse, overlapping features. 
1.1 Previous Work on Indian Language 
POS Tagging 
Although some work has been done on POS tag-
ging of different Indian languages, the systems 
are still in their infancy due to resource poverty. 
Very little work has been done previously on 
POS tagging of Bengali. Bengali is the main 
language spoken in Bangladesh, the second most 
commonly spoken language in India, and the 
fourth most commonly spoken language in the 
world. Ray et al (2003) describes a morphology-
based disambiguation for Hindi POS tagging. 
System using a decision tree based learning algo-
rithm (CN2) has been developed for statistical 
Hindi POS tagging (Singh et al, 2006). A rea-
sonably good accuracy POS tagger for Hindi has 
been developed using Maximum Entropy 
Markov Model (Dalal et al, 2007). The system 
uses linguistic suffix and POS categories of a 
word along with other contextual features. 
2 Our Approach 
The problem of POS tagging can be formally 
stated as follows. Given a sequence of words w1 
? wn, we want to find the corresponding se-
quence of tags t1 ? tn, drawn from a set of tags T. 
We use a tagset of 40 tags1. In this work, we ex-
plore supervised and semi-supervised bi-gram 
                                                 
1 http://www.mla.iitkgp.ernet.in/Tag.html 
221
 HMM and a ME based model. The bi-gram as-
sumption states that the POS-tag of a word de-
pends on the current word and the POS tag of the 
previous word. An ME model estimates the prob-
abilities based on the imposed constraints. Such 
constraints are derived from the training data, 
maintaining some relationship between features 
and outcomes. The most probable tag sequence 
for a given word sequence satisfies equation (1) 
and (2) respectively for HMM and ME model: 
1
1
... 1,
( | ) ( | )arg max i i i i
t tn i n
S P w t P t t ?
=
= ?      (1) 
1 1
1,
( ... | ... ) ( | )n n i i
i n
p t t w w p t h
=
= ?       (2) 
Here, hi is the context for word wi. Since the ba-
sic bigram model of HMM as well as the equiva-
lent ME models do not yield satisfactory accu-
racy, we wish to explore whether other available 
resources like a morphological analyzer can be 
used appropriately for better accuracy.  
2.1 HMM and ME based Taggers 
Three taggers have been implemented based on 
bigram HMM and ME model. The first tagger 
(we shall call it HMM-S) makes use of the su-
pervised HMM model parameters, whereas the 
second tagger (we shall call it HMM-SS) uses 
the semi supervised model parameters. The third 
tagger uses ME based model to find the most 
probable tag sequence for a given sequence of 
words.  
 
In order to further improve the tagging accuracy, 
we use a Morphological Analyzer (MA) and in-
tegrate morphological information with the mod-
els. We assume that the POS-tag of a word w can 
take values from the set TMA(w), where TMA(w) is 
computed by the Morphological Analyzer. Note 
that the size of TMA(w) is much smaller than T. 
Thus, we have a restricted choice of tags as well 
as tag sequences for a given sentence. Since the 
correct tag t for w is always in TMA(w) (assuming 
that the morphological analyzer is complete), it is 
always possible to find out the correct tag se-
quence for a sentence even after applying the 
morphological restriction. Due to a much re-
duced set of possibilities, this model is expected 
to perform better for both the HMM (HMM-S 
and HMM-SS) and ME models even when only a 
small amount of labeled training text is available. 
We shall call these new models HMM-S+MA, 
HMM-SS+ MA and ME+MA.  
 
Our MA has high accuracy and coverage but it 
still has some missing words and a few errors. 
For the purpose of these experiments we have 
made sure that all words of the test set are pre-
sent in the root dictionary that an MA uses. 
 
While MA helps us to restrict the possible choice 
of tags for a given word, one can also use suffix 
information (i.e., the sequence of last few charac-
ters of a word) to further improve the models. 
For HMM models, suffix information has been 
used during smoothing of emission probabilities, 
whereas for ME models, suffix information is 
used as another type of feature. We shall denote 
the models with suffix information with a ?+suf? 
marker. Thus, we have ? HMM-S+suf, HMM-
S+suf+MA, HMM-SS+suf etc. 
2.1.1 Unknown Word Hypothesis in HMM 
The transition probabilities are estimated by lin-
ear interpolation of unigrams and bigrams. For 
the estimation of emission probabilities add-one 
smoothing or suffix information is used for the 
unknown words. If the word is unknown to the 
morphological analyzer, we assume that the 
POS-tag of that word belongs to any of the open 
class grammatical categories (all classes of 
Noun, Verb, Adjective, Adverb and Interjection). 
2.1.2 Features of the ME Model 
Experiments were carried out to find out the 
most suitable binary valued features for the POS 
tagging in the ME model. The main features for 
the POS tagging task have been identified based 
on the different possible combination of the 
available word and tag context. The features also 
include prefix and suffix up to length four. We 
considered different combinations from the fol-
lowing set for obtaining the best feature set for 
the POS tagging task with the data we have. 
 { }11 2 2 1 2, , , , , , , 4, 4ii i i i i iF w w w w w t t pre suf+? ? + ? ?= ? ?  
 
Forty different experiments were conducted tak-
ing several combinations from set ?F? to identify 
the best suited feature set for the POS tagging 
task. From our empirical analysis we found that 
the combination of contextual features (current 
word and previous tag), prefixes and suffixes of 
length ? 4 gives the best performance for the ME 
model. It is interesting to note that the inclusion 
of prefix and suffix for all words gives better 
result instead of using only for rare words as is 
described in Ratnaparkhi (1996). This can be 
explained by the fact that due to small amount of 
annotated data, a significant number of instances 
222
 are not found for most of the word of the 
language vocabulary.   
3 Experiments 
We have a total of 12 models as described in 
subsection 2.1 under different stochastic tagging 
schemes. The same training text has been used to 
estimate the parameters for all the models. The 
model parameters for supervised HMM and ME 
models are estimated from the annotated text 
corpus. For semi-supervised learning, the HMM 
learned through supervised training is considered 
as the initial model. Further, a larger unlabelled 
training data has been used to re-estimate the 
model parameters of the semi-supervised HMM. 
The experiments were conducted with three dif-
ferent sizes (10K, 20K and 40K words) of the 
training data to understand the relative perform-
ance of the models as we keep on increasing the 
size of the annotated data.   
3.1 Training Data 
The training data includes manually annotated 
3625 sentences (approximately 40,000 words) 
for both supervised HMM and ME model. A 
fixed set of 11,000 unlabeled sentences (ap-
proximately 100,000 words) taken from CIIL 
corpus 2  are used to re-estimate the model pa-
rameter during semi-supervised learning. It has 
been observed that the corpus ambiguity (mean 
number of possible tags for each word) in the 
training text is 1.77 which is much larger com-
pared to the European languages (Dermatas et 
al., 1995).  
3.2 Test Data 
All the models have been tested on a set of ran-
domly drawn 400 sentences (5000 words) dis-
joint from the training corpus. It has been noted 
that 14% words in the open testing text are un-
known with respect to the training set, which is 
also a little higher compared to the European 
languages (Dermatas et al, 1995) 
3.3 Results 
We define the tagging accuracy as the ratio of 
the correctly tagged words to the total number of 
words. Table 1 summarizes the final accuracies 
achieved by different learning methods with the 
varying size of the training data. Note that the 
baseline model (i.e., the tag probabilities depends 
                                                 
2 A part of the EMILE/CIIL corpus developed at Cen-
tral Institute of Indian Languages (CIIL), Mysore. 
only on the current word) has an accuracy of 
76.8%.  
 
Accuracy Method 
10K 20K 40K 
HMM-S 57.53 70.61 77.29 
HMM-S+suf 75.12 79.76 83.85 
HMM-S+MA 82.39 84.06 86.64 
HMM-S+suf+MA 84.73 87.35 88.75 
HMM-SS 63.40 70.67 77.16 
HMM-SS+suf 75.08 79.31 83.76 
HMM-SS+MA 83.04 84.47 86.41 
HMM-SS+suf+MA 84.41 87.16 87.95 
ME 74.37 79.50 84.56 
ME+suf 77.38 82.63 86.78 
ME+MA 82.34 84.97 87.38 
ME+suf+MA 84.13 87.07 88.41 
Table 1: Tagging accuracies (in %) of different 
models with 10K, 20K and 40K training data. 
3.4 Observations 
We find that in both the HMM based models 
(HMM-S and HMM-SS), the use of suffix in-
formation as well as the use of a morphological 
analyzer improves the accuracy of POS tagging 
with respect to the base models. The use of MA 
gives better results than the use of suffix infor-
mation. When we use both suffix information as 
well as MA, the results is even better. 
 
HMM-SS does better than HMM-S when very 
little tagged data is available, for example, when 
we use 10K training corpus. However, the accu-
racy of the semi-supervised HMM models are 
slightly poorer than that of the supervised HMM 
models for moderate size training data and use of 
suffix information. This discrepancy arises due 
to the over-fitting of the supervised models in the 
case of small training data; the problem is allevi-
ated with the increase in the annotated data. 
 
As we have noted already the use of MA and/or 
suffix information improves the accuracy of the 
POS tagger. But what is significant to note is that 
the percentage of improvement is higher when 
the amount of training data is less. The HMM-
S+suf model gives an improvement of around 
18%, 9% and 6% over the HMM-S model for 
10K, 20K and 40K training data respectively. 
Similar trends are observed in the case of the 
semi-supervised HMM and the ME models. The 
use of morphological restriction (HMM-S+MA) 
gives an improvement of 25%, 14% and 9% re-
spectively over the HMM-S in case of 10K, 20K 
223
 and 40K training data. As the improvement due 
to MA decreases with increasing data, it might 
be concluded that the use of morphological re-
striction may not improve the accuracy when a 
large amount of training data is available. From 
our empirical observations we found that both 
suffix and morphological restriction (HMM-
S+suf+MA) gives an improvement of 27%, 17% 
and 12% over the HMM-S model respectively 
for the three different sizes of training data. 
 
The Maximum Entropy model does better than 
the HMM models for smaller training data. But 
with higher amount of training data the perform-
ance of the HMM and ME model are compara-
ble. Here also we observe that suffix information 
and MA have positive effect, and the effect is 
higher with poor resources.  
 
Furthermore, in order to estimate the relative per-
formance of the models, experiments were car-
ried out with two existing taggers: TnT (Brants, 
2000) and ACOPOST3. The accuracy achieved 
using TnT are 87.44% and 87.36% respectively 
with bigram and trigram model for 40K training 
data. The accuracy with ACOPOST is 86.3%.  
This reflects that the higher order Markov mod-
els do not work well under the current experi-
mental setup.  
3.5 Assessment of Error Types 
Table 2 shows the top five confusion classes for 
HMM-S+MA model. The most common types of 
errors are the confusion between proper noun 
and common noun and the confusion between 
adjective and common noun. This results from 
the fact that most of the proper nouns can be 
used as common nouns and most of the adjec-
tives can be used as common nouns in Bengali.  
 
Actual 
Class 
(frequency) 
Predicted 
Class 
% of total 
errors 
% of 
class 
errors 
NP(251) NN 21.03 43.82 
JJ(311) NN 5.16 8.68 
NN(1483) JJ 4.78 1.68 
DTA(100) PP 2.87 15.0 
NN(1483) VN 2.29 0.81 
Table 2: Five most common types of errors  
Almost all the confusions are wrong assignment 
due to less number of instances in the training 
corpora, including errors due to long distance 
phenomena. 
                                                 
3 http://maxent.sourceforge.net 
4 Conclusion 
In this paper we have described an approach for 
automatic stochastic tagging of natural language 
text for Bengali. The models described here are 
very simple and efficient for automatic tagging 
even when the amount of available annotated 
text is small. The models have a much higher 
accuracy than the na?ve baseline model. How-
ever, the performance of the current system is 
not as good as that of the contemporary POS-
taggers available for English and other European 
languages. The best performance is achieved for 
the supervised learning model along with suffix 
information and morphological restriction on the 
possible grammatical categories of a word. In 
fact, the use of MA in any of the models dis-
cussed above enhances the performance of the 
POS tagger significantly. We conclude that the 
use of morphological features is especially help-
ful to develop a reasonable POS tagger when 
tagged resources are limited.  
References 
A. Dalal, K. Nagaraj, U. Swant, S. Shelke and P. 
Bhattacharyya. 2007. Building Feature Rich POS 
Tagger for Morphologically Rich Languages: Ex-
perience in Hindi. ICON, 2007. 
A. Ratnaparkhi, 1996. A maximum entropy part-of-
speech tagger. EMNLP 1996. pp. 133-142. 
D. Cutting, J. Kupiec, J. Pederson and P. Sibun. 1992. 
A practical part-of-speech tagger. In Proc. of the 
3rd Conference on Applied NLP, pp. 133-140.  
E. Dermatas and K. George. 1995. Automatic stochas-
tic tagging of natural language texts. Computa-
tional Linguistics, 21(2): 137-163. 
M. Shrivastav, R. Melz, S. Singh, K. Gupta and 
P. Bhattacharyya, 2006. Conditional Random 
Field Based POS Tagger for Hindi. In Pro-
ceedings of the MSPIL, pp. 63-68. 
P. R. Ray, V. Harish, A. Basu and S. Sarkar, 2003. 
Part of Speech Tagging and Local Word Grouping 
Techniques for Natural Language Processing.  
ICON 2003. 
S. Singh, K. Gupta, M. Shrivastav and P. Bhat-
tacharyya, 2006. Morphological Richness Offset 
Resource Demand ? Experience in constructing a 
POS Tagger for Hindi. COLING/ACL 2006, pp. 
779-786. 
T. Brants. 2000. TnT ? A statistical part-of-sppech 
tagger. In Proc. of the 6th Applied NLP Conference, 
pp. 224-231.  
224
Proceedings of ACL-08: HLT, pages 488?495,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Word Clustering and Word Selection based Feature Reduction for MaxEnt
based Hindi NER
Sujan Kumar Saha
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
sujan.kr.saha@gmail.com
Pabitra Mitra
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
pabitra@gmail.com
Sudeshna Sarkar
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
shudeshna@gmail.com
Abstract
Statistical machine learning methods are em-
ployed to train a Named Entity Recognizer
from annotated data. Methods like Maxi-
mum Entropy and Conditional Random Fields
make use of features for the training purpose.
These methods tend to overfit when the avail-
able training corpus is limited especially if the
number of features is large or the number of
values for a feature is large. To overcome
this we proposed two techniques for feature
reduction based on word clustering and se-
lection. A number of word similarity mea-
sures are proposed for clustering words for
the Named Entity Recognition task. A few
corpus based statistical measures are used for
important word selection. The feature reduc-
tion techniques lead to a substantial perfor-
mance improvement over baseline Maximum
Entropy technique.
1 Introduction
Named Entity Recognition (NER) involves locat-
ing and classifying the names in a text. NER is
an important task, having applications in informa-
tion extraction, question answering, machine trans-
lation and in most other Natural Language Process-
ing (NLP) applications. NER systems have been de-
veloped for English and few other languages with
high accuracy. These belong to two main cate-
gories based on machine learning (Bikel et al, 1997;
Borthwick, 1999; McCallum and Li, 2003) and lan-
guage or domain specific rules (Grishman, 1995;
Wakao et al, 1996).
In English, the names are usually capitalized
which is an important clue for identifying a name.
Absence of capitalization makes the Hindi NER task
difficult. Also, person names are more diverse in In-
dian languages, many common words being used as
names.
A pioneering work on Hindi NER is by Li and
McCallum (2003) where they used Conditional Ran-
dom Fields (CRF) and feature induction to auto-
matically construct only the features that are impor-
tant for recognition. In an effort to reduce overfit-
ting, they use a combination of a Gaussian prior and
early-stopping.
In their Maximum Entropy (MaxEnt) based ap-
proach for Hindi NER development, Saha et al
(2008) also observed that the performance of the
MaxEnt based model often decreases when huge
number of features are used in the model. This is
due to overfitting which is a serious problem in most
of the NLP tasks in resource poor languages where
annotated data is scarce.
This paper is a study on effectiveness of word
clustering and selection as feature reduction tech-
niques for MaxEnt based NER. For clustering we
use a number of word similarities like cosine sim-
ilarity among words and co-occurrence, along with
the k-means clustering algorithm. The clusters are
then used as features instead of words. For impor-
tant word selection we use corpus based statistical
measurements to find the importance of the words in
the NER task. A significant performance improve-
ment over baseline MaxEnt was observed after using
the above feature reduction techniques.
The paper is organized as follows. The MaxEnt
488
based NER system is described in Section 2. Vari-
ous approaches for word clustering are discussed in
Section 3. Next section presents the procedure for
selecting the important words. In Section 5 experi-
mental results and related discussions are given. Fi-
nally Section 6 concludes the paper.
2 Maximum Entropy Based Model for
Hindi NER
Maximum Entropy (MaxEnt) principle is a com-
monly used technique which provides probability of
belongingness of a token to a class. MaxEnt com-
putes the probability p(o|h) for any o from the space
of all possible outcomes O, and for every h from
the space of all possible histories H . In NER, his-
tory can be viewed as all information derivable from
the training corpus relative to the current token. The
computation of probability (p(o|h)) of an outcome
for a token in MaxEnt depends on a set of features
that are helpful in making predictions about the out-
come. The features may be binary-valued or multi-
valued. Given a set of features and a training corpus,
the MaxEnt estimation process produces a model in
which every feature fi has a weight ?i. We can
compute the conditional probability as (Berger et al,
1996):
p(o|h) =
1
Z(h)
?
i
?i
fi(h,o) (1)
Z(h) =
?
o
?
i
?i
fi(h,o) (2)
The conditional probability of the outcome is the
product of the weights of all active features, normal-
ized over the products of all the features. For our
development we have used a Java based open-nlp
MaxEnt toolkit1. A beam search algorithm is used
to get the most probable class from the probabilities.
2.1 Training Corpus
The training data for the Hindi NER task is com-
posed of about 243K words which is collected
from the popular daily Hindi newspaper ?Dainik
Jagaran?. This corpus has been manually anno-
tated and contains about 16,491 Named Entities
(NEs). In this study we have considered 4 types
1http://sourceforge.net/projects/maxent/
Type Features
Word wi, wi?1, wi?2, wi+1, wi+2
NE Tag ti?1, ti?2
Digit infor-
mation
Contains digit, Only digit, Four
digit, Numerical word
Affix infor-
mation
Fixed length suffix, Suffix list,
Fixed length prefix
POS infor-
mation
POS of words, Coarse-grained
POS, POS based binary features
Table 1: Features used in the MaxEnt based Hindi NER
system
of NEs, these are Person (Per), Location (Loc),
Organization (Org) and Date (Dat). To recognize
entity boundaries each name class N has 4 types
of labels: N Begin, N Continue, N End and
N Unique. For example, Kharagpur is annotated
as Loc Unique and Atal Bihari Vajpeyi is annotated
as Per Begin Per Continue Per End. Hence,
there are a total of 17 classes including one class for
not-name. The corpus contains 6298 person, 4696
location, 3652 organization and 1845 date entities.
2.2 Feature Description
We have identified a number of candidate features
for the Hindi NER task. Several experiments were
conducted with the identified features, individually
and in combination. Some of the features are men-
tioned below. They are summarized in Table 1.
Static Word Feature: Recognition of NE is
highly dependent on contexts. So the surrounding
words of a particular word (wi) are used as fea-
tures. During our experiments different combina-
tions of previous 3 words (wi?3...wi?1) to next 3
words (wi+1...wi+3) are treated as features. This is
represented by L binary features where L is the size
of lexicon.
Dynamic NE tag: NE tags of the previous words
(ti?m...ti?1) are used as features. During decoding,
the value of this feature for a word (wi) is obtained
only after the computation of the NE tag for the pre-
vious word (wi?1).
Digit Information: If a word (wi) contains
digit(s) then the feature ContainsDigit is set to 1.
This feature is used with some modifications also.
OnlyDigit, which is set to 1 if the word contains
489
Feature Id Feature Per Loc Org Dat Total
F1 wi, wi?1, wi+1 61.36 68.29 52.12 88.9 67.26
F2 wi, wi?1, wi?2, wi+1, wi+2 64.10 67.81 58 92.30 69.09
F3 wi, wi?1, wi?2, wi?3, wi+1,
wi+2, wi+3
60.42 67.81 51.48 90.18 66.84
F4 wi, wi?1, wi?2, wi+1, wi+2,
ti?1, ti?2, Suffix
66.67 73.36 58.58 89.09 71.2
F5 wi, wi?1, wi+1, ti?1, Suffix 69.65 75.8 59.31 89.09 73.42
F6 wi, wi?1, wi+1, ti?1, Prefix 66.67 71 58.58 87.8 70.02
F7 wi, wi?1, wi+1, ti?1, Prefix,
Suffix
70.61 71 59.31 89.09 72.5
F8 wi, wi?1, wi+1, ti?1, Suffix,
Digit
70.61 75.8 60.54 93.8 74.26
F9 wi, wi?1, wi+1, ti?1, POS (28
tags)
64.25 71 60.54 89.09 70.39
F10 wi, wi?1, wi+1, ti?1, POS
(coarse grained)
69.65 75.8 59.31 92.82 74.16
F11 wi, wi?1, wi+1, Ti?1, Suffix,
Digit, NomPSP
72.26 78.6 61.36 92.82 75.6
F12 wi, wi?1, wi+1, wi?2, wi+2,
Ti?1, Prefix, Suffix, Digit,
NomPSP
65.26 78.01 52.12 93.33 72.65
Table 2: F-values for different features in the MaxEnt based Hindi NER system
only digits, 4Digit, which is set to 1 if the word
contains only 4 digits, etc. are some modifications
of the feature which are helpful.
Numerical Word: For a word (wi) if it is a nu-
merical word i.e. word denoting a number (e.g. eka2
(one), do (two), tina (three) etc.) then the feature
NumWord is set to 1.
Word Suffix: Word suffix information is helpful
to identify the NEs. Two types of suffix features
have been used. Firstly a fixed length word suffix
(set of characters occurring at the end of the word) of
the current and surrounding words are used as fea-
tures. Secondly we compiled list of common suf-
fixes of place names in Hindi. For example, pura,
bAda, nagara etc. are location suffixes. We used
binary feature corresponding to the list - whether a
given word has a suffix from the list.
Word Prefix: Prefix information of a word may
be also helpful in identifying whether it is a NE. A
2All Hindi words are written in italics using the ?Itrans?
transliteration.
fixed length word prefix (set of characters occur-
ring at the beginning of the word) of current and
surrounding words are treated as features. List of
important prefixes, which are used frequently in the
NEs, are also effective.
Parts-of-Speech (POS) Information: The POS
of the current word and the surrounding words are
used as feature for NER. We have used a Hindi POS
tagger developed at IIT Kharagpur, India which has
an accuracy about 90%. We have used the POS val-
ues of the current and surrounding words as features.
We realized that the detailed POS tagging is not
very relevant. Since NEs are noun phrases, the noun
tag is very relevant. Further the postposition follow-
ing a name may give a clue to the NE type. So we de-
cided to use a coarse-grained tagset with only three
tags - nominal (Nom), postposition (PSP) and other
(O).
The POS information is also used by defining sev-
eral binary features. An example is the NomPSP
binary feature. The value of this feature is defined
to be 1 if the current word is nominal and the next
490
word is a PSP.
2.3 Performance of Hindi NER using MaxEnt
Method
The performance of the MaxEnt based Hindi NER
using the above mentioned features is reported here
as a baseline. We have evaluated the system us-
ing a blind test corpus of 25K words. The test
corpus contains 521 person, 728 location, 262 or-
ganization and 236 date entities. The accuracies
are measured in terms of the f-measure, which is
the weighted harmonic mean of precision and re-
call. Precision is the fraction of the correct anno-
tations and recall is the fraction of the total NEs
that are successfully annotated. The general formula
for measuring the f-measure or f-value is, F? =
(1+?2) . (precision . recall) \ (?2 . precision +
recall). Here the value of ? is taken as 1. In Table 2
we have shown the accuracy values for few feature
sets.
While experimenting with static word features,
we have observed that a window of previous and
next two words (wi?2...wi+2) gives best result
(69.09) using the word features only. But whenwi?3
and wi+3 are added with it, the f-value is reduced
to 66.84. Again when wi?2 and wi+2 are deducted
from the feature set (i.e. only wi?1 and wi+1 as fea-
ture), the f-value is reduced to 67.26. This demon-
strates thatwi?2 andwi+2 are helpful features in NE
identification.
When suffix, prefix and digit information are
added to the feature set, the f-value is increased upto
74.26. The value is obtained using the feature set
F8 [wi, wi?1, wi+1, ti?1, Suffix, Digit]. It is ob-
served that when wi?2 and wi+2 are added with the
feature, the accuracy decreases by 2%. It contra-
dicts the results using the word features only. An-
other interesting observation is that prefix informa-
tion are helpful features in NE identification as these
increase accuracy when separately added with the
word features (F6). Similarly the suffix information
helps in increasing the accuracy. But when both the
suffix and prefix information are used in combina-
tion along with the word features, the f-value is de-
creased. From Table 2, a f-value of 73.42 is obtained
using F5 [wi, wi?1, wi+1, ti?1, Suffix] but when
prefix information are added with it (F7), the f-value
is reduced to 72.5.
POS information are important features in NER.
In general it is observed that coarse grained POS
information performs better than the finer grained
POS information. The best accuracy (75.6 f-value)
of the baseline system is obtained using the binary
NomPSP feature along with word feature (wi?1,
wi+1), suffix and digit information. It is noted that
when wi?2, wi+2 and prefix information are added
with the best feature, the f-value is reduced to 72.65.
From the above discussion it is clear that the sys-
tem suffers from overfitting if a large number of fea-
tures are used to train the system. Note that the sur-
rounding word (wi?2, wi?1, wi+1, wi+2 etc.) fea-
tures can take any value from the lexicon and hence
are of high dimensionality. These cause the degra-
dation of performance of the system. However it is
obvious that few words in the lexicon are important
in identification of NEs.
To solve the problem of high dimensionality we
use clustering to group the words present in the cor-
pus into much smaller number of clusters. Then
the word clusters are used as features instead of
the word features (for surrounding words). For ex-
ample, our Hindi corpus contains 17,456 different
words, which are grouped into N (say 100) clusters.
Then for a particular word, it is assigned to a cluster
and the corresponding cluster-id is used as feature.
Hence the number of features is reduced to 100 in-
stead of 17,456.
Similarly, selection of important words can also
solve the problem of high dimensionality. As some
of the words in the lexicon play important role in
the NE identification process, we aim to select these
particular words. Only these important words are
used in NE identification instead of all words in the
corpus.
3 Word Clustering
Clustering is the process of grouping together ob-
jects based on their similarity. The measure of sim-
ilarity is critical for good quality clustering. We
have experimented with some approaches to com-
pute word-word similarity. These are described in
details in the following section.
491
3.1 Cosine Similarity based on Sentence Level
Co-occurrence
A word is represented by a binary vector of dimen-
sion same as the number of sentences in the cor-
pus. A component of the vector is 1 if the word
occurs in the corresponding sentence and zero oth-
erwise. Then we measure cosine similarity between
the word vectors. The cosine similarity between two
word vectors ( ~A and ~B) with dimension d is mea-
sured as:
CosSim( ~A, ~B) =
?
dAdBd
(
?
dA
2
d)
1
2 ? (
?
dB
2
d)
1
2
(3)
This measures the number of co-occurring sen-
tences.
3.2 Cosine Similarity based on Proximal
Words
In this measure a word is represented by a vector
having dimension same as the lexicon size. For
ease of implementation we have taken a dimen-
sion of 2 ? 200, where each component of the vec-
tor corresponds to one of the 200 most frequent
preceding and following words of a token word.
List Prev containing the most frequent (top 200)
previous words (wi?1 or wi?2 if wi is the first word
of a NE) and List Next contains 200 most frequent
next words (wi+1 or wi+2 if wi is the last word of a
NE). A particular word wk may occur several times
(say n) in the corpus. For each occurrence of wk
find if its previous word (wk?1 or wk?2) matches
any element of List Prev. If matches, then set 1 to
the corresponding position of the vector and set zero
to all other positions related to List Prev. Sim-
ilarly check the next word (wk+1 or wk+2) in the
List Next and find the values of the corresponding
positions. The final word vector ~Wk is obtained by
taking the average of all occurrences of wk. Then
the cosine similarity is measured between the word
vectors. This measures the similarity of the contexts
of the occurrences of the word in terms of the prox-
imal words.
3.3 Similarity based on Proximity to NE
Categories
Here, for each word (wi) in the corpus four binary
vectors are defined corresponding to two preceding
and two following positions (i-1, i-2, i+1, i+2). Each
binary vector is of dimension five corresponding
to four NE classes (Cj) and one for the not-name
class. For a particular word wk, find all the words
occur in a particular position (say, +1). Measure
the fraction (Pj(wk)) of these words belonging to a
class Cj . The component of the word vector ~Wk for
the position corresponding to Cj is Pj(wk).
Pj(wk) =
No. of times wk+1 is a NE of class Cj
Total occurrence of wk in corpus
The Euclidean distance is used to find the simi-
larity between the above word vectors as a similar-
ity measure. Some of the word vectors for the +1
position are given in Table 3. In this table we have
given the word vectors for a few Hindi words, which
are, sthita (located), shahara (city), jAkara (go), na-
gara (township), gA.nva (village), nivAsI (resident),
mishrA (a surname) and limiTeDa (ltd.). From the
table we observe that the word vectors are close for
sthita [0 0.478 0 0 0.522], shahara [0 0.585 0.001
0.024 0.39], nagara [0 0.507 0.019 0 0.474] and
gA.nva [0 0.551 0 0 0.449]. So these words are con-
sidered as close.
Word Per Loc Org Dat Not
sthita 0 0.478 0 0 0.522
shahara 0 0.585 0.001 0.024 0.39
jAkara 0 0.22 0 0 0.88
nagara 0 0.507 0.019 0 0.474
gA.nva 0 0.551 0 0 0.449
nivAsI 0.108 0.622 0 0 0.27
mishrA 0.889 0 0 0 0.111
limiTeDa 0 0 1 0 0
Table 3: Example of some word vectors for next (+1)
position (see text for glosses)
3.4 K-means Clustering
Using the above similarity measures we have used
the k-means algorithm. The seeds were randomly
selected. The value of k (number of clusters) was
varied till the best result is obtained.
4 Important Word Selection
It is noted that not all words are equally important
in determining the NE category. Some of the words
492
in the lexicon are typically associated with a partic-
ular NE category and hence have important role to
play in the classification process. We describe be-
low a few statistical techniques that has been used to
identify the important words.
4.1 Class Independent Important Word
Selection
We define context words as those which occur in
proximity of a NE. In other words, context words
are the words present in the wi?2, wi?1, wi+1
or wi+2 position if wi is a NE. Note that only a
subset of the lexicon are context words. For all
the context words, its N weight is calculated as
the ratio between the occurrence of the word as a
context word and its total number of occurrence in
the corpus. The context words having the higher
N weight are considered as important words for
NER. For our experiments we have considered top
500 words as important words.
N weight(wi) =
Occurrence of wi as context word
Total occurrence of wi in corpus
4.2 Important Words for Each Class
Similar to the class independent important word se-
lection from the contexts, important words are se-
lected for individual classes also. This is an exten-
sion of the previous context word considering only
NEs of a particular class. For person, location, or-
ganization and date classes we have considered top
150, 120, 50 and 50 words respectively as impor-
tant words. Four binary features are also defined for
these four classes. These are defined as having value
1 if any of the context words belongs to the impor-
tant words list for a particular class.
4.3 Important Words for Each Position
Position based important words are also selected
from the corpus. Here instead of context, particu-
lar positions are considered. Four lists are compiled
for two preceding and two following positions (-2,
-1, +1 and +2).
5 Evaluation of NE Recognition
The following subsections contain the experimental
results using word clustering and important word se-
lection. The results demonstrate the effectiveness of
k Per Loc Org Dat Total
20 66.33 74.57 43.64 91.30 69.54
50 64.13 76.35 52 93.62 71.7
80 66.33 74.57 53.85 93.62 72.08
100 70.1 73.1 57.7 96.62 72.78
120 66.15 73.43 54.9 93.62 71.52
150 66.88 74.94 53.06 95.65 72.33
200 66.09 73.82 52 92 71.13
Table 4: Variation of MaxEnt based system accuracy de-
pending on number of clusters (k)
word clustering and important word selection over
the baseline MaxEnt model.
5.1 Using Word Clusters
To evaluate the effectiveness of the clustering ap-
proaches in Hindi NER, we have used cluster fea-
tures instead of word features. For the surrounding
words, corresponding cluster-ids are used as feature.
Choice of k : We have already mentioned that,
for k-means clustering number of classes (k) should
be determined initially. To find suitable k we had
conducted the following experiments. We have se-
lected a feature F1 (mentioned in Table 2) and ap-
plied the clusters with different k as features replac-
ing the word features. In Table 4 we have summa-
rized the experimental results, in order to find a suit-
able k for clustering, the word vectors obtained us-
ing the procedure described in Section 3.3. From
the table we observe that the best result is obtained
when k is 100. We have used k = 100 for the sub-
sequent experiments for comparing the effectiveness
of the features. Similarly when we deal with all the
words in the corpus (17,465 words), we got best re-
sults when the words are clustered into 1100 clus-
ters. ?
The details of the comparison between the base-
line word features and the reduced features obtained
using clustering are given in Table 5. In general it
is observed that clustering has improved the perfor-
mance over baseline features. Using only cluster
features the system provides a maximum f-value of
74.26 where the corresponding word features give
f-value of 69.09.
Among the various similarity measures of clus-
tering, improved results are obtained using the clus-
493
Feature Using
Word
Features
Using
Clusters
(C1)
Using
Clusters
(C2)
Using
Clusters
(C3)
wi, window(-1, +1) 67.26 69.67 72.05 72.78
wi, window(-2, +2) 69.09 71.52 72.65 74.26
wi, window(-1, +1), Suffix 73.42 74.24 75.44 75.84
wi, window(-1, +1), Prefix, Suffix 72.5 74.76 75.7 76.33
wi, window(-1, +1), Prefix, Suffix, Digit 74.26 75.09 75.91 76.41
wi, window(-1, +1), Prefix, Suffix, Digit,
NomPSP
75.6 77.2 77.39 77.61
wi, window(-2, +2), Prefix, Suffix, Digit,
NomPSP
72.65 77.86 78.61 79.03
Table 5: F-values for different features in a MaxEnt based Hindi NER with clustering based feature reduction
[window(?m,+n) refers to the cluster or word features corresponding to previous m positions and next n posi-
tions; C1 is the clusters which use sentence level co-occurrence based cosine similarity (3.1), C2 denotes the clusters
which use proximal word based cosine similarity (3.2), C3 denotes the clusters for each positions related to NE (3.3)]
ters which uses the similarity measurement based on
proximity of the words to NE categories (defined in
Section 3.3).
Using clustering features the best f-value (79.03)
is obtained using clusters for previous two and next
two words along with the suffix, prefix, digit and
POS information.
It is observed that the prefix information increases
the accuracy if applied along with suffix informa-
tion when cluster features are used. More interest-
ingly, addition of cluster features for positions ?2
and +2 over the feature [window(-1, +1), Suffix,
Prefix, Digit, NomPSP] increase the f-value from
77.61 to 79.03. But in the baseline system addition
of word features (wi?2 and wi+2) over the same fea-
ture decrease the f-value from 75.6 to 72.65.
5.2 Using Important Word Selection
The details of the comparison between the word fea-
ture and the reduced features based on important
word selection are given in Table 6. For the sur-
rounding word features, find whether the particular
word (e.g. at position -1, -2 etc.) presents in the
important words list (corresponding to the particu-
lar position if position based important words are
considered). If the word occurs in the list then the
word is used as features. In general it is observed
that word selection also improves performance over
baseline features. Among the different approaches,
the best result is obtained when important words for
two preceding and two following positions (defined
in Section 4.3) are selected. Using important word
based features, the highest f-value of 79.85 is ob-
tained by using the important words for previous two
and next two positions along with the suffix, prefix,
digit and POS information.
5.3 Relative Effectiveness of Clustering and
Word Selection
In most of the cases clustering based features per-
form better then the important word based feature
reduction. But the best f-value (79.85) of the sys-
tem (using the clustering based and important word
based features separately) is obtained by using im-
portant word based features.
Next we have made an experiment by consider-
ing both the clusters and important words combined.
We have defined the combined feature as, if the word
(wi) is in the corresponding important word list then
the word is used as feature otherwise the correspond-
ing cluster-id (in which wi belongs to) is considered
as feature. Using the combined feature, we have
achieved further improvement. Here we are able to
achieve the highest f-value of 80.01.
6 Conclusion
A hierarchical word clustering technique, where
clusters are driven automatically from large unan-
494
Feature Using
Word
Features
Using
Words
(I1)
Using
Words
(I2)
Using
Words
(I3)
wi, window(-1, +1) 67.26 66.31 67.53 66.8
wi, window(-2, +2) 69.09 72.04 72.9 73.34
wi, window(-1, +1), Suffix 73.42 73.85 73.12 74.61
wi, window(-1, +1), Prefix, Suffix 72.5 73.52 73.94 74.87
wi, window(-1, +1), Prefix, Suffix, Digit 74.26 73.97 74.13 74.7
wi, window(-1, +1), Prefix, Suffix, Digit,
NomPSP
75.6 75.84 76.6 77.22
wi, window(-2, +2), Prefix, Suffix, Digit,
NomPSP
72.65 76.69 77.42 79.85
Table 6: F-values for different features in a MaxEnt based Hindi NER with important word based feature reduction
[window(?m,+n) refers to the important word or baseline word features corresponding to previous m positions and
next n positions; I1 is the class independent important words (4.1), I2 denotes the important words for each class (4.2),
I3 denotes the important words for each positions (4.3)]
notated corpus, is used by Miller et al (2004) for
augmenting annotated training data. Note that our
clustering approach is different, where the clusters
are obtained using some statistics derived from the
annotated corpus, and also the purpose is different
as we have used the clusters for feature reduction.
In this paper we propose two feature reduction
techniques for Hindi NER based on word cluster-
ing and word selection. A number of word similar-
ity measures are used for clustering. A few statisti-
cal approaches are used for the selection of impor-
tant words. It is observed that significant enhance-
ment of accuracy over the baseline system which use
word features is obtained. This is probably due to
reduction of overfitting. This is more important for
a resource poor languages like Hindi where there is
scarcity in annotated training data and other NER
resources (like, gazetteer lists).
7 Acknowledgement
The work is partially funded by Microsoft Research
India.
References
Berger A L, Pietra S D and Pietra V D 1996. A Maxi-
mum Entropy Approach to Natural Language Process-
ing. Computational Linguistic, 22(1):39?71.
Bikel D M, Miller S, Schwartz R and W Ralph. 1997.
Nymble: A High Performance Learning Name-finder.
In Proceedings of the Fifth Conference on Applied Nat-
ural Language Processing, pages 194?201.
Borthwick A. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, Computer
Science Department, New York University.
Grishman R. 1995. The New York University System
MUC-6 or Where?s the syntax? In Proceedings of the
Sixth Message Understanding Conference.
Li W and McCallum A. 2003. Rapid Development of
Hindi Named Entity Recognition using Conditional
Random Fields and Feature Induction. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 2(3):290?294.
McCallum A and Li W. 2003. Early Results for Named
Entity Recognition with Conditional Random fields,
feature induction and web-enhanced lexicons. In Pro-
ceedings of the Seventh Conference on Natural Lan-
guage Learning at HLT-NAACL.
Miller S, Guinness J and Zamanian A. 2004. Name Tag-
ging with Word Clusters and Discriminative Training.
In Proceedings of the HLT-NAACL 2004, pages 337?
342.
Saha S K, Sarkar S and Mitra P. 2008. A Hybrid Fea-
ture Set based Maximum Entropy Hindi Named En-
tity Recognition. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP-08), pages 343?349.
Wakao T, Gaizauskas R and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
495
A Diachronic Approach for Schwa Deletion in Indo Aryan Languages 
Monojit CHOUDHURY, Anupam BASU and Sudeshna SARKAR 
Dept.. of Computer Science & Engineering, 
Indian Institute of Technology, Kharagpur 
INDIA, PIN-721302 
{ monojit, anupam, sudeshna } @cse.iitkgp.ernet.in  
 
 
Abstract 
Schwa deletion is an important issue in 
grapheme-to-phoneme conversion for Indo-
Aryan languages (IAL). In this paper, we 
describe a syllable minimization based 
algorithm for dealing with this that 
outperforms the existing methods in terms of 
efficiency and accuracy. The algorithm is 
motivated by the fact that deletion of schwa is 
a diachronic and sociolinguistic phenomenon 
that facilitates faster communication through 
syllable economy. The contribution of the 
paper is not just a better algorithm for schwa 
deletion; rather we describe here a constrained 
optimization based framework that can partly 
model the evolution of languages, and hence, 
can be used for solving many problems in 
computational linguistics that call for 
diachronic explanations. 
 
1 Introduction 
Linguists propose new models for languages 
in order to explain language acquisition and 
processing by humans. Irregularities and 
exceptions to the theories are often explained by 
evidence from diachronic linguistics and other 
social and external phenomena. Absence of 
diachronic analysis in computational modelling of 
languages results in a large number of exceptions, 
which are commonly handled by ad hoc rules or 
exhaustive enumeration. These techniques lead to 
poor scalability and lack of graceful degradation of 
the systems along with increased complexity. 
Although complete modelling of the evolution of 
language is impossible due to the involvement of 
myriads of socio-political and cultural factors, it is 
definitely possible to model certain basic 
principles of language change. 
In this paper we describe an algorithm for 
schwa deletion in Indo-Aryan Languages (IAL) 
that is motivated by the diachronic evolution of the 
languages. The proposed computational framework 
models languages as a constrained optimization 
system, where a language evolves by optimizing 
the rate of communication, subjected to a set of 
constraints such as ease of articulation and 
learning, and acoustic distinctiveness. A syllable 
minimization based optimization function fitted to 
the aforementioned model has been used for 
solving the problem of schwa deletion with 
considerable success.  
The paper is organized as follows: Section 2 
defines the problem and discusses some of the 
previous works. Section 3 describes the current 
models of language evolution, which has been 
used to develop a computational framework 
described in the next section. Section 5 and 6 
presents the algorithm and its experimental 
analysis respectively. Section 7 concludes the 
paper summarizing our contributions. 
2 The Problem  
Schwa is defined as the mid-central vowel that 
occurs in unstressed syllables. The first vowel of 
the IAL alphabet {a}1 is the schwa. Normally, it is 
pronounced as /?/ in Hindi and Sanskrit, and as /?/ 
in Bengali. Schwa deletion is a phonological 
phenomenon where schwa is absent in the 
pronunciation of a particular word, although 
ideally it should have been pronounced (Ohala, 
1983).  
Sanskrit and some of the modern IAL that have 
evolved from it (e.g. Hindi and Bengali), are 
written from left to right using Brahmi-derived 
scripts. All the vowels are explicitly represented 
using diacritical or non-diacritical marks around 
the consonant except for the schwa, which is the 
inherent vowel. Unlike Sanskrit, many modern 
IAL like Hindi and Bengali allow deletion of 
schwa in certain contexts. Table I illustrates this 
phenomenon for the three languages. In order to 
determine the proper pronunciation of the words, it 
is necessary to predict which schwas are deleted 
and which are not. Thus, schwa deletion is an 
                                                     
1 The graphemes for Indo-Aryan languages are written within ?{? and ?}? 
according to the scheme adopted by the International Congress of Orientalists at 
Athens in 1992. The phonetic transcriptions are written within two ?/? using the 
IPA symbols. 
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
important issue for grapheme-to-phoneme 
conversion of IAL, which in turn is required for a 
good Text-to-Speech synthesizer (Narasimhan et 
al, 2001).  
 
Pronunciation   The  
Spelling Sanskri
t 
Hind
i 
Bengali 
s?phaly
a 
(succes
s) 
sa??ly
? 
(3) 
sa??
lj? 
(3) 
?a?ol lo 
(3) 
racan? 
(creati
on) 
r?c?na 
(3) 
r?cn
a 
(2) 
r?cona 
(3) 
veda 
(Veda) 
ved? 
(2) 
ved 
(1) 
bed 
(1) 
 
Table 1. Pronunciation of three different words 
in three different IAL. The number of syllables is 
denoted within parenthesis below the 
pronunciations. In Bengali {a} can also be 
pronounced as /o/ in certain contexts. 
 
Several theories have been proposed on the 
linguistic aspects of schwa deletion in Hindi (Pray, 
1970; Kaira 1976; Ohala, 1977, 1983) and its 
diachronic evolution (Misra, 1967). Ohala (1983) 
has summarized the rule for schwa deletion in 
Hindi as  
? ?   ? / VC __ CV 
Condition 1: There may be no morpheme 
boundary in the environment to the left. 
Condition 2: The output of the rule should not 
violate the phonotactic constraints of Hindi 
Convention: The rule applies from right to left 
 
The explanation of the rule was based on 
psycholinguistic evidence; diachronic facts were 
used only to explain the exceptions. Narsimhan et 
al (2001) designed an algorithm for schwa deletion 
in Hindi based on this work. The reported accuracy 
of the algorithm is 89%. Some rules for word final 
schwa deletion in Bengali have been proposed by 
Chatterji (1926), but we do not know of any work 
on computational modelling. 
 
3 Factors governing language change 
The fact that schwa deletion in IAL is a diachronic 
phenomenon has been substantiated by Misra 
(1967). According to Ohala (1983) the deletion of 
schwas is more frequent in casual and fast speech 
compared to formal and slower ones. It can be 
inferred from these facts that the motivation behind 
schwa deletion is faster communication through 
minimization of syllables (Tranel 1999).  
 Some recent works on mathematical and 
simulation based modelling of language evolution 
(Boer, 2000; Cangelosi and Parisi, 2002; Nowak et 
al, 2002) suggests that several features of 
languages emerge due to some basic cognitive and 
articulatory factors. These models assume a) ease 
of articulation, b) ease of learning, and c) acoustic 
distinctiveness as the primary driving forces 
behind language evolution. The three forces 
operate simultaneously over the language in order 
to maximize the rate of successful communication 
in terms of time and effort spent by the language 
users to generate, understand and learn the 
language. Thus, language can be modelled as a 
multi-objective optimization system, where the 
optimization criteria are 
 
? Minimization of effort (in terms of energy and 
time spent while conveying a piece of 
information)  
? Minimization of learning time and effort 
? Minimization of probability of 
misunderstanding (in the sense of confusing 
one word with another) 
 
These three criteria are mutually contradictory and 
therefore there exists no global optimum. Let us 
examine the phenomenon of schwa deletion under 
this multi-objective optimization model for 
language evolution. When a vowel is deleted from 
a word the number of syllables reduces by one. For 
example, in Table 1, for the second word, Sanskrit 
and Bengali have three syllables, whereas due to 
the deletion of a schwa, the Hindi pronunciation 
has only two syllables. Reduction of syllables 
implies shorter time for pronunciation of a word, 
and hence faster communication. However, 
deletion of schwas in certain contexts might result 
in a consonant cluster which the native speakers 
find very difficult or impossible to pronounce. This 
beats the very purpose of schwa deletion, i.e. the 
minimization of effort of articulation and therefore, 
is unacceptable. The second condition for the rule 
proposed by Ohala (section 2) refers to this 
constraint. 
 There are contexts where deletion of schwa 
would not give rise to inadmissible consonant 
clusters. For example, in the Hindi/Bengali word 
pari (fairy, /p?ri/ in Hindi), if the first schwa is 
deleted, the pronunciation would be /pri/, which 
does not violate the phonotactic constraints of the 
languages. The schwa, however, is not deleted, 
because /p?ri/ and /pri/ are too distinct from each 
other to be interpreted as the same word. 
Moreover, /pri/ is closer to other Hindi words like 
priya (favorite, /prij?/). In this case, the deletion of 
schwa reduces the acoustic distinctiveness of the 
word from other words in the lexicon, which 
increases the probability of misunderstanding, and 
hence the schwa might not be deleted in such a 
context. 
4 Computational framework 
We propose the following diachronic 
explanation for schwa deletion in IAL.  
In old IAL none of the schwas are deleted. The 
modern IAL use the script and spelling 
conventions similar to Sanskrit. Due to a higher 
evolutionary pressure on the spoken forms of the 
languages than on the written forms, schwas are 
deleted in the pronunciation, but are still present in 
the graphemic forms. The deletion is a slow 
diachronic phenomenon, where in order to 
communicate faster, initially the speakers 
unintentionally deleted the schwas. Only those 
deletions were acceptable that did not lead to a 
syllable structure which was too difficult to 
pronounce, learn or understand for the native 
speakers. Gradually, the pattern of deletion spread 
across the population and over the different items 
in the lexicon. 
In this section, we describe a computational 
framework for modelling the aforementioned 
hypothesis based on the three optimization criteria 
stated in the last section. The aim of the proposed 
framework is not to validate the hypothesis 
through micro-simulation (Cangelosi and Parisi, 
2002); rather it tries to predict the schwa deletion 
pattern based on the optimizations that might have 
affected the deletion of schwas diachronically. In 
the next section, we present an efficient algorithm 
for schwa deletion in IAL, which can be 
automatically constructed from this model, without 
the help of any other evidence. 
 
4.1 Basic definitions 
 All the unexplained symbols used below stand 
for their usual meaning in the context of formal 
language theory. Please refer to (Hopcroft and 
Ullman, 1979) for details. 
 
?g (?p): A finite set of the graphemes 2 
(phonemes) in the language  
?g = Vg ? Cg,  ?p = Vp  ? Cp  
                                                     
2  Graphemes here do not refer to glyphs. Free vowels and their 
corresponding diacritical marks are considered to be the same symbol 
Where 
Vg (Vp): Finite set of graphemes (phonemes), 
which are vowels 
Cg (Cp): Finite set of graphemes (phonemes), 
which are consonants. Semivowels are also 
considered as consonants. 
 
? ? Vg is a special symbol that represents schwa. 
We define, 
fg2p: ?g ? ?p 
fg2p is the default mapping of the graphemes to 
the phonemes. This oversimplification is made 
here for two reasons. First, since IAL use a 
phonetic script, this in general is true3 and second, 
this assumption does not have any affect on the 
schwa deletion algorithm.  
 
A word w is defined as a 2-tuple <wg, wp>, 
where  
wg ? ?g+ and wp? ?p+ 
 
A lexicon ? is the union of all the valid words w 
of a language. A grapheme-to-phoneme converter 
is defined as a function Fg2p: ?g+? ?p+, such that 
?w < wg, wp >? ?, Fg2p(wg) = wp 
 
4.2 Phonotactic constraints 
In order to model the ease of articulation, we 
start with the modelling of phonotactic constraints. 
A consonant cluster is a string of the form CpCp+. 
Phonotactic constraints restrict the presence of 
some of the consonant clusters in the phonetic 
representation of a word (wp). At the most generic 
level we can think of a consonant cluster ranking 
(CCR) function, where ? is the set of natural 
numbers       
                 
CCR: Cp+ ? ? 
 
The function CCR is independent of any 
language and every language has a threshold ?CCR, 
such that a consonant cluster x ? Cp+ is allowed in 
the language if and only if    
           
CCR (x) ? ?CCR 
 
We define two special variants of CCR, O_CCR 
and C_CCR, which ranks the admissibility of the 
consonant clusters at the onset and coda positions 
respectively. The definition is similar to that of 
CCR, and ?OCCR and ?CCCR are the corresponding 
threshold values.  
                                                     
3 This assumption is not strictly valid since a cluster of consonant might be 
mapped to a single consonant or a different cluster.    
The sonority hierarchy (Vennemann, 1988) and 
markedness conditions (Kager, 1999) along with 
the physiology of the articulatory mechanism point 
towards the existence of a language independent 
ranking function as hypothesized above. However, 
there might be accidental gaps in the list of 
admissible consonant clusters of a language 
(Ohala, 1983), which can not be explained on the 
basis of CCR alone. Therefore, we define a 
Boolean function ADM that tell us about the 
admissibility of consonant clusters in a language. 
 
ADM: Cp+ ? {0, 1}, such that for s ? Cp+ 
 (ADM (s) = 1) ?  (s is an admissible cluster) 
 
In general, we can derive this function from 
CCR as 
ADM (s) = sign (?CCR    ? CCR (s)) 
 
However, we might have to forcefully convert 
some values to 0 due to accidental gaps.   
 
4.3 Syllable and Syllabification 
We define a syllable ? as a regular expression, 
with the assumption that the nucleus contains a 
single vowel. Thus, 
? ? Cp* Vp Cp* 
 
The syllabification function SYL maps the 
phonetic representation wp of a word w to a string 
of syllables ?1?2??m such that the effort of 
articulation and learning are minimum. 
 
We model the effort of articulation using a 
syllable ranking function SR, which is similar to 
CCR. 
 SR : Cp*VpCp* ? ? 
 
SR is mainly dependent on the structure of the 
syllable. We enumerate the first few terms of the 
function SR.          
 
SR (Vp) = 1,  SR (CpVp) = 2   
SR (CpVpCp) = 3, SR (VpCp) = 4 
SR (CpCpVp) = 5,  SR (CpCpVpCp) = 6 
SR (CpCpCpVp) = 7,  SR (CpVpCpCp) = 8 
 
For all other possible syllable structures ??,  
SR (??) > 8 
 
Also, for any syllable ?,    
[O_CCR (onset(?)) >  ?OCCR] ? 
[C_CCR (coda(?)) >  ?CCCR] ? (SR (?) = ? ) 
 
This means that if either the coda or the onset of 
a syllable is inadmissible, then the ranking 
function maps the syllable to the highest possible 
rank, represented symbolically by the infinity (?). 
onset and coda are projection functions that project 
the longest valid prefix and suffix of a syllable 
respectively that are elements of  Cp*. 
 
We define a syllabification to be valid if all the 
syllables are valid (i.e. strings of the form 
Cp*VpCp*) and every symbol in the word is a part 
of one and only one syllable in the syllabification. 
We can define a partial ordering, ??, among the 
possible valid syllabifications of a given word 
based on SRp such that the syllabification with 
smaller number of high ranked syllables is 
preferred to one that has more hard (high ranked) 
syllables. Now we define SYL (wp) as the set of all 
possible syllabifications ?1?2??m such that (i) 
?1?2??m is a valid syllabification of  wp and (ii) 
there exist no other valid syllabification v of wp 
such that v ?? ?1?2??m.  
 
The definitions of syllable and syllabification are 
motivated by the markedness conditions (Kager, 
1999) and experimental results on child language 
acquisition (MacNeilage and Davis, 2000), that 
show that some syllables and syllabifications are 
easier to learn and pronounce than others. 
 
4.4 Acoustic distinctiveness constraints 
Perceptual experiments show that speakers 
always articulate the onset of the syllables more 
clearly and correctly compared to the articulations 
of the vowel and the coda (Fosler-Lussier et al 
1999; Greenberg, 1999). Therefore, it is likely that 
the hearer distinguish between syllables by paying 
more weight to the onset than to the coda. A 
continuous distance metric D? might be defined 
based on these experimental results, such that the 
probability of confusion (interpreting one syllable 
as another) between two syllables ? and ?? 
increases as the value of D?(? , ??) decreases. We 
can further define an acoustic distance function Dw 
using the function D? , which measures the 
probability of confusion between two arbitrary 
words in the phonetic domain. 
In the case of schwa deletion, however, we want 
the acoustic distance between the ideal 
pronunciation (without any schwa deletion) and 
the normal pronunciation (with schwa deletion) to 
be smaller, so that the word is not confused with 
other words in the lexicon. Formally, for the 
graphemic representation of a word wg = x1x2 ? 
xn, 
Dw(fg2p (x1).fg2p (x2)? fg2p (xn), Fg2p(wg)) < 
?critical, where ?critical is the maximum allowable 
distance and ?.? is the concatenation operator. 
Rather than modelling this as an optimization 
criterion, we reformulate this as a constraint. The 
simplification in this case serves our purpose. 
 
We define, where x ? Cp   
D?(x. fg2p(?), ?) = 0                  (4a) 
 D?(?, ?.x) = 0          (4b) 
For all other cases D?  is infinity (?), unless the 
two syllables are identical.             (4c) 
 
(4a) allows the deletion of a schwa from an open 
syllable; (4b) allows the concatenation of a 
consonant at the coda position. This is motivated 
by the fact that coda has least distinctiveness 
(Greenberg, 1999). (4c) restricts any change at the 
onset of a syllable or the vowels other than schwa.  
 
On the basis of D? we can define  Dw(wp1, wp2) = 
0 if and only if there exists an alignment between 
the sequences SYL (wp1) and SYL (wp2), with 
possible gaps (? or null syllables) such that for all 
the corresponding pairs of syllable taken from the 
two sequences, the acoustic distinctiveness (D?) 
between them is 0. Thus, only operations allowed 
are deletion of a schwa and addition of a consonant 
at the coda position. Anything else is forbidden for 
the sake of acoustic distinctiveness.  
 
We conclude this section by summarizing below 
the salient features of the model by comparing it 
with the optimization criteria stated in section 3. 
 
? The functions SR, CCR and its variants 
that rank the phonotactic constraints is a measure 
of the effort of articulation, learning and the 
probability of misunderstanding. Therefore we 
want to minimize it. However, it has been 
modelled as a constraint (ADM). 
? The function SYL is so defined that the 
efforts of articulation and learning are minimized. 
? Dw models the acoustic distinctiveness i.e. 
the criterion 3c, but it has been reformulated as a 
constraint as well.  
5 The algorithm 
We want to define Fg2p for a language given 
ADM and Dw. Fg2p should be such that it enables 
faster communication by minimization of syllables 
by deletion of schwa. 
 
5.1 Formal definition 
Let wg be an input sequence of graphemes to the 
function Fg2p. Let wp ? ?p* be obtained by 
replacing all graphemes x in wg by fg2p(x). Let wp? 
be obtained by deletion of some (possibly all or 
none) of the schwas (fg2p(?)) in wp. Fg2p(wg) = wp?, 
if and only if Dw(wp, wp?) = 0  and (? vp)[( vp can 
be obtained by deleting schwas from  wp) ? 
(Dw(wp, vp) = 0) ? |SYLg(wp?)| ? |SYLg(vp)| ] 
 
In words it means that among all wp? obtainable 
by deletion of some of the schwas from wp, that 
respects both the ADM (phonotactic) and Dw 
(acoustic distinctiveness) constraints, the one with 
the minimum number of syllables is chosen as the 
output of Fg2p.  
 
procedure SYL : 
input: wp, O_CCR, C_CCR 
output:  ?1?2??m  //The syllabification 
 
1. Include up to the first vowel in wp in ?1 
2. If there are 2 consonants c1c2 between the 
current vowel and the next vowel, include c1 in the 
current syllable and c2 in the next syllable. 
3. If there are 3 consonants c1c2c3 between the 
current vowel and the next vowel,  
     3.1 if O_CCRp(c2c3)? ?OCCR , include c1 in the 
current syllable and c2c3 in the next syllable  
     3.2 else if C_CCRp(c1c2)? ?CCCR include c1c2 in the 
current syllable and c3 in the next syllable 
     3.3 else NO syllabification is possible 
4. If there is one or no consonant between the 
current vowel and the next vowel, terminate the 
current syllable and begin the next syllable 
5. Continue from step 2 till there are symbols not 
included in any syllable.  
end procedure 
 
 
Figure 1. Algorithm for syllabification 
 
5.2 A greedy strategy  
Figure 1 describes a linear time algorithm for 
syllabification (SYL) that conforms to the 
definition provided in section 4.3. This uses the 
fact that the maximum length of allowable 
consonant clusters for IAL is three. After 
syllabification of wp, we try to greedily delete the 
schwas so that the constraints specified by 4a, 4b 
and 4c are not violated. 4a states that only a schwa 
which is a part of an open syllable (c?, where c ? 
Cp) can be deleted and 4b states that after schwa 
deletion, 
the consonant c is appended to the coda of the 
previous syllable. Therefore, both of them together 
imply schwas in two consecutive syllables cannot 
be deleted. Along with that, the following 
constraints can also be derived from the Dw 
constraints (the reasons are omitted due to space 
constraints): 
R1. Schwa of the first syllable cannot be deleted 
R2. Schwa cannot be deleted before a consonant 
cluster. 
R3. The word final schwa can always be deleted 
unless the appending of the penultimate 
consonant to the previous syllable results in an 
inadmissible cluster. 
R4. For Bengali, which does not allow complex 
codas, schwas cannot be deleted after 
consonant clusters. 
R5. A schwa followed by a vowel cannot be 
deleted. 
 
procedure Fg2p: 
input: wg , ADM 
output:  wp  //The pronunciation 
 
1. wp?  = fg2p(x1).fg2p (x2)? fg2p (xn), where wg is 
<x1x2 ? xn > 
2. Syllabify wp?  using procedure SYL 
3. Using rules R1 to R6 and ADM constraints mark 
the schwas which cannot be deleted as F 
4.  While traversing the word from right to left 
         4.1 Delete a schwa if it is not marked F 
         4.2 Appended the dangling consonant to the 
coda of the adjacent syllable (to the left) 
         4.3 If the adjacent syllable (to the left) has a 
schwa which is unmarked, mark it F 
         4.4 Go to 4.1 if there are more schwas to the left 
of the current position. 
5. At the end of step 4 we get the syllabified string of 
phonemes <x?1x?2 ? x?m >, which is the required 
output 
end procedure 
 
Figure 2. Algorithm for schwa deletion 
 
 We have the following rule that cannot be 
captured by the constraints: 
R6. Schwa following a y (pronounced as /j/) 
cannot be deleted if it is preceded by a high 
vowel because /j/ is a glide from high vowel to 
a low/medium vowel (schwa), deletion of 
schwa would make the presence of the glide 
imperceptible. 
This rule could have been captured by the D? 
constraints but we state it here as a separate rule 
for the sake of simplicity. Figure 2 describes an 
algorithm for schwa deletion using the rules above. 
It is easy to see that the time complexity of the 
algorithm is O(|wg|). Due to limited space, we omit 
the proof that the algorithm for Fg2p indeed 
minimizes the number of syllables without 
violating the constraints specified by ADM and Dw. 
However, there might be more than one (precisely 
2) possible solutions and in that case the algorithm 
chooses one of the solutions on the basis of the 
direction of traversal at step 4. The right to left 
traversal gives better results (as has been 
confirmed by Ohala, 1983) because the duration of 
syllables reduces towards the end of the word and 
hence the tendency to delete schwas at the word 
final position increases. 
6 Experimental Results and Discussions 
The algorithm was implemented for Bengali and 
Hindi and tested on a set of words. Table 2 
summarizes the results for Hindi (tested on the 
words in a pocket dictionary (Hindi-Bangla-
English, 2001)). The algorithm for Bengali was 
tested on 1000 randomly selected words from a 
corpus and found to be around 85% accurate.  
Some of the important features of the algorithm 
are as follows. 
? Efficiency: The algorithm runs in linear time 
on the input word length. It scans the whole 
word just twice. Thus, the hidden constant is 
also very small. 
? Polymorphemic Words: The algorithm can 
handle polymorphemic words, if the 
morphological information about the word is 
provided. This is because schwa deletion is not 
carried across morpheme boundaries. 
Morphological analyzer for Hindi and Bengali 
were implemented and integrated with the 
algorithm. For Hindi, the results were nearly 
perfect (99.89%) 
Exceptions: For Hindi there was hardly any 
exception to the algorithm. For Bengali, the types 
of words that were incorrectly processed by the 
algorithm include a class of very frequently used, 
disyllabic modifier adjectives, certain suffixes, 
borrowed words from Sanskrit and compound 
words. In Bengali, the schwa which is retained (as 
opposed to the predictions by the algorithm) are 
pronounced as /o/ and not as / ?/. Since, /o/ is not a 
central vowel, deletion of /o/ is marked as 
compared to deletion of / ?/ which is unmarked. 
Transformation of schwa to some non-neutral 
vowel in Hindi is unknown and therefore, the 
algorithm works perfectly for Hindi. 
 
Experiment
al results for 
Hindi 
Test 
Size 
(No. of 
words) 
Incorre
ct results 
Accurac
y 
Without 
MA 
1109
5 
431 96.12% 
With MA 1109
5 
12 99.89% 
 
Table 2. Experimental results for Hindi schwa 
deletion. The results are for individual words. MA 
stands for Morphological Analysis 
7 Conclusion 
In this paper, we have described the 
phenomenon of schwa deletion in the IAL and 
proposed a diachronic explanation for it. In order 
to model the diachronic evolution, we used the 
concepts of ease of articulation, ease of learning 
and acoustic distinctiveness. We developed a 
computational framework, where we reformulated 
some of the optimization criteria as constraints and 
one of them (the syllable minimization) as the 
basic optimization function. The outcome of this is 
an efficient and accurate algorithm for solving 
schwa deletion in IAL. 
The contribution of this paper is not just a 
better algorithm for schwa deletion, which is 
necessary for developing Text-to-speech 
synthesizers for IAL, but a new approach based on 
a constrained optimization framework, motivated 
by the diachronic evolution of languages. A closer 
look at the algorithm will reveal that it is not much 
different from the schwa deletion rule proposed by 
Ohala (1983). However, Ohala?s rule was based on 
psycholinguistic and empirical observations, 
whereas we have derived the rule from a set of 
very basic assumptions (minimization of syllables 
and certain constraints). The algorithm itself can 
provide an explanation for the phenomenon. 
It must be mentioned that neither the aim nor 
the findings of this work are meant to propose a 
new model of language change. The models and 
concepts used here were all present previously and 
we have assumed and included some of them 
directly in our model. Our finding is not a proof of 
those models and can be considered only as a 
further validation. Our only claim here is that 
diachronic clues can help solve important 
problems in computational linguistics and for this 
we provide a computational framework and a 
specific example. 
 Some of the questions that we would like to 
address in the future include modelling of optional 
schwa deletion in Bengali compound words, 
evolution of morpho-phonology for Bengali verb 
systems, and modelling of dialect diversity using 
diachronic clues. More realistic, yet manageable 
computational frameworks for holistic or detailed 
modelling of language evolution can also be an 
interesting area of future research. 
References  
Bart de Boer 2000. Self Organization in Vowel 
Systems. Journal of Phonetics, 28:441-465 
Angelo Cangelosi and Domenico Parisi (Eds) 
2002. Simulating the Evolution of Language. 
Springer-Verlag, London  
Suniti K. Chatterji 1926. The Origin and 
Development of the Bengali Language. Rupa and 
Co. 
Eric Fosler-Lussier, Steven Greenberg and N 
Morgan  1999. Incorporating contextual 
phonetics into automatic speech recognition. 
Proc. Int. Cong. Phon. Sci., San Francisco, pp. 
611-614. 
Steven Greenberg 1999. Speaking in shorthand - A 
syllablecentric perspective for understanding 
pronunciation variation. Speech Communication, 
29:159-176.  
Hindi Bangla English ? Tribhasa Abhidhaan. 2001 
Sandhya Publication  
John E. Hopcroft and Jeffery D. Ullman 1979. 
Introduction to Automata Theory, Languages 
and Computation, Addison-Wesley, USA  
Rene Kager 1999. Optimality Theory. Cambridge 
University Press 
S. Kaira 1976. Schwa-deletion in Hindi. Language 
forum (back volumes), Bhari publications, 2 (1)  
Peter F. MacNeilage and Barbara L. Davis 2000. 
On the Origin of Internal Structure of Word 
Forms. Science, 288:527-31 
B. G. Misra 1967. Historical Phonology of 
Standard Hindi: Proto Indo European to the 
present. Cornell University Ph. D. dissertation 
Manjari Ohala 1977. The Treatment of 
Phonological variation: An example from Hindi. 
Lingua, 42: 161-76 
Manjari Ohala. 1983. Aspects of Hindi Phonology, 
volume II. MLBD Series in Linguistics, Motilal 
Banarsidass, New Delhi.  
Bhuvana Narasimhan, Richard Sproat and G Kiraz. 
2001. Schwa-deletion in Hindi Text-to-Speech 
Synthesis. Workshop on Computational 
Linguistics in South Asian Languages, 21st 
SALA, Konstanz  
Martin A. Nowak, Natalia L. Komarova and Partha 
Niyogi 2002. Computational and Evolutionary 
Aspects of Language, Nature, 417:611-17 
B. R. Pray 1970. Topics in Hindi ? Urdu grammar. 
Research Monograph 1, Berkeley: Center for 
South and Southeast Asia Studies, University of 
California 
Bernard Tranel 1999. Optional Schwa Deletion: on 
syllable economy in French. Formal 
Perspectives on Romance Linguistics, Ed. By J. 
Mark Authier, Barbar S. Bullock, & Lisa A. 
Reed.  
T. Vennemann 1988. Preference Laws for Syllable 
Structures. Mouton de Gruyter, Berlin 
 
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 51?56,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
A Multilingual Analysis of the Notion of Instrumentality
Asanee Kawtrakul, Mukda Suktarachan (Kasetsart univ. Bangkok, Thailand),
Bali Ranaivo-Malancon, Pek Kuan Ng, (Univ. Sains Malaysia, Penang, Malaysia),
Achla Raina (IIT Kanpur, India),
Sudeshna Sarkar (IIT Kharagpur, India),
Alda Mari (Enst-Cnrs, Paris, France),
Sina Zarriess (Universita?t Potsdam, Germany),
Elixabete Murguia (Univ. Deusto, Bilbao, Spain),
Patrick Saint-Dizier (Irit-Cnrs, Toulouse, France)
Abstract
Instruments are expressed in language by
various means: prepositions, postposi-
tions, affixes including case marks, nonfi-
nite verbs, etc. We consider here 12 lan-
guages from five families in order to be
able to identify the different meaning com-
ponents that structure instrumentality.
1 Credits
This work has been made possible partly via the
STIC-Asia cooperation framework.
2 Introduction
It is difficult to give a comprehensive definition of
what instrumentality is. In WordNet it is defined
as ?an artifact, or a set of artifacts, that are instru-
mental (i.e. behave as instruments) in accomplish-
ing some end?, i.e. reaching a certain goal. In
this definition, the triple relation agent-instrument-
goal (as in: John cuts the bread with a knife, where
John is agent, knife is instrument that does the cut-
ting, and bread cut is the goal), is left vague in
what concerns the exact involvement of the agent
and the instrument in the action, and the control
the agent has on the instrument and on the action
(Mari and St-Dizier 01).
If almost anything can be an instrument, we can
nevertheless formulate a few criteria that we will
try to elaborate in this paper. First, an instrument
is basically non volitional. When humans play the
role of instruments, they are obviously volitional,
but the action is controlled by another agent who
acts as an ?initiator agent?, taking the initiative of
the action. Instruments cannot be easily associated
with traditional thematic roles, in fact this is not
of much interest, because this is too superficial a
notion and also because instruments are generally
modifiers, not arguments.
In this paper, we address instrumentality as con-
veyed by prepositions or equivalent means (e.g.
postpositions, affixes). Our aim in this study
is twofold: (1) to identify the conceptual facets
of instrumentality so that a conceptual seman-
tics can be defined in the spirit of (Talmy, 01,
03), (Wierzbicka 92, 96) and (2) to elaborate an
accurate enough model for answering questions
about instruments within a cooperative question-
answering system. Instead of focusing on a spe-
cific language to elaborate all possible forms of
instrumentality, we found it more adequate to de-
velop a multilingual approach, considering lan-
guages from various families.
We consider for Europe: German, Spanish,
French, Italian; for India: Kashmiri, Urdu, Hindi
and Bengali; for the far-east: Thai and Malay;
for Northern Africa and the Middle East: Ara-
bic, and Berber dialects (in the group of Amazigh
languages). In this paper, we use the first let-
ter of each language to identify it: Thai, Malay,
Hindi, Urdu, Kashmiri, Bengali, German, Span-
ish, French, Italian, Arabic, BeRber). We have
also more or less adequately transcribed characters
into latin characters. The upper case A is equiva-
lent to aa.
3 An overview of preposition structures
3.1 European languages
German, Spanish, Italian and French, like most
European languages have prepositions that in-
troduce instrumental PPs. The most current
prepositions are:
- German: mit, mit Hilfe von,
mittels, durch, anhand, kraft,
51
dank, per.
- French: avec, par, au moyen de,
gra?ce a`, a` l?aide de, a` travers.
- Spanish: con, en, por, a trave?s de,
mediante, por medio de, a base
de, con la ayuda de, gracias a.
- Italian: con, per mezzo di, tramite,
per, grazie a, con l?aiuto di.
3.2 Arabic and Berber
Arabic essentially has the preposition bi, used as
a prefix of the noun it heads:
Aktoub bi al kalami (I write with a pen).
bi can also be associated with a specific noun
or deverbal form (e.g. ?by applying?) to char-
acterize in more depth the instrumental relation.
This entails generic forms such as bi-tarika,
bi-istemali (by means of, the first form rein-
forces the importance of the instrument, while the
latter is more formal) and bi-fadli (thanks to).
For example, we have a kind of nominal form is-
tikhdam (= using or with the use of, constructed
from the root ?use?) in:
Fasser el massala bi istikhdam mithel (explain a
problem ?with the use of? example). Arabic makes
explicit the metonymy we have in European lan-
guages.
There are a few other prepositions such as min
khilal (through) and min a to express the du-
ality source + instrument of the argument (as in
drink with a bottle, litterally). In the spatial con-
text, au can be used (to reach the top by this trail),
and ala is used to express a channel of commu-
nication.
Berber is composed of a large number of di-
alects, some just spoken in small ?tribes?, others
in larger communities. It is basically dialectal, but
a number of common elements can be identified.
We consider here Berber from the Moroccan Rif,
and the Algerian Kabyle. The main instrumental
preposition is e`g, but prefixes are also used s-,
ge-, th-, kh- which affect the morphology
of the noun they are attached to. The prefix s-
is widely used, with some variants (si, sei,
so in Kabyle), e`g (also realized as g? or ge in
some places), is appropriate only when the action
is under full control of the agent. It is not em-
ployed when the instrument is abstract. In the Rif
area, s- focusses on the instrument contributing to
the action, it makes a kind of fusion between the
action verb and the instrumental noun. ge is only
used in spatial contexts involving means of trans-
portation (travelling by plane), kh is used only in
spatial contexts involving paths.
3.3 The Indic language family
The four languages considered, Kashmiri (Raina,
02), Urdu, Hindi and Bengali, share some simil-
itudes due to their common origins but also
contrasts that result from independent evolutions
which are useful to our analysis. They all have suf-
fixes and postpositions. Case suffixes (vibakhti)
are added to nouns or pronouns, but case suf-
fixes do not correspond strictly to thematic roles
(kAraka). Postpositions may also be used instead
of vibhaktis or in conjunction with them, as a kind
or re-inforcement. Basically, but with some nu-
ances, Hindi, Urdu, Bengali and Kashmiri lan-
guages have the following kernels:
? direct instrument: se (H/U), sity (K), -e/-te,
diye after null, dwArA after -r (B). Example:
H: raam ne chaabi se taala khola (ram - erg.
key with lock open-past= Ram openened the
lock with a key).
? means instrument: me (H/U), ke zariye (U),
manz (K), -e/-te (B). Example: U: raam
gaadi ke zariye daftar gayaa (ram car by
means of office go-past = Ram went to work
by car).
? causal instrument: ke kaaran (H), ki vajah se
(U), kiny (K), kArANe, kripAya (B). Exam-
ple: K: dil chi sayaahat kiny amir (Delhi be-
present tourism with rich = Delhi is rich with
tourism),
B: dillI paryatan-er khAtire samriddha or dillI
paryatan-er kripAya samriddha (Delhi be-
rich-er thanks to tourism)
? agentive instrument: ke dwaraa (H) ke zariye
(U) zariy, desi (K), diye (B) after -ke when
the nominal form is animate and specific,
diye after null when the nominal form is an-
imate and general (B). Example: H: raam ne
shyaam dwara apna kaam karvaaya (ram erg.
shyam by self?s work do-cause-past = Ram
got his work done by Shyam)
? action instrumentalised: kar (H/U), kerith
(K). Example: U: raam kuud kar ghar ke an-
dar daakhil huaa (ram jump participle house
52
into enter-past = Ram entered the house by
jumping in it)
There are less important cases that capture no-
tions like containment, which will be detailed in
section 5. Postpositions may vary also depending
on the semantic type of the NP they head.
In Bengali, case is indicated in several ways.
Vibhaktis are suffixes that are added to the stems
to form surface forms of words. In Bengali the
nominal stems take one of the following suffixes:
null or shunya vibhakti, -e, -te, -r, etc. Case is also
indicated in Bengali by the use of postpositions.
We have, for example: Cut bread with a knife, re-
alized as either:
1. chhuri diye ruti kATa (knife diye bread cut), or:
2. chhuri-te ruti kAta (knife-te bread cut)
Postpositions in Bengali are derived from certain
inflected forms of nouns, and also certain verbs
in participle form. When these words are used
as postpositions they are often not considered in
their original sense but define a specific type of re-
lationship of the noun phrase with the finite verb
phrase in the sentence. These words are appear
in a fixed form (indeclinables) as postpositions.
When a postposition is used to denote the case,
the nominal word preceding the postposition takes
on a vibhakti that is determined by the particu-
lar postposition. ?diye? is used after null vibhakti,
?dwArA? after -r vibhakti, etc.
3.4 Thai and Malay
Thai and Malay, although spoken in neighbour
countries, are substantially different.
Thai, from the Thai-Kadai family, has 6 prepo-
sitions (Silapasarn, 98) to denote instruments, the
most common being doi and duai which are
used for concrete instruments, means of trans-
portation, instruments close to manners, etc. kap
is used when the instrument is a part of the body,
while thang is used for means of transportation
only. tam characterizes control of the agent, and
chak is restricted to the instruments that convey
an idea of source. Examples:
khian - duai - din so (Write - with - pencil)
pai - pa ris - doi - khrueang bin (Go - Paris - by -
plane)
These semantic distinctions are, however, often vi-
olated in colloquial Thai.
Malay, from the Malayo-polynesian family, has
three ways to introduce instruments: preposition +
NP, affixes and compounding. Affixed words are
built from stems which are instrumental nouns,
this allows for the construction of the equivalent
of PPs, based on the prototypical use of the instru-
mental noun. The most common being: prefixes:
beR- (from kuda, horse, berkuda, on horseback),
meN- (from kunci, key, mengunci, lock with key),
prefix + suffix: meN- + -kan (from paku, nail,
memakukan, to fasten with nails), and with suffix
-i (from ubat, medicine, mengubati, by means of
medicine). Prepositions occur as the head of PPs,
and in verb particle constructions. PPs may also
be subject complements, avoiding the use of verbs
(dia di rumah, she at home). Besides affixes,
Malay has 6 prepositions that denote instrumen-
tality: dengan, melalui, mengikut,
menerusi, dengan menggunakan,
secara.
A simple example is:
berhubung - melalui - telefon (communicate - by -
telephone).
4 The meaning components of
instrumentality
Let us now consider the different meaning compo-
nents that emerge from our multilingual analysis.
The results presented below are still exploratory
due to the complexity of the notion. The distinc-
tions made (e.g. between concrete and abstract in-
struments) may seem arbitrary: they are just meant
to structure the presentation.
4.1 Concrete instruments
All languages studied have at least one basic in-
strumental mark operating over concrete objects
(T: duai, M: dengan, H: se, U: se, K: sity, B: diye,
-e, -te, G: mit, S: con, F: avec, A: bi, BR: e`g). Sev-
eral refinements are identified, for specific types of
NPs, or to denote a specific intention:
? the instrument is a recipient (S: en) or, more
generally, conveys an idea of container (e.g.
spoon) (B: -e kare), the idea behing is that
the container is used to carry the object along
a certain trajectory,
? the instrument is a part of the body (e.g.
hand): T: kap. In this case, the instrument
is not strictly artifactual.
? the goal is difficult to reach, it requires some
efforts from the agent (S: a base de),
53
? the focus can be emphasized by using ded-
icated marks (G: mit Hilfe (von), Mittels
(more fomal: Das Gericht hat mittels einst-
weiliger Verfu?gung den Drogenhandel unter-
sagt (the court has with provisional ordinance
the drug traffic prohibited))).
The second major difficulty is prototypicality
(Rosch, 78). When the instrument used is not very
prototypical of the action, several languages re-
inforce the instrumental prepositions to, sort of,
coerce the type of the noun so that it can become
an acceptable instrument. We have examples in S:
por medio de, B: sAhAjye, sahojoge,
I: per mezzo di, F: au moyen de, par
le biais de (biais= bias which directly ex-
presses this idea), as in:
F: Il a ouvert la porte au moyen d?un cric (he
opened the door by means of a jack).
At a conceptual level, it is quite difficult to
characterize what is a prototypical instrument
for a given action (characterized by subject-verb-
object: John opens the door). Each event has its
own prototypical instrument, making corpus stud-
ies extremely large, probably unfeasable. When
searching on the web, we find an incredible variety
of instruments to open a door, almost impossible
to classify. Next, prototypicality is not a boolean
notion: instruments are more or less prototypical.
Since the instrument is very much dependent on
the verb and on the object, we cannot foresee any
form of incorporation in the verb that would give
us indications. A direction could be to assume
Qualia structures (Pustejovsky 91) associated with
each potential instrument that describes the func-
tion of the object in the telic role. For example,
key(X) would have open(X, door), with door be-
ing quite generic. This approach could work via a
large lexical development for concrete nouns, it is
much more risky when terms are abstract.
4.2 Abstract instruments
Abstract instruments (theorems, regulations, ex-
amples, etc.) are realized identically to concrete
instruments, but with some typical marks such
as: T: tam, H: dwAra, K: zariyi, B: dwArA,
M. mengikut. At this stage, it is difficult to ex-
plain why marks are different from concrete in-
struments. An hypothesis could be that abstract in-
struments are closer to causes (see 5.5), or to more
formal situations for which specific terms were de-
veloped (e.g. for G: kraft).
There are additional marks dedicated to partic-
ular fields: B: sahajoge, and A: min khilal
when instruments are of type ?example? (explain
with an example). U: -ke zariye, S: por
medio de and G: Anhand, Kraft are more
formal, stronger for Kraft and apply particularly
to areas like juridical or psychological domains.
People and organizations can be seen as appro-
priate intermediaries for reaching a goal. They
may be conceived as metaphorical instruments.
Investigations show that people can get controlled
much in the same way as concrete objects:
F: Elle a informe? Paul de son de?part par Pauline
(She informed Paul of her leaving ?by? Pauline).
If we now consider: S: Juan env??o este paquete
por correo (John sent this parcel ?by? post)
Since post is the by-default medium to send pack-
ages, por is the only choice. Using more precise
services, like FedEx, is considered to be an alter-
native way, in that case F: par, avec S: por,
con are both acceptable.
4.3 Metaphorical instruments
Both concrete and abstract objects can be used
metaphorically as instruments. Examples abound
in the literature and on the Web. In 5.6 we ex-
amine the path metaphor which is very produc-
tive. Besides this case, we have a number of
metaphors, such as: write with your heart, fight
with your head, etc. These are not essentially dif-
ferent from metaphors observed in other situations
(Lakoff and Johnson 99).
4.4 The overlap instrument-manner
In a number of cases, it is not very easy to make
a distinction between instrument and manner. It
seems there is a continuum between these two no-
tions or even some form of overlap, where the ob-
ject is both an instrument and a manner at various
degrees, which may depend on context. A vari-
ety of marks contribute to characterize this over-
lap, manners at stake being quite diverse, but we
will not go into the study of manners. Specific
marks dealing with the manner/instrument am-
biguity are: T: doi, G: durch (which is also
used for metaphorical spatial uses), M: dengan
menggunakan, S: en, con, a as in S: es-
cribir en/con rojo (write in red),
T: khian - duai - muek - daeng (write - with - red -
ink)
BR: te`te s-e?fe`ssen (She-eats with-hands).
54
4.5 Causality
It is clear that, a priori, instruments can be viewed
at various degrees as causes of an event. There is
a kind of overlap between these two notions. In-
struments are not volitional, so they are under the
partial or full control of an agent (humans playing
the role of instruments are also controlled by an
agent). Typically I: a causa di, F: a cause
de, S: a causa de signal that the instrument
has brought about an event:
I : Il castello e distrutto a causa di un violento in-
cendio. (The castle has been destroyed ?because
of? a violent fire.)
Causality (e.g. Talmy, 01) being a complex no-
tion, it is not surprising that instruments, viewed
as intermediaries at various degrees, share some
features with causes. For example in cut the bread
with a knife, the cause of the bread being cut is
the action of the agent, but also the use of a pro-
totypical property of the knife: the knife does the
cutting. In (Talmy 01), the instrument is embed-
ded into the causing event:
(caused event) RESULTS FROM (causing event)
where the causing event has the structure:
Instrument ACTON object, where object is bound
or related in some way to the object in the caused
event.
As analyzed in (Mari and Saint-Dizier, 01), in-
strumentality is the convergence of several factors:
? the degree of involvement of the instrument
in the action, therefore, the fact that the in-
strument causes the action or is just a means
managed by the agent who is the main cause,
? the type of control the agent has on the instru-
ment for the action at stake, from full control
to lack of control,
? the control the agent has over the action as a
whole.
Indic languages and Thai are particular explicit on
these matters. They have specific marks for two
major cases:
1. agentive instrument, action not controlled by
the agent: H: ke dwAra, U: ke zariye
K: zariy, desi, T: doi,
2. causal instrument that does most of the ac-
tion, under the control of the agent: H ke
kAran, U: ki vajah se, K. kiny, T:
duai,
Berber allows e`g only when the agent controls the
instrument. The other cases are expressed by non
prepositional forms.
4.6 Instruments and paths
Another productive situation is the use of spatial
metaphors to express instrumentality. The use of
F: par and other marks (e.g. in B., U.), show
that there is a close link between instrumentality
and path descriptions (spatial as well as temporal
paths). This is a kind of metaphorical use of paths
viewed as instruments (as can be seen in (Lakoff
et al 99): ?action is motion, goals are paths, actors
are travellers?). Using an instrument parallels the
use of paths in the domain of space.
Marks denoting paths or sources are of much
interest. Some have really restricted uses, whereas
others are more flexible. We observe the following
main components:
? paths: T: tam, A: min khilal, S: por, a trave?s
de(por correo, by post), de, G: durch, F:
a` travers, note the distinctions, e.g. in M:
melalui (metaphorical paths: M : berhubung
melalui telefon (communicate by telephone)),
menerusi (channel of transmission), H, U:
me, se, T: thang. In B, -e and -te denote
paths where the agent that does the action has
no control, whereas diye and dhare involve at
least a partial control from the agent. In M,
metaphorical passages require melalui.
? sources: F: a`, A: min a, T: chak (for con-
crete and abstract sources). Example: A:
Achroubou mina Karoura (I am drinking with
bottle), which is also a kind of manner.
The duality path/instrument is particularly visi-
ble in, e.g.:
K: raam vot tshochi vati kiny gari (ram reach-past
short route via home = Ram reached home by the
short route).
Another interesting phenomenon occurs when
an argument is both an instrument and a path,
as in look at the moon in a telescope. Tele-
scope is indeed the instrument used and also
the path through which one looks, or which the
light traverses. This double facet of the argu-
ment is visible in surface realizations, where the
preposition used is ambiguous between instrument
(first preposition) and path (second one) readings:
G: mit, durch, S: con, por, M: dengan,
melalui, F: avec, dans. When one wants
55
to strongly stress the path interpretation, then a
more path-oriented preposition is used, e.g. S: a
trave`s de.
4.7 Means of transportation as instruments
Means of transportation (trains, spoons, boxes, en-
velopes, etc.), sometimes viewed as containers,
and mediums of transportation (by air) receive a
special treatment in a number of languages: T:
doi, thang, M: menerusi, melalui (for metaphorical
mediums and passages), H: me, U: ke zariye, K:
manz, zariy, B: kare, -e kare, -ya kare, A: ala, BR:
ge-, kh-, G: per, S: por, en, F: par. We have, for ex-
ample: U: raam gaadi ke zariye daftar gayaa (ram
car by means of office go-past = ram went to post
office by car)
T: pai - pa ris - doi - khrueang bin (Go to - Paris -
by - plane)
B: Nouko-ya kare phuketa jAo or Nouko-ya
phuketa jAo (boat-e kare phuket go or boat-e
phuket go = go by boat to Phuket)
A distinction is made between the medium and
the means as for: M: secara, which is used for
means of communication such as email or letters.
If the agent has effective control over the means,
then, for example, S uses con.
4.8 Language levels
Some marks are proper to formal discourse: G:
Mittels, Kraft, Anhand, H: dwAra.
4.9 Positive or negative orientation
The languages we studied also abound in positive-
oriented marks that express in a certain way the
idea of ?thanks to?: T: khop khun (+kah for fem-
inime and krup for masculine), H: ke kAran, U:
ki vajah, K: kiny, B: -er khAtire, (-er) kripAya, G:
dank, S: gracias a, F: gra?ce a`.
There are also several negative-oriented marks
such as the following prepositional compounds:
F: de la faute de, I: per colpa di, S:
por colpa de (by the fault of), where the term
?fault? conveys a negative orientation.
4.10 Metonymies
In most languages, the prototypical action denoted
by the instrument is implicit, it is analyzed as a
metonymy: object for action. Action is inferred
from the instrument and the verb in the given con-
text. In a number of situations, A and M need
to make explicit the action. For particular cases,
gerundive forms may be prefered to PPs (but not
to be confused with manners, e.g. ?by swim-
ming?), so that the verb that lexicalizes the action
is present.
For example, in M, ?by the trail? in to reach the
top of a mountain by the trail requires to make ex-
plicit how the trail is used: dengan mengikuti de-
nai itu (litt.: with follow trail DET (same cases
in Arabic and German)). Another case is: G:
Mit Flugzeugen la?sst sich Geld verdienen, (With
planes you can money earn), where a concrete ob-
ject replaces the whole procedure.
The metonymy could be reconstructed, for sim-
ple cases, by the Generative Lexicon (Pustejovsky
86), whose role is precisely to make explicit pro-
totypical functions of objects via their telic role, as
advocated above.
References
Dowty, D., 1989, On the Semantic Content of the No-
tion of Thematic Role, in G. Cherchia, B. Partee,
R. Turner (eds), Properties, Types and meaning,
Kluwer Academic.
Dowty, D., 1991, Thematic Proto-roles and Argument
Selection, Language, vol. 67-3.
Lakoff G., JohnsonM., 1999. Philosophy in the Flesh.
Basic books, NY, USA.
Mari, A., Saint-Dizier, P., 2001, A Conceptual Se-
mantics for Prepositions Denoting Instrumentality,
in proc. 1st workshop on prepositions, Toulouse, and
in Syntax and semantics of prepositions, P. Saint-
Dizier (ed), Kluwer academic, 2006.
Pustejovsky, J., 1991, The Generative Lexicon, Com-
putational Linguistics, vol. 17, MIT Press.
Raina, Achla M., 2002. The Verb Second Phe-
nomenon, O.N. Koul and K Wali (eds.), Topics in
Kashmiri Linguistics. Creative Books, New Delhi,
India.
Rosch, E., 1978. Principles of Categorization. In E.
Rosch and B.B. Lloyd (eds.), Cognition and Catego-
rization. Hillsdale : Lawrence Erlbaum Associates
Publishers.
Sinlapasarn, Upakitt. 1998. Thai Grammar. Thai
Watthana Panich, Bangkok, Thailand.
Talmy L., 2001, 2003. Towards a Cognitive Seman-
tics, vol. 1 and 2. MIT Press.
Wiezbicka, A.,1996. Semantics primes and universals.
Oxford: Oxford University Press.
Wierzbicka, A., 1992, Semantic Primitives and Se-
mantic Fields, in A. Lehrer and E.F. Kittay (eds.),
Frames, Fields and Contrasts. Hillsdale: Lawrence
Erlbaum Associates, pp. 208-227.
56
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 65?74,
Prague, June 2007. c?2007 Association for Computational Linguistics
Evolution, Optimization, and Language Change:
The Case of Bengali Verb Inflections
Monojit Choudhury1, Vaibhav Jalan2, Sudeshna Sarkar1, Anupam Basu1
1 Department of Computer Science and Engineering
Indian Institute of Technology, Kharagpur, India
{monojit,sudeshna,anupam}@cse.iitkgp.ernet.in
2 Department of Computer Engineering
Malaviya National Institute of Technology, Jaipur, India
vaibhavjalan.mnit@gmail.com
Abstract
The verb inflections of Bengali underwent
a series of phonological change between
10th and 18th centuries, which gave rise
to several modern dialects of the language.
In this paper, we offer a functional ex-
planation for this change by quantifying
the functional pressures of ease of artic-
ulation, perceptual contrast and learnabil-
ity through objective functions or con-
straints, or both. The multi-objective and
multi-constraint optimization problem has
been solved through genetic algorithm,
whereby we have observed the emergence
of Pareto-optimal dialects in the system
that closely resemble some of the real
ones.
1 Introduction
Numerous theories have been proposed to explain
the phenomenon of linguistic change, which, of late,
are also being supported by allied mathematical or
computational models. See (Steels, 1997; Perfors,
2002) for surveys on computational models of lan-
guage evolution, and (Wang et al, 2005; Niyogi,
2006) for reviews of works on language change.
The aim of these models is to explain why and how
languages change under specific socio-cognitive as-
sumptions. Although computational modeling is a
useful tool in exploring linguistic change (Cangelosi
and Parisi, 2002), due to the inherent complexi-
ties of our linguistic and social structures, modeling
of real language change turns out to be extremely
hard. Consequently, with the exception of a few
(e.g., Hare and Elman (1995); Dras et al (2003);
Ke et al (2003); Choudhury et al (2006b)), all the
mathematical and computational models developed
for explaining language change are built for artifi-
cial toy languages. This has led several researchers
to cast a doubt on the validity of the current compu-
tational models as well as the general applicability
of computational techniques in diachronic explana-
tions (Hauser et al, 2002; Poibeau, 2006).
In this paper, we offer a functional explanation1
of a real world language change ? the morpho-
phonological change affecting the Bengali verb
inflections (BVI). We model the problem as a
multi-objective and multi-constraint optimization
and solve the same using Multi-Objective Genetic
Algorithm2 (MOGA). We show that the different
forms of the BVIs, as found in the several modern
dialects, automatically emerge in the MOGA frame-
work under suitable modeling of the objective and
constraint functions. The model also predicts several
1Functionalist accounts of language change invoke the basic
function of language, i.e. communication, as the driving force
behind linguistic change (Boersma, 1998). Stated differently,
languages change in a way to optimize their function, such that
speakers can communicate maximum information with min-
imum effort (ease of articulation) and ambiguity (perceptual
contrast). Often, ease of learnability is also considered a func-
tional benefit. For an overview of different explanations in di-
achronic linguistics see (Kroch, 2001) and Ch. 3 of (Blevins,
2004).
2Genetic algorithm was initially proposed by Hol-
land (1975) as a self-organizing adaptation process mimicking
the biological evolution. They are also used for optimization
and machine learning purposes, especially when the nature of
the solution space is unknown or there are more than one objec-
tive functions. See Goldberg (1989) for an accessible introduc-
tion to single and multi-objective Genetic algorithms. Note that
in case of a multi-objective optimization problem, MOGA gives
a set of Pareto-optimal solutions rather than a single optimum.
The concept of Pareto-optimality is defined later.
65
other possible dialectal forms of Bengali that seems
linguistically plausible and might exist or have ex-
isted in the past, present or future. Note that the
evolutionary algorithm (i.e., MOGA) has been used
here as a tool for optimization, and has no relevance
to the evolution of the dialects as such.
Previously, Redford et al (2001) has modeled the
emergence of syllable systems in a multi-constraint
and multi-objective framework using Genetic al-
gorithms. Since the model fuses the individual
objectives into a single objective function through
a weighted linear combination, it is not a multi-
objective optimization in its true sense and nei-
ther does it use MOGA for the optimization pro-
cess. Nevertheless, the present work draws heavily
from the quantitative formulation of the objectives
and constraints described in (Redford, 1999; Red-
ford and Diehl, 1999; Redford et al, 2001). Ke et
al. (2003) has demonstrated the applicability and ad-
vantages of MOGA in the context of the vowel and
tonal systems, but the model is not explicit about the
process of change that could give rise to the optimal
vowel systems. As we shall see that the conception
of the genotype, which is arguably the most impor-
tant part of any MOGA model, is a novel and signif-
icant contribution of this work. The present formu-
lation of the genotype not only captures a snapshot
of the linguistic system, but also explicitly models
the course of change that has given rise to the partic-
ular system. Thus, we believe that the current model
is more suitable in explaining a case of linguistic
change.
The paper is organized as follows: Sec. 2 intro-
duces the problem of historical change affecting the
BVIs and presents a mathematical formulation of the
same; Sec. 3 describes the MOGA model; Sec. 4
reports the experiments, observations and their in-
terpretations; Sec. 5 concludes the paper by sum-
marizing the contributions. In this paper, Bengali
graphemes are represented in Roman script follow-
ing the ITRANS notation (Chopde, 2001). Since
Bengali uses a phonemic orthography, the phonemes
are also transcribed using ITRANS within two /s.
2 The Problem
Bengali is an agglutinative language. There are
more than 150 different inflected forms of a single
Attributes Classical (?0) SCB ACB Sylheti
PrS1 kari kori kori kori
PrS2 kara karo kara kara
PrS3 kare kare kare kare
PrSF karen karen karen karoin
PrC1 kariteChi korChi kartAsi koirtAsi
PrC2 kariteCha korCho kartAsa koirtAsae
PrC3 kariteChe korChe kartAse koirtAse
PrCF kariteChen korChen kartAsen kortAsoin
PrP1 kariAChi koreChi korsi koirsi
PrP2 kariACha koreCho karsa koirsae
PrP3 kariAChe koreChe karse koirse
PrPF kariAChen koreChen karsen korsoin
Table 1: The different inflected verb forms of Clas-
sical Bengali and three other modern dialects. All
the forms are in the phonetic forms and for the verb
root kar. Legend: (tense) Pr ? present; (aspects) S
? simple, C ? continuous, P ? perfect, ; (person) 1
? first, 2 ? second normal, 3 ? third, F ? formal in
second and third persons. See (Bhattacharya et al,
2005) for list of all the forms.
verb root in Bengali, which are obtained through af-
fixation of one of the 52 inflectional suffixes, option-
ally followed by the emphasizers. The suffixes mark
for the tense, aspect, modality, person and polarity
information (Bhattacharya et al, 2005). The ori-
gin of modern Bengali can be traced back to Vedic
Sanskrit (circa 1500 BC 600 BC), which during
the middle Indo-Aryan period gave rise to the di-
alects like Ma?gadhi?, and Ardhama?gadhi? (circa
600 BC 200 AD), followed by the Ma?gadhi? ?
apabhramsha, and finally crystallizing to Bengali
(circa 10th century AD) (Chatterji, 1926). The ver-
bal inflections underwent a series of phonological
changes during the middle Bengali period (1200 -
1800 AD), which gave rise to the several dialectal
forms of Bengali, including the standard form ? the
Standard Colloquial Bengali (SCB).
The Bengali literature of the 19th century was
written in the Classical Bengali dialect or the
sa?dhubha?sha? that used the older verb forms and
drew heavily from the Sanskrit vocabulary, even
though the forms had disappeared from the spoken
dialects by 17th century. Here, we shall take the lib-
erty to use the terms ?classical forms? and ?Classi-
cal Bengali? to refer to the dialectal forms of middle
Bengali and not Classical Bengali of the 19th cen-
66
tury literature. Table 1 enlists some of the corre-
sponding verb forms of classical Bengali and SCB.
Table 3 shows the derivation of some of the current
verb inflections of SCB from its classical counter-
parts as reported in (Chatterji, 1926).
2.1 Dialect Data
Presently, there are several dialects of Bengali that
vary mainly in terms of the verb inflections and in-
tonation, but rarely over syntax or semantics. We do
not know of any previous study, during which the
different dialectal forms for BVI were collected and
systematically listed. Therefore, we have collected
dialectal data for the following three modern dialects
of Bengali by enquiring the na?ive informants.
? Standard Colloquial Bengali (SCB) spoken in a
region around Kolkata, the capital of West Ben-
gal,
? Agartala Colloquial Bengali (ACB) spoken in
and around Agartala, the capital of Tripura, and
? Sylheti, the dialect of the Sylhet region of
Bangladesh.
Some of the dialectal forms are listed in Table 1.
The scope of the current study is restricted to 28 in-
flected forms (12 present tense forms + 12 past tense
forms + 4 forms of habitual past) of a single verb
root, i.e., kar.
2.2 Problem Formulation
Choudhury et al (2006a) has shown that a sequence
of simple phonological changes, which we shall
call the Atomic Phonological Operators or APO for
short, when applied to the classical Bengali lexicon,
gives rise to the modern dialects. We conceive of
four basic types of APOs, namely Del or deletion,
Met or metathesis, Asm or assimilation, and Mut
or mutation. The complete specification of an APO
includes specification of its type, the phoneme(s)
that is(are) affected by the operation and the left and
right context of application of the operator specified
as regular expressions on phonemes. The seman-
tics of the basic APOs in terms of rewrite rules are
shown in Table 2.2. Since Bengali features assim-
ilation only with respect to vowel height, here we
shall interpret Asm(p, LC,RC) as the height as-
similation of the vowel p in the context of LC or
APO Semantics
Del(p, LC,RC) p? ?/LC?RC
Met(pipj , LC,RC) pipj ? pjpi/LC?RC
Asm(p, LC,RC) p? p?/LC?RC
Mut(p, p?, LC,RC) p? p?/LC?RC
Table 2: Semantics of the basic APOs in terms of
rewrite rules. LC and RC are regular expressions
specifying the left and right contexts respectively. p,
p?, pi and pj represent phonemes.
Rule APO Example Derivations
No. kar ? iteChe kar ? iten kar ? iAChi
1 Del(e, ?, Ch) kar ? itChe NA NA
2 Del(t, ?, Ch) kar ? iChe NA NA
3 Met(ri, ?, ?) kair ? Che kair ? ten kair ?AChi
5 Mut(A, e, ?, Ch) NA NA kair-eChi
6 Asm(a, i, ?, ?) koir ? Che koir ? ten koir ? eChi
7 Del(i, o, ?) kor ? Che kor ? ten kor ? eChi
Table 3: Derivations of the verb forms of SCB from
classical Bengali using APOs. ?NA? means the rule
is not applicable for the form. See (Choudhury et
al., 2006a) for the complete list of APOs involved in
the derivation of SCB and ACB forms
RC. Also, we do not consider epenthesis or inser-
tion as an APO, because epenthesis is not observed
for the case of the change affecting BVI.
The motivation behind defining APOs rather than
representing the change in terms of rewrite rules is
as follows. Rewrite rules are quite expressive and
therefore, it is possible to represent complex phono-
logical changes using a single rewrite rule. On the
other hand, APOs are simple phonological changes
that can be explained independently in terms of pho-
netic factors (Ohala, 1993). In fact, there are also
computational models satisfactorily accounting for
cases of vowel deletion (Choudhury et al, 2004;
Choudhury et al, 2006b) and assimilation (Dras et
al., 2003).
Table 3 shows the derivation of the SCB verb
forms from classical Bengali in terms of APOs. The
derivations are constructed based on the data pro-
vided in (Chatterji, 1926).
2.3 Functional Explanation for Change of BVI
Let ?0 be the lexicon of classical Bengali verb
forms. Let ? : ?1, ?2, ? ? ? ?r be a sequence of r
APOs. Application of an APO on a lexicon implies
the application of the operator on every word of the
67
lexicon. The sequence of operators ?, thus, repre-
sent a dialect obtained through the process of change
from ?0, which can be represented as follows.
?(?0) = ?r(? ? ? ?2(?1(?0)) ? ? ?) = ?d
The derivation of the dialect ?d from ?0 can be con-
structed by following the APOs in the sequence of
their application.
We propose the following functional explanation
for the change of BVI.
A sequence of APOs, ? is preferred if ?(?0) has
some functional benefit over ?0. Thus, the modern
Bengali dialects are those, which have some func-
tional advantage over the classical dialect.
We would like to emphasize the word ?some? in
the aforementioned statements, because the modern
dialects are not better than the classical one (i.e., the
ancestor language) in an absolute sense. Rather, the
classical dialect is suboptimal compared to the mod-
ern dialects only with respect to ?some? of the func-
tional forces and is better than the them with respect
to ?some other? forces. Stated differently, we expect
both the classical as well as the modern dialects of
Bengali to be Pareto-optimal3 with respect to the set
of functional forces.
In order to validate the aforementioned hypoth-
esis, we carry out a multi-objective and multi-
constraint optimization over the possible dialectal
forms of Bengali, thereby obtaining the Pareto-
optimal set, which has been achieved through
MOGA.
3 The MOGA Model
Specification of a problem within the MOGA frame-
work requires the definition of the genotype, phe-
notype and genotype-to-phenotype mapping plus the
objective functions and constraints. In this section,
we discuss the design choices explored for the prob-
lem of BVI.
3Consider an optimization problem with n objective func-
tions f1 to fn, where we want to minimize all the objectives.
Let S be the solution space, representing the set of all possible
solutions. A soulution sinS is said to be Pareto-optimal with re-
spect to the objective functions f1 to fn, if and only if there does
not exist any other solution s? ? S such that fi(s?) ? fi(s) for
all 1 ? i ? n and fi(s?) < fi(s) for at least one i.
3.1 Phenotype and Genotype
We define the phenotype of a dialect d to be the lex-
icon of the dialect, ?d, consisting of the 28 inflected
forms of the root verb kar. This choice of phenotype
is justified because, at the end of the optimization
process, we would like to obtain the Pareto-optimal
dialects of Bengali and compare them with their real
counterparts.
The genotype of a dialect d could also be defined
as ?d, where the word forms are the genes. How-
ever, for such a choice of genotype, crossover and
mutation lead to counter-intuitive results. For ex-
ample, mutation would affect only a single word in
the lexicon, which is against the regularity principle
of sound change (see Bhat (2001) for explanation).
Similarly, exchanging a set of words between a pair
of lexica, as crossover would lead to, seems insensi-
ble.
Therefore, considering the basic properties of
sound change as well as the genetic operators used
in MOGA, we define a chromosome (and thus the
genotype) as a sequence of APOs. The salient fea-
tures of the genotype are described below.
? Gene: A gene is defined as an APO. Since in
order to implement the MOGA, every gene must be
mapped to a number, we have chosen an 8-bit binary
representation for a gene. This allows us to spec-
ify 256 distinct genes or APOs. However, for rea-
sons described below, we use the first bit of a gene
to denote whether the gene (i.e., the APO) is active
(the bit is set to 1) or not. Thus, we are left with
128 distinct choices for APOs. Since the number of
words in the lexicon is only 28, the APOs for Del,
Asm andMet are limited, even after accounting for
the various contexts in which an APO is applicable.
Nevertheless, there are numerous choices for Mut.
To restrain the possible repertoire of APOs to 128,
we avoided any APO related to the mutation of con-
sonants. This allowed us to design a comprehensive
set of APOs that are applicable on the classical Ben-
gali lexicon and its derivatives.
? Chromosome: A chromosome is a sequence of
15 genes. The number 15 has been arrived through
experimentation, where we have observed that in-
creasing the length of a chromosome beyond 15
does not yield richer results for the current choice
of APOs and ?0. Since the probability of any gene
68
Figure 1: Schematic of genotype, phenotype and
genotype-to-phenotype mapping.
being switched off (i.e., the first bit being 0) is 0.5,
the expected number of active APOs on a chromo-
some with 15 genes is 7.5. It is interesting to note
that this value is almost equal to the number of APOs
required (7 to be precise) for derivation of the SCB
verb forms.
? Genotype to phenotype mapping: Let for a given
chromosome, the set of active APOs (whose first bit
is 1) in sequence be ?1, ?2, ? ? ? , ?r. Then the pheno-
type corresponding to this chromosome is the lex-
icon ?d = ?r(? ? ? ?2(?1(?0)) ? ? ?). In other words,
the phenotype is the lexicon obtained by successive
application of the active APOs on the chromosome
on the lexicon of classical Bengali.
The concepts of gene, chromosome and the map-
ping from genotype to the phenotype are illustrated
in Fig. 3.1. It is easy to see that the regularity hy-
pothesis regarding the sound change holds good for
the aforementioned choice of genotype. Further-
more, crossover in this context can be interpreted as
a shift in the course of language change. Similarly,
mutation of the first bit turns a gene on or off, and of
the other bits changes the APO. Note that according
to this formulation, a chromosome not only models
a dialect, but also the steps of its evolution from the
classical forms.
3.2 Objectives and Constraints
Formulation of the objective functions and con-
straints are crucial to the model, because the linguis-
tic plausibility, computational tractability and the re-
sults of the model are overtly dependent on them.
We shall define here three basic objectives of ease
of articulation, perceptual contrast and learnability,
which can be expressed as functions or constraints.
Several models have been proposed in the past for
estimating the articulatory effort (Boersma (1998),
Ch. 2, 5 and 7) and perceptual distance between
phonemes and/or syllables (Boersma (1998), Ch.
3, 4 and 8). Nevertheless, as we are interested in
modeling the effort and perceptual contrast of the
whole lexicon rather than a syllable, we have cho-
sen to work with simpler formulations of the objec-
tive functions. Due to paucity of space, we are not
able to provide adequate details and justification for
the choices made.
3.2.1 fe: Articulatory Effort
Articulatory effort of a lexicon ? is a positive real
number that gives an estimate of the effort required
to articulate the words in ? in some unit. If fe de-
notes the effort function, then
fe(?) =
1
|?|
?
w??
fe(w) (1)
The term fe(w) depends on three parameters: 1)
the length of w in terms of phonemes, 2) the struc-
ture of the syllables, and 3) the features of adjacent
phonemes, as they control the effort spent in co-
articulation. We define fe(w) to be a weighted sum
of these three.
fe(w) = ?1fe1(w) + ?2fe2(w) + ?3fe3(w) (2)
where, ?1 = 1, ?2 = 1 and ?3 = 0.1 are the relative
weights.
The value of fe1 is simply the length of the word,
that is
fe1(w) = |w| (3)
Suppose ? = ?1?2 ? ? ??k is the usual syllabifica-
tion of w, where the usual or optimal syllabification
for Bengali is defined similar to that of Hindi as de-
scribed in (Choudhury et al, 2004). Then, fe2 is
defined as follows.
fe2(w) =
k?
i=1
hr(?i) (4)
hr(?) measures the hardness of the syllable ? and is
a function of the syllable structure (i.e. the CV pat-
tern) of ?. The values of hr(?) for different syllable
structures are taken from (Choudhury et al, 2004).
69
Since vowel height assimilation is the primary
co-articulation phenomenon observed across the di-
alects of Bengali, we define fe3 so as to model
only the effort required due to the difference in the
heights of the adjacent vowels.
Let there be n vowels in w represented by Vi,
where 1 ? i ? n. Then fe3 is defined by the fol-
lowing equation.
fe3(w) =
n?1?
i=1
|ht(Vi)? ht(Vi+1)| (5)
The function ht(Vi) is the tongue height associ-
ated with the vowel Vi. The value of the function
ht(Vi) for the vowels /A/, /a/, /E/ /o/, /e/, /i/
and /u/ are 0, 1, 1, 2, 2, 3, and 3 respectively. Note
that the values are indicative of the ordering of the
vowels with respect to tongue height, and do not re-
flect the absolute height of the tongue in any sense.
3.2.2 fd and Cd: Acoustic Distinctiveness
We define the acoustic distinctiveness between
two words wi and wj as the edit distance between
them, which is denoted as ed(wi, wj). The cost of
insertion and deletion of any phoneme is assumed to
be 1; the cost of substitution of a vowel (consonant)
for a vowel (consonant) is also 1, whereas that of a
vowel (consonant) for a consonant (vowel) is 2, ir-
respective of the phonemes being compared. Since
languages are expected to increase the acoustic dis-
tinctiveness between the words, we define a mini-
mizing objective function fd over a lexicon ? as the
sum of the inverse of the edit distance between all
pair of words in ?.
fd(?) =
2
|?|(|?| ? 1)
?
ij,i6=j
ed(wi, wj)
?1 (6)
If for any pair of words wi and wj , ed(wi, wj) =
0, we redefine ed(wi, wj)?1 as 20 (a large penalty).
We say that a lexicon ? violates the acoustic dis-
tinctiveness constraintCd, if there are more than two
pairs of words in ?, which are identical.
3.2.3 Cp: Phonotactic constraints
A lexicon ? is said to violate the constraint Cp if
any of the words in ? violates the phonotactic con-
straints of Bengali. As described in (Choudhury et
al., 2004), the PCs are defined at the level of sylla-
ble onsets and codas and therefore, syllabification is
a preprocessing step before evaluation of Cp.
3.2.4 fr and Cr: Regularity
Although learnability is a complex notion, one
can safely equate the learnability of a system to the
regularity of the patterns within the system. In fact,
in the context of morphology, it has been observed
that the so called learning bottleneck has a regular-
izing effect on the morphological structures, thereby
leaving out only the most frequently used roots to
behave irregularly (Hare and Elman, 1995; Kirby,
2001).
In the present context, we define the regularity
of the verb forms in a lexicon as the predictability
of the inflectional suffix on the basis of the mor-
phological attributes. Brighton et al (2005) discuss
the use of Pearson correlation between phonologi-
cal edit distance and semantic/morphological ham-
ming distance measures as a metric for learnabil-
ity. On a similar note, we define the regularity func-
tion fr as follows. For two words wi, wj ? ?, the
(dis)similarity between them is given by ed(wi, wj).
Let ma(wi, wj) be the number of morphological at-
tributes shared by wi and wj . We define the reg-
ularity of ?, fr(?), as the Pearson correlation co-
efficient between ed(wi, wj) and ma(wi, wj) for
all pairs of words in ?. Note that for a regular
lexicon, ed(wi, wj) decreases with an increase in
ma(wi, wj). Therefore, fr(?) is negative for a reg-
ular lexicon and 0 or positive for an irregular one.
In other words, fr(?) is also a minimizing objective
function.
We also define a regularity constraint Cr, such
that a lexicon ? violates Cr if fr(?) > ?0.8.
4 Experiments and Observations
In order to implement the MOGA model, we have
used the Non-dominated Sorting GA-II or NSGA-
II (Deb et al, 2002), which is a multi-objective,
multi-constraint elitist GA. Different MOGA mod-
els have been incrementally constructed by intro-
ducing the different objectives and constraints. The
motivation behind the incorporation of a new ob-
jective or constraint comes from the observations
made on the emergent dialects of the previous mod-
els. For instance, with two objectives fe and fd,
70
and no constraints, we obtain dialects that violate
phonotactic constraints or/and are highly irregular.
One such example of an emergent dialect4 is ? =
{ kor, kara, kar, kore, korea, kore, karA, karAa,
karA, *korAlm, *korl, korla, *koreAlm, korel, ko-
rela, *karAlm, karAl, karAla }. The * marked forms
violate the phonotactic constraints. Also note that
the forms are quite indistinct or close to each other.
These observations led to the formulation of the con-
straints Cp and Cd.
Through a series of similar experiments, finally
we arrived at a model, where we could observe the
emergence of dialects, some of which closely resem-
ble the real dialects and others also seem linguisti-
cally plausible. In this final model, there are two
objectives, fe and fd, and 3 constraints, Cp, Cd and
Cr. Table 4 lists the corresponding forms of some
of the emergent dialects, whose real counterparts are
shown in Table 1.
Fig. 2 shows the Pareto-optimal front obtained
for the aforementioned model after 500 generations,
with a population size of 1000. Since the objectives
are minimizing in nature, the area on the plot below
and left of the Pareto-optimal front represents im-
possible languages, whereas the area to the right and
top of the curve pertains to unstable or suboptimal
languages. It is interesting to note that the four real
dialects lie very close to the Pareto-optimal front. In
fact, ACB and SCB lie on the front, whereas clas-
sical Bengali and Sylheti appears to be slightly sub-
optimal. Nevertheless, one should always be aware
that impossibility and suboptimality are to be inter-
preted in the context of the model and any general-
ization or extrapolation of these concepts for the real
languages is controversial and better avoided.
Several inferences can be drawn from the exper-
iments with the MOGA models. We have observed
that the Pareto-optimal fronts for all the MOGA
Models look like rectangular hyperbola with a hori-
zontal and vertical limb; the specific curve of Fig. 2
satisfies the equation:
fd(?)
0.3(fe(?)? 5.6) = 0.26 (7)
Several interesting facts, can be inferred from the
above equation. First, the minimum value of fe un-
der the constraints Cr and Cd, and for the given
4Due to space constraints, we intentionally omit the corre-
sponding classical forms.
Figure 2: The Pareto-optimal front. The gray trian-
gles (light blue in colored version available online)
show the position of the real dialects: 0 ? Classi-
cal Bengali, 1 ? SCB, 2 ? ACB, 3 ? Sylheti. The
top-most dot in the plot corresponds to the emergent
dialect D0 shown in Table 4.
repertoire of APOs is 5.6. Second, at fe(?) = 6,
the slope of the front, i.e. dfd/dfe, is approximately
?2, and the second derivative d2fd/df2e is around
20. This implies that there is sharp transition be-
tween the vertical and horizontal limbs at around
fe(?) = 6.
Interestingly, all the real dialects studied here lie
on the horizontal limb of the Pareto-optimal front
(i.e., fe(?) ? 6), classical Bengali being placed at
the extreme right. We also note the negative corre-
lation between the value of fe for the real dialects,
and the number of APOs invoked during derivation
of these dialects from classical Bengali. These facts
together imply that the natural direction of language
change in the case of BVIs has been along the hor-
izontal limb of the Pareto-optimal front, leading to
the formation of dialects with higher and higher ar-
ticulatory ease. Among the four dialects, SCB has
the minimum value for fe(?) and it is positioned on
the horizontal limb of the front just before the begin-
ning of the vertical limb.
Therefore, it is natural to ask whether there are
any real dialects of modern Bengali that lie on the
vertical limb of the Pareto-optimal front; and if not,
what may be the possible reasons behind their inex-
istence? In the absence of any comprehensive col-
lection of Bengali dialects, we do not have a clear
answer to the above questions. Nevertheless, it may
71
Attributes D0 D1 D2 D3
PrS1 kar kor kori kori
PrS2 kara kora kora kora
PrS3 kare kore kore korA
PrSF karen koren koren koren
PrC1 kartA karChi karteChi kairteChi
PrC2 kartAa karCha karteCha kairteCha
PrC3 kartAe karChe karteChe kairteChA
PrCF kartAen karChen karteChen kairteChen
PrP1 karA korChi koriChi koriChAi
PrP2 karAa korCha koriCha koriACha
PrP3 karAe korChe koriChe koriAChA
PrPF karAen korChen koriChen koriAChen
Table 4: Examples of emergent dialects in the
MOGA model. Note that the dialects D1, D2 and
D3 resemble SCB, ACB and Sylheti, whereas D0
seems to be linguistically implausible. For legends,
refer to Table 1
be worthwhile to analyze the emergent dialects of
the MOGA models that lie on the vertical limb. We
have observed that the vertical limb consists of di-
alects similar to D0 ? the one shown in the first
column of Table 4. Besides poor distinctiveness,
D0 also features a large number of diphthongs that
might result in poorer perception or higher effort of
articulation of the forms. Thus, in order to eliminate
the emergence of such seemingly implausible cases
in the model, the formulations of the objectives fe
and fd require further refinements.
Similarly, it can also be argued that the structure
of the whole lexicon, which has not been modeled
here, has also a strong effect on the BVIs. This is
because even though we have measured the acous-
tic distinctiveness fd with respect to the 28 inflected
forms of a single verb root kar, ideally fd should be
computed with respect to the entire lexicon. Thus,
change in other lexical items (borrowing or extinc-
tion of words or change in the phonological struc-
tures) can trigger or restrain an event of change in
the BVIs.
Furthermore, merging, extinction or appearence
of morphological attributes can also have significant
effects on the phonological change of inflections. It
is interesting to note that while Vedic Sanskrit had
different morphological markers for three numbers
(singular, dual and plural) and no gender markers
for the verbs, Hindi makes a distinction between the
genders (masculine and feminine) as well as num-
bers (but only singular and plural), and Bengali has
markers for neither gender nor number. Since both
Hindi and Bengali are offshoots of Vedic Sanskrit,
presumably the differences between the phonologi-
cal structure of the verb inflections of these two lan-
guages must have also been affected by the loss or
addition of morphological attributes. It would be in-
teresting to study the precise nature of the interac-
tion between the inflections and attributes within the
current computational framework, which we deem
to be a future extension of this work.
5 Conclusions
In this paper, we have described a MOGA based
model for the morpho-phonological change of BVIs.
The salient contributions of the work include: (1) the
conception of the genotype as a sequence of APOs,
whereby we have been able to capture not only the
emergent dialects, but also the path towards their
emergence, and (2) a plausible functional explana-
tion for the morpho-phonological changes affecting
the BVIs. Nevertheless, the results of the experi-
ments with the MOGA models must be interpreted
with caution. This is because, the results are very
much dependent on the formulation of the fitness
functions and the choice of the constraints. The set
of APOs in the repertoire also play a major role in
shaping the Pareto-optimal front of the model.
Before we conclude, we would like to re-
emphasize that the model proposed here is a func-
tional one, and it does not tell us how the dialects
of Bengali have self-organized themselves to strike
a balance between the functional pressures, if at all
this had been the case. The evolutionary algorithm
(i.e., MOGA) has been used here as a tool for op-
timization, and has no relevance to the evolution of
the dialects as such. Nevertheless, if it is possible
to provide linguistically grounded accounts of the
sources of variation and the process of selection,
then the MOGA model could qualify as an evolu-
tionary explanation of language change as well. Al-
though such models have been proposed in the liter-
ature (Croft, 2000; Baxter et al, 2006), the fact, that
global optimization can be an outcome of local inter-
actions between the speakers (e.g., Kirby (1999), de
72
Boer (2001), Choudhury et al (2006b)), alone pro-
vides sufficient ground to believe that there is also an
underlying self-organizational model for the present
functional explanation.
References
G. J. Baxter, R. A. Blythe, W. Croft, and A. J. McKane.
2006. Utterance selection model of language change.
Physical Review E, 73(046118).
D.N.S. Bhat. 2001. Sound Change. Motilal Banarsidass,
New Delhi.
S. Bhattacharya, M. Choudhury, S. Sarkar, and A. Basu.
2005. Inflectional morphology synthesis for bengali
noun, pronoun and verb systems. In Proc. of NCCPB,
pages 34?43, Dhaka.
Julia Blevins. 2004. Evolutionary Phonology. Cam-
bridge University Press, Cambridge, MA.
P. Boersma. 1998. Functional Phonology: Formaliz-
ing the interactions between articulatory and percep-
tual drives. Uitgave van Holland Academic Graphics,
Hague.
Henry Brighton, Kenny Smith, and Simon Kirby. 2005.
Language as an evolutionary system. Physics of Life
Reviews, 2(3):177?226, September.
A. Cangelosi and D. Parisi. 2002. Comuputer simula-
tion: A new scientific approach to the study of lan-
guage evolution. In Simulating the Evolution of Lan-
guage, pages 3?28. Springer Verlag, London.
S. K. Chatterji. 1926. The Origin and Development of
the Bengali Language. Rupa and Co., New Delhi.
A. Chopde. 2001. Itrans version 5.30: A package
for printing text in indian languages using english-
encoded input. http://www.aczoom.com/itrans/.
M. Choudhury, A. Basu, and S. Sarkar. 2004. A di-
achronic approach for schwa deletion in indo-aryan
languages. In Proc. of ACL SIGPHON-04, pages 20?
26, Barcelona.
M. Choudhury, M. Alam, S. Sarkar, and A. Basu.
2006a. A rewrite rule based model of bangla morpho-
phonological change. In Proc. of ICCPB, pages 64?
71, Dhaka.
M. Choudhury, A. Basu, and S. Sarkar. 2006b. Multi-
agent simulation of emergence of the schwa deletion
pattern in hindi. JASSS, 9(2).
W. Croft. 2000. Explaining Language Change: An Evo-
lutionary Approach. Longman Linguistic Library.
B. de Boer. 2001. The Origins of Vowel Systems. Oxford
University Press.
K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. 2002.
A fast and elitist multi-objective genetic algorithm:
NSGA-II. IEEE Transactions on Evolutionary Com-
putation, 6:182?197.
M. Dras, D. Harrison, and B. Kapicioglu. 2003. Emer-
gent behavior in phonological pattern change. In Arti-
ficial Life VIII. MIT Press.
David E. Goldberg. 1989. Genetic Algorithms in Search,
Optimization and Machine Learning. Addison-
Wesley.
M. Hare and J. L. Elman. 1995. Learning and morpho-
logical change. Cognition, 56(1):61?98, July.
M. D. Hauser, N. Chomsky, and W. T. Fitch. 2002. The
faculty of language: What is it, who has it, and how
did it evolve? Science, 298:1569?1579, 11.
John H. Holland. 1975. Adaptation in Natural and Arti-
ficial Systems. The University of Michigan Press, Ann
Arbor.
Jinyun Ke, Mieko Ogura, and William S-Y. Wang. 2003.
Modeling evolution of sound systems with genetic al-
gorithm. Computational Linguistics, 29(1):1?18.
S. Kirby. 1999. Function, Selection and Innateness: the
Emergence of Language Universals. Oxford Univer-
sity Press. The full-text is only a sample (chapter 1: A
Puzzle of Fit).
S. Kirby. 2001. Spontaneous evolution of linguistic
structure: an iterated learning model of the emergence
of regularity and irregularity. IEEE Transactions on
Evolutionary Computation, 5(2):102?110.
Anthony Kroch. 2001. Syntactic change. In Mark baltin
and Chris Collins, editors, Handbook of Syntax, pages
699?729. Blackwell.
P. Niyogi. 2006. The Computational Nature of Language
Learning and Evolution. MIT Press, Cambridge, MA.
J. Ohala. 1993. The phonetics of sound change. In
C. Jones, editor, Historical linguistics: Problems and
perspectives, page 237278. Longman, London.
A. Perfors. 2002. Simulated evolution of language: a
review of the field. Journal of Artificial Societies and
Social Simulation, 5(2).
T. Poibeau. 2006. Linguistically grounded models of
language change. In Proc. of CogSci 2006, pages 255?
276.
73
Melissa A. Redford and R. L. Diehl. 1999. The rela-
tive perceptibility of syllable-initial and syllable-final
consonants. Journal of Acoustic Society of America,
106:1555?1565.
Melissa A. Redford, Chun Chi Chen, and Risto Mi-
ikkulainen. 2001. Constrained emergence of univer-
sals and variation in syllable systems. Language and
Speech, 44:27?56.
Melissa A. Redford. 1999. An Articulatory Basis for
the Syllable. Ph.D. thesis, Psychology, University of
Texas, Austin.
L. Steels. 1997. The synthetic modeling of language
origins. Evolution of Communication, 1(1):1?34.
W. S-Y. Wang, J. Ke, and J. W. Minett. 2005. Computa-
tional studies of language evolution. In Computational
Linguistics and Beyond: Perspectives at the beginning
of the 21st Century, Frontiers in Linguistics 1. Lan-
guage and Linguistics.
74
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 61?64,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Learning Multi Character Alignment Rules and Classification of training
data for Transliteration
Dipankar Bose
Dept. of Computer Science and Engg.
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
dipankarcsiit@gmail.com
Sudeshna Sarkar
Dept. of Computer Science and Engg.
Indian Institute of Technology
Kharagpur, West Bengal
India - 721302
shudeshna@gmail.com
Abstract
We address the issues of transliteration be-
tween Indian languages and English, es-
pecially for named entities. We use an
EM algorithm to learn the alignment be-
tween the languages. We find that there
are lot of ambiguities in the rules map-
ping the characters in the source language
to the corresponding characters in the tar-
get language. Some of these ambiguities
can be handled by capturing context by
learning multi-character based alignments
and use of character n-gram models. We
observed that a word in the source script
may have actually originated from differ-
ent languages. Instead of learning one
model for the language pair, we propose
that one may use multiple models and a
classifier to decide which model to use. A
contribution of this work is that the models
and classifiers are learned in a completely
unsupervised manner. Using our system
we were able to get quite accurate translit-
eration models.
1 Introduction
Transliteration is the practice of transcribing a
word or text written in one writing system into an-
other writing system which may have a different
script (wikipedia 1). The rules are often quite am-
biguous, and they are often related with the pro-
nunciation of the word.
Many applications like Machine Transla-
tion (MT), Cross Language Information Re-
trieval (CLIR), Question Answering (QA) require
1http://www.wikipedia.org
transliteration of named entities, which are the ma-
jor component of out-of-vocabulary (OOV) words,
and they are most often transliterated and not
translated, in any cross language system. For ex-
ample ,?Europe? is transliterated as ?iuropa? and
?Michael? transliterates to ?maaikela? in Bengali.2
In this paper we develop a scheme of translit-
eration, which captures context by creating a dic-
tionary of multi-character transliteration rules. We
have tested our system for English and several In-
dian languages. For Indian Languages, we have an
additional preprocessor which enhances the per-
formance.
2 Related Work
Brown et al (1993) have come up with their revo-
lutionary IBM alignment models, and the Giza++
(Och and Ney, 2000) is a well appreciated imple-
mentation which work with parallel data in two
languages. Though originally designed for ma-
chine translation, the package can as well be used
for transliteration, where the alignment is between
the characters in the languages. Moses further en-
hances the accuracy by using phrase based decod-
ing, which can capture context. We have Moses3
as our baseline system.
Li et al (2004) have pointed out the prob-
lems of using language information. Apart from
the difficulty of collecting the language informa-
tion, they pointed out that, although written in
the same script, the origin of the source names
may vary widely. For example French and Eng-
lish names may vary a lot. But it is difficult
to collect information for each and every lan-
guage. They came up with a joint source chan-
2above Bengali words are scripted using ITrans, instead
of traditional Bengali script.
3http://www.statmt.org/moses/
61
nel model, to transliterate foreign names to Chi-
nese, Korean, and Japanese, which uses, direct or-
thographic mapping (DOM), between two differ-
ent languages, to find out how the source and tar-
get words can be generated simultaneously. Ekbal
et al (2006) also used this model for English-
Bengali Transliteration. Ganesh et al (2008)
used Hidden Markov Model (HMM) alignment
and Conditional Random Field (CRF), a discrim-
inative model together. Surana et al (2008) used
fuzzy string matching algorithms to identify the
origin of the source word, and then apply rules of
transliteration accordingly. However the classifier
makes use of labeled training data, which is often
not available.
3 Issues
Transliteration is ambiguous. Firstly, the translit-
eration rules depend on the context. For exam-
ple, ?a? in English may transliterate to ?a? or ?A?
in Hindi, but ?aa? almost definitely maps to ?A?.
Secondly, there can be multiple transliterations
of the same source word. For example ?abhi-
jIta? may transliterate to ?abhijit? and ?abhijeet? as
well. Thirdly, the transliteration rules also vary,
depending on the origin of the word. For exam-
ple, when considering Hindi to English translitera-
tion the English characters used vary depending on
whether the word originated from Arabic or from
Sanskrit. We elaborate more on this in the section
on classification of corpus.
4 Approach
Our method is primarily based on IBM models
used in machine translation based on the EM al-
gorithm. But before we move on to the IBM mod-
els, we first preprocess the training data. Other
than marking the ?Start? and ?End?, for each of the
parallel words, we can do further preprocessing if
any of the scripts is Indian. All Indian language
scripts consist of a set of consonants and vowels.
Independent vowels and their corresponding dia-
critic markers (Matra) are considered as the same
character in the standard analysis of words into
their constituent characters (varna vishleshhana).
Unlike ITrans, Unicode assigns different codes to
them. We found in our experiment that treating
them as one, improves the accuracy of the system.
Our preprocessor thus transforms Unicode data to
ITrans format. We have seen that preprocessor im-
proves the accuracy by around 10-15%.
After preprocessing, we align the letters us-
ing the expectation maximization (EM) algorithm
of IBM model 1, using the parallel corpus of
named entities as input. We use only the IBM
model 1; the subsequent models are omitted since
in transliteration we need not consider the re-
ordering of letters. Both Unicode and transliter-
ated text are in phonetic order, and re-ordering of
letters are rarely observed. As an output of the EM
learner we get a table of translation probabilities
TP , of source letters to target letters. If, si and
tj are source and target letters, ?si, tj , TP si,tj ?
[0, 1], denotes the corresponding translation prob-
ability. For example after EM learning, the values
of TPbha,v and TPbha,b will be much more than
TPbha,k, since ?bha? rarely transliterates to ?k?.
4.1 Learning Phrase Mappings
We now move on to capture context. For each
word in the parallel data, we compute an align-
ment array, Ae, where e ? [0, E], and I and E
are the corresponding lengths of the words in In-
dian and English script respectively. So, we have,
?e ? [0, E], Ae ? [0, I]. Following is an example:
Let, source word be: Start s1 s2 s3 End, target
word be: Start t1 t2 t3 t4 End, and Alignment ar-
ray be: 0 1 1 2 3 4. This means that s1 maps to
t1 and t2; s2 maps to t3 and so on. We further
enforce Ae1 ? Ae2 iff e1 ? e2, since we neglect
re-ordering of letters. The aim is to figure out null
mappings, filter out noises in the TP-table, and fi-
nally create a phrase to phrase mapped dictionary.
Using the TP-table values, we propose an iterative
algorithm to find the alignment array A. WL[i] de-
notes the ith letter of a word in language ?L?. Ini-
tially Ai = 0 if i = 0, Ai = I?1 if i < E, otherwise
Ai = I . The first and last characters are always the
?Start? and ?End? tags, in all the words.
Initially letters are allowed a larger window to
fit to. After each iteration, the window size de-
creases and thus the margins are made more strin-
gent. Using iterations we are being less greedy in
deciding the alignment, so that noises in the TP-
table are filtered out. Finally after 5 iterations,
we freeze the alignment array. It may happen that
?i ? [0, I], such that ?j ? [0, E], Aj 6= i. It
means that the letter, WInd[i] maps to ?null? in this
case, and thus it is a ?Schwa? character.
4.2 Scoring the alignment
In spite of all our attempts, it may happen that the
words are not well aligned; the reason may be a
62
Algorithm 1 Method to compute Alignment
for window = 5 to 1 do
for e = 1 to E ? 1 do
left = Max(1, Ae?1 ? window + 1)
right = Min(I, Ae+1 + window)
Ae = s : s ? [left, right] such that
TPWInd[s],WEng[e] ? (1? |s/I ? e/E|)
is maximum
end for
for e = 1 to E ? 1 do
if ?(Ae?1 ? Ae ? Ae+1) then
{try to smooth out anomalies}
Ae = (Ae?1 +Ae+1)/2
end if
end for
end for
deficiency in the Algorithm 1, or a badly transliter-
ated parallel word as input. For example the train-
ing data may contain ?mississippi river? translit-
erated to Bengali as ?misisipi nadI?. In this case
we see that the second word is translated and not
transliterated. Retaining this in the training set
will introduce noise in the model. There may also
be typographical errors also. We have developed
a filtering mechanism, so that we can eliminate
these words, otherwise we will end up learning
spurious mappings. We find the score of an align-
ment,
SA = ?N?1e=1 (TPWInd[Ae],WEng [e] ? (1? |Ae/I ?
e/E|).
We were trying to maximize SA under certain
constraints in algorithm 1. The value of SA is
an estimate of how good our alignment is. Next
we set thresholds to distinguish between different
?Classes? of alignments.
4.3 Classifying the training corpus
The training corpus may consist of words from
varied origins. Though they are written in
the same script, pronunciation varies widely.
For example Urdu origin names like Farooque
(pharUka), Razzaq (rajjAka) tend to replace ?q? in
place of ?ka?, but Hindi names like Latika (latika),
Nakul (nakula), tend to replace ?k? for ?ka?. Unlike
Surana et al (2008) who extracted 5-gram models
from labeled data in different languages, we pro-
pose Algorithm 2, to classify the parallel corpus
into groups, which does not need any labeled data.
We define, Classes C1, C2, ..., CN , where Ci con-
sists of a set of parallel words < Ij , Ej >, (Ij ,
Ej being the jth word in Indian and English lan-
guage, in the training corpus), such that the align-
ment score of the word pairs, lie between the pre-
defined thresholds, thi+1 and thi. Let us assume
that C1 is initialized with the parallel training cor-
pus from input.
Algorithm 2 Classify the Corpus
for i = 1 to N do
Set threshold, thi for Class Ci: thi ? thi?1
while size of Class Ci does not decrease do
Compute TP-table using IBM model 1. on
Ci
for each parallel word pair < Ij , Ej > in
Ci do
Compute Alignment using Algorithm 1.
Compute Score of Alignment, SA.
if Score < thi then
{Move the word pair to the next
class}
Ci+1 = Ci+1? < Ij , Ej >
Ci = Ci\ < Ij , Ej >
end if
end for
end while{move on to next Class}
end for
We continuously discard word pairs from a
class until there is no word pair to be discarded.
We use IBM Model 1 to re-learn the TP-table, on
the latest content of the class. Since the poor word
pairs have been removed, learning the TP-table
afresh, helps in improving the TPsi,tj values. It
helps in removing the bad word pairs yet left, in
the subsequent iterations. It is to be noted that CN
consists of word pairs, which are of no use, and we
discard them completely. We had 5 useful classes,
and the thresholds of C1 to C5 were 0.4, 0.35, 0.3,
0.25, 0.2 respectively. In each class, for each word
pair, we extract all possible ngrams on Indian lan-
guage side and collect their corresponding English
characters, using the alignment array. We keep fre-
quency counts of these ngram mappings, and use
this score in decoding. We use a language model,
which uses Good Turing smoothing technique. We
have used greedy beam search based decoder.
All that remains is to guess the class of an un-
known word. Given a test word, in source script
we calculate probability Pi of it being in class,
Ci, based on ngram similarities. The decoders of
each of the classes returns a list of feasible translit-
eration candidates along with their ?local scores?
63
Language Accuracy in Top1 Mean F-Score MRR MAPref MAP10 MAPsys
En2Ta 0.404 0.883 0.539 0.398 0.182 0.182
En2Hi 0.366 0.854 0.493 0.360 0.164 0.164
En2Ka 0.335 0.856 0.457 0.328 0.154 0.154
Table 1: Transliteration Accuracies. En2Ta: English to Tamil, En2Hi: English to Hindi, En2Ka: English
to Kannada
(score according to that class), We denote the lo-
cal score of a candidate from Class Ci as LS[Ci].
We calculate the global score, GS for each candi-
date, using GS= ?N?1i=1 (LS[Ci]?Pi). The candi-
dates are sorted in decreasing order of their global
scores and top ?K? of them produced as output.
5 Results
We have evaluated our system, against datasets
with Hindi, Tamil, Kannada and English parallel
named entities (Kumaran and Kellner, 2007). The
results are in Table 1. The data consists of named
entities from varied origins: almost all Indian lan-
guages and English. We combined the training and
development sets to create the new training set.
There are about 9000 parallel words in the train-
ing sets and 1000 words for testing.
Algorithm 2 classifies the training corpus, into
5 sets of corpus. Following are some details af-
ter classifying the Tamil-English dataset. Corpus
1, consists of Sanskrit derived words mostly; they
get perfectly aligned and Schwa deletions rarely
occur; Ex: Keena, Asiya, Nehra, Hemaraaj, Vi-
jendra. This corpus contains 2167 words. Cor-
pus 2 also is mostly comprised of Sanskrit de-
rived words and also English words which eas-
ily align; like Wilton, Natesh, Raghu, Gerry,
Achintya, Amaanat. Schwa deletions does occur,
and hence the alignment scores are a little low.
Size of this corpus is 2168.
Corpus 3 consists more of Urdu origin and
English words, which are not fit for the normal
transliteration rules. The corpus consists of words
like Tarzan, Anoife, Sevier, Zahid Fazal, Floriane,
where letters like ?q?, ?zz?, ?y? are more likely than
?k?, ?j?, ?i? respectively. The size of Corpus 3 is
1835. Corpus 4 & 5 consists largely of English
origin words, like Lucky number, Ian Healy, Clea-
vant, Fort Vancouver, Virginia Reel, Bundesver-
dienstkreuz. These words need completely differ-
ent set of rules, and moreover if these words were
in any other class, it would corrupt their learning
rules. Size of these corpora are 1234 and 1455 re-
spectively.
6 Conclusion
Our system is robust in the sense that it can filter
out noise in the training corpus, can handle words
of different origins by classifying them into dif-
ferent classes. Our classifying algorithm improves
the accuracy, but we believe that there is scope of
further improvement and we are working on it.
References
Asif Ekbal, Sudip Kumar Naskar, Sivaji Bandyopad-
hyay. 2006. A modified joint source-channel
model for transliteration. Proceedings of the
COLING/ACL on Main conference poster ses-
sions.Sydney, Australia.
Harshit Surana and A. K. Singh 2008. A More Dis-
cerning and Adaptable Multilingual Transliteration
Mechanism for Indian Languages. The Third In-
ternational Joint Conference on Natural Language
Processing (IJCNLP). Hyderabad, India.
Kumaran A. and Kellner Tobias. 2007. A generic
framework for machine transliteration SIGIR ?07:
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 721?722.
Li Haizhou, Zhang Min, Su Jian. 2004. A joint
source-channel model for machine transliteration.
Proceedings of the 42nd Annual Meeting on As-
sociation for Computational Linguistics. Barcelona,
Spain.
Och Franz Josef and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447, Hong Kong, China.
Peter F. Brown, Vincent J. Delta Pietra, Stephen A.
Delta Pietra and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: parame-
ter estimation. MIT Press Cambridge, MA, USA.
Surya Ganesh, Sree Harsha, Prasad Pingali, Vasudeva
Verma. 2008. Statistical Transliteration for Cross
Language Information Retrieval using HMM align-
ment model and CRF. CLIA-2008, 2nd International
workshop on Cross Language Information Access,
3rd International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2008), January 7-12,
2008, Hyderabad, India.
64
Bengali and Hindi to English CLIR Evaluation  
 
Debasis Mandal, Sandipan Dandapat, Mayank Gupta, Pratyush Banerjee, 
Sudeshna Sarkar 
 
Abstract 
 
Our participation in CLEF 2007 consisted of two Cross-lingual and one 
monolingual text retrieval in the Ad-hoc bilingual track. The cross-language 
task includes the retrieval of English documents in response to queries in 
two Indian languages, Hindi and Bengali. The Hindi and Bengali queries 
were first processed using a morphological analyzer (Bengali), a stemmer 
(Hindi) and a set of 200 Hindi and 273 Bengali stop words. The refined 
hindi queries were then looked into the Hindi-English bilingual lexicon, 
?Shabdanjali? (approx. 26K Hindi words) and all of the corresponding 
translations were considered for the equivalent English query generation, if a 
match was found. Rest of the query words were transliterated using the 
ITRANS scheme. For the Bengali query, we had to depend mostly on the 
translietrations due to the lack of any effective Bengali-English bilingual 
lexicon. The final equivalent English query was then fed into the Lucene 
Search engine for the monolingual retrieval of the English documents. The 
CLEF evaluations suggested the need for a rich bilingual lexicon, a good 
Named Entity Recognizer and a better transliterator for CLIR involving 
Indian languages. The best MAP values for Bengali and Hindi CLIR for our 
experiment were 7.26 and 4.77 which are 0.20 and 0.13 of our monolingual 
retrieval, respectively. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 35?42,
Beijing, August 2010
Co-occurrence Graph Based Iterative Bilingual Lexicon Extraction From
Comparable Corpora
Diptesh Chatterjee and Sudeshna Sarkar and Arpit Mishra
Department of Computer Science and Engineering
Indian Institute of Technology Kharagpur
{diptesh,sudeshna,arpit}@cse.iitkgp.ernet.in
Abstract
This paper presents an iterative algorithm
for bilingual lexicon extraction from com-
parable corpora. It is based on a bag-
of-words model generated at the level of
sentences. We present our results of ex-
perimentation on corpora of multiple de-
grees of comparability derived from the
FIRE 2010 dataset. Evaluation results on
100 nouns shows that this method outper-
forms the standard context-vector based
approaches.
1 Introduction
Bilingual dictionaries play a pivotal role in a num-
ber of Natural Language Processing tasks like
Machine Translation and Cross Lingual Informa-
tion Retrieval(CLIR). Machine Translation sys-
tems often use bilingual dictionaries in order to
augment word and phrase alignment (Och and
Ney, 2003). CLIR systems use bilingual dictio-
naries in the query translation step (Grefenstette,
1998). However, high coverage electronic bilin-
gual dictionaries are not available for all language
pairs. So a major research area in Machine Trans-
lation and CLIR is bilingual dictionary extraction.
The most common approach for extracting bilin-
gual dictionary is applying some statistical align-
ment algorithm on a parallel corpus. However,
parallel corpora are not readily available for most
language pairs. Also, it takes a lot of effort to ac-
tually get the accurate translations of sentences.
Hence, constructing parallel corpora involves a lot
of effort and time. So in recent years, extract-
ing bilingual dictionaries from comparable cor-
pora has become an important area of research.
Comparable corpora consist of documents on sim-
ilar topics in different languages. Unlike parallel
corpora, they are not sentence aligned. In fact,
the sentences in one language do not have to be
the exact translations of the sentence in the other
language. However, the two corpora must be on
the same domain or topic. Comparable corpora
can be obtained more easily than parallel corpora.
For example, a collection of news articles from
the same time period but in different languages
can form a comparable corpora. But after care-
ful study of news articles in English and Hindi
published on same days at the same city, we have
observed that along with articles on similar top-
ics, the corpora also contain a lot of articles which
have no topical similarity. Thus, the corpora are
quite noisy, which makes it unsuitable for lexicon
extraction. Thus another important factor in com-
parable corpora construction is the degree of sim-
ilarity of the corpora.
Approaches for lexicon extraction from compara-
ble corpora have been proposed that use the bag-
of-words model to find words that occur in similar
lexical contexts (Rapp, 1995). There have been
approaches proposed which improve upon this
model by using some linguistic information (Yuu
and Tsujii, 2009). However, these require some
linguistic tool like dependency parsers which are
not commonly obtainable for resource-poor lan-
guages. For example, in case of Indian languages
like Hindi and Bengali, we still do not have good
enough dependency parsers. In this paper, we
propose a word co-occurrence based approach for
lexicon extraction from comparable corpora using
English and Hindi as the source and target lan-
guages respectively. We do not use any language-
35
specific resource in our approach.
We did experiments with 100 words in En-
glish,and show that our approach performs signif-
icantly better than the the Context Heterogeneity
approach (Fung, 1995). We show the results over
corpora with varying degrees of comparability.
The outline of the paper is as follows. In section
2, we analyze the different approaches for lexicon
extraction from comparable corpora. In section 3,
we present our algorithm and the experimental re-
sults. In section 4, we present an analysis of the
results followed by the conclusion and future re-
search directions in section 5.
2 Previous Work
One of the first works in the area of comparable
corpora mining was based on word co-occurrence
based approach (Rapp, 1995). The basic assump-
tion behind this approach was two words are likely
to occur together in the same context if their joint
probability of occurrence in a corpus exceeds the
probability that the words occur randomly. In his
paper, Rapp made use of a similarity matrix and
using a joint probability estimate determined the
word maps. However this approach did not yield
significantly good results.
The ?Context Heterogeneity? approach was one
of the pioneering works in this area. It uses a 2-
dimensional context vector for each word based
on the right and left context. The context vector
depended on how many distinct words occur in the
particular context and also the unigram frequency
of the word to be translated. Euclidean distance
between context vectors was used as a similarity
measure.
Another approach used Distributed Clustering of
Translational Equivalents for word sense acqui-
sition from bilingual comparable corpora (Kaji,
2003). However, the major drawback of this paper
is the assumption that translation equivalents usu-
ally represent only one sense of the target word.
This may not be the case for languages having
similar origin, for example, Hindi and Bengali.
Approaches using context information for extract-
ing lexical translations from comparable corpora
have also been proposed (Fung and Yee, 1998;
Rapp, 1999). But they resulted in very poor cov-
erage. These approaches were improved upon
by extracting phrasal alignments from comparable
corpora using joint probability SMT model (Ku-
mano et al, 2007) .
Another proposed method uses dependency pars-
ing and Dependency Heterogeneity for extracting
bilingual lexicon (Yuu and Tsujii, 2009) . This
approach was similar to that of Fung, except they
used a dependency parser to get the tags for each
word and depending on the frequency of each tag
they defined a vector to represent each word in
question. Here too, Euclidean similarity was used
to compute the similarity between two words us-
ing their context vectors. However, this method is
dependent on availability of a dependency parser
for the languages and is not feasible for languages
for which resources are scarce.
3 Bilingual Dictionary Extraction Using
Co-occurrence Information
3.1 Motivation
The Context Heterogeneity and Dependency Het-
erogeneity approaches suffer from one major
drawback. They do not use any kind of infor-
mation about how individual words combine in a
particular context to form a meaningful sentence.
They only use some statistics about the number of
words that co-occur in a particular context or the
number of times a word receives a particular tag
in dependency parsing. So, we wished to study if
the quality of dictionary extracted would improve
if we consider how individual words co-occur in
text and store that information in the form of a
vector, with one dimension representing one word
in the corpus. One important point to note here
is that the function words in a language are usu-
ally very small in number. If we need to construct
a dictionary of function words in two languages,
that can be done without much effort manually.
Also, the function words do not play an impor-
tant role in CLIR applications, as they are usually
stripped off.
Our algorithm is based on the intuition that words
having similar semantic connotations occur to-
gether. For example, the words ?bread? is more
likely to occur with ?eat? than with ?play?. Our
algorithm uses this distribution of co-occurrence
frequency along with a small initial seed dictio-
36
nary to extract words that are translations of one
another. We define a co-occurrence vector of
words in both the languages, and also record the
number of times two words co-occur. To find
the translation for word Wx, we check for the
words co-occurring with Wx such that this word
already has a map in the other language, and com-
pute a scoring function using all such words co-
occurring with Wx. In short, we use the already
existing information to find new translations and
add them to the existing lexicon to grow it. Be-
low is a snapshot of a part of the data from one
of our experiments using the FIRE 20101 cor-
pus. For each word in English and Hindi, the co-
occurrence data is expressed as a list of tuples.
Each tuple has the form (word, co-occurrence
frequency). For the Hindi words, the English
meaning has been provided in parenthesis. For
the seed lexicon and final lexicon, the format is
(source word, target word, strength).
English:
1. teacher:{(training,49),(colleges,138),
(man,22)}
2. car:{(drive,238),(place,21)}
3. drive:{(car,238),(steer,125),(city,12),
(road,123)}
Hindi:
1. ghar(home):{(khidki(window),133),(makAn
(house),172), (rAstA(road),6)}
2. gAdi(car):{(rAsta,92),(chAlak(driver),121),
(signal,17)}
3. shikshaka(teacher):{(vidyalaya(school),312),
(makAn(house),6)}
Seed lexicon:
1. (colleges,vidyalaya,0.4)
2. (colleges,mahavidyalaya(college),0.6)
3. (car,gAdi,1.0)
The following is a snapshot from the final results
given by the algorithm:
1Forum For Information Retrieval
http://www.isical.ac.in/?clia/index.html
1. (car,gAdi,1.0)
2. (teacher,shikshak,0.62)
3. (teacher, vidyalaya,0.19)
4. (road, rAsta, 0.55)
3.2 The Algorithm
For extracting bilingual lexicon, we have not con-
sidered the function words of the two languages.
In order to filter out the function words, we have
made use of the assumption that content words
usually have low frequency in the corpus, whereas
function words have very high frequency. First,
we define some quantities:
Let the languages be E and H.
We = Set of words in E = {e1, e2, ...., eN}
Wh = Set of words in H = {h1, h2, ...., hM}
|We| = N
|Wh| = M
MAP = Initial map given
= {(ei, hj , wij)|wij = wt(ei, hj), ei ? We, hj ? Wh}
EM = Set of words in E which are included in
entries of MAP
HM = Set of words in H which are included in
entries of MAP
Co occ(x) = Set of words which co-occur with word x
Co occ?(x) =
(
Co occ(x) ? EM if x ? We
Co occ(x) ?HM if x ? Wh
Wte(x) = {Wey|y ? We and y ? Co occ(x)}
Wth(x) = {Why|y ? Wh and y ? Co occ(x)}
Given a comparable corpus, we follow the fol-
lowing steps of processing:
1. A sentence segmentation code is run to seg-
ment the corpus into sentences.
2. The sentence-segmented corpus is cleaned of
all punctuation marks and special symbols by
replacing them with spaces.
37
Algorithm 1 Algorithm to Extract Bilingual Dictionary by using word Co-occurrence Information
repeat
for ei ? We do
for hj ? Wh do
if (ei, hj , 0) ? MAP then
wt(ei, hj) =
P
e?Co occ?(ei)
P
h?Co occ?(hj)(WijWeeiWhhj )P
e?Co occ?(ei)
P
h?Co occ?(hj)(WeeiWhhj )end if
end for
end for
Select the pair with highest value of wt(ei, bj) and add it to the existing map and normalize
until termination
3. The collection frequency of all the terms are
computed and based on a threshold, the func-
tion words are filtered out.
4. The co-occurrence information is computed
at sentence-level for the remaining terms. In
a sentence, if words wi and wj both occur,
then wi ? Co occ(wj) and vice versa.
5. Since we can visualize the co-occurrence in-
formation in the form of a graph, we next
cluster the graph into C clusters.
6. From each cluster Ci, we choose some fixed
number number of words and manually find
out their translation in the target language.
This constitutes the initial map.
7. Next we apply Algorithm 1 to compute the
word maps.
The time complexity of the algorithm is
O(IM2N2), where I is the number of itera-
tions of the algorithm.
3.3 Corpus Construction
The corpora used for evaluating our algorithm
were derived from the FIRE 2010 English and
Hindi corpora for the ad-hoc retrieval task. These
corpora contained news articles spanning over a
time period of three years from two Indian news-
papers, ?The Dainik Jagaran? in Hindi and ?The
Telegraph? in English. However, due to the ex-
treme level of variation of the topics in these cor-
pora, we applied a filtering algorithm to select a
subset of the corpora.
Our approach to make the text similar involved
reducing the corora based on matching Named
Entities. Named Entities of English and Hindi
corpus were listed using LingPipe2 and a Hindi
NER system built at IIT Kharagpur(Saha et al,
1999). The listed Named Entities of the two cor-
pora were compared to find the matching Named
Entities. Named Entities in Hindi Unicode were
converted to iTRANS3 format and matched with
English Named Entities using edit distance. Unit
cost was defined for each insert and delete opera-
tion. Similar sounding characters like ?s?, ?c?,?a?,
?e? etc were assigned a replacement cost of 1 and
other characters were assigned a replacement cost
of 2. Two Named Entities were adjudged match-
ing if:
(2 ? Cost)/(WLh +WLe) < 0.5
where,
WLh = Length of Hindi word
WLe = Length of English word
Using this matching scheme, accuracy of match-
ing of Hindi and English Named Entities was
found to be > 95%. It was observed that there
are large number of Named Entities with small
frequency and few Named Entities with large fre-
quency. So a matching list was prepared which
contained only those Named Entities which had
frequency larger than a ?MaxFreq . This en-
sured that matching list had words with high fre-
quency in both corpus.So English words with fre-
quency larger than 368 and Hindi words with
frequency larger than 223 were considered for
matching. Based on this matching list, the two
2http://alias-i.com/lingpipe/
3http://www.aczoom.com/itrans/
38
Language Total NE Unique
NE
NE with freq
larger than?MaxFreq
NE
Matched
Total No
of docs
% of NE covered
According
to Zipf?s
Law
In the
actual
corpus
Hindi 1195474 37606 686 360 54271 63.0% 74.3%
English 5723292 137252 2258 360 87387 65.2% 71.0%
Table 1: Statistics of the main corpora used for extraction
Corpus Max Freq
Word
Max
Freq
?MaxFreq
Hindi bharat 50072 223
English calcutta 135780 368
Table 2: Criteria used for thresholding in the two
corpora
Matching
% of
NE per
document
Total documents in
corpora
Hindi English
> 10% 34694 16950
> 20% 14872 4927
> 30% 2938 1650
Table 3: Statistics of extracted corpora
corpora were reduced by including only those files
each of which contained more than a certain fixed
percentage of total matching Named Entities. The
corpus statistics are provided in tables 1, 2 and 3.
We assume that distribution of Named Entities
follows Zipf?s law (Zipf, 1949). And analysis
shows that Named Entities with frequency greater
than the chosen threshold lead to high cover-
age both theoretically and in practice (Table 1).
Hence, the threshold was chosen as ?MaxFreq.
The differences in the theoretical and actual val-
ues can be attributed to the poor performance of
the NER systems, especially the Hindi NER sys-
tem, whose output contained a number of false
positives.
3.4 Experimental Setup
The languages we used for our experiments were
English and Hindi. English was the source lan-
guage and Hindi was chosen as the target. For
our experiments, we used a collection frequency
threshold of 400 to filter out the function words.
The words having a collection frequency more
than 400 were discarded. This threshold was ob-
tained manually by ?Trial and Error? method in
order to perform an effective function word fil-
tering. For each corpora, we extracted the co-
occurrence information and then clustered the co-
occurrence graph into 20 clusters. From each
cluster we chose 15 words, thus giving us an over-
all initial seed dictionary size of 300. We ran the
algorithm for 3000 iterations.
For graph clustering, we used the Graclus system
(Dhillon et al, 2007) which uses a weighted ker-
nel k-means clustering algorithm at various levels
of coarseness of the input graph.
3.5 Evaluation Method and Results
For evaluation, we have used the Accuracy and
MMR measure (Voorhees, 1999). The measures
are defined as follows:
Accuracy = 1N
PN
i=1 ti
where, ti =
(
1 if correct translation in top n
0 otherwise
MMR = 1N
PN
i=1
1
ranki
where, ranki =
(
ri if ri ? n
0 otherwise
n means top n evaluation
ri means rank of correct translation in top n ranking
N means total number of words used for evaluation
For our experiments, we have used:
39
Corpus Context Het-
erogeneity
Co-
occurrence
Acc MMR Acc MMR
> 10% 0.14 0.112 0.16 0.135
> 20% 0.21 0.205 0.27 0.265
> 30% 0.31 0.285 0.35 0.333
Table 4: Comparison of performance between
Context Heterogeneity and Co-occurrence Ap-
proach for manual evaluation
n = 5
N = 100
The 100 words used for evaluation were chosen
randomly from the source language.
Two evaluation methods were followed - manual
and automated. In the manual evaluation, a
person who knows both English and Hindi was
asked to find the candidate translation in the target
language for the words in the source language.
Using this gold standard map, the Accuracy and
MMR values were computed.
In the second phase (automated), lexicon ex-
tracted is evaluated against English to Hindi
wordnet4. The evaluation process proceeds as
follows:
1. Hashmap is created with English words as
keys and Hindi meanings as values.
2. English words in the extracted lexicon are
crudely stemmed so that inflected words
match the root words in the dictionary. Stem-
ming is done by removing the last 4 charac-
ters, one at a time and checking if word found
in dictionary.
3. Accuracy and MMR are computed.
As a reference measure, we have used Fung?s
method of Context Heterogeneity with a context
window size of 4. The results are tabulated in
Tables 4 and 6. We can see that our proposed
algorithm shows a significant improvement over
the Context Heterogeneity method. The degree
of improvement over the Context Heterogeneity
4Downloadable from
http://sanskritdocuments.org/hindi/dict/eng-hin-itrans.html
Corpus Accuracy MMR
> 10% ? 14.28% ? 20.53%
> 20% ? 28.57% ? 29.27%
> 30% ? 12.9% ? 16.84%
Table 5: Degree of improvement shown by Co-
occurrence approach over Context Heterogeneity
for manual evaluation
Corpus Context Het-
erogeneity
Co-
occurrence
Acc MMR Acc MMR
> 10% 0.05 0.08 0.05 0.08
> 20% 0.06 0.06 0.11 0.10
> 30% 0.13 0.11 0.15 0.13
Table 6: Comparison of performance between
Context Heterogeneity and Co-occurrence Ap-
proach for auto-evaluation
is summarized in Tables 5 and 7. For auto
evaluation, We see that the proposed approach
shows the maximum improvement (83.33% in
Accuracy and 66.67% in MMR) in performance
when the corpus size is medium. For very large
(too general) corpora, both the approaches give
identical result while for very small (too specific)
corpora, the proposed approach gives slightly
better results than the reference.
The trends are similar for manual evaluation.
Once again, the maximum improvement is
observed for the medium sized corpus (> 20%).
However, in this evaluation system, the proposed
approach performs much better than the reference
even for the large (more general) corpora.
Corpus Accuracy MMR
> 10% 0.0% 0.0%
> 20% ? 83.33% ? 66.67%
> 30% ? 15.38% ? 18.18%
Table 7: Degree of improvement shown by Co-
occurrence approach over Context Heterogeneity
for auto-evaluation
40
4 Discussion
The co-occurrence based approach used in this
paper is quite a simple approach in the sense that
it does not make use of any kind of linguistic
information. From the aforementioned results
we can see that a model based on simple word
co-occurrence highly outperforms the ?Context
Heterogeneity? model in almost all the cases.
One possible reason behind this is the amount of
information captured by our model is more than
that captured by the ?Context Heterogeneity?
model. ?Context Heterogeneity? does not model
actual word-word interactions. Each word is
represented by a function of the number of
different contexts it can occur in. However, we
represent the word by a co-occurrence vector.
This captures all possible contexts of the word.
Also, we can actually determine which are the
words which co-occur with any other word. So
our model captures more semantics of the word in
question than the ?Context Heterogeneity? model,
thereby leading to better results. Another possible
factor is the nature in which we compute the
translation scores. Due to the iterative nature of
the algorithm and since we normalize after each
iteration, some of the word pairs that received
unduly high score in an earlier iteration end up
having a substantially low score. However, since
the ?Context Heterogeneity? does only a single
pass over the set of words, it fails to tackle this
problem.
The seed dictionary plays an important role in
our algorithm. A good seed dictionary gives us
some initial information to work with. However,
since ?Context Heterogeneity? does not use a
seed dictionary, it loses out on the amount of
information initially available to it. Since the seed
dictionary size for our approach is quite small,
it can be easily constructed manually. However,
how the seed dictionary size varies with corpus
size is an issue that remains to be seen.
Another important factor in our algorithm is the
way in which we have defined the co-occurrence
vectors. This is not the same as the context vector
that we define in case of Context Heterogeneity.
In a windowed context vector, we fail to capture a
lot of dependencies that might be captured using
a sentence-level co-occurrence. This problem is
especially more visible in case of free-word-order
languages like the Indo-European group of lan-
guages. For these languages, a windowed context
vector is also likely to introduce many spurious
dependencies. Since Hindi is a language of this
family, our algorithm captures many more correct
semantic dependencies than Context Heterogene-
ity algorithm, resulting in better preformance.
Another strong point of our proposed approach
is the closeness of the values of Accuracy and
MMR. This shows that the translation candidates
extracted by our algorithm are not only correct,
but also the best translation candidate gets the
highest score with high probability. This is a very
important factor in Machine Translation systems,
where a more accurate dictionary would give us
an improved performance.
A noticeable point about the evaluation scores is
the difference in scores given by the automated
system and the manual system. This can be
attributed to synonymy and spelling errors. In
the target language Hindi, synonymy plays a
very important part. It is not expected that all
synonyms of a particular word may be present
in an online dictionary. In such cases, the
manual evaluator marks a translation pair as
True, whereas the automated system marks it as
False. Instances of spelling errors have also been
found. For example, for the word ?neighbors?,
the top translation provided by the system was
?paDosana?(female neighbor). If we consider
root form of words, this is correct. But the actual
translation should be ?paDosiyAn?(neighbors,
may refer to both male and female). Thus the
auto evaluation system tags it as False, whereas
the manual evaluator tags it as True. There are
many more such occurrences throughout.
Apart from that, the manual evaluation process
has been quite relaxed. Even if the properties like
tense, number of words does not match, as long
as the root forms match the manual evaluator has
marked it as True. But this is not the case for
the automated evaluator. Although stemming has
been done, but problems still persist which can be
only solved by lemmatization, because Hindi is a
highly inflected language.
41
5 Conclusion and Future Work
In this paper we present a completely new ap-
proach for extracting bilingual lexicon from com-
parable corpora. We show the results of experi-
mentation on corpora of different levels of com-
parability. The basic feature of this approach is
that it is language independent and needs no ad-
ditional resource. We could not compare its per-
formance with the Dependency Heterogeneity al-
gorithm due to the lack of resources for Hindi.
So this can be taken up as a future work. Also,
the algorithm is quite inefficient. Another direc-
tion of research can be in trying to explore ways
to reduce the complexity of this algorithm. We
can also try to incorporate more linguistic infor-
mation into this model instead of just word co-
occurrence. It remains to be seen how these fac-
tors affect the performance of the algorithm. An-
other important question is what should be the size
of the seed dictionary for optimum performance
of the algorithm. This too can be taken up as a
future research direction.
References
Dhillon, I., Y. Guan, and B. Kulis. 2007. Weighted
graph cuts without eigenvectors: A multilevel ap-
proach. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 29:11:1944?
1957, November.
Fung, Pascale and Lo Yuen Yee. 1998. An ir ap-
proach for translating new words from nonparallel,
comparable texts. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics / the 17th International Conference on
Computational Linguistics, pages 414?420.
Fung, Pascale. 1995. Compiling bilingual lexicon
entries from a non-parallel english-chinese corpus.
In Third Annual Workshop on Very Large Corpora,
Boston, Massachusetts, June.
Grefenstette, G. 1998. The problem of cross-language
information retrieval. Cross-language Information
Retrieval.
Kaji, H. 2003. Word sense acquisition from bilingual
comparable corpora. In Proc. of HLT-NAACL 2003
Main papers, pages 32?39.
Kumano, T., H. Takana, and T. Tokunaga. 2007. Ex-
tracting phrasal alignments from comparable cor-
pora by using joint probability smt model. In Proc.
of TMI.
Och, F. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
Rapp, Reinhard. 1995. Identifying word translations
in non-parallel texts. In Proc. of TMI.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated english and ger-
man corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics, pages 519?526.
Saha, Sujan Kumar, Sudeshna Sarkar, and Pabitra Mi-
tra. 1999. A hybrid feature set based maximum
entropy hindi named entity recognition. In Proceed-
ings of the Third International Joint Conference on
Natural Language Processing, pages 343?349, Hy-
derabad, India, January.
Voorhees, E.M. 1999. The trec-8 question answer-
ing track report. In Proceedings of the 8th Text Re-
trieval Conference.
Yuu, K. and J. Tsujii. 2009. Extracting bilingual dic-
tionary from comparable corpora with dependency
heterogeneity. In Proc. of NAACL-HLT, short pa-
pers, pages 121?124.
Zipf, George Kingsley. 1949. Human Behaviour and
the Principle of Least Effort: an Introduction to Hu-
man Ecology. Addison-Wesley.
42
