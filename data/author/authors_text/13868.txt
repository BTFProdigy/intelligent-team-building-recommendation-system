Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 57?64,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Anchoring Dutch Cultural Heritage Thesauri to WordNet: two case studies
Ve?ronique Malaise? and Antoine Isaac
Vrije Universiteit
Amsterdam
The Netherlands
{vmalaise, aisaac}@few.vu.nl
Luit Gazendam
Telematica Instituut
Enschede
The Netherlands
Luit.Gazendam@telin.nl
Hennie Brugman
Max Planck Institute
for Psycholinguistics, Nijmegen
The Netherlands
Hennie.Brugman@mpi.nl
Abstract
In this paper, we argue on the interest of an-
choring Dutch Cultural Heritage controlled
vocabularies to WordNet, and demonstrate
a reusable methodology for achieving this
anchoring. We test it on two controlled
vocabularies, namely the GTAA thesaurus,
used at the Netherlands Institute for Sound
and Vision (the Dutch radio and television
archives), and the GTT thesaurus, used to in-
dex books of the Dutch National Library. We
evaluate the two anchorings having in mind a
concrete use case, namely generic alignment
scenarios where concepts from one thesaurus
must be aligned to concepts from the other.
1 Introduction
Cultural Heritage Institutions are the keepers of large
collections of data. To optimize the core tasks of
indexing and searching through these collections,
controlled vocabularies like thesauri are often used.
These vocabularies are structured concept networks1
and help indexers to select proper subjects for de-
scription, and users to formulate queries or to browse
1The typical semantic relationships found between elements
from thesauri are Broader Term linking a specialized concept
to a more general one, Narrower Term, its inverse relationship,
and Related Term, which denotes a general associative link.
Thesauri also contain lexical information, where the preferred
terms used for description are given synonyms or non-preferred
terms (Use and Used for links), as well as general scope notes
giving indexers instructions regarding the use of a term.
collections using the concepts that appear in the
metadata.
The Netherlands Institute for Sound and Vision2,
for example, uses the GTAA thesaurus for indexing
public radio and TV programs ? GTAA is a Dutch
abbreviation for ?Common Thesaurus [for] Audio-
visual Archives?. Its hierarchy of subjects contains
about 3800 Preferred Terms and 2000 Non Preferred
terms. A second example is the GTT thesaurus, which
contains 35000 concepts, gathering 50000 preferred
and non-preferred Dutch terms. This thesaurus is
used to index and retrieve books from the Dutch Na-
tional Library3 ? GTT is a Dutch abbreviation for
?GOO keyword thesaurus?, GOO referring to the Joint
Subject Indexing system used by many Dutch li-
braries.
Besides this classic scenario, thesauri can also al-
low for (semi-)automatic optimization of search pro-
cesses, like query expansion exploiting their hierar-
chical structure. But the available structure might
not be rich and regular enough for such purposes. In
fact, it has been shown that a mapping to a richer
and sounder terminology, like the English Word-
Net (Fellbaum, 1998), would enable more sophisti-
cated query expansion or other inferencing possibil-
ities (Voorhees, 1994; Hollink, 2006). This will be-
come especially true now that WordNet exists in the
form of an RDF ontology (van Assem et al, 2006).
Mapping Cultural Heritage controlled vocabular-
2http://www.beeldengeluid.nl
3http://www.kb.nl
57
ies in Dutch to WordNet can also be beneficial for
sharing information across institutions, which is dif-
ficult when the metadata attached to the different doc-
uments come from different thesauri. This issue can
be solved by building equivalence links between the
elements from these different vocabularies, as in (van
Gendt et al, 2006). This vocabulary alignment prob-
lem is comparable to the ontology matching one, and
techniques similar to the ones developed by the Se-
mantic Web research community can be applied here.
As found e.g. in (Euzenat, 2004), the existing meth-
ods are quite diverse, and proposed strategies often
mix several individual techniques:
? lexical techniques, trying to compare the labels
found in vocabularies;
? structural techniques, assessing similarities be-
tween concepts from the structure of vocabular-
ies (e.g. hierarchical links);
? instance-based techniques, looking at the ob-
jects that are actually populating the ontologies
to infer from their similarities correspondences
between the concepts they instantiate.
? techniques making use of some background
knowledge source, by trying to derive from the
information found there relations between the
elements from the original vocabularies.
Here, we are interested in the last kind of techniques.
In these approaches, concepts from the vocabular-
ies to be aligned are first attached ? ?anchored? ?
to the concepts from a third vocabulary (Aleksovski,
2006). Then, these anchors in the background vo-
cabulary are compared together. When a relation is
found between them4, a similar relation can be in-
ferred between the elements from the vocabularies
to be aligned. This is especially interesting when
the lexical overlap between the vocabularies is low
or when the vocabularies are quite poorly structured:
it is expected then that the background knowledge
will alleviate these shortcomings. The choice of
4The reader can turn to (Budanitsky and Hirst, 2006) for an
overview of the different methods that have been proposed in this
field.
this knowledge is therefore crucial, and WordNet,
which has a rich structure and a broad coverage, has
been exploited in many existing alignment methods
(Giunchiglia et al, 2005; Castano et al, 2005).
For these reasons ? even if this paper will only
focus on the alignment scenario ? we wanted to ex-
periment the anchoring of two aforementioned Dutch
thesauri to WordNet. Unlike literature about linking
English thesauri to WordNet, we propose in this pa-
per an anchoring method for vocabularies in other
languages, and experiment it on these two thesauri,
testing its usefulness in terms of possibilities for vo-
cabulary alignment. The remainder of the paper is
organized as follows: in section 2, we present the
general anchoring methodology. The anchoring ex-
periment is described in section 3: first the GTAA
case (section 3.1) and then the GTT one (section 3.2),
as a reusability test. We evaluate the two anchoring
processes in section 3.3 and conclude on general re-
flexions about this method. Then, we show exam-
ples of such anchorings in the context of a possible
alignment between GTAA and GTT in section 4. We
conclude on perspectives to this research in section 5.
2 Anchoring methodology
The anchoring experiment presented in this paper is
based on a comparison of lexical descriptions of the
thesaurus terms with the ones of WordNet synsets,
the glosses: WordNet is a lexical database of En-
glish, which entries are grouped ?into sets of cog-
nitive synonyms (synsets), each expressing a distinct
concept?5. In contrast to many anchoring methods,
like the one in (Khan and Hovy, 1997), we do not
compare the terms from our thesauri to the labels of
synsets, but measure the lexical overlap of their de-
scriptions. The same approach has already been fol-
lowed, for example, by (Knight and Luk, 1994).
As the thesauri we focus on in this paper are in
Dutch, we first need to map their terms to English de-
scriptions, and possibly translations, to make a com-
parison with the English glosses. Given the fact that
these thesauri cover a broad range of topics, we hy-
pothesize that using a general language bilingual dic-
5http://wordnet.princeton.edu/
58
tionary will lead to a good coverage of their content.
Additionally, it might give on top of the definitions
? i.e. the natural language descriptions of a term?s
meaning ? useful information such as term transla-
tions and Part Of Speech (POS) tags ? their gram-
matical category: noun, verb, etc. For each thesaurus
term which has been associated to an English defini-
tion, the rest of the anchoring procedure consists in
checking the overlap between the lexical content of
the definitions and the one of the different WordNet
glosses, considered as bags of words. The hypothesis
is that the closest gloss should give us a pointer to a
synset semantically equivalent to the intended mean-
ing of a thesaurus term.
3 Anchoring feasibility experiments and
evaluations
3.1 Anchoring GTAA concepts
First step: Finding English definitions for GTAA
terms The first step in mapping Dutch terms from
the GTAA to WordNet was to select an online dic-
tionary that would cover a significant part of the the-
saurus entries and that would allow automatic queries
for these terms. We have tested the bilingual dictio-
nary LookWAYup6, which returned a 2222 results ?
definitions and translations ? on our query set.
This query set consisted in the list of GTAA Pre-
ferred terms (3800), Non preferred terms (2000) and
their singular forms7 (3200). These singular forms
were computed in the context of a MultimediaN
project8, on the basis of linguistic derivational rules
and a manual correction.
Given the fact that most of the thesaurus terms are
in plural form, but not all of them9, and knowing that
the dictionary entries are only standard lemma forms
(most of the time in singular), we first assumed that
6Built by RES Inc., Canada, online at the URL: http://
lookwayup.com/free/.
7Following the recommendations of the ISO standard, most
of GTAA terms are in plural form.
8MultimediaN Project 5 ? Semantic Multimedia Ac-
cess, http://monetdb.cwi.nl/projects/trecvid/
MN5/index.php/Main_Page, transformation done by Gijs
Geleijnse, from the Philips Research group.
9For example, the term corresponding to Baptism is in singu-
lar form.
queries on the dictionary with a plural form would
not generate a result, and simply added the singu-
lar forms to the singular ones in the query set. It
turned out that the dictionary gave result for some
plural forms, creating noise: some plural forms cor-
responded to lemmas of verbs, and a spelling cor-
rection facility provided definitions for some plural
forms.
Removing doubles We cleaned manually the first
set of errors, and automatically the last one, based
on POS tag information. In the future, we will avoid
introducing duplicate lemmas in our the query set.
After cleaning, 1748 terms had one or more trans-
lation in English together with their associated POS
tag(s) and definition(s)10. This low number, com-
pared with the original set of 5800 distinct thesaurus
terms can be explained by the fact that our vocabu-
lary contains numerous multi-words terms and also
compound entries, both of which are rarely dictio-
nary entries. We discuss possible solutions to this
shortcoming in section 3.3.
POS tag-based cleaning We did then a rough man-
ual evaluation of these candidate definitions. The
evaluation was conducted by three people and took
about one day each. It turned out that some of the
definitions were irrelevant for our task: the Dutch Bij
was associated with the English Bee and Honey bee,
but also with the preposition by. We used again the
information given by the POS tag to remove these
irrelevant definitions: we kept only definitions of
Nouns and (relevant) Verbs. After this last cleaning,
some terms still had more then one definition.
Cleaning based on thesaurus relationships We
used the hierarchical relationship in the thesaurus to
check the intended meaning of these terms: for ex-
ample, Universiteit (University) had a Broader Term
relationship with Wetenschappelijk onderwijs (Scien-
tific education), so its meaning is restricted to the
?Educational aspect?, and it should not be used to
describe TV programs about University buildings for
instance. We used this information to restrict the
101299 terms have more than one definition.
59
Step Result
Gathering query set 3800 + 2000 + 3200
terms
Querying dictionary 2222 defined terms
Removing doubles 1748 different defined
terms
POS tag-based cleaning
Thesaurus-based cleaning
1655 def. terms, 7530
definitions
Anchoring to WordNet 1060 anchored con-
cepts
Table 1: GTAA term anchoring experiment
number of valid candidate definitions associated with
every GTAA term. But in some cases the distinc-
tion was hard to make between the different defini-
tions, or no clue was provided by the thesaurus to
dismabiguate the senses of the term: sometimes it
did not have any relationship to other concepts nor
explanatory text (Scope Note).
Conclusion of the first step As a final result, as
summarized in table 1, 1655 GTAA terms had one
or more English equivalent and their related candi-
date definitions (7530). We decided to postpone a
more in-depth validation to the evaluation of anchor-
ing results with WordNet: we kept all candidate def-
initions and translations that were not obviously in-
correct, and checked the WordNet anchoring result
to see if some further refinement had to be done. The
idea was that the anchoring process would only work
for parts of the definitions, so we wanted to keep as
many data as possible.
Second step: Anchoring to WordNet synsets We
stemmed the candidate definitions of GTAA terms and
the glosses from WordNet with the Porter stemmer to
augment mapping possibilities. Stemming is the op-
eration of reducing words to a root, for example by
removing the ?s? character at the end of an (English)
word in plural form. This process can reduce differ-
ent unrelated words to a same root, and hence should
be handled with care, but it requires less resources
then a full fledged lemmatizing and helps compar-
ing a larger number of words then on the basis of the
graphical forms only. As announced, in order to map
synset to GTAA terms, we compared their lexical de-
scriptions: we compared the different sets of stems in
a simple bag-of-words approach. We actually found
out that the definitions of the online dictionary were
exact matches with WordNet glosses, thus all defined
terms could be straightforwardly anchored to one or
more synsets. In the end, 1060 concepts from GTAA
are successfully anchored to a synset, which repre-
sents 28% of the total number of concepts.
Evaluation of the results We evaluated the num-
ber of semantically relevant anchorings for a ran-
dom representative part of the the 1655 GTAA terms
that had one or more WordNet anchor: we evaluated
1789 mappings out of 7530. On these 1789 map-
pings, 85 were not equivalence links: 5 out of these
85 links were relating Related Terms (like zeerov an-
chored to corsair, the first being in GTAA a profes-
sion and the second a ship in Wordnet), 17 pointed
to Broader Terms, and the others were mapping a
term with a correct translation that was correct per
se but did not correspond to the intended meaning
of the term in GTAA. For example, two anchorings
were proposed for Vrouwen: married woman and fe-
male person, the latter one being the only valid for
our thesaurus. The first cases (RT and BT relation-
ships between the original term and its anchoring)
still provide useful information for aligning vocab-
ularies, but we took only equivalence relationships
into account in this experiment.
An additional evaluation that was also performed
on a sample set was to check that non-preferred terms
that were given a definition were pointing to the same
synset as their related preferred terms. It turned to be
correct for the evaluated pairs.
On a qualitative perspective, we found different
types of mappings:
? some GTAA terms had more then one transla-
tion, all of them pointing to the same synset: this
was the confirmation that the mapping from the
term to the synset was correct;
? some GTAA terms had more then one trans-
lation, pointing to different but close synsets:
nothing in the thesaurus content could help us
distinguish between the different synsets, thus
we kept the different possibilities;
60
? some different GTAA terms pointed to a same
synset and, although they were not linked in
the thesaurus, they had a semantic relationship.
This information can be used to enrich the struc-
ture of the GTAA.
We can conclude that the anchoring was quite suc-
cessful: only 4.7% of the anchorings were incorrect
in the test sample. And this was due to cases where
multiple senses were linked to a same term, which
would not cause a big problem in a semi-automated
anchoring process. Moreover, this process can bring
an additional value to the thesaurus structure itself,
on top of the possible applications mentioned in the
introduction.
3.2 Anchoring GTT concepts
Setting We carried out for GTT the same exper-
iment as for GTAA, but did not compute singular
forms, although GTT terms are generally in plural
form. Also, because GTT had 70% of its concepts al-
ready translated to English by human experts, we de-
cided that we would measure the global performance
of our method based on this translation gold standard,
additionally to manually assess the relevance of the
produced anchorings from GTT to WordNet.
Results Out of the 35194 GTT general subjects,
only 2458 were given some English definition and
translation by the dictionary service we used. For the
set of 25775 concepts for which there was already a
translation, the figure drops down to 2279, slightly
less than 9%.
As said, we tested the validity of these definitions
and translations by comparing them to the expert
translations. Our assumption was that an English def-
inition for a concept would prove to be correct if its
associated term matched one of the expert transla-
tions of the concept11. We found that 1479 of the
2279 concepts being given both expert and automatic
translations had the expert translation confirming one
11A manual checking of this assumption on the first 150 con-
cepts matching the criterion demonstrated an error rate of 4%:
4% of the concepts had no correct definition in their associated
glosses while there was a match between the expert translation
and one of the terms linked to the definitions.
of the automatically found ones, i.e. a precision rate
of 65% in terms of defined concepts.
When measuring accuracy of the found English
definitions for the 2279 defined concepts, we saw that
out of a total 3813 English definitions associated to a
concept, 2626 ? 69% ? had an associated term con-
firmed by the expert translation.
We also tried to assess the quality of the trans-
lations associated to the concepts of this set by our
method: out of 5747 terms proposed as translations,
1479 matched the expert translation. This precision
rate is low (25.7%) but it actually highlights one of
the problem of the expert translations found in the
thesaurus: the manual translation had a very low lex-
ical coverage, having provided with very few syn-
onyms for the ?preferred? translations. The set of
25775 translated GTT concepts only brings 26954
English terms in total. . .
The evaluation by comparison to the expert trans-
lation brings useful information, but it has some
drawbacks, especially the limited coverage of the
translation work and a correctness assumption bring-
ing a (small) error rate. To complete it, we carried
out a manual investigation, inspired by what had been
done for the GTAA thesaurus.
For this, we selected the 179 concepts that were
translated by our method but had not previously been
assigned English labels by experts. For this subset,
441 glosses had been assigned. Of these, 172 were
correct, concerning 138 concepts. We therefore ob-
tain a 77% precision rate in terms of anchored con-
cepts. However, if we aim at assessing the quality
of the method and its potential to be used in a semi-
automatic anchoring process, we have to consider the
obtained glosses themselves. And here precision falls
to 39%, which is a far less satisfactory figure.
Feasibility of the proposed method in GTT case
Some of the previously mentioned anchorings to
wrong glosses could have been successfully found
by applying the heuristics mentioned in section 3.1.
The use of POS tags and the checking of the singu-
lar form of terms allowed to manually spot 41 ob-
viously wrong results. The other irrelevant glosses
were mainly found using the thesaurus information:
61
Comparison with expert Gold Standard
Concepts with expert translation 25775
Concepts with a definition 2279
Concepts with def. confirmed by GS 1479
Total definitions given 3813
Definitions confirmed by GS 2626
Total translations given 5747
Translations confirmed by GS 1479
Manual evaluation
Concepts 179
Concepts with correct definition 138
Total definitions given 441
Correct definitions 172
Global results
Total GTT concepts 35194
Concepts with a definition 2458
Concepts with correct definition 1617
Total definitions given 4254
Correct definitions 2798
Table 2: GTT term anchoring evaluation
the Broader Term information helped to discriminate
68 cases, compared with 6 for Related Term, 6 for
synonyms and 15 for scope notes.
It is however still uncertain whether these differ-
ent kinds of information can be used in a more au-
tomatised setting. If we could count on translation
of broader and related terms to be done by the pro-
cess we have applied, taking into account scope notes
would require more effort. And the poor structure of
thesauri such as GTT ? some 20000 concepts have no
parents at all ? makes such validations by semantic
links difficult. It is also important to notice that in
14 cases, it was necessary to check the books which
have been indexed by a concept to find out its precise
meaning.
This could yet be compensated by an interesting
result we have observed: the anchoring method gave
us material for inferring new semantic links, as in the
GTAA case. Amongst the translated GTT concepts,
689 concepts are sharing at least one synset and are
not connected by a thesaurus link. We found inter-
esting matches, such as gratie (pardon) and absolutie
(absolution) or between honger (hunger) and dorst
(thirst). This potential for enriching thesauri could
actually be used to spark some positive feedback loop
for the anchoring process itself: a richer vocabulary
enables for example to use with greater profit the se-
lection strategies based on thesaurus structure.
An important problem for the implementation of
such strategies remains to deal with disambiguation
(when several English definitions are found, which
one shall be selected?) in a context of fine-grained
vocabularies. Both GTT and WordNet have a high
level of precision, but they are focused on different
matters. Especially, for a same GTT term the dic-
tionary pointed at several meanings that were very
close, but considered as different synsets in Word-
Net. A typical example is the distinction made be-
tween the gloss attached to moderation and temper-
ance, ?the trait of avoiding excesses?, and the one
attached to moderateness and moderation, ?quality
of being moderate and avoiding extremes?. Look-
ing at the books indexed by the concepts which these
glosses were attached to, it was not clear whether the
indexers systematically considered such a distinction.
Finally, we made rough estimattions of recall ?
the number of concepts that were correctly anchored
compared to the number of concepts anchored in the
ideal case. If we compare the 1479 correctly defined
concepts to the 25775 concepts being given an expert
translation, we find a very disappointing recall rate
of 5.7%. This very low performance is in fact largely
due to three recurrent situations in which the online
dictionary could not give any translation:
? terms containing some special Dutch characters
? especially the so-called Dutch ij, where i and j
make a single character ? and which occurs for
more than 2000 concepts;
? specialized scientific terms, like kwantum-
halleffect;
? complex notions, rendered in Dutch by com-
pound words (e.g. gebruikersinterfaces for user
interfaces), multi words (Algemene kosten for
general costs) or a mixture of the two (Grafis-
che gebruikersinterfaces for graphic user inter-
faces).
Whereas the encoding problem appears fairly sim-
ple, the last ones are more serious ? they were indeed
also encountered in the GTAA case ? and shall be dis-
cussed further.
62
3.3 Conclusion on the anchoring methodology
As just mentioned, a drawback of our anchoring
method is the fact that there are very few multi-
word entries in dictionaries but they compose a large
part of thesauri, and particularly thesauri in Dutch.
Previous work about assigning a semantic relation-
ship between a multi-word term and its components
(see (Ibekwe, 2005)) could be used in order to give
elements of solution to this problem. Using this pre-
processing, we could apply our method to the single-
word part that corresponds to the generic meaning
of the original multi-word term, and try to anchor
the single-word corresponding to the semantic root of
the thesaurus? multi-word term (Kosten for Algemene
kosten ? Cost for General cost ? for instance).
From a more conceptual point of view, however,
further effort would be needed to adapt our anchor-
ing method ? and the subsequent alignment of one
vocabulary with the other ? to the cases where a
concept from one vocabulary should be anchored to
more than one element from WordNet. More com-
plex heuristics come closer to traditional anchoring
problems cases ? without translation ? and could
be solved using existing solutions, as proposed by
(Giunchiglia et al, 2005; Castano et al, 2005).
The last problem encountered in the anchoring
process was the fact that specialized notions, that also
appear in general purpose thesauri, have usually no
definition in a general language dictionary. Special-
ized dictionaries should be used as a complementary
resource.
These different shortcomings reduced the cover-
age of the anchoring, but our method has still posi-
tive points: the number of obviously wrong anchors
was rather low for the found pairs and additional links
could be provided for both of the source thesauri.
This method also provides a starting point for an-
choring complex and large vocabularies to WordNet,
which is also a large lexical resource, and both are
hard to grasp completely by a human expert.
4 GTAA and GTT alignment using WordNet
anchoring: a qualitative evaluation
Once the anchoring is performed, the synsets cor-
responding to the terms from the different thesauri
can be compared, in order to infer from them equi-
valences between the original concepts, as is done
in classical alignment techniques using background
knowledge. In this section, we present some exam-
ples illustrating the kind of alignment results one can
expect from a proper anchoring of our Dutch con-
trolled vocabularies.
First, we can confirm alignments of equal Dutch
labels: gtaa:arbeiders is aligned to gtt:arbeiders
since they are both anchored to the synset ?some-
one who works with their hand, someone engaged
in manual labor?. In some cases, though, a first
stemming or lemmatizing process would have been
needed to achieve alignment, as in the example of
gtaa:bekeringen and gtt:bekering (Conversion, re-
spectively in plural and singular form), or gtaa:biljart
and gtt:biljartspel12 (Billiard and Billiard game).
Nevertheless, the more interesting cases are the
ones involving concepts with large semantic overlap
but a small lexical one, as in the case of gtaa:plant
(Plant) and gtt:begroeiing (Excessive growth of ve-
getation) via the WordNet flora synset. Begroeiing is
actually semantically related in the GTT to the con-
cept Planting. Here, the translation process compen-
sates for the lack of lexical coverage in the respective
vocabularies, which precisely corresponds to one of
the traditional features background knowledge-based
techniques boast. We can also derive general con-
ceptual similarity relationship based on the overlap
between glosses, such as the one between gtaa:drank
and gtt:alcohol, which are not direct matches but for
which our method has found some common glosses
like ?an alcoholic beverage that is distilled rather than
fermented?.
12Notice that substring-based matching could also give these
results, but this method is usually very noisy for alignment pro-
cesses and therefore must be used cautiously.
63
5 Conclusion and perspectives
Our experiments showed that the partial anchoring
of large Dutch controlled vocabularies to WordNet
can be done via a bilingual dictionary, even though
there is an obvious loss in information: not every
thesaurus concept can easily be found in a general
language bilingual dictionary, and a preprocessing of
multi-word and compound thesaurus entries has to be
done. Yet, a significant part of the GTAA thesaurus
could be anchored, and with some improvement to
the method this could be true for GTT too. Besides
multi-word and compound words processing, useful
extensions should also take into account specialized
dictionaries and have a closer look at methodologies
for anchoring a thesaurus term to multiple WordNet
synsets with close meanings. We plan to test such
strategies in future experiments, and hope to obtain a
better coverage of the thesauri.
In this paper, we have sketched a way to use of
these anchorings in a vocabulary alignment scenario,
and underlined the potential gains on test examples.
Even if the number of results given by the current im-
plementation of our method is quite low, the reader
should notice that the process can already, as is,
suggest new relationships between concepts of the
source thesauri. Moreover, proposed strategies in
the alignment field often advocate using combined
methods: combined contributions can be used to pro-
ceed with some cross validation if they overlap, or
to provide with larger number of candidate for fur-
ther (semi-)automatic selection. In such a setting, ev-
ery contribution of candidate links is welcome. In
this respect, what is useful here is the ability of a
WordNet-based method to provide with results that
could not be obtained with other techniques because
of the lack of explicit semantic information and hier-
archical structure in the original vocabularies.
Finally, as mentioned in the introduction, there are
other motivating use cases that we plan to experiment
with. Especially interesting is the way a mapping
withWordNet can enhance the existing access to doc-
ument collections of the Dutch Cultural Heritage In-
stitutes by providing with query refinement services
and browsing possibilities.
Acknowledgements
This research was carried out in the context of the
CATCH projects CHOICE and STITCH, funded by
NWO, the Dutch organization for scientific research.
References
Aleksovski Z. 2006. Matching Unstructured Vocabularies
using a Background Ontology. 15th International Confer-
ence on Knowledge Engineering and Knowledge Manage-
ment (EKAW 2006).
van Assem M., Gangemi A. and Schreiber G. 2006. RDF/OWL
Representation ofWordNet. W3C Working Draft, 19 June
2006. http://www.w3.org/TR/wordnet-rdf/
Budanitsky A. and Hirst G. 2006. EvaluatingWordNet-based
Measures of Lexical Semantic Relatedness, volume 32(1).
Computational Linguistics, 13?47.
Castano S., Ferrara A. and Montanelli S. 2005. Matching On-
tologies in Open Networked Systems: Techniques and Appli-
cations, volume 5. Journal on Data Semantics (JoDS).
Euzenat J., coordinator. 2004. State of the art on ontology align-
ment. KnowledgeWeb Deliverable 2.2.3.
Fellbaum C. 1998. WordNet An Electronic Lexical Database.
MIT Press.
van Gendt M., Isaac A., van der Meij L. and Schlobach S. 2006.
Semantic Web Techniques for Multiple Views on Heteroge-
neous Collections: a Case Study. 10th European Conference
on Research and Advanced Technology for Digital Libraries
(ECDL 2006), 426?437.
Giunchiglia F., Shvaiko P., and Yatskevich M. 2005. Semantic
Schema Matching. 13th International Conference on Cooper-
ative Information Systems (CoopIS 2005).
Hollink L. 2006. Semantic annotation for retrieval of visual
resources. PHD Thesis, Vrije Universiteit Amsterdam.
Ibekwe-SanJuan F. 2005. Clustering semantic relations for con-
structing and maintaining knowledge organization tools. vol-
ume 62 (2). Journal of Documentation, Emerald Publishing
Group, 229?250.
Khan L. R. and Hovy E. 1997. Improving the Precision of
Lexicon-to-Ontology Alignment Algorithm. AMTA/SIG-IL
First Workshop on Interlinguas, San Diego, CA, October 28.
Knight K. and Luk S. 1994. Building a Large-Scale Knowledge
Base for Machine Translation. In Proceedings of the AAAI-
94 Conference.
Voorhees E. 1994. Query expansion using lexical-semantic re-
lations. 17th International ACM/SIGIR Conference on Re-
search and Development in Information Retrieval, 61?69.
64
Detecting semantic relations between terms in definitions
V?ronique MALAIS? 1,2
1 STIM/AP-HP, ERM 202 INSERM
& CRIM-INaLCO
91, boulevard de l?H?pital
75013 Paris,
France,
{vma,pz}@biomath.jussieu.fr
Pierre ZWEIGENBAUM1 Bruno BACHIMONT 2
2DRE de l?INA
Institut National de l?Audiovisuel
4, avenue de l?Europe
94366 Bry-sur-Marne Cedex,
France,
{vmalaise,bbachimont}@ina.fr
Abstract
Terminology structuring aims to elicit semantic re-
lations between the terms of a domain. We propose
here to exploit definitions found in corpora to ob-
tain such semantic relations. Definition typologies
show that definitions can be introduced by differ-
ent semantic relations, some of these relations be-
ing likely to structure terminologies. Our aim is
therefore to mine ?defining expressions? in domain-
specific corpora, and to detect the semantic rela-
tions they involve between their main terms. We
use lexico-syntactic markers and patterns to detect
at the same time both a definition and its main se-
mantic relation. 46 markers and 74 patterns have
been designed and tuned on a first corpus in the field
of anthropology. We report on their evaluation on a
second corpus in the field of dietetics, where they
obtained 4% to 36% recall and from 61 to 66% pre-
cision, and discuss the relative accuracy of different
subclasses of markers for this task.
1 Introduction
A terminology is an artifact structuring terms ac-
cording to some semantic relations. Grabar and
Hamon (2004) present the different semantic rela-
tions likely to be found in terminologies. These
can be divided into lexical (synonymy), vertical
(hypernymy, meronymy) and transversal relations
(domain-specific relations). A study of definition
typologies, like the one of (Auger, 1997), shows
that these different relations are also present in def-
initions. We can then hypothesise that mining defi-
nitions along with the detection of their inherent se-
mantic relation can help to organise terms according
to the relations used in structured terminologies. We
focus in this paper on the detection of terms related
by hypernymy and synonymy in definitions.
The automatic detection of definitions can rely
on different types of existing works. We can, first,
consider the studies describing what definition is,
and more particularly what definition in corpus is
like. In this respect, we can cite the work of Trim-
ble (1985), Flowerdew (1992), Sager (2001) and
Meyer (2001). Another type of interesting exist-
ing work is about typologies of definitions: Mar-
tin (1983), Chukwu and Thoiron (1989) and Auger
(1997), amongst others, provide, in their classifica-
tions of definitions, linguistic clues to find defin-
ing statements in corpus. We propose to integrate
the typologies that we mention in section 2.2, along
with the linguistic clues they give: the definition
markers. And, at last, some works have already fo-
cused on mining definitions from corpora, including
Cartier (1997), Pearson (1996), Rebeyrolle (2000)
and Muresan and Klavans (2002), mostly through
the use of lexical definition markers. These works
provide us with methodological guidelines and an-
other set of lexical markers for our own experiment.
As (Pearson (1996); Rebeyrolle (2000)), our
method is based on lexico-syntactic patterns, so that
we can build on the work on French language by
Rebeyrolle (2000). We extended her work in two
respects: an analysis of the parenthesis as low-level
linguistic clue for definitions, and the concomitant
extraction of the semantic relation involved in a
?defining expression?, along with the extraction of
the definition itself. Previous works have, for in-
stance, mined definitions to find terms specific to a
particular domain of knowledge (Chukwu and Tho-
iron (1989)), and to describe their meaning (Rebey-
rolle, 2000); we focus on the detection of the seman-
tic relations between the main terms of a definition
in order to help a terminologist to build a structured
terminology following these relations.
We implemented an interface to visualise these
definitions and semantic relations extractions. We
tuned markers and patterns for extracting defini-
tions and semantic relations on a first corpus about
anthropology; we then tested the validity of these
markers and patterns on another corpus focused on
dietetics. The purpose of this test was, on the one
hand, to observe whether definitions were still cor-
rectly extracted on the basis of patterns trained on
a corpus differing in the domain of knowledge and
in the genre of documents involved, and, on the
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 55
other hand, to detect if the semantic relation associ-
ated with each pattern was the same as the one ob-
served in the first corpus. The markers and patterns
showed to be comparable to the other experiments
mentioned in terms of definition extraction: the pre-
cision reached from 61 to 66%. As for the seman-
tic relation associated with the patterns, it obtained
different scores, depending on the marker. But, in
most cases, one main semantic relation is associ-
ated with a pattern in the scope of a single domain,
event though a few patterns convey the same rela-
tion across our two corpora.
The remainder of this paper is organised as fol-
lows: we first present previous work (section 2), de-
scribe our method and experiment (section 3), then
present and discuss results (section 4) and conclude
with directions for future work (section 5).
2 Previous work
2.1 Description of definitions in corpus
As a first approach for detecting and extracting
defining statements in corpora, we have to... de-
fine this object. In the literature (Trimble (1985);
Flowerdew (1992),. . . ), three categories of defini-
tions are often mentioned: the formal definition, the
semi-formal and the ?non-formal? one. The formal
definition follows the Aristotelian schema: X = Y +
specific characteristics, where X is the defined term
(the ?definiendum?), ?=? means an equivalence re-
lation, Y stands for the generic class to which X
belongs (the ?Genus?), and specific characteris-
tics detail in which respect X is different from the
other items composing the same generic class. A
semi-formal definition relates the definiendum only
with specific characteristics, or with its attribute(s)
(Meyer, 2001). Formal and semi-formal definitions
can be of simple type (expressed in one sentence),
or complex (expressed in two, or more sentences).
A non-formal definition aims ?to define in a gen-
eral sense so that a reader can see the familiar el-
ement in whatever the new term may be? (Trimble,
1985). It can be an association with a synonym, a
paraphrase or grammatical derivation.
The common point between all these points of
views on the same linguistic object, or between all
these different objects sharing the same appellation
?definition?, is that they all follow the same didac-
tic purpose of disambiguating the meaning of a lex-
ical item, that is to distinguish it from the others in
the general language, or inside a specific vocabu-
lary. These definition descriptions present them as
the association between a term and its hypernym (its
?genus?), or between a term and its specific charac-
teristics. But there are yet other ways to express
definitions, as the works on their typology shows.
2.2 Typology of definitions
Existing definitions typologies are all dedicated to
a specific purpose. We are particularly interested in
those which aim at eliciting linguistic clues that can
be used to mine defining contexts from corpora. We
work on French, for which Martin (1983) has classi-
fied dictionary definitions in order to give guidelines
for a consistent (electronic) dictionary. In the con-
text of corpus-based research, Chukwu and Thoiron
(1989) gave another classification, aiming at finding
domain-specific terms in corpora. A unified typol-
ogy is provided by Auger (1997), compiling both
cited typologies along with three others, and from
which we draw the following three categories:
? Definitions expressed by ?low level? linguistic
markers: punctuation clues such as parenthe-
sis, quote, dash, colon;
? Definitions expressed by lexical markers: lin-
guistic or metalinguistic lexical items;
? Definitions expressed by ?high level? linguistic
markers: syntactic patterns such as anaphora or
apposition.
The definitions introduced by lexical means are di-
vided in two branches, characterised by the lexi-
cal markers in table 1. We added elements from
other studies ((Rebeyrolle, 2000) and (Fuchs, 1994)
amongst others), and augmented this typology with
Definitions introduced by linguistic markers
Copulative ?a X is a Y that?
Equivalence ?equivalent to?
Characterisation ?attribute of?, ?qual-
ity?,. . .
Analysis ?composed of?, ?equipped
with?, ?made of?,. . .
Function ?to have the function?, ?the
role of?, ?to use X to do
Y?,. . .
Causality ?to cause X by Y?, ?to ob-
tain X by?,. . .
Definitions introduced by metalinguistic mark-
ers
Designation ?to designate?, ?to
mean?,. . .
Denomination ?to name?
Systemic ?to write?, ?to spell?, ?the
noun?,. . .
Table 1: Lexical markers (English translation)
CompuTerm 2004  -  3rd International Workshop on Computational Terminology56
new markers, including some items introducing re-
formulation contexts (?that is?, ?to say?, ?for in-
stance?, . . . ).
The Aristotelian definition type is presented here
as a ?copulative? definition, as it is linguistically
marked by the copula ??tre? (to be). It involves a
hypernymic relation (and specific differences) to de-
scribe the meaning of a term, so we consider it as a
?hypernymic definition?. But we can see in table 1
that other semantic relations can also be used to de-
fine a term: synonymy (definition of ?equivalence?
type), meronymy (?analysis? type), causality and
other domain-specific transversal relations (?func-
tion?, ?characterisation? types). Mining a defini-
tion of ?synonymic type? provides different denom-
inations for the same concept; one of ?hypernymy
type? can help modelling the vertical structure be-
tween the ?definiendum? and the first term of the
?definiens? (conceptual ?father? and ?son? associa-
tion); and definitions following transversal relations
allow the expression of specific knowledge. We fo-
cus in this paper on the extraction of definitions in-
volving hypernymy and synonymy, which are the
most generally considered relations in terminology
building.
2.3 Automatic definition mining
Automatic definition mining from corpora can
be divided in different groups, according to the
methodologies followed. We will illustrate them by
describing three recent families of works: (i) Cartier
(1997), (ii) Pearson (1996) and Rebeyrolle (2000),
(iii) Muresan and Klavans (2002). They have
used respectively ?contextual exploration?, lexico-
syntactic patterns and linguistic analysis and rules.
The former one extracts defining statements on
the basis of the match of linguistic clues, when they
are relayed in the sentence by some linguistic rules.
These rules are developped by the author, withing
the schema defined in the ?contextual exploration?
methodology (Descl?s, 1996).
Pearson (1996) and Rebeyrolle (2000) have fol-
lowed the methodology described by Hearst (1992),
up to now mainly applied to discover hyponymous
terms. It consists in describing the lexico-syntactic
context of an occurrence of a pair of terms known to
share a semantic relation. Modelling the context in
which they occur provides a ?pattern? to apply to the
corpus, in order to extract other pairs of terms con-
nected by the same relation. Pearson and Rebeyrolle
have modelled lexico-syntactic contexts around lex-
ical clues interpreted as ?definition markers?. Re-
beyrolle, working on French, evaluated the different
pattern types she modelled, across different corpora:
she obtained a precision range of 17.95 ? 79.19%,
and a recall of 94.75 ? 100%. The difference be-
tween the two numeric boundaries of the precision
range is due to the kind of markers involved in
the lexico-syntactic pattern evaluated: metalinguis-
tic markers obtained a high precision rate, but not
linguistic lexical markers.
The latter pair of authors have based their system
DEFINDER (http://www1.cs.columbia.
edu/~smara/DEFINDER/) on the lexical and
syntactic analysis of a medical corpus, with semi-
automatic definition acquisition. Their evaluation
is focused on the usefulness of the system, as
compared with existing specialised medical dictio-
naries. They reach a 86.95% precision and 75.47%
recall, following their evaluation methodology.
We chose to follow the first methodology in our
experiment (see section 3), in which we additionally
explore definition mining in some cases where the
definition is not introduced by lexical items. Fol-
lowing this methodology enables us to build on ex-
isting work dedicated to French, which showed to
be interesting and efficient. The lexico-syntactic
pattern methodology also enables us to access the
different linguistic elements we were interested in
mining: the definition itself, the main terms of the
definition and the semantic relation between them.
We focus this experimentation more particularly
on identifying the semantic relations of synonymy
and hypernymy involved in the different definitions
likely to be found in corpora. We aim at testing
whether a stable link can be established between the
definition extraction pattern and a specific semantic
relation.
3 Detecting Semantic Relations
Our goal is to automatically detect some of the se-
mantic relations that might be found in definitions
and to propose them to a human validator in charge
of structuring a terminology. We focus on hyper-
nymy and synonymy, which are the most classical
relations found in terminology. If the relation is
hypernymy, the terms are to be modelled in a hi-
erarchical way, if it is synonymy, both terms can
be used to express the same concept. The rela-
tions and the definitions are extracted together from
corpora, by the same lexico-syntactic patterns. We
present in the next subsections our two corpora (sec-
tion 3.1), then the lexico-syntactic patterns we used
(section 3.2) and their experimental evaluation (sec-
tion 3.3): we analyse whether a relation found in
connection with a lexico-syntactic pattern in the
training corpus can be unchanged in the context of
the same lexico-syntactic pattern, when applied to a
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 57
different corpus.
3.1 Description and preparation of the corpora
Our training corpus (76 Kwords) is focused on
childhood, from the point of view of anthropolo-
gists. It is composed of different genres of docu-
ments (documentary descriptions, thesis report ex-
tracts, Web documents). Documentary descriptions
were humanly collected, whereas electronic doc-
uments were automatically collected from Inter-
net via the tools of (Grabar and Berland, 2001).
Our evaluation corpus (480 Kwords), in the do-
main of dietetics, is composed of Web documents
indexed by the CISMeF quality-controlled cata-
log of French medical Web sites (http://www.
chu-rouen.fr/cismef/) in the subtrees ?Di-
etetics? and ?Nutrition? of the MeSH thesaurus.
It is mainly composed of medical courses and
Web pages presenting information about nutri-
tion in different medical contexts. Both cor-
pora were morpho-syntactically analysed by Cor-
dial Analyser (Synapse Developpement, http://
www.synapse-fr.com/). Cordial tags, lem-
matises and parses a corpus, yielding grammatical
functions (subject, object, . . . ) between chunks.
3.2 Lexico-syntactic patterns
A given linguistic marker (see, e.g., table 1) can oc-
cur in different contexts, some of which are defini-
tions, and can be a clue for different semantic rela-
tions. Lexico-syntactic patterns aim at reducing this
ambiguity by specifying more restricted contexts in
which a definition is found, and, furthermore, in
which one specific semantic relation is involved.
Unlike (Hearst, 1992), we started the pattern de-
sign by analysing marker occurrences in our train-
ing corpus. We designed and tuned our lexico-
syntactic patterns on this corpus, patterns dedicated
to the extraction of definitions and specific relations:
hypernymy and synonymy. Our patterns use the in-
formation output by the parser, including lemma,
morpho-syntactic category and grammatical func-
tion. For instance: ?N (N)? specifies that the marker
?(? has to be preceded by a noun, and immediately
followed by a single common noun, followed by a
closing parenthesis. In this specific case, ?(? intro-
duces a hypernymic definition.
Each pattern drives different kinds of processing:
? extraction of the defining sentence on the basis
of the whole pattern;
? selection of one ?preferred? relation associated
with the specific pattern, among the set of pos-
sible relations associated with the marker; this
relation stands between the interdefined terms
of the definition;
? extraction of the interdefined terms following
two strategies (contextual or based on depen-
dencies around the marker), depending on the
morphosyntactic category of the marker. When
the marker is a punctuation or a noun, we usu-
ally extract its left and right syntactic contexts1
(roughly the first chunk before the marker, and
the first chunk after the marker in the sen-
tence). When the marker is a verb, we extract
its subject and object if they exist in the sen-
tence, otherwise we extract its left and right
chunks, as in the previous case.
Our patterns are implemented in XSLT and the re-
sulting extractions are shown to a human validator
through a Web interface (figure 1): an HTML form
allowing the validator to complete and correct the
extractions. It is possible for the validator to correct
the terms extracted from the definition, in particu-
lar because the chunk often includes punctuation,
which is usually not considered as part of the term,
and it is possible to select a different semantic rela-
tion than the one proposed when it happens not to
be the correct one. A combo box shows all the pos-
sible relations related to the marker involved in the
lexico-syntactic pattern which provided the extrac-
tion of the defining sentence.
3.3 Experimental setup
We tuned our lexico-syntactic patterns to extract
definitions from the test corpus. We associated
with each pattern a ?preferential? semantic relation,
which human corpus analysis showed to be the more
likely to be connected to the definitions extracted by
the means of this pattern. The aim of the experiment
is to test the stability of this connection, by applying
the patterns to the evaluation corpus.
A random sample of the test corpus (13 texts
among 132) was manually processed to tag its def-
initions, in order to have a standard measure for
the evaluation of recall. Table 2 shows the number
of definitions of synonymic and hypernymic types
found in that sample, and provides the percentages
of these definitions among all the different kinds of
tagged definitions (?% definitions?) in that sample.
Some definitions involved more than one semantic
relation, so we also present the percentage of hyper-
nymic and synonymic relations among all the se-
mantic relations (?% relations?).
1Depending on the position of the marker in the sentence, it
might be the two following or two preceding chunks.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology58
Figure 1: Human validation interface for definitions extracted with the parenthesis marker
Hypernymy Synonymy
# definitions 90 22
% definitions 44, 5% 10, 8%
% relations 39, 1% 9, 5%
Table 2: Number and percentages of hypernymic
and synonymic definitions in a random sample of
the test corpus, according to the human evaluator
In our experiment, we evaluate in turn the quality
of the extracted definitions, then that of semantic
relations (hypernymy and synonymy).
4 Results and discussion
Table 3 shows the number of markers and patterns
prepared and tuned on the training corpus to extract
definitions based on hypernymy or synonymy. Note
that a given marker can be used in different patterns
to extract different semantic relations. Some mark-
ers were also associated in one pattern: the met-
alinguistic nouns and verbs. We combined them
because their individual recall was not lowered by
this association and their precision score was im-
proved. The sentences below are examples of sen-
Hypernymy Synonymy
# markers 3 43
# patterns 4 70
Table 3: Number of markers and patterns
tences extracted by our system; the underlined part
is the marker:
? Hypernymic relation:
?Les acides gras de la s?rie omega-3 ( MAX-
epa ) peuvent ?galement ?tre prescrits .?,
?[. . . ]les fromages ? p?te cuite ( tels que
par exemple le fromage de Hollande ).?
? Synonymic relation:
?L? activit? physique est d?finie comme tout
mouvement corporel produit par la contraction
des muscles squelettiques ,[. . . ]?,
?une relation inverse entre l? activit? physique
et l? insulin?mie ou la sensibilit? ? l? insuline
est habituellement observ?e .?
Table 4 presents the evaluation results: we divide
them according to the semantic relation extracted.
It shows the number of definitions retrieved, and the
associated precision and recall. Precision is divided
in two measures.
Hypernymy Synonymy
# extracted
sentences
270 585
Precision (def) 61% 66%
Precision (rel) 26% 15%
Recall (rel) 4% 36%
Table 4: Evaluation of precision (test corpus) and
recall (random sample of test corpus)
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 59
? the proportion of extracted sentences that cor-
responded to definitions (def ), and
? the proportion of correct semantic relations
found in retrieved definitions (rel).
Recall is the proportion of retrieved definitions
which correctly display the semantic relation iden-
tified in the sample corpus among all the definitions
present in this sample which were tagged as having
this semantic relation by the human evaluator.2
The precision of extracted definitions is compara-
ble to Rebeyrolle?s results. The precision of seman-
tic relations is much lower, but a global evaluation
does not show the particular behavior of some of the
markers. We list below the markers which were ac-
tually involved in the extraction of definitions in the
test corpus.
? Markers implied in hypernymic definition re-
trieval: ?parenth?se? (parenthesis), ?par ex-
emple? (for instance), ?sorte de? (a kind of);
? Markers implied in synonymic definition re-
trieval: ?parenth?se? (parenthesis), ?il s?agit
de? (as for), ?indiquer? (to indicate), ?soit?
(that is), ?expliquer? (to explain), ?pr?ciser?
(to specify), ?marquer? (to mark), ?enfin?
(say), ?ou? (or), ?comme? (as), ?? savoir?
(that is), ?autrement dit? (in other words), ?au
sens de? (meaning), ??quivaloir? (to be equiv-
alent), ?c?est-?-dire? (that is), ?d?finir? (to
define), ?d?signer? (to designate), ?nommer?
(to name), ?d?nommer? (to name), ?r?f?rer?
(to refer), ?expression? (expression), ?terme?
(term).
Table 5 presents the different semantic relations
found in the definitions retrieved by each marker.
The first column references the markers involved in
the extraction of the definition, the second (?Ex-
pected?) presents the number of definitions, ex-
tracted by each marker, following the expected re-
lation. ?Other? gives the number of retrieved def-
initions following another semantic relation, ?Un-
decidable? represents the number of definitions for
which we could not determine the semantic rela-
tion,3 and ?Non definition? presents the number of
retrieved sentences that were not definitions.4
2The percentage of definitions of hypernymic and syn-
onymic type among all definitions in the sample of the test cor-
pus is given in table 2.
3Because our system extracts only one sentence, and a
larger context was necessary to understand the semantic rela-
tion involved, or because of a problem in the conversion of
some HTML documents to texts for the evaluation corpus.
4Except sentences presenting terms in a paradigm context,
which is also interesting for terminology structuring. We in-
Definitions retrieved with the hypernymy patterns
involved very generic markers, and they introduced
a number of other semantic relations. The pattern
around ?for instance?, for which 16 extracted sen-
tences out of 95 were not definitions, can still be
specified to discriminate defining contexts from oth-
ers. We can notice, though, that it is one of the
most productive patterns (95 extractions) and that it
reaches a 47, 3% precision. But the patterns around
the parenthesis show that the same syntactic con-
text can introduce different kinds of relations: in
this case, the lexico-syntactic pattern cannot disam-
biguate the relation any further. The pattern ?N
(N)? introduced ?hypernymic definitions?, as well
as ?synonymic? or ?meronymic? ones, the same
syntactic context being even likely to be interpreted
as a transversal relation between a treatment and a
disease, for instance. It is the sentence as a whole
that has to be interpreted in order to be able to define
the relevant semantic relation between the terms in
that syntactic context.
Some linguistic markers (as ?comme?) are re-
liable for detecting a semantic relation: 9 sen-
tences out of 13 were ?synonymic definitions?.
But surprisingly enough, some metalinguistic verbs
(?d?finir?, for instance) were not as effective as
them in that purpose. ?D?finir? introduced only
22 ?synonymic definitions? out of 68 sentences re-
trieved. One could think that a verb with metalin-
guistic function could be less polysemic than an-
other of more ?generic purpose?. This naive hope
happens to be wrong: ?D?finir? means ?to fix
(a limit)? as often as ?to define?. Some markers
steadily introduced a semantic relation, but not the
one they were supposed to: this variation is prob-
ably due to the change in domains across our two
corpora. And some patterns obviously introduced a
definition, but the defined element was in the previ-
ous sentence (this is the case of 92 extractions with
patterns involving the marker ?Il s?agit de?). As our
system, up to now, extracts only one sentence, we
could not determine whether the semantic relation
was the one expected. We must address this prob-
lem, and we can hope that the precision rate will
then be better than the one presented here: some
sentences for which we could not interpret the se-
mantic relation might convey the one we expected.
The best precision score is reached by patterns in-
volving two markers: a metalinguistic noun associ-
ated with a metalinguistic verb. In a more general
way, analysing the defining sentences extracted, we
could see that sentences that were the ?best? defi-
nitions (the closest to dictionary definitions) often
cluded this paradigm context in the ?Other? column.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology60
Marker Expected Other Undecidable Non definition Total
Parenthesis (Parenth?se) Hyp: 25 Meronymy: 1,
Syn-
onymy: 38 (+3),
Transversal: 7
4 84 163
For instance (Par exemple) Hyp: 45 Transversal: 2 32 16 95
A kind of (Une sorte de) Hyp: 1 Transversal: 2 5 5 13
Parenthesis (Parenth?se) Syn: 10 Paradigm: 9 2 4 25
As for (Il s?agit de) Syn: 10 Transversal: 4,
Hypernymy: 1
92 9 115
To indicate (Indiquer) Syn: 5 Transversal: 12 6 77 100
That is (Soit) Syn: 7 Paradigm: 31,
Transversal: 13
15 1 66
To explain (Expliquer) Syn: 1 Transversal: 21 15 28 65
To specify (Sp?cifier) Syn:1 Transversal: 5 9 26 41
To mark (Marquer) Syn: 1 Transversal: 7 6 12 26
Say (Enfin) Syn: 0 Paradigm: 3 2 1 6
Or (Ou) Syn: 3 Paradigm: 23 1 0 27
As (Comme) Syn: 9 Paradigm: 1 1 2 13
That is (A savoir) Syn: 4 Hypernymy: 3 5 0 12
In other words (Autrement dit) Syn: 1 0 2 0 3
Equivalent to (?quivaloir) Syn: 0 0 4 0 4
To define (D?finir) Syn: 22 Transversal: 8 19 19 68
To designate (D?signer) Syn: 3 Hypernymy: 0 0 0 3
Term (Terme) Syn: 1 0 1 0 2
Meaning (Au sens de) Syn: 0 0 1 0 1
That is (C?est-?-dire) Syn: 1 0 0 0 1
To name (Nommer) Syn: 0 0 2 1 3
To name (D?nommer) Syn: 1 0 0 0 1
To refer to (R?f?rer) Syn: 0 0 0 2 2
Expression (Expression) Syn: 0 1 1 0 2
Table 5: Semantic relations in retrieved definitions
involved two or even three markers. This underlines
the interest of introducing a relevance measure that
takes into account the number of markers present in
the sentence.
5 Conclusions
Our experiment tried to link the semantic relation
inherent to different kinds of definitions with the
marker (the heart of our lexico-syntactic patterns)
and more specifically with the lexico-syntactic pat-
terns at the origin of the extraction of the definition
itself. Having a close look at some of the mark-
ers, we can observe that some linguistic items can
be very reliable markers for definition extraction as-
sociated with a semantic relation. We can also find
out that the polysemy of some markers is related
to the domain of the corpus. In that respect, the
reusability of the lexico-syntactic patterns is limited
to a set of markers which were found to be reli-
able across our two corpora. What is more prob-
lematic is the fact that it is sometimes not possible
to make a specific distinction between different se-
mantic relations detected with the same marker in
the context of definitions sharing most of their syn-
tactic contexts. But most of the patterns retrieve a
good rate of defining sentences, some patterns be-
ing more reliable than others; and the more numer-
ous the markers involved, the more likely it is that
we have a definition. And usually these patterns re-
trieve definitions following one main semantic rela-
tion (this is not the case however for parenthesis and
the patterns involving the marker ?? savoir?). This
leads to the hypothesis that if lexico-syntactic pat-
terns may not be used to propose semantic relations
that are valid across different domains, they remain
a good clue for mining definitions, especially defini-
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 61
tions of one type of semantic relation inside a given
domain. Moreover, given a new corpus, applying
the existing patterns to a sub-corpus could lead to
the elicitation of the associated semantic relations
for that corpus, which could be a relevant method-
ology to discover pairs of terms following these as-
sociated relations.
References
A. Auger. 1997. Rep?rage des ?nonc?s d?int?r?t
d?finitoire dans les bases de donn?es textuelles.
Th?se de doctorat, Universit? de Neuch?tel.
E. Cartier. 1997. La d?finition dans les textes
scientifiques et techniques : pr?sentation d?un
outil d?extraction automatique de relations d?fini-
toires. 2e Rencontres "Terminologie et In-
telligence Artificielle" (TIA?97), Equipe de
Recherche en Syntaxe et S?mantique. Toulouse,
3-4 avril 1997:127?140.
U. Chukwu and P. Thoiron. 1989. Reformulation
et rep?rage des termes. La Banque des Mots,
Num?ro sp?cial CTN - INaLF - CNRS:23?53.
J.-P. Descl?s. 1996. Syst?mes d?exploration con-
textuelle. Table ronde sur le Contexte, avril 1996,
Caen.
J. Flowerdew. 1992. Definitions in science lectures.
Linguistics, vol.13 (2):202?221.
C. Fuchs. 1994. Paraphrase et ?nonciation. Paris,
Ophrys.
N. Grabar and S. Berland. 2001. Construire un
corpus web pour l?acquisition terminologique.
4e rencontres Terminologie et Intelligence Arti-
ficielle (TIA 2001), Nancy:44?54.
N. Grabar and T. Hamon. 2004. Les relations dans
les terminologies structur?es : de la th?orie ? la
pratique. Revue d?Intelligence Artificielle (RIA),
18-1:57?85.
M. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. 15th Interna-
tional Conference on Computational Linguistics
(COLING 1992), Nantes:539?545.
R. Martin. 1983. Pour une logique du sens. Paris,
PUF.
I. Meyer. 2001. Extracting knowledge-rich con-
texts for terminography. In D. Bourigault, edi-
tor, Recent advances in Computational Terminol-
ogy, pages 279?302. John Benjamins Publishing
Company, Philadelphia, PA.
S. Muresan and J. L. Klavans. 2002. A method
for automatically building and evaluating dic-
tionary resources. the language Resources and
Evaluation Conference (LREC 2002), Las Pal-
mas, Spain:231?234.
J. Pearson. 1996. The expression of definitions
in specialised texts: a corpus-based analysis.
In M. Gellerstam, J. J?rborg, S. G. Malmgren,
K. Nor?n, L.Rogstr?m, and C. Papmehl, edi-
tors, 7th International Congress on Lexicography
(EURALEX?96), pages 817?824. G?teborg Uni-
versity, G?teborg, Sweden.
J. Rebeyrolle. 2000. Forme et fonction de la d?fi-
nition en discours. Th?se de doctorat, Universit?
de Toulouse II - Le Mirail.
J. C. Sager. 2001. Essays on Definition. John Ben-
jamins, Amsterdam.
L. Trimble. 1985. English for Science and Technol-
ogy: A Discourse Approach. Cambridge Univer-
sity Press, Cambridge.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology62
