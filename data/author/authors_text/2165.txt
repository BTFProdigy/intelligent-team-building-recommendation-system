Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 228?237, Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Supervised Classification for Extracting Protein Interaction Sentences
using Dependency Parsing
Gu?nes? Erkan
University of Michigan
gerkan@umich.edu
Arzucan ?Ozgu?r
University of Michigan
ozgur@umich.edu
Dragomir R. Radev
University of Michigan
radev@umich.edu
Abstract
We introduce a relation extraction method to
identify the sentences in biomedical text that
indicate an interaction among the protein
names mentioned. Our approach is based on
the analysis of the paths between two protein
names in the dependency parse trees of the
sentences. Given two dependency trees, we
define two separate similarity functions (ker-
nels) based on cosine similarity and edit dis-
tance among the paths between the protein
names. Using these similarity functions, we
investigate the performances of two classes
of learning algorithms, Support Vector Ma-
chines and k-nearest-neighbor, and the semi-
supervised counterparts of these algorithms,
transductive SVMs and harmonic functions,
respectively. Significant improvement over
the previous results in the literature is re-
ported as well as a new benchmark dataset
is introduced. Semi-supervised algorithms
perform better than their supervised ver-
sion by a wide margin especially when the
amount of labeled data is limited.
1 Introduction
Protein-protein interactions play an important role
in vital biological processes such as metabolic and
signaling pathways, cell cycle control, and DNA
replication and transcription (Phizicky and Fields,
1995). A number of (mostly manually curated)
databases such as MINT (Zanzoni et al, 2002),
BIND (Bader et al, 2003), and SwissProt (Bairoch
and Apweiler, 2000) have been created to store pro-
tein interaction information in structured and stan-
dard formats. However, the amount of biomedical
literature regarding protein interactions is increas-
ing rapidly and it is difficult for interaction database
curators to detect and curate protein interaction in-
formation manually. Thus, most of the protein in-
teraction information remains hidden in the text of
the papers in the biomedical literature. Therefore,
the development of information extraction and text
mining techniques for automatic extraction of pro-
tein interaction information from free texts has be-
come an important research area.
In this paper, we introduce an information extrac-
tion approach to identify sentences in text that in-
dicate an interaction relation between two proteins.
Our method is different than most of the previous
studies (see Section 2) on this problem in two as-
pects: First, we generate the dependency parses of
the sentences that we analyze, making use of the
dependency relationships among the words. This
enables us to make more syntax-aware inferences
about the roles of the proteins in a sentence com-
pared to the classical pattern-matching information
extraction methods. Second, we investigate semi-
supervised machine learning methods on top of the
dependency features we generate. Although there
have been a number of learning-based studies in this
domain, our methods are the first semi-supervised
efforts to our knowledge. The high cost of label-
ing free text for this problem makes semi-supervised
methods particularly valuable.
We focus on two semi-supervised learning meth-
ods: transductive SVMs (TSVM) (Joachims, 1999),
228
and harmonic functions (Zhu et al, 2003). We also
compare these two methods with their supervised
counterparts, namely SVMs and k-nearest neigh-
bor algorithm. Because of the nature of these al-
gorithms, we propose two similarity functions (ker-
nels in SVM terminology) among the instances of
the learning problem. The instances in this problem
are natural language sentences with protein names in
them, and the similarity functions are defined on the
positions of the protein names in the corresponding
parse trees. Our motivating assumption is that the
path between two protein names in a dependency
tree is a good description of the semantic relation
between them in the corresponding sentence. We
consider two similarity functions; one based on the
cosine similarity and the other based on the edit dis-
tance among such paths.
2 Related Work
There have been many approaches to extract pro-
tein interactions from free text. One of them is
based on matching pre-specified patterns and rules
(Blaschke et al, 1999; Ono et al, 2001). How-
ever, complex cases that are not covered by the
pre-defined patterns and rules cannot be extracted
by these methods. Huang et al (2004) proposed a
method where patterns are discovered automatically
from a set of sentences by dynamic programming.
Bunescu et al (2005) have studied the performance
of rule learning algorithms. They propose two meth-
ods for protein interaction extraction. One is based
on the rule learning method Rapier and the other
on longest common subsequences. They show that
these methods outperform hand-written rules.
Another class of approaches is using more syntax-
aware natural language processing (NLP) tech-
niques. Both full and partial (shallow) parsing
strategies have been applied in the literature. In
partial parsing the sentence structure is decomposed
partially and local dependencies between certain
phrasal components are extracted. An example of
the application of this method is relational parsing
for the inhibition relation (Pustejovsky et al, 2002).
In full parsing, however, the full sentence structure
is taken into account. Temkin and Gilder (2003)
used a full parser with a lexical analyzer and a con-
text free grammar (CFG) to extract protein-protein
interaction from text. Another study that uses full-
sentence parsing to extract human protein interac-
tions is (Daraselia et al, 2004). Alternatively,
Yakushiji et al (2005) propose a system based on
head-driven phrase structure grammar (HPSG). In
their system protein interaction expressions are pre-
sented as predicate argument structure patterns from
the HPSG parser. These parsing approaches con-
sider only syntactic properties of the sentences and
do not take into account semantic properties. Thus,
although they are complicated and require many re-
sources, their performance is not satisfactory.
Machine learning techniques for extracting pro-
tein interaction information have gained interest in
the recent years. The PreBIND system uses SVM to
identify the existence of protein interactions in ab-
stracts and uses this type of information to enhance
manual expert reviewing for the BIND database
(Donaldson et al, 2003). Words and word bigrams
are used as binary features. This system is also
tested with the Naive Bayes classifier, but SVM is
reported to perform better. Mitsumori et al (2006)
also use SVM to extract protein-protein interac-
tions. They use bag-of-words features, specifically
the words around the protein names. These sys-
tems do not use any syntactic or semantic informa-
tion. Sugiyama et al (2003) extract features from
the sentences based on the verbs and nouns in the
sentences such as the verbal forms, and the part of
speech tags of the 20 words surrounding the verb
(10 before and 10 after it). Further features are used
to indicate whether a noun is found, as well as the
part of speech tags for the 20 words surrounding
the noun, and whether the noun contains numeri-
cal characters, non-alpha characters, or uppercase
letters. They construct k-nearest neighbor, decision
tree, neural network, and SVM classifiers by using
these features. They report that the SVM classifier
performs the best. They use part-of-speech informa-
tion, but do not consider any dependency or seman-
tic information.
The paper is organized as follows. In Section 3 we
describe our method of extracting features from the
dependency parse trees of the sentences and defin-
ing the similarity between two sentences. In Section
4 we discuss our supervised and semi-supervised
methods. In Section 5 we describe the data sets and
evaluation metrics that we used, and present our re-
229
sults. We conclude in Section 6.
3 Sentence Similarity Based on
Dependency Parsing
In order to apply the semi-supervised harmonic
functions and its supervised counterpart kNN, and
the kernel based TSVM and SVM methods, we need
to define a similarity measure between two sen-
tences. For this purpose, we use the dependency
parse trees of the sentences. Unlike a syntactic parse
(which describes the syntactic constituent structure
of a sentence), the dependency parse of a sentence
captures the semantic predicate-argument relation-
ships among its words. The idea of using depen-
dency parse trees for relation extraction in general
was studied by Bunescu and Mooney (2005a). To
extract the relationship between two entities, they
design a kernel function that uses the shortest path in
the dependency tree between them. The motivation
is based on the observation that the shortest path be-
tween the entities usually captures the necessary in-
formation to identify their relationship. They show
that their approach outperforms the dependency tree
kernel of Culotta and Sorensen (2004), which is
based on the subtree that contains the two entities.
We adapt the idea of Bunescu and Mooney (2005a)
to the task of identifying protein-protein interaction
sentences. We define the similarity between two
sentences based on the paths between two proteins
in the dependency parse trees of the sentences.
In this study we assume that the protein names
have already been annotated and focus instead on
the task of extracting protein-protein interaction sen-
tences for a given protein pair. We parse the sen-
tences with the Stanford Parser1 (de Marneffe et al,
2006). From the dependency parse trees of each sen-
tence we extract the shortest path between a protein
pair.
For example, Figure 1 shows the dependency tree
we got for the sentence ?The results demonstrated
that KaiC interacts rhythmically with KaiA, KaiB,
and SasA.? This example sentence illustrates that
the dependency path between a protein pair captures
the relevant information regarding the relationship
between the proteins better compared to using the
words in the unparsed sentence. Consider the pro-
1http://nlp.stanford.edu/software/lex-parser.shtml
tein pair KaiC and SasA. The words in the sentence
between these proteins are interacts, rhythmically,
with, KaiA, KaiB, and and. Among these words
rhythmically, KaiA, and and KaiB are not directly
related to the interaction relationship between KaiC
and SasA. On the other hand, the words in the depen-
dency path between this protein pair give sufficient
information to identify their relationship.
In this sentence we have four proteins (KaiC,
KaiA, KaiB, and SasA). So there are six pairs of
proteins for which a sentence may or may not be de-
scribing an interaction. The following are the paths
between the six protein pairs. In this example there
is a single path between each protein pair. However,
there may be more than one paths between a pro-
tein pair, if one or both appear multiple times in the
sentence. In such cases, we select the shortest paths
between the protein pairs.
ccomp
prep_with
results interacts
The
KaiA KaiB
rhytmically SasAthat KaiC
demonstrated
nsubj
complm nsubj
advmod
conj_and conj_and
det
Figure 1: The dependency tree of the sentence ?The
results demonstrated that KaiC interacts rhythmi-
cally with KaiA, KaiB, and SasA.?
1. KaiC - nsubj - interacts - prep with - SasA
2. KaiC - nsubj - interacts - prep with - SasA - conj and -
KaiA
3. KaiC - nsubj - interacts - prep with ? SasA - conj and -
KaiB
4. SasA - conj and - KaiA
5. SasA - conj and - KaiB
6. KaiA ? conj and ? SasA - conj and - KaiB
If a sentence contains n different proteins, there
are
(n
2
)
different pairs of proteins. We use machine
learning approaches to classify each sentence as an
interaction sentence or not for a protein pair. A sen-
tence may be an interaction sentence for one protein
230
pair, while not for another protein pair. For instance,
our example sentence is a positive interaction sen-
tence for the KaiC and SasA protein pair. However,
it is a negative interaction sentence for the KaiA and
SasA protein pair, i.e., it does not describe an inter-
action between this pair of proteins. Thus, before
parsing a sentence, we make multiple copies of it,
one for each protein pair. To reduce data sparseness,
we rename the proteins in the pair as PROTX1 and
PROTX2, and all the other proteins in the sentence
as PROTX0. So, for our example sentence we have
the following instances in the training set:
1. PROTX1 - nsubj - interacts - prep with - PROTX2
2. PROTX1 - nsubj - interacts - prep with - PROTX0 -
conj and - PROTX2
3. PROTX1 - nsubj - interacts - prep with ? PROTX0 -
conj and - PROTX2
4. PROTX1 - conj and - PROTX2
5. PROTX1 - conj and - PROTX2
6. PROTX1 ? conj and ? PROTX0 - conj and - PROTX2
The first three instances are positive as they describe
an interaction between PROTX1 and PROTX2. The
last three are negative, as they do not describe an
interaction between PROTX1 and PROTX2.
We define the similarity between two instances
based on cosine similarity and edit distance based
similarity between the paths in the instances.
3.1 Cosine Similarity
Suppose pi and pj are the paths between PROTX1
and PROTX2 in instance xi and instance xj , respec-
tively. We represent pi and pj as vectors of term
frequencies in the vector-space model. The cosine
similarity measure is the cosine of the angle between
these two vectors and is calculated as follows:
cos sim(pi, pj) = cos(pi,pj) =
pi ? pj
?pi??pj?
(1)
that is, it is the dot product of pi and pj divided by
the lengths of pi and pj. The cosine similarity mea-
sure takes values in the range [0, 1]. If all the terms
in pi and pj are common, then it takes the maximum
value of 1. If none of the terms are common, then it
takes the minimum value of 0.
3.2 Similarity Based on Edit Distance
A shortcoming of cosine similarity is that it only
takes into account the common terms, but does not
consider their order in the path. For this reason, we
also use a similarity measure based on edit distance
(also called Levenshtein distance). Edit distance be-
tween two strings is the minimum number of op-
erations that have to be performed to transform the
first string to the second. In the original character-
based edit distance there are three types of opera-
tions. These are insertion, deletion, or substitution
of a single character. We modify the character-based
edit distance into a word-based one, where the oper-
ations are defined as insertion, deletion, or substitu-
tion of a single word.
The edit distance between path 1 and path 2 of
our example sentence is 2. We insert PROTX0 and
conj and to path 1 to convert it to path 2.
1. PROTX1 - nsubj - interacts - prep with - insert (PROTX0)
- insert (conj and) ? PROTX2
2. PROTX1 - nsubj - interacts - prep with - PROTX0 -
conj and - PROTX2
We normalize edit distance by dividing it by the
length (number of words) of the longer path, so that
it takes values in the range [0, 1]. We convert the dis-
tance measure into a similarity measure as follows.
edit sim(pi, pj) = e??(edit distance(pi,pj)) (2)
Bunescu and Mooney (2005a) propose a similar
method for relation extraction in general. However,
their similarity measure is based on the number of
the overlapping words between two paths. When
two paths have different lengths, they assume the
similarity between them is zero. On the other hand,
our edit distance based measure can also account for
deletions and insertions of words.
4 Semi-Supervised Machine Learning
Approaches
4.1 kNN and Harmonic Functions
When a similarity measure is defined among the in-
stances of a learning problem, a simple and natural
choice is to use a nearest neighbor based approach
that classifies each instance by looking at the labels
of the instances that are most similar to it. Per-
haps the simplest and most popular similarity-based
231
learning algorithm is the k-nearest neighbor classifi-
cation method (kNN). Let U be the set of unlabeled
instances, and L be the set of labeled instances in
a learning problem. Given an instance x ? U , let
NLk (x) be the set of top k instances in L that are
most similar to x with respect to some similarity
measure. The kNN equation for a binary classifi-
cation problem can be written as:
y(x) =
?
z?NLk (x)
sim(x, z)y(z)
?
z??NLk (x)
sim(x, z?) (3)
where y(z) ? {0, 1} is the label of the instance z.2
Note that y(x) can take any real value in the [0, 1]
interval. The final classification decision is made by
setting a threshold in this interval (e.g. 0.5) and clas-
sifying the instances above the threshold as positive
and others as negative. For our problem, each in-
stance is a dependency path between the proteins in
the pair and the similarity function can be one of the
functions we have defined in Section 3.
Equation 3 can be seen as averaging the labels (0
or 1) of the nearest neighbors of each unlabeled in-
stance. This suggests a generalized semi-supervised
version of the same algorithm by incorporating un-
labeled instances as neighbors as well:
y(x) =
?
z?NL?Uk (x)
sim(x, z)y(z)
?
z??NL?Uk (x)
sim(x, z?) (4)
Unlike Equation 3, the unlabeled instances are also
considered in Equation 4 when finding the nearest
neighbors. We can visualize this as an undirected
graph, where each data instance (labeled or unla-
beled) is a node that is connected to its k nearest
neighbor nodes. The value of y(?) is set to 0 or 1
for labeled nodes depending on their class. For each
unlabeled node x, y(x) is equal to the average of the
y(?) values of its neighbors. Such a function that
satisfies the average property on all unlabeled nodes
is called a harmonic function and is known to exist
and have a unique solution (Doyle and Snell, 1984).
Harmonic functions were first introduced as a semi-
supervised learning method by Zhu et al (2003).
There are interesting alternative interpretations of
2Equation 3 is the weighted (or soft) version of the kNN
algorithm. In the classical voting scheme, x is classified in the
category that the majority of its neighbors belong to.
a harmonic function on a graph. One of them can
be explained in terms of random walks on a graph.
Consider a random walk on a graph where at each
time point we move from the current node to one of
its neighbors. The next node is chosen among the
neighbors of the current node with probability pro-
portional to the weight (similarity) of the edge that
connects the two nodes. Assuming we start the ran-
dom walk from the node x, y(x) in Equation 4 is
then equal to the probability that this random walk
will hit a node labeled 1 before it hits a node labeled
0.
4.2 Transductive SVM
Support vector machines (SVM) is a supervised ma-
chine learning approach designed for solving two-
class pattern recognition problems. The aim is to
find the decision surface that separates the positive
and negative labeled training examples of a class
with maximum margin (Burges, 1998).
Transductive support vector machines (TSVM)
are an extension of SVM, where unlabeled data is
used in addition to labeled data. The aim now is
to assign labels to the unlabeled data and find a de-
cision surface that separates the positive and nega-
tive instances of the original labeled data and the
(now labeled) unlabeled data with maximum mar-
gin. Intuitively, the unlabeled data pushes the deci-
sion boundary away from the dense regions. How-
ever, unlike SVM, the optimization problem now
is NP-hard (Zhu, 2005). Pointers to studies for
approximation algorithms can be found in (Zhu,
2005).
In Section 3 we defined the similarity between
two instances based on the cosine similarity and
the edit distance based similarity between the paths
in the instances. Here, we use these path similar-
ity measures as kernels for SVM and TSVM and
modify the SV M light package (Joachims, 1999) by
plugging in our two kernel functions.
A well-defined kernel function should be sym-
metric positive definite. While cosine kernel is well-
defined, Cortes et al (2004) proved that edit kernel
is not always positive definite. However, it is pos-
sible to make the kernel matrix positive definite by
adjusting the ? parameter, which is a positive real
number. Li and Jiang (2005) applied the edit kernel
to predict initiation sites in eucaryotic mRNAs and
232
obtained improved results compared to polynomial
kernel.
5 Experimental Results
5.1 Data Sets
One of the problems in the field of protein-protein
interaction extraction is that different studies gen-
erally use different data sets and evaluation met-
rics. Thus, it is difficult to compare their re-
sults. Bunescu et al (2005) manually developed the
AIMED corpus3 for protein-protein interaction and
protein name recognition. They tagged 199 Medline
abstracts, obtained from the Database of Interacting
Proteins (DIP) (Xenarios et al, 2001) and known to
contain protein interactions. This corpus is becom-
ing a standard, as it has been used in the recent stud-
ies by (Bunescu et al, 2005; Bunescu and Mooney,
2005b; Bunescu and Mooney, 2006; Mitsumori et
al., 2006; Yakushiji et al, 2005).
In our study we used the AIMED corpus and the
CB (Christine Brun) corpus that is provided as a re-
source by BioCreAtIvE II (Critical Assessment for
Information Extraction in Biology) challenge eval-
uation4. We pre-processed the CB corpus by first
annotating the protein names in the corpus automat-
ically and then, refining the annotation manually. As
discussed in Section 3, we pre-processed both of the
data sets as follows. We replicated each sentence
for each different protein pair. For n different pro-
teins in a sentence,
(n
2
)
new sentences are created,
as there are that many different pairs of proteins.
In each newly created sentence we marked the pro-
tein pair considered for interaction as PROTX1 and
PROTX2, and all the remaining proteins in the sen-
tence as PROTX0. If a sentence describes an inter-
action between PROTX1 and PROTX2, it is labeled
as positive, otherwise it is labeled as negative. The
summary of the data sets after pre-processing is dis-
played in Table 15.
Since previous studies that use AIMED corpus
perform 10-fold cross-validation. We also per-
formed 10-fold cross-validation in both data sets and
report the average results over the runs.
3ftp://ftp.cs.utexas.edu/pub/mooney/bio-data/
4http://biocreative.sourceforge.net/biocreative 2.html
5The pre-processed data sets are available at
http://belobog.si.umich.edu/clair/biocreative
Data Set Sentences + Sentences - Sentences
AIMED 4026 951 3075
CB 4056 2202 1854
Table 1: Data Sets
5.2 Evaluation Metrics
We use precision, recall, and F-score as our metrics
to evaluate the performances of the methods. Preci-
sion (pi) and recall (?) are defined as follows:
pi = TPTP + FP ; ? =
TP
TP + FN (5)
Here, TP (True Positives) is the number of sen-
tences classified correctly as positive; FP (False
Positives) is the number of negative sentences that
are classified as positive incorrectly by the classifier;
and FN (False Negatives) is the number of positive
sentences that are classified as negative incorrectly
by the classifier.
F-score is the harmonic mean of recall and precision.
F -score = 2pi?pi + ? (6)
5.3 Results and Discussion
We evaluate and compare the performances of
the semi-supervised machine learning approaches
(TSVM and harmonic functions) with their super-
vised counterparts (SVM and kNN) for the task of
protein-protein interaction extraction. As discussed
in Section 3, we use cosine similarity and edit dis-
tance based similarity as similarity functions in har-
monic functions and kNN, and as kernel functions
in TSVM and SVM. Our instances consist of the
shortest paths between the protein pairs in the de-
pendency parse trees of the sentences. In our ex-
periments, we tuned the ? parameter of the edit
distance based path similarity function to 4.5 with
cross-validation. The results in Table 2 and Table 3
are obtained with 10-fold cross-validation. We re-
port the average results over the runs.
Table 2 shows the results obtained for the AIMED
data set. Edit distance based path similarity function
performs considerably better than the cosine sim-
ilarity function with harmonic functions and kNN
and usually slightly better with SVM and TSVM.
We achieve our best F-score performance of 59.96%
with TSVM with edit kernel. While SVM with edit
233
kernel achieves the highest precision of 77.52%, it
performs slightly worse than SVM with cosine ker-
nel in terms of F-score measure. TSVM performs
slightly better than SVM, both of which perform bet-
ter than harmonic functions. kNN is the worst per-
forming algorithm for this data set.
In Table 2, we also show the results obtained pre-
viously in the literature by using the same data set.
Yakushiji et al (2005) use an HPSG parser to pro-
duce predicate argument structures. They utilize
these structures to automatically construct protein
interaction extraction rules. Mitsumori et al (2006)
use SVM with the unparsed text around the pro-
tein names as features to extract protein interac-
tion sentences. Here, we show their best result ob-
tained by using the three words to the left and to the
right of the proteins. The most closely related study
to ours is that by Bunescu and Mooney (2005a).
They define a kernel function based on the short-
est path between two entities of a relationship in
the dependency parse tree of a sentence (the SPK
method). They apply this method to the domain
of protein-protein interaction extraction in (Bunescu
and Mooney, 2006). Here, they also test the meth-
ods ELCS (Extraction Using Longest Common Sub-
sequences) (Bunescu et al, 2005) and SSK (Sub-
sequence Kernel) (Bunescu and Mooney, 2005b).
We cannot compare our results to theirs directly,
because they report their results as a precision-
recall graph. However, the best F-score in their
graph seems to be around 0.50 and definitely lower
than the best F-scores we have achieved (? 0.59).
Bunescu and Mooney (2006) also use SVM as their
learning method in their SPK approach. They define
their similarity based on the number of overlapping
words between two paths and assign a similarity of
zero if the two paths have different lengths. Our
improved performance with SVM and the shortest
path dependency features may be due to the edit-
distance based kernel, which takes into account not
only the overlapping words, but also word order and
accounts for deletions and insertions of words. Our
results show that, SVM, TSVM, and harmonic func-
tions achieve better F-score and recall performances
than the previous studies by Yakushiji et al (2005),
Mitsumori et al (2006), and the SSK and ELCS ap-
proaches of Bunescu and Mooney (2006). SVM and
TSVM also achieve higher precision scores. Since,
Mitsumori et al (2006) also use SVM in their study,
our improved results with SVM confirms our moti-
vation of using dependency paths as features.
Table 3 shows the results we got with the CB
data set. The F-score performance with the edit
distance based similarity function is always better
than that of cosine similarity function for this data
set. The difference in performances is considerable
for harmonic functions and kNN. Our best F-score
is achieved with TSVM with edit kernel (85.22%).
TSVM performs slightly better than SVM. When
cosine similarity function is used, kNN performs
better than harmonic functions. However, when edit
distance based similarity is used, harmonic functions
achieve better performance. SVM and TSVM per-
form better than harmonic functions. But, the gap in
performance is low when edit distance based simi-
larity is used with harmonic functions.
Method Precision Recall F-Score
SVM-edit 77.52 43.51 55.61
SVM-cos 61.99 54.99 58.09
TSVM-edit 59.59 60.68 59.96
TSVM-cos 58.37 61.19 59.62
Harmonic-edit 44.17 74.20 55.29
Harmonic-cos 36.02 67.65 46.97
kNN-edit 68.77 42.17 52.20
kNN-cos 40.37 49.49 44.36
(Yakushiji et al, 2005) 33.70 33.10 33.40
(Mitsumori et al, 2006) 54.20 42.60 47.70
Table 2: Experimental Results ? AIMED Data Set
Method Precision Recall F-Score
SVM-edit 85.15 84.79 84.96
SVM-cos 87.83 81.45 84.49
TSVM-edit 85.62 84.89 85.22
TSVM-cos 85.67 84.31 84.96
Harmonic-edit 86.69 80.15 83.26
Harmonic-cos 72.28 70.91 71.56
kNN-edit 72.89 86.95 79.28
kNN-cos 65.42 89.49 75.54
Table 3: Experimental Results ? CB Data Set
Semi-supervised approaches are usually more ef-
fective when there is less labeled data than unlabeled
data, which is usually the case in real applications.
To see the effect of semi-supervised approaches we
perform experiments by varying the amount of la-
234
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
300020001000500200100502010
F-
Sc
o
re
Number of Labeled Sentences
kNN
Harmonic
SVM
TSVM
Figure 2: The F-score on the AIMED dataset with
varying sizes of training data
beled training sentences in the range [10, 3000]. For
each labeled training set size, sentences are selected
randomly among all the sentences, and the remain-
ing sentences are used as the unlabeled test set. The
results that we report are the averages over 10 such
random runs for each labeled training set size. We
report the results for the algorithms when edit dis-
tance based similarity is used, as it mostly performs
better than cosine similarity. Figure 2 shows the
results obtained over the AIMED data set. Semi-
supervised approaches TSVM and harmonic func-
tions perform considerably better than their super-
vised counterparts SVM and kNN when we have
small number of labeled training data. It is inter-
esting to note that, although SVM is one of the best
performing algorithms with more training data, it is
the worst performing algorithm with small amount
of labeled training sentences. Its performance starts
to increase when number of training data is larger
than 200. Eventually, its performance gets close to
that of the other algorithms. Harmonic functions is
the best performing algorithm when we have less
than 200 labeled training data. TSVM achieves bet-
ter performance when there are more than 500 la-
beled training sentences.
Figure 3 shows the results obtained over the CB
data set. When we have less than 500 labeled sen-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
300020001000500200100502010
F-
Sc
o
re
Number of Labeled Sentences
kNN
Harmonic
SVM
TSVM
Figure 3: The F-score on the CB dataset with vary-
ing sizes of training data
tences, harmonic functions and TSVM perform sig-
nificantly better than kNN, while SVM is the worst
performing algorithm. When we have more than
500 labeled training sentences, kNN is the worst per-
forming algorithm, while the performance of SVM
increases and gets similar to that of TSVM and
slightly better than that of harmonic functions.
6 Conclusion
We introduced a relation extraction approach based
on dependency parsing and machine learning to
identify protein interaction sentences in biomedical
text. Unlike syntactic parsing, dependency parsing
captures the semantic predicate argument relation-
ships between the entities in addition to the syntac-
tic relationships. We extracted the shortest paths be-
tween protein pairs in the dependency parse trees of
the sentences and defined similarity functions (ker-
nels in SVM terminology) for these paths based on
cosine similarity and edit distance. Supervised ma-
chine learning approaches have been applied to this
domain. However, they rely only on labeled training
data, which is difficult to gather. To our knowledge,
this is the first effort in this domain to apply semi-
supervised algorithms, which make use of both la-
beled and unlabeled data. We evaluated and com-
pared the performances of two semi-supervised ma-
235
chine learning approaches (harmonic functions and
TSVM), with their supervised counterparts (kNN
and SVM). We showed that, edit distance based sim-
ilarity function performs better than cosine simi-
larity function since it takes into account not only
common words, but also word order. Our 10-fold
cross validation results showed that, TSVM per-
forms slightly better than SVM, both of which per-
form better than harmonic functions. The worst per-
forming algorithm is kNN. We compared our results
with previous results published with the AIMED
data set. We achieved the best F-score performance
with TSVM with the edit distance kernel (59.96%)
which is significantly higher than the previously re-
ported results for the same data set.
In most real-world applications there are much
more unlabeled data than labeled data. Semi-
supervised approaches are usually more effective in
these cases, because they make use of both the la-
beled and unlabeled instances when making deci-
sions. To test this hypothesis for the application
of extracting protein interaction sentences from text,
we performed experiments by varying the number
of labeled training sentences. Our results show
that, semi-supervised algorithms perform consider-
ably better than their supervised counterparts, when
there are small number of labeled training sentences.
An interesting result is that, in such cases SVM per-
forms significantly worse than the other algorithms.
Harmonic functions achieve the best performance
when there are only a few labeled training sentences.
As number of labeled training sentences increases
the performance gap between supervised and semi-
supervised algorithms decreases.
Acknowledgments
This work was supported in part by grants R01-
LM008106 and U54-DA021519 from the US Na-
tional Institutes of Health.
References
G. Bader, D. Betel, and C. Hogue. 2003. Bind - the
biomolecular interaction network database. Nucleic
Acids Research, 31(1):248?250.
A. Bairoch and R. Apweiler. 2000. The swiss-prot pro-
tein sequence database and its supplement trembl in
2000. Nucleic Acids Research, 28(1):45?48.
C. Blaschke, M. A. Andrade, C. A. Ouzounis, and A. Va-
lencia. 1999. Automatic extraction of biological in-
formation from scientific text: Protein-protein interac-
tions. In Proceedings of the AAAI Conference on In-
telligent Systems for Molecular Biology (ISMB 1999),
pages 60?67.
R. C. Bunescu and R. J. Mooney. 2005a. A shortest
path dependency kernel for relation extraction. In Pro-
ceedings of the Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 724?731, Vancouver,
B.C, October.
R. C. Bunescu and R. J. Mooney. 2005b. Subsequence
kernels for relation extraction. In Proceedings of the
19th Conference on Neural Information Processing
Systems (NIPS), Vancouver, B.C, December.
R. C. Bunescu and R. J. Mooney, 2006. Text Mining and
Natural Language Processing, chapter Extracting Re-
lations from Text: From Word Sequences to Depen-
dency Paths. forthcoming book.
R. Bunescu, R. Ge, J. R. Kate, M. E. Marcotte, R. J.
Mooney, K. A. Ramani, and W. Y. Wong. 2005. Com-
parative experiments on learning information extrac-
tors for proteins and their interactions. Artificial Intel-
ligence in Medicine, 33(2):139?155, February.
C. J. C. Burges. 1998. A tutorial on support vector
machines for pattern recognition. Data Mining and
Knowledge Discovery, 2(2):121?167.
C. Cortes, P. Haffner, and M. Mohri. 2004. Rational
kernels: Theory and algorithms. Journal of Machine
Learning Research, (5):1035?1062, August.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL-04), Barcelona, Spain, July.
N. Daraselia, A. Yuryev, S. Egorov, S. Novichkova,
A. Nikitin, and I. Mazo. 2004. Extracting human
protein interactions from medline using a full-sentence
parser. Bioinformatics, 20(5):604?611.
M-C. de Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In Proceedings of the IEEE /
ACL 2006 Workshop on Spoken Language Technology.
The Stanford Natural Language Processing Group.
I. Donaldson, J. Martin, B. de Bruijn, C. Wolting,
V. Lay, B. Tuekam, S. Zhang, B. Baskin, G. D. Bader,
K. Michalockova, T. Pawson, and C. W. V. Hogue.
2003. Prebind and textomy - mining the biomedical
literature for protein-protein interactions using a sup-
port vector machine. BMC Bioinformatics, 4:11.
236
P. G. Doyle and J. L. Snell. 1984. Random Walks
and Electric Networks. Mathematical Association of
America.
M. Huang, X. Zhu, Y. Hao, D. G. Payan, K. Qu, and
M. Li. 2004. Discovering patterns to extract protein-
protein interactions from full texts. Bioinformatics,
20(18):3604?3612.
T. Joachims. 1999. Transductive inference for text
classification using support vector machines. In Ivan
Bratko and Saso Dzeroski, editors, Proceedings of
ICML-99, 16th International Conference on Machine
Learning, pages 200?209. Morgan Kaufmann Publish-
ers, San Francisco, US.
H. Li and T. Jiang. 2005. A class of edit kernels for
svms to predict translation initiation sites in eukaryotic
mrnas. Journal of Computational Biology, 12(6):702?
718.
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with svm. IEICE Trans-
actions on Information and Systems, E89-D(8):2464?
2466.
T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi.
2001. Automated extraction of information on
protein-protein interactions from the biological liter-
ature. Bioinformatics, 17(2):155?161.
E. M. Phizicky and S. Fields. 1995. Protein-protein in-
teractions: methods for detection and analysis. Micro-
biol. Rev., 59(1):94?123, March.
J. Pustejovsky, J. Castano, J. Zhang, M. Kotecki, and
B. Cochran. 2002. Robust relational parsing over
biomedical literature: Extracting inhibit relations. In
Proceedings of the seventh Pacific Symposium on Bio-
computing (PSB 2002), pages 362?373.
K. Sugiyama, K. Hatano, M. Yoshikawa, and S. Uemura.
2003. Extracting information on protein-protein in-
teractions from biological literature based on machine
learning approaches. Genome Informatics, 14:699?
700.
J. M. Temkin and M. R. Gilder. 2003. Extraction of pro-
tein interaction information from unstructured text us-
ing a context-free grammar. Bioinformatics, 19:2046?
2053.
I. Xenarios, E. Fernandez, L. Salwinski, X. J. Duan, M. J.
Thompson, E. M. Marcotte, and D. Eisenberg. 2001.
Dip: The database of interacting proteins: 2001 up-
date. Nucleic Acids Res., 29:239 ? 241, January.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In Proceedings of The
Eleventh Annual Meeting of The Association for Natu-
ral Language Processing, pages 93?96.
A. Zanzoni, L. Montecchi-Palazzi, M. Quondam,
G. Ausiello, M. Helmer-Citterich, and G. Cesareni.
2002. Mint: A molecular interaction database. FEBS
Letters, 513:135?140.
X. Zhu, Z. Ghahramani, and J. D. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In T. Fawcett and N. Mishra, editors,
ICML, pages 912?919. AAAI Press.
X. Zhu. 2005. Semi-supervised learning lit-
erature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
237
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 915?922, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Random Walks for Question-focused Sentence Retrieval
Jahna Otterbacher1, Gu?nes? Erkan2, Dragomir R. Radev1,2
1School of Information, 2Department of EECS
University of Michigan
{jahna,gerkan,radev}@umich.edu
Abstract
We consider the problem of question-
focused sentence retrieval from complex
news articles describing multi-event sto-
ries published over time. Annotators gen-
erated a list of questions central to under-
standing each story in our corpus. Be-
cause of the dynamic nature of the stories,
many questions are time-sensitive (e.g.
?How many victims have been found??)
Judges found sentences providing an an-
swer to each question. To address the
sentence retrieval problem, we apply a
stochastic, graph-based method for com-
paring the relative importance of the tex-
tual units, which was previously used suc-
cessfully for generic summarization. Cur-
rently, we present a topic-sensitive version
of our method and hypothesize that it can
outperform a competitive baseline, which
compares the similarity of each sentence
to the input question via IDF-weighted
word overlap. In our experiments, the
method achieves a TRDR score that is sig-
nificantly higher than that of the baseline.
1 Introduction
Recent work has motivated the need for systems
that support ?Information Synthesis? tasks, in which
a user seeks a global understanding of a topic or
story (Amigo et al, 2004). In contrast to the clas-
sical question answering setting (e.g. TREC-style
Q&A (Voorhees and Tice, 2000)), in which the user
presents a single question and the system returns a
corresponding answer (or a set of likely answers), in
this case the user has a more complex information
need.
Similarly, when reading about a complex news
story, such as an emergency situation, users might
seek answers to a set of questions in order to un-
derstand it better. For example, Figure 1 shows
the interface to our Web-based news summarization
system, which a user has queried for information
about Hurricane Isabel. Understanding such stories
is challenging for a number of reasons. In particular,
complex stories contain many sub-events (e.g. the
devastation of the hurricane, the relief effort, etc.) In
addition, while some facts surrounding the situation
do not change (such as ?Which area did the hurri-
cane first hit??), others may change with time (?How
many people have been left homeless??). There-
fore, we are working towards developing a system
for question answering from clusters of complex sto-
ries published over time. As can be seen at the bot-
tom of Figure 1, we plan to add a component to our
current system that allows users to ask questions as
they read a story. They may then choose to receive
either a precise answer or a question-focused sum-
mary.
Currently, we address the question-focused sen-
tence retrieval task. While passage retrieval (PR) is
clearly not a new problem (e.g. (Robertson et al,
1992; Salton et al, 1993)), it remains important and
yet often overlooked. As noted by (Gaizauskas et al,
2004), while PR is the crucial first step for question
answering, Q&A research has typically not empha-
915
Hurricane Isabel's outer bands moving onshore
produced on 09/18, 6:18 AM
2% Summary
The North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as far
away as Pennsylvania prepared for possibly ruinous flooding. (2:3) A hurricane warning was in effect from Cape
Fear in southern North Carolina to the Virginia-Maryland line, and tropical storm warnings extended from South Carolina
to New Jersey. (2:14)
While the outer edge of the hurricane approached the North Carolina coast Wednesday, the center of the storm was still
400 miles south-southeast of Cape Hatteras, N.C., late Wednesday morning. (3:10) BBC NEWS World Americas
Hurricane Isabel prompts US shutdown (4:1)
Ask us:
What states have been affected by the hurricane so far?
Around 200,000 people in coastal areas of North Carolina and Virginia were ordered to evacuate or risk getting trapped
by flooding from storm surges up to 11 feet. (5:8) The storm was expected to hit with its full fury today, slamming into
the North Carolina coast with 105-mph winds and 45-foot wave crests, before moving through Virginia and bashing the
capital with gusts of about 60 mph. (7:6)
Figure 1: Question tracking interface to a summa-
rization system.
sized it. The specific problem we consider differs
from the classic task of PR for a Q&A system in
interesting ways, due to the time-sensitive nature of
the stories in our corpus. For example, one challenge
is that the answer to a user?s question may be up-
dated and reworded over time by journalists in order
to keep a running story fresh, or because the facts
themselves change. Therefore, there is often more
than one correct answer to a question.
We aim to develop a method for sentence re-
trieval that goes beyond finding sentences that are
similar to a single query. To this end, we pro-
pose to use a stochastic, graph-based method. Re-
cently, graph-based methods have proved useful for
a number of NLP and IR tasks such as document
re-ranking in ad hoc IR (Kurland and Lee, 2005)
and analyzing sentiments in text (Pang and Lee,
2004). In (Erkan and Radev, 2004), we introduced
the LexRank method and successfully applied it to
generic, multi-document summarization. Presently,
we introduce topic-sensitive LexRank in creating a
sentence retrieval system. We evaluate its perfor-
mance against a competitive baseline, which con-
siders the similarity between each sentence and the
question (using IDF-weighed word overlap). We
demonstrate that LexRank significantly improves
question-focused sentence selection over the base-
line.
2 Formal description of the problem
Our goal is to build a question-focused sentence re-
trieval mechanism using a topic-sensitive version of
the LexRank method. In contrast to previous PR sys-
tems such as Okapi (Robertson et al, 1992), which
ranks documents for relevancy and then proceeds to
find paragraphs related to a question, we address the
finer-grained problem of finding sentences contain-
ing answers. In addition, the input to our system is
a set of documents relevant to the topic of the query
that the user has already identified (e.g. via a search
engine). Our system does not rank the input docu-
ments, nor is it restricted in terms of the number of
sentences that may be selected from the same docu-
ment.
The output of our system, a ranked list of sen-
tences relevant to the user?s question, can be sub-
sequently used as input to an answer selection sys-
tem in order to find specific answers from the ex-
tracted sentences. Alternatively, the sentences can
be returned to the user as a question-focused sum-
mary. This is similar to ?snippet retrieval? (Wu et
al., 2004). However, in our system answers are ex-
tracted from a set of multiple documents rather than
on a document-by-document basis.
3 Our approach: topic-sensitive LexRank
3.1 The LexRank method
In (Erkan and Radev, 2004), the concept of graph-
based centrality was used to rank a set of sentences,
in producing generic multi-document summaries.
To apply LexRank, a similarity graph is produced
for the sentences in an input document set. In the
graph, each node represents a sentence. There are
edges between nodes for which the cosine similar-
ity between the respective pair of sentences exceeds
a given threshold. The degree of a given node is
an indication of how much information the respec-
tive sentence has in common with other sentences.
Therefore, sentences that contain the most salient in-
formation in the document set should be very central
within the graph.
Figure 2 shows an example of a similarity graph
for a set of five input sentences, using a cosine simi-
larity threshold of 0.15. Once the similarity graph is
constructed, the sentences are then ranked according
to their eigenvector centrality. As previously men-
tioned, the original LexRank method performed well
in the context of generic summarization. Below,
we describe a topic-sensitive version of LexRank,
which is more appropriate for the question-focused
sentence retrieval problem. In the new approach, the
916
score of a sentence is determined by a mixture model
of the relevance of the sentence to the query and the
similarity of the sentence to other high-scoring sen-
tences.
3.2 Relevance to the question
In topic-sensitive LexRank, we first stem all of the
sentences in a set of articles and compute word IDFs
by the following formula:
idfw = log
(
N + 1
0.5 + sfw
)
(1)
where N is the total number of sentences in the clus-
ter, and sf
w
is the number of sentences that the word
w appears in.
We also stem the question and remove the stop
words from it. Then the relevance of a sentence s to
the question q is computed by:
rel(s|q) =
X
w?q
log(tfw,s + 1) ? log(tfw,q + 1) ? idfw (2)
where tf
w,s
and tf
w,q
are the number of times w
appears in s and q, respectively. This model has
proven to be successful in query-based sentence re-
trieval (Allan et al, 2003), and is used as our com-
petitive baseline in this study (e.g. Tables 4, 5 and
7).
3.3 The mixture model
The baseline system explained above does not make
use of any inter-sentence information in a cluster.
We hypothesize that a sentence that is similar to
the high scoring sentences in the cluster should also
have a high score. For instance, if a sentence that
gets a high score in our baseline model is likely to
contain an answer to the question, then a related sen-
tence, which may not be similar to the question it-
self, is also likely to contain an answer.
This idea is captured by the following mixture
model, where p(s|q), the score of a sentence s given
a question q, is determined as the sum of its rele-
vance to the question (using the same measure as
the baseline described above) and the similarity to
the other sentences in the document cluster:
p(s|q) = d
rel(s|q)
P
z?C rel(z|q)
+(1?d)
X
v?C
sim(s, v)
P
z?C sim(z, v)
p(v|q) (3)
where C is the set of all sentences in the cluster. The
value of d, which we will also refer to as the ?ques-
tion bias,? is a trade-off between two terms in the
Vertices:
Sentence Index Salience Sentence
4 0.1973852892722677 Milan fire brigade officials said that...
1 0.03614457831325301 At least two people are dead, inclu...
0 0.28454242157110576 Officials said the plane was carryin...
2 0.1973852892722677 Italian police said the plane was car..
3 0.28454242157110576 Rescue officials said that at least th...
Graph
Figure 2: LexRank example: sentence similarity
graph with a cosine threshold of 0.15.
equation and is determined empirically. For higher
values of d, we give more importance to the rele-
vance to the question compared to the similarity to
the other sentences in the cluster. The denominators
in both terms are for normalization, which are de-
scribed below. We use the cosine measure weighted
by word IDFs as the similarity between two sen-
tences in a cluster:
sim(x, y) =
P
w?x,y tfw,xtfw,y(idfw)
2
q
P
xi?x
(tfxi,xidfxi )2 ?
q
P
yi?y
(tfyi,y idfyi )2
(4)
Equation 3 can be written in matrix notation as
follows:
p = [dA+ (1 ? d)B]Tp (5)
A is the square matrix such that for a given index i,
all the elements in the ith column are proportional
to rel(i|q). B is also a square matrix such that each
entry B(i, j) is proportional to sim(i, j). Both ma-
trices are normalized so that row sums add up to 1.
Note that as a result of this normalization, all rows
of the resulting square matrixQ = [dA+(1?d)B]
also add up to 1. Such a matrix is called stochastic
and defines a Markov chain. If we view each sen-
tence as a state in a Markov chain, thenQ(i, j) spec-
ifies the transition probability from state i to state j
in the corresponding Markov chain. The vector p
we are looking for in Equation 5 is the stationary
distribution of the Markov chain. An intuitive inter-
pretation of the stationary distribution can be under-
917
stood by the concept of a random walk on the graph
representation of the Markov chain.
With probability d, a transition is made from the
current node (sentence) to the nodes that are simi-
lar to the query. With probability (1-d), a transition
is made to the nodes that are lexically similar to the
current node. Every transition is weighted according
to the similarity distributions. Each element of the
vector p gives the asymptotic probability of ending
up at the corresponding state in the long run regard-
less of the starting state. The stationary distribution
of a Markov chain can be computed by a simple it-
erative algorithm, called power method.1
A simpler version of Equation 5, where A is a
uniform matrix andB is a normalized binary matrix,
is known as PageRank (Brin and Page, 1998; Page
et al, 1998) and used to rank the web pages by the
Google search engine. It was also the model used to
rank sentences in (Erkan and Radev, 2004).
3.4 Experiments with topic-sensitive LexRank
We experimented with different values of d on our
training data. We also considered several threshold
values for inter-sentence cosine similarities, where
we ignored the similarities between the sentences
that are below the threshold. In the training phase
of the experiment, we evaluated all combinations
of LexRank with d in the range of [0, 1] (in incre-
ments of 0.10) and with a similarity threshold rang-
ing from [0, 0.9] (in increments of 0.05). We then
found all configurations that outperformed the base-
line. These configurations were then applied to our
development/test set. Finally, our best sentence re-
trieval system was applied to our test data set and
evaluated against the baseline. The remainder of the
paper will explain this process and the results in de-
tail.
4 Experimental setup
4.1 Corpus
We built a corpus of 20 multi-document clusters of
complex news stories, such as plane crashes, polit-
ical controversies and natural disasters. The data
1The stationary distribution is unique and the power method
is guaranteed to converge provided that the Markov chain is
ergodic (Seneta, 1981). A non-ergodic Markov chain can be
made ergodic by reserving a small probability for jumping to
any other state from the current state (Page et al, 1998).
clusters and their characteristics are shown in Ta-
ble 1. The news articles were collected from various
sources. ?Newstracker? clusters were collected au-
tomatically by our Web-based news summarization
system. The number of clusters randomly assigned
to the training, development/test and test data sets
were 11, 3 and 6, respectively.
Next, we assigned each cluster of articles to an
annotator, who was asked to read all articles in the
cluster. He or she then generated a list of factual
questions key to understanding the story. Once we
collected the questions for each cluster, two judges
independently annotated nine of the training clus-
ters. For each sentence and question pair in a given
cluster, the judges were asked to indicate whether
or not the sentence contained a complete answer
to the question. Once an acceptable rate of inter-
judge agreement was verified on the first nine clus-
ters (Kappa (Carletta, 1996) of 0.68), the remaining
11 clusters were annotated by one judge each.
In some cases, the judges did not find any sen-
tences containing the answer for a given question.
Such questions were removed from the corpus. The
final number of questions annotated for answers
over the entire corpus was 341, and the distributions
of questions per cluster can be found in Table 1.
4.2 Evaluation metrics and methods
To evaluate our sentence retrieval mechanism, we
produced extract files, which contain a list of sen-
tences deemed to be relevant to the question, for the
system and from human judgment. To compare dif-
ferent configurations of our system to the baseline
system, we produced extracts at a fixed length of 20
sentences. While evaluations of question answering
systems are often based on a shorter list of ranked
sentences, we chose to generate longer lists for sev-
eral reasons. One is that we are developing a PR
system, of which the output can then be input to an
answer extraction system for further processing. In
such a setting, we would most likely want to gener-
ate a relatively longer list of candidate sentences. As
previously mentioned, in our corpus the questions
often have more than one relevant answer, so ideally,
our PR system would find many of the relevant sen-
tences, sending them on to the answer component
to decide which answer(s) should be returned to the
user. Each system?s extract file lists the document
918
Cluster Sources Articles Questions Data set Sample question
Algerian terror AFP, UPI 2 12 train What is the condition under which
threat GIA will take its action?
Milan plane MSNBC, CNN, ABC, 9 15 train How many people were in the
crash Fox, USAToday building at the time of the crash?
Turkish plane BBC, ABC, 10 12 train To where was the plane headed?
crash FoxNews, Yahoo
Moscow terror UPI, AFP, AP 7 7 train How many people were killed in
attack the most recent explosion?
Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train Who was to blame for
club fire Fox, BBC, Ananova the fire?
FBI most AFP, UPI 3 14 train How much is the State Department offering
wanted for information leading to bin Laden?s arrest?
Russia bombing AP, AFP 2 11 train What was the cause of the blast?
Bali terror CNN, FoxNews, ABC, 10 30 train What were the motivations
attack BBC, Ananova of the attackers?
Washington DC FoxNews, Ha?aretz, BBC, 8 28 train What kinds of equipment or weapons
sniper BBC, Washington Times, CBS were used in the killings?
GSPC terror Newstracker 8 29 train What are the charges against
group the GSPC suspects?
China Novelty 43 25 18 train What was the magnitude of the
earthquake earthquake in Zhangjiakou?
Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test How many people
FoxNews, Washington Post were on board?
David Beckham AFP 20 28 dev/test How long had Beckham been playing for
trade MU before he moved to RM?
Miami airport Newstracker 12 15 dev/test How many concourses does
evacuation the airport have?
US hurricane DUC d04a 14 14 test In which places had the hurricane landed?
EgyptAir crash Novelty 4 25 29 test How many people were killed?
Kursk submarine Novelty 33 25 30 test When did the Kursk sink?
Hebrew University bombing Newstracker 11 27 test How many people were injured?
Finland mall bombing Newstracker 9 15 test How many people were in the mall
at the time of the bombing?
Putin visits Newstracker 12 20 test What issue concerned British
England human rights groups?
Table 1: Corpus of complex news stories.
and sentence numbers of the top 20 sentences. The
?gold standard? extracts list the sentences judged as
containing answers to a given question by the anno-
tators (and therefore have variable sizes) in no par-
ticular order.2
We evaluated the performance of the systems us-
ing two metrics - Mean Reciprocal Rank (MRR)
(Voorhees and Tice, 2000) and Total Reciprocal
Document Rank (TRDR) (Radev et al, 2005).
MRR, used in the TREC Q&A evaluations, is the
reciprocal rank of the first correct answer (or sen-
tence, in our case) to a given question. This measure
gives us an idea of how far down we must look in the
ranked list in order to find a correct answer. To con-
trast, TRDR is the total of the reciprocal ranks of all
answers found by the system. In the context of an-
swering questions from complex stories, where there
is often more than one correct answer to a question,
and where answers are typically time-dependent, we
should focus on maximizing TRDR, which gives us
2For clusters annotated by two judges, all sentences chosen
by at least one judge were included.
a measure of how many of the relevant sentences
were identified by the system. However, we report
both the average MRR and TRDR over all questions
in a given data set.
5 LexRank versus the baseline system
In the training phase, we searched the parameter
space for the values of d (the question bias) and the
similarity threshold in order to optimize the resulting
TRDR scores. For our problem, we expected that a
relatively low similarity threshold pair with a high
question bias would achieve the best results. Table 2
shows the effect of varying the similarity threshold.3
The notation LR[a, d] is used, where a is the simi-
larity threshold and d is the question bias. The opti-
mal range for the parameter a was between 0.14 and
0.20. This is intuitive because if the threshold is too
high, such that only the most lexically similar sen-
tences are represented in the graph, the method does
not find sentences that are related but are more lex-
3A threshold of -1 means that no threshold was used such
that all sentences were included in the graph.
919
System Ave. MRR Ave. TRDR
LR[-1.0,0.65] 0.5270 0.8117
LR[0.02,0.65] 0.5261 0.7950
LR[0.16,0.65] 0.5131 0.8134
LR[0.18,0.65] 0.5062 0.8020
LR[0.20,0.65] 0.5091 0.7944
LR[-1.0,0.80] 0.5288 0.8152
LR[0.02,0.80] 0.5324 0.8043
LR[0.16,0.80] 0.5184 0.8160
LR[0.18,0.80] 0.5199 0.8154
LR[0.20,0.80] 0.5282 0.8152
Table 2: Training phase: effect of similarity thresh-
old (a) on Ave. MRR and TRDR.
System Ave. MRR Ave. TRDR
LR[0.02,0.65] 0.5261 0.7950
LR[0.02,0.70] 0.5290 0.7997
LR[0.02,0.75] 0.5299 0.8013
LR[0.02,0.80] 0.5324 0.8043
LR[0.02,0.85] 0.5322 0.8038
LR[0.02,0.90] 0.5323 0.8077
LR[0.20,0.65] 0.5091 0.7944
LR[0.20,0.70] 0.5244 0.8105
LR[0.20,0.75] 0.5285 0.8137
LR[0.20,0.80] 0.5282 0.8152
LR[0.20,0.85] 0.5317 0.8203
LR[0.20,0.90] 0.5368 0.8265
Table 3: Training phase: effect of question bias (d)
on Ave. MRR and TRDR.
ically diverse (e.g. paraphrases). Table 3 shows the
effect of varying the question bias at two different
similarity thresholds (0.02 and 0.20). It is clear that a
high question bias is needed. However, a small prob-
ability for jumping to a node that is lexically simi-
lar to the given sentence (rather than the question
itself) is needed. Table 4 shows the configurations
of LexRank that performed better than the baseline
system on the training data, based on mean TRDR
scores over the 184 training questions. We applied
all four of these configurations to our unseen devel-
opment/test data, in order to see if we could further
differentiate their performances.
5.1 Development/testing phase
The scores for the four LexRank systems and the
baseline on the development/test data are shown in
System Ave. MRR Ave. TRDR
Baseline 0.5518 0.8297
LR[0.14,0.95] 0.5267 0.8305
LR[0.18,0.90] 0.5376 0.8382
LR[0.18,0.95] 0.5421 0.8382
LR[0.20,0.95] 0.5404 0.8311
Table 4: Training phase: systems outperforming the
baseline in terms of TRDR score.
System Ave. MRR Ave. TRDR
Baseline 0.5709 1.0002
LR[0.14,0.95] 0.5882 1.0469
LR[0.18,0.90] 0.5820 1.0288
LR[0.18,0.95] 0.5956 1.0411
LR[0.20,0.95] 0.6068 1.0601
Table 5: Development testing evaluation.
Cluster B-MRR LR-MRR B-TRDR LR-TRDR
Gulfair 0.5446 0.5461 0.9116 0.9797
David Beckham trade 0.5074 0.5919 0.7088 0.7991
Miami airport 0.7401 0.7517 1.7157 1.7028
evacuation
Table 6: Average scores by cluster: baseline versus
LR[0.20,0.95].
Table 5. This time, all four LexRank systems outper-
formed the baseline, both in terms of average MRR
and TRDR scores. An analysis of the average scores
over the 72 questions within each of the three clus-
ters for the best system, LR[0.20,0.95], is shown
in Table 6. While LexRank outperforms the base-
line system on the first two clusters both in terms
of MRR and TRDR, their performances are not sub-
stantially different on the third cluster. Therefore,
we examined properties of the questions within each
cluster in order to see what effect they might have on
system performance.
We hypothesized that the baseline system, which
compares the similarity of each sentence to the ques-
tion using IDF-weighted word overlap, should per-
form well on questions that provide many content
words. To contrast, LexRank might perform bet-
ter when the question provides fewer content words,
since it considers both similarity to the query and
inter-sentence similarity. Out of the 72 questions in
the development/test set, the baseline system outper-
formed LexRank on 22 of the questions. In fact, the
average number of content words among these 22
questions was slightly, but not significantly, higher
than the average on the remaining questions (3.63
words per question versus 3.46). Given this obser-
vation, we experimented with two mixed strategies,
in which the number of content words in a question
determined whether LexRank or the baseline system
was used for sentence retrieval. We tried threshold
values of 4 and 6 content words, however, this did
not improve the performance over the pure strategy
of system LR[0.20,0.95]. Therefore, we applied this
920
Ave. MRR Ave. TRDR
Baseline 0.5780 0.8673
LR[0.20,0.95] 0.6189 0.9906
p-value na 0.0619
Table 7: Testing phase: baseline vs. LR[0.20,0.95].
system versus the baseline to our unseen test set of
134 questions.
5.2 Testing phase
As shown in Table 7, LR[0.20,0.95] outperformed
the baseline system on the test data both in terms
of average MRR and TRDR scores. The improve-
ment in average TRDR score was statistically sig-
nificant with a p-value of 0.0619. Since we are in-
terested in a passage retrieval mechanism that finds
sentences relevant to a given question, providing in-
put to the question answering component of our sys-
tem, the improvement in average TRDR score is
very promising. While we saw in Section 5.1 that
LR[0.20,0.95] may perform better on some question
or cluster types than others, we conclude that it beats
the competitive baseline when one is looking to op-
timize mean TRDR scores over a large set of ques-
tions. However, in future work, we will continue
to improve the performance, perhaps by develop-
ing mixed strategies using different configurations
of LexRank.
6 Discussion
The idea behind using LexRank for sentence re-
trieval is that a system that considers only the sim-
ilarity between candidate sentences and the input
query, and not the similarity between the candidate
sentences themselves, is likely to miss some impor-
tant sentences. When using any metric to compare
sentences and a query, there is always likely to be
a tie between multiple sentences (or, similarly, there
may be cases where fewer than the number of de-
sired sentences have similarity scores above zero).
LexRank effectively provides a means to break such
ties. An example of such a scenario is illustrated in
Tables 8 and 9, which show the top ranked sentences
by the baseline and LexRank, respectively for the
question ?What caused the Kursk to sink?? from the
Kursk submarine cluster. It can be seen that all top
five sentences chosen by the baseline system have
Rank Sentence Score Relevant?
1 The Russian governmental commission on the 4.2282 N
accident of the submarine Kursk sinking in
the Barents Sea on August 12 has rejected
11 original explanations for the disaster,
but still cannot conclude what caused the
tragedy indeed, Russian Deputy Premier Ilya
Klebanov said here Friday.
2 There has been no final word on what caused 4.2282 N
the submarine to sink while participating
in a major naval exercise, but Defense
Minister Igor Sergeyev said the theory
that Kursk may have collided with another
object is receiving increasingly
concrete confirmation.
3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y
said Thursday that collision with a big
object caused the Kursk nuclear submarine
to sink to the bottom of the Barents Sea.
4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y
said Thursday that collision with a big
object caused the Kursk nuclear submarine
to sink to the bottom of the Barents Sea.
5 President Clinton?s national security adviser, 4.2282 N
Samuel Berger, has provided his Russian
counterpart with a written summary of what
U.S. naval and intelligence officials believe
caused the nuclear-powered submarine Kursk to
sink last month in the Barents Sea, officials
said Wednesday.
Table 8: Top ranked sentences using baseline system
on the question ?What caused the Kursk to sink??.
the same sentence score (similarity to the query), yet
the top ranking two sentences are not actually rele-
vant according to the judges. To contrast, LexRank
achieved a better ranking of the sentences since it is
better able to differentiate between them. It should
be noted that both for the LexRank and baseline sys-
tems, chronological ordering of the documents and
sentences is preserved, such that in cases where two
sentences have the same score, the one published
earlier is ranked higher.
7 Conclusion
We presented topic-sensitive LexRank and applied
it to the problem of sentence retrieval. In a Web-
based news summarization setting, users of our sys-
tem could choose to see the retrieved sentences (as
in Table 9) as a question-focused summary. As in-
dicated in Table 9, each of the top three sentences
were judged by our annotators as providing a com-
plete answer to the respective question. While the
first two sentences provide the same answer (a col-
lision caused the Kursk to sink), the third sentence
provides a different answer (an explosion caused the
disaster). While the last two sentences do not pro-
vide answers according to our judges, they do pro-
vide context information about the situation. Alter-
natively, the user might prefer to see the extracted
921
Rank Sentence Score Relevant?
1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Y
said Thursday that collision with a big
object caused the Kursk nuclear submarine
to sink to the bottom of the Barents Sea.
2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Y
said Thursday that collision with a big
object caused the Kursk nuclear submarine
to sink to the bottom of the Barents Sea.
3 The Russian navy refused to confirm this, 0.0125 Y
but officers have said an explosion in the
torpedo compartment at the front of the
submarine apparently caused the Kursk to sink.
4 President Clinton?s national security adviser, 0.0124 N
Samuel Berger, has provided his Russian
counterpart with a written summary of what
U.S. naval and intelligence officials believe
caused the nuclear-powered submarine Kursk to
sink last month in the Barents Sea, officials
said Wednesday.
5 There has been no final word on what caused 0.0123 N
the submarine to sink while participating
in a major naval exercise, but Defense
Minister Igor Sergeyev said the theory
that Kursk may have collided with another
object is receiving increasingly
concrete confirmation.
Table 9: Top ranked sentences using the
LR[0.20,0.95] system on the question ?What caused
the Kursk to sink??
answers from the retrieved sentences. In this case,
the sentences selected by our system would be sent
to an answer identification component for further
processing. As discussed in Section 2, our goal was
to develop a topic-sensitive version of LexRank and
to use it to improve a baseline system, which had
previously been used successfully for query-based
sentence retrieval (Allan et al, 2003). In terms of
this task, we have shown that over a large set of unal-
tered questions written by our annotators, LexRank
can, on average, outperform the baseline system,
particularly in terms of TRDR scores.
8 Acknowledgments
We would like to thank the members of the CLAIR
group at Michigan and in particular Siwei Shen and
Yang Ye for their assistance with this project.
References
James Allan, Courtney Wade, and Alvaro Bolivar. 2003.
Retrieval and novelty detection at the sentence level.
In SIGIR ?03: Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in informaion retrieval, pages 314?321. ACM
Press.
Enrique Amigo, Julio Gonzalo, Victor Peinado, Anselmo
Pen?as, and Felisa Verdejo. 2004. An Empirical Study
of Information Synthesis Task. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 207?214,
Barcelona, Spain, July.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic. CL, 22(2):249?254.
Gunes Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text.
JAIR, 22:457?479.
Robert Gaizauskas, Mark Hepple, and Mark Greenwood.
2004. Information Retrieval for Question Answering:
a SIGIR 2004 Workshop. In SIGIR 2004 Workshop on
Information Retrieval for Question Answering.
Oren Kurland and Lillian Lee. 2005. PageRank without
hyperlinks: Structural re-ranking using links induced
by language models. In SIGIR 2005, Salvador, Brazil,
August.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford University, Stanford,
CA.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Association for
Computational Linguistics.
Dragomir Radev, Weiguo Fan, Hong Qi, Harris Wu, and
Amardeep Grewal. 2005. Probabilistic Question An-
swering on the Web. Journal of the American So-
ciety for Information Science and Technology, 56(3),
March.
Stephen E. Robertson, Steve Walker, Micheline
Hancock-Beaulieu, Aarron Gull, and Marianna Lau.
1992. Okapi at TREC. In Text REtrieval Conference,
pages 21?30.
G. Salton, J. Allan, and C. Buckley. 1993. Approaches
to Passage REtrieval in Full Text Information Systems.
In Proceedings of the 16th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 49?58.
E. Seneta. 1981. Non-negative matrices and markov
chains. Springer-Verlag, New York.
Ellen Voorhees and Dawn Tice. 2000. The TREC-8
Question Answering Track Evaluation. In Text Re-
trieval Conference TREC-8, Gaithersburg, MD.
Harris Wu, Dragomir R. Radev, and Weiguo Fan.
2004. Towards Answer-focused Summarization Using
Search Engines. New Directions in Question Answer-
ing.
922
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 479?486,
New York, June 2006. c?2006 Association for Computational Linguistics
Language Model-Based Document Clustering Using Random Walks
Gu?nes? Erkan
Department of EECS
University of Michigan
Ann Arbor, MI 48109-2121
gerkan@umich.edu
Abstract
We propose a new document vector represen-
tation specifically designed for the document
clustering task. Instead of the traditional term-
based vectors, a document is represented as an
 
-dimensional vector, where   is the number of
documents in the cluster. The value at each di-
mension of the vector is closely related to the
generation probability based on the language
model of the corresponding document. In-
spired by the recent graph-based NLP methods,
we reinforce the generation probabilities by it-
erating random walks on the underlying graph
representation. Experiments with k-means and
hierarchical clustering algorithms show signif-
icant improvements over the alternative  
vector representation.
1 Introduction
Document clustering is one of the oldest and most studied
problems of information retrieval (van Rijsbergen, 1979).
Almost all document clustering approaches to date have
represented documents as vectors in a bag-of-words vec-
tor space model, where each dimension of a document
vector corresponds to a term in the corpus (Salton and
McGill, 1983). General clustering algorithms are then
applied to these vectors to cluster the given corpus. There
have been attempts to use bigrams or even higher-order n-
grams to represent documents in text categorization, the
supervised counterpart of document clustering, with little
success (Caropreso et al, 2001; Tan et al, 2002).
Clustering can be viewed as partitioning a set of data
objects into groups such that the similarities between the
objects in a same group is high while inter-group simi-
larities are weaker. The fundamental assumption in this
work is that the documents that are likely to have been
generated from similar language models are likely to be
in the same cluster. Under this assumption, we propose a
new representation for document vectors specifically de-
signed for clustering purposes.
Given a corpus, we are interested in the generation
probabilities of a document based on the language models
induced by other documents in the corpus. Using these
probabilities, we propose a vector representation where
each dimension of a document vector corresponds to a
document in the corpus instead of a term in the classical
representation. In other words, our document vectors are
 
-dimensional, where   is the number of documents in
the corpus to be clustered. For the vector   of docu-
ment
	
, the 
 th element of    is closely related to the
generation probability of 	 based on the language model
induced by document  . The main steps of our method
are as follows:
 For each ordered document pair 	    in a given
corpus, we compute the generation probability of

	 from the language model induced by  mak-
ing use of language-model approaches in informa-
tion retrieval (Ponte and Croft, 1998).
 We represent each document by a vector of its gen-
eration probabilities based on other documents? lan-
guage models. At this point, these vectors can be
used in any clustering algorithm instead of the tradi-
tional term-based document vectors.
 Following (Kurland and Lee, 2005), our new doc-
ument vectors are used to construct the underlying
generation graph; the directed graph where docu-
ments are the nodes and link weights are propor-
tional to the generation probabilities.
 We use restricted random walk probabilities to rein-
force the generation probabilities and discover hid-
den relationships in the graph that are not obvious
by the generation links. Our random walk model
is similar to the one proposed by Harel and Kohen
479
(2001) for general spatial data represented as undi-
rected graphs. We have extended their model to the
directed graph case. We use new probabilities de-
rived from random walks as the vector representa-
tion of the documents.
2 Generation Probabilities as Document
Vectors
2.1 Language Models
The language modeling approach to information retrieval
was first introduced by Ponte and Croft (1998) as an al-
ternative (or an improvement) to the traditional   
relevance models. In the language modeling framework,
each document in the database defines a language model.
The relevance of a document to a given query is ranked
according to the generation probability of the query based
on the underlying language model of the document. To
induce a (unigram) language model from a document, we
start with the maximum likelihood (ML) estimation of
the term probabilities. For each term  that occurs in a
document  , the ML estimation of  with respect to 
is defined as
  
 
  Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 747?754,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Adding Syntax to Dynamic Programming for Aligning Comparable Texts
for the Generation of Paraphrases
Siwei Shen1, Dragomir R. Radev1;2, Agam Patel1, Gu?nes? Erkan1
Department of Electrical Engineering and Computer Science
School of Information
University of Michigan
Ann Arbor, MI 48109
fshens, radev, agamrp, gerkang@umich.edu
Abstract
Multiple sequence alignment techniques
have recently gained popularity in the Nat-
ural Language community, especially for
tasks such as machine translation, text
generation, and paraphrase identification.
Prior work falls into two categories, de-
pending on the type of input used: (a)
parallel corpora (e.g., multiple translations
of the same text) or (b) comparable texts
(non-parallel but on the same topic). So
far, only techniques based on parallel texts
have successfully used syntactic informa-
tion to guide alignments. In this paper,
we describe an algorithm for incorporat-
ing syntactic features in the alignment pro-
cess for non-parallel texts with the goal of
generating novel paraphrases of existing
texts. Our method uses dynamic program-
ming with alignment decision based on
the local syntactic similarity between two
sentences. Our results show that syntac-
tic alignment outrivals syntax-free meth-
ods by 20% in both grammaticality and fi-
delity when computed over the novel sen-
tences generated by alignment-induced fi-
nite state automata.
1 Introduction
In real life, we often encounter comparable texts
such as news on the same events reported by dif-
ferent sources and papers on the same topic au-
thored by different people. It is useful to recog-
nize if one text cites another in cases like news
sharing among media agencies or citations in aca-
demic work. Applications of such recognition in-
clude machine translation, text generation, para-
phrase identification, and question answering, all
of which have recently drawn the attention of a
number of researchers in natural language pro-
cessing community.
Multiple sequence alignment (MSA) is the ba-
sis for accomplishing these tasks. Previous work
aligns a group of sentences into a compact word
lattice (Barzilay and Lee, 2003), a finite state au-
tomaton representation that can be used to iden-
tify commonality or variability among compara-
ble texts and generate paraphrases. Nevertheless,
this approach has a drawback of over-generating
ungrammatical sentences due to its ?almost-free?
alignment. Pang et al provide a remedy to this
problem by performing alignment on the Charniak
parse trees of the clustered sentences (Pang et al,
2003). Although it is so far the most similar work
to ours, Pang?s solution assumes the input sen-
tences to be semantically equivalent. Two other
important references for string-based alignments
algorithms, mostly with applications in Biology,
are (Gusfield, 1997) and (Durbin et al, 1998).
In our approach, we work on comparable texts
(not necessarily equivalent in their semantic mean-
ings) as Barzilay and Lee did. However, we use lo-
cal syntactic similarity (as opposed to lexical simi-
larity) in doing the alignment on the raw sentences
instead of on their parse trees. Because of the se-
mantic discrepancies among the inputs, applying
syntactic features in the alignment has a larger im-
pact on the grammaticality and fidelity of the gen-
erated unseen sentences. While previous work po-
sitions the primary focus on the quality of para-
phrases and/or translations, we are more interested
in the relation between the use of syntactic fea-
tures and the correctness of the sentences being
generated, including those that are not paraphrases
of the original input. Figure 1 illustrates the dif-
ference between alignment based solely on lexi-
cal similarity and alignment with consideration of
syntactic features.
Ignoring syntax, the word ?Milan? in both sen-
tences is aligned. But it would unfortunately gen-
erate an ungrammatical sentence ?I went to Mi-
lan is beautiful?. Aligning according to syntac-
747
Start
II
Milan
Milan
wentwent
isis
AcceptAccept
to
to
Milan
beautifulbeautiful Accept
Start
II
Milan
Milan
wentwent
is
is
toto Milan
Milan
Accept
Accept
beautifulbeautiful
Accept
Figure 1: Alignment on lexical similarity and alignment with syntactic features of the sentences ?Milan
is beautiful? and ?I went to Milan?.
tic features, on the other hand, would avoid this
improper alignment by detecting that the syntactic
feature values of the two ?Milan? differ too much.
We shall explain syntactic features and their us-
ages later. In this small example, our syntax-based
alignment will align nothing (the bottom FSA in
Figure 1) since ?Milan? is the only lexically com-
mon word in both sentences. For much larger
clusters in our experiments, we are able to pro-
duce a significant number of novel sentences from
our alignment with such tightened syntactic con-
ditions. Figure 2 shows one of the actual clusters
used in our work that has 18 unique sentences.
Two of the many automatically generated gram-
matical sentences are also shown.
Another piece of related work, (Quirk et al,
2004), starts off with parallel inputs and uses
monolingual Statistical Machine Translation tech-
niques to align them and generate novel sentences.
In our work, the input text does not need to be
nearly as parallel.
The main contribution of this paper is a syntax-
based alignment technique for generating novel
paraphrases of sentences that describe a par-
ticular fact. Such techniques can be poten-
tially useful in multi-document summarizers such
as Newsblaster (http://newsblaster.cs.
columbia.edu) and NewsInEssence (http:
//www.newsinessence.com). Such sys-
tems are notorious for mostly reusing text from
existing news stories. We believe that allowing
them to use novel formulations of known facts will
make these systems much more successful.
2 Related work
Our work is closest in spirit to the two papers that
inspired us (Barzilay and Lee, 2003) and (Pang
et al, 2003). Both of these papers describe how
multiple sequence alignment can be used for ex-
tracting paraphrases from clustered texts. Pang et
al. use as their input the multiple human English
translations of Chinese documents provided by the
LDC as part of the NIST machine translation eval-
uation. Their approach is to merge multiple parse
trees into a single finite state automaton in which
identical input subconstituents are merged while
alternatives are converted to parallel paths in the
output FSA. Barzilay and Lee, on the other hand,
make use of classic techniques in biological se-
quence analysis to identify paraphrases from com-
parable texts (news from different sources on the
same event).
In summary, Pang et al use syntactic align-
ment of parallel texts while Barzilay and Lee
use comparable (not parallel) input but ignore
syntax. Our work differs from the two in that
we apply syntactic information on aligning com-
parable texts and that the syntactic clues we
use are drawn from Chunklink ilk.uvt.nl/
?sabine/homepage/software.html out-
put, which is further analysis from the syntactic
parse trees.
Another related paper using multiple sequence
alignment for text generation was (Barzilay and
Lee, 2002). In that work, the authors were able
to automatically acquire different lexicalizations
of the same concept from ?multiple-parallel cor-
pora?. We also draw some ideas from the Fitch-
Margoliash method for building evolutionary trees
748
1. A police official said it was a Piper tourist plane and that the crash had set the top floors on fire.
2. According to ABCNEWS aviation expert John Nance, Piper planes have no history of mechanical troubles or
other problems that would lead a pilot to lose control.
3. April 18, 2002 8212; A small Piper aircraft crashes into the 417-foot-tall Pirelli skyscraper in Milan,
setting the top floors of the 32-story building on fire.
4. Authorities said the pilot of a small Piper plane called in a problem with the landing gear to the Milan?s
Linate airport at 5:54 p.m., the smaller airport that has a landing strip for private planes.
5. Initial reports described the plane as a Piper, but did not note the specific model.
6. Italian rescue officials reported that at least two people were killed after the Piper aircraft struck the
32-story Pirelli building, which is in the heart of the city s financial district.
7. MILAN, Italy AP A small piper plane with only the pilot on board crashed Thursday into a 30-story landmark
skyscraper, killing at least two people and injuring at least 30.
8. Police officer Celerissimo De Simone said the pilot of the Piper Air Commander plane had sent out a
distress call at 5:50 p.m. just before the crash near Milan?s main train station.
9. Police officer Celerissimo De Simone said the pilot of the Piper aircraft had sent out a distress call at
5:50 p.m. 11:50 a.m.
10. Police officer Celerissimo De Simone said the pilot of the Piper aircraft had sent out a distress
call at 5:50 p.m. just before the crash near Milan?s main train station.
11. Police officer Celerissimo De Simone said the pilot of the Piper aircraft sent out a distress call at
5:50 p.m. just before the crash near Milan?s main train station.
12. Police officer Celerissimo De Simone told The AP the pilot of the Piper aircraft had sent out a distress
call at 5:50 p.m. just before crashing.
13. Police say the aircraft was a Piper tourism plane with only the pilot on board.
14. Police say the plane was an Air Commando 8212; a small plane similar to a Piper.
15. Rescue officials said that at least three people were killed, including the pilot, while dozens were
injured after the Piper aircraft struck the Pirelli high-rise in the heart of the city s financial
district.
16. The crash by the Piper tourist plane into the 26th floor occurred at 5:50 p.m. 1450 GMT on Thursday, said
journalist Desideria Cavina.
17. The pilot of the Piper aircraft, en route from Switzerland, sent out a distress call at 5:54 p.m. just
before the crash, said police officer Celerissimo De Simone.
18. There were conflicting reports as to whether it was a terrorist attack or an accident after the pilot of
the Piper tourist plane reported that he had lost control.
1. Police officer Celerissimo De Simone said the pilot of the Piper aircraft, en route from Switzerland, sent
out a distress call at 5:54 p.m. just before the crash near Milan?s main train station.
2. Italian rescue officials reported that at least three people were killed, including the pilot, while
dozens were injured after the Piper aircraft struck the 32-story Pirelli building, which is in the heart
of the city s financial district.
Figure 2: A comparable cluster of size 18 and 2 novel sentences produced by syntax-based alignment.
described in (Fitch and Margoliash, 1967). That
method and related techniques in Bioinformatics
such as (Felsenstein, 1995) also make use of a sim-
ilarity matrix for aligning a number of sequences.
3 Alignment Algorithms
Our alignment algorithm can be described as mod-
ifying Levenshtein Edit Distance by assigning dif-
ferent scores to lexically matched words according
to their syntactic similarity. And the decision of
whether to align a pair of words is based on such
syntax scores.
3.1 Modified Levenshtein Edit Distance
The Levenshtein Edit Distance (LED) is a mea-
sure of similarity between two strings named after
the Russian scientist Vladimir Levenshtein, who
devised the algorithm in 1965. It is the num-
ber of substitutions, deletions or insertions (hence
?edits?) needed to transform one string into the
other. We extend LED to sentence level by count-
ing the substitutions, deletions and insertions of
words necessary to transform a sentence into the
other. We abbreviate this sentence-level edit dis-
tance as MLED. Similar to LED, MLED compu-
tation produces an M+1 by N+1 distance matrix,
D, given two input sentences of length M and N
respectively. This matrix is constructed through
dynamic programming as shown in Figure 3.
D[i][j] =
8
>
>
<
>
>
:
0 if j = 0
0 if i = 0
max
 
D[i   1][j   1] + match;
D[i   1][j] + gap;
D[i][j   1] + gap
!
otherwise
Figure 3: Dynamic programming in computing
MLED of two sentences of length M and N.
?match? is 2 if the ith word in Sentence 1 and
the jth word in Sentence 2 syntactically match,
and is -1 otherwise. ?gap? represents the score
for inserting a gap rather than aligning, and is set
to -1. The matching conditions of two words are
far more complicated than lexical equality. Rather,
we judge whether two lexically equal words match
based on a predefined set of syntactic features.
The output matrix is used to guide the align-
ment. Starting from the bottom right entry of the
matrix, we go to the matrix entry from which the
value of the current cell is derived in the recursion
of the dynamic programming. Call the current en-
try D[i][j]. If it gets its value from D[i 1][j 1],
the ith word in Sentence 1 and the jth word in Sen-
tence 2 are either aligned or both aligned to a gap
depending on whether they syntactically match; if
the value of D[i][j] is derived from D[i][j   1] +
749
?gap?, the ith word in Sentence 1 is aligned to a
gap inserted into Sentence 2 (the jth word in Sen-
tence 2 is not consumed); otherwise, the jth word
in Sentence 2 is aligned to a gap inserted into Sen-
tence 1.
Now that we know how to align two sentences,
aligning a cluster of sentences is done progres-
sively. We start with the overall most similar pair
and then respect the initial ordering of the cluster,
aligning remaining sentences sequentially. Each
sentence is aligned against its best match in the
pool of already-aligned ones. This approach is
a hybrid of the Feng-Doolittle?s Algorithm (Feng
and Doolittle, 1987) and a variant described in
(Fitch and Margoliash, 1967).
3.2 Syntax-based Alignment
As remarked earlier, our alignment scheme judges
whether two words match according to their
syntactic similarity on top of lexical equality.
The syntactic features are obtained from run-
ning Chunklink (Buchholz, 2000) on the Charniak
parses of the clustered sentences.
3.2.1 Syntactic Features
Among all the information Chunklink provides,
we use in particular the part-of-speech tags, the
Chunk tags, and the syntactic dependence traces.
The Chunk tag shows the constituent of a word
and its relative position in that constituent. It can
take one of the three values,
 ?O? meaning that the word is outside of any
chunk;
 ?I-XP? meaning that this word is inside an
XP chunk where X = N, V, P, ADV, ...;
 ?B-XP? meaning that the word is at the be-
ginning of an XP chunk.
From now on, we shall refer to the Chunk
tag of a word as its IOB value (IOB was named
by Tjong Kim Sang and Jorn Veeenstra (Tjong
Kim Sang and Veenstra, 1999) after Ratnaparkhi
(Ratnaparkhi, 1998)). For example, in the sen-
tence ?I visited Milan Theater?, the IOB value for
?I? is B-NP since it marks the beginning of a noun-
phrase (NP). On the other hand, ?Theater? has an
IOB value of I-NP because it is inside a noun-
phrase (Milan Theater) and is not at the beginning
of that constituent. Finally, the syntactic depen-
dence trace of a word is the path of IOB values
from the root of the tree to the word itself. The
last element in the trace is hence the IOB of the
word itself.
3.2.2 The Algorithm
Lexically matched words but with different
POS are considered not syntactically matched
(e.g., race VB vs. race NN). Hence, our focus
is really on pairs of lexically matched words with
the same POS. We first compare their IOB values.
Two IOB values are exactly matched only if they
are identical (same constituent and same position);
they are partially matched if they share a common
constituent but have different position (e.g., B-PP
vs. I-PP); and they are unmatched otherwise. For
a pair of words with exactly matched IOB values,
we assign 1 as their IOB-score; for those with par-
tially matched IOB values, 0; and -1 for those with
unmatched IOB values. The numeric values of the
score are from experimental experience.
The next step is to compare syntactic depen-
dence traces of the two words. We start with the
second last element in the traces and go backward
because the last one is already taken care of by the
previous step. We also discard the front element of
both traces since it is ?I-S? for all words. The cor-
responding elements in the two traces are checked
by the IOB-comparison described above and the
scores accumulated. The process terminates as
soon as one of the two traces is exhausted. Last,
we adjust down the cumulative score by the length
difference between the two traces. Such final score
is named the trace-score of the two words.
We declare ?unmatched? if the sum of the IOB-
score and the trace-score falls below 0. Otherwise,
we perform one last measurement ? the relative
position of the two words in their respective sen-
tences. The relative position is defined to be the
word?s absolute position divided by the length of
the sentence it appears in (e.g. the 4th word of a
20-word sentence has a relative position of 0.2).
If the difference between two relative positions
is larger than 0.4 (empirically chosen before run-
ning the experiments), we consider the two words
?unmatched?. Otherwise, they are syntactically
matched.
The pseudo-code of checking syntactic match is
shown in Figure 4.
750
Algorithm Check Syntactic Match of Two Words
For a pair of words W
1
, W
2
if W
1
6= W
2
or pos(W
1
) 6= pos(W
2
) then
return ?unmatched?
endif
score := 0
iob
1
:= iob(W
1
)
iob
2
:= iob(W
2
)
score += compare iobs(iob
1
; iob
2
)
trace
1
:= trace(W
1
)
trace
2
:= trace(W
2
)
score += compare traces(trace
1
; trace
2
)
if score < 0 then
return ?unmatched?
endif
relpos
1
:= pos(W
1
)/lengthOf(S
1
)
relpos
2
:= pos(W
2
)/lengthOf(S
2
)
if jrelpos
1
  relpos
2
j  0:4 then
return ?unmatched?
endif
return ?matched?
Function compare iobs(iob
1
; iob
2
)
if iob
1
= iob
2
then
return 1
endif
if substring(iob
1
; 1) = substring(iob
2
; 1) then
return 0
endif
return  1
Function compare traces(trace
1
; trace
2
)
Remove first and last elements from both traces
score := 0
i := lengthOf(trace
1
)   1
j := lengthOf(trace
2
)  1
while i  0 and j  0 do
next := compare iobs(trace
1
[i]; trace
2
[j])
score += next  0:5
i   
j   
endwhile
score   = jlengthOf(trace
1
)  
lengthOf(trace
2
)j  0:5
return score
Figure 4: Algorithm for checking the syntactic
match between two words.
4 Evaluation
4.1 Experimental Setup
4.1.1 Data
The data we use in our experiment come from
a number of sentence clusters on a variety of top-
ics, but all related to the Milan plane crash event.
This cluster was collected manually from the Web
of five different news agencies (ABC, CNN, Fox,
MSNBC, and USAToday). It concerns the April
2002 crash of a small plane into a building in Mi-
lan, Italy and contains a total of 56 documents
published over a period of 1.5 days. To divide this
corpus into representative smaller clusters, we had
a colleague thoroughly read all 56 documents in
the cluster and then create a list of important facts
surrounding the story. We then picked key terms
related to these facts, such as names (Fasulo - the
pilot) and locations (Locarno - the city from which
the plane had departed). Finally, we automatically
clustered sentences based on the presence of these
key terms, resulting in 21 clusters of topically re-
lated (comparable) sentences. The 21 clusters are
grouped into three categories: 7 in training set, 3
in dev-testing set, and the remaining 11 in testing
set. Table 1 shows the name and size of each clus-
ter.
Cluster Number of Sentences
Training clusters
ambulance 10
belie 14
built 6
malpensa 4
piper 18
president 17
route 11
Dev-test clusters
hospital 17
rescue 12
witness 6
Test clusters
accident 30
cause 18
fasulo 33
floor 79
government 22
injur 43
linate 21
rockwell 9
spokes 18
suicide 22
terror 62
Table 1: Experimental clusters.
751
4.1.2 Different Versions of Alignment
To test the usefulness of our work, we ran 5 dif-
ferent alignments on the clusters. The first three
represent different levels of baseline performance
(without syntax consideration) whereas the last
two fully employ the syntactic features but treat
stop words differently. Table 2 describes the 5 ver-
sions of alignment.
Run Description
V1 Lexical alignment on everything possible
V2 Lexical alignment on everything but commas
V3 Lexical alignment on everything but commas and stop words
V4 Syntactic alignment on everything but commas and stop words
V5 Syntactic alignment on everything but commas
Table 2: Alignment techniques used in the experi-
ments.
Alignment Grammaticality Fidelity
V1 2.89 2.98
V2 3.00 2.95
V3 3.15 3.22
V4 3.68 3.59
V5 3.47 3.30
Table 3: Evaluation results on training and dev-
testing clusters. For the results on the test clusters,
see Table 6
The motivation of trying such variations is as
follows. Stop words often cause invalid alignment
because of their high frequencies, and so do punc-
tuations. Aligning on commas, in particular, is
likely to produce long sentences that contain mul-
tiple sentence segments ungrammatically patched
together.
4.1.3 Training and Testing
In order to get the best possible performance
of the syntactic alignment versions, we use clus-
ters in the training and dev-test sets to tune up
the parameter values in our algorithm for check-
ing syntactic match. The parameters in our algo-
rithm are not independent. We pay special atten-
tion to the threshold of relative position difference,
the discount factor of the trace length difference
penalty, and the scores for exactly matched and
partially matched IOB values. We try different pa-
rameter settings on the training clusters, and apply
the top ranking combinations (according to human
judgments described later) on clusters in the dev-
testing set. The values presented in this paper are
the manually selected ones that yield the best per-
formance on the training and dev-testing sets.
Experimenting on the testing data, we have
two hypotheses to verify: 1) the 2 syntactic ver-
sions outperform the 3 baseline versions by both
grammaticality and fidelity (discussed later) of the
novel sentences produced by alignment; and 2)
disallowing alignment on stop words and commas
enhances the performance.
4.2 Experimental Results
For each cluster, we ran the 5 alignment versions
and produce 5 FSA?s. From each FSA (corre-
sponding to a cluster A and alignment version i),
100 sentences are randomly generated. We re-
moved those that appear in the original cluster.
The remaining ones are hence novel sentences,
among which we randomly chose 10 to test the
performance of alignment version i on cluster A.
In the human evaluation, each sentence received
two scores ? grammaticality and fidelity. These
two properties are independent since a sentence
could possibly score high on fidelity even if it is
not fully grammatical. Four different scores are
possible for both criteria: (4) perfect (fully gram-
matical or faithful); (3) good (occasional errors or
quite faithful); (2) bad (many grammar errors or
unfaithful pieces); and (1) nonsense.
4.2.1 Results from the Training Phase
Four judges help our evaluation in the training
phase. They are provided with the original clusters
during the evaluation process, yet they are given
the sentences in shuffled order so that they have
no knowledge about from which alignment ver-
sion each sentence is generated. Table 3 shows
the averages of their evaluation on the 10 clusters
in training and dev-testing set. Each cell corre-
sponds to 400 data points as we presented 10 sen-
tences per cluster per alignment version to each of
the 4 judges (10 x 10 x 4 = 400).
4.2.2 Results from the Testing Phase
After we have optimized the parameter config-
uration for our syntactic alignment in the training
phase, we ask another 6 human judges to evaluate
our work on the testing data. These 6 judges come
from diverse background including Information,
Computer Science, Linguistics, and Bioinformat-
ics. We distribute the 11 testing clusters among
them so that each cluster gets evaluated by at least
3 judges. The workload for each judge is 6 clus-
ters x 5 versions/cluster x 10 sentences/cluster-
version = 300 sentences. Similar to the training
phase, they receive the sentences in shuffled or-
der without knowing the correspondence between
752
sentences and alignment versions. Detailed aver-
age statistics are shown in Table 4 and Table 5 for
grammaticality and fidelity, respectively. Each cell
is the average over 30 - 40 data points, and notice
the last row is not the mean of the other rows since
the number of sentences evaluated for each cluster
varies.
Cluster V1 V2 V3 V4 V5
rockwell 2.27 2.93 3.00 3.60 3.03
cause 2.77 2.83 3.07 3.10 2.93
spokes 2.87 3.07 3.57 3.83 3.50
linate 2.93 3.14 3.26 3.64 3.77
government 2.75 2.83 3.27 3.80 3.20
suicide 2.19 2.51 3.29 3.57 3.11
accident 2.92 3.27 3.54 3.72 3.56
fasulo 2.52 2.52 3.15 3.54 3.32
injur 2.29 2.92 3.03 3.62 3.29
terror 3.04 3.11 3.61 3.23 3.63
floor 2.47 2.77 3.40 3.47 3.27
Overall 2.74 2.75 3.12 3.74 3.29
Table 4: Average grammaticality scores on testing
clusters.
Cluster V1 V2 V3 V4 V5
rockwell 2.25 2.75 3.20 3.80 2.70
cause 2.42 3.04 2.92 3.48 3.17
spokes 2.65 2.50 3.20 3.00 3.05
linate 3.15 3.27 3.15 3.36 3.42
government 2.85 3.24 3.14 3.81 3.20
suicide 2.38 2.69 2.93 3.68 3.23
accident 3.14 3.42 3.56 3.91 3.57
fasulo 2.30 2.48 3.14 3.50 3.48
injur 2.56 2.28 2.29 3.18 3.22
terror 2.65 2.48 3.68 3.47 3.20
floor 2.80 2.90 3.10 3.70 3.30
Overall 2.67 2.69 3.07 3.77 3.23
Table 5: Average fidelity scores on testing clusters.
2.00
2.20
2.40
2.60
2.80
3.00
3.20
3.40
3.60
3.80
4.00
ro
ck
w
ell
ca
us
e
sp
ok
es
lin
ate
go
ve
rn
m
en
t
su
icid
e
ac
cid
en
t
fas
ulo inju
r
ter
ro
r
flo
or
V 1
V 2
V 3
V 4
V 5
Figure 5: Performance of 5 alignment versions by
grammaticality.
2.00
2.20
2.40
2.60
2.80
3.00
3.20
3.40
3.60
3.80
4.00
ro
ck
w
ell
ca
us
e
sp
ok
es
lin
ate
go
ve
rn
m
en
t
su
ici
de
ac
cid
en
t
fas
ulo inju
r
ter
ro
r
flo
or
V 1
V 2
V 3
V 4
V 5
Figure 6: Performance of 5 alignment versions by
fidelity.
4.3 Result Analysis
The results support both our hypotheses. For Hy-
pothesis I, we see that the performance of the
two syntactic alignments was higher than the non-
syntactic versions. In particular, Version 4 outper-
forms the the best baseline version by 19.9% on
grammaticality and by 22.8% on fidelity. Our sec-
ond hypothesis is also verified ? disallowing align-
ment on stop words and commas yields better re-
sults. This is reflected by the fact that Version 4
beats Version 5, and Version 3 wins over the other
two baseline versions by both criteria.
At the level of individual clusters, the syntactic
versions are also found to outrival the syntax-blind
baselines. Applying a t-test on the score sets for
the 5 versions, we can reject the null hypothesis
with 99.5% confidence to ensure that the syntactic
alignment performs better. Similarly, for hypoth-
esis II, the same is true for the versions with and
without stop word alignment. Figures 5 and 6 pro-
vide a graphical view of how each alignment ver-
sion performs on the testing clusters. The clusters
along the x-axis are listed in the order of increas-
ing size.
We have also done an analysis on interjudge
agreement in the evaluation. The judges are in-
structed about the evaluation scheme individually,
and do their work independently. We do not en-
force them to be mutually consistent, as long as
they are self-consistent. However, Table 6 shows
the mean and standard deviation of human judg-
ments (grammaticality and fidelity) on each ver-
sion. The small deviation values indicate a fairly
high agreement.
Finally, because human evaluation is expensive,
we additionally tried to use a language-model ap-
753
Alignment Gr. Mean Gr. StdDev Fi. Mean Fi. StdDev
V1 2.74 0.11 2.67 0.43
V2 2.75 0.08 2.69 0.30
V3 3.12 0.07 3.07 0.27
V4 3.74 0.08 3.77 0.16
V5 3.29 0.16 3.23 0.33
Table 6: Mean and standard deviation of human
judgments.
proach in the training phase for automatic eval-
uation of grammaticality. We have used BLEU
scores(Papineni et al, 2001), but have observed
that they are not consistent with those of human
judges. In particular, BLEU assigns too high
scores to segmented sentences that are otherwise
grammatical. It has been noted in the literature
that metrics like BLEU that are solely based on
N-grams might not be suitable for checking gram-
maticality.
5 Conclusion
In this paper, we presented a paraphrase genera-
tion method based on multiple sequence alignment
which combines traditional dynamic program-
ming techniques with linguistically motivated syn-
tactic information. We apply our work on compa-
rable texts for which syntax has not been success-
fully explored in alignment by previous work. We
showed that using syntactic features improves the
quality of the alignment-induced finite state au-
tomaton when it is used for generating novel sen-
tences. The strongest syntax guided alignment sig-
nificantly outperformed all other versions in both
grammaticality and fidelity of the novel sentences.
In this paper we showed the effectiveness of us-
ing syntax in the alignment of structurally diverse
comparable texts as needed for text generation.
References
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
Lexical Choice via Multiple-Sequence Alignment.
In Proceedings of EMNLP 2002, Philadelphia.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
NAACL-HLT03, Edmonton.
Sabine Buchholz. 2000. Readme
for perl script chunklink.pl.
http://ilk.uvt.nl/ sabine/chunklink/README.html.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis. Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Joseph Felsenstein. 1995. PHYLIP:
Phylogeny Inference Package.
http://evolution.genetics.washington.edu/phylip.html.
DF. Feng and Russell F. Doolittle. 1987. Progres-
sive sequence alignment as a prerequisite to correct
phylogenetic trees. Journal of Molecular Evolution,
25(4).
Walter M. Fitch and Emanuel Margoliash. 1967.
Construction of Phylogenetic Trees. Science,
155(3760):279?284, January.
Dan Gusfield, 1997. Algorithms On Strings: A Dual
View from Computer Science and Computational
Molecular Biology. Cambridge University Press.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations:
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of HLT/NAACL 2003, Ed-
monton, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A Method for Automatic
Evaluation of Machine Translation. Research Re-
port RC22176, IBM.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 142?
149, Barcelona, Spain, July. Association for Com-
putational Linguistics.
A Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Language Ambiguity Resolution. Phd. The-
sis, University of Pennsylvania.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In EACL, pages 173?179.
754
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 45?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LexNet: A Graphical Environment for Graph-Based NLP
Dragomir R. Radev
 
, Gu?nes? Erkan
 
, Anthony Fader

,
Patrick Jordan
 
, Siwei Shen
 
, and James P. Sweeney

Department of Electrical Engineering and Computer Science
School of Information
Department of Mathematics
University of Michigan
Ann Arbor, MI 48109

radev, gerkan, afader, prjordan, shens, jpsweeney@umich.edu
Abstract
This interactive presentation describes
LexNet, a graphical environment for
graph-based NLP developed at the Uni-
versity of Michigan. LexNet includes
LexRank (for text summarization), bi-
ased LexRank (for passage retrieval), and
TUMBL (for binary classification). All
tools in the collection are based on random
walks on lexical graphs, that is graphs
where different NLP objects (e.g., sen-
tences or phrases) are represented as nodes
linked by edges proportional to the lexi-
cal similarity between the two nodes. We
will demonstrate these tools on a variety of
NLP tasks including summarization, ques-
tion answering, and prepositional phrase
attachment.
1 Introduction
We will present a series of graph-based tools for a
variety of NLP tasks such as text summarization,
passage retrieval, prepositional phrase attachment,
and binary classification in general.
Recently proposed graph-based methods
(Szummer and Jaakkola, 2001; Zhu and Ghahra-
mani, 2002b; Zhu and Ghahramani, 2002a;
Toutanova et al, 2004) are particularly well
suited for transductive learning (Vapnik, 1998;
Joachims, 1999). Transductive learning is based
on the idea (Vapnik, 1998) that instead of splitting
a learning problem into two possibly harder
problems, namely induction and deduction, one
can build a model that covers both labeled and
unlabeled data. Unlabeled data are abundant as
well as significantly cheaper than labeled data in
a variety of natural language applications. Parsing
and machine translation both offer examples of
this relationship, with unparsed text from the Web
and untranslated texts being computationally less
costly. These can then be used to supplement
manually translated and aligned corpora. Hence
transductive methods are of great potential for
NLP problems and, as a result, LexNet includes a
number of transductive methods.
2 LexRank: text summarization
LexRank (Erkan and Radev, 2004) embodies the
idea of representing a text (e.g., a document or a
collection of related documents) as a graph. Each
node corresponds to a sentence in the input and the
edge between two nodes is related to the lexical
similarity (either cosine similarity or n-gram gen-
eration probability) between the two sentences.
LexRank computes the steady-state distribution of
the random walk probabilities on this similarity
graph. The LexRank score of each node gives
the probability of a random walk ending up in
that node in the long run. An extractive summary
is generated by retrieving the sentences with the
highest score in the graph. Such sentences typ-
ically correspond to the nodes that have strong
connections to other nodes with high scores in the
graph. Figure 1 demonstrates LexRank.
3 Biased LexRank: passage retrieval
The basic idea behind Biased LexRank is to label
a small number of sentences (or passages) that are
relevant to a particular query and then propagate
relevance from these sentences to other (unanno-
tated) sentences. Relevance propagation is per-
formed on a bipartite graph. In that graph, one
of the modes corresponds to the sentences and
the other ? to certain words from these sentences.
Each sentence is connected to the words that ap-
pear in it. Thus indirectly, each sentence is two
hops away from any other sentence that shares
words in it. Intuitively, the sentences that are
close to the labeled sentences tend to get higher
scores. However, the relevance propagation en-
45
Figure 1: A sample snapshot of LexRank. A 3-
sentence summary is produced from a set of 11
related input sentences. The summary sentences
are shown as larger squares.
ables us to mark certain sentences that are not im-
mediate neighbors of the labeled sentences via in-
direct connections. The effect of the propagation
is discounted by a parameter at each step so that
the relationships between closer nodes are favored
more. Biased LexRank also allows for negative
relevance to be propagated through the network as
the example shows. See Figures 2? 3 for a demon-
stration of Biased LexRank.
Figure 2: Display of Biased LexRank. One sen-
tence at the top is annotated as positive while an-
other at the bottom is marked negative. Sentences
are displayed as circles and the word features are
shown as squares.
Figure 3: After convergence of Biased LexRank.
4 TUMBL: prepositional phrase
attachment
A number of NLP problems such as word sense
disambiguation, text categorization, and extractive
summarization can be cast as classification prob-
lems. This fact is used to great effect in the de-
sign and application of many machine learning
methods used in modern NLP, including TUMBL,
through the utilization of vector representations.
Each object is represented as a vector   of fea-
tures. The main assumption made is that a pair of
objects   and  will be classified the same way
if the distance between them in some space  is
small (Zhu and Ghahramani, 2002a).
This algorithm propagates polarity information
first from the labeled data to the features, capturing
whether each feature is more indicative of posi-
tive class or more negative learned. Such informa-
tion is further transferred to the unlabeled set. The
backward steps update feature polarity with infor-
mation learned from the structure of the unlabeled
data. This process is repeated with a damping fac-
tor to discount later rounds. This process is illus-
tracted in Figure 4. TUMBL was first described
in (Radev, 2004). A series of snapshots showing
TUMBL in Figures 5? 7.
5 Technical information
5.1 Code implementation
The LexRank and TUMBL demonstrations are
provided as both an applet and an application.
The user is presented with a graphical visualiza-
tion of the algorithm that was conveniently de-
veloped using the JUNG API (http://jung.
sourceforge.net/faq.html).
46
(a) Initial graph (b) Forward pass
(c) Backward
pass
(d) Convergence
Figure 4: TUMBL snapshots: the circular vertices
are objects while the square vertices are features.
(a) The initial graph with features showing no bias.
(b) The forward pass where objects propagate la-
bels forward. (c) The backward pass where fea-
tures propagate labels backward. (d) Convergence
of the TUMBL algorithm after successive itera-
tions.
Figure 5: A 10-pp prepositional phrase attachment
problem is displayed. The head of each preposi-
tional phrase is ine middle column. Four types of
features are represented in four columns. The first
column is Noun1 in the 4-tuple. The second col-
umn is Noun2. The first column from the right is
verb of the 4-tuple while the second column from
the right is the actual head of the prepositional
phrase. At this time one positive and one negative
example (high and low attachment) are annotated.
The rest of the circles correspond to the unlabeled
examples.
Figure 6: The final configuration.
47
Figure 7: XML file corresponding to the PP at-
tachment problem. The XML DTD allows layout
information to be encoded along with algorithmic
information such as label and polarity.
In TUMBL, each object is represented by a cir-
cular vertex in the graph and each feature as a
square. Vertices are assigned a color according to
their label. The colors are assignable by the user
and designate the probability of membership of a
class.
To allow for a range of uses, data can be
entered either though the GUI or read in from
an XML file. The schema for TUMBL files is
shown at http://tangra.si.umich.edu/
clair/tumbl.
In the LexRank demo, each sentence becomes a
node. Selected nodes for the summary are shown
in larger size and in blue while the rest are smaller
and drawn in red. The link between two nodes has
a weight proportional to the lexical similarity be-
tween the two corresponding sentences. The demo
also reports the metrics precision, recall, and F-
measure.
5.2 Availability
The demos are available both as locally based and
remotely accessible from http://tangra.
si.umich.edu/clair/lexrank and
http://tangra.si.umich.edu/clair/
tumbl.
6 Acknowledgments
This work was partially supported by the U.S.
National Science Foundation under the follow-
ing two grants: 0329043 ?Probabilistic and link-
based Methods for Exploiting Very Large Textual
Repositories? administered through the IDM pro-
gram and 0308024 ?Collaborative Research: Se-
mantic Entity and Relation Extraction from Web-
Scale Text Document Collections? run by the HLC
program. All opinions, findings, conclusions, and
recommendations in this paper are made by the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
References
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
ICML ?99.
Dragomir Radev. 2004. Weakly supervised graph-
based methods for classification. Technical Report
CSE-TR-500-04, University of Michigan.
Martin Szummer and Tommi Jaakkola. 2001. Partially
labeled classification with Markov random walks. In
NIPS ?01, volume 14. MIT Pres.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk mod-
els for inducing word dependency distributions. In
ICML ?04, New York, New York, USA. ACM Press.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley-Interscience.
Xiaojin Zhu and Zoubin Ghahramani. 2002a. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, CMU-CALD-02-107.
Xiaojin Zhu and Zoubin Ghahramani. 2002b. Towards
semi-supervised classification with Markov random
fields. Technical report, CMU-CALD-02-106.
48
LexPageRank: Prestige in Multi-Document Text Summarization
Gu?nes? Erkan
 
, Dragomir R. Radev
  
 
Department of EECS,

School of Information
University of Michigan

gerkan,radev  @umich.edu
Abstract
Multidocument extractive summarization relies on
the concept of sentence centrality to identify the
most important sentences in a document. Central-
ity is typically defined in terms of the presence of
particular important words or in terms of similarity
to a centroid pseudo-sentence. We are now consid-
ering an approach for computing sentence impor-
tance based on the concept of eigenvector centrality
(prestige) that we call LexPageRank. In this model,
a sentence connectivity matrix is constructed based
on cosine similarity. If the cosine similarity be-
tween two sentences exceeds a particular predefined
threshold, a corresponding edge is added to the con-
nectivity matrix. We provide an evaluation of our
method on DUC 2004 data. The results show that
our approach outperforms centroid-based summa-
rization and is quite successful compared to other
summarization systems.
1 Introduction
Text summarization is the process of automatically
creating a compressed version of a given text that
provides useful information for the user. In this pa-
per, we focus on multi-document generic text sum-
marization, where the goal is to produce a summary
of multiple documents about the same, but unspeci-
fied topic.
Our summarization approach is to assess the cen-
trality of each sentence in a cluster and include the
most important ones in the summary. In Section 2,
we present centroid-based summarization, a well-
known method for judging sentence centrality. Then
we introduce two new measures for centrality, De-
gree and LexPageRank, inspired from the ?prestige?
concept in social networks and based on our new ap-
proach. We compare our new methods and centroid-
based summarization using a feature-based generic
summarization toolkit, MEAD, and show that new
features outperform Centroid in most of the cases.
Test data for our experiments is taken from Docu-
ment Understanding Conferences (DUC) 2004 sum-
marization evaluation to compare our system also
with other state-of-the-art summarization systems.
2 Sentence centrality and centroid-based
summarization
Extractive summarization produces summaries by
choosing a subset of the sentences in the original
documents. This process can be viewed as choosing
the most central sentences in a (multi-document)
cluster that give the necessary and enough amount
of information related to the main theme of the clus-
ter. Centrality of a sentence is often defined in terms
of the centrality of the words that it contains. A
common way of assessing word centrality is to look
at the centroid. The centroid of a cluster is a pseudo-
document which consists of words that have fre-
quency*IDF scores above a predefined threshold. In
centroid-based summarization (Radev et al, 2000),
the sentences that contain more words from the cen-
troid of the cluster are considered as central. For-
mally, the centroid score of a sentence is the co-
sine of the angle between the centroid vector of the
whole cluster and the individual centroid of the sen-
tence. This is a measure of how close the sentence is
to the centroid of the cluster. Centroid-based sum-
marization has given promising results in the past
(Radev et al, 2001).
3 Prestige-based sentence centrality
In this section, we propose a new method to mea-
sure sentence centrality based on prestige in social
networks, which has also inspired many ideas in the
computer networks and information retrieval.
A cluster of documents can be viewed as a net-
work of sentences that are related to each other.
Some sentences are more similar to each other while
some others may share only a little information with
the rest of the sentences. We hypothesize that the
sentences that are similar to many of the other sen-
tences in a cluster are more central (or prestigious)
to the topic. There are two points to clarify in this
definition of centrality. First is how to define sim-
ilarity between two sentences. Second is how to
compute the overall prestige of a sentence given its
similarity to other sentences. For the similarity met-
ric, we use cosine. A cluster may be represented by
a cosine similarity matrix where each entry in the
matrix is the similarity between the corresponding
sentence pair. Figure 1 shows a subset of a cluster
used in DUC 2004, and the corresponding cosine
similarity matrix. Sentence ID d   s  indicates the
 th sentence in the   th document. In the follow-
ing sections, we discuss two methods to compute
sentence prestige using this matrix.
3.1 Degree centrality
In a cluster of related documents, many of the sen-
tences are expected to be somewhat similar to each
other since they are all about the same topic. This
can be seen in Figure 1 where the majority of the
values in the similarity matrix are nonzero. Since
we are interested in significant similarities, we can
eliminate some low values in this matrix by defining
a threshold so that the cluster can be viewed as an
(undirected) graph, where each sentence of the clus-
ter is a node, and significantly similar sentences are
connected to each other. Figure 2 shows the graphs
that correspond to the adjacency matrix derived by
assuming the pair of sentences that have a similarity
above 	
 and  , respectively, in Figure 1 are
similar to each other. We define degree centrality as
the degree of each node in the similarity graph. As
seen in Table 1, the choice of cosine threshold dra-
matically influences the interpretation of centrality.
Too low thresholds may mistakenly take weak simi-
larities into consideration while too high thresholds
may lose much of the similarity relations in a clus-
ter.
ID Degree (0.1) Degree (0.2) Degree (0.3)
d1s1 4 3 1
d2s1 6 2 1
d2s2 1 0 0
d2s3 5 2 0
d3s1 4 1 0
d3s2 6 3 0
d3s3 1 1 0
d4s1 8 4 0
d5s1 4 3 1
d5s2 5 3 0
d5s3 4 1 1
Table 1: Degree centrality scores for the graphs in
Figure 2. Sentence d4s1 is the most central sentence
for thresholds 0.1 and 0.2.
3.2 Eigenvector centrality and LexPageRank
When computing degree centrality, we have treated
each edge as a vote to determine the overall pres-
tige value of each node. This is a totally democratic
method where each vote counts the same. How-
ever, this may have a negative effect in the qual-
ity of the summaries in some cases where several
unwanted sentences vote for each and raise their
prestiges. As an extreme example, consider a noisy
cluster where all the documents are related to each
other, but only one of them is about a somewhat dif-
ferent topic. Obviously, we wouldn?t want any of
the sentences in the unrelated document to be in-
cluded in a generic summary of the cluster. How-
ever, assume that the unrelated document contains
some sentences that are very prestigious consider-
ing only the votes in that document. These sen-
tences will get artificially high centrality scores by
the local votes from a specific set of sentences. This
situation can be avoided by considering where the
votes come from and taking the prestige of the vot-
ing node into account in weighting each vote. Our
approach is inspired by a similar idea used in com-
puting web page prestiges.
One of the most successful applications of pres-
tige is PageRank (Page et al, 1998), the underly-
ing technology behind the Google search engine.
PageRank is a method proposed for assigning a
prestige score to each page in the Web independent
of a specific query. In PageRank, the score of a page
is determined depending on the number of pages
that link to that page as well as the individual scores
of the linking pages. More formally, the PageRank
of a page  is given as follows:
PR 