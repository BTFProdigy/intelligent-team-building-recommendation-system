Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 913?921,
Beijing, August 2010
The Bag-of-Opinions Method for Review Rating Prediction from Sparse
Text Patterns
Lizhen Qu
Max-Planck Institute
for Informatics
lqu@mpii.mpg.de
Georgiana Ifrim
Bioinformatics Research
Centre
ifrim@birc.au.dk
Gerhard Weikum
Max-Planck Institute
for Informatics
weikum@mpii.mpg.de
Abstract
The problem addressed in this paper is to
predict a user?s numeric rating in a prod-
uct review from the text of the review. Un-
igram and n-gram representations of text
are common choices in opinion mining.
However, unigrams cannot capture impor-
tant expressions like ?could have been bet-
ter?, which are essential for prediction
models of ratings. N-grams of words, on
the other hand, capture such phrases, but
typically occur too sparsely in the train-
ing set and thus fail to yield robust pre-
dictors. This paper overcomes the limita-
tions of these two models, by introducing
a novel kind of bag-of-opinions represen-
tation, where an opinion, within a review,
consists of three components: a root word,
a set of modifier words from the same sen-
tence, and one or more negation words.
Each opinion is assigned a numeric score
which is learned, by ridge regression,
from a large, domain-independent cor-
pus of reviews. For the actual test case
of a domain-dependent review, the re-
view?s rating is predicted by aggregat-
ing the scores of all opinions in the re-
view and combining it with a domain-
dependent unigram model. The paper
presents a constrained ridge regression al-
gorithm for learning opinion scores. Ex-
periments show that the bag-of-opinions
method outperforms prior state-of-the-art
techniques for review rating prediction.
1 Introduction
1.1 Motivation
Opinion mining and sentiment analysis has be-
come a hot research area (Pang and Lee, 2008).
There is ample work on analyzing the sentiments
of online-review communities where users com-
ment on products (movies, books, consumer elec-
tronics, etc.), implicitly expressing their opinion
polarities (positive, negative, neutral), and also
provide numeric ratings of products (Titov and
McDonald, 2008b; Lerman et al, 2009; Hu and
Liu, 2004; Titov and McDonald, 2008a; Pang
and Lee, 2005; Popescu and Etzioni, 2005a). Al-
though ratings are more informative than polari-
ties, most prior work focused on classifying text
fragments (phrases, sentences, entire reviews) by
polarity. However, a product receiving mostly 5-
star reviews exhibits better customer purchase be-
havior compared to a product with mostly 4-star
reviews. In this paper we address the learning and
prediction of numerical ratings from review texts,
and we model this as a metric regression problem
over an appropriately defined feature space.
Formally, the input is a set of rated documents
(i.e., reviews), {xi, yi}Ni=1, where xi is a sequence
of word-level unigrams (w1, ..., wl) and yi ? R is
a rating. The goal is to learn a function f(x) that
maps the word vector x into a numerical rating y?,
which indicates both the polarity and strength of
the opinions expressed in a document.
Numerical review rating prediction is harder
than classifying by polarity. Consider the follow-
ing example from Amazon book reviews:
The organization of the book is hard to follow
and the chapter titles are not very helpful, so go-
ing back and trying to find information is quite
913
difficult.
We note that there are many subjective words
(hard, helpful, difficult) modified by opinion mod-
ifiers such as (very, quite) and negation words like
(not). For rating prediction, considering opin-
ion modifiers is crucial; very helpful is a much
stronger sentiment than helpful. Negation words
also need attention. As pointed out by Liu and
Seneff (2009) we cannot simply reverse the polar-
ity. For example, if we assign a higher positive
score to very helpful than to helpful, simply re-
versing the sign of the scores would incorrectly
suggest that not helpful is less negative than not
very helpful.
The widely used unigram (bag-of-words)
model (Pang and Lee, 2005; Snyder and Barzilay,
2007; Goldberg and Zhu, 2006; Ganu et al, 2009)
cannot properly capture phrase patterns. Con-
sider the following example: not so helpful vs.
not so bad. In a unigram-based regression model
each unigram gets a weight indicating its polarity
and strength. High positive/negative weights are
strongly positive/negative clues. It is reasonable
to assign a positive weight to helpful and a nega-
tive weight to bad. The fundamental problem of
unigrams arises when assigning a weight to not.
If not had a strongly negative weight, the posi-
tive weight of helpful would be strongly reduced
while the negative weight of bad would be ampli-
fied (by combining weights). This clearly fails to
capture the true intentions of the opinion phrases.
The same problem holds for so, which is an inten-
sifier that should keep the same sign as the word
it modifies. We refer to this limitation of the uni-
gram model as polarity incoherence.
A promising way of overcoming this weakness
is to include n-grams, generalizing the bag-of-
words model into a bag-of-phrases model (Bac-
cianella et al, 2009; Pang and Lee, 2008). How-
ever, regression models over the feature space
of all n-grams (for either fixed maximal n or
variable-length phrases) are computationally ex-
pensive in their training phase. Moreover and
most importantly for our setting, including n-
grams in the model results in a very high dimen-
sional feature space: many features will then oc-
cur only very rarely in the training data. There-
fore, it is difficult if not impossible to reliably
learn n-gram weights from limited-size training
sets. We refer to this problem as the n-gram spar-
sity bottleneck. In our experiments we inves-
tigate the effect of using bigrams and variable-
length ngrams for improving review rating predic-
tion.
1.2 Contribution
To overcome the above limitations of unigram and
n-gram features, we have developed a novel kind
of bag-of-opinions model, which exploits domain-
independent corpora of opinions (e.g., all Amazon
reviews), but is finally applied for learning predic-
tors on domain-specific reviews (e.g., movies as
rated in IMDB or Rottentomatoes). A document
is represented as a bag of opinions each of which
has three components: a root word, a set of modi-
fier words and one or more negation words. In the
phrase not very helpful, the opinion root is help-
ful, one (of potentially many) opinion modifier(s)
is very, and a negation word is not. We enforce po-
larity coherence by the design of a learnable func-
tion that assigns a score to an opinion.
Our approach generalizes the cumulative linear
offset model (CLO) presented in (Liu and Seneff,
2009). The CLO model makes several restrictive
assumptions, most notably, that all opinion scores
within one document are the same as the overall
document rating. This assumption does not hold
in practice, not even in reviews with extremely
positive/negative ratings. For example, in a 5-
star Amazon review the phrases most impressive
book and it helps explain should receive different
scores. Otherwise, the later transfer step to dif-
ferent domains would yield poor predictions. Due
to this restriction, CLO works well on particular
types of reviews that have pro/con entries listing
characteristic major opinions about the object un-
der review. For settings with individual reviews
whose texts do not exhibit any specific structure,
the CLO model faces its limitations.
In our bag-of-opinions method, we address the
learning of opinion scores as a constrained ridge
regression problem. We consider the opinion
scores in a given review to be drawn from an
unknown probability distribution (so they do not
have to be the same within a document). We es-
timate the review rating based on a set of statis-
914
tics (e.g., expectation, variance, etc.) derived from
the scores of opinions in a document. Thus, our
method has a sound statistical foundation and can
be applied to arbitrary reviews with mixed opin-
ion polarities and strengths. We avoid the n-gram
sparsity problem by the limited-size structured
feature space of (root,modifiers,negators) opin-
ions.
We treat domain-independent and domain-
dependent opinions differently in our system. In
the first step we learn a bag-of-opinions model on
a large dataset of online reviews to obtain scores
for domain-independent opinions. Since the po-
larity of opinions is not bound to a topic, one
can learn opinion scores from a pooled corpus
of reviews for various categories, e.g., movies,
books, etc., and then use these scored opinions
for predicting the ratings of reviews belonging
to a particular category. In order to also capture
domain-dependent information (possibly comple-
mentary to the opinion lexicon used for learn-
ing domain-independent opinions), we combine
the bag-of-opinions model with an unigram model
trained on the domain-dependent corpus. Since
domain-dependent training is typically limited,
we model it using unigram models rather than
bag-of-opinions. By combining the two models,
even if an opinion does not occur in the domain-
dependent training set but it occurs in a test re-
view, we can still accurately predict the review rat-
ing based on the globally learned opinion score. In
some sense our combined learning scheme is sim-
ilar to smoothing in standard learning techniques,
where the estimate based on a limited training
set is smoothed using a large background corpus
(Zhai and Lafferty, 2004).
In summary, the contributions of this paper are
the following:
1. We introduce the bag-of-opinions model, for
capturing the influence of n-grams, but in a
structured way with root words, modifiers,
and negators, to avoid the explosion of the
feature space caused by explicit n-gram mod-
els.
2. We develop a constrained ridge regression
method for learning scores of opinions from
domain-independent corpora of rated re-
views.
3. For transferring the regression model to
newly given domain-dependent applications,
we derive a set of statistics over opinion
scores in documents and use these as fea-
tures, together with standard unigrams, for
predicting the rating of a review.
4. Our experiments with Amazon reviews from
different categories (books, movies, music)
show that the bag-of-opinions method out-
performs prior state-of-the-art techniques.
2 Bag-of-Opinions Model
In this section we first introduce the bag-of-
opinions model, followed by the method for
learning (domain-independent) model parameters.
Then we show how we annotate opinions and how
we adapt the model to domain-dependent data.
2.1 Model Representation
We model each document as a bag-of-opinions
{opk}Kk=1, where the number of opinionsK varies
among documents. Each opinion opk consists
of an opinion root wr, r ? SR, a set of opin-
ion modifiers {wm}Mm=1, m ? SM and a set of
negation words {wz}Zz=1, z ? SZ , where the sets
SR, SM , SZ are component index sets of opinion
roots, opinion modifiers and negation words re-
spectively. The union of these sets forms a global
component index set S ? Nd, where d is the di-
mension of the index space. The opinion root de-
termines the prior polarity of the opinion. Modi-
fiers intensify or weaken the strength of the prior
polarity. Negation words strongly reduce or re-
verse the prior polarity. For each opinion, the
set of negation words consists of at most a nega-
tion valence shifter like not (Kennedy and Inkpen,
2006) and its intensifiers like capitalization of the
valence shifter. Each opinion component is asso-
ciated with a score. We assemble the scores of
opinion elements into an opinion-score by using
a score function. For example, in the opinion not
very helpful, the opinion root helpful determines
the prior polarity positive say with a score 0.9, the
modifier very intensifies the polarity say with a
915
score 0.5. The prior polarity is further strongly re-
duced by the negation word not with e.g., a score
-1.2. Then we sum up the scores to get a score of
0.2 for the opinion not very helpful.
Formally, we define the function score(op) as
a linear function of opinion components, which
takes the form
score(op) = sign(r)?rxr
+
M?
m=1
sign(r)?mxm
+
Z?
z=1
sign(r)?zxz (1)
where {xz, xm, xr} are binary variables denoting
the presence or absence of negation words, modi-
fiers and opinion root. {?z, ?m, ?r} are weights of
each opinion elements. sign(r) : wr ? {?1, 1}
is the opinion polarity function of the opinion root
wr. It assigns a value 1/-1 if an opinion root is
positive/negative. Due to the semantics of opin-
ion elements, we have constraints that ?r ? 0
and ?z ? 0. The sign of ?m is determined in the
learning phase, since we have no prior knowledge
whether it intensifies or weakens the prior polar-
ity.
Since a document is modeled as a bag-of-
opinions, we can simply consider the expec-
tation of opinion scores as the document rat-
ing. If we assume the scores are uniformly dis-
tributed, the prediction function is then f(x) =
1
K
?K
k=1 score(opk) which assigns the average of
opinion scores to the document x.
2.2 Learning Regression Parameters
We assume that we can identify the opinion roots
and negation words from a subjectivity lexicon. In
this work we use MPQA (Wilson et al, 2005). In
addition, the lexicon provides the prior polarity of
the opinion roots. In the training phase, we are
given a set of documents with ratings {xi, yi}Ni=1,
and our goal is to find an optimal function f?
whose predictions {y?i}Ni=1 are as close as possi-
bile to the original ratings {yi}Ni=1. Formally, we
aim to minimize the following loss function:
L = 12N
N?
i=1
(f(xi)? yi)2 (2)
where f(xi) is modeled as the average score of
opinions in review xi.
First, we rewrite score(op) as the dot
product ??,p? between a weight vector
? = [?z,?m, ?r] and a feature vector
p = [sign(r)xz, sign(r)xm, sign(r)xr].
In order to normalize the vectors, we
rewrite the weight and feature vectors in
the d dimensional vector space of all root
words, modifiers and negation words. Then
? = [..,?z, 0, ..,?m, 0, .., ?r, 0..] ? Rd and p =
[sign(r)xz, 0, .., sign(r)xm, 0, .., sign(r)xr, ...] ?
Rd. The function f(xi) can then be written as
the dot product ??,vi?, where vi = 1Ki
?Ki
k=1 pk,
with Ki the number of opinions in review xi.
By using this feature representation, the learning
problem is equivalent to:
min
?
L(?) = 12N
N?
i=1
(??,vi?+ ?0 ? yi)2
s.t.
?z ? 0 z ? SZ
?r ? 0 r ? SR (3)
where ? ? Rd, ? = [?z,?m,?r]. ?0 is the inter-
cept of the regression function, which is estimated
as the mean of the ratings in the training set. We
define a new variable y?i = yi ? ?0.
In order to avoid overfitting, we add an l2 norm
regularizer to the loss function with the parameter
? > 0.
LR(?) = 12N
N?
i=1
(??,vi? ? y?i)2 +
?
2 ? ? ?
2
2
s.t.
?z ? 0 z ? SZ
?r ? 0 r ? SR (4)
We solve the above optimization problem by Al-
gorithm 1 using coordinate descent. The proce-
dure starts with ?0 = 0, ?0 ? Rd. Then it up-
dates iteratively every coordinate of the vector ?
until convergence. Algorithm 1 updates every co-
ordinate ?j , j ? {1, 2, ..., d} of ? by solving the
following one-variable sub-problem:
minlj??j?cjLR(?1, ..., ?j , ..., ?d)
916
where lj and cj denote the lower and upper
bounds of ?j . If j ? SZ , lj = ?? and cj = 0.
If j ? SR, lj = 0 and cj = ?. Otherwise both
bounds are infinity.
According to (Luo and Tseng, 1992), the solu-
tion of this one-variable sub-problem is
??j = max{lj ,min{cj , gj}}
where
gj =
1
N
?N
i=1 vij(y?i ?
?
l 6=j ?lvl)
1
N
?N
i=1 v2ij + ?
Here gj is the close form solution of standard
ridge regression at coordinate j (for details see
(Friedman et al, 2008)). We prove the conver-
gence of Algorithm 1, by the following theorem
using techniques in (Luo and Tseng, 1992).
Theorem 1 A sequence of ? generated by Algo-
rithm 1 globally converges to an optimal solution
?? ? ?? of problem (4), where ?? is the set of
optimal solutions.
Proof: Luo and Tseng (1992) show that coordi-
nate descent for constrained quadratic functions
in the following form converges to one of its global
optimal solutions.
min? h(?) = ??,Q??/2 + ?q,??
s.t. ET? ? b
where Q is a d?d symmetric positive-definite ma-
trix, E is a d? d matrix having no zero column, q
is a d-vector and b is a d-vector.
We rewrite LR in matrix form as
1
2N (y? ?V?)
T (y? ?V?) + ?2?
T?
= 12N (V?)
T (V?) + ?2?
T? ? 12N ((V?)
T y?
? 12N y?
T (V?)) + 12N y?
T y?
= ??,Q??/2 + ?q,??+ constant
where
Q = BTB,B =
[ ?
1
NV?
?Id?d
]
,q = ?1N (V
T y?)
where Id?d is the identity matrix. Because ? >
0, all columns of B are linearly independent. As
Q = BTB and symmetric, Q is positive definite.
We define E as a d ? d diagonal matrix with
all entries on the main diagonal equal to 1 except
eii = ?1, i ? SZ and b is a d-vector with all
entries equal to ?? except bi = 0, for i ? SZ or
i ? SR.
Because the almost cyclic rule is applied to
generate the sequence {?t}, the algorithm con-
verges to a solution ?? ? ??.
Algorithm 1 Constrained Ridge Regression
1: Input: ? and {vn, y?n}Nn=1
2: Output: optimal ?
3: repeat
4: for j = 1, ..., d do
5: gj =
1
N
PN
i=1 vij(y?i?
P
l 6=j ?lvl)
1
N
PN
i=1 v2ij+?6:
??j =
?
?
?
0, if j ? SR and gj < 0
0, if j ? SZ and gj > 0
gj , else
7: end for
8: until Convergence condition is satisfied
2.3 Annotating Opinions
The MPQA lexicon contains separate lexicons for
subjectivity clues, intensifiers and valence shifters
(Wilson et al, 2005), which are used for identify-
ing opinion roots, modifiers and negation words.
Opinion roots are identified as the positive and
negative subjectivity clues in the subjectivity lex-
icon. In the same manner, intensifiers and va-
lence shifters of the type {negation, shiftneg} are
mapped to modifiers and negation words. Other
modifier candidates are adverbs, conjunctions and
modal verbs around opinion roots. We consider
non-words modifiers as well, e.g., punctuations,
capitalization and repetition of opinion roots. If
the opinion root is a noun, adjectives are also in-
cluded into modifier sets.
The automatic opinion annotation starts with
locating the continous subjectivity clue sequence.
Once we find such a sequence and at least one
of the subjectivity clue is positive or negative, we
search to the left up to 4 words for negation words
and modifier candidates, and stop if encountering
another opinion root. Similarly, we search to the
917
right up to 3 unigrams for modifiers and stop if
we find negation words or any other opinion roots.
The prior polarity of the subjectivity sequence is
determined by the polarity of the last subjectivity
clue with either positive or negative polarity in the
sequence. The other subjectivity clues in the same
sequence are treated as modifiers.
2.4 Adaptation to Domain-Dependent Data
The adaptation of the learned (domain-
independent) opinion scores to the target
domain and the integration of domain-dependent
unigrams is done in a second ridge-regression
task. Note that this is a simpler problem than
typical domain-adaptation, since we already know
from the sentiment lexicon which are the domain-
independent features. Additionally, its relatively
easy to obtain a large mixed-domain corpus for
reliable estimation of domain-independent opin-
ion scores (e.g., use all Amazon product reviews).
Furthermore, we need a domain-adaptation step
since domain-dependent and domain-independent
data have generally different rating distributions.
The differences are mainly reflected in the
intercept of the regression function (estimated
as the mean of the ratings). This means that
we need to scale the positive/negative mean of
the opinion scores differently before using it
for prediction on domain-dependent reviews.
Moreover, other statistics further characterize the
opinion score distribution. We use the variance
of opinion scores to capture the reliability of
the mean, multiplied by the negative sign of the
mean to show how much it strengthens/weakens
the estimation of the mean. The mean score of
the dominant polarity (major exp) is also used
to reduce the influence of outliers. Because
positive and negative means should be scaled
differently, we represent positive and negative
values of the mean and major exp as 4 different
features. Together with variance, they are the 5
statistics of the opinion score distribution. The
second learning step on opinion score statistics
and domain-dependent unigrams as features,
re-weights the importance of domain-independent
and domain-dependent information according to
the target domain bias.
3 Experimental Setup
We performed experiments on three target do-
mains of Amazon reviews: books, movies
(DVDs), and music (CDs). For each domain,
we use ca. 8000 Amazon reviews for evalua-
tion; an additional set of ca. 4000 reviews are
withheld for parameter tuning (regularization pa-
rameter, etc.). For learning weights for domain-
independent opinions, we use a mixed-domain
corpus of ca. 350,000 reviews from Amazon
(electronics, books, dvds, etc.); this data is dis-
joint from the test sets and contains no reviews
from the music domain. In order to learn un-
biased scores, we select about the same number
of positive and negative reviews (where reviews
with more/less than 3 stars are regarded as posi-
tive/negative). The regularization parameters used
for this corpus are tuned on withheld data with ca.
6000 thematically mixed reviews.1.
We compare our method, subsequently referred
to as CRR-BoO (Constrained Ridge Regression
for Bag-of-Opinions), to a number of alternative
state-of-the-art methods. These competitors are
varied along two dimensions: 1) feature space,
and 2) training set. Along the first dimension,
we consider a) unigrams coined uni, b) unigrams
and bigrams together, coined uni+bi, c) variable-
length n-grams coined n-gram, d) the opinion
model by (Liu and Seneff, 2009) coined CLO (cu-
mulative linear offset model). As learning pro-
cedure, we use ridge regression for a), b), and
d), and bounded cyclic regression, coined BCR,
for c). Along the second - orthogonal - di-
mension, we consider 3 different training sets:
i) domain-dependent training set coined DD, ii)
the large mixed-domain training set coined MD,
iii) domain-dependent training set and the large
mixed-domain training set coined DD+MD. For
the DD+MD training set, we apply our two stage
approach for CRR-BoO and CLO, i.e., we use
the mixed-domain corpus for learning the opinion
scores in the first stage, and integrate unigrams
from DD in a second domain-adaptation stage.
We train the remaining feature models directly on
the combination of the whole mixed-domain cor-
1All datasets are available from
http://www.mpi-inf.mpg.de/?lqu
918
feature models uni uni+bi n-gram CLO CRR-BoO
DD
book 1.004 0.961 0.997 1.469 0.942
dvd 1.062 1.018 1.054 1.554 0.946
music 0.686 0.672 0.683 0.870 0.638
MD
book 1.696 1.446 1.643 1.714 1.427
dvd 1.919 1.703 1.858 1.890 1.565
music 2.395 2.160 2.340 2.301 1.731
DD+MD
book 1.649 1.403 1.611 1.032 0.884
dvd 1.592 1.389 1.533 1.086 0.928
music 1.471 1.281 1.398 0.698 0.627
Table 1: Mean squared error for rating prediction methods on Amazon reviews.
pus and the training part of DD.
The CLO model is adapted as follows. Since
bags-of-opinions generalize CLO, adjectives and
adverbs are mapped to opinion roots and modi-
fiers, respectively; negation words are treated the
same as CLO. Subsequently we use our regression
technique. As Amazon reviews do not contain pro
and con entries, we learn from the entire review.
For BCR, we adapt the variable-length n-grams
method of (Ifrim et al, 2008) to elastic-net-
regression (Friedman et al, 2008) in order to ob-
tain a fast regularized regression algorithm for
variable-length n-grams. We search for signifi-
cant n-grams by incremental expansion in back-
ward direction (e.g., expand bad to not bad). BCR
pursues a dense solution for unigrams and a sparse
solution for n-grams. Further details on the BCR
learning algorithm will be found on a subsequent
technical report.
As for the regression techniques, we show
only results with ridge regression (for all fea-
ture and training options except BCR). It outper-
formed -support vector regression (SVR) of lib-
svm (Chang and Lin, 2001), lasso (Tibshirani,
1996), and elastic net (Zou and Hastie, 2005) in
our experiments.
4 Results and Discussion
Table 1 shows the mean square error (MSE) from
each of the three domain-specific test sets. The er-
ror is defined as MSE = 1N
?N
i=1(f(xi) ? yi)2.
The right most two columns of the table show re-
sults for the full-fledge two-stage learning for our
method and CLO, with domain-dependent weight
learning and the domain adaptation step. The
other models are trained directly on the given
training sets. For the DD and DD+MD train-
ing sets, we use five-fold cross-validation on the
domain-specific sets. For the MD training set, we
take the domain-specific test sets as hold-out data
for evaluation.
Table 1 clearly shows that our CRR-BoO
method outperforms all alternative methods by a
significant margin. Most noteworthy is the mu-
sic domain, which is not covered by the mixed-
domain corpus. As expected, unigrams only per-
form poorly, and adding bigrams leads only to
marginal improvements. BCR pursues a dense
solution for unigrams and a sparse solution for
variable-length n-grams, but due to the sparsity
of occurence of long n-grams, it filters out many
interesting-but-infrequent ngrams and therefore
performs worse than the dense solution of the
uni+bi model. The CLO method of (Liu and Sen-
eff, 2009) shows unexpectedly poor performance.
Its main limitation is the assumption that opinion
scores are identical within one document. This
does not hold in documents with mixed opinion
polarities. It also results in conflicts for opinion
components that occur in both positive and nega-
tive documents. In contrast, CRR-BoO naturally
captures the mixture of opinions as a bag of pos-
itive/negative scores. We only require that the
mean of opinion scores equals the overall docu-
ment rating.
The right most column of Table 1 shows that
our method can be improved by learning opinion
scores from the large mixed-domain corpus. How-
919
opinion score
good 0.18
recommend 1.64
most difficult -1.66
but it gets very good! 2.37
would highly recommend 2.73
would not recommend -1.93
Table 2: Example opinions learned from the Ama-
zon mixed-domain corpus.
ever, the high error rates of the models learned di-
rectly on the MD corpus show that direct training
on the mixed-domain data can introduce a signifi-
cant amount of noise into the prediction models.
Although the noise can be reduced by learning
from MD and DD together, the performance is
still worse than when learning directly from the
domain-dependent corpora. Additionally, when
the domain is not covered by the mixed-domain
corpus (e.g., music), the results are even worse.
Thus, the two stages of our method (learning
domain-independent opinion scores plus domain-
adaptation) are decisive for a good performance,
and the sentiment-lexicon-based BoO model leads
to robust learning of domain-independent opinion
scores.
Another useful property of BoO is its high in-
terpretability. Table 2 shows example opinion
scores learned from the mixed-domain corpus.
We observe that the scores corelate well with our
intuitive interpretation of opinions.
Our CRR-BoO method is highly scalable.
Excluding the preprocessing steps (same for
all methods), the learning of opinion compo-
nent weights from the ca. 350,000 domain-
independent reviews takes only 11 seconds.
5 Related Work
Rating prediction is modeled as an ordinal re-
gression problem in (Pang and Lee, 2005; Gold-
berg and Zhu, 2006; Snyder and Barzilay, 2007).
They simply use the bag-of-words model with re-
gression algorithms, but as seen previously this
cannot capture the expressive power of phrases.
The resulting models are not highly interpretable.
Baccianella et al (2009) restrict the n-grams to
the ones having certain POS patterns. However,
the long n-grams matching the patterns still suffer
from sparsity. The same seems to hold for sparse
n-gram models (BCR in this paper) in the spirit
of Ifrim et al (2008). Although sparse n-gram
models can explore arbitrarily large n-gram fea-
ture spaces, they can be of little help if the n-grams
of interests occur sparsely in the datasets.
Since our approach can be regarded as learning
a domain-independent sentiment lexicon, it is re-
lated to the area of automatically building domain-
independent sentiment lexicons (Esuli and Sebas-
tiani, 2006; Godbole et al, 2007; Kim and Hovy,
2004). However, this prior work focused mainly
on the opinion polarity of opinion words, neglect-
ing the opinion strength. Recently, the lexicon
based approaches were extended to learn domain-
dependent lexicons (Kanayama and Nasukawa,
2006; Qiu et al, 2009), but these approaches
also neglect the aspect of opinion strength. Our
method requires only the prior polarity of opinion
roots and can thus be used on top of those meth-
ods for learning the scores of domain-dependent
opinion components. The methods proposed in
(Hu and Liu, 2004; Popescu and Etzioni, 2005b)
can also be categorized into the lexicon based
framework because their procedure starts with a
set of seed words whose polarities are propagated
to other opinion bearing words.
6 Conclusion and Future Work
In this paper we show that the bag-of-opinions
(BoO) representation is better suited for captur-
ing the expressive power of n-grams while at the
same time overcoming their sparsity bottleneck.
Although in this paper we use the BoO represen-
tation to model domain-independent opinions, we
believe the same framework can be extended to
domain-dependent opinions and other NLP appli-
cations which can benefit from modelling n-grams
(given that the n-grams are decomposable in some
way). Moreover, the learned model can be re-
garded as a domain-independent opinion lexicon
with each entry in the lexicon having an associated
score indicating its polarity and strength. This in
turn has potential applications in sentiment sum-
marization, opinionated information retrieval and
opinion extraction.
920
References
Baccianella, S., A. Esuli, and F. Sebastiani. 2009.
Multi-facet rating of product reviews. In ECIR.
Springer.
Chang, C.C. and C.J. Lin, 2001. LIBSVM: a
library for support vector machines. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Esuli, A. and F. Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion min-
ing. In LREC, pages 417?422.
Friedman, J., T. Hastie, and R. Tibshirani. 2008.
Regularization paths for generalized linear models
via coordinate descent. Technical report, Techni-
cal Report, Available at http://www-stat. stanford.
edu/jhf/ftp/glmnet. pdf.
Ganu, G., N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In 12th International Workshop on the
Web and Databases.
Godbole, Namrata, Manjunath Srinivasaiah, and
Steven Skiena. 2007. Large-scale sentiment anal-
ysis for news and blogs. In ICWSM.
Goldberg, A. B. and X.J. Zhu. 2006. See-
ing stars when there aren?t many stars: Graph-
based semi-supervised learning for sentiment cat-
egorization. In HLT-NAACL 2006 Workshop on
Textgraphs: Graph-based Algorithms for Natural
Language Processing.
Hu, M.Q. and B. Liu. 2004. Mining and summarizing
customer reviews. In CIKM, pages 168?177. ACM
New York,USA.
Ifrim, G., G. Bakir, and G. Weikum. 2008. Fast logis-
tic regression for text categorization with variable-
length n-grams. In KDD, pages 354?362, New
York,USA. ACM.
Kanayama, H. and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented senti-
ment analysis. In EMNLP, pages 355?363.
Kennedy, A. and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?
125.
Kim, S.M. and E. Hovy. 2004. Determining the senti-
ment of opinions. In COLING, pages 1367?1373.
Lerman, K., S. Blair-Goldensohn, and R. McDonald.
2009. Sentiment summarization: Evaluating and
learning user preferences. In EACL, pages 514?522.
ACL.
Liu, J.J. and S. Seneff. 2009. Review Sentiment
Scoring via a Parse-and-Paraphrase Paradigm. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
161?169. ACL.
Luo, Z.Q. and Q. Tseng. 1992. On the convergence of
the coordinate descent method for convex differen-
tiable minimization. Journal of Optimization The-
ory and Applications, 72(1):7?35.
Pang, B. and L. Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL, page 124. ACL.
Pang, B. and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Popescu, A.M. and O. Etzioni. 2005a. Extracting
product features and opinions from reviews. In
HLT/EMNLP, volume 5, pages 339?346. Springer.
Popescu, A.M. and O. Etzioni. 2005b. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP, volume 5, pages 339?
346. Springer.
Qiu, G., B. Liu, J.J. Bu, and C. Chen. 2009. Ex-
panding Domain Sentiment Lexicon through Dou-
ble Propagation. In IJCAI.
Snyder, B. and R. Barzilay. 2007. Multiple as-
pect ranking using the good grief algorithm. In
NAACL/HLT, pages 300?307.
Tibshirani, R. 1996. Regression shrinkage and selec-
tion via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Titov, I. and R. McDonald. 2008a. A joint model of
text and aspect ratings for sentiment summarization.
In HLT/ACL, pages 308?316.
Titov, I. and R. McDonald. 2008b. Modeling online
reviews with multi-grain topic models. In WWW,
pages 111?120. ACM.
Wilson, T., J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT/ACL, pages 347?354.
Zhai, C. X. and J. Lafferty. 2004. A study of smooth-
ing methods for language models applied to infor-
mation retrieval. ACM Transactions on Information
Systems, 22(2):179?214.
Zou, H. and T. Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of
the Royal Statistical Society Series B(Statistical
Methodology), 67(2):301?320.
921
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 149?159, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Weakly Supervised Model for Sentence-Level Semantic Orientation
Analysis with Multiple Experts
Lizhen Qu and Rainer Gemulla and Gerhard Weikum
Max Planck Institute for Informatics
Saarbru?cken, Germany
{lqu,rgemulla,weikum}@mpi-inf.mpg.de
Abstract
We propose the weakly supervised Multi-
Experts Model (MEM) for analyzing the se-
mantic orientation of opinions expressed in
natural language reviews. In contrast to most
prior work, MEM predicts both opinion po-
larity and opinion strength at the level of in-
dividual sentences; such fine-grained analysis
helps to understand better why users like or
dislike the entity under review. A key chal-
lenge in this setting is that it is hard to ob-
tain sentence-level training data for both po-
larity and strength. For this reason, MEM is
weakly supervised: It starts with potentially
noisy indicators obtained from coarse-grained
training data (i.e., document-level ratings), a
small set of diverse base predictors, and, if
available, small amounts of fine-grained train-
ing data. We integrate these noisy indicators
into a unified probabilistic framework using
ideas from ensemble learning and graph-based
semi-supervised learning. Our experiments in-
dicate that MEM outperforms state-of-the-art
methods by a significant margin.
1 Introduction
Opinion mining is concerned with analyzing opin-
ions expressed in natural language text. For example,
many internet websites allow their users to provide
both natural language reviews and numerical ratings
to items of interest (such as products or movies).
In this context, opinion mining aims to uncover the
relationship between users and (features of) items.
Preferences of users to items can be well understood
by coarse-grained methods of opinion mining, which
focus on analyzing the semantic orientation of doc-
uments as a whole. To understand why users like or
dislike certain items, however, we need to perform
more fine-grained analysis of the review text itself.
In this paper, we focus on sentence-level analy-
sis of semantic orientation (SO) in online reviews.
The SO consists of polarity (positive, negative, or
other1) and strength (degree to which a sentence is
positive or negative). Both quantities can be ana-
lyzed jointly by mapping them to numerical ratings:
Large negative/positive ratings indicate a strong neg-
ative/positive orientation. A key challenge in fine-
grained rating prediction is that fine-grained train-
ing data for both polarity and strength is hard to
obtain. We thus focus on a weakly supervised set-
ting in which only coarse-level training data (such
as document ratings and subjectivity lexicons) and,
optionally, a small amount of fine-grained training
data (such as sentence polarities) is available.
A number of lexicon-based approaches for phrase-
level rating prediction has been proposed in the liter-
ature (Taboada et al2011; Qu et al2010). These
methods utilize a subjectivity lexicon of words along
with information about their semantic orientation;
they focus on phrases that contain words from the
lexicon. A key advantage of sentence-level methods
is that they are able to cover all sentences in a review
and that phrase identification is avoided. To the best
of our knowledge, the problem of rating prediction
at the sentence level has not been addressed in the
literature. A naive approach would be to simply aver-
age phrase-level ratings. Such an approach performs
1We assign polarity other to text fragments that are off-topic
or not directly related to the entity under review.
149
poorly, however, since (1) phrases are analyzed out
of context (e.g., modal verbs or conditional clauses),
(2) domain-dependent information about semantic
orientation is not captured in the lexicons, (3) only
phrases that contain lexicon words are covered. Here
(1) and (2) lead to low precision, (3) to low recall.
To address the challenges outlined above, we pro-
pose the weakly supervised Multi-Experts Model
(MEM) for sentence-level rating prediction. MEM
starts with a set of potentially noisy indicators of SO
including phrase-level predictions, language heuris-
tics, and co-occurrence counts. We refer to these
indicators as base predictors; they constitute the set
of experts used in our model. MEM is designed
such that new base predictors can be easily integrated.
Since the information provided by the base predictors
can be contradicting, we use ideas from ensemble
learning (Dietterichl, 2002) to learn the most con-
fident indicators and to exploit domain-dependent
information revealed by document ratings. Thus, in-
stead of averaging base predictors, MEM integrates
their features along with the available coarse-grained
training data into a unified probabilistic model.
The integrated model can be regarded as a Gaus-
sian process (GP) model (Rasmussen, 2004) with
a novel multi-expert prior. The multi-expert prior
decomposes into two component distributions. The
first component distribution integrates sentence-local
information obtained from the base predictors. It
forms a special realization of stacking (Dzeroski and
Zenko, 2004) but uses the features from the base pre-
dictors instead of the actual predictions. The second
component distribution propagates SO information
across similar sentences using techniques from graph-
based semi-supervised learning (GSSL) (Zhu et al
2003; Belkin et al2006). It aims to improve the
predictions on sentences that are not covered well
enough by our base predictors. Traditional GSSL al-
gorithms support either discrete labels (classification)
or numerical labels (regression); we extend these
techniques to support both types of labels simulta-
neously. We use a novel variant of word sequence
kernels (Cancedda et al2003) to measure sentence
similarity. Our kernel takes the relative positions of
words but also their SO and synonymity into account.
Our experiments indicate that MEM significantly
outperforms prior work in both sentence-level rating
prediction and sentence-level polarity classification.
2 Related Work
There exists a large body of work on analyzing the
semantic orientation of natural language text. Our
approach is unique in that it is weakly supervised,
predicts both polarity and strength, and operates on
the sentence level.
Supervised approaches for sentiment analysis fo-
cus mainly on opinion mining at the document
level (Pang and Lee, 2004; Pang et al2002; Pang
and Lee, 2005; Goldberg and Zhu, 2006), but have
also been applied to sentence-level polarity classifi-
cation in specific domains (Mao and Lebanon, 2006;
Pang and Lee, 2004; McDonald et al2007). In
these settings, a sufficient amount of training data is
available. In contrast, we focus on opinion mining
tasks with little or no fine-grained training data.
The weakly supervised HCRF model (Ta?ckstro?m
and McDonald, 2011b; Ta?ckstro?m and McDonald,
2011a) for sentence-level polarity classification is per-
haps closest to our work in spirit. Similar to MEM,
HCRF uses coarse-grained training data and, when
available, a small amount of fine-grained sentence
polarities. In contrast to MEM, HCRF does not pre-
dict the strength of semantic orientation and ignores
the order of words within sentences.
There exists a large number of lexicon-based meth-
ods for polarity classification (Ding et al2008; Choi
and Cardie, 2009; Hu and Liu, 2004; Zhuang et al
2006; Fu and Wang, 2010; Ku et al2008). The
lexicon-based methods of (Taboada et al2011; Qu
et al2010) also predict ratings at the phrase level;
these methods are used as experts in our model.
MEM leverages ideas from ensemble learning (Di-
etterichl, 2002; Bishop, 2006) and GSSL meth-
ods (Zhu et al2003; Zhu and Ghahramani, 2002;
Chapelle et al2006; Belkin et al2006). We extend
GSSL with support for multiple, heterogenous labels.
This allows us to integrate our base predictors as well
as the available training data into a unified model
that exploits that strengths of algorithms from both
families.
3 Base Predictors
Each of our base predictors predicts the polarity or
the rating of a single phrase. As indicated above,
we do not use these predictions directly in MEM but
instead integrate the features of the base predictors
150
(see Sec. 4.4). MEM is designed such that new base
predictors can be integrated easily.
Our base predictors use a diverse set of available
web and linguistic resources. The hope is that this di-
versity increases overall prediction performance (Di-
etterichl, 2002): The statistical polarity predictor fo-
cuses on local syntactic patterns; it is based on corpus
statistics for SO-carrying words and opinion topic
words. The heuristic polarity predictor uses manu-
ally constructed rules to achieve high precision but
low recall. Both the bag-of-opinions rating predictor
and the SO-CAL rating predictor are based on lexi-
cons. The BoO predictor uses a lexicon trained from
a large generic-domain corpus and is recall-oriented;
the SO-CAL predictor uses a different lexicon with
manually assigned weights and is precision-oriented.
3.1 Statistical Polarity Predictor
The polarity of an SO-carrying word strongly de-
pends on its target word. For example, consider the
phrase ?I began this novel with the greatest of hopes
[...]?. Here, ?greatest? has a positive semantic orien-
tation in all subjectivity lexicons, but the combination
?greatest of hopes? often indicates a negative senti-
ment. We refer to a pair of SO-carrying word (?great-
est?) and a target word (?hopes?) as an opinion-target
pair. Our statistical polarity predictor learns the po-
larity of opinions and targets jointly, which increases
the robustness of its predictions.
Syntactic dependency relations of the form
A
R
?? B are a strong indicator for opinion-target
pairs (Qiu et al2009; Zhuang et al2006); e.g.,
?great?
nmod
?????product?. To achieve high precision,
we only consider pairs connected by the follow-
ing predefined set of shortest dependency paths:
verb
subj
??? noun, verb
obj
?? noun, adj
nmod
???? noun,
adj
prd
??? verb
subj
??? noun. We only retain opinion-
target pairs that are sufficiently frequent.
For each extracted pair z, we count how often it
co-occurs with each document polarity y ? Y , where
Y = {positive, negative, other} denotes the set of po-
larities. If z occurs in a document but is preceded by
a negator, we treat it as a co-occurrence of opposite
document polarity. If z occurs in a document with po-
larity other, we count the occurrence with only half
weight, i.e., we increase both #z and #(other, z)
by 0.5. These documents are typically a mixture of
positive and negative opinions so that we want to
reduce their impact. The marginal distribution of
polarity label y given that z occurs in a sentence is
estimated as P (y | z) = #(y, z)/#z. The predictor
is trained using the text and ratings of the reviews in
the training data, i.e., without relying on fine-grained
annotations.
The statistical polarity predictor can be used to pre-
dict sentence-level polarities by averaging the phrase-
level predictions. As discussed previously, such an
approach is problematic; we use it as a baseline ap-
proach in our experimental study. We also employ
phrase-level averaging to estimate the variance of
base predictors; see Sec. 4.3. Denote by Z(x) the set
of opinion-target pairs in sentence x. To predict the
sentence polarity y ? Y , we take the Bayesian aver-
age of the phrase-level predictors: P (y | Z(x)) =
?
z?Z(x) P (y | z)P (z) =
?
z?Z(x) P (y, z). Thus
the most likely polarity is the one with the highest
co-occurrence count.
3.2 Heuristic Polarity Predictor
Heuristic patterns can also serve as base predictors.
In particular, we found that some authors list positive
and negative aspects separately after keywords such
as ?pros? and ?cons?. A heuristic that exploits such
patterns achieved a high precision (> 90%) but low
recall (< 5%) in our experiments.
3.3 Bag-of-Opinions Rating Predictor
We leverage the bag-of-opinion (BoO) model of Qu et
al. (2010) as a base predictor for phrase-level ratings.
The BoO model was trained from a large generic
corpus without fine-grained annotations.
In BoO, an opinion consists of three components:
an SO-carrying word (e.g., ?good?), a set of intensi-
fiers (e.g., ?very?) and a set of negators (e.g., ?not?).
Each opinion is scored based on these words (repre-
sented as a boolean vector b) and the polarity of the
SO-carrying word (represented as sgn(r) ? {?1, 1})
as indicated by the MPQA lexicon of Wilson et
al. (2005). In particular, the score is computed as
sgn(r)?Tb, where ? is the learned weight vector.
The sign function sgn(r) ensures consistent weight
assignment for intensifiers and negators. For exam-
ple, an intensifier like ?very? can obtain a large posi-
tive or a large negative weight depending on whether
it is used with a positive or negative SO-carrying
151
word, respectively.
3.4 SO-CAL Rating Predictor
The Semantic Orientation Calculator (SO-CAL) of
Taboada et al2011) also predicts phrase-level rat-
ings via a scoring function similar to the one of BoO.
The SO-CAL predictor uses a manually created lexi-
con, in which each word is classified as either an SO-
carrying word (associated with a numerical score), an
intensifier (associated with a modifier on the numer-
ical score), or a negator. SO-CAL employs various
heuristics to detect irrealis and to correct for the pos-
itive bias inherent in most lexicon-based classifiers.
Compared to BoO, SO-CAL has lower recall but
higher precision.
4 Multi-Experts Model
Our multi-experts model incorporates features from
the individual base predictors, coarse-grained labels
(i.e., document ratings or polarities), similarities be-
tween sentences, and optionally a small amount of
sentence polarity labels into an unified probabilistic
model. We first give an overview of MEM, and then
describe its components in detail.
4.1 Model Overview
Denote by X = {x1, . . . ,xN} a set of sentences.
We associate each sentence xi with a set of initial
labels y?i, which are strong indicators of semantic
orientation: the coarse-grained rating of the corre-
sponding document, the polarity label of our heuristic
polarity predictor, the phrase-level ratings from the
SO-CAL predictor, and optionally a manual polarity
label. Note that the number of initial labels may vary
from sentence to sentence and that initial labels are
heterogeneous in that they refer to either polarities
or ratings. Let Y? = {y?1, . . . , y?N}. Our goal is to
predict the unobserved ratings r = {r1, . . . , rN} of
each sentence.
Our multi-expert model is a probabilistic model
for X, Y?, and r. In particular, we model the rating
vector r via a multi-expert prior PE(r | X,?) with
parameter ? (Sec. 4.2). PE integrates both features
from the base predictors and sentence similarities.
We correlate ratings to initial labels via a set of con-
ditional distributions Pb(y?b | r), where b denotes the
type of initial label (Sec. 4.3). The posterior of r is
then given by
P (r | X, Y?,?) ?
?
b
Pb(y?
b | r)PE(r | X,?).
Note that the posterior is influenced by both the multi-
expert prior and the set of initial labels.
We use MAP inference to obtain the most likely
rating of each sentence, i.e., we solve
argmin
r,?
?
?
b
log(Pb(y?
b | r))? log(PE(r | X,?)),
where as before ? denotes the model parameters. We
solve the above optimization problem using cyclic
coordinate descent (Friedman et al2008).
4.2 Multi-Expert Prior
The multi-expert prior PE(r | X,?) consists of two
component distributions N1 and N2. Distribution
N1 integrates features from the base predictors, N2
incorporates sentence similarities to propagate infor-
mation across sentences.
In a slight abuse of notation, denote by xi the set of
features for the i-th sentence. Vector xi contains the
features of all the base predictors but also includes bi-
gram features for increased coverage of syntactic pat-
terns; see Sec. 4.4 for details about the feature design.
Letm(xi) = ?Txi be a linear predictor for ri, where
? is a real weight vector. Assuming Gaussian noise,
ri follows a Gaussian distribution N1(ri | mi, ?2)
with mean mi = m(xi) and variance ?2. Note that
predictor m can be regarded as a linear combination
of base predictors because both m and each of the
base predictors are linear functions. By integrating
all features into a single function, the base predictors
are trained jointly so that weight vector ? automati-
cally adapts to domain-dependent properties of the
data. This integrated approach significantly outper-
formed the alternative approach of using a weighted
vote of the individual predictions made by the base
predictors. We regularize the weight vector ? us-
ing a Laplace prior P (? | ?) with parameter ? to
encourage sparsity.
Note that the bigram features in xi partially cap-
ture sentence similarity. However, such features can-
not be extended to longer subsequences such as tri-
grams due to data sparsity: useful features become
as infrequent as noisy terms. Moreover, we would
152
like to capture sentence similarity using gapped (i.e.,
non-consecutive) subsequences. For example, the
sentences ?The book is an easy read.? and ?It is easy
to read.? are similar but do not share any consecutive
bigrams. They do share the subsequence ?easy read?,
however. To capture this similarity, we make use of a
novel sentiment-augmented variant of word sequence
kernels (Cancedda et al2003). Our kernel is used
to construct a similarity matrix W among sentences
and the corresponding regularized Laplacian L?. To
capture the intuition that similar sentences should
have similar ratings, we introduce a Gaussian prior
N2(r | 0, L??1) as a component into our multi-expert
prior; see Sec. 4.5 for details and a discussion of
why this prior encourages similar ratings for similar
sentences.
Since the two component distributions feature dif-
ferent expertise, we take their product and obtain the
multi-expert prior
PE(r | X,?) ? N1(r |m, I?
2)N2(r | 0, L??1)P (? | ?),
where m = (m1, . . . ,mN ). Note that the normal-
izing constant of PE can be ignored during MAP
inference since it does not depend on ?.
4.3 Incorporating Initial Labels
Recall that the initial labels Y? are strong indica-
tors of semantic orientation associated with each
sentence; they correspond to either discrete polarity
labels or to continuous rating labels. This hetero-
geneity constitutes the main difficulty for incorporat-
ing the initial labels via the conditional distributions
Pb(y?b | r). We assume independence throughout so
that Pb(y?b | r) =
?
i Pb(y?
b
i | ri).
Rating Labels For continuous labels, we assume
Gaussian noise and set Pb(y?bi | ri) = N (y?
b
i | ri, ?
b
i ),
where variance ?bi is a type- and sentence-dependent.
For SO-CAL labels, we simply set ?SO-CALi =
?SO-CAL, where ?SO-CAL is a hyperparameter. The
SO-CAL scores have limited influence in our overall
model; we found that more complex designs lead to
little improvement. We proceed differently for docu-
ment ratings. Our experiment suggests that document
ratings constitute the most important indicator of the
SO of a sentence. Thus sentence ratings should be
close to document ratings unless strong evidence to
the contrary exists. In other words, we want variance
?Doci to be small.
When no manually created sentence-level polar-
ity labels are available, we set the value of ?Doci de-
pending on the polarity class. In particular, we set
?Doci = 1 for both positive and negative documents,
and ?Doci = 2 for neutral documents. The reasoning
behind this choice is that sentence ratings in neu-
tral documents express higher variance because these
documents often contain a mixture of positive and
negative sentences.
When a small set of manually created sentence
polarity labels is available, we train a classifier that
predicts whether the sentence polarity coincides with
the document polarity. If so, we set the corresponding
variance ?Doci to a small value; otherwise, we choose
a larger value. In particular, we train a logistic regres-
sion classifier (Bishop, 2006) using the following
binary features: (1) an indicator variable for each
document polarity, and (2) an indicator variable for
each triple of base predictor, predicted polarity, and
document polarity (set to 1 if the polarities match).
We then set ?Doci = (?pi)
?1, where pi is the probabil-
ity of matching polarities obtained from the classifier
and ? is a hyperparameter that ensures correct scal-
ing.
Polarity Labels We now describe how to model
the correlation between the polarity of a sentence and
its rating. An simple and effective approach is to
partition the range of ratings into three consecutive
partitions, one for each polarity class. We thus consid-
ering the polarity classes {positive, other, negative}
as ordered and formulate polarity classification as an
ordinal regression problem (Chu and Ghahramani,
2006). We immediately obtain the distribution
Pb(y?
b
i = pos | ri) = ?
(
ri ? b+
?
?b
)
Pb(y?
b
i = oth | ri) = ?
(
b+ ? ri
?
?b
)
? ?
(
b? ? ri
?
?b
)
Pb(y?
b
i = neg | ri) = ?
(
b? ? ri
?
?b
)
,
where b+ and b? are the partition boundaries between
positive/other and other/negative, respectively,2 ?(x)
denotes the cumulative distribution function of the
2We set b+ = 0.3 and b? = ?0.3 to calibrate to SO-CAL,
which treats ratings in [?0.3, 0, 3] as polarity other.
153
Figure 1: Distribution of polarity given rating.
Gaussian distribution, and variance ?b is a hyper-
parameter. It is easy to verify that
?
y?bi?Y
p(y?bi |
ri) = 1. The resulting distribution is shown in Fig. 1.
We can use the same distribution to use MEM for
sentence-level polarity classification; in this case, we
pick the polarity with the highest probability.
4.4 Incorporating Base Predictors
Base predictors are integrated into MEM via compo-
nent N1(ri | mi, ?2) of the multi-expert prior (see
Sec. 4.2). Recall that mi is a linear function of the
features xi of each sentence. In this section, we dis-
cuss how xi is constructed from the features of the
base predictors. New base predictors can be inte-
grated easily by exposing their features to MEM.
Most base predictors operate on the phrase level;
our goal is to construct features for the entire sen-
tence. Denote by nbi the number of phrases in the
i-th sentence covered by base predictor b, and let
obij denote a set of associated features. Features o
b
ij
may or may not correspond directly to the features
of base predictor b; see the discussion below. A
straightforward strategy is to set xbi = (n
b
i)
?1?
j o
b
ij .
We proceed slightly differently and average the fea-
tures associated with phrases of positive prior polar-
ity separately from those of phrases with negative
prior polarity (Taboada et al2011). We then con-
catenate the averaged feature vectors, i.e., we set
xbi = (o?
b,pos
ij o?
b,neg
ij ), where o?
b,p
ij denotes the average
of the feature vectors obij associated with phrases of
prior polarity p. This procedure allows us to learn
a different weight for each feature depending on its
context (e.g., the weight of intensifier ?very? may dif-
fer for positive and negative phrases). We construct
xi by concatenating the sentence-level features xbi of
each base predictor and a feature vector of bigrams.
To integrate a base predictor, we only need to
specify the relevant features and, if applicable, prior
phrase polarities. For our choice of base predictors,
we use the following features:
SO-CAL predictor. The prior polarity of a SO-
CAL phrase is given by the polarity of its SO-
carrying word in the SO-CAL lexicon. The feature
vector oSO-CALij consists of the weight of the SO-
carrying word from the lexicon as well the set of
negator words, irrealis marker words, and intensifier
words in the phrase. Moreover, we add the first two
words preceding the SO-carrying word as context
features (skipping nouns, negators, irrealis markers,
and intensifiers, and stopping at clause boundaries).
All words are encoded as binary indicator features.
BoO predictor. Similar to SO-CAL, we deter-
mine the prior polarity of a phrase based on the BoO
dictionary. In contrast to SO-CAL, we directly use
the BoO score as a feature because the BoO predictor
weights have been trained on a very large corpus and
are thus reliable. We also add irrealis marker words
in the form of indicator features.
Statistical polarity predictor. Recall that the sta-
tistical polarity predictor is based on co-occurrence
counts of opinion-topic pairs and document polar-
ities. We treat each opinion-topic pair as a phrase
and use the most frequently co-occurring polarity
as the phrase?s prior polarity. We use the logarithm
of the co-occurrence counts with positive, negative,
and other polarity as features; this set of features per-
formed better than using the co-occurrence counts or
estimated class probabilities directly. We also add
the same type of context features as for SO-CAL, but
rescale each binary feature by the logarithm of the
occurrence count #z of the opinion-topic pair (i.e.,
the features take values in {0, log #z}).
4.5 Incorporating Sentence Similarities
The component distribution N2(r | 0, L??1) in the
multi-expert prior encourages similar sentences to
have similar ratings. The main purpose of N2 is to
propagate information from sentences on which the
base predictors perform well to sentences for which
base prediction is unreliable or unavailable (e.g., be-
154
cause they do not contain SO-carrying words). To
obtain this distribution, we first construct an N ?N
sentence similarity matrix W using a sentiment-
augmented word sequence kernel (see below). We
then compute the regularized graph Laplacian L? =
L+I/?2 based on the unnormalized graph Laplacian
L = D?W (Chapelle et al2006), where D be a
diagonal matrix with dii =
?
j wij and hyperparam-
eter ?2 controls the scale of sentence ratings.
To gain insight into distribution N2, observe that
N2(r | 0, L??1)
? exp
(
?
1
2
?
i,j
wij(ri ? rj)
2 ? ?r?22/?
2
)
.
The left term in the exponent forces the ratings of
similar sentences to be similar: the larger the sen-
tence similarity wij , the more penalty is paid for dis-
similar ratings. For this reason, N2 has a smoothing
effect. The right term is an L2 regularizer and encour-
ages small ratings; it is controlled by hyperparameter
?2.
The entries wij in the sentence similarity matrix
determine the degree of smoothing for each pair of
sentence ratings. We compute these values by a novel
sentiment-augmented word sequence kernel, which
extends the well-known word sequence kernel of Can-
cedda et al2003) by (1) BoO weights to strengthen
the correlation of sentence similarity and rating sim-
ilarity and (2) synonym resolution based on Word-
Net (Miller, 1995).
In general, a word sequence kernel computes a
similarity score of two sequences based on their
shared subsequences. In more detail, we first de-
fine a score function for a pair of shared subse-
quences, and then sum up these scores to obtain
the overall similarity score. Consider for example
the two sentences ?The book is an easy read.? (s1)
and ?It is easy to read.? (s2) along with the shared
subsequence ?is easy read? (u). Observe that the
words ?an? and ?to? serve as gaps as they are not
part of the subsequence. We represent subsequence
u in sentence s via a real-valued projection function
?u(s). In our example, ?u(s1) = ?is?
g
an?easy?read
and ?u(s2) = ?is?easy?
g
to?read. The decay factors
?w ? (0, 1] for matching words characterize the
importance of a word (large values for significant
words). On the contrary, decay factors ?gw ? (0, 1]
for gap words are penalty terms for mismatches
(small values for significant words). The score of
subsequence u is defined as ?u(s1)?u(s2). Thus
two shared subsequences have high similarity if they
share significant words and few gaps. Following Can-
cedda et al2003), we define the similarity between
two sequences as
kn(si, sj) =
?
u??n
?u(si)?u(sj),
where ? is a finite set of words and n denotes the
length of the considered subsequences. This sim-
ilarity function can be computed efficiently using
dynamic programming.
To apply the word sequence kernel, we need to
specify the decay factors. A traditional choice is
?w = log( NNw )/ log(N), where Nw is the document
frequency of the word w and N is the total number
of documents. This IDF decay factor is not well-
suited to our setting: Important opinion words such
as ?great? have a low IDF value due to their high
document frequency. To overcome this problem,
we incorporate additional weights for SO-carrying
words using the BoO lexicon. To do so, we first
rescale the BoO weights into [0, 1] using the sig-
moid g(w) = (1 + exp(?a?w + b))?1, where ?w
denotes the BoO weight of word w.3 We then set
?w = min(log( NNw )/ log(N) + g(w), 0.9). The de-
cay factor for gaps is given by ?gw = 1 ? ?w. Thus
we strongly penalize gaps that consist of infrequent
words or opinion words.
To address data sparsity, we incorporate synonyms
and hypernyms from WordNet into our kernel. In
particular, we represent words found in WordNet by
their first two synset names (for verbs, adjectives,
nouns) and their direct hypernym (nouns only). Two
words are considered the same when their synsets
overlap. Thus, for example, ?writer? has the same
representation as ?author?.
To build the similarity matrix W, we construct
a k-nearest-neighbor graph for all sentences.4 We
consider subsequences consisting of three words (i.e.,
wij = k3(si, sj)); longer subsequences are overly
sparse, shorter subsequences are covered by the bi-
grams features in N1.
3We set a = 2 and b = 1 in our experiments.
4We use k = 15 and only consider neighbors with a similar-
ity above 0.001.
155
5 Experiments
We evaluated both MEM and a number of alternative
approaches for both sentence-level polarity classifi-
cation and sentence-level strength prediction across
a number of domains. We found that MEM out-
performs state-of-the-art approaches by a significant
margin.
5.1 Experimental Setup
We implemented MEM as well as the HCRF classi-
fier of (Ta?ckstro?m and McDonald, 2011a; Ta?ckstro?m
and McDonald, 2011b), which is the best-performing
estimator of sentence-level polarity in the weakly-
supervised setting reported in the literature. We train
both methods using (1) only coarse labels (MEM-
Coarse, HCRF-Coarse) and (2) additionally a small
number of sentence polarities (MEM-Fine, HCRF-
Fine5). We also implemented a number of baselines
for both polarity classification and strength predic-
tion: a document oracle (DocOracle) that simply uses
the document label for each sentence, the BoO rat-
ing predictor (BaseBoO), and the SO-CAL rating pre-
dictor (BaseSO-CAL). For polarity classification, we
compare our methods also to the statistical polarity
predictor (Basepolarity). To judge on the effectiveness
of our multi-export prior for combining base predic-
tors, we take the majority vote of all base predic-
tors and document polarity as an additional baseline
(Majority-Vote). Similarly, for strength prediction,
we take the arithmetic mean of the document rat-
ing and the phrase-level predictions of BaseBoO and
BaseSO-CAL as a baseline (Mean-Rating). We use the
same hyperparameter setting for MEM across all our
experiments.
We evaluated all methods on Amazon reviews
from different domains using the corpus of Ding et al
(2008) and the test set of Ta?ckstro?m and McDonald
(2011a). For each domain, we constructed a large bal-
anced dataset by randomly sampling 33,000 reviews
from the corpus of Ding et al2008). We chose
the books, electronics, and music domains for our
experiments; the dvd domain was used for develop-
ment. For sentence polarity classification, we use the
test set of Ta?ckstro?m and McDonald (2011a), which
5We used the best-performing model that fuses HCRF-Coarse
and the supervised model (McDonald et al2007) by interpola-
tion.
contains roughly 60 reviews per domain (20 for each
polarity). For strength evaluation, we created a test
set of 300 pairs of sentences per domain from the
polarity test set. Each pair consisted of two sentences
of the same polarity; we manually determined which
of the sentences is more positive. We chose this pair-
wise approach because (1) we wanted the evaluation
to be invariant to the scale of the predicted ratings,
and (2) it much easier for human annotators to rank
a pair of sentences than to rank a large collection of
sentences.
We followed Ta?ckstro?m and McDonald (2011b)
and used 3-fold cross-validation, where each fold
consisted of a set of roughly 20 documents from the
test set. In each fold, we merged the test set with the
reviews from the corresponding domain. For MEM-
Fine and HCRF-Fine, we use the data from the other
two folds as fine-grained polarity annotations. For
our experiments on polarity classification, we con-
verted the predicted ratings of MEM, BaseBoO, and
BaseSO-CAL into polarities by the method described
in Sec. 4.3. We compare the performance of each
method in terms of accuracy, which is defined as the
fraction of correct predictions on the test set (correct
label for polarity / correct ranking for strength). All
reported numbers are averages over the three folds. In
our tables, boldface numbers are statistically signifi-
cant against all other methods (t-test, p-value 0.05).
5.2 Results for Polarity Classification
Table 1 summarizes the results of our experiments for
sentence polarity classification. The base predictors
perform poorly across all domains, mainly due to
the aforementioned problems associated with aver-
aging phrase-level predictions. In fact, DocOracle
performs almost always better than any of the base
predictors. However, accurracy increases when we
combine base predictors and DocOracle using ma-
jority voting, which indicates that ensemble methods
work well.
When no fine-grained annotations are available
(HCRF-Coarse, MEM-Coarse), both MEM-Coarse
and Majority-Vote outperformed HCRF-Coarse,
which in turn has been shown to outperform a num-
ber of lexicon-based methods as well as classifiers
trained on document labels (Ta?ckstro?m and McDon-
ald, 2011a). MEM-Coarse also performs better than
Majority-Vote. This is because MEM propagates
156
Book Electronics Music Avg
Basepolarity 43.7 40.3 43.8 42.6
BaseBoO 50.9 48.9 52.6 50.8
BaseSO-CAL 44.6 50.2 45.0 46.6
DocOracle 51.9 49.6 59.3 53.6
Majority-Vote 53.7 53.4 58.7 55.2
HCRF-Coarse 52.2 53.4 57.2 54.3
MEM-Coarse 54.4 54.9 64.5 57.9
HCRF-Fine 55.9 61.0 58.7 58.5
MEM-Fine 59.7 59.6 63.8 61.0
Table 1: Accuracy of polarity classification per do-
main and averaged across domains.
evidence across similar sentences, which is espe-
cially useful when no explicit SO-carrying words
exist. Also, MEM learns weights of features of base
predictors, which leads to a more adaptive integration,
and our ordinal regression formulation for polarity
prediction allows direct competition among positive
and negative evidence for improved accuracy.
When we incorporate a small amount of sentence
polarity labels (HCRF-Fine, MEM-Fine), the accu-
racy of all models greatly improves. HCRF-Fine has
been shown to outperform the strongest supervised
method on the same dataset (McDonald et al2007;
Ta?ckstro?m and McDonald, 2011b). MEM-Fine falls
short of HCRF-Fine only in the electronics domain
but performs better on all other domains. In the book
and music domains, where MEM-Fine is particularly
effective, many sentences feature complex syntac-
tic structure and SO-carrying words are often used
without reference to the quality of the product (but to
describe contents, e.g., ?a love story? or ?a horrible
accident?).
Our models perform especially well when they are
applied to sentences containing no or few opinion
words from lexicons. Table 2 reports the evaluation
results for both sentences containing SO-carrying
words from either MPQA or SO-CAL lexicons and
for sentences containing no such words. The re-
sults explain why our model falls short of HCRF-
Fine in the electronics domain: reviews of electronic
products contain many SO-carrying words, which
almost always express opinions. Nevertheless, MEM-
Fine handles sentences without explicit SO-carrying
words well across all domains; here the propagation
of information across sentences helps to learn the SO
Book Electronics Music
op fact op fact op fact
HCRF-Fine 55.7 55.9 63.3 54.6 59.0 57.4
MEM-Fine 58.9 62.4 60.7 56.7 64.5 60.8
Table 2: Accuracy of polarity classification for sen-
tences with opinion words (op) and without opinion
words (fact).
of facts (such as ?short battery life?).
We found that for all methods, most of the errors
are caused by misclassifying positive/negative sen-
tences as other and vice versa. Moreover, sentences
with polarity opposite to the document polarity are
hard cases if they do not feature frequent strong pat-
terns. Another difficulty lies in off-topic sentences,
which may contain explicit SO-carrying words but
are not related to the item under review. This is one
of the main reasons for the poor performance of the
lexicon-based methods.
Overall, we found that MEM-Fine is the method of
choice. Thus our multi-expert model can indeed bal-
ance the strength of the individual experts to obtain
better estimation accuracy.
5.3 Results for Strength Prediction
Table 3 shows the accuracy results for strength pre-
diction. Here our models outperformed all baselines
by a large margin. Although document ratings are
strong indicators in the polarity classification task,
they lead to worse performance than lexicon-based
methods. The main reason for this drop in accuracy
is that the document oracle assigns the same rating
to all sentences within a review. Thus DocOracle
cannot rank sentences from the same review, which
is a severe limitation. This shortage can be partly
compensated by averaging the base predictions and
document rating (Mean-Rating). Note that it is non-
trivial to apply existing ensemble methods for the
weights of individual base predictors because of the
absence of the sentence ratings as training labels. In
contrast, our MEM models use indirect supervision
to adaptively assign weights to the features from base
predictors. Similar to polarity classification, a small
amount of sentence polarity labels often improved
the performance of MEM.
157
Book Electronics Music Avg
BaseBoO 58.3 51.6 53.5 54.5
BaseSO-CAL 60.6 57.1 47.6 55.1
DocOracle 45.1 36.2 41.4 40.9
Mean-Rating 70.3 57.0 60.8 62.7
MEM-Coarse 68.7 60.5 69.5 66.2
MEM-Fine 72.4 63.3 67.2 67.6
Table 3: Accuracy of strength prediction.
6 Conclusion
We proposed the Multi-Experts Model for analyz-
ing both opinion polarity and opinion strength at
the sentence level. MEM is weakly supervised; it
can run without any fine-grained annotations but is
also able to leverage such annotations when avail-
able. MEM is driven by a novel multi-expert prior,
which integrates a number of diverse base predictors
and propagates information across sentences using a
sentiment-augmented word sequence kernel. Our ex-
periments indicate that MEM achieves better overall
accuracy than alternative methods.
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples.
The Journal of Machine Learning Research, 7:2399?
2434.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning, volume 4. Springer New York.
Nicola Cancedda, E?ric Gaussier, Cyril Goutte, and Jean-
Michel Renders. 2003. Word-sequence kernels. Jour-
nal of Machine Learning Research, 3:1059?1082.
Oliver Chapelle, Bernhard Scho?lkopf, and Alexander Zien.
2006. Semi-Supervised Learning. MIT Press.
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, volume 2, pages 590?598.
Wei Chu and Zoubin Ghahramani. 2006. Gaussian pro-
cesses for ordinal regression. Journal of Machine
Learning Research, 6(1):1019.
Thomas G. Dietterichl. 2002. Ensemble learning. The
Handbook of Brain Theory and Neural Networks, pages
405?408.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the International Conference on Web
Search and Data Mining, pages 231?240.
Saso Dzeroski and Bernard Zenko. 2004. Is combining
classifiers with stacking better than selecting the best
one? Machine Learning, 54(3):255?273.
Jerome H. Friedman, Trevor Hastie, and Rob Tibshirani.
2008. Regularization paths for generalized linear mod-
els via coordinate descent. Technical report.
Guohong Fu and Xin Wang. 2010. Chinese sentence-
level sentiment classification based on fuzzy sets. In
Proceedings of the International Conference on Com-
putational Linguistics, pages 312?319. Association for
Computational Linguistics.
Andrew B. Goldberg and Xiaojun Zhu. 2006. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 168?177.
Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan hua Chen,
and Hsin-Hsi Chen. 2008. Sentence-level opinion anal-
ysis by copeopi in ntcir-7. In Proceedings of NTCIR-7
Workshop Meeting.
Yi Mao and Guy Lebanon. 2006. Isotonic Conditional
Random Fields and Local Sentiment Flow. Advances
in Neural Information Processing Systems, pages 961?
968.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured models
for fine-to-coarse sentiment analysis. In Proceedings of
the Annual Meeting on Association for Computational
Linguistics, volume 45, page 432.
George A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39?41.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Annual
Meeting on Association for Computational Linguistics,
pages 271?278.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics, pages 124?131.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 79?86.
158
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding Domain Sentiment Lexicon through Dou-
ble Propagation. In International Joint Conference on
Artificial Intelligence, pages 1199?1204.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010.
The bag-of-opinions method for review rating predic-
tion from sparse text patterns. In Proceedings of the
International Conference on Computational Linguis-
tics, pages 913?921.
Carl Edward Rasmussen. 2004. Gaussian processes in
machine learning. Springer.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267?307.
Oscar Ta?ckstro?m and Ryan T. McDonald. 2011a. Dis-
covering Fine-Grained Sentiment with Latent Variable
Structured Prediction Models. In Proceedings of the
European Conference on Information Retrieval, pages
368?374.
Oscar Ta?ckstro?m and Ryan T. McDonald. 2011b. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 569?574.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Human Language
Technology Conference and the Conference on Empir-
ical Methods in Natural Language Processing, pages
347?354.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propagation.
Technical report.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian fields
and harmonic functions. In Proceedings of the Inter-
national Conference on Machine Learning, pages 912?
919.
Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the ACM international conference on Information and
knowledge management, pages 43?50.
159
Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction
with Latent Structural SVM
Lizhen Qu
Max Planck Institute
for Informatics
lqu@mpi-inf.mpg.de
Yi Zhang
Nuance Communications
yi.zhang@nuance.com
Rui Wang
DFKI GmbH
mars198356@hotmail.com
Lili Jiang
Max Planck Institute
for Informatics
ljiang@mpi-inf.mpg.de
Rainer Gemulla
Max Planck Institute
for Informatics
rgemulla@mpi-inf.mpg.de
Gerhard Weikum
Max Planck Institute
for Informatics
weikum@mpi-inf.mpg.de
Abstract
Extracting instances of sentiment-oriented re-
lations from user-generated web documents is
important for online marketing analysis. Un-
like previous work, we formulate this extrac-
tion task as a structured prediction problem
and design the corresponding inference as an
integer linear program. Our latent structural
SVM based model can learn from training cor-
pora that do not contain explicit annotations of
sentiment-bearing expressions, and it can si-
multaneously recognize instances of both bi-
nary (polarity) and ternary (comparative) re-
lations with regard to entity mentions of in-
terest. The empirical evaluation shows that
our approach significantly outperforms state-
of-the-art systems across domains (cameras
and movies) and across genres (reviews and
forum posts). The gold standard corpus that
we built will also be a valuable resource for
the community.
1 Introduction
Sentiment-oriented relation extraction (Choi et al.,
2006) is concerned with recognizing sentiment po-
larities and comparative relations between entities
from natural language text. Identifying such rela-
tions often requires syntactic and semantic analysis
at both sentence and phrase level. Most prior work
on sentiment analysis consider either i) subjective
sentence detection (Yu and K?bler, 2011), ii) po-
larity classification (Johansson and Moschitti, 2011;
Wilson et al., 2005), or iii) comparative relation
identification (Jindal and Liu, 2006; Ganapathib-
hotla and Liu, 2008). In practice, however, differ-
ent types of sentiment-oriented relations frequently
coexist in documents. In particular, we found that
more than 38% of the sentences in our test corpus
contain more than one type of relations. The iso-
lated analysis approach is inappropriate because i) it
sacrifices acuracy by ignoring the intricate interplay
among different types of relations; ii) it could lead to
conflicting predictions such as estimating a relation
candidate as both negative and comparative. There-
fore, in this paper, we identify instances of both sen-
timent polarities and comparative relations for enti-
ties of interest simultaneously. We assume that all
the mentions of entities and attributes are given, and
entities are disambiguated. It is a widely used as-
sumption when evaluating a module in a pipeline
system that the outputs of preceding modules are
error-free.
To the best of our knowledge, the only exist-
ing system capable of extracting both comparisons
and sentiment polarities is a rule-based system pro-
posed by Ding et al. (2009). We argue that it is
better to tackle the task by using a unified model
with structured outputs. It allows us to consider a
set of correlated relation instances jointly and char-
acterize their interaction through a set of soft and
hard constraints. For example, we can encode con-
straints to discourage an attribute to participate in
a polarity relation and a comparative relation at the
same time. As a result, the system extracts a set of
correlated instances of sentiment-oriented relations
from a given sentence. For example, with the sen-
tence about the camera Canon 7D, ?The sensor is
great, but the price is higher than Nikon D7000.?
the expected output is positive(Canon 7D, sensor)
155
Transactions of the Association for Computational Linguistics, 2 (2014) 155?168. Action Editor: Janyce Wiebe.
Submitted 6/2013; Revised 11/2013; Published 4/2014. c?2014 Association for Computational Linguistics.
and preferred(Nikon D7000, Canon 7D, textit-
price).
However, constructing a fully annotated train-
ing corpus for this task is labor-intensive and re-
quires strong linguistic background. We minimize
this overhead by applying a simplified annotation
scheme, in which annotators mark mentions of en-
tities and attributes, disambiguate the entities, and
label instances of relations for each sentence. Based
on the new scheme, we have created a small Senti-
ment Relation Graph (SRG) corpus for the domains
of cameras and movies, which significantly differs
from the corpora used in prior work (Wei and Gulla,
2010; Kessler et al., 2010; Toprak et al., 2010;
Wiebe et al., 2005; Hu and Liu, 2004) in the follow-
ing ways: i) both sentiment polarities and compar-
ative relations are annotated; ii) all mentioned en-
tities are disambiguated; and iii) no subjective ex-
pressions are annotated, unless they are part of entity
mentions.
The new annotation scheme raises a new chal-
lenge for learning algorithms in that they need to
automatically find textual evidences for each anno-
tated relation during training. For example, with the
sentence ?I like the Rebel a little better, but that is
another price jump?, simply assigning a sentiment-
bearing expression to the nearest relation candidate
is insufficient, especially when the sentiment is not
explicitly expressed.
In this paper, we propose SENTI-LSSVM, a latent
structural SVM based model for sentiment-oriented
relation extraction. SENTI-LSSVM is applied to find
the most likely set of the relation instances expressed
in a given sentence, where the latent variables are
used to assign the most appropriate textual evidences
to the respective instances.
In summary, the contributions of this paper are the
following:
? We propose SENTI-LSSVM: the first unified sta-
tistical model with the capability of extracting
instances of both binary and ternary sentiment-
oriented relations.
? We design a task-specific integer linear pro-
gramming (ILP) formulation for inference.
? We construct a new SRG corpus as a valuable
asset for the evaluation of sentiment relation
extraction.
? We conduct extensive experiments with on-
line reviews and forum posts, showing that
SENTI-LSSVM model can effectively learn from
a training corpus without explicitly annotated
subjective expressions and that its performance
significantly outperforms state-of-the-art sys-
tems.
2 Related Work
There are ample works on analyzing sentiment po-
larities and entity comparisons, but the majority of
them studied the two tasks in isolation.
Most prior approaches for fine-grained sentiment
analysis focus on polarity classification. Super-
vised approaches on expression-level analysis re-
quire the annotation of sentiment-bearing expres-
sions as training data (Jin et al., 2009; Choi
and Cardie, 2010; Johansson and Moschitti, 2011;
Yessenalina and Cardie, 2011; Wei and Gulla,
2010). However, the corresponding annotation pro-
cess is time-consuming. Although sentence-level
annotations are easier to obtain, the analysis at this
level cannot cope with sentences conveying relations
of multiple types (McDonald et al., 2007; T?ckstr?m
and McDonald, 2011; Socher et al., 2012). Lexicon-
based approaches require no training data (Ku et al.,
2006; Kim and Hovy, 2006; Godbole et al., 2007;
Ding et al., 2008; Popescu and Etzioni, 2005; Liu et
al., 2005) but suffer from inferior performance (Wil-
son et al., 2005; Qu et al., 2012). In contrast, our
method requires no annotation of sentiment-bearing
expressions for training and can predict both senti-
ment polarities and comparative relations.
Sentiment-oriented comparative relations have
been studied in the context of user-generated dis-
course (Jindal and Liu, 2006; Ganapathibhotla and
Liu, 2008). Approaches rely on linguistically moti-
vated rules and assume the existence of independent
keywords in sentences which indicate comparative
relations. Therefore, these methods fall short of ex-
tracting comparative relations based on domain de-
pendent information.
Both Johansson and Moschitti (2011) and Wu et
al. (2011) formulate fine-grained sentiment analy-
sis as a learning problem with structured outputs.
However, they focus only on polarity classification
156
of expressions and require annotation of sentiment-
bearing expressions for training as well.
While ILP has been previously applied for infer-
ence in sentiment analysis (Choi and Cardie, 2009;
Somasundaran and Wiebe, 2009; Wu et al., 2011),
our task requires a complete ILP reformulation due
to 1) the absence of annotated sentiment expressions
and 2) the constraints imposed by the joint extrac-
tion of both sentiment polarity and comparative re-
lations.
3 System Overview
This section gives an overview of the whole system
for extracting sentiment-oriented relation instances.
Prior to presenting the system architecture, we in-
troduce the essential concepts and the definitions of
two kinds of directed hypergraphs as the represen-
tation of correlated relation instances extracted from
sentences.
3.1 Concepts and Definitions
Entity. An entity is an abstract or concrete thing,
which needs not be of material existence. An entity
in this paper refers to either a product or a brand.
Attribute. An attribute is an object closely associ-
ated with or belonging to an entity, such as the lens
of digital camera.
Sentiment-Oriented Relation. A sentiment-
oriented relation is either a sentiment polarity or a
comparative relation, defined on tuples of entities
and attributes. A sentiment polarity relation conveys
either a positive or a negative attitude towards enti-
ties or their attributes, whereas a comparative rela-
tion indicates the preference of one entity over the
other entity w.r.t. an attribute.
Relation Instance. An instance of sentiment polar-
ity takes the form r(entity, attribute) with r ? {pos-
itive, negative}, such as positive(Canon 7D, sen-
sor). The polarity instances expressed in the form
of unary relations, such as ?Nikon D7000 is ex-
cellent.?, are denoted as binary relations r(entity,
whole), where the attribute whole indicates the en-
tity as a whole. In contrast, an instance of compar-
ative relation is in the form of preferred{entity, en-
tity, attribute}, e.g. preferred(Canon 7D, Nikon
D7000, price). For brevity, we refer to an instance
set of sentiment-oriented relations extracted from a
sentence as an sSoR. To represent the instances
of the remaining relations, we represent them as
other{entity, attribute}, such as textitpartOf{wheel,
car}. These relations include objective relations
and the subjective relations other than sentiment-
oriented relations.
Mention-Based Relation Instances. A mention-
based relation instance refers to a tuple of entity
mentions with a certain relation. This concept is in-
troduced as the representation of instances in a sen-
tence by replacing entities with the corresponding
entity mentions, such as positive(?Canon SD880i?,
?wide angle view?).
Figure 1: An example of MRG.
Mention-Based Relation Graph. A mention-based
relation graph (or MRG ) represents a collection of
mention-based relation instances expressed in a sen-
tence. As illustrated in Figure 1, an MRG is a di-
rected hypergraph G = ?M,E? with a vertex set
M and an edge set E. A vertex mi ? M denotes
a mention of an entity or an attribute occurring ei-
ther within the sentence or in its context. We say
that a mention is from the context if it is mentioned
in the previous sentence or is an attribute implied
in the current sentence. An instance of a binary re-
lation in an MRG takes the form of a binary edge
el = (mi,ma), where mi and ma denote an en-
tity mention and an attribute mention respectively,
and the type l ? {positive, negative, other}. A
ternary edge el indicating comparative relation is
represented as el = (mi,mj ,ma), where two en-
tity mentions mi and mj are compared with respect
to the attribute mention ma. We define the type
l ? {better,worse} to indicate two possible direc-
tions of the relation and assume mi occurs before
mj . As a result, we have a set L of five relation
types: positive, negative, better, worse or other. Ac-
cording to these definitions, the annotations in the
SRG corpus are actually MRGs and disambiguated
entities. If there are multiple mentions referring to
the same entity, annotators are asked to choose the
157
most obvious one because it saves annotation time
and is less demanding for the entity recognition and
diambiguation modules.
Figure 2: An example of eMRG. The textual evi-
dences are wrapped by green dashed boxes.
Evidentiary Mention-Based Relation Graph. An
evidentiary mention-based relation graph, coined
eMRG , extends an MRG by associating each edge
with a textual evidence to support the corresponding
relation assertions (see Figure 2). Consequently, an
edge in an eMRG is denoted by a pair (a, c), where
a represents a mention-based relation instance and
c is the associated textual evidence. It is also re-
ferred to as an evidentiary edge. represented as
el = (mi,mj ,ma), an MRG as an evidentiary MRG
(eMRG) and the edges of eMRGs as evidentiary
edges, as shown in Figure 2.
3.2 System Architecture
Figure 3: System architecture.
As illustrated by Figure 3, at the core of our sys-
tem is the SENTI-LSSVM model, which extracts sets
of mention-based relationships in the form of eMRGs
from sentences. For a given sentence with known
entity mentions, we select all possible mention sets
as relation candidates, where each set includes at
least one entity mention. Then we associate each
relation candidate with a set of constituents or the
whole sentence as the textual evidence candidates
(cf. Section 6.1). Subsequently, the inference com-
ponent aims to find the most likely eMRG from all
possible combinations of mention-based relation in-
stances and their textual evidences (cf. Section 6.2).
The representation eMRG is chosen because it char-
acterizes exactly the model outputs by letting each
edge correspond to an instance of mention-based re-
lation and the associated textual evidence. Finally,
the model parameters of this model are learned by
an online algorithm (cf. Section 7).
Since instance sets of sentiment-oriented relations
(sSoRs) are the expected outputs, we can obtain
sSoRs from MRGs by using a simple rule-based al-
gorithm. The algorithm essentially maps the men-
tions from an MRG into entities and attributes in an
sSoR and label the corresponding tuples with the re-
lation types of the edges from an MRG. For instances
of comparative relation, the label better or worse is
mapped to the relation type preferred.
4 SENTI-LSSVM Model
The task of sentiment-oriented relation extraction
is to determine the most likely sSoR in a sentence.
Since sSoRs are derived from the corresponding
MRGs as described in Section 3, the task is reduced
to find the most likely MRG for each sentence. Since
an MRG is created by assigning relation types to a
subset of all relation candidates, which are possible
tuples of mentions with unknown relation types, the
number of MRGs can be extremely high.
To tackle the task, one solution is to employ
an edge-factored linear model in the framework of
structural SVM (Martins et al., 2009; Tsochantaridis
et al., 2004). The model suggests that a bag of fea-
tures should be specified for each relation candidate,
and then the model predicts the most likely candi-
date sets along with their relation types to form the
optimal MRGs. As we observed, for a relation can-
didate, the most informative features are the words
near its entity mentions in the original text. How-
158
ever, if we represent a candidate by all these words,
it is very likely that the instances of different relation
types share overly similar features, because a men-
tion is often involved in more than one relation can-
didate, as shown in Figure 2. As a consequence, the
instances of different relations represented by overly
similar features can easily confuse the learning algo-
rithm. Thus, it is critical to select proper constituents
or sentences as textual evidences for each relation
candidate in both training and testing.
Consequently, we divide the task of sentiment-
oriented relation extraction into two subtasks : i)
identifying the most likely MRGs; ii) assigning
proper textual evidences to each edge of MRGs to
support their relation assertions. It is desirable to
carry out the two subtasks jointly as these two sub-
tasks could enhance each other. First, the identifi-
cation of relation types requires proper textual ev-
idences; second, the soft and hard constraints im-
posed by the correlated relation instances facilitate
the recognition of the corresponding textual evi-
dences. Since the eMRGs are created by attaching
every MRG with a set of textual evidences, tackling
the two subtasks simultaneously is equivalent to se-
lecting the most likely eMRG from a set of eMRG
candidates. It is challenging because our SRG corpus
does not contain any annotation of textual evidences.
Formally, let X denote the set of all available sen-
tences, and we define y ? Y(x)(x ? X ) as the set
of labeled edges of an MRG and Y = ?x?XY(x).
Since the assignments of textual evidences are not
observed, an assignment of evidences to y is de-
noted by a latent variable h ? H(x) and H =
?x?XH(x). Then (y, h) corresponds to an eMRG,
and (a, c) ? (y, h) is a labeled edge a attached
with a textual evidence c. Given a labeled dataset
D = {(x1, y1), ..., (xn, yn)} ? (X ? Y)n, we aim
to learn a discriminant function f : X ? Y?H that
outputs the optimal eMRG (y, h) ? Y(x)?H(x) for
a given sentence x.
Due to the introduction of latent variables, we
adopt the latent structural SVM (Yu and Joachims,
2009) for structural classification. Our discriminant
function is defined as
f(x) = argmax(y,h)?Y(x)?H(x)?>?(x, y, h) (1)
where ?(x, y, h) is the feature function of an eMRG
(y, h) and ? is the corresponding weight vector.
To ensure tractability, we also employ edge-based
factorization for our model. Let Mp denote a set of
entity mentions and yr(mi) be a set of edges labeled
with sentiment-oriented relations incident to mi, the
factorization of ?(x, y, h) is given as
?(x, y, h) =
?
(a,c)?(y,h)
?e(x, a, c) + (2)
?
mi?Mp
?
a,a??yr(mi),a 6=a?
?c(a, a?)
where ?e(x, a, c) is a local edge feature function
for a labeled edge a attached with a textual evidence
c and ?c(a, a?) is a feature function capturing co-
occurrence of two labeled edges ami and a?mi inci-dent to an entity mention mi.
5 Feature Space
The following features are used in the feature func-
tions (Equation 2):
Unigrams: As mentioned before, a textual evi-
dence attached to an edge in MRG is either a word,
phrase or sentence. We consider all lemmatized un-
igrams in the textual evidence as unigram features.
Context: Since web users usually express related
sentiments about the same entity across sentence
boundaries, we describe the sentiment flow using a
set of contextual binary features. For example, if en-
tity A is mentioned in both the previous sentence and
the current sentence, a set of contextual binary fea-
tures are used to indicate all possible combinations
of the current and the previous mentioned sentiment-
oriented relations regarding to entity A.
Co-occurrence: We have mentioned the co-
occurrence feature in Equation 2, indicated by
?c(a, a?). It captures the co-occurrence of two la-
beled edges incident to the same entity mention.
Note that the co-occurrence feature function is con-
sidered only if there is a contrast conjunction such as
?but? between the non-shared entity mentions inci-
dent to the two labeled edges.
Senti-predictors: Following the idea of (Qu et
al., 2012), we encode the prediction results from
the rule-based phrase-level multi-relation predic-
tor (Ding et al., 2009) and from the bag-of-opinions
predictor (Qu et al., 2010) as features based on the
textual evidence. The output of the first predictor
is an integer value, while the output of the second
predictor is a sentiment relation, such as ?positive?,
159
?negative?, ?better? or ?worse?. We map the rela-
tional outputs into integer values and then encode
the outputs from both predictors as senti-predictor
features.
Others: The commonly used part-of-speech tags
are also included as features. Moreover, for an edge
candidate, a set of binary features are used to denote
the types of the edge and its entity mentions. For in-
stance, a binary feature indicates whether an edge is
a binary edge related to an entity mentioned in con-
text. To characterize the syntactic dependencies be-
tween two adjacent entity mentions, we use the path
in the dependency tree between the heads of the cor-
responding constituents, the number of words and
other mentions in-between as features. Additionally,
if the textual evidence is a constituent, its feature
w.r.t. an edge is the dependency path to the clos-
est mention of the edge that does not overlap with
this constituent.
6 Structural Inference
In order to find the best eMRG for a given sentence
with a well trained model, we need to determine
the most likely relation type for each relation candi-
date and support the corresponding assertions with
proper textual evidences. We formulate this task
as an Integer Linear Programming (ILP). Instead of
considering all constituents of a sentence, we empir-
ically select a subset as textual evidences for each
relation candidate.
6.1 Textual Evidence Candidates Selection
Textual evidences are selected based on the con-
stituent trees of sentences parsed by the Stanford
parser (Klein and Manning, 2003). For each men-
tion in a sentence, we first locate a constituent in
the tree with the maximal overlap by Jaccard sim-
ilarity. Starting from this constituent, we consider
two types of candidates: type I candidates are con-
stituents at the highest level which contain neither
any word of another mention nor any contrast con-
junctions such as ?but?; type II candidates are con-
stituents at the highest level which cover exactly two
mentions of an edge and do not overlap with any
other mentions. For a binary edge connecting an en-
tity mention and an attribute mention, we consider
a type I candidate starting from the attribute men-
tion. For a binary edge connecting two entity men-
tions, we consider type I candidates starting from
both mentions. Moreover, for a comparative ternary
edge, we consider both type I and type II candidates
starting from the attribute mention. This strategy is
based on our observation that these candidates of-
ten cover the most important information w.r.t. the
covered entity mentions.
6.2 ILP Formulation
We formulate the inference problem of finding the
best eMRG as an ILP problem due to its convenient
integration of both soft and hard constraints.
Given the model parameters ?, we reformulate
the score of an eMRG in the discriminant function
(1) as follows,
?>?(x, y, h) =
?
(a,c)?(y,h)
saczac +
?
mi?Mp
?
a,a??yr(mi),a 6=a?
saa?zaa?
where sac = ?>?e(x, a, c) denotes the score of a
labeled edge a attached with a textual evidence c,
saa? = ?>?c(a, a?) is the edge co-occurrence score,
the binary variable zac indicates the presence or ab-
sence of the corresponding edge, and zaa? indicates
if two edges co-occurr. As not every edge set can
form an eMRG, we require that a valid eMRG should
satisfy a set of linear constraints, which form our
constraint space. Then function (1) is equivalent to
max
z?B s
>z + ?zd
s.t. A
?
?
z
?
?
?
? ? d
z,?, ? ? B
where B = 2S with S = {0, 1}, and ? and ? are
auxiliary binary variables that help define the con-
straint space. The above optimization problem takes
exactly the form of an ILP because both the con-
straints and the objective function are linear, and all
variables take only integer values.
In the following, we consider two types of con-
straint space, 1) an eMRG with only binary edges and
2) an eMRG with both binary and ternary edges.
160
eMRG with only Binary Edges: An eMRG has
only binary edges if a sentence contains no attribute
mention or at most one entity mention. We expect
that each edge has only one relation type and is sup-
ported by a single textual evidence. To facilitate the
formulation of constraints, we introduce ?el to de-
note the presence or absence of a labeled edge el,
and ?ec to indicate if a textual evidence c is assigned
to an unlabeled edge e. Then the binary variable for
the corresponding evidentiary edge zelc = ?ec ? ?el ,
where the ILP formulation of conjunction can be
found in (Martins et al., 2009).
Let Ce denote the set of textual evidence candi-
dates of an unlabeled edge e. The constraint of at
most one textual evidence per edge is formulated as:
?
c?Ce
?ec ? 1 (3)
Once a textual evidence is assigned to an edge,
their relation labels should match and the number
of labeled edges must agree with the number of at-
tached textual evidences. Further, we assume that a
textual evidence c conveys at most one relation so
that an evidence will not be assigned to the relations
of different types, which is the main problem for the
structural SVM based model. Let ?cl indicate that
the textual evidence c is labeled by the relation type
l. The corresponding constraints are expressed as,
?
l?Le
?el =
?
c?Ce
?ec; zelc ? ?cl;
?
l?L
?cl ? 1
where Le denotes the set of all possible labels for
an unlabeled edge e, and L is the set of all relation
types of MRGs (cf. Section 3).
In order to avoid a textual evidence being overly
reused by multiple relation candidates, we first pe-
nalize the assignment of a textual evidence c to a
labeled edge a by associating the corresponding zac
with a fixed negative cost ?? in the objective func-
tion. Then the selection of one textual evidence per
edge a is encouraged by associating ? to zdc in the
objective function, where zdc =
?
e?Sc ?ec and Sc isthe set of edges that the textual evidence c serves as
a candidate. The disjunction zdc is expressed as:
zdc ? ?e, e ? Sc
zdc ?
?
e?Sc
?e
(a) Binary edge structure
(b) Ternary edge structure
Figure 4: Alternative structures associated with an
attribute mention.
This soft constraint not only encourages one textual
evidence per edge, but also keeps it eligible for mul-
tiple assignments.
For any two labeled edge a and a? incident
to the same entity mention, the edge-to-edge co-
occurrence is described by zca,a? = za ? za? .
eMRG with both Binary and Ternary Edges: If
there are more than one entity mentions and at least
one attribute mention in a sentence, an eMRG can
potentially have both binary and ternary edges. In
this case, we assume that each mention of attributes
can participate either in binary relations or in ternary
relations. The assumption holds in more than 99.9%
of the sentences in our SRG corpus, thus we describe
it as a set of hard constraints. Geometrically, the as-
sumption can be visualized as the selection between
two alternative structures incident to the same at-
tribute mention, as shown in Figure 4. Note that,
in the binary edge structure, we include not only the
edges incident to the attribute mention but also the
edge between the two entity mentions.
Let Sbmi be the set of all possible labeled edgesin a binary edge structure of an attribute mention
mi. Variable ? bmi =
?
el?Sbmi
?el indicates whether
the attribute mention is associated with a binary
edge structure or not. In the same manner, we use
? tmi =
?
el?Stmi
?el to indicate the association of the
an attribute mention mi with an ternary edge struc-
ture from the set of all incident ternary edges Stmi .The selection between two alternative structures is
161
formulated as ? bmi + ? tmi = 1. As this influencesonly the edges incident to an attribute mention, we
keep all the constraints introduced in the previous
section unchanged except for constraint (3), which
is modified as
?
c?Ce
?ec ? ? bmi ;
?
c?Ce
?ec ? ? tmi
Therefore, we can have either binary edges or
ternary edges for an attribute mention.
7 Learning Model Parameters
Given a set of training sentences D =
{(x1, y1), . . . , (xn, yn)}, the best weight vec-
tor ? of the discriminant function (1) is found by
solving the following optimization problem:
min
?
1
n
n?
i=1
[ max
(y?,h?)?Y(x)?H(x)
(?>?(x, y?, h?)+?(h?, y?, y))
? max
h??H(x)
?>?(x, y, h?)] + ?|?|] (4)
where ?(h?, y?, y) is a loss function measuring the dis-
crepancies between an eMRG (y, h?) with gold stan-
dard edge labels y and an eMRG (y?, h?) with inferred
labeled edges y? and textual evidences h?. Due to the
sparse nature of the lexical features, we apply L1
regularizer to the weight vector ?, and the degree of
sparsity is controlled by the hyperparameter ?.
Since the L1 norm in the above optimization
problem is not differentiable at zero, we apply the
online forward-backward splitting (FOBOS) algo-
rithm (Duchi and Singer, 2009). It requires two steps
for updating the weight vector ? by using a single
training sentence x on each iteration t.
?t+ 12 = ?t ? ?t?t
?t+1 = arg min
?
1
2?? ? ?t?
2 + ?t?|?|
where ?t is the subgradient computed without con-
sidering the L1 norm and ?t is the learning rate.
For a labeled sentence x, ?t = ?(x, y??, h??) ?
?(x, y, h??), where the feature functions of the corre-
sponding eMRGs are inferred by solving (y??, h??) =
arg max(h?,y?)?H(x)?Y(x)[?
>?(x, y?, h?) + ?(h?, y?, y)]
and (y, h??) = arg maxh??H(x) ?>?(x, y, h?), as in-
dicated in the optimization problem (4).
The former inference problem is similar to the
one we considered in the previous section except
the inclusion of the loss function. We incorporate
the loss function into the ILP formulation by defin-
ing the loss between an MRG (y, h) and a gold stan-
dard MRG as the sum of per-edge costs. In our ex-
periments, we consider a positive cost ? for each
wrongly labeled edge a, so that if an edge a has a
different label from the gold standard, we add ? to
the coefficient sac of the corresponding variable zac
in the objective function of the ILP formulation.
In addition, since the non-positive weights of edge
labels in the initial learning phrase often lead to
eMRGs with many unlabeled edges, which harms the
learning performance, we fix it by adding a con-
straint for the minimal number of labeled edges in
an eMRG, ?
a?A
?
c?Ca
?ac ? ? (5)
where A is the set of all labeled edge candidates and
? denotes the minimal number of labeled edges.
Empirically, the best way to determine ? is to
make it equal to the maximal number of labeled
edges in an eMRG with the restriction that a tex-
tual evidence can be assigned to at most one edge.
By considering all the edge candidates A and all the
textual evidence candidates C as two vertex sets in a
bipartite graph G? = ?V = (A,C), E? (with edges in
E indicating which textual evidence can be assigned
to which edge), ? corresponds to exactly the size of
a maximum matching of the bipartite graph1.
To find the optimal eMRG (y, h??), for the gold la-
bel k of each edge, we consider the following set of
constraints for inference since the labels of the edges
are known for the training data,
?
c?Ce
?ec ? 1; ?ec ? lck
?
k??L
lck? ? 1;
?
e?Sc
?ec ? 1
We include also the soft constraints, which avoid
a textual evidence being overly reused by multiple
relations, and the constraints similar to (5) to ensure
a minimal number of labeled edges and a minimal
number of sentiment-oriented relations.
1It is computed by the Hopcroft-Karp algorithm (Hopcroft
and Karp, 1973) in our implementation.
162
8 SRG Corpus
For evaluation we constructed the SRG corpus,
which in total consists of 1686 manually annotated
online reviews and forum posts in the digital camera
and movie domains2. For each domain, we maintain
a set of attributes and a list of entity names.
The annotation scheme for the sentiment repre-
sentation asserts minimal linguistic knowledge from
our annotators. By focusing on the meanings of the
sentences, the annotators make decisions based on
their language intuition, not restricted by specific
syntactic structures. Taking the example in Figure
2, the annotators only need to mark the mentions of
entities and attributes from both the sentences and
the context, disambiguate them, and label (?Canon
7D?, ?Nikon D7000?, price) as worse and (?Canon
7D?, ?sensor?) as positive, whereas in prior work,
people have annotated the sentiment-bearing expres-
sions such as ?great? and link them to the respective
relation instances as well. This also enables them
to annotate instances of both sentiment polarity and
comparative relaton, which are conveyed by not only
explicit sentiment-bearing expressions like ?excel-
lent performance?, but also factual expressions im-
plying evaluations such as ?The 7V has 10x optical
zoom and the 9V has 16x.?.
Camera Movie
Reviews Forums Reviews Forums
positive 386 1539 879 905
negative 165 363 529 331
comparison 30 480 39 35
Table 1: Distribution of relation instances in SRG corpus.
14 annotators participated in the annotation
project. After a short training period, annotators
worked on randomly assigned documents one at a
time. For product reviews, the system lists all rel-
evant information about the entity and the prede-
fined attributes. For forum posts, the system shows
only the attribute list. For each sentence in a doc-
ument, the annotator first determines if it refers to
an entity of interest. If not, the sentence is marked
2The 107 camera reviews are from bestbuy.com and Ama-
zon.com; the 667 camera forum posts are downloaded from fo-
rum.digitalcamerareview.com; the 138 movie reviews and 774
forum posts are from imdb.com and boards.ie respectively
as off-topic. Otherwise, the annotator will identify
the most obvious mentions, disambiguate them, and
mark the MRGs. We evaluate the inter-annotator
agreement on sSoRs in terms of Cohen?s Kappa
(?) (Cohen, 1968). An average Kappa value of 0.698
was achieved on a randomly selected set consisting
of 412 sentences.
Table 1 shows the corpus distribution after nor-
malizing them into sSoRs. Camera forum posts con-
tain the largest proportion of comparisons because
they are mainly about the recommendation of dig-
ital cameras. In contrast, web users are much less
interested in comparing movies, in both reviews and
forums. In all subsets, positive relations play a dom-
inant role since web users intend to express more
positive attitudes online than negative ones (Pang
and Lee, 2007).
9 Experiments
This section describes the empirical evaluation of
SENTI-LSSVM together with two competitive base-
lines on the SRG corpus.
9.1 Experimental Setup
We implemented a rule-based baseline (DING-
RULE) and a structural SVM (Tsochantaridis et
al., 2004) baseline (SENTI-SSVM) for comparison.
The former system extends the work of Ding et
al. (2009), which designed several linguistically-
motivated rules based on a sentiment polarity lexi-
con for relation identification and assumes there is
only one type of sentiment relation in a sentence. In
our implementation, we keep all the rules of (Ding et
al., 2009) and add one phrase-level rule when there
are more than one mention in a sentence. The ad-
ditional rule assigns sentiment-bearing words and
negators to its nearest relation candidates based on
the absolute surface distance between the words and
the corresponding mentions. In this case, the phrase-
level sentiment-oriented relations depend only on
the assigned sentiment words and negators. The lat-
ter system is based on a structural SVM and does
not consider the assignment of textual evidences to
relation instances during inference. The textual fea-
tures of a relation candidate are all lexical and sen-
timent predictor features within a surface distance
of four words from the mentions of the candidate.
163
Thus, this baseline does not need the inference con-
straints of SENTI-LSSVM for the selection of textual
evidences. To gain more insights into the model,
we also evaluate the contribution of individual fea-
tures of SENTI-LSSVM. In addition, to show if identi-
fying sentiment polarities and comparative relations
jointly works better than tackling each task on its
own, we train SENTI-LSSVM for each task separately
and combine their predictions according to compat-
ibility rules and the corresponding graph scores.
For each domain and text genre, we withheld 15%
documents for development and use the remaining
for cross validation. The hyperparameters of all sys-
tems are tuned on the development datasets. For all
experiments of SENTI-LSSVM, we use ? = 0.0001
for the L1 regularizer in Eq.(4) and ? = 0.05 for
the loss function; and for SENTI-SSVM, ? = 0.0001
and ? = 0.01. Since the relation type of off-topic
sentences is certainly other, we evaluate all systems
with 5-fold cross-validation only on the on-topic
sentences in the evaluation dataset. Since the same
sSoR can have several equivalent MRGs and the rela-
tion type other is not of our interest, we evaluate the
sSoRs in terms of precision, recall and F-measure.
All reported numbers are averages over the 5 folds.
9.2 Results
Table 2 shows the complete results of all sys-
tems. Here our model SENTI-LSSVM outperformed
all baselines in terms of the average F-measure
scores and recalls by a large margin. The F-measure
on movie reviews is about 14% over the best base-
line. The rule-based system has higher precision
than recall in most cases. However, simply increas-
ing the coverage of the domain independent senti-
ment polarity lexicon might lead to worse perfor-
mance (Taboada et al., 2011) because many sen-
timent oriented relations are conveyed by domain
dependent expressions and factual expressions im-
plying evaluations, such as ?This camera does not
have manual control.? Compared to DING-RULE,
SENTI-SSVM performs better in the camera domain
but worse for the movies due to many misclassi-
fication of negative relation instances as other. It
also wrongly predicted more positive instances as
other than SENTI-LSSVM. We found that the recalls
of these instances are low because they often have
overly similar features with the instances of the type
other linking to the same mentions. The problem
gets worse in the movie domain since i) many sen-
tences contain no explicit sentiment-bearing words;
ii) the prior polarity of the sentiment-bearing words
do not agree with their contextual polarity in the
sentences. Consider the following example from a
forum post about the movie ?Superman Returns?:
?Have a look at Superman: the Animated Series or
Justice League Unlimited . . . that is how the char-
acters of Superman and Lex Luthor should be.?. In
contrast, our model minimizes the overlapping fea-
tures by assigning them to the most likely relation
candidates. This leads to significantly better per-
formance. Although SENTI-SSVM has low recall for
both positive and negative relations, it achieves the
highest recall for the comparative relation among all
systems in the movie domain and camera reviews.
Since less than 1% of all instances are for compara-
tive relations in these document sets and all models
are trained to optimize the overall accuracy, SENTI-
LSSVM intends to trade off the minority class for the
overall better performance. This advantage disap-
pears on the camera forum posts, where the number
of instances of comparative relation is 12 times more
than that in the other data sets.
All systems perform better in predicting positive
relations than the negative ones. This corresponds
well to the empirical findings in (Wilson, 2008) that
people intend to use more complex expressions for
negative sentiments than their affirmative counter-
parts. It is also in accordance with the distribution of
these relations in our SRG corpus which is randomly
sampled from the online documents. For learning
systems, it can also be explained by the fact that the
training data for positive relations are considerably
more than those for negative ones. The comparative
relation is the hardest one to process since we found
that many corresponding expressions do not contain
explicit keywords for comparison.
To understand the performance of the key fea-
ture groups in our model better, we remove each
group from the full SENTI-LSSVM system and eval-
uate the variations with movie reviews and camera
forum posts, which have relatively balanced distri-
bution of relation types. As shown in Table 3, the
features from the sentiment predictors make signif-
icant contributions for both datasets. The differ-
ent drops of the performance indicate that the po-
164
Positive Negative Comparison Micro-average
P R F P R F P R F P R F
Ca
me
ra
Fo
rum
DING-RULE 56.4 39.0 46.1 46.2 24.0 31.6 42.6 14.0 21.0 53.4 30.8 39.0
SENTI-SSVM 60.2 35.6 44.8 44.2 38.5 41.2 28.0 40.1 32.9 43.7 36.7 39.9
SENTI-LSSVM 69.2 38.9 49.8 50.8 39.3 44.3 42.6 35.1 38.5 56.5 38.0 45.4
Ca
me
ra
Re
-
vie
w DING-RULE 83.6 69.0 75.6 68.6 38.8 49.6 30.0 16.9 21.6 81.1 58.6 68.1SENTI-SSVM 72.6 75.4 74.0 63.9 62.5 63.2 28.0 38.9 32.5 68.1 70.4 69.3
SENTI-LSSVM 77.3 85.4 81.2 68.9 61.3 64.9 22.3 20.7 21.6 73.1 73.4 73.7
Mo
vie
Fo
rum
DING-RULE 63.7 37.4 47.1 27.6 34.3 30.6 8.9 5.6 6.8 48.2 35.9 41.2
SENTI-SSVM 66.2 30.1 41.3 25.6 17.3 20.7 44.2 56.7 49.7 53.3 27.9 36.6
SENTI-LSSVM 63.3 44.2 52.1 29.7 45.6 36.0 40.1 45.0 42.4 49.7 44.6 47.0
Mo
vie Re
-
vie
w DING-RULE 66.5 47.2 55.2 42.0 39.1 40.5 31.4 12.0 17.4 56.2 44.0 49.4SENTI-SSVM 61.3 54.0 57.4 45.2 13.7 21.1 24.5 63.3 35.3 54.6 39.2 45.7
SENTI-LSSVM 59.0 79.1 67.6 53.3 51.4 52.3 28.3 34.0 30.9 57.9 68.8 62.9
Table 2: Evaluation results for DING-RULE, SENTI-SSVM and SENTI-LSSVM. Boldface figures are statistically
significantly better than all others in the same comparison group under t-test with p = 0.05.
Feature Models Movie Reviews Camera Forums
full system 62.9 45.4
?unigram 63.2 (+0.3) 41.2 (-4.2)
?context 54.5 (-8.4) 46.0 (+0.6)
?co-occurrence 62.6 (-0.3) 44.9 (-0.5)
?senti-predictors 61.3 (-1.6) 34.3 (-11.1)
Table 3: Micro-average F-measure of SENTI-LSSVM
with different feature models
larities predicted by rules are more consistent in
camera forum posts than in movie reviews. Due
to the complexity of expressions in the movie re-
views our model cannot benefit from the unigram
features but these features are a good compensation
for the sentiment predictor features in camera fo-
rum posts. The sharp drop by removing the context
features from our model on movie reviews indicates
that the sentiments in movie reviews depend highly
on the relations of the previous sentences. In con-
trast, the sentiment-oriented relations of the previ-
ous sentences could be a reason of overfitting for
camera forum data. The edge co-occurrence fea-
tures do not play an important role in our model
since the number of co-occurred sentiment-oriented
relations in the sentences with contrast conjunctions
like ?but? is small. However, we found that allow-
ing the co-occurrence of any sentiment-oriented re-
lations would harm the performance of the model.
In addition, our experiments showed that the sep-
arated approach, which trains a model for senti-
ment polarities and comparative relations respec-
tively, leads to a decrease by almost 1% in terms of
the F-measure averaged over all four datasets. The
largest drop of F-measure is 3% on camera forum
posts, since this dataset contains the largest propor-
tion of comparative relations. We found that the er-
rors are increased when the trained models make
conflicting predictions. In this case, the joint ap-
proach can take all factors into account and make
more consistent decisions than the separated ap-
proaches.
10 Conclusion
We proposed SENTI-LSSVM model for extracting in-
stances of both sentiment polarities and comparative
relations. For evaluating and training the model, we
created an SRG corpus by using a lightweight an-
notation scheme. We showed that our model can
automatically find textual evidences to support its
relation predictions and achieves significantly bet-
ter F-measure scores than alternative state-of-the-art
methods.
References
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
165
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 590?598, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the Annual meeting of
the Association for Computational Linguistics, pages
269?274. Association for Computational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 431?
439, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or Par-
tial Credit. Psychological bulletin, 70(4):213.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the 2008 International Conference on
Web Search and Data Mining, pages 231?240, New
York, NY, USA. ACM.
Xiaowen Ding, Bing Liu, and Lei Zhang. 2009. Entity
discovery and assignment for opinion mining applica-
tions. In Proceedings of the ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining, pages
1125?1134.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
The Journal of Machine Learning Research, 10:2899?
2934.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, pages 241?248, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs (system demonstration). In Proceed-
ings of the International AAAI Conference on Weblogs
and Social Media.
John E Hopcroft and Richard M Karp. 1973. An n?5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on computing, 2(4):225?231.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Proceedings of the
ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining, pages 168?177, New York, NY,
USA. ACM.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
Opinionminer: a novel machine learning system for
web opinion mining and extraction. In Proceedings
of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1195?
1204, New York, NY, USA. ACM.
Nitin Jindal and Bing Liu. 2006. Mining comparative
sentences and relations. In Proceedings of the 21st In-
ternational Conference on Artificial Intelligence - Vol-
ume 2, AAAI?06, pages 1331?1336. AAAI Press.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities?
exploration of pipelines and joint models. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics, volume 11, pages 101?106.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sent-
ment corpus for the automotive domain. In 4th Inter-
national AAAI Conference on Weblogs and Social Me-
dia Data Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, SST ?06, pages 1?8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006.
Opinion extraction, summarization and tracking in
news and blog corpora. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 100?107.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of the 14th international
conference on World Wide Web, pages 342?351, New
York, NY, USA. ACM.
Andr? L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Annual meeting of the Association for Computational
Linguistics, pages 342?350.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
166
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rat-
ing prediction from sparse text patterns. In Chu-Ren
Huang and Dan Jurafsky, editors, Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), ACL Anthology, pages 913?
921, Beijing, China. Tsinghua University Press.
Lizhen Qu, Rainer Gemulla, and Gerhard Weikum. 2012.
A weakly supervised model for sentence-level seman-
tic orientation analysis with multiple experts. In Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 149?159,
Jeju Island, Korea, July. Proceedings of the Annual
meeting of the Association for Computational Linguis-
tics.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1201?1211.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
the Joint conference of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, pages 226?234.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267?307.
Oscar T?ckstr?m and Ryan McDonald. 2011. Discov-
ering fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the 33rd
European conference on Advances in information re-
trieval, ECIR?11, pages 368?374, Berlin, Heidelberg.
Springer-Verlag.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 575?584,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the International
Conference on Machine Learning, pages 104?112.
Wei Wei and Jon Atle Gulla. 2010. Sentiment learn-
ing on product reviews via sentiment ontology tree. In
Proceedings of the Annual meeting of the Association
for Computational Linguistics, pages 404?413.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Ann Wilson. 2008. Fine-grained subjectivity
and sentiment analysis: recognizing the intensity, po-
larity, and attitudes of private states. Ph.D. thesis,
UNIVERSITY OF PITTSBURGH.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2011. Structural opinion mining for graph-based sen-
timent representation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1332?1341.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In Pro-
ceedings of the International Conference on Machine
Learning, page 147.
Ning Yu and Sandra K?bler. 2011. Filling the gap:
Semi-supervised learning for opinion detection across
domains. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning, pages
200?209. Association for Computational Linguistics.
167
168
