Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 33?40,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
The Latin Dependency Treebank in a Cultural Heritage Digital Library
David Bamman
The Perseus Project
Tufts University
Medford, MA
david.bamman@tufts.edu
Gregory Crane
The Perseus Project
Tufts University
Medford, MA
gregory.crane@tufts.edu
Abstract
This paper describes the mutually benefi-
cial relationship between a cultural heritage
digital library and a historical treebank: an
established digital library can provide the
resources and structure necessary for effi-
ciently building a treebank, while a tree-
bank, as a language resource, is a valuable
tool for audiences traditionally served by
such libraries.
1 Introduction
The composition of historical treebanks is funda-
mentally different from that of modern ones. While
modern treebanks are generally comprised of news-
paper articles,1 historical treebanks are built from
texts that have been the focus of study for centuries,
if not millennia. The Penn-Helsinki Parsed Cor-
pus of Middle English (Kroch and Taylor, 2000),
for example, includes Chaucer?s 14th-century Par-
son?s Tale, while the York Poetry Corpus (Pintzuk
and Leendert, 2001) includes the entire text of Be-
owulf. The scholarship that has attended these texts
since their writing has produced a wealth of contex-
tual materials, including commentaries, translations,
and linguistic resources.
1To name just three, the Penn Treebank (Marcus et al, 1994)
is comprised of texts from the Wall Street Journal; the Ger-
man TIGER Treebank (Brants et al, 2002) is built from texts
taken from the Frankfurter Rundschau; and the Prague De-
pendency Treebank (Hajic?, 1998) includes articles from sev-
eral daily newspapers (Lidove? noviny and Mlada? fronta Dnes), a
business magazine (C?eskomoravsky? Profit) and a scientific jour-
nal (Vesm??r).
For the past twenty years, the Perseus digital li-
brary (Crane, 1987; Crane et al, 2001) has collected
materials of this sort to create an open reading envi-
ronment for the study of Classical texts. This envi-
ronment presents the Greek or Latin source text and
contextualizes it with secondary publications (e.g.,
translations, commentaries, references in dictionar-
ies), along with a morphological analysis of every
word in the text and variant manuscript readings as
well (when available).
We have recently begun work on syntactically an-
notating the texts in our collection to create a Latin
Dependency Treebank. In the course of developing
this treebank, the resources already invested in the
digital library have been crucial: the digital library
provides a modular structure on which to build addi-
tional services, contains a large corpus of Classical
source texts, and provides a wealth of contextual in-
formation for annotators who are non-native speak-
ers of the language.
In this the digital library has had a profound im-
pact on the creation of our treebank, but the influ-
ence goes both ways. The digital library is a heav-
ily trafficked website with a wide range of users, in-
cluding professional scholars, students and hobby-
ists. By incorporating the treebank as a language
resource into this digital library, we have the poten-
tial to introduce a fundamental NLP tool to an audi-
ence outside the traditional disciplines of computer
science or computational linguistics that would nor-
mally use it. Students of the language can profit
from the syntactic information encoded in a tree-
bank, while traditional scholars can benefit from the
textual searching it makes possible as well.
33
Figure 1: A screenshot of Vergil?s Aeneid from the Perseus digital library.
2 The Perseus Digital Library
Figure 1 shows a screenshot from our digital library.
In this view, the reader is looking at the first seven
lines of Vergil?s Aeneid. The source text is provided
in the middle, with contextualizing information fill-
ing the right column. This information includes:
? Translations. Here two English translations
are provided, one by the 17th-century English
poet John Dryden and a more modern one by
Theodore Williams.
? Commentaries. Two commentaries are also
provided, one in Latin by the Roman grammar-
ian Servius, and one in English by the 19th-
century scholar John Conington.
? Citations in reference works. Classical refer-
ence works such as grammars and lexica of-
ten cite particular passages in literary works as
examples of use. Here, all of the citations to
any word or phrase in these seven lines are pre-
sented at the right.
Additionally, every word in the source text is
linked to its morphological analysis, which lists
every lemma and morphological feature associated
with that particular word form. Here the reader has
clicked on arma in the source text. This tool reveals
that the word can be derived from two lemmas (the
verb armo and the noun arma), and gives a full mor-
phological analysis for each. A recommender sys-
tem automatically selects the most probable analysis
for a word given its surrounding context, and users
can also vote for the form they think is correct.2
3 Latin Dependency Treebank
Now in version 1.3, the Latin Dependency Treebank
is comprised of excerpts from four texts: Cicero?s
Oratio in Catilinam, Caesar?s Commentarii de Bello
Gallico, Vergil?s Aeneid and Jerome?s Vulgate.
Since Latin has a highly flexible word order, we
have based our annotation style on the dependency
grammar used by the Prague Dependency Tree-
bank (PDT) (Hajic?, 1998) for Czech (another non-
projective language) while tailoring it for Latin via
2These user contributions have the potential to significantly
improve the morphological tagging of these texts: any single
user vote assigns the correct morphological analysis to a word
89% of the time, while the recommender system does so with
an accuracy of 76% (Crane et al, 2006).
34
Date Author Words
63 BCE Cicero 1,189
51 BCE Caesar 1,486
19 BCE Vergil 2,647
405 CE Jerome 8,382
Total: 13,683
Table 1: Treebank composition by author.
the grammar of Pinkster (1990).3
In addition to the index of its syntactic head and
the type of relation to it, each word in the treebank
is also annotated with the lemma from which it is
inflected and its morphological code. We plan to re-
lease the treebank incrementally with each new ma-
jor textual addition (so that version 1.4, for instance,
will include the treebank of 1.3 plus Sallust?s Bellum
Catilinae, the text currently in production).
4 The Influence of a Digital Library
A cultural heritage digital library has provided a fer-
tile ground for our historical treebank in two funda-
mental ways: by providing a structure on which to
build new services and by providing reading support
to expedite the process of annotation.
4.1 Structure
By anchoring the treebank in a cultural heritage dig-
ital library, we are able to take advantage of a struc-
tured reading environment with canonical standards
for the presentation of text and a large body of dig-
itized resources, which include XML source texts,
morphological analyzers, machine-readable dictio-
naries, and an online user interface.
Texts. Our digital library contains 3.4 million
words of Latin source texts (along with 4.9 mil-
lion words of Greek). The texts are all public-
domain materials that have been scanned, OCR?d
and formatted into TEI-compliant XML. The value
of this prior labor is twofold: most immediately,
the existence of clean, digital editions of these
texts has saved us a considerable amount of time
and resources, as we would otherwise have to
3We are also collaborating with other Latin treebanks (no-
tably the Index Thomisticus on the works of Thomas Aquinas)
to create a common set of annotation guidelines to be used as a
standard for Latin of any period (Bamman et al, 2007).
create them before annotating them syntactically;
but their encoding as repurposeable XML docu-
ments in a larger library also allows us to refer
to them under standardized citations. The pas-
sage of Vergil displayed in Figure 1 is not simply
a string of unstructured text; it is a subdocument
(Book=1:card=1) that is itself part of a larger doc-
ument object (Perseus:text:1999.02.0055), with sis-
ters (Book=1:card=8) and children of its own (e.g.,
line=4). This XML structure allows us to situate any
given treebank sentence within its larger context.
Morphological Analysis. As a highly inflected
language, Latin has an intricate morphological sys-
tem, in which a full morphological analysis is the
product of nine features: part of speech, person,
number, tense, mood, voice, gender, case and de-
gree. Our digital library has included a morphologi-
cal analyzer from its beginning. This resource maps
an inflected form of a word (such as arma above)
to all of the possible analyses for all of the dictio-
nary entries associated with it. In addition to provid-
ing a common morphological standard, this mapping
greatly helps to constrain the problem of morpho-
logical tagging (selecting the correct form from all
possible forms), since a statistical tagger only needs
to consider the morphological analyses licensed by
the inflection rather than all possible combinations.
User interface. The user interface of our library
is designed to be modular, since different texts have
different contextual resources associated with them
(while some have translations, others may have
commentaries). This modularity allows us to easily
introduce new features, since the underlying archi-
tecture of the page doesn?t change ? a new feature
can simply be added.
Figure 2 presents a screenshot of the digital li-
brary with an annotation tool built into the inter-
face. In the widget on the right, the source text in
view (the first chunk of Tacitus? Annales) has been
automatically segmented into sentences; an annota-
tor can click on any sentence to assign it a syntac-
tic annotation. Here the user has clicked on the first
sentence (Vrbem Romam a principio reges habuere);
this action brings up an annotation screen in which
a partial automatic parse is provided, along with the
most likely morphological analysis for each word.
The annotator can then correct this automatic output
35
Figure 2: A screenshot of Tacitus? Annales from the Perseus digital library.
and move on to the next segmented sentence, with
all of the contextual resources still in view.
4.2 Reading support
Modern treebanks also differ from historical ones in
the fluency of their annotators. The efficient anno-
tation of historical languages is hindered by the fact
that no native speakers exist, and this is especially
true of Latin, a difficult language with a high de-
gree of non-projectivity. While the Penn Treebank
can report a productivity rate of between 750 and
1000 words per hour for their annotators after four
months of training (Taylor et al, 2003) and the Penn
Chinese treebank can report a rate of 240-480 words
per hour (Chiou et al, 2001), our annotation speeds
are significantly slower, ranging from 90 words per
hour to 281. Our best approach for Latin is to de-
velop strategies that can speed up the annotation pro-
cess, and here the resources found in a digital library
are crucial. There are three varieties of contextual
resources in our digital library that aid in the un-
derstanding of a text: translations, commentaries,
and dictionaries. These resources shed light on a
text, from the level of sentences to that of individual
words.
Translations. Translations provide reading sup-
port on a large scale: while loose translations may
not be able to inform readers about the meaning and
syntactic role of any single word, they do provide
a broad description of the action taking place, and
this can often help to establish the semantic struc-
ture of the sentence ? who did what to whom, and
how. In a language with a free word order (and with
poetry especially), this kind of high-level structure
can be important for establishing a quick initial un-
derstanding of the sentence before narrowing down
to individual syntactic roles.
Commentaries. Classical commentaries provide
information about the specific use of individ-
ual words, often noting morphological information
(such as case) for ambiguous words or giving ex-
planatory information for unusual structures. This
information often comes at crucial decision points
36
in the annotation process, and represents judgments
by authorities in the field with expertise in that par-
ticular text.
Figure 3: An excerpt from Conington?s commentary
on Vergil?s Aeneid (Conington, 1876), here referring
to Book 1, lines 4 and 5.
Machine-Readable Dictionaries. In addition to
providing lists of stems for morphological analyzers,
machine-readable dictionaries also provide valuable
reading support for the process of lemma selection.
Every available morphological analysis for a word is
paired with the word stem (a lemma) from which it is
derived, but analyses are often ambiguous between
different lemmas. The extremely common form est,
for example, is a third person singular present in-
dicative active verb, but can be inflected from two
different lemmas: the verb sum (to be) and the verb
edo (to eat). In this case, we can use the text already
tagged to suggest a more probable form (sum ap-
pears much more frequently and is therefore the like-
lier candidate), but in less dominant cases, we can
use the dictionary: since the word stems involved
in morphological analysis have been derived from
the dictionary lemmas, we can map each analysis
to a dictionary definition, so that, for instance, if an
annotator is unfamiliar with the distinction between
the lemmas occido1 (to strike down) and occido2 (to
fall), their respective definitions can clarify it.
Machine-readable dictionaries, however, are also
a valuable annotation resource in that they often pro-
vide exemplary syntactic information as part of their
definitions. Consider, for example, the following
line from Book 6, line 2 of Vergil?s Aeneid: et tan-
dem Euboicis Cumarum adlabitur oris (?and at last
it glides to the Euboean shores of Cumae?). The
noun oris (shores) here is technically ambiguous,
and can be derived from a single lemma (ora) as a
noun in either the dative or ablative case. The dic-
tionary definition of allabor (to glide), however, dis-
ambiguates this for us, since it notes that the verb is
often constructed with either the dative or the ac-
cusative case.
Figure 4: Definition of allabor (the dictionary entry
for adlabitur) from Lewis and Short (1879).
Every word in our digital library is linked to a list
of its possible morphological analyses, and each of
those analyses is linked to its respective dictionary
entry. The place of a treebank in a digital library
allows for this tight level of integration.
5 The Impact of a Historical Treebank
The traffic in our library currently exceeds 10 mil-
lion page views by 400,000 distinct users per month
(as approximated by unique IP addresses). These
users are not computational linguists or computer
scientists who would typically make use of a tree-
bank; they are a mix of Classical scholars, stu-
dents, and amateurs. These different audiences have
equally different uses for a large corpus of syntacti-
cally annotated sentences: for one group it can pro-
vide additional reading support, and for the other a
scholarly resource to be queried.
5.1 Treebank as Reading Support
Our digital library is predominantly a reading en-
vironment: source texts in Greek and Latin are
presented with attendant materials to help facilitate
their understanding. The broadest of these materials
are translations, which present sentence-level equiv-
alents of the original; commentaries provide a more
detailed analysis of individual words and phrases. A
37
treebank has the potential to be a valuable contex-
tual resource by providing syntactic information for
every word in a sentence, not simply those chosen
by a commentator for discussion.
5.2 Treebank as a Scholarly Resource
For Classical scholars, a treebank can also be used
as a scholarly resource. Not all Classicists are pro-
grammers, however, and many of those who would
like to use such a resource would profit little from
an XML source file. We have already released ver-
sion 1.3 of the Latin Dependency Treebank in its
XML source, but we also plan to incorporate it into
the digital library as an object to be queried. This
will yield a powerful range of search options, in-
cluding lemmatized and morpho-syntactic search-
ing, and will be especially valuable for research in-
volving lexicography and semantic classification.
Lemmatized searching. The ability to conduct a
lemma-based textual search has long been a desider-
atum in Classics,4 where any given Latin word form
has 3.1 possible analyses on average.5 Locating all
inflections of edo (to eat) in the texts of Caesar, for
example, would involve two things:
1. Searching for all possible inflections of the root
word. This amounts to 202 different word
forms attested in our texts (including com-
pounds with enclitics).
2. Eliminating all results that are homonyms de-
rived from a different lemma. Since several in-
flections of edo are homonyms with inflections
of the far more common sum (to be), many
of the found results will be false positives and
have to be discarded.
This is a laborious process and, as such, is rarely
undertaken by Classical scholars: the lack of such
a resource has constrained the set of questions we
4Both the Perseus Project and the Thesaurus Linguae Grae-
cae (http://www.tlg.uci.edu) allow users to search for all in-
flected forms of a lemma in their texts, but neither filters results
that are homonyms derived from different lemmas.
5Based on the average number of lemma + morphology
combinations for all unique word tokens in our 3.4 million word
corpus. The word form amor, for example, has 3 analyses: as
a first-person singular present indicative passive verb derived
from the lemma amo (to love) and as either a nominative or
vocative masculine singular noun derived from amor (love).
can ask about a text. Since a treebank encodes each
word?s lemma in addition to its morphological and
syntactic analysis, this information is now free for
the taking.
Morpho-syntactic searching. A treebank?s major
contribution to scholarship is that it encodes the
syntax of a sentence, along with a morphological
analysis of each word. These two together can be
combined into elaborate searches. Treebanks allow
scholars to find all instances of any particular con-
struction. For example:
? When the conjunction cum is the head of a sub-
ordinate clause whose verb is indicative, it is
often recognized as a temporal clause, qualify-
ing the time of the main clause?s action;
? When that verb is subjunctive, however, the
clause retains a different meaning, as either cir-
cumstantial, causal, or adversative.
These different clause types can be found by
querying the treebank: in the first case, by search-
ing for indicative verbs that syntactically depend on
cum; in the second, for subjunctive verbs that de-
pend on it. In version 1.3 of the Latin Dependency
Treebank, cum is the head of a subordinate clause
38 times: in 7 of these clauses an indicative verb de-
pends on it, while in 31 of them a subjunctive one
does. This type of searching allows us to gather sta-
tistical data while also locating all instances for fur-
ther qualitative analysis.6
Lexicography. Searching for a combination of
lemma and morpho-syntactic information can yield
powerful results, which we can illustrate with a
question from Latin lexicography: how does the
meaning of a word change across authors and over
time? If we take a single verb ? libero (to free, lib-
erate) ? we can chart its use in various authors by
asking a more specific question: what do different
Latin authors want to be liberated from? We can
imagine that an orator of the republic has little need
to speak of liberation from eternal death, while an
apostolic father is just as unlikely to speak of being
freed from another?s monetary debt.
6For the importance of a treebank in expediting morpho-
syntactic research in Latin rhetoric and historical linguistics, see
Bamman and Crane (2006).
38
We can answer this more general question by
transforming it into a syntactic one: what are the
most common complements of the lemma libero that
are expressed in oblique cases (e.g., ablative, geni-
tive, etc.) or as prepositional phrases? In a small test
of 100 instances of the lemma in Cicero and Jerome,
we find an interesting answer, presented in Table 2.
Cicero Jerome
periculo 14 manu 22
metu 8 morte 3
cura 6 ore 3
aere 3 latronibus 2
scelere 3 inimico 2
suspicione 3 bello 2
Table 2: Count of objects liberated from in Cicero
and Jerome that occur with frequency greater than 1
in a corpus of 100 sentences from each author con-
taining any inflected form of the verb libero.
The most common entities that Cicero speaks
of being liberated from clearly reflect the cares of
an orator of the republic: periculo (danger), metu
(fear), cura (care), and aere (debt). Jerome, how-
ever, uses libero to speak of liberation from a very
different set of things: his actors speak of deliver-
ance from manu (e.g., the hand of the Egyptians),
from ore (e.g., the mouth of the lion) and from
morte (death). A treebank encoded with lemma and
morpho-syntactic information lets us quantify these
typical arguments and thereby identify the use of the
word at any given time.
Named entity labeling. Our treebank?s place in
a digital library also means that complex searches
can draw on the resources that already lie therein.
Two of our major reference works include Smith?s
Dictionary of Greek and Roman Geography (1854),
which contains 11,564 place names, and Smith?s
Dictionary of Greek and Roman Biography and
Mythology (1873), which contains 20,336 personal
names. By mapping the lemmas in our treebank to
the entries in these dictionaries, we can determine
each lemma?s broad semantic class. After supple-
menting the Classical Dictionary with names from
the Vulgate, we find that the most common people
in the treebank are Iesus, Aeneas, Caesar, Catilina,
Satanas, Sibylla, Phoebus, Misenus and Iohannes;
the most common place names are Gallia, Babylon,
Troia, Hierusalem, Avernus and Sardis.
One use of such classification is to search for
verbs that are typically found with sentient agents.
We can find this by simply searching the treebank
for all active verbs with subjects known to be people
(i.e., subjects whose lemmas can be mapped to an
entry in Smith?s Dictionary). An excerpt of the list
that results is given in Table 3.
mitto to send
iubeo to order
duco to lead
impono to place
amo to love
incipio to begin
condo to hide
Table 3: Common verbs with people as subjects in
the Latin Dependency Treebank 1.3.
Aside from its intrinsic value of providing a cata-
logue of such verbs, a list like this is also useful for
classifying common nouns: if a verb is frequently
found with a person as its subject, all of its sub-
jects in general will likely be sentient as well. Table
4 presents a complete list of subjects of the active
voice of the verb mitto (to send) as attested in our
treebank.
angelus angel
Caesar Caesar
deus God
diabolus devil
Remi Gallic tribe
serpens serpent
ficus fig tree
Table 4: Subjects of active mitto in the Latin Depen-
dency Treebank 1.3.
Only two of these subjects are proper names (Cae-
sar and Remi) that can be found in Smith?s Dictio-
nary, but almost all of these nouns clearly belong
to the same semantic class ? angelus, deus, diabo-
lus and serpens (at least in this text) are entities with
cognition.
Inducing semantic relationships of this sort is the
typical domain of clustering techniques such as la-
39
tent semantic analysis (Deerwester et al, 1990), but
those methods generally work best on large corpora.
By embedding this syntactic resource in a digital li-
brary and linking it to external resources such as ref-
erence works, we can find similar semantic relation-
ships with a much smaller corpus.
6 Conclusion
Treebanks already fill a niche in the NLP community
by providing valuable datasets for automatic pro-
cesses such as parsing and grammar induction. Their
utility, however, does not end there. The linguis-
tic information that treebanks encode is of value to a
wide range of potential users, including professional
scholars, students and amateurs, and we must en-
courage the use of these resources by making them
available to such a diverse community. The digital
library described in this paper has proved to be cru-
cial for the development and deployment of our tree-
bank: since the natural intuitions of native speakers
are hard to come by for historical languagues, it is all
the more important to leverage the cultural heritage
resources we already have.
7 Acknowledgments
Grants from the Digital Library Initiative Phrase 2
(IIS-9817484) and the National Science Foundation
(BCS-0616521) provided support for this work.
References
David Bamman and Gregory Crane. 2006. The design
and use of a Latin dependency treebank. In Proceed-
ings of the Fifth Workshop on Treebanks and Linguistic
Theories (TLT2006), pages 67?78.
David Bamman, Marco Passarotti, Gregory Crane, and
Savina Raynaud. 2007. Guidelines for the syntactic
annotation of Latin treebanks, version 1.3. Technical
report, Tufts Digital Library, Medford.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the First Workshop on Tree-
banks and Linguistic Theories, pages 24?41, Sozopol.
Fu-Dong Chiou, David Chiang, and Martha Palmer.
2001. Facilitating treebank annotation using a statis-
tical parser. In Proceedings of the First International
Conference on Human Language Technology Research
HLT ?01, pages 1?4.
John Conington, editor. 1876. P. Vergili Maronis Opera.
The Works of Virgil, with Commentary. Whittaker and
Co, London.
Gregory Crane, Robert F. Chavez, Anne Mahoney,
Thomas L. Milbank, Jeffrey A. Rydberg-Cox,
David A. Smith, and Clifford E. Wulfman. 2001.
Drudgery and deep thought: Designing digital li-
braries for the humanities. Communications of the
ACM, 44(5):34?40.
Gregory Crane, David Bamman, Lisa Cerrato, Alison
Jones, David M. Mimno, Adrian Packel, David Scul-
ley, and Gabriel Weaver. 2006. Beyond digital in-
cunabula: Modeling the next generation of digital li-
braries. In ECDL 2006, pages 353?366.
Gregory Crane. 1987. From the old to the new: Integrat-
ing hypertext into traditional scholarship. In Hyper-
text ?87: Proceedings of the 1st ACM conference on
Hypertext, pages 51?56. ACM Press.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
Prague Karolinum, Charles University Press.
A. Kroch and A. Taylor. 2000. Penn-Helsinki
Parsed Corpus of Middle English, second edi-
tion. http://www.ling.upenn.edu/hist-corpora/ppcme2-
release-2/.
Charles T. Lewis and Charles Short, editors. 1879. A
Latin Dictionary. Clarendon Press, Oxford.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Harm Pinkster. 1990. Latin Syntax and Semantics. Rout-
ledge, London.
Susan Pintzuk and Plug Leendert. 2001. York-Helsinki
Parsed Corpus of Old English Poetry.
William Smith. 1854. A Dictionary of Greek and Roman
Geography. Walton and Maberly, London.
William Smith. 1873. A Dictionary of Greek and Roman
Biography and Mythology. Spottiswoode, London.
Ann Taylor, Mitchell Marcus, and Beatrice Santorini.
2003. The Penn Treebank: An overview. In Anne
Abeille?, editor, Treebanks: Building and Using Parsed
Corpora, pages 5?22. Kluwer Academic Publishers.
40
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352?361,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Latent Personas of Film Characters
David Bamman Brendan O?Connor Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dbamman,brenocon,nasmith}@cs.cmu.edu
Abstract
We present two latent variable models for
learning character types, or personas, in
film, in which a persona is defined as a
set of mixtures over latent lexical classes.
These lexical classes capture the stereo-
typical actions of which a character is the
agent and patient, as well as attributes by
which they are described. As the first
attempt to solve this problem explicitly,
we also present a new dataset for the
text-driven analysis of film, along with
a benchmark testbed to help drive future
work in this area.
1 Introduction
Philosophers and dramatists have long argued
whether the most important element of narrative
is plot or character. Under a classical Aristotelian
perspective, plot is supreme;1 modern theoretical
dramatists and screenwriters disagree.2
Without addressing this debate directly, much
computational work on narrative has focused on
learning the sequence of events by which a story
is defined; in this tradition we might situate sem-
inal work on learning procedural scripts (Schank
and Abelson, 1977; Regneri et al, 2010), narrative
chains (Chambers and Jurafsky, 2008), and plot
structure (Finlayson, 2011; Elsner, 2012; McIn-
tyre and Lapata, 2010; Goyal et al, 2010).
We present a complementary perspective that
addresses the importance of character in defining
1?Dramatic action . . . is not with a view to the representa-
tion of character: character comes in as subsidiary to the ac-
tions . . . The Plot, then, is the first principle, and, as it were,
the soul of a tragedy: Character holds the second place.? Po-
etics I.VI (Aristotle, 335 BCE).
2?Aristotle was mistaken in his time, and our scholars are
mistaken today when they accept his rulings concerning char-
acter. Character was a great factor in Aristotle?s time, and no
fine play ever was or ever will be written without it? (Egri,
1946, p. 94); ?What the reader wants is fascinating, complex
characters? (McKee, 1997, 100).
a story. Our testbed is film. Under this perspec-
tive, a character?s latent internal nature drives the
action we observe. Articulating narrative in this
way leads to a natural generative story: we first de-
cide that we?re going to make a particular kind of
movie (e.g., a romantic comedy), then decide on a
set of character types, or personas, we want to see
involved (the PROTAGONIST, the LOVE INTER-
EST, the BEST FRIEND). After picking this set, we
fill out each of these roles with specific attributes
(female, 28 years old, klutzy); with this cast of
characters, we then sketch out the set of events by
which they interact with the world and with each
other (runs but just misses the train, spills coffee
on her boss) ? through which they reveal to the
viewer those inherent qualities about themselves.
This work is inspired by past approaches that in-
fer typed semantic arguments along with narra-
tive schemas (Chambers and Jurafsky, 2009; Reg-
neri et al, 2011), but seeks a more holistic view
of character, one that learns from stereotypical at-
tributes in addition to plot events. This work also
naturally draws on earlier work on the unsuper-
vised learning of verbal arguments and semantic
roles (Pereira et al, 1993; Grenager and Manning,
2006; Titov and Klementiev, 2012) and unsuper-
vised relation discovery (Yao et al, 2011).
This character-centric perspective leads to two
natural questions. First, can we learn what those
standard personas are by how individual charac-
ters (who instantiate those types) are portrayed?
Second, can we learn the set of attributes and ac-
tions by which we recognize those common types?
How do we, as viewers, recognize a VILLIAN?
At its most extreme, this perspective reduces
to learning the grand archetypes of Joseph Camp-
bell (1949) or Carl Jung (1981), such as the HERO
or TRICKSTER. We seek, however, a more fine-
grained set that includes not only archetypes, but
stereotypes as well ? characters defined by a fixed
set of actions widely known to be representative of
352
a class. This work offers a data-driven method for
answering these questions, presenting two proba-
blistic generative models for inferring latent char-
acter types.
This is the first work that attempts to learn ex-
plicit character personas in detail; as such, we
present a new dataset for character type induction
in film and a benchmark testbed for evaluating fu-
ture work.3
2 Data
2.1 Text
Our primary source of data comes from 42,306
movie plot summaries extracted from the
November 2, 2012 dump of English-language
Wikipedia.4 These summaries, which have a
median length of approximately 176 words,5
contain a concise synopsis of the movie?s events,
along with implicit descriptions of the characters
(e.g., ?rebel leader Princess Leia,? ?evil lord Darth
Vader?). To extract structure from this data, we
use the Stanford CoreNLP library6 to tag and
syntactically parse the text, extract entities, and
resolve coreference within the document. With
this structured representation, we extract linguistic
features for each character, looking at immediate
verb governors and attribute syntactic dependen-
cies to all of the entity?s mention headwords,
extracted from the typed dependency tuples pro-
duced by the parser; we refer to ?CCprocessed?
syntactic relations described in de Marneffe and
Manning (2008):
? Agent verbs. Verbs for which the entity is an
agent argument (nsubj or agent).
? Patient verbs. Verbs for which the entity is
the patient, theme or other argument (dobj,
nsubjpass, iobj, or any prepositional argu-
ment prep *).
? Attributes. Adjectives and common noun
words that relate to the mention as adjecti-
val modifiers, noun-noun compounds, appos-
itives, or copulas (nsubj or appos governors,
or nsubj, appos, amod, nn dependents of an
entity mention).
3All datasets and software for replication can be found at
http://www.ark.cs.cmu.edu/personas.
4http://dumps.wikimedia.org/enwiki/
5More popular movies naturally attract more attention on
Wikipedia and hence more detail: the top 1,000 movies by
box office revenue have a median length of 715 words.
6http://nlp.stanford.edu/software/
corenlp.shtml
These three roles capture three different ways in
which character personas are revealed: the actions
they take on others, the actions done to them, and
the attributes by which they are described. For ev-
ery character we thus extract a bag of (r, w) tu-
ples, where w is the word lemma and r is one
of {agent verb,patient verb, attribute} as iden-
tified by the above rules.
2.2 Metadata
Our second source of information consists of char-
acter and movie metadata drawn from the Novem-
ber 4, 2012 dump of Freebase.7 At the movie
level, this includes data on the language, country,
release date and detailed genre (365 non-mutually
exclusive categories, including ?Epic Western,?
?Revenge,? and ?Hip Hop Movies?). Many of the
characters in movies are also associated with the
actors who play them; since many actors also have
detailed biographical information, we can ground
the characters in what we know of those real peo-
ple ? including their gender and estimated age at
the time of the movie?s release (the difference be-
tween the release date of the movie and the actor?s
date of birth).
Across all 42,306 movies, entities average 3.4
agent events, 2.0 patient events, and 2.1 attributes.
For all experiments described below, we restrict
our dataset to only those events that are among the
1,000 most frequent overall, and only characters
with at least 3 events. 120,345 characters meet this
criterion; of these, 33,559 can be matched to Free-
base actors with a specified gender, and 29,802 can
be matched to actors with a given date of birth. Of
all actors in the Freebase data whose age is given,
the average age at the time of movie is 37.9 (stan-
dard deviation 14.1); of all actors whose gender
is known, 66.7% are male.8 The age distribution
is strongly bimodal when conditioning on gender:
the average age of a female actress at the time of a
movie?s release is 33.0 (s.d. 13.4), while that of a
male actor is 40.5 (s.d. 13.7).
3 Personas
One way we recognize a character?s latent type
is by observing the stereotypical actions they
7http://download.freebase.com/
datadumps/
8Whether this extreme 2:1 male/female ratio reflects an
inherent bias in film or a bias in attention on Freebase (or
Wikipedia, on which it draws) is an interesting research ques-
tion in itself.
353
perform (e.g., VILLAINS strangle), the actions
done to them (e.g., VILLAINS are foiled and ar-
rested) and the words by which they are described
(VILLAINS are evil). To capture this intuition, we
define a persona as a set of three typed distribu-
tions: one for the words for which the character is
the agent, one for which it is the patient, and one
for words by which the character is attributively
modified. Each distribution ranges over a fixed set
of latent word classes, or topics. Figure 1 illus-
trates this definition for a toy example: a ZOMBIE
persona may be characterized as being the agent
of primarily eating and killing actions, the patient
of killing actions, and the object of dead attributes.
The topic labeled eat may include words like eat,
drink, and devour.
eat kill lov
e
dea
d
hap
py
agent
0.0
0.2
0.4
0.6
0.8
1.0
eat kill lov
e
dea
d
hap
py
patient
0.0
0.2
0.4
0.6
0.8
1.0
eat kill lov
e
dea
d
hap
py
attribute
0.0
0.2
0.4
0.6
0.8
1.0
Figure 1: A persona is a set of three distributions
over latent topics. In this toy example, the ZOM-
BIE persona is primarily characterized by being
the agent of words from the eat and kill topics, the
patient of kill words, and the object of words from
the dead topic.
4 Models
Both models that we present here simultaneously
learn three things: 1.) a soft clustering over words
to topics (e.g., the verb ?strangle? is mostly a type
of Assault word); 2.) a soft clustering over top-
ics to personas (e.g., VILLIANS perform a lot of
Assault actions); and 3.) a hard clustering over
characters to personas (e.g., Darth Vader is a VIL-
LAIN.) They each use different evidence: since
our data includes not only textual features (in the
form of actions and attributes of the characters) but
also non-textual information (such as movie genre,
age and gender), we design a model that exploits
this additional source of information in discrimi-
nating between character types; since this extra-
linguistic information may not always be avail-
able, we also design a model that learns only from
the text itself. We present the text-only model first
?
?
p
z?
w
r
?
?
?
W
E
D
?
p me
md
?
?
?
2
z?
w
r
?
?
?
W
E
D
P Number of personas (hyperparameter)
K Number of word topics (hyperparameter)
D Number of movie plot summaries
E Number of characters in movie d
W Number of (role, word) tuples used by character e
?k Topic k?s distribution over V words.
r Tuple role: agent verb, patient verb, attribute
?p,r Distribution over topics for persona p in role r
?d Movie d?s distribution over personas
pe Character e?s persona (integer, p ? {1..P})
j A specific (r, w) tuple in the data
zj Word topic for tuple j
wj Word for tuple j
? Concentration parameter for Dirichlet model
? Feature weights for regression model
?, ?2 Gaussian mean and variance (for regularizing ?)
md Movie features (from movie metadata)
me Entity features (from movie actor metadata)
?r , ? Dirichlet concentration parameters
Figure 2: Above: Dirichlet persona model (left)
and persona regression model (right). Bottom:
Definition of variables.
for simplicity. Throughout, V is the word vocab-
ulary size, P is the number of personas, and K is
the number of topics.
4.1 Dirichlet Persona Model
In the most basic model, we only use informa-
tion from the structured text, which comes as a
bag of (r, w) tuples for each character in a movie,
where w is the word lemma and r is the rela-
tion of the word with respect to the character (one
of agent verb, patient verb or attribute, as out-
lined in ?2.1 above). The generative story runs as
follows. First, let there be K latent word topics;
as in LDA (Blei et al, 2003), these are words that
will be soft-clustered together by virtue of appear-
ing in similar contexts. Each latent word cluster
354
?k ? Dir(?) is a multinomial over the V words in
the vocabulary, drawn from a Dirichlet parameter-
ized by ?. Next, let a persona p be defined as a set
of three multinomials ?p over these K topics, one
for each typed role r, each drawn from a Dirichlet
with a role-specific hyperparameter (?r).
Every document (a movie plot summary) con-
tains a set of characters, each of which is associ-
ated with a single latent persona p; for every ob-
served (r, w) tuple associated with the character,
we sample a latent topic k from the role-specific
?p,r. Conditioned on this topic assignment, the
observed word is drawn from ?k. The distribu-
tion of these personas for a given document is de-
termined by a document-specific multinomial ?,
drawn from a Dirichlet parameterized by ?.
Figure 2 (above left) illustrates the form of the
model. To simplify inference, we collapse out the
persona-topic distributions ?, the topic-word dis-
tributions ? and the persona distribution ? for each
document. Inference on the remaining latent vari-
ables ? the persona p for each character type and
the topic z for each word associated with that char-
acter ? is conducted via collapsed Gibbs sampling
(Griffiths and Steyvers, 2004); at each iteration,
for each character e, we sample their persona pe:
P (pe = k | p?e, z, ?, ?) ?
(
c?ed,k + ?k
)
??j
(c?erj ,k,zj+?rj )
(c?erj ,k,?+K?rj )
(1)
Here, c?ed,k is the count of all characters in docu-ment d whose current persona sample is also k
(not counting the current character e under con-
sideration);9 j ranges over all (rj , wj) tuples asso-
ciated with character e. Each c?erj ,k,zj is the countof all tuples with role rj and current topic zj used
with persona k. c?erj ,k,? is the same count, summingover all topics z. In other words, the probabil-
ity that character e embodies persona k is propor-
tional to the number of other characters in the plot
summary who also embody that persona (plus the
Dirichlet hyperparameter ?k) times the contribu-
tion of each observed word wj for that character,
given its current topic assignment zj .
Once all personas have been sampled, we sam-
9The?e superscript denotes counts taken without consid-
ering the current sample for character e.
ple the latent topics for each tuple as the following.
P (zj = k | p, z?j , w, r, ?, ?) ?
(c?jrj ,p,k+?rj )
(c?jrj ,p,?+K?rj )
?
(c?jk,wj+?)
(c?jk,?+V ?)
(2)
Here, conditioned on the current sample p for
the character?s persona, the probability that tuple
j originates in topic k is proportional to the num-
ber of other tuples with that same role rj drawn
from the same topic for that persona (c?jrj ,p,k), nor-malized by the number of other rj tuples associ-
ated with that persona overall (c?jrj ,p,?), multiplied
by the number of times word wj is associated with
that topic (c?jk,wj ) normalized by the total numberof other words associated with that topic overall
(c?jk,?).We optimize the values of the Dirichlet hyper-
parameters ?, ? and ? using slice sampling with a
uniform prior every 20 iterations for the first 500
iterations, and every 100 iterations thereafter. Af-
ter a burn-in phase of 10,000 iterations, we collect
samples every 10 iterations (to lessen autocorrela-
tion) until a total of 100 have been collected.
4.2 Persona Regression
To incorporate observed metadata in the form of
movie genre, character age and character gen-
der, we adopt an ?upstream? modeling approach
(Mimno and McCallum, 2008), letting those ob-
served features influence the conditional probabil-
ity with which a given character is expected to as-
sume a particular persona, prior to observing any
of their actions. This captures the increased likeli-
hood, for example, that a 25-year-old male actor in
an action movie will play an ACTION HERO than
he will play a VALLEY GIRL.
To capture these effects, each character?s la-
tent persona is no longer drawn from a document-
specific Dirichlet; instead, the P -dimensional sim-
plex is the output of a multiclass logistic regres-
sion, where the document genre metadata md and
the character age and gender metadatame together
form a feature vector that combines with persona-
specific feature weights to form the following log-
linear distribution over personas, with the proba-
bility for persona k being:
P (p = k | md,me, ?) = exp([md;me]
>?k)
1+PP?1j=1 exp([md;me]>?j)(3)
The persona-specific ? coefficients are learned
through Monte Carlo Expectation Maximization
355
(Wei and Tanner, 1990), in which we alternate be-
tween the following:
1. Given current values for ?, for all characters
e in all plot summaries, sample values of pe
and zj for all associated tuples.
2. Given input metadata features m and the as-
sociated sampled values of p, find the values
of ? that maximize the standard multiclass lo-
gistic regression log likelihood, subject to `2
regularization.
Figure 2 (above right) illustrates this model. As
with the Dirichlet persona model, inference on p
for step 1 is conducted with collapsed Gibbs sam-
pling; the only difference in the sampling prob-
ability from equation 1 is the effect of the prior,
which here is deterministically fixed as the output
of the regression.
P (pe = k | p?e, z, ?,md,me, ?) ?
exp([md;me]>?k)?
?
j
(c?erj ,k,zj+?rj )
(c?erj ,k,?+K?rj )
(4)
The sampling equation for the topic assign-
ments z is identical to that in equation 2. In
practice we optimize ? every 1,000 iterations, un-
til a burn-in phase of 10,000 iterations has been
reached; at this point we following the same sam-
pling regime as for the Dirichlet persona model.
5 Evaluation
We evaluate our methods in two quantitative ways
by measuring the degree to which we recover two
different sets of gold-standard clusterings. This
evaluation also helps offer guidance for model se-
lection (in choosing the number of latent topics
and personas) by measuring performance on an
objective task.
5.1 Character Names
First, we consider all character names that occur in
at least two separate movies, generally as a conse-
quence of remakes or sequels; this includes proper
names such as ?Rocky Balboa,? ?Oliver Twist,?
and ?Indiana Jones,? as well as generic type names
such as ?Gang Member? and ?The Thief?; to mini-
mize ambiguity, we only consider character names
consisting of at least two tokens. Each of these
names is used by at least two different characters;
for example, a character named ?Jason Bourne?
is portrayed in The Bourne Identity, The Bourne
Supremacy, and The Bourne Ultimatum. While
these characters are certainly free to assume dif-
ferent roles in different movies, we believe that,
in the aggregate, they should tend to embody the
same character type and thus prove to be a natu-
ral clustering to recover. 970 character names oc-
cur at least twice in our data, and 2,666 individual
characters use one of those names. Let those 970
character names define 970 unique gold clusters
whose members include the individual characters
who use that name.
5.2 TV Tropes
As a second external measure of validation, we
consider a manually created clustering presented
at the website TV Tropes,10 a wiki that col-
lects user-submitted examples of common tropes
(narrative, character and plot devices) found in
television, film, and fiction, among other me-
dia. While TV Tropes contains a wide range of
such conventions, we manually identified a set of
72 tropes that could reasonably be labeled char-
acter types, including THE CORRUPT CORPO-
RATE EXECUTIVE, THE HARDBOILED DETEC-
TIVE, THE JERK JOCK, THE KLUTZ and THE
SURFER DUDE.
We manually aligned user-submitted examples
of characters embodying these 72 character types
with the canonical references in Freebase to cre-
ate a test set of 501 individual characters. While
the 72 character tropes represented here are a more
subjective measure, we expect to be able to at least
partially recover this clustering.
5.3 Variation of Information
To measure the similarity between the two clus-
terings of movie characters, gold clusters G and
induced latent persona clusters C, we calculate the
variation of information (Meila?, 2007):
V I(G, C) = H(G) +H(C)? 2I(G, C) (5)
= H(G|C) +H(C|G) (6)
VI measures the information-theoretic distance
between the two clusterings: a lower value means
greater similarity, and VI = 0 if they are iden-
tical. Low VI indicates that (induced) clusters
and (gold) clusters tend to overlap; i.e., knowing a
character?s (induced) cluster usually tells us their
(gold) cluster, and vice versa. Variation of infor-
mation is a metric (symmetric and obeys triangle
10http://tvtropes.org
356
Character Names ?5.1 TV Tropes ?5.2
K Model P = 25 P = 50 P = 100 P = 25 P = 50 P = 100
25 Persona regression 7.73 7.32 6.79 6.26 6.13 5.74Dirichlet persona 7.83 7.11 6.44 6.29 6.01 5.57
50 Persona regression 7.59 7.08 6.46 6.30 5.99 5.65Dirichlet persona 7.57 7.04 6.35 6.23 5.88 5.60
100 Persona regression 7.58 6.95 6.32 6.11 6.05 5.49Dirichlet persona 7.64 6.95 6.25 6.24 5.91 5.42
Table 1: Variation of information between learned personas and gold clusters for different numbers of
topics K and personas P . Lower values are better. All values are reported in bits.
Character Names ?5.1 TV Tropes ?5.2
K Model P = 25 P = 50 P = 100 P = 25 P = 50 P = 100
25 Persona regression 62.8 (?41%) 59.5 (?40%) 53.7 (?33%) 42.3 (?31%) 38.5 (?24%) 33.1 (?25%)Dirichlet persona 54.7 (?27%) 50.5 (?26%) 45.4 (?17%) 39.5 (?20%) 31.7 (?28%) 25.1 (?21%)
50 Persona regression 63.1 (?42%) 59.8 (?42%) 53.6 (?34%) 42.9 (?30%) 39.1 (?33%) 31.3 (?20%)Dirichlet persona 57.2 (?34%) 49.0 (?23%) 44.7 (?16%) 39.7 (?30%) 31.5 (?32%) 24.6 (?22%)
100 Persona regression 63.1 (?42%) 57.7 (?39%) 53.0 (?34%) 43.5 (?33%) 32.1 (?28%) 26.5 (?22%)Dirichlet persona 55.3 (?30%) 49.5 (?24%) 45.2 (?18%) 39.7 (?34%) 29.9 (?24%) 23.6 (?19%)
Table 2: Purity scores of recovering gold clusters. Higher values are better. Each absolute purity score
is paired with its improvement over a controlled baseline of permuting the learned labels while keeping
the cluster proportions the same.
inequality), and has a number of other desirable
properties.
Table 1 presents the VI between the learned per-
sona clusters and gold clusters, for varying num-
bers of personas (P = {25, 50, 100}) and top-
ics (K = {25, 50, 100}). To determine signifi-
cance with respect to a random baseline, we con-
duct a permutation test (Fisher, 1935; Pitman,
1937) in which we randomly shuffle the labels of
the learned persona clusters and count the num-
ber of times in 1,000 such trials that the VI of
the observed persona labels is lower than the VI
of the permuted labels; this defines a nonparamet-
ric p-value. All results presented are significant at
p < 0.001 (i.e. observed VI is never lower than
the simulation VI).
Over all tests in comparison to both gold clus-
terings, we see VI improve as both P and, to
a lesser extent, K increase. While this may be
expected as the number of personas increase to
match the number of distinct types in the gold
clusters (970 and 72, respectively), the fact that VI
improves as the number of latent topics increases
suggests that more fine-grained topics are helpful
for capturing nuanced character types.11
The difference between the persona regression
model and the Dirichlet persona model here is not
11This trend is robust to the choice of cluster metric: here
VI and F -score have a correlation of ?0.87; as more latent
topics and personas are added, clustering improves (causing
the F -score to go up and the VI distance to go down).
significant; while VI allows us to compare mod-
els with different numbers of latent clusters, its re-
quirement that clusterings be mutually informative
places a high overhead on models that are funda-
mentally unidirectional (in Table 1, for example,
the room for improvement between two models
of the same P and K is naturally smaller than
the bigger difference between different P or K).
While we would naturally prefer a text-only model
to be as expressive as a model that requires po-
tentially hard to acquire metadata, we tease apart
whether a distinction actually does exist by evalu-
ating the purity of the gold clusters with respect to
the labels assigned them.
5.4 Purity
For gold clusters G = {g1 . . . gk} and inferred
clusters C = {c1 . . . cj} we calculate purity as:
Purity = 1N
?
k
max
j
|gk ? cj | (7)
While purity cannot be used to compare models of
different persona size P , it can help us distinguish
between models of the same size. A model can
attain perfect purity, however, by placing all char-
acters into a single cluster; to control for this, we
present a controlled baseline in which each char-
acter is assigned a latent character type label pro-
portional to the size of the latent clusters we have
learned (so that, for example, if one latent per-
sona cluster contains 3.2% of the total characters,
357
Batman
Jim 
Gordon
dark, major, henchman
shoot, aim, overpower
sentence, arrest, assign
Tony 
Stark
Jason 
Bourne
The 
Joker
shoot, aim, overpower
testify, rebuff, confess
hatch, vow, undergo
Van Helsing
Colin 
Sullivan
Dracula
The Departed
The Dark 
Knight
Iron Man
The Bourne 
Identity
approve, die, suffer
relent, refuse, agree
inherit live imagine
Jack 
Dawson
Rachel
Titanic
Figure 3: Dramatis personae of The Dark Knight (2008), illustrating 3 of the 100 character types learned
by the persona regression model, along with links from other characters in those latent classes to other
movies. Each character type is listed with the top three latent topics with which it is associated.
the probability of selecting that persona at random
is 3.2%). Table 2 presents each model?s absolute
purity score paired with its improvement over its
controlled permutation (e.g., ?41%).
Within each fixed-size partition, the use of
metadata yields a substantial improvement over
the Dirichlet model, both in terms of absolute pu-
rity and in its relative improvement over its sized-
controlled baseline. In practice, we find that while
the Dirichlet model distinguishes between charac-
ter personas in different movies, the persona re-
gression model helps distinguish between differ-
ent personas within the same movie.
6 Exploratory Data Analysis
As with other generative approaches, latent per-
sona models enable exploratory data analysis. To
illustrate this, we present results from the persona
regression model learned above, with 50 latent
lexical classes and 100 latent personas. Figure 3
visualizes this data by focusing on a single movie,
The Dark Knight (2008); the movie?s protagonist,
Batman, belongs to the same latent persona as De-
tective Jim Gordon, as well as other action movie
protagonists Jason Bourne and Tony Stark (Iron
Man). The movie?s antagonist, The Joker, belongs
to the same latent persona as Dracula from Van
Helsing and Colin Sullivan from The Departed, il-
lustrating the ability of personas to be informed
by, but still cut across, different genres.
Table 3 presents an exhaustive list of all 50 top-
ics, along with an assigned label that consists of
the single word with the highest PMI for that class.
Of note are topics relating to romance (unite,
marry, woo, elope, court), commercial transac-
tions (purchase, sign, sell, owe, buy), and the clas-
sic criminal schema from Chambers (2011) (sen-
tence, arrest, assign, convict, promote).
Table 4 presents the most frequent 14 personas
in our dataset, illustrated with characters from
the 500 highest grossing movies. The personas
learned are each three separate mixtures of the
50 latent topics (one for agent relations, one for
patient relations, and one for attributes), as illus-
trated in figure 1 above. Rather than presenting
a 3 ? 50 histogram for each persona, we illus-
trate them by listing the most characteristic top-
ics, movie characters, and metadata features asso-
ciated with it. Characteristic actions and features
are defined as those having the highest smoothed
pointwise mutual information with that class; ex-
emplary characters are those with the highest pos-
terior probability of being drawn from that class.
Among the personas learned are canonical male
action heroes (exemplified by the protagonists of
The Bourne Supremacy, Speed, and Taken), super-
heroes (Hulk, Batman and Robin, Hector of Troy)
and several romantic comedy types, largely char-
acterized by words drawn from the FLIRT topic,
including flirt, reconcile, date, dance and forgive.
358
Label Most characteristic words Label Most characteristic words
UNITE unite marry woo elope court SWITCH switch confirm escort report instruct
PURCHASE purchase sign sell owe buy INFATUATE infatuate obsess acquaint revolve concern
SHOOT shoot aim overpower interrogate kill ALIEN alien child governor bandit priest
EXPLORE explore investigate uncover deduce CAPTURE capture corner transport imprison trap
WOMAN woman friend wife sister husband MAYA maya monster monk goon dragon
WITCH witch villager kid boy mom INHERIT inherit live imagine experience share
INVADE invade sail travel land explore TESTIFY testify rebuff confess admit deny
DEFEAT defeat destroy transform battle inject APPLY apply struggle earn graduate develop
CHASE chase scare hit punch eat EXPEL expel inspire humiliate bully grant
TALK talk tell reassure assure calm DIG dig take welcome sink revolve
POP pop lift crawl laugh shake COMMAND command abduct invade seize surrender
SING sing perform cast produce dance RELENT relent refuse agree insist hope
APPROVE approve die suffer forbid collapse EMBARK embark befriend enlist recall meet
WEREWOLF werewolf mother parent killer father MANIPULATE manipulate conclude investigate conduct
DINER diner grandfather brother terrorist ELOPE elope forget succumb pretend like
DECAPITATE decapitate bite impale strangle stalk FLEE flee escape swim hide manage
REPLY reply say mention answer shout BABY baby sheriff vampire knight spirit
DEMON demon narrator mayor duck crime BIND bind select belong refer represent
CONGRATULATE congratulate cheer thank recommend REJOIN rejoin fly recruit include disguise
INTRODUCE introduce bring mock read hatch DARK dark major henchman warrior sergeant
HATCH hatch don exist vow undergo SENTENCE sentence arrest assign convict promote
FLIRT flirt reconcile date dance forgive DISTURB disturb frighten confuse tease scare
ADOPT adopt raise bear punish feed RIP rip vanish crawl drive smash
FAIRY fairy kidnapper soul slave president INFILTRATE infiltrate deduce leap evade obtain
BUG bug zombie warden king princess SCREAM scream faint wake clean hear
Table 3: Latent topics learned for K = 50 and P = 100. The words shown for each class are those with
the highest smoothed PMI, with the label being the single word with the highest PMI.
Freq Actions Characters Features
0.109 DARKm, SHOOTa,
SHOOTp
Jason Bourne (The Bourne Supremacy), Jack Traven
(Speed), Jean-Claude (Taken)
Action, Male, War
film
0.079 CAPTUREp,
INFILTRATEa, FLEEa
Aang (The Last Airbender), Carly (Transformers: Dark of
the Moon), Susan Murphy/Ginormica (Monsters vs. Aliens)
Female, Action,
Adventure
0.067 DEFEATa, DEFEATp,
INFILTRATEa
Glenn Talbot (Hulk), Batman (Batman and Robin), Hector
(Troy)
Action, Animation,
Adventure
0.060 COMMANDa, DEFEATp,
CAPTUREp
Zoe Neville (I Am Legend), Ursula (The Little Mermaid),
Joker (Batman)
Action, Adventure,
Male
0.046 INFILTRATEa,
EXPLOREa, EMBARKa
Peter Parker (Spider-Man 3), Ethan Hunt (Mission:
Impossible), Jason Bourne (The Bourne Ultimatum)
Male, Action, Age
34-36
0.036 FLIRTa, FLIRTp,
TESTIFYa
Mark Darcy (Bridget Jones: The Edge of Reason), Jerry
Maguire (Jerry Maguire), Donna (Mamma Mia!)
Female, Romance
Film, Comedy
0.033 EMBARKa, INFILTRATEa,
INVADEa
Perseus (Wrath of the Titans), Maximus Decimus Meridius
(Gladiator), Julius (Twins)
Male, Chinese
Movies, Spy
0.027 CONGRATULATEa,
CONGRATULATEp,
SWITCHa
Professor Albus Dumbledore (Harry Potter and the
Philosopher?s Stone), Magic Mirror (Shrek), Josephine
Anwhistle (Lemony Snicket?s A Series of Unfortunate
Events)
Age 58+, Family
Film, Age 51-57
0.025 SWITCHa, SWITCHp,
MANIPULATEa
Clarice Starling (The Silence of the Lambs), Hannibal
Lecter (The Silence of the Lambs), Colonel Bagley (The
Last Samurai)
Age 58+, Male,
Age 45-50
0.022 REPLYa, TALKp, FLIRTp Graham (The Holiday), Abby Richter (The Ugly Truth),
Anna Scott (Notting Hill)
Female, Comedy,
Romance Film
0.020 EXPLOREa, EMBARKa,
CAPTUREp
Harry Potter (Harry Potter and the Philosopher?s Stone),
Harry Potter (Harry Potter and the Chamber of Secrets),
Captain Leo Davidson (Planet of the Apes)
Adventure, Family
Film, Horror
0.018 FAIRYm, COMMANDa,
CAPTUREp
Captain Jack Sparrow (Pirates of the Caribbean: At
World?s End), Shrek (Shrek), Shrek (Shrek Forever After)
Action, Family
Film, Animation
0.018 DECAPITATEa,
DECAPITATEp, RIPa
Jericho Cane (End of Days), Martin Riggs (Lethal Weapon
2), Gabriel Van Helsing (Van Helsing)
Horror, Slasher,
Teen
0.017 APPLYa, EXPELp,
PURCHASEp
Oscar (Shark Tale), Elizabeth Halsey (Bad Teacher), Dre
Parker (The Karate Kid)
Female, Teen,
Under Age 22
Table 4: Of 100 latent personas learned, we present the top 14 by frequency. Actions index the latent
topic classes presented in table 3; subscripts denote whether the character is predominantly the agent (a),
patient (p) or is modified by an attribute (m).
359
7 Conclusion
We present a method for automatically inferring
latent character personas from text (and metadata,
when available). While our testbed has been tex-
tual synopses of film, this approach is easily ex-
tended to other genres (such as novelistic fiction)
and to non-fictional domains as well, where the
choice of portraying a real-life person as embody-
ing a particular kind of persona may, for instance,
give insight into questions of media framing and
bias in newswire; self-presentation of individual
personas likewise has a long history in communi-
cation theory (Goffman, 1959) and may be use-
ful for inferring user types for personalization sys-
tems (El-Arini et al, 2012). While the goal of this
work has been to induce a set of latent character
classes and partition all characters among them,
one interesting question that remains is how a spe-
cific character?s actions may informatively be at
odds with their inferred persona, given the choice
of that persona as the single best fit to explain the
actions we observe. By examining how any indi-
vidual character deviates from the behavior indica-
tive of their type, we might be able to paint a more
nuanced picture of how a character can embody a
specific persona while resisting it at the same time.
Acknowledgments
We thank Megan Morrison at the CMU School of
Drama for early conversations guiding our work,
as well as the anonymous reviewers for helpful
comments. The research reported in this article
was supported by U.S. National Science Founda-
tion grant IIS-0915187 and by an ARCS scholar-
ship to D.B. This work was made possible through
the use of computing resources made available by
the Pittsburgh Supercomputing Center.
References
Aristotle. 335 BCE. Poetics, translated by Samuel H.
Butcher (1902). Macmillan, London.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Joseph Campbell. 1949. The Hero with a Thousand
Faces. Pantheon Books.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meet-
ing of the ACL.
Nathanael Chambers. 2011. Inducing Event Schemas
and their Participants from Unlabeled Text. Ph.D.
thesis, Stanford University.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.
Lajos Egri. 1946. The Art of Dramatic Writing. Simon
and Schuster, New York.
Khalid El-Arini, Ulrich Paquet, Ralf Herbrich, Jurgen
Van Gael, and Blaise Agu?era y Arcas. 2012. Trans-
parent user models for personalization. In Proceed-
ings of the 18th ACM SIGKDD.
Micha Elsner. 2012. Character-based kernels for nov-
elistic plot structure. In Proceedings of the 13th
Conference of the EACL.
Mark Alan Finlayson. 2011. Learning Narrative
Structure from Annotated Folktales. Ph.D. thesis,
MIT.
R. A. Fisher. 1935. The Design of Experiments. Oliver
and Boyde, Edinburgh and London.
Erving Goffman. 1959. The Presentation of the Self in
Everyday Life. Anchor.
Amit Goyal, Ellen Riloff, and Hal Daume?, III. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of the 2010 Con-
ference on EMNLP.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on EMNLP.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Carl Jung. 1981. The Archetypes and The Collective
Unconscious, volume 9 of Collected Works. Bollin-
gen, Princeton, NJ, 2nd edition.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the ACL.
Association for Computational Linguistics.
Robert McKee. 1997. Story: Substance, Structure,
Style and the Principles of Screenwriting. Harper-
Colllins.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
David Mimno and Andrew McCallum. 2008. Topic
models conditioned on arbitrary features with
dirichlet-multinomial regression. In Proceedings of
UAI.
360
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st Annual Meeting of the ACL.
E. J. G. Pitman. 1937. Significance tests which may
be applied to samples from any population. Supple-
ment to the Journal of the Royal Statistical Society,
4(1):119?130.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the ACL.
Michaela Regneri, Alexander Koller, Josef Ruppen-
hofer, and Manfred Pinkal. 2011. Learning script
participants from unlabeled data. In Proceedings of
the Conference on Recent Advances in Natural Lan-
guage Processing.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals, and understanding: An inquiry into
human knowledge structures. Lawrence Erlbaum,
Hillsdale, NJ.
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
EACL.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo implementation of the EM algorithm and the
poor man?s data augmentation algorithms. Journal
of the American Statistical Association, 85:699?704.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on EMNLP.
361
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 370?379,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Bayesian Mixed Effects Model of Literary Character
David Bamman
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dbamman@cs.cmu.edu
Ted Underwood
Department of English
University of Illinois
Urbana, IL 61801, USA
tunder@illinois.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of automatically
inferring latent character types in a collec-
tion of 15,099 English novels published
between 1700 and 1899. Unlike prior
work in which character types are assumed
responsible for probabilistically generat-
ing all text associated with a character,
we introduce a model that employs mul-
tiple effects to account for the influence
of extra-linguistic information (such as au-
thor). In an empirical evaluation, we find
that this method leads to improved agree-
ment with the preregistered judgments of a
literary scholar, complementing the results
of alternative models.
1 Introduction
Recent work in NLP has begun to exploit the
potential of entity-centric modeling for a vari-
ety of tasks: Chambers (2013) places entities at
the center of probabilistic frame induction, show-
ing gains over a comparable event-centric model
(Cheung et al, 2013); Bamman et al (2013) ex-
plicitly learn character types (or ?personas?) in a
dataset of Wikipedia movie plot summaries; and
entity-centric models form one dominant approach
in coreference resolution (Durrett et al, 2013;
Haghighi and Klein, 2010).
One commonality among all of these very dif-
ferent probabilistic approaches is that each learns
statistical regularities about how entities are de-
picted in text (whether for the sake of learning
a set of semantic roles, character types, or link-
ing anaphora to the entities to which they refer).
In each case, the text we observe associated with
an entity in a document is directly dependent on
the class of entity?and only that class. This re-
lationship between entity and text is a theoreti-
cal assumption, with important consequences for
learning: entity types learned in this way will
be increasingly similar the more similar the do-
main, author, and other extra-linguistic effects are
between them.
1
While in many cases the topi-
cally similar types learned under this assumption
may be desirable, we explore here the alterna-
tive, in which entity types are learned in a way
that controls for such effects. In introducing a
model based on different assumptions, we provide
a method that complements past work and pro-
vides researchers with more flexible tools to infer
different kinds of character types.
We focus here on the literary domain, exploring
a large collection of 15,099 English novels pub-
lished in the 18th and 19th centuries. By account-
ing for the influence of individual authors while in-
ferring latent character types, we are able to learn
personas that cut across different authors more ef-
fectively than if we learned types conditioned on
the text alone. Modeling the language used to de-
scribe a character as the joint result of that charac-
ter?s latent type and of other formal variables al-
lows us to test multiple models of character and
assess their value for different interpretive prob-
lems. As a test case, we focus on separating char-
acter from authorial diction, but this approach can
readily be generalized to produce models that pro-
visionally distinguish character from other factors
(such as period, genre, or point of view) as well.
2 Literary Background
Inferring character is challenging from a liter-
ary perspective partly because scholars have not
reached consensus about the meaning of the term.
It may seem obvious that a ?character? is a repre-
sentation of a (real or imagined) person, and many
humanists do use the term that way. But there is
1
For example, many entities in Early Modern English
texts may be judged to be more similar to each other than
to entities from later texts simply by virtue of using hath and
other archaic verb forms.
370
an equally strong critical tradition that treats char-
acter as a formal dimension of narrative. To de-
scribe a character as a ?blocking figure? or ?first-
person narrator,? for instance, is a statement less
about the attributes of an imagined person than
about a narrative function (Keen, 2003). Charac-
ters are in one sense collections of psychological
or moral attributes, but in another sense ?word-
masses? (Forster, 1927). This tension between
?referential? and ?formalist? models of character
has been a centrally ?divisive question in . . . liter-
ary theory? (Woloch, 2003).
Considering primary source texts (as distinct
from plot summaries) forces us to confront new
theoretical questions about character. In a plot
summary (such as those explored by Bamman et
al., 2013), a human reader may already have used
implicit models of character to extract high-level
features. To infer character types from raw narra-
tive text, researchers need to explicitly model the
relationship of character to narrative form. This is
not a solved problem, even for human readers.
For instance, it has frequently been remarked
that the characters of Charles Dickens share
certain similarities?including a reliance on tag
phrases and recurring tics. A referential model
of character might try to distinguish this common
stylistic element from underlying ?personalities.?
A strictly formalist model might refuse to separate
authorial diction from character at all. In prac-
tice, human readers can adopt either perspective:
we recognize that characters have a ?Dickensian?
quality but also recognize that a Dickens villain is
(in one sense) more like villains in other authors
than like a Dickensian philanthropist. Our goal is
to show that computational methods can support
the same range of perspectives?allowing a provi-
sional, flexible separation between the referential
and formal dimensions of narrative.
3 Data
The dataset for this work consists of 15,099 dis-
tinct narratives drawn from HathiTrust Digital Li-
brary.
2
From an initial collection of 469,200 vol-
umes written in English and published between
1700 and 1899 (including poetry, drama, and non-
fiction as well as prose narrative), we extract
32,209 volumes of prose fiction, remove dupli-
cates and fuse multi-volume works to create the fi-
nal dataset. Since the original texts were produced
2
http://www.hathitrust.org
by scanning and running OCR on physical books,
we automatically correct common OCR errors and
trim front and back matter from the volumes using
the page-level classifiers and HMM of Underwood
et al (2013)
Many aspects of this process would be sim-
pler if we used manually-corrected texts, such as
those drawn from Project Gutenberg. But we hope
to produce research that has historical as well as
computational significance, and doing so depends
on the provenance of a collection. Gutenberg?s
decentralized selection process tends to produce
exceptionally good coverage of currently-popular
genres like science fiction, whereas HathiTrust ag-
gregates university libraries. Library collections
are not guaranteed to represent the past perfectly,
but they are larger, and less strongly shaped by
contemporary preferences.
The goal of this work is to provide a method to
infer a set of character types in an unsupervised
fashion from the data. As with prior work (Bam-
man et al, 2013), we define this target, a character
persona, as a distribution over several categories
of typed dependency relations:
3
1. agent: the actions of which a character is
the agent (i.e., verbs for which the character
holds an nsubj or agent relation).
2. patient: the actions of which a character is
the patient (i.e., verbs for which the character
holds a dobj or nsubjpass relation).
3. possessive: the objects that a character pos-
sesses (i.e., all words for which the character
holds a poss relation).
4. predicative: attributes predicated of a char-
acter (i.e., adjectives or nouns holding an
nsubj relation to the character, with an inflec-
tion of be as a child).
This set captures the constellation of what a
character does and has done to them, what they
possess, and what they are described as being.
While previous work uses the Stanford
CoreNLP toolkit to identify characters and extract
typed dependencies for them, we found this
approach to be too slow for the scale of our data (a
total of 1.8 billion tokens); in particular, syntactic
parsing, with cubic complexity in sentence length,
and out-of-the-box coreference resolution (with
thousands of potential antecedents) prove to be
3
All categories are described using the Stanford typed de-
pendencies (de Marneffe and Manning, 2008), but any syn-
tactic formalism is equally applicable.
371
the biggest bottlenecks.
Before addressing character inference, we
present here a prerequisite NLP pipeline that
scales well to book-length documents.
4
This
pipeline uses the Stanford POS tagger (Toutanova
et al, 2003), the linear-time MaltParser (Nivre et
al., 2007) for dependency parsing (trained on Stan-
ford typed dependencies), and the Stanford named
entity recognizer (Finkel et al, 2005). It includes
the following components for clustering charac-
ter name mentions, resolving pronominal corefer-
ence, and reducing vocabulary dimensionality.
3.1 Character Clustering
First, let us terminologically distinguish between a
character mention in a text (e.g., the token Tom on
page 141 of The Adventures of Tom Sawyer) and a
character entity (e.g., TOM SAWYER the character,
to which that token refers). To resolve the former
to the latter, we largely follow Davis et al (2003)
and Elson et al (2010): we define a set of initial
characters corresponding to each unique charac-
ter name that is not a subset of another (e.g., Mr.
Tom Sawyer) and deterministically create a set of
allowable variants for each one (Mr. Tom Sawyer
? Tom, Sawyer, Tom Sawyer, Mr. Sawyer, and
Mr. Tom); then, from the beginning of the book
to the end, we greedily assign each mention to the
most recently linked entity for whom it is a vari-
ant. The result constitutes our set of characters,
with all mentions partitioned among them.
3.2 Pronominal Coreference Resolution
While the character clustering stage is essentially
performing proper noun coreference resolution,
approximately 74% of references to characters in
books come in the form of pronouns.
5
To resolve
this more difficult class at the scale of an entire
book, we train a log-linear discriminative classifier
only on the task of resolving pronominal anaphora
(i.e., ignoring generic noun phrases such as the
paint or the rascal).
For this task, we annotated a set of 832 coref-
erence links in 3 books (Pride and Prejudice, The
Turn of the Screw, and Heart of Darkness) and fea-
turized coreference/antecedent pairs with:
4
All code is available at http://www.ark.cs.cmu.
edu/literaryCharacter
5
Over all 15,099 narratives, the average number of char-
acter proper name mentions is 1,673; the average number of
gendered singular pronouns (he, she, him, his, her) is 4,641.
1. The syntactic dependency path from a
pronoun to its potential antecedent (e.g.,
dobj?pred??pred?nsubj (where ? de-
notes movement across sentence boundaries).
2. The salience of the antecedent character (de-
fined as the count of that character?s named
mentions in the previous 500 words).
3. The antecedent part of speech.
4. Whether or not the pronoun and antecedent
appear in the same quotation scope (false if
one appears in a quotation and one outside).
5. Whether or not the two agree for gender.
6. The syntactic tree distance between the two.
7. The linear (word) distance between the two.
With this featurization and training data, we train
a binary logistic regression classifier with `
1
regu-
larization (where negative examples are comprised
of all character entities in the previous 100 words
not labeled as the true antecedent). In a 10-fold
cross-validation on predicting the true nearest an-
tecedent for a pronominal anaphor, this method
achieves an average accuracy of 82.7%.
With this trained model, we then select the
highest-scoring antecedent within 100 words for
each pronominal anaphor in our data.
3.3 Dimensionality Reduction
To manage the degrees of freedom in the model
described in ?4, we perform dimensionality reduc-
tion on the vocabulary by learning word embed-
dings with a log-linear continuous skip-gram lan-
guage model (Mikolov et al, 2013) on the entire
collection of 15,099 books. This method learns a
low-dimensional real-valued vector representation
of each word to predict all of the words in a win-
dow around it; empirically, we find that with a suf-
ficient window size (we use n = 10), these word
embeddings capture semantic similarity (placing
topically similar words near each other in vector
space).
6
We learn a 100-dimensional embedding
for each of the 512,344 words in our vocabulary.
To create a partition over the vocabulary, we
use hard K-means clustering (with Euclidean dis-
tance) to group the 512,344 word types into 1,000
clusters. We then agglomeratively cluster those
1,000 groups to assign bitstring representations to
each one, forming a balanced binary tree by only
merging existing clusters at equal levels in the hi-
6
In comparison, Brown et al (1992) clusters learned from
the same data capture syntactic similarity (placing function-
ally similar words in the same cluster).
372
01
0
1
0
1
0111001110: hat coat cap cloak handkerchief
0111001111: pair boots shoes gloves leather
0111001100: dressed costume uniform clad clothed
0111001101: dress clothes wore worn wear
01110011 ?
Figure 1: Bitstring representations of neural agglomerative clusters, illustrating the leaf nodes in a binary tree rooted in the
prefix 01110011. Bitstring encodings of intermediate nodes and terminal leaves result by following the left (0) and right (1)
branches of the merge tree created through agglomerative clustering.
erarchy. We use Euclidean distance as a funda-
mental metric and a group-average similarity func-
tion for calculating the distance between groups.
Fig. 1 illustrates four of the 1,000 learned clusters.
4 Model
In order to separate out the effects that a charac-
ter?s persona has on the words that are associated
with them (as opposed to other factors, such as
time period, genre, or author), we adopt a hierar-
chical Bayesian approach in which the words we
observe are generated conditional on a combina-
tion of different effects captured in a log-linear (or
?maximum entropy?) distribution.
Maximum entropy approaches to language
modeling have been used since Rosenfeld (1996)
to incorporate long-distance information, such as
previously-mentioned trigger words, into n-gram
language models. This work has since been ex-
tended to a Bayesian setting by applying both
a Gaussian prior (Chen and Rosenfeld, 2000),
which dampens the impact of any individual fea-
ture, and sparsity-inducing priors (Kazama and
Tsujii, 2003; Goodman, 2004), which can drive
many feature weights to 0. The latter have been
applied specifically to the problem of estimating
word probabilities with sparse additive generative
(SAGE) models (Eisenstein et al, 2011), where
sparse extra-linguistic effects can influence a word
probability in a larger generative setting.
In contrast to previous work in which the prob-
ability of a word linked to a character is depen-
dent entirely on the character?s latent persona, in
our model, we see the probability of a word as
dependent on: (i) the background likelihood of
the word, (ii) the author, so that a word becomes
more probable if a particular author tends to use it
more, and (iii) the character?s persona, so that a
word is more probable if appearing with a partic-
ular persona. Intuitively, if the author Jane Austen
is associated with a high weight for the word man-
ners, and all personas have little effect for this
word, then manners will have little impact on de-
ciding which persona a particular Austen character
embodies, since its presence is explained largely
by Austen having penned the word. While we ad-
dress only the author as an observed effect, this
model is easily extended to other features as well,
including period, genre, point of view, and others.
The generative story runs as follows (Figure 2
depicts the full graphical model): Let there be
M unique authors in the data, P latent personas
(a hyperparameter to be set), and V words in
the vocabulary (in the general setting these may
be word types; in our data the vocabulary is the
set of 1,000 unique cluster IDs). Each role type
r ? {agent,patient,possessive,predicative}
and vocabulary word v (here, a cluster ID)
is associated with a real-valued vector ?
r,v
=
[?
meta
r,v
, ?
pers
r,v
, ?
0
r,v
] of length M + P + 1. The first
M + P elements are drawn from a Laplace prior
with mean ? = 0 and scale ? = 1; the last el-
ement ?
0
r,v
is an unregularized bias term account-
ing for the background. Each element in this vec-
tor captures the log-additive effect of each author,
persona, and the background distribution on the
word?s probability (Eq. 1, below).
Much like latent Dirichlet alocation (Blei et al,
2003), each document d in our dataset draws a
multinomial distribution ?
d
over personas from a
shared Dirichlet prior ?, which captures the pro-
portion of each character type in that particular
document. Every character c in the document
draws its persona p from this document-specific
multinomial. Given document metadata m (here,
one of a set of M authors) and persona p, each tu-
ple of a role r with word w is assumed to be drawn
from Eq. 1 in Fig. 3. This SAGE model can be
understood as a log-linear distribution with three
kinds of features (metadata, persona, and back-
373
P (w | m, p, r, ?) = exp
(
?
meta
r,w
[m] + ?
pers
r,w
[p] + ?
0
r,w
)
/
V
?
v=1
exp
(
?
meta
r,v
[m] + ?
pers
r,v
[p] + ?
0
r,v
)
(1)
P (b | m, p, r, ?) =
n?1
?
j=0
?
?
?
logit
?1
(
?
meta
r,b
1:j
[m] + ?
pers
r,b
1:j
[p] + ?
0
r,b
1:j
)
if b
j+1
= 1
1? logit
?1
(
?
meta
r,b
1:j
[m] + ?
pers
r,b
1:j
[p] + ?
0
r,b
1:j
)
otherwise
(2)
Figure 3: Parameterizations of the SAGE word distribution. Eq. 1 is a ?flat? multinomial logistic regression with one ?-vector
per role and word. Eq. 2 uses the hierarchical softmax formulation, with one ?-vector per role and node in the binary tree of
word clusters, giving a distribution over bit strings (b) with the same number of parameters as Eq. 1.
ground bias).
4.1 Hierarchical Softmax
The partition function in Eq. 1 can lead to slow
inference for any reasonably-sized vocabulary. To
address this, we reparameterize the model by ex-
ploiting the structure of the agglomerative clus-
tering in ?3.3 to perform a hierarchical softmax,
following Goodman (2001), Morin and Bengio
(2005) and Mikolov et al (2013).
The bitstring representations by which we en-
code each word in the vocabulary serve as natural,
and inherently meaningful, intermediate classes
that correspond to semantically related subsets of
the vocabulary, with each bitstring prefix denoting
one such class. Longer bitstrings correspond to
more fine-grained classes. In the example shown
in Figure 1, 011100111 is one such intermediate
class, containing the union of pair, boots, shoes,
gloves leather and hat, coat, cap cloak, handker-
chief. Because these classes recursively partition
the vocabulary, they offer a convenient way to
reparameterize the model through the chain rule
of probability.
Consider, for example, a word represented as
the bitstring c = 01011; calculating P (c =
01011)?we suppress conditioning variables for
clarity?involves the product: P (c
1
= 0) ?
P (c
2
= 1 | c
1
= 0) ? P (c
3
= 0 | c
1:2
=
01) ? P (c
4
= 1 | c
1:3
= 010) ? P (c
5
= 1 |
c
1:4
= 0101).
Since each multiplicand involves a binary pre-
diction, we can avoid partition functions and use
the classic binary logistic regression.
7
We have
converted the V -way multiclass logistic regression
problem of Eq. 1 into a sequence of log V evalua-
tions (assuming a perfectly balanced tree). Given
7
Recall that logistic regression lets P
LR
(y = 1 | x, ?) =
logit
?1
(x
>
?) = 1/(1 + exp?x
>
?) for binary dependent
variable y, independent variables x, and coefficients ?.
m, p, and r (as above) we let b = b
1
b
2
? ? ? b
n
de-
note the bitstring representation of a word cluster,
and the distribution is given by Eq. 2 in Fig. 3.
In this paramaterization, rather than one ?-
vector for each role and vocabulary term, we have
one ?-vector for each role and conditional binary
decision in the tree (each bitstring prefix). Since
the tree is binary with V leaves, this yields the
same total number of parameters. As Goodman
(2001) points out, while this reparameterization is
exact for true probabilities, it remains an approx-
imation for estimated models (with generalization
behavior dependent on how well the class hierar-
chy is supported by the data). In addition to en-
abling faster inference, one advantage of the bit-
string representation and the hierarchical softmax
parameterization is that we can easily calculate
probabilities of clusters at different granularities.
4.2 Inference
Our primary quantities of interest in this model
are p (the personas for each character) and ?, the
effects that each author and persona have on the
probability of a word. Rather than adopting a fully
Bayesian approach (e.g., sampling all variables),
we infer these values using stochastic EM, alter-
nating between collapsed Gibbs sampling for each
p and maximizing with respect to ?.
Collapsed Gibbs for personas.
8
At each step,
the required quantity is the probability that char-
acter c in document d has persona z, given ev-
erything else. This is proportional to the number
of other characters in document d who also (cur-
rently) have that persona (plus the Dirichlet hy-
perparameter which acts as a smoother) times the
probability (under p
d,c
= z) of all of the words
8
We assume the reader is familiar with collapsed Gibbs
sampling as used in latent-variable NLP models.
374
??
p
w
r
m
?
?
?
W
C
D
P Number of personas (hyperparameter)
D Number of documents
C
d
Number of characters in document d
W
d,c
Number of (cluster, role) tuples for character c
m
d
Metadata for document d (ranges over M authors)
?
d
Document d?s distribution over personas
p
d,c
Character c?s persona
j An index for a ?r, w? tuple in the data
w
j
Word cluster ID for tuple j
r
j
Role for tuple j ? {agent, patient, poss, pred}
? Coefficients for the log-linear language model
?, ? Laplace mean and scale (for regularizing ?)
? Dirichlet concentration parameter
Figure 2: Above: Probabilistic graphical model. Observed
variables are shaded, latent variables are clear, and collapsed
variables are dotted. Below: Definition of variables.
observed in each role r for that character:
(count(z; p
d,?c
) + ?
z
)?
R
?
r=1
?
j:r
j
=r
P (b
j
| m, p, r, ?)
(3)
The metadata features (like author, etc.) influence
this probability by being constant for all choices
of z; e.g., if the coefficient learned for Austen for
vocabulary term manners is high and all coeffi-
cients for all z are close to zero, then the proba-
bility of manners will change little under different
choices of z. Eq. 3 contains one multiplicand for
every word associated with a character, and only
one term reflecting the influence of the shared doc-
ument multinomial. The implication is that, for
major characters with many observed words, the
words will dominate the choice of persona; where
the document influence would have a bigger effect
is with characters for whom we don?t have much
data. In that case, it can act as a kind of informed
background; given what little data we have for that
character, it would nudge us toward the character
types that the other characters in the book embody.
Given an assignment of all p, we choose ?
to maximize the conditional log-likelihood of the
words, as represented by their bitstring cluster IDs,
given the observed author and background effects
and the sampled personas. This equates to solving
4V `
1
-regularized logistic regressions (see Eq. 2
in Figure 3), one for each role type and bitstring
prefix, each with M + P + 1 parameters. We ap-
ply OWL-QN (Andrew and Gao, 2007) to mini-
mize the `
1
-regularized objective with an absolute
convergence threshold of 10
?5
.
5 Evaluation
While standard NLP and machine learning prac-
tice is to evaluate the performance of an algorithm
on a held-out gold standard, articulating what a
true ?persona? might be for a character is inher-
ently problematic. Rather, we evaluate the perfor-
mance and output of our model by preregistering
a set of 29 hypotheses of varying scope and diffi-
culty and comparing the performance of different
models in either confirming, or failing to confirm,
those hypotheses. This kind of evaluation was pre-
viously applied to a subjective text measurement
problem by Sim et al (2013).
All hypotheses were created by a literary
scholar with specialization in the period to not
only give an empirical measure of the strengths
and weaknesses of different models, but also to
help explore exactly what the different models
may, or may not, be learning. All preregistered hy-
potheses establish the degrees of similarity among
three characters, taking the form: ?character X is
more similar to character Y than either X or Y is
to a distractor character Z?; for a given model and
definition of distance under that model, each hy-
pothesis yields two yes/no decisions that we can
evaluate:
? distance(X,Y ) < distance(X,Z)
? distance(X,Y ) < distance(Y,Z)
To tease apart the different kinds of similarities
we hope to explore, we divide the hypotheses into
four classes:
375
A. This class constitutes sanity checks: charac-
ter X and Y are more similar to each other
in every way than to character Z. E.g.: Eliz-
abeth Bennet in Pride and Prejudice resem-
bles Elinor Dashwood in Sense and Sensibil-
ity (Jane Austen) more than either character
resembles Allen Quatermain in Allen Quater-
main (H. Rider Haggard). (Austenian protag-
onists should resemble each other more than
they resemble a grizzled hunter.)
B. This class captures our ability to identify two
characters in the same author as being more
similar to each other than to a closely re-
lated character in a different author. E.g.:
Wickham in Pride and Prejudice resembles
Willoughby in Sense and Sensibility (Jane
Austen) more than either character resem-
bles Mr. Rochester in Jane Eyre (Charlotte
Bront?e).
C. This class captures our ability to discrimi-
nate among similar characters in the same au-
thor. In these hypotheses, two characters X
and Y from the same author are more simi-
lar to each other than to a third character Z
from that same author. E.g.: Wickham in
Pride and Prejudice (Jane Austen) resembles
Willoughby in Sense and Sensibility more
than either character resembles Mr. Darcy in
Pride and Prejudice.
D. This class constitutes more difficult, ex-
ploratory hypotheses, including differences
among point of view. E.g.: Montoni in
Mysteries of Udolpho (Radcliffe) resem-
bles Heathcliff in Wuthering Heights (Emily
Bront?e) more than either resembles Mr. Ben-
net in Pride and Prejudice. (Testing our
model?s ability to discern similarities in spite
of elapsed time.)
All 29 hypotheses can be found in a supplemen-
tary technical report (Bamman et al, 2014). We
emphasize that the full set of hypotheses was
locked before the model was estimated.
6 Experiments
Part of the motivation of our mixed effects model
is to be able to tackle hypothesis class C?by fac-
toring out the influence of a particular author on
the learning of personas, we would like to be able
to discriminate between characters that all have
a common authorial voice. In contrast, the Per-
sona Regression model of Bamman et al (2013),
which uses metadata variables (like authorship)
to encourage entities with similar covariates to
have similar personas, reflects an assumption that
makes it likely to perform well at class B.
To judge their respective strengths on different
hypothesis classes, we evaluate three models:
1. The mixed-effects Author/Persona model
(described above), which includes author in-
formation as a metadata effect; here, each
?-vector (of length M + P + 1) contains a
parameter for each of the distinct authors in
our data, a parameter for each persona, and a
background parameter.
2. A Basic persona model, which ablates au-
thor information but retains the same log-
linear architecture; here, the ?-vector is of
size P +1 and does not model author effects.
3. The Persona Regression model of Bam-
man et al (2013).
All models are run with P ? {10, 25, 50, 100,
250} personas; Persona Regression addition-
ally uses K = 25 latent topics. All configura-
tions use the full dataset of 15,099 novels, and all
characters with at least 25 total roles (a total of
257,298 entities). All experiments are run with
50 iterations of Gibbs sampling to collect samples
for the personas p, alternating with maximization
steps for ?. The value of ? is optimized using slice
sampling (with a non-informative prior) every 5
iterations. The value of ? is held constant at 1.
At the end of inference, we calculate the posterior
distributions over personas for all characters as the
sampling probability of the final iteration.
To formally evaluate ?similarity? between two
characters, we measure the Jensen-Shannon diver-
gence between personas (calculated as the average
JS distance over the cluster distributions for each
role type), marginalizing over the characters? pos-
terior distributions over personas; two characters
with a lower JS divergence are judged to be more
similar than two characters with a higher one.
As a Baseline, we also evaluate all hypotheses
on a model with no latent variables whatsoever,
which instead measures similarity as the average
JS divergence between the empirical word distri-
butions over each role type.
Table 1 presents the results of this compari-
son; for all models with latent variables, we re-
port the average of 5 sampling runs with different
random initializations. Figure 4 provides a syn-
376
P Model
Hypothesis Class
A B C D
250
Author/Persona 1.00 0.58 0.75 0.42
Basic Persona 1.00 0.73 0.58 0.53
Persona Reg. 0.90 0.70 0.58 0.44
100
Author/Persona 0.98 0.68 0.70 0.46
Basic Persona 0.95 0.73 0.53 0.47
Persona Reg. 0.93 0.78 0.63 0.49
50
Author/Persona 0.95 0.73 0.63 0.50
Basic Persona 0.98 0.75 0.48 0.53
Persona Reg. 1.00 0.75 0.65 0.38
25
Author/Persona 1.00 0.63 0.65 0.50
Basic Persona 1.00 0.63 0.50 0.50
Persona Reg. 0.90 0.78 0.60 0.39
10
Author/Persona 0.95 0.63 0.70 0.51
Basic Persona 0.78 0.80 0.48 0.46
Persona Reg. 0.90 0.73 0.43 0.41
Baseline 1.00 0.63 0.58 0.37
Table 1: Agreement rates with preregistered hypotheses, av-
eraged over 5 sampling runs with different initializations.
0
25
50
75
100
A B C D
Hypothesis class
Ac
cu
ra
cy
Author/Persona  Basic  Persona Reg.  Baseline
Figure 4: Synopsis of table 1: average accuracy across all P .
Persona regression is best able to judge characters in one
author to be more similar to each other than to characters in
another (B), while our mixed-effects Author/Persona model
outperforms other models at discriminating characters in the
same author (C).
opsis of this table by illustrating the average ac-
curacy across all choice of P . All models, in-
cluding the baseline, perform well on the sanity
checks (A). As expected, the Persona Regres-
sion model performs best at hypothesis class B
(correctly judging two characters from the same
author to be more similar to each other than to a
character from a different author); this behavior is
encouraged in this model by allowing an author (as
an external metadata variable) to directly influence
the persona choice, which has the effect of push-
ing characters from the same author to embody
the same character type. Our mixed effects Au-
thor/Persona model, in contrast, outperforms the
other models at hypothesis class C (correctly dis-
criminating different character types present in the
same author). By discounting author-specific lexi-
cal effects during persona inference, we are better
able to detect variation among the characters of a
single author that we are not able to capture oth-
erwise. While these different models complement
each other in this manner, we note that there is
no absolute separation among them, which may be
suggestive of the degree to which the formal and
referential dimensions are fused in novels. Nev-
ertheless, the strengths of these different models
on these different hypothesis classes gives us flex-
ible alternatives to use depending on the kinds of
character types we are looking to infer.
7 Analysis
The latent personas inferred from this model will
support further exploratory analysis of literary his-
tory. Figure 2 illustrates this with a selection of
three character types learned, displaying charac-
teristic clusters for all role types, along with the
distribution of that persona?s use across time and
the gender distribution of characters embodying
that persona. In general, the personas learned so
far do not align neatly with character types known
to literary historians. But they do have legible as-
sociations both with literary genres and with social
categories. Even though gender is not an observ-
able variable known to the model during inference,
personas tend to be clearly gendered. This is not
in itself surprising (since literary scholars know
that assumptions about character are strongly gen-
dered), but it does suggest that diachronic analysis
of latent character types might cast new light on
the history of gender in fiction. This is especially
true since the distribution of personas across the
time axis similarly reveals coherent trends.
Table 3 likewise illustrates what our model
learns by presenting a sample of the fixed effects
learned for a set of five major 19th-century au-
thors. These are clusters that are conditionally
more likely to appear associated with a character
in a work by the given author than they are in the
overall data; by factoring this information out of
the inference process for learning character types
(by attributing its presence in a text to the author
377
1800 1820 1840 1860 1880 1900 1800 1820 1840 1860 1880 1900 1800 1820 1840 1860 1880 1900
Agent
carried ran threw sent received arrived turns begins returns
rose fell suddenly appeared struck showed thinks loves calls
is seems returned immediately waiting does knows comes
Patient
wounded killed murdered wounded killed murdered thinks loves calls
suffer yield acknowledge destroy bind crush love hope true
free saved unknown attend haste proceed turn hold show
Poss
death happiness future army officers troops lips cheek brow
lips cheek brow soldiers band armed eyes face eye
mouth fingers tongue party join camp table bed chair
Pred
crime guilty murder king emperor throne beautiful fair fine
youth lover hers general officer guard good kind ill
dead living died soldier knight hero dead living died
% Female 12.2 3.7 54.7
Table 2: Snapshots of three personas learned from the P = 50, Author/Persona model. Gender and time proportions are
calculated by summing and normalizing the posterior distributions over all characters with that feature. We truncate time series
at 1800 due to data sparsity before that date; the y-axis illustrates the frequency of its use in a given year, relative to its lifetime.
Author clusters
Jane Austen
praise gift consolation
letter read write
character natural taste
Charlotte Bront?e
lips cheek brow
book paper books
hat coat cap
Charles Dickens
hat coat cap
table bed chair
hand head hands
Herman Melville
boat ship board
hat coat cap
feet ground foot
Jules Verne
journey travel voyage
master company presence
success plan progress
Table 3: Characteristic possessive clusters in a sample of
major 19th-century authors.
rather than the persona), we are able to learn per-
sonas that cut across different topics more effec-
tively than if a character type is responsible for
explaining the presence of these terms as well.
8 Conclusion
Our method establishes the possibility of repre-
senting the relationship between character and nar-
rative form in a hierarchical Bayesian model. Pos-
tulating an interaction between authorial diction
and character allows models that consider the ef-
fect of the author to more closely reproduce a hu-
man reader?s judgments, especially by learning to
distinguish different character types within a sin-
gle author?s oeuvre. This opens the door to con-
sidering other structural and formal dimensions of
narration. For instance, representation of charac-
ter is notoriously complicated by narrative point of
view (Booth, 1961); and indeed, comparisons be-
tween first-person narrators and other characters
are a primary source of error for all models tested
above. The strategy we have demonstrated sug-
gests that it might be productive to address this by
modeling the interaction of character and point of
view as a separate effect analogous to authorship.
It is also worth noting that the models tested
above diverge from many structuralist theories of
narrative (Propp, 1998) by allowing multiple in-
stances of the same persona in a single work.
Learning structural limitations on the number of
?protagonists? likely to coexist in a single story,
for example, may be another fruitful area to ex-
plore. In all cases, the machinery of hierarchical
models gives us the flexibility to incorporate such
effects at will, while also being explicit about the
theoretical assumptions that attend them.
9 Acknowledgments
We thank the reviewers for their helpful com-
ments. The research reported here was supported
by a National Endowment for the Humanities
start-up grant to T.U., U.S. National Science Foun-
dation grant CAREER IIS-1054319 to N.A.S., and
an ARCS scholarship to D.B. This work was made
possible through the use of computing resources
made available by the Pittsburgh Supercomputing
Center. Eleanor Courtemanche provided advice
about the history of narrative theory.
378
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l
1
-regularized log-linear models. In Proc. of
ICML.
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2013. Learning latent personas of film char-
acters. Proc. of ACL.
David Bamman, Ted Underwood, and Noah A. Smith.
2014. Appendix to ?A Bayesian mixed effects
model of literary character?. Technical report,
Carnegie Mellon University, University of Illinois-
Urbana Champaign.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Wayne Booth. 1961. The Rhetoric of Fiction. Univer-
sity of Chicago Press, Chicago.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In Proc. of
EMNLP, Seattle, Washington, USA.
Stanley F. Chen and Roni Rosenfeld. 2000. A
survey of smoothing techniques for me models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proc. of NAACL.
Peter T. Davis, David K. Elson, and Judith L. Klavans.
2003. Methods for precise named entity matching in
digital collections. In Proc. of JCDL, Washington,
DC, USA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.
Greg Durrett, David Hall, and Dan Klein. 2013.
Decentralized entity-level modeling for coreference
resolution. In Proc. of ACL.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Proc.
of ICML.
David K. Elson, Nicholas Dames, and Kathleen R.
McKeown. 2010. Extracting social networks from
literary fiction. In Proc. of ACL, Stroudsburg, PA,
USA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. of ACL.
E. M. Forster. 1927. Aspects of the Novel. Harcourt,
Brace & Co.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Proc. of ICASSP.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. of NAACL.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Proc.
of NAACL.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. of EMNLP.
Suzanne Keen. 2003. Narrative Form. Palgrave
Macmillan, Basingstoke.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proc. of ICLR.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proc. of AISTATS.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13:95?135, 5.
Vladimir Propp. 1998. Morphology of the Folktale.
University of Texas Press, 2nd edition.
Roni Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modelling. Com-
puter Speech and Language, 10(3):187 ? 228.
Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological pro-
portions in political speeches. In Proc. of EMNLP,
Seattle, Washington, USA.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proc. of NAACL.
Ted Underwood, Michael L Black, Loretta Auvil, and
Boris Capitanu. 2013. Mapping mutable genres in
structurally complex volumes. In Proc. of IEEE In-
ternational Conference on Big Data.
Alex Woloch. 2003. The One vs. the Many: Minor
Characters and the Space of the Protagonist in the
Novel. Princeton University Press, Princeton NJ.
379
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828?834,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Distributed Representations of Geographically Situated Language
David Bamman Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dbamman,cdyer,nasmith}@cs.cmu.edu
Abstract
We introduce a model for incorporating
contextual information (such as geogra-
phy) in learning vector-space representa-
tions of situated language. In contrast to
approaches to multimodal representation
learning that have used properties of the
object being described (such as its color),
our model includes information about the
subject (i.e., the speaker), allowing us to
learn the contours of a word?s meaning
that are shaped by the context in which
it is uttered. In a quantitative evaluation
on the task of judging geographically in-
formed semantic similarity between repre-
sentations learned from 1.1 billion words
of geo-located tweets, our joint model out-
performs comparable independent models
that learn meaning in isolation.
1 Introduction
The vast textual resources used in NLP ?
newswire, web text, parliamentary proceedings ?
can encourage a view of language as a disembod-
ied phenomenon. The rise of social media, how-
ever, with its large volume of text paired with in-
formation about its author and social context, re-
minds us that each word is uttered by a particular
person at a particular place and time. In short: lan-
guage is situated.
The coupling of text with demographic infor-
mation has enabled computational modeling of
linguistic variation, including uncovering words
and topics that are characteristic of geographical
regions (Eisenstein et al, 2010; O?Connor et al,
2010; Hong et al, 2012; Doyle, 2014), learning
correlations between words and socioeconomic
variables (Rao et al, 2010; Eisenstein et al, 2011;
Pennacchiotti and Popescu, 2011; Bamman et al,
2014); and charting how new terms spread geo-
graphically (Eisenstein et al, 2012). These models
can tell us that hella was (at one time) used most
often by a particular demographic group in north-
ern California, echoing earlier linguistic studies
(Bucholtz, 2006), and that wicked is used most
often in New England (Ravindranath, 2011); and
they have practical applications, facilitating tasks
like text-based geolocation (Wing and Baldridge,
2011; Roller et al, 2012; Ikawa et al, 2012).
One desideratum that remains, however, is how the
meaning of these terms is shaped by geographical
influences ? while wicked is used throughout the
United States to mean bad or evil (?he is a wicked
man?), in New England it is used as an adverbial
intensifier (?my boy?s wicked smart?). In lever-
aging grounded social media to uncover linguistic
variation, what we want to learn is how a word?s
meaning is shaped by its geography.
In this paper, we introduce a method that ex-
tends vector-space lexical semantic models to
learn representations of geographically situated
language. Vector-space models of lexical seman-
tics have been a popular and effective approach
to learning representations of word meaning (Lin,
1998; Turney and Pantel, 2010; Reisinger and
Mooney, 2010; Socher et al, 2013; Mikolov et al,
2013, inter alia). In bringing in extra-linguistic in-
formation to learn word representations, our work
falls into the general domain of multimodal learn-
ing; while other work has used visual informa-
tion to improve distributed representations (An-
drews et al, 2009; Feng and Lapata, 2010; Bruni
et al, 2011; Bruni et al, 2012a; Bruni et al,
2012b; Roller and im Walde, 2013), this work
generally exploits information about the object be-
ing described (e.g., strawberry and a picture of a
strawberry); in contrast, we use information about
the speaker to learn representations that vary ac-
cording to contextual variables from the speaker?s
perspective. Unlike classic multimodal systems
that incorporate multiple active modalities (such
as gesture) from a user (Oviatt, 2003; Yu and
828
...
W
X
Main
Alabama Alaska
Arizona
Arkansas
h
o
Figure 1: Model. Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item
#2) spoken in Alaska, along with a single output. Parameter matrixW consists of the learned low-dimensional embeddings.
Ballard, 2004), our primary input is textual data,
supplemented with metadata about the author and
the moment of authorship. This information en-
ables learning models of word meaning that are
sensitive to such factors, allowing us to distin-
guish, for example, between the usage of wicked
in Massachusetts from the usage of that word else-
where, and letting us better associate geographi-
cally grounded named entities (e.g, Boston) with
their hypernyms (city) in their respective regions.
2 Model
The model we introduce is grounded in the distri-
butional hypothesis (Harris, 1954), that two words
are similar by appearing in the same kinds of con-
texts (where ?context? itself can be variously de-
fined as the bag or sequence of tokens around a tar-
get word, either by linear distance or dependency
path). We can invoke the distributional hypothe-
sis for many instances of regional variation by ob-
serving that such variants often appear in similar
contexts. For example:
? my boy?s wicked smart
? my boy?s hella smart
? my boy?s very smart
Here, all three variants can often be seen in an im-
mediately pre-adjectival position (as is common
with intensifying adverbs).
Given the empirical success of vector-space rep-
resentations in capturing semantic properties and
their success at a variety of NLP tasks (Turian et
al., 2010; Socher et al, 2011; Collobert et al,
2011; Socher et al, 2013), we use a simple, but
state-of-the-art neural architecture (Mikolov et al,
2013) to learn low-dimensional real-valued repre-
sentations of words. The graphical form of this
model is illustrated in figure 1.
This model corresponds to an extension of
the ?skip-gram? language model (Mikolov et al,
2013) (hereafter SGLM). Given an input sentence
s and a context window of size t, each word s
i
is
conditioned on in turn to predict the identities of
all of the tokens within t words around it. For a
vocabulary V , each input word s
i
is represented
as a one-hot vector w
i
of length |V |. The SGLM
has two sets of parameters. The first is the rep-
resentation matrix W ? R
|V |?k
, which encodes
the real-valued embeddings for each word in the
vocabulary. A matrix multiply h = w
>
W,? R
k
serves to index the particular embedding for word
w, which constitutes the model?s hidden layer. To
predict the value of the context word y (again, a
one-hot vector of dimensionality |V |), this hidden
representation h is then multiplied by a second pa-
rameter matrix X ? R
|V |?k
. The final prediction
over the output vocabulary is then found by pass-
ing this resulting vector through the softmax func-
tion o = softmax(Xh), giving a vector in the |V |-
dimensional unit simplex. Backpropagation using
(input x, output y) word tuples learns the values
of W (the embeddings) and X (the output param-
eter matrix) that maximize the likelihood of y (i.e.,
the context words) conditioned on x (i.e., the s
i
?s).
During backpropagation, the errors propagated are
the difference between o (a probability distribu-
tion with k outcomes) and the true (one-hot) out-
put y.
Let us define a set of contextual variables
C; in the experiments that follow, C is com-
prised solely of geographical state C
state
=
{AK,AL, . . . ,WY}) but could in principle in-
clude any number of features, such as calendar
829
month, day of week, or other demographic vari-
ables of the speaker. Let |C| denote the sum of the
cardinalities of all variables in C (i.e., 51 states,
including the District of Columbia). Rather than
using a single embedding matrix W that contains
low-dimensional representations for every word in
the vocabulary, we define a global embedding ma-
trix W
main
? R
|V |?k
and an additional |C| such
matrices (each again of size |V | ? k, which cap-
ture the effect that each variable value has on each
word in the vocabulary. Given an input word w
and set of active variable values A (e.g., A =
{state = MA}), we calculate the hidden layer
h as the sum of these independent embeddings:
h = w
>
W
main
+
?
a?A
w
>
W
a
. While the word
wicked has a common low-dimensional represen-
tation in W
main,wicked
that is invoked for every
instance of its use (regardless of the place), the
corresponding vector W
MA,wicked
indicates how
that common representation should shift in k-
dimensional space when used in Massachusetts.
Backpropagation functions as in standard SGLM,
with gradient updates for each training example
{x, y} touching not onlyW
main
(as in SGLM), but
all active W
A
as well.
The additional W embeddings we add lead to
an increase in the number of total parameters by
a factor of |C|. To control for the extra degrees
of freedom this entails, we add squared `
2
regu-
larization to all parameters, using stochastic gra-
dient descent for backpropagation with minibatch
updates for the regularization term. As in Mikolov
et al (2013), we speed up computation using the
hierarchical softmax (Morin and Bengio, 2005) on
the output matrix X .
This model defines a joint parameterization over
all variable values in the data, where information
from data originating in California, for instance,
can influence the representations learned for Wis-
consin; a naive alternative would be to simply train
individual models on each variable value (a ?Cal-
ifornia? model using data only from California,
etc.). A joint model has three a priori advantages
over independent models: (i) sharing data across
variable values encourages representations across
those values to be similar; e.g., while city may be
closer to Boston in Massachusetts and Chicago in
Illinois, in both places it still generally connotes
a municipality; (ii) such sharing can mitigate data
sparseness for less-witnessed areas; and (iii) with
a joint model, all representations are guaranteed to
be in the same vector space and can therefore be
compared to each other; with individual models
(each with different initializations), word vectors
across different states may not be directly com-
pared.
3 Evaluation
We evaluate our model by confirming its face
validity in a qualitative analysis and estimating
its accuracy at the quantitative task of judging
geographically-informed semantic similarity. We
use 1.1 billion tokens from 93 million geolocated
tweets gathered between September 1, 2011 and
August 30, 2013 (approximately 127,000 tweets
per day evenly sampled over those two years).
This data only includes tweets that have been ge-
olocated to state-level granularity in the United
States using high-precision pattern matching on
the user-specified location field (e.g., ?new york
ny? ? NY, ?chicago? ? IL, etc.). As a pre-
processing step, we identify a set of target mul-
tiword expressions in this corpus as the maximal
sequence of adjectives + nouns with the highest
pointwise mutual information; in all experiments
described below, we define the vocabulary V as
the most frequent 100,000 terms (either unigrams
or multiword expressions) in the total data, and set
the dimensionality of the embedding k = 100. In
all experiments, the contextual variable is the ob-
served US state (including DC), so that |C| = 51;
the vector space representation of word w in state
s is w
>
W
main
+ w
>
W
s
.
3.1 Qualitative Evaluation
To illustrate how the model described above can
learn geographically-informed semantic represen-
tations of words, table 1 displays the terms with
the highest cosine similarity to wicked in Kansas
and Massachusetts after running our joint model
on the full 1.1 billion words of Twitter data; while
wicked in Kansas is close to other evaluative terms
like evil and pure and religious terms like gods and
spirit, in Massachusetts it is most similar to other
intensifiers like super, ridiculously and insanely.
Table 2 likewise presents the terms with the
highest cosine similarity to city in both Califor-
nia and New York; while the terms most evoked
by city in California include regional locations
like Chinatown, Los Angeles? South Bay and San
Francisco?s East Bay, in New York the most sim-
ilar terms include hamptons, upstate and borough
830
Kansas Massachusetts
term cosine term cosine
wicked 1.000 wicked 1.000
evil 0.884 super 0.855
pure 0.841 ridiculously 0.851
gods 0.841 insanely 0.820
mystery 0.830 extremely 0.793
spirit 0.830 goddamn 0.781
king 0.828 surprisingly 0.774
above 0.825 kinda 0.772
righteous 0.823 #sarcasm 0.772
magic 0.822 sooooooo 0.770
Table 1: Terms with the highest cosine similarity to wicked
in Kansas and Massachusetts.
California New York
term cosine term cosine
city 1.000 city 1.000
valley 0.880 suburbs 0.866
bay 0.874 town 0.855
downtown 0.873 hamptons 0.852
chinatown 0.854 big city 0.842
south bay 0.854 borough 0.837
area 0.851 neighborhood 0.835
east bay 0.845 downtown 0.827
neighborhood 0.843 upstate 0.826
peninsula 0.840 big apple 0.825
Table 2: Terms with the highest cosine similarity to city in
California and New York.
(New York City?s term of administrative division).
3.2 Quantitative Evaluation
As a quantitative measure of our model?s perfor-
mance, we consider the task of judging semantic
similarity among words whose meanings are likely
to evoke strong geographical correlations. In the
absence of a sizable number of linguistically in-
teresting terms (like wicked) that are known to be
geographically variable, we consider the proxy of
estimating the named entities evoked by specific
terms in different geographical regions. As noted
above, geographic terms like city provide one such
example: in Massachusetts we expect the term city
to be more strongly connected to grounded named
entities like Boston than to other US cities. We
consider seven categories for which we can rea-
sonably expect the connotations of each term to
vary by geography; in each case, we calculate the
distance between two terms x and y using repre-
sentations learned for a given state (?
state
(x, y)).
1. city. For each state, we measure the distance
between the word city and the state?s most
populous city; e.g., ?
AZ
(city , phoenix ).
2. state. For each state, the distance between
the word state and the state?s name; e.g.,
?
WI
(state,wisconsin).
3. football. For all NFL teams, the distance be-
tween the word football and the team name;
e.g., ?
IL
(football , bears).
4. basketball. For all NBA teams from
a US state, the distance between the
word basketball and the team name; e.g.,
?
FL
(basketball , heat).
5. baseball. For all MLB teams from a US
state, the distance between the word baseball
and the team name; e.g., ?
IL
(baseball , cubs),
?
IL
(baseball ,white sox ).
6. hockey. For all NHL teams from a US state,
the distance between the word hockey and the
team name; e.g., ?
PA
(hockey , penguins).
7. park. For all US national parks, the distance
between the word park and the park name;
e.g., ?
AK
(park , denali).
Each of these questions asks the following:
what words are evoked for a given target word
(like football)? While football may everywhere
evoke similar sports like baseball or soccer or
more specific football-related terms like touch-
down or field goal, we expect that particular sports
teams will be evoked more strongly by the word
football in their particular geographical region: in
Wisconsin, football should evoke packers, while
in Pennsylvania, football evokes steelers. Note
that this is not the same as simply asking which
sports team is most frequently (or most character-
istically) mentioned in a given area; by measuring
the distance to a target word (football), we are at-
tempting to estimate the varying strengths of asso-
ciation between concepts in different regions.
For each category, we measure similarity as the
average cosine similarity between the vector for
the target word for that category (e.g., city) and the
corresponding vector for each state-specific an-
swer (e.g., chicago for IL; boston for MA). We
compare three different models:
1. JOINT. The full model described in section
2, in which we learn a global representation
for each word along with deviations from that
common representation for each state.
2. INDIVIDUAL. For comparison, we also parti-
tion the data among all 51 states, and train a
single model for each state using only data
from that state. In this model, there is no
sharing among states; California has the most
831
0.00
0.25
0.50
0.75
city state baseball basketball football hockey park
sim
ila
rity
Model
Joint
Individual
?Geo
Figure 2: Average cosine similarity for all models across all categories, with 95% confidence intervals on the mean.
data with 11,604,637 tweets; Wyoming has
the least with 47,503 tweets.
3. ?GEO. We also train a single model on all of
the training data, but ignore any state meta-
data. In this case the distance ? between two
terms is their overall distance within the en-
tire United States.
As one concrete example of these differences
between individual data points, the cosine similar-
ity between city and seattle in the ?GEO model
is 0.728 (seattle is ranked as the 188th most sim-
ilar term to city overall); in the INDIVIDUAL
model using only tweets from Washington state,
?
WA
(city, seattle) = 0.780 (rank #32); and in
the JOINT model, using information from the en-
tire United States with deviations for Washington,
?
WA
(city, seattle) = 0.858 (rank #6). The over-
all similarity for the city category of each model is
the average of 51 such tests (one for each city).
Figure 2 present the results of the full evalua-
tion, including 95% confidence intervals for each
mean. While the two models that include ge-
ographical information naturally outperform the
model that does not, the JOINT model generally
far outperforms the INDIVIDUAL models trained
on state-specific subsets of the data.
1
A model that
can exploit all of the information in the data, learn-
ing core vector-space representations for all words
along with deviations for each contextual variable,
is able to learn more geographically-informed rep-
resentations for this task than strict geographical
models alone.
1
This result is robust to the choice of distance metric; an
evaluation measuring the Euclidean distance between vectors
shows the JOINT model to outperform the INDIVIDUAL and
?GEO models across all seven categories.
4 Conclusion
We introduced a model for leveraging situational
information in learning vector-space representa-
tions of words that are sensitive to the speaker?s
social context. While our results use geographical
information in learning low-dimensional represen-
tations, other contextual variables are straightfor-
ward to include as well; incorporating effects for
time ? such as time of day, month of year and ab-
solute year ? may be a powerful tool for reveal-
ing periodic and historical influences on lexical se-
mantics.
Our approach explores the degree to which ge-
ography, and other contextual factors, influence
word meaning in addition to frequency of usage.
By allowing all words in different regions (or more
generally, with different metadata factors) to ex-
ist in the same vector space, we are able com-
pare different points in that space ? for example,
to ask what terms used in Chicago are most simi-
lar to hot dog in New York, or what word groups
shift together in the same region in comparison
to the background (indicating the shift of an en-
tire semantic field). All datasets and software to
support these geographically-informed represen-
tations can be found at: http://www.ark.
cs.cmu.edu/geoSGLM.
5 Acknowledgments
The research reported in this article was supported
by US NSF grants IIS-1251131 and CAREER IIS-
1054319, and by an ARCS scholarship to D.B.
This work was made possible through the use of
computing resources made available by the Open
Cloud Consortium, Yahoo and the Pittsburgh Su-
percomputing Center.
832
References
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463?498.
David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014. Gender identity and lexical variation
in social media. Journal of Sociolinguistics, 18(2).
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proc. of the Workshop on Geometrical Models of
Natural Language Semantics.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012a. Distributional semantics in
technicolor. In Proc. of ACL.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes:
Using image analysis to improve computational rep-
resentations of word meaning. In Proc. of the ACM
International Conference on Multimedia.
Mary Bucholtz. 2006. Word up: Social meanings of
slang in California youth culture. In Jane Goodman
and Leila Monaghan, editors, A Cultural Approach
to Interpersonal Communication: Essential Read-
ings, Malden, MA. Blackwell.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Gabriel Doyle. 2014. Mapping dialectal variation by
querying social media. In Proc. of EACL.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. of ACL.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2012. Mapping the geographical
diffusion of new words. arXiv, abs/1210.5268.
Yansong Feng and Mirella Lapata. 2010. Visual in-
formation in semantic representation. In Proc. of
NAACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the Twit-
ter stream. In Proc. of WWW.
Yohei Ikawa, Miki Enoki, and Michiaki Tatsubori.
2012. Location inference using microblog mes-
sages. In Proc. of WWW.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLING-ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Robert G. Cowell and Zoubin Ghahramani, editors,
Proc. of AISTATS.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. Discovering demographic
language variation. In NIPS Workshop on Machine
Learning and Social Computing.
Sharon Oviatt. 2003. Multimodal interfaces.
In Julie A. Jacko and Andrew Sears, editors,
The Human-computer Interaction Handbook, pages
286?304, Hillsdale, NJ, USA. L. Erlbaum Asso-
ciates Inc.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proc. of KDD.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. of the Workshop on
Search and Mining User-generated Contents.
Maya Ravindranath. 2011. A wicked good reason to
study intensifiers in New Hampshire. In NWAV 40.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proc. of NAACL.
Stephen Roller and Sabine Schulte im Walde. 2013. A
multimodal LDA model integrating textual, cogni-
tive and visual modalities. In Proc. of EMNLP.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proc. of EMNLP-CoNLL.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proc. of EMNLP.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188, January.
833
Benjamin P. Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proc. of ACL.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perception, 1(1):57?80.
834
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 176?180,
Dublin, Ireland, August 23-24, 2014.
CMU: Arc-Factored, Discriminative Semantic Dependency Parsing
Sam Thomson Brendan O?Connor Jeffrey Flanigan David Bamman
Jesse Dodge Swabha Swayamdipta Nathan Schneider Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sthomson,brenocon,jflanigan,dbamman,jessed,
swabha,nschneid,cdyer,nasmith}@cs.cmu.edu
Abstract
We present an arc-factored statistical model
for semantic dependency parsing, as de-
fined by the SemEval 2014 Shared Task 8
on Broad-Coverage Semantic Dependency
Parsing. Our entry in the open track placed
second in the competition.
1 Introduction
The task of broad coverage semantic dependency
parsing aims to provide a shallow semantic analysis
of text not limited to a specific domain. As distinct
from deeper semantic analysis (e.g., parsing to a
full lambda-calculus logical form), shallow seman-
tic parsing captures relationships between pairs
of words or concepts in a sentence, and has wide
application for information extraction, knowledge
base population, and question answering (among
others).
We present here two systems that produce seman-
tic dependency parses in the three formalisms of the
SemEval 2014 Shared Task 8 on Broad-Coverage
Semantic Dependency Parsing (Oepen et al., 2014).
These systems generate parses by extracting fea-
tures for each potential dependency arc and learn-
ing a statistical model to discriminate between good
arcs and bad; the first treats each labeled edge de-
cision as an independent multiclass logistic regres-
sion (?3.2.1), while the second predicts arcs as part
of a graph-based structured support vector machine
(?3.2.2). Common to both models is a rich set of
features on arcs, described in ?3.2.3. We include a
discussion of features found to have no discernable
effect, or negative effect, during development (?4).
Our system placed second in the open track of
the Broad-Coverage Semantic Dependency Parsing
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
Figure 1: Example annotations for DM (top), PAS (middle),
and PCEDT (bottom).
task (in which output from syntactic parsers and
other outside resources can be used). We present
our results in ?5.
2 Formalisms
The Shared Task 8 dataset consists of annota-
tions of the WSJ Corpus in three different se-
mantic dependency formalisms. DM is derived
from LinGO English Resource Grammar (ERG)
annotations in DeepBank (Flickinger et al., 2012).
PAS is derived from the Enju HPSG treebank us-
ing the conversion rules of Miyao et al. (2004).
PCEDT is derived from the tectogrammatical layer
of the Prague Czech-English Dependency Treebank
(Haji?c, 1998). See Figure 1 for an example.
The three formalisms come from very different
linguistic theories, but all are represented as labeled
directed graphs, with words as vertices, and all
have ?top? annotations, corresponding roughly to
the semantic focus of the sentence. (A ?top? need
not be a root of the graph.) This allows us to use
the same machinery (?3) for training and testing
statistical models for the three formalisms.
3 Models
We treat the problem as a three-stage pipeline. The
first stage prunes words by predicting whether they
have any incoming or outgoing edges at all (?3.1);
if a word does not, then it is not considered for
any attachments in later stages. The second stage
176
predicts where edges are present, and their labels
(?3.2). The third stage predicts whether a predicate
word is a top or not (?3.3). Formalisms sometimes
annotate more than one ?top? per sentence, but we
found that we achieve the best performance on all
formalisms by predicting only the one best-scoring
?top? under the model.
3.1 Singleton Classification
For each formalism, we train a classifier to rec-
ognize singletons, nodes that have no parents or
children. (For example, punctuation tokens are of-
ten singletons.) This makes the system faster with-
out affecting accuracy. For singleton prediction,
we use a token-level logistic regression classifier,
with features including the word, its lemma, and
its part-of-speech tag. If the classifier predicts a
probability of 99% or higher the token is pruned;
this removes around 10% of tokens. (The classi-
fier performs differently on different formalisms;
on PAS it has perfect accuracy, while on DM and
PCEDT accuracy is in the mid-90?s.)
3.2 Edge Prediction
In the second stage of the pipeline, we predict the
set of labeled directed edges in the graph. We use
the same set of edge-factored features (?3.2.3) in
two alternative models: an edge-independent mul-
ticlass logistic regression model (LOGISTICEDGE,
?3.2.1); and a structured SVM (Taskar et al., 2003;
Tsochantaridis et al., 2004) that enforces a deter-
minism constraint for certain labels, which allows
each word to have at most one outgoing edge with
that label (SVMEDGE, ?3.2.2). For each formalism,
we trained both models with varying features en-
abled and hyperparameter settings and submitted
the configuration that produced the best labeled F
1
on the development set. For DM and PCEDT, this
was LOGISTICEDGE; for PAS, this was SVMEDGE.
We report results only for the submitted configu-
rations, with different features enabled. Due to
time constraints, full hyperparameter sweeps and
comparable feature sweeps were not possible.
3.2.1 LOGISTICEDGE Parser
The LOGISTICEDGE model considers only token
index pairs (i, j) where |i ? j| ? 10, i 6= j,
and both t
i
and t
j
have been predicted to be non-
singletons by the first stage. Although this prunes
some gold edges, among the formalisms, 95%?97%
of all gold edges are between tokens of distance
10 or less. Both directions i ? j and j ? i are
considered between every pair.
Let L be the set of K + 1 possible output labels:
the formalism?s original K edge labels, plus the
additional label NOEDGE, which indicates that no
edge exists from i to j. The model treats every pair
of token indices (i, j) as an independent multiclass
logistic regression over output space L. Let x be
an input sentence. For candidate parent index i,
child index j, and edge label `, we extract a feature
vector f(x, i, j, `), where ` is conjoined with every
feature described in ?3.2.3. The multiclass logis-
tic regression model defines a distribution over L,
parametrized by weights ?:
P (` | ?, x, i, j) =
exp{? ? f(x, i, j, `)}
?
`
?
?L
exp{? ? f(x, i, j, `
?
)}
.
? is learned by minimizing total negative log-
likelihood of the above (with weighting; see be-
low), plus `
2
regularization. AdaGrad (Duchi et al.,
2011) is used for optimization. This seemed to opti-
mize faster than L-BFGS (Liu and Nocedal, 1989),
at least for earlier iterations, though we did no sys-
tematic comparison. Stochastic gradient steps are
applied one at a time from individual examples,
and a gradient step for the regularizer is applied
once per epoch.
The output labels have a class imbalance; in all
three formalisms, there are many more NOEDGE
examples than true edge examples. We improved
F
1
performance by downweighting NOEDGE
examples through a weighted log-likelihood
objective,
?
i,j
?
`
w
`
logP (` |?, x, i, j), with
w
NOEDGE
= 0.3 (selected on development set) and
w
`
= 1 otherwise.
Decoding: To predict a graph structure at test-time
for a new sentence, the most likely edge label is pre-
dicted for every candidate (i, j) pair of unpruned
tokens. If an edge is predicted for both directions
for a single (i, j) pair, only the edge with the higher
score is chosen. (There are no such bidirectional
edges in the training data.) This post-processing ac-
tually did not improve accuracy on DM or PCEDT;
it did improve PAS by ?0.2% absolute F
1
, but we
did not submit LOGISTICEDGE for PAS.
3.2.2 SVMEDGE Parser
In the SVMEDGE model, we use a structured SVM
with a determinism constraint. This constraint en-
sures that each word token has at most one outgoing
edge for each label in a set of deterministic labels
L
d
. For example, in DM a predicate never has more
177
than one child with edge label ?ARG1.? L
d
was
chosen to be the set of edges that were > 99.9%
deterministic in the training data.
1
Consider the fully dense graph of all edges be-
tween all words predicted as not singletons by the
singleton classifier ?3.1 (in all directions with all
possible labels). Unlike LOGISTICEDGE, the la-
bel set L does not include an explicit NOEDGE
label. If ? denotes the model weights, and f de-
notes the features, then an edge from i to j with
label ` in the dense graph has a weight c(i, j, `)
assigned to it using the linear scoring function
c(i, j, `) = ? ? f(x, i, j, `).
Decoding: For each node and each label `, if ` ?
L
d
, the decoder adds the highest scoring outgoing
edge, if its weight is positive. For ` 6? L
d
, every
outgoing edge with positive weight is added. This
procedure is guaranteed to find the highest scoring
subgraph (largest sum of edge weights) of the dense
graph subject to the determinism constraints. Its
runtime is O(n
2
).
The model weights are trained using the struc-
tured SVM loss. If x is a sentence and y is a
graph over that sentence, let the features be de-
noted f(x, y) =
?
(i,j,`)?y
f(x, i, j, `). The SVM
loss for each training example (x
i
, y
i
) is:
??
>
f(x
i
, y
i
)+max
y
?
>
f(x
i
, y)+cost(y, y
i
)
where cost(y, y
i
) = ?|y \ y
i
| + ?|y
i
\ y|. ? and
? trade off between precision and recall for the
edges (Gimpel and Smith, 2010). The loss is min-
imized with AdaGrad using early-stopping on a
development set.
3.2.3 Edge Features
Table 1 describes the features we used for predict-
ing edges. These features were computed over an
edge e with parent token s at index i and child
token t at index j. Unless otherwise stated, each
feature template listed has an indicator feature that
fires for each value it can take on. For the sub-
mitted results, LOGISTICEDGE uses all features
except Dependency Path v2, POS Path, and Dis-
tance Thresholds, and SVMEDGE uses all features
except Dependency Path v1. This was due to
SVMEDGE being faster to train than LOGISTIC-
EDGE when including POS Path features, and due
1
By this we mean that of the nodes that have at least
one outgoing ` edge, 99.9% of them have only one outgo-
ing ` edge. For DM, L
d
= L\{? and c,? ? or c,? ? then c,?
?loc,? ?mwe,? ?subord?}; for PAS, L
d
= L; and for PCEDT,
L
d
={?DPHR,? ?INTF,? ?VOCAT?}.
Tokens: The tokens s and t themselves.
Lemmas: Lemmas of s and t.
POS tags: Part of speech tags of s and t.
Linear Order: Fires if i < j.
Linear Distance: i? j.
Dependency Path v1 (LOGISTICEDGE only): The
concatenation of all POS tags, arc labels and up/down
directions on the path in the syntactic dependency tree
from s to t. Conjoined with s, with t, and without either.
Dependency Path v2 (SVMEDGE only): Same as De-
pendency Path v1, but with the lemma of s or t instead
of the word, and substituting the token for any ?IN? POS
tag.
Up/Down Dependency Path: The sequence of upward
and downward moves needed to get from s to t in the
syntactic dependency tree.
Up/Down/Left/Right Dependency Path: The unla-
beled path through the syntactic dependency tree from s
to t, annotated with whether each step through the tree
was up or down, and whether it was to the right or left in
the sentence.
Is Parent: Fires if s is the parent of t in the syntactic
dependency parse.
Dependency Path Length: Distance between s and t in
the syntactic dependency parse.
POS Context: Concatenated POS tags of tokens at i?1,
i, i+ 1, j ? 1, j, and j + 1. Concatenated POS tags of
tokens at i? 1, i, j ? 1, and j. Concatenated POS tags
of tokens at i, i+ 1, j, and j + 1.
Subcategorization Sequence: The sequence of depen-
dency arc labels out of s, ordered by the index of the
child. Distinguish left children from right children. If t
is a direct child of s, distinguish its arc label with a ?+?.
Conjoin this sequence with the POS tag of s.
Subcategorization Sequence with POS: As above, but
add the POS tag of each child to its arc label.
POS Path (SVMEDGE only): Concatenated POS tags
between and including i and j. Conjoined with head
lemma, with dependent lemma, and without either.
Distance Thresholds (SVMEDGE only): Fires for ev-
ery integer between 1 and blog(|i? j|+1)/ log(1.39)c
inclusive.
Table 1: Features used in edge prediction
to time constraints for the submission we were un-
able to retrain LOGISTICEDGE with these features.
3.2.4 Feature Hashing
The biggest memory usage was in the map from
feature names to integer indices during feature
extraction. For experimental expedience, we im-
plemented multitask feature hashing (Weinberger
et al., 2009), which hashes feature names to indices,
under the theory that errors due to collisions tend
to cancel. No drop in accuracy was observed.
3.3 Top Prediction
We trained a separate token-level binary logistic
regression model to classify whether a token?s node
had the ?top? attribute or not. At decoding time, all
predicted predicates (i.e., nodes where there is at
178
least one outbound edge) are possible candidates
to be ?top?; the classifier probabilities are evalu-
ated, and the highest-scoring node is chosen to be
?top.? This is suboptimal, since some graphs have
multiple tops (in PCEDT this is more common);
but selection rules based on probability thresholds
gave worse F
1
performance on the dev set. For a
given token t at index i, the top classifier?s features
included t?s POS tag, i, those two conjoined, and
the depth of t in the syntactic dependency tree.
4 Negative Results
We followed a forward-selection process during
feature engineering. For each potential feature,
we tested the current feature set versus the current
feature set plus the new potential feature. If the
new feature did not improve performance, we did
not add it. We list in table 2 some of the features
which we tested but did not improve performance.
In order to save time, we ran these feature se-
lection experiments on a subsample of the training
data, for a reduced number of iterations. These re-
sults thus have a strong caveat that the experiments
were not exhaustive. It may be that some of these
features could help under more careful study.
5 Experimental Setup
We participated in the Open Track, and used the
syntactic dependency parses supplied by the orga-
nizers. Feature engineering was performed on a
development set (?20), training on ??00?19. We
evaluate labeled precision (LP), labeled recall (LR),
labeled F
1
(LF), and labeled whole-sentence match
(LM) on the held-out test data using the evaluation
script provided by the organizers. LF was aver-
aged over the formalisms to determine the winning
system. Table 3 shows our scores.
6 Conclusion and Future Work
We found that feature-rich discriminative models
perform well at the task of mapping from sentences
to semantic dependency parses. While our final
approach is fairly standard for work in parsing,
we note here additional features and constraints
which did not appear to help (contrary to expecta-
tion). There are a number of clear extensions to
this work that could improve performance. While
an edge-factored model allows for efficient infer-
ence, there is much to be gained from higher-order
features (McDonald and Pereira, 2006; Martins
et al., 2013). The amount of information shared
Word vectors: Features derived from 64-dimensional
vectors from (Faruqui and Dyer, 2014), including the
concatenation, difference, inner product, and element-
wise multiplication of the two vectors associated with
a parent-child edge. We also trained a Random Forest
on the word vectors using Liaw and Wiener?s (2002) R
implementation. The predicted labels were then used as
features in LOGISTICEDGE.
Brown clusters Features derived from Brown clusters
(Brown et al., 1992) trained on a large corpus of web data.
Parent, child, and conjoined parent-child edge features
from cluster prefixes of length 2, 4, 6, 8, 10, and 12.
Conjunctions of those features with the POS tags of the
parent and child tokens.
Active/passive: Active/passive voice feature (as in Jo-
hansson and Nugues (2008)) conjoined with both the
Linear Distance features and the Subcategorization Se-
quence features. Voice information may already be cap-
tured by features from the Stanford dependency?style
parses, which include passivization information in arc
labels such as nsubjpass and auxpass (de Marneffe and
Manning, 2008).
Connectivity constraint: Enforcing that the graph is
connected (ignoring singletons), similar to Flanigan et al.
(2014). Almost all semantic dependency graphs in the
training data are connected (ignoring singletons), but
we found that enforcing this constraint significantly hurt
precision.
Tree constraint: Enforces that the graph is a tree. Un-
surprisingly, we found that enforcing a tree constraint
hurt performance.
Table 2: Features and constraints giving negative results.
LP LR LF LM
DM 0.8446 0.8348 0.8397 0.0875
PAS 0.9078 0.8851 0.8963 0.2604
PCEDT 0.7681 0.7072 0.7364 0.0712
Average 0.8402 0.8090 0.8241 0.1397
Table 3: Labeled precision (LP), recall (LR), F
1
(LF), and
whole-sentence match (LM) on the held-out test data.
between the three formalisms suggests that a multi-
task learning (Evgeniou and Pontil, 2004) frame-
work could lead to gains. And finally, there is
additional structure in the formalisms which could
be exploited (such as the deterministic processes
by which an original PCEDT tree annotation was
converted into a graph); formulating more subtle
graph constraints to capture this a priori knowl-
edge could lead to improved performance. We
leave such explorations to future work.
Acknowledgements
We are grateful to Manaal Faruqui for his help in word vector
experiments, and to reviewers for helpful comments. The re-
search reported in this paper was sponsored by the U.S. Army
Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533, DARPA
grant FA8750-12-2-0342 funded under the DEFT program,
U.S. NSF grants IIS-1251131 and IIS-1054319, and Google?s
support of the Reading is Believing project at CMU.
179
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Lin-
guistics, 18(4):467?479.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
Coling 2008: Proc. of the Workshop on Cross-Framework
and Cross-Domain Parser Evaluation, pages 1?8. Manch-
ester, UK.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adap-
tive subgradient methods for online learning and stochas-
tic optimization. Journal of Machine Learning Research,
12:2121?2159.
Theodoros Evgeniou and Massimiliano Pontil. 2004. Regular-
ized multitask learning. In Proc. of KDD, pages 109?117.
Seattle, WA, USA.
Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correlation.
In Proc. of EACL, pages 462?471. Gothenburg, Sweden.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer,
and Noah A. Smith. 2014. A discriminative graph-based
parser for the Abstract Meaning Representation. In Proc.
of ACL, pages 1426?1436. Baltimore, MD, USA.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012. Deep-
Bank: a dynamically annotated treebank of the Wall Street
Journal. In Proc. of the Eleventh International Workshop on
Treebanks and Linguistic Theories, pages 85?96. Lisbon,
Portugal.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity. URL http://lti.cs.cmu.edu/sites/
default/files/research/reports/2010/
cmulti10008.pdf.
Jan Haji?c. 1998. Building a syntactically annotated corpus:
the Prague Dependency Treebank. In Eva Haji?cov?a, ed-
itor, Issues of Valency and Meaning. Studies in Honour
of Jarmila Panevov?a, pages 106?132. Prague Karolinum,
Charles University Press, Prague.
Richard Johansson and Pierre Nugues. 2008. Dependency-
based semantic role labeling of PropBank. In Proc. of
EMNLP, pages 69?78. Honolulu, HI, USA.
Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomForest. R News, 2(3):18?
22. URL http://cran.r-project.org/web/
packages/randomForest/.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45(3):503?528.
Andr?e F. T. Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL, pages 617?622. Sofia,
Bulgaria.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proc. of
EACL, pages 81?88. Trento, Italy.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
head-driven phrase structure grammar from the Penn Tree-
bank. In Proc. of IJCNLP, pages 684?693. Hainan Island,
China.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel
Zeman, Dan Flickinger, Jan Haji?c, Angelina Ivanova, and
Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage
semantic dependency parsing. In Proc. of SemEval. Dublin,
Ireland.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-
margin Markov networks. In Proc. of NIPS, pages 25?32.
Vancouver, British Columbia, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,
and Yasemin Altun. 2004. Support vector machine learning
for interdependent and structured output spaces. In Proc.
of ICML, pages 104?111. Banff, Alberta, Canada.
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In Proc. of ICML, pages
1113?1120. Montreal, Quebec, Canada.
180
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 51?60,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
A Framework for (Under)specifying Dependency Syntax
without Overloading Annotators
Nathan Schneider?? Brendan O?Connor? Naomi Saphra? David Bamman?
Manaal Faruqui? Noah A. Smith? Chris Dyer? Jason Baldridge?
?School of Computer Science, Carnegie Mellon University
?Department of Linguistics, The University of Texas at Austin
Abstract
We introduce a framework for lightweight
dependency syntax annotation. Our for-
malism builds upon the typical represen-
tation for unlabeled dependencies, per-
mitting a simple notation and annotation
workflow. Moreover, the formalism en-
courages annotators to underspecify parts
of the syntax if doing so would streamline
the annotation process. We demonstrate
the efficacy of this annotation on three lan-
guages and develop algorithms to evaluate
and compare underspecified annotations.
1 Introduction
Computational representations for natural lan-
guage syntax are borne of competing design con-
siderations. When designing such representations,
there may be a tradeoff between parsimony and
expressiveness. A range of linguistic theories at-
tract support due to differing purposes and aes-
thetic principles (Chomsky, 1957; Tesni?re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988,
inter alia). Formalisms concerned with tractable
computation may care chiefly about learnabil-
ity or parsing efficiency (Shieber, 1992; Sleator
and Temperly, 1993; Kuhlmann and Nivre, 2006).
Further considerations may include psychologi-
cal and evolutionary plausibility (Croft, 2001;
Tomasello, 2003; Steels et al, 2011; Fossum and
Levy, 2012), integration with other representa-
tions such as semantics (Steedman, 2000; Bergen
and Chang, 2005), or suitability for particular ap-
plications (e.g., translation).
Here we elevate ease of annotation as a pri-
mary design concern for a syntactic annotation
formalism. Currently, a lack of annotated data
is a huge bottleneck for robust NLP, standing in
the way of parsers for social media text (Foster
et al, 2011) and many low-resourced languages
(to name two examples). Traditional syntactic an-
notation projects like the Penn Treebank (Marcus
?Corresponding author: nschneid@cs.cmu.edu
et al, 1993) or Prague Dependency Treebank (Ha-
jic?, 1998) require highly trained annotators and
huge amounts of effort. Lowering the cost of an-
notation, by making it easier and more accessi-
ble, could greatly facilitate robust NLP in new lan-
guages and genres.
To that end, we design and test new, lightweight
methodologies for syntactic annotation. We pro-
pose a formalism, Fragmentary Unlabeled De-
pendency Grammar (FUDG) for unlabeled de-
pendency syntax that addresses some of the most
glaring deficiencies of basic unlabeled dependen-
cies (?2), with little added burden on annotators.
FUDG requires minimal theoretical commitments,
and can be supplemented with a project-specific
style guide (we provide a brief one for English).
We contribute a simple ASCII markup language?
Graph Fragment Language (GFL; ?3)?that al-
lows annotations to be authored using any text ed-
itor, along with tools for validating, normalizing,
and visualizing GFL annotations.1
An important characteristic of our framework is
annotator flexibility. The formalism supports this
by allowing underspecification of structural por-
tions that are unclear or unnecessary for the pur-
poses of a project. Fully leveraging this power re-
quires new algorithms for evaluation, e.g., of inter-
annotator agreement, where annotations are par-
tial; such algorithms are presented in ?4.2
Finally, small-scale case studies (?5) apply our
framework (formalism, notation, and evaluations)
to syntactically annotate web text in English, news
in Malagasy, and dialogues in Kinyarwanda.
2 A Dependency Grammar for
Annotation
Although dependency-based approaches to syntax
play a major role in computational linguistics, the
nature of dependency representations is far from
uniform. Exemplifying one end of the spectrum
is the Prague Dependency Treebank, which articu-
lates an elaborate dependency-based syntactic the-
1https://github.com/brendano/gfl_syntax/
2Parsing algorithms are left for future work.
51
Found the scarriest mystery door in my school . I?M SO CURIOUS D:
Found** < (the scarriest mystery door*)
Found < in < (my > school)
I?M** < (SO > CURIOUS)
D:**
my = I?M
thers still like 1 1/2 hours till Biebs bday here :P
thers** < still
thers < ((1 1/2) > hours < till < (Biebs > bday))
(thers like 1 1/2 hours)
thers < here
:P**
Figure 1: Two tweets with example GFL annotations. (The formalism and notation are described in ?3.)
ory in a rich, multi-tiered formalism (Hajic?, 1998;
B?hmov? et al, 2003). On the opposite end of
the spectrum are the structures used in dependency
parsing research which organize all the tokens of
a sentence into a tree, sometimes with category la-
bels on the edges (K?bler et al, 2009). Insofar as
they reflect a theory of syntax, these vanilla de-
pendency grammars provide a highly reduction-
ist view of structure?indeed, parses used to train
and evaluate dependency parses are often simpli-
fications of Prague-style parses, or else converted
from constituent treebanks.
In addition to the binary dependency links of
vanilla dependency representations, we offer three
devices to capture certain linguistic phenomena
more straightforwardly:3
1. We make explicit the meaningful lexical units
over which syntactic structure is represented. Our
approach (a) allows punctuation and other extrane-
ous tokens to be excluded so as not to distract from
the essential structure; and (b) permits tokens to be
grouped into shallow multiword lexical units.4
2. Coordination is problematic to represent with
unlabeled dependencies due to its non-binary na-
ture. A coordinating conjunction typically joins
multiple expressions (conjuncts) with equal sta-
tus, and other expressions may relate to the com-
pound structure as a unit. There are several differ-
ent conventions for forcing coordinate structures
into a head-modifier straightjacket (Nivre, 2005;
de Marneffe and Manning, 2008; Marec?ek et al,
2013). Conjuncts, coordinators, and shared de-
pendents can be distinguished with edge labels;
we equivalently use a special notation, permitting
the coordinate structure to be automatically trans-
formed with any of the existing conventions.5
3Some of this is inspired by the conventions of Reed-
Kellogg sentence diagramming, a graphical dependency an-
notation system for English pedagogy (Reed and Kellogg,
1877; Kolln and Funk, 1994; Florey, 2006).
4The Stanford representation supports a limited notion of
multiword expressions (de Marneffe and Manning, 2008).
For simplicity, our formalism treats multiwords as unana-
lyzed (syntactically opaque) wholes, though some multiword
expressions may have syntactic descriptions (Baldwin and
Kim, 2010).
5Tesni?re (1959) and Hudson (1984) similarly use
special structures for coordination (Schneider, 1998;
3. Following Tesni?re (1959), our formalism
offers a simple facility to express anaphora-
antecedent relations (a subset of semantic relation-
ships) that are salient in particular syntactic phe-
nomena such as relative clauses, appositives, and
wh-expressions.
Underspecification. Our desire to facilitate
lightweight annotation scenarios requires us to
abandon the expectation that syntactic informants
provide a complete parse for every sentence. On
one hand, an annotator may be uncertain about the
appropriate parse due to lack of expertise, insuf-
ficiently mature annotation conventions, or actual
ambiguity in the sentence. On the other hand, an-
notators may be indifferent to certain phenomena.
This can happen for a variety of reasons:
? Some projects may only need annotations of
specific constructions. For example, building a
semantic resource for events may require anno-
tation of syntactic verb-argument relations, but
not internal noun phrase structure.
? As a project matures, it may be more useful to
annotate only infrequent lexical items.
? Semisupervised learning from partial annota-
tions may be sufficient to learn complete parsers
(Hwa, 1999; Clark and Curran, 2006).
? Beginning annotators may wish to focus on eas-
ily understood syntactic phenomena.
? Different members of a project may wish to spe-
cialize in different syntactic phenomena, reduc-
ing training cost and cognitive load.
Rather than treating annotations as invalid unless
and until they are complete trees, we formally rep-
resent and reason about partial parse structures.
Annotators produce annotations, which encode
constraints on the (inferred) analysis, the parse
structure, of a sentence. We say that a valid anno-
tation supports (is compatible with) one or more
analyses. Both annotations and analyses are rep-
resented as graphs (the graph representation is de-
scribed below in ?3.2). We require that the di-
rected edges in an analysis graph must form a tree
over all the lexical items in the sentence.6 Less
Sangati and Mazza, 2009).
6While some linguistic phenomena (e.g., relative clauses,
control constructions) can be represented using non-tree
52
stringent well-formedness constraints on the an-
notation graph leave room for underspecification.
Briefly, an annotation can be underspecified in
two ways: (a) an expression may not be attached to
any parent, indicating it might depend on any non-
descendant in a full analysis?this is useful for an-
notating sentences piece by piece; and (b) multiple
expressions may be grouped together in a fudge
expression (?3.3), a constraint that the elements
form a connected subgraph in the full analysis
while leaving the precise nature of that subgraph
indeterminate?this is useful for marking relation-
ships between chunks (possibly constituents).
A formalism, not a theory. Our framework for
dependency grammar annotation is a syntactic
formalism, but it is not sufficiently comprehen-
sive to constitute a theory of syntax. Though
it standardizes the basic treatment of a few ba-
sic phenomena, simplicity of the formalism re-
quires us to be conservative about making such
extensions. Therefore, just as with simpler for-
malisms, language- and project-specific conven-
tions will have to be developed for specific linguis-
tic phenomena. By embracing underspecified an-
notation, however, our formalism aims to encour-
age efficient corpus coverage in a nascent anno-
tation project, without forcing annotators to make
premature decisions.
3 Syntactic Formalism and GFL
In our framework, a syntactic annotation of a sen-
tence follows an extended dependency formalism
based on the desiderata enumerated in the previ-
ous section. We call our formalism Fragmentary
Unlabeled Dependency Grammar (FUDG).
To make it simple to create FUDG annotations
with a text editor, we provide a plain-text de-
pendency notation called Graph Fragment Lan-
guage (GFL). Fragments of the FUDG graph?
nodes and dependencies linking them?are en-
coded in this language; taken together, these frag-
ments describe the annotation in its entirety. The
ordering of GFL fragments, and of tokens within
each fragment, is of no formal consequence. Since
the underlying FUDG representation is transpar-
ently related to GFL constructions, GFL notation
will be introduced alongside the discussion of each
kind of FUDG node.7
structures, we find that being able to alert annotators when
they inadvertently violate the tree constraint is more useful
than the expressive flexibility.
7In principle, FUDG annotations could be created with
3.1 Tokens
We expect a tokenized string, such as a sentence
or short message. The provided tokenization is re-
spected in the annotation. For human readability,
GFL fragments refer to tokens as strings (rather
than offsets), so all tokens that participate in an
annotation must be unambiguous in the input.8 A
token may be referenced multiple times in the an-
notation.
3.2 Graph Encoding
Directed arcs. As in other dependency
formalisms, dependency arcs are directed
links indicating the syntactic headedness
relationship between pairs of nodes. In
GFL, directed arcs are indicated with an-
gle brackets pointing from the dependent to
its head, as in black > cat or (equivalently)
cat < black. Multiple arcs can be chained to-
gether: the > cat < black < jet describes three
arcs. Parentheses help group portions of a chain:
(the > cat < black < jet) > likes < fish (the
structure black < jet > likes, in which jet
appears to have two heads, is disallowed). Note
that another encoding for this structure would be
to place the contents of the parentheses and the
chain cat > likes < fish on separate lines. Curly
braces can be used to list multiple dependents of
the same head: {cat fish} > likes.
Anaphoric links. These undirected links join
coreferent anaphora to each other and to their an-
tecedent(s). In English this includes personal pro-
nouns, relative pronouns (who, which, that), and
anaphoric do and so (Leo loves Ulla and so does
Max). This introduces a bit of semantics into our
annotation, though at present we do not attempt to
mark non-anaphoric coreference. It also allows a
more satisfying treatment of appositives and rel-
ative clauses than would be possible from just the
directed tree (the third example in figures 2 and 3).
Lexical nodes. Whereas in vanilla dependency
grammar syntactic links are between pairs of to-
ken nodes, FUDG abstracts away from the indi-
vidual tokens in the input. The lowest level of a
FUDG annotation consists of lexical nodes, i.e.,
an alternative mechanism such as a GUI, as in Hajic? et al
(2001).
8If a word is repeated within the sentence, it must be in-
dexed in the input string in order to be referred to from a
fragment. In our notation, successive instances of the same
word are suffixed with ~1, ~2, ~3, etc. Punctuation and other
tokens omitted from an annotation do not need to be indexed.
53
'll
If
's
I wake_up
restin' it~1
it~2
weapons
Our three
are
$a
fear surprise efficiency
ruthless
and~1 and~2
are
We knights
the
who
say
Ni
Figure 2: FUDG graphs corresponding to the examples in figure 3. The two special kinds of directed edges are for attaching
conjuncts (bolded) and their coordinators (dotted) in a coordinate structure. Anaphoric links are undirected. The root node of
each sentence is omitted.
If it~1 's restin' I 'll wake it~2 up .
If < (it~1 > 's < restin')
I > 'll < [wake up] < it~2
If > 'll**
it~1 = it~2
Our three weapons are fear and~1 surprise and~2
ruthless efficiency ...
{Our three} > weapons > are < $a
$a :: {fear surprise efficiency} :: {and~1 and~2}
ruthless > efficiency
We are the knights who say ... Ni !
We > are < knights < the
knights < (who > say < Ni)
who = knights
Figure 3: GFL for the FUDG graphs in figure 2.
lexical item occurrences. Every token node maps
to 0 or 1 lexical nodes (punctuation, for instance,
can be ignored).
A multiword is a lexical node incorporating
more than one input token and is atomic (does
not contain internal structure). A multiword node
may group any subset of input tokens; this allows
for multiword expressions which are not neces-
sarily contiguous in the sentence (e.g., the verb-
particle construction make up in make the story
up). GFL notates multiwords with square brack-
ets, e.g., [break a leg].
Coordination nodes. Coordinate structures re-
quire at least two kinds of dependents: co-
ordinators (i.e., lexical nodes for coordinat-
ing conjunctions?at least one per coordina-
tion node) and conjuncts (heads of the con-
joined subgraphs?at least one per coordination
node). The GFL annotation has three parts:
a variable representing the node, a set of con-
juncts, and a set of coordinator nodes. For in-
stance, $a :: {[peanut butter] honey} :: {and}
(peanut butter and honey) can be embedded
within a phrase via the coordination node
variable $a; a [fresh [[peanut butter] and
honey] sandwich] snack would be formed with
{fresh $a} > sandwich > snack < a. A graphical
example of coordination can be seen in figure 2?
note the bolded conjunct edges and the dotted co-
ordinator edges. If the conjoined phrase as a whole
takes modifiers, these are attached to the coordina-
tion node with regular directed arcs. For example,
in Sam really adores kittens and abhors puppies.,
the shared subject Sam and adverb really attach to
the entire conjoined phrase. In GFL:
$a :: {adores abhors} :: {and}
Sam > $a < really
adores < kittens abhors < puppies
Root node. This is a special top-level node used
to indicate that a graph fragment constitutes a stan-
dalone utterance or a discourse connective. For an
input with multiple utterances, the head of each
should be designated with ** to indicate that it at-
taches to the root.
3.3 Means of Underspecification
As discussed in ?2, our framework distinguishes
annotations from full syntactic analyses. With re-
spect to dependency structure (directed edges), the
former may underspecify the latter, allowing the
annotator to commit only to a partial analysis.
For an annotationA, we define support(A) to be
the set of full analyses compatible with that anno-
tation. A full analysis is required to be a directed
rooted tree over all lexical nodes in the annotation.
An annotation is valid if its support is non-empty.
The 2 mechanisms for dependency underspeci-
fication are unattached nodes and fudge nodes.
Unattached nodes. For any node in an annota-
tion, the annotator is free to simply leave it not
attached to any head. This is interpreted as al-
lowing its head to be any other node (including
the root node), subject to the tree constraint. We
call a node?s possible heads its supported par-
ents. Formally, for an unattached node v in an-
notation A, suppParentsA(v) = nodes(A) \ ({v} ?
descendants(v)).
Fudge nodes. Sometimes, however, it is desir-
able to represent a sort of skeletal structure with-
out filling in all the details. A fudge expres-
sion (FE) asserts that a group of nodes (the ex-
pression?s members) belong together in a con-
nected subgraph, while leaving the internal struc-
ture of that subgraph unspecified.9 The notation
9This underspecification semantics is, to the best of our
knowledge, novel, though it has been proposed that con-
nected dependency subgraphs (known as catenae) are of the-
oretical importance in syntax (Osborne et al, 2012).
54
FN2
a
b
f
FN1
c d e
f
b
f
b
b c
b
b a
a
d
a
c a
d
a
d
c db e fe c e fe
a
d d
cf
e e f
c
Figure 4: Left: An annotation graph with 2 fudge nodes and 6 lexical nodes; it can be encoded with GFL fragments
((a b)* c d) < e and b < f. Right: All of its supported analyses: prom(A) = 6. com(A) = 1 ?
log 6
log 75
= .816.
for this is a list of two or more nodes within
parentheses: an annotation for Few if any witches
are friends with Maria. might contain the FE
(Few if any) so as to be compatible with the
structures Few < if < any, Few > if > any, etc.?
but not, for instance, Few > witches < any. In
the FUDG graph, this is represented with a fudge
node to which members are attached by special
member arcs. Fudge nodes may be linked to other
nodes: the GFL fragment (Few if any) > witches
is compatible with (Few < if < any) > witches,
(Few < (if > any)) > witches, and so forth.
Properties. Let f be a fudge expression. From
the connected subgraph definition and the tree
constraint on analyses, it follows that:
? Exactly 1 member of f must, in any compatible
analysis, have a parent that is not a member of f.
Call this node the top of the fudge expression,
denoted f ?. f ? dominates all other members of
f; it can be considered f?s ?internal head.?
? f does not necessarily form a full subtree. Any
of its members may have dependents that are
not themselves members of the fudge expres-
sion. (Such dependencies can be specified in
additional GFL fragments.)
Top designation. A single member of a fudge
expression may optionally be designated as its top
(internal head). This is specified with an asterisk:
(Few* if any) > witches indicates that Few must
attach to witches and also dominate both if and
any. In the FUDG graph, this is represented with
a special top arc as depicted in bold in figure 4.
Nesting. One fudge expression may nest
within another, e.g. (Few (if any)) > witches;
the word analyzed as attaching to witches might
be Few or whichever of (if any) heads the other.
A nested fudge expression can be designated as
top: (Vanishingly few (if any)*).
Modifiers. An arc attaching a node to a
fudge expression as a whole asserts that the
external node should modify the top of the fudge
expression (whether or not that top is designated
in the annotation). For instance, two of the
interpretations of British left waffles on Falklands
would be preserved by specifying British > left
and (left waffles) < on < Falklands. Analyses
British > left < waffles < on < Falklands and
(British > left < on < Falklands) > waffles
would be excluded because the preposition does
not attach to the head of (left waffles).10
Multiple membership. A node may be a mem-
ber of multiple fudge expressions, or a member
of an FE while attached to some other node via
an explicit arc. Each connected component of
the FUDG graph is therefore a polytree (not nec-
essarily a tree). The annotation graph minus all
member edges of fudge nodes and all (undirected)
anaphoric links must be a directed tree or forest.
Enumerating supported parents. Fudge ex-
pressions complicate the procedure for listing a
node?s supported parents (see above). Consider an
FE f having some member v. v might be the top
of f (unless some other node is so designated), in
which case anything the fudge node can attach to
is a potential parent of v. If some node other than
v might be the top of f, then v?s head could be any
member of f. Below (?4.1) we develop an algo-
rithm for enumerating supported parents for any
annotation graph node.
4 Annotation Evaluation Measures
For an annotation task which allows for a great
deal of latitude?as in our case, where a syntac-
tic annotation may be full or partial?quantitative
evaluation of data quality becomes a challenge. In
the context of our formalism, we propose mea-
sures that address:
? Annotation efficiency, quantified in terms of
annotator productivity (tokens per hour).
? The amount of information in an underspeci-
fied annotation. Intuitively, an annotation that
flirts with many full analyses conveys less syn-
tactic information than one which supports few
analyses. We define an annotation?s promiscu-
ity to be the number of full analyses it supports,
and develop an algorithm to compute it (?4.1).
10Not all attachment ambiguities can be precisely encoded
in FUDG. For instance, there is no way to forbid an attach-
ment to a word that lies along the path between the pos-
sible heads. The best that can be done given a sentence
like They conspired to defenestrate themselves on Tuesday. is
They > conspired < to < defenestrate < themselves and
(conspired* to defenestrate (on < Tuesday)).
55
? Inter-annotator agreement between two par-
tial annotations. Our measures for dependency
structure agreement (?4.2) incorporate the no-
tion of promiscuity.
We test these evaluations on our pilot annotation
data in the case studies (?5).
4.1 Promiscuity vs. Commitment
Given a FUDG annotation of a sentence, we quan-
tify the extent to which it underspecifies the full
structure by counting the number of analyses that
are compatible with the constraints in the annota-
tion. We call this number the promiscuity of the
annotation. Each analysis tree is rooted with the
root node and must span all lexical nodes.11
A na?ve algorithm for computing promiscuity
would be to enumerate all directed spanning trees
over the lexical nodes, and then check each of
them for compatibility with the annotation. But
this quickly becomes intractable: for n nodes,
one of which is designated as the root, there are
nn?2 spanning trees. However, we can filter out
edges that are known to be incompatible with
the annotation before searching for spanning
trees. Our ?upward-downward? method for
constructing a graph of supported edges first
enumerates a set of candidate top nodes for every
fudge expression, then uses that information
to infer a set of supported parents for every
node.12 The supported edge graph then consists
of vertices lexnodes(A) ? {root} and edges
?
v?lexnodes(A) {(v? v?) ? v? ? suppParentsA(v)}.
From this graph we can count all directed span-
ning trees in cubic time using Kirchhoff?s matrix
tree theorem (Chaiken and Kleitman, 1978; Smith
and Smith, 2007; Margoliash, 2010).13 If some
lexical node has no supported parents, this reflects
conflicting constraints in the annotation, and no
spanning tree will be found.
Promiscuity will tend to be higher for longer
sentences. To control for this, we define a second
quantity, the annotation?s commitment quotient
(commitment being the opposite of promiscuity),
11This measure assumes a fixed lexical analysis (set of lex-
ical nodes) and does not consider anaphoric links. Coordinate
structures are simplified into ordinary dependencies, with co-
ordinate phrases headed by the coordinator?s lexical node. If
a coordination node has multiple coordinators, one is arbi-
trarily chosen as the head and the others as its dependents.
12Python code for these algorithms appears in Schneider
et al (2013) and the accompanying software release.
13Due to a technicality with non-member attachments to
fudge nodes, for some annotations this is only an upper bound
on promiscuity; see Schneider et al (2013).
which normalizes for the number of possible span-
ning trees given the sentence length. The commit-
ment quotient for an annotation of a sentence with
n?1 lexical nodes and one root node is given by:
com(A) = 1 ?
log prom(A)
log nn?2
(the logs are to attenuate the dominance of the ex-
ponential term). This will be 1 if only a single
tree is supported by the annotation, and 0 if the
annotation does not constrain the structure at all.
(If the constraints in the annotation are internally
inconsistent, then promiscuity will be 0 and com-
mitment undefined.) In practice, there is a trade-
off between efficiency and commitment: more de-
tailed annotations require more time. The value of
minimizing promiscuity will therefore depend on
the resources and goals of the annotation project.
4.2 Inter-Annotator Agreement
FUDG can encode flat groupings and coreference
at the lexical level, as well as syntactic structure
over lexical items. Inter-annotator agreement can
be measured separately for each of these facets.
Pilot annotator feedback indicated that our initial
lexical-level guidelines were inadequate, so we fo-
cus here on measuring structural agreement pend-
ing further clarification of the lexical conventions.
Attachment accuracy, a standard measure for
evaluating dependency parsers, cannot be com-
puted between two FUDG annotations if either of
them underspecifies any part of the dependency
structure. One solution is to consider the inter-
section of supported full trees, in the spirit of
our promiscuity measure. For annotations A1 and
A2 of sentence s, one annotation?s supported an-
alyses can be enumerated and then filtered sub-
ject to the constraints of the other annotation.
The tradeoff between inter-annotator compatibil-
ity and commitment can be accounted for by tak-
ing their product, i.e. comPrec(A1 | A2) =
com(A1)
|supp(A1)?supp(A2)|
|supp(A1)|
.
A limitation of this support-intersection ap-
proach is that if the two annotations are not
compatible, the intersection will be empty. A
more fine-grained approach is to decompose
the comparison by lexical node: we general-
ize attachment accuracy with softComPrec(A1 |
A2) = com(A1)
?
`?s
?
i?{1,2} suppParentsAi (`)?
`?s suppParentsA1 (`)
, comput-
ing com(?) and suppParents(?) as in the previous
section. As lexical nodes may differ between the
two annotations, a reconciliation step is required
56
Language Tokens Rate (tokens/hr)
English Tweets (partial) 667 430
English Tweets (full) 388 250
Malagasy 4,184 47
Kinyarwanda 8,036 80
Table 1: Productivity estimates from pilot annotation project.
All annotators were native speakers of English.
to compare the structures: multiwords proposed in
only one of the two annotations are converted to
fudge expressions. Tokens annotated by neither
annotator are ignored. Like with the promiscuity
measure, we simplify coordinate structures to or-
dinary dependencies (see footnote 11).
5 Case Studies
5.1 Annotation Time
To estimate annotation efficiency, we performed
a pilot annotation project consisting of annotating
several hundred English tweets, about 1,000 sen-
tences in Malagasy, and a further 1,000 sentences
in Kinyarwanda.14 Table 1 summarizes the num-
ber of tokens annotated and the effort required. For
the two Twitter cases, the same annotator was first
permitted to do partial annotation of 100 tweets,
and then spend the same amount of time doing a
complete annotation of all tokens. Although this is
a very small study, the results clearly suggest she
was able to make much more rapid progress when
partial annotation was an option.15
This pilot study helped us to identify linguistic
phenomena warranting specific conventions: these
include wh-expressions, comparatives, vocatives,
discourse connectives, null copula constructions,
and many others. We documented these cases in a
20-page style guide for English,16 which informed
the subsequent pilot studies discussed below.
5.2 Underspecification and Agreement
We annotated 2 small English data samples in
order to study annotators? use of underspecifica-
tion. The first is drawn from Owoputi et al?s 2013
Twitter part-of-speech corpus; the second is from
the Reviews portion of the English Web Treebank
14Malagasy is a VOS Austronesian language spoken by 15
million people, mostly in Madagascar. Kinyarwanda is an
SVO Bantu language spoken by 12 million people mostly in
Rwanda. All annotations were done by native speakers of En-
glish. The Kinyarwanda and Malagasy annotators had basic
proficiency in these languages.
15As a point of comparison, during the Penn Treebank
project, annotators corrected the syntactic bracketings pro-
duced by a high-quality hand-written parser (Fidditch) and
achieved a rate of only 375 tokens/hour using a specialized
GUI interface (Marcus et al, 1993).
16Included with the data and software release (footnote 1).
Omit. prom Hist. Mean
1Ws MWs Tkns FEs 1 >1 ?10 ?102 com
Tweets 60 messages, 957 tokens
A 597 56 304 23 43 17 11 5 .96
B 644 47 266 28 37 23 12 6 .95
Reviews 55 sentences, 778 tokens
A 609 33 136 2 53 2 2 1 1.00
C ? D 643 19 116 114 11 44 38 21 .82
T 704 ? 74 ? 55 0 0 0 1
Table 2: Measures of our annotation samples. Note that
annotator ?D? specialized in noun phrase?internal structure,
while annotator ?C? specialized in verb phrase/clausal phe-
nomena; C ? D denotes the combination of their annotation
fragments. ?T? denotes our dependency conversion of the
English Web Treebank parses. (The value 1.00 was rounded
up from .9994.)
(EWTB) (Bies et al, 2012). (Our annotators only
saw the tokenized text.) Both datasets are infor-
mal and conversational in nature, and are dom-
inated by short messages/sentences. In spite of
their brevity, many of the items were deemed to
contain multiple ?utterances,? which we define to
include discourse connectives and emoticons (at
best marginal parts of the syntax); utterance heads
are marked with ** in figure 1.
Table 2 indicates the sizes of the two data sam-
ples, and gives statistics over the output of each
annotator: total counts of single-word and mul-
tiword lexical nodes, tokens not represented by
any lexical node, and fudge nodes; as well as
a histogram of promiscuity counts and the aver-
age of commitment quotients (see ?4.1). For in-
stance, the two sets of annotations obtained for the
Tweets sample used underspecification in 17/60
and 23/60 tweets, respectively, though the promis-
cuity rarely exceeded 100 compatible trees per an-
notation. Examples can be seen in figure 1, where
annotator ?A? marked only the noun phrase head
for the scarriest mystery door, opted not to choose
a head within the quantity 1 1/2, and left ambigu-
ous the attachment of the hedge like. The strong
but not utter commitment to the dependency struc-
ture is reflected in the mean commitment quotients
for this dataset, both of which exceed 0.95.
Inter-annotator agreement (IAA) is quantified in
table 3. The row marked A ? B, for instance,
considers the agreement between annotator ?A?
and annotator ?B?. Measuring IAA on the depen-
dency structure requires a common set of lexical
nodes, so a lexical reconciliation step ensures that
(a) any token used by either annotation is present
in both, and (b) no multiword node is present
in only one annotation?solved by relaxing in-
compatible multiwords to FEs (which increases
promiscuity). For Tweets, lexical reconciliation
57
thus reduces the commitment averages for each
annotation?to a greater extent for annotator ?A?
(.96 in table 2 vs. .82 in table 3) because ?A?
marked more multiwords. An analysis fully com-
patible with both annotations exists for only 27/60
sentences; the finer-grained softComPrec measure
(?4.2), however, offers insight into the balance be-
tween commitment and agreement.
Qualitatively, we observe three leading causes
of incompatibilities (disagreements): obvious an-
notator mistakes (such as the marked as a head);
inconsistent handling of verbal auxiliaries; and un-
certainty whether to attach expressions to a verb
or the root node, as with here in figure 1.17 An-
notators noticed occasional ambiguous cases and
attempted to encode the ambiguity with fudge ex-
pressions: again in the tweet maybe put it off un-
til you feel like ~ talking again ? is one example.
More often, fudge expressions proved useful for
syntactically difficult constructions, such as those
shown in figure 1 as well as: 2 shy of breaking it,
asked what tribe I was from, a $ 13 / day charge,
you two, and the most awkward thing ever.
5.3 Annotator Specialization
As an experiment in using underspecification for
labor division, two of the annotators of Reviews
data were assigned specific linguistic phenomena
to focus on. Annotator ?D? was tasked with the in-
ternal structure of base noun phrases, including re-
solving the antecedents of personal pronouns. ?C?
was asked to mark the remaining phenomena?
i.e., utterance/clause/verb phrase structure?but to
mark base noun phrases as fudge expressions,
leaving their internal structure unspecified. Both
annotators provided a full lexical analysis. For
comparison, a third individual, ?A,? annotated the
same data in full. The three annotators worked
completely independently.
Of the results in tables 2 and 3, the most notable
difference between full and specialized annotation
is that the combination of independent specialized
annotations (C ? D) produces somewhat higher
promiscuity/lower commitment. This is unsurpris-
ing because annotators sometimes overlook rela-
tionships that fall under their specialty.18 Still, an-
notators reported that specialization made the task
17Another example: Some uses of conjunctions like and
and so can be interpreted as either phrasal coordinators or dis-
course connectives (cf. The PDTB Research Group, 2007).
18A more practical and less error-prone approach might be
for specialists to work sequentially or collaboratively (rather
than independently) on each sentence.
com softComPrec
IAA 1 2 N|?|>0 1|2 2|1 F1
Tweets (N=60)
A ? B .82 .91 27 .57 .72 .63
Reviews (N=55)
A ? (C ? D) .95 .76 30 .64 .40 .50
A ? T .92 1 26 .48 .91 .63
(C ? D) ? T .73 1.00 28 .33 .93 .49
Table 3: Measures of inter-annotator agreement. Annotator
labels are as in table 2. Per-annotator com (with lexical rec-
onciliation) and inter-annotator softComPrec are aggregated
over sentences by arithmetic mean.
less burdensome, and the specialized annotations
did prove complementary to each other.19
5.4 Treebank Comparison
Though the annotators in our study were native
speakers well acquainted with representations of
English syntax, we sought to quantify their agree-
ment with the expert treebankers who created the
EWTB (the source of the Reviews sentences). We
converted the EWTB?s constituent parses to de-
pendencies via the PennConverter tool (Johansson
and Nugues, 2007),20 then removed punctuation.
Agreement with the converted treebank parses
appears in the bottom two rows of table 3. Be-
cause the EWTB commits to a single analysis,
precision scores are quite lopsided. Most of its
attachments are consistent with our annotations
(softComPrec > 0.9), but these allow many ad-
ditional analyses (hence the scores below 0.5).
6 Conclusion
We have presented a framework for simple depen-
dency annotation that overcomes some of the rep-
resentational limitations of unlabeled dependency
grammar and embraces the practical realities of
resource-building efforts. Pilot studies (in multiple
languages and domains, supported by a human-
readable notation and a suite of open-source tools)
showed this approach lends itself to rapid annota-
tion with minimal training.
The next step will be to develop algorithms ex-
ploiting these representations for learning parsers.
Other future extensions might include additional
expressive mechanisms (e.g., multi-headedness,
labels), crowdsourcing of FUDG annotations
(Snow et al, 2008), or even a semantic counter-
part to the syntactic representation.
19In fact, for only 2 sentences did ?C? and ?D? have in-
compatible annotations, and both were due to simple mis-
takes that were then fixed in the combination.
20We ran PennConverter with options chosen to emulate
our annotation conventions; see Schneider et al (2013).
58
Acknowledgments
We thank Lukas Biewald, Yoav Goldberg, Kyle Jerro, Vi-
jay John, Lori Levin, Andr? Martins, and several anony-
mous reviewers for their insights. This research was sup-
ported in part by the U. S. Army Research Laboratory and
the U. S. Army Research Office under contract/grant number
W911NF-10-1-0533 and by NSF grant IIS-1054319.
References
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group, Boca Raton, FL.
Benjamin K. Bergen and Nancy Chang. 2005. Embod-
ied Construction Grammar in simulation-based lan-
guage understanding. In Jan-Ola ?stman and Mir-
jam Fried, editors, Construction grammars: cog-
nitive grounding and theoretical extensions, pages
147?190. John Benjamins, Amsterdam.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, Barbora
Hladk?, and Anne Abeill?. 2003. The Prague De-
pendency Treebank: a three-level annotation sce-
nario. In Treebanks: building and using parsed cor-
pora, pages 103?127. Springer.
Seth Chaiken and Daniel J. Kleitman. 1978. Matrix
Tree Theorems. Journal of Combinatorial Theory,
Series A, 24(3):377?381.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
La Haye.
Stephen Clark and James Curran. 2006. Partial training
for a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL (HLT-NAACL 2006), pages 144?151. As-
sociation for Computational Linguistics, New York
City, USA.
William Croft. 2001. Radical Construction Grammar:
Syntactic Theory in Typological Perspective. Oxford
University Press, Oxford.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies man-
ual. http://nlp.stanford.edu/downloads/
dependencies_manual.pdf.
Kitty Burns Florey. 2006. Sister Bernadette?s Barking
Dog: The quirky history and lost art of diagramming
sentences. Melville House, New York.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incre-
mental sentence processing. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics (CMCL 2012), pages 61?69. As-
sociation for Computational Linguistics, Montr?al,
Canada.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS Tagging and Parsing the Twitter-
verse. In Proceedings of the 2011 AAAI Workshop
on Analyzing Microtext, pages 20?25. AAAI Press,
San Francisco, CA.
The PDTB Research Group. 2007. The Penn Discourse
Treebank 2.0 annotation manual. Technical Report
IRCS-08-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania, Philadelphia, PA.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Hajic?ov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Jan Hajic?, Barbora Vidov? Hladk?, and Petr Pajas.
2001. The Prague Dependency Treebank: anno-
tation structure and support. In Proceedings of
the IRCS Workshop on Linguistic Databases, pages
105?114. University of Pennsylvania, Philadelphia,
USA.
Richard A. Hudson. 1984. Word Grammar. Blackwell,
Oxford.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-99), pages 73?79. Association for Computa-
tional Linguistics, College Park, Maryland, USA.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of the
16th Nordic Conference of Computational Linguis-
tics (NODALIDA-2007), pages 105?112. Tartu, Es-
tonia.
Martha Kolln and Robert Funk. 1994. Understanding
English Grammar. Macmillan, New York.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool, San Rafael, CA.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 507?514. Association for Computa-
tional Linguistics, Sydney, Australia.
59
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Marec?ek, Martin Popel, Loganathan Ramasamy,
Jan ?te?p?nek, Daniel Zeman, Zdene?k ?abokrtsk?,
and Jan Hajic?. 2013. Cross-language study on in-
fluence of coordination style on dependency parsing
performance. Technial Report 49, ?FAL MFF UK.
Jonathan Margoliash. 2010. Matrix-Tree Theorem for
directed graphs. http://www.math.uchicago.
edu/~may/VIGRE/VIGRE2010/REUPapers/
Margoliash.pdf.
Igor Aleksandrovic? Mel?c?uk. 1988. Dependency Syn-
tax: Theory and Practice. SUNY Press, Albany,
NY.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical Report MSI report 05133,
V?xj? University School of Mathematics and Sys-
tems Engineering, V?xj?, Sweden.
Timothy Osborne, Michael Putnam, and Thomas Gro?.
2012. Catenae: introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390. Association for Computational Lin-
guistics, Atlanta, Georgia, USA.
Alonzo Reed and Brainerd Kellogg. 1877. Work on
English grammar & composition. Clark & Maynard.
Federico Sangati and Chiara Mazza. 2009. An English
dependency treebank ? la Tesni?re. In Marco Pas-
sarotti, Adam Przepi?rkowski, Savina Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eigth
International Workshop on Treebanks and Linguistic
Theories, pages 173?184. EDUCatt, Milan, Italy.
Gerold Schneider. 1998. A linguistic comparison of
constituency, dependency and link grammar. Mas-
ter?s thesis, University of Zurich.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. arXiv:1306.2091
[cs.CL]. arxiv.org/pdf/1306.2091.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986. The Meaning of the Sentence in its Seman-
tic and Pragmatic Aspects. Reidel, Dordrecht and
Academia, Prague.
Stuart M. Shieber. 1992. Constraint-Based Grammar
Formalisms. MIT Press, Cambridge, MA.
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the
Third International Workshop on Parsing Technol-
ogy (IWPT?93), pages 277?292. Tilburg, Nether-
lands.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 132?140. Associa-
tion for Computational Linguistics, Prague, Czech
Republic.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2008), pages 254?263. As-
sociation for Computational Linguistics, Honolulu,
Hawaii.
Mark Steedman. 2000. The Syntatic Process. MIT
Press, Cambridge, MA.
Luc Steels, Jan-Ola ?stman, and Kyoko Ohara, editors.
2011. Design patterns in Fluid Construction Gram-
mar. Number 11 in Constructional Approaches to
Language. John Benjamins, Amsterdam.
Lucien Tesni?re. 1959. El?ments de Syntaxe Struc-
turale. Klincksieck, Paris.
Michael Tomasello. 2003. Constructing a Language: A
Usage-Based Theory of Language Acquisition. Har-
vard University Press, Cambridge, MA.
60
