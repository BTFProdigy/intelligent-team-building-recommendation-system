St ructura l  d i sambiguat ion  of  morpho-syntact i c  categor ia l  pars ing 
for Korean  * 
Jeongwon Cha and Geunbae Lee 
Department of Computer Science & Engineering 
Pohang University of Science & Technology 
Pohang, Korea 
{himen, gblee}@postech.ac.kr 
Abstract 
The Korean Combinatory Categorial Grammar 
(KCCG) tbrmalism can unitbrmly handle word 
order variation among arguments and adjuncts 
within a clause as well as in complex clauses 
and across clause boundaries, i.e., long distance 
scrambling. Ill this paper, incremental pars- 
ing technique of a morpheme graph is devel- 
oped using the KCCG. We present echniques 
for choosing the most plausible parse tree us- 
ing lexical information such as category merge 
probability, head-head co-occurrence heuristic, 
and the heuristic based on the coverage of sub- 
trees. The performance r sults for various mod- 
els for choosing the most plausible parse tree are 
compared. 
1 I n t roduct ion  
Korean is a non-configurational, t)ostpositional, 
agglutinative language. Postpositions, uch as 
noun-endings, verb-endings, and prefinal verb- 
endings, are morphemes that determine the 
fnnctional role of NPs (noun phrases) and VPs 
(verb phrases) in sentences and also transform 
VPs into NPs or APs (adjective phrases). Since 
a sequence of prefinal verb-endings, auxiliary 
verbs and verb-endings can generate hundreds 
of different usages of the same verb, morpheme- 
based grammar modeling is considered as a nat- 
ural consequence for Korean. 
There have been various researches to dis- 
ambiguate the structural ambiguities in pars- 
ing. Lexical and contextual information has 
been shown to be most crucial for many pars- 
ing decisions, such as prepositional-phrase at- 
tachment (Hindle and Rooth, 1993). (Charniak, 
1995; Collins, 1996) use the lexical intbrmation 
* This  research was part ial ly supported by KOSEF  spe- 
cial basic resem'ch 1)rogram (1997.9 ~ 2000.8). 
and (Magerman and Marcus, 1991; Magerman 
and Weir, 1992) use the contextual information 
for struct;nral disambiguation. But, there have 
been few researches that used probability intbr- 
marion for reducing the spurious ambiguities in 
choosing the most plausible parse tree of CCG 
formalism, especially for morpho-syntactic pars- 
ing of agglutinative language. 
In this paper, we describe the probabilistic 
nmthod (e.g., category merge probability, head- 
head co-occurrence, coverage heuristics) to re- 
duce the spurious atnbiguities and choose the 
most plausible parse tree for agglutinative lan- 
guages uch as Korean. 
2 Overv iew of  KCCG 
This section briefly reviews the basic KCCG for- 
malism. 
Following (Steedman, 1985), order-preserving 
type-raising rules are used to convert nouns in 
grammar into the functors over a verb. The 
following rules are obligatorily activated uring 
parsing when case-marking morphemes attach 
to  nora1 s tems.  
? Type Raising Rules: 
np + case-marker 
feature\]) 
v/(v\np\[case- 
This rule indicates that a noun in the pres- 
ence of a case morpheme becomes a functor 
looking for a verb on its right; this verb is also 
a flmctor looking for the original noun with the 
appropriate case on its left. Alter tile noun 
functor combines with the appropriate verb, the 
result is a flmctor, which is looking for the re- 
maining arguments of the verb. 'v' is a w~ri- 
able tbr a verb phrase at ally level, e.g., the 
verb of a matrix clause or the verb of an em- 
bedded clause. And 'v' is matched to all of 
1002 
the "v\[X\]\Args" patterns of the verl, categories. 
Since all case-marked ilouns in Korean occur in 
front of the verb, we don't need to e, mploy the 
directional rules introduced by (Hoffman, 1995). 
We extend the combinatory rules ibr uncm'- 
ried flmctions as follows. The sets indicated by 
braces in these rules are order-free. 
? Forward Application (A>): 
x/(args u {Y}) Y X/Args 
? Backward Application (A<): 
Y X\(Args U {Y}) ==4- X\Args 
Using these rules, a verb can apply to its 
arguments in any order, or as in most cases, 
the casednarked noun phrases, which are type- 
raised flmctors, can apply to the, al)t)roi)riate 
verbs. 
Coordination constructions are moditied to 
allow two type-raised noml 1)hrases that are 
looking tbr the saxne verb to combine together. 
Since noun phrases, or a noun phrase and ad- 
verb phrase, are fimctors, the following compo- 
sition rules combine two flmctions with a set 
vahle al'gulnents. 
? Forward Composition (B>): 
X/(X\Ar.q.sx) Y/(Y\Arg.sy) ==~ 
x /  (X\ ( A,.:j<,: u ) ), 
Y = X\Arqsx 
? Backward Comi)osition (B<): 
Y\Arg.sy X\(Ar.q.sx U {Y}) ===> 
X\(A'rgsx U Arosy) 
? Coordination (~): 
X CONJX  ~ X 
3 Bas ic  morph-syntact i c  char t  
pars ing  
Korean chart parser has been developed based 
on our KCCG modeling with a 10(},0()0 mor- 
pheme dictionary. Each morpheme entry in 
the dictionary has morphological category, mor- 
photactics connectivity and KCCG syntax (:at- 
egories tbr the morpheme. 
In the morphological analysis stage, a un- 
known word treatment nmthod based on a mor- 
pheme pattern dictionary and syllable bigrams 
is used after (Cha et al, 1998). POS(part -of  
speech) tagger which is tightly coupled with 
the morphological analyzer removes the irrele- 
wmt morpheme candidates from the lnorpheme 
graph. The morpheme graph is a compact 
representation method of Korean morphologi- 
cal structure. KCCG parser analyzes the mor- 
pheme graph at once through the morpheme 
graph embedding technique (Lee et al, 1996). 
The KCCG parser incrementally analyzes the 
sentence, eojeol by eojeol :1 Whenever an eo- 
jeol is newly processed by the morphological n- 
alyzer, the morphenms resulted in a new mor- 
pheme graph are embedded in a chart and an- 
alyzed and combined with the previous parsing 
results. 
4 Statistical structured 
disambiguation for  KCCG parsing 
Th(' statistics which have been used in the ex- 
perinlents have been collected fronl the KCCG 
parsed corpora. The data required for train- 
ing have been collected by parsing the stan- 
dard Korean sentence types 2, example sentences 
of grammar book, and colloquial sentences in 
trade interview domain 3 and hotel reservation 
domain 4. We use about; 1500 sentences for 
training and 591 indq)endent sentences for eval- 
uation. 
The evaluation is based on parsewfl 
method (Black el, a\]., 1991). In the evalu- 
ation, "No-crossing" is 1;11o number of selltellces 
which have no crossing brackets between the 
result and |;tie corresponding correct trees of 
the sentences. "Ave. crossing" is the average 
number of crossings per sentence. 
4.1 Bas ic  s ta t i s t i ca l  model  
A basic method of choosing the nlost plausible 
parse tree is to order the prot)abilities by the lex- 
ical preib, rences 5 and the syntactic merge prob- 
ability. In general, a statistical parsing model 
defines the conditional probability, 1"(71S), for 
each candidate tree r tbr a sentence S. A gener- 
ative model uses the observation that maximis- 
ing P(% S) is equivalent to maximising P(r IS)  6. 
1Eojeol is a spacing unit in Korean and is similar to 
an English word. 
2Sentences of length < 11. 
aSentences of length < 25. 
4Sentences ofhmgth _< 13. 
5The frequency with which a certain category is as- 
sociated with a morpheme tagged for part-of-speech. 
c'P(S) is constmlt. 
1003 
Thus, when S is a sentence consisted of a se- 
quence of morphemes tagged for part-of-speech, 
(w~, t~), (w2, t2), ..., (w,,, tu), where wi is a i th 
morpheme, ti is the part-of-speech tag of the 
morpheme wi, and cij is a category with rela- 
tive position i, j, the basic statistical model will 
be given by: 
r* = arg ,~x P(rl,S' ) (1) 
(2) = argn~x P(S) 
,~ argmaxP(T,S ). (3) 
T 
The r* is the probabilities of the optimM parse 
tree. 
P(r, S) is then estimated by attaching proba- 
bilities to a bottom-up composition of the tree. 
P(r,S) = I I  P(cij) (4) 
c i j  ~T  
= H (P(eiilcik'ck+'J) 
c i j  ET  
xP(cik)P(cl~+lj)), (5) 
i<k<j ,  
i f  cij is a terminal, 
the,  P(c j) = 
and 
frcquency(cij, ti, wi) 
frequency(ti, wi) ' (6) 
frequency(eli, cik, Ch+lj) (7) 
P(eijleik, C~+lj) ~ frequency(cik, ck+lj) 
The basic statistical model has been applied 
to morpheme/part-of-speech/category 3-tuple. 
Due to the sparseness of the data, we have 
used part-of-speech/category pairs 7 together, 
i.e., collected the frequencies of the categories 
associated with the part-of-speeches assigned to 
the morpheme. Table 1 illustrates the sample 
entries of the category probability database. In 
table, 'nal (fly)' has two categories with 0.6375 
mid 0.3625 probability respectively. Table 2 il- 
lustrates the sample entries of the merge prob- 
ability database using equation 7. 
f requency  (old ,tl ) 7We def ine  th i s  as  P(c l j l t l )  ~ fvcq . . . . . .  y ( tD  " 
Table 3: 
Model 
Results fl'om the Basic Statistical 
Total sentences 
No-crossing 
Ave. crossing 
Labeled Recall 
Labeled Precision 
591 
74.62% 
1.00 
77.02 
79.15 
Figure 1: Sub-constituents for head-head co- 
occurrence heuristics 
Table 3 summarizes the results on an open 
test set of 591 sentences. 
4.2 Head-head co -occur rence  heur i s t i cs  
In the basic statistical model, lexicM depen- 
dencies between morphemes that take part in 
merging process cannot be incorporated into the 
model. When there is a different morpheme 
with the same syntactic category, it can be a 
miss match on merging process. This linfita- 
tion can be overcome through the co-occurrence 
between the head morphemes of left and right 
sub-constituent. 
When B h is a head morphenm of left sub- 
constituent, r is a case relation, C h is a head 
morpheme of right sub-constituent as shown in 
figure 1, head-head co-occurrence heuristics are 
defined by: 
p(B,LI,. ,Ch ) ~ f requency(B h,r, C h) 
frequency(r, C h) " (8) 
Tile head-head co-occurrence heuristics have 
been augmented to equation 5 to model the lex- 
ical co-occurrence preference in category merg- 
ing process. Table 4 illustrates the sample en- 
tries of the co-occurrence probability database. 
In Table 4, a morpheme 'sac (means 'bird')', 
which has a "MCK (common noun)" ms POS 
tag, has been used a nominative of verb 'hal 
(means 'fly')' with 0.8925 probability. 
1004 
Table 1: Sample entries of the category probal)ility database ('DII' Ineans an '1' irregular verb.) 
P()S, morpheme category probability 
DII, nal v\[D\]\ {np\[noln\]} 0.6375 
DI1, hal v\[D\]\{np\[noln\],nl)\[acc\]} 0.362,5 
DI1 v \[D\]  {rip \[nora\] } 0.3079 
DI1 v\[D\]\ {np\[llOm\],np\[acc\] } 0.2020 
Table 2: Sample entries of' syntactic merge probability database 
left; category 
~, / ( ~ \,u,\[,,o,,l\]) 
~,/(~, \ ,p lace\]) 
right category 
v\[D\]\ {np\[noml,np\[acc\]} 
v\[D\]\ {,,p \[,lo,,,\],,u,\[acd } 
inerged category 
v\[D\]\{,,p\[acd} 
v\[D\]\ {ni)\[nonl\] } 
probability 
0.0473 
0.6250 
nl, (v / (v \nont ) ) \n  t , v/(v\np\[nom\]) I).2197 
The modified model has been tested Oil the 
same set of the open sentences as in the 1)asic 
model ext)eriment. 'l~fl)le 5 smnmarizes the re- 
sult of these expcwiments. 
? Ezperimcnt: (linear combination af th, c ba- 
sic model and the head-h, cad co-occurrence 
heuristics). 
P(% s) 
eij { r 
+/~p( \ ] / '  I,,., c*')) 
? P(~,ik)~'(~,k+,;)), (9) 
i < k < j, 
i f  cij is a terminal, 
~J,.,;',~ p(c#i) = P(c.~:i I~g, td. 
Ta,bh; 5: Results from the Basic: Statistical 
Model t)lns head-head co-occurrence heuristics 
Total sentences 591 
No-crossing 81.05% 
Ave. crossing 0.70 
Labeled Recall 84.02 
Labeled Precision 85.30 
4.3 The  coverage heur is t ics  
If" there is a case relation or a modification re- 
lation in two constituents, coverage heuristics 
designate it is easier to add the smaller tree to 
the larger one ttlan to merge the two medium 
sized trees. On the contrary, in the coordination 
relation, it is easier to nmrge two medium sized 
trees. We implemented these heuristics using 
/;tie tbllowing coverage score: 
Case relation, modification relation: 
COV_scorc = 
le f t  subtrec coverage + riqh, t sub/roe coverage. (j_()~ 
4 ? ~7~ ,~,,bt,.~,.,~ o,,,,',,e ;< ','i:jl,,i ~,,b>'.e eo,,~',',,.~,'. " 
Coo~d'iuatio'n: 
COV_sco'rc = 
'e x x / le f t  .~,a,l.,'~c. ~o.,,,.~,.,,.:,.. x ,'#lht .~,O,l,,.,, ,:o.,,~,.,,,~ 1~ 
leJ't subtree cove,.aqe + R~ ~b~r('.e ~; .~t  . 
A coverage heuristics are added to the basic: 
model to model the structural preferences. Ta- 
ble 6 shows the results of the experinlents on 
the same set of the open sentences. 
? Ezpcriment: (the basic model to th, c 
COV_scorc heuristics). We have used (;tie 
COV_.sco're as the exponent weight feature 
for this experiment since the two nmnl)ers 
arc; in the different nature of statistics. 
P(7-, S) = H (P(ciJ\] cik, Ok+l J) l-COV-'sc?rc 
eij CT 
?p(~k)p(c~+,j)), (1~,) 
i<k<j ,  
i f  Cij iS  a terminal, 
o,: , ,  P(~.j) = 1)(c~.jl~,~, ~d. 
1005 
Table 4: Sample entries of co-occurrence probability database. 
head-head co-occurrence probability 
(MCC <ganeungseong>,np\[nom\],HIl.< nob>) 0.8932 
(MCK<sae>,np\[nom\],DIl<nal>) 0.8925 
(MCK<galeuchim>,np\[acc\],DIeu<ddaleu>) 0.8743 
Table 6: Results from the Basic Statistical 
model plus Coverage heuristics 
Total sentences 591 
No-crossing 80.13% 
Ave. crossing 0.81 
Labeled Recall 82.59 
Labeled Precision 83.75 
5 Summary 
We developed a morpho-syntactic categorial 
parser of Korean and devised a morpheme- 
based statistical structural disambiguation 
s(;henles. 
Through the KCCG model, we successthlly 
handled ifficult Korean modeling problems, in- 
chtding relative free-word ordering, coordina- 
tion, and case-marking, during the parsing. 
To extract he most plausible parse trees ti'om 
the parse forest, we have presented basic statis- 
tical techniques using the lexical and contextual 
information such as morpheme-category p oba- 
bility and category merge probability. 
Two different nature of heuristics, head-head 
co-occurrence and coverage scores, are also de- 
veloped and tested to augment the basic statis- 
tical model. Each of them demonstrates reason- 
able t)ertbrmance increase. 
The next step will be to devise more heuristics 
and good combination strategies tbr the differ- 
ent nature of heuristics. 
References 
E. Black, S. Abney, D. Flickenger, C. Gdaniec, 
R. Grishman, P. Harrison, D. Hindle, R. In- 
gria, F. Jelinek, J. Klavans, M. Liberman, 
M. Marcus, S. Roukos, B. Santorini, and 
T. Strzalkowski. 1991. A Procedm'e for 
Quantitatively Comparing the Syntactic Coy- 
erage of English Grammars. In Prec. of 
Fourth DARPA Speech and Natural Lan- 
guage Workshop. 
Jeongwon Cha, Gcunbae Lee, and Jong-Hyeok 
Lee. 1998. Generalized unknown morpheme 
guessing for hybrid pos tagging of korean. 
In Pwceedings of Sixth Workshop on Very 
Large Corpora in Coling-ACL 98, Montreal, 
Canada.  
E. Charniak. 1995. Prsing with Context-Free 
Grammars and Word Statistics. Technical 
Report CS-95-28, Brown University. 
M. Collins. 1996. A New Statistical Parser 
Based on Bigram Lexical Dependencies. In 
Proceedings of th, e 3/tth Annual Meeting of the 
A CL, Santa Cruz. 
D. Hindle and M. Rooth. 1993. Structural am- 
biguity and lexical relations. Computational 
Linguistics, 19(1):103-120. 
B. Hoffman. 1995. ~7~,c Computational Analy- 
sis of the Syntax and Interpretation of 'if;roe" 
Word Order in Turkish. Ph.D. thesis, Univer- 
sity of Pennsylwmia. IRCS Report 95-17. 
Wonil Lee, Gennb:m Lee, and Jong-Hyeok Lee. 
1996. Chart-driven connectionist categorial 
t)arsing of spoken korean. Computer process- 
ing of oriental languages, Vol 10, No 2:147-- 
159. 
D. M. Magerman and M. P. Marcus. 1991. 
Parsing the voyager domain using t)earl. In 
In Prec. Of the DARPA Speech and Natural 
Language Workshop~ pages 231-236. 
D. M. Magerman and C. Weir. 1992. E f  
ficiency, robustness and accuracy in picky 
chart parsing. In In Prec. Of the 30th An- 
nual Meeting of the Assoc. For Computa- 
tional Linfluisties(ACL-92), pages 40 47. 
Mark Steedman. 1985. Dependency and Coor- 
dination in the Grammar of Dutch and En- 
glish. Language, 61:523 568. 
1006 
Syllable-Pattern-Based Unknown-
Morpheme Segmentation and Estimation
for Hybrid Part-of-Speech Tagging of
Korean
Gary Geunbae Lee Jeongwon Chay
Pohang University of Science and
Technology
Pohang University of Science and
Technology
Jong-Hyeok Leez
Pohang University of Science and
Technology
Most errors in Korean morphological analysis and part-of-speech (POS) tagging are caused
by unknown morphemes. This paper presents a syllable-pattern-based generalized unknown-
morpheme-estimation method with POSTAG (POStech TAGger),1 which is a statistical and
rule-based hybrid POS tagging system. This method of guessing unknown morphemes is based
on a combination of a morpheme pattern dictionary that encodes general lexical patterns of Korean
morphemes with a posteriori syllable trigram estimation. The syllable trigrams help to calculate
lexical probabilities of the unknown morphemes and are utilized to search for the best tagging
result. This method can guess the POS tags of unknown morphemes regardless of their numbers
and/or positions in an eojeol (a Korean spacing unit similar to an English word), which is not
possible with other systems for tagging Korean. In a series of experiments using three different
domain corpora, the system achieved a 97% tagging accuracy even though 10% of the morphemes
in the test corpora were unknown. It also achieved very high coverage and accuracy of estimation
for all classes of unknown morphemes.
1. Introduction
Part-of-speech (POS) tagging involves many difficult problems, such as insufficient
amounts of training data, inherent POS ambiguities, and (most seriously) many types
of unknown words. Unknown words are ubiquitous in any application and cause
major tagging failures in many cases. Since Korean is an agglutinative language, it
presents more serious problems with unknown morphemes than with unknown words
because more than one morpheme can be unknown in a single word and morpheme
segmentation is usually very difficult.
 NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: gblee@postech.ac.kr.
y NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: himen@postech.ac.kr.
z NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: jhlee@postech.ac.kr.
1 The binary code of POSTAG is open to the public for research and evaluation purposes at
http://nlp.postech.ac.kr/. Follow the link OpenResources!DownLoad.
c? 2002 Association for Computational Linguistics
Computational Linguistics Volume 28, Number 1
Previous techniques for guessing unknown words mostly utilize the guessing rules
to analyze the word features by looking at leading and trailing characters. Most of them
employ the analysis of trailing characters and other features such as capitalization and
hyphenation (Kupiec 1992; Weischedel et al 1993). Some of them use more morpho-
logically oriented word features such as suffixes, prefixes, and character lengths (Brill
1995; Voutilainen 1995). The guessing rules are usually handcrafted using knowledge
of morphology but sometimes are acquired automatically using lexicons and corpora
(Brill 1995; Mikheev 1996; Oflazer and Tu?r 1996). Previously developed methods for
guessing unknown morphemes in Korean are not much different from the methods
used for English. Basically, they rely on the rules that reflect knowledge of Korean
morphology and word formation. The usual way of handling unknown morphemes is
to guess all the possible POS tags for an unknown morpheme by checking connectable
functional morphemes in the same eojeol (Kang 1993).2 However, in this way, it is only
possible to guess probable POS tags for a single unknown morpheme when it occurs
at the beginning of an eojeol. Unlike in English, in Korean, more than one unknown
morpheme can appear in a single eojeol because an eojeol can include complex compo-
nents such as Chinese characters, Japanese words, and other foreign words. If an eojeol
contains more than one unknown morpheme or if the unknown morphemes appear
in other than first position in the eojeol, all previous methods fail to efficiently estimate
them. This is the reason why we try to avoid conventional guessing rules using word
morphology features such as those proposed in Mikheev (1996) and Oflazer and Tu?r
(1996).3
In this paper, we propose a syllable-pattern-based generalized unknown-morph-
eme estimation method using a morpheme pattern dictionary that enables us to treat
unknown morphemes in the same way as registered known morphemes, and thereby
to guess them regardless of their numbers or positions in an eojeol. The method for
estimating unknown morphemes using the morpheme pattern dictionary in Korean
needs to be tightly integrated into morphological analysis and POS disambiguation
systems.
POS disambiguation has usually been performed by statistical approaches, mainly
using the hidden Markov model (HMM) in English research communities (Cutting
et al 1992; Kupiec 1992; Weischedel et al 1993). These approaches are also domi-
nant for Korean, with slight improvements to accommodate the agglutinative nature
of Korean. For Korean, early HMM tagging was based on eojeols. The eojeol-based
tagging model calculates lexical and transition probabilities with eojeols as a unit; it
suffers from severe data sparseness problems since a single eojeol consists of many
different morphemes (Lee, Choi, and Kim 1993). Later, morpheme-based HMM tag-
ging was tried; such models assign a single tag to a morpheme regardless of the
space in a sentence. Morpheme-based tagging can reduce data sparseness problems
but incurs multiple observation sequences in Viterbi decoding since an eojeol can be
segmented in many different ways. Researchers then tried many ways of reducing
computation due to multiple observation sequences, such as shared word sequences
and virtual words (Kim, Lim, and Seo 1995) and two-ply HMM for morpheme unit
computation but restricted within an eojeol (Kim, Im, and Im 1996). However, since
statistical approaches take neighboring tags into account only within a limited win-
2 An eojeol is a Korean spacing unit (similar to an English word), which usually consists of one or more
stem morphemes and a series of functional morphemes.
3 Even though Turkish and Finnish are in the same class of agglutinative languages and German also
has very complex morphological structures, in our view word formation is more diverse and complex
in Korean than in these Western languages because of its mix of Oriental and Western culture.
54
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
dow (usually two or three), sometimes the decision fails to cover important linguistic
contexts necessary for POS disambiguation. Also, approaches using only statistical
methods are inappropriate for idiomatic expressions, for which lexical terms need
to be directly referenced. And especially, statistical approaches alone do not suffice
for agglutinative languages, which usually have complex morphological structures.
In agglutinative languages, a word usually consists of one or more stem morphemes
plus a series of functional morphemes; therefore, each morpheme should receive a
POS tag appropriate to its functional role to cope with the complex morphological
phenomena in such languages. Recently, rule-based approaches, which learn symbolic
tagging rules automatically from a corpus, have been reconsidered, to overcome the
limitations of statistical approaches (Brill 1995). Some systems even perform POS tag-
ging as part of a syntactic analysis process (Voutilainen 1995). Following the success
of transformation-based approaches, attempts have been made to use transformation
rules in systems for tagging Korean (Im, Kim, and Im 1996). However, in general,
rule-based approaches alone are not very robust and are not portable enough to be
adjusted to new tagsets or new languages. Also, they usually perform no better than
their statistical counterparts (Brill 1995). To gain portability and robustness and also
to overcome the limited coverage of statistical approaches, we need to somehow com-
bine the two approaches to gain the advantages of each. In this paper, we propose
a hybrid method that combines statistical and rule-based approaches to POS disam-
biguation and can be tightly coupled with generalized unknown-morpheme-guessing
techniques.
2. Linguistic Characteristics of Korean
Korean is classified as an agglutinative language. In Korean, an eojeol consists of sev-
eral morphemes that have clear-cut morpheme boundaries. For example, na-neun gam-
gi-e geol-lyeoss-dda ?I caught a cold? consists of 3 eojeols and 7 morphemes:4 na(?I?)/T
+ neun(?auxiliary particle?)/jS, gam-gi(?cold?)/MC + e(?adverb and conjunctive parti-
cle?)/jO, geol-li(?catch?)/DR + eoss(?past tense?)/eGS + dda(?final ending?)/eGE. Below
are the characteristics of Korean that must be considered for morphological-level nat-
ural language processing and POS tagging.
 POS tagging of Korean is usually performed on a morpheme basis rather
than on an eojeol basis. Accordingly, morphological analysis is essential
to POS tagging because morpheme segmentation is much more
important and difficult than POS assignment. Moreover, morphological
analysis should segment eojeols that contain unknown morphemes as
well as known morphemes. Hence, unknown-morpheme handling
should be integrated into the morphological analysis process. Because a
single eojeol can have many possible analyses (e.g., na-neun: na(?I?)/T +
neun(?topic marker?)/jS, na(?sprout?)/DR + neun(?adnominal?)/eCNMG,
nal(?fly?)/DI + neun(?adnominal?)/eCNMG, morpheme segmentation is
inherently ambiguous.
 Korean is a postpositional language with many kinds of noun endings
(particles), verb endings, and prefinal verb endings. It is these functional
morphemes, rather than the order of eojeols, that determine grammatical
4 Here, ?+? represents a morpheme boundary in an eojeol and ?/? introduces the POS tag symbols (see
Table 2).
55
Computational Linguistics Volume 28, Number 1
Table 1
Sample distribution of unknown morphemes in
Korean.
Tag # morphemes Tag # morphemes
MC 2,888 (29.7%) S 1,358 (14.0%)
MPN 650 (6.7%) B 603 (6.2%)
MPP 235 (2.4%) T 50 (0.5%)
MPC 56 (0.6%) Symbol 10 (0.1%)
MPO 728 (7.5%) Foreign word 3,140 (32.3%)
relations such as a noun?s syntactic function, a verb?s tense, aspect,
modals, and even modifying relations between eojeols. For example,
ga/jC is a case particle, so the eojeol uri(we)-ga has a subject role due to
the particle ga/jC. Korean has a clear syllable structure within the
morpheme; most nominal content morphemes keep their surface form
when they are combined with functional morphemes.
 Korean is basically an SOV language but has relatively free word order
compared with English. The weight , in Equation (1) (Section 4.1)
reflects the fact that transition probability is less important in Korean
than in English. However, Korean does have some word order
constraints: verbs must appear in sentence-final position, and modifiers
must be placed before the element they modify. So some order
constraints must be selectively utilized as contextual information in the
POS tagging process, which is taken well into account in the design of
error correction rules (Section 4.3).
 Complex spelling changes (irregular conjugations) frequently occur
between morphemes when two morphemes combine to form an eojeol.
These spelling changes make it difficult to segment the original
morphemes before the POS tag symbols are assigned.
 The unknown-morpheme problem in Korean differs in some ways from
the unknown-word problem in English. In English, it is easy to identify
unknown words because they occur between spaces. However, in
Korean, since unknown morphemes are hidden in an eojeol, we only
know that morphological analysis failed in that eojeol; pinpointing the
exact unknown morphemes is usually difficult. This is why, unlike in
English, it is not possible to fully guess an unknown morpheme using
only affixes. The distribution of POS tags for unknown morphemes
extracted from a 130,000-morpheme training corpus (9,718 unknown
morphemes) is shown in Table 1. The distribution from even a small
corpus shows that we need to estimate various parts of speech for
unknown morphemes rather than simply guess them as nouns.
Table 2 shows the tagset that was used in the experiments reported in Section 5.
The tagset was selected from hierarchically organized POS tags for Korean. We defined
about 100 different POS tags, which can be used in morphological analysis as well as
in POS tagging. We also designed over 300 morphotactic adjacency symbols to be
used in morpheme connectivity checks for correct morpheme segmentation (to be
explained in the next section). The POS tags are hierarchically organized symbols
56
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 2
A tagset with 41 tags.
Major category Tag Description
Nominal MC common noun
MPN person name
MPC country name
MPP place name
MPO other proper noun
MD bound noun
T pronoun
S numeral
Predicate DR regular verb
DI irregular verb
HR regular adjective
HI irregular adjective
I i-predicative particle
E existential predicate
b auxiliary verb
Modifier G adnoun
B adverb
Particle y predicative particle
jC case particle
jS auxiliary particle
jO adverb and conjunctive particle
Ending eGE final ending
eGS prefinal ending
eCNDI aux conj ending
eCNDC quote conj ending
eCNMM nominal ending
eCNMG adnominal ending
eCNB adverbial ending
eCC conjunctive ending
Affix + prefix
? suffix
Special symbol su unit symbol
s? left parenthesis
s? right parenthesis
s. sentence closer
s- sentence connection
s, sentence comma
sf foreign word
sh Chinese character
so other symbol
Interjection K interjection
57
Computational Linguistics Volume 28, Number 1
that were iteratively refined from the eight major grammatical categories of Korean:
nominal, predicate, modifier, particle, ending, affix, special symbol, and interjection.
For a given morpheme, the acronym of a path name in the symbol hierarchy up to a
certain level is assigned as a POS tag.5 The rest of the detailed hierarchies, which are
related only to morpheme connectivity, are independently assigned as morphotactic
adjacency symbols. Therefore, we can use either full or partial path names as POS tags
in order to adjust the total number of tags. The size of the tagset can thus be adapted
by refining grammatical categories that are more pertinent to a given application. For
example, for text-indexing applications, we refine nominals more than predicates since
index terms are usually nominals in these applications.
3. Unknown-Morpheme Segmentation during Morphological Analysis
The agglutinative nature of Korean inevitably requires doing morphological analysis
before POS tagging. Morphological analysis, which segments input texts into morpho-
tactically connectable morphemes and assigns all possible POS tags to each morpheme
by looking them up in a morpheme dictionary, is a basic step in natural language pro-
cessing.
Our morphological analysis follows three general steps (Sproat 1992): morpheme
segmentation, recovering original morphemes from spelling changes, and morpho-
tactic modeling. Input texts are scanned from left to right, character by character,6
to be matched with morphemes in a morpheme dictionary. The morpheme dictio-
nary has a trie structured index for fast matching. It also has an independent entry
for each variant surface form (called allomorph) of the original morpheme so the
original morphemes can easily be reconstructed from spelling changes (see Table 3).
For morphotactic modeling, we used the POS tags and the morphotactic adjacency
symbols in the dictionary. The POS tags provide information about morpheme class,
while the morphotactic adjacency symbols provide information about grammatical
connectivity between morphemes needed to form an eojeol. The full hierarchy of POS
tags and morphotactic adjacency symbols is encoded in the morpheme dictionary
for each morpheme. Besides the morpheme dictionary, to model morphemes? con-
nectability to one another the system uses an independent morpheme connectivity ta-
ble that encodes all the connectable pairs of morpheme groups using the morphemes?
tags and morphotactic adjacency symbol patterns. After an input eojeol is segmented
by trie indexed dictionary searches, the morphological analysis checks whether each
segmentation is grammatically connectable by looking in the morpheme connectivity
table.
For unknown-morpheme segmentation, we developed a generalized method for
estimating unknown morphemes regardless of their position and number. Using a
morpheme pattern dictionary, our system can look up unknown morphemes exactly
the same way it looks up known registered morphemes. The morpheme pattern dic-
tionary covers all the necessary syllable patterns for unknown morphemes, including
common nouns, proper nouns, adverbs, regular and irregular verbs, regular and ir-
regular adjectives, and special symbols for foreign words. The lexical patterns for
morphemes are collected from previous studies (Kang 1993) where the constraints on
Korean syllable patterns regarding morpheme connectivity are well described. Table 4
shows some sample entries in the morpheme pattern dictionary, where Z, V, ?*? are
5 For example, nominal(M):proper-noun(P):person-name(N) is a three-level path name.
6 The character sequence in na-neun is n, a, n, eu, n.
58
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 3
Examples of morpheme dictionary entries. MCC is a full POS tag that
identifies a common noun consisting of Chinese characters. MCK identifies a
common noun consisting only of Korean characters. DIgeo-la represents a
geo-la irregular verb, and HIl represents an l irregular adjective. Yu represents
that ga-gong has a final consonant (ng). D-ha, H-ha, and D-doe are morphotactic
adjacency symbols for predicate particles. Nominals that have a D-ha as a
morphotactic adjacency symbol can be connected with predicate particles, and
they play the role of a verb or adjective. In verb or adjective, gyu represents a
regular form of an irregular conjugation, bul represents an irregular form of
an irregular conjugation. Eo is a morphotactic adjacency symbol for vowel
harmony when connecting with endings. Chug-yag represents that a particular
verb (or adjective) contains the special contracted ending. ?>? is a special
symbol for adjacent direction (?>?= right connection; ?<?= left connection).
POS-tag<original form> (Allomorph) [Morphotactic adjacency symbols]
MCC<ga-gong> (ga-gong) [yu>D-ha>H-ha>D-doe>]
MCK<geo-leum> (geo-leum) [yu>D-ha>]
DIgeo-la<geon-neo-ga> (geon-neo-ga) [gyu>chug-yag>]
DId<al-a-deud> (al-a-deud) [gyu>]
DId<al-a-deud> (al-a-deul) [bul>eo>]
DIs<heu-li-jeos> (heu-li-jeo) [bul>eo>]
DIs<heu-li-jeos> (heu-li-jeos) [gyu>]
HIl<ga-neul> (ga-neu) [bul>]
HIl<ga-neul> (ga-neul) [gyu>eo>]
Table 4
Sample entries in the morpheme pattern dictionary. Symbol meanings are
explained in Table 3.
POS-tag<original form> (Allomorph) [Morphotactic adjacency symbols]
HIl<ZV*gal> (ZV*gal) [gyu>eo>]
HIl<ZV*gal> (ZV*ga) [bul>]
HIb<ZV*ZVb> (ZV*u) [bul>]
HIb<ZV*ZVb> (ZV*weo) [chug-yag>]
HIb<ZV*ZVb> (ZV*wa) [chug-yag>]
DIs<ZV*jeos> (ZV*jeos) [gyu>]
DIs<ZV*jeos> (ZV*jeo) [bul>eo>]
DId<ZV*deud> (ZV*deud) [gyu>]
DId<ZV*deud> (ZV*deul) [bul>eo>]
metacharacters that indicate a consonant, a vowel, and any number of Korean charac-
ters, respectively. For example, go-ma-weo ?thanks?, which is a morpheme and an eojeol
at the same time, is matched to (ZV*weo) (shown in Table 4, where it is b, irregular ad-
jective (HIb)) in the morpheme pattern dictionary, which allows the system to recover
its original morpheme form go-mab.
Once the unknown morphemes are identified and recovered using the pattern
dictionary, when checking the unknown morphemes to see if they are connectable,
the system can use the same information about adjacent morphemes in the unknown
morphemes? eojeol that it would use if they were known morphemes. This is the reason
why our method can be called ?generalized? and can identify unknown morphemes
regardless of their position and number in an eojeol. The actual POS estimation is inte-
grated into the POS tagging process that will be described in Section 4.2. The essential
59
Computational Linguistics Volume 28, Number 1
morphological
analyzer
statistical
POS tagger
post error-
corrector
morpheme
dictionary
morpheme
pattern
dictionary
morpheme
connectivity
table
Input
sentence
lexical /
transition
probabilities
syllable
trigram
morpheme
graph
morpheme
graph
tagged
sentence
error-
correcting
rules
Figure 1
Statistical and rule-based hybrid architecture for POS tagging of Korean.
idea of the morpheme pattern dictionary is to pre-collect all the possible general lexical
patterns of Korean morphemes and encode each lexical syllable pattern with all the
candidate POS tags. Therefore, the system can assign initial POS tags to each unknown
morpheme simply by matching the syllable patterns in the pattern dictionary. In this
way, unlike previous approaches, ours does not need to incorporate a special rule-
based unknown-morpheme-handling module into its morphological analyzer, and all
the possible POS tags can be assigned to unknown morphemes just as they are to
known morphemes.
4. A Statistical and Rule-Based Hybrid Tagging Model
Figure 1 shows a proposed hybrid architecture for POS tagging of Korean with syllable-
pattern-based generalized unknown-morpheme guessing. It has three major compo-
nents: the morphological analyzer with unknown-morpheme handler, the statistical
POS tagger, and the rule-based error corrector. The morphological analyzer segments
the morphemes from input eojeols and reconstructs the original morphemes from
spelling changes by recovering the irregular conjugations. It also assigns all possi-
ble POS tags to each morpheme by consulting a morpheme dictionary. The unknown-
morpheme handler, which is tightly integrated into the morphological analyzer, assigns
initial POS tags to morphemes that are not registered in the dictionary, as explained
in the previous section. The statistical POS tagger runs the Viterbi algorithm (Forney
1973) on the morpheme graph to search for the optimal tag sequence for POS dis-
ambiguation. To remedy the defects of a statistical POS tagger, we developed an a
posteriori error correction mechanism. The error corrector is a rule-based transformer
60
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
(Brill 1995), and it corrects mistagged morphemes by consulting lexical patterns and
necessary contextual information.
4.1 The Statistical POS Tagger
The statistical POS tagging model takes the morpheme graph (output of the morpho-
logical analyzer) and selects the best morpheme and POS tag sequence7 for sentences
represented in the morpheme graph. The morpheme graph is a compact way of repre-
senting multiple morpheme sequences for a sentence. Each morpheme?s tag is a node
in the graph and its morpheme connectivity is a link. Our statistical tagging model is
modified from the standard bigrams (Cutting et al 1992) using Viterbi search plus on-
the-fly extra computing of lexical probabilities for unknown morphemes. The equation
used for the statistical tagging model is a modified bigram model with left-to-right
search,
T = argmax T
n
Y
i=1
Pr(ti j ti?1)

Pr(ti j mi)
Pr(ti)

(1)
where T is an optimal tag sequence that maximizes the forward Viterbi scores.
Pr(ti j ti?1) is a bigram tag transition probability and Pr(tijmi)Pr(ti) is a modified morpheme
lexical probability.  and  are weights and are set at 0.4 and 0.6, respectively, which
means that lexical probability is more important than transition probability given the
relatively free word order of Korean. This equation was finally selected after extensive
experiments using the following six equations:
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(mi j ti) (2)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(mi j ti) (3)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(ti j mi) (4)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(ti j mi) (5)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)
Pr(ti j mi)
Pr(ti)
(6)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)

Pr(ti j mi)
Pr(ti)

(7)
In the experiments, we used the 10,204-morpheme training corpus from the Kemong
Encyclopedia.8 Table 5 shows the tagging performance of each equation.
Training of the statistical tagging model requires a parameter estimation process
for two different parameters, that is, morpheme lexical probabilities and bigram tag
transition probabilities. Several studies show that using as much tagged material as
7 Because a Korean eojeol can be segmented in many different ways, selecting the best morpheme
segmentation sequence is as important as selecting the best POS sequence in POS tagging.
8 Provided by the Electronics and Telecommunications Research Institute (ETRI).
61
Computational Linguistics Volume 28, Number 1
Table 5
Tagging performance (all in %) of each equation. The ?eojeol? row shows
eojeol-unit tagging performance, and the ?morpheme? row shows
morpheme-unit performance.
Eq. (2) Eq. (3) Eq. (4) Eq. (5) Eq. (6) Eq. (7) (Eq. (1))
Eojeol 86.80 90.48 89.40 89.62 91.73 92.48
Morpheme 91.32 94.93 94.40 94.48 95.77 96.12
possible for training gives much better performance than unsupervised training using
the Baum-Welch reestimation algorithm (Merialdo 1994). We therefore decided to use
supervised training using tagged corpora with relative frequency counts. The three
necessary probabilities can be estimated as in Equations (8)?(10),
Pr(ti j mi)  f (ti j mi) =
N(mi, ti)
N(mi)
(8)
Pr(ti)  f (ti) =
N(ti)
PNts
n=1 N(tn)
(9)
Pr(ti j ti?1)  f (ti j ti?1) =
N(ti?1, ti)
N(ti?1)
(10)
where N(mi, ti) indicates the total number of occurrences of the morpheme mi together
with the specific tag ti, while N(mi) indicates the total number of occurrences of the
morpheme mi in the tagged training corpus. Nts indicates the total number of POS tags
in the tagset. N(ti?1, ti) and N(ti?1) can be interpreted similarly for two consecutive
tags ti?1 and ti.
A beam search strategy is utilized for high-speed tagging. For each morpheme in
the sentence, the highest probability, Ph, of the tag is recorded. All other tags associated
with the same morpheme must have probabilities greater than Ph? for some constant
beam size ?; otherwise, they are discarded. The beam may introduce search errors,
but, in practice, search efficiency can be greatly improved with virtually no loss of
accuracy.
4.2 Lexical Probability Estimation for Unknown-Morpheme Guessing
The lexical probabilities for unknown morphemes cannot be precalculated using Equa-
tion (8) since we assume the unknown morphemes do not appear in the training cor-
pus, so a special on-the-fly estimation method must be applied. We suggest using
syllable trigrams since Korean syllables can play an important role in restricting units
for guessing the POS of a morpheme. The lexical probability Pr(tijmi)Pr(ti) for unknown mor-
phemes can be estimated using the frequency of syllable trigram products according
to the formula in (11)?(13) (Nagata 1994),
m = e1e2 : : : en (11)
Pr(t j m)
Pr(t)
 Pr t(e1 j #, #)Pr t(e2 j #, e1)

n
Y
i=3
Pr t(ei j ei?2, ei?1)
62
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
 Pr(# j en?1, en) (12)
Pr t(ei j ei?2, ei?1)  ft(ei j ei?2, ei?1)
+ ft(ei j ei?1)
+ ft(ei) (13)
where m is a morpheme, e is a syllable, t is a POS tag, ?#? is a morpheme boundary
symbol, and ft(ei j ei?2, ei?1) is a frequency datum for tag t with co-occurrence syllables
ei?2, ei?1, and ei. Trigram probabilities are smoothed by Equation (13) to cope with the
data sparseness problem. For example, Park-jong-man is the name of a person, so it is
an unknown morpheme. The lexical probability that Park-jong-man should be assigned
the tag MPN (person name) is estimated using the following formula:
Pr(MPN j Park ? jong ?man)
Pr(MPN )
 PrMPN (Park j #, #)
 PrMPN (jong j #, Park)
 PrMPN (man j Park , jong)
 PrMPN (# j jong , man) (14)
In Park-jong-man, Park is usually a family name. If the first position of this mor-
pheme is a family name, the probability that MPN is the correct tag becomes higher
than the probability that the other tags are correct. Table 6 shows the distribution of
Pr(Park j #, #) for each possible tag. In Equation (14), PrMPN (Park j #, #) represents
the popularity of the tag MPN for the morpheme Park-jong-man.
All the trigrams for Korean syllables were precalculated and stored in the database
and are applied with the candidate tags during the unknown-morpheme POS guessing
and smoothing portion of the statistical tagging process.
4.3 A Posteriori Error Correction Rules
Statistical morpheme tagging is widely known to cover only a limited range of con-
textual information. Moreover, it cannot refer to lexical patterns as a context for POS
disambiguation. As mentioned earlier, because Korean eojeols have very complex mor-
phological structures, it is necessary to look at the functional morphemes selectively to
determine the grammatical relations between eojeols. For these reasons, we designed
error correction rules for eojeols to compensate for the estimation and modeling errors
Table 6
The distribution of Pr(Park j #, #) for each tag.
MC MPN MPC MPP MPO T
No. of ?##Park? 125 2081 0 0 8 0
No. of ?##? 115,841 25,915 589 1,209 50,671 4,255
Pr(Park j #, #) 0.001 0.080 0.000 0.000 0.000 0.000
B DR DI HR HI
No. of ?##Park? 5 17 2 0 9
No. of ?##? 9,169 21,119 13,555 2,243 5,217
Pr(Park j #, #) 0.000 0.000 0.000 0.000 0.001
63
Computational Linguistics Volume 28, Number 1
Table 7
Examples of rule schemata used to extract the error correction rules automatically from the
tagged corpus. The POSTAG system has about 24 rule schemata of this form.
Rule schema Acronym description
N1FT the tag of the first morpheme (FT) of the next eojeol (N1)
P1LT the tag of the last morpheme (LT) of the previous eojeol (P1)
N2FT the tag of the first morpheme (FT) of the eojeol after the next one (N2)
N3FT the tag of the first morpheme (FT) of the second eojeol after the next one (N3)
P1LM the lexical form of the last morpheme (LM) of the previous eojeol (P1)
P1FM the lexical form of the first morpheme (FM) of the previous eojeol (P1)
N1FM the lexical form of the first morpheme (FM) of the next eojeol (N1)
[current eojeol or morpheme] [rule schemata, referenced morpheme or tag]
! [corrected eojeol or morpheme]
Figure 2
Error correction rule format.
of the statistical morpheme tagger. However, designing the error correction rules with
knowledge engineering is tedious and error prone. Instead, we adopted Brill?s ap-
proach (Brill 1995) whereby the error correction rules are learned automatically from a
small amount of tagged corpus. Fortunately, Brill showed that one does not normally
need a large amount of tagged corpus to extract the symbolic tagging rules compared
with statistical tagging. Table 7 shows examples of carefully designed rule schemata
used to extract the error correction rules for Korean, where a rule schema designates
the context of rule applications (i.e., the morpheme position and the lexical/tag deci-
sion in a context eojeol).
The form of the rules that can be automatically learned using the schemata in
Table 7 is shown in Figure 2, where [current eojeol or morpheme] consists of the mor-
pheme (with current tag) sequence in an eojeol, and [corrected eojeol or morpheme]
consists of the morpheme (with corrected tag) sequence in the same eojeol. For exam-
ple, the rule [meog(?Chinese ink0)=MC + eun=jS ][N1FT , MC ] ! [meog(?to eat0)=DR +
eun=eCNMG ] says that the current eojeol was statistically tagged as a common noun
(MC) plus auxiliary particle (jS), but if the next eojeol?s (N1) first-position morpheme
tag (FT) is also MC, the eojeol should be tagged as a regular verb (DR) plus ad-
nominal ending (eCNMG). This statistical error is caused by the ambiguity of the
morpheme meog, which has two meanings: ?Chinese ink? (noun) and ?to eat? (verb).
Since morpheme segmentation is very difficult in Korean, many tagging errors also
arise from the morpheme segmentation errors. Our error correction rules can also cope
with these morpheme segmentation errors by correcting the errors in the whole eojeol
at once. For example, the following rule can correct morpheme segmentation errors:
[jul=MC + i ? go=jO ][P1LM , ] ! [jul ? i=DR + go=eCC ]. This rule says that the eojeol
jul-i-go is usually segmented as a common noun, jul ?string, rope?, plus the adverb
and conjunctive particle i-go, but when the morpheme eul appears before the eojeol,
it should be segmented as a regular verb, jul-i ?shrink?, plus the conjunctive ending
go. This kind of segmentation error correction can greatly enhance the tagging perfor-
mance. The rules are automatically learned by comparing the correctly tagged corpus
with the output of the statistical tagger. The training is leveraged so the error correc-
64
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 8
Performance of the statistical tagger (all in %) on
three document sets, using three progressively
degraded versions of the tagger.
Document set Version 1 Version 2 Version 3
Set 1 96.4 89.5 87.1
Set 2 96.0 92.8 89.0
Set 3 96.7 88.7 84.8
Total 96.4 90.3 87.0
tion rules are gradually learned as the statistically tagged texts are corrected by the
rules learned so far.
5. Experimental Results
5.1 Embedded Performance with Hybrid POS Tagging
For morphological analysis and POS tagging experiments, we used a 130,000-morph-
eme balanced training corpus for statistical parameter estimation and a 50,000-morph-
eme corpus for learning the a posteriori error correction rules. The training corpus was
collected from various sources such as Internet documents, encyclopedias, newspapers,
and school textbooks.
For test sets, we carefully selected three different document sets, aiming for broad
coverage. The first document set (Set 1: 25,299 morphemes, 1,338 sentences), which
was collected from the Kemong Encyclopedia,9 a hotel reservation dialogue corpus,10
and assorted Internet documents, contains about 10% unknown morphemes. The sec-
ond document set (Set 2: 15,250 morphemes, 574 sentences), which consists solely of
Internet documents from assorted domains, such as broadcasting scripts and news-
papers, contains about 8.5% unknown morphemes. The third document set (Set 3:
20,919 morphemes, 555 sentences), which comes from a standard Korean document
set called KTSET 2.011 including academic articles and electronic newspapers, con-
tains about 14% unknown morphemes (mainly technical jargon). Table 8 shows our
system?s statistical tagging performance for these three document sets, using three
progressively degraded versions of the tagging mechanism. Version 1 is a full version
using the statistical method. Version 2 is a somewhat degraded version that does not
use the system?s unknown-morpheme guessing capability but treats all the segmented
unknown morphemes as nouns (the typical method of estimation). Version 3 is an even
more degraded version that rejects all unknown morphemes as tagging failures; this
version does not even perform unknown-morpheme segmentation during morpho-
logical analysis. This experiment verifies the effectiveness of our unknown-morpheme
segmentation and guessing techniques, as shown by the sharp performance drops
between Versions 1, 2, and 3. As another experiment showed, the automatically ac-
quired a posteriori error correction rules also proved to be useful. In this experiment,
two versions of the hybrid tagger were tested on the three document sets. Version 1
was the full POSTAG system with unknown-morpheme segmentation, guessing, and
9 From the Electronics and Telecommunications Research Institute (ETRI).
10 From Sogang University, Seoul, Korea.
11 From KT (Korea Telecom).
65
Computational Linguistics Volume 28, Number 1
Table 9
Performance of the hybrid tagger (all
in %) on three document sets, using
two versions of the tagger.
Document set Version 1 Version 2
Set 1 97.2 96.4
Set 2 96.9 96.0
Set 3 97.4 96.7
Total 97.2 96.4
Table 10
Unknown-morpheme estimation performance
(all in %). Experiments were performed on
three different document sets as before. #UKM
designates the number of unknown morphemes
in each document set and their percentage.
Recall (Rec.) measures the coverage of the
estimation and precision (Pre.) demonstrates its
accuracy.
Document set #UKM Rec. Pre.
Set 1 2,531 (10.0%) 93.9 94.8
Set 2 1,303 (8.5%) 92.9 88.9
Set 3 2,889 (13.8%) 98.0 85.5
Total 6,723 (10.8%) 94.9 89.7
rule-based error correction. Version 2 did not employ a posteriori error correction rules
(the same system as Version 1 in the first experiment). Performance dropped between
Version 1 and Version 2 (see Table 9); however, the drop rates were mild due to the
performance saturation at Version 1, which means that our statistical tagger alone
already achieves state-of-the-art performance for tagging of Korean morphemes.
5.2 Unknown-Morpheme Segmentation and Guessing Performance
To see the independent performance of unknown-morpheme handling more precisely
(explained in Sections 3 and 4.2), we separated the unknown-morpheme performance
from hybrid tagging experiments. Using the same test corpus, we measured the cover-
age and correctness of our unknown-morpheme estimation techniques. Table 10 shows
the results, which were evaluated by the metrics defined as follows:
Recall =
#unknown morphemes detected
#unknown morphemes
(segmentation performance)
Precision =
#unknown morphemes correctly estimated
#unknown morphemes detected
(guessing performance)
When the morphological analyzer meets an unknown morpheme, it is important
to detect first whether it is unknown or not, because sometimes, due to incorrect
segmentation, an unknown morpheme can be incorrectly processed as a known one.
Our system reached an average recall level of 94.9%. Once the unknown morphemes
are detected, the correct POS needs to be estimated. Our system tries to guess the POS
66
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 11
Unknown-morpheme estimation performance (all in %) for each POS tag. N/A
means the morpheme with the corresponding tag does not appear in the
corpus. Recall (Rec.) measures the coverage of the estimation, and precision
(Pre.) demonstrates its accuracy.
Set 1 Set 2 Set 3 Total
POS tag Rec. Pre. Rec. Pre. Rec. Pre. Rec. Pre.
MC 96.9 95.4 94.5 91.7 93.9 72.5 95.1 86.5
MPN 80.0 86.7 87.4 95.0 100.0 100.0 89.1 93.9
MPC 54.3 73.7 72.7 37.5 N/A N/A 42.3 37.1
MPP 75.2 63.3 86.9 75.0 100.0 100.0 87.4 79.4
MPO 79.4 79.4 94.8 68.3 100.0 93.8 91.4 79.7
B 87.9 100.0 42.9 66.7 100.0 100.0 76.9 88.9
T N/A N/A 100.0 100.0 N/A N/A 100.0 100.0
S 99.8 100.0 99.0 100.0 100.0 100.0 99.6 100.0
Foreign word 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
Special symbol 100.0 100.0 N/A N/A 100.0 100.0 100.0 100.0
of every open class item including common nouns, proper nouns, pronouns, numbers,
adverbs, and others.12 The average precision of 89.7% reflects very accurate guessing
considering the range of POSs that need to be estimated. Table 11 shows the system?s
unknown-morpheme guessing performance for each POS tag.
To show the pattern dictionary?s utility, we conducted another experiment in which
we gradually reduced the morpheme dictionary size to see the smooth hybrid tagging
performance (same as in Table 9) drops. As the morpheme dictionary gets smaller,
POSTAG becomes more dependent on the morpheme pattern dictionary and also on
the unknown-morpheme estimation process. From the full dictionary (with 65,000
nouns), we randomly deleted 5,000 nouns step by step for this series of experiments.
(We deleted only nouns because noun estimation is the best arena for showing the sys-
tem?s unknown-morpheme estimation power.) Figure 3 shows the results. Even if the
POSTAG system relies heavily on unknown-morpheme estimation instead of on more
accurate dictionary lookups, the performance drop is very slow. This result explains
why POSTAG can be used on open domain materials such as Internet documents even
when only a small morpheme dictionary is available.
6. Conclusion
This paper presents a pattern-dictionary-based unknown-morpheme estimation method
for generalized and powerful unknown-morpheme segmentation and guessing for a
hybrid POS tagging system. Generalized unknown-morpheme handling is a new and
powerful idea that adopts a morpheme pattern dictionary and syllable-based lexical
probability estimation. The morpheme pattern dictionary enables the system to seg-
ment unknown morphemes in the same way as registered morphemes without any
separate rules for Korean, and thereby to handle them regardless of their numbers
or positions in an eojeol. The paper also presents an error-corrective statistical and
12 Pronouns, numbers, and adverbs may be considered as closed classes. However, in real-world corpora,
we frequently find unexpectedly coined terms in these classes since Korean word formation is affected
by very diverse sources such as foreign words, old Chinese words, archaic pure-Korean words, and so
on.
67
Computational Linguistics Volume 28, Number 1
90
92
94
96
98
100
0 2 4 6 8 10 12
?set1?
?set2?
?set3?
?total?
Figure 3
Hybrid tagging performance change (all in %), showing the utility of the pattern dictionary.
Experiments were performed on three different document sets as before. The x-axis designates
the number of deletion steps whereby the morpheme dictionary was decreased (by 5,000s)
from its full size of 65,000 nouns (Step 0) to 5,000 nouns (Step 12).
rule-based hybrid POS tagging method that exhibits many novel features such as an
experiment-based statistical model for Korean, rule-based error correction, and hier-
archically expandable tagsets. The POSTAG system was developed to test these novel
ideas, especially for agglutinative languages such as Korean. (Japanese, being similar
to Korean in linguistic characteristics, will be a good target for testing these ideas.)
Unlike previous systems, POSTAG is a hybrid tagging system; such a system has never
been tried before, but it turns out to be most suitable for agglutinative languages such
as Korean. POSTAG mainly applies a state-of-the-art HMM tagger for morphemes
but considers multiple observations in the Viterbi score calculation. Because of the
complexity of the morpheme sequence in a Korean eojeol, a morpheme-based HMM?s
tagging accuracy is relatively low for Korean, compared with its accuracy for English.
POSTAG compensates extremely well for the limitations of HMMs by rule-based error
correction. The error correction rules are automatically learned to selectively correct
HMM tagging errors. Similar hybrid methods have been tried for English, but they
integrate HMM tagging and rule-based tagging at the same level (Tapanainen and
Voutilainen 1994). POSTAG integrates morphological analysis with the generalized
68
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
unknown-morpheme segmentation so that unknown morphemes can be processed in
the same manner as registered morphemes during tagging. POSTAG also employs
hierarchical tagsets that are flexible enough to expand/shrink according to the given
application. The hierarchical tagset is a novel idea. Most tagging systems for Korean
have applied flat, fixed tagsets and have suffered from using varying tagsets in various
applications. However, POSTAG?s tagsets, based on the over 100 finely differentiated
POS symbols for Korean are hierarchically organized and are flexibly reorganizable ac-
cording to the application at hand. The hierarchical tagsets can be mapped to any other
existing tagset as long as they are fairly well classified and therefore can encourage cor-
pus sharing in the Korean-tagging community. POSTAG is constantly being improved
through expansion of its morpheme dictionary, pattern dictionary, and tagged cor-
pus for statistical and rule-based learning. Since the generalized unknown-morpheme
handling is integrated into the system, POSTAG proves to be a good tagger for open
domain applications such as Internet indexing, filtering, and summarization, and we
are now developing a Web indexer using POSTAG technology.
Acknowledgments
This project was partly supported by
KOSEF (teukjeongkicho #970-1020-301-3,
1997.9-2000.8) and a Ministry of Education
BK21 program awarded to the Electrical
and Computer Engineering Division of
POSTECH. We would like to thank
JunHyeok Shim for coding the
unknown-morpheme estimation
experiments. An earlier version of this
paper was presented at the 6th Workshop
on Very Large Corpora in Montreal, 15?16
August 1998.
References
Brill, E. 1995. Transformation-based
error-driven learning and natural
language processing: A case study in
part-of-speech tagging. Computational
Linguistics, 21:543?565.
Cutting, D., J. Kupiec, J. Pedersen, and P.
Sibun. 1992. A practical part-of-speech
tagger. In Proceedings of the 3rd Conference
on Applied Natural Language Processing,
pages 133?140.
Forney, G. 1973. The Viterbi algorithm.
Proceedings of the IEEE, 61:268?278.
Im, H. S., J. D. Kim, and H. C. Im. 1996.
Transformation rule-based tagging
considering Korean characteristics. In
Proceedings of the Spring Conference of the AI
SIG Meeting of the Korean Information Science
Society, pages 3?10. (Written in Korean.)
Kang, S. S. 1993. Korean Morphological
Analysis Using Syllable Information and
Multiple-Word Units. Ph.D. thesis,
Department of Computer Engineering,
Seoul National University. (Written in
Korean.)
Kim, J. D., H. S. Im, and H. C. Im. 1996.
Morpheme-based Korean part-of-speech
tagging model considering eojeol-unit
contexts. In Proceedings of the Spring
Conference of the Korean Cognitive Science
Society, pages 97?106. (Written in Korean.)
Kim, J. H., C. S. Lim, and J. Seo. 1995. An
efficient Korean part-of-speech tagging
using a hidden Markov model. Journal of
the Korean Information Science Society,
22:136?146. (Written in Korean.)
Kupiec, J. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language, 6:225?242.
Lee, U. J., K. S. Choi, and G. C. Kim. 1993.
Korean text-tagging system. In Proceedings
of the Spring Conference of the Korean
Information Science Society, pages 805?808.
(Written in Korean.)
Merialdo, B. 1994. Tagging English text with
a probabilistic model. Computational
Linguistics, 20:155?171.
Mikheev, A. 1996. Unsupervised learning of
word-category guessing rules. In
Proceedings of the 34th Annual Meeting of the
Association for the Computational Linguistics,
pages 327?334.
Nagata, M. 1994. A stochastic Japanese
morphological analyzer using a
forward-DP backward-A N-best search
algorithm. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 201?207.
Oflazer, K. and G. Tu?r. 1996. Combining
hand-crafted rules and unsupervised
learning in constraint-based
morphological disambiguation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 69?81.
69
Computational Linguistics Volume 28, Number 1
Sproat, R. 1992. Morphology and Computation.
MIT Press, Cambridge, MA.
Tapanainen, P. and A. Voutilainen. 1994.
Tagging accurately?don?t guess if you
know. In Proceedings of the Conference on
Applied Natural Language Processing,
pages 149?156.
Voutilainen, A. 1995. A syntax-based
part-of-speech analyzer. In Proceedings of
the 7th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 157?164.
Weischedel, R., M. Meteer, R. Schwartz, L.
Rawshaw, and J. Ralmucci. 1993. Coping
with ambiguity and unknown words
through probabilistic models.
Computational Linguistics, 19:359?382.
70
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 69?72,
New York, June 2006. c?2006 Association for Computational Linguistics
MMR-based Active Machine Learning  
for Bio Named Entity Recognition 
Seokhwan Kim1 Yu Song2 Kyungduk Kim1 Jeong-Won Cha3 Gary Geunbae Lee1
1 Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea 
2 AIA Information Technology Co., Ltd. Beijing, China 
3 Dept. of Computer Science, Changwon National University, Changwon, Korea 
megaup@postech.ac.kr, Song-Y.Song@AIG.com, getta@postech.ac.kr
jcha@changwon.ac.kr, gblee@postech.ac.kr
Abstract 
This paper presents a new active learning 
paradigm which considers not only the 
uncertainty of the classifier but also the 
diversity of the corpus. The two measures 
for uncertainty and diversity were com-
bined using the MMR (Maximal Marginal 
Relevance) method to give the sampling 
scores in our active learning strategy. We 
incorporated MMR-based active machine-
learning idea into the biomedical named-
entity recognition system. Our experimen-
tal results indicated that our strategies for 
active-learning based sample selection 
could significantly reduce the human ef-
fort. 
1 Introduction 
Named-entity recognition is one of the most ele-
mentary and core problems in biomedical text min-
ing. To achieve good recognition performance, we 
use a supervised machine-learning based approach 
which is a standard in the named-entity recognition 
task. The obstacle of supervised machine-learning 
methods is the lack of the annotated training data 
which is essential for achieving good performance. 
Building a training corpus manually is time con-
suming, labor intensive, and expensive. Creating 
training corpora for the biomedical domain is par-
ticularly expensive as it requires domain specific 
expert knowledge. 
One way to solve this problem is through active 
learning method to select the most informative 
samples for training. Active selection of the train-
ing examples can significantly reduce the neces-
sary number of labeled training examples without 
degrading the performance. 
Existing work for active learning explores two 
approaches: certainty or uncertainty-based methods 
(Lewis and Gale 1994; Scheffer and Wrobel 2001; 
Thompson et al 1999) and committee-based 
methods (Cohn et al 1994; Dagan and Engelson 
1995; Freund et al 1997; Liere and Tadepalli 
1997). Uncertainty-based systems begin with an 
initial classifier and the systems assign some un-
certainty scores to the un-annotated examples. The 
k examples with the highest scores will be anno-
tated by human experts and the classifier will be 
retrained. In the committee-based systems, diverse 
committees of classifiers were generated. Each 
committee member will examine the un-annotated 
examples. The degree of disagreement among the 
committee members will be evaluated and the ex-
amples with the highest disagreement will be se-
lected for manual annotation. 
Our efforts are different from the previous ac-
tive learning approaches and are devoted to two 
aspects: we propose an entropy-based measure to 
quantify the uncertainty that the current classifier 
holds. The most uncertain samples are selected for 
human annotation. However, we also assume that 
the selected training samples should give the dif-
ferent aspects of learning features to the classifica-
tion system. So, we try to catch the most 
representative sentences in each sampling. The 
divergence measures of the two sentences are for 
the novelty of the features and their representative 
levels, and are described by the minimum similar-
ity among the examples. The two measures for un-
certainty and diversity will be combined using the 
MMR (Maximal Marginal Relevance) method 
(Carbonell and Goldstein 1998) to give the sam-
pling scores in our active learning strategy. 
69
We incorporate MMR-based active machine-
learning idea into the POSBIOTM/NER (Song et 
al. 2005) system which is a trainable biomedical 
named-entity recognition system using the Condi-
tional Random Fields (Lafferty et al 2001) ma-
chine learning technique to automatically identify 
different sets of biological entities in the text. 
2 MMR-based Active Learning for Bio-
medical Named-entity Recognition 
2.1 Active Learning 
We integrate active learning methods into the 
POSBIOTM/NER (Song et al 2005) system by the 
following procedure: Given an active learning 
scoring strategy S and a threshold value th, at each 
iteration t, the learner uses training corpus TMt   to 
train the NER module Mt. Each time a user wants 
to annotate a set of un-labeled sentences U, the 
system first tags the sentences using the current 
NER module Mt. At the same time, each tagged 
sentence is assigned with a score according to our 
scoring strategy S. Sentences will be marked if its 
score is larger than the threshold value th. The tag 
result is presented to the user, and those marked 
ones are rectified by the user and added to the 
training corpus. Once the training data accumulates 
to a certain amount, the NER module Mt will be 
retrained. 
2.2 Uncertainty-based Sample Selection 
We evaluate the uncertainty degree that the current 
NER module holds for a given sentence in terms of 
the entropy of the sentence. Given an input se-
quence o, the state sequence set S is a finite set. 
And  is the probability distribu-
tion over S. By using the equation for CRF 
(Lafferty et al 2001) module, we can calculate the 
probability of any possible state sequence s given 
an input sequence o. Then the entropy of  
is defined to be: 
Sso|s ??   ),(p
)( o|s?p
? ???=
s
o|so|s )]([log)( 2 PPH  
The number of possible state sequences grows 
exponentially as the sentence length increases. In 
order to measure the uncertainty by entropy, it is 
inconvenient and unnecessary to compute the 
probability of all the possible state sequences. In-
stead we implement N-best Viterbi search to find 
the N state sequences with the highest probabilities. 
The entropy H(N) is defined as the entropy of the 
distribution of the N-best state sequences: 
? ??= = ?
?
= ?
?
???
?
???
?
?=
N
i
N
i i
i
N
i i
i
P
P
P
P
NH
1
1
2
1
)(
)(
log
)(
)(
)(
o|s
o|s
o|s
o|s .  (1) 
The range of the entropy H(N) is [0, 
N
1
log 2? ] which varies according to different N. 
We could use the equation (2) to normalize the 
H(N) to [0, 1]. 
N
NH
NH
1
log
)(
)(
2?
=? .  (2) 
2.3 Diversity-based Sample Selection 
We measure the sentence structure similarity to 
represent the diversity and catch the most represen-
tative ones in order to give more diverse features to 
the machine learning-based classification systems. 
We propose a three-level hierarchy to represent 
the structure of a sentence. The first level is NP 
chunk, the second level is Part-Of-Speech tag, and 
the third level is the word itself. Each word is rep-
resented using this hierarchy structure. For exam-
ple in the sentence "I am a boy", the word "boy" is 
represented as w
r
=[NP, NN, boy]. The similarity 
score of two words is defined as: 
)()(
),(2
)(
21
21
21 wDepthwDepth
wwDepth
wwsim rr
rrrr
+
?=?  
Where ),( 21 wwDepth
rr
 is defined from the top 
level as the number of levels that the two words are 
in common. Under our three-level hierarchy 
scheme above, each word representation has depth 
of 3. 
The structure of a sentence S is represented as 
the word representation vectors ],  ,,[ 21 Nwww
rKrr . 
We measure the similarity of two sentences by the 
standard cosine-similarity measure. The similarity 
score of two sentences is defined as: 
2211
21
21 ),(
SSSS
SS
SSsimilarity rrrr
rrrr
??
?= , 
?? ?=?
i j
ji wwsimSS )( 2121
rrrr
. 
70
2.4 MMR Combination for Sample Selection 
We would like to score the sample sentences with 
respect to both the uncertainty and the diversity. 
The following MMR (Maximal Marginal Rele-
vance) (Carbonell and Goldstein 1998) formula is 
used to calculate the active learning score: 
),(Similaritymax                   
)1(),(yUncertaint)(
jiTs
i
def
i
ss
Mssscore
Mj??
???= ??   (3) 
where si is the sentence to be selected, Uncertainty 
is the entropy of si given current NER module M, 
and Similarity indicates the divergence degree be-
tween the si and the sentence sj in the training cor-
pus TM of M. The combination rule could be 
interpreted as assigning a higher score to a sen-
tence of which the NER module is uncertain and 
whose configuration differs from the sentences in 
the existing training corpus. The value of parame-
ter ?  coordinates those two different aspects of 
the desirable sample sentences. 
After initializing a NER module M and an ap-
propriate value of the parameter? , we can assign 
each candidate sentence a score under the control 
of the uncertainty and the diversity. 
3 Experiment and Discussion 
3.1 Experiment Setup 
We conducted our active learning experiments us-
ing pool-based sample selection (Lewis and Gale 
1994). The pool-based sample selection, in which 
the learner chooses the best instances for labeling 
from a given pool of unlabelled examples, is the 
most practical approach for problems in which 
unlabelled data is relatively easily available. 
For our empirical evaluation of the active learn-
ing methods, we used the training and test data 
released by JNLPBA (Kim et al 2004). The train-
ing corpus contains 2000 MEDLINE abstracts, and 
the test data contains 404 abstracts from the 
GENIA corpus. 100 abstracts were used to train 
our initial NER module. The remaining training 
data were taken as the pool. Each time, we chose k 
examples from the given pool to train the new 
NER module and the number k varied from 1000 
to 17000 with a step size 1000. 
We test 4 different active learning methods: Ran-
dom selection, Entropy-based uncertainty selection, 
Entropy combined with Diversity, and Normalized 
Entropy (equation (2)) combined with Diversity. 
When we compute the active learning score using 
the entropy based method and the combining 
methods we set the values of parameter N (from 
equation (1)) to 3 and ?  (from equation (3)) to 0.8 
empirically. 
 
Fig1. Comparison of active learning strategies with the ran-
l in the y-axis shows the 
per
bin  
ies consistently outperform 
the
dom selection 
3.2 Results and Analyses 
The initial NER module gets an F-score of 52.54, 
while the F-score performance of the NER module 
using the whole training data set is 67.19. We plot-
ted the learning curves for the different sample 
selection strategies. The interval in the x-axis be-
tween the curves shows the number of examples 
selected and the interva
formance improved. 
We compared the entropy, entropy combined 
with sentence diversity, normalized entropy com-
ed with sentence diversity and random selection.
The curves in Figure 1 show the relative per-
formance. The F-score increases along with the 
number of selected examples and receives the best 
performance when all the examples in the pool are 
selected. The results suggest that all three kinds of 
active learning strateg
 random selection.  
The entropy-based example selection has im-
proved performance compared with the random 
selection. The entropy (N=3) curve approaches to 
the random selection around 13000 sentences se-
lected, which is reasonable since all the methods 
choose the examples from the same given pool. As 
71
the number of selected sentences approaches the 
pool size, the performance difference among the 
different methods gets small. The best performance 
of the entropy strategy is 67.31 when 17000 exam-
ple
the
 normalized combined strategy 
behaves the worst. 
4 Conclusion 
ction could significantly reduce 
the human effort. 
by Minis-
try of Commerce, Industry and Energy. 
s are selected. 
Comparing with the entropy curve, the com-
bined strategy curve shows an interesting charac-
teristic. Up to 4000 sentences, the entropy strategy 
and the combined strategy perform similarly. After 
the 11000 sentence point, the combined strategy 
surpasses the entropy strategy. It accords with our 
belief that the diversity increases the classifier's 
performance when the large amount of samples is 
selected.  The normalized combined strategy dif-
fers from the combined strategy. It exceeds the 
other strategies from the beginning and maintains 
 best performance up until 12000 sentence point. 
   The entropy strategy reaches 67.00 in F-score 
when 11000 sentences are selected. The combined 
strategy receives 67.17 in F-score while 13000 sen-
tences are selected, while the end performance is 
67.19 using the whole training data. The combined 
strategy reduces 24.64 % of training examples 
compared with the random selection. The normal-
ized combined strategy achieves 67.17 in F-score 
when 11000 sentences are selected, so 35.43% of 
the training examples do not need to be labeled to 
achieve almost the same performance as the end 
performance. The normalized combined strategy's 
performance becomes similar to the random selec-
tion strategy at around 13000 sentences, and after 
14000 sentences the
 
We incorporate active learning into the biomedical 
named-entity recognition system to enhance the 
system's performance with only small amount of 
training data. We presented the entropy-based un-
certainty sample selection and combined selection 
strategies using the corpus diversity. Experiments 
indicate that our strategies for active-learning 
based sample sele
Acknowledgement  
This research was supported as a Brain Neuroin-
formatics Research Program sponsored 
References 
Carbonell J., & Goldstein J. (1998). The Use of MMR, 
Diversity-Based Reranking for Reordering Docu-
ments and Producing Summaries. In Proceedings of 
the 21st Annual International ACM-SIGIR Confer-
ence on Research and Development in Information 
Retrieval, pages 335-336. 
Cohn, D. A., Atlas, L., & Ladner, R. E. (1994). Improv-
ing generalization with active learning, Machine 
Learning, 15(2), 201-221. 
Dagan, I., & Engelson S. (1995). Committee-based 
sampling for training probabilistic classifiers. In Pro-
ceedings of the Twelfth International Conference on 
Machine Learning, pages 150-157, San Francisco, 
CA, Morgan Kaufman. 
Freund Y., Seung H.S., Shamir E., & Tishby N. (1997). 
Selective sampling using the query by committee al-
gorithm, Machine Learning, 28, 133-168. 
Kim JD., Ohta T., Tsuruoka Y., & Tateisi Y. (2004). 
Introduction to the Bio-Entity Recognition Task at 
JNLPBA, Proceedings of the International Workshop 
on Natural Language Processing in Biomedicine and 
its Application (JNLPBA). 
Lafferty, J., McCallum, A., & Pereira, F. (2001). Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the 
18th International Conf. on Machine Learning, pages 
282-289, Williamstown, MA, Morgan Kaufmann. 
Lewis D., & Gale W. (1994). A Sequential Algorithm 
for Training Text Classifiers, In: Proceedings of the 
Seventeenth Annual International ACM-SIGIR Con-
ference on Research and Development in Information 
Retrieval. pp. 3-12, Springer-Verlag. 
Liere, R., & Tadepalli, P. (1997). Active learning with 
committees for text categorization, In proceedings of 
the Fourteenth National Conference on Artificial In-
telligence, pp. 591-596 Providence, RI. 
Scheffer T., & Wrobel S. (2001). Active learning of 
partially hidden markov models. In Proceedings of 
the ECML/PKDD Workshop on Instance Selection. 
Song Y., Kim E., Lee G.G., & Yi B-k. (2005). 
POSBIOTM-NER: a trainable biomedical named-
entity recognition system. Bioinformatics, 21 (11): 
2794-2796. 
Thompson C.A., Califf M.E., & Mooney R.J. (1999). 
Active Learning for Natural Language Parsing and 
Information Extraction, In Proceedings of the Six-
teenth International Machine Learning Conference, 
pp.406-414, Bled, Slovenia. 
72
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 61?64,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Joint Statistical Model for Simultaneous Word Spacing and 
 Spelling Error Correction for Korean 
Hyungjong Noh* Jeong-Won Cha** Gary Geunbae Lee* 
*Department of Computer Science and Engineering 
Pohang University of Science & Technology (POSTECH) 
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea 
 
** Changwon National University 
Department of Computer information & Communication 
9 Sarim-dong, Changwon Gyeongnam, Korea 641-773  
 
nohhj@postech.ac.kr jcha@changwon.ac.kr gblee@postech.ac.kr 
Abstract 
This paper presents noisy-channel based 
Korean preprocessor system, which cor-
rects word spacing and typographical errors. 
The proposed algorithm corrects both er-
rors simultaneously. Using Eojeol transi-
tion pattern dictionary and statistical data 
such as Eumjeol n-gram and Jaso transition 
probabilities, the algorithm minimizes the 
usage of huge word dictionaries. 
1 Introduction 
With increasing usages of messenger and SMS, we 
need an efficient text normalizer that processes 
colloquial style sentences. As in the case of general 
literary sentences, correcting word spacing error 
and spelling error is the very essential problem 
with colloquial style sentences. 
In order to correct word spacing errors, many 
algorithms were used, which can be divided into 
statistical algorithms and rule-based algorithms. 
Statistical algorithms generally use character n-
gram (Eojeol 1  or Eumjeol 2  n-gram in Korean) 
(Kang and Woo, 2001; Kwon, 2002) or noisy-
channel model (Gao et. al., 2003). Rule-based al-
gorithms are mostly heuristic algorithms that re-
flect linguistic knowledge (Yang et al, 2005) to 
solve word spacing problem. Word spacing prob-
lem is treated especially in Japanese or Chinese, 
                                                 
1 Eojeol is a Korean spacing unit which consists of one or 
more Eumjeols (morphemes). 
2 Eumjeol is a Korean syllable. 
which does not use word boundary, or Korean, 
which is normally segmented into Eojeols, not into 
words or morphemes. 
The previous algorithms for spelling error cor-
rection basically use a word dictionary. Each word 
in a sentence is compared to word dictionary en-
tries, and if the word is not in the dictionary, then 
the system assumes that the word has spelling er-
rors. Then corrected candidate words are suggested 
by the system from the word dictionary, according 
to some metric to measure the similarity between 
the target word and its candidate word, such as 
edit-distance (Kashyap and Oommen, 1984; Mays 
et al, 1991). 
But these previous algorithms have a critical li-
mitation: They all corrected word spacing errors 
and spelling errors separately. Word spacing algo-
rithms define the problem as a task for determining 
whether to insert the delimiter between characters 
or not. Since the determination is made according 
to the characters, the algorithms cannot work if the 
characters have spelling errors. Likewise, algo-
rithms for solving spelling error problem cannot 
work well with word spacing errors. 
To cope with the limitation, there is an algo-
rithm proposed for Japanese (Nagata, 1996). Japa-
nese sentence cannot be divided into words, but 
into chunks (bunsetsu in Japanese), like Eojeol in 
Korean. The proposed system is for sentences rec-
ognized by OCR, and it uses character transition 
probabilities and POS (part of speech) tag n-gram. 
However it needs a word dictionary and takes long 
time for searching many character combinations. 
61
We propose a new algorithm which can correct 
both word spacing error and spelling error simulta-
neously for Korean. This algorithm is based on 
noisy-channel model, which uses Jaso3  transition 
probabilities and Eojeol transition probabilities to 
create spelling correction candidates. Candidates 
are increased in number by inserting the blank cha-
racters on the created candidates, which cover the 
spacing error correction candidates. We find the 
best candidate sentence from the networks of Ja-
so/Eojeol candidates. This method decreases the 
size of Eojeol transition pattern dictionary and cor-
rects the patterns which are not in the dictionary. 
The remainder of this paper is as follows: Sec-
tion 2 describes why we use Jaso transition prob-
ability for Korean. Section 3 describes the pro-
posed model in detail. Section 4 provides the ex-
periment results and analyses. Finally, section 5 
presents our conclusion. 
2 Spelling Error Correction with Jaso 
Transition4 Probabilities 
We can use Eumjeol transition probabilities or Jaso 
transition probabilities for spelling error correction 
for Korean. We choose Jaso transition probabilities 
because there are several advantages. Since an 
Eumjeol is a combination of 3 Jasos, the number of 
all possible Eumjeols is much larger than that of all 
possible Jasos. In other words, Jaso-based 
language model is smaller than Eumjeol-based 
language model. Various errors in Eumjeol (even if 
they do not appear as an Eumjeol pattern in a 
training corpus) can be corrected by correction in 
Jaso unit. Also, Jaso transition probabilities can be 
extracted from relatively small corpus. This merit 
is very important since we do not normally have 
such a huge corpus which is very hard to collect, 
since we have to pair the spelling errors with 
corresponding corrections.  
We obtain probabilities differently for each 
case: single Jaso transition case, two Jaso?s transi-
tion case, and more than two Jasos transition case. 
In single Jaso transition case, the spelling errors 
are corrected by only one Jaso transition (e.g. 
??????? / ???). The case of correcting 
by deleting Jaso is also one of the single Jaso tran-
                                                 
3 Jaso is a Korean character. 
4 ?Transition? means the correct character is changed to other 
character due to some causes, such as typographical errors. 
sition case (??????? / ??X5). The Jaso 
transition probabilities are calculated by counting 
the transition frequencies in a training corpus. 
In two Jaso?s transition case, the spelling errors 
are corrected by adjacent two Jasos transition 
(????? / ???X?). In this case, we treat 
two Jaso?s as one transition unit. The transition 
probability calculation is the same as above. 
In more than two Jaso?s transition case, the spel-
ling errors cannot be corrected only by Jaso transi-
tion (????). In this case, we treat the whole 
Eojeols as one transition unit, and build an Eojeol 
transition pattern dictionary for these special cases. 
3 A Joint Statistical Model for Word 
Spacing and Spelling Error Correction 
3.1 Problem Definition 
Given a sentence T  which includes both word 
spacing errors and spelling errors, we create 
correction candidates C  from T , and find the best 
candidate that has the highest transition 
probability from C . 
'C
).|(maxarg' TCPC C=               (1) 
3.2 Model Description 
A given sentence T  and candidates  consist of 
Eumjeol  and the blank character . 
C
is ib
nnbsbsbsbsT ...332211= . 
....332211 nnbsbsbsbsC =                (2) 
(n is the number of Eumjeols) 
Eumjeol  consists of 3 Jasos, Choseong (on-
set), Jungseong (nucleus), and Jongseong (coda). 
The empty Jaso is defined as ?X?.  is ?
is
ib B ? when 
the blank exists, and ?? ? when the blank does not 
exist. 
321 iiii jjjs = .                        (3) 
( : Choseong, : Jungseong, : Jongseong) 1ij 2ij 3ij
Now we apply Bayes? Rule for : 'C
)|(maxarg' TCPC C=  
).()|(maxarg
)(/)()|(maxarg
CPCTP
TPCPCTP
C
C
=
=
             (4) 
                                                 
5 ?X? indicates that there is no Jaso in that position. 
62
)(CP  can be obtained using trigrams of Eum-
jeols (with the blank character) that  includes. C
?
=
??=
n
i
iii cccPCP
1
21 )|()( ,  or b .    (5) sc =
And  can be written as multiplication 
of each Jaso transition probability and the blank 
character transition probability. 
)|( CTP
)|()|(
1
'?
=
=
n
i
ii ssPCTP  
.)]|()|()|()|([
1
''
33
'
22
'
11?
=
=
n
i
iiiiiiii bbPjjPjjPjjP  
(6) 
We use logarithm of  in implementa-
tion. Figure 1 shows how the system creates the 
Jaso candidates network. 
)|( TCP
 
Figure 1: An example6 of Jaso candidate network. 
 
In Figure 1, the topmost line is the sequence of 
Jasos of the input sentence. Each Eumjeol in the 
sentence is decomposed into 3 Jasos as above, and 
each Jaso has its own correction candidates. For 
example, Jaso ??? at 4th column has its candidates 
???, ??? and ?X?. And two jaso?s ?X?? at 13th 
and 14th column has its candidates ????, 
????, ????, ????, and ????. The undermost 
gray square is an Eojeol (which is decomposed into 
Jasos) candidate ???X?????X? created 
from ???X??X?. Each jaso candidate has its 
own transition probability, 7)|(log 'ikik jjP , that is 
used for calculating . )|( TCP
In order to calculate , we need Eumjeol-
based candidate network. Hence, we convert the 
above Jaso candidate network into Eumjeol/Eojeol 
candidate network. Figure 2 shows part of the final 
)(CP
                                                 
6 The example sentence is ??????????????. 
7 In real implementation, we used ?a*logP(jik|j?ik) + b? by 
determining constants a and b with parameter optimization  
(a = 1.0, b = 3.0). 
network briefly. At this time, the blank characters 
? B ? and ? ? ? are inserted into each Eum-
jeol/Eojeol candidates. To find the best path from 
the candidates, we conduct viterbi-search from 
leftmost node corresponding to the beginning of 
the sentence. When Eumjeol/Eojeol candidates are 
selected, the algorithm prunes the candidates ac-
cording to the accumulated probabilities, doing 
beam search. Once the best path is found, the sen-
tence corrected by both spacing and spelling errors 
is extracted by backtracking the path. In Figure 2, 
thick squares represent the nodes selected by the 
best path.  
 
Figure 2: A final Eumjeol/Eojeol candidate network8
4  Experiments and Analyses 
4.1  Corpus Information 
 Table 1: Corpus information 
 
Table 1 shows the information of corpus which is 
used for experiments. All corpora are obtained 
from Korean web chatting site log. Each corpus 
has pair of sentences, sentences containing errors 
and sentences with those errors corrected. Jaso 
transition patterns and Eojeol transition patterns 
are extracted from training corpus. Also, Eumjeol 
n-grams are also obtained as a language model. 
                                                 
8 The final corrected sentence is ??? ??? ??? 
??? ???. 
 Training Test 
Sentences 60076 6006 
Eojeols 302397 30376 
Error Sentences (%) 15335  (25.53) 
1512 
 (25.17) 
Error Eojeols (%) 31297 (10.35) 
3111 
(10.24) 
63
4.2  Experiment Results and Analyses 
 We used two separate Eumjeol n-grams as lan-
guage models for experiments. N-gram A is ob-
tained from only training corpus and n-gram B is 
obtained from all training and test corpora. All ac-
curacies are measured based on Eojeol unit. 
Table 2 shows the results of word spacing error 
correction only for the test corpus. 
 Table 2: The word spacing error correction results 
 
The results of both word spacing error and spell-
ing error correction are shown in Table 3. Error 
containing test corpus (the blank characters are all 
deleted) was applied to this evaluation. 
 Table 3: The joint model results 
 
Table 4 shows the results of the same experi-
ment, without deleting the blank characters in the 
test corpus. The experiment shows that our joint 
model has a flexibility of utilizing already existing 
blanks (spacing) in the input sentence. 
 Table 4: The joint model results without deleting the 
exist spaces 
  
As shown above, the performance is dependent 
of the language model (n-gram) performance. Jaso 
transition probabilities can be obtained easily from 
small corpus because the number of Jaso is very 
small, under 100, in contrast with Eumjeol. 
 Using the existing blank information is also an 
important factor. If test sentences have no or few 
blank characters, then we simply use joint algo-
rithm to correct both errors. But when the test sen-
tences already have some blank characters, we can 
use the information since some of the spacing can 
be given by the user. By keeping the blank charac-
ters, we can get better accuracy because blank in-
sertion errors are generally fewer than the blank 
deletion errors in the corpus. 
5 Conclusions 
 We proposed a joint text preprocessing model 
that can correct both word spacing and spelling 
errors simultaneously for Korean. To our best 
knowledge, this is the first model which can handle 
inter-related errors between spacing and spelling in 
Korean. The usage and size of the word dictionar-
ies are decreased by using Jaso statistical prob-
abilities effectively. 
6 Acknowledgement 
This work was supported in part by MIC & IITA 
through IT Leading R&D Support Project. 
References 
 Jianfeng Gao, Mu Li and Chang-Ning Huang. 2003. 
Improved Source-Channel Models for Chinese Word 
Segmentation. Proceedings of the 41st Annual Meet-
ing of the ACL, pp. 272-279 
Seung-Shik Kang and Chong-Woo Woo. 2001. Auto-
matic Segmentation of Words Using Syllable Bigram 
Statistics. Proceedings of 6th Natural Language Proc-
essing Pacific Rim Symposium, pp. 729-732 
R. L Kashyap, B. J. Oommen. 1984. Spelling Correc-
tion Using Probabilistic Methods. Pattern Recogni-
tion Letters, pp. 147-154 
Oh-Wook Kwon. 2002. Korean Word Segmentation and 
Compound-noun Decomposition Using Markov 
Chain and Syllable N-gram. The Journal of the 
Acoustical Society of Korea, pp. 274-283.  
Mu Li, Muhua Zhu, Yang Zhang and Ming Zhou. 2006. 
Exploring Distributional Similarity Based Models for 
Query Spelling Correction. Proceedings of the 21st 
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pp. 1025-
1032 
Eric Mays, Fred J. Damerau and Robert L. Mercer. 
1991. Context Based Spelling Correction. IP&M, pp. 
517-522.  
Masaaki Nagata. 1996. Context-Based Spelling Correc-
tion for Japanese OCR. Proceedings of the 16th con-
ference on Computational Linguistics, pp. 806-811 
Christoper C. Yang and K. W. Li. 2005. A Heuristic 
Method Based on a Statistical Approach for Chinese 
Text Segmentation. Journal of the American Society 
for Information Science and Technology, pp. 1438-
1447. 
 n-gram A n-gram B 
Accuracy 91.03% 96.00% 
System n-gram A n-gram B
Basic joint model 88.34% 93.83%
System n-gram A n-gram B
Baseline 89.35% 89.35%
Basic joint model with keep-
ing the blank characters 90.35% 95.25%
64
