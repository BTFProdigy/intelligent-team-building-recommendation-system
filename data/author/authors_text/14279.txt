Proceedings of the NAACL HLT 2010: Demonstration Session, pages 41?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
SIMPLIFICA: a tool for authoring simplified texts in  
Brazilian Portuguese guided by readability assessments 
 
 
Carolina  Scarton, Matheus de Oliveira,  Arnaldo Candido Jr.,  
Caroline Gasperin and Sandra Maria Alu?sio  
Department of Computer Sciences, University of S?o Paulo 
Av. Trabalhador S?o-Carlense, 400. 13560-970 - S?o Carlos/SP, Brazil 
{carolina@grad,matheusol@grad,arnaldoc@,cgasperin@,sandra@}icmc.usp.br 
 
  
 
 
Abstract 
SIMPLIFICA is an authoring tool for produc-
ing simplified texts in Portuguese. It provides 
functionalities for lexical and syntactic simpli-
fication and for readability assessment. This 
tool is the first of its kind for Portuguese; it 
brings innovative aspects for simplification 
tools in general, since the authoring process is 
guided by readability assessment based on the 
levels of literacy of the Brazilian population. 
1 Introduction 
In order to promote digital inclusion and accessi-
bility for people with low levels of literacy, partic-
ularly access to documents available on the web, it 
is important to provide textual information in a 
simple and easy way. Indeed, the Web Content 
Accessibility Guidelines (WCAG) 2.01 establishes 
a set of guidelines that discuss accessibility issues 
and provide accessibility design solutions. WCAG 
requirements address not only structure and tech-
nological aspects, but also how the content should 
be made available to users. However, Web devel-
opers are not always responsible for content prepa-
ration and authoring in a Website. Moreover, in the 
context of Web 2.0 it becomes extremely difficult 
to develop completely WCAG conformant Web-
sites, since users without any prior knowledge 
about the guidelines directly participate on the con-
tent authoring process of Web applications.  
                                                        
1 http://www.w3.org/TR/WCAG20/ 
In Brazil, since 2001, the INAF index (National 
Indicator of Functional Literacy) has been com-
puted annually to measure the levels of literacy of 
the Brazilian population. The 2009 report pre-
sented a still worrying scenario: 7% of the individ-
uals were classified as illiterate; 21% as literate at 
the rudimentary level; 47% as literate at the basic 
level; and only 25% as literate at the advanced lev-
el (INAF, 2009). These literacy levels are defined 
as: (1) Illiterate: individuals who cannot perform 
simple tasks such as reading words and phrases; 
(2) Rudimentary: individuals who can find expli-
cit information in short and familiar texts (such as 
an advertisement or a short letter); (3) Basic: indi-
viduals who can read and understand texts of aver-
age length, and find information even when it is 
necessary to make some inference; and (4) Ad-
vanced/Fully: individuals who can read longer 
texts, relating their parts, comparing and interpret-
ing information, distinguish fact from opinion, 
make inferences and synthesize. 
We present in this paper the current version of 
an authoring tool named SIMPLIFICA. It helps 
authors to create simple texts targeted at poor lite-
rate readers. It extends the previous version pre-
sented in Candido et al (2009) with two new mod-
ules: lexical simplification and the assessment of 
the level of complexity of the input texts. The 
study is part of the PorSimples project2 (Simplifi-
cation of Portuguese Text for Digital Inclusion and 
Accessibility) (Aluisio et al, 2008).  
This paper is organized as follows. In Section 2 
                                                        
2 http://caravelas.icmc.usp.br/wiki/index.php/Principal 
41
we describe SIMPLIFICA and the underlying 
technology for lexical and syntactic simplification, 
and for readability assessment. In Section 3 we 
summarize the interaction steps that we propose to 
show in the demonstration session targeting texts 
for low-literate readers of Portuguese. Section 4 
presents final remarks with emphasis on why de-
monstrating this system is relevant.  
2 SIMPLIFICA authoring tool  
SIMLIFICA is a web-based WYSIWYG editor, 
based on TinyMCE web editor3. The user inputs a 
text in the editor and customizes the simplification 
settings, where he/she can choose: (i) strong sim-
plification, where all the complex syntactic phe-
nomena (see details in Section 2.2) are treated for 
each sentence, or customized simplification, where 
the user chooses one or more syntactic simplifica-
tion phenomena to be treated for each sentence, 
and (ii) one or more thesauri to be used in the syn-
tactic and lexical simplification processes. Then 
the user activates the readability assessment mod-
ule to predict the complexity level of a text. This 
module maps the text to one of the three levels of 
literacy defined by INAF: rudimentary, basic or 
advanced. According to the resulting readability 
level the user can trigger the lexical and/or syntac-
tic simplifications modules, revise the automatic 
simplification and restart the cycle by checking the 
readability level of the current version of the text.  
Figure 1 summarizes how the three modules are 
integrated and below we describe in more detail 
the SIMPLIFICA modules. 
 
Figure 1. Steps of the authoring process. 
 
                                                        
3 http://tinymce.moxiecode.com/ 
2.1 Lexical Simplification  
Basically, the first part of the lexical simplification 
process consists of tokenizing the original text and 
marking the words that are considered complex. In 
order to judge a word as complex or not, we use 3 
dictionaries created for the PorSimples project: one 
containing words common to youngsters, a second 
one composed by frequent words extracted from 
news texts for children and nationwide newspa-
pers, and a third one containing concrete words.  
The lexical simplification module also uses the 
Unitex-PB dictionary4 for finding the lemma of the 
words in the text, so that it is possible to look for it 
in the simple words dictionaries. The problem of 
looking for a lemma directly in a dictionary is that 
there are ambiguous words and we are not able to 
deal with different word senses. For dealing with 
part-of-speech (POS) ambiguity, we use the 
MXPOST POS tagger5 trained over NILC tagset6. 
After the text is tagged, the words that are not 
proper nouns, prepositions and numerals are se-
lected, and their POS tags are used to look for their 
lemmas in the dictionaries. As the tagger has not a 
100% precision and some words may not be in the 
dictionary, we look for the lemma only (without 
the tag) when we are not able to find the lemma-
tag combination in the dictionary. Still, if we are 
not able to find the word, the lexical simplification 
module assumes that the word is complex and 
marks it for simplification. 
The last step of the process consists in providing 
simpler synonyms for the marked words. For this 
task, we use the thesauri for Portuguese TeP 2.07 
and the lexical ontology for Portuguese PAPEL8. 
This task is carried out when the user clicks on a 
marked word, which triggers a search in the the-
sauri for synonyms that are also present in the 
common words dictionary. If simpler words are 
found, they are listed in order, from the simpler to 
the more complex ones. To determine this order, 
we used Google API to search each word in the 
web: we assume that the higher a word frequency, 
the simpler it is. Automatic word sense disambigu-
ation is left for future work. 
                                                        
4 http://www.nilc.icmc.usp.br/nilc/projects/unitex-pb/web 
/dicionarios.html 
5 http://sites.google.com/site/adwaitratnaparkhi/home 
6 www.nilc.icmc.usp.br/nilc/TagSet/ManualEtiquetagem.htm  
7 http://www.nilc.icmc.usp.br/tep2/ 
8 http://www.linguateca.pt/PAPEL/ 
42
2.2 Syntactic Simplification 
Syntactic simplification is accomplished by a rule-
based system, which comprises seven operations 
that are applied sentence-by-sentence to a text in 
order to make its syntactic structure simpler.  
Our rule-based text simplification system is 
based on a manual for Brazilian Portuguese syntac-
tic simplification (Specia et al, 2008). According 
to this manual, simplification operations should be 
applied when any of the 22 linguistic phenomena 
covered by our system (see Candido et al (2009) 
for details) is detected. Our system treats apposi-
tive, relative, coordinate and subordinate clauses, 
which had already been addressed by previous 
work on text simplification (Siddharthan, 2003). 
Additionally, we treat passive voice, sentences in 
an order other than Subject-Verb-Object (SVO), 
and long adverbial phrases. The simplification op-
erations available to treat these phenomena are: 
split sentence, change particular discourse markers 
by simpler ones, change passive to active voice, 
invert the order of clauses, convert to subject-verb-
object ordering, and move long adverbial phrases. 
Each sentence is parsed in order to identify syn-
tactic phenomena for simplification and to segment 
the sentence into portions that will be handled by 
the operations. We use the parser PALAVRAS 
(Bick, 2000) for Portuguese. Gasperin et al (2010) 
present the evaluation of the performance of our 
syntactic simplification system.  
Since our syntactic simplifications are conserva-
tive, the simplified texts become longer than the 
original ones due to sentence splitting. We ac-
knowledge that low-literacy readers prefer short 
texts, and in the future we aim to provide summa-
rization within SIMPLIFICA (see (Watanabe et al, 
2009)). Here, the shortening of the text is a respon-
sibility of the author. 
2.3 Readability assessment 
With our readability assessment module, we can 
predict the readability level of a text, which cor-
responds to the literacy level expected from the 
target reader: rudimentary, basic or advanced.  
We have adopted a machine-learning classifier 
to identify the level of the input text; we use the 
Support Vector Machines implementation from 
Weka9 toolkit (SMO). We have used 7 corpora 
                                                        
9 http://www.cs.waikato.ac.nz/ml/weka/ 
within 2 different genres (general news and popu-
lar science articles) to train the classifier. Three of 
these corpora contain original texts published in 
online newspapers and magazines. The other cor-
pora contain manually simplified versions of most 
of the original texts. These were simplified by a 
linguist, specialized in text simplification, accord-
ing to the two levels of simplification proposed in 
our project, natural and strong, which result in 
texts adequate for the basic and rudimentary litera-
cy levels, respectively. 
Our feature set is composed by cognitively-
motivated features derived from the Coh-Metrix-
PORT tool10, which is an adaptation for Brazilian 
Portuguese of Coh-Metrix 2.0 (free version of 
Coh-Metrix (Graesser et al 2003)) also developed 
in the context of the PorSimples project. Coh-
Metrix-PORT implements the metrics in Table 1. 
 
Categories Subcategories Metrics 
Shallow 
Readabili-
ty metric 
- Flesch Reading Ease index 
for Portuguese. 
Words and 
textual 
informa-
tion 
Basic counts Number of words, sen-
tences, paragraphs, words 
per sentence, sentences per 
paragraph, syllables per 
word, incidence of verbs, 
nouns, adjectives and ad-
verbs. 
Frequencies Raw frequencies of content 
words and minimum fre-
quency of content words. 
Hyperonymy Average number of hyper-
nyms of verbs. 
Syntactic 
informa-
tion 
Constituents Incidence of nominal 
phrases, modifiers per noun 
phrase and words preced-
ing main verbs. 
Pronouns, 
Types and 
Tokens 
Incidence of personal pro-
nouns, number of pronouns 
per noun phrase, types and 
tokens. 
Connectives Number of connectives, 
number of positive and 
negative additive connec-
tives, causal / temporal / 
logical positive and nega-
tive connectives. 
Logical 
operators 
- Incidence of the particles 
?e? (and), ?ou? (or), ?se? 
(if), incidence of negation 
and logical operators. 
Table 1. Metrics of Coh-Metrix-PORT. 
 
                                                        
10 http://caravelas.icmc.usp.br:3000/ 
43
We also included seven new metrics to Coh-
Metrix-PORT: average verb, noun, adjective and 
adverb ambiguity, incidence of high-level constitu-
ents, content words and functional words.  
We measured the performance of the classifier 
on identifying the levels of the input texts by a 
cross-validation experiment. We trained the clas-
sifier on our 7 corpora and reached 90% F-measure 
on identifying texts at advanced level, 48% at basic 
level, and 73% at rudimentary level. 
 
3. A working session at SIMPLIFICA  
 
In the NAACL demonstration section we aim to 
present all functionalities of the tool for authoring 
simple texts, SIMPLIFICA. We will run all steps 
of the authoring process ? readability assessment, 
lexical simplification and syntactic simplification ? 
in order to demonstrate the use of the tool in pro-
ducing a text for basic and rudimentary readers of 
Portuguese, regarding the lexical and the syntactic 
complexity of an original text.  
We outline a script of our demonstration at 
http://www.nilc.icmc.usp.br/porsimples/demo/dem
o_script.htm. In order to help the understanding by 
non-speakers of Portuguese we provide the transla-
tions of the example texts shown. 
 
4. Final Remarks 
 
A tool for authoring simple texts in Portuguese is 
an innovative software, as are all the modules that 
form the tool. Such tool is extremely important in 
the construction of texts understandable by the ma-
jority of the Brazilian population. SIMPLIFICA?s 
target audience is varied and includes: teachers that 
use online text for reading practices; publishers; 
journalists aiming to reach poor literate readers; 
content providers for distance learning programs; 
government agencies that aim to communicate to 
the population as a whole; companies that produce 
technical manuals and medicine instructions; users 
of legal language, in order to facilitate the under-
standing of legal documents by lay people; and 
experts in language studies and computational lin-
guistics for future research. 
Future versions of SIMPLIFICA will also pro-
vide natural simplification, where the target sen-
tences for simplifications are chosen by a machine 
learning classifier (Gasperin et al, 2009). 
 
Acknowledgments 
We thank FAPESP and Microsoft Research for 
supporting the PorSimples project 
References  
Sandra Alu?sio, Lucia Specia, Thiago Pardo, Erick Ma-
ziero and Renata Fortes. 2008. Towards Brazilian 
Portuguese Automatic Text Simplification Systems. In 
Proceedings of The Eight ACM Symposium on Doc-
ument Engineering (DocEng 2008),  240-248, S?o 
Paulo, Brasil. 
Eckhard Bick. 2000. The Parsing System "Palavras": 
Automatic Grammatical Analysis of Portuguese in a 
Constraint Grammar Framework. PhD thesis. Aa-
rhus University. 
Arnaldo Candido Junior, Erick Maziero, Caroline Gas-
perin, Thiago Pardo, Lucia Specia and Sandra M. A-
luisio. 2009. Supporting the Adaptation of Texts for 
Poor Literacy Readers: a Text Simplification Editor 
for Brazilian Portuguese. In the Proceedings of the 
NAACL HLT Workshop on Innovative Use of NLP 
for Building Educational Applications, pages 34?42, 
Boulder, Colorado, June 2009. 
Caroline Gasperin; Lucia Specia; Tiago Pereira and  
Sandra Alu?sio. 2009. Learning When to Simplify 
Sentences for Natural Text Simplification. In: Pro-
ceedings of ENIA 2009,  809-818. 
Caroline Gasperin, Erick Masiero and Sandra M. Alui-
sio. 2010. Challenging choices for text simplifica-
tion. Accepted for publication in Propor 2010 
(http://www.inf.pucrs.br/~propor2010/). 
Arthur Graesser, Danielle McNamara, Max  Louwerse 
and Zhiqiang Cai. 2004. Coh-Metrix: Analysis of 
text on cohesion and language. In: Behavioral Re-
search Methods, Instruments, and Computers, 36, 
p?ginas 193-202. 
INAF. 2009. Instituto P. Montenegro and A??o Educa-
tiva. INAF Brasil - Indicador de Alfabetismo Funcio-
nal - 2009. Online available at http://www.ibope. 
com.br/ipm/relatorios/relatorio_inaf_2009.pdf  
Advaith Siddharthan. 2003. Syntactic Simplification and 
Text Cohesion. PhD Thesis. University of 
Cambridge. 
Lucia Specia, Sandra Aluisio and Tiago Pardo. 2008. 
Manual de Simplifica??o Sint?tica para o Portugu?s. 
Technical Report NILC-TR-08-06, 27 p. Junho 2008, 
S?o Carlos-SP. 
Willian Watanabe, Arnaldo Candido Junior, Vin?cius 
Uz?da, Renata Fortes, Tiago Pardo and Sandra Alu?-
sio. 2009. Facilita: reading assistance for low-
literacy readers. In Proceedings of the 27th ACM In-
ternational Conference on Design of Communication. 
SIGDOC '09. ACM, New York, NY, 29-36.  
44
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Readability Assessment for Text Simplification  
 
Sandra Aluisio1, Lucia Specia2, Caroline Gasperin1 and Carolina Scarton1 
1Center of Computational Linguistics (NILC) 2Research Group in Computational Linguistics 
University of S?o Paulo University of Wolverhampton 
S?o Carlos - SP, Brazil Wolverhampton, UK 
{sandra,cgasperin}@icmc.usp.br, 
carol.scarton@gmail.com 
L.Specia@wlv.ac.uk 
 
 
 
 
Abstract 
We describe a readability assessment ap-
proach to support the process of text simplifi-
cation for poor literacy readers. Given an in-
put text, the goal is to predict its readability 
level, which corresponds to the literacy level 
that is expected from the target reader: rudi-
mentary, basic or advanced. We complement 
features traditionally used for readability as-
sessment with a number of new features, and 
experiment with alternative ways to model 
this problem using machine learning methods, 
namely classification, regression and ranking. 
The best resulting model is embedded in an 
authoring tool for Text Simplification. 
1 Introduction 
In Brazil, the National Indicator of Functional Lite-
racy (INAF) index has been computed annually 
since 2001 to measure the levels of literacy of the 
Brazilian population. The 2009 report presented a 
worrying scenario: 7% of the individuals are illite-
rate; 21% are literate at the rudimentary level; 47% 
are literate at the basic level; only 25% are literate 
at the advanced level (INAF, 2009). These literacy 
levels are defined as:  
(1) Illiterate: individuals who cannot perform 
simple tasks such as reading words and phrases;  
(2) Rudimentary: individuals who can find ex-
plicit information in short and familiar texts (such 
as an advertisement or a short letter);  
(3) Basic: individuals who are functionally lite-
rate, i.e., they can read and understand texts of av-
erage length, and find information even when it is 
necessary to make some inference; and  
(4) Advanced: fully literate individuals, who can 
read longer texts, relating their parts, comparing 
and interpreting information, distinguish fact from 
opinion, make inferences and synthesize.   
In order to promote digital inclusion and acces-
sibility for people with low levels of literacy, par-
ticularly to documents available on the web, it is 
important to provide text in a simple and easy-to- 
read way. This is a requirement of the Web Con-
tent Accessibility Guidelines 2.0?s principle of 
comprehensibility and accessibility of Web con-
tent1. It states that for texts which demand reading 
skills more advanced than that of individuals with 
lower secondary education, one should offer an al-
ternative version of the same content suitable for 
those individuals. While readability formulas for 
English have a long history ? 200 formulas have 
been reported from 1920 to 1980s (Dubay, 2004) ? 
the only tool available for Portuguese is an adapta-
tion of the Flesch Reading Ease index.  It evaluates 
the complexity of texts in a 4-level scale corres-
ponding to grade levels (Martins et al, 1996).  
In the PorSimples project (Alu?sio et al, 2008) 
we develop text adaptation methods (via text sim-
plification and elaboration approaches) to improve 
the comprehensibility of texts published on gov-
ernment websites or by renowned news agencies, 
which are expected to be relevant to a large au-
dience with various literacy levels. The project 
provides automatic simplification tools to aid (1) 
poorly literate readers to understand online content 
? a browser plug-in for automatically simplifying 
websites ? and (2) authors producing texts for this 
audience ? an authoring tool for guiding the crea-
tion of simplified versions of texts.  
This paper focuses on a readability assessment 
approach to assist the simplification process in the 
authoring tool, SIMPLIFICA. The current version 
of SIMPLIFICA offers simplification operations 
addressing a number of lexical and syntactic phe-
nomena to make the text more readable. The au-
                                                          
1 http://www.w3.org/TR/WCAG20/ 
1
thor has the freedom to choose when and whether 
to apply the available simplification operations, a 
decision based on the level of complexity of the 
current text and on the target reader.  
A method for automatically identifying such 
level of complexity is therefore of great value. 
With our readability assessment tool, the author is 
able to automatically check the complexi-
ty/readability level of the original text, as well as 
modified versions of such text produced as he/she 
applies simplification operations offered by 
SIMPLIFICA, until the text reaches the expected 
level, adequate for the target reader. 
In this paper we present such readability as-
sessment tool, developed as part of the PorSimples 
project, and discuss its application within the au-
thoring tool. Different from previous work, the tool 
does not model text difficulty according to linear 
grade levels (e.g., Heilman et al, 2008), but in-
stead maps the text into the three levels of literacy 
defined by INAF: rudimentary, basic or advanced. 
Moreover, it uses a more comprehensive set of fea-
tures, different learning techniques and targets a 
new language and application, as we discuss in 
Section 4. More specifically, we address the fol-
lowing research questions: 
 
1. Given some training material, is it possible to 
detect the complexity level of Portuguese texts, 
which corresponds to the different literacy levels 
defined by INAF? 
2. What is the best way to model this problem 
and which features are relevant? 
 
We experiment with nominal, ordinal and interval-
based modeling techniques and exploit a number 
of the cognitively motivated features proposed by 
Coh-Metrix 2.0 (Graesser et al, 2004) and adapted 
to Portuguese (called Coh-Metrix-PORT), along 
with a set of new features, including syntactic fea-
tures to capture simplification operations and n-
gram language model features.  
In the remainder of this paper, we first provide 
some background information on the need for a 
readability assessment tool within our text simpli-
fication system (Section 2) and discuss prior work 
on readability assessment (Section 3), to then 
present our features and modeling techniques (Sec-
tion 4) and the experiments performed to answer 
our research questions (Section 5). 
2. Text Simplification in PorSimples 
Text Simplification (TS) aims to maximize reading 
comprehension of written texts through their sim-
plification. Simplification usually involves substi-
tuting complex by simpler words and breaking 
down and changing the syntax of complex, long 
sentences (Max, 2006; Siddharthan, 2003).   
To meet the needs of people with different le-
vels of literacy, in the PorSimples project we pro-
pose two types of simplification: natural and 
strong. The first type results in texts adequate for 
people with a basic literacy level and the second, 
rudimentary level. The difference between these 
two is the degree of application of simplification 
operations to complex sentences. In strong simpli-
fication, operations are applied to all complex syn-
tactic phenomena present in the text in order to 
make it as simple as possible, while in natural sim-
plification these operations are applied selectively, 
only when the resulting text remains ?natural?. 
One example of original text (a), along with its 
natural (b) and strong (c) manual simplifications, is 
given in Table 1. 
 
(a) The cinema theaters around the world were show-
ing a production by director Joe Dante in which a 
shoal of piranhas escaped from a military laborato-
ry and attacked participants of an aquatic show. 
(...) More than 20 people were bitten by palometas 
(Serrasalmus spilopleura, a species of piranhas) 
that live in the waters of the Sanchuri dam. 
(b) The cinema theaters around the world were show-
ing a production by director Joe Dante. In the pro-
duction a shoal of piranhas escaped from a military 
laboratory and attacked participants of an aquatic 
show. (?) More than 20 people were bitten by pa-
lometas that live in the waters of the Sanchuri dam. 
Palometas are Serrasalmus spilopleura, a species 
of piranhas. 
(c) The cinema theaters around the world were show-
ing a movie by director Joe Dante. In the movie a 
shoal of piranhas escaped from a military laborato-
ry. The shoal of piranhas attacked participants of 
an aquatic show. (...). Palometas have bitten more 
than 20 people. Palometas live in the waters of the 
Sanchuri dam. Palometas are Serrasalmus spilop-
leura, a species of piranhas. 
Table 1: Example of original and simplified texts 
 
The association between these two types of simpli-
fication and the literacy levels was identified by 
means of a corpus study. We have manually built a 
corpus of simplified texts at both natural and 
2
strong levels and analyzed their linguistic struc-
tures according to the description of the two litera-
cy levels. We verified that strong simplified sen-
tences are more adequate for rudimentary level 
readers, and natural ones for basic level readers. 
This claim is supported by several studies which 
relate capabilities and performance of the working 
memory with reading levels (Siddharthan, 2003; 
McNamara et al, 2002). 
2.1 The Rule-based Simplification System 
The association between simplification operations 
and the syntactic phenomena they address is im-
plemented within a rule-based syntactic simplifica-
tion system (Candido Jr. et al, 2009). This system 
is able to identify complex syntactic phenomena in 
a sentence and perform the appropriate operations 
to simplify each phenomenon.  
The simplification rules follow a manual for 
syntactic simplification in Portuguese also devel-
oped in PorSimples. They cover syntactic con-
structions such as apposition, relative clauses, 
coordination and subordination, which had already 
been addressed by previous work on text simplifi-
cation (Siddharthan, 2003). Additionally, they ad-
dress the transformation of sentences from passive 
into active voice, normalization of sentences into 
the Subject-Verb-Object order, and simplification 
of adverbial phrases. The simplification operations 
available are: sentence splitting, changing particu-
lar discourse markers by simpler ones, transform-
ing passive into active voice, inverting the order of 
clauses, converting to subject-verb-object order, 
relocating long adverbial phrases.  
2.2 The SIMPLIFICA Tool 
The rule-based simplification system is part of 
SIMPLIFICA, an authoring tool for writers to 
adapt original texts into simplified texts. Within 
SIMPLIFICA, the author plays an active role in 
generating natural or strong simplified texts by ac-
cepting or rejecting the simplifications offered by 
the system on a sentence basis and post-editing 
them if necessary. 
 Despite the ability to make such choices at the 
sentence level, it is not straightforward for the au-
thor to judge the complexity level of the text as 
whole in order to decide whether it is ready for a 
certain audience. This is the main motivation for 
the development of a readability assessment tool.  
The readability assessment tool automatically 
detects the level of complexity of a text at any 
moment of the authoring process, and therefore 
guides the author towards producing the adequate 
simplification level according to the type of reader. 
It classifies a text in one of three levels: rudimenta-
ry, basic or advanced.  
Figure 1 shows the interface of SIMPLIFICA, 
where the complexity level of the current text as 
given by the readability assessment tool is shown 
at the bottom, in red (in this case, ?N?vel Pleno?, 
which corresponds to advanced). To update the 
readability assessment of a text the author can 
choose ?N?vel de Inteligibilidade? (readability lev-
el) at any moment.  
The text shown in Figure 1 is composed of 13 
sentences, 218 words. The lexical simplification 
module (not shown in the Figure 1) finds 10 candi-
date words for simplification in this text, and the 
syntactic simplification module selects 10 sen-
tences to be simplified (highlighted in gray).  
When the author selects a highlighted sentence, 
he/she is presented with all possible simplifications 
proposed by the rule-based system for this sen-
tence. Figure 2 shows the options for the first sen-
tence in Figure 1. The first two options cover non-
finite clause and adverbial adjuncts, respectively, 
while the third option covers both phenomena in 
one single step. The original sentence is also given 
as an option.  
It is possible that certain suggestions of auto-
matic simplifications result in ungrammatical or 
inadequate sentences (mainly due to parsing er-
rors). The author can choose not to use such sug-
gestions as well as manually edit the original or 
automatically simplified versions. The impact of 
the author?s choice on the overall readability level 
of the text is not always clear to the author. The 
goal of the readability assessment function is to 
provide such information. 
Simplified texts are usually longer than the 
original ones, due to sentence  splittings and 
repetition of information to connect such 
sentences.  We  acknowledge  that  low literacy 
readers prefer short texts, but in this tool the 
shortening of the text is a responsibility of the 
author. Our focus is on the linguistic structure of 
the texts; the length of the text actually is a feature 
considered by our readability assessment system. 
3
Figure 1: SIMPLIFICA interface 
Figure 2. Simplification options available for the first sentence of the text presented in Figure 1
3. Readability Assessment 
Recent work on readability assessment for the 
English language focus on: (i) the feature set used 
to capture the various aspects of readability, to 
evaluate the contribution of lexical, syntactic, se-
mantic and discursive features; (ii) the audience of 
the texts the readability measurement is intended 
to; (iii) the genre effects on the calculation of text 
difficult; (iv) the type of learning technique 
which is more appropriate: those producing nomi-
nal, ordinal or interval scales of measurement, and 
(v) providing an application for the automatic as-
sessment of reading difficulty.  
Pitler and Nenkova (2008) propose a unified 
framework composed of vocabulary, syntactic, 
elements of lexical cohesion, entity coherence and 
discourse relations to measure text quality, which 
resembles the composition of rubrics in the area of 
essay scoring (Burstein et al, 2003).  
 The following studies address readability as-
sessment for specific audiences: learners of Eng-
lish as second language (Schwarm and Ostendorf, 
2005; Heilman et al, 2007), people with intellec-
tual disabilities (Feng et al, 2009), and people with 
cognitive impairment caused by Alzheimer (Roark 
at al, 2007). 
Sheehan et al (2007) focus on models for 
literary and expository texts, given that traditional 
metrics like Flesch-Kincaid Level score tend to 
overpredict the difficulty of literary texts and 
underpredict the difficulty of expository texts.  
Heilman et al (2008) investigate an appropriate 
scale of measurement for reading difficulty ? 
nominal, ordinal, or interval ? by comparing the 
effectiveness of statistical models for each type of 
data. Petersen and Ostendorf (2009) use 
classification and regression techniques to predict a 
readability score. 
Miltsakali and Troutt (2007; 2008) propose an 
automatic tool to evaluate reading difficulty of 
Web texts in real time, addressing teenagers and 
adults with low literacy levels. Using machine 
learning, Gl?ckner et al (2006) present a tool for 
automatically rating the readability of German 
texts using several linguistic information sources 
and a global readability score similar to the Flesch 
Reading Ease.   
4
4. A Tool for Readability Assessment 
In this section we present our approach to readabil-
ity assessment.  It differs from previous work in 
the following aspects: (i) it uses a feature set with 
cognitively-motivated metrics and a number of ad-
ditional features to provide a better explanation of 
the complexity of a text; (ii) it targets a new audi-
ence: people with different literacy levels; (iii) it 
investigates different statistical models for non- 
linear data scales: the levels of literacy defined by 
INAF, (iv) it focus on a new application: the use of 
readability assessment for text simplification sys-
tems; and (v) it is aimed at Portuguese. 
4.1 Features for Assessing Readability 
Our feature set (Table 2) consists of 3 groups of 
features. The first group contains cognitively-
motivated features (features 1-42), derived from 
the Coh-Metrix-PORT tool (see Section 4.1.1). 
The second group contains features that reflect the 
incidence of particular syntactic constructions 
which we target in our text simplification system 
(features 43-49). The third group (the remaining 
features in Table 2) contains features derived from 
n-gram language models built considering uni-
grams, bigrams and trigrams probability and per-
plexity plus out-of-vocabulary rate scores. We later 
refer to a set of basic features, which consist of 
simple counts that do not require any linguistic tool 
or external resources to be computed. This set cor-
responds to features 1-3 and 9-11. 
4.1.1 Coh-Metrix-Port 
The Coh-Metrix tool was developed to compute 
features potentially relevant to the comprehension 
of English texts through a number of measures in-
formed by linguistics, psychology and cognitive 
studies. The main aspects covered by the measures 
are cohesion and coherence (Graesser et al, 2004). 
Coh-Metrix 2.0, the free version of the tool, con-
tains 60 readability metrics. The Coh-Metrix-
PORT tool (Scarton et al, 2009) computes similar 
metrics for texts in Brazilian Portuguese. The ma-
jor challenge to create such tool is the lack of some 
of the necessary linguistic resources. The follow-
ing metrics are currently available in the tool (we 
refer to Table 2 for details): 
1. Readability metric: feature 12. 
 
2. Words and textual information:  
 Basic counts: features 1 to 11. 
1 Number of words 
2 Number of sentences 
3 Number of paragraphs 
4 Number of verbs 
5 Number of nouns 
6 Number of adjectives 
7 Number of adverbs 
8 Number of pronouns 
9 Average number of words per sentence 
10 Average number of sentences per paragraph 
11 Average number of syllables per word 
12 Flesch index for Portuguese 
13 Incidence of content words 
14 Incidence of functional words  
15 Raw Frequency of content words  
16 Minimal frequency of content words  
17 Average number of verb hypernyms 
18 Incidence of NPs 
19 Number of NP modifiers 
20 Number of words before the main verb 
21 Number of high level constituents 
22 Number of personal pronouns 
23 Type-token ratio 
24 Pronoun-NP ratio 
25 Number of ?e? (and) 
26 Number of ?ou? (or)  
27 Number of ?se? (if) 
28 Number of negations 
29 Number of logic operators 
30 Number of connectives  
31 Number of positive additive connectives 
32 Number of negative additive connectives 
33 Number of positive temporal connectives 
34 Number of negative temporal connectives 
35 Number of positive causal connectives 
36 Number of negative causal connectives 
37 Number of positive logic connectives 
38 Number of negative logic connectives 
39 Verb ambiguity ratio 
40 Noun ambiguity ratio 
41 Adverb ambiguity ratio 
42 Adjective ambiguity ratio 
43 Incidence of clauses 
44 Incidence of adverbial phrases 
45 Incidence of apposition 
46 Incidence of passive voice 
47 Incidence of relative clauses 
48 Incidence of coordination 
49 Incidence of subordination 
50 Out-of-vocabulary words  
51 LM probability of unigrams  
52 LM perplexity of unigrams  
53 LM perplexity of unigrams, without line break  
54 LM probability of bigrams  
55 LM perplexity of bigrams  
56 LM perplexity of bigrams, without line break  
57 LM probability of trigrams  
58 LM perplexity of trigrams  
59 LM perplexity of trigrams, without line break  
Table 2. Feature set 
5
 Frequencies: features 15 to 16. 
 Hypernymy: feature 17. 
 
3. Syntactic information:  
 Constituents: features 18 to 20. 
 Pronouns: feature 22 
 Types and Tokens: features 23 to 24. 
 Connectives: features 30 to 38. 
 
4. Logical operators: features 25 to 29. 
 
The following resources for Portuguese were used: 
the MXPOST POS tagger (Ratnaparkhi, 1996), a 
word frequency list compiled from a 700 million-
token corpus2, a tool to identify reduced noun 
phrases (Oliveira et al, 2006), a list of connectives 
classified as positives/negatives and according to 
cohesion type (causal, temporal, additive or logi-
cal), a list of logical operators and WordNet.Br 
(Dias-da-Silva et al, 2008).  
In this paper we include seven new metrics to 
Coh-Metrix-PORT: features 13, 14, 21, and 39 to 
42. We used TEP3 (Dias-da-Silva et al, 2003) to 
obtain the number of senses of words (and thus 
their ambiguity level), and the Palavras parser 
(Bick, 2000) to identify the higher level constitu-
ents. The remaining metrics were computed based 
on the POS tags. 
According to a report on the performance of 
each Coh-Metrix-PORT metric (Scarton et al, 
2009), no individual feature provides sufficient in-
dication to measure text complexity, and therefore 
the need to exploit their combination, and also to 
combine them with the other types of features de-
scribed in this section. 
4.1.2 Language-model Features 
Language model features were derived from a 
large corpus composed of a sample of the Brazilian 
newspaper Folha de S?o Paulo containing issues 
from 12 months taken at random from 1994 to 
2005. The corpus contains 96,868 texts and 
26,425,483 tokens. SRILM (Stolcke, 2002), a 
standard language modelling toolkit, was used to 
produce the language model features.  
4.2 Learning Techniques 
Given that the boundaries of literacy level classes 
are one of the subjects of our study, we exploit 
three different types of models in order to check 
                                                          
2 http://www2.lael.pucsp.br/corpora/bp/index.htm 
3 http://www.nilc.icmc.usp.br/tep2/index.htm 
which of them can better distinguish among the 
three literacy levels. We therefore experiment with 
three types of machine learning algorithms: a stan-
dard classifier, an ordinal (ranking) classifier and a 
regressor. Each algorithm assumes different rela-
tions among the groups: the classifier assumes no 
relation, the ordinal classifier assumes that the 
groups are ordered, and the regressor assumes that 
the groups are continuous.  
As classifier we use the Support Vector Ma-
chines (SVM) implementation in the Weka4 toolkit 
(SMO). As ordinal classifier we use a meta clas-
sifier in Weka which takes SMO as the base classi-
fication algorithm and performs pairwise classifi-
cations (OrdinalClassClassifier). For regression we 
use the SVM regression implementation in Weka 
(SMO-reg). We use the linear versions of the algo-
rithms for classification, ordinal classification and 
regression, and also experiment with a radial basis 
function (RBF) kernel for regression. 
5. Experiments 
5.1 Corpora 
In order to train (and test) the different machine 
learning algorithms to automatically identify the 
readability level of the texts we make use of ma-
nually simplified corpora created in the PorSimples 
project. Seven corpora covering our three literacy 
levels (advanced, basic and rudimentary) and two 
different genres were compiled. The first corpus is 
composed of general news articles from the Brazil-
ian newspaper Zero Hora (ZH original). These ar-
ticles were manually simplified by a linguist, ex-
pert in text simplification, according to the two 
levels of simplification: natural (ZH natural) and 
strong (ZH strong). The remaining corpora are 
composed of popular science articles from differ-
ent sources: (a) the Caderno Ci?ncia section of the 
Brazilian newspaper Folha de S?o Paulo, a main-
stream newspaper in Brazil (CC original) and a 
manually simplified version of this corpus using 
the natural (CC natural) and strong (CC strong) 
levels; and (b) advanced level texts from a popular 
science magazine called Ci?ncia Hoje (CH). Table 
3 shows a few statistics about these seven corpora. 
5.2 Feature Analysis 
As a simple way to check the contribution of dif-
ferent features to our three literacy levels, we com- 
                                                          
4 http://www.cs.waikato.ac.nz/ml/weka/ 
6
  
Corpus Doc Sent Words Avg. words 
per text (std. 
deviation) 
Avg. 
words p. 
sentence 
ZH original 104 2184 46190 444.1 (133.7) 21.1 
ZH natural 104 3234 47296 454.7 (134.2) 14.6 
ZH strong 104 3668 47938 460.9 (137.5) 13.0 
CC original 50 882 20263 405.2 (175.6) 22.9 
CC natural 50 975 19603 392.0 (176.0) 20.1 
CC strong 50 1454 20518 410.3 (169.6) 14.1 
CH 130 3624 95866 737.4 (226.1) 26.4 
Table 3. Corpus statistics 
 
puted the (absolute) Pearson correlation between 
our features and the expected literacy level for the 
two sets of corpora that contain versions of the 
three classes of interest (original, natural and 
strong). Table 4 lists the most highly correlated 
features. 
 
 Feature Corr. 
1 Words per sentence 0.693 
2 Incidence of apposition 0.688 
3 Incidence of clauses 0.614 
4 Flesch index  0.580 
5 Words before main verb  0.516 
6 Sentences per paragraph  0.509 
7 Incidence of relative clauses  0.417 
8 Syllables per word 0.414 
9 Number of positive additive connectives  0.397 
10 Number of negative causal connectives 0.388 
Table 4: Correlation between features and literacy levels 
 
Among the top features are mostly basic and syn-
tactic features representing the number of apposi-
tive and relative clauses and clauses in general, and 
also features from Coh-Metrix-PORT. This shows 
that traditional cognitively-motivated features can 
be complemented with more superficial features 
for readability assessment. 
5.3 Predicting Complexity Levels 
As previously discussed, the goal is to predict the 
complexity level of a text as original, naturally or 
strongly simplified, which correspond to the three 
literacy levels of INAF: rudimentary, basic and ad-
vanced level.  
Tables 5-7 show the results of our experiments 
using 10-fold cross-validation and standard classi-
fication (Table 5), ordinal classification (Table 6) 
and regression (Table 7), in terms of F-measure 
(F), Pearson correlation with true score (Corr.) and 
mean absolute error (MAE). Results using our 
complete feature set (All) and different subsets of 
it are shown so that we can analyze the 
performance of each group of features. We also 
experiment with the Flesch index on its own as a 
feature. 
 
Features Class F Corr. MAE 
All original 0.913 0.84 0.276 
natural 0.483 
strong 0.732 
Language 
Model 
original 0.669 0.25 0.381 
natural 0.025 
strong 0.221 
Basic original 0.846 0.76 0.302 
natural 0.149 
strong 0.707 
Syntactic original 0.891 0.82 0.285 
natural 0.32 
strong 0.74 
Coh-
Metrix-
PORT 
original 0.873 0.79 0.290 
natural 0.381 
strong 0.712 
Flesch original 0.751 0.52 0.348 
natural 0.152 
strong 0.546 
Table 5: Standard Classification 
 
Features Class F Corr. MAE 
All original 0.904 0.83 0.163 
natural 0.484 
strong 0.731 
Language 
Model 
original 0.634 0.49 0.344 
natural 0.497 
strong 0.05 
Basic original 0.83 0.73 0.231 
natural 0.334 
strong 0.637 
Syntactic original 0.891 0.81 0.180 
natural 0.382 
strong 0.714 
Coh-
Metrix-
PORT 
original 0.878 0.8 0.183 
natural 0.432 
strong 0.709 
Flesch original 0.746 0.56 0.310 
natural 0.489 
strong 0 
Table 6: Ordinal classification 
 
The results of the standard and ordinal classifica-
tion are comparable in terms of F-measure and cor-
relation, but the mean absolute error is lower for 
the ordinal classification. This indicates that ordi-
nal classification is more adequate to handle our 
classes, similarly to the results found in (Heilman 
et al, 2008). Results also show that distinguishing 
between natural and strong simplifications is a 
harder problem than distinguishing between these 
and original texts. This was expected, since these 
two levels of simplification share many features. 
However, the average performance achieved is 
considered satisfactory. 
Concerning the regression model (Table 7), the 
RBF kernel reaches the best correlation scores 
7
among all models. However, its mean error rates 
are above the ones found for classification. A lin-
ear SVM (not shown here) achieves very poor re-
sults across all metrics. 
   
Features Corr. MAE 
All 0.8502 0.3478 
Language Model 0.6245 0.5448 
Basic 0.7266 0.4538 
Syntactic 0.8063 0.3878 
Coh-Metrix-PORT 0.8051 0.3895 
Flesch 0.5772 0.5492 
Table 7: Regression with RBF kernel 
 
With respect to the different feature sets, we can 
observe that the combination of all features consis-
tently yields better results according to all metrics 
across all our models. The performances obtained 
with the subsets of features vary considerably from 
model to model, which shows that the combination 
of features is more robust across different learning 
techniques. Considering each feature set independ-
ently, the syntactic features, followed by Coh-
Metrix-PORT, achieve the best correlation scores, 
while the language model features performed the 
poorest. 
These results show that it is possible to predict 
with satisfactory accuracy the readability level of 
texts according to our three classes of interest: 
original, naturally simplified and strongly simpli-
fied texts. Given such results we embedded the 
classification model (Table 5) as a tool for read-
ability assessment into our text simplification au-
thoring system. The linear classification is our 
simplest model, has achieved the highest F-
measure and its correlation scores are comparable 
to those of the other models.  
6. Conclusions 
We have experimented with different machine 
learning algorithms and features in order to verify 
whether it was possible to automatically distin-
guish among the three readability levels: original 
texts aimed at advanced readers, naturally simpli-
fied texts aimed at people with basic literacy level, 
and strongly simplified texts aimed at people with 
rudimentary literacy level. All algorithms achieved 
satisfactory performance with the combination of 
all features and we embedded the simplest model 
into our authoring tool. 
As future work, we plan to investigate the con-
tribution of deeper cognitive features to this prob-
lem, more specifically, semantic, co-reference and 
mental model dimensions metrics. Having this ca-
pacity for readability assessment is useful not only 
to inform authors preparing simplified material 
about the complexity of the current material, but 
also to guide automatic simplification systems to 
produce simplifications with the adequate level of 
complexity according to the target user.  
The authoring tool, as well as its text simplifica-
tion and readability assessment systems, can be 
used not only for improving text accessibility, but 
also for educational purposes: the author can pre-
pare texts that are adequate according to the level 
of the reader and it will also allow them to improve 
their reading skills. 
References  
Sandra M. Alu?sio, Lucia Specia, Thiago A. S. Pardo, 
Erick G. Maziero, Renata P. M. Fortes (2008). To-
wards Brazilian Portuguese Automatic Text Simpli-
fication Systems. In the Proceedings of the 8th ACM 
Symposium on Document Engineering, pp. 240-248. 
Eckhard Bick (2000). The Parsing System "Palavras": 
Automatic Grammatical Analysis of Portuguese in a 
Constraint Grammar Framework. PhD Thesis. Uni-
versity of ?rhus, Denmark. 
Jill Burstein, Martin Chodorow and Claudia Leacock 
(2003). CriterionSM Online Essay Evaluation: An 
Application for Automated Evaluation of Student 
Essays. In the Proceedings of the Fifteenth Annual 
Conference on Innovative Applications of Artificial 
Intelligence, Acapulco, Mexico.  
Arnaldo Candido Jr., Erick Maziero, Caroline Gasperin, 
Thiago A. S. Pardo, Lucia Specia, and Sandra M. 
Aluisio (2009). Supporting the Adaptation of Texts 
for Poor Literacy Readers: a Text Simplification 
Editor for Brazilian Portuguese. In NAACL-HLT 
Workshop on Innovative Use of NLP for Building 
Educational Applications, pages 34?42, Boulder?.  
Helena de M. Caseli, Tiago de F. Pereira, L?cia Specia, 
Thiago A. S. Pardo, Caroline Gasperin and Sandra 
Maria Alu?sio (2009). Building a Brazilian Portu-
guese Parallel Corpus of Original and Simplified 
Texts. In the Proceedings of CICLing. 
Max Coltheart (1981). The MRC psycholinguistic data-
base. In Quartely Jounal of Experimental Psycholo-
gy, 33A, pages 497-505. 
Scott Deerwester, Susan T. Dumais, George W. Furnas, 
Thomas K. Landauer e Richard Harshman (1990). 
Indexing By Latent Semantic Analysis. In Journal of 
the American Society For Information Science, V. 
41, pages 391-407. 
Bento C. Dias-da-Silva and Helio R. Moraes (2003). A 
constru??o de um thesaurus eletr?nico para o portu-
gu?s do Brasil. In ALFA- Revista de Ling??stica, V. 
8
47, N. 2, pages 101-115.    
Bento C Dias-da-Silva, Ariani Di Felippo and Maria das 
Gra?as V. Nunes (2008). The automatic mapping of 
Princeton WordNet lexical conceptual relations onto 
the Brazilian Portuguese WordNet database. In Pro-
ceedings of the 6th LREC, Marrakech, Morocco. 
William H. DuBay (2004). The principles of readability. 
Costa Mesa, CA: Impact Information: http://www.i 
mpact-information.com/impactinfo/readability02.pdf 
Christiane Fellbaum (1998). WordNet: An electronic 
lexical database. Cambridge, MA: MIT Press. 
Lijun Feng, No?mie Elhadad and Matt Huenerfauth 
(2009). Cognitively Motivated Features for Reada-
bility Assessment. In the Proceedings of EACL 
2009, pages 229-237. 
Ingo Gl?ckner, Sven Hartrumpf, Hermann Helbig, Jo-
hannes Leveling and Rainer Osswald (2006b). An 
architecture for rating and controlling text readabili-
ty. In Proceedings of KONVENS 2006, pages 32-35. 
Konstanz, Germany.  
Arthur C. Graesser, Danielle S. McNamara, Max M. 
Louwerse and Zhiqiang Cai (2004). Coh-Metrix: 
Analysis of text on cohesion and language. In Beha-
vioral Research Methods, Instruments, and Comput-
ers, V. 36, pages 193-202. 
Ronald K. Hambleton, H. Swaminathan and H. Jane 
Rogers (1991). Fundamentals of item response 
theory. Newbury Park, CA: Sage Press. 
Michael Heilman, Kevyn Collins-Thompson, Jamie 
Callan and Max Eskenazi (2007). Combining lexical 
and grammatical features to improve readability 
measures for first and second language texts. In the 
Proceedings of NAACL HLT 2007, pages 460-467. 
Michael Heilman, Kevyn Collins-Thompson and Max-
ine Eskenazi (2008). An Analysis of Statistical 
Models and Features for Reading Difficulty Predic-
tion. In Proceedings of the 3rd Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 71-79. 
INAF (2009). Instituto P. Montenegro and A??o Educa-
tiva. INAF Brasil - Indicador de Alfabetismo Funcio-
nal - 2009. Available online at http://www. ibope. 
com.br/ipm/relatorios/relatorio_inaf_2009.pdf  
Teresa B. F. Martins, Claudete M. Ghiraldelo, Maria 
das Gra?as V. Nunes e Osvaldo N. de Oliveira Jr. 
(1996). Readability formulas applied to textbooks in 
brazilian portuguese. ICMC Technical Report, N. 
28, 11p.  
Aur?lien Max (2006). Writing for Language-impaired 
Readers. In Proceedings of CICLing, pages 567-570. 
Danielle McNamara, Max Louwerse, and Art Graesser, 
2002. Coh-Metrix: Automated cohesion and coher-
ence scores to predict text readability and facilitate 
comprehension. Grant proposal. http://cohmetrix. 
memphis.edu/cohmetrixpr/publications.html 
Eleni Miltsakaki and Audrey Troutt (2007). Read-X: 
Automatic Evaluation of Reading Difficulty of Web 
Text. In the Proceedings of E-Learn 2007, Quebec, 
Canada. 
Eleni Miltsakaki and Audrey Troutt (2008). Real Time 
Web Text Classification and Analysis of Reading 
Difficulty. In the Proceedings of the 3rd Workshop 
on Innovative Use of NLP for Building Educational 
Applications, Columbus, OH. 
Cl?udia Oliveira, Maria C. Freitas, Violeta Quental, C?-
cero N. dos Santos, Renato P. L. and Lucas Souza 
(2006). A Set of NP-extraction rules for Portuguese: 
defining and learning. In 7th Workshop on Computa-
tional Processing of Written and Spoken Portuguese, 
Itatiaia, Brazil.  
Sarah E. Petersen and Mari Ostendorf (2009). A ma-
chine learning approach to reading level assess-
ment. Computer Speech and Language 23, 89-106. 
Emily Pitler and Ani Nenkova (2008). Revisiting reada-
bility: A unified framework for predicting text quali-
ty. In Proceedings of EMNLP, 2008. 
Adwait Ratnaparkhi (1996). A Maximum Entropy Part-
of-Speech Tagger. In Proceedings of the First Em-
pirical Methods in Natural Language Processing 
Conference, pages133-142. 
Brian Roark, Margaret Mitchell and Kristy Holling-
shead (2007). Syntactic complexity measures for de-
tecting mild cognitive impairment. In the Proceed-
ings of the Workshop on BioNLP 2007: Biological, 
Translational, and Clinical Language Processing, 
Prague, Czech Republic. 
Caroline E. Scarton, Daniel M. Almeida, Sandra M. A-
lu?sio (2009). An?lise da Inteligibilidade de textos 
via ferramentas de Processamento de L?ngua Natu-
ral: adaptando as m?tricas do Coh-Metrix para o 
Portugu?s. In Proceedings of STIL-2009, S?o Carlos, 
Brazil.   
Sarah E. Schwarm and Mari Ostendorf (2005). Reading 
Level Assessment Using Support Vector Machines 
and Statistical Language Models. In the Proceedings 
of the 43rd Annual Meeting of the ACL, pp 523?530. 
Kathleen M. Sheehan, Irene Kostin and Yoko Futagi 
(2007). Reading Level Assessment for Literary and 
Expository Texts. In D. S. McNamara and J. G. 
Trafton (Eds.), Proceedings of the 29th Annual Cog-
nitive Science Society, page 1853. Austin, TX: Cog-
nitive Science Society. 
Advaith Siddharthan (2003). Syntactic Simplification 
and Text Cohesion. PhD Thesis. University of Cam-
bridge. 
Andreas Stolcke. SRILM -- an extensible language 
modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing, 2002. 
9
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 93?100,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Identifying Pronominal Verbs: Towards Automatic
Disambiguation of the Clitic se in Portuguese
Magali Sanches Duran?, Carolina Evaristo Scarton?,
Sandra Maria Alu??sio?, Carlos Ramisch?
? University of Sa?o Paulo (Brazil)
? Joseph Fourier University (France)
magali.duran@uol.com.br, carol.scarton@gmail.com
sandra@icmc.usp.br, carlosramisch@gmail.com
Abstract
A challenging topic in Portuguese language
processing is the multifunctional and ambigu-
ous use of the clitic pronoun se, which impacts
NLP tasks such as syntactic parsing, semantic
role labeling and machine translation. Aiming
to give a step forward towards the automatic
disambiguation of se, our study focuses on the
identification of pronominal verbs, which cor-
respond to one of the six uses of se as a clitic
pronoun, when se is considered a CONSTITU-
TIVE PARTICLE of the verb lemma to which
it is bound, as a multiword unit. Our strategy
to identify such verbs is to analyze the results
of a corpus search and to rule out all the other
possible uses of se. This process evidenced
the features needed in a computational lexicon
to automatically perform the disambiguation
task. The availability of the resulting lexicon
of pronominal verbs on the web enables their
inclusion in broader lexical resources, such as
the Portuguese versions of Wordnet, Propbank
and VerbNet. Moreover, it will allow the revi-
sion of parsers and dictionaries already in use.
1 Introduction
In Portuguese, the word se is multifunctional. POS
taggers have succeeded in distinguishing between se
as a conjunction (meaning if or whether) and se as
a pronoun (see Martins et al (1999) for more details
on the complexity of such task). As a clitic1 pro-
1A clitic is a bound form, phonologically unstressed, at-
tached to a word from an open class (noun, verb, adjective, ad-
verbial). It belongs to closed classes, that is, classes that have
grammatical rather than lexical meaning (pronouns, auxiliary
verbs, determiners, conjunctions, prepositions, numerals).
noun, however, se has six uses:
1. marker of SUBJECT INDETERMINATION:
Ja? se falou muito nesse assunto.
*Has-SE already spoken a lot about this matter.
One has already spoken a lot about this matter.
2. marker of pronominal PASSIVE voice (syn-
thetic passive voice):
Sugeriram-se muitas alternativas.
*Have-SE suggested many alternatives.
Many alternatives have been suggested.
3. REFLEXIVE pronoun (-self pronouns):
Voce? deveria se olhar no espelho.
*You should look-SE on the mirror.
You should look at yourself on the mirror.
4. RECIPROCAL pronoun (each other):
Eles se cumprimentaram com um aperto de ma?o.
*They greeted-SE with a handshake.
They greeted each other with a handshake.
5. marker of causative-INCHOATIVE alternation2:
Esse esporte popularizou-se no Brasil.
*This sport popularED-SE in Brazil.
This sport became popular in Brazil.
6. CONSTITUTIVE PARTICLE of the verb lexical
item (pronominal verb):
Eles se queixaram de dor no joelho.
*They complained-SE about knee pain.
They complained about knee pain.
2Causative-inchoative alternation: a same verb can be used
two different ways, one transitive, in which the subject position
is occupied by the argument which causes the action or process
described by the verb (causative use), and one intransitive, in
which the subject position is occupied by the argument affected
by the action or process (inchoative use).
93
Clitic se uses Syntactic
function
Semantic
function
SUBJECT INDE-
TERMINATION
NO YES3
PASSIVE
YES YES3
REFLEXIVE
YES YES
RECIPROCAL
YES YES
INCHOATIVE
YES NO
CONSTITUTIVE
PARTICLE
NO NO
Table 1: Uses of the clitic se from the point of view of
syntax and semantics.
The identification of these uses is very important
for Portuguese language processing, notably for syn-
tactic parsing, semantic role labeling (SRL) and ma-
chine translation. Table 1 shows which of these six
uses support syntactic and/or semantic functions.
Since superficial syntactic features seem not suffi-
cient to disambiguate the uses of the pronoun se, we
propose the use of a computational lexicon to con-
tribute to this task. To give a step forward to solve
this problem, we decided to survey the verbs un-
dergoing se as an integral part of their lexical form
(item 6), called herein pronominal verbs, but also
known as inherent reflexive verbs (Rosa?rio Ribeiro,
2011). Grammars usually mention this kind of verbs
and give two classical examples: queixar-se (to com-
plain) and arrepender-se (to repent). For the best of
our knowledge, a comprehensive list of these multi-
word verbs is not available in electronic format for
NLP uses, and not even in a paper-based format,
such as a printed dictionary.
An example of the relevance of pronominal verbs
is that, in spite of not being argumental, that is, not
being eligible for a semantic role label, the use of se
as a CONSTITUTIVE PARTICLE should integrate the
verb that evokes the argumental structure, as may be
seen in Figure 1.
The identification of pronominal verbs is not a
trivial task because a pronominal verb has a nega-
3In these cases, the clitic may support the semantic role label
of the suppressed external argument (agent).
Figure 1: Sentence The broadcasters refused to apologize
includes pronominal verbs negar-se (refuse) and retratar-
se (apologize) that evoke frames in SRL.
tive definition: if se does not match the restrictions
imposed by the other five uses, so it is a CONSTI-
TUTIVE PARTICLE of the verb, that is, it composes a
multiword. Therefore, the identification of pronom-
inal verbs requires linguistic knowledge to distin-
guish se as a CONSTITUTIVE PARTICLE from the
other uses of the the pronoun se (SUBJECT INDE-
TERMINATION, PASSIVE, REFLEXIVE, RECIPRO-
CAL and INCHOATIVE.)
There are several theoretical linguistic studies
about the clitic pronoun se in Portuguese. Some of
these studies present an overview of the se pronoun
uses, but none of them prioritized the identification
of pronominal verbs. The study we report in this pa-
per is intended to fill this gap.
2 Related Work
From a linguistic perspective, the clitic pronoun
se has been the subject of studies focusing on:
SUBJECT INDETERMINATION and PASSIVE uses
(Morais Nunes, 1990; Cyrino, 2007; Pereira-Santos,
2010); REFLEXIVE use (Godoy, 2012), and IN-
CHOATIVE use (Fonseca, 2010; Nunes-Ribeiro,
2010; Rosa?rio Ribeiro, 2011). Despite none of these
works concerning specifically pronominal verbs,
they provided us an important theoretical basis for
the analysis undertaken herein.
The problem of the multifunctional use of clitic
pronouns is not restricted to Portuguese. Romance
languages, Hebrew, Russian, Bulgarian and oth-
ers also have similar constructions. There are
94
crosslinguistic studies regarding this matter reported
in Siloni (2001) and Slavcheva (2006), showing
that there are partial coincidence of verbs taking
clitic pronouns to produce alternations and reflexive
voice.
From an NLP perspective, the problem of the
ambiguity of the clitic pronoun se was studied by
Martins et al (1999) to solve a problem of catego-
rization, that is, to decide which part-of-speech tag
should be assigned to se. However, we have not
found studies regarding pronominal verbs aiming at
Portuguese automatic language processing.
Even though in Portuguese all the uses of the clitic
pronoun se share the same realization at the surface
form level, the use as a CONSTITUTIVE PARTICLE of
pronominal verbs is the only one in which the verb
and the clitic form a multiword lexical unit on its
own. In the other uses, the clitic keeps a separate
syntactic and/or semantic function, as presented in
Table 1.
The particle se is an integral part of pronominal
verbs in the same way as the particles of English
phrasal verbs. As future work, we would like to in-
vestigate possible semantic contributions of the se
particle to the meaning of pronominal verbs, as done
by Cook and Stevenson (2006), for example, who try
to automatically classify the uses of the particle up in
verb-particle constructions. Like in the present pa-
per, they estimate a set of linguistic features which
are in turn used to train a Support Vector Machine
(SVM) classifier citecook:2006:mwe.
3 Methodology
For the automatic identification of multiword
verb+se occurrences, we performed corpus searches
on the PLN-BR-FULL corpus (Muniz et al, 2007),
which consists of news texts extracted from a ma-
jor Brazilian newspaper, Folha de Sa?o Paulo, from
1994 to 2005, with 29,014,089 tokens. The cor-
pus was first preprocessed for sentence splitting,
case homogenization, lemmatization, morphologi-
cal analysis and POS tagging using the PALAVRAS
parser (Bick, 2000). Then, we executed the corpus
searches using the mwetoolkit (Ramisch et al,
2010). The tool allowed us to define two multilevel
word patterns, for proclitic and enclitic cases, based
on surface forms, morphology and POS. The pat-
terns covered all the verbs in third person singular
(POS=V*, morphology=3S) followed/preceded by
the clitic pronoun se (surface form=se, POS=PERS).
The patterns returned a set of se occurrences, that
is, for each verb, a set of sentences in the corpus in
which this verb is followed/preceded by the clitic se.
In our analysis, we looked at all the verbs tak-
ing an enclitic se, that is, where the clitic se is at-
tached after the verb. We could as well have in-
cluded the occurrences of verbs with a proclitic se
(clitic attached before the verb). However, we sus-
pected that this would increase the number of occur-
rences (sentences) to analyze without a proportional
increase in verb lemmas. Indeed, our search for pro-
clitic se occurrences returned 40% more verb lem-
mas and 264% more sentences than for the enclitic
se (59,874 sentences), thus confirming our hypothe-
sis. Moreover, as we could see at a first glance, pro-
clitic se results included se conjunctions erroneously
tagged as pronouns (when the parser fails the cate-
gorial disambiguation). This error does not occur
when the pronoun is enclitic because Portuguese or-
thographic rules require a hyphen between the verb
and the clitic when se is enclitic, but never when it
is proclitic.
We decided to look at sentences as opposed to
looking only at candidate verb lemmas, because we
did not trust that our intuition as native speakers
would be sufficient to identify all the uses of the
clitic se for a given verb, specially as some verbs
allow more than one of the six uses we listed herein.
For performing the annotation, we used a table
with the verb lemmas in the lines and a column for
each one of the six uses of se as a clitic pronoun.
Working with two screens (one for the table and the
other for the sentences), we read the sentences and,
once a new use was verified, we ticked the appro-
priate column. This annotation setup accelerated the
analyses, as we only stopped the reading when we
identified a new use. The annotation was performed
manually by a linguist, expert in semantics of Por-
tuguese verbs, and also an author of this paper.
After having summarized the results obtained
from corpus analysis, we realized that some cliti-
cized verb uses that we know as native speakers did
not appear in the corpus (mainly reflexive and recip-
rocal uses). In these cases, we added a comment on
our table which indicates the need to look for the use
95
in another corpus aiming to confirm it.
For example, the most frequent cliticized verb,
tratar-se has no occurrence with the meaning of to
take medical treatment. We checked this meaning in
another corpus and found one example: O senador
se tratou com tecido embriona?rio. . . (*The senator
treated himself with embryonic tissue. . . ), proving
that our intuition may help us to improve the results
with specific corpus searches. A comparative multi-
corpus extension of the present study is planned as
future work.
The strategy we adopted to analyze the sentences
in order to identify pronominal verbs was to make a
series of questions to rule out the other possible se
uses.
Question 1 Does the se particle function as a
marker of PASSIVE voice or SUBJECT INDETERMI-
NATION?
In order to answer this question, it is important to
know that both uses involve the suppression of the
external argument of the verb. The difference is that,
in the pronominal PASSIVE voice, the remaining NP
(noun phrase) is shifted to the subject position (and
the verb must then be inflected according to such
subject), whereas in SUBJECT INDETERMINATION,
the remaining argument, always a PP (prepositional
phrase), remains as an indirect object. For example:
? Pronominal PASSIVE voice:
Fizeram-se va?rias tentativas.
*Made-SE several trials.
Several trials were made.
? SUBJECT INDETERMINATION:
Reclamou-se de falta de hygiene.
*Complained-SE about the lack of hygiene.
One has complained about the lack of hygiene.
Question 2 Is it possible to substitute se for a si
mesmo (-self )?
If so, it is a case of REFLEXIVE use. A clue for
this is that it is always possible to substitute se for
another personal pronoun, creating a non-reflexive
use keeping the same subject. For example:
? Ele perguntou-se se aquilo era certo.
He asked himself whether that was correct.
? Ele perguntou-me se aquilo era certo.
He asked me whether that was correct.
Question 3 Is it possible to substitute se for um ao
outro (each other)?
If so, it is a case of RECIPROCAL use. A clue for
this interpretation is that, in this case, the verb is al-
ways in plural form as the subject refers to more than
one person. RECIPROCAL uses were not included in
the corpus searches, as we only looked for cliticized
verbs in third person singular. However, aiming to
gather data for future work, we have ticked the table
every time we annotated sentences of a verb that ad-
mits reciprocal use. The reciprocal use of such verbs
have been later verified in other corpora.
? Eles se beijaram.
They kissed each other.
Question 4 Has the verb, without se, a transi-
tive use? If so, are the senses related to causative-
inchoative alternation? In other words, is the mean-
ing of the transitive use to cause X become Y?
If so, it is a case of INCHOATIVE use, for example:
? A porta abriu-se.
The door opened.
Compare with the basic transitive use:
? Ele abriu a porta.
He opened the door.
It is important to mention that verbs which allow
causative-inchoative alternation in Portuguese may
not have an equivalent in English that allows this al-
ternation, and vice-versa. For example, the inchoa-
tive use of the verb tornar corresponds to the verb
to become and the causative use corresponds to the
verb to make:
? Esse fato tornou-se conhecido em todo o
mundo.
This fact became known all around the world.
? A imprensa tornou o fato conhecido em todo o
mundo.
The press made the fact known all around the world.
If the verb being analyzed failed the four tests, the
clitic se has neither semantic nor syntactic function
and is considered a CONSTITUTIVE PARTICLE of the
verb, for example:
96
? Ele vangloriou-se de seus talentos.
He boasted of his talents.
Therefore, we made the identification of pronom-
inal verbs based on the negation of the other possi-
bilities.
4 Discussion
The corpus search resulted in 22,618 sentences of
cliticized verbs, corresponding to 1,333 verb lem-
mas. Some verbs allow only one of the uses of
the clitic se (unambiguous cliticized verbs), whereas
others allow more than one use (ambiguous cliti-
cized verbs), as shown in Table 2. Therefore, a
lexicon can only disambiguate part of the cliticized
verbs (others need additional features to be disam-
biguated).
The analysis of the verbs? distribution reveals that
10% of them (133) account for 73% of the sentences.
Moreover, among the remaining 90% verb lemmas,
there are 477 hapax legomena, that is, verbs that oc-
cur only once. Such distribution indicates that com-
putational models which focus on very frequently
cliticized verbs might significantly improve NLP ap-
plications.
Contrary to our expectations, very frequently
cliticized verbs did not necessarily present high pol-
ysemy. For example, the most frequent verb of our
corpus is tratar, with 2,130 occurrences. Although
tratar-se has more than one possible use, only one
appeared in the corpus, as a marker of SUBJECT IN-
DETERMINATION, for example:
? Trata-se de uma nova tende?ncia.
It is the case of a new tendency.
Despite being very frequent, when we search for
translations of tratar-se de in bilingual (parallel)
Portuguese-English corpora and dictionaries avail-
able on the web,4,5,6 we observed that there are sev-
eral solutions to convey this idea in English (deter-
mining a subject, as English does not allow subject
omission). Six examples extracted from the Com-
para corpus illustrate this fact:
4http://www.linguateca.pt/COMPARA/
5http://www.linguee.com.br/
portugues-ingles
6http://pt.bab.la/dicionario/
portugues-ingles
se uses Unamb. Amb. Total
SUBJECT INDE-
TERMINATION
17 6 23
PASSIVE
467 630 1097
REFLEXIVE
25 333 358
INCHOATIVE
190 64 254
RECIPROCAL
0 33 33
CONSTITUTIVE
PARTICLE
83 104 187
Total 782 1170 1952
Table 2: Proportion of unambiguous (Unamb.) and am-
biguous (Amb.) verbs that allow each se use.
? Trata-se de recriar o pro?prio passado.
It?s a question of re-creating your own past.
? Mas o assunto era curioso, trata-se do casa-
mento, e a viu?va interessa-me.
But the subject was a curious one; it was about her
marriage, and the widow interests me.
? Na?o ha? mais du?vidas, trata-se realmente de um
louco.
There?s no longer any doubt; we?re truly dealing
with a maniac.
? Trata-se realmente de uma emerge?ncia, Sr.
Hoffman.
This really is a matter of some urgency, Mr Hoff-
man.
? Trata-se de um regime repousante e civilizado.
It is a restful, civilized re?gime.
? Trata-se de um simples caso de confusa?o de
identidades, dizem voce?s.
(??) Simple case of mistaken identity.
In what concerns specifically pronominal verbs,
our analysis of the data showed they are of three
kinds:
1. Verbs that are used exclusively in pronominal
form, as abster-se (to abstain). This does not
mean that the pronominal form is unambigu-
ous, as we found some pronominal verbs that
present more than one sense, as for example the
verb referir-se, which means to refer or to con-
cern, depending on the subject?s animacy status
[+ human] or [? human], respectively;
97
2. Verbs that have a non-pronominal and a pro-
nominal form, but both forms are not related,
e.g.: realizar (to make or to carry on, which
allows the passive alternation realizar-se); and
the pronominal form realizar-se (to feel ful-
filled);
3. Verbs that have pronominal form, but accept
clitic drop in some varieties of Portuguese
without change of meaning, as esquecer-se and
esquecer (both mean to forget)
We did not study the clitic drop (3), but we un-
covered several pronominal verbs of the second kind
above (2). The ambiguity among the uses of se in-
creases with such cases. The verb desculpar (to
forgive), for example, allows the REFLEXIVE use
desculpar-se (to forgive oneself ), but also consti-
tutes a pronominal verb: desculpar-se (to apolo-
gize). The verb encontrar (to find) allows the RE-
FLEXIVE use (to find oneself, from a psychological
point of view) and the PASSIVE use (to be found).
The same verb also constitutes a pronominal verb
which means to meet (1) or functions as a copula
verb, as to be (2):
1. Ele encontrou-se com o irma?o.
He met his brother.
2. Ele encontra-se doente.
He is ill.
In most sentences of cliticized verbs? occurrences,
it is easy to observe that, as a rule of thumb:7
? SUBJECT INDETERMINATION uses of se do not
present an NP before the verb, present a PP af-
ter the verb and the verb is always inflected in
the third person singular;
? PASSIVE uses of se present an NP after the verb
and no NP before the verb;
? INCHOATIVE uses of se present an NP before
the verb and almost always neither a PP nor a
NP after the verb;
? CONSTITUTIVE PARTICLE uses of se present
an NP before the verb and a PP after the verb;
7Syntactic clues do not help to identify REFLEXIVE verbs.
The distinction depends on the semantic level, as the reflexive
use requires a [+ animate] subject to play simultaneously the
roles of agent and patient.
? RECIPROCAL uses of se only occur with verbs
taking a plural inflection.
Problems arise when a sentence follows none of
these rules. For example, subjects in PASSIVE use
of se usually come on the right of the verb. Thus,
when the subject appears before the verb, it looks, at
a first glance, to be an active sentence. For example:
? O IDH baseia-se em dados sobre renda, esco-
laridade e expectativa de vida.
*The HDI bases-SE on income, education and life
expectancy data.
The HDI is based on income, education and life ex-
pectancy data.
These cases usually occur with stative passives
(see Rosa?rio Ribeiro (2011, p. 196)) or with ditran-
sitive action verbs8 when a [? animate] NP takes
the place usually occupied by a [+ animate] NP. Se-
mantic features, again, help to disambiguate and to
reveal a non-canonical passive.
The opposite also occurs, that is, the subject, usu-
ally placed on the left of the verb in active voice,
appears on the right, giving to the sentence a false
passive appearance:
? Desesperaram-se todos os passageiros.
*Fell-SE into despair all the passengers.
All the passengers fell into despair.
Sometimes the meaning distinctions of a verb are
very subtle, making the matter more complex. In
the following sections, we comment two examples
of difficult disambiguation.
4.1 Distinguishing Pronominal PASSIVE Voice
from Pronominal Verbs
The verb seguir (to follow) conveys the idea of obey-
ing when it has a [+ human] subject in the active
voice (an agent). The passive voice may be con-
structed using se, like in (2). Additionally, this verb
has a pronominal active use, seguir-se, which means
to occur after, as shown in (3):
1. Active voice:
? [Eles]Agent seguem [uma se?rie de conven-
c?o?es]Theme - thing followed.
They follow a series of conventions.
8Ditransitive verbs take two internal arguments: an NP as
direct object and a PP as indirect object.
98
2. PASSIVE voice:
? Segue-se [uma se?rie de conven-
c?o?es]Theme - thing followed.
A series of conventions are followed.
3. Pronominal verb ? active voice:
? [A queda]Theme - thing occurring after seguiu-
se [a` divulgac?a?o dos dados de desemprego
em o pa??s]Theme - thing occurring before.
The drop followed the announcement of unem-
ployment figures in the country.
The preposition a introducing one of the argu-
ments in (3) distinguishes the two meanings, as the
PASSIVE voice presents an NP and not a PP imme-
diately after or before the verb.
4.2 Distinguishing REFLEXIVE, INCHOATIVE
and PASSIVE Uses
The verb transformar, when cliticized, may be in-
terpreted as a PASSIVE (to be transformed), as a RE-
FLEXIVE (to transform oneself ) or as an INCHOA-
TIVE use (to become transformed). The PASSIVE
voice is identified by the subject position, after the
verb (1). The difference between the REFLEXIVE (2)
and INCHOATIVE (3) uses, on its turn, is a semantic
feature: only a [+ human] subject may act to be-
come something (REFLEXIVE use):
1. PASSIVE:
Transformou-se o encontro em uma
grande festa.
The meeting was transformed into a big party.
2. REFLEXIVE:
? A mulher jovem transformou-se em uma
pessoa sofisticada.
The young woman transformed herself into a
sophisticated person.
3. INCHOATIVE:
? O encontro transformou-se em uma gran-
de festa.
The meeting transformed into a big party.
5 Conclusions and Future Work
The lexicon gathered through this research will par-
tially enable disambiguating the uses of the clitic
pronoun se, as there are several verbs that allow only
one of the se clitic uses. For the other verbs, whose
polysemy entails more than one possible use of se, it
is necessary to add further information on each verb
sense.
The analysis we reported here evidenced the need
for enriching Portuguese computational lexicons,
encompassing (a) the semantic role labels assigned
by each verb sense, (b) the selectional restrictions
a verb imposes to its arguments, and (c) the alter-
nations a verb (dis)allows. The semantic predicate
decomposition used by Levin (1993) has proved to
be worthy to formalize the use of se in reflexive con-
structions (Godoy, 2012) and we think it should be
adopted to describe other uses of the pronoun se.
Another alternative is to construct a detailed com-
putational verb lexicon along the lines suggested
by Gardent et al (2005), based on Maurice Gross?
lexicon-grammar.
The data generated by this study can also be used
to automatically learn classifiers for ambiguous uses
of the clitic se. On the one hand, the annotation
of uses can be semi-automatically projected on the
sentences extracted from the corpus. On the other
hand, the findings of this work in terms of syntac-
tic and semantic characteristics can be used to pro-
pose features for the classifier, trying to reproduce
those that can be automatically obtained (e.g., sub-
categorization frame) and to simulate those that can-
not be easily automated (e.g., whether the subject
is animate). For these future experiments, we in-
tend to compare different learning models, based on
SVM and on sequence models like conditional ran-
dom fields (Vincze, 2012).
As languages are different in what concerns al-
lowed alternations, the use of clitic se in Portuguese
becomes even more complex when approached from
a bilingual point of view. Depending on how differ-
ent the languages compared are, the classification of
se adopted here may be of little use. For example,
several verbs classified as reflexive in Portuguese,
like vestir-se (to dress), barbear-se (to shave) and
demitir-se (to resign) are not translated into a re-
flexive form in English (*to dress oneself, *to shave
oneself and *to dismiss oneself ). Similarly, typical
inchoative verb uses in Portuguese need to be trans-
lated into a periphrasis in English, like surpreender-
se (to be surprised at), orgulhar-se (to be proud of )
and irritar-se (to get angry). Such evidences lead
99
us to conclude that it would be useful to count on
a bilingual description not only of pronominal, but
also of the other se uses.
The results of this work are available at www.
nilc.icmc.usp.br/portlex.
Acknowledgments
This study was funded by FAPESP (process
2011/22337-1) and by the CAMELEON project
(CAPES-COFECUB 707-11).
References
Eckhard Bick. 2000. The parsing system Palavras.
Aarhus University Press. 411 p.
Paul Cook and Suzanne Stevenson. 2006. Classifying
particle semantics in English verb-particle construc-
tions. In Proceedings of MWE 2006, pages 45?53,
Sydney, Australia.
Sonia Maria Lazzarino Cyrino. 2007. Construc?o?es com
SE e promoc?a?o de argumento no portugue?s brasileiro:
Uma investigac?a?o diacro?nica. Revista da ABRALIN,
6:85?116.
Paula Fonseca. 2010. Os verbos pseudo-reflexos em
Portugue?s Europeu. Master?s thesis, Universidade do
Porto.
Claire Gardent, Bruno Guillaume, Guy Perrier, and In-
grid Falk. 2005. Maurice gross? grammar lexicon and
natural language processing. In Proceedings of the
2nd Language and Technology Conference, Poznan?,
Poland.
Luisa Andrade Gomes Godoy. 2012. A reflexivizac?a?o no
PB e a decomposic?a?o sema?ntica de predicados. Ph.D.
thesis, Universidade Federal de Minas Gerais.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: a preliminary investigation. The University of
Chicago Press, Chicago, USA.
Ronaldo Teixeira Martins, Gisele Montilha, Lucia He-
lena Machado Rino, and Maria da Grac?a Volpe Nunes.
1999. Dos modelos de resoluc?a?o da ambiguidade cat-
egorial: o problema do SE. In Proceedings of IV
Encontro para o Processamento Computacional da
L??ngua Portuguesa Escrita e Falada (PROPOR 1999),
pages 115?128, E?vora, Portugal, September.
Jairo Morais Nunes. 1990. O famigerado SE: uma
ana?lise sincro?nica e diacro?nica das construc?o?es com
SE apassivador e indeterminador. Master?s thesis,
Universidade Estadual de Campinas.
Marcelo Muniz, Fernando V. Paulovich, Rosane
Minghim, Kleber Infante, Fernando Muniz, Renata
Vieira, and Sandra Alu??sio. 2007. Taming the tiger
topic: an XCES compliant corpus portal to generate
subcorpus based on automatic text topic identification.
In Proceedings of The Corpus Linguistics Conference
(CL 2007), Birmingham, UK.
Pablo Nunes-Ribeiro. 2010. A alterna?ncia causativa
no Portugue?s do Brasil: a distribuic?a?o do cl??tico SE.
Master?s thesis, Universidade Federal do Rio Grande
do Sul.
Jose? Ricardo Pereira-Santos. 2010. Alterna?ncia pas-
siva com verbos transitivos indiretos do portugue?s do
Brasil. Master?s thesis, Universidade de Bras??lia.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. Multiword expressions in the wild?
the mwetoolkit comes in handy. In Proceedings of
the 23rd COLING (COLING 2010) - Demonstrations,
pages 57?60, Beijing, China.
S??lvia Isabel do Rosa?rio Ribeiro. 2011. Estruturas
com ?se? Anafo?rico, Impessoal e Decausativo em Por-
tugue?s. Ph.D. thesis, Faculdade de Letras da Universi-
dade de Coimbra.
Tal Siloni. 2001. Reciprocal verbs. In Online Proceed-
ings of IATL 17, Jerusalem, Israel.
Milena Slavcheva. 2006. Semantic descriptors: The
case of reflexive verbs. In Proceedings of LREC 2006,
pages 1009?1014, Genoa, Italy.
Veronika Vincze. 2012. Light verb constructions in the
szegedparalellFX English?Hungarian parallel corpus.
In Proceedings of LREC 2012, Istanbul, Turkey.
100
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 342?347,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Exploring Consensus in Machine Translation for Quality Estimation
Carolina Scarton and Lucia Specia
Department of Computer Science, University of Sheffield
Regent Court, 211 Portobello, Sheffield, S1 4DP, UK
{c.scarton,l.specia}@sheffield.ac.uk
Abstract
This paper presents the use of consensus
among Machine Translation (MT) systems
for the WMT14 Quality Estimation shared
task. Consensus is explored here by com-
paring the MT system output against sev-
eral alternative machine translations using
standard evaluation metrics. Figures ex-
tracted from such metrics are used as fea-
tures to complement baseline prediction
models. The hypothesis is that knowing
whether the translation of interest is simi-
lar or dissimilar to translations from multi-
ple different MT systems can provide use-
ful information regarding the quality of
such a translation.
1 Introduction
While Machine Translation (MT) evaluation met-
rics can rely on the similarity of the MT system
output to reference (human) translations as a proxy
to quality assessment, this is not possible for MT
systems in use, translating unseen texts. Quality
Estimation (QE) metrics are used in such settings
as a way of predicting translation quality. While
reference translations are not available for QE,
previous work has explored the so called pseudo-
references (Soricut and Echihabi, 2010; Soricut et
al., 2012; Soricut and Narsale, 2012; Shah et al.,
2013). Pseudo-references are alternative transla-
tions produced by MT systems different from the
system that we intend to predict quality for (Al-
brecht and Hwa, 2008). These can be used to pro-
vide additional features to train QE models. Such
features are normally figures resulting from au-
tomatic metrics (such as BLEU, Papineni et al.
(2002)) computed between pseudo-references and
the output of the given MT system.
Soricut and Echihabi (2010) explore pseudo-
references for document-level QE prediction to
rank outputs from an MT system. The pseudo-
references-based features are BLEU scores ex-
tracted by comparing the output of the MT sys-
tem under investigation and the output of an off-
the-shelf MT system, for both the target and the
source texts. The statistical MT system training
data is also used as pseudo-references to compute
training data-based features. The use of pseudo-
references has been shown to outperform strong
baseline results. Soricut and Narsale (2012) pro-
pose a method that uses sentence-level prediction
models for document-level QE. They also use a
pseudo-references-based feature (based in BLEU)
and claim that this feature is one of the most pow-
erful in the framework.
For QE at sentence-level, Soricut et al. (2012)
use BLEU based on pseudo-references combined
with other features to build the best QE system of
the WMT12 QE shared task.
1
Shah et al. (2013)
use pseudo-references in the same way to ex-
tract a BLEU feature for sentence-level prediction.
Feature analysis on a number of datasets showed
that this feature contributed the most across all
datasets.
Louis and Nenkova (2013) apply pseudo-
references for summary evaluation. They use six
systems classified as ?best systems?, ?mediocre
systems? or ?worst systems? to make the compar-
ison, with ROUGE (Lin and Och, 2004) as quality
score. They also experiment with a combination of
the ?best systems? and the ?worst systems?. The
use of only ?best systems? led to the best results.
Examples of ?bad summaries? are said not to be
very useful because a summary close to the worst
systems outputs can mean that either it is bad or
it is too different from the best systems outputs in
terms of content. Albrecht and Hwa (2008) use
pseudo-references to improve MT evaluation by
combining them with a single human reference.
They show that the use of pseudo-references im-
1
http://www.statmt.org/wmt12/
342
proves the correlation with human judgements.
Soricut and Echihabi (2010) claim that pseudo-
references should be produced by systems as dif-
ferent as possible from the MT system of interest.
This ensures that the similarities found among the
systems? translations are not related to the similar-
ities of the systems themselves. Therefore, the as-
sumption that a translation from system X shares
some characteristics with a translation from sys-
tem Y is not a mere coincidence. Another way to
make the most of pseudo-references is to use an
MT system known as generally better (or worse)
than the MT system of interest. In that case, the
comparison will lead to whether the MT system of
interest is similar to a good (or bad) MT system.
However, in most scenarios it is difficult to rely
on the average translation quality of a given sys-
tem as an absolute indicator of its quality. This
is particularly true for sentence-level QE, where
the quality of a given system can vary signifi-
cantly across sentences. Finding translations from
MT systems that are considerably different can
also be a challenge. In this paper we exploit
pseudo-references in a different way: measuring
the consensus among different MT systems in the
translations they produce. As sources of pseudo-
references, we use translations given in a multi-
translation dataset or those produced by the par-
ticipants in the WMT translation task for the same
data. While some MT systems can be similar
to each other, for some language pairs, such as
English-Spanish, a wide range of MT systems
with different average qualities are available. Our
hypothesis is that by using translations from sev-
eral MT systems we can find consensual infor-
mation (even if some of the systems are similar
to the one of interest). The use of more than one
MT system is expected to smooth out the effect
of ?coincidences? in the similarities between sys-
tems? translations.
This paper describes the use of consensual
information for the WMT14 QE shared task
(USHEFF-consensus system), simulating a sce-
nario where we do not know the quality of the
pseudo-references, nor the characteristics of any
MT systems (the system of interest or the systems
which generated the pseudo-references). We par-
ticipated in all variants of Task 1, sentence-level
QE, for both for scoring and ranking. Section 2
explains how we extracted consensual information
for all tasks. Section 3 shows our official results
compared to the baselines provided. Section 4
presents some conclusions.
2 Consensual information extraction
The consensual information is exploited in two
different ways in Task 1. Task 1.1 used?perceived?
post-editing effort labels as quality scores for scor-
ing and ranking in four languages pairs. These la-
bels vary within [1-3], where:
? 1 = perfect translation
? 2 = near miss translation (sentences with 2-3
errors that are easy to fix)
? 3 = very low quality sentence.
The training and test sets for each language
pair in Task 1.1 contain 3-4 translations of the
same source sentences. The language pairs are
German-English (DE-EN) with 150 source sen-
tences for test and 350 source sentences for train-
ing, English-German (EN-DE) with 150 source
sentences for test and 350 source sentences
for training, English-Spanish (EN-ES) with 150
source sentences for test and 954 source sentences
for training, and Spanish-English (ES-EN) with
150 source sentences for test and 350 source sen-
tences for training. The translations for each lan-
guage pair include a human translation and trans-
lations produced by a statistical MT (SMT) sys-
tem, a rule-based MT (RBMT) system, and a hy-
brid system (for the EN-DE and EN-ES language
pairs only).
By inspecting the source side of the training set,
we noticed that the translations were ordered per
systems, since the source file had sentences re-
peated in batches. For example, the EN-ES lan-
guage pair had 954 English sentences and 3,816
Spanish sentences. In the source file, the English
sentences were repeated in batches of 954 sen-
tences. Based on that, we assumed that in the tar-
get file each set of 954 translations in sequence
corresponded to a given MT system (or human).
For each system (human translation is consid-
ered as a system, since we do not know the or-
der of the translations), we calculate the consen-
sual information considering the other 2-3 systems
available as pseudo-references.
The quality scores for Task 1.2 and Task 1.3
were computed as HTER (Human Translation Er-
ror Rate (Snover et al., 2006)) and post-editing
time, respectively, for both scoring and ranking.
343
The datasets were a mixture of test sets from the
WMT13 and WMT12 translation shared tasks for
the EN-ES language pair only. In this case, the
consensual information was extracted by using
systems submitted to the WMT translation shared
tasks of both years. Therefore, for each source
sentence in the WMT12/13 data, all translations
produced by the participating MT systems of that
year were used as pseudo-references. The uedin
system outputs for both WMT13 and WMT12
were not considered, since the datasets in Tasks
1.2 and 1.3 were created from translations gener-
ated by this system.
2
The Asyia Toolkit
3
(Gim?enez and M`arquez,
2010) was used to extract the automatic metrics
considered as features. BLEU, TER (Snover et
al., 2006), METEOR (Banerjee and Lavie, 2005)
and ROUGE (Lin and Och, 2004) are used in
all task variants. For Tasks 1.2 and 1.3 we also
use metrics based on syntactic similarities from
shallow and dependency parser information (met-
rics SPOc(*) and DPmHWCM c1, respectively, in
Asyia). BLEU is a precision-oriented metric that
compares n-grams (n=1-4 in our case) from refer-
ence documents against n-grams of the MT out-
put, measuring how close the output of a system
is to one or more references. TER (Translation
Error Rate) measures the minimum number of ed-
its required to transform the MT output into the
closest reference document. METEOR (Metric
for Evaluation of Translation with Explicit OR-
dering) scores MT outputs by aligning them with
given references. This alignment can be done by
exact, stem, synonym and paraphrases matching
(here, exact matching was used). ROUGE is a
recall-oriented metric that measures similarity be-
tween sentences by considering the longest com-
mon n-gram statistics between a translation sen-
tence and the corresponding reference sentence.
SPOc(*) measures the lexical overlap according to
the chunk types of the syntactic realisation. The
?*? means that an average of all chunk types is
computed. DPmHWCM c1 is based on the match-
ing of head-word chains. We considered the match
of grammatical categories of only one head-word.
These consensual features are combined with
the 17 QuEst baseline features provided by the
shared task organisers.
2
WMT14 QE shared task organisers, personal communi-
cation, March 2014.
3
http://asiya.lsi.upc.edu/
3 Experiments and Results
The results reported herein are the official shared
task results, that is, they were computed using the
true scores of the test set made available by the
organisers after our submission.
For training the QE models, we used Sup-
port Vector Machines (SVM) regression algorithm
with a radial basis function (RBF) kernel with
the hyperparameters optimised via grid search.
The scikit-learn algorithm available in the QuEst
Framework
4
(Specia et al., 2013) was used for
that.
We compared the results obtained against using
only the QuEst baseline (BL) features, which is
the same system used as the official baseline for
the shared task. For the scoring variant we also
compare our results against a baseline that ?pre-
dicts? the average of the true scores of the train-
ing set as scores for each sentence of the test set
(Mean ? each sentence has the same predicted
score).
For all language pairs in Task 1.1, Table 1 shows
the average results for the scoring variant using
MAE (Mean Absolute Error) as evaluation met-
ric, while Table 2 shows the results for the ranking
variant using DeltaAvg.
The results for scoring improved over the base-
lines with the use of consensual information for
language pairs DE-EN and EN-ES. For EN-DE
and ES-EN the consensual features achieved simi-
lar results to BL. The best result for consensual in-
formation features was achieved with EN-ES (0.03
of MAE difference from BL).
For the ranking variant, the consensual informa-
tion improved the results for all language pairs.
The largest improvement from consensual-based
features was achieved for ES-EN, with a differ-
ence of 0.11 from the baseline. It is worth men-
tioning that for ES-EN our system achieved the
best ranking result in Task 1.1.
Since the results varied for different languages
pairs, we further inspected them for each language
pair. First, we looked at the true scores distribution
and realised that the first batch of translations for
each language pair was probably the human refer-
ence since the percentage of 1s ? the best quality
score ? was much higher for this system (see Fig-
ure 1 for EN-DE as an example). By using this
human translation as a reference for the other MT
systems, we computed BLEU for each sentence
4
http://www.quest.dcs.shef.ac.uk/
344
DE-EN EN-DE EN-ES ES-EN
Mean 0.67 0.68 0.46 0.58
BL 0.65 0.64 0.52 0.57
BL+Consensus 0.63 0.64 0.49 0.57
Table 1: Scoring results for Task 1.1 in terms of MAE
DE-EN EN-DE EN-ES ES-EN
BL 0.21 0.23 0.14 0.12
BL+Consensus 0.28 0.26 0.21 0.23
Table 2: Ranking results for Task 1.1 in terms of DeltaAvg
and averaged these values. The results are shown
in Table 3.
For DE-EN, EN-DE and EN-ES, the various
systems appeared to be less dissimilar in terms
of BLEU, when compared to ES-EN. For ES-EN,
the difference between the two MT systems was
higher than for other language pairs (0.12 for the
test set and 0.11 for the training set). Moreover,
for DE-EN, EN-DE and EN-ES, the difference be-
tween the averaged BLEU score of the training set
and the average BLEU score of the test set is very
small (smaller than 0.01). For ES-EN, however,
the difference between the scores for the training
and test sets was also higher (0.04 for System1 and
0.03 for System2). This can be one reason why the
consensual features did not show improvements
for this language pair. Since the systems are con-
siderably different and also there is a considerable
difference between training and test sets, the data
can be too noisy to be used as pseudo-references.
For EN-DE, the reasons for the bad perfor-
mance of consensual features are not clear. This
language pair showed the worst average quality
scores for all systems. Reasons for this can include
characteristics of German language, such as com-
pound words which are not well treated in MT, and
complex grammar. One hypothesis is that these
low BLEU scores (as Table 3 shows) introduce
noise instead of useful information for QE. An-
other difference that appeared only in EN-DE was
the distributions of the scores across the different
systems. As Figure 1 shows, System1 has a dis-
tribution considerably different from the other two
systems. For the other language pairs, the distribu-
tions across different systems were more uniform.
This difference can be another factor influencing
the results for this language pair.
Table 4 shows the results for scoring (MAE) and
Table 5 shows the results for ranking (DeltaAvg)
for Tasks 1.2 and 1.3.
Task 1.2 Task 1.3
Mean 16.93 23.34
BL 15.23 21.49
BL+Consensus 13.61 21.48
Table 4: Scoring results of Tasks 1.2 and 1.3 in
terms of MAE
Task 1.2 Task 1.3
BL 5.08 14.71
BL+Consensus 7.93 14.98
Table 5: Ranking results of Tasks 1.2 and 1.3 in
terms of DeltaAvg
For Tasks 1.2 and 1.3 the use of consensual
information only slightly improved the baseline
results for scoring. For the ranking variant,
BL+Consensus achieved better results, but only
significantly so for Task 1.2. Therefore, consen-
sual information seems useful to rank sentences
according to predicted HTER, its contribution to
predicting actual HTER is not noticeable. For
post-editing time as quality labels, the improve-
ment achieved with the use of consensual infor-
mation was marginal.
4 Conclusions
The use of consensual information of MT systems
can be useful to improve state-of-the-art results for
QE. For some scenarios, it is possible to acquire
several translations for a given source segment,
but with no additional information on the qual-
ity or type of MT systems used to produce them.
Therefore, these translations could not be used as
pseudo-references in the same way as in (Soricut
and Echihabi, 2010).
345
DE-EN EN-DE EN-ES ES-EN
Sys1 Sys2 Sys1 Sys2 Sys3 Sys1 Sys2 Sys3 Sys1 Sys2
Average BLEU
(test) 0.31 0.25 0.20 0.19 0.21 0.36 0.29 0.32 0.44 0.32
Average BLEU
(training) 0.31 0.26 0.21 0.18 0.22 0.35 0.29 0.31 0.40 0.29
Table 3: Average BLEU of systems in Task 1.1
Figure 1: Distribution of true quality scores for the EN-DE language pair
The use of several references with the hypoth-
esis that they share consensual information has
been shown useful in some settings, particularly
in Task 1.1. In others, the results were inconclu-
sive. In particular, the approach does not seem ap-
propriate for scenarios where the MT systems are
considerably different (as shown in Table 3). In
those cases, better ways to exploit consensual in-
formation need to be investigated further.
Acknowledgements: This work was supported
by the EXPERT (EU Marie Curie ITN No.
317471) project.
References
Joshua S. Albrecht and Rebecca Hwa. 2008. The
role of pseudo references in mt evaluation. In Pro-
ceedings of WMT 2008, pages 187?190, Columbus,
Ohio, USA.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the ACL 2005Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Jes?us Gim?enez and Llu??s M`arquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Chin-Yew Lin and Franz J. Och. 2004. Automatic
Evaluation of Machine Translation Quality Using
Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL 2004, Barcelona,
Spain.
Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39(2):267?
300, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL
2002, pages 311?318, Philadelphia, USA.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
the XIV MT Summit, pages 167?174, Nice, France.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA 2006, pages 223?231.
Radu Soricut and Abdessamad Echihabi. 2010.
TrustRank: Inducing Trust in Automatic Transla-
346
tions via Ranking. In Proceedings of the ACL 2010,
pages 612?621, Uppsala, Sweden.
Radu Soricut and Sushant Narsale. 2012. Combin-
ing Quality Prediction and System Selection for Im-
proved Automatic Translation Output. In Proceed-
ings of WMT 2012, Montreal, Canada.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
WMT 2012, Montreal, Canada.
Lucia Specia, Kashif Shah, Jose G.C. de Souza, and
Trevor Cohn. 2013. Quest - a translation quality es-
timation framework. In Proceedings of WMT 2013:
System Demonstrations, ACL-2013, pages 79?84,
Sofia, Bulgaria.
347
