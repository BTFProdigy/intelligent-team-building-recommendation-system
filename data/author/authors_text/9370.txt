Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 783?790, Prague, June 2007. c?2007 Association for Computational Linguistics
Active Learning for Word Sense Disambiguation with Methods for    
Addressing the Class Imbalance Problem 
Jingbo Zhu 
University of Southern California 
Information Sciences Institute 
Northeastern University, P.R.China 
Natural Language Processing Lab 
Zhujingbo@mail.neu.edu.cn 
Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu 
 
 
Abstract 
In this paper, we analyze the effect of 
resampling techniques, including under-
sampling and over-sampling used in active 
learning for word sense disambiguation 
(WSD). Experimental results show that un-
der-sampling causes negative effects on ac-
tive learning, but over-sampling is a rela-
tively good choice. To alleviate the within-
class imbalance problem of over-sampling, 
we propose a bootstrap-based over-
sampling (BootOS) method that works bet-
ter than ordinary over-sampling in active 
learning for WSD. Finally, we investigate 
when to stop active learning, and adopt two 
strategies, max-confidence and min-error, 
as stopping conditions for active learning. 
According to experimental results, we sug-
gest a prediction solution by considering 
max-confidence as the upper bound and 
min-error as the lower bound for stopping 
conditions. 
1 Introduction 
Word sense ambiguity is a major obstacle to accu-
rate information extraction, summarization, and 
machine translation (Ide and Veronis, 1998). In 
recent years, a variety of techniques for machine 
learning algorithms have demonstrated remarkable 
performance for automated word sense disam-
biguation (WSD) (Chan and Ng, 2006; Dagan et. 
al., 2006; Xue et. al., 2006; Kohomban and Lee. 
2005; Dang and Palmer, 2005), when enough la-
beled training data is available. However, creating 
a large sense-tagged corpus is very expensive and 
time-consuming, because these data have to be an-
notated by human experts.  
Among the techniques to solve the knowledge 
bottleneck problem, active learning is a promising 
way (Lewis and Gale, 1994; McCallum and Ni-
gram, 1998). The purpose of active learning is to 
minimize the amount of human labeling effort by 
having the system automatically select for human 
annotation the most informative unannotated case.  
In real-world data, the distribution of the senses 
of a word is often very skewed. Some studies re-
ported that simply selecting the predominant sense 
provides superior performance, when a highly 
skewed sense distribution and insufficient context 
exist (Hoste et al, 2001; McCarthy et. al., 2004). 
The data set is imbalanced when at least one of the 
senses is heavily underrepresented compared to the 
other senses. In general, a WSD classifier is de-
signed to optimize overall accuracy without taking 
into account the class imbalance distribution in a 
real-world data set. The result is that the classifier 
induced from imbalanced data tends to over-fit the 
predominant class and to ignore small classes 
(Japkowicz and Stephen, 2002). Recently, much 
work has been done in addressing the class 
imbalance problem, reporting that resampling 
methods such as over-sampling and under-
sampling are useful in supervised learning with 
imbalanced data sets to induce more effective 
classifiers (Estabrooks et al, 2004; Zhou and Liu, 
2006).  
In general framework of active learning, the 
learner (i.e. supervised classifier) is formed by us-
ing supervised learning algorithms. To date, how-
ever, no-one has studied the effects of over-
sampling and under-sampling on active learning 
783
methods. In this paper, we study active learning 
with resampling methods addressing the class im-
balance problem for WSD. It is noteworthy that 
neither of these techniques need modify the 
architecture or learning algorithm, making them 
very easy to use and extend to other domains. 
Another problem in active learning is  knowing 
when to stop the process. We address this problem 
in this paper, and discuss how to form the final 
classifier for use. This is a problem of estimation 
of classifier effectiveness (Lewis and Gale, 1994). 
Because it is difficult to know when the classifier 
reaches maximum effectiveness, previous work 
used a simple stopping condition when the training 
set reaches desirable size. However, in fact it is 
almost impossible to predefine an appropriate size 
of desirable training data for inducing the most 
effective classifier. To solve the problem, we 
consider the problem of estimation of classifier 
effectiveness as a second task of estimating 
classifier confidence. This paper adopts two 
strategies: max-confidence and min-error, and sug-
gests a prediction solution by considering max-
confidence as the upper bound and min-error as the 
lower bound for the stopping conditions. 
2 Related Work 
The ability of the active learner can be referred to 
as selective sampling, of which two major schemes 
exist: uncertainty sampling and committee-based 
sampling. The former method, for example pro-
posed by Lewis and Gale (1994), is to use only one 
classifier to identify unlabeled examples on which 
the classifier is least confident. The latter method 
(McCallum and Nigam, 1998) generates a commit-
tee of classifiers (always more than two classifiers) 
and selects the next unlabeled example by the prin-
ciple of maximal disagreement among these classi-
fiers. With selective sampling, the size of the train-
ing data can be significantly reduced for text 
classification (Lewis and Gale, 1994; McCallum 
and Nigam, 1998), and word sense disambiguation 
(Chen, et al 2006).  
A method similar to committee-based sampling 
is co-testing proposed by Muslea et al (2000), 
which trains two learners individually on two 
compatible and uncorrelated views that should be 
able to reach the same classification accuracy. In 
practice, however, these conditions of view selec-
tion are difficult to meet in real-world word sense 
disambiguation tasks.  
Recently, much work has been done on the class 
imbalance problem. The well-known approach is 
resampling, in which some training material is du-
plicated. Two types of popular resampling methods 
exist for addressing the class imbalance problem: 
over-sampling and under-sampling. The basic idea 
of resampling methods is to change the training 
data distribution and make the data more balanced. 
It works ok in supervised learning, but has not 
been tested in active learning. Previous work re-
ports that cost-sensitive learning is a good solution 
to the class imbalance problem (Weiss, 2004). In 
practice, for WSD, the costs of various senses of a 
disambiguated word are unequal and unknown, 
and they are difficult to evaluate in the process of 
learning.   
In recent years, there have been attempts to ap-
ply active learning for word sense disambiguation 
(Chen et al, 2006). However, to our best knowl-
edge, there has been no such attempt to consider 
the class imbalance problem in the process of ac-
tive learning for WSD tasks. 
3 Resampling Methods 
3.1 Under-sampling 
Under-sampling is a popular method in addressing 
the class imbalance problem by changing the train-
ing data distribution by removing some exemplars 
of the majority class at random. Some previous 
work reported that under-sampling is effective in 
learning on large imbalanced data sets (Japkowicz 
and Stephen, 2002). However, as under-sampling 
removes some potentially useful training samples, 
it could cause negative effects on the classifier per-
formance.  
One-sided sampling is a method similar to un-
der-sampling, in which redundant and borderline 
training examples are identified and removed from 
training data (Kubat and Matwin, 1997). Kuban 
and Matwin reported that one-sided sampling is 
effective in learning with two-class large imbal-
anced data sets. However, the relative computa-
tional cost of one-sided sampling in active learning 
is very high, because sampling computations must 
be implemented for each learning iteration. Our 
primitive experimental results show that, in the 
multi-class problem of WSD, one-sided sampling 
degrades the performance of active learning. And 
784
due to the high computation complexity of one-
sided sampling, we use random under-sampling in 
our comparison experiments instead.  
To control the degree of change of the training 
data distribution, the ratio of examples from the 
majority and the minority class after removal from 
the majority class is called the removal rate (Jo 
and Japkowicz, 2004). If the removal rate is 1.0, 
then under-sampling methods build data sets with 
complete class balance. However, it was reported 
previously that perfect balance is not always the 
optimal rate (Estabrooks et al, 2004). In our com-
parison experiments, we set the removal rate for 
under-sampling to 0.8, since some cases have 0.8 
as the optimal rate reported in (Estabrooks et al, 
2004). 
3.2 Over-sampling 
Over-sampling is also a popular method in ad-
dressing the class imbalance problem by resam-
pling the small class until it contains as many ex-
amples as the large one. In contrast to under-
sampling, over-sampling is the process of adding 
examples to the minority class, and is accom-
plished by random sampling and duplication. Be-
cause the process of over-sampling involves 
making exact copies of examples, it usually in-
creases the training cost and may lead to overfit-
ting. There is a recent variant of over-sampling 
named SMOTE (Chawla et al, 2002) which is a 
synthetic minority over-sampling technique. The 
authors reported that a combination of SMOTE 
and under-sampling can achieve better classifier 
performance in ROC space than only under-
sampling the majority class. 
In our comparison experiments, we use over-
sampling, measured by a resampling rate called the 
addition rate (Jo and Japkowicz, 2004) that indi-
cates the number of examples that should be added 
into the minority class. The addition rate for over-
sampling is also set to 0.8 in our experiments. 
3.3 Bootstrap-based Over-sampling 
While over-sampling decreases the between-class 
imbalance, it increases the within-class imbalance 
(Jo and Japkowicz, 2004) because of the increase 
of exact copies of examples at random. To allevi-
ate this within-class imbalance problem, we pro-
pose a bootstrap-based over-sampling method 
(BootOS) that uses a bootstrap resampling tech-
nique in the process of over-sampling.  Bootstrap-
ping, explained below, is a resampling technique 
similar to jackknifing.  
There are two reasons for choosing a bootstrap 
method as resampling technique in the process of 
over-sampling. First, using a bootstrap set can 
avoid exactly copying samples in the minority 
class. Second, the bootstrap method may give a 
smoothing of the distribution of the training sam-
ples (Hamamoto et al, 1997), which can alleviate 
the within-class imbalance problem cased by over-
sampling.  
To generate the bootstrap set, we use a well-
known bootstrap technique proposed by Hama-
moto et al (1997) that does not select samples ran-
domly, allowing all samples in the minority 
class(es) an equal chance to be selected.  
Algorithm BootOS(X, N, r, k) 
Input: Minority class sample set X={x1, x2, ?, xn} of 
size n; Difference in number of examples between the 
majority and the minority class = N; Addition rate = r 
(< 1.0); Number of nearest neighbors = k. 
Output: bootstrap sample set XB of size N*r 
=X (xB1, xB2, ?, xB(N*r)). ?
1. For i = 1 To N*r 
2.       If i == n then (*all samples in minority class 
sample set have been used*) 
3.             j = 1; //the first sample is selected again  
4.       Else     
5.             j = i; // the i-th sample is selected 
6.       Endif 
7.       Select j-th sample xj (also as xj,0) from X 
8.       Find the k nearest neighbor samples xj,1, xj,2, 
?, xj,k using similarity functions. 
9.       Compute a bootstrap sample xBi: 
,01
k
Bi j ll
1x x
k =
= + ?  
10. Endfor 
11. return 
Figure 1. The BootOS algorithm 
4 Active Learning with Resampling 
In this work, we are interested in selective sam-
pling for pool-based active learning, and focus on 
uncertainty sampling (Lewis and Gale, 1994). The 
key point is how to measure the uncertainty of an 
unlabeled exemplar, and select a new exemplar 
with maximum uncertainty to augment the training 
data. The maximum uncertainty implies that the 
current classifier has the least confidence in its 
classification of this exemplar. The well-known 
entropy is a good uncertainty measurement widely 
785
used in active learning (zhang and Chen, 2002; 
Chen et al, 2006): 
1
( ) ( ) ( | ) log ( | )i j i j
j
U i H P p s w p s w
=
= = ??in i    (1) 
where U is the uncertainty measurement function 
H represents the entropy function. In the WSD 
task, p(sj|wi) is the predicted probability of sense sj 
outputted by the current classifier, when given a 
sample i containing a disambiguated word wi.  
Algorithm Active-Learning-with-Resampling(L,U,m) 
Input: Let L be initial small training data set; U the 
pool of unlabeled exemplars 
Output: labeled training data set L 
1. Resample L to generate new training data set L* 
using resampling techniques such as under-
sampling, over-sampling or BootOS, and then use 
L* to train the initial classifier 
2. Loop while adding new instances into L 
a. use the current classifier to probabilistically la-
bel all unlabeled exemplars in U 
b. Based on active learning rules, present m top-
ranked exemplars to oracle for labeling 
c. Augment L with the m new exemplars, and re-
move them from U 
d. Resample L to generate new training data set 
L* using resampling techniques such as under-
sampling, over-sampling, or BootOS, and use 
L* to retrain the current classifier         
3. Until the predefined stopping condition is met. 
4. return 
Figure 2. Active learning with resampling 
 
In step 1 and 2(d) in Fig. 2, if we do not gener-
ate L*, and L is used directly to train the current 
classifier, we call it ordinary active learning. In the 
process of active learning, we used the entropy-
based uncertainty measurement for all active learn-
ing frameworks in our comparison experiments. 
Actually our active learning with resampling is a 
heterogeneous approach in which the classifier 
used to select new instances is different from the 
resulting classifier (Lewis and Catlett, 1994).  
We utilize a maximum entropy (ME) model 
(Berger et al, 1996) to design the basic classifier 
used in active learning for WSD. The advantage of 
the ME model is the ability to freely incorporate 
features from diverse sources into a single, well-
grounded statistical model. A publicly available 
ME toolkit (Zhang et. al., 2004) was used in our 
experiments. In order to extract the linguistic fea-
tures necessary for the ME model, all sentences 
containing the target word were automatically part-
of-speech (POS) tagged using the Brill POS tagger 
(Brill, 1992). Three knowledge sources were used 
to capture contextual information: unordered single 
words in topical context, POS of neighboring 
words with position information, and local colloca-
tions.  These are same as three of the four knowl-
edge sources used in (Lee and Ng, 2002). Their 
fourth knowledge source (named syntactic rela-
tions) was not used in our work. 
5 Stopping Conditions 
In active learning algorithm, defining the stopping 
condition for active learning is a critical problem, 
because it is almost impossible for the human an-
notator to label all unlabeled samples. This is a 
problem of estimation of classifier effectiveness 
(Lewis and Gale 1994). In fact, it is difficult to 
know when the classifier reaches maximum 
effectiveness. In previous work some researchers 
used a simple stopping condition when the training 
set reached a predefined desired size. It is almost 
impossible to predefine an appropriate size of 
desirable training data for inducing the most 
effective classifier.  
To solve the problem, we consider the problem 
of estimating  classifier effectiveness as the 
problem of confidence estimation of classifier on 
the remaining unlabeled samples. Concretely, if we 
find that the current classifier already has 
acceptably strong confidence on its classification 
results for all remained unlabeled data, we assume 
the current training data is sufficient to train the 
classifier with maximum effectiveness. In other 
words, if a classifier induced from the current 
training data has strong classification confidence 
on an unlabeled example, we could consider it as a 
redundant example. 
Based on above analyses, we adopt here two 
stopping conditions for active learning: 
? Max-confidence: This strategy is based on 
uncertainty measurement, considering whether 
the entropy of each selected unlabeled example 
is less than a very small predefined threshold 
close to zero, such as 0.001.  
? Min-error: This strategy is based on feedback 
from the oracle when the active learner asks 
for true labels for selected unlabeled examples, 
considering whether the current trained 
classifier could correctly predict the labels or 
the accuracy performance of predictions on 
786
selected unlabeled examples is already larger 
than a predefined accuacy threshold. 
Once max-confidence and min-error conditions 
are met, the current classifier is assumed to have 
strong enough confidence on the classification 
results of all remained unlabeled data. 
6 Evaluation 
6.1 Data 
The data used for our comparison experiments 
were developed as part of the OntoNotes project 
(Hovy et al, 2006), which uses the WSJ part of the 
Penn Treebank (Marcus et al, 1993). The senses 
of noun words occurring in OntoNotes are linked 
to the Omega ontology. In OntoNotes, at least two 
humans manually annotate the coarse-grained 
senses of selected nouns and verbs in their natural 
sentence context. To date, OntoNotes has 
annotated several tens of thousands of examples, 
covering several hundred nouns and verbs, with an 
inter-annotator agreement rate of at least 90%.   
Those 38 random chosen ambiguous nouns used 
in all following experiments are shown in Table 1. 
It is apparent that the sense distributions of most 
nouns are very skewed (frequencies shown in the 
table, separated by /). 
Words sense distribution  words sense distribution 
Rate 1025/182 president 936/157/17 
People 815/67/7/5 part 456/102/75/16 
Point 471/88/37/19/9/6 director 517/23 
Revenue 517/23 bill 348/130/40 
Future 413/82/23 order 354/61/54/6/6 
Plant 376/51 board 369/15 
Today 238/149 policy 308/74 
Capital 325/21/8 term 147/137/52/13 
management 210/130 move 302/13/5 
Position 97/75/67/61/10/7 amount 236/57/16 
Home 267/17/16 power 154/134/15 
Leader 244/38 return 191/35/29/12/9 
administration 266/11 payment 201/69 
Account 233/18/13 control 90/66/64/21/12/5 
Lot 221/20 activity 218/23 
Drug 160/74 building 177/48/5 
Estate 214/11 house 112/71/25 
development 165/46/6 network 127/53/29 
Strategy 198/11 place 69/63/50/18/5 
Table 1. Data set used in experiments 
6.2 Results 
In the following active learning comparison 
experiments, we tested with five resampling 
methods including random sampling (Random), 
uncertainty sampling (Ordinary), under-sampling, 
over-sampling, and BootOS. The 1-NN technique 
was used for bootstrap-based resampling of 
BootOS in our experiments. A 5 by 5-fold cross-
validation was performed on each noun?s data.  
We used 20% randomly chosen data for held-out 
evaluation  and the other 80% as the pool of 
unlabeled data for each round of the active 
learning.  For all words, we started with a 
randomly chosen initial training set of 10 
examples, and we made 10 queries after each 
learning iteration.  
In the evaluation, average accuracy and recall 
are used as measures of performances for each 
active learning method. Note that the macro-
average way is adopted for recall evaluation in 
each noun WSD task. The accuracy measure 
indicates the percentage of testing instances 
correctly identified by the system. The macro-
average recall measure indicates how well the 
system performs on each sense.   
 
Experiment 1: Performance comparison ex-
periments on active learning 
 0.78
 0.8
 0.82
 0.84
 0.86
 0.88
 10  30  50  70  90  110  130  150  170  190  210  230  250  270  290
A
ve
ra
ge
 A
cc
ur
ac
y
Number of learned samples
Active learning for WSD
Random
Ordinary
Under-sampling
Over-sampling
BootOS
 
Figure 3. Average accuracy performance com-
parison experiments 
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0.48
 0.5
 0.52
 10  30  50  70  90  110  130  150  170  190  210  230  250  270  290
A
ve
ra
ge
 R
ec
al
l
Number of learned samples
Active learning for WSD
Random
Ordinary
Under-sampling
Over-sampling
BootOS
 
Figure 4. Average recall performance comparison 
experiments 
787
As shown in Fig. 3 and Fig. 4, when the number of 
learned samples for each noun is smaller than 120, 
the BootOS has the best performance, followed by 
over-sampling and ordinary method. As the num-
ber of learned samples increases, ordinary, over-
sampling and BootOS have similar performances 
on accuracy and recall. Our experiments also ex-
hibit that random sampling method is the worst on 
both accuracy and recall.  
Previous work (Estabrooks et al, 2004) reported 
that under-sampling of the majority class (pre-
dominant sense) has been proposed as a good 
means of increasing the sensitivity of a classifier to 
the minority class (infrequent sense). However, in 
our active learning experiments, under-sampling is 
apparently worse than ordinary, over-sampling and 
our BootOS. The reason is that in highly imbal-
anced data, too many useful training samples of 
majority class are discarded in under-sampling, 
causing the performance of active learning to de-
grade.  
 
Experiment 2: Effectiveness of learning in-
stances for infrequent senses 
It is important to enrich the corpora by learning 
more instances for infrequent senses using active 
learning with less human labeling. This procedure 
not only makes the corpora ?richer?, but also 
alleviates  the domain dependence problem faced 
by corpus-based supervised approaches to WSD.  
The objective of this experiment is to evaluate 
the performance of active learning in learning 
samples of infrequent senses from an unlabeled 
corpus. Due to highly skewed word sense 
distributions in our data set, we consider all senses 
other than the predominant sense as infrequent 
senses in this experiment.  
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0  20  40  60  80  100  120  140  160  180  200  220  240  260  280  300pe
rc
en
ta
ge
 o
f l
ea
rn
ed
 in
st
an
ce
s 
fo
r 
in
fr
eq
ue
nt
 s
en
se
s
Number of learned samples
Active learning for WSD
Random
Ordinary
Under-sampling
Over-sampling
BootOS
 
Figure 5. Comparison experiments on learning in-
stances for infrequent senses 
Fig. 5 shows that random sampling is the worst 
in active learning for infrequent senses. The reason 
is very obvious: the sense distribution of the 
learned sample set by random sampling is almost 
identical to that of the original data set. 
 Under-sampling is apparently worse than ordi-
nary active learning, over-sampling and BootOS 
methods. When the number of learned samples for 
each noun is smaller than 80, BootOS achieves 
slight better performance than ordinary active 
learning and over-sampling.  
When the number of learned samples is larger 
than 80 and smaller than 160, these three methods 
exhibit similar performance. As the number of it-
erations increases, ordinary active learning is 
slightly better than over-sampling and BootOS. In 
fact, after the 16th iteration (10 samples chosen in 
each iteration), results indicate that most instances 
for infrequent senses have been learned.  
 
Experiment 3: Effectiveness of Stopping Condi-
tions for active learning 
To evaluate the effectiveness of two strategies 
max-confidence and min-error as stopping condi-
tions of active learning, we first construct an ideal 
stopping condition when the classifier could reach 
the highest accuracy performance at the first time 
in the procedure of active learning. When the ideal 
stopping condition is met, it means that the current 
classifier has reached maximum effectiveness. In 
practice, it is impossible to exactly know when the 
ideal stopping condition is met before all unlabeled 
data are labeled by a human annotator. We only 
use this ideal method in our comparison experi-
ments to analyze the effectiveness of our two pro-
posed stopping conditions. 
For general purpose, we focus on the ordinary 
active learning to design the basic system, and to 
evaluate the effectiveness of three stop conditions. 
In the following experiments, the entropy threshold 
used in max-confidence strategy is set to 0.001, and 
the accuracy threshold used in min-error strategy 
is set to 0.9.   
In Table 2, the column ?Size? stands for the size 
of unlabeled data set of corresponding noun word 
used in active learning. There are two columns for 
each stopping condition: the left column ?num? 
presents number of learned instances and the right 
column ?%? presents its percentage over all data 
when the corresponding stopping condition is met. 
 
788
Ideal Max-confidence Min-error Words Size 
num % num % num % 
Rate 966 200 .23 410 .41 290 .29 
People 715 140 .20 290 .41 200 .28 
Point 504 90 .18 220 .44 120 .24 
Revenue 432 70 .16 110 .25 80 .19 
Future 414 120 .29 140 .34 60 .14 
Plant 342 210 .61 180 .53 110 .32 
Today 382 250 .65 240 .63 230 .60 
Capital 283 70 .25 180 .64 90 .32 
Management 272 200 .74 210 .77 210 .77 
Position 254 210 .83 230 .91 220 .87 
Home 240 60 .25 160 .67 60 .25 
Leader 226 60 .27 120 .53 70 .31 
administration 222 30 .14 90 .41 50 .23 
Account 211 50 .24 130 .62 70 .33 
Lot 185 30 .16 60 .32 40 .22 
Drug 187 130 .70 140 .75 120 .64 
Estate 180 20 .11 50 .28 30 .17 
Development 174 40 .23 150 .86 80 .46 
Strategy 167 10 .06 100 .60 10 .06 
President 888 120 .14 220 .25 120 .14 
Part 519 110 .21 240 .46 130 .25 
Director 432 110 .25 130 .30 90 .21 
Bill 414 120 .29 280 .68 150 .36 
Order 385 130 .34 220 .57 140 .36 
Board 307 40 .13 190 .62 40 .13 
Policy 306 90 .29 200 .65 150 .49 
Term 279 120 .43 190 .68 130 .47 
Move 256 50 .20 140 .55 50 .20 
Amount 247 210 .85 200 .81 140 .57 
Power 242 190 .78 190 .78 190 .78 
Return 221 90 .41 160 .72 100 .45 
Payment 216 120 .56 160 .74 150 .69 
Control 206 160 .78 200 .97 200 .97 
Activity 193 30 .16 130 .67 70 .36 
Building 184 90 .49 130 .71 110 .60 
House 166 100 .60 150 .90 110 .66 
Network 167 110 .66 130 .78 100 .60 
Place 164 120 .73 150 .91 120 .73 
Table 2 Effectiveness of three stopping conditions 
 
As shown in Table 2, the min-error strategy 
based on feedback of human annotator is very 
close to the ideal method. Therefore, when com-
paring to ideal stopping condition, min-error strat-
egy is a good choice as stopping condition for ac-
tive learning. It is important to note that the min-
error method does not need more additional 
computational costs, it only depends upon the 
feedback of human annotator when labeling the 
chosen unlabeled samples.   
From experimental results, we can see that max-
confidence strategy is worse than min-error 
method. However, we believe that the entropy of 
each unlabeled sample is a good signal to stop ac-
tive learning. So we suggest that there may be a 
good prediction solution in which the min-error 
strategy is used as the lower-bound of stopping 
condition, and max-confidence strategy as the up-
per-bound of stopping condition for active learning. 
7 Discussion 
As discussed above, finding more instances for 
infrequent senses at the earlier stages of active 
learning is very significant in making the corpus 
richer, meaning less effort for human labeling. In 
practice, another way to learn more instances for 
infrequent senses is to first build a training data set 
by active learning or by human efforts, and then 
build a supervised classifier to find more instances 
for infrequent sense. However, it is interesting to 
know how much initial training data is enough for 
this task, and how much human labeling efforts 
could be saved.  
From experimental results, we found that among 
these chosen unlabeled instances by active learner, 
some instances are informative samples helpful for 
improving classification performance, and other 
instances are borderline samples which are unreli-
able because even a small amount of noise can lead 
the sample to the wrong side of the decision 
boundary. The removal of these borderline samples 
might improve the performance of active learning. 
The proposed prediction solution based on max-
confidence and min-error strategies is a coarse 
framework. To predict when to stop active learning 
procedure, it is logical to consider the changes of 
accuracy performance of the classifier as a signal 
to stop the learning iteration. In other words, dur-
ing the range predicted by the proposed solution, if 
the change of accuracy performance of the learner 
(classifier) is very small, we could assume that the 
current classifier has reached maximum effective-
ness. 
8 Conclusion and Future Work 
In this paper, we consider the class imbalance 
problem in WSD tasks, and analyze the effect of 
resampling techniques including over-sampling 
and under-sampling in active learning. Experimen-
tal results show that over-sampling is a relatively 
good choice in active learning for WSD in highly 
imbalanced data. Under-sampling causes negative 
effect on active learning. A new over-sampling 
method named BootOS based on bootstrap tech-
nique is proposed to alleviate the within-class im-
balance problem of over-sampling, and works bet-
ter than ordinary over-sampling in active learning 
for WSD. It is noteworthy that none of these 
techniques require to modify the architecture or 
789
learning algorithm; therefore, they are very easy to 
use and extend to other applications. To predict 
when to stop active learning, we adopt two 
strategies including max-confidence and min-error 
as stopping conditions. According to our 
experimental results, we suggest a prediction 
solution by considering max-confidence as the 
upper bound and min-error as the lower bound of 
stopping conditions for active learning.  
In the future work, we will study how to exactly 
identify these borderline samples thus they are not 
firstly selected in active learning procedure. The 
borderline samples have the higher entropy values 
meaning least confident for the current classifier. 
The borderline instances can be detected using the 
concept of Tomek links (Tomek 1976). It is also 
worth studying cost-sensitive learning for active 
learning with imbalanced data, and using such 
techniques for WSD. 
References 
A. L. Berger, S. A. Della, and V. J  Della. 1996. A maximum 
entropy approach to natural language processing. Compu-
tational Linguistics 22(1):39?71. 
E Brill. 1992. A simple rule-based part of speech tagger. In 
the Proceedings of the Third Conference on Applied Natu-
ral Language Processing. 
Y. S. Chan and H. T. Ng. 2006. Estimating class priors in 
domain adaptation. In Proc. of ACL06.  
N. Chawla, K. Bowyer, L. Hall, W. Kegelmeyer. 2002. 
SMOTE: synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 2002(16): 321-357 
J. Chen, A. Schein, L. Ungar, M. Palmer. 2006. An empirical 
study of the behavior of active learning for word sense dis-
ambiguation. In Proc. of HLT-NAACL06 
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein, and C. 
Strapparava. 2006. Direct Word Sense Matching for Lexi-
cal Substitution. In Proc. of ACL'06 
H. T. Dang and M. Palmer. 2005. The Role of Semantic Roles 
in Disambiguating Verb Senses. In Proc. of ACL'05. 
A. Estabrooks, T. Jo and N. Japkowicz. 2004. A multiple re-
sampling method for learning from imbalanced data set. 
Computational Intelligence, 20(1):18-36 
Y. Hamamoto, S. Uchimura and S. Tomita. 1997. A bootstrap 
technique for nearest neighbor classifier design. IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 
19(1):73-79 
V. Hoste, A. Kool, and W. Daelemans. 2001. Classifier opti-
mization and combination in the English all words task. In 
Proc. of the SENSEVAL-2 workshop 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R. 
Weischedel. 2006. Ontonotes: The 90% Solution. In Proc. 
of HLT-NAACL06. 
N. Ide and J. Veronis. 1998. Introduction to the special issue 
on word sense disambiguation: the state of the art. Compu-
tational Linguistics, 24(1):1-37 
N. Japkowicz and S. Stephen. 2002. The class imbalance 
problem: a systematic study. Intelligent Data Analysis, 
6(5):429-450 
T. Jo and N. Japkowicz. 2004. Class imbalances versus small 
disjuncts. SIGKSS Explorations, 6(1):40-49 
U. S. Kohomban and W. S. Lee. 2005. Learning Semantic 
Classes for Word Sense Disambiguation. In Proc. of 
ACL'05 
M. Kubat and S. Matwin. 1997. Addressing the curse of im-
balanced training sets: one-sided selection. In Proc. of 
ICML97 
Y.K. Lee and. H.T. Ng. 2002. An empirical evaluation of 
knowledge sources and learning algorithm for word sense 
disambiguation. In Proc. of EMNLP-2002 
D. D. Lewis and W. A. Gale. 1994. A sequential algorithm for 
training text classifiers. In Proc. of SIGIR-94 
D.D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty 
sampling for supervised learning. In Proc. of ICML94 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the Penn 
Treebank. Computational Linguistics,  19(2):313-330 
A. McCallum and K. Nigram. 1998. Employing EM in pool-
based active learning for text classification. In Proc. 15th 
ICML 
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004. 
Finding predominant senses in untagged text. In Proc. of 
ACL04 
I. Muslea, S. Minton, and C. A. Knoblock. 2000. Selective 
sampling with redundant views. In Proc. of National Con-
ference on Artificial Intelligence 
I. Tomek. 1976. Two modifications of CNN. IEEE Transac-
tions on Systems, Man and Cybernetics, 6(6):769-772 
G. M. Weiss. 2004. Mining with rarity ? problems and solu-
tions: a unifying framework. SIGKDD Explorations, 
6(1):7-19 
N. Xue, J. Chen and M. Palmer. 2006. Aligning Features with 
Sense Distinction Dimensions. In Proc. of ACL'06 
Z. Zhou, X. Liu. 2006. Training cost-sensitive neural networks 
with methods addressing the class imbalance problem. 
IEEE Transactions on Knowledge and Data Engineering, 
18(1):63-77 
L. Zhang, J. Zhu, and T. Yao. 2004. An evaluation of statisti-
cal spam filtering techniques. ACM Transactions on Asian 
Language Information Processing, 3(4):243?269. 
C. Zhang and T. Chen. 2002. An active learning frame-
work for content-based information retrieval. IEEE 
Transactions on Multimedia, 4(2):260-268 
790
Towards Automated Semantic Analysis on Biomedical Research Articles
 
Donghui Feng         Gully Burns         Jingbo Zhu         Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, jingboz, hovy}@isi.edu 
 
Abstract 
In this paper, we present an empirical 
study on adapting Conditional Random 
Fields (CRF) models to conduct semantic 
analysis on biomedical articles using ac-
tive learning. We explore uncertainty-
based active learning with the CRF model 
to dynamically select the most informa-
tive training examples. This abridges the 
power of the supervised methods and ex-
pensive human annotation cost. 
1 Introduction 
Researchers have experienced an increasing need 
for automated/semi-automated knowledge acquisi-
tion from the research literature. This situation is 
especially serious in the biomedical domain where 
the number of individual facts that need to be 
memorized is very high. 
Many successful information extraction (IE) 
systems, work in a supervised fashion, requiring 
human annotations for training. However, human 
annotations are either too expensive or not always 
available and this has become a bottleneck to de-
veloping supervised IE methods to new domains. 
Fortunately, active learning systems design 
strategies to select the most informative training 
examples. This process can achieve certain levels 
of performance faster and reduce human annota-
tion (e.g., Thompson et al, 1999; Shen et al, 2004). 
In this paper, we present an empirical study on 
adapting CRF model to conduct semantic analysis 
on biomedical research literature. We integrate an 
uncertainty-based active learning framework with 
the CRF model to dynamically select the most in-
formative training examples and reduce human 
annotation cost. A systematic study with exhaus-
tive experimental evaluations shows that it can 
achieve satisfactory performance on biomedical 
data while requiring less human annotation. 
Unlike direct estimation on target individuals in 
traditional active learning, we use two heuristic 
certainty scores, peer comparison certainty and set 
comparison certainty, to indirectly estimate se-
quences labeling quality in CRF models. 
We partition biomedical research literature by 
experimental types. In this paper, our goal is to 
analyze various aspects of useful knowledge about 
tract-tracing experiments (TTE). This type of ex-
periments has prompted the development of sev-
eral curated databases but they have only partial 
coverage of the available literature (e.g., Stephan et 
al., 2001). 
2 Related Work 
Knowledge Base Management Systems allow 
individual users to construct personalized 
repositories of knowledge statements based on 
their own interaction with the research literature 
(Stephan et al, 2001; Burns and Cheng, 2006). But 
this process of data entry and curation is manual. 
Current approaches on biomedical text mining (e.g., 
Srinivas et al, 2005; OKanohara et al, 2006) tend 
to address the tasks of named entity recognition or 
relation extraction, and our goal is more complex: 
to extract computational representations of the 
minimum information in a given experiment type. 
Pattern-based IE approaches employ seed data 
to learn useful patterns to pinpoint required fields 
values (e.g. Ravichandran and Hovy, 2002; Mann 
and Yarowsky, 2005; Feng et al, 2006). However, 
this only works if the data corpus is rich enough to 
learn variant surface patterns and does not neces-
sarily generalize to more complex situations, such 
as our domain problem. Within biomedical articles, 
sentences tend to be long and the prose structure 
tends to be more complex than newsprint. 
871
The CRF model (Lafferty et al, 2001) provides 
a compact way to integrate different types of fea-
tures for sequential labeling problems. Reported 
work includes improved model variants (e.g., Jiao 
et al, 2006) and applications such as web data ex-
traction (Pinto et al, 2003), scientific citation ex-
traction (Peng and McCallum, 2004), word align-
ment (Blunsom and Cohn, 2006), and discourse-
level chunking (Feng et al, 2007). 
Pool-based active learning was first successfully 
applied to language processing on text classifica-
tion (Lewis and Gale, 1994; McCallum and Nigam, 
1998; Tong and Koller, 2000). It was also gradu-
ally applied to NLP tasks, such as information ex-
traction (Thompson et al, 1999); semantic parsing 
(Thompson et al, 1999); statistical parsing (Tang 
et al, 2002); NER (Shen et al, 2004); and Word 
Sense Disambiguation (Chen et al, 2006). In this 
paper, we use CRF models to perform a more com-
plex task on the primary TTE experimental results 
and adapt it to process new biomedical data. 
3 Semantic Analysis with CRF Model 
3.1 What knowledge is of interest? 
The goal of TTE is to chart the interconnectivity of 
the brain by injecting tracer chemicals into a region 
of the brain and then identifying corresponding 
labeled regions where the tracer is transported to. 
A typical TTE paper may report experiments about 
one or many labeled regions.  
Name Description 
injectionLocation the named brain region where the injection was made. 
tracerChemical the tracer chemical used. 
labelingLocation the region/location where the labeling was found. 
labelingDescription a description of labeling, den-sity or label type. 
Table 1. Minimum knowledge schema for a TTE. 
 
 
 
 
 
 
 
 
 
 
Figure 1. An extraction example of TTE description. 
In order to construct the minimum information 
required to interpret a TTE, we consider a set of 
specific components as shown in Table 1. 
Figure 1 gives an example of description of a 
complete TTE in a single sentence. In the research 
articles, this information is usually spread over 
many such sentences.  
3.2 CRF Labeling 
We use a plain text sentence for input and attempt 
to label each token with a field label. In addition to 
the four pre-defined fields, a default label, ?O?, is 
used to denote tokens beyond our concern.  
In this task, we consider five types of features 
based on language analysis as shown in Table 2. 
Name Feature Description 
TOPOGRAPHY Is word topog-
raphic? 
BRAIN_REGION Is word a region 
name? 
TRACER Is word a tracer 
chemical? 
DENSITY Is word a density 
term? 
Lexical 
Knowledge 
LABELING_TYPE Does word denote 
a labeling type? 
Surface Word Word Current word 
Context    
Window 
CONT_INJ If current word if 
within a window 
of injection con-
text 
Prev-word Previous word Window 
Words Next-word Next word 
Root-form Root form of the 
word if different 
Gov-verb The governing 
verb 
Subj The sentence 
subject  
Dependency 
Features 
Obj The sentence 
object 
Table 2. The features for system labeling. 
Lexical Knowledge. We define lexical items rep-
resenting different aspects of prior knowledge. To 
this end we use names of brain structures taken 
from brain atlases, standard terms to denote neuro-
anatomical topographical spatial relationships, and 
common sense words for labeling descriptions. We 
collect five separate lexicons as shown in Table 3. 
Lexicons # of terms # of words 
BRAIN_REGION 1123 5536 
DENSITY 8 10 
LABELING_TYPE 9 13 
TRACER 30 30 
TOPOGRAPHY 9 36 
Total 1179 5625 
Table 3. The five lexicons. 
The NY injection ( Fig . 9B ) encompassed  
 
tracerChemical 
most of the pons and was very dense in  
 
injectionLocation 
the region of the MLF. 
 
labelingLocation 
872
Surface word. The word token is an important 
indicator of the probable label for itself.  
Context Window. The TTE is a description of the 
inject-label-findings context. Whenever we find a 
word with a root form of ?injection? or ?deposit?, 
we generate a context window around this word 
and all the words falling into this window are as-
signed a feature of ?CON_INJ?. This means when 
labeling these words the system should consider 
the very current context. 
Window Words. We also use all the words occur-
ring in the window around the current word. We 
set the window size to only include the previous 
and following words (window size = 1).  
Dependency Features. To untangle word relation-
ships within each sentence, we apply the depend-
ency parser MiniPar (Lin, 1998) to parse each sen-
tence, and then derive four types of features. These 
features are (a) root form of word, (b) the subject 
in the sentence, (c) the object in the sentence, and 
(d) the governing verb for each word. 
4 Uncertainty-based Active Learning 
Active learning was initially introduced for 
classification tasks. The intuition is to always add 
the most informative examples to the training set to 
improve the system as much as possible.  
We apply an uncertainty/certainty score-based 
approach. Unlike traditional classification tasks, 
where disagreement or uncertainty is easy to obtain 
on target individuals, information extraction tasks 
in our problem take a whole sequence of tokens 
that might include several slots as processing units. 
We therefore need to make decisions on whether a 
full sequence should be returned for labeling. 
Estimations on confidence for single segments 
in the CRF model have been proposed by (Culotta 
and McCallum, 2004; Kristjannson et al, 2004). 
However as every processing unit in the data set is 
at the sentence level and we make decisions at the 
sentence level to train better sequential labeling 
models, we define heuristic scores at the sentence 
level.  
Symons et al (2006) presents multi-criterion for 
active learning with CRF models, but our motiva-
tion is from a different perspective. The labeling 
result for every sentence corresponds to a decoding 
path in the state transition network. Inspired by the 
decoding and re-ranking approaches in statistical 
machine translation, we use two heuristic scores to 
measure the degree of correctness of the top label-
ing path, namely, peer comparison certainty and 
set comparison certainty. 
Suppose a sentence S includes n words/tokens 
and a labeling path at position m in the ranked N-
best list is represented by ),...,,( 110 ?= nm lllL . Then 
the probability of this labeling path is represented 
by )( mLP , and we have the following two equa-
tions to define the peer comparison certainty 
score, )(SScore peer  and set comparison certainty 
score, )(SScoreset : 
)(
)(
)(
2
1
LP
LP
SScorepeer =                                      (1) 
?
=
=
N
k
k
set
LP
LP
SScore
1
1
)(
)(
)(                                    (2) 
For peer comparison certainty (Eq. 1), we calcu-
late the ratio of the top-scoring labeling path prob-
ability to the second labeling path probability. A 
high ratio means there is a big jump from the top 
labeling path to the second one. The higher the ra-
tio score, the higher the relative degree of correct-
ness for the top labeling path, giving system higher 
confidence for those with higher peer comparison 
certainty scores. Sentences with lowest certainty 
score will be sent to the oracle for manual labeling. 
In the labeling path space, if a labeling path is 
strong enough, its probability score should domi-
nate all the other path scores. In Equation 2, we 
compute the set comparison certainty score by con-
sidering the portion of the probability of the path in 
the overall N-best labeling path space. A large 
value means the top path dominates all the other 
labeling paths together giving the system a higher 
confidence on the current path over others. 
We start with a seed training set including k la-
beled sentences. We then train a CRF model with 
the training data and use it to label unlabeled data. 
The results are compared based on the certainty 
scores and those sentences with the lowest cer-
tainty scores are sent to an oracle for human label-
ing. The new labeled sentences are then added to 
the training set for next iteration.  
5 Experimental Results 
We first investigated how the active learning steps 
could help for the task. Second, we evaluated how 
the CRF labeling system worked with different sets 
of features. We finally applied the model to new 
873
biomedical articles and examined its performance 
on one of its subsets. 
5.1 Experimental Setup 
We have obtained 9474 Journal of Comparative 
Neurology (JCN)1 articles from 1982 to 2005. For 
sentence labeling, we collected 21 TTE articles 
from the JCN corpus. They were converted from 
PDF files to XML files, and all of the article sec-
tions were identified using a simple rule-based ap-
proach. As most of the meaningful descriptions of 
TTEs appear in the Results section, we only proc-
essed the Results section. The 21 files in total in-
clude 2009 sentences, in which 1029 sentences are 
meaningful descriptions for TTEs and 980 sen-
tences are not related to TTEs.  
We randomly split the sentences into a training 
pool and a testing pool, under a ratio 2:1. The 
training pool includes 1338 sentences, with 685 of 
them related to TTEs, while 653 not. Testing was 
based on meaningful sentences in the testing pool. 
Table 4 gives the configurations in the data pools. 
 # of        
Related 
Sentences  
# of        
Unrelated 
Sentences 
Sum 
Training Pool 685 653 1338 
Testing Pool 344 327 671 
Sum 1029 980 2009 
Table 4. Training and testing pool configurations. 
5.2 Evaluation Metrics 
As the label ?O? dominates the data set (70% out 
of all tokens), a simple accuracy score would pro-
vide an inappropriate high score for a baseline sys-
tem that always chooses ?O?. We used Precision, 
Recall, and F_Score to evaluate only meaningful 
labels. 
5.3 How well does active learning work? 
For the active learning procedure, we initially se-
lected a set of seed sentences related to TTEs from 
the training pool. At every step we trained a CRF 
model and labeled sentences in the rest of the train-
ing pool. As described in section 4, those with the 
lowest rank on certainty scores were selected. If 
they are related to a TTE, human annotation will 
be added to the training set. Otherwise, the system 
will keep on selecting sentences until it finds 
enough related sentences. 
                                                 
1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 
People have found active learning in batch mode 
is more efficient, as in some cases a single addi-
tional training example will not improve a classi-
fier/system that much. In our task, we chose the 
bottom k related sentences with the lowest cer-
tainty scores. We conducted various experiments 
for k = 2, 5, and 10. We also compared experi-
ments with passive learning, where at every step 
the new k related sentences were randomly se-
lected from the corpus. Figures 2, 3, and 4 give the 
learning curves for precision, recall, and F_Scores 
when k = 10. 
 
Figure 2. Learning curve for Precision. 
 
Figure 3. Learning curve for Recall. 
 
Figure 4. Learning curve for F_Score. 
From these figures, we can see active learning 
approaches required fewer training examples to 
achieve the same level of performance. As we it-
eratively added new labeled sentences into the 
training set, the precision scores of active learning 
were steadily better than that of passive learning as 
the uncertain examples were added to strengthen 
874
existing labels. However, the recall curve is 
slightly different. Before some point, the recall 
score of passive learning was a little better than 
active learning. The reason is that examples se-
lected by active learning are mainly used to foster 
existing labels but have relatively weaker im-
provements for new labels, while passive learning 
has the freedom to add new knowledge for new 
labels and improve recall scores faster. As we keep 
on using more examples, the active learning 
catches up with and overtakes passive learning on 
recall score. 
These experiments demonstrate that under the 
framework of active learning, examples needed to 
train a CRF model can be greatly reduced and 
therefore make it feasible to adapt to other domains. 
5.4 How well does CRF labeling work? 
As we added selected annotated sentences, the sys-
tem performance kept improving. We investigated 
system performance at the final step when all the 
related sentences in the training pool are selected 
into the training set. The testing set alo only in-
cludes the related sentences. This results in 685 
training sentences and 344 testing sentences. 
To establish a baseline for our labeling task, we 
simply scanned every sentence for words or 
phrases from each lexicon. If the term was present, 
then we labeled the word based on the lexicon in 
which it appeared. If words appeared in multiple 
lexicons, we assigned labels randomly. 
System Features Prec. Recall F_Score 
Baseline 0.4067 0.1761 0.2458 
Lexicon 0.5998 0.3734 0.4602 
Lexicon                   
+ Surface Words 
0.7663 0.7302 0.7478 
Lexicon                   
+ Surface Words     
+ Context Window 
0.7717 0.7279 0.7491 
Lexicon + Surface 
Words + Context 
Window + Window 
Words 
0.8076 0.7451 0.7751 
Lexicon + Surface 
Words + Context 
Window + Window 
Words + Depend-
ency Features  
0.7991 0.7828 0.7909 
Table 5. Precision, Recall, and F_Score for labeling. 
We tried exhaustive feature combinations. Table 
5 shows system performance with different feature 
combinations. All systems performed significantly 
higher than the baseline. The sole use of lexicon 
knowledge produced poor performance, and the 
inclusion of surface words produced significant 
improvement. The use of window words boosted 
precision and recall. The performance with all the 
features generated an F_score of 0.7909. 
We explored how system performance reflects 
different labels. Figure 5 and 6 depict the detailed 
distribution of system labeling from the perspec-
tive of precision and recall respectively for the sys-
tem with the best performance. Most errors oc-
curred in the confusion of injectionLocation and 
labelingLocation, or of the meaningful labels and 
?O?. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
injLoc labelDesp labelLoc tracer
O
injLoc
labelDesp
labelLoc
tracer
 
Figure 5. Precision confusion matrix distribution. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
injLoc labelDesp labelLoc tracer
O
injLoc
labelDesp
labelLoc
tracer
 
Figure 6. Recall confusion matrix distribution. 
The worst performance occurred for files that 
distinguish themselves from others by using fairly 
different writing styles. We believe given more 
training data with different writing styles, the sys-
tem could achieve a better overall performance. 
5.5 On New Biomedical Data 
Under this active learning framework, we have 
shown a CRF model can be trained with less anno-
tation cost than using traditional passive learning. 
We adapted the trained CRF model to new bio-
medical research articles. 
Out of the 9474 collected JCN articles, more 
than 230 research articles are on TTEs. The whole 
processing time for each document varies from 20 
seconds to 90 seconds. We sent the new system-
labeled files back to a biomedical knowledge ex-
pert for manual annotation. The time to correct one 
automatically labeled document is dramatically 
reduced, around 1/3 of that spent on raw text. 
We processed 214 new research articles and ex-
amined a subset including 16 articles. We evalu-
875
ated it in two aspects: the overall performance and 
the performance averaged at the document level. 
Table 6 gives the performance on the whole new 
subset and that averaged on 16 documents. The 
performance is a little bit lower than reported in 
the previous section as the new document set might 
include different styles of documents. We exam-
ined system performance at each document. Figure 
7 gives the detailed evaluation for each of the 16 
documents. The average F_Score of the document 
level is around 74%. For those documents with 
reasonable TTE description, the system can 
achieve an F_Score of 87%. The bad documents 
had a different description style and usually mixed 
the TTE descriptions with general discussion.  
 Prec. Recall F_Score 
Overall 0.7683 0.7155 0.7410 
Averaged per Doc. 0.7686 0.7209 0.7418 
Table 6. Performance on the whole new subset and                 
the averaged performance per document. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
11 4 12 0 6 2 14 7 8 15 3 1 13 10 9 5 Doc No.
Precision 
Recall 
F-score
 
Figure 7. System performance per document. 
6 Conclusions and Future Work 
In this paper, we explored adapting a supervised 
CRF model for semantic analysis on biomedical 
articles using an active learning framework. It 
abridges the power of the supervised approach and 
expensive human costs. We are also investigating 
the use of other certainty measures, such as aver-
aged field confidence scores over each sentence. 
In the long run we wish to generalize the frame-
work to be able to mine other types of experiments 
within the biomedical research literature and im-
pact research in those domains. 
References 
Blunsom, P. and Cohn, T. 2006. Discriminative word align-
ment with conditional random fields. In ACL-2006. 
Burns, G.A. and Cheng, W.C. 2006. Tools for knowledge 
acquisition within the NeuroScholar system and their ap-
plication to anatomical tract-tracing data. In Journal of 
Biomedical Discovery and Collaboration. 
Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006. An em-
pirical study of the behavior of active learning for word 
sense disambiguation. In Proc. of HLT-NAACL 2006.  
Culotta, A. and McCallum, A. 2004. Confidence estimation 
for information extraction. In HLT-NAACL-2004, short pa-
pers. 
Feng, D., Burns, G., and Hovy, E.H. 2007. Extracting data 
records from unstructured biomedical full text. In 
Proc. of EMNLP-CONLL-2007. 
Feng, D., Ravichandran, D., and Hovy, E.H. 2006. Mining and 
re-ranking for answering biographical queries on the web. 
In Proc. of AAAI-2006. 
Jiao, F., Wang, S., Lee, C., Greiner, R., and Schuurmans, D. 
2006. Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Proc. of 
ACL-2006. 
Kristjannson, T., Culotta, A., Viola, P., and McCallum, A. 
2004. Interactive information extraction with constrained 
conditional random fields. In Proc. of AAAI-2004.  
Lafferty, J., McCallum, A., and Pereira, F. 2001. Conditional 
random fields: probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML-2001. 
Lewis, D.D. and Gale, W.A. 1994. A sequential algorithm for 
training text classifiers. In Proc. of SIGIR-1994. 
Lin, D. 1998. Dependency-based evaluation of MINIPAR. In 
Workshop on the Evaluation of Parsing Systems. 
Mann, G.S. and Yarowsky, D. 2005. Multi-field information 
extraction and cross-document fusion. In Proc. of ACL-
2005. 
McCallum, A.K. 2002. MALLET: a machine Learning for 
language toolkit. http://mallet.cs.umass.edu.  
McCallum, A. and Nigam, K. 1998. Employing EM in pool-
based active learning for text classification. In Proc. of 
ICML-98.  
OKanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J. 2006. 
Improving the scalability of semi-markov conditional ran-
dom fields for named entity recognition. In ACL-2006. 
Peng, F. and McCallum, A. 2004. Accurate information ex-
traction from research papers using conditional random 
fields. In Proc. of HLT-NAACL-2004. 
Pinto, D., McCallum, A., Wei, X., and Croft, W.B. 2003. Ta-
ble extraction using conditional random fields. In SIGIR-
2003. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface text 
patterns for a question answering system. In ACL-2002.  
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan, C.L. 2004. 
Multi-criteria-based active learning for named entity rec-
ognition. In Proc. of ACL-2004. 
Srinivas, et al, 2005. Comparison of vector space model 
methodologies to reconcile cross-species neuroanatomical 
concepts. Neuroinformatics, 3(2). 
Stephan, K.E., et al, 2001. Advanced database methodology 
for the Collation of Connectivity data on the Macaque 
brain (CoCoMac). Philos Trans R Soc Lond B Biol Sci. 
Symons et al, 2006. Multi-Criterion Active Learning in Con-
ditional Random Fields.  In ICTAI-2006. 
Tang, M., Luo, X., and Roukos, S. 2002. Active learning for 
statistical natural language parsing. In ACL-2002. 
Thompson, C.A., Califf, M.E., and Mooney, R.J. 1999. Active 
learning for natural language parsing and information ex-
traction. In Proc. of ICML-99. 
Tong, S. and Koller, D. 2000. Support vector machine active 
learning with applications to text classification. In Proc. of 
ICML-2000. 
876
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1129?1136
Manchester, August 2008
Multi-Criteria-based Strategy to Stop Active Learning for Data An-
notation 
Jingbo Zhu   Huizhen Wang 
Natural Language Processing Laboratory 
Northeastern University 
Shenyang, Liaoning, P.R.China 110004 
zhujingbo@mail.neu.edu.cn 
wanghuizhen@mail.neu.edu.cn 
Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu 
 
Abstract 
In this paper, we address the issue of de-
ciding when to stop active learning for 
building a labeled training corpus. Firstly, 
this paper presents a new stopping crite-
rion, classification-change, which con-
siders the potential ability of each unla-
beled example on changing decision 
boundaries. Secondly, a multi-criteria-
based combination strategy is proposed 
to solve the problem of predefining an 
appropriate threshold for each confi-
dence-based stopping criterion, such as 
max-confidence, min-error, and overall-
uncertainty. Finally, we examine the ef-
fectiveness of these stopping criteria on 
uncertainty sampling and heterogeneous 
uncertainty sampling for active learning. 
Experimental results show that these 
stopping criteria work well on evaluation 
data sets, and the combination strategies 
outperform individual criteria. 
1 Introduction 
Creating a large labeled training corpus is very 
expensive and time-consuming in some real-
world applications. For example, it is a crucial 
issue for automated word sense disambiguation 
task, because validations of sense definitions and 
sense-tagged data annotation have to be done by 
human experts, e.g. OntoNotes project (Hovy et 
al., 2006).  
Active learning aims to minimize the amount 
of human labeling effort by automatically select-
ing the most informative unlabeled example for 
human annotation. In recent years active learning 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
has been widely studied in natural language 
processing (NLP) applications, such as word 
sense disambiguation (WSD) (Chen et al, 2006; 
Zhu and Hovy, 2007), text classification (TC) 
(Lewis and Gale, 1994; McCallum and Nigam, 
1998a), named entity recognition (Shen et al, 
2004), chunking (Ngai and Yarowsky, 2000), 
and statistical parsing (Tang et al, 2002). 
However, deciding when to stop active learn-
ing is still an unsolved problem and seldom men-
tioned issue in previous studies. Actually it is a 
very important practical issue in real-world ap-
plications, because it obviously makes no sense 
to continue the active learning procedure until 
the whole unlabeled corpus has been labeled. 
The active learning process can be ended when 
the current classifier reaches the maximum effec-
tiveness. In principle, how to learn a stopping 
criterion is a problem of estimation of classifier 
(i.e. learner) effectiveness during active learning 
(Lewis and Gale, 1994).  
In this paper, we address the issue of a stop-
ping criterion for pool-based active learning with 
uncertainty sampling (Lewis and Gale, 1994), 
and propose a multi-criteria-based approach to 
determining when to stop active learning process. 
Firstly, this paper makes a comprehensive analy-
sis on some confidence-based stopping criteria 
(Zhu and Hovy, 2007), including max-
confidence, min-error and overall-uncertainty, 
then proposes a new stopping criterion, classifi-
cation-change, which considers the potential 
ability of each unlabeled example on changing 
decision boundaries. Secondly, a combination 
strategy is proposed to solve the problem of pre-
defining an appropriate threshold for each confi-
dence-based stopping criterion in a specific task.  
In uncertainty sampling scheme, the most un-
certain unlabeled example is considered as the 
most informative case selected by active learner 
at each learning cycle. However, an uncertain 
example for one classifier may be not an uncer-
1129
tain example for other classifiers. When using 
active learning for real-world applications such 
as WSD, it is possible that a classifier of one type 
selects samples for training a classifier of another 
type, called the heterogeneous approach (Lewis 
and Catlett, 1994). For example, the final trained 
classifier for WSD is often different from the 
classifier used in active learning for constructing 
the training corpus.  
To date, no one has studied the stopping crite-
rion issue for the heterogeneous approach. In this 
paper, we examine the effectiveness of each 
stopping criterion on both traditional uncertainty 
sampling and heterogeneous uncertainty sam-
pling for active learning. Experimental results of 
active learning for WSD and TC tasks show that 
these proposed stopping criteria work well on 
evaluation data sets, and the combination strate-
gies outperform individual criteria. 
2 Active Learning Process 
In this paper, we are interested in uncertainty 
sampling for pool-based active learning (Lewis 
and Gale, 1994), in which an unlabeled example 
x with maximum uncertainty is selected to aug-
ment the training data at each learning cycle. The 
maximum uncertainty implies that the current 
classifier has the least confidence on its classifi-
cation of this unlabeled example.  
Actually active learning is a two-stage process 
in which a small number of labeled samples and 
a large number of unlabeled examples are first 
collected in the initialization stage, and a closed-
loop stage of query and retraining is adopted.  
Procedure: Active Learning Process 
Input: initial small training set L, and pool of unla-
beled data set U 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique to select m 
most informative unlabeled examples, and ask 
oracle H for labeling 
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning with uncertainty sam-
pling technique 
3 Stopping Criteria for Active Learning  
In this section, we mainly address the problem of 
general stopping criteria for active leanring, and 
study how to define a reasonable and appropriate 
stopping criterion SC shown in Fig. 1. 
3.1 Effectiveness Estimation and Confi-
dence Estimation 
To examine whether the classifier has reached 
the maximum effectiveness during active learn-
ing procedure, it seems an appealing solution 
when repeated learning cycles show no signifi-
cant performance improvement. However, this is 
often not feasible. To investigate the impact of 
performance change on defining a stopping crite-
rion for active learning, we first give an example 
of active learning for WSD shown in Fig. 2. 
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  20  40  60  80  100  120  140  160  180  200  220  240  260  280  300
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD task
interest
 
Figure 2. An example of active learning for WSD 
on word ?interest?. 
 
Fig. 2 shows that the accuracy performance 
generally increases, but apparently degrades at 
iterations 30, 90 and 190, and does not change 
anymore during iterations 220-260 in the active 
learning process. Actually the first time of the 
highest performance of 91.5% is achieved at 900 
which is not shown in Fig. 2. Although the accu-
racy performance curve shows an increasing 
trend, it is not monotonically increasing. It is not 
easy to automatically determine the point of no 
significant performance improvement on the 
validation set, because points such as 30 or 90 
would mislead a final judgment.  
Besides, there is a problem of performance es-
timation of the current classifier during active 
learning process, because a separate validation 
set should be prepared in advance, a procedure 
that causes additional (high) cost since it is often 
done manually. Besides, how many samples are 
required for the pregiven separate validation set 
is an open question. Too few samples may not be 
adequate for a reasonable estimate and may re-
sult in an incorrect result. Too many samples 
would increase the building cost.  
To define a stopping criterion for active learn-
ing, Zhu and hovy (2007) considered the estima-
tion of the classifier?s effectiveness as the second 
1130
task of confidence estimation of the classifier on 
its classification of all remaining unlabeled data. 
In the following section, we first introduce two 
confidence-based criteria, max-confidence and 
min-error, proposed by Zhu and Hovy (2007). 
3.2 Max-Confidence 
In uncertainty sampling scheme, if the uncer-
tainty value of the most informative unlabeled 
example is sufficiently small, we can assume that 
the current classifier has sufficient confidence on 
its classification of the remaining unlabeled data. 
So the active learning process can be ended. 
Based on such assumption, Zhu and Hovy (2007) 
proposed max-confidence criterion based on the 
uncertainty estimation of the most informative 
unlabeled example. Its strategy is to consider 
whether the uncertainty value of the most infor-
mative unlabeled example is less than a very 
small predefined threshold. 
3.3 Min-Error 
As shown in Fig. 1, in uncertainty sampling 
scheme, the current classifier has the least confi-
dence on its classification of these top-m selected 
unlabeled examples. If the current classifier can 
correctly classify these most informative exam-
ples, we can assume that the current classifier 
have sufficient confidence on its classification of 
the remaining unlabeled data. Based on such as-
sumption, Zhu and Hovy (2007) proposed min-
error criterion based on feedback from the oracle. 
Its strategy is to consider whether the current 
classifier can correctly predict the labels on these 
selected unlabeled examples, or the accuracy 
performance of the current classifier on these 
most informative examples is larger than a prede-
fined threshold.  
3.4 Overall-Uncertainty 
The motivation behind the overall-uncertainty 
method is similar to that of the max-confidence 
method. However, the max-confidence method 
only considers the most informative example at 
each learning cycle. The overall-uncertainty 
method considers the overall uncertainty on all 
unlabeled examples. If the overall uncertainty of 
all unlabeled examples becomes very small, we 
can assume that the current classifier has suffi-
cient confidence on its classification of the re-
maining unlabeled data. Based on such assump-
tion, we propose overall-uncertainty method 
which is to consider whether the average uncer-
tainty value of all remaining unlabeled examples 
is less than a very small predefined threshold. 
3.5 Classification-Change 
There is another problem of estimating classifier 
performance during active learning process. 
Cross-validation on the training set is almost im-
practical during the active learning procedure, 
because the alternative of requiring a held-out 
validation set for active learning is counterpro-
ductive. Hence we should look for a self-
contained method. 
Actually the motivation behind uncertainty 
sampling is to find some unlabeled examples 
near decision boundaries, and use them to clarify 
the position of decision boundaries. The current 
classifier considers such unlabeled examples near 
decision boundaries as the most informative ex-
amples in uncertainty sampling scheme for active 
learning. In other words, we assume that an 
unlabeled example with maximum uncertainty 
has the highest chance to change the decision 
boundaries. 
Based on the above analysis, we think the ac-
tive learning process can stop if there is no unla-
beled example that can potentially change the 
decision boundaries. However, in practice, it is 
almost impossible to exactly recognize which 
unlabeled example can truly change the decision 
boundaries in the next learning cycle, because 
the true label of each unlabeled example is un-
known. 
To solve this problem, we make an assump-
tion that labeling an unlabeled example may shift 
the decision boundaries if this example was pre-
viously ?outside? and is now ?inside?. In other 
words, if an unlabeled example is automatically 
assigned to two different labels during two recent 
learning cycles 2 , we think that the labeling of 
this unlabeled example has a good chance to 
change the decision boundaries.  
Based on such assumption, we propose a new 
approach based on classification change of each 
unlabeled example during two recent consecutive 
learning cycles (?previous? and ?current?), called 
the classification-change method. Its strategy is 
to stop the active learning process by considering 
whether no classification change happens to the 
remaining unlabeled examples during two recent 
consecutive learning cycles. If true, we assume 
that the current classifier has sufficient confi-
dence on its classification of the remaining unla-
                                                 
2 For example, an unlabeled example x was classified into 
class A at ith iteration, and class B at i+1th iteration. 
1131
beled data, because all unlabeled examples near 
decision boundaries have been exhausted, and no 
further labeling will affect active learner. 
4 Combination Strategy  
As for the above three confidence-based stopping 
criteria such as max-confidence, min-error and 
overall-uncertainty, how to automatically deter-
mine an appropriate threshold in a specific task is 
a crucial problem. We think that different appro-
priate thresholds are needed for various active 
learning applications.  
To solve this problem, in this section we pro-
pose a general combination strategy by consider-
ing the best of both classification-change and a 
confidence-based criterion, in which the prede-
fined threshold of the confidence-based stopping 
criterion can be automatically updated during 
active learning.  
The motivation behind the general combina-
tion strategy is to check whether the active learn-
ing becomes stable (i.e. check whether the classi-
fication-change method is met) when the current 
confidence-based stopping criterion is satisfied. 
If not, we think there are some remaining unla-
beled examples that can potentially shift the de-
cision boundaries, even if they are considered as 
certain cases from the current classifier?s view-
points. In this case, the threshold of the current 
confidence-based stopping criterion should be 
automatically revised to keep continuing the ac-
tive learning process. The general combination 
strategy can be summarized as follows. 
Procedure: General combination strategy 
Given: 
z stopping criterion 1: max-confidence or min-
error or overall-uncertainty 
z Stopping criterion 2: classification-change 
z The predefined threshold for stopping criterion 1 
is initially set to ? 
Steps(during active learning process): 
1. First check whether stopping criterion 1 is satis-
fied. If yes, go to 2; 
2. Then check whether stopping criterion 2 is satis-
fied. If yes, goto 4), otherwise goto 3; 
3. Automatically update the current threshold to be 
a new smaller value for max-confidence and 
overall-uncertainty, or to be a new larger value 
for min-error, and then goto 1. 
4. Stop active learning process.  
Figure 3. General combination strategy 
 
? Strategy 1: This strategy combines the max-
confidence and classification-change meth-
ods simultaneously.  
? Strategy 2: This strategy combines the min-
error and classification-change methods si-
multaneously. 
? Strategy 3: This strategy combines the over-
all-uncertainty and classification-change 
methods simultaneously. 
5 Evaluation 
5.1 Experimental Settings 
In the following sections, we evaluate the 
effectiveness of seven stopping criteria for active 
learning for WSD and TC tasks, including max-
confidence (MC), min-error (ME), overall-
uncertainty (OU), classification-change (CC), 
strategy 1 (CC-MC), strategy 2 (CC-ME), and 
strategy 3 (CC-OU). Following previous studies 
(Zhu and Hovy, 2007), the predefined thresh-
olds3 used for MC, ME and OU are set to 0.01, 
0.9 and 0.01, respectively. 
To evaluate the effectiveness of each stopping 
criterion, we first construct two types of baseline 
methods called ?All? and ?First? methods. ?All? 
method is defined as when all unlabeled exam-
ples in the pool are learned. ?First? method is 
defined as when the current classifier reaches the 
same performance of the ?All? method at the 
first time during the active learning process.  
A better stopping criterion can not only 
achieve almost the same performance given by 
the ?All? baseline method (i.e. accuracy 
performance), but also learn almost the same 
number of unlabeled examples by the ?First? 
baseline method (i.e. percentage performance).  
In uncertainty sampling scheme, the well-
known entropy-based uncertainty measurement 
(Chen et al, 2006; Schein and Ungar, 2007) is 
used in our active learning study as follows: 
( ) ( | ) log ( | )
y Y
UM x P y x P y x
?
= ??         (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. UM 
is the uncertainty measurement function based on 
the entropy estimation of the classifier?s 
posterior distribution. 
We utilize maximum entropy (MaxEnt) model 
(Berger et al, 1996) to design the basic classifier 
used in active learning for WSD and TC tasks. 
The advantage of the MaxEnt model is the ability 
to freely incorporate features from diverse 
sources into a single, well-grounded statistical 
                                                 
3 In the following experiments, these thresholds are also 
used as initial values of ? for individual criteria in the gen-
eral combination strategy shown in Fig. 3. 
1132
model. A publicly available MaxEnt toolkit4 was 
used in our experiments. To build the MaxEnt-
based classifier for WSD, three knowledge 
sources are used to capture contextual informa-
tion: unordered single words in topical context, 
POS of neighboring words with position infor-
mation, and local collocations, which are the 
same as the knowledge sources used in (Lee and 
Ng, 2002). In the design of text classifier, the 
maximum entropy model is also utilized, and no 
feature selection technique is used. 
In the following active learning comparison 
experiments, the algorithm starts with a 
randomly chosen initial training set of 10 labeled 
examples, and makes 10 queries after each 
learning iteration. A 10 by 10-fold cross-
validation was performed. All results reported 
are the average of 10 trials in each active 
learning process. In the following comparison 
experiments, the performance reported on 
Ontonotes data set is the macro-average on ten 
nouns, and the performance on TWA data set is 
the macro-average on six words. 
5.2 Data Sets 
Six publicly available natural data sets have been 
used in the following active learning comparison 
experiments. Three data sets are used for TC 
tasks: WebKB, Comp2a and Comp2b. The other 
three data sets are used for WSD tasks: 
OntoNotes, Interest and TWA.  
The WebKB dataset was widely used in TC 
research. Following previous studies (McCallum 
and Nigam, 1998b), we use the four most popu-
lous categories: student, faculty, course and pro-
ject, altogether containing 4199 web pages. In 
the preprocessing step, we only remove those 
words that occur merely once without using 
stemming. The resulting vocabulary has 23803 
words. 
The Comp2a data set consists of comp.os.ms-
windows.misc and comp.sys.ibm.pc.hardware 
subset of NewsGroups. The Comp2b data set 
consists of comp.graphics and comp.windows.x 
categories from NewsGroups. Both two data sets 
have been previously used in active learning for 
TC (Roy and McCallum, 2001; Schein and Un-
gar, 2007). 
The OntoNotes project (Hovy et al, 2006) 
uses the WSJ part of the Penn Treebank. The 
senses of noun words occurring in OntoNotes are 
linked to the Omega ontology. Ontonotes has 
                                                 
                                                4See  http://homepages.inf.ed.ac.uk/s0450736/maxent_ 
toolkit.html 
been used previously in active learning for WSD 
tasks (Zhu and Hovy, 2007). In the following 
comparison experiments, we focus on 10 most 
frequent nouns 5  previously used in (Zhu and 
Hovy, 2007): rate, president, people, part, point, 
director, revenue, bill, future, and order.  
The Interest data set developed by Bruce and 
Wiebe (1994) has been previously used for WSD 
(Ng and Lee, 1996). This data set consists of 
2369 sentences of the noun ?interest? with its 
correct sense manually labeled. The noun 
?interest? has six different senses in this data set.  
TWA developed by Mihalcea and Yang on 2003, 
is sense tagged data for six words with two-way 
ambiguities, previously used in WSD research. 
These six words are bass, crane, motion, palm, 
plant and tank. All instances were drawn from 
the British National Corpus. 
5.3 Stopping Criteria for Uncertainty Sam-
pling 
In order to evaluate the effectiveness of our stop-
ping criteria, we first apply them to uncertainty 
sampling for active learning for WSD and TC 
tasks. Table 1 shows that ?First? method gener-
ally achieves higher performance than that of the 
?All? method.  We can see from the ?Average? 
row that stopping criteria MC, ME, CC-MC, CC-
ME and CC-OU achieve close average accuracy 
performance to the ?All? method whereas OU 
and CC achieve lower average accuracy 
performance. OU method achieves the lowest 
average accuracy performance. CC-ME achieves 
the highest average accuracy of 89.6%, followed 
by CC-MC. 
Compared to the ?First? method, CC-OU 
achieves the best average percentage 
performance of 37.03% (i.e. the closest one to 
the ?First? method), followed by ME method. On 
six evaluation data sets, Table 1 shows that CC-
ME method achieves 4 out of 6 highest accuracy 
performances, followed by CC-MC and MC 
methods. And CC-ME method also achieves 3 
out of 6 best percentage performance, followed 
by CC, CC-OU and ME methods.  
Among these four individual stopping criteria, 
ME outperforms MC, OU and CC. However, ME 
method can only be applied to batch-based 
selection because ME criterion is based on the 
feedback from Oracle. Too few informative 
candidates may not be adequate for obtaining a 
reasonable feedback for ME criterion.  
 
5 See http://www.nlplab.com/ontonotes-10-nouns.rar 
1133
Data set All First MC ME OU CC CC-MC CC-ME CC-OU
0.910 0.911 0.910 0.910 0.837 0.912 0.912 0.913 0.912 WebKB 
100% 31.50% 27.11% 29.11% 8.42% 31.53% 32.37% 33.02% 31.53%
0.880 0.884 0.877 0.879 0.868 0.876 0.879 0.880 0.876 Comp2a 
100% 35.12% 31.35% 31.28% 23.29% 27.35% 32.36% 36.80% 27.35% 
0.900 0.901 0.887 0.888 0.880 0.879 0.891 0.893 0.882 Comp2b 
100% 41.66% 37.52% 36.76% 28.36% 30.80% 37.95% 40.03% 31.81% 
0.939 0.942 0.929 0.934 0.928 0.936 0.940 0.939 0.939 Ontonotes 
100% 22.81% 30.19% 22.14% 21.81% 18.96% 34.77% 25.60% 24.75% 
0.908 0.910 0.910 0.906 0.906 0.901 0.910 0.906 0.906 Interest 
100% 29.83% 37.54% 28.25% 28.51% 25.55% 37.54% 28.67% 28.62% 
0.846 0.858 0.843 0.844 0.837 0.820 0.841 0.845 0.838 TWA 
100% 59.67% 80.34% 72.71% 70.47% 61.54% 86.99% 80.15% 78.12% 
0.897 0.901 0.892 0.893 0.876 0.887 0.895 0.896 0.892 Average 
100% 37.43% 40.67% 36.71% 30.14% 32.62% 43.66% 40.71% 37.03%
Table 1. Effectiveness of seven stopping criteria for uncertainty sampling for active learning. For each 
data set, Table 1 shows the accuracy of the classifier and percentage of learned instances over all 
unlabeled data when each stopping criterion is met. The boldface numbers indicate the best corre-
sponding performances. 
Data set All MC ME OU CC CC-MC CC-ME CC-OU 
0.858 0.808 0.818 0.601 0.820 0.820 0.824 0.820 WebKB 
100% 27.11% 29.11% 8.42% 31.53% 32.37% 33.02% 31.53% 
0.894 0.838 0.839 0.825 0.837 0.838 0.846 0.837 Comp2a 
100% 31.35% 31.28% 23.29% 27.35% 32.36% 36.80% 27.35% 
0.922 0.884 0.882 0.878 0.874 0.885 0.883 0.879 Comp2b 
100% 37.52% 36.76% 28.36% 30.80% 37.95% 40.03% 31.81% 
0.925 0.923 0.924 0.921 0.921 0.932 0.927 0.929 Ontonotes 
100% 30.19% 22.14% 21.81% 18.96% 34.77% 25.60% 24.75% 
0.899 0.906 0.890 0.890 0.885 0.906 0.891 0.890 Interest 
100% 37.54% 28.25% 28.51% 25.55% 37.54% 28.67% 28.62% 
0.812 0.784 0.793 0.765 0.775 0.799 0.810 0.794 TWA 
100% 80.34% 72.71% 70.47% 61.54% 86.99% 80.15% 78.12% 
0.885 0.857 0.857 0.813 0.852 0.863 0.863 0.858 Average 
100% 40.67% 36.71% 30.14% 32.62% 43.66% 40.71% 37.03% 
Table 2. Effectiveness of seven stopping criteria for heterogeneous uncertainty sampling for active 
learning. Table 2 shows the accuracy of the classifier and percentage of learned instances over all 
unlabeled data when each stopping criterion is met. The boldface numbers indicate the best corre-
sponding performances. 
 
Interestingly, our proposed CC method 
acheves the best macro-average percentage 
performance on the TWA data set, however, 
other criteria work poorly, compared to the 
?First? method. Actually the sense distribution of 
each noun in TWA set is very skewed. From 
WSD experimental results on TWA, we found 
that only few learned instances can train the 
MaxEnt-based classifier with the highest 
accuracy performance.  
In Table 1, the boldface numbers indicate the 
best performances. Three combination strategies 
achieve 12 out of 16 best performances 6 . We 
                                                                                                                          
6 CC and CC-OU methods achieve the same best percentage 
performance of 31.53% on WebKB data set. MC and CC-
think the general combination strategy 
outperform individual stopping criteria for 
uncertainty sampling for active learning, because 
four individual stopping criteria only totally 
achieve 4 out of 16 best performances. 
5.4 Stopping Criteria for Heterogeneous 
Uncertainty Sampling 
In the following comparison experiments on het-
erogeneous uncertainty sampling, a MaxEnt-
based classifier is used to select the most infor-
mative examples for training an another type of 
classifier based on multinomial na?ve Bayes (NB) 
model (McCallum and Nigam, 1998b).  
 
MC methods achieve the same highest accuracy perform-
ance of 91% on Interest data set.  
1134
Table 2 shows that the NB-based classifier 
trained on all data (i.e. ?All method?) achieves 
only 1.2% lower average accuracy performance 
than that of MaxEnt-based classifier. However, 
we can see from Table 2 that accuracy perform-
ances of each stopping criterion for heterogene-
ous uncertainty sampling are apparently lower 
than that for uncertainty sampling shown in Ta-
ble 1. The main reason is that an uncertain ex-
ample for one classifier (i.e. MaxEnt) may not be 
an uncertain example for other classifiers (i.e. 
NB). This comparison experiments aim to ana-
lyze the accuracy effectiveness of stopping crite-
ria for heterogeneous uncertainty sampling, 
compared to that for uncertainty sampling shown 
in Table 1. Therefore we do not provide the re-
sults of the ?First? method for heterogeneous 
uncertainty sampling. The ?Average? row shows 
that CC-MC and CC-ME achieve the highest 
average accuracy performance of 86.3%, fol-
lowed by CC-OU. On six data sets, CC-ME 
achieves 3 out of 6 highest accuracy perform-
ances.  
Interestingly, these stopping criteria work very 
well on the Ontonotes and Interest data sets. 
Three combination strategies achieve higher ac-
curacy performance than the ?All? method on 
Ontonotes. However, the accuracy performances 
of these seven stopping criteria for heterogene-
ous uncertainty sampling on WebKB, Comp2a, 
Comp2b, and TWA degrade, compared to the 
?All? method.  
The general combination strategy achieves 7 
out of 9 boldface accuracy performances7. And 
only MC method achieves other 2 boldface accu-
racy performances. Experimental results show 
that the general combination strategy outper-
forms individual stopping criteria in overall for 
heterogeneous uncertainty sampling.   
6 Related Work 
 Zhu and Hovy (2007) proposed a confidence-
based framework to predict the upper bound and 
the lower bound for a stopping criterion in active 
learning. Actually this framework is a very 
coarse solution that simply uses max-confidence 
method to predict the upper bound, and uses min-
error method to predict the lower bound. Zhu et. 
al. (2008) proposed a minimum expected error 
strategy to learn a stopping criterion through es-
                                                 
7 MC and CC-MC methods achieve the same highest accu-
racy performance of 90.6% on Interest data set. CC-MC and 
CC-CA methods achieve the same highest average accuracy 
performance of 86.3%. 
timation of the classifier?s expected error on fu-
ture unlabeled examples. However, both two 
studies did not give an answer to the problem of 
how to define an appropriate threshold for the 
stopping criterion in a specific task.   
Vlachos (2008) also studied a stopping crite-
rion of active learning based on the estimate of 
the classifier?s confidence, in which a separate 
and large dataset is prepared in advance to esti-
mate the classifier?s confidence. However, there 
is a risk to be misleading because how many 
examples are required for this pregiven separate 
dataset is an open question in real-world 
applications, and it can not guarantee that the 
classifier shows a rise-peak-drop confidence 
pattern during active learning process.  
Schohn and Cohn (2000) proposed a stopping 
criterion for active learning with support vector 
machines based on an assumption that the data 
used is linearly separable. However, in most real-
world cases this assumption seems to be 
unreasonable and difficult to satisfy. And their 
stopping criterion cannot be applied for active 
learning with other type of classifier such as NB, 
MaxEnt models.  
7 Discussion 
We believe that a classifier?s performance 
change is a good signal of stopping the active 
learning process. It is worth studying further how 
to combine the factor of performance change 
with our proposed stopping criteria. 
Among these stopping criteria, ME, CC, CC-
ME can be used directly for committee-based 
sampling (Engelson and Dagan, 1999) for active 
learning. However, to use MC, OU, CC-MC and 
CC-OU for committee-based sampling, we 
should adopt a new uncertainty measurement 
such as vote entropy to measure the uncertainty 
of each unlabled example in the pool. 
In the above active learning comparison 
experiments, the confidence estimation for each 
confidence-based stopping criterion is done 
within the unlabeled pool U. We think that for 
these confidence-based stopping criteria except 
SA method, confidence estimation on a large-
scale outside unlabeled data set is worth studying 
in the future work. 
8 Conclusion and Future Work 
In this paper, we address the stopping criterion 
issue of active learning, and propose a new stop-
ping criterion, classification-change, which con-
siders the potential ability of each unlabeled ex-
1135
ample on changing decision boundaries. To solve 
the problem of predefining an appropriate 
threshold for each confidence-based stopping 
criterion, a multi-criteria-based general combina-
tion strategy is proposed. Experimental results on 
uncertainty sampling and heterogeneous uncer-
tainty sampling show that these stopping criteria 
work well on evaluation data sets, and combina-
tion strategies can achieve better performance 
than individual criteria. Some interesting future 
work is to investigate further how to combine the 
best of these criteria, and how to consider per-
formance change to define an appropriate stop-
ping criterion for active learning.  
Acknowledgments 
This work was supported in part by the National 
863 High-tech Project (2006AA01Z154) and the 
Program for New Century Excellent Talents in 
University (NCET-05-0287). 
References 
Berger Adam L., Vincent J. Della Pietra, Stephen A. 
Della Pietra. 1996. A maximum entropy approach 
to natural language processing. Computational 
Linguistics 22(1):39?71. 
Bruce Rebecca and Janyce Wiebe. 1994. Word sense 
disambiguation using decomposable models. Pro-
ceedings of the 32nd annual meeting on Associa-
tion for Computational Linguistics, pp. 139-146. 
Chen Jinying, Andrew Schein, Lyle Ungar and Mar-
tha Palmer. 2006. An empirical study of the behav-
ior of active learning for word sense disambigua-
tion. Proceedings of the main conference on Hu-
man Language Technology Conference of the 
North American Chapter of the Association of 
Computational Linguistics, pp. 120-127 
Engelson S. Argamon and I. Dagan. 1999. Commit-
tee-based sample selection for probabilistic classi-
fiers. Journal of Artificial Intelligence Research 
(11):335-360. 
Hovy Eduard, Mitchell Marcus, Martha Palmer, 
Lance Ramshaw and Ralph Weischedel. 2006. 
Ontonotes: The 90% Solution. In Proceedings of 
the Human Language Technology Conference of 
the NAACL, pp. 57-60. 
Lee Yoong Keok and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithm for word sense disambiguation. In 
Proceedings of the ACL conference on Empirical 
methods in natural language processing, pp. 41-48 
Lewis David D. and Jason Catlett. 1994. Heterogene-
ous uncertainty sampling for supervised learning. 
In Proceedings of 11th International Conference on 
Machine Learning, pp. 148-156 
Lewis David D. and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In 
Proceedings of the 17th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, pp. 3-12 
McCallum Andrew and Kamal Nigam. 1998a. Em-
ploying EM in pool-based active learning for text 
classification. In Proceedings of the 15th Interna-
tional Conference on Machine Learning, pp.350-
358 
McCallum Andrew and Kamal Nigam. 1998b. A 
comparison of event models for na?ve bayes text 
classification. In AAAI-98 workshop on learning 
for text categorization. 
Ng Hwee Tou and Hian Beng Lee. 1996. Integrating 
multiple knowledge sources to disambiguate word 
sense: an exemplar-based approach. In Proceed-
ings of the 34th Annual Meeting of the Association 
for Computational Linguistics, pp.40-47 
Ngai Grace and David Yarowsky. 2000. Rule writing 
or annotation: cost-efficient resource usage for 
based noun phrase chunking. In Proceedings of the 
38th Annual Meeting of the Association for Com-
putational Linguistics, pp. 117-125 
Roy Nicholas and Andrew McCallum. 2001. Toward 
optimal active learning through sampling estima-
tion of error reduction. In Proceedings of the 
Eighteenth International Conference on Machine 
Learning, pp. 441-448 
Schein Andrew I. and Lyle H. Ungar. 2007. Active 
learning for logistic regression: an evaluation. 
Machine Learning 68(3): 235-265 
Schohn Greg and David Cohn. 2000. Less is more: 
Active learning with support vector machines. In 
Proceedings of the Seventeenth International Con-
ference on Machine Learning, pp. 839-846 
Shen Dan, Jie Zhang, Jian Su, Guodong Zhou and 
Chew-Lim Tan. 2004. Multi-criteria-based active 
learning for named entity recognition. In Proceed-
ings of the 42nd Annual Meeting on Association 
for Computational Linguistics. 
Tang Min, Xiaoqiang Luo and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, 
pp. 120-127 
Vlachos Andreas. 2008. A stopping criterion for ac-
tive learning. Computer Speech and Language. 
22(3): 295-312 
Zhu Jingbo and Eduard Hovy. 2007. Active learning 
for word sense disambiguation with methods for 
addressing the class imbalance problem. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pp. 
783-790 
Zhu Jingbo, Huizhen Wang and Eduard Hovy. 2008. 
Learning a stopping criterion for active learning 
for word sense disambiguation and text classifica-
tion. In Proceedings of the Third International Joint 
Conference on Natural Language Processing, pp. 
366-372 
1136
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1137?1144
Manchester, August 2008
Active Learning with Sampling by Uncertainty and Density for Word 
Sense Disambiguation and Text Classification  
Jingbo Zhu  Huizhen Wang  Tianshun Yao 
Natural Language Processing Laboratory 
Northeastern University 
Shenyang, Liaoning, P.R.China 110004 
zhujingbo@mail.neu.edu.cn 
wanghuizhen@mail.neu.edu.cn 
Benjamin K Tsou 
Language Information Sciences 
Research Centre 
City University of Hong Kong 
HK, P.R.China 
rlbtsou@cityu.edu.hk 
 
Abstract 
This paper addresses two issues of active 
learning. Firstly, to solve a problem of 
uncertainty sampling that it often fails by 
selecting outliers, this paper presents a 
new selective sampling technique, sam-
pling by uncertainty and density (SUD), 
in which a k-Nearest-Neighbor-based 
density measure is adopted to determine 
whether an unlabeled example is an out-
lier. Secondly, a technique of sampling 
by clustering (SBC) is applied to build a 
representative initial training data set for 
active learning. Finally, we implement a 
new algorithm of active learning with 
SUD and SBC techniques. The experi-
mental results from three real-world data 
sets show that our method outperforms 
competing methods, particularly at the 
early stages of active learning.  
1 Introduction 
Creating a large labeled training corpus is expen-
sive and time-consuming in some real-world ap-
plications (e.g. word sense annotation), and is 
often a bottleneck to build a supervised classifier 
for a new application or domain. Our study aims 
to minimize the amount of human labeling ef-
forts required for a supervised classifier (e.g. for 
automated word sense disambiguation) to 
achieve a satisfactory performance by using ac-
tive learning.  
Among the techniques to solve the knowledge 
bottleneck problem, active learning is a widely 
used framework in which the learner has the abil-
ity to automatically select the most informative 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
unlabeled examples for human annotation. The 
ability of the active learner can be referred to as 
selective sampling. Uncertainty sampling (Lewis 
and Gale, 1994) is a popular selective sampling 
technique, and has been widely studied in natural 
language processing (NLP) applications such as 
word sense disambiguation (WSD) (Chen et al, 
2006; Chan and Ng, 2007), text classification 
(TC) (Lewis and Gale, 1994; Zhu et al, 2008), 
statistical syntactic parsing (Tang et al, 2002), 
and named entity recognition (Shen et al, 2004).  
Actually the motivation behind uncertainty 
sampling is to find some unlabeled examples 
near decision boundaries, and use them to clarify 
the position of decision boundaries. However, 
uncertainly sampling often fails by selecting out-
liers (Roy and McCallum, 2001; Tang et al, 
2002). These selected outliers (i.e. unlabeled ex-
amples) have high uncertainty, but can not pro-
vide much help to the learner. To solve the out-
lier problem, we proposed in this paper a new 
method, sampling by uncertainty and density 
(SUD), in which a K-Nearest-Neighbor-based 
density (KNN-density) measure is used to deter-
mine whether an unlabeled example is an outlier, 
and a combination strategy based on KNN-
density measure and uncertainty measure is de-
signed to select the most informative unlabeled 
examples for human annotation at each learning 
iteration.  
The second effort we made is to study how to 
build a representative initial training data set for 
active learning. We think building a more repre-
sentative initial training data set is very helpful 
for active learning. In previous studies on active 
learning, the initial training data set is generally 
generated at random, based on an assumption 
that random sampling will be likely to build the 
initial training set with same prior data distribu-
tion as that of whole corpus. However, this situa-
tion seldom occurs in real-world applications due 
to the small size of initial training set used. In 
1137
this paper, we utilize an approach, sampling by 
clustering (SBC), to selecting the most represen-
tative examples to form initial training data set 
for active learning. To do it, the whole unlabeled 
corpus should be first clustered into predefined 
number of clusters (i.e. the predefined size of the 
initial training data set). The example closest to 
the centroid of each cluster will be selected to 
augment initial training data set, which is consid-
ered as the most representative case.  
Finally, we describe an implementation of ac-
tive learning with SUD and SBC techniques. Ex-
perimental results of active learning for WSD 
and TC tasks show that our proposed method 
outperforms competing methods, particularly at 
the early stages of active learning process. It is 
noteworthy that these proposed techniques are 
easy to implement, and can be easily applied to 
several learners, such as Maximum Entropy 
(ME), na?ve Bayes (NB) and Support Vector 
Machines (SVMs). 
2 Active Learning Process 
In this work, we are interested in uncertainty 
sampling (Lewis and Gale, 1994) for pool-based 
active learning, in which an unlabeled example x 
with maximum uncertainty is selected for human 
annotation at each learning cycle. The maximum 
uncertainty implies that the current classifier (i.e. 
the learner) has the least confidence on its classi-
fication of this unlabeled example.  
Actually active learning is a two-stage process 
in which a small number of labeled samples and 
a large number of unlabeled examples are first 
collected in the initialization stage, and a closed-
loop stage of query and retraining is adopted.  
Procedure: Active Learning Process 
Input: initial small training set L, and pool of unla-
beled data set U 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique to select 
m2  most informative unlabeled examples, and 
ask oracle H for labeling 
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning with uncertainty sam-
pling technique 
                                                 
2 A batch-based sample selection labels the top-m most 
informative unlabeled examples at each learning cycle to 
decrease the number times the learner is retrained. 
3 Uncertainty Measures 
In real-world applications, only limited size of 
training sample set can be provided to train a 
supervised classifier. Due to manual efforts in-
volved, such brings up a considerable issue: what 
is the best subset of examples to annotate. In the 
uncertainty sampling scheme, the unlabeled ex-
ample with maximum uncertainty is viewed as 
the most informative case. The key point of un-
certainty sampling is how to measure the uncer-
tainty of an unlabeled example x. 
3.1 Entropy Measure 
The well-known entropy is a popular uncertainty 
measurement widely used in previous studies on 
active learning (Tang et al, 2002; Chen et al 
2006; Zhu and Hovy, 2007): 
?
?
?=
Yy
xyPxyPxH )|(log)|()(             (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. H is 
the uncertainty measurement function based on 
the entropy estimation of the classifier?s 
posterior distribution. 
In the following comparison experiments, the 
uncertainty sampling based on entropy criterion 
is considered as the baseline method, also called 
traditional uncertainty sampling.  
3.2 Density*Entropy Measure 
To analyze the outlier problem of traditional un-
certainty sampling, we first give an example to 
explain our motivation. 
 
Figure 2. An example of two points A and B with 
maximum uncertainty at the ith learning iteration 
 
As mentioned in Section 1, the motivation be-
hind uncertainty sampling is to find some unla-
beled examples near decision boundaries, and 
assume that these examples have the maximum 
uncertainty. Fig. 2 shows two unlabeled exam-
ples A and B with maximum uncertainty at the ith 
1138
learning cycle. Roughly speaking, there are three 
unlabeled examples near or similar to B, but, 
none for A. We think example B has higher rep-
resentativeness than example A, and A is likely 
to be an outlier. We think adding B to the train-
ing set will help the learner more than A.  
The motivation of our study is that we prefer 
not only the most informative example in terms 
of uncertainty measure, but also the most repre-
sentative example in terms of density measure. 
The density measure can be evaluated based on 
how many examples there are similar or near to it. 
An example with high density degree is less 
likely to be an outlier.  
In most real-world applications, because the 
scale of unlabeled corpus would be very large, 
Tang et al (2002) and Shen et al (2004) evalu-
ated the density of an example within a cluster. 
Unlike their work 3 , we adopt a new approach, 
called K-Nearest-Neighbor-based density (KNN-
density) measure, to evaluating the density of an 
unlabeled example x. Given a set of K (i.e. =20 
used in our experiments) most similar examples 
S(x)={s1, s2, ?, sK} of the example x,  the KNN-
density DS(.) of example x is defined as: 
K
sx
xDS xSs
i
i
?
?= )(
),cos(
)(                     (2) 
As discussed above, we prefer to select exam-
ples with maximum uncertainty and highest den-
sity for human annotation. We think getting their 
labels can help the learner greatly. To do it, we 
proposed a new method, sampling by uncertainty 
and density (SUD), in which entropy-based un-
certainty measure and KNN-density measure are 
considered simultaneously.  
In SUD scheme, a new uncertainty measure, 
called density*entropy measure4 , is defined as: 
)()()( xHxDSxDSH ?=                 (3) 
4 Initial Training Set Generation 
As shown in Fig. 1, only a small number of train-
ing samples are provided at the beginning of ac-
tive learning process. In previous studies on ac-
tive learning, the initial training set is generally 
generated by random sampling from the whole 
unlabeled corpus. However, random sampling 
technique can not guarantee selecting a most rep-
                                                 
3 We also tried their cluster-based density measure, but per-
formance was essentially degraded.  
4 We also tried other ways like ?*DS(x)+(1-?) H(x) 
measure used in previous studies, but it seems to be random. 
Actually it is very difficult to determine an appropriate? 
value for a specific task.  
resentative subset, because the size of initial 
training set is generally too small (e.g. 10). We 
think selecting some representative examples to 
form initial training set can help the active 
learner.  
In this section we utilize an approach, sam-
pling by clustering (SBC), to selecting the most 
representative examples to form initial training 
data set. In the SBC scheme, the whole unlabeled 
corpus has been first clustered into a predefined 
number of clusters (i.e. the predefined size of the 
initial training set). The example closest to the 
centroid of each cluster will be selected to aug-
ment initial training set, which is viewed as the 
most representative case.  
We use the K-means clustering algorithm 
(Duda and Hart, 1973) to cluster examples in the 
whole unlabeled corpus. In the following K-
means clustering algorithm, the traditional cosine 
measure is adopted to estimate the similarity be-
tween two examples, that is 
ji
ji
ji ww
ww
ww ?
?=),cos(                     (4) 
where wi and wj are the feature vectors of the ex-
amples i and j.  
To summarize the SBC-based initial training 
set generation algorithm, let U={U1, U2, ?, UN} 
be the set of unlabeled examples to be clustered, 
and k be the predefined size of initial training 
data set. In other words, SBC technique selects k 
most representative unlabeled examples from U 
to generate the initial training data set. The SBC-
based initial training set generation procedure is 
summarized as follows: 
SBC-based Initial Training Set Generation 
Input: U, k 
Phrase 1: Cluster the corpus U into k clusters 
? j(j=1,?,k) by using K-means clustering algo-
rithm as follows: 
1. Initialization. Randomly choosing k exam-
ples as the centroid ?j(j=1,?,k) for initial 
clusters ? j(j=1,?,k), respectively.  
2. Re-partition {U1, U2, ?, UN} into k clus-
ters ?  j(j=1,?,k), where 
}.),,cos(),cos(:{ jtUUU tijiij ??=? ??  
3. Re-estimate the centroid ?j for each clus-
ters ? j, that is: 
m
U
jiU
i
j
?
??=? , where m is the size of ?  j.
4. Repeat Step 2 and Step 3 until the algo-
rithm converges. 
1139
Phrase 2: Select the example uj closest to the 
centroid?j for each cluster j to augment ini-
tial training data set ?, where 
?
]},1[,),,cos(),cos(:{ kjUuUuu ijjijjj ???=? ??
Return?; 
 
The computation complexity of the K-means 
clustering algorithm is O(NdkT), where d is the 
number of features and T is the number of itera-
tions. In practice, we can define the stopping cri-
terion (i.e. shown in Step 4) of K-means cluster-
ing algorithm that relative change of the total 
distortion is smaller than a threshold.  
5 Active Learning with SUD and SBC  
Procedure: Active Learning with SUD and SBC 
Input: Pool of unlabeled data set U; k is the prede-
fined size of initial training data set 
Initialization.  
z Evaluate the density of each unlabeled example 
in terms of KNN-density measure; 
z Use SBC technique to generate the small initial 
training data set of size k. 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique in terms 
of density*entropy measure to select m most 
informative unlabeled examples, and ask ora-
cle H for labeling, namely SUD scheme.  
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 3. Active learning with SUD and SBC  
 
Fig. 3 shows the algorithm of active learning 
with SUD and SBC techniques. Actually there 
are some variations. For example, if the initial 
training data set is generated by SBC, and en-
tropy-based uncertainty measure is used, it is 
active learning with SBC. Similarly, if the initial 
training data set is generated at random, and the 
density*entropy uncertainty measure is used, it is 
active learning with SUD. If both SBC and SUD 
techniques are not used, we call it (traditional) 
uncertainty sampling as baseline method.  
6 Evaluation 
In the following comparison experiments, we 
evaluate the effectiveness of various active learn-
ing methods for WSD and TC tasks on three pub-
licly available real-world data sets. 
6.1 Deficiency Measure 
To compare various active learning methods, 
deficiency is a statistic developed to compare 
performance of active learning methods globally 
across the learning curve, which has been used in 
previous studies (Schein and Unga, 2007). The 
deficiency measure can be defined as: 
?
?
=
=
?
?= n
t tn
n
t tn
n
REFaccREFacc
ALaccREFacc
REFALDef
1
1
))()((
))()((
),( (5) 
where acct is the average accuracy at tth learning 
iteration. REF is the baseline active learning 
method, and AL is the active learning variant of 
the learning algorithm of REF, e.g. active learn-
ing with SUD and SBC. n refers to the evaluation 
stopping points (i.e. the number of learned ex-
amples). Smaller deficiency value (i.e. <1.0) in-
dicates AL method is better than REF method. 
Conversely, a larger value (i.e. >1.0) indicates a 
negative result. 
In the following comparison experiments, we 
evaluate the effectiveness of six active learning 
methods, including random sampling (random), 
uncertainty sampling (uncertainty), SUD, ran-
dom sampling with SBC (random+SBC), uncer-
tainty sampling with SBC (uncertainty+SBC), 
and SUD with SBC (SUD+SBC). ?+SBC? indi-
cates initial training data set generated by SBC 
technique. Otherwise, initial training set is gen-
erated at random. To evaluate deficiency of each 
method, the REF method (i.e. the baseline 
method) defined in Equation (5) refers to (tradi-
tional) uncertainty sampling. 
6.2 Experimental Settings 
We utilize a maximum entropy (ME) model 
(Berger et al, 1996) to design the basic classifier 
for WSD and TC tasks. The advantage of the ME 
model is the ability to freely incorporate features 
from diverse sources into a single, well-grounded 
statistical model. A publicly available ME tool-
kit 5  was used in our experiments. To build the 
ME-based classifier for WSD, three knowledge 
sources are used to capture contextual informa-
tion: unordered single words in topical context, 
POS of neighboring words with position infor-
mation, and local collocations, which are the 
same as the knowledge sources used in (Lee and 
Ng, 2002). In the design of text classifier, the 
maximum entropy model is also utilized, and no 
feature selection technique is used. 
                                                 
5See  http://homepages.inf.ed.ac.uk/s0450736/maxent_ 
toolkit.html 
1140
In the following comparison experiments, the 
algorithm starts with a initial training set of 10 
labeled examples, and make 10 queries after each 
learning iteration. A 10 by 10-fold cross-
validation was performed. All results reported 
are the average of 10 trials in each active 
learning process.  
6.3 Data Sets 
Three publicly available natural data sets have 
been used in the following active learning com-
parison experiments. Interest data set is used for 
WSD tasks. Comp2 and WebKB data sets are 
used for TC tasks.  
The Interest data set developed by Bruce and 
Wiebe (1994) has been previously used for WSD 
(Ng and Lee, 1996). This data set consists of 
2369 sentences of the noun ?interest? with its 
correct sense manually labeled. The noun 
?interest? has six different senses in this data set.  
The Comp2 data set consists of comp.graphics 
and comp.windows.x categories from News-
Groups,  which has been previously used in ac-
tive learning for TC (Roy and McCallum, 2001; 
Schein and Ungar, 2007). 
The WebKB dataset was widely used in TC 
research. Following previous studies (McCallum 
and Nigam, 1998), we use the four most popu-
lous categories: student, faculty, course and pro-
ject, altogether containing 4199 web pages. In 
the preprocessing step, we remove those words 
that occur merely once without using stemming. 
The resulting vocabulary has 23803 words. 
 
Data sets Interest Comp2 WebKB 
Accuracy 0.908 0.90 0.91 
Table 1. Average accuracy of supervised learning 
on each data set when all examples have been 
learned. 
6.4 Active Learning for WSD Task 
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250  300
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD on Interest
random
random + SBC
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 4. Active learning curve for WSD on In-
terest data set 
 
 
Random Random+SBC Uncertainty 
1.926 1.886 NA 
Uncertainty+SBC SUD SUD+SBC 
0.947 0.811 0.758 
Table 2. Average deficiency achieved by various 
active learning methods on Interest data set. The 
stopping point is 300.  
 
Fig. 4 depicts performance curves of various ac-
tive learning methods for WSD task on Interest 
data set. Among these six methods, random sam-
pling method shows the worst performance. SUD 
method constantly outperforms uncertainty sam-
pling. As discussed above, SUD method prefers 
not only the most uncertainty examples, but also 
the most representative examples. In the SUD 
scheme, the factor of KNN-density can effec-
tively avoid selecting the outliers that often cause 
uncertainty sampling to fail.  
It is noteworthy that using SBC to generate 
initial training data set can improve random (-
0.04 deficiency), uncertainty (-0.053 deficiency) 
and SUD (-0.053 deficiency) methods, respec-
tively. If the initial training data set is generated 
at random, the initial accuracy is only 55.6%. 
Interestingly, SBC achieves 62.2% initial accu-
racy, and makes 6.6% accuracy performance im-
provement. However, SBC only makes perform-
ance improvement for each method at the early 
stages of active learning. After 50 unlabeled ex-
amples have been learned, it seems that SBC has 
very little contribution to random, uncertainty 
and SUD methods. Table 2 shows that the best 
method is SUD with SBC (0.758 deficiency), 
followed by SUD method. 
6.5 Active Learning for TC Tasks 
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  50  100  150
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for Text Classification on Comp2
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 5. Active learning curve for text classifi-
cation on Comp2 data set 
 
Uncertainty Uncertainty+SBC SUD SUD+SBC
NA 0.409 0.588 0.257 
Table 3. Average deficiency achieved by various 
active learning methods on Comp2 data set. The 
stopping point is 150.  
1141
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0  50  100  150
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for Text Classification on WebKB
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 6. Active learning curve for text classifi-
cation on WebKB data set 
 
Uncertainty Uncertainty+SBC SUD SUD+SBC 
NA 0.669 0.748 0.595 
Table 4. Average deficiency achieved by various 
active learning methods on WebKB data set. The 
stopping point is 150.  
 
Fig. 5 and 6 show the effectiveness of various 
active learning methods for text classification 
tasks. Since random sampling performs poorly as 
shown in Fig. 4, it is not further shown in Fig. 5 
and 6. We only compare uncertainty sampling 
and our proposed methods for both text classifi-
cation tasks.  
Similarly, SUD method constantly outper-
forms uncertainty sampling on two data sets. 
SBC greatly improves uncertainty sampling (i.e. 
0.591 and 0.331 deficiencies degraded) and SUD 
method (i.e. 0.331 and 0.153 deficiencies de-
graded), respectively. Interestingly, unlike WSD 
task shown in Fig. 4, Table 3 and 4 show that 
uncertainty sampling with SBC outperforms our 
SUD method for text classification on both data 
sets. The reason is that SBC makes about 15% 
initial accuracy improvement on Comp2 data set, 
and about 23% initial accuracy improvement on 
WebKB data set. Such improvements indicate 
that selecting high representative initial training 
set is very necessary and helpful for active learn-
ing. Table 3 and 4 show that the best active 
learning method for TC task is SUD with SBC, 
following by uncertainty sampling with SBC 
method. It is noteworthy that on WebKB uncer-
tainty sampling with SBC (0.669 deficiency) 
achieves only slight better performance than 
SUD method (0.748 deficiency) as shown in Ta-
ble 4, simply because SBC only introduce good 
performance improvement at the early stages. 
Actually on WebKB SUD method achieves 
slight better performance than uncertainty sam-
pling with SBC after about 50 unlabeled exam-
ples have been learned. 
7 Related Work 
In recent years active learning has been widely 
studied in various natural language processing 
(NLP) tasks, such as word sense disambiguation 
(Chen et al, 2006; Zhu and Hovy, 2007), text 
classification (TC) (Lewis and Gale, 1994; 
McCallum and Nigam, 1998), named entity 
recognition (NER) (Shen et al, 2004), chunking 
(Ngai and Yarowsky, 2000), information 
extraction (IE) (Thompson et al, 1999), and 
statistical parsing (Tang et al, 2002). 
In addition to uncertainty sampling, there is 
another popular selective sampling scheme, 
Query-by-committee (Engelson and Dagan, 
1999), which generates a committee of classifiers 
(always more than two classifiers) and selects the 
next unlabeled example by the principle of 
maximal disagreement among these classifiers. A 
method similar to committee-based sampling is 
co-testing proposed by Muslea et al (2000), 
which trains two learners individually on two 
compatible and uncorrelated views that should be 
able to reach the same classification accuracy. In 
practice, however, these conditions of view se-
lection are difficult to meet in real-world applica-
tions. Cohn et al (1996) and Roy and McCallum 
(2001) proposed a method that directly optimizes 
expected future error on future test examples. 
However, the computational complexity of their 
methods is very high.  
There are some similar previous studies (Tang 
et al, 2002; Shen et al, 2004) in which the rep-
resentativeness criterion in active learning is 
considered. Unlike our sampling by uncertainty 
and density technique, Tang et al (2002) adopted 
a sampling scheme of most uncertain per cluster 
for NLP parsing, in which the learner selects the 
sentence with the highest uncertain score from 
each cluster, and use the density to weight the 
selected examples while we use density informa-
tion to select the most informative examples. Ac-
tually the scheme of most uncertain per cluster 
still can not solve the outlier problem faced by 
uncertainty sampling technique. Shen et al 
(2004) proposed an approach to selecting exam-
ples based on informativeness, representativeness 
and diversity criteria. In their work, the density 
of an example is evaluated within a cluster, and 
multiple criteria have been linearly combined 
with some coefficients. However, it is difficult to 
automatically determine sufficient coefficients in 
real-world applications. Perhaps there are differ-
ent appropriate coefficients for various applica-
tions.  
1142
8 Discussion 
For batch mode active learning, we found some-
times there is a redundancy problem that some 
selected examples are identical or similar. Such 
situation would reduce the representativeness of 
selected examples. To solve this problem, we 
tried the sampling scheme of ?most uncertain per 
cluster? (Tang et al, 2002) to select the most 
informative examples. We think selecting exam-
ples from each cluster can alleviate the redun-
dancy problem. However, this sampling scheme 
works poorly for WSD and TC on the three data 
sets, compared to traditional uncertainty sam-
pling. From the clustering results, we found these 
resulting clusters are very imbalanced. It makes 
sense that more informative examples are con-
tained in a bigger cluster. In this work, we only 
use SUD technique to select the most informative 
examples for active learning. We plan to study 
how combining SBC and SUD techniques can 
enhance the selection of the most informative 
examples in the future work. 
Furthermore, we think that a misclassified 
unlabeled example may convey more 
information than a correctly classified unlabeled 
example which is closer to the decision boundary. 
But there is a difficulty that the true label of each 
unlabeled example is unknown. To use misclassi-
fication information to select the most informa-
tive examples, we should study how to automati-
cally determine whether an unlabeled example 
has been misclassified. For example, we can 
make an assumption that an unlabeled example 
may be misclassified if this example was previ-
ously ?outside? and is now ?inside?. We will 
study this issue in the future work.  
Actually these proposed techniques can be 
easily applied for committee-based sampling for 
active learning. However, to do so, we should 
adopt a new uncertainty measurement such as 
vote entropy to measure the uncertaity of each 
unlabled example in committee-based sampling 
scheme. 
9 Conclusion and Future Work 
In this paper, we have addressed two issues of 
active learning, involving the outlier problem of 
traditional uncertainty sampling, and initial train-
ing data set generation. To solve the outlier prob-
lem of traditional uncertainly sampling, we pro-
posed a new method of sampling by uncertainty 
and density (SUD) in which KNN-density meas-
ure and uncertainty measure are combined to-
gether to select the most informative unlabeled 
example for human annotation at each learning 
cycle. We employ a method of sampling by clus-
tering (SBC) to generate a representative initial 
training data set. Experimental results on three 
evaluation data sets show that our combined 
SUD with SBC method achieved the best per-
formance compared to other competing methods, 
particularly at the early stages of active learning 
process. In future work, we will focus on the re-
dundancy problem faced by batch mode active 
learning, and how to make use of misclassified 
information to select the most useful examples 
for human annotation. 
Acknowledgments 
This work was supported in part by the National 
863 High-tech Project (2006AA01Z154) and the 
Program for New Century Excellent Talents in 
University (NCET-05-0287). 
References 
Berger Adam L., Vincent J. Della Pietra, Stephen A. 
Della Pietra. 1996. A maximum entropy approach 
to natural language processing. Computational 
Linguistics 22(1):39?71. 
Bruce Rebecca and Janyce Wiebe. 1994. Word sense 
disambiguation using decomposable models. Pro-
ceedings of the 32nd annual meeting on Associa-
tion for Computational Linguistics, pp. 139-146. 
Chan Yee Seng and Hwee Tou Ng. 2007. Domain 
adaptation with active learning for word sense dis-
ambiguation. Proceedings of the 45th annual meet-
ing on Association for Computational Linguistics, 
pp. 49-56 
Chen Jinying, Andrew Schein, Lyle Ungar and 
Martha Palmer. 2006. An empirical study of the 
behavior of active learning for word sense disam-
biguation. Proceedings of the main conference on 
Human Language Technology Conference of the 
North American Chapter of the Association of 
Computational Linguistics, pp. 120-127 
Cohn David A., Zoubin Ghahramani and Michael I. 
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research, 4, 
129?145. 
Duda Richard O. and Peter E. Hart. 1973. Pattern 
classification and scene analysis. New York: 
Wiley. 
Engelson S. Argamon and I. Dagan. 1999. Commit-
tee-based sample selection for probabilistic classi-
fiers. Journal of Artificial Intelligence Research 
(11):335-360. 
1143
Lee Yoong Keok and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithm for word sense disambiguation. In 
Proceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, pp. 41-
48 
Lewis David D. and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In 
Proceedings of the 17th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, pp. 3-12 
McCallum Andrew and Kamal Nigam. 1998. A com-
parison of event models for na?ve bayes text classi-
fication. In AAAI-98 workshop on learning for text 
categorization. 
Muslea Ion, Steven Minton and Craig A. Knoblock. 
2000. Selective sampling with redundant views. In 
Proceedings of the Seventeenth National Confer-
ence on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intel-
ligence, pp. 621-626. 
Ng Hwee Tou and Hian Beng Lee. 1996. Integrating 
multiple knowledge sources to disambiguate word 
sense: an exemplar-based approach. In Proceed-
ings of the Thirty-Fourth Annual Meeting of the 
Association for Computational Linguistics, pp. 40-
47 
 Ngai Grace and David Yarowsky. 2000. Rule writing 
or annotation: cost-efficient resource usage for 
based noun phrase chunking. In Proceedings of the 
38th Annual Meeting of the Association for Com-
putational Linguistics, pp. 117-125 
Roy Nicholas and Andrew McCallum. 2001. Toward 
optimal active learning through sampling estima-
tion of error reduction. In Proceedings of the 
Eighteenth International Conference on Machine 
Learning, pp. 441-448 
Schein Andrew I. and Lyle H. Ungar. 2007. Active 
learning for logistic regression: an evaluation. 
Machine Learning 68(3): 235-265 
Schohn Greg and David Cohn. 2000. Less is more: 
Active learning with support vector machines. In 
Proceedings of the Seventeenth International Con-
ference on Machine Learning, pp. 839-846 
Shen Dan, Jie Zhang, Jian Su, Guodong Zhou and 
Chew-Lim Tan. 2004. Multi-criteria-based active 
learning for named entity recognition. In Proceed-
ings of the 42nd Annual Meeting on Association 
for Computational Linguistics. 
Tang Min, Xiaoqiang Luo and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, 
pp. 120-127 
Thompson Cynthia A., Mary Elaine Califf and Ray-
mond J. Mooney. 1999. Active learning for natural 
language parsing and information extraction. In 
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, pp. 406-414 
Zhu Jingbo and Eduard Hovy. 2007. Active learning 
for word sense disambiguation with methods for 
addressing the class imbalance problem. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pp. 
783-790 
Zhu Jingbo, Huizhen Wang and Eduard Hovy. 2008. 
Learning a stopping criterion for active learning 
for word sense disambiguation and text classifica-
tion. In Proceedings of the Third International Joint 
Conference on Natural Language Processing, pp. 
366-372 
1144
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 362?370,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Better Synchronous Binarization for Machine Translation 
 
 
Tong Xiao*, Mu Li+, Dongdong Zhang+, Jingbo Zhu*, Ming Zhou+ 
 
*Natural Language Processing Lab 
Northeastern University 
Shenyang, China, 110004 
xiaotong@mail.neu.edu.cn 
zhujingbo@mail.neu.edu.cn 
 
+Microsoft Research Asia 
Sigma Center 
Beijing, China, 100080 
muli@microsoft.com 
dozhang@microsoft.com 
mingzhou@microsoft.com 
 
  
 
Abstract 
Binarization of Synchronous Context Free 
Grammars (SCFG) is essential for achieving 
polynomial time complexity of decoding for 
SCFG parsing based machine translation sys-
tems. In this paper, we first investigate the 
excess edge competition issue caused by a left-
heavy binary SCFG derived with the method 
of Zhang et al (2006). Then we propose a new 
binarization method to mitigate the problem 
by exploring other alternative equivalent bi-
nary SCFGs. We present an algorithm that ite-
ratively improves the resulting binary SCFG, 
and empirically show that our method can im-
prove a string-to-tree statistical machine trans-
lations system based on the synchronous bina-
rization method in Zhang et al (2006) on the 
NIST machine translation evaluation tasks. 
1 Introduction 
Recently Statistical Machine Translation (SMT) 
systems based on Synchronous Context Free 
Grammar (SCFG) have been extensively investi-
gated (Chiang, 2005; Galley et al, 2004; Galley 
et al, 2006) and have achieved state-of-the-art 
performance. In these systems, machine transla-
tion decoding is cast as a synchronous parsing 
task. Because general SCFG parsing is an NP-
hard problem (Satta and Peserico, 2005), practic-
al SMT decoders based on SCFG parsing re-
quires an equivalent binary SCFG that is directly 
learned from training data to achieve polynomial 
time complexity using the CKY algorithm (Ka-
sami, 1965; Younger, 1967) borrowed from CFG 
parsing techniques. Zhang et al (2006) proposed 
synchronous binarization, a principled method to 
binarize an SCFG in such a way that both the 
source-side and target-side virtual non-terminals 
have contiguous spans. This property of syn-
chronous binarization guarantees the polynomial 
time complexity of SCFG parsers even when an 
n-gram language model is integrated, which has 
been proved to be one of the keys to the success 
of a string-to-tree syntax-based SMT system. 
However, as shown by Chiang (2007), SCFG-
based decoding with an integrated n-gram lan-
guage model still has a time complexity of  
?(?3 ? 4(??1)), where m is the source sentence 
length, and  ?  is the vocabulary size of the lan-
guage model. Although it is not exponential in 
theory, the actual complexity can still be very 
high in practice. Here is an example extracted 
from real data. Given the following SCFG rule: 
     VP   ?   VB  NP  ?  JJR  , 
               VB  NP  will be  JJR 
we can obtain a set of equivalent binary rules 
using the synchronous binarization method 
(Zhang et al, 2006)  as follows: 
        VP ? V1  JJR ,   V1  JJR 
            V1 ? VB  V2 ,   VB  V2 
        V2 ? NP ? ,   NP  will be 
This binarization is shown with the solid lines as 
binarization (a) in Figure 1. We can see that bi-
narization (a) requires that ?NP ?? should be 
reduced at first. Data analysis shows that ?NP ?? 
is a frequent pattern in the training corpus, and 
there are 874 binary rules of which the source 
language sides are ?NP ??. Consequently these 
binary rules generate a large number of compet-
ing edges in the chart when ?NP ?? is matched 
in decoding. To reduce the number of edges pro-
362
posed in decoding, hypothesis re-combination is 
used to combine the equivalent edges in terms of 
dynamic programming. Generally, two edges can 
be re-combined if they satisfy the following two 
constraints:  1) the LHS (left-hand side) non-
terminals are identical and the sub-alignments 
are the same (Zhang et al, 2006); and 2) the 
boundary words 1  on both sides of the partial 
translations are equal between the two edges 
(Chiang, 2007). However, as shown in Figure 2, 
the decoder still generates 801 edges after the 
hypothesis re-combination. As a result, aggres-
sive pruning with beam search has to be em-
ployed to reduce the search space to make the 
decoding practical. Usually in beam search only 
a very small number of edges are kept in the 
beam of each chart cell (e.g. less than 100). 
These edges have to compete with each other to 
survive from the pruning. Obviously, more com-
peting edges proposed during decoding can lead 
to a higher risk of making search errors.  
 
VB NP ? JJR
(a)(b)
V2
V1
V2'
V1'
VP
VB NP will be JJR
 
Figure 1: Two different binarizations (a) and 
(b) of the same SCFG rule distinguished by the 
solid lines and dashed lines 
 
??   ??   ??   ?   ?? ?
(We hope the situation will be better .)
??   ??   NP   ?   JJR   ?
decoding
match 874 rules match 62 rules
competing edges: 801 competing edges: 57
Figure 2: Edge competitions caused by different 
binarizations 
 
The edge competition problem for SMT de-
coding is not addressed in previous work (Zhang 
et al, 2006; Huang, 2007) in which each SCFG 
rule is binarized in a fixed way. Actually the re-
sults of synchronous binarization may not be the 
only solution. As illustrated in Figure 1, the rule 
                                                 
1 For the case of n-gram language model integration, 
2 ? (? ? 1) boundary words needs to be examined. 
can also be binarized as binarization (b) which is 
shown with the dashed lines.  
We think that this problem can be alleviated 
by choosing better binarizations for SMT decod-
ers, since there is generally more than one bina-
rization for a SCFG rule. In our investigation, 
about 96% rules that need to be binarized have 
more than one binarization under the contiguous 
constraint. As shown in binarization (b) (Figure 
1), ?? JJR? is reduced first. In the decoder, the 
number of binary rules with the source-side ?? 
JJR? is 62, and the corresponding number of 
edges is 57 (Figure 2). The two numbers are both 
much smaller than those of ?NP ?? in (a). This 
is an informative clue that the binarization (b) 
could be better than the binarization (a) based on 
the following: the probability of pruning the rule 
in (a) is higher than that in (b) as the rule in (b) 
has fewer competitors and has more chances to 
survive during pruning. 
In this paper we propose a novel binarization 
method, aiming to find better binarizations to 
improve an SCFG-based machine translation 
system. We formulate the binarization optimiza-
tion as a cost reduction process, where the cost is 
defined as the number of rules sharing a common 
source-side derivation in an SCFG. We present 
an algorithm, iterative cost reduction algorithm, 
to obtain better binarization for the SCFG learnt 
automatically from the training corpus. It can 
work with an efficient CKY-style binarizer to 
search for the lowest-cost binarization. We apply 
our method into a state-of-the-art string-to-tree 
SMT system. The experimental results show that 
our method outperforms the synchronous binari-
zation method (Zhang et al, 2006) with over 0.8 
BLEU scores on both NIST 2005 and NIST 2008 
Chinese-to-English evaluation data sets. 
2 Related Work 
The problem of binarization originates from the 
parsing problem in which several binarization 
methods are studied such as left/right binariza-
tion (Charniak et al, 1998; Tsuruoka and Tsujii, 
2004) and head binarization (Charniak et al, 
2006). Generally, the pruning issue in SMT de-
coding is unnecessary for the parsing problem, 
and the accuracy of parsing does not rely on the 
binarization method heavily. Thus, many efforts 
on the binarization in parsing are made for the 
efficiency improvement instead of the accuracy 
improvement (Song et al, 2008). 
Binarization is also an important topic in the 
research of syntax-based SMT. A synchronous 
363
binarization method is proposed in (Zhang et al, 
2006) whose basic idea is to build a left-heavy 
binary synchronous tree (Shapiro and Stephens, 
1991) with a left-to-right shift-reduce algorithm. 
Target-side binarization is another binarization 
method which is proposed by Huang (2007). It 
works in a left-to-right way on the target lan-
guage side. Although this method is compara-
tively easy to be implemented, it just achieves 
the same performance as the synchronous binari-
zation method (Zhang et al, 2006) for syntax-
based SMT systems. In addition, it cannot be 
easily integrated into the decoding of some syn-
tax-based models (Galley et al, 2004; Marcu et 
al., 2006), because it does not guarantee conti-
guous spans on the source language side. 
3 Synchronous Binarization Optimiza-
tion by Cost Reduction 
As discussed in Section 1, binarizing an SCFG in 
a fixed (left-heavy) way (Zhang et al, 2006) may 
lead to a large number of competing edges and 
consequently high risk of making search errors. 
Fortunately, in most cases a binarizable SCFG 
can be binarized in different ways, which pro-
vides us with an opportunity to find a better solu-
tion than the default left-heavy binarization. An 
ideal solution to this problem could be that we 
define an exact edge competition estimation 
function and choose the best binary SCFG based 
on it. However, even for the rules with a com-
mon source-side, generally it is difficult to esti-
mate the exact number of competing edges in the 
dynamic SCFG parsing process for machine 
translation, because in order to integrate an n-
gram language model, the actual number of 
edges not only depends on SCFG rules, but also 
depends on language model states which are spe-
cific to input sentences. Instead, we have to em-
ploy certain kinds of approximation of it. First 
we will introduce some notations frequently used 
in later discussions. 
3.1 Notations 
We use ? = {?? ?  ?? ? ?? ,??}  to denote an 
SCFG, where ??  is the ?
??  rule in ? ; ??  is the 
LHS (left hand side) non-terminal of ?? ; ??  and 
??  are the source-side and target-side RHS (right 
hand side) derivations of ??  respectively. We use 
? ?  to denote the set of equivalent binary 
SCFG of ?. The goal of SCFG binarization is to 
find an appropriate binary SCFG ?? ? ? ? . For 
?? , ? ?? = {??? } ? ?? ? ? ?  is the set of 
equivalent binary rules based on ?? , where ???  is 
the ???  binary rule in ? ?? . Figure 3 illustrates 
the meanings of these notations with a sample 
grammar. 
 
VP ?  VB NP ? JJR  ,   VB NP will be JJR
S   ?  NP ? VP  ,           NP will VP
R1 :
R2 :
G
VP ? V
12
 JJR ,    V
12
 JJR
 (R1)
G? 
V
12
 ? VB V
13
 ,     VB V
13
V
13
 ? NP ? ,       NP  will be
v
11 
:
v
12 
:
v
13 
:
S   ? V
22
 VP ,      V
22
 VP
V
22
 ? NP ? ,      NP will
v
21 
:
v
22 
:
 (R2)
binarization
...
v
11 
v
12 
v
22 
S(?VB NP ? JJR ?, G?) S(?VB NP ??, G?) S(?NP ??, G?)
L(v12)=?VB NP ??
v
13 
rule bucket
 
 
Figure 3: Binarization on a sample grammar 
 
The function ?(?) is defined to map a result-
ing binary rule ??? ??? to the sub-sequence in ??  
derived from ??? . For example, as shown in Fig-
ure 3, the binary rule ?13 covers the source sub-
sequence ?NP ?? in ?1 , so ? ?13 = "NP ?". 
Similarly, ? ?12 = "VB NP ?".  
The function ?(?) is used to group the rules in 
?? with a common right-hand side derivation for 
source language. Given a binary rule ? ? ??, we 
can put it into a bucket in which all the binary 
rules have the same source sub-sequence ?(?). 
For example (Figure 3), as ? ?12 = "VB NP ?", 
?12 is put into the bucket indexed by ?VB NP ??. 
And ?13  and ?22  are put into the same bucket, 
since they have the same source sub-sequence 
?NP ??. Obviously, ?? can be divided into a set 
of mutual exclusive rule buckets by ?(?). 
In this paper, we use ?(?(?),??) to denote the 
bucket for the binary rules having the source sub-
sequence ?(?). For example, ?("?? ?",??) de-
notes the bucket for the binary rules having the 
source-side ?NP ??. For simplicity, we also use 
?(?,??) to denote ? ? ? ,?? .  
3.2 Cost Reduction for SCFG Binarization 
Given a binary SCFG ??, it can be easily noticed 
that if a rule ? in  the bucket ?(?,??) can be ap-
plied to generate one or more new edges in 
SCFG parsing, any other rules in this bucket can 
also be applied because all of them can be re-
duced from the same underlying derivation ?(?). 
364
Each application of other rules in the bucket 
?(?,??) can generate competing edges with the 
one based on ? . Intuitively, the size of bucket 
can be used to approximately indicate the actual 
number of competing edges on average, and re-
ducing the size of bucket could help reduce the 
edges generated in a parsing chart by applying 
the rules in the bucket. Therefore, if we can find 
a method to greedily reduce the size of each 
bucket ?(?,??), we can reduce the overall ex-
pected edge competitions when parsing with ??. 
However, it can be easily proved that the 
numbers of binary rules in any ?? ? ? ?  are 
same, which implies that we cannot reduce the 
sizes of all buckets at the same time ? removing 
a rule from one bucket means adding it to anoth-
er. Allowing for this fact, the excess edge com-
petition example shown in Section 1 is essential-
ly caused by the uneven distribution of rules 
among different buckets ? ? . Accordingly, our 
optimization objective should be a more even 
distribution of rules among buckets. 
In the following, we formally define a metric 
to model the evenness of rule distribution over 
buckets. Given a binary SCFG ?? and a binary 
SCFG rule ? ? ?? , ?(?) is defined as the cost 
function that maps ?  to the size of the bucket  
? ?,?? : 
? ? =  ? ?,??   (1) 
Obviously, all the binary rules in ? ?,??  share a 
common cost value  ? ?,??  . For example (Fig-
ure 3), both ?13  and ?22  are put into the same 
bucket ? "?? ?",?? , so ? ?13 = ? ?22 = 2. 
The cost of the SCFG ??  is computed by 
summing up all the costs of SCFG rules in it: 
? ?? = ?(?)
??? ?
 (2) 
Back to our task, we are to find an equivalent 
binary SCFG ??  of ?  with the lowest cost in 
terms of the cost function ?(. ) given in Equation 
(2): 
?? = argmin???? ? ?(??) (3) 
Next we will show how ??  is related to the 
evenness of rule distribution among different 
buckets. Let ? ?? = {?1,? , ??}  be the set of 
rule buckets containing rules in ??, then the value 
of ?(??) can also be written as: 
? ?? =  ?? 
2
1????
 (4) 
Assume ?? =  ??  is an empirical distribution of a 
discrete random variable ?, then the square devi-
ation of the empirical distribution is: 
?2 =
1
?
 ( ?? ? ? )
2
?
 (5) 
Noticing that ? ?? =  ?
?   and ? =  ?? /?, Equ-
ation (5) can be written as: 
?2 =
1
?
 ? ? ? ?
 ?? 2
?
  (6) 
Since both ? and |??| are constants, minimizing 
the cost function ?(??) is equivalent to minimiz-
ing the square deviation of the distribution of 
rules among different buckets. A binary SCFG 
with the lower cost indicates the rules are more 
evenly distributed in terms of derivation patterns 
on the source language side. 
3.3 Static Cost Reduction 
Before moving on discussing the algorithm 
which can optimize Equation (3) based on rule 
costs specified in Equation (1), we first present 
an algorithm to find the optimal solution to Eq-
uation (3) if we have known the cost setting of 
?? and can use the costs as static values during 
binarization. Using this simplification, the prob-
lem of finding the binary SCFG  ?? with minim-
al costs can be reduced to find the optimal bina-
rization ??(??) for each rule ??  in ?. 
To obtain ??(??) , we can employ a CKY-
style binarization algorithm which builds a com-
pact binarization forest for the rule ??  in bottom-
up direction. The algorithm combines two adja-
cent spans of ??  each time, in which two spans 
can be combined if and only if they observe the 
BTG constraints? their translations are either 
sequentially or reversely adjacent in ?? , the tar-
get-side derivation of ?? . The key idea of this 
algorithm is that we only use the binarization tree 
with the lowest cost of each span for later com-
bination, which can avoid enumerating all the 
possible binarization trees of ??  using dynamic 
programming. 
Let ??
?
 be the sub-sequence spanning from p 
to q on the source-side, ?[?, ?] be optimal bina-
rization tree spanning ??
?
, ??[?, ?] be the cost of 
?[?, ?], and ?? [?, ?] be the cost of any binary 
rules whose source-side is ??
?
, then the cost of 
optimal binarization tree spanning ??
?
 can be 
computed as: 
??[?, ?] = min
??????1
(?? [?, ?] + ??[?,?] + ??[? + 1, ?]) 
365
The algorithm is shown as follows: 
CYK-based binarization algorithm 
Input: a SCFG rule ??  and the cost function ?(. ).  
Output: the lowest cost binarization on ??  
1:  Function CKYBINARIZATION(?? , ?) 
2:      for l = 2 to n do  ?  Length of span 
3:        for p = 1 to n ? l + 1 do ?  Start of span 
4:               q = p + l  ?  End of span 
5:             for k = p to q ? 1 do ?  Partition of span  
6:               if not CONSECUTIVE(? ?, ? , ? ? + 1,? )  
                         then next loop 
7:                   ?? [?, ?] ? ?(??
?)    
8:                   curCost ? ?? ?, ? +?? ?, ? +??[? + 1,?] 
9:                 if curCost  <  minCost then 
10:                   minCost ? curCost 
11:                    ?[?, ?] ? COMBINE(?[?, ?], ?[? + 1,?]) 
12:             ?? ?, ?  ? minCost 
13:    return ?[1,?]     
14: Function CONSECUTIVE(( a, b), (c, d)) 
15:    return (b = c ? 1) or (d = a ? 1)   
where n is the number of tokens (consecutive 
terminals are viewed as a single token) on the 
source-side of ?? . COMBINE(?[?, ?], ?[? + 1,?]) 
combines the two binary sub-trees into a larger 
sub-tree over ??
?
. ? ?, ? = (?, ?) means that the 
non-terminals covering ??
?
 have the consecutive 
indices ranging from a to b on the target-side. If 
the target non-terminal indices are not consecu-
tive, we set ? ?, ? = (?1,?1). ? ??
?
 = ?(??) 
where ?? is any rule in the bucket ? ??
? ,?? . 
In the algorithm, lines 9-11 implement dynam-
ic programming, and the function CONSECUTIVE 
checks whether the two spans can be combined. 
VB NP ?
V[1,2] V[3,4]
VP
JJR
V[2,3]
V[1,3] V[2,4]
c=6619 c=874 c=62
c=884 c=876 c=64c=6629
c=885
c=6682
c=65
VB NP will be JJR
lowest cost
c=0 c=0 c=0 c=0
 
Figure 4: Binarization forest for an SCFG rule 
 
?(?) ?(?) ?(?) ?(?) 
 VB NP 6619 VB NP ? 10 
 NP ? 874 NP ? JJR 2 
 ? JJR 62 VB NP ? JJR 1 
Table 1: Sub-sequences and corresponding costs 
Figure 4 shows an example of the compact 
forest the algorithm builds, where the solid lines 
indicate the optimal binarization of the rule, 
while other alternatives pruned by dynamic pro-
gramming are shown in dashed lines. The costs 
for binarization trees are computed based on the 
cost table given in Table 1. 
The time complexity of the CKY-based bina-
rization algorithm is ?(n3), which is higher than 
that of the linear binarization such as the syn-
chronous binarization (Zhang et al, 2006). But it 
is still efficient enough in practice, as there are 
generally only a few tokens (n < 5) on the 
source-sides of SCFG rules. In our experiments, 
the linear binarization method is just 2 times 
faster than the CKY-based binarization. 
3.4 Iterative Cost Reduction 
However, ?(?) cannot be easily predetermined in 
a static way as is assumed in Section 3.3 because 
it depends on ?? and should be updated whenever 
a rule in ? is binarized differently. In our work 
this problem is solved using the iterative cost 
reduction algorithm, in which the update of ?? 
and the cost function ?(?) are coupled together. 
Iterative cost reduction algorithm 
Input: An SCFG ? 
Output: An equivalent binary SCFG ?? of ? 
1: Function ITERATIVECOSTREDUCTION(?) 
2:   ?? ? ?0 
3:   for each ? ? ?0do 
4:        ?(?) =  ? ?,?0   
5:   while ?(??) does not converge do 
6:        for each ?? ? ? do 
7:            ?[???] ? ?? ?  ?(??) 
8:            for each ? ? ?(??) do 
9:                for each ?? ? ? ?,??  do 
10:                  ? ?? ? ? ?? ? 1 
11:          ?(??) ? CKYBINARIZATION(?? , ?) 
12:          ?? ? ?[???] ?  ?(??) 
13:          for each ? ? ?(??) do 
14:              for each ?? ? ? ?,??  do 
15:                  ? ?? ? ? ?? + 1 
16: return ?? 
In the iterative cost reduction algorithm, we 
first obtain an initial binary SCFG ?0 using the 
synchronous binarization method proposed in 
(Zhang et al, 2006). Then ?0 is assigned to an 
iterative variable ??. The cost of each binary rule 
in ?0 is computed based on ?0 according to Equ-
ation (1) (lines 3-4 in the algorithm). 
After initialization, ?? is updated by iteratively 
finding better binarization for each rule in ?. The 
basic idea is: for each ??  in ? , we remove the 
current binarization result for ??  from ?? (line 7), 
while the cost function ?(?)  is updated accor-
dingly since the removal of binary rule ? ? 
?(??) results in the reduction of the size of the 
corresponding bucket ? ?,?? . Lines 8-10 im-
366
plement the cost reduction of each binary rule in 
the bucket ? ?,? ? . 
Next, we find the lowest cost binarization for 
??  based on the updated cost function ?(?) with 
the CKY-based binarization algorithm presented 
in Section 3.3 (line 11).  
At last, the new binarization for ??  is added 
back to ?? and ?(?) is re-updated to synchronize 
with this change (lines 12-15). Figure 5 illu-
strates the differences between the static cost 
reduction and the iterative cost reduction. 
Ri
Ri-1
Ri+1
...
...
the i
th
 
rule
G
binarizer
Q(?)
binarize
(a) static cost reduction
Ri
Ri-1
Ri+1
...
...
the i
th
 
rule
G
binarizer
Q(?)
G0
(b) iterative cost reduction
update
static
dynamic
binarize
 
Figure 5: Comparison between the static cost 
reduction and the iterative cost reduction 
 
The algorithm stops when ?(??) does not de-
crease any more. Next we will show that ?(??)  
is guaranteed not to increase in the iterative 
process. 
For any ?(??) on ?? , we have 
               ?  ?[???] ?  ? ??   
        = 2 ? ? ? ??  +  ? ??  + ? ?[???]  
As both  ? ??   and ? ?[???]  are constants with 
respect to ?(? ?? ), ?  ?[???] ?  ? ??   is a li-
near function of ?(? ?? ), and the correspond-
ing slope is positive. Thus ?  ?[???] ?  ? ??   
reaches the lowest value only when ?(? ?? ) 
reaches the lowest value. So ?  ?[???] ?  ? ??   
achieves the lowest cost when we replace the 
current binarization with the new binarization  
??(??)  (line 12). Therefore ?  ?[???] ?  ? ??   
does not increase in the processing on each ??  
(lines 7-15), and ?(??) will finally converge to a 
local minimum when the algorithm stops. 
4 Experiments 
The experiments are conducted on Chinese-to-
English translation in a state-of-the-art string-to-
tree SMT system. All the results are reported in 
terms of case-insensitive BLEU4(%). 
4.1 Experimental Setup 
Our bilingual training corpus consists of about 
350K bilingual sentences (9M Chinese words + 
10M English words)2 . Giza++ is employed to 
perform word alignment on the bilingual sen-
tences. The parse trees on the English side are 
generated using the Berkeley Parser3. A 5-gram 
language model is trained on the English part of 
LDC bilingual training data and the Xinhua part 
of Gigaword corpus. Our development data set 
comes from NIST2003 evaluation data in which 
the sentences of more than 20 words are ex-
cluded to speed up the Minimum Error Rate 
Training (MERT). The test data sets are the 
NIST evaluation sets of 2005 and 2008. 
Our string-to-tree SMT system is built based 
on the work of (Galley et al, 2006; Marcu et al, 
2006), where both the minimal GHKM and 
SPMT rules are extracted from the training cor-
pus, and the composed rules are generated by 
combining two or three minimal GHKM and 
SPMT rules. Before the rule extraction, we also 
binarize the parse trees on the English side using 
Wang et al (2007) ?s method to increase the 
coverage of GHKM and SPMT rules. There are 
totally 4.26M rules after the low frequency rules 
are filtered out. The pruning strategy is similar to 
the cube pruning described in (Chiang, 2007). To 
achieve acceptable translation speed, the beam 
size is set to 50 by default. The baseline system 
is based on the synchronous binarization (Zhang 
et al, 2006).  
4.2 Binarization Schemes 
Besides the baseline (Zhang et al, 2006) and 
iterative cost reduction binarization methods, we 
also perform right-heavy and random synchron-
ous binarizations for comparison. In this paper, 
the random synchronous binarization is obtained 
by: 1) performing the CKY binarization to build 
the binarization forest for an SCFG rule; then 2) 
performing a top-down traversal of the forest. In 
the traversal, we randomly pick a feasible binari-
zation for each span, and then go on the traversal 
in the two branches of the picked binarization. 
Table 2 shows the costs of resulting binary 
SCFGs generated using different binarization 
methods. The costs of the baseline (left-heavy) 
                                                 
2 LDC2003E14, LDC2003E07, LDC2005T06 and 
LDC2005T10 
3 http://code.google.com/p/berkeleyparser/ 
367
and right-heavy binarization are similar, while 
the cost of the random synchronous binarization 
is lower than that of the baseline method4. As 
expected, the iterative cost reduction method ob-
tains the lowest cost, which is much lower than 
that of the other three methods.  
 
Method cost of binary SCFG ?? 
Baseline 4,897M 
Right-heavy 5,182M 
Random 3,479M 
Iterative cost reduction    185M 
Table 2: Costs of the binary SCFGs generated 
using different binarization methods. 
4.3 Evaluation of Translations 
Table 3 shows the performance of SMT systems 
based on different binarization methods. The 
iterative cost reduction binarization method 
achieves the best performance on the test sets as 
well as the development set. Compared with the 
baseline method, it obtains gains of 0.82 and 
0.84 BLEU scores on NIST05 and NIST08 test 
sets respectively. Using the statistical signific-
ance test described by Koehn (2004), the im-
provements are significant  (p < 0.05). 
 
Method Dev NIST05 NIST08 
Baseline 40.02 37.90 27.53  
Right-heavy 40.05 37.87 27.40 
Random 40.10 37.99 27.58 
Iterative cost 
reduction 
40.97* 38.72* 28.37* 
Table 3: Performance (BLUE4(%)) of different 
binarization methods. * = significantly better than 
baseline (p < 0.05).  
 
The baseline method and the right-heavy bina-
rization method achieve similar performance, 
while the random synchronous binarization me-
thod performs slightly better than the baseline 
method, which agrees with the fact of the cost 
reduction shown in Table 2. A possible reason 
that the random synchronous binarization me-
thod can outperform the baseline method lies in 
that compared with binarizing SCFG in a fixed 
way, the random synchronous binarization tends 
to give a more even distribution of rules among 
buckets, which alleviates the problem of edge 
competition. However, since the high-frequency 
source sub-sequences still have high probabilities 
to be generated in the binarization and lead to the 
                                                 
4 We perform random synchronous binarization for 5 
times and report the average cost. 
excess competing edges, it just achieves a very 
small improvement. 
4.4 Translation Accuracy vs. Cost of Binary 
SCFG 
We also study the impacts of cost reduction on 
translation accuracy over iterations in iterative 
cost reduction. Figure 6 and Figure 7 show the 
results on NIST05 and NIST08 test sets. We can 
see that the cost of the resulting binary SCFG 
drops greatly as the iteration count increases, 
especially in the first iteration, and the BLEU 
scores increase as the cost decreases. 
 
Figure 6: Cost of binary SCFG vs. BLEU4 (NIST05) 
 
 
Figure 7: Cost of binary SCFG vs. BLEU4 (NIST08) 
4.5 Impact of Beam Size 
In this section, we study the impacts of beam 
sizes on translation accuracy as well as compet-
ing edges. To explicitly investigate the issue un-
der large beam sizes, we use a subset of NIST05 
and NIST08 test sets for test, which has 50 Chi-
nese sentences of no longer than 10 words. 
Figure 8 shows that the iterative cost reduction 
method is consistently better than the baseline 
method under various beam settings. Besides the 
experiment on the test set of short sentences, we 
also conduct the experiment on NIST05 test set. 
To achieve acceptable decoding speed, we range 
the beam size from 10 to 70. As shown in Figure 
9, the iterative cost reduction method also out-
performs the baseline method under various 
beam settings on the large test set. 
Though enlarging beam size can reduce the 
search errors and improve the system perfor-
mance, the decoding speed of string-to-tree SMT 
drops dramatically when we enlarge the beam 
size. The problem is more serious when long 
1.0E+08
1.0E+09
1.0E+10
37.8
38
38.2
38.4
38.6
38.8
0 1 2 3 4 5
performance(BLEU4) cost
iteration
BLEU4(%) cost of G'
1.0E+08
1.0E+09
1.0E+10
27.4
27.6
27.8
28
28.2
28.4
0 1 2 3 4 5
performance(BLEU4) cost
BLEU4(%) cost of G'
iteration
368
sentences are translated. For example, when the 
beam size is set to a larger number (e.g. 200), our 
decoder takes nearly one hour to translate a sen-
tence whose length is about 20 on a 3GHz CPU. 
Decoding on the entire NIST05 and NIST08 test 
sets with large beam sizes is impractical. 
 
Figure 8: BLEU4 against beam size (small test set) 
 
 
Figure 9: BLEU4 against beam size (NIST05) 
 
Figure 10 compares the baseline method and 
the iterative cost reduction method in terms of 
translation accuracy against the number of edges 
proposed during decoding. Actually, the number 
of edges proposed during decoding can be re-
garded as a measure of the size of search space. 
We can see that the iterative cost reduction me-
thod outperforms the baseline method under var-
ious search effort.  
 
Figure 10: BLEU4 against competing edges  
 
The experimental results of this section show 
that compared with the baseline method, the iter-
ative cost reduction method can lead to much 
fewer edges (about 25% reduction) as well as the 
higher BLEU scores under various beam settings. 
4.6 Edge Competition vs. Cost of Binary 
SCFG 
In this section, we study the impacts of cost re-
duction on the edge competition in the chart cells 
of our CKY-based decoder. Two metrics are 
used to evaluate the degree of edge competition. 
They are the variance and the mean of the num-
ber of competing edges in the chart cells, where 
high variance means that in some chart cells the 
rules have high risk to be pruned due to the large 
number of competing edges. The same situation 
holds for the mean as well. Both of the two me-
trics are calculated on NIST05 test set, varying 
with the span length of chart cell. 
Figure 11 shows the cost of resulting binary 
SCFG and the variance of competing edges 
against iteration count in iterative cost reduction. 
We can see that both the cost and the variance 
reduce greatly as the iteration count increases. 
Figure 12 shows the case for mean, where the 
reduction of cost also leads to the reduction of 
the mean value. The results shown in Figure 11 
and Figure 12 indicate that the cost reduction is 
helpful to reduce edge competition in the chart 
cells.  
 
Figure 11: Cost of binary SCFG vs. variance of 
competing edge number (NIST05) 
 
 
Figure 12: Cost of binary SCFG vs. mean of 
competing edge number (NIST05) 
 
We also perform decoding without pruning 
(i.e. beam size = ?) on a very small set which 
has 20 sentences of no longer than 7 words. In 
this experiment, the baseline system and our iter-
ative cost reduction based system propose 
14,454M and 10,846M competing edges respec-
tively. These numbers can be seen as the real 
numbers of the edges proposed during decoding 
instead of an approximate number observed in 
the pruned search space. It suggests that our me-
thod can reduce the number of the edges in real 
search space effectively. A possible reason to 
32
34
36
38
40
42
10 50 100 500 1000 5000
baseline
cost reduction
BLEU4(%)
beam 
size 
35
36
37
38
39
10 20 30 40 50 70
baseline
cost reduction
beam
size
BLEU4(%)
32
34
36
38
40
42
1E+07 1E+08 1E+09 1E+10
baseline
cost reduction
BLEU4(%)
# of
edges
1.0E+5
1.0E+6
1.0E+7
1.0E+8
1.0E+9
1.0E+10
1.0E+7
1.0E+8
1.0E+9
1.0E+10
0 1 2 3 4 5
span=2
span=3
span=5
span=7
span=10
span=20
cost
iteration
variance cost of G'
1.0E+6
1.0E+7
1.0E+8
1.0E+9
1.0E+10
8.0E+3
1.0E+5
0 1 2 3 4 5
span=2
span=3
span=5
span=7
span=10
span=20
cost
iteration
mean cost of G'
369
this result is that the cost reduction based binari-
zation could reduce the probability of rule mis-
matching caused by binarization, which results in 
the reduction of the number of edges proposed 
during decoding. 
5 Conclusion and Future Work 
This paper introduces a new binarization method, 
aiming at choosing better binarization for SCFG-
based SMT systems. We demonstrate the effec-
tiveness of our method on a state-of-the-art 
string-to-tree SMT system. Experimental results 
show that our method can significantly outper-
form the conventional synchronous binarization 
method, which indicates that better binarization 
selection is very beneficial to SCFG-based SMT 
systems. 
In this paper the cost of a binary rule is de-
fined based on the competition among the binary 
rules that have the same source-sides. However, 
some binary rules with different source-sides 
may also have competitions in a chart cell. We 
think that the cost of a binary rule can be better 
estimated by taking the rules with different 
source-sides into account. We intend to study 
this issue in our future work. 
Acknowledgements 
The authors would like to thank the anonymous 
reviewers for their pertinent comments, and Xi-
nying Song, Nan Duan and Shasha Li for their 
valuable suggestions for improving this paper. 
References 
Eugene Charniak,  Mark Johnson, Micha Elsner, Jo-
seph Austerweil, David Ellis, Isaac Haxton, Cathe-
rine Hill, R. Shrivaths, Jeremy Moore, Michael Po-
zar, and Theresa Vu. 2006. Multilevel Coarse-to-
Fine PCFG Parsing. In Proc. of HLT-NAACL 2006, 
New York, USA, 168-175.  
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-Based Best-First Chart Parsing. In 
Proc. of the Six Workshop on Very Large Corpora, 
pages: 127-133. 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Proc. 
of ACL 2005, Ann Arbor, Michigan, pages: 263-
270. 
David Chiang. 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics. 33(2): 
202-208. 
Michel Galley, Jonathan Graehl, Kevin Knight, Da-
niel Marcu, Steve DeNeefe, Wei Wang, and Igna-
cio Thayer. 2006. Scalable Inference and Training 
of Context-Rich Syntactic Translation Models. In 
Proc. of ACL 2006, Sydney, Australia, pages: 961-
968. 
Michel Galley, Mark Hopkins, Kevin Knight, and 
Daniel Marcu. 2004. What?s in a translation rule? 
In Proc. of HLT-NAACL 2004, Boston, USA, pag-
es: 273-280. 
Liang Huang. 2007. Binarization, Synchronous Bina-
rization, and Target-side binarization.  In Proc. of 
HLT-NAACL 2007 / AMTA workshop on Syntax 
and Structure in Statistical Translation, New York, 
USA, pages: 33-40. 
Tadao Kasami. 1965. An Efficient Recognition and 
Syntax Analysis Algorithm for Context-Free Lan-
guages. Technical Report AFCRL-65-758, Air 
Force Cambridge Research Laboratory, Bedford, 
Massachusetts. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of 
EMNLP 2004, Barcelona, Spain , pages: 388?395. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phras-
es. In Proc. of EMNLP 2006, Sydney, Australia, 
pages: 44-52. 
Giorgio Satta and Enoch Peserico. 2005. Some Com-
putational Complexity Results for Synchronous 
Context-Free Grammars. In Proc. of HLT-EMNLP 
2005, Vancouver, pages: 803-810. 
L. Shapiro and A. B. Stephens. 1991. Bootstrap per-
colation, the Sch? oder numbers, and the n-kings 
problem. SIAM Journal on Discrete Mathematics, 
4(2):275-280. 
Xinying Song, Shilin Ding and Chin-Yew Lin. 2008. 
Better Binarization for the CKY Parsing. In Proc. 
of EMNLP 2008, Hawaii, pages: 167-176. 
Yoshimasa Tsuruoka and Junichi Tsujii. 2004. Itera-
tive CKY Parsing for Probabilistic Context-Free 
Grammars. In Proc. of IJCNLP 2004, pages: 52-
60. 
Wei Wang  and  Kevin Knight and Daniel Marcu. 
2007. Binarizing Syntax Trees to Improve Syntax-
Based Machine Translation Accuracy. In Proc. of 
EMNLP-CoNLL 2007, Prague, Czech Republic, 
pages: 746-754. 
D. H. Younger. 1967. Recognition and Parsing of 
Context-Free Languages in Time n3. Information 
and Control, 10(2):189-208. 
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin 
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006, 
New York, USA, pages: 256- 263. 
370
Some Studies on Chinese Domain Knowledge Dictionary and Its
Application to Text Classification 
Jingbo Zhu 
 Natural Language Processing Lab 
Institute of Computer Software & Theory 
Northeastern University, Shenyang 
zhujingbo@mail.neu.edu.cn
Wenliang Chen 
Natural Language Processing Lab 
Institute of Computer Software & Theory 
Northeastern University, Shenyang 
Chenwl@mail.neu.edu.cn
Abstract
In this paper, we study some issues on 
Chinese domain knowledge dictionary 
and its application to text classification 
task. First a domain knowledge hierar-
chy description framework and our 
Chinese domain knowledge dictionary 
named NEUKD are introduced. Second, 
to alleviate the cost of construction of 
domain knowledge dictionary by hand, 
we use a boostrapping-based algorithm 
to learn new domain associated terms 
from a large amount of unlabeled data. 
Third, we propose two models (BOTW 
and BOF) which use domain knowl-
edge as textual features for text catego-
rization. But due to limitation of size of 
domain knowledge dictionary, we fur-
ther study machine learning technique 
to solve the problem, and propose a 
BOL model which could be considered 
as the extended version of BOF model. 
Na?ve Bayes classifier based on BOW 
model is used as baseline system in the 
comparison experiments. Experimental 
results show that domain knowledge is 
very useful for text categorization, and 
BOL model performs better than other 
three models, including BOW, BOTW 
and BOF models. 
1 Introduction 
It is natural for people to know the topic of the 
document when they see some specific words in 
the document. For example, when we read a 
news, if title of the news includes a word ???
(Yao Ming)?, as we know, ???(Yao Ming)? is 
a famous China basketball athlete in US NBA 
game, so we could recognize the topic of the 
document is about ??? ,?? (Basketball,
Sports)? with our domain knowledge. In this 
paper, we call the specific word ??? (Yao 
Ming)? as a Domain Associated Term (DAT). A 
DAT is a word or a phrase (compound words) 
that enable humans to recognize intuitively a 
topic of text with their domain knowledge. In 
fact, domain knowledge is a kind of common-
sense knowledge. We think that domain knowl-
edge is very useful for text understanding tasks, 
such as text classification, document summariza-
tion, and information retrieval. 
In previous literatures, some researchers 
used knowledge bases for text understanding 
tasks(Scott et al, 1998), such as WordNet for 
English and HowNet for Chinese. We know that 
WordNet and HowNet are lexical and semantic 
knowledge resources. Other researchers tried to 
use commonsense knowledge such as field-
associated terms for text understanding tasks(M. 
Fuketa et al, 2000, Sangkon Lee and Masami 
Shishibori, 2002). But the problem of limitation 
of size of such knowledge base is still a key bot-
tleneck for using domain knowledge dictionary 
for text understanding tasks, and how to solve it 
is an ongoing research focus. 
In the following content, we try to give an-
swers to four questions: 1)What is our Chinese 
domain knowledge dictionary NEUKD? 2)How 
to learn DATs from a large amount of unlabelled 
data? 3)How to use the Chinese domain knowl-
edge dictionary NEUKD for text classification? 
4)Due to the problem of limitation of size of 
domain knowledge dictionary, how to solve the 
110
problem and improve performance of text classi-
fication using domain knowledge dictionary?
2 Domain Knowledge Dictionary 
We first introduce briefly domain knowledge 
hierarchy description framework (DKF) which 
includes three levels: Domain Level (DL), Do-
main Feature Level (DFL) and Domain Associ-
ated Term Level (DATL). The DL is the top 
level which defines many domains, such as ??
?(Sports)?, ???(Military Affairs)?. The DFL 
is the second level which defines many domain 
features. A domain defined in the DL has a lot of 
domain features defined in the DFL. For 
example, domain ???(Military Affairs)? has 
many domain features, such as ??? (Army 
Feature)?, ???(Weapon Feature)? and ???
(War Feature)?. The DATL is the third level 
which defines many domain associated terms. 
Many domain associated terms could indicate a 
same domain feature defined in the DFL. For 
example, some domain associated terms, such as 
some domain associated terms, such as ????
?(Mid-East War)?, ??????(Iraq War)? 
and ??????(Afghanistan War)?, indicate 
domain feature ???(War)?. 
Since 1996 we employed a semi-automatic 
machine learning technique to acquire domain 
knowledge from a large amount of labeled and 
unlabeled corpus, and built a general-purpose 
domain knowledge dictionary named NEUKD 
according to the domain knowledge hierarchy 
description framework(Zhu Jingbo et al, 2002). 
Items defined in the NEUKD include domain 
associated term, domain feature and domain. 
Currently 40 domains, 982 domain features and 
more than 610,000 domain associated terms are 
defined in the NEUKD. Some instances of 
NEUKD are given in Table 1. Because the size 
of NEUKD is limited, so in following content 
we will study machine learning techniques to 
solve the problem of using NEUKD for text 
classification task. 
Domain Associated Terms Domain Features Domain 
??
(Yao Ming) 
??, ???
(Basketball, Athlete)  
??
(Sports ) 
????
(The Sanxia project) 
????
(Irrigation Project) 
??
(Irrigation Works) 
??
(Match Season) 
??
(Match)
??
(Sports ) 
????
(Arsenal Team) 
??
(Football)
??
(Sports)
??????
(Industrial and commercial bank of China) 
??
(Bank)
??
(Finance)
Table 1. Some instances defined in the NEUKD 
3 Bootstrapping-based DAT Learning 
Algorithm
To extend domain knowledge dictionary, in this 
paper, we will use a feature learning algorithm 
based on bootstrapping (FLB)(Zhu Jingbo et al, 
2004) to learning new DATs. In the FLB learn-
ing procedure, some seed words are given in 
advance. In fact, seed words are some important 
DATs. For example, ten seed words of domain 
???(finance)? are??(stock), ??(finance), 
??(loan), ??(stock), ??(finance and eco-
nomics), ??(bank), ??(tax), ??(foreign
exchange), ?? (investment) and ?? (stock
market).
The FLB learning procedure is described as 
follows:
z Initialization: Use a small number of seed 
words initialize DAT set 
z Iterate Bootstrapping:
? Candidate DAT Learner: Learn some 
new DATs as candidate DATs from 
unlabeled data. 
? Evaluation: Score all candidate DATs, 
and select top-n best DATs as new 
seed words, and add them into DAT set. 
In the beginning of algorithm, all words ex-
cept stopwords in the unlabeled corpus could be 
111
considered as candidate DATs. In fact, we can 
regard bootstrapping as iterative clustering. In 
the evaluation step of FLB algorithm, RlogF 
metric method(Ellen Riloff, Rosie Jones, 1999) 
is used as evaluation function which assigns a 
score to a word(candidate DAT). The score of a 
word is computed as: 
iii RXwFLogwM u ),()( 2            (1) 
Where F(wi,X) is the frequency of co-occurrence 
of word wi and X (set of seed words) in the same 
sentence, F(wi) is the frequency of wi in the cor-
pus, and Ri=F(wi,X)/F(wi). The RlogF metric 
tries to strike a balance between reliability and 
frequency: R is high when the word is highly 
correlated with set of seed words, and F is high 
when the word and X highly co-occur in the 
same sentence.
    In the experiments, we use the corpus from 
1996-1998 People?s Daily as unlabeled data 
which has about 50 million words. For domain 
??? (finance)?, we select ten seed words 
shown in above example, the bootstrapping-
based DAT learning algorithm obtains 65% pre-
cision performance within top-1000 new learned 
DATs according to human judgment. 
4 Domain Knowledge based Text Clas-
sification
In this paper, na?ve Bayes(NB) model 
(McCallum and K.Nigam, 1998) is used to build 
text classifier. We want to study how to use our 
Chinese domain knowledge dictionary NEUKD 
to improve text categorization. 
4.1 BOW Model
The most commonly used document representa-
tion is the so called vector space model(G.Salton 
and M.J.McGill, 1983). In the vector space 
model, documents are represented by vectors of 
terms (textual features, e.g. words, phases, etc.). 
Conventional bag-of-words model (BOW) uses 
common words as textual features. In the com-
parison experiments, we use the BOW model as 
baseline NB system.  
4.2 BOTW Model
As above mentioned, more than 610000 domain 
associated terms (DATs) are defined in the 
NEUKD, such as ???(Yao Ming) ?, ????
?(The Sanxia project)?, and ???????
(Industrial and commercial bank of China)? 
shown in table 1. We use domain associated 
terms and common words as textual features, 
called BOTW models (short for bag-of-terms 
and words model). For example, in the previous 
examples, the DAT ?????(The Sanxia pro-
ject, Sanxia is a LOCATION name of China)? 
can be used as a textual feature in BOTW model. 
But in BOW model(baseline system) we con-
sider two common words ???(The Sanxia)? 
and ???(project)? as two different textual fea-
tures.
4.3 BOF Model
Similar to BOTW model, we use domain fea-
tures as textual features in the NB classifier, 
called BOF model (short for bag-of-features 
model). In BOF model, we first transform all 
DATs into domain features according to defini-
tions in the NEUKD, and group DATs with 
same domain features as a cluster, called Topic 
Cluster. For Examples, Topic Cluster ???
(sports)? includes some DATs, such as ???
(match season)?, ?????(Arsenal)?, ????
(Olympic Games)?, ???? (Table Tennis)?, 
???(Yao Ming)?. In BOF model, we use topic 
clusters as textual features for text categorization. 
Also the classification computation procedure of 
BOF model is same as of BOW model.  
4.4 BOL Model 
To solve the problem of the limitation of 
NEUKD, in this paper, we propose a machine 
learning technique to improve BOF model. The 
basic ideas are that we wish to learn new DATs 
from pre-classified documents, and group them 
into the predefined topic clusters which are 
formed and used as textual features in BOF 
model discussed in section 4.3. Then these new 
topic clusters could be used as textual features 
for text categorization. We call the new model as 
BOL model(short for bag-of-learned features 
model) which could be consider as an extended 
version of BOF model.  
First we group all DATs originally defined in 
NEUKD into a lot of topic clusters as described 
in BOF model, which are used as seeds in fol-
lowing learning procedure. Then we group other 
words (not be defined in NEUKD) into these 
topic clusters. The Learning algorithm is de-
scribed as following: 
- Preprocessing: Text segmentation, extract-
ing candidate words, and sort the candidate 
words by CHI method. As above mentioned, 
all candidate words except stopwords which 
112
are not defined in NEUKD will be grouped 
into topic clusters in this process. 
- Initialization: These words, which are de-
fined in NEUKD, are first added to corre-
sponding topic clusters according to their 
associated domain features, respectively. 
- Iteration: Loop until all candidate words 
have been put into topic clusters: 
? Measure similarity of a candidate word 
and each topic cluster, respectively. 
? Put the candidate word into the most 
similar topic cluster(Note that a word 
can only be grouped into one cluster).
The important issue of above procedures is 
how to measure the similarity between a word 
and a topic cluster. Chen Wenliang et. al.(2004)
proposed a measure for word clustering algo-
rithm used in text classification. So in this paper, 
we use Chen?s measure to measure the similarity 
between a word and a topic cluster in above 
learning algorithm. The similarity of a word wt
and a topic cluster fj is defined as 
| |
1
( )( ( , ) ( , ))
( ) ( )
( )
( ) | |
t t t j j t j
t j
t L
i
i
S w w w f f w f
N w N f
w
N f W
O [ [
O
 
 ?  ?
 
?
   (2) 
Where 
( )( , )
( ) ( )
( ( | ) || ( | ))
( )
( , )
( ) ( )
( ( | ) || ( | ))
t
t t j
t j
t t j
j
j t j
t j
j t j
P ww w f
P w P f
D P C w P C w f
P f
f w f
P w P f
D P C f P C w f
[
[
?  
u ?
?  
u ?
Where we define the distribution P(C|wt) as the 
random variable over classes C, and its distribu-
tion given a particular word wt. N(fi) denote the 
number of words in the topic cluster fi, W is the 
list of candidate words. 
    To describe how to estimate distribution 
P(C|f) , we first assume that in the beginning of 
learning procedure, only a word w1 is included 
in topic cluster f1, we could say that P(C|f1) = 
P(C|w1). When a new word w2 is added into 
topic cluster f1, we could get a new topic cluster 
f2. How to estimate the new distribution P(C|f2)
is key step, where f2=w2?f1. We could use the 
following formula (3) to estimate distribution 
P(C|f2) =P(C|w2?f1). Similarly, we could know 
if the new word wn is added into topic cluster fn-1
to form a new topic cluster fn, we also could es-
timate P(C|fn)=P(C|wn?fn-1) following this way, 
and so on. 
2 2 1
2
2
2 1
1
1
2 1
( | ) ( | )
( ) ( | )
( ) ( )
( ) ( | )
( ) ( )
P C f P C w f
P w P C w
P w P f
P f P C f
P w P f
 ?
 
 
                (3) 
We turn back the question about how to meas-
ure the difference between two probability 
distributions. Kullback-Leibler divergence is 
used to do this. The KL divergence between two 
class distributions induced by wt and ws is writ-
ten as 
| |
1
( ( | ) || ( | ))
( | )
( | ) log( )
( | )
t s
C
j t
j t
j j s
D P C w P C w
P c w
P c w
P c w 
 
?              (4) 
In preprocessing step, the CHI statistic meas-
ures the lack of independence of feature t and 
category c.  
D)B)(CD)(AC)(B(A
BC)-N(ADc)2(t,
2
 F
Where t refers to a feature and c refers to a cate-
gory, A is the number of times t and c co-occur, 
B is the number of times t occurs without c, C is 
the number of times c occurs without t, D is the 
number of times neither c nor t co-occur, and N 
is the total number of documents.
5 Experimental Results 
In this paper, we use na?ve Bayes for classifying 
documents. Here we only describe multinomial 
na?ve Bayes briefly since full details have been 
presented in the paper(McCallum and K.Nigam, 
1998). The basic idea in na?ve Bayes approaches 
is to use the joint probabilities of words and 
categories to estimate the probabilities of catego-
ries when a document is given. Given a docu-
ment d for classification, we calculate the 
probabilities of each category c as follows: 
( | )| |
1
( ) ( | )( | ) ( )
( | )( ) ( | )!
iN t dT
i
i i
P c P d cP c d P d
P t cP c N t d 
 
v ?
Where N(ti|d) is the frequency of word ti in 
document d, T is the vocabulary and |T| is the 
113
size of T, ti is the ith word in the vocabulary, and 
P(ti|c) thus represents the probability that a ran-
domly drawn word from a randomly drawn 
document in category c will be the word ti.
In the experiments, we use NEU_TC data 
set(Chen Wenliang et. al. 2004) to evaluate the 
performance of baseline NB classifier and our 
classifiers. The NEU_TC data set contains Chi-
nese web pages collected from web sites. The 
pages are divided into 37 classes according to 
?Chinese Library Categorization?(CLCEB, 
1999). It consists of 14,459 documents. We do 
not use tag information of pages. We use the 
toolkit CipSegSDK(Yao Tianshun et. al. 2002) 
for word segmentation. We removed all words 
that had less than two occurrences. The resulting 
vocabulary has about 60000 words.
In the experiments, we use 5-fold cross vali-
dation where we randomly and uniformly split 
each class into 5 folds and we take four folds for 
training and one fold for testing. In the cross-
validated experiments we report on the average 
performance. For evaluating the effectiveness of 
category assignments by classifiers to docu-
ments, we use the conventional recall, precision 
and F1 measures. Recall is defined to be the ra-
tio of correct assignments by the system divided 
by the total number of correct assignments. Pre-
cision is the ratio of correct assignments by the 
system divided by the total number of the sys-
tem?s assignments. The F1 measure combines 
recall (r) and precision (p) with an equal weight 
in the following form: 
pr
rpprF  
2),(1
In fact, these scores can be computed for the 
binary decisions on each individual category 
first and then be averaged over categories. The 
way is called macro-averaging method. For 
evaluating performance average across class, we 
use the former way called micro averaging 
method in this paper which balances recall and 
precision in a way that gives them equal weight. 
The micro-averaged F1 measure has been widely 
used in cross-method comparisons.  
To evaluate the performance of these four 
models based on NB classifier, we construct four 
systems in the experiments, including BOW, 
BOTW, BOF and BOL classifier. CHI measure 
is used to feature selection in all text classifiers.
Figure 1. Experimental results of BOW, BOTW, BOF, BOL classifiers 
In figure 1, we could find that BOTW classi-
fier always performs better than BOW classifier 
when the number of features is larger than about 
500. From comparative experimental results of 
BOTW and BOW classifiers, we think that do-
main associated items are a richer and more pre-
cise representation of meaning than common 
words. Because the total number of domain fea-
tures in NEUKD is only 982, in figure 1 we find 
the maximum number of features (domain fea-
114
tures) for BOF and BOL classifier is less than 
1000. When the number of features is between 
200 and 1000, BOF classifier performs better 
than BOW and BOTW classifiers. It is also ob-
vious that BOL classifier always performs better 
than other three classifiers when the number of 
features is less than 1000. As above mentioned, 
in BOL model, we use a machine learning tech-
nique to solve the problem of limitation of size 
of NEUKD, and group rest 65.01% words into 
predefined topic clusters as textual features in 
BOL model. So the classifier based on BOL 
model can yield better performance than BOF 
model. 
6 Conclusions and Future Work 
In this paper, we first introduce our Chinese do-
main knowledge dictionary NEUKD. To allevi-
ate the cost of construction of domain 
knowledge dictionary by hand, we propose a 
boostrapping-based algorithm to learn new do-
main associated terms from a large amount of 
unlabeled data. This paper studies how to im-
prove text categorization by using domain 
knowledge dictionary. To do it, we propose two 
models using domain knowledge as textual fea-
tures. The first one is BOTW model which uses 
domain associated terms and common words as 
textual features. The other one is BOF model 
which uses domain features as textual features. 
But due to limitation of size of domain knowl-
edge dictionary, many useful words are lost in 
the training procedure. We study and use a ma-
chine learning technique to solve the problem to 
improve knowledge-based text categorization, 
and propose a BOL model which could be con-
sidered as the extension version of BOF model. 
Comparison experimental results of those four 
models (BOW, BOTW, BOF and BOL) show 
that domain knowledge is very useful for im-
proving text categorization. In fact, a lot of 
knowledge-based NLP application systems have 
to face the problem of limitation of size of 
knowledge bases. Like our work discussed in 
this paper, we think that using machine learning 
techniques is a good way to solve such problem. 
In the future work, we will study how to apply 
the domain knowledge to improve other text un-
derstanding tasks, such as information retrieval, 
information extraction, topic detection and track-
ing (TDT).
Acknowledgements 
This research was supported in part by the Na-
tional Natural Science Foundation of China & 
Microsoft Research Asia (No. 60203019), the 
Key Project of Chinese Ministry of Education 
(No. 104065), and the National Natural Science 
Foundation of China (No. 60473140). 
References
Chen Wenliang, Chang Xingzhi, Wang Huizhen, Zhu 
Jingbo, and Yao Tianshun. 2004. Automatic 
Word Clustering for Text Categorization Using 
Global Information. First Asia Information Re-
trieval Symposium (AIRS 2004), LNCS, Beijing, 
pp.1-6
CLCEB. 1999. China Library Categorization Edito-
rial Board. China Library Categorization (The 4th 
ed.) (In Chinese), Beijing, Beijing Library Press.
Ellen Riloff, Rosie Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level 
Bootstrapping, Proceedings of the Sixteenth Na-
tional Conference on Artificial Intelligence.
G.Salton and M.J.McGill, 1983. An introduction to 
modern information retrieval, McGraw-Hill.
McCallum and K.Nigam. 1998. A Comparison of 
Event Models for na?ve Bayes Text Classification, 
In AAAI-98 Workshop on Learning for Text Cate-
gorization.
M. Fuketa, S.Lee, T.Tsuji, M.Okada and J. Aoe. 2000. 
A document classification method by using field 
associated words. International Journal of Infor-
mation Sciences. 126(1-4), p57-70 
Sangkon Lee, Masami Shishibori. 2002. Passage seg-
mentation based on topic matter, International 
journal of computer processing of oriental lan-
guages, 15(3), p305-339. 
Scott, Sam?Stan Matwin. 1998. Text classification 
using WordNet hypernyms. Proceedings of the 
COLING/ACL Workshop on Usage of WordNet in 
Natural Language Processing Systems, Montreal. 
Yao Tianshun, Zhu Jingbo, Zhang li, and Yang Ying, 
2002. Natural Language Processing- research on 
making computers understand human languages, 
Tsinghua University Press, (In Chinese). 
Zhu Jingbo and Yao Tianshun. 2002. FIFA-based 
Text Classification, Journal of Chinese Informa-
tion Processing, V16, No3.(In Chinese) 
Zhu Jingbo, Chen Wenliang, and Yao Tianshun. 2004. 
Using Seed Words to Learn to Categorize Chinese 
Text. Advances in Natural Language Processing: 
4th International Conference (EsTAL 2004),
pp.464-473 
115
Learning a Stopping Criterion for Active Learning for Word Sense 
Disambiguation and Text Classification 
Jingbo Zhu   Huizhen Wang 
Natural Language Processing Lab  
Northeastern University 
Shenyang, Liaoning, P.R.China, 110004 
Zhujingbo@mail.neu.edu.cn
wanghuizhen@mail.neu.edu.cn 
Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu
 
Abstract 
In this paper, we address the problem of 
knowing when to stop the process of active 
learning. We propose a new statistical 
learning approach, called minimum 
expected error strategy, to defining a 
stopping criterion through estimation of the 
classifier?s expected error on future 
unlabeled examples in the active learning 
process. In experiments on active learning 
for word sense disambiguation and text 
classification tasks, experimental results 
show that the new proposed stopping 
criterion can reduce approximately 50% 
human labeling costs in word sense 
disambiguation with degradation of 0.5% 
average accuracy, and approximately 90% 
costs in text classification with degradation 
of 2% average accuracy. 
1 Introduction 
Supervised learning models set their parameters 
using given labeled training data, and generally 
outperform unsupervised learning methods when 
trained on equal amount of training data. However, 
creating a large labeled training corpus is very 
expensive and time-consuming in some real-world 
cases such as word sense disambiguation (WSD).  
Active learning is a promising way to minimize 
the amount of human labeling effort by building an 
system that automatically selects the most informa-
tive unlabeled example for human annotation at 
each annotation cycle. In recent years active learn-
ing  has attracted a lot of research interest, and has 
been studied in many natural language processing 
(NLP) tasks, such as text classification (TC) 
(Lewis and Gale, 1994; McCallum and Nigam, 
1998), chunking (Ngai and Yarowsky, 2000), 
named entity recognition (NER) (Shen et al, 2004; 
Tomanek et al, 2007), part-of-speech tagging 
(Engelson and Dagan, 1999), information 
extraction (Thompson et  al., 1999), statistical 
parsing (Steedman et al, 2003), and word sense 
disambiguation (Zhu and Hovy, 2007).  
Previous studies reported that active learning 
can help in reducing human labeling effort. With 
selective sampling techniques such as uncertainty 
sampling (Lewis and Gale, 1994) and committee-
based sampling (McCallum and Nigam, 1998), the 
size of the training data can be significantly re-
duced for text classification (Lewis and Gale, 
1994; McCallum and Nigam, 1998), word sense 
disambiguation (Chen, et al 2006; Zhu and Hovy, 
2007), and named entity recognition (Shen et al, 
2004; Tomanek et al, 2007) tasks.  
Interestingly, deciding when to stop active 
learning is an issue seldom mentioned issue in 
these studies. However, it is an important practical 
topic, since it obviously makes no sense to 
continue the active learning procedure until the 
whole corpus has been labeled. How to define an 
adequate stopping criterion remains an unsolved 
problem in active learning. In principle, this is a 
problem of estimation of classifier effectiveness 
(Lewis and Gale, 1994). However, in real-world 
applications, it is difficult to know when the 
classifier reaches its maximum effectiveness 
before all unlabeled examples have been 
annotated. And when the unlabeled data set 
becomes very large, full annotation is almost 
impossible for human annotator.  
In this paper, we address the issue of a stopping 
criterion for active learning, and propose a new 
statistical learning approach, called minimum ex-
366
pected error strategy, that defines a stopping crite-
rion through estimation of the classifier?s expected 
error on future unlabeled examples. The intuition is 
that the classifier reaches maximum effectiveness 
when it results in the lowest expected error on 
remaining unlabeled examples. This proposed 
method is easy to implement, involves small 
additional computation costs, and can be applied to 
several different learners, such as Naive Bayes 
(NB), Maximum Entropy (ME), and Support 
Vector Machines (SVMs) models. Comparing with 
the confidence-based stopping criteria proposed by 
Zhu and Hovy (2007), experimental results show 
that the new proposed stopping criterion achieves 
better performance in active learning for both the 
WSD and TC tasks. 
2 Active Learning Process and Problem 
of General Stopping Criterion 
2.1 Active Learning Process 
Active learning is a two-step semi-supervised 
learning process in which a small number of la-
beled samples and a large number of unlabeled 
examples are first collected in the initialization 
stage, and a close-loop stage of query and retrain-
ing is adopted. The purpose of active learning is to 
minimize the amount of human labeling effort by 
having the system in each cycle automatically se-
lect for human annotation the most informative 
unannotated case.   
Procedure: Active Learning Process 
Input: initial small training set L, and pool of 
unlabeled data set U 
Use L to train the initial classifier C (i.e. a classi-
fier for uncertainty sampling or a set of classifiers 
for committee-based sampling) 
Repeat 
? Use the current classifier C  to label all 
unlabeled examples in U 
? Based on active learning rules R such as un-
certainty sampling or committee-based sam-
pling, present m top-ranked unlabeled ex-
amples to oracle H for labeling 
? Augment L with the m new examples, and 
remove them from U 
? Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning process 
In this work, we are interested in selective sam-
pling for pool-based active learning, and focus on 
uncertainty sampling (Lewis and Gale, 1994). The 
key point is how to measure the uncertainty of an 
unlabeled example, in order to select a new exam-
ple with maximum uncertainty to augment the 
training data. The maximum uncertainty implies 
that the current classifier has the least confidence 
in its classification of this unlabeled example x. 
The well-known entropy is a good uncertainty 
measurement widely used in active learning: 
( ) ( | ) log ( | )
y Y
UM x P y x P y x
?
= ??         (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. UM is 
the uncertainty measurement function based on the 
entropy estimation of the classifier?s posterior 
distribution. 
2.2 General Stopping Criteria 
As shown in Fig. 1, the active learning process 
repeatedly provides the most informative unlabeled 
examples to an oracle for annotation, and update 
the training set, until the predefined stopping 
criterion SC is met. In practice, it is not clear how 
much annotation is sufficient for inducing a 
classifier with maximum effectiveness (Lewis and 
Gale, 1994). This procedure can be implemented 
by defining an appropriate stopping criterion for 
active learning.  
In active learning process, a general stopping 
criterion SC can be defined as: 
1 (
0 ,AL
effectiveness C
SC
otherwise
) ???= ??         (2) 
where ? is a user predefined constant and the func-
tion effectiveness(C) evaluates the effectiveness of 
the current classifier. The learning process ends 
only if the stopping criterion function SCAL is equal 
to 1. The value of constant ? represents a tradeoff 
between the cost of annotation and the effective-
ness of the resulting classifier. A larger ? would 
cause more unlabeled examples to be selected for 
human annotation, and the resulting classifier 
would be more robust. A smaller ? means the re-
sulting classifier would be less robust, and less 
unlabeled examples would be selected to annotate.  
In previous work (Shen et al, 2004; Chen et al, 
2006; Li and Sethi, 2006; Tomanek et al, 2007), 
there are several common ways to define the func-
367
tion effectiveness(C). First, previous work always 
used a simple stopping condition, namely, when 
the training set reaches desirable size. However, it 
is almost impossible to predefine an appropriate 
size of desirable training data guaranteed to induce 
the most effective classifier. Secondly, the learning 
loop can end if no uncertain unlabeled examples 
can be found in the pool. That is, all informative 
examples have been selected for annotation. 
However, this situation seldom occurs in real-
world applications. Thirdly, the active learning 
process can stop if the targeted performance level 
is achieved. However, it is difficult to predefine an 
appropriate and achievable performance, since it 
should depend on the problem at hand and the 
users? requirements.  
2.3 Problem of Performance Estimation 
An appealing solution has the active learning 
process end when repeated cycles show no 
significant performance improvement on the test 
set. However, there are two open problems. The 
first question is how to measure the performance of 
a classifier in active learning. The second one is 
how to know when the resulting classifier reaches 
the highest or adequate performance. It seems 
feasible that a separate validation set can solve 
both problems. That is, the active learning process 
can end if there is no significant performance 
improvement on the validation set. But how many 
samples are required for the pregiven separate 
validation set is an open question. Too few 
samples may not be adequate for a reasonable 
estimation and may result in an incorrect result. 
Too many samples would cause additional high 
cost because the separate validation set is generally 
constructed manually in advance.  
3 Statistical Learning Approach 
3.1 Confidence-based Strategy 
To avoid the problem of performance estimation 
mentioned above, Zhu and Hovy (2007) proposed 
a confidence-based framework to predict the upper 
bound and the lower bound for a stopping criterion 
in active learning. The motivation is to assume that 
the current training data is sufficient to train the 
classifier with maximum effectiveness if the cur-
rent classifier already has acceptably strong confi-
dence on its classification results for all remained 
unlabeled data.  
The first method to estimate the confidence of 
the classifier is based on uncertainty measurement, 
considering whether the entropy of each selected 
unlabeled example is less than a small predefined 
threshold. Here we call it Entropy-MCS. The 
stopping criterion SC Entropy-MCS can be defined as: 
 
1 , ( )
0 ,
E
Entropy MCS
x U UM x
SC
otherwise
?
?
? ? ??= ??
    (3) 
where ?E is a user predefined entropy threshold and 
the function UM(x) evaluates the uncertainty of 
each unlabeled example x.  
The second method to estimate the confidence 
of the classifier is based on feedback from the ora-
cle when the active learner asks for true labels for 
selected unlabeled examples, by considering 
whether the current trained classifier could 
correctly predict the labels or the accuracy 
performance of predictions on selected unlabeled 
examples is already larger than a predefined 
accuracy threshold. Here we call it OracleAcc-
MCS. The stopping criterion SCOracleAcc-MCS can be 
defined as: 
1 (
0 ,
) A
OracleAcc MCS
OracleAcc C
SC
otherwise
?
?
??= ??
    (4) 
where ?A is a user predefined accuracy threshold 
and function OracleAcc(C) evaluates accuracy per-
formance of the classifier on these selected unla-
beled examples through feedback of the Oracle.  
3.2 Minimum Expected Error Strategy 
In fact, these above two confidence-based methods 
do not directly estimate classifier performance that 
closely reflects the classifier effectiveness, because 
they only consider entropy of each unlabeled 
example and accuracy on selected informative 
examples at each iteration step. In this section we 
therefore propose a new statistical learning ap-
proach to defining a stopping criterion through es-
timation of the classifier?s expected error on all 
future unlabeled examples, which we call minimum 
expected error strategy (MES). The motivation 
behind MES is that the classifier C (a classifier for 
uncertainty sampling or set of classifiers for com-
mittee-based sampling) with maximum effective-
ness is the one that results in the lowest expected 
368
error on whole test set in the learning process. The 
stopping criterion SC MES is defined as: 
1 ( )
0 ,
err
MES
Error C
SC
otherwise
???= ??           (5) 
where ?err is a user predefined expected error 
threshold and the function Error(C) evaluates the 
expected error of the classifier C that closely re-
flects the classifier effectiveness. So the key point 
of defining MES-based stopping criterion SC MES is 
how to calculate the function Error(C) that denotes 
the expected error of the classifier C.  
Suppose given a training set L and an input 
sample x, we can write the expected error of the 
classifier C as follows: 
( ) ( ( ) | ) ( )Error C R C x x P x dx= ?           (6) 
where P(x) represents the known marginal distribu-
tion of x. C(x) represents the classifier?s decision 
that is one of k classes: y?Y={y1, y2, ?, yk}. R(yi|x) 
denotes a conditional loss for classifying the input 
sample x into a class yi that can be defined as 
1
( | ) [ , ] ( | )
k
i j
j
R y x i j P y x?
=
=?             (7) 
where P(yj|x) is the a posteriori probability pro-
duced by the classifier C. ?[i,j] represents a zero-
one loss function for every class pair {i,j} that as-
signs no loss to a correct classification, and assigns 
a unit loss to any error. 
In this paper, we focus on pool-based active 
learning in which a large unlabeled data pool U is 
available, as described Fig. 1. In active learning 
process, our interest is to estimate the classifier?s 
expected error on future unlabeled examples in the 
pool U. That is, we can stop the active learning 
process when the active learner results in the low-
est expected error over the unlabeled examples in 
U. The pool U can provide an estimate of P(x). So 
for minimum error rate classification (Duda and 
Hart. 1973) on unlabeled examples, the expected 
error of the classifier C can be rewritten as 
1
( ) (1 max ( | ))
y Y
x U
Error C P y x
U ??
= ??        (8) 
Assuming N unlabeled examples in the pool U, 
the total time is O(N) for automatically determin-
ing whether the proposed stopping criterion SCMES 
is satisfied in the active learning.  
If the pool U is very large (e.g. more than 
100000 examples), it would still cause high com-
putation cost at each iteration of active learning. A 
good approximation is to estimate the expected 
error of the classifier using a subset of the pool, not 
using all unlabeled examples in U. In practice, a 
good estimation of expected error can be formed 
with few thousand examples. 
4 Evaluation 
In this section, we evaluate the effectiveness of 
three stopping criteria for active learning for word 
sense disambiguation and text classification as 
follows: 
? Entropy-MCS ? stopping active learning 
process when the stopping criterion function 
SCEntropy-MCS defined in (3) is equal to 1, where 
?E=0.01, 0.001,  0.0001.  
? OracleAcc-MCS ? stopping active learning 
process when the stopping criterion function 
SCOracleAcc-MCS defined in (4) is equal to 1, 
where ?A=0.9, 1.0.  
? MES ? stopping active learning process when 
the stopping criterion function SCMES defined 
in (5) is equal to 1, where ?err=0.01, 0.001, 
0.0001.  
The purpose of defining stopping criterion of 
active learning is to study how much annotation is 
sufficient for a specific task. To comparatively 
analyze the effectiveness of each stopping criterion, 
a baseline stopping criterion is predefined as when 
all unlabeled examples in the pool U are learned. 
Comparing with the baseline stopping criterion, a 
better stopping criterion not only achieves almost 
the same performance, but also has needed to learn 
fewer unlabeled examples when the active learning 
process is ended. In other words, for a stopping 
criterion of active learning, the fewer unlabeled 
examples that have been leaned when it is met, the 
bigger reduction in human labeling cost is made. 
In the following active learning experiments, a 
10 by 10-fold cross-validation was performed. All 
results reported are the average of 10 trials in each 
active learning process.  
4.1 Word Sense Disambiguation 
The first comparison experiment is active learning 
for word sense disambiguation. We utilize a 
maximum entropy (ME) model (Berger et al, 
1996) to design the basic classifier used in active 
learning for WSD. The advantage of the ME model 
is the ability to freely incorporate features from 
369
diverse sources into a single, well-grounded statis-
tical model. A publicly available ME toolkit 
(Zhang et. al., 2004) was used in our experiments. 
In order to extract the linguistic features necessary 
for the ME model in WSD tasks, all sentences con-
taining the target word are automatically part-of-
speech (POS) tagged using the Brill POS tagger 
(Brill, 1992). Three knowledge sources are used to 
capture contextual information: unordered single 
words in topical context, POS of neighboring 
words with position information, and local colloca-
tions. These are same as the knowledge sources 
used in (Lee and Ng, 2002) for supervised auto-
mated WSD tasks.  
The data used for comparison experiments was 
developed as part of the OntoNotes project (Hovy 
et al, 2006), which uses the WSJ part of the Penn 
Treebank (Marcus et al, 1993). The senses of 
noun words occurring in OntoNotes are linked to 
the Omega ontology (philpot et al, 2005). In 
OntoNotes, at least two human annotators 
manually annotate the coarse-grained senses of 
selected nouns and verbs in their natural sentence 
context. In this experiment, we used several tens of 
thousands of annotated OntoNotes examples, 
covering in total 421 nouns with an inter-annotator 
agreement rate of at least 90%. We find that 302 
out of 421 nouns occurring in OntoNotes are 
ambiguous, and thus are used in the following 
WSD experiments. For these 302 ambiguous 
nouns, there are 3.2 senses per noun, and 172 
instances per noun.  
The active learning algorithms start with a 
randomly chosen initial training set of 10 labeled 
samples for each noun, and make 10 queries after 
each learning iteration. Table 1 shows the 
effectiveness of each stopping criterion tested on 
active learning for WSD on these ambiguous 
nouns? WSD tasks. We analyze average accuracy 
performance of the classifier and average 
percentage of unlabeled examples learned when 
each stopping criterion is satisfied in active 
learning for WSD tasks. All accuracies and 
percentages reported in Table 1 are macro-
averages over these 302 ambiguous nouns. 
 
 
 
 
 
 
Stopping Criterion Average accuracy 
Average 
percentage 
all unlabeled examples learned 87.3% 100% 
Entropy-MCS method (0.0001) 86.8% 81.8% 
Entropy-MCS method (0.001) 86.8% 75.8% 
Entropy-MCS method (0.01) 86.8% 68.6% 
OracleAcc-MCS method (0.9) 86.8% 56.5% 
OracleAcc-MCS method (1.0) 86.8% 62.4% 
MES method (0.0001) 86.8% 67.1% 
MES method (0.001) 86.8% 58.8% 
MES method (0.01) 86.8% 52.7% 
Table 1. Effectiveness of each stopping criterion of 
active learning for WSD on OnteNotes. 
 
Table 1 shows that these stopping criteria 
achieve the same accuracy of 86.8% which is 
within 0.5% of the accuracy of the baseline method 
(all unlabeled examples are labeled). It is obvious 
that these stopping criteria can help reduce the hu-
man labeling costs, comparing with the baseline 
method. The best criterion is MES method 
(?err=0.01), following by OracleAcc-MCS method 
(?A=0.9). MES method (?err=0.01) and OracleAcc-
MCS method (?A=0.9) can make 47.3% and 44.5% 
reductions in labeling costs, respectively. Entropy-
MCS method is apparently worse than MES and 
OracleAcc-MCS methods. The best of the 
Entropy-MCS method is the one with ?E=0.01 
which makes approximately 1/3 reduction in 
labeling costs. We also can see from Table 1 that 
for Entropy-MCS and MES methods, reduction 
rate becomes smaller as the ? becomes smaller. 
4.2 Text Classification 
The second data set is for active learning for text 
classification using the WebKB corpus 1  
(McCallum et al, 1998). The WebKB dataset was 
formed by web pages gathered from various uni-
versity computer science departments. In the fol-
lowing active learning experiment, we use four 
most populous categories: student, faculty, course 
and project, altogether containing 4,199 web pages. 
Following previous studies (McCallum et al, 
1998), we only remove those words that occur 
merely once without using stemming or stop-list. 
The resulting vocabulary has 23,803 words. In the 
design of the text classifier, the maximum entropy 
model is also utilized, and no feature selection 
technique is used. 
                                                 
1 See http://www.cs.cmu.edu/~textlearning 
370
The algorithm is initially given 20 labeled ex-
amples, 5 from each class. Table 2 shows the 
effectiveness of each stopping criterion of active 
learning for text classification on WebKB corpus. 
All results reported are the average of 10 trials. 
Stopping Criterion Average accuracy 
Average 
percentage 
all unlabeled examples learned 93.5% 100% 
Entropy-MCS method (0.0001) 92.5% 23.8% 
Entropy-MCS method (0.001) 92.4% 22.3% 
Entropy-MCS method (0.01) 92.5% 21.8% 
OracleAcc-MCS method (0.9) 91.5% 13.1% 
OracleAcc-MCS method (1.0) 92.5% 24.5% 
MES method (0.0001) 92.1% 17.9% 
MES method (0.001) 92.0% 15.6% 
MES method (0.01) 91.5% 10.9% 
Table 2. Effectiveness of each stopping criterion of 
active learning for TC on WebKB corpus. 
 
From results shown in Table 2, we can see that 
MES method (?err=0.01) already achieves 91.5% 
accuracy in 10.9% unlabeled examples learned. 
The accuracy of all unlabeled examples learned is 
93.5%. This situation means the approximately 
90% remaining unlabeled examples only make 
only 2% performance improvement. Like the 
results of WSD shown in Table 1, for Entropy-
MCS and MES methods used in active learning for 
text classification tasks, the corresponding 
reduction rate becomes smaller as the value of ? 
becomes smaller. MES method (?err=0.01) can 
make approximately 90% reduction in human la-
beling costs and results in 2% accuracy perform-
ance degradation. The Entropy-MCS method 
(?E=0.01) can make approximate 80% reduction in 
costs and results in 1% accuracy performance 
degradation. Unlike the results of WSD shown in 
Table 1, the OracleAcc-MCS method (?A=1.0) 
makes the smallest reduction rate of 75.5%. 
Actually in real-world applications, the selection of 
a stopping criterion is a tradeoff issue between 
labeling cost and effectiveness of the classifier.  
5 Discussion 
It is interesting to investigate the impact of per-
formance change on defining a stopping criterion, 
so we show an example of active learning for 
WSD task in Fig. 2.  
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0  20  40  60  80  100  120  140  160  180  200  220
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD task
rate-n
 
Figure 2. An example of active learning for WSD 
on noun ?rate? in OntoNotes. 
 
Fig. 2 shows that the accuracy performance gen-
erally increases, but apparently degrades at the it-
erations ?20?, ?80?, ?170?, ?190?, and ?200?, and 
does not change anymore during the iterations 
[?130?-?150?] or [?200?-?220?] in the active learn-
ing process. Actually the first time of the highest 
performance of 95% achieved is at ?450?, which is 
not shown in Fig. 2. In other words, although the 
accuracy performance curve shows an increasing 
trend, it is not monotonously increasing. From Fig. 
2 we can see that it is not easy to automatically 
determine the point of no significant performance 
improvement on the validation set, because points 
such as ?20? or ?80? would mislead final judgment. 
However, we do believe that the change of per-
formance is a good signal to stop active learning 
process. So it is worth studying further how to 
combine the factor of performance change with our 
proposed stopping criteria of active learning.  
The OracleAcc-MCS method would not work if 
only one or too few informative examples are 
queried at the each iteration step in the active 
learning. There is an open issue how many selected 
unlabeled examples at each iteration are adequate 
for the batch-based sample selection.  
For these stopping crieria, there is no general 
method to automatically determine the best 
threshold for any given task. It may therefore be 
necessary to use a dynamic threshold change tech-
nique in which the predefined threshold can be 
automatically modified if the performance is still 
significantly improving when the stopping crite-
rion is met during active learning process.  
371
6 Conclusion and Future Work 
In this paper, we address the stopping criterion is-
sue of active learning, and analyze the problems 
faced by some common ways to stop the active 
learning process. In essence, defining a stopping 
criterion of active learning is a problem of estimat-
ing classifier effectiveness. The purpose of defin-
ing stopping criterion of active learning is to know 
how much annotation is sufficient for a special task. 
To determine this, this paper proposes a new statis-
tical learning approach, called minimum expected 
error strategy, for defining a stopping criterion 
through estimation of the classifier?s expected er-
ror on future unlabeled examples during the active 
learning process. Experimental results on word 
sense disambiguation and text classification tasks 
show that new proposed minimum expected error 
strategy outperforms the confidence-based strategy, 
and achieves promising results. The interesting 
future work is to study how to combine the best of 
both strategies, and how to consider performance 
change to define an appropriate stopping criterion 
for active learning.  
Acknowledgments 
This work was supported in part by the National 
Natural Science Foundation of China under Grant 
(60473140), the National 863 High-tech Project 
(2006AA01Z154); the Program for New Century 
Excellent Talents in University(NCET-05-0287).  
References 
A. L. Berger, S. A. Della, and V. J  Della. 1996. A 
maximum entropy approach to natural language 
processing. Computational Linguistics 22(1):39?71. 
E Brill. 1992. A simple rule-based part of speech tag-
ger. In the Proceedings of the Third Conference on 
Applied Natural Language Processing. 
J. Chen, A. Schein, L. Ungar, M. Palmer. 2006. An 
empirical study of the behavior of active learning for 
word sense disambiguation. In Proc. of HLT-
NAACL06 
R. O. Duda and P. E. Hart. 1973. Pattern classification 
and scene analysis. New York: Wiley.  
S. A. Engelson and I. Dagan. 1999. Committee-based 
sample selection for probabilistic classifiers. Journal 
of Artificial Intelligence Research. 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R. 
Weischedel. 2006. Ontonotes: The 90% Solution. In 
Proc. of HLT-NAACL06. 
Y.K. Lee and. H.T. Ng. 2002. An empirical evaluation 
of knowledge sources and learning algorithm for 
word sense disambiguation. In Proc. of EMNLP02 
D. D. Lewis and W. A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In Proc. of SIGIR-
94 
M. Li, I. K. Sethi. 2006. Confidence-based active learn-
ing. IEEE transaction on pattern analysis and ma-
chine intelligence, 28(8):1251-1261. 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics, 
19(2):313-330 
A. McCallum and K. Nigram. 1998. Employing EM in 
pool-based active learning for text classification. In 
Proc. of 15th ICML 
G. Ngai and D. Yarowsky. 2000. Rule writing or anno-
tation: cost-efficient resource usage for based noun 
phrase chunking. In Proc. of ACL-02  
A. Philpot, E. Hovy and P. Pantel. 2005. The Omega 
Ontology. In Proc. of ONTOLEX Workshop at 
IJCNLP. 
D. Shen, J. Zhang, J. Su, G. Zhou and C. Tan. 2004. 
Multi-criteria-based active learning for named entity 
recognition. In Prof. of ACL-04. 
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sakar, 
J. Hockenmaier, P. Ruhlen, S. Baker and J. Crim. 
2003. Example selection for bootstrapping statistical 
parsers. In Proc. of HLT-NAACL-03 
C. A. Thompson, M. E. Califf and R. J. Mooney. 1999. 
Active learning for natural language parsing and in-
formation extraction. In Proc. of ICML-99. 
K. Tomanek, J. Wermter and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts anno-
tation costs and maintains reusability of annotated 
data. In Proc. of EMNLP/CoNLL07 
L. Zhang, J. Zhu, and T. Yao. 2004. An evaluation of 
statistical spam filtering techniques. ACM Transac-
tions on Asian Language Information Processing, 
3(4):243?269. 
J. Zhu, E. Hovy. 2007. Active learning for word sense 
disambiguation with methods for addressing the 
class imbalance problem. In Proc. of 
EMNLP/CoNLL07 
372
An Effective Hybrid Machine Learning Approach for Coreference 
Resolution 
Feiliang Ren 
Natural Language Processing Lab 
College of Information Science and En-
gineering 
Northeastern University, P.R.China 
renfeiliang@ise.neu.edu.cn 
Jingbo Zhu 
Natural Language Processing Lab 
College of Information Science and En-
gineering 
Northeastern University, P.R.China 
zhujingbo@ise.neu.edu.cn 
 
 
Abstract 
We present a hybrid machine learning ap-
proach for coreference resolution. In our 
method, we use CRFs as basic training 
model, use active learning method to gen-
erate combined features so as to make ex-
isted features used more effectively; at last, 
we proposed a novel clustering algorithm 
which used both the linguistics knowledge 
and the statistical knowledge. We built a 
coreference resolution system based on the 
proposed method and evaluate its perform-
ance from three aspects: the contributions 
of active learning; the effects of different 
clustering algorithms; and the resolution 
performance of different kinds of NPs. Ex-
perimental results show that additional per-
formance gain can be obtained by using ac-
tive learning method; clustering algorithm 
has a great effect on coreference resolu-
tion?s performance and our clustering algo-
rithm is very effective; and the key of 
coreference resolution is to improve the 
performance of the normal noun?s resolu-
tion, especially the pronoun?s resolution. 
1 Introduction 
Coreference resolution is the process of determin-
ing whether two noun phrases (NPs) refer to the 
same entity in a document. It is an important task 
in natural language processing and can be classi-
fied into pronoun phrase (denoted as PRO) resolu-
tion, normal noun phrase (denoted as NOM) reso-
lution, and named noun phrase (denoted as NAM) 
resolution. Machine learning approaches recast this 
problem as a classification task based on con-
straints that are learned from an annotated corpus. 
Then a separate clustering mechanism is used to 
construct a partition on the set of NPs.  
Previous machine learning approaches for 
coreference resolution (Soon et al 2001; Ng et al 
2002; Florian et al 2004, etc) usually selected a 
machine learning approach to train a classification 
model, used as many as possible features for the 
training of this classification model, and finally 
used a clustering algorithm to construct a partition 
on the set of NPs based on the statistical data ob-
tained from trained classification model. Their ex-
perimental results showed that different kinds of 
features had different contributions for system?s 
performance, and usually the more features used, 
the better performance obtained. But they rarely 
focused on how to make existed features used 
more effectively; besides, they proposed their own 
clustering algorithm respectively mainly used the 
statistical data obtained from trained classification 
model, they rarely used the linguistics knowledge 
when clustering different kinds of NPs. Also, there 
were fewer experiments conducted to find out the 
effect of a clustering algorithm on final system?s 
performance.  
In this paper, we propose a new hybrid machine 
learning method for coreference resolution. We use 
NP pairs to create training examples; use CRFs as 
a basic classification model, and use active learn-
ing method to generate some combined features so 
as to make existed features used more effectively; 
at last, cluster NPs into entities by a novel cascade 
clustering algorithm.  
The rest of the paper is organized as follows. 
Section 2 presents our coreference resolution sys-
24
Sixth SIGHAN Workshop on Chinese Language Processing
tem in detail. Section 3 is our experiments and dis-
cussions. And at last, we conclude our work in sec-
tion 4. 
2 Coreference Resolution 
There are three basic components for a coreference 
resolution system that uses machine learning ap-
proach: the training set creation, the feature selec-
tion, and the coreference clustering algorithm. We 
will introduce our methods for these components 
respectively as follows. 
2.1 Training Set Creation 
Previous researchers (Soon et al, 2001, Vincent 
Ng et al, 2002, etc) took different creation strate-
gies for positive examples and negative examples. 
Because there were no experimental results 
showed that these kinds of example creation meth-
ods were helpful for system?s performance, we 
create both positive examples and negative exam-
ples in a unified NP pair wise manner.  
Given an input NP chain of an annotated docu-
ment, select a NP in this NP chain from left to right 
one by one, and take every of its right side?s NP, 
we generate a positive example if they refer to the 
same entity or a negative example if they don?t 
refer to the same entity. For example, there is a NP 
chain n1-n2-n3-n4 found in document, we will 
generate following training examples: (n1-n2, 1?  ), 
(n1-n3,  ), (n1-n4,  ), (n2-n3,1? 1? 1?  ), (n2-
n4,  ), and (n3-n4,  ). Where denotes that 
this is a positive example, and denotes that this 
is a negative example.  
1? 1? 1+
1?
2.2 Feature Sets 
In our system, two kinds of features are used. One 
is atomic feature, the other is combined feature. 
We define the features that have only one genera-
tion condition as atomic features, and define the 
union of some atomic features as combined fea-
tures.  
2.2.1 Atomic Features 
All of the atomic features used in our system are 
listed as follows.  
String Match Feature (denoted as Sm): Its possi-
ble values are exact, left, right, included, part, 
alias, and other. If two NPs are exactly string 
matched, return exact; if one NP is the left sub-
string of the other, return left; if one NP is the right 
substring of the other, return right; if all the char-
acters in one NP are appeared in the other but not 
belong to set {left, right}, return included; if some 
(not all) characters in one NP are appeared in the 
other, return part; if one NP is the alias of the other, 
return alias; if two NPs don?t have any common 
characters, return other.  
Lexical Similarity Features (denoted as Ls): 
compute two NP?s similarity and their head words? 
similarity using following formula 1. 
1 2
1 2
1 2
2 (
( , )
( ) ( )
SameChar n n
Sim n n
Len n Len n
, )?= +   (1) 
Here means the common 
characters? number in  and ;  is the 
total characters? number in . 
( , )1 2SameChar n n
1n 2n ( )Len ni
ni
Edit Distance Features (denoted as Ed): compute 
two NP?s edit distance and their head words? edit 
distance (Wagner and Fischer, 1974), and the pos-
sible values are true and false. If the edit distance 
of two NPs (or the head words of these two NPs) 
are less than or equal to 1, return true, else return 
false. 
Distance Features (denoted as Dis): distance be-
tween two NPs in words, NPs, sentences, para-
graphs, and characters. 
Length Ratio Features (denoted as Lr): the length 
ratio of two NPs, and their head words. Their pos-
sible values belong to the range (0 . ,1]
NP?s Semantic Features (denoted as Sem): the 
POSs of two NPs? head words; the types of the two 
NPs (NAM, NOM or PRO); besides, if one of the 
NP is PRO, the semantic features will also include 
this NP?s gender information and plurality infor-
mation. 
Other Features (denoted as Oth): whether two 
NPs are completely made up of capital English 
characters; whether two NPs are completely made 
up of lowercase English characters; whether two 
NPs are completely made up of digits. 
2.2.2 Combined Features Generated by Ac-
tive Learning 
During the process of model training for corefer-
ence resolution, we found that we had very fewer 
available resources compared with previous re-
searchers. In their works, they usually had some 
extra knowledge-based features such as alias table,  
abbreviation table, wordnet and so on; or they  had 
25
Sixth SIGHAN Workshop on Chinese Language Processing
some extra in-house analysis tools such as proper 
name parser, chunk parser, rule-based shallow 
coreference resolution parser, and so on (Hal 
Daume III, etc, 2005; R.Florian, etc, 2004; Vincent 
Ng, etc, 2002; etc). Although we also collected 
some aliases and abbreviations, the amounts are 
very small compared with previous researchers?. 
We hope we can make up for this by making ex-
isted features used more effectively by active 
learning method.  
Formally, active learning studies the closed-loop 
phenomenon of a learner selecting actions or mak-
ing queries that influence what data are added to its 
training set. When actions or queries are selected 
properly, the data requirements for some problems 
decrease drastically (Angluin, 1988; Baum & Lang, 
1991). In our system, we used a pool-based active 
learning framework that is similar as Manabu Sas-
sano (2002) used, this is shown in figure 1.  
 
Figure 1: Our Active Learning Framework 
In this active learning framework, an initial clas-
sifier is trained by CRFs [1] that uses only atomic 
features, and then two human teachers are asked to 
correct some selected wrong classified examples 
independently. During the process of correction, 
without any other available information, system 
only shows the examples that are made up of fea-
tures to the human teachers; then these two human 
teachers have to use the information of some 
atomic features? combinations to decide whether 
two NPs refer to the same entity. We record all 
these atomic features? combinations that used by 
both of these human teachers, and take them as 
combined features. 
For example, if both of these human teachers 
correct a wrong classified example based on the 
knowledge that ?if two NPs are left substring 
                                                 
1 http://www.chasen.org/~taku/software/CRF++/ 
matched, lexical similarity feature is greater than 
0.5, I think they will refer to the same entity?, the 
corresponding combined feature would be de-
scribed as: ?Sm(NPs)-Ls(NPs)?, which denotes the 
human teachers made their decisions based on the 
combination information of ?String Match Fea-
tures? and ?Lexical Similarity Features?. 
 
Figure 2: Selection Algorithm 
1. Select all the wrong classified examples whose 
CRFs? probability  belongs to range [0.4, 0.6] 
2.  Sort these examples in decreasing order. 
3.  Select the top m examples 
In figure 1, ?information? means the valuable 
data that can improve the system?s performance 
after correcting their classification. The selection 
algorithm for ?informative? is the most important 
component in an active learning framework. We 
designed it from the degree of correcting difficulty. 
We know 0.5 is a critical value for an example?s 
classification. For a wrong classified example, the 
closer its probability value to 0.5, the easier for us 
to correct its classification. Following this, our se-
lection algorithm for ?informative? is designed as 
shown in figure 2. 
1. Build an initial classifier 
2. While teacher can correct examples based on
feature combinations 
a) Apply the current classifier to training ex-
amples 
b) Find m most informative training examples
c) Have two teachers correct these examples
based on feature combinations 
d) Add the feature combinations that are used
by both of these two teachers to feature
sets in CRFs and train a new classifier. 
When add new combined features won?t lead to 
a performance improvement, we end active learn-
ing process. Totally we obtained 21 combined fea-
tures from active learning. Some of them are listed 
in table 1.  
Table 1: Some Combined Features 
Sm(NPs)-Sm(HWs)-Ls(NPs)-Ls(HWs) 
Sm(NPs)-Sm(HWs)-Ls(NPs) 
Sm(NPs)-Sm(HWs)-Ls(HWs) 
Sm(NPs)-Sm(HWs)-Lr(NPs)-Lr(HWs) 
Sm(NPs)-Sm(HWs)-Lr(NPs) 
Sm(NPs)-Sm(HWs)-Sem(HW1)-Sem(HW2) 
Sm(NPs)-Sm(HWs)-Sem(NP1)-Sem(NP2) 
Sm(NPs)-Sm(HWs)-Lr(HWs) 
?? 
Here ?Sm(NPs)? means the string match fea-
ture?s value of two NPs, ?Sm(HWs)? means the 
string match feature?s value of two NPs? head 
words. ?HWs? means the head words of two NPs. 
Combined feature ?Sm(NPs)-Sm(HWs)-Ls(NPs)? 
means when correcting a wrong classified example, 
both these human teachers made their decisions 
based on the combination information of Sm(NPs), 
Sm(HWs), and Ls(NPs) . Other combined features 
have the similar explanation. 
26
Sixth SIGHAN Workshop on Chinese Language Processing
And at last, we take all the atomic features and 
the combined features as final features to train the 
final CRFs classifier.  
2.3 Clustering Algorithm 
Formally, let { : be NPs in a docu-
ment. Let us define the set of 
NPs whose types are all NAMs; define 
the set of NPs whose types are 
all NOMs; define the set of 
NPs whose types are all PROs. Let be 
the map from NP index i to entity index
1 }im i n? ? n
1{ ,..., }a a afS N N=
1{ ,..., }o o oS N N= g
k1{ ,..., }p p pS N N=
:g i ja
j . For a 
NP index , let us define 
the set of indices of the 
partially-established entities before clustering , 
and , the set of the partially-
established entities. Let  be the 
(1 )k k n? ?
?{ (1),..., ( 1)}kJ g g k=
km
{ : }k t kE e t J= ?
ije j th?  NP in 
entity. Let i th? ( , )i jprob m m be the probability 
that  and refer to the same entity, and im jm
( , )i jprob m m can be trained from CRFs. 
Given that has been formed before cluster-
ing , can take two possible actions: 
if , then the active NP is said to link 
with the entity
kE
km km
( ) kg k J? km
( )g ke ; otherwise it starts a new en-
tity ( )g ke .  
In this work, P L is used to 
compute the link probability, where t , is 1 
iff links with ; the random variable A is the 
index of the partial entity to which m is linking.  
( 1| , ,k k )E m A t= =
J? k L
km te
k
Our clustering algorithm is shown in figure 3. 
The basic idea of our clustering algorithm is that 
NAMs, NOMs and PROs have different abilities 
starting an entity. For NAMs, they are inherent 
antecedents in entities, so we start entities based on 
them first.  
For NOMs, they have a higher ability of acting 
as antecedents in entities than PROs, but lower 
than NAMs. We cluster them secondly, and add a 
NOM in an existed entity as long as their link 
probability is higher than a threshold. And during 
the process of the link probabilities computations, 
we select a NP in an existed entity carefully, and 
take these two NPs? link probability as the link 
probability between this NOM and current entity. 
The selection strategy is to try to make these link 
probabilities have the greatest distinction.  
And for PROs, they have the lowest ability of 
acting as antecedents in entities, most of the time, 
they won?t be antecedents in entities; so we cluster 
them into an existed entity as long as there is a 
non-zero link probability. 
3 Experiments and Discussions  
Our experiments are conducted on Chinese EDR 
(Entity Detection and Recognize) &EMD (Entity 
Mention Detection) corpora from LDC. These cor-
pora are the training data for ACE (Automatic 
Content Extraction) evaluation 2004 and ACE 
evaluation 2005. These corpora are annotated and 
can be used to train and test the coreference resolu-
tion task directly. 
 
Figure 3: Our Clustering Algorithm 
Input: M = { :1 }im i n? ?  
Output: a partition E of the set M  
Initialize: 0 { {{ : }}}i i i aH e m m S? = ?  
if x c y dm e m e? ? ? ? , c d? , and xm is alias of 
ym , then  ' \{ } { }d c dH H e e e? ? ?  
foreach k om S? that hasn?t been clustered 
    if 0ke is NAM and d? makes ( , ) 0tde NOM? ?
      P= arg max
te
 
| | min
{ ( , ) ( , )}tdk td
d k
prob m e e NOM?
? =
?
esleif 0ke is NAM and , ( , ) 0tdd e NOM?? ==  
     P= arg max
te
 
| | min
{ ( , ) ( , )}tdk td
d k
prob m e e NAM?
? =
?
esleif 0ke is NOM 
P= arg max
te
 0( , )k tprob m e  
 if P ?? , ' \{ } { { }}t t kH H e e m? ? ?  
 else ' { }kH H m? ?  
foreach k pm S? that hasn?t been clustered 
      P= arg max ( , )
t
k
m e
prob m m
?
  
  if 0P > , ' \{ } { { }}t t kH H e e m? ? ?  
  else ' { }kH H m? ?  
return H
27
Sixth SIGHAN Workshop on Chinese Language Processing
In ACE 2004 corpus, there are two types of 
documents: paper news (denoted as newswire) and 
broadcast news (denoted as broadca); for ACE 
2005 corpus, a new type added: web log docu-
ments (denoted as weblogs). Totally there are 438 
documents in ACE 2004 corpus and 636 docu-
ments in ACE 2005 corpus. We randomly divide 
these two corpora into two parts respectively, 75% 
of them for training CRFs model, and 25% of them 
for test. By this way, we get 354 documents for 
training and 84 documents for test in ACE 2004 
corpus; and 513 documents for training and 123 
documents for test in ACE 2005 corpus.  
Some statistics of ACE2005 corpus and 
ACE2004 corpus are shown in table 2. 
Our experiments were classified into three 
groups. Group 1 (denoted as ExperimentA) is de-
signed to evaluate the contributions of active learn-
ing for the system?s performance. We developed 
two systems for ExperimentA, one is a system that 
used only the atomic features for CRFs training 
and we took it as a baseline system, the other is a 
system that used both the atomic features and the 
combined features for CRFs training and we took it 
as our final system. The experimental results are 
shown in table 3 and table 4 for different corpus 
respectively. Bold font is the results of our final 
system, and normal font is the results of baseline 
system. Here we used the clustering algorithm as 
described in figure 3. 
Group 2 (denoted as ExperimentB) is designed 
to investigate the effects of different clustering al-
gorithm for coreference resolution. We imple-
mented another two clustering algorithms: algo-
rithm1 that is proposed by Ng et al (2002) and 
algorithm2 that is proposed by Florian et al (2004). 
We compared the performance of them with our 
clustering algorithm and experimental results are 
shown in table 5. 
Group 3 (denoted as ExperimentC) is designed 
to evaluate the resolution performances of different 
kinds of NPs. We think this is very helpful for us 
to find out the difficulties and bottlenecks of 
coreference resolution; and also is helpful for our 
future work. Experimental results are shown in 
table 6. 
In ExperimentB and ExperimentC, we used 
both atomic features and combined features for 
CRFs classification model training. And in table5, 
table6 and table7, the data before ?/? are experi-
mental results for ACE2005 corpus and the data 
after ?/? are experimental results for ACE2004 
corpus.  
In all of our experiments, we use recall, preci-
sion, and F-measure as evaluation metrics, and de-
noted as R, P, and F for short respectively.  
Table 2: Statistics of ACE2005/2004 Corpora 
 Training Test 
# of all documents 513/354 123/84 
# of broadca 204/204 52/47 
# of newswire 229/150 54/47 
#of weblogs 80/0 17/0 
# of characters 248972/164443 55263/35255
# of NPs 28173/18995 6257/3966
# of entities 12664/8723 2783/1828
# of neg examples 722919/488762 142949/89894
# of  pos examples 72000/44682 15808/8935
Table3: ExperimentA for ACE2005 Corpora 
 R P F 
broadca 79.0/76.2 75.4/72.9 77.2/74.5
newswire 73.2/72.9 68.7/67.8 70.9/70.3
weblogs 72.3/68.5 65.5/63.3 68.8/65.8
total 75.4/73.7 70.9/69.3 73.1/71.4
Table4: ExperimentA for ACE2004 Corpora 
 R P F 
broadca 74.7/71.0 72.4/68.9 73.5/69.9
newswire 77.7/73.1 73.0/68.6 75.2/70.7
Total 76.2/72.0 72.7/68.7 74.4/70.4
Table5: ExperimentB for ACE2005/2004 Corpora 
 R P F 
algorithm1 61.0/63.5 59.5/62.8 60.2/63.2
algorithm2 61.0/62.4 60.7/62.8 60.9/62.6
Ours 75.4/76.2 70.9/72.7 73.1/74.4
Table6: ExperimentC for ACE2005/2004 Corpora 
 R P F 
NAM 80.5/81.4 77.9/79.2 79.2/80.1
NOM 62.6/62.5 54.4/56.8 58.2/59.5
PRO 28.4/29.8 22.7/24.0 25.2/26.6
From table 3 and table 4 we can see that the fi-
nal system?s performance made a notable im-
provement compared with the baseline system in 
both corpora. We know the only difference of 
these two systems is whether used active learning 
method. This indicates that by using active learn-
ing method, we make the existed features used 
more effectively and obtain additional performance 
gain accordingly. One may say that even without 
active learning method, he still can add some com-
bined features during CRFs model training. But 
this can?t guarantee it would make a performance 
28
Sixth SIGHAN Workshop on Chinese Language Processing
improvement at anytime. Active learning method 
provides us a way that makes this combined fea-
tures? selection process goes in a proper manner. 
Generally, a system can obtain an obvious per-
formance improvement after several active learn-
ing iterations. We still noticed that the contribu-
tions of active learning for different kinds of 
documents are different. In ACE04 corpus, both 
kinds of documents? performance obtained almost 
equal improvements; in ACE05 corpus, there is 
almost no performance improvement for newswire 
documents, but broadcast documents? performance 
and web log documents? performance obtained 
greater improvements. We think this is because for 
different kinds of documents, they have different 
kinds of correcting rules (these rules refer to the 
combination methods of atomic features) for the 
wrong classified examples, some of these rules 
may be consistent, but some of them may be con-
flicting. Active learning mechanism will balance 
these conflicts and select a most appropriate global 
optimization for these rules. This can also explain 
why ACE04 corpus obtains more performance im-
provement than ACE05 corpus, because there are 
more kinds of documents in ACE05 corpus, and 
thus it is more likely to lead to rule conflicts during 
active learning process.  
Experimental results in table 5 show that if other 
experimental conditions are the same, there are 
obvious differences among the performances with 
different clustering algorithms. This surprised us 
very much because both algorithm1 and algo-
rithm2 worked very well in their own learning 
frameworks. We know R.Florian et al (2004) first 
proposed algorithm2 using maximum entropy 
model. Is this the reason for the poor performance 
of algorithm2 and algorithm1? To make sure this, 
we conducted other experiments that changed the 
CRFs model to maximum entropy model [2] with-
out changing any other conditions and the experi-
mental results are shown in table 7.  
The experimental results are the same: our clus-
tering algorithm achieved better performance. We 
think this is mainly because the following reason, 
that in our clustering algorithm, we notice the fact 
that different kinds of NPs have different abilities 
of acting as antecedents in an entity, and take dif-
ferent clustering strategy for them respectively, 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html 
this is obvious better than the methods that only 
use statistical data.  
Table7: ExperimentB for ACE2005/2004 Corpora 
with ME Model 
 R P F 
algorithm1 48.9/48.3 44.2/50.3 46.4/49.3
algorithm2 57.4/59.5 52.3/61.4 54.7/60.4
Ours 68.1/69.8 65.7/72.6 66.9/71.2
We also noticed that the experimental results 
with maximum entropy model are poorer than with 
CRFs model. We think this maybe because that the 
combined features are obtained under CRFs model, 
thus they will be more suitable for CRFs model 
than for maximum entropy model, that is to say 
these obtained combined features don?t play the 
same role in maximum entropy model as they do in 
CRFs model. 
Experimental results in table 6 surprised us 
greatly. PRO resolution gets so poor a performance 
that it is only about 1/3 of the NAM resolution?s 
performance. And NOM resolution?s performance 
is also pessimistic, which reaches about 80% of the 
NAM resolution?s performance. After analyses we 
found this is because there is too much confusing 
information for NOM?s resolution and PRO?s reso-
lution and system can hardly distinguish them cor-
rectly with current features description for an ex-
ample. For example, in a Chinese document, a 
NOM ???? (means president) may refer to a 
person A at sometime, but refer to person B at an-
other time, and there is no enough information for 
system to distinguish A and B. It is worse for PRO 
resolution because a PRO can refer to any NAM or 
NOM from a very long distance, there is little in-
formation for the system to distinguish which one 
it really refers to. For example, two PROs that both 
of whom are ??? (means he) , one refers to person 
A, the other refers to person B, even our human can 
hardly distinguish them, not to say the system. 
Fortunately, generally there are more NAMs and 
NOMs in a document, but less PROs. If they have 
similar amounts in a document, you can image 
how poor the performance of the coreference reso-
lution system would be.  
4 Conclusions 
In this paper, we present a hybrid machine learning 
approach for coreference resolution task. It uses 
CRFs as a basic classification model and uses ac-
tive learning method to generate some combined 
29
Sixth SIGHAN Workshop on Chinese Language Processing
features to make existed features used more effec-
tively; and we also proposed an effective clustering 
algorithm that used both the linguistics knowledge 
and the statistical knowledge. Experimental results 
show that additional performance gain can be ob-
tained by using active learning method, clustering 
algorithm has a great effect on coreference resolu-
tion?s performance and our clustering algorithm is 
very effective. Our experimental results also indi-
cate the key of coreference resolution is to improve 
the performance of the NOM resolution, especially 
the PRO resolution; both of them remain chal-
lenges for a coreference resolution system. 
Acknowledgments 
We used the method proposed in this paper for 
Chinese EDR (Entity Detection and Recognition) 
task of ACE07 (Automatic Content Extraction 
2007) and achieved very encouraging result. 
And this work was supported in part by the Na-
tional Natural Science Foundation of China under 
Grant No.60473140; the National 863 High-tech 
Project No.2006AA01Z154; the Program for New 
Century Excellent Talents in University No.NCET-
05-0287; and the National 985 Project No.985-2-
DB-C03. 
Reference 
Andrew Kachites McCallum and Kamal Nigam, 1998. 
Employing EM and pool-based active learning for 
text classification. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pp 
359-367  
Cohn, D., Grahramani, Z., & Jordan, M.1996. Active 
learning with statistical models. Journal of Artificial 
Intelligence Research, 4. pp 129-145 
Cynthia A.Thompson, Mary Leaine Califf, and Ray-
mond J.Mooney. 1999. Active learning for natural 
language parsing and information extraction. In Pro-
ceedings of the Seventeenth International Conference 
on Machine Learning, pp 406-414  
Hal Daume III and Daniel Marcu, 2005, A large-scale 
exploration of effective global features for a joint en-
tity detection and tracking model. Proceedings of 
HLT/EMNLP, 2005 
http://www.nist.gov/speech/tests/ace/ace07/doc, The 
ACE 2007 (ACE07) Evaluation Plan, Evaluation of 
the Detection and Recognition of ACE Entities, Val-
ues, Temporal Expressions, Relations and Events 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data. 
In International Conference on Machine Lear-
ingin(ICML01) 
Lafferty, J., McCallum, A., & Pereira, F. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proc. ICML 
Manabu Sassano. 2002. An Empirical Study of Active 
Learning with Support Vector Machines for Japanese 
Word Segmentation. Proceeding of the 40th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), 2002, pp 505-512  
Min Tang, Xiaoqiang Luo, Salim Roukos.2002. Active 
Learning for Statistical Natural Language Parsing. 
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), 2002, 
pp.120-127. 
Pinto, D., McCallum, A., Lee, X., & Croft, W.B. 2003. 
combining classifiers in text categorization. SIGIR? 
03: Proceedings of the Twenty-sixth Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval.  
Radu Florian, Hongyan Jing, Nanda Kambhatla and 
Imed Zitouni, ?Factoring Complex Models: A Case 
Study in Mention Detection?, in Procedings of the 
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the ACL, pages 
473-480, Sydney, July 2006. 
R Florian, H Hassan et al 2004. A statistical model for 
multilingual entity detection and tracking. In Proc. Of 
HLT/NAACL-04, pp1-8 
Sha, F., & Pereira, F. 2003. Shallow parsing with condi-
tional random fields. Proceedings of Human Lan-
guage Technology, NAACL. 
Simon Tong, Daphne Koller. 2001. Support Vector Ma-
chine Active Learning with Applications to Text 
Classification. Journal of Machine Learning Re-
search,(2001) pp45-66. 
V.Ng and C.Cardie. 2002. Improving machine learning 
approaches to coreference resolution. In Proceedings 
of the ACL?02, pp.104-111. 
W.M.Soon, H.T.Ng, et al2001. A Machine Learning 
Approach to Coreference Resolution of Noun 
Phrases. Computational Linguistics, 27(4):521-544 
30
Sixth SIGHAN Workshop on Chinese Language Processing
Which Performs Better on In-Vocabulary Word Segmentation:  
Based on Word or Character? 
Zhenxing Wang1,2, Changning Huang2 and Jingbo Zhu1 
1 Institute of Computer Software and Theory, Northeastern University,  
Shenyang, China, 110004 
2 Microsoft Research Asia, 49, Zhichun Road,  
Haidian District, Beijing, China, 100080 
zxwang@ics.neu.edu.cn 
v-cnh@microsoft.com 
zhujingbo@mail.neu.edu.cn 
 
Abstract* 
Since the first Chinese Word Segmenta-
tion (CWS) Bakeoff on 2003, CWS has 
experienced a prominent flourish be-
cause Bakeoff provides a platform for 
the participants, which helps them rec-
ognize the merits and drawbacks of their 
segmenters. However, the evaluation 
metric of bakeoff is not sufficient 
enough to measure the performance tho-
roughly, sometimes even misleading.  
One typical example caused by this in-
sufficiency is that there is a popular be-
lief existing in the research field that 
segmentation based on word can yield a 
better result than character-based tag-
ging (CT) on in-vocabulary (IV) word 
segmentation even within closed tests of 
Bakeoff. Many efforts were paid to bal-
ance the performance on IV and out-of-
vocabulary (OOV) words by combining 
these two methods according to this be-
lief. In this paper, we provide a more de-
tailed evaluation metric of IV and OOV 
words than Bakeoff to analyze CT me-
thod and combination method, which is 
a typical way to seek such balance. Our 
evaluation metric shows that CT outper-
forms dictionary-based (or so called 
word-based in general) segmentation on 
both IV and OOV words within Bakeoff 
                                               
* The work is done when the first author is working 
in MSRA as an intern. 
closed tests. Furthermore, our analysis 
shows that using confidence measure to 
combine the two segmentation results 
should be under certain limitation. 
1 Introduction 
Chinese Word Segmentation (CWS) has been 
witnessed a prominent progress in the last three 
Bakeoffs (Sproat and Emerson, 2003), (Emer-
son, 2005), (Levow, 2006). One of the reasons 
for this progress is that Bakeoff provides stan-
dard corpora and objective metric, which makes 
the result of each system comparable. Through 
those evaluations researchers can recognize the 
advantage and disadvantage of their methods 
and improve their systems accordingly. Howev-
er, in the evaluation metric of Bakeoff, only the 
overall F measure, precision, recall, IV (in-
vocabulary) recall and OOV (out-of-vocabulary) 
recall are included and such a metric is not suffi-
cient to give a completely measure on the per-
formance, especially when the performance on 
IV and OOV word segmentation need to be eva-
luated. An important issue is that segmentation 
based on which, word or character, can yield the 
better performance on IV words. We give a de-
tailed explanation about this issue as following. 
      Since CWS was firstly treated as a character-
based tagging task (we call it ?CT? for short he-
reafter) in (Xue and Converse, 2002), this me-
thod has been widely accepted and further de-
veloped by researchers (Peng et al, 2004), 
(Tseng et al, 2005), (Low et al, 2005), (Zhao et 
al., 2006). Relatively to dictionary-based 
61
Sixth SIGHAN Workshop on Chinese Language Processing
segmentation (we call it ?DS? for short hereaf-
ter), CT method can achieve a higher accuracy 
on OOV word recognition and a better perfor-
mance of segmentation in whole. Thus, CT has 
drawn more and more attention and became the 
dominant method in the Bakeoff 2005 and 2006.    
  Although CT has shown its merits in word 
segmentation task, some researchers still hold 
the belief that on IV words DS can perform bet-
ter than CT even in the restriction of Bakeoff 
closed test. Consequently, many strategies are 
proposed to balance the IV and OOV perfor-
mance (Goh et al, 2005), (Zhang et al, 2006a). 
Among these strategies, the confidence measure 
used to combine the results of CT and DS is a 
straight-forward one, which is introduced in 
(Zhang et al, 2006a). The basic assumption of 
such combination is that DS method performs 
better on IV words and Zhang derives this belief 
from the fact that DS achieves higher IV recall 
rate as Table 1 shows. In which AS, CityU, 
MSRA and PKU are four corpora used in Ba-
keoff 2005 (also see Table 2 for detail). We pro-
vide a more detailed evaluation metric to ana-
lyze these two methods, including precision and 
F measure of IV and OOV respectively and our 
experiments show that CT outperforms DS on 
both IV and OOV words within Bakeoff closed 
test. The precision and F measure are existing 
metrics and the definitions of them are clear. 
Here we just employ them to evaluate segmenta-
tion results.   Furthermore, our error analysis on 
the results of combination reveals that confi-
dence measure in (Zhang et al, 2006a) has a 
representation flaw and we propose an EIV tag 
method to revise it. Finally, we give an empiri-
cal comparison between existing pure CT me-
thod and combination, which shows that pure 
CT method can produce state-of-the-art results 
on both IV word and overall segmentation.    
Corpus 
RIV ROOV 
DS CT DS CT 
AS 0.982 0.967 0.038 0.647 
CityU 0.989 0.967 0.164 0.736 
MSRA 0.993 0.972 0.048 0.716 
PKU 0.981 0.955 0.408 0.754 
Table 1 IV and OOV recall in  
(Zhang et al,   2006a) 
       The rest of this paper is organized as fol-
lows. In Section 2, we give a brief introduction 
to Zhang?s DS method and subword-based tag-
ging, which is a special CT method. And by 
comparing the results of this special CT method 
and DS according our detailed metric, we show 
that CT performs better on both IV and OOV. 
We review in Section 3 how confidence measure 
works and indicate its representation flaw. Fur-
thermore, an ?EIV? tag method is proposed to 
revise the confidence measure. In Section 4, the 
experimental results of existing pure CT method 
are demonstrated to compare with combination 
result, based on which we discuss the related 
work. In Section 5, we conclude the contribu-
tions of this paper and discuss the future work. 
2 Comparison between DS and CT 
Based on Detailed Metric 
We proposed a detailed evaluation metric for IV 
and OOV word identification in this section and 
experiments based on the new metric show that 
CT outperforms DS not only on OOV words but 
also on IV words with F-measure of IV.  All the 
experiments in this paper conform to the 
constraints of closed test in Bakeoff 2005 
(Emerson, 2005). It means that any resource 
beyond the training corpus is excluded. We first 
review how DS and CT work and then present 
our evaluation metric and experiment results. 
There is one thing should be emphasized, by 
comparing DS and CT result we just want to 
verify that our new metric can show the 
performance on IV words more objectively. 
Since either DS or CT implementation has 
specific setting here we should not extend the 
comparison result to a general sense between 
those generative models and discriminative 
models. 
2.1 Dictionary-based segmentation 
For the dictionary-based word segmentation, we 
collect a dictionary from training corpus first. 
Instead of Maximum Match, trigram language 
model2 trained on training corpus is employed 
for disambiguation. During the disambiguation 
procedure, a beam search decoder is used to seek 
the most possible segmentation. Since the setting 
in our paper is consistent with the closed test of 
                                               
2 Language model used in this paper is SLRIM from 
http://www.speech.sri.com/projects/srilm/ 
62
Sixth SIGHAN Workshop on Chinese Language Processing
Bakeoff, we can only use the information we 
learn from training corpus though other open 
resources may be helpful to improve the perfor-
mance further. For detail, the decoder reads cha-
racters from the input sentence one at a time, 
and generates candidate segmentations incre-
mentally. At each stage, the next incoming cha-
racter is combined with an existing candidate in 
two different ways to generate new candidates: it 
is either appended to the last word in the candi-
date, or taken as the start of a new word. This 
method guarantees exhaustive generation of 
possible segmentations for any input sentence. 
However, the exponential time and space of the 
length of the input sentence are needed for such 
a search and it is always intractable in practice. 
Thus, we use the trigram language model to se-
lect top B (B is a constant predefined before 
search and in our experiment 3 is used) best 
candidates with highest probability at each stage 
so that the search algorithm can work in practice. 
Finally, when the whole sentence has been read, 
the best candidate with the highest probability 
will be selected as the segmentation result.  
Here, the term ?dictionary-based? is exactly the 
method implemented in (Zhang et al, 2006a), it 
does not mean the generative language model in 
general.  
2.2 Character-based tagging  
Under CT scheme, each character in one sen-
tence is labeled as ?B? if it is the beginning of a 
word, ?O? tag means the current character is a 
single-character word, other character is labeled 
as ?I?. For example, ???? (whole China)? is 
labeled as ??  (whole)/O ?  (central)/B ? 
(country)/I?. 
     In (Zhang et al, 2006a), the above CT me-
thod is developed as subword-based tagging. 
First, the most frequent multi-character words 
and all single characters in training corpus are 
collected as subwords. During the subword-
based tagging, a subword is viewed as an unit 
instead of several separate characters and given 
only one tag. For example, in subword-based 
tagging, ???? (whole China)? is labeled as ?
? (whole)/O ?? (China)/O?, if the word ??
? (China)? is collected as a subword. As the 
preprocessing, both training and test corpora are 
segmented by maximum match with subword set 
as dictionary. After this preprocessing, every 
sentence in both training and test corpora be-
comes subword sequence. Finally, the tagger is 
trained by CRFs approach3 on the training data. 
Although word information is integrated into 
this method, it still works in the scheme of 
?IOB? tagging. Thus, we still call subword-
based tagging as a special CT method and in the 
reminder of this paper ?CT? means subword-
based tagging in Zhang?s paper and ?Pure CT? 
means CT without subword. 
2.3 A detailed evaluation metric 
In this paper, data provided by Bakeoff 2005 is 
used in our experiments in order to compare 
with the published results in (Zhang et al, 
2006a).   The statistics of the corpora for Ba-
keoff 2005 are listed in Table 2 (Emerson, 2005). 
Corpus Encoding 
#Training 
words 
#Test 
words 
OOV 
rate 
AS Big5 5.45M 122K 0.043 
CityU Big5 1.46M 41K 0.074 
MSRA GB 2.37M 107K 0.026 
PKU GB 1.1M 104K 0.058 
Table 2 Corpora statistics of Bakeoff 2005 
      Evaluation standard is also provided by Ba-
keoff, including overall precision, recall, F 
measure, IV recall and OOV recall (Sproat and 
Emerson, 2003), (Emerson, 2005). However, 
some important metrics, such as F measure and 
precision of both IV and OOV words are omit-
ted, which are necessary when the performance 
of IV or OOV word identification need to be 
judged. Thus, in order to judge the results of 
each experiment, a more detailed evaluation 
with precision and F measure of both IV and 
OOV words included is used. To calculate the 
IV and OOV precision and recall, we firstly di-
vide words of the segmenter?s output and gold 
data into IV word and OOV word sets respec-
tively with the dictionary collected from the 
training corpus. Then, for IV and OOV word 
sets respectively, the IV (or OOV) recall is the 
proportion of the correctly segmented IV (or 
OOV) word tokens to all IV (or OOV) word to-
kens in the gold data, and IV (or OOV) precision 
is the proportion of the correctly segmented IV 
                                               
3 CRF tagger in this paper  is implemented by CRF++   
downloaded from http://crfpp.sourceforge.net/ 
63
Sixth SIGHAN Workshop on Chinese Language Processing
(or OOV) word tokens to all IV (or OOV) word 
tokens in the segmenter?s output. One thing have 
to be emphasized is that the single character in 
test corpus will be defined as OOV if it does not 
appear in training corpus. We will see later in 
this section, by this evaluation, some facts cov-
ered by the bakeoff evaluation can be illustrated 
by our new evaluation metric.  
     Here, we repeat two experiments described in 
(Zhang et al, 2006a), namely dictionary-based 
approach and subword-based tagging. For CT 
method, top 2000 most frequent multi-character 
words and all single characters in training corpus 
are selected as subwords and the feature tem-
plates used for CRF model is listed in Table 3.  
We present all the segmentation results in Table 
6 to see the strength and weakness of each me-
thod conveniently.     
Based on IV and OOV recall as we show in 
Table 1, Zhang argues that the DS performs bet-
ter on IV word identification while CT performs 
better on OOV words. But we can see from the 
results in Table 6 (the lines about DS and CT), 
the IV precision of DS approach is much lower 
than that of CT on all the four corpora, which 
also causes a lower F measure of IV. The reason 
for low IV precision of DS is that many OOV 
words are segmented into two IV words by DS. 
For example, OOV word ????(choral)? is 
segmented into???(sing) ?(class)? by DS. 
These wrongly identified IV words increase the 
number of all IV words in the segmenter?s out-
put and cause the low IV precision of the DS 
result. Since the F measure of IV is a more rea-
sonable metric of performance of IV than IV 
recall only, Table 6 shows that CT method out-
performs the DS on IV word segmentation over 
all four corpora. The comparison also shows that 
CT outperforms the DS on OOV and overall 
segmentation as well. 
Type Feature Function 
Unigram C-2, C-1, C0, C1, C2 Previous two, current and next two subword 
Bigram C-2 C-1, C-1 C0, C0 C1, C1 C2 Two adjacent subwords  
Jump C-1 C1 Previous character and next subwords 
  Table 3 Feature templates used for CRF in our experiments
3 Balance between IV and OOV Per-
formance 
There are other strategies such as (Goh et al, 
2005) trying to seek balance between IV and 
OOV performance. In (Goh et al 2005), infor-
mation in a dictionary is used in a statistical 
model. In this way, the dictionary-based ap-
proach and the statistical model are combined. 
We choose the confidence measure to study be-
cause it is straight-forward. We show in this sec-
tion that there is a representation flaw in the 
formula of confidence measure in (Zhang et al, 
2006a). And we propose an ?EIV? tag method to 
solve this problem. Our experiments show that 
confidence measure with EIV tag outperforms 
CT and DS alone. 
3.1 Confidence measure 
Confidence Measure (CM) means to seek an 
optimal tradeoff between performance on IV and 
OOV words. The basic idea of CM comes from 
the belief that CT performs better on OOV 
words while DS performs better on IV words. 
When both results of CT and DS are available, 
the CM can be calculated according to the fol-
lowing formula in (Zhang et al, 2006a): 
ngiobwiobiobiob ttwtCMwt ),()1()|()|(CM ??? ???  
Here, w  is a subword, iobt  is ?IOB? tag given 
by CT and 
wt  is ?IOB? tag generated by DS. In 
the first term of the right hand side of the formu-
la, )|( wtCM iobiob  is the marginal probability of 
iobt (we call this marginal probability ?MP? for 
short). And in the second term, 
ngiobw tt ),(?  is a 
Kronecker delta function, returning 1 if and only 
if 
wt  and  iobt  are identical, else returning 0. But 
if 1),( ?ngiobw tt?  , there is no requirement of re-
placement at all. While if 0),( ?ngiobw tt?  , when 
iobw tt ? , CM depends on the first term of its 
right hand side only and ? is unnecessary to be 
set as a weight. Finally, ?  in the formula is a 
weight to seek balance between CT tag and DS 
tag. Another parameter here is a threshold t  for 
the CM. If CM is less than t , wt  replaces iobt as 
64
Sixth SIGHAN Workshop on Chinese Language Processing
the final tag, otherwise 
iobt will be remained as 
the final tag. However, two parameters in the 
CM, namely ?  and t , are unnecessary, because 
when MP is greater than or equal to ?/t , iobt  
will be kept, otherwise it will be replaced with 
wt .  Thus, the CM ultimately is the marginal 
probability of the ?IOB? tag (MP). In the expe-
riment of this paper, MP is used as CM because 
it is equivalent to Zhang?s CM but more conve-
nient to express. 
3.2 Experiments and error analysis about 
combination 
We repeat the experiments about CM in Zhang?s 
paper (Zhang et al, 2006a) and show that there 
is a representation flaw in the CM formula. Fur-
thermore, we propose an EIV tag method to 
make CM yield a better result. 
     In this paper, ? = 0.8 and t = 0.7 (Parameters 
in two papers, Zhang et al 2006a and Zhang et 
al. 2006b, are different. And our parameters are 
consistent with Zhang et al 2006b which is con-
firmed by Dr Zhang through email) are used in 
CM, namely MP= 0.875 is the threshold. Here, 
in Table 4, we provide some statistics on the 
results of CT when MP is less than 0.875. From 
Table 4 we can see that even with MP less than 
0.875, most of the subwords are still tagged cor-
rectly by CT and should not be revised by DS 
result. Besides, lots of the subwords with low 
MP contained by OOV words in test data, espe-
cially for the corpus whose OOV rate is high 
(i.e. on CityU corpus more than one third sub-
words with low MP belong to OOV word) and 
performance on OOV recognition is the advan-
tage of CT rather than that of DS approach. Thus 
when combining the results of the two methods, 
it is the 
iobt should be maintained if the subword 
is contained by an OOV word. Therefore, the 
CM formula seems somewhat unreasonable.  
      The error analysis about how many original 
errors are eliminated and how many new errors 
are introduced by CM is provided in Table 5 (the 
columns about CM). Table 5 illustrates that, af-
ter combining the two results, most original er-
rors on IV words are corrected because DS can 
achieve higher IV recall as described in Zhang?s 
paper. But on OOV part, more new errors are 
introduced by CM and these new errors decrease 
the precision of the IV words. For example, the 
OOV words ????? (guard member)? and ?
??? (design fee)? is recognized correctly by 
CT but with low CM. In the combining proce-
dure, these words are wrongly split as IV errors: 
??? (guard) ?? (member)? and ??? (de-
sign) ?  (fee)?.  Thus, for two corpora (i.e. 
CityU and AS), F measure of IV and overall F 
measure decreases since there are more new er-
rors introduced than original ones eliminated 
and only on the other two corpora (MSRA and 
PKU), overall F measure of combination method 
is higher than CT alone, which is shown in Ta-
ble 6 by the lines about combination. 
3.3 EIV tag method 
Since combining the two results by CM may 
produce an even worse performance in some 
case, it is worthy to study how to use this CM to 
get an enhanced result. Intuitively, if we can 
change only the CT tags of the subwords which 
contained in IV word while keep the CT tags of 
those contained in OOV words unchanged, we 
will improve the final result according to our 
error analysis in Table 5. Unfortunately, only 
from the test data, we can get the information 
whether a subword contained in an IV word, just 
as what we do to get Table 4. However, we can 
get an approximate estimation from DS result. 
When using subwords to re-segment DS result4, 
all the fractions re-segmented out of multiple-
character words, including both multiple-
character words and single characters, will be 
given an ?EIV? tag, which means that the cur-
rent multiple-character word or single character 
is contained in an IV word with high probability. 
For example, ????? (human resource)? in 
DS result is a whole word. However, only ??? 
(resource)? belongs to the subword set, so dur-
ing the re-segmentation ????? (human re-
source)? will be re-segmented as ?? (people) ? 
(force) ?? (resource)?. All these three frac-
tions will be labeled with an ?EIV? tag respec-
tively. It is reasonable because all the multiple-
character words in the DS result can match an 
IV word. After this procedure, when combining 
                                               
4 For the detail, please refer to (Zhang et al, 2006a). 
65
Sixth SIGHAN Workshop on Chinese Language Processing
Corpus AS CityU MSRA PKU 
# subword tokens belong to IV 10010 4404 9552 9619 
# subword tokens belong to IV and tagged correctly by CT 7452 3434 7452 7213 
# subword tokens belong to IV and tagged wrongly by CT 2558 970 2100 2406 
# subword tokens belong to OOV  5924 2524 2685 3580 
# subword tokens belong to OOV and tagged correctly by CT 3177 1656 1725 2208 
# subword tokens belong to OOV and tagged wrongly by CT 2747 868 960 1372 
Table 4 Results of CT when MP is less than 0.875  
Corpus AS CityU MSRA PKU 
Method CM EIV CM EIV CM EIV CM EIV 
#original errors eliminated on IV  1905 1003 904 469 1959 1077 1923 1187 
#original errors eliminated on OOV 755 75 155 80 104 30 230 76 
#original errors eliminated totally 2660 1078 1059 549 2063 1107 2153 1263 
#new errors introduced on IV 441 185 80 50 148 68 211 118 
#new errors introduced on OOV 2487 77 1320 103 1517 57 1681 58 
# new errors introduced totally 2928 262 1400 153 1665 125 1892 176 
Table 5 Error analysis of confidence measure with and without EIV tag 
the two results, only the CT tag with EIV tags 
and low MP will be replaced by DS tag, other-
wise the original CT tag will be maintained. Un-
der this condition the errors introduced by OOV 
will not happen and enhanced results are listed 
in Table 6 lines about EIV. We can see that on 
all four corpora the overall F measure of EIV 
result is higher than that of CT alone, which 
show that our EIV method works well. Now, 
let?s check what changes happened in the num-
ber of error tags after EIV condition added into 
the CM. We can see from the Table 5 columns 
about EIV, there are more errors eliminated than 
the new errors introduced after EIV condition 
added into CM and most CT tags of subwords 
contained in OOV words maintained unchanged 
as we supposed. And then, our results (in Table 
6 lines about EIV) are comparable with that in 
Zhang?s paper. Thus, there may be some similar 
strategies in Zhang?s CM too but not presented 
in Zhang?s paper. 
4  Discussion and Related Works 
Although the method such as confidence meas-
ure can be helpful at some circumstance, our 
experiment shows that pure character-based tag-
ging (pure CT) can work well with reasonable 
features and tag set. In (Zhao et al, 2006), an 
enhanced CRF tag set is proposed to distinguish 
different positions in the multi-character words 
when the word length is less than 6. In this me-
thod, feature templates are almost the same as 
shown in Table 3 with a 3-character window and 
a 6-tag set {B, B2, B3, M, E, O} is used. Here, 
tag B and E stand for the first and the last posi-
tion in a multi-character word, respectively. S 
stands up a single-character word. B2 and B3 
stand for the second and the third position in a 
multi-character word, whose length is larger 
than two-character or three-character. M stands 
for the fourth or more rear position in a multi-
character word, whose length is larger than four-
character. 
     In Table 6, the lines about ?pure CT? provide 
the results generated by pure CT with 6-tag set. 
We can see from the Table 6 this pure CT ap-
proach achieves the state-of-the-art results on all 
the corpora. On three of the four corpora (AS, 
MSRA and PKU) this pure CT method gets the 
best result. Even on IV word, this pure CT ap-
proach outperforms Zhang?s CT method and 
produces comparable results with combination 
with EIV tags, which shows that pure CT me-
thod can perform well on IV words too. Moreo-
ver, this character-based tagging approach is 
more clear and simple than the confidence 
measure method.  
Although character-based tagging became 
mainstream approach in the last two Bakeoffs, it 
does not mean that word information is valueless 
in Chinese word segmentation.  A word-based 
perceptron algorithm is proposed recently 
(Zhang and Clark, 2007), which views Chinese 
word segmentation task from a new angle in-
stead of character-based tagging and gets com-
parable results with the best results of Bakeoff. 
66
Sixth SIGHAN Workshop on Chinese Language Processing
 Corpus Method R P F RIV PIV FIV ROOV POOV FOOV 
AS DS 0.943 0.881 0.911 0.984 0.892 0.935 0.044 0.217 0.076 
CT 0.954 0.938 0.946 0.967 0.960 0.964 0.666 0.606 0.635 
Combination 0.958 0.929 0.943 0.980 0.945 0.962 0.487 0.593 0.535 
EIV tag 0.960 0.942 0.951 0.973 0.962 0.968 0.667 0.624 0.645 
Pure CT 0.958 0.947 0.953 0.971 0.963 0.967 0.682 0.618 0.648 
CityU DS 0.928  0.848 0.886 0.989 0.865 0.923 0.162 0.353 0.223 
CT 0.947 0.940 0.944 0.963 0.964 0.964 0.739 0.717 0.728 
Combination 0.954 0.922 0.938 0.984 0.938 0.961 0.581 0.693 0.632 
EIV tag 0.953 0.949 0.951 0.970 0.968 0.969 0.744 0.750 0.747 
Pure CT 0.947 0.948 0.948 0.967 0.973 0.970 0.692 0.660 0.676 
MSRA DS 0.969 0.927 0.947 0.994 0.930 0.961 0.036 0.358 0.066 
CT 0.963 0.964 0.963 0.970 0.979 0.975 0.698 0.662 0.680 
Combination 0.977 0.961 0.969 0.990 0.970 0.980 0.511 0.653 0.574 
EIV tag 0.972 0.970 0.971 0.980 0.982 0.981 0.696 0.679 0.688 
Pure CT 0.972  0.975 0.973 0.978 0.986 0.982 0.750 0.632 0.686 
PKU DS 0.948 0.911 0.929 0.981 0.920 0.950 0.403 0.711 0.515 
CT 0.944 0.945 0.945 0.955 0.966 0.961 0.763 0.727 0.745 
Combination 0.955 0.942 0.949 0.973 0.953 0.963 0.664 0.782 0.718 
EIV tag 0.950 0.952 0.951 0.961 0.970 0.966 0.768 0.753 0.760 
Pure CT 0.946 0.957 0.951 0.956 0.973 0.964 0.672 0.580 0.623 
           Table 6 Results of different approach used in our experiments (White background lines are 
           the results we repeat Zhang?s methods and they have some trivial difference with Table 1.) 
Therefore, the most important thing worth to pay 
attention in future study is how to integrate lin-
guistic information into the statistical model effec-
tively, no matter character or word information. 
5 Conclusions and Future Work 
In this paper, we first provided a detailed evalua-
tion metric, which provides the necessary infor-
mation to judge the performance of each method 
on IV and OOV word identification. Second, by 
this evaluation metric, we show that character-
based tagging outperforms dictionary-based seg-
mentation not only on OOV words but also on IV 
words within Bakeoff closed tests. Furthermore, 
our experiments show that confidence measure in 
Zhang?s paper has a representation flaw and we 
propose an EIV tag method to revise the combina-
tion. Finally, our experiments show that pure cha-
racter-based approach also can achieve good IV 
word and overall performance. Perhaps, there are 
two reasons that existing combination results 
don?t outperform the pure CT. One is that most 
information contained in statistic language model 
is already captured by the CT feature templates in 
CRF framework. The other is that confidence 
measure may not be the effective way to combine 
the DS and CT results.  
 In the future work, our research will focus on 
how to integrate word information into CRF fea-
tures rather than using it to modify the results of 
CRF tagging. In this way, we can capture the 
word information meanwhile avoid destroying the 
optimal output of CRF tagging. 
Acknowledgement 
The authors appreciate Dr. Hai Zhao in City Uni-
versity of Hong Kong and Dr. Ruiqiang Zhang in 
Spoken Language Communications Lab, ATR, 
Japan providing a lot of help for this paper. Thank 
those reviewers who gave the valuable comment 
to improve this paper. 
References 
Thomas Emerson. 2005. The Second International Chi-
nese Word Segmentation Bakeoff. In Proceedings of the 
Fourth SIGHAN Workshop on Chinese Language 
Processing, pages 123-133, Jeju Island, Korea:  
Chooi-Ling Goh, Masayuku Asahara and Yuji Matsu-
moto. 2005. Chinese Word Segmentatin by Classifi-
cation of Characters. Computational Linguistics and 
67
Sixth SIGHAN Workshop on Chinese Language Processing
Chinese Language Processing, Vol. 10(3): pages 
381-396. 
Gina-Anne Levow. 2006. The Third International Chi-
nese Language Processing Bakeoff: Word Segmen-
tation and Named Entity Recognition. In  Proceed-
ings of the Fifth SIGHAN Workshop on Chinese 
Language Processing , pages 108-117, Sydney: Ju-
ly. 
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. 
A Maximum Entropy Approach to Chinese Word 
Segmentation. In Proceedings of the Fourth SIGHAN 
Workshop on Chinese Language Processing, pages 161-
164, Jeju Island, Korea. 
Fuchun Peng, Fangfang Feng, and Andrew McCallum. 
2004. Chinese segmentation and new word detection 
using conditional random fields. In COLING 2004, 
pages 562?568. Geneva, Switzerland. 
Richard Sproat and Thomas Emerson. 2003. The First 
International Chinese Word Segmentation Bakeoff. In 
Proceedings of the Second SIGHAN Workshop on Chi-
nese Language Processing, pages 133-143, Sapporo, Ja-
pan: July 11-12,  
Huihsin Tseng, Pichuan Chang et al 2005. A Conditional 
Random Field Word Segmenter for SIGHAN Bakeoff 
2005. In Proceedings of the Fourth SIGHAN Workshop 
on Chinese Language Processing, pages 168-171, Jeju 
Island, Korea. 
Neinwen Xue and Susan P. Converse. 2002. Combin-
ing Classifiers for Chinese Word Segmentation. In 
Proceedings of the First SIGHAN Workshop on 
Chinese Language Processing, pages 63-70, Taipei, 
Taiwan.  
Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita. 
2006a. Subword-based Tagging by Conditional 
Random Fields for Chinese Word Segmentation. In 
Proceedings of the Human Language Technology 
Conference of the NAACL, Companion volume, pag-
es 193-196. New York, USA. 
Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita. 
2006b. Subword-based Tagging for Confidence-
dependent Chinese Word Segmentaion. In Proceed-
ings of the COLING/ACL, Main Conference Poster 
Sessions, pages 961-968. Sydney, Australia. 
Yue Zhang and Stephen Clark. 2007. Chinese Segmenta-
tion with a Word-Based Perceptron Algorithm. In 
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, pages 840-
847. Prague, Czech Republic.  
Hai Zhao, Changning Huang et al 2006. Effective Tag 
Set Selection in Chinese Word Segmentation via 
Conditional Random Field Modeling. In Proceed-
ings of PACLIC-20. pages 87-94. Wuhan, China, 
Novemeber. 
 
68
Sixth SIGHAN Workshop on Chinese Language Processing
The Character-based CRF Segmenter of MSRA&NEU 
 for the 4th Bakeoff 
Zhenxing Wang1,2, Changning Huang2 and Jingbo Zhu1 
1 Institute of Computer Software and Theory, Northeastern University,  
Shenyang, China, 110004 
2 Microsoft Research Asia, 49, Zhichun Road,  
Haidian District, Beijing, China, 100080 
zxwang@ics.neu.edu.cn 
v-cnh@microsoft.com 
zhujingbo@mail.neu.edu.cn 
 
Abstract 
This paper describes the Chinese Word 
Segmenter for the fourth International 
Chinese Language Processing Bakeoff. 
Base on Conditional Random Field (CRF) 
model, a basic segmenter is designed as a 
problem of character-based tagging.  To 
further improve the performance of our 
segmenter, we employ a word-based ap-
proach to increase the in-vocabulary (IV) 
word recall and a post-processing to in-
crease the out-of-vocabulary (OOV) word 
recall. We participate in the word segmen-
tation closed test on all five corpora and 
our system achieved four second best and 
one the fifth in all the five corpora.  
1 Introduction 
Since Chinese Word Segmentation was firstly 
treated as a character-based tagging task in (Xue 
and Converse, 2002), this method has been widely 
accepted and further developed by researchers 
(Peng et al, 2004), (Tseng et al, 2005), (Low et 
al., 2005), (Zhao et al, 2006). Thus, as a powerful 
sequence tagging model, CRF became the domi-
nant method in the Bakeoff 2006 (Levow, 2006).  
      In this paper, we improve basic segmenter un-
der the CRF work frame in two aspects, namely 
IV and OOV identification respectively. We use 
the result from word-based segmentation to revise 
the CRF output so that we gain a higher IV word 
recall. For the OOV part a post-processing rule is 
proposed to find those OOV words which are 
wrongly segmented into several fractions. Our 
system performs well in the Fourth Bakeoff, 
achieving four second best and on the fifth in all 
the five corpora. In the following of this paper, we 
describe our method in more detail.    
      The rest of this paper is organized as follows. 
In Section 2, we first give a brief review to the 
basic CRF tagging approach and then we propose 
our methods to improve IV and OOV performance 
respectively. In Section 3 we give the experiment 
results on the fourth Bakeoff corpora to show that 
our method is effective to improve the perfor-
mance of the segmenter. In Section 4, we con-
clude our work. 
2 Our Word Segmentation System 
In this section, we describe our system in more 
detail. Our system includes three modules: a basic 
CRF tagger, a word-base segmenter to improve 
the IV recall and a post-processing rule to 
improve the OOV recall. In the following of this 
section, we introduce these three modules 
respectively. 
2.1 Basic CRF tagger 
Sequence tagging approach treat Word Segmenta-
tion task as a labeling problem. Every character in 
input sentences will be given a label which indi-
cates whether this character is a word boundary. 
Our basic CRF1 tagger is almost the same as the 
system described in (Zhao et al, 2006) except we 
add a feature to incorporate word information, 
which is learned from training corpus.  
                                               
1 CRF tagger in this paper  is implemented by CRF++ 
which is downloaded from http://crfpp.sourceforge.net/ 
98
Sixth SIGHAN Workshop on Chinese Language Processing
Type Feature Function 
Unigram C-1, C0, C1 Previous, current and next character 
Bigram C-1 C0, C0 C1 Two adjacent character  
Jump C-1 C1 Previous character and next character 
Word Flag F0 F1 Whether adjacent characters form an IV word 
  Table 1 Feature templates used for CRF in our system 
 Under the CRF tagging scheme, each character 
in one sentence will be given a label by CRF 
model to indicate which position this character 
occupies in a word. In our system, CRF tag set is 
proposed to distinguish different positions in the 
multi-character words when the word length is 
less than 6, namely 6-tag set {B, B2, B3, M, E, 
O}. Here, Tag B and E stand for the first and the 
last position in a multi-character word, respective-
ly. S stands up a single-character word. B2 and B3 
stand for the second and the third position in a 
multi-character word, whose length is larger than 
two-character or three-character. M stands for the 
fourth or more rear position in a multi-character 
word, whose length is larger than four-character. 
      We add a new feature, which also used in 
maximum entropy model for word segmentation 
task by (Low et al, 2005), to the feature templates 
for CRF model while keep the other features same 
as (Zhao et al, 2006). The feature templates are 
defined in table 1. In the feature template, only the 
Word Flag feature needs an explanation. The bi-
nary function F0 = 1 if and only if C-1 C0  form a IV 
word, else F0 = 0 and F1 = 1 if and only if C0 C1 
form a IV word, else F1 = 0.   
2.2 Word based segmenter and revise rules 
For the word-based word segmentation, we collect 
dictionary from training corpus first. Instead of 
Maximum Match, trigram language model 2 
trained on training corpus is employed for disam-
biguation. During the disambiguation procedure, a 
beam search decoder is used to seek the most 
possible segmentation. For detail, the decoder 
reads characters from the input sentence one at a 
time, and generates candidate segmentations in-
crementally. At each stage, the next incoming cha-
racter is combined with an existing candidate in 
two different ways to generate new candidates: it 
is either appended to the last word in the candidate, 
or taken as the start of a new word. This method 
guarantees exhaustive generation of possible seg-
                                               
2 Language model used in this paper is SLRIM down-
loaded from http://www.speech.sri.com/projects/srilm/ 
mentations for any input sentence. However, the 
exponential time and space of the length of the 
input sentence are needed for such a search and it 
is always intractable in practice. Thus, we use the 
trigram language model to select top B (B is a 
constant predefined before search and in our expe-
riment 3 is used) best candidates with highest 
probability at each stage so that the search algo-
rithm can work in practice. Finally, when the 
whole sentence has been read, the best candidate 
with the highest probability will be selected as the 
segmentation result.  
      After we get word-based segmentation result, 
we use it to revise the CRF tagging result similar 
to (Zhang et al, 2006). Since word-based segmen-
tation result also corresponds to a tag sequence 
according to the 6-tag set, we now have two tags 
for each character, word-based tag (WT) and CRF 
tag (CT). Which tag will be kept as the final result 
depends on Marginal Probability (MP) of the CT. 
      Here, we give a short explanation about what 
is the MP of the CT. Suppose there is a sentence 
McccC ...10? , where ic  is the character this sen-
tence containing. CRF model gives this sentence a 
optimal tag sequence 
MtttT ...10? , where it is the 
tag for 
ic . If tti ? and },,,,,{ 32 SEMBBBt ? , 
the MP of 
it is defined as: 
?
? ???
T
ttT
i CTP
CTP
tt i )|(
)|(
)(MP ,
 
Here, )|( CTP is the conditional probability giv-
en by CRF model. For more detail about how to 
calculate this conditional probability, please refer 
to (Lafferty et al, 2001). 
      Assume that the tag assigned to the current 
character is CT by CRF and WT by word-based 
segmenter respectively. The rules under which we 
revise CRF result with word-based result is that if 
MP(CT) of a character is less than a predefined 
threshold and WT is not ?S?, the WT of this cha-
racter will be kept as the final result, else the CT 
of the character will be kept as the final result.  
99
Sixth SIGHAN Workshop on Chinese Language Processing
      The restriction that WT should not be ?S? is 
reasonable because word-based segmentation is 
incapable to recognize the OOV word and always 
segments OOV word into single characters. Be-
sides CRF model is better at dealing with OOV 
word than our word-based segmentation. When 
WT is ?S? it is possible that current word is an 
OOV word and segmented into single character 
wrongly by the word-based segmenter, so the CT 
of the character should be kept under such situa-
tion. For more detail about this analysis please 
refer to (Wang et al, 2008). 
2.3 Post-processing rule  
The rules we described in last subsection is help-
ful to improve the IV word recall and now we in-
troduce our post-processing rule to improve the 
OOV recall.  
      Our post-processing rule is designed to deal 
with one typical type of OOV errors, namely an 
OOV word wrongly segmented into several parts.  
In practice many OOV errors belong to such type. 
      The rule is quite simple. When we read a sen-
tence from the result we get by the last step, we 
also kept the last N sentences in memory, in our 
system we set N equals to 20. We do this because 
adjacent sentences are always relevant and some 
named entity likely occurs repeatedly in these sen-
tences. Then, we scan these sentences to find all 
n-grams (n from 2 to 7) and count their occur-
rence. If certain n-gram appears more than a thre-
shold and this n-gram never appears in training 
corpus, the n-gram will be selected as a word can-
didate. Then, we filter these word candidates ac-
cording to the context entropy (Luo and Song, 
2004). Assume w  is a word candidate appears 
n times in the current sentence and last N sen-
tences and },...,,{ 10 laaa?? is the set of left side 
characters of w . Left Context Entropy (LCE) can 
be defined as: 
?
?
?
?ia i
i waC
nwaCnwLCE ),(log),(
1)(
 
Here, ),( waC i is the count of concurrence of 
ia and w . For the Right Context Entropy, the de-
finition is the same except change left into right. 
Now, we define Context Entropy (CE) of a word 
candidate w as ))(),(min( wRCEwLCE . The 
word candidates with CE larger than a predefined 
threshold will be bind as a whole word in test cor-
pus no matter what tag sequence the segmenter 
giving it. If a shorter n-gram is contained in a 
longer n-gram and both of them satisfy the above 
condition, the shorter n-gram will be overlooked 
and the longer n-gram is bind as a whole word. 
3 Evaluation of Our System 
On the corpora of the Fourth Bakeoff, we evaluate 
our system.  We carry out our evaluation on the 
closed tracks. It means that we do not use any ad-
ditional knowledge beyond the training corpus. 
The thresholds set for MP and CE on each corpus 
are tuned on left-out data of training corpus by 
cross validation. To analyze our methods on IV 
and OOV words, we use a detailed evaluation me-
tric than Bakeoff 2006 (Levow, 2006) which in-
cludes Foov and Fiv. Our results are shown in Ta-
ble 2. In Table 2, the row ?Basic Model? means 
the results produced by our basic CRF tagger, the 
row ?+IV? means the results produced by the 
combination of CRF tagger and word-based seg-
menter and the row ?+IV+OOV? means the result 
we get by executing post-processing rule on the 
combination results. The F measure of the basic 
CRF tagger alone in the Table 2 is within the top 
three in the closed tests except Cityu. Performance 
on Cityu corpus is not so good because the incon-
sistencies existing in Cityu training and test corpo-
ra. In the training corpus the quotation marks are
??while in test corpus quotation marks are??, 
which never apper in the training corpus. As a 
reult, a lot of errors were caused by quotation 
marks. For example, the following four character
????were combined as a one word in our 
result and fragment????was tagged as two 
words?? and ??. Because CRF tagger never 
met ? and ? in training corpus so the tagger 
gave the most common tags, namely B and E to 
the quotation marks, which cause segmentation 
errors not only on quotation marks themselves but 
also on the characters adjacent to them. We 
remove these inconsistencies munually and got 
the F measure 0.5 percentage higer than the rusult 
in table 2. This result is within the top three in the 
closed tests. On all the five corpora, our ?+IV? 
module can increase the Fiv and our ?+OOV? 
module can increase Foov respectively. However, 
these improvements are not significant.  
100
Sixth SIGHAN Workshop on Chinese Language Processing
Corpus Method R P F ROOV POOV FOOV RIV PIV FIV 
CKIP 
Basic Model 0.946 0.923 0.940 0.651 0.719 0.683 0.969 0.948 0.958 
+ IV 0.949 0.935 0.942 0.647 0.741 0.691 0.973 0.948 0.960 
+ IV + OOV 0.950 0.936 0.943 0.656 0.748 0.699 0.973 0.949 0.961 
CityU 
Basic Model 0.944 0.934 0.939 0.654 0.721 0.686 0.970 0.951 0.960 
+ IV 0.946 0.936 0.941 0.655 0.738 0.694 0.972 0.951 0.962 
+ IV + OOV 0.949 0.937 0.943 0.678 0.759 0.716 0.973 0.951 0.962 
CTB 
Basic Model 0.953 0.951 0.952 0.703 0.727 0.715 0.967 0.964 0.965 
+ IV 0.954 0.952 0.953 0.697 0.747 0.721 0.969 0.963 0.966 
+ IV + OOV 0.954 0.953 0.953 0.703 0.749 0.725 0.969 0.964 0.966 
NCC 
Basic Model 0.940 0.928 0.934 0.438 0.580 0.499 0.965 0.940 0.952 
+ IV 0.944 0.930 0.936 0.434 0.603 0.504 0.969 0.941 0.955 
+ IV + OOV 0.945 0.932 0.939 0.450 0.620 0.522 0.970 0.943 0.956 
SXU 
Basic Model 0.960 0.953 0.956 0.636 0.674 0.654 0.977 0.967 0.972 
+ IV 0.962 0.955 0.958 0.637 0.696 0.665 0.980 0.967 0.973 
+ IV + OOV 0.962 0.955 0.959 0.645 0.702 0.673 0.979 0.968 0.974 
Table 2 performance each step of our system achieves 
4 Conclusions and Future Work 
In this paper, we propose a three-stage strategy in 
Chinese Word Segmentation. Based on the results 
produced by basic CRF, our word-based segmen-
tation module and post-processing module are 
designed to improve IV and OOV performance 
respectively. The results above show that our sys-
tem achieves the state-of-the-art performance. 
Since only the CRF tagger is good enough as we 
shown in our experiment, in the future work we 
will pay effort on the semi-supervised learning for 
CRF model in order to mining more useful infor-
mation from training and test corpus for CRF tag-
ger. 
References 
John Lafferty, Andrew McCallum, and Fernando Perei-
ra. 2001. Conditional random fields: probabilistic 
models for segmenting and labeling sequence data. 
In Proceedings of ICML-2001, pages 591?598. 
Gina-Anne Levow. 2006. The Third International Chi-
nese Language Processing Bakeoff: Word Segmen-
tation and Named Entity Recognition. In  Proceed-
ings of the Fifth SIGHAN Workshop on Chinese 
Language Processing , pages 108-117, Sydney: Ju-
ly. 
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. 
A Maximum Entropy Approach to Chinese Word 
Segmentation. In Proceedings of the Fourth SIGHAN 
Workshop on Chinese Language Processing, pages 161-
164, Jeju Island, Korea. 
Zhiyong Luo, Rou Song, 2004. ?An integrated method 
for Chinese unknown word extraction?, In Proceed-
ings of Third SIGHAN Workshop on Chinese Lan-
guage Processing, pages 148-154. Barcelona, Spain. 
Fuchun Peng, Fangfang Feng, and Andrew McCallum. 
2004. Chinese segmentation and new word detection 
using conditional random fields. In COLING 2004, 
pages 562?568. Geneva, Switzerland. 
Huihsin Tseng, Pichuan Chang et al 2005. A Conditional 
Random Field Word Segmenter for SIGHAN Bakeoff 
2005. In Proceedings of the Fourth SIGHAN Workshop 
on Chinese Language Processing, pages 168-171, Jeju 
Island, Korea. 
Zhenxing Wang, Changning Huang and Jingbo Zhu. 
2008. Which Performs Better on In-Vocabulary 
Word Segmentation: Based on Word or Character? 
In Proceeding of the Sixth Sighan Workshop on 
Chinese Language Processing. To be published.  
Neinwen Xue and Susan P. Converse. 2002. Combin-
ing Classifiers for Chinese Word Segmentation. In 
Proceedings of the First SIGHAN Workshop on 
Chinese Language Processing, pages 63-70, Taipei, 
Taiwan. 
Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita. 
2006. Subword-based Tagging by Conditional Ran-
dom Fields for Chinese Word Segmentation. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Companion volume, pages 
193-196. New York, USA. 
Hai Zhao, Changning Huang et al 2006. Effective Tag 
Set Selection in Chinese Word Segmentation via 
Conditional Random Field Modeling. In Proceed-
ings of PACLIC-20. pages 87-94. Wuhan, China, 
Novemeber. 
 
101
Sixth SIGHAN Workshop on Chinese Language Processing
A Knowledge-based Approach to Text Classification 
Zhu Jingbo Yao Tianshun 
Institute of Computer Software & Theory Institute of Computer Software & Theory 
Northeastern University, Shenyang Liaoning, 
P.R.China 110006 
Northeastern University, Shenyang Liaoning, 
P.R.China 110006 
zhujingbo@yahoo.com tsyao@china.com  
 
Abstract 
The paper presents a simple and effective 
knowledge-based approach for the task of text 
classification. The approach uses topic 
identification algorithm named FIFA to text 
classification. In this paper the basic process of 
text classification task and FIFA algorithm are 
described in detail. At last some results of 
experiment and evaluations are discussed. 
Keywords: FIFA algorithm, topic identification, 
text classification, natural language processing 
 
Introduction  
The text automatic classification method is based 
on the content analysis automatically to allocate 
the text into pre-determined catalogue. The 
methods of text automatic classification mainly 
use information retrieval techniques. Traditional 
information retrieval mainly retrieves relevant 
documents by using keyword-based or 
statistic-based techniques (Salton.G1989). 
Generally, three famous models are used: vector 
space model, Boolean model and probability 
model, based on the three models, some 
researchers brought forward extended models such 
as John M.Picrrc(2001), Thomas Bayer, Ingrid 
Renz,Michael Stein(1996), Antal van den Bosch, 
Walter Daelemans, Ton Weijters(1996), Manuel de 
Buenaga Rodriguez, Jose Maria Gomez-llidalgo, 
Belen Diaz-agudo(1997), Ellen Riloff and Wendy 
Lehnert(1994). 
One central step in automatic text classification 
is to identify the major topics of the texts. We 
present a simple and effective knowledge-based 
approach to text automatic classification. The 
approach uses topic identification algorithm 
named FIFA to text classification. In this paper the 
basic process of text classification task and FIFA 
algorithm are described in detail. At last some 
results of experiment and evaluations are 
discussed. 
 
1 Knowledge-based text classification 
The principal process for the Knowledge -based 
text classification is illustrated as following: 
 
Text set 
 
 
NULL?                    End 
 
 
Select a text for analysis 
 
 
Topic identification 
 
 
Output topic tagging of the text 
  
Figure 1 The principal process for the knowledge-based text classification 
 
From the figure 1 we can know that the crucial 
technique of the text classification is the topic 
identification parser. The topic tagging of the text 
is identified as its catalogue. 
Dictionary 
Rule base 
Topic feature aggregation
formula library 
N 
Y 
2 Automatic Topic Identification 
2.1 Feature Dictionary 
The feature dictionary is mainly used to store 
some terms that can illustrate the topic feature 
concept, and we call these terms as ?feature 
terms?. The data structure of the feature 
dictionary is consist of word, POS, semantic, 
location and field attribute. Some examples of 
feature dictionary are described as following: 
 
Feature terms Attributes 
??? 
 
HuiTong County 
S(),,?,(???),,??(?)??(*+)(??),Z?(?) 
S(),,LOCATION,LOCATION(HuitongCounty),,COUNTRY(China)PRO
VINCE (HuNan)CITY(HuaiHua),FIELD(geography) 
??  
Bank of China 
N(),,? ,X(?Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 217?220,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Designing Special Post-processing Rules for SVM-based
Chinese Word Segmentation
Muhua Zhu, Yilin Wang, Zhenxing Wang, Huizhen Wang, Jingbo Zhu
Natural Language Processing Lab
Northeastern University
No.3-11, Wenhua Road, Shenyang, Liaoning, China, 110004
{zhumh, wangyl, wangzx, wanghz}@ics.neu.edu.cn
zhujingbo@mail.neu.edu.cn
Abstract
We participated in the Third Interna-
tional Chinese Word Segmentation Bake-
off. Specifically, we evaluated our Chi-
nese word segmenter NEUCipSeg in
the close track, on all four corpora,
namely Academis Sinica (AS), City Uni-
versity of Hong Kong (CITYU), Mi-
crosoft Research (MSRA), and Univer-
sity of Pennsylvania/University of Col-
orado (UPENN). Based on Support Vec-
tor Machines (SVMs), a basic segmenter
is designed regarding Chinese word seg-
mentation as a problem of character-based
tagging. Moreover, we proposed post-
processing rules specially taking into ac-
count the properties of results brought
out by the basic segmenter. Our system
achieved good ranks in all four corpora.
1 SVM-based Chinese Word Segmenter
We built out segmentation system following (Xue
and Shen, 2003), regarding Chinese word segmen-
tation as a problem of character-based tagging.
Instead of Maximum Entropy, we utilized Sup-
port Vector Machines as an alternate. SVMs are
a state-of-the-art learning algorithm, owing their
success mainly to the ability in control of general-
ization error upper-bound, and the smooth integra-
tion with kernel methods. See details in (Vapnik,
1995). We adopted svm-light1 as the specific
implementation of the model.
1.1 Problem Formalization
By formalizing Chinese word segmentation into
the problem of character-based tagging, we as-
1http://svmlight.joachims.org/
signed each character to one and only one of the
four classes: word-prefix, word-suffix,
word-stem and single-character. For
example, given a two-word sequence????
??, the Chinese words for ?Southeast Asia(?
??) people(?) ?, the character ???is as-
signed to the category word-prefix, indicating
the beginning of a word;???is assigned to the
category word-stem, indicating the middle po-
sition of a word; ???belongs to the category
word-suffix, meaning the ending of a Chinese
word; and last,???is assigned to the category
single-character, indicating that the single
character itself is a word.
1.2 Feature Templates
We utilized four of the five basic feature templates
suggested in (Low et al , 2005), described as
follows:
? Cn(n = ?2,?1, 0, 1, 2)
? CnCn+ 1(n = ?2,?1, 0, 1)
? Pu(C0)
? T (C?2)T (C?1)T (C0)T (C1)T (C2)
where C refers to a Chinese character. The first
two templates specify a context window with the
size of five characters, where C0 stands for the
current character: the former describes individual
characters and the latter presents bigrams within
the context window. The third template checks
if current character is a punctuation or not, and
the last one encodes characters? type, including
four types: numbers, dates, English letters and
the type representing other characters. See de-
tail description and the example in (Low et al
, 2005). We dropped template C?1C1, since,
217
in experiments, it seemed not to perform well
when incorporated by SVMs. Slightly different
from (Low et al , 2005), character set repre-
senting dates are expanded to include ????
???????????????????,
the Chinese characters for ?day?, ?month?, ?year?,
?hour?,?minute?,?second?, respectively.
2 Post-processing Rules
Segmentation results of SVM-based segmenter
have their particular properties. In respect to the
properties of segmentation results produced by the
SVM-based segmenter, we extracted solely from
training data comprehensive and effective post-
processing rules, which are grouped into two cate-
gories: The rules, termed IV rules, make ef-
forts to fix segmentation errors of character se-
quences, which appear both in training and test-
ing data; Rules seek to recall some OOV(Out
Of Vocabulary) words, termed OOV rules. In
practice, we sampled out a subset from train-
ing dataset as a development set for the analysis
of segmentation results produced by SVM-based
segmenter. Note that, in the following, we defined
Vocabulary to be the collection of words ap-
pearing in training dataset and Segmentation
Unit to be any isolated character sequence as-
sumed to be a valid word by a segmenter. A
segmentation unit can be a correctly seg-
mented word or an incorrectly segmented charac-
ter sequence.
2.1 IV Rules
The following rules are named IV rules, pur-
suing the consistence between segmentation re-
sults and training data. The intuition underlying
the rules is that since training data give somewhat
specific descriptions for most of the words in it, a
character sequence in testing data should be seg-
mented in accordance with training data as much
as possible.
Ahead of post-processing, all words in the
training data are grouped into two distinct sets:
the uniquity set, which consists of words
with unique segmentation in training data and the
ambiguity set, which includes words having
more than one distinct segmentations in training
data. For example, the character sequence???
??has two kinds of segmentations, as?? ?
??(new century) and?????(as a compo-
nent of some Named-Entity, such as the name of a
restaurant).
? For each word in the uniquity set, check
whether it is wrongly segmented into more
than one segmentation units by the SVM-
based segmenter. If true, the continuous seg-
mentation units corresponding to the word
are grouped into the united one. The in-
tuition underlying this post-processing rule
is that SVM-based segmenter prefers two-
character words or single-character words
when confronting the case that the segmenter
has low self-confidence in some character-
sequence segmentation. For example, ??
???(duplicate) was segmented as ??
???and ????(unify) was split
into ?? ??. This phenomenon is
caused by the imbalanced data distribution.
Specifically, characters belonging to category
word-stem are much less than other three
categories.
? For each segmentation unit in the result
produced by SVM-based segmenter, check
whether the unit can be segmented into more
than one IV words and, meanwhile, the words
exist in a successive form for at least once in
training data . If true, replace the segmen-
tation unit with corresponding continuously
existing words. The intuition underlying this
rule is that SVM-based segmenter tends to
combine a word with some suffix, such as
???????, two Chinese characters
representing ?person?. For example, ??
? ??(Person in registration) tends to be
grouped as a single unit.
? For any sequence in the ambiguity set, such
as ?????, check if the correct seg-
mentation can be determined by the con-
text surrounding the sequence. Without los-
ing the generality, in the following explana-
tion, we assume each sequence in the am-
biguity set has two distinct segmentations.
we collected from training data the word
preceding a sequence where each existence
of the sequence has one of its segmenta-
tions, into a collection, named preceding
word set, and, correspondingly, the fol-
lowing word into another set, which is
termed following word set. Analog-
ically, we can produce preceding word
218
set and following word set for an-
other case of segmentation. When an am-
biguous sequence appears in testing data, the
surrounding context (in fact, just one preced-
ing word and a following word) is extracted.
If the context has overlapping with either of
the pre-extracted contexts of the same se-
quence which are from training data, the seg-
mentation corresponding to one of the con-
texts is retained.
? More over, we took a look into the annotation
errors existing in training data. We assume
there unavoidably exist some annotation mis-
takes. For example, in UPENN, the sequence
????(abbreviation for China and Amer-
ica) exists, for eighty-seven times, as a whole
word and only one time, exists as?? ??.
We regarded the segmentation?? ??as
an annotation error. Generally, when the ra-
tio of two kinds of segmentations is greater
than a pre-determined threshold (the value is
set seven in our system), the sequence is re-
moved from the ambiguity set and added as
a word of unique segmentation into the uniq-
uity set.
2.2 OOV Rules
The following rules are termed OOV rules,
since they are utilized to recall some of the
wrongly segmented OOV words. A OOV word
is frequently segmented into two continuous OOV
segmentation units. For example, the OOV
word?????(Vatican) was frequently seg-
mented as ??? ??, where both ??
??and ???are OOV character sequences.
Continuous OOVs present a strong clue of po-
tential segmentation errors. A rule is designed
to merge some of continuous OOVs into a cor-
rect segmentation unit. The designed rule is ap-
plicable to all four corpora. Moreover, since dis-
tinction between different segmentation standards
frequently leads to very different segmentation of
a same OOV words in different corpora, we de-
signed rules particularly for MSRA and UPENN
respectively, to recall more OOVs.
? For two continuous OOVs, check whether
at least one of them is a single-character
word. If true, group the continuous OOVs
into a segmentation unit. The reason for
the constraint of at least one of continuous
OOVs being single-character word is that not
all continuous OOVs should be combined,
for example, ??? ???, both ??
??(Germany merchant) and????(the
company name) are OOVs, but this sequence
is a valid segmentation unit. On the other
hand, we assume appropriately that most of
the cases for character being single-character
word have been covered by training data.
That is, once a single character is a OOV seg-
mentation unit, there exists a segmentation
error with high possibility.
? MSRA has very different segmentation stan-
dard from other three corpora, mainly be-
cause it requires to group several continuous
words together into a Name Entity. For ex-
ample, the word???????(the Min-
istry of Foreign Affairs of China) appear-
ing in MSRA is generally annotated into two
words in other corpora, as????(China)
and?????(the Ministry of Foreign Af-
fairs). In our system, we first gathered all
the words from the training data whose length
are greater than six Chinese characters, filter-
ing out dates and numbers, which was cov-
ered by Finite State Automation as
a pre-processing stage. For each words col-
lected, regard the first two and three charac-
ters as NE prefix, which indicates the be-
ginning of a Name Entity. The collection of
prefixes is termed Sp(refix). Analogously, the
collection Ss(uffix) of suffixes is brought up
in the same way. Obviously not all the pre-
fixes (suffixes) are good indicators for Name
Entities. Partly inheriting from (Brill, 1995),
we applied error-driven learning to filter pre-
fixes in Sp and suffixes in Ss. Specifically,
if a prefix and a suffix are both matched in
a sequence, all the characters between them,
together with the prefix and the suffix, are
merged into a single segmentation unit. The
resulted unit is compared with corresponding
sequence in training data. If they were not ex-
actly matched, the prefix and suffix were re-
moved from collections respectively. Finally
resulted Sp and Ss are utilized to recognize
Name Entities in the initial segmentation re-
sults.
? UPENN has different segmentation standard
from other three corpora in that, for some
219
Corpus R P F ROOV RIV
AS 0.949 0.940 0.944 0.694 0.960
MSRA 0.955 0.956 0.956 0.650 0.966
UPENN 0.940 0.914 0.927 0.634 0.969
CITYU 0.965 0.971 0.968 0.719 0.981
Table 1: Our official SIGHAN bakeoff results
Locations, such as ?????(Beijing
) and Organizations, such as ???
??(the Ministry of Foreign Affairs), the
last Chinese character presents a clue that
the character with high possibility is a suf-
fix of some words. In fact, SVM-based seg-
menter sometimes mistakenly split an OOV
word into a segmentation unit followed by a
suffix. Thus, when some suffixes exist as a
single-character segmentation unit, it should
be grouped with the preceding segmentation
unit. Undoubtedly not all suffixes are appro-
priate to this rule. To gather a clean collec-
tion of suffixes, we first clustered together the
words with the same suffix, filtering accord-
ing to the number of instances in each clus-
ter. Second, the same as above, error-driven
method is utilized to retain effective suffixes.
3 Evaluation Results
We evaluated the Chinese word segmentation
system in the close track, on all four cor-
pora, namely Academis Sinica (AS), City Uni-
versity of Hong Kong (CITYU), Microsoft Re-
search (MSRA), and University of Pennsylva-
nia/University of Colorado (UPENN). The results
are depicted in Table 1, where columns R,P and
F refer to Recall, Precision, F measure
respectively, and ROOV , RIV for the recall of out-
of-vocabulary words and in-vocabulary words.
In addition to final results reported in Bake-
off, we also conducted a series of experiments to
evaluate the contributions of IV rules and OOV
rules. The experimental results are showed in
Table 2, where V1, V2, V3 represent versions
of our segmenters, which compose differently of
components. In detail, V1 represents the basic
SVM-based segmenter; V2 represents the seg-
menter which applied IV rules following SVM-
based segmentation; V3 represents the segmenter
composing of all the components, that is, includ-
ing SVM-based segmenter, IV rules and OOV
rules. Since the OOV ratio is much lower than IV
correspondence, the improvement made by OOV
rules is not so dramatic as IV rules.
Corpus V1 v2 v3
AS 0.932 0.94 0.944
MSRA 0.939 0.954 0.956
UPENN 0.914 0.923 0.927
CITYU 0.955 0.966 0.968
Table 2: Word segmentation accuracy(F Measure)
resulted from post-processing rules
4 Conclusions and future work
We added post-processing rules to SVM-based
segmenter. By doing so, we our segmentation sys-
tem achieved comparable results in the close track,
on all four corpora. But on the other hand, post-
processing rules have the problems of confliction,
which limits the number of rules. We expect to
transform rules into features of SVM-based seg-
menter, thus incorporating information carried by
rules in a more elaborate manner.
Acknowledgements
This research was supported in part by the Na-
tional Natural Science Foundation of China(No.
60473140) and by Program for New Century Ex-
cellent Talents in University(No. NCET-05-0287).
References
Nianwen Xue and Libin Shen. 2003. Chinese Word
segmentation as LMR tagging. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing,pages 176-179.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Berlin: Springer-Verlag.
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. In Proceeding of the Fifth SIGHAN
Workshop on Chinese Language Processing, pages
161-164.
Eric.Brill. 1995. Transformation-based error-driven
learning and natural language processing:A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543-565.
220
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 143?151,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Chinese-English Organization Name Translation Based on Correla-
tive Expansion 
Feiliang Ren, Muhua Zhu,  Huizhen Wang,   Jingbo Zhu  
Natural Language Processing Lab, Northeastern University, Shenyang, China 
{renfeiliang,zhumuhua}@gmail.com 
{wanghuizhen,zhujingbo}@mail.neu.edu.cn 
Abstract 
This paper presents an approach to trans-
lating Chinese organization names into 
English based on correlative expansion. 
Firstly, some candidate translations are 
generated by using statistical translation 
method. And several correlative named 
entities for the input are retrieved from a 
correlative named entity list. Secondly, 
three kinds of expansion methods are 
used to generate some expanded queries. 
Finally, these queries are submitted to a 
search engine, and the refined translation 
results are mined and re-ranked by using 
the returned web pages. Experimental re-
sults show that this approach outperforms 
the compared system in overall transla-
tion accuracy.  
1 Introduction 
There are three main types of named entity: loca-
tion name, person name, and organization name. 
Organization name translation is a subtask of 
named entity translation. It is crucial for many 
NLP tasks, such as cross-language information 
retrieval, machine translation, question and an-
swering system. For organization name transla-
tion, there are two problems among it which are 
very difficult to handle.  
Problem I: There is no uniform rule that can 
be abided by to select proper translation methods 
for the inside words of an organization name. For 
example, a Chinese word ????, when it is used 
as a modifier for a university, it is translated to 
Northeastern for ?????/Northeastern Uni-
versity?, and is translated to Northeast for ???
????/Northeast Forestry University?, and is 
mapped to Chinese Pinyin Dongbei for ????
???/Dongbei University of Finance and Eco-
nomics?. It is difficult to decide which transla-
tion method should be chosen when we translate 
the inside words of an organization name.  
Problem II: There is no uniform rule that can 
be abided by to select proper translation order 
and proper treatment of particles Here particles 
refer to prepositions and articles) for an input 
organization name. For example, the organiza-
tion name ???????/China Construction 
Bank? and the organization name ??????
?/Agricultural Bank of China?, they are very 
similar both in surface forms and in syntax struc-
tures, but their translation orders are different, 
and their treatments of particles are also different. 
Generally, there are two strategies usually 
used for named entity translation in previous re-
search. One is alignment based approach, and the 
other is generation based approach. Alignment 
based approach (Chen et al 2003; Huang et al 
2003; Hassan and Sorensen, 2005; and so on) 
extracts named entities translation pairs from 
parallel or comparable corpus by some alignment 
technologies, and this approach is not suitable to 
solve the above two problems. Because new or-
ganization names are constantly being created, 
and alignment based method usually fails to 
cover these new organization names that don?t 
occur in the bilingual corpus.  
Traditional generation based approach (Al-
Onaizan and Knight, 2002; Jiang et al.2007; 
Yang et al 2008; and so on) usually consists of 
two parts. Firstly, it will generate some candidate 
translations for the input; then it will re-rank 
these candidate translations to assign the correct 
translations high ranks. Cheng and Zong [2008] 
proposed another generation based approach for 
organization name translation, which directly 
translates organization names according to their 
inherent structures. But their approach still can?t 
solve the above two problems. This is because 
the amount of organization names is so huge and 
many of them have their own special translation 
rules to handle the above two problems. And the 
inherent structures don?t reveal these translation 
rules. Traditional generation based approach is 
suitable for organization name translation. But in 
previous research, the final translation perform-
ance depends on the candidate translation gen-
143
eration process greatly. If this generation process 
failed, it is impossible to obtain correct result 
from the re-ranking process. In response to this, 
Huang et al [2005] proposed a novel method that 
mined key phrase translation form web by using 
topic-relevant hint words. But in their approach, 
they removed the candidate translation genera-
tion process, which will improve extra difficult 
during mining phrase. Besides, in their approach, 
the features considered to obtain topic-relevant 
words are not so comprehensive, which will af-
fect the quality of returned web pages where the 
correct translations are expected to be included. 
There is still much room for the improvement 
process of the topic-relevant words extraction.  
Inspired by the traditional generation based 
named entity translation strategy and the ap-
proach proposed by Huang et al, we propose an 
organization name translation approach that min-
ing the correct translations of input organization 
name from the web. Our aim is to solve the 
above two problems indirectly by retrieving the 
web pages that contain the correct translation of 
the input and mining the correct translation from 
them. Given an input organization name, firstly, 
some candidate translations are generated by us-
ing statistical translation method. And several 
correlative named entities for the input are re-
trieved from a correlative named entity list. Sec-
ondly, expanded queries are generated by using 
three kinds of query expansion methods. Thirdly, 
these queries are submitted to a search engine, 
and the final translation results are mined and re-
ranked by using the returned web pages.  
The rest of this paper is organized as follows, 
section 2 presents the extraction process of cor-
relative named entities, section 3 presents a detail 
description of our translation method for Chinese 
organization name, and section 4 introduces our 
parameter evaluation method, and section 5 is the 
experiments and discussions part, finally conclu-
sions and future work are given in section 6.  
2 Extraction of Correlative Named En-
tities 
The key of our approach is to find some web 
pages that contain the correct translation of the 
input. With the help of correlative named entities 
(here if two named entities are correlative, it 
means that they are usually used to describe the 
same topic), it is easier to find such web pages. 
This is because that in the web, one web page 
usually has one topic. Thus if two named entities 
are correlative, they are very likely to occur in 
pair in some web pages.  
The correlative named entity list is constructed 
in advance. During translation, the correlative 
named entities for the input organization name 
are retrieved from this list directly. To set up this 
correlative named entity list, an about 180GB-
sized collection of web pages are used. Totally 
there are about 100M web pages in this collec-
tion. Named entities are recognized from every 
web page by using a NER tool. This NER tool is 
trained by CRF model 1  with the corpus from 
SIGHAN-20082.  
2.1 Features Used 
During the extraction of correlative named enti-
ties, the following features are considered.  
Co-occurrence in a Document The more of-
ten two named entities co-occur in a document, 
the more likely they are correlative. This feature 
is denoted as 1 2( , )iCoD n n , which means the co-
occurrence of named entities 1n and 2n  in a docu-
ment iD . This feature is also the main feature 
used in Huang et al [2005].   
Co-occurrence in Documents The more often 
two named entities co-occur in different docu-
ments, the more likely they are correlative. This 
feature is denoted as 1 2( , )CoDs n n , which means 
the number of documents that both 1n  and 2n oc-
cur in. 
Distance The closer two named entities is in a 
document, the more likely they are correlative. 
This feature is denoted as 1 2( , )iDistD n n , which 
means the number of words between 1n and 2n  
in a document iD . 
Mutual Information Mutual information is a 
metric to measure the correlation degree of two 
words. The higher two named entities? mutual 
information, the more likely they are correlative. 
And the mutual information of named entities 
1n and 2n  in a document iD is computed as fol-
lowing formula. 
1 2
1 2 1 2
1 2
( , )
( , ) ( , ) log
( ) ( )i
p n n
MID n n p n n
p n p n
= ?  (1) 
Jaccard Similarity Jaccard similarity is also a 
metric to measure the correlative degree of two 
words. The higher two named entities? Jaccard 
                                                 
1 http://www.chasen.org/~taku/software/CRF++/ 
2 http://www.china-language.gov.cn/bakeoff08/ 
144
similarity, the more likely they are correlative. 
And Jaccard similarity is computed as following 
formula. 
1 2
1 2
1 2 1 2
( , )
( , )
( ) ( ) ( , )
CoDs n n
Jaccard n n
D n D n CoDs n n
= + ? (2) 
where ( )iD n  is the number of documents that 
in occurs in, and  ( , )i jCoDs n n  is the number of 
documents that both in  and jn occur in. 
TF-IDF TF-IDF is a weight computation 
method usually used in information retrieval. 
Here for a named entity in , TF-IDF is used to 
measure the importance of its correlative named 
entities. The TF-IDF value of jn in a document 
iD is computed as following formula. 
( ) log
( )i j ij j
N
TF IDF n tf
D n
? = ?               (3) 
where ijtf is the frequency of jn in docu-
ment iD , N is the number of total documents, 
and ( )jD n is the number of documents that 
jn occurs in.  
2.2 Feature Combination 
During the process of feature combination, every 
feature is normalized, and the final correlative 
degree of two named entities is the linear combi-
nation of these normalized features, and it is 
computed as following formula.   
1 2
( , ) ( , )
( , )
( , ) ( , )
k i j
i jk
i j
k i j i j
j k j
CoD n n CoDs n n
C n n
CoD n n CoDs n n
? ?= +
?
?? ?
3 4
1 ( , )( , )
1 ( , )
( , )
k i j
k i jk k
k i j
k i j j kj k
MID n nDistD n n
MID n n
DistD n n
? ?+ +
? ?
????
5 6
( )( , )
( , ) ( )
k j
i j k
i j k j
j k j
TF IDF nJaccard n n
Jaccard n n TF IDF n
? ?
?
+ + ?
?
? ??
(4) 
Finally, for every organization name in , its 
top-K correlative named entities are selected to 
construct the correlative named entity list.  
During translation, the correlative words for 
the input can be retrieved from this correlative 
list directly. If the input is not included in this list, 
the same method as in Huang et al [2005] is 
used to obtain the needed correlative words.  
3 Organization Name Translation 
Based on Correlative Expansion 
3.1 Statistical Translation Module 
The first step of our approach is to generate some 
candidate translations for every input organiza-
tion name. As shown in table 1, these candidate 
translations are used as query stems during query 
expansion. We use Moses3, a state of the art pub-
lic machine translation tool, to generate such 
candidate translations. Here Moses is trained 
with the bilingual corpus that is from the 4th 
China Workshop on Machine Translation4. Total 
there are 868,947 bilingual Chinese-English sen-
tence pairs on news domain in this bilingual cor-
pus. Moses receives an organization name as in-
put, and outputs the N-best results as the candi-
date translations of the input organization name. 
Total there are six features used in Moses: phrase 
translation probability, inverse phrase translation 
probability, lexical translation probability, in-
verse lexical translation probability, language 
model, and sentence length penalty. All the 
needed parameters are trained with MERT 
method (Och, 2003) by using a held-out devel-
opment set.  
3.2 Query Expansions 
Because the amount of available web pages is so 
huge, the query submitted to search engine must 
be well designed. Otherwise, the search engine 
will return large amount of un-related web pages. 
This will enlarge the difficulty of mining phase. 
Here three kinds of expansion methods are pro-
posed to generate some queries by combining the 
clues given by statistical translation method and 
the clues given by correlative named entities of 
the input. And these correlative named entities 
are retrieved from the correlative named entities 
list before the query expansions process. These 
three kinds of expansions are explained as fol-
lows. 
3.2.1 Monolingual Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, and jn is 
one of its correlative named entities. If jn can be 
reliably translated5, we expand is with this reli-
                                                 
3 http://www.statmt.org/moses/  
4 http://www.nlpr.ia.ac.cn/cwmt-2008  
5 A word can be reliably translated means either it has 
a unique dictionary translation or it is a Chinese 
145
able translation ( )jt n  to form a query 
? is + ( )jt n ?. This kind of expansion is called as 
monolingual expansion.  
For two named entities, if they are correlative, 
their translations are likely correlative too. So 
their translations are also likely to occur in pair 
in some web pages. Suppose a query generated 
by this expansion is ? is + ( )jt n ?, if the candidate 
translation is is the correct translation of the in-
put, there must be some returned web pages that 
contain is completely. Otherwise, it is still possi-
ble to obtain some returned web pages that con-
tain the correct translation. This is because that 
the search engine will return both the web pages 
that include the query completely and the web 
pages that include the query partly. And for a 
translation candidate is and the correct transla-
tion 'is , they are very likely to have some com-
mon words, so some of their returned web pages 
may overlap each other. Thus it can be expected 
that when we submit ? is + ( )jt n ? to search en-
gine, it will return some web pages that include 
? 'is + ( )jt n ? or include 'is .  This is very helpful 
for the mining phase. 
3.2.2 Bilingual Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, we ex-
pand is with in  to form a query ? is + in ?. This 
kind of expansion is called as bilingual expan-
sion. 
Bilingual expansion is very useful to verify 
whether a candidate translation is the correct 
translation. To give readers more information or 
they are not sure about the translation of original 
named entity, the Chinese authors usually in-
clude both the original form of a named entity 
and its translation in the mix-language web pages 
[Fei Huang et al 2005]. So the correct translation 
pair is likely to obtain more supports from the 
returned web pages than those incorrect transla-
tion pairs. Thus bilingual expansion is very use-
ful for the re-ranking phase. 
Besides, for an input organization name, if one 
of its incorrect candidate translations is  is very 
                                                                          
person name and can be translated by Pinyin map-
ping.  
similar to the correct translation 'is  in surface 
form, the correct translation is also likely to be 
contained in the returned web pages by using this 
kind of queries. The reason for this is the search 
mechanism of search engine, which has been 
explained above in monolingual expansion. 
3.2.3 Mix-language Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, and jn is 
one of its correlative named entities. We ex-
pand is with jn  to form a query ? is + jn ?. This 
kind of expansion is called as mix-language ex-
pansion.  
Mix-language expansion is a necessary com-
plement to the other two expansions. Besides, 
this mix-language expansion is more prone to 
obtain some mix-language web pages that may 
contain both the original input organization name 
and its correct translation.  
3.3 Mining 
When the expanded queries are submitted to 
search engine, the correct translation of the input 
organization name may be contained in the re-
turned web pages. Because the translation of an 
organization name must be also an organization 
name, we mine the correct translation of the in-
put among the English organization names. Here 
we use the Stanford named entity recognition 
toolkits6  to recognize all the English organiza-
tion names in the returned web pages. Then align 
these recognized organization names to the input 
by considering the following features. 
Mutual Translation Probability The transla-
tion probability measures the semantic equiva-
lence between a source organization name and its 
target candidate translation. And mutual transla-
tion probability measures this semantic equiva-
lence in two directions. For simplicity, here we 
use IBM model-1(Brown et al 1993), which 
computes two organization names? translation 
probability using the following formula. 
11
1
( | ) ( | )
J L
j lJ
lj
p f e p f e
L ==
= ??                  (6) 
where ( | )j lp f e is the lexical translation prob-
ability. Suppose the input organization name 
is in , is is one of the recognized English organi-
                                                 
6  http://nlp.stanford.edu/software/CRF-NER.shtml 
146
zation names, the mutual translation probability 
of in and is  is computed as: 
( , ) ( | ) (1 ) ( | )i i i i i imp n s p n s p s n? ?= + ?      (7) 
Golden Translation Ratio For two organiza-
tion names, their golden translation ratio is de-
fined as the percentage of words in one organiza-
tion name whose reliable transactions can be 
found in another organization name. This feature 
is used to measure the probability of one named 
entity is the translation of the other. It is com-
puted as following formula. 
( , ) ( , )
( , ) (1 )
| | | |
i j j i
i j
i j
G n s G s n
GR n s
n s
? ?= + ?   (8) 
where ( , )i jG n s is the number of golden trans-
lated words from in to js , and ( , )j iG s n  is the 
number of golden translated words from js to in .  
Co-occurrence In Web Pages For an input 
organization name in and a recognized candidate 
translation js , the more often they co-occur in 
different web pages, the more likely they are 
translations of each other. This feature is denoted 
as ( , )i jCoS n s , which means the number of web 
pages that both 1n  and js occur in. 
Input Matching Ratio This feature is defined 
as the percentage of the words in the input that 
can be found in a returned web page. For those 
mix-language web pages, this feature is used to 
measure the probability of the correct translation 
occurring in a returned web page. It is computed 
as the following formula. 
| |
( , )
| |
i k
i k
i
n s
IMR n s
n
?=                             (9) 
where ks is the k th?  returned web page. 
Correlative Named Entities Matching Ratio 
This feature is defined as the percentage of the 
words in a correlative named entity that can be 
found in a returned web page. This feature is also 
used to measure the probability of the correct 
translation occurring in a returned web page. It is 
computed as the following formula. 
| |
_ ( , )
| |
i k
i k
i
c s
CW MR c s
c
?=                   (10) 
The final confidence score of in and jt to be a 
translation pair is measured by following formula. 
As in formula 4, here every factor will be is nor-
malized during computation.   
1 2( , ) ( , ) ( , )i j i j i jC n t mp n t GR n t? ?= +  
4
3
( , )
( , )
( , )
i j
i k
ki j
j
CoSs n n
IMR n s
CoS n n K
??+ + ??
5 _ ( , )i k
i k
CW MR c s
K I
?+ ? ??           (11) 
where K is the number of returned web pages, 
I is the number of correlative named entities for 
the input organization name. 
For every input organization name, we remain 
a fixed number of mined candidate translations 
with the highest confidence scores. And add 
them to the original candidate translation set to 
form a revised candidate translation set.  
3.4 Re-ranking 
The aim of mining is to improve recall. And in 
the re-ranking phase, we hope to improve preci-
sion by assigning the correct translation a higher 
rank. The features considered here for the re-
ranking phase are listed as follows.  
Confidence Score The confidence score of 
in and jt  is not only useful for the mining phase, 
but also is useful for the re-ranking phase. The 
higher this score, the higher rank this candidate 
translation should be assigned.  
Inclusion Ratio For Bilingual Query This 
feature is defined as the percentage of the re-
turned web pages that the bilingual query is 
completely matched. It is computed as the fol-
lowing formula. 
( )
_ ( )
( )
i
i
i
h q
EHR BQ q
H q
=                           (12) 
where ( )ih q is the number of web pages that 
match the query iq completely, and ( )iH q is the 
total number of returned web pages for query iq . 
Candidate Inclusion Ratio for Monolingual 
Query and Mix-language Query This feature is 
defined as the percentage of the returned web 
pages that the candidate translation is completed 
matched. This feature for monolingual query is 
computed as formula 13, and this feature for 
mix-language query is computed as formula 14. 
( )_ ( ) ( )
i
i
i
h sECHR MlQ s H q=                (13) 
( )_ ( ) ( )
i
i
i
h sECHR MixQ s H q=              (14) 
where ( )ih s  is the number of web pages that 
match the candidate translation is completely, and 
147
( )iH q is the total number of returned web pages 
for query iq .  
Finally, the above features are combined with 
following formula.  
2
1( , ) ( , ) _ ( )i j i j i
i
R n t C n t EHR BQ q
N
??= + ?  
3 _ ( )i
i
ECHR MlQ s
M
?+ ?
4 _ ( )i
i
ECHR MixQ s
L
?+ ?              (15) 
where N is the number of candidate transla-
tions, M and L  are the number of monolingual 
queries and mix-language queries respectively. 
At last the revised candidate translation set is 
re-ranked according to this formula, and the top-
K results are outputted as the input?s translation 
results.  
4 Parameters Evaluations 
In above formula (4), formula (11) and formula 
(15), the parameters i? are interpolation feature 
weights, which reflect the importance of different 
features. We use some held-our organization 
name pairs as development set to train these pa-
rameters. For those parameters in formula (4), we 
used those considered features solely one by one, 
and evaluated their importance according to their 
corresponding inclusion ratio of correct transla-
tions when using mix-language expansion and 
the final weights are assigned according to the 
following formula. 
i
i
i
i
InclusionRate
InclusionRate
? = ?                   (16) 
Where iInclusionRate  is the inclusion rate 
when considered feature if  only. The inclusion 
rate is defined as the percentage of correct trans-
lations that are contained in the returned web 
pages as Huang et al[2005] did. 
To obtain the parameters in formula (11), we 
used those considered features solely one by one, 
and computed their corresponding precision on 
development set respectively, and final weights 
are assigned according to following formula. 
i
i
i
i
P
P
? = ?                              (17) 
Where iP  is the precision when considered 
feature if  only. And for the parameters in for-
mula (15), their assignment method is the same 
with the method used for formula (11). 
5 Experiments and Discussions 
We use a Chinese to English organization name 
translation task to evaluate our approach. The 
experiments consist of four parts. Firstly, we 
evaluate the contribution of the correlative 
named entities for obtaining the web pages that 
contain the correct translation of the input. Sec-
ondly, we evaluate the contribution of different 
query expansion methods. Thirdly, we investi-
gate to which extents our approach can solve the 
two problems mentioned in section 1. Finally, we 
evaluate how much our approach can improve 
the overall recall and precision. Note that for 
simplicity, we use 10-best outputs from Moses as 
the original candidate translations for every input. 
And the search engine used here is Live7. 
5.1 Test Set 
The test set consists of 247 Chinese organization 
names recognized from 2,000 web pages that are 
downloaded from Sina8. These test organization 
names are translated by a bilingual speaker given 
the text they appear in. And these translations are 
verified from their official government web 
pages respectively. During translation, we don?t 
use any contextual information. 
5.2 Contribution of Correlative Named En-
tities 
The contribution of correlative named entities is 
evaluated by inclusion rate, and we compare the 
inclusion rate with different amount of correla-
tive named entities and different amount of re-
turned web pages. The experimental results are 
shown in Table 1 (here we use all these three 
kinds of expanding strategies).  
# of correlative named enti-
ties used 
 
1 5 10 
1 0.17 0.39 0.47 
5 0.29 0.63 0.78 
#of web 
pages used
10    0.32 0.76 0.82 
Table 1. Comparisons of inclusion rate  
From these results we can find that our ap-
proach obtains an inclusion rate of 82% when we 
use 10 correlative named entities and 10 returned 
web pages. We notice that there are some Chi-
nese organization names whose correct English 
translations have multiple standards. For exam-
ple,  the organization name ?????is translated 
                                                 
7  http://www.live.com/ 
8  http://news.sina.com.cn/ 
148
into ?Department of Defense? when it refers to a 
department in US, but  is translated into ?Minis-
try of Defence? when it refers to a department in 
UK or in Singapore. This problem affects the 
actual inclusion rate of our approach. Another 
factor that affects the inclusion rate is the search 
engine used. There is a small difference in the 
inclusion rate when different search engines are 
used. For example, the Chinese organization 
name ?????/China CITIC Bank?, because 
the word ???? is an out-of-vocabulary word,  
the best output from Moses is ?of the bank?. 
With such candidate translation, none of our 
three expansion methods works. But when we 
used Google as search engine instead, we mined 
the correct translation. 
From these results we can conclude that by us-
ing correlative named entities, the returned web 
pages are more likely to contain the correct trans-
lations of the input organization names. 
5.3 Contribution of Three Query Expansion 
Methods 
In this section, we evaluate the contribution of 
these three query expansion methods respectively. 
To do this, we use them one by one during trans-
lation, and compare their inclusion rates respec-
tively. Experimental results are shown in Table 2. 
#of web pages 
used 
 
1 5 10
1 0.002 0.0020.004
5 0.017 0.0190.019
Monolingual 
Expansion 
Only 10 0.021 0.0370.051
1 0.112 0.1590.174
5 0.267 0.3270.472
Bilingual 
 Expansion
Only 10 0.285 0.4140.669
1 0.098 0.1380.161
5 0.231 0.3070.386
# of  
correlative 
named enti-
ties used 
Mix-language 
Expansion
Only 10 0.249 0.3980.652
Table 2. Inclusion rate of different kinds of query 
expansion methods 
From Table 2 we can see that bilingual expan-
sion and mix-language expansion play greater 
roles than monolingual expansion in obtaining 
the web pages that contain the correct transla-
tions of the inputs. This is because the condition 
of generating monolingual queries is too strict, 
which requires a reliable translation for the cor-
relative named entity. In most cases, this condi-
tion cannot be satisfied. So for many input or-
ganization names, we cannot generate any mono-
lingual queries for them at all. This is the reason 
why monolingual expansion obtains so poorer an 
inclusion rate compared with the other two ex-
pansions. To evaluate the true contribution of 
monolingual expansion method, we carry out 
another experiment. We select 10 organization 
names randomly from the test set, and translate 
all of their correlative named entities into English 
by a bilingual speaker. Then we evaluate the in-
clusion rate again on this new test set. The ex-
perimental results are shown in Table 3. 
# of correlative named enti-
ties used 
 
1 5 10 
1 0.2 0.3 0.6 
5 0.4 0.7 0.9 
#of web 
pages used
10    0.4 0.8 0.9 
Table 3. Inclusion rate for monolingual expan-
sion on new test set 
From Table 3 we can conclude that, if most of 
the correlative named entities can be reliably 
translated, the queries generated by this mono-
lingual expansion will play greater role in obtain-
ing the web pages that contain the correct trans-
lations of the inputs. 
From those results in Table 2 we can conclude 
that, these three kinds of expansions complement 
each other. Using them together can obtain 
higher inclusion rate than using anyone of them 
only. 
5.4 Efficiency on Solving Problem I and 
Problem II 
In this section, we investigate to which extents 
our approach can solve the two problems men-
tioned in section 1.We compare the wrong trans-
lation numbers caused by these two problems 
(another main kind of translation error is caused 
by the translation of out-of-vocabulary words) 
between Moses and our approach. The experi-
mental results are shown in Table 4.  
 Moses Results Our method
Problem I 44 3 
Problem II 30 0 
Table 4. Comparison of error numbers 
From Table 4 we can see that our approach is 
very effective on solving these two problems. 
Almost all of the errors caused by these two 
problems are corrected by our approach. Only 
three wrong translations are not corrected. This is 
because that there are some Chinese organization 
names whose correct English translations have 
multiple standards, such as the correct translation 
of organization name ?????depends on its 
nationality, which has been explained in section 
5.2. 
149
5.5 Our Approach vs. Other Approaches  
In this section, we compare our approach with 
other two methods: Moses and the approach pro-
posed by Huang et al [2005]. We compare their 
accuracy of Top-K results. For both our approach 
and Huang et al?s approach, we use 10 correla-
tive words for each input organization name and 
use 10 returned web pages for mining the correct 
translation result. The experimental results are 
shown in Table 5. 
 Moses  
Results 
Huang?s 
Results 
Our  
Results 
Top 1 0.09 0.44 0.53 
Top 5 0.18 0.61 0.73 
Top 10 0.31 0.68 0.79 
Table 5. Moses results vs. our results 
Moses is a state-of-the-art translation method, 
but it can hardly handle the organization name 
translation well. In addition to the errors caused 
by the above two problems mentioned in section 
1, the out-of-vocabulary problem is another ob-
stacle for Moses. For example, when translating 
the organization name ?????????
/International Tsunami Information Centre?, be-
cause the word ???? is an out-of-vocabulary 
word, Moses fails to give correct translation. But 
for those approaches that have a web mining 
process during translation, both the out-of-
vocabulary problem and the two problems men-
tioned in section 1 are less serious. This is the 
reason that Moses obtains the lowest perform-
ance compared with the other two approaches. 
Our approach is also superior to Huang?s method, 
as shown in the above table. We think this is be-
cause of the following three reasons. The first 
reason is that in our approach, we use a transla-
tion candidate generation process. Although 
these candidates are usually not so good, they 
can still provide some very useful clue informa-
tion for the web retrieval process. The second 
reason is that the features considered for correla-
tive words extraction in our approach are more 
comprehensive. Most of the time (except for the 
case that the input is not included in the correla-
tive word list) our approach is more prone to ob-
tain better correlative words for the input. The 
third reason is that our approach use more query 
expansion strategies than Huang?s approach. 
These expansion strategies may complement 
each other and improve the probability of obtain-
ing the web pages that contain the correct trans-
lations For example, both Moses and Huang?s 
approach failed to translate the organization 
name ??????????. But in our approach, 
with the candidate translation ?International In-
formation Centre? that is generated by Moses, 
our approach still can obtain the web page that 
contains the correct translation when using bilin-
gual expansion. Thus the correct translation ?In-
ternational Tsunami Information Centre? is 
mined out during the sequent mining process.  
From table 5 we also notice that the final re-
call of our approach is a little lower than the in-
clusion rate as show in table 1. This means that 
our approach doesn?t mine all the correct transla-
tions that are contained in the returned web pages. 
One of the reasons is that some of the input or-
ganization names are not clearly expressed. For 
example, an input organization name ?????
??, although its correct translation ?University 
of California, Berkeley? is contained in the re-
turned web pages, this correct translation cannot 
be mined out by our approach. But if it is ex-
pressed as ??????????????, its 
correct translation can be mined from the re-
turned web pages easily. Besides, the recognition 
errors of NER toolkits will also reduce the final 
recall of our approach.  
6 Conclusions and Future Work 
In this paper, we present a new organization 
name translation approach. It uses some correla-
tive named entities of the input and some query 
expansion strategies to help the search engine to 
retrieve those web pages that contain the correct 
translation of the input. Experimental results 
show that for most of the inputs, their correct 
translations are contained in the returned web 
pages. By mining these correct translations and 
re-ranking them, the two problems mentioned in 
section 1 are solved effectively. And recall and 
precision are also improved correspondingly.  
In the future, we will try to improve the ex-
traction perform of correlative named entities. 
We will also try to apply this approach to the 
person name translation and location name trans-
lation. 
Acknowledgments  
This work was supported by the open fund of 
National Laboratory of Pattern Recognition, In-
stitute of Automation Chinese Academy of Sci-
ence, P.R.C, and was also supported in part by 
National Science Foundation of China 
(60873091), Natural Science Foundation of 
Liaoning Province (20072032) and Shenyang 
Science and Technology Plan (1081235-1-00). 
150
References 
Chen Hsin-Hsi, Changhua Yang, and Ying Lin. 2003. 
Learning formulation and transformation rules for 
multilingual named entities. Proceedings of the 
ACL 2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition. pp1-8. 
Dekang Lin, Shaojun Zhao, Durme Benjamin Van 
Drume, Marius Pasca. Mining Parenthetical Trans-
lations from the Web by Word Alignment,  ACL08. 
pp994-1002. 
Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu. 
2008. Chinese-English Backward Transliteration 
Assisted with Mining Monolingual Web Pages. 
ACL2008. pp541-549. 
Fei Huang, Stephan Vogel and Alex Waibel. 2003. 
Automatic Extraction of Named Entity Translin-
gual Equivalence Based on Multi-feature Cost 
Minimization. Proceedings of the 2003 Annual 
Conference of the Association for Computational 
Linguistics, Workshop on Multilingual and Mixed-
language Named Entity Recognition.   
Fei Huang, Stephan vogel and Alex Waibel. 2004. 
Improving Named Entity Translation Combining 
Phonetic and Semantic Similarities. Proceedings of 
the HLT/NAACL. pp281-288.  
Fei Huang, Ying Zhang, Stephan Vogel. 2005. Min-
ing Key Phrase Translations from Web Corpora. 
HLT-EMNLP2005, pp483-490. 
Feng, Donghui, Yajuan LV, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), pp372-379. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. ACL2003. 
pp160-167. 
Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. Learning 
Transliteration Lexicon from the Web. COL-
ING/ACL2006. pp1129-1136. 
Hany Hassan and Jeffrey Sorensen. 2005. An Inte-
grated Approach for Arabic-English Named Entity 
Translation. Proceedings of ACL Workshop on 
Computational Approaches to Semitic Languages. 
pp87-93. 
Lee, Chun-Jen and Jason S.Chang and Jyh-Shing 
Roger Jang. 2004a. Bilingual named-entity pairs 
extraction from parallel corpora. Proceedings of 
IJCNLP-04 Workshop on Named Entity Recogni-
tion for Natural Language Processing Application. 
pp9-16. 
Lee, Chun-Jen, Jason S.Chang and Thomas C. 
Chuang. 2004b. Alignment of bilingual named en-
tities in parallel corpora using statistical model. 
Lecture Notes in Artificial Intelligence. 3265:144-
153. 
Lee, Chun-Jen, Jason S.Chang, and Jyh-Shing Roger 
Jang. 2005. Extraction of transliteration pairs from 
parallel corpora using a sta Acquisition of English-
Chinese transliterated word pairs from parallel-
aligned text using a statistical transliteration model. 
Information Sciences.  
Long Jiang, Ming Zhou, Lee-Feng Chien, Cheng Niu. 
[2007]. Named Entity Translation with Web Min-
ing and Transliteration. IJCAI-2007. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases form parallel corpora. ACL-
2003. pp259-266. 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311.  
Y. Al-Onaizan and K. Knight. 2002. Translating 
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting 
of the Association for Computational Linguistics, 
pp400-408. 
Ying Zhang and Phil Vines Using the Web for Auto-
mated Translation Extraction in Cross-Language 
Information Retrieval. SIGIR2004,pp162-169. 
Yufeng Chen, Chengqing Zong. A Structure-based 
Model for Chinese Organization Name Translation. 
ACM Transactions on Asian Language Information 
Processing, 2008, 7(1), pp1-30. 
151
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1344?1352,
Beijing, August 2010
Heterogeneous Parsing via Collaborative Decoding
Muhua Zhu Jingbo Zhu Tong Xiao
Natural Language Processing Lab.
Northeastern University
zhumuhua@gmail.com
{zhujingbo, xiaotong}@mail.neu.edu.cn
Abstract
There often exist multiple corpora for the
same natural language processing (NLP)
tasks. However, such corpora are gen-
erally used independently due to distinc-
tions in annotation standards. For the pur-
pose of full use of readily available hu-
man annotations, it is significant to simul-
taneously utilize multiple corpora of dif-
ferent annotation standards. In this pa-
per, we focus on the challenge of con-
stituent syntactic parsing with treebanks
of different annotations and propose a col-
laborative decoding (or co-decoding) ap-
proach to improve parsing accuracy by
leveraging bracket structure consensus be-
tween multiple parsing decoders trained
on individual treebanks. Experimental re-
sults show the effectiveness of the pro-
posed approach, which outperforms state-
of-the-art baselines, especially on long
sentences.
1 Introduction
Recent years have seen extensive applications of
machine learning methods to natural language
processing problems. Typically, increase in the
scale of training data boosts the performance of
machine learning methods, which in turn en-
hances the quality of learning-based NLP systems
(Banko and Brill, 2001). However, annotating
data by human is expensive in time and labor. For
this reason, human-annotated corpora are consid-
ered as the most valuable resource for NLP.
In practice, there often exist more than one cor-
pus for the same NLP tasks. For example, for
constituent syntactic parsing (Collins, 1999; Char-
niak, 2000; Petrov et al, 2006) in Chinese, in ad-
dition to the most popular treebank Chinese Tree-
bank (CTB) (Xue et al, 2002), there are also
other treebanks such as Tsinghua Chinese Tree-
bank (TCT) (Zhou, 1996). For the purpose of
full use of readily available human annotations
for the same tasks, it is significant if such cor-
pora can be used jointly. At first sight, a di-
rect combination of multiple corpora is a way to
this end. However, corpora created for the same
NLP tasks are generally built by different orga-
nizations. Thus such corpora often follow dif-
ferent annotation standards and/or even different
linguistic theories. We take CTB and TCT as
a case study. Although both CTB and TCT are
Chomskian-style treebanks, they have annotation
divergences in at least two dimensions: a) CTB
and TCT have dramatically different tag sets, in-
cluding parts-of-speech and grammar labels, and
the tags cannot be mapped one to one; b) CTB
and TCT have distinct hierarchical structures. For
example, the words ??? (Chinese) ?? (tradi-
tional) ?? (culture)? are grouped as a flat noun
phrase according to the CTB standard (right side
in Fig. 1), but in TCT, the last two words are in-
stead grouped together beforehand (left side in
Fig. 1). The differences cause such treebanks
of different annotations to be generally used in-
dependently. This paper is dedicated to solving
the problem of how to use jointly multiple dis-
parate treebanks for constituent syntactic parsing.
Hereafter, treebanks of different annotations are
1344
called heterogeneous treebanks, and correspond-
ingly, the problem of syntactic parsing with het-
erogeneous treebanks is referred to as heteroge-
neous parsing.
Previous work on heterogeneous parsing is of-
ten based on treebank transformation (or treebank
conversion) (Wang et al, 1994; Niu et al, 2009).
The basic idea is to transform annotations of one
treebank (source treebank) to fit the standard of
another treebank (target treebank). Due to diver-
gences of treebank annotations, such transforma-
tion is generally achieved in an indirect way by
selecting transformation results from the output of
a parser trained on the target treebank. A com-
mon property of all the work mentioned above is
that transformation accuracy is heavily dependent
on the performance of parsers trained on the tar-
get treebank. Sometimes transformation accuracy
is not so satisfactory that techniques like instance
pruning are needed in order to refine transforma-
tion results (Niu et al, 2009).
We claim there exists another way, interesting
but less studied for heterogeneous parsing. The
basic idea is that, although there are annotation
divergences between heterogenous treebanks, ac-
tually we can also find consensus in annotations
of bracket structures. Thus we would like to train
parsers on individual heterogeneous treebanks and
guide the parsers to gain output with consensus in
bracket structures as much as possible when they
are parsing the same sentences.
To realize this idea, we propose a generic col-
laborative decoding (or co-decoding) framework
where decoders trained on heterogeneous tree-
banks can exchange consensus information be-
tween each other during the decoding phase. The-
oretically the framework is able to incorporate a
large number of treebanks and various functions
that formalize consensus statistics.
Our contributions can be summarized: 1) we
propose a co-decoding approach to directly uti-
lizing heterogeneous treebanks; 2) we propose a
novel function to measure parsing consensus be-
tween multiple decoders. We also conduct ex-
periments on two Chinese treebanks: CTB and
TCT. The results show that our approach achieves
promising improvements over baseline systems
which make no use of consensus information.
np
nS
??
np
a
??
n
??
NP
NR
??
NN
??
NN
??
??????
(Chinese) (traditional) (culture)
Figure 1: Example tree fragments with TCT (left)
and CTB (right) annotations
2 Collaborative Decoding-based
Heterogeneous Parsing
2.1 Motivation
This section describes the motivation to use
co-decoding for heterogeneous parsing. We first
use the example in Fig. 1 to illustrate what con-
sensus information exists between heterogenous
treebanks and why such information might help
to improve parsing accuracy. This figure contains
two partial parse trees corresponding to the
words ??? (Chinese) ?? (traditional) ??
(culture)?, annotated according to the TCT (left
side) and CTB (right side) standards respectively.
Despite the distinctions in tag sets and bracket
structures, these parse trees actually have partial
agreements in bracket structures. That is, not all
bracket structures in the parse trees are different.
Specifically put, although the internal structures
of the parse trees are different, both CTB and
TCT agree to take ??? ?? ??? as a noun
phrase. Motivated by this observation, we would
like to guide parsers that are trained on CTB and
TCT respectively to verify their output interac-
tively by using consensus information implicitly
contained in these treebanks. Better performance
is expected when such information is considered.
A feasible framework to make use of consensus
information is n-best combination (Henderson
and Brill, 1999; Sagae and Lavie, 2006; Zhang et
al., 2009; Fossum and Knight, 2009). In contrast
1345
to previous work on n-best combination where
multiple parsers, say, Collins parser (Collins,
1999) and Berkeley parser (Petrov et al, 2006)
are trained on the same training data, n-best
combination for heterogeneous parsing is instead
allowed to use either a single parser or multiple
parsers which are trained on heterogeneous
treebanks. Consensus information can be incor-
porated during the combination of the output
(n-best list of full parse trees following distinct
annotation standards) of individual parsers. How-
ever, despite the success of n-best combination
methods, they suffer from the limited scope of
n-best list. Taking this into account, we prefer
to apply the co-decoding approach such that
consensus information is expected to affect the
entire procedure of searching hypothesis space.
2.2 System Overview
The idea of co-decoding is recently extensively
studied in the literature of SMT (Li et al, 2009;
Liu et al, 2009). As the name shows, co-decoding
requires multiple decoders be combined and pro-
ceed collaboratively. As with n-best combination,
there are at least two ways to build multiple de-
coders: we can either use multiple parsers trained
on the same training data (use of diversity of mod-
els), or use a single parser on different training
data (use of diversity of datasets) 1. Both ways
can build multiple decoders which are to be inte-
grated into co-decoding. For the latter case, one
method to get diverse training data is to use dif-
ferent portions of the same training set. In this
study we extend the case to an extreme situation
where heterogeneous treebanks are used to build
multiple decoders.
Fig. 2 represents a basic flow chart of heteroge-
neous parsing via co-decoding. Note that here we
discuss the case of co-decoding with only two de-
coders, but the framework is generic enough to in-
tegrate more than two decoders. For convenience
of reference, we call a decoder without incorpo-
rating consensus information as baseline decoder
1To make terminologies clear, we use parser as its regular
sense, including training models (ex. Collins model 2) and
parsing algorithms (ex. the CKY algorithm used in Collins
parser), and we use decoder to represent parsing algorithms
with specified parameter values
treebank1 treebank2
decoder1 decoder2
co-decoding
test data
Figure 2: Basic flow chart of co-decoding
and correspondingly refer to a decoder augmented
with consensus information as member decoder.
So the basic steps of co-decoding for heteroge-
neous parsing is to first build baseline decoders on
heterogeneous treebanks and then use the baseline
decoders to parse sentences with consensus infor-
mation exchanged between each other.
To complete co-decoding for heterogeneous
parsing, three key components should be consid-
ered in the system:
? Co-decoding model. A co-decoder con-
sists of multiple member decoders which are
baseline decoders augmented with consen-
sus information. Co-decoding model de-
fines how baseline decoders and consensus
information are correlated to get member de-
coders.
? Decoder coordination. Decoders in the co-
decoding model cannot proceed indepen-
dently but should have interactions between
each other in order to exchange consensus in-
formation. A decoder coordination strategy
decides on when, where, and how the inter-
actions happen.
? Consensus-based score function. Consensus-
based score functions formalize consensus
information between member decoders. Tak-
ing time complexity into consideration, con-
sensus statistics should be able to be com-
puted efficiently.
1346
In the following subsections, we first present
the generic co-decoding model and then describe
in detail how member decoders collaborate. Fi-
nally we introduce a novel consensus-based score
function which is used to quantify consensus in-
formation exchanged between member decoders.
2.3 Generic Co-decoding Model
The generic co-decoding model described here is
also used in (Li et al, 2009) for co-decoding of
machine translators. For a given sentence S, a
parsing algorithm (decoder) seeks a parse tree T ?
which is optimal in the sense that it maximizes
some score function F (T ), as shown in Eq. 1.
T ? = argmax
Ts.t.S=yield(T )
F (T ) (1)
where Ts.t.S = yield(T ) represents the set of
parse trees that yield the input sentence S. For
baseline decoders, the score function F (T ) is
generally just the inside probability P (T ) 2 of
a tree T , defined as the product of probabili-
ties of grammar rules appearing in parse tree T :?
r?R(T ) P (r). In the co-decoding framework,
F (T ) is extended so as to integrate consensus-
based score functions which measure consensus
information between member decoders, as shown
in Eq. 2.
Fm(T ) = Pm(T ) +
n?
k,k 6=m
?k(Hk(S), T ) (2)
We use dk to denote the kth decoder and use
Hk(S) to denote corresponding parsing hypoth-
esis space of decoder dk. Moreover, Pm(T ) is
referred to as baseline score given by baseline
decoders and ?k(Hk(S), T ) is consensus score
between decoders dm and dk, which is defined
as a linear combination of consensus-based score
functions, as shown in Eq. 3.
?k(Hk(S), T ) =
?
l
?k,lfk,l(Hk(S), T ) (3)
where fk,l(Hk(S), T ) represents a consensus-
based score function between T and Hk(S),
and ?k,l is the corresponding weight. Index l
2Actually, the joint probability P(S,T) of sentence S and
parse tree T is used, but we can prove that P (S, T ) = P (T ).
ranges over all consensus-based score functions
in Eq. 3. Theoretically we can define a variety
of consensus-based score functions.
For the simplest case where there are only two
member decoders and one consensus-based score
function, Eq. 2 and Eq. 3 can be combined and
simplified into the equation
Fi(T ) = Pi(T ) + ?1?if(H1?i(S), T ) (4)
where index i is set to the value of either 1 or 0.
This simplified version is used in the experiments
of this study.
2.4 Decoder Coordination
This subsection discusses the problem of decoder
coordination. Note that although Eq. 2 is defined
at sentence level, the co-decoding model actu-
ally should be applied to the parsing procedure
of any subsequence (word span) of sentence S.
So it is natural to render member decoders col-
laborate when they are processing the same word
spans. To this end, we would like to adopt best-
first CKY-style parsing algorithms as baseline de-
coders, since CKY-style decoders have the prop-
erty that they process word spans in the ascend-
ing order of span sizes. Moreover, the hypothe-
ses 3 spanning the same range of words are read-
ily stacked together in a chart cell before CKY-
style decoders move on to process other spans.
Thus, member decoders can process the same
word spans collaboratively from small ones to big
ones until they finally complete parsing the entire
sentence.
A second issue in Eq. 2 is that consensus-
based score functions are dependent on hypoth-
esis space Hk(S). Unfortunately, the whole hy-
pothesis space is not available most of the time.
To address this issue, one practical method is to
approximate Hk(S) with a n-best hypothesis list.
For best-first CKY parsing, we actually retain all
unpruned partial hypotheses over the same span
as the approximation. Hereafter, the approxima-
tion is denoted as H?k(S)
Finally, we notice in Eq. 2 that consensus score
3In the literature of syntactic parsing, especially in chart
parsing, hypotheses is often called edges. This paper will
continue to use the terminology hypothesis when no ambigu-
ity exists.
1347
?k(Hk(S), T ) and Hk(S) form a circular depen-
dency: searching for Hk(S) requires both base-
line score and consensus score; on the other hand,
calculating consensus score needs Hk(S) (its ap-
proximation in practice) to be known beforehand.
Li et al (2009) solves this dilemma with a boot-
strapping method. It starts with seedy n-best lists
generated by baseline decoders and then alter-
nates between calculating consensus scores and
updating n-best hypothesis lists. Such bootstrap-
ping method is a natural choice to break down the
circular dependency, but multi-pass re-decoding
might dramatically reduce decoding efficiency.
Actually, Li et al (2009) restricts the iteration
number to two in their experiments. In this paper,
we instead use an alternative to the bootstrapping
method. The process is described as follows.
1. In traditional best-first CKY-style parsing al-
gorithms, hypotheses over the same word
spans are grouped according to some crite-
rion of hypothesis equivalence 4. Among
equivalent hypotheses, only a single optimal
hypothesis is retained. In this paper, we in-
stead keep top k of equivalent hypotheses in
a data structure called best-first cache.
2. Use hypotheses in best-first caches to ap-
proximate Hk(S), and calculate consensus
score ?k(Hk(S), T ) between decoders.
3. Use baseline score and consensus score to lo-
cally rerank hypotheses in best-first caches.
Then remove hypotheses in caches except the
top one hypothesis.
In this study, we choose the best-first CKY-style
parsing algorithm used in Collins parser (Collins,
1999). Algorithm 1 extends this algorithm for co-
decoding. The first two steps initialize baseline
decoders and assign appropriate POS tags to sen-
tence St. Since baseline decoders are built on het-
erogeneous treebanks, POS taggers correspond-
ing to each baseline decoder are demanded, unless
gold POS tags are provided. The third step is the
core of the co-decoding algorithm. Here the com-
plete procedure invokes baseline decoders to com-
4the simplest criterion of equivalence is whether hypothe-
ses have the same grammar labels.
Algorithm 1 CKY-style Co-decoding
Argument: dk{the set of baseline decoders}
St{a sentence to be parsed}
Begin
Steps:
1. assign POS tags to sentence St
2. initialize baseline decoders dk
3. for span from 2 to sentence length do
for start from 1 to (sentence length-span+1) do
end := (start + span - 1)
for each base decoder dk do
complete(dk , start, end)
do co-decoding(start, end)
End
Subroutine:
complete(dk, start, end): base decoder dk generates
hypotheses over the span (begin.end), and fills in best-
first caches.
co-decoding(start, end): calculate consensus score
and rerank hypotheses in best-first caches. The top 1 is
chosen to be the best-first hypothesis.
plete parsing on the span [start, end] and gener-
ates H?k(s). The co-decoding procedure calculates
consensus score and locally reranks hypotheses in
best-first caches.
2.5 Consensus-based Score Function
There are at least two feasible ways to mea-
sure consensus between constituency parse trees.
By viewing parse trees from diverse perspectives,
we can either use functions on bracket structures
of parse trees, as in (Wang et al, 1994), or
use functions on head-dependent relations by first
transforming constituency trees into dependency
trees, as in (Niu et al, 2009). Although the co-
decoding model is generic enough to integrate var-
ious consensus-based score functions in a uniform
way, this paper only uses a bracket structure-based
function.
As mentioned above, the function proposed in
(Wang et al, 1994) is based on bracket struc-
tures. Unfortunately, that function is not appli-
cable in the situation of this paper. The reason is
that, the function in (Wang et al, 1994) is de-
fined to work on two parse trees, but this paper
instead needs a function on a tree T and a set of
trees (the approximation H?k(S)). To this end, we
first introduce the concept of constituent set (CS)
of a parse tree. Conceptually, CS of a parse tree is
a set of word spans corresponding to all the sub-
1348
64
1
5
2 3
[1,3],[2,3],[1,1]
[1,1]
[2,3],[2,2],[3,3]
[1,1]
[2,2]
[3,3]
Figure 3: Constituent set of a synthetic parse tree
trees of the tree, as illustrated in Fig. 3. For exam-
ple, the constituent set of the tree rooted at node
6 has three elements: [1, 1], [1, 3], and [1, 2]. For
H?k(S), the constituent set is defined as the union
of constituent sets of all elements it contains.
CS(H?k(S)) =
?
T?H?k(S)
CS(T )
In practice, we need to cut off elements in
CS(H?k(S)) in order to retain most confident
word spans.
With the concept of constituent set, a
consensus-based score function on T and H?k(S)
can be defined as follows.
f(H?k(S), T ) =
?
c?CS(T ) I(c, CS(H?k(S)))
|CS(T )| (5)
where I(c, CS(H?k(S))) is an indicator function
which returns one if c ? CS(T ) is compatible
with all the elements in CS(H?k(S)), zero oth-
erwise. Two spans, [a, b] and [i, j] are said to
be compatible if they satisfy one of the following
conditions: 1) i > b; 2) a > j; 3) a ? i ? b and
j ? b; 4) i ? a ? j and b ? j. Fig 4 uses two
example to illustrate the concept of compatibility.
3 Experiments
3.1 Data and Performance Metric
The most recent version of the CTB corpus, CTB
6.0 and the CIPS ParsEval data are used as hetero-
geneous treebanks in the experiments. Following
the split utilized in (Huang et al, 2007), we di-
vided the dataset into blocks of 10 files. For each
w1 w2 w3 w4 w1 w2 w3 w4
Figure 4: left) two spans conflict; right) two spans
are compatible
block, the first file was added to the CTB develop-
ment data, the second file was added to the CTB
testing data, and the remaining 8 files were added
to the CTB training data. For the sake of parsing
efficiency, we randomly sampled 1,000 sentences
of no more than 40 words from the CTB test set.
CTB-Partitions Train Dev Test
#Sentences 22,724 2,855 1,000
#Words 627,833 78,653 25,100
Ave-Length 30.1 30.0 20.3
TCT-Partitions Train Dev Test
#Sentences 32,771 N/A 1,000
#Words 354,767 N/A 10,400
Ave-Length 10.6 N/A 10.4
Table 1: Basic statistics on the CTB and TCT data
CIPS-ParsEval data is publicly available for the
first Chinese syntactic parsing competition, CIPS-
ParsEval 2009. Compared to CTB, sentences in
CIPS-ParsEval data are much shorter in length.
We removed sentences which have words less
than three. CIPS-ParsEval test set has 7,995 sen-
tences after sentence pruning. As with the CTB
test set, we randomly sampled 1,000 sentences
for evaluating co-decoding performance. Since
CIPS-ParsEval data is actually a portion of the
TCT corpus, for convenience of reference, we will
refer to CIPS-ParsEval data as TCT in the follow-
ing sections. Table 1 contains statistics on CTB
and TCT.
The two training sets are used individually to
build baseline decoders. With regard to the test
sets, each sentence in the test sets should have
two kinds of POS tags, according to the CTB and
TCT standards respectively. To this end, we ap-
plied a HMM-based method for POS annotation
transformation (Zhu and Zhu, 2009). During the
POS transformation, the divergences of word seg-
mentation are omitted.
For all experiments, bracketing F1 is used as
the performance metric, provided by EVALB 5.
5http://nlp.cs.nyu.edu/evalb
1349
3.2 Baseline Decoders
As already mentioned above, we apply Collins
parser in this paper. Specifically speaking, two
CKY-style baseline decoders to participate co-
decoding are built on CTB and TCT respectively
with Collins model two. For the CTB-based de-
coder, we use the CTB training data with slight
modifications: we replaced POS tags of punctua-
tions with specific punctuation symbols.
To get the TCT-based decoder, we made follow-
ing modifications. Firstly, TCT is available with
manually annotated head indices for all the con-
stituents in parse trees. For example, a grammar
label, say, np-1, means that the constituent is a
noun phrase with the second child being its head
child. In order to relax context independence as-
sumptions made in PCFG, we appended head in-
dices to grammar labels to get new labels, for ex-
ample np1. Secondly, since Collins parser is a
lexicalized parser, head rules specific to the TCT
corpus were manually created, which are used to-
gether with readily available head indices. Such
adaptation is also used in (Chen et al, 2009);
3.3 Parsing Results
We conduct experiments on both CTB and TCT
test sets. Two parameters need to be set: the cut-
off threshold for constructing constituent set of
H?k(S) and the weight ? 6 of consensus score in
Eq. 4. We tuned the parameters on the CTB de-
velopment set and finally set them to 5 and 20
respectively in the experiments. Table 2 presents
bracketing F1 scores of baseline systems and the
co-decoding approach. Here, the row of baseline
represents the performance of individual baseline
decoders, and the comparison of baseline and co-
decoding on a test set, say CTB, demonstrates
how much boosting the other side, say TCT, can
supply. For the co-decoding approach, the size
of best-first cache is set to 5 which achieves the
best result among the cache sizes we have experi-
mented.
As the results show, co-decoding achieves
promising improvements over baseline systems
on both test sets. Interestingly, we see that the
improvement on the TCT test set is larger than
6We use the same ? for both member decoders.
Test Set CTB TCT
Baseline 79.82 81.02
Co-decoding 80.33 81.77
Table 2: Baseline and Co-decoding on the CTB
and TCT test sets
that on the CTB test set. In general, a relatively
strong decoder can improve co-decoding perfor-
mance more than a relatively weak decoder does.
At the first sight, the TCT-based decoder seems to
have better performance than the CTB-based de-
coder. But if taking sentence length into consid-
eration, we can find that the TCT-based decoder
is actually relatively weak. Table 3 shows the
performance of the CTB-based decoder on short
sentences.
3.4 Analysis
Fig. 5 shows the bracketing F1 on the CTB test set
at different settings of the best-first cache size C .
F1 scores reach the peak before C increases to 6.
As a result, we set C to 5 in all our experiments.
 79
 79.5
 80
 80.5
 81
 0  1  2  3  4  5  6
br
ac
ke
tin
g 
F1
size of best-first cache
CTB
Figure 5: Bracketing F1 with varying best-first
cache size
To evaluate the effect of sentence length on co-
decoding, Table 3 presents F1 scores on portions
of the CTB test set, partitioned according to sen-
tence length. From the results we can see that
co-decoding performs better on long sentences.
One possible reason is that member decoders have
more consensus on big spans. Taking this obser-
vation into consideration, one enhancement to the
co-decoding approach is to enable co-decoding
only on long sentences. This way, parsing ef-
1350
Partitions [0,10] (10,20] (20,30] (30,40]
# Sentence 276 254 266 204
Ave-Length 6.07 15.64 25.43 35.20
Baseline 92.83 84.34 78.98 76.69
Co-decoding 92.84 84.36 79.43 77.65
Table 3: Effect of sentence length on co-decoding
performance
ficiency of co-decoding can be improved. It is
worth emphasizing that co-decoding is still help-
ful for parsers whose performance on short sen-
tences is not satisfactory, as shown in Table 2.
Another interesting analysis is to check how
many parsing results are affected by co-decoding,
compared to baseline decoders. Table 4 shows
the statistics.
Test Set # All # Improved # Decreased
CTB 1000 225 109
TCT 1000 263 92
Table 4: Statistics on sentences of test data
As the table shows, although overall accuracy is
increased, we find that on some sentences, co-
decoding instead worsens parsing accuracy. In
order to get insights on error sources, we manu-
ally analyzed 20 sentences on which co-decoding
achieves negative results. We find a large por-
tion (14 of 20) of sentences are short sentences
(of words less than 20). Actually, due to high ac-
curacy of the CTB-based decoder on short sen-
tences, co-decoding is indifferent when this de-
coder is processing short sentences. And we also
find that some errors are derived from differences
in annotation standards. Fortunately, the diver-
gence of annotations mainly exists in relatively
small spans. So one solution to the problem is to
enable co-decoding on relatively big spans. These
will be done in our future work.
4 Related Work
4.1 System Combination
In the literature of syntactic parsing, n-best com-
bination methods include parse selection, con-
stituent recombination, production recombina-
tion, and n-best reranking. Henderson and Brill
(1999) performs parse selection by maximizing
the expected precision of selected parse with re-
spect to the set of parses to be combined. Sagae
and Lavie (2006) proposes to recombine con-
stituents from the output of individual parsers.
More recently, Fossum and Knight (2009) studies
a combination method at production level. Zhang
et al (2009) reranks n-best list of one parser with
scores derived from another parser.
Compared to n-best combination, co-decoding
(Li et al, 2009; Liu et al, 2009) combines sys-
tems during decoding phase. Theoretically, sys-
tem combination during decoding phase helps de-
coders to select better approximation to hypothe-
sis space, since pruning is practically unavoidable.
To the best of our knowledge, co-decoding meth-
ods have not been applied to syntactic parsing.
4.2 Treebank Transformation
The focus of this study is heterogeneous parsing.
Previous work on this challenge is generally based
on treebank transformation. Wang et al (1994)
describes a method for transformation between
constituency treebanks. The basic idea is to train
a parser on a target treebank and generate a n-best
list for each sentence in source treebank(s). Then,
a matching metric which is a function on the num-
ber of the same word spans between two trees is
defined to select a best parse from each n-best list.
Niu et al (2009) applies a closely similar frame-
work as with (Wang et al, 1994) to transform a
dependency treebank to a constituency one.
5 Conclusions
This paper proposed a co-decoding approach to
the challenge of heterogeneous parsing. Com-
pared to previous work on this challenge, co-
decoding is able to directly utilize heterogeneous
treebanks by incorporating consensus information
between partial output of individual parsers dur-
ing the decoding phase. Experiments demonstrate
the effectiveness of the co-decoding approach, es-
pecially the effectiveness on long sentences.
Acknowledgments
This work was supported in part by the National
Science Foundation of China (60873091). We
would like to thank our anonymous reviewers for
their comments.
1351
References
Banko, Michele and Eric Brill. 2001. Scaling to
very very large corpora for natural language dis-
ambiguation. In Proc. of ACL 2001, pages 26-33.
Chen, Xiao, Changning Huang, Mu Li, and Chunyu
Kit. 2009. Better Parser Combination. Technique
Report of CIPS-ParsEval 2009.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proc. of NAACL 2000, pages
132-139.
Fossum, Victoria and Kevin Knight. 2009. Combin-
ing constituent parsers. In Proc. of NAACL 2009,
pages 253-256.
Henderson, John and Eric Brill. 1999. Exploiting di-
versity in natural language processing. In Proc. of
SIGDAT-EMNLP 1999, pages 187-194.
Huang, Zhongqiang, Mary P. Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In Proc. of EMNLP-CoNLL
2007, pages 1093-1102.
Li, Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009. Collaborative decoding: par-
tial hypothesis re-ranking using trnaslationconsen-
sus between decoders. In Proc. of ACL 2009, pages
585-592.
Liu, Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint Decoding with Multiple Translation Models.
In Proc. of ACL 2009, pages 576-584.
Niu, Zheng-Yu, Haifeng Wang, Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In
Proc. of ACL 2009, pages 46-54.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440.
Sage, Kenji and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proc. of NAACL 2006, pages
129-132.
Xue, Nianwen, Fu dong Chiou, and Martha Palmer.
2002. Building a large-scale Annotated Chinese
corpus. In Proc. of COLING 2002, pages 1-8.
Wang, Jong-Nae, Jing-Shin Chang, and Keh-Yih Su.
1994. An automatic treebank conversion algorithm
for corpus sharing. In Proc. of ACL 1994, pages
248-254.
Zhang, Hui, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proc. of EMNLP 2009, pages 1552-1560.
Zhou, Qiang. 1996. Phrase bracketing and annotating
on Chinese language corpus. (in Chinese) Ph.D.
thesis, Beijing University.
Zhu, Muhua and Jingbo Zhu. 2009. Label Corre-
spondence Learning for Part-of-Speech Annotation
Transformation. In Proc. of CIKM 2009, pages
1461-1464.
1352
Coling 2010: Poster Volume, pages 1345?1353,
Beijing, August 2010
An Empirical Study of Translation Rule Extraction with Multiple 
Parsers 
 
Tong Xiao??, Jingbo Zhu??, Hao Zhang?, Muhua Zhu?? 
 
?Natural Language Processing Lab., Northeastern University 
?Key Laboratory of Medical Image Computing, Ministry of Education 
{xiaotong,zhujingbo}@mail.neu.edu.cn 
zhanghao@ics.neu.edu.cn, zhumuhua@gmail.com 
 
Abstract 
Translation rule extraction is an impor-
tant issue in syntax-based Statistical Ma-
chine Translation (SMT). Recent studies 
show that rule coverage is one of the key 
factors affecting the success of syntax-
based systems. In this paper, we first 
present a simple and effective method to 
improve rule coverage by using multiple 
parsers in translation rule extraction, and 
then empirically investigate the effec-
tiveness of our method on Chinese-
English translation tasks. Experimental 
results show that extracting translation 
rules using multiple parsers improves a 
string-to-tree system by over 0.9 BLEU 
points on both NIST 2004 and 2005 test 
corpora. 
1 Introduction 
Recently various syntax-based models have been 
extensively investigated in Statistical Machine 
Translation (SMT), including models between 
source trees and target strings (Quirk et al, 2005; 
Liu et al, 2006; Huang et al, 2006), source 
strings and target trees (Yamada and Knight, 
2001; Galley et al, 2006; Shen et al, 2008), or 
source trees and target trees (Eisner, 2003; Ding 
and Palmer, 2005; Cowan et al, 2006; Zhang et 
al., 2008; Liu et al, 2009). In these models, au-
tomatic extraction of translation rules is an im-
portant issue, in which translation rules are typi-
cally extracted using parse trees on 
source/target-language side or both sides of the 
bilingual text. Exploiting the syntactic informa-
tion encoded in translation rules, syntax-based 
systems have shown to achieve comparable per-
formance with phrase-based systems, even out-
perform them in some cases (Marcu et al, 2006). 
Among all the factors contributing to the suc-
cess of syntax-based systems, rule coverage has 
been proved to be an important one that affects 
the translation accuracy of syntax-based systems 
(DeNeefe et al, 2007; Shen et al, 2008). How-
ever, these systems suffer from a problem that 
translation rules are extracted using only 1-best 
parse tree generated by a single parser, which 
generally results in relatively low rule coverage 
due to the limited scope in rule extraction (Mi 
and Huang, 2008). To alleviate this problem, a 
straightforward solution is to enlarge the scope 
of rule extraction, and obtain translation rules by 
using a group of diversified parse trees instead 
of a single parse tree. For example, Mi and 
Huang (2008) used k-best parses and forest to 
extract translation rules for improving the rule 
coverage in their forest-based SMT system, and 
achieved promising results. However, most pre-
vious work used the parse trees generated by 
only one parser, which still suffered somewhat 
from the relatively low diversity in the outputs 
of a single parser. 
Addressing this issue, we investigate how to 
extract diversified translation rules using multi-
ple parsers. As different parsers (or parsing 
models) can provide us with parse trees having 
relatively large diversity, we believe that it is 
beneficial to employ multiple different parsers to 
obtain diversified translation rules and thus en-
large the rule coverage. Motivated by this idea, 
we propose a simple and effective method to 
improve rule coverage by using multiple parsers 
1345
in rule extraction. Furthermore, we conduct an 
empirical study to investigate the effectiveness 
of our method on Chinese-English translation in 
a string-to-tree system. Experimental results 
show that our method improves the baseline sys-
tem by over 0.9 BLEU points on both NIST 
2004 and 2005 test corpora, even achieves a +1 
BLEU improvement when working with the k-
best extraction method. More interestingly, we 
observe that the MT performance is not very 
sensitive to the parsing performance of the pars-
ers used in rule extraction. Actually, the MT sys-
tem does not show different preferences for dif-
ferent parsers. 
2 Related Work 
In machine translation, some efforts have been 
made to improve rule coverage and advance the 
performance of syntax-based systems. For ex-
ample, Galley et al (2006) proposed the idea of 
rule composing which composes two or more 
rules with shared states to form a larger, com-
posed rule. Their experimental results showed 
that the rule composing method could signifi-
cantly improve the translation accuracy of their 
syntax-based system. Following Galley et al 
(2006)?s work, Marcu et al (2006) proposed 
SPMT models to improve the coverage of phras-
al rules, and demonstrated that the system per-
formance could be further improved by using 
their proposed models. Wang et al (2007) de-
scribed a binarization method that binarized 
parse trees to improve the rule coverage on non-
syntactic mappings. DeNeefe et al (2007) analy-
ized the phrasal coverage problem, and com-
pared the phrasal coverage as well as translation 
accuracy for various rule extraction methods 
(Galley et al, 2006; Marcu et al, 2006; Wang et 
al., 2007). 
As another research direction, some work is 
focused on enlarging the scope of rule extraction 
to improve rule coverage. For example, (Venu-
gopal et al, 2008) and (Mi and Huang, 2008) 
extracted rules from the k-best parses and forest 
generated by a single parser to alleviate the 
problem of the limited scope of 1-best parse, and 
achieved promising results. 
Our work differs from previous work in that 
we are concerned with obtaining diversified 
translation rules using multiple different parsers 
(or parsing models) instead of a single parser (or 
parsing model). It can be regarded as an en-
hancement of previous studies. As shown in the 
following parts of this paper, it works very well 
with the existing techniques, such as rule com-
posing (Galley et al, 2006), SPMT models 
(Marcu et al, 2006) and rule extraction with k-
best parses (Venugopal et al, 2008). 
3 Translation Rule Extraction 
In this work, the issue of translation rule extrac-
tion is studied in the string-to-tree model pro-
posed by Galley et al (2006).  We choose this 
model because it has been shown to be one of 
the state-of-the-art syntax-based models, and has 
been adopted in the most successful systems in 
NIST 2009 MT evaluation.  
Typically, (string-to-tree) translation rules are 
learned from the word-aligned bilingual text 
whose target-side has been parsed using a syn-
tactic parser. As the basic unit of translation, a 
translation rule consists of sequence words or 
variables in the source language, and a syntax 
tree in the target language having words (termi-
nals) and variables (non-terminals) at leaves. 
Figure 1 shows the translation rules extracted 
from a word-aligned sentence pair with a target-
side parse tree. 
 
Figure 1: Translation rules extracted from a 
string-tree pair. 
1346
 
Figure 2: Rule extraction using two different parsers (Berkeley Parser and Collins Parser). The 
shaded rectangles denote the translation rules that can be extracted from the parse tree generated by 
one parser but cannot be extracted from the parse tree generated by the other parser. 
 
To obtain basic translation rules, the (minimal) 
GHKM extraction method proposed in (Galley 
et al 2004) is utilized. The basic idea of GHKM 
extraction is to compute the set of the mini-
mally-sized translation rules that can explain the 
mappings between source-language string and 
target-language tree while respecting the align-
ment and reordering between the two languages. 
For example, from the string-tree pair shown at 
the top of Figure 1, we extract the minimal 
GHKM translation rules r1-6. In addition to 
GHKM extraction, the SPMT models (Marcu et 
al., 2006) are employed to obtain phrasal rules 
that are not covered by GHKM extraction.  For 
example, rule r8  in Figure 1 is a SPMT rule that 
is not obtained in GHKM extraction. Finally, the 
rule composing method (Galley et al, 2006) is 
used to compose two or more minimal GHKM 
or SPMT rules having shared states to form lar-
ger rules. For example, rule r7 in Figure 1 is gen-
erated by composing rules r2 and r6. 
4 Differences in Coverage between Rule 
Extractions with Different Parsers 
As described above, translation rule extraction 
relies on the outputs (parse trees) of parsers. As 
different parsers generally have large diversity 
between their outputs, rule extractions with dif-
ferent parsers generally result in very different 
sets of rules. For example, Figure 2 shows the 
rule extractions on a word-aligned sentence pair 
having two target-trees generated by Berkeley 
Parser and Collins Parser, respectively. It is ob-
served that Figure 2 (a) and (b) cover different 
sets of rule due to the different target-trees used 
in rule extraction. Particularly, well-formed rules 
ra7-a9 are extracted in Figure 2 (a), while they do 
not appear in Figure 2 (b). Also, rules rb7-b9 in 
Figure 2 (b) have the similar situation. This ob-
servation gives us an intuition that there is a 
?complementarity? between the rules extracted 
using different parsers. 
1347
We also conduct a quantitative study to inves-
tigate the impact of using different parsers 
(Berkeley Parser and Collins Parser) on rule 
coverage. Tables 1 shows the statistics of the 
rules extracted from 370K Chinese-English par-
allel sentence pairs1 using the method described 
in Section 3. In addition to the total number of 
rules extracted, the numbers of phrasal rules and 
useful rules are also reported to indicate the rule 
coverage of a rule set. Here phrasal rule refers 
to the rule whose source-side and the yield of its 
target-side contains only one phrase each, with 
optional surrounding variables. According to 
(DeNeefe et al, 2007), the number of phrasal 
rules is a good indicator of the coverage of a rule 
set. useful rule refers to the rule that can be ap-
plied when decoding the test sentences 2 . As 
shown in Table 1, the two resulting rule sets on-
ly have about 70% overlaps (Column 4), and the 
rule coverage increases by about 20% when we 
combine them together (Column 5). This finding 
confirms that the rule coverage can be improved 
by using multiple different parsers in rule extrac-
tion. 
 # of rules # of phrasal 
rules 
# of  
useful rules
Berkeley 3,538,332 2,515,243 549,783 
Collins 3,526,166 2,481,195 553,893 
Overlap 2,542,380 1,907,521 386,983 
Union 4,522,118 3,088,920 716,693 
Table 1: Comparison of rule coverage between 
different rule sets. 
5 Translation Rule Extraction with 
Multiple Parsers 
5.1 Rule Extraction Algorithm 
Motivated by the above observations, we pro-
pose a rule extraction method to improve the 
rule coverage by using multiple parsers.  
Let <f, e, a> be a tuple of <source sentence, 
target sentence, bi-directional word alignments>, 
                                                 
1 LDC2005T10, LDC2003E07, LDC2003E14 and 
LDC2005T06 
2 In this experiment, the test sentences come from 
NIST 2004 and 2005 MT evaluation sets. It should be 
noted that due to the pruning in decoding we cannot 
count the exact number of rules that can be used dur-
ing decoding. In this work, we use an alternative ? 
the number of rules matched with test sentences ? to 
estimate an upper-bound approximately. 
and {P1, ..., PN} be N syntactic parsers in target-
language. The following pseudocode formulizes 
the algorithm for extracting translation rules 
from <f, e, a> using parsers {P1, ..., PN}, where 
Pi(e) returns the parse tree generated by the i-th 
parser Pi. Function GENERATERULES() com-
putes the set of rules for <f, ti, a> by using vari-
ous rule extraction methods, such as  the method 
described in Section 3. 
Multi-Parser based Rule Extraction  
Input: <f, e, a> and P = {P1, ..., PN} 
Output: rule set R 
1 Function MULTIPAREREXTRACTOIN(<f, e, a>, P )
2     for i = 1 to N do                           <  for each parser
3        ti = Pi(e)                                      <  target-tree 
4       Ri = GENERATERULES (f, ti, a) <  rule extraction 
5       R.append(Ri) 
6     return R 
7 Function GENERATERULES ( f, ti, a ) 
8     return rules extracted from <f, ti, a> 
5.2 Learning Rule Probabilities 
In multi-parser based rule extraction, more than 
one parse trees are used, and each of them is as-
sociated with a parsing confidence (e.g. genera-
tive probability of the tree). Ideally, if the parse 
trees used in rule extraction can be accurately 
weighted, the rule probabilities will be better 
estimated according to the parse weights, for 
example, the rules extracted from a parse tree 
having a low weight should be penalized accord-
ingly in the estimation of rule probabilities. Un-
fortunately, the tree probabilities are generally 
incomparable between different parsers due to 
the different parsing models used and ways of 
implementation. Thus we cannot use the poste-
rior probability of a rule?s target-side to estimate 
the fractional count (Mi and Huang, 2008; Liu et 
al., 2009), which is used in maximum-likelihood 
estimation of rule probabilities. In this work, to 
simplify the problem, we assume that all the 
parsers have the same and maximum degrees of 
confidence on their outputs. For a rule r ex-
tracted from a string-tree pair, the count of r is 
defined to be: 
1
( , )
( )
N
i
r i
c r
N
?== ?                     (1) 
where ( , )r i? is 1 if r is extracted by using the i-
th parser, otherwise 0.  
1348
Following Mi and Huang (2008)?s work, three 
conditional rule probabilities are employed for 
experimenting with our method. 
': ( ') ( )
( )Pr( | ( ))
( )
r root r root r
c rr root r
c r=
= ?        (2) 
': ( ') ( )
( )Pr( | ( ))
( )
r lhs r lhs r
c rr lhs r
c r=
= ?             (3) 
': ( ') ( )
( )Pr( | ( ))
( )
r rhs r rhs r
c rr rhs r
c r=
= ?            (4) 
where lhs(r) and rhs(r) are the source-hand and 
target-hand sides of r respectively, and root(r) is 
the root of r?s target-tree. 
5.3 Parser Indicator Features 
For each rule, we define N indicator features (i.e. 
( , )r i? ) to indicate a rule is extracted by using 
which parsers, and add them into the translation 
model. By training the feature weights with Min-
imum Error Rate Training (MERT), the system 
can learn preferences for different parsers auto-
matically. 
6 Experiments 
The experiments are conducted on Chinese-
English translation in a state-of-the-art string-to-
tree SMT system.  
6.1 Experimental Setup 
Our bilingual data consists of 370K sentence 
pairs (9M Chinese words + 10M English words) 
which have been used in the experiment in Sec-
tion 4. GIZA++ is employed to perform the bidi-
rectional word alignment between the source and 
target sentences, and the final word alignment is 
generated using the inter-sect-diag-grow method. 
A 5-gram language model is trained on the tar-
get-side of the bilingual data and the Xinhua 
portion of English Gigaword corpus. The devel-
opment data set comes from NIST MT 2003 
evaluation set. To speed up MERT, sentences 
with more than 20 Chinese words are removed. 
The test sets are the NIST MT evaluation sets of 
2004 and 2005.  
Our baseline MT system is built based on the 
string-to-tree model proposed in (Galley et al, 
2006). In this system, both of minimal GHKM 
(Galley et al, 2004) and SPMT rules (Marcu et 
al., 2006) are extracted from the bilingual corpus, 
and the composed rules are generated by com-
posing two or three minimal GHKM and SPMT 
rules3. We use a CKY-style decoder with cube 
pruning (Huang and Chiang, 2007) and beam 
search to decode new Chinese sentences. By de-
fault, the beam size is set to 30. For integrating 
n-gram language model into decoding efficiently, 
rules containing more than two variables or 
source word sequences are binarized using the 
synchronous binarization method (Zhang et al, 
2006; Xiao et al, 2009).  
The system is evaluated in terms of the case-
insensitive NIST version BLEU (using the 
shortest reference length), and statistical signifi-
cant test is conducted using the re-sampling me-
thod proposed by Koehn (2004). 
6.2 The Parsers 
Four syntactic parsers are chosen for the ex-
periments. They are Stanford Parser4, Berkeley 
Parser 5 , Collins Parser (Dan Bikel?s reimple-
mentation of Collins Model 2) 6  and Charniak 
Parser7. The former two are state-of-the-art non-
lexicalized parsers, while the latter two are state-
of-the-art lexicalized parsers. All the parsers are 
trained on sections 02-21 of the Wall Street 
Journal (WSJ) Treebank, and tuned on section 
22. Table 2 summarizes the performance of the 
parsers. 
Parser Recall Precision F1 
Stanford 86.29% 87.21% 86.75% 
Berkeley 90.18% 90.45% 90.32% 
Collins 89.14% 88.85% 88.99% 
Charniak 89.99% 90.28% 90.13% 
Table 2: Performance of the four parsers on sec-
tion 23 of the WSJ Treebank. 
We parse the target-side of the bilingual data 
using the four parsers individually. From the 1-
best parses generated by these parsers, we obtain 
four baseline rule sets using the method de-
scribed in Section 3, as well as the rule sets usi- 
                                                 
3 Generally a higher baseline can be obtained by 
combining more (unit) rules. However, we find that 
using more composed rules does not affect the impact 
of using multiple parsers. Thus, we choose this set-
ting in order to finish all experiments in time. 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://code.google.com/p/berkeleyparser/ 
6 http://www.cis.upenn.edu/~dbikel/download.html 
7 http://www.cs.brown.edu/people/ec/#software 
1349
Rule Coverage BLEU4 (%)  Rule set 
# of rules # of  
phrasal rules
# of  
useful rules 
Dev. MT04 MT05 
Stanford (S) 3,679 K 2,581 K 573 K 39.36 36.02 36.98 
Berkeley (B) 3,538 K 2,515 K 549 K 39.32 36.05 36.98 
Collins (Co) 3,526 K 2,481 K 553 K 39.16 36.07 36.91 
B
as
el
in
e 
Charniak (Ch) 3,450 K 2,435 K 540 K 39.24 35.90 36.89 
S + B 4,567 K 3,105 K 726 K 39.87+ 36.57+ 37.47+ 
S + Co 4,734 K 3,202 K 752 K 39.94+ 36.57+ 37.52+ 
S + Ch 4,764 K 3,258 K 751 K 40.01+ 36.51 37.59+ 
B + Co 4,522 K 3,088 K 716 K 39.84+ 36.60+ 37.46+ 
B +  Ch 4,562 K 3,129 K 717 K 39.81+ 36.49 37.41 
2 
pa
rs
er
s 
Co + Ch 4,592 K 3,125 K 727 K 39.75 36.55+ 37.43+ 
S + B + Co 5,331 K 3,543 K 852 K 40.14++ 36.83++ 37.78++ 
S + B + Ch 5,380 K 3,590 K 854 K 40.05+ 36.82++ 37.70+ 
S + Co + Ch 5,551 K 3,663 K 877 K 40.35++ 36.70+ 37.70+ 3 p
ar
se
rs
 
B + Co + Ch 5,294 K 3,544 K 840 K 40.04+ 36.76+ 37.65+ 
4 S + B + Co + Ch 6,005 K 3,940 K 958 K 40.28++ 36.99++ 37.89++ 
Table 5: Evaluation results. + or ++ = significantly better than all the baseline systems (using single 
parser) at the 95% or 99% confidence level. 
 
 Stanford Berkeley Collins Charniak
Stanford 100% 76.72% 73.32% 74.89% 
Berkeley 76.72% 100% 75.69% 76.76% 
Collins 73.32% 75.69% 100% 74.84% 
Charniak 74.89% 76.76% 74.84% 100% 
Table 3: Agreement between different parsers. 
 
ng the multi-parser based rule extraction method.  
Before conducting primary experiments, we first 
investigate the differences between the 1-best 
outputs of the parsers. Table 3 shows the agree-
ment between each pair of parsers. Here the de-
gree of agreement shown in each cell is com-
puted by using one parser?s output as a good 
standard to evaluate the other parser?s output in 
terms of F1 score, and a higher agreement score 
(i.e. F1 score) means that the 1-best outputs of 
the two parsers are more similar to each other. 
We see that the agreement scores between dif-
ferent parsers are always below 80%. This result 
reflects a large diversity in parse trees generated 
by different parsers, and thus confirms our ob-
servations in Section 4. 
We also examine the ?complementarity? be-
tween the baseline rule sets generated by using 
different parsers individually. Table 4 shows the 
results, where the degree of ?complementarity? 
between two rule sets is defined as the percent-
age of the rules in one rule set that are not cov-
ered by the other rule set. It can be regarded as a 
measure of the disagreement between two rule 
sets, and a higher number indicates large ?com-
plementarity?. For example, in Row 2, Column 3 
(Table 4), ?25.09%? means that 25.09% rules in 
the first rule set (using Stanford Parser) are not 
covered by the second rule set (using Berkeley 
Parser). Table 4 shows that there is always a dis-
agreement of over 25% between different rule 
sets. These results indicate that using different 
parsers can lead to a relatively large ?comple-
mentarity? between the rule sets.  
 Stanford Berkeley Collins Charniak
Stanford 0% 25.09% 29.91% 31.43% 
Berkeley 27.98% 0% 27.90% 29.68% 
Collins 32.84% 28.15% 0% 30.89% 
Charniak 35.70% 31.43% 32.37% 0% 
Table 4: Disagreement between the rule sets ob-
tained using different parsers individually. 
6.3 Evaluation of Translations 
We then study the impact of multi-parser based 
rule extraction on translation accuracy.  Table 5 
shows the BLEU scores as well as the rule cov-
erage for various rule extraction methods. We 
see, first of all, that the rule coverage is im-
proved significantly by multi-parser based rule 
extraction. Compared to the baseline method (i.e. 
single-parser based rule extraction), the multi-
parser based rule extraction achieves over 20% 
coverage improvements when only two parsers 
are used, even yields gains of over 50 percentage 
1350
points when all the four parsers are used together. 
Also, BLEU score is improved by multi-parser 
based rule extraction. When two parsers are em-
ployed in rule extraction, there is generally a 
gain of over 0.4 BLEU points on both MT04 and 
MT05 test sets. Further improvements are 
achieved when more parsers are involved. On 
both test sets, using three parsers in rule extrac-
tion generally yields a +0.7 BLEU improvement, 
and using all the parsers together yields a +0.9 
BLEU improvement which is the biggest im-
provement achieved in this set of experiment. 
All these results show that multi-parser based 
rule extraction is an effective way to improve the 
rule coverage as well as the BLEU score of the 
syntax-based MT system. 
An interesting finding is that there seems no 
significant differences in BLEU scores between 
the baseline systems (using single parsers), 
though the parsing performance of the corre-
sponding parsers is very different from each 
other. For example, the MT performance corre-
sponding to Berkeley Parser is very similar to 
that corresponding to Stanford Parser despite a 
4-point difference in F1 score between the two 
parsers. Another example is that Charniak parser 
performs slightly worse than the other three on 
MT task, though it achieves the 2nd best parsing 
performance in all the parsers. This interesting 
finding shows that the performance of syntax-
based MT systems is not very sensitive to the 
parsing performance of the parsers used in rule 
extraction. 
6.4 Preferences for Parsers 
We also investigate the preferences for different 
parsers in our system. Table 6 shows the weights 
of the parser indicator features learned by 
MERT, as well as the number of edges gener-
ated by applying the rules corresponding to dif-
ferent parsers during decoding. Both of the met-
rics are used to evaluate the contributions of the 
parsers to MT decoding. We see that though 
Stanford Parser and Berkeley Parser are shown 
to be relatively more preferred by the decoder, 
there are actually no significant differences in 
the degrees of the contributions of different 
parsers. This result also confirms the fact ob-
served in Table 5 that the MT system does not 
have special preferences for different parsers. 
 
Indicator Weight # of edges 
(Dev.) 
# of edges 
 (MT04) 
# of edges 
(MT05) 
Stanford 0.1990 7.7 M 169.2 M 101.7 M
Berkeley 0.1982 7.7 M 166.3 M 100.2 M
Collins 0.1690 6.9 M 149.9 M   93.1 M
Charniak 0.1729 7.1 M 156.5 M   97.2 M
Table 6: Preferences for different parsers. 
Though Table 6 provides some information 
about the contributions of different parsers, it 
still does not answer how often these rules are 
really used to generate final (1-best) translation. 
Table 7 gives an answer to this question. We see 
that, following the similar trend in Table 5, dif-
ferent parsers have nearly equal contributions in 
generating final translation. 
Indicator # of rules 
used in 1-best  
(Dev.) 
# of rules 
used in 1-best  
(MT04) 
# of rules 
used in 1-best  
(MT05) 
Stanford     2,410 23,513 14,357 
Berkeley     2,455 23,878 14,670 
Collins     2,309 22,654 13,815 
Charniak     2,269 22,406 13,731 
Table 7: Numbers of rules used in generating 
final (1-best) translation. 
6.5 Rule Extraction with k-best Parses 
We also conduct experiments to compare the 
effectiveness of multi-parser based rule extrac-
tion and rule extraction with k-best parses gener-
ated by a single parser. As Berkeley parser is 
one of the best-performing parsers in previous 
experiments, we employ it to generate k-best 
parses in this set of experiment. As shown in 
Figure 3, both of the methods improve the 
BLEU scores by enlarging the set of parse trees 
used in rule extraction. Compared to k-best ex-
traction, multi-parser extraction shows consiste- 
 36.8
 37
 37.2
 37.4
 37.6
 37.8
 38
 38.2
3.5 5.0 6.5
B
LE
U
4(
%
)
# of rules (million)
1-best
4-best
10-best
20-best
30-best 50-best
2 parsers
3 parsers
4 parsers
multi-parser extraction
k-best extraction
 
Figure 3: Multi-parser based rule extraction vs. 
rule extraction with k-best parses (MT05). 
1351
ntly better BLEU scores. Using 4 different pars-
ers, it achieves an improvement of 0.6 BLEU 
points over k-best extraction where even 50-best 
parses are used. 
Finally, we extend multi-parser based rule ex-
traction to extracting rules from the k-best parses 
generated by multiple parsers. Figure 4 shows 
the results on ?S + B + Co + Ch? system. We see 
that multi-parser based rule extraction can bene-
fit from k-best parses, and yields a modest (+0.2 
BLEU points) improvement when extracting 
from 10-best parses. However, since k-best ex-
traction generally results in much slower extrac-
tion speed, it might not be a good choice to use 
k-best parses to improve our method in practice. 
 37.7
 37.8
 37.9
 38
 38.1
 38.2
 38.3
 38.4
 38.5
5.5 6.5 7.5 8.5
B
LE
U
4(
%
)
# of rules (million)
1-best
2-best
5-best
10-best
multi-parser + k-best extraction
 
Figure 4: Multi-parser based rule extraction & 
rule extraction with k-best parses (MT05). 
7 Discussion and Future Work 
In this work, all the parsers are trained using the 
same treebank. To obtain diversified parse trees 
for multi-parser based rule extraction, an alterna-
tive way is to learn parsers on treebanks anno-
tated by different organizations (e.g. Penn Tree-
bank and ICE-GB corpus). Since different tree-
banks can provide us with more diversity in 
parsing, we believe that our system can benefit a 
lot from the parsers that are learned on multiple 
different treebanks individually. But here is a 
problem that due to the different annotation 
standards used, there is generally an incompati-
bility between treebanks annotated by different 
organizations. It will result in that we cannot 
straightforwardly mix the resulting rule sets (or 
heterogeneous grammars for short) for probabil-
ity estimation as well as the use for decoding. To 
solve this problem, a simple solution might be 
that we transform the incompatible rules into a 
unified form. Alternatively, we can use hetero-
geneous decoding (or parsing) techniques (Zhu 
et al, 2010) to make use of heterogeneous 
grammars in the stage of decoding. Both topics 
are very interesting and worth studying in our 
future work.  
Besides k-best extraction, our method can also 
be applied to other rule extraction schemes, such 
as forest-based rule extraction. As (Mi and 
Huang, 2008) has shown that forest-based ex-
traction is more effective than k-best extraction 
in improving translation accuracy, it is expected 
to achieve further improvements by using multi-
parser based rule extraction and forest-based rule 
extraction together. 
8 Conclusions  
In this paper, we present a simple and effective 
method to improve rule coverage by using mul-
tiple parsers in translation rule extraction. Ex-
perimental results show that  
z Using multiple parsers in rule extraction 
achieves large improvements of rule cover-
age over the baseline method where only a 
single parser is used, as well as a +0.9 
BLEU improvement on both NIST 2004 
and 2005 test corpora. 
z The MT system can be further improved by 
using multiple parsers and k-best parses to-
gether. However, with the consideration of 
extraction speed, it might not be a good 
choice to use k-best parses to improve mul-
ti-parser based rule extraction in practice. 
z The MT performance is not influenced by 
the parsing performance of the parsers used 
in rule extraction very much. Actually, the 
MT system does not show different prefer-
ences for different parsers. 
Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and 
the Fundamental Research Funds for the Central 
Universities (N090604008). The authors would 
like to thank the anonymous reviewers and Ton-
gran Liu for their pertinent comments for im-
proving the early version of this paper, and Ru-
shan Chen for building parts of the baseline sys-
tem. 
1352
References 
Brooke Cowan, Ivona Ku?erov? and Michael Collins. 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP 2006, pages 232-
241. 
Steve DeNeefe, Kevin Knight, Wei Wang and Daniel 
Marcu. 2007. What Can Syntax-based MT Learn 
from Phrase-based MT? In Proc. of EMNLP 2007, 
pages 755-763. 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. In Proc. of ACL 2005, Ann 
Arbor, Michigan, pages 541-548. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Michel Galley, Mark Hopkins, Kevin Knight and 
Daniel Marcu. 2004. What's in a translation rule? 
In Proc. of HLT-NAACL 2004, Boston, USA, 
pages 273-280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Da-
niel Marcu, Steve DeNeefe, Wei Wang and Igna-
cio Thayer. 2006. Scalable inferences and training 
of context-rich syntax translation models. In Proc. 
of COLING/ACL 2006, Sydney, Australia, pages 
961-968. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. In Proc. of ACL 2007, Prague, Czech Re-
public, pages 144-151. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical syntax-directed translation with ex-
tended domain of locality. In Proc. of AMTA 2006, 
pages 66-73. 
Philipp Koehn. 2004. Statistical Significance Tests 
for Machine Translation Evaluation. In Proc. of 
EMNLP 2004, Barcelona, Spain, pages 388-395. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of COLING/ACL 2006, Syd-
ney, Australia, pages 609-616. 
Yang Liu, Yajuan L? and Qun Liu. 2009. Improving 
Tree-to-Tree Translation with Packed Forest. In 
Proc. of ACL 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phras-
es. In Proc. of EMNLP 2006, Sydney, Australia, 
pages 44-52. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. In Proc. of EMNLP 
2008, pages 206-214. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of ACL 2005, pages 
271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation al-
gorithm with a target dependency language model. 
In Proc. of ACL/HLT 2008, pages 577-585. 
Ashish Venugopal, Andreas Zollmann, Noah A. 
Smith and Stephan Vogel. 2008. Wider Pipelines: 
K-best Alignments and Parses in MT Training. In 
Proc. of AMTA 2008, pages 192-201. 
Wei Wang, Kevin Knight and Daniel Marcu. 2007. 
Binarizing Syntax Trees to Improve Syntax-Based 
Machine Translation Accuracy. In Proc. of 
EMNLP-CoNLL 2007, Prague, Czech Republic, 
pages 746-754. 
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and 
Ming Zhou. 2009. Better Synchronous Binariza-
tion for Machine Translation. In Proc. of EMNLP 
2009, Singapore, pages 362-370. 
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical machine translation model. In 
Proc. of ACL 2001, pages 132-139. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006, 
New York, USA, pages 256- 263. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation 
Model. In Proc. of ACL/HLT 2008, pages 559-567. 
Muhua Zhu, Jingbo Zhu and Tong Xiao. 2010. Het-
erogeneous Parsing via Collaborative Decoding. In 
Proc. of COLING 2010. 
1353
Coling 2010: Poster Volume, pages 1541?1549,
Beijing, August 2010
Automatic Treebank Conversion via Informed Decoding
Muhua Zhu
Natural Language Processing Lab.
Northeastern University
zhumuhua@gmail.com
Jingbo Zhu
Natural Language Processing Lab.
Northeastern University
zhujingbo@mail.neu.edu.cn
Abstract
In this paper, we focus on the challenge
of automatically converting a constituency
treebank (source treebank) to fit the stan-
dard of another constituency treebank (tar-
get treebank). We formalize the conver-
sion problem as an informed decoding
procedure: information from original an-
notations in a source treebank is incorpo-
rated into the decoding phase of a parser
trained on a target treebank during the
parser assigning parse trees to sentences in
the source treebank. Experiments on two
Chinese treebanks show significant im-
provements in conversion accuracy over
baseline systems, especially when training
data used for building the parser is small
in size.
1 Introduction
Recent years have seen extensive applications of
machine learning methods to natural language
processing problems. Typically, increase in the
scale of training data boosts the performance of
machine learning methods, which in turn en-
hances the quality of learning-based NLP systems
(Banko and Brill, 2001). However, annotating
data by human is time consuming and labor inten-
sive. For this reason, human-annotated corpora
are considered as the most valuable resource for
NLP.
In practice, there often exist more than one cor-
pus for the same NLP tasks. For example, for
constituent syntactic parsing (Collins, 1999; Char-
niak, 2000; Petrov et al, 2006) for Chinese, in ad-
dition to the most popular treebank Chinese Tree-
bank (CTB) (Xue et al, 2002), there are also
other treebanks such as Tsinghua Chinese Tree-
bank (TCT) (Zhou, 1996). For the purpose of
full use of readily available human annotations
for the same tasks, it is significant if such cor-
pora can be used jointly. Such attempt is es-
pecially significant for some languages that have
limited size of labeled data. At first sight, a di-
rect combination of multiple corpora is a way to
this end. However, corpora created for the same
NLP tasks are generally built by different orga-
nizations. Thus such corpora often follow dif-
ferent annotation standards and/or even different
linguistic theories. We take CTB and TCT as
a case study. Although both CTB and TCT are
Chomskian-style treebanks, they have annotation
divergences in at least two dimensions: a) CTB
and TCT have dramatically different tag sets, in-
cluding parts-of-speech and grammar labels, and
the tags cannot be mapped one to one; b) CTB and
TCT have distinct hierarchical structures. For ex-
ample, the Chinese words ??? (Chinese) ??
(traditional) ?? (culture)? are grouped as a flat
noun phrase according to the CTB standard (right
side in Fig. 1), but in TCT, the last two words are
instead grouped together beforehand (left side in
Fig. 1). The differences cause such treebanks of
different annotation standard to be generally used
independently.
In this paper, we focus on unifying multiple
constituency treebanks of distinct annotation stan-
dards through treebank conversion. The task of
treebank conversion is defined to be conversion of
annotations in one treebank (source treebank) to
1541
np
nS
??
np
a
??
n
??
NP
NR
??
NN
??
NN
??
??????
(Chinese) (traditional) (culture)
Figure 1: Example tree fragments with TCT (left)
and CTB (right) annotations
fit the standard of another treebank (target tree-
bank). To this end, we propose a language in-
dependent approach called informed decoding 1,
in which a parser trained on a target treebank au-
tomatically assigns new parse trees to sentences
in a source treebank with the aid of informa-
tion derived from annotations in the source tree-
bank. We conduct experiments on two open Chi-
nese treebanks 2: CTB and TCT. Experimental re-
sults show that our approach achieves significant
improvements over baseline systems, especially
when training data used for building the parser is
small in size.
The rest of the paper is structured as follows. In
Section 2 we describe previous work on treebank
conversion. In Section 3, we describe in detail the
informed decoding approach. Section 4 presents
experimental results which demonstrate the effec-
tiveness of our approach. Finally, Section 5 con-
cludes our work.
2 Related Work
Previous work on treebank conversion can be
grouped into two categories according to whether
grammar formalisms of treebanks are identical.
One type focuses on converting treebanks of dif-
ferent grammar formalisms. Collins et al (1999)
1The terminology decoding is referred to the parsing
phase of a parser.
2Note that although we use Chinese treebanks, our ap-
proach is language independent.
addressed constituent syntactic parsing on Czech
using a treebank converted from a Prague depen-
dency treebank, where conversion rules derived
from head-dependent pairs and heuristic rules are
applied. Xia and Palmer (2001) compared three
algorithms for conversion from dependency struc-
tures to phrase structures. The algorithms ex-
panded each node in input dependency structures
into a projection chain, and labeled the newly in-
serted node with syntactic categories. The three
algorithms differ only in heuristics adopted to
build projection chains. Xia et al (2008) auto-
matically extracted conversion rules from a tar-
get treebank and proposed strategies to handle the
case when more than one conversion rule are ap-
plicable. Instead of using conversion rules, Niu
et al (2009) proposed to convert a dependency
treebank to a constituency one by using a parser
trained on a constituency treebank to generate k-
best lists for sentences in the dependency tree-
bank. Optimal conversion results are selected
from the k-best lists. There also exists work in the
reverse direction: from a constituency treebank to
a dependency treebank (Nivre, 2006; Johansson
and Nugues, 2007).
Relatively few efforts have been put on conver-
sion between treebanks that have the same gram-
mar formalisms but follow different annotation
standards. Wang et al (1994) applied a similar
framework as in (Niu et al, 2009) to convert from
a simple constituency treebank to a more infor-
mative one. The basic idea is to apply a parser
built on a target treebank to generate k-best lists
for sentences in the source treebank. Then, a
matching metric is defined on the number of iden-
tical bracketing spans between two trees. Such a
function computes a score for each parse tree in
a k-best list and its corresponding parse tree in
the source treebank. Finally, the parse tree with
the highest score in a k-best list is selected to be
the conversion result. The difference between our
work and (Wang et al, 1994) is that, instead of us-
ing trees from the source treebank to select parse
trees from k-best lists, we propose to use such
trees to guide the decoding phase of the parser
built on the target treebank. Making use of the
source treebank in such a novel way is believed to
be the major contribution of our work.
1542
3 Treebank Conversion via Informed
Decoding
The task of treebank conversion is defined to con-
vert parse trees in a source treebank to fit the stan-
dard of a target treebank. In the informed de-
coding approach, treebank conversion proceeds in
two steps: 1) build a parser on a target treebank;
2) apply the parser to decode sentences in a source
treebank with the aid of information derived from
the source treebank. For convenience, parse trees
in a source treebank are referred to as source trees
and corresponding, trees from a target treebank
are referred to as target trees. Moreover, a parser
built on a target treebank is referred to as target
parser. In the following sections, we first describe
motivation of our work and then present details of
the informed decoding approach.
3.1 Motivation
We use the example in Fig. 2 to illustrate why
original annotations in a source treebank can help
in treebank conversion. The figure depicts three
tree fragments for the Chinese words ? (pay) ?
(already) ? (one) ? (day) ? (of) ??(salary),
among which Fig. 2(a) and Fig. 2(b) are tree frag-
ments of the CTB standard and Fig. 2(c) is a tree
fragment of the TCT standard. From the fig-
ure, we can see that these Chinese words actu-
ally have (at least) two plausible interpretations
of the meaning. In Fig. 2(a), the words mean
pay salary for one-day work while in Fig. 2(b),
the words mean spend one day on paying salary.
If Fig. 2(c) is a source tree to be converted into
the CTB standard, then Fig. 2(b) will be rejected
since it conflicts with Fig. 2(c) with respect to tree
structures. Note that structures reflect underlying
sentence meaning. On the other hand, although
Fig. 2(a) also has (minor) differences in tree struc-
tures from Fig. 2(c), it is preferred as the conver-
sion result3. From the example we can get in-
spired by the observation that original annotations
in a source treebank are informative and necessary
to converting parse trees in the source treebank.
In general, conversion like that from Fig. 2(c)
3Note that we don?t deny existence of annotation distinc-
tions between the treebanks, but we aim to make use of what
they both agree on. We assume that consensus is the major-
ity.
to Fig. 2(a) requires sentence-specific conversion
rules which are difficult to obtain in practice. In
order to make use of information provided by
original annotations in a source treebank, Wang
et al (1994) proposed a selecting-from-k-best ap-
proach where source trees are used to select one
?optimal? parse tree from each k-best list gener-
ated by a target parser. In this paper, we instead in-
corporate information of original annotations into
the parsing phase. The underlying motivation is
two-fold:
? The decoding phase of a parser is essentially
a search process. Due to the extreme mag-
nitude of searching space, pruning of search
paths is practically necessary. If reliable in-
formation is provided to guide the pruning of
search paths, more efficient parsing and bet-
ter results are expected.
? Selecting-from-k-best works on the basis of
k-best lists. Unfortunately, we often see very
few variations in k-best lists. For exam-
ple, 50-best trees present only 5 to 6 varia-
tions (Huang, 2008). The lack of diversi-
ties in k-best lists makes information from
the source treebank less effective in selecting
parse trees. By contrast, incorporating such
information into decoding makes the infor-
mation affect the whole parse forest.
3.2 Formalization of Information from
Source Treebank
In this paper, information from a source treebank
translates into two strategies which help a target
parser to prune illegal partial parse trees and to
rank legal partial parse trees higher. Following are
the two strategies:
? Pruning strategy: despite distinctions exist-
ing between annotation standards of a source
treebank and a target treebank, a source tree-
bank indeed provides treebank conversion
with indicative information on bracketing
structures and grammar labels. So when a
partial parse tree is generated, it should be
examined against the corresponding source
tree. Unless the partial parse tree does not
conflict with any constituent in the source
tree, it should be pruned out.
1543
VP
VV
?
AS
?
NP
DNP
QP
CD
?
CLP
M
?
DEC
?
NN
??
VP
DVP
VP
VV
?
AS
?
QP
CD
?
CLP
M
?
DEC
?
NP
NN
??
vp
v
?
uA
?
np
mp
m
?
qN
?
uJDE
?
n
??
(a) (b) (c)
???????
(pay) (already) (one) (day) (of) (salary)
Figure 2: tree fragments of words???????: (a) and (b) show two plausible tree fragments of
the words using the CTB standard; (c) shows a tree fragment of the TCT standard which has the same
interpretation as (a).
6
4
1
5
2 3
[1,3],[2,3],[1,1]
[1,1]
[2,3],[2,2],[3,3]
[1,1]
[2,2]
[3,3]
Figure 3: Constituent set of a synthetic parse tree
? Rescoring strategy: in practice, decoding is
often a local optimal search process. In some
cases even if a correct parse tree exits in the
parse forest, parsers may fail to rank it to the
top position. Rescoring strategy is used to
increase scores for partial parse trees which
are confidently thought to be valid.
3.2.1 Pruning Strategy
The pruning strategy used in this paper is based
on the concept of conflict which is defined in two
dimensions: structures and grammar labels. Since
a tree structure can be equivalently represented
as its span (interval of word indices) set, we can
check whether two trees conflict by checking their
spans. See Fig. 3 for an illustration of spans of a
tree. Following are criteria determining whether
two trees conflict in their structures.
? If one node in tree A is raised to be a child
of the node?s grandfather in tree B, and the
grandfather has more than two children, then
tree A and tree B conflict in structures.
? If tree A has a span [a, b] and tree B has a
span [m,k] and these two spans satisfy the
condition of either a < m ? b < k or m <
a ? k < b, then tree A and B conflict in
structures.
Fig. 4 illustrates criteria mentioned above, where
Fig. 4(a) is compatible (not conflict) with Fig. 4(b)
although they have different structures. But
Fig. 4(a) conflicts with Fig. 4(c) (according to cri-
terion 1; node 3 is raised) and (d) (according to
criterion 2).
1544
65
1 2 3
4
7
6
5
1 2
3
4
6
5
1 2
3 4
7
5
1 2
6
3 4
(a) (b) (c) (d)
Figure 4: Illustrating example of the concept of conflict: (a) and (b) are compatible (not conflict); (a)
conflicts with (c) (condition 1) and (d) (condition 2)
For the dimension of grammar labels, we manu-
ally construct a mapping between label sets (POS
tags excluded) of source and target treebanks.
Such a mapping is frequently a many-to-many
mapping. Two labels are said to be conflicting if
they are from different label sets and they cannot
be mapped.
By combining these two strategies, two parse
trees (of different standards) which yield the same
sentence are said to be conflicting if they conflict
in both structures and labels. Note that we de-
scribe pruning strategy for the case of two parse
trees. In informed decoding process, this strategy
is actually applied to every partial parse tree gen-
erated during decoding.
3.2.2 Rescoring Strategy
As mentioned above, despite that the pruning
strategy helps in improving conversion accuracy,
we are faced with the problem of how to rank
valid parse trees higher in a parse forest. To solve
the problem, we adjust the scores of those partial
parse trees that are considered to be confidently
?good?. The criteria which is used to judge ?good-
ness? of a partial parse are listed as follows:
? The partial parse tree can find in the source
tree a constituent that has the same structure
as it.
? When the first criterion is satisfied, gram-
mar categories of this partial parse should not
conflict with the grammar categories of its
counterpart.
In practice, we use a parameter ? to adjust the
score.
Pnew(e) = ? ? P (e) (1)
Here e represents any partial tree that is rescored,
and P (e) and Pnew(e) refer to original and new
scores, respectively.
3.3 Parsing Model
Theoretically all parsing models are applicable in
informed decoding, but we prefer to adopt a CKY-
style parser for two reasons: CKY style parsers
are dynamically bottom-up and always have edges
(or parsing items) belonging to the same span
stacked together in the same chart4 cell. The
property of CKY-style parsers being dynamically
bottom-up can make the pruning strategy efficient
by avoiding rechecking subtrees that have already
been checked. The property of stacking edges in
the same chart cell makes CKY-style parsers eas-
ily portable to the situaiton of informed decod-
ing. In this paper, Collins parser (Collins, 1999)
is used. Algorithm 1 presents the extended ver-
sion of the decoding algorithm used in Collins
parser. What the algorithm needs to do is to
generate edges for each span. And before edges
are allowed to enter the chart, pruning conditions
4Data structure used to store paring items that are not
pruned
1545
Algorithm 1 CKY-style decoding
Argument: a parsing decoder
a sentence to be parsed and corresponding
source tree
Begin
Steps:
1. initialization steps
2. for span from 2 to sentence length do
for start from 1 to (sentence length-span+1) do
end := (start + span - 1)
for each edge e for span [start, end] do
generate(e, start, end)
prune(e, start, end)
rescore(e, start, end)
add edge(e, start, end)
End
Subroutine:
generate: generates an edge which belongs to the
span [start, end].
prune: apply pruning strategy to check whether the
edge should be pruned.
rescore: apply rescoring strategy to weight the edge.
add edge: add the edge into chart.
should be checked in prune subroutine and rescor-
ing should be conducted in rescore subroutine
with respect to the corresponding source tree.
4 Experiments
4.1 Experimental Setup
In this paper, we conduct two groups of experi-
ments in order to evaluate 1) treebank conversion
accuracy and 2) how much newly generated data
can boost syntactic parsing accuracy. For the ex-
periments of treebank conversion, Penn Chinese
Treebank (CTB) 5.1 is used as the target treebank.
That is, the CTB standard is the one we are inter-
ested in. Following the conventional data split-
ting of CTB5.1, articles 001-270 and 400-1151
(18,100 sentences, 493,869 words) are used for
training, articles 271-300 (348 sentences, 8,008
words) are used as test data, and articles 301-325
(352 sentences, 6,821 words) are used as devel-
opment data 5. Moreover, in order to directly
evaluate conversion accuracy, we randomly sam-
pled 150 sentences from the CTB test set and have
three annotators manually label sentences of these
parse trees according to the standard of Tsinghua
Chinese Treebank (TCT). Thus each of the 150
sentences has two parse trees, following the CTB
5Development set is not used in this paper.
and TCT standard, respectively. For convenience
of reference, the set of 150 parse trees of the
CTB standard is referred to as Sample-CTB and
its counterpart which follows the TCT standard is
referred to as Sample-TCT. In such setting, the ex-
periments of treebank conversion is designed to
use the informed decoding approach to convert
Sample-TCT to the standard of CTB and conver-
sion results are evaluated with respect to Sample-
CTB. The CTB training data (or portion of it) is
used as target training data on which parsers are
trained for conversion.
For the experiments of syntactic parsing, the
TCT corpus is used as the source treebank.
The TCT corpus contains 27,268 sentences and
587,298 words, which are collected from the lit-
erature and newswire domains. In this group of
experiments, the CTB training data is again used
as target training data and the whole TCT cor-
pus is converted using the informed decoding ap-
proach. The newly-gained parse trees are used as
additional training data for syntactic parsing on
the CTB test data. One thing worth noting in the
experiments is that, using Collins parser to con-
vert the TCT corpus requires Part-of-Speech tags
of the CTB standard be assigned to sentences in
TCT ahead of conversion being conducted. To this
end, instead of using POS taggers, we use the la-
bel correspondence learning method described in
(Zhu and Zhu, 2009) in order to get high POS tag-
ging accuracy.
For all the experiments in this paper, bracketing
F1 is used as the performance metric, provided by
the EVALB program 6. ? in Eq.1 is set to 3.0 since
it provides best conversion results in our experi-
ments.
4.2 Experiments on Conversion
The setup of conversion experiments is described
above. In the experiments, we use two representa-
tive baseline systems. One, named directly pars-
ing (DP) converts Sample-TCT by directly pars-
ing using Collins parser which is trained on tar-
get training data, and the other is the method pro-
posed in (Wang et al, 1994) (hereafter referred
to as Wang94). For the latter baseline, we use
Berkeley parser (Petrov et al, 2006) instead of
6http://nlp.cs.nyu.edu/evalb
1546
Ratio 20% 40% 60% 80% 100%
DP 73.19 75.21 79.43 80.64 81.40
Wang94 75.00 76.82 78.08 81.50 82.47
This paper 82.71 83.00 83.37 84.80 84.34
Table 1: Conversion accuracy with varying size of
target training data
Collins parser. The reason is that we want to build
a strong baseline since Berkeley parser is able
to generate better k-best lists than Collins parser
does (Zhang et al, 2009). In detail, Wang94 pro-
ceeds in two steps: 1) use Berkeley parser to gen-
erate k-best lists for sentences in Sample-TCT; 2)
select a parse tree from each k-best list with re-
spect to original annotations in Sample-TCT. Here
we set k to 50. Table 1 reports F1 scores of the
baseline systems and our informed decoding ap-
proach with varying size of target training data.
The first row of the table represents fractions of
the CTB training data which are used as target
training data. For example, 40% means 7,240
parse trees (of 18,100) in the CTB training data
are used. To relieve the effect of ordering, we
randomly shuffled parse trees in the CTB training
data.
From the table, we can see that our ap-
proach performs significantly better than DP and
Wang94. In detail, when 100% CTB training data
is used as target training data, 2.95% absolute im-
provement is achieved. When the size of target
training data decreases, absolute improvements of
our approach over baseline systems are further en-
larged. More interestingly, decreasing in target
training data only results in marginal decrement
in conversion accuracy of our approach. This is of
significant importance in the situation where tar-
get treebank is small in size.
In order to evaluate the accuracy of conversion
methods on different span lengths, we compare
the results of Wang94 and informed decoding pro-
duced by using 100% CTB training data. Table 2
shows the statistics.
From the results we can see that our ap-
proach performs significantly better on long spans
and achieves marginally lower accuracy on small
ones. But notice that the informed decoding ap-
proach is implemented on the base of Collins
Span Length 2 4 6 8 10
Wang94 82.45 83.97 80.72 77.83 71.72
This paper 83.72 82.95 79.84 77.27 70.67
Span Length 12 14 16 18 20
Wang94 75.29 68.00 77.27 70.83 76.66
This paper 71.79 75.00 86.27 80.00 80.00
Table 2: Conversion accuracy on different span
lengths
Category ADJP VCD CP DNP ADVP
Wang94 79.62 57.14 65.43 84.76 91.73
This paper 88.00 66.67 71.60 88.31 93.44
Table 3: Conversion results with respect to differ-
ent grammar categories
parser and that Wang94 works on the basis of
Berkeley parser. Taking the performance gap of
Collins parser and Berkeley parser, we actually
can conclude that on small spans, our approach is
able to achieve results comparable with or even
better than Wang94. We can also infer from
the observation that our approach can outperform
Wang94 when converting parse trees which yield
long sentences.
Another line of analysis is to compare the
results of Wang94 and our approach, with re-
spect to different grammar categories. Table 3
lists five grammar categories in which our ap-
proach achieves most improvements. For cat-
egories NP and VP, absolute improvements are
1.1% and 1.4% respectively. Take into account
large amounts of instances of NP and VP, the im-
provements are also quite significant.
4.3 Experiments on Parsing
Before doing the experiments of parsing, we first
converted the whole TCT corpus using 100%
CTB training data as target training data. Us-
ing the newly-gained data only as training data
for Collins parser, we can get F1 score 75.4%
on the CTB test data. We can see that the score
is much lower than the accuracy achieved by us-
ing the CTB training data (75.4% vs. 82.04%).
Possible reasons that result in lower accuracy in-
cludes: 1) divergences in word segmentation stan-
dards between TCT and CTB; 2) divergences of
domains of TCT and CTB; 3) conversions errors
in newly-gained data. Although the newly-gained
1547
data cannot replace the CTB training data thor-
oughly, we would like to use it as additional train-
ing data besides the CTB training data. Following
experiments aim to examine effectiveness of the
newly-gained data when used as additional train-
ing data.
In the first parsing experiment, the TCT cor-
pus is converted using portions of the CTB train-
ing data. As in the conversion experiments, parse
trees in the CTB training data are randomly or-
dered before splitting of the training set. For each
portion, newly-gained data together with the por-
tion of the CTB training data are used to train a
new parser. Evaluation results on the CTB test
data are presented in Table 4.
Ratio 20% 40% 60% 80% 100%
Collins 75.74 77.65 79.43 81.22 82.04
Collins+ 78.86 79.52 80.06 81.77 82.38
Table 4: Parsing accuracy with new data added in
Here in Table 4, the first row represents ratios
of parse trees from the CTB training data. For
example, 40% means the first 40% parse trees in
the CTB training data are used. The Collins row
represents the results of only using portions of the
CTB training data, and the Collins+ row contains
the results achieved with enlarged training data.
From the results, we find that new data indeed
provides complementary information to the CTB
training data, especially when the training data is
small in size. But benefits of Collins parser gained
from additional training data level out with the in-
crement of the training data size. Actually if tech-
niques like corpus weighting (Niu et al, 2009) are
applied to weight differently training data and the
additional data, higher parsing accuracy is reason-
ably expected.
Another obversion from Table 4 is that the
parser trained on 40% CTB training data plus
additional training data achieves higher accuracy
than using 60% CTB training data. We incre-
mentally add labeled training data and automatic
training data respectively to 40% CTB training
data. The purpose of this experiment is to see the
magnitude of automatic training data which can
achieve the same effect as labeled training data
does. The results are depicted in Table 5.
# of Added Data 2k 4k 6k 8k
Labeled Data 78.51 79.52 80.01 81.37
Auto Data 78.23 79.11 79.85 79.67
Table 5: Parsing accuracy with new data added in
From the results we see that accuracy gaps be-
tween using labeled data and using automatic data
get large with the increment of added data. One
possible reason is that more noise is taken when
more data is added. This observation further veri-
fies that refining techniques like corpus weighting
are necessary for using automatically-gained data.
5 Conclusions
In this paper we proposed an approach called in-
formed decoding for the task of conversion be-
tween treebanks which have different annotation
standards. Experiments which evaluate conver-
sion accuracy directly showed that our approach
significantly outperform baseline systems. More
interestingly we found that the size of target train-
ing data have limited effect on the conversion ac-
curacy of our approach. This is extremely impor-
tant for languages which lack enough treebanks in
whose standards we are interested.
We also added newly-gained data to target
training data to check whether new data can boost
parsing results. Experiments showed additional
training data provided by treebank conversion
could boost parsing accuracy.
References
Banko, Michele and Eric Brill. 2001. Scaling to
very very large corpora for natural language dis-
ambiguation. In Proc. of ACL 2001, pages 26-33.
Charniak, Eugene. 2000. A Maximum-Entropy-
Inspired Parser. In Proc. of NAACL 2000, pages
132-139.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Collins, Michael, Lance Ramshaw, Jan Hajic, and
Christoph Tillmann. 1999. A Statistical Parser for
Czech. In Proc. of ACL 1999, pages 505-512.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proc. of NAACL 2000, pages
132-139.
1548
Huang, Liang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proc. of
ACL 2008, pages 586-594.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of NODALIDA 2007, pages 105-
112.
Nivre, Joakim. 2006. Inductive Dependency Parsing.
In Springer, Volume 34.
Niu, Zheng-Yu, Haifeng Wang, Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In
Proc. of ACL 2009, pages 46-54.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440.
Xue, Nianwen, Fu dong Chiou, and Martha Palmer.
2002. Building a large-scale Annotated Chinese
corpus. In Proc. of COLING 2002, pages 1-8.
Wang, Jong-Nae, Jing-Shin Chang, and Keh-Yih Su.
1994. An automatic treebank conversion algorithm
for corpus sharing. In Proc. of ACL 1994, pages
248-254.
Xia, Fei, Rajesh Bhatt, Owen Rambow, Martha
Palmer, and Dipti M. Sharma. 2008. Towards a
Multi-Representational Treebank. In Proc. of the
7th International Workshop on Treebanks and Lin-
guistic THeories, pages 159-170.
Zhang, Hui, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proc. of EMNLP 2009, pages 1552-1560.
Zhou, Qiang. 1996. Phrase bracketing and annotating
on Chinese language corpus. (in Chinese) Ph.D.
thesis, Beijing University.
Zhu, Muhua and Jingbo Zhu. 2009. Label Corre-
spondence Learning for Part-of-Speech Annotation
Transformation. In Proc. of CIKM 2009, pages
1461-1464.
1549
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2064?2074, Dublin, Ireland, August 23-29 2014.
Effective Incorporation of Source Syntax into
Hierarchical Phrase-based Translation
Tong Xiao??, Adri
`
a de Gispert?, Jingbo Zhu??, Bill Byrne?
? Northeastern University, Shenyang 110819, China
? Hangzhou YaTuo Company, Hangzhou 310012, China
? University of Cambridge, CB2 1PZ Cambridge, U.K.
{xiaotong,zhujingbo}@mail.neu.edu.cn
{ad465,wjb31}@eng.cam.ac.uk
Abstract
In this paper we explicitly consider source language syntactic information in both rule extraction
and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the
GHKM method and use them to complement Hiero-style rules. All these rules are then employed
to decode new sentences with source language parse trees. We experiment with our approach in
a state-of-the-art Chinese-English system and demonstrate +1.2 and +0.8 BLEU improvements
on the NIST newswire and web evaluation data of MT08 and MT12.
1 Introduction
Synchronous context free grammars (SCFGs) are widely used in statistical machine translation (SMT),
with hierarchical phrase-based translation (Chiang, 2005) as the dominant approach. Hiero grammars
are easily extracted from word-aligned parallel corpora and can capture complex nested translation re-
lationships. Hiero grammars are formally syntactic, but rules are not constrained by source or target
language syntax. This lack of constraint can lead to intractable decoding and bad performance due to
the over-generation of derivations in translation. To avoid these problems, the extraction and application
of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised;
and rules are limited to two non-terminals which are not allowed to be adjacent in the source language.
These constraints can yield good performing translation systems, although at a sacrifice in the ability to
model long-distance movement and complex reordering of multiple constituents.
By contrast, the GHKM approach to translation (Galley et al., 2006) relies on a syntactic parse on
either the source or target language side to guide SCFG extraction and translation. The parse tree provides
linguistically-motivated constraints both in grammar extraction and in translation. This allows for looser
span constraints; rules need not be lexicalised; and rules can have more than two non-terminals to model
complex reordering multiple constituents. There are also modelling benefits as more meaningful features
can be used to encourage derivations with ?well-formed? syntactic tree structures. However, GHKM can
have robustness problems in that translation relies on the quality of the parse tree and the diversity of
rule types can lead to sparsity and limited coverage.
In this paper we describe a simple but effective approach to introducing source language syntax into
hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we
do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style
rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal,
2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the
baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not.
We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation
task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on
the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM
formalism and find, for example, that our approach works well with binarized trees.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2064
IP
NP
PN
?
VP
PP
P
?
NP
NN
??
VP
VV
??
NN
??
he
was
satisfied with the
answer
Hiero-style SCFG Rules
h
1
X? ??, he?
h
2
X? ??, with?
h
3
X? ???, the answer?
h
4
X? ?????, was satisfied?
h
5
X? ?X
1
????, was satisfied X
1
?
h
6
X? ?X
1
?? X
2
, was X
2
X
1
?
h
7
X? ?X
1
? X
2
????,
X
1
was satisfied with X
2
?
Tree-to-String Rules
r
1
NP(PN(?))? he
r
2
P(?)? with
r
3
NP(NN(??))? the answer
r
4
VP(VV(??) NN(??))? was satisfied
r
5
PP(x
1
:P x
2
:NP)? x
1
x
2
r
6
VP(x
1
:PP x
2
:VP)? x
2
x
1
r
7
IP(x
1
:NP x
2
:VP)? x
1
x
2
r
8
VP(PP(P(?) x
1
:NP) x
2
:VP)? x
2
with x
1
Figure 1: Hiero-syle and tree-to-string rules extracted from a pair of word-aligned Chinese-English
sentences with a source language (Chinese) parse tree.
2 Background
2.1 Hierarchical Phrase-based Translation
In the hierarchical phrase-based approach, translation is modelled using SCFGs. In general, probabilistic
SCFGs can be learned from word-aligned parallel data using heuristic methods (Chiang, 2007). We can
first extract initial phrase pairs and then obtain hierarchical phrase rules (i.e., rules with non-terminals
on the right hand side). Once the SCFG is obtained, new sentences can be decoded by finding the most
likely derivation of SCFG rules. See Figure 1 for example rules extracted from a sentence pair with word
alignments. A sequence of such rules covering the words of the source sentence is a SCFG derivation,
e.g., rules h
7
, h
1
and h
3
generate a derivation for the sentence pair.
The Hiero SCFG allows vast numbers of derivations which can make unconstrained decoding in-
tractable. In practice, several constraints are applied to control the model size and reduce ambiguity.
Typically these are: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction,
set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set
to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the
glue rules).
2.2 Tree-to-String Translation
Instead of modelling the problem based on surface strings, tree-to-string systems model the translation
equivalency relations from source language syntactic trees to target language strings using derivations
of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A
tree-to-string rule is a tuple ?s
r
, t
r
,??, where s
r
is a source language tree-fragment with terminals and
non-terminals at leaves; t
r
is a string of target-language terminals and non-terminals; and ? is a 1-to-1
alignment between the non-terminals of s
r
and t
r
, for example, VP(VV(??) x
1
:NN)? increases x
1
is a tree-to-string rule, where the non-terminals labeled with the same index x
1
indicate the alignment.
To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al.,
2006) on the bilingual sentences with both word alignment and source (or target) language phrase-
structure tree annotations. In GHKM extraction, we first compute the set of the minimally-sized transla-
tion rules that can explain the mappings between source language tree and target-language string while
respecting the alignment and reordering between the two languages. More complex rules are then learned
by composing two or more minimal rules. See Figure 1 for rules extracted using GHKM.
One of the advantages of the above model is that non-terminals in tree-to-string rules are linguistically
2065
rule
match
decoding
input
string
Hiero
SCFG
ouput
string
(a) decoding with Hiero rules only
rule
match
decoding
input
string&tree
larger
SCFG
Hiero
SCFG
t-to-s
rules
ouput
string
(b) decoding with Hiero and tree-to-string rules
Figure 2: Overview of the Hiero baseline (a) and
our approach (b). ?means input or output of the
decoder. t-to-s is a short for tree-to-string.
VP
PP
P
?
x
1
:NP
x
2
:VP
x
2
with
x
1
X
?
?
?
X
1
X
2
, X
2
with
X
1
?
tree-to-string:
Hiero:
Figure 3: Converting the tree-to-string rule r
8
from Figure 1 to a Hiero-style rule.
motivated and can span word sequences with arbitrary length. Also, one can use rules with consecutive
(or more than two) source language non-terminals when the source language parse tree is available. For
example, r
8
in Figure 1 has a good Chinese syntactic structure indicating the reordered translations of NP
and VP. However, such a rule would not normally be included in a Hiero grammar, as it would require
consecutive source language non-terminals (see Figure 3).
3 The Proposed Approach
Both the tree-to-string model and the hierarchical phrase-based model have their own strengths and
weaknesses. For example, tree-to-string systems are good at modelling long distance reordering, while
hierarchical phrase-based systems are relatively more powerful in handling ill-formed sentences
1
and
free translations (Zhao and Al-Onaizan, 2008; Vilar et al., 2010). Here we present a method to enhance
hierarchical phrase-based systems with tree-to-string rules and benefit from both models. The idea is
simple: we obtain both the tree-to-string grammar and the Hiero-style SCFG from the training data, and
then use tree-to-string rules as additional rules in decoding with the SCFG.
Figure 2 shows an overview of our approach and the usual hierarchical phrase-based approach. Our
approach requires source language parse trees to be input in both rule extraction and decoding. In rule
extraction, we acquire tree-to-string rules using the GHKM method and Hiero-style rules using the Hiero-
style rule extraction method to form a larger SCFG. Then, we make use of both the input string and parse
tree to decode with the SCFG rules. We now describe our approach.
3.1 Transforming Tree-to-String Rules into SCFG Rules
As described in Section 2, tree-to-string rules have a different form from that of SCFG rules. We will use
tree-to-string rules in our hierarchical phrase-based systems by converting each tree-to-string rule into an
SCFG rule. The purpose of doing this is to make tree-to-string rules directly accessible to the Hiero-style
decoder which performs decoding with SCFG rules.
The rule mapping is straightforward: given a tree-to-string rule ?s
r
, t
r
,??, we take the frontier nodes
of s
r
as the source language part of the right hand side of the resulting SCFG rule, and keep t
r
and
? unchanged. Then we replace the non-terminal label with that used in the hierarchical phrase-based
system (e.g., X). See Figure 3 for rule mapping of rule r
8
of Figure 1.
In this way, every tree-to-string rule is associated with exactly one SCFG rule. Therefore we can
obtain a larger SCFG by combining the rules from the original Hiero-style SCFG and the transformed
tree-to-string rules. As explained next, to prevent computational problems we will apply these new rules
1
For example, the parser fails for 4% of the sentences in our training corpus, and 3% and 6% of the newswire and web
development/test sentences, indicating that the data is sometimes ill-formed.
2066
only on the spans that are consistent with the input parse trees. The main goal is to use the tree and the
adapted tree-to-string rules to provide the decoder with new linguistically-sensible translation hypotheses
that may be prevented by the usual Hiero constraints, and to do so without incurring a computational
explosion.
We categorize SCFG rules into two categories based on their availability in Hiero and GHKM extrac-
tion. If an SCFG rule is obtained from Hiero extraction, it is a type 1 rule; If not (i.e., this rule is only
available in GHKM extraction), it is a type 2 rule. E.g., the SCFG rule in Figure 3 is a type 2 rule because
it is not available in the original Hiero-style SCFG but can be generated from the tree-to-string rule.
Next we describe how each of these rule types are applied in decoding. We also describe which
features are used and how they are computed for each rule type.
3.2 Decoding
Both types of SCFG rules can be employed by usual Hiero decoders with a slight modification. Here
we follow the description of Hiero decoding by Iglesias et al. (2011). The source sentence is parsed
under the Hiero grammar using the CYK algorithm. Each cell in the CYK grid has associated with it a
list of rules that apply to its span; these rules are used to construct a recursive transition network (RTN)
which represents all translations of the source sentence under the grammar. The RTN is expanded to a
weighted finite state automaton for composition with n-gram language models (de Gispert et al., 2010).
Translations are produced via shortest path computation.
This procedure accommodates type 1 rules directly. For tree-to-string rules associated with type 2, we
attempt to match rules to the source syntactic tree. If a match is found: the source span of the matching
tree fragment is noted and the CYK cell for that span is selected; the tree-to-string rule is converted to
a Hiero-style rule; and that rule is added to the list of rules in the selected CYK cell. Once this process
is finished, RTN construction, expansion, and language model composition proceeds as usual. Similar
modifications could be made to incorporate these rules into cube pruning (Chiang, 2007), cube growing
(Huang and Chiang, 2007), and PDT intersection and expansion (Iglesias et al., 2011). We now elaborate
on the rule matching strategy.
Type 1 Rules The source sentence is parsed as is usual in Hiero-style translation, with the exception
that we impose no span limit on rule applications for source spans corresponding to constituents in the
Chinese syntactic tree. Rule matching, the procedure that determines if a rule applies to a source span, is
based on string matching (see Figure 4(a)). For example, the type 1 rule h
9
in Figure 4(c) can be applied
to spans (1,13) and (2,13) since both of them agree with tree constituents (see Figure 4(b)). But h
9
is
not applied to span (3,13) because that span is longer than 10 words and agrees with no syntactic tree
constituent.
Type 2 Rules If the source side of a tree-to-string rule matches an input tree fragment: 1) that rule
is converted to a Hiero-style SCFG rule (Section 3.1); and 2) the Hiero-style rule is added to the rules
linked with the CYK grid cell associated with the span of the source syntactic tree fragment. Here, rules
are applied via tree matching. For example, rule h
11
in Figure 4(b) matches the tree fragment spanning
positions (2,13).
It is worth noting that some type 1 rules may be found via both Hiero-style and tree-to-string grammar
extraction. In this case we monitor whether a rule can be applied as a tree-to-string rule using tree-
matching so that features (Section 3.3) and weights can be set appropriately. As an example, rule h
10
in
Figure 4 is available in both extraction methods. For span (2,11), this rule can be matched via both string
matching and tree matching. We then note that we can apply h
10
as a tree-to-string rule for span (2, 11)
and activate the corresponding features defined in Section 3.3. For other spans (e.g., spans (2,3)-(2,10)),
no tree fragments can be matched and the baseline features are used for h
10
.
3.3 Features
The baseline feature set used in this work consists of 12 features (Pino et al., 2013), including a 4-gram
language model, a strong 5-gram language model, bidirectional translation probabilities, bidirectional
lexical weights, a word count, a phrase count, a glue rule count, a frequency-1 rule count, a frequency-2
2067
h9
: X? ?
X
1
?? , satisfied with X1 ?
???
1
?
2
??
3
?
4
?
5
?
6
?
7
??
8
?
9
??
10
??
11
??
12
??
13
. . .
.
.
.
.
.
.
.
.
.
Chart Used in Decoding
span
(10,13)
matching
(a) matching a type 1 rule (h
9
) with the input string
IP
NP
NR
???
1
VP
PP
P
?
2
NP
??
3
?
4
?
5
?
6
?
7
??
8
?
9
??
10
??
11
VP
VV
??
12
NN
??
13
VP(PP(P(?) x
1
:NP) x
2
:VP)
? x
2
with x
1
h
11
: X? ?? X
1
X
2
,
X
2
with X
1
?
converting
. . .
.
.
.
.
.
.
.
.
.
Chart Used in Decoding
matching
span
(2,13)
(b) matching a type 2 rule (h
11
) with the input parse tree
ID Type Hiero-style Rule Tree-to-string Rule Applicable Spans
h
8
type 1 X? ?????, is satisfied ? N/A (12,13)
h
9
type 1 X? ? X
1
??, satisfied with X
1
? N/A (i,13), i = 1, 2 or 4 ? i ? 12
h
10
type 1 X? ?? X
1
, with X
1
? PP(P(?) x
1
NP)? with NPx
1
(2,j), 3 ? j ? 11 or j = 13
h
11
type 2 X? ?? X
1
X
2
, X
2
with X
1
? VP(PP(P(?) x
1
:NP) x
2
:VP) (2,13)
? x
2
with x
1
(c) example rules used in decoding
Figure 4: Decoding with both Hiero-style and tree-to-string grammars (span limit = 10). A span (i,j)
means spanning from position i to position j.
rule count, and a larger-than-frequency-2 rule count
2
. In addition, we introduce several features for
applying tree-to-string rules.
? Rule type indicators. We consider four indicator features, indicating tree-to-string rules, lexicalized
tree-to-string rules, rules with consecutive non-terminals, and non-lexicalized rules. Note that the tree-
to-string rule indicator feature is in principle a generalization of the soft syntactic features (Marton and
Resnik, 2008), in that a bonus (or penalty) is applied when a rule application is consistent with a source
tree constituent. The difference lies in that the tree-to-string rule indicator feature does not distinguish
between different syntactic labels, whereas soft syntactic features do.
? Features in syntactic MT. In general tree-to-string rules have their own features which are different
from those used in Hiero-style systems. For example, the features in syntactic MT systems can be
defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we
choose five popular features used in syntactic MT systems, including the bi-directional phrase-based
conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabil-
ities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates.
For example, the phrase-based features are the probabilities of translating between the frontier nodes
of s
r
and t
r
. The syntax-based features are the probabilities of generating r conditioned on its root,
2
We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline
system.
2068
source and target language sides, respectively. More formally, we use the following estimates for these
probabilities:
P
phr
(t
r
| s
r
) =
?
r
??
:?(s
r
??
)=?(s
r
)?t
r
??
=t
r
c(r
??
)
?
r
?
:?(s
r
?
)=?(s
r
)
c(r
?
)
P
phr
(s
r
| t
r
) =
?
r
??
:?(s
r
??
)=?(s
r
)?t
r
??
=t
r
c(r
??
)
?
r
?
:t
r
?
=t
r
c(r
?
)
P(r | root(r)) =
c(r)
?
r
?
:root(r
?
)=root(r)
c(r
?
)
P(r | s
r
) =
c(r)
?
r
?
:s
r
?
=s
r
c(r
?
)
P(r | t
r
) =
c(r)
?
r
?
:t
r
?
=t
r
c(r
?
)
where c(r) is the count of r, and root(?) and ?(?) are functions that return the source root symbol for
a tree-to-string rule and the sequence of leaf nodes for a tree-fragment respectively.
4 Evaluation
4.1 Experimental Setup
We report results in the NIST MT12 Chinese-English task, where our baseline system was among the top
academic systems. The parallel training corpus consists of 9.2 million sentence pairs which are provided
within the NIST Chinese-English MT12 track. Word alignments are obtained using MTTK (Deng and
Byrne, 2008) in both Chinese-to-English and English-to-Chinese directions, and then unioning the links.
The data from newswire and web genres was used for tuning and test. The development sets contain
1,755 sentences and 2160 sentences for the two genres respectively. The test sets (newswire: 1,779
sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12
(mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test
sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language
model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English
side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST
MT12 and the Google counts corpus using the ?stupid? backoff method (Brants et al., 2007).
For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert
et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the
translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring
of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from
the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al.,
2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were
optimized using lattice-based minimum error rate training (Macherey et al., 2008).
For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and
extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we
restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded
lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules
with a Chinese-to-English translation probability of < 0.10.
4.2 Results
We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments
are organized as follows:
? Baseline and Span Limits (exp01 and exp02)
First we study the effect of removing the span limit for tree constituents, that is, SCFG rules can be
2069
Entry System Newswire Web
tune mt08 mt12 mt08.p all test tune mt08 mt12 mt08.p all test
(1755) (691) (400) (688) (1779) (2160) (666) (420) (682) (1768)
exp01 baseline 35.84 35.85 35.47 35.50 35.63* 29.98 25.15 23.07 27.19 25.33*
exp02 += no span limit 36.05 36.08 35.70 35.54 35.79* 30.11 25.28 23.08 27.17 25.37*
exp03 += t-to-s rules 36.63 36.51 36.08 36.09 36.25* 30.80 26.00 23.08 27.80 25.83*
exp04 += t-to-s features 36.82 36.49 36.53 36.16 36.38* 30.91 26.03 23.27 27.85 25.98*
exp05 t-to-s baseline 34.63 34.44 34.87 33.66 34.25* 28.30 23.40 21.38 25.30 23.56*
exp06 exp04 on spans > 10 36.17 36.11 35.71 35.86 35.92* 30.18 25.30 23.12 27.36 25.45*
exp07 exp04 with null trans. 36.10 36.03 35.35 34.86 35.42* 29.96 25.32 22.58 23.33 24.12*
exp08 exp04 + left binariz. 37.11 37.46 37.03 36.30 36.91* 31.18 26.15 23.54 27.98 26.13*
exp09 exp04 + right binariz. 36.58 36.56 36.41 35.70 36.20* 31.06 25.94 23.47 27.48 25.88*
exp10 exp04 + forest binariz. 37.03 37.27 37.09 36.62 36.98* 31.20 25.99 23.59 28.09 26.15*
Table 1: Case-insensitive BLEU[%] scores of various systems. += means incrementally adding method-
s/features to the previous system. * means that a system is significantly different than the exp01 baseline
at p < 0.01.
applied to any spans when they respect the tree constituents of the input tree. It can be regarded as
the simplest way of using source syntax in Hiero-style systems. Seen from Table 1, removing the
span limit shows modest BLEU improvements. It agrees with the previous result that loosening the
constraints on spans is helpful to systems based on the hard syntactic constraints (Li et al., 2013).
? GHKM+Hiero (exp03 and exp04)
The results of our proposed approach (w/o new features) are reported in exp03 and exp04. We see that
incorporating tree-to-string rules yields +0.6 and +0.5 improvements on the collected newswire and
web test sets (exp03 vs exp01). The new features (Section 3.3) give a further improvement (exp04 vs
exp03). This result confirms that the system can learn a preference for certain types of rules using the
new features.
? Impact of Search Space (exp05)
We also study the impact of search space on system performance. To do this, we force the improved
system (exp04) to respect source tree constituents and to discard any hypotheses which violate the
tree constituent constraints. Seen from exp05, this system has a lower BLEU score than both the
Hiero baseline (exp01) and GHKM+Hiero system (exp04), strongly suggesting that restricting MT
systems to a smaller space of hypotheses is harmful.
? GHKM+Hiero, Spans > 10 Only (exp06)
Another interesting question is whether tree-to-string rules and features are more helpful to larger
spans. We restricted our approach to spans > 10 only and conducted another experiment. As is shown
in exp06, applying tree-to-string rules and features for large spans is beneficial (exp06 vs. exp01). But
it underperforms the system with the full use of tree-to-string rules (exp06 vs. exp04). This interesting
observation implies that applying tree-to-string rules on smaller spans introduces good hypotheses that
can be selected with our additional features.
? Impact of Failed Parses (exp07)
As noted in Section 3, the parser fails to parse some of the sentences in our experiments. In this case
our approach generates the baseline result using the Hiero model (i.e., type 1 rules only). To investigate
the effect of failed parse trees on system performance, we also report the BLEU score including null
translations for which the parser fails. As shown in exp07, there are significantly lower BLEU scores
when null translations are included. It indicates that our approach is more robust than standard tree-
to-string systems which would generate an empty translation if the source language parser fails.
? Results on Binarization (exp08-10)
Tree binarization is a widely used method to improve syntactic MT systems (Wang et al., 2010).
exp08-10 show the results of our improved system with left-heavy, right-heavy and forest-based bina-
2070
Reference: After North Korea demanded concessions from U.S. again before the start of a new round of six-nation talks , ...
Baseline: In the new round of six-nation talks on North Korea again demanded that U.S. in the former promise concessions , ...
GHKM+Hiero: After
North Korea again demanded that U.S. promised concessions before the new round of six-nation talks
, ...
a Hiero rule X? ?? X
1
?, after X
1
? is applied on span (1,15)
Input:
IP
PP
P
?
1
LCP
IP
??
2
??
3
??
4
??
5
?
6
?
7
??
8
?
9
?
10
??
11
?
12
??
13
??
14
LC
?
15
PU
,
VP
...
Reference: The Chinese star performance troupe presented a wonderful Peking opera as well as singing and dancing
Reference: performance to Hong Kong audience .
Baseline: Star troupe of China, highlights of Peking opera and dance show to the audience of Hong Kong .
GHKM+Hiero: Chinese star troupe presented a wonderful Peking opera singing and dancing
to
Hong Kong audience
.
Input:
A tree-to-string rule is applied:
(VP BA(?) x
1
:NP x
2
:VP PP(P(?) x
3
:NP))
? x
2
x
1
to x
3
IP
NP
??
1
??
2
???
3
VP
BA
?
4
NP
?
5
?
6
??
7
?
8
??
9
??
10
VP
VV
??
11
PP
P
?
12
NP
??
13
??
14
.
Figure 5: Comparison of translations generated by the baseline and improved systems.
rization
3
. We see that left-heavy binarization is very helpful and exp08 achieves overall improvements
of 1.2 and 0.8 BLEU points on the newsire and web data. In contrast, right-heavy binarization does
not yield promising performance. This agrees with the previous report (Wang et al., 2010) that MT
systems prefer to use certain ways of binarization in most cases. exp10 shows that the additional trees
introduced in our forest-based scheme are not sufficient to make a big impact on BLEU scores. Pos-
sibly larger gains can be obtained if taking a forest of parse trees from the source parser, but this is
outside the scope of this paper.
4.3 Analysis
We then analyse rule usage in the 1-best derivations for our improved system on the tuning set. We find
that type 2 rules represent 13.97% of the rules used in the 1-best derivations. Also, 44.45% of the applied
rules are available from the tree-to-string model (i.e., rules that use the features described in Section 3.3).
These numbers indicate that the tree-to-string rules are beneficial and our decoder likes to use them.
Finally, we discuss two real translation examples from our tuning set. See Figure 5 for translations
generated by different systems. In the first example, the Chinese input sentence contains? ...? which
is usually translated into after ... (i.e., a Hiero rule X? ?? X
1
?, after X
1
?). However, because the
?? ...?? pattern spans 15 words and that is beyond the span limit, our baseline is unable to apply this
desired rule and chooses a wrong translation in for the Chinese word ?. When the source parse tree
3
We found that the CTB-style parse trees usually have a very flat top-level IP (i.e., single clause) tree structure. As the IP
structure in Chinese is very complicated, the system might prefer a flexible binarization scheme. Thus we considered both left
and right-heavy binarization to form a binarization forest for IPs in Chinese parse trees, and binarized other tree constituents in
a left-heavy fashion.
2071
is available, our approach removes the span limit for spans that agree with the tree constituents. In this
case, the MT system successfully applies the rule on span (1, 15) and generates a much better translation.
In the second example, the translation of the input sentence requires complex reordering of adjacent
constituents. The baseline system cannot handle this case and generates a monotonic translation using
the glue rules. This results in a wrong order for the translation of Chinese verb?? (show). By contrast,
the improved system chooses a tree-to-string rule with three non-terminals (some of which are adjacent
in the source language) and perfectly performs a syntactic movement of the required tree constituents.
5 Related Work
Recently linguistically-motivated models have been intensively investigated in MT. In particular, source
tree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al.,
2009a; Xie et al., 2011) have received growing interest due to their good abilities in modelling source
language syntax for better lexicon selection and reordering. Alternatively, the hierarchical phrase-based
approach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not re-
quire linguistically syntactic trees on either language side.
There are several lines of work for augmenting hierarchical phrase-based systems with the use of
source language phrase-structure trees. Liu et al. (2009b) describe novel approaches to translation under
multiple translation grammars. Their approach is very much motivated by system combination, and they
develop procedures for joint decoding and optimisation within a single system that give the benefit of
combining hypotheses from multiple systems. They demonstrate their approach by combining full tree-
to-string and Hiero systems. Our approach is much simpler and emphasises changes to the grammar
rather than the decoder or its parameter optimisation (MERT). Our aim is to augment the search space
of Hiero with linguistically-motivated hypotheses, and not to develop a new decoder that is capable of
translation under multiple grammars. Moreover, we consider Hiero as the backbone model and only
introduce tree-to-string rules where they can contribute; we show that extracting tree-to-string rules from
just 10% of the data suffices to get good gains. This results in a small number of tree-to-string rules and
does not slow down the decoder.
Another related line of work is to introduce syntactic constraints or annotations to hierarchical phrase-
based systems. Marton and Resnik (2008) and Li et al. (2013) proposed several soft or hard constraints to
model syntactic compatibility of Hiero derivations and input source language parse trees. We note that,
despite significant development effort, we were not able to improve our baseline through the use of these
soft syntactic constraints; it was this experience that led us to develop the hybrid approach described in
this paper.
Several research groups used syntactic labels as non-terminal symbols in their SCFG rules and develop
new features (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010; Hoang and
Koehn, 2010). However, all these methods still resort to rule extraction procedures similar to that of the
standard phrase/hierarchical rule extraction method. In contrast, we use the GHKM method which is a
mature technique to extract rules from tree-string pairs but does not impose those Hiero-style constraints
on rule extraction. More importantly, we consider the hierarchical syntactic tree structure to make use of
well-formed rules in decoding, while such information is not used in standard SCFG-based systems. We
also keep to the simpler non-terminals of Hiero, and do not ?decorate? any non-terminals with syntactic
or other information.
6 Conclusion
We have presented an approach to improving Hiero-style systems by augmenting the SCFG with tree-
to-string rules and syntax-based features. The input parse trees are used to introduce new linguistically-
sensible hypotheses into the translation search space while maintaining the Hiero robustness qualities
and avoiding computational explosion. We obtain significant improvements over a strong Hiero baseline
in Chinese-to-English. Further improvements are achieved when applying tree binarization.
2072
Acknowledgements
This work was done while the first author was visiting the speech group at University of Cambridge, and
was supported in part by the National Science Foundation of China (Grants 61272376 and 61300097),
and the China Postdoctoral Science Foundation (Grant 2013M530131). We would like to thank the
anonymous reviewers for their pertinent and insightful comments. We also would like to thank Juan
Pino, Rory Waite, Federico Flego and Gonzalo Iglesias for building parts of the baseline system.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large Language Models in
Machine Translation. In Proceedings of EMNLP-CoNLL, pages 858?867, Prague, Czech Republic.
David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, USA.
David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33:45?60.
David Chiang. 2010. Learning to Translate with Source and Target Syntax. In Proceedings of ACL, pages 1443?
1452, Uppsala, Sweden.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchi-
cal Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars. Computational
Linguistics, 36(3):505?533.
Yonggang Deng and William Byrne. 2008. HMM Word and Phrase Alignment for Statistical Machine Translation.
IEEE Transactions on Audio, Speech & Language Processing, 16(3):494?507.
Jason Eisner. 2003. Learning Non-Isomorphic Tree Mappings for Machine Translation. In Proceedings of ACL,
pages 205?208, Sapporo, Japan.
Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn. 2012. Left-to-Right Tree-to-String Decoding with Prediction.
In Proceedings of EMNLP-CoNLL, pages 1191?1200, Jeju Island, Korea.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of
COLING-ACL, pages 961?968, Sydney, Australia.
Hieu Hoang and Philipp Koehn. 2010. Improved translation with source syntax labels. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 409?417, Uppsala, Sweden.
Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In
Proceedings of ACL, pages 144?151, Prague, Czech Republic.
Liang Huang and Haitao Mi. 2010. Efficient Incremental Decoding for Tree-to-String Translation. In Proceedings
of EMNLP, pages 273?283, Cambridge, MA, USA.
Liang Huang, Knight Kevin, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73, Cambridge, MA, USA.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule Filtering by Pattern for
Efficient Hierarchical Translation. In Proceedings of EACL, pages 380?388, Athens, Greece.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adri`a de Gispert, and Michael Riley. 2011. Hierarchical Phrase-
based Translation Representations. In Proceedings of EMNLP, pages 1373?1383, Edinburgh, Scotland, UK.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013. Modeling Syntactic and Semantic Structures in Hierarchical
Phrase-based Translation. In Proceedings of NAACL-HLT, pages 540?549, Atlanta, Georgia.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Transla-
tion. In Proceedings of COLING-ACL, pages 609?616, Sydney, Australia.
Yang Liu, Yajuan L?u, and Qun Liu. 2009a. Improving Tree-to-Tree Translation with Packed Forests. In Proceed-
ings of ACL-IJCNLP, pages 558?566, Suntec, Singapore.
2073
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b. Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, Suntec, Singapore.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based Minimum Error Rate
Training for Statistical Machine Translation. In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phrases. In Proceedings of EMNLP, pages 44?52, Sydney, Australia.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-Based Translation. In
Proceedings of ACL-HLT, pages 1003?1011, Columbus, Ohio.
Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. In Proceedings of EMNLP, pages
206?214, Honolulu, Hawaii, USA.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-Based Translation. In Proceedings of ACL-HLT, pages
192?199, Columbus, Ohio.
Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. Bleu: a Method for Automatic Evaluation
of Machine Translation. In Proceedings of ACL, pages 311?318, Philadelphia, PA, USA.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL,
pages 404?411, Rochester, New York, USA.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gispert, Federico Flego, and William Byrne. 2013. The University
of Cambridge Russian-English system at WMT13. In Proceedings of WMT, pages 200?205, Sofia, Bulgaria.
David Vilar, Daniel Stein, Stephan Peitz, and Hermann Ney. 2010. If i only had a parser: poor man?s syntax for
hierarchical machine translation. In Proceedings of IWSLT, pages 345?352.
Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, Re-labeling, and Re-aligning
for Syntax-Based Machine Translation. Computational Linguistics, 36(2):247?277.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li. 2012. NiuTrans: An Open Source Toolkit for Phrase-based
and Syntax-based Machine Translation. In Proceedings of ACL: System Demonstrations, pages 19?24, Jeju
Island, Korea.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of EMNLP, pages 216?226, Edinburgh, Scotland.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A Tree Sequence
Alignment-based Tree-to-Tree Translation Model. In Proceedings of ACL-HLT, pages 559?567, Columbus,
Ohio, USA.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing Local and Non-Local Word-Reordering Patterns for Syntax-
Based Machine Translation. In Proceedings of EMNLP, pages 572?581, Honolulu, Hawaii.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In
Proceedings of WMT, pages 138?141, New York City.
2074
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177?182,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Syntactic SMT Using a Discriminative Text Generation Model
Yue Zhang Kai Song? Linfeng Song?
SUTD, Singapore NEU, China ICT/CAS, China
yue zhang@sutd.edu.sg songkai.sk@alibaba-inc.com songlinfeng@ict.ac.cn
Jingbo Zhu Qun Liu
NEU, China CNGL, Ireland and ICT/CAS, China
zhujingbo@mail.neu.edu.cn qliu@computing.dcu.ie
Abstract
We study a novel architecture for syntactic
SMT. In contrast to the dominant approach
in the literature, the system does not rely
on translation rules, but treat translation
as an unconstrained target sentence gen-
eration task, using soft features to cap-
ture lexical and syntactic correspondences
between the source and target languages.
Target syntax features and bilingual trans-
lation features are trained consistently in
a discriminative model. Experiments us-
ing the IWSLT 2010 dataset show that the
system achieves BLEU comparable to the
state-of-the-art syntactic SMT systems.
1 Introduction
Translation rules have been central to hierarchi-
cal phrase-based and syntactic statistical machine
translation (SMT) (Galley et al., 2004; Chiang,
2005; Liu et al., 2006; Quirk et al., 2005; Marcu et
al., 2006; Shen and Joshi, 2008; Xie et al., 2011).
They are attractive by capturing the recursiveness
of languages and syntactic correspondences be-
tween them. One important advantage of trans-
lation rules is that they allow efficient decoding
by treating MT as a statistical parsing task, trans-
forming a source sentence to its translation via re-
cursive rule application.
The efficiency takes root in the fact that target
word orders are encoded in translation rules. This
fact, however, also leads to rule explosion, noise
and coverage problems (Auli et al., 2009), which
can hurt translation quality. Flexibility of function
word usage, rich morphology and paraphrasing all
add to the difficulty of rule extraction. In addition,
restricting target word orders by hard translation
rules can also hurt output fluency.
?
* Work done while visiting Singapore University of
Technology and Design (SUTD)
Figure 1: Overall system architecture.
A potential solution to the problems above is to
treat translation as a generation task, represent-
ing syntactic correspondences using soft features.
Both adequacy and fluency can potentially be im-
proved by giving full flexibility to target synthe-
sis, and leaving all options to the statistical model.
The main challenge to this method is a signifi-
cant increase in the search space (Knight, 1999).
To this end, recent advances in tackling complex
search tasks for text generation offer some so-
lutions (White and Rajkumar, 2009; Zhang and
Clark, 2011).
In this short paper, we present a preliminary in-
vestigation on the possibility of building a syn-
tactic SMT system that does not use hard transla-
tion rules, by utilizing recent advances in statisti-
cal natural language generation (NLG). The over-
all architecture is shown in Figure 1. Translation
is performed by first parsing the source sentence,
then transferring source words and phrases to their
target equivalences, and finally synthesizing the
target output.
We choose dependency grammar for both the
source and the target syntax, and adapt the syntac-
tic text synthesis system of Zhang (2013), which
performs dependency-based linearization. The
linearization task for MT is different from the
monolingual task in that not all translation options
are used to build the output, and that bilingual cor-
respondences need to be taken into account dur-
177
ing synthesis. The algorithms of Zhang (2013) are
modified to perform word selection as well as or-
dering, using two sets of features to control trans-
lation adequacy and fluency, respectively.
Preliminary experiments on the IWSLT1 2010
data show that the system gives BLEU compara-
ble to traditional tree-to-string and string-to-tree
translation systems. It demonstrates the feasibility
of leveraging statistical NLG techniques for SMT,
and the possibility of building a statistical transfer-
based MT system.
2 Approach
The main goal being proof of concept, we keep
the system simple by utilizing existing methods
for the main components, minimizing engineer-
ing efforts. Shown in Figure 1, the end-to-end
system consists of two main components: lexical
transfer and synthesis. The former provides can-
didate translations for (overlapping) source words
and phrases. Although lexicons and rules can
be used for this step, we take a simple statisti-
cal alignment-based approach. The latter searches
for a target translation by constructing dependency
trees bottom-up. The process can be viewed as
a syntax-based generation process from a bag of
overlapping translation options.
2.1 Lexical transfer
We perform word alignment using IBM model 4
(Brown et al., 1993), and then extract phrase pairs
according to the alignment and automatically-
annotated target syntax. In particular, consistent
(Och et al., 1999) and cohesive (Fox, 2002) phrase
pairs are extracted from intersected alignments in
both directions: the target side must form a pro-
jective span, with a single root, and the source side
must be contiguous. A resulting phrase pair con-
sists of the source phrase, its target translation, as
well as the head position and head part-of-speech
(POS) of the target span, which are useful for tar-
get synthesis. We further restrict that neither the
source nor the target side of a valid phrase pair
contains over s words.
Given an input source sentence, the lexical
transfer unit finds all valid target translation op-
tions for overlapping source phrases up to size s,
and feeds them as inputs to the target synthesis de-
coder. The translation options with a probability
1International Workshop on Spoken Language Transla-
tion, http://iwslt2010.fbk.eu
below ? ? P
max
are filtered out, where P
max
is the
probability of the most probable translation. Here
the probability of a target translation is calculated
as the count of the translation divided by the count
of all translations of the source phrase.
2.2 Synthesis
The synthesis module is based on the monolingual
text synthesis algorithm of Zhang (2013), which
constructs an ordered dependency tree given a bag
of words. In the bilingual setting, inputs to the al-
gorithm are translation options, which can be over-
lapping and mutually exclusive, and not necessar-
ily all of which are included in the output. As a
result, the decoder needs to perform word selec-
tion in addition to word ordering. Another differ-
ence between the bilingual and monolingual set-
tings is that the former requires translation ade-
quacy in addition to output fluency.
We largely rely on the monolingual system for
MT decoding. To deal with overlapping transla-
tion options, a source coverage vector is used to
impose mutual exclusiveness on input words and
phrases. Each element in the coverage vector is
a binary value that indicates whether a particular
source word has been translated in the correspond-
ing target hypothesis. For translation adequacy,
we use a set of bilingual features on top of the set
of monolingual features for text synthesis.
2.2.1 Search
The search algorithm is the best-first algorithm of
Zhang (2013). Each search hypothesis is a par-
tial or full target-language dependency tree, and
hypotheses are constructed bottom-up from leaf
nodes, which are translation options. An agenda
is used to maintain a list of search hypothesis to
be expanded, and a chart is used to record a set
of accepted hypotheses. Initially empty, the chart
is a beam of size k ? n, where n is the number
of source words and k is a positive integer. The
agenda is a priority queue, initialized with all leaf
hypotheses (i.e. translation options). At each step,
the highest-scored hypothesis e is popped off the
agenda, and expanded by combination with all hy-
potheses on the chart in all possible ways, with
the set of newly generated hypotheses e
1
, e
2
, ...e
N
being put onto the agenda, and e being put onto
the chart. When two hypotheses are combined,
they can be put in two different orders, and in each
case different dependencies can be constructed be-
tween their head words, leading to different new
178
dependency syntax
WORD(h) ? POS(h) ? NORM(size) ,
WORD(h) ? NORM(size), POS(h) ? NORM(size)
POS(h) ? POS(m) ? POS(b) ? dir
POS(h) ? POS(h
l
) ? POS(m) ? POS(m
r
) ? dir (h > m),
POS(h) ? POS(h
r
) ? POS(m) ? POS(m
l
) ? dir (h < m)
WORD(h) ? POS(m) ? POS(m
l
) ? dir ,
WORD(h) ? POS(m) ? POS(m
r
) ? dir
POS(h) ? POS(m) ? POS(m
1
) ? dir ,
POS(h) ? POS(m
1
) ? dir , POS(m) ? POS(m
1
) ? dir
WORD(h) ? POS(m) ? POS(m
1
) ? POS(m
2
) ? dir ,
POS(h) ? POS(m) ? POS(m
1
) ? POS(m
2
) ? dir ,
...
dependency syntax for completed words
WORD(h) ? POS(h) ? WORD(h
l
) ? POS(h
l
),
POS(h) ? POS(h
l
),
WORD(h) ? POS(h) ? POS(h
l
),
POS(h) ? WORD(h
l
) ? POS(h
l
) ,
WORD(h) ? POS(h) ? WORD(h
r
) ? POS(h
r
),
POS(h) ? POS(h
r
),
...
surface string patterns (B?bordering index)
WORD(B ? 1) ? WORD(B), POS(B ? 1) ? POS(B),
WORD(B ? 1) ? POS(B), POS(B ? 1) ? WORD(B),
WORD(B ? 1) ? WORD(B) ? WORD(B + 1),
WORD(B ? 2) ? WORD(B ? 1) ? WORD(B),
POS(B ? 1) ? POS(B) ? POS(B + 1),
...
surface string patterns for complete sentences
WORD(0), WORD(0) ? WORD(1),
WORD(size ? 1),
WORD(size ? 1) ? WORD(size ? 2),
POS(0), POS(0) ? POS(1),
POS(0) ? POS(1) ? POS(2),
...
Table 1: Monolingual feature templates.
hypotheses. The decoder expands a fixed number
L hypotheses, and then takes the highest-scored
chart hypothesis that contains over ? ? n words as
the output, where ? is a real number near 1.0.
2.2.2 Model and training
A scaled linear model is used by the decoder to
score search hypotheses:
Score(e) =
~
? ? ?(e)
|e|
,
where ?(e) is the global feature vector of the hy-
pothesis e, ~? is the parameter vector of the model,
and |e| is the number of leaf nodes in e. The
scaling factor |e| is necessary because hypothe-
ses with different numbers of words are compared
with each other in the search process to capture
translation equivalence.
While the monolingual features of Zhang
(2013) are applied (example feature templates
from the system are shown in Table 1), an addi-
tional set of bilingual features is defined, shown
phrase translation features
PHRASE(m) ? PHRASE(t), P (trans),
bilingual syntactic features
POS(th) ? POS(tm) ? dir ? LEN(path),
WORD(th) ? POS(tm) ? dir ? LEN(path),
POS(th) ? WORD(tm) ? dir ? LEN(path),
WORD(th) ? WORD(tm) ? dir ? LEN(path),
WORD(sh) ? WORD(sm) ? dir ? LEN(path),
WORD(sh) ? WORD(th) ? dir ? LEN(path),
WORD(sm) ? WORD(tm) ? dir ? LEN(path),
bilingual syntactic features (LEN(path) ? 3)
POS(th) ? POS(tm) ? dir ? LABELS(path),
WORD(th) ? POS(tm) ? dir ? LABELS(path),
POS(th) ? WORD(tm) ? dir ? LABELS(path),
WORD(th) ? WORD(tm) ? dir ? LABELS(path),
WORD(sh) ? WORD(sm) ? dir ? LABELS(path),
WORD(sh) ? WORD(th) ? dir ? LABELS(path),
WORD(sm) ? WORD(tm) ? dir ? LABELS(path),
POS(th) ? POS(tm) ? dir ? LABELSPOS(path),
WORD(th) ? POS(tm) ? dir ? LABELSPOS(path),
POS(th) ? WORD(tm) ? dir ? LABELSPOS(path),
WORD(th) ? WORD(tm) ? dir ? LABELSPOS(path),
WORD(sh) ? WORD(sm) ? dir ? LABELSPOS(path),
WORD(sh) ? WORD(th) ? dir ? LABELSPOS(path),
WORD(sm) ? WORD(tm) ? dir ? LABELSPOS(path),
Table 2: Bilingual feature templates.
in Table 2. In the tables, s and t represent the
source and target, respectively; h and m repre-
sent the head and modifier in a dependency arc,
respectively; h
l
and h
r
represent the neighboring
words on the left and right of h, respectively; m
l
and m
r
represent the neighboring words on the left
and right of m, respectively; m
1
and m
2
repre-
sent the closest and second closest sibling of m on
the side of h, respectively. dir represents the arc
direction (i.e. left or right); PHRASE represents
a lexical phrase; P(trans) represents the source-
to-target translation probability from the phrase-
table, used as a real-valued feature; path repre-
sents the shortest path in the source dependency
tree between the two nodes that correspond to the
target head and modifier, respectively; LEN(path)
represents the number of arcs on path, normalized
to bins of [5, 10, 20, 40+]; LABELS(path) repre-
sents the array of dependency arc labels on path;
LABELSPOS(path) represents the array of depen-
dency arc labels and source POS on path. In addi-
tion, a real-valued four-gram language model fea-
ture is also used, with four-grams extracted from
the surface boundary when two hypothesis are
combined.
We apply the discriminative learning algorithm
of Zhang (2013) to train the parameters ~?. The al-
gorithm requires training examples that consist of
full target derivations, with leaf nodes being input
translation options. However, the readily available
179
training examples are automatically-parsed target
derivations, with leaf nodes being the reference
translation. As a result, we apply a search pro-
cedure to find a derivation process, through which
the target dependency tree is constructed from a
subset of input translation options. The search
procedure can be treated as a constrained decod-
ing process, where only the oracle tree and its sub
trees can be constructed. In case the set of transla-
tion options cannot lead to the oracle tree, we ig-
nore the training instance.2 Although the ignored
training sentence pairs cannot be utilized for train-
ing the discriminative synthesizer, they are never-
theless used for building the phrase table and train-
ing the language model.
3 Experiments
We perform experiments on the IWSLT 2010
Chinese-English dataset, which consists of train-
ing sentence pairs from the dialog task (dialog)
and Basic Travel and Expression Corpus (BTEC).
The union of dialog and BTEC are taken as our
training set, which contains 30,033 sentence pairs.
For system tuning, we use the IWSLT 2004 test set
(also released as the second development test set
of IWSLT 2010), which contains 500 sentences.
For final test, we use the IWSLT 2003 test set (also
released as the first development test set of IWSLT
2010), which contains 506 sentences.
The Chinese sentences in the datasets are seg-
mented using NiuTrans3 (Xiao et al., 2012), while
POS-tagging of both English and Chinese is per-
formed using ZPar4 version 0.5 (Zhang and Clark,
2011). We train the English POS-tagger using the
WSJ sections of the Penn Treebank (Marcus et al.,
1993), turned into lower-case. For syntactic pars-
ing of both English and Chinese, we use the de-
fault models of ZPar 0.5.
We choose three baseline systems: a string-to-
tree (S2T) system, a tree-to-string (T2S) system
and a tree-to-tree (T2T) system (Koehn, 2010).
The Moses release 1.0 implementations of all
three systems are used, with default parameter set-
tings. IRSTLM5 release 5.80.03 (Federico et al.,
2008) is used to train a four-gram language models
2This led to the ignoring of over 40% of the training sen-
tence pairs. For future work, we will consider substitute or-
acles from reachable target derivations by using maximum
sentence level BLEU approximation (Nakov et al., 2012) or
METEOR (Denkowski and Lavie, 2011) as selection criteria.
3http://www.nlplab.com/NiuPlan/NiuTrans.ch.html
4http://sourceforge.net/projects/zpar/
5http://sourceforge.net/apps/mediawiki/irstlm
System T2S S2T T2T OURS
BLEU 32.65 36.07 28.46 34.24
Table 3: Final results.
SOURCE:?????????
REF: I have a terrible headache .
OURS: now , I have a headache .
SOURCE:??????????
REF: I ?d like a twin room with a bath please .
OURS: a twin room , I ?ll find a room with a bath .
SOURCE:??????????
REF: can you change yen into dollars ?
OURS: please change yen into dollars .
SOURCE:????? ?
REF: roast chicken , please .
OURS: please have roast chicken .
SOURCE:?????????
REF: take two tablets after every meal .
OURS: please eat after each meal .
SOURCE:????
REF: check , please .
OURS: I have to check - out , please .
SOURCE:?????????????
REF: yes , well , that ?s our specialty .
OURS: ah , the food that ?s right .
SOURCE:?????
REF: my air conditioner is n?t working .
OURS: the air - conditioner does n?t work .
Table 4: Sample output sentences.
over the English training data, which is applied to
the baseline systems and our system. Kneser-Ney
smoothing is used to train the language model.
We use the tuning set to determine the optimal
number of training iterations. The translation op-
tion filter ? is set to 0.1; the phrase size limit s is
set to 5 in order to verify the effectiveness of syn-
thesis; the number of expanded nodes L is set to
200; the chart factor k is set to 16 for a balance be-
tween efficiency and accuracy; the goal parameter
? is set to 0.8.
The final scores of our system and the baselines
are shown in Table 3. Our system gives a BLEU
of 34.24, which is comparable to the baseline sys-
tems. Some example outputs are shown in Table 4.
Manual comparison does not show significant dif-
ferences in overall translation adequacy or fluency
between the outputs of the four systems. However,
an observation is that, while our system can pro-
duce more fluent outputs, the choice of translation
options can be more frequently incorrect. This
suggests that while the target synthesis component
is effective under the bilingual setting, a stronger
lexical selection component may be necessary for
better translation quality.
180
4 Related work
As discussed in the introduction, our work is
closely related to previous studies on syntactic
MT, with the salient difference that we do not rely
on hard translation rules, but allow free target syn-
thesis. The contrast can be summarized as ?trans-
lation by parsing? vs ?translation by generation?.
There has been a line of research on genera-
tion for translation. Soricut and Marcu (2006) use
a form of weighted IDL-expressions (Nederhof
and Satta, 2004) for generation. Bangalore et al.
(2007) treats MT as a combination of global lex-
ical transfer and word ordering; their generation
component does not perform lexical selection, re-
lying on an n-gram language model to order target
words. Goto et al. (2012) use a monotonic phrase-
based system to perform target word selection, and
treats target ordering as a post-processing step.
More recently, Chen et al. (2014) translate source
dependencies arc-by-arc to generate pseudo target
dependencies, and generate the translation by re-
ordering of arcs. In contrast with these systems,
our system relies more heavily on a syntax-based
synthesis component, in order to study the useful-
ness of statistical NLG on SMT.
With respect to syntax-based word ordering,
Chang and Toutanova (2007) and He et al. (2009)
study a simplified word ordering problem by as-
suming that the un-ordered target dependency tree
is given. Wan et al. (2009) and Zhang and Clark
(2011) study the ordering of a bag of words, with-
out input syntax. Zhang et al. (2012), Zhang
(2013) and Song et al. (2014) further extended this
line of research by adding input syntax and allow-
ing joint inflection and ordering. de Gispert et al.
(2014) use a phrase-structure grammer for word
ordering. Our generation system is based on the
work of Zhang (2013), but further allows lexical
selection.
Our work is also in line with the work of Liang
et al. (2006), Blunsom et al. (2008), Flanigan et
al. (2013) and Yu et al. (2013) in that we build a
discriminative model for SMT.
5 Conclusion
We investigated a novel system for syntactic ma-
chine translation, treating MT as an unconstrained
generation task, solved by using a single discrim-
inative model with both monolingual syntax and
bilingual translation features. Syntactic corre-
spondence is captured by using soft features rather
than hard translation rules, which are used by most
syntax-based statistical methods in the literature.
Our results are preliminary in the sense that
the experiments were performed using a relatively
small dataset, and little engineering effort was
made on fine-tuning of parameters for the base-
line and proposed models. Our Python imple-
mentation gives the same level of BLEU scores
compared with baseline syntactic SMT systems,
but is an order of magnitude slower than Moses.
However, the results demonstrate the feasibility of
leveraging text generation techniques for machine
translation, directly connecting the two currently
rather separated research fields. The system is not
strongly dependent on the specific generation al-
gorithm, and one potential of the SMT architec-
ture is that it can directly benefit from advances in
statistical NLG technology.
Acknowledgement
The work has been supported by the Singa-
pore Ministration of Education Tier 2 project
T2MOE201301 and the startup grant SRG ISTD
2012 038 from SUTD. We thank the anonymous
reviewers for their constructive comments.
References
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. WMT, pages 224?
232.
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical machine translation through
global lexical selection and sentence reconstruction.
In Proc. ACL, pages 152?159.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL, pages 200?208.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Pi-Chuan Chang and Kristina Toutanova. 2007. A dis-
criminative syntactic word order model for machine
translation. In Proc. ACL, pages 9?16.
Hongshen Chen, Jun Xie, Fandong Meng, Wenbin
Jiang, and Qun Liu. 2014. A dependency edge-
based transfer model for statistical machine transla-
tion. In Proc. COLING 2014, pages 1103?1113.
181
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
ACL, pages 263?270.
Adria` de Gispert, Marcus Tomalin, and Bill Byrne.
2014. Word ordering with phrase-based grammars.
In Proc. EACL, pages 259?268.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Proc.
WMT, pages 85?91.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proc. In-
terspeech, pages 1618?1621.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proc. NAACL, pages 248?258.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. EMNLP, pages 304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL, pages 273?280.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for Japanese-English sta-
tistical machine translation. In Proc. ACL, pages
311?316.
Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu.
2009. Dependency based Chinese sentence realiza-
tion. In Proc. ACL/AFNLP, pages 809?816.
Kevin Knight. 1999. Squibs and Discussions: Decod-
ing Complexity in Word-Replacement Translation
Models. Computational Linguistics, 25(4):607?
615.
Phillip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In Proc. COLING/ACL, pages
761?768.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. COLING/ACL, pages 609?616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. EMNLP, pages 44?52.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The penn treebank. Com-
putational linguistics, 19(2):313?330.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proc. Coling, pages
1979?1994.
Mark-Jan Nederhof and Giorgio Satta. 2004. Idl-
expressions: a formalism for representing and pars-
ing finite languages in natural language processing.
J. Artif. Intell. Res.(JAIR), 21:287?317.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statis-
tical machine translation. In Proc. EMNLP, pages
20?28.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proc. ACL, pages 271?279.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proc. EMNLP, pages 495?504.
Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.
2014. Joint morphological generation and syntactic
linearization. In Proc. AAAI, pages 1522?1528.
Radu Soricut and Daniel Marcu. 2006. Stochastic lan-
guage generation using widl-expressions and its ap-
plication in machine translation and summarization.
In Proc. ACL, pages 1105?1112.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proc. EACL, pages 852?860.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Proc.
the EMNLP, pages 410?419.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012. NiuTrans: An open source toolkit for phrase-
based and syntax-based machine translation. In
Proc. ACL Demos, pages 19?24.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proc. EMNLP, pages 216?226.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proc. EMNLP,
pages 1112?1123.
Yue Zhang and Stephen Clark. 2011. Syntax-based
grammaticality improvement using CCG and guided
search. In Proc. EMNLP, pages 1147?1157.
Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating a
large-scale language model. In Proc. EACL, pages
736?746.
Yue Zhang. 2013. Partial-tree linearization: General-
ized word ordering for text synthesis. In Proc. IJ-
CAI, pages 2232?2238.
182
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 739?748,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Boosting-based System Combination for Machine Translation 
 
Tong Xiao, Jingbo Zhu, Muhua Zhu, Huizhen Wang 
 
Natural Language Processing Lab.  
Northeastern University, China 
{xiaotong,zhujingbo,wanghuizhen}@mail.neu.edu.cn 
zhumuhua@gmail.com 
 
 
Abstract 
In this paper, we present a simple and effective 
method to address the issue of how to generate 
diversified translation systems from a single 
Statistical Machine Translation (SMT) engine 
for system combination. Our method is based 
on the framework of boosting. First, a se-
quence of weak translation systems is gener-
ated from a baseline system in an iterative 
manner. Then, a strong translation system is 
built from the ensemble of these weak transla-
tion systems. To adapt boosting to SMT sys-
tem combination, several key components of 
the original boosting algorithms are redes-
igned in this work. We evaluate our method on 
Chinese-to-English Machine Translation (MT) 
tasks in three baseline systems, including a 
phrase-based system, a hierarchical phrase-
based system and a syntax-based system. The 
experimental results on three NIST evaluation 
test sets show that our method leads to signifi-
cant improvements in translation accuracy 
over the baseline systems. 
1 Introduction 
Recent research on Statistical Machine Transla-
tion (SMT) has achieved substantial progress. 
Many SMT frameworks have been developed, 
including phrase-based SMT (Koehn et al, 2003), 
hierarchical phrase-based SMT (Chiang, 2005), 
syntax-based SMT (Eisner, 2003; Ding and 
Palmer, 2005; Liu et al, 2006; Galley et al, 2006; 
Cowan et al, 2006), etc. With the emergence of 
various structurally different SMT systems, more 
and more studies are focused on combining mul-
tiple SMT systems for achieving higher transla-
tion accuracy rather than using a single transla-
tion system. 
The basic idea of system combination is to ex-
tract or generate a translation by voting from an 
ensemble of translation outputs. Depending on 
how the translation is combined and what voting 
strategy is adopted, several methods can be used 
for system combination, e.g. sentence-level com-
bination (Hildebrand and Vogel, 2008) simply 
selects one from original translations, while 
some more sophisticated methods, such as word-
level and phrase-level combination (Matusov et 
al., 2006; Rosti et al, 2007), can generate new 
translations differing from any of the original 
translations. 
One of the key factors in SMT system combi-
nation is the diversity in the ensemble of transla-
tion outputs (Macherey and Och, 2007). To ob-
tain diversified translation outputs, most of the 
current system combination methods require 
multiple translation engines based on different 
models. However, this requirement cannot be 
met in many cases, since we do not always have 
the access to multiple SMT engines due to the 
high cost of developing and tuning SMT systems. 
To reduce the burden of system development, it 
might be a nice way to combine a set of transla-
tion systems built from a single translation en-
gine. A key issue here is how to generate an en-
semble of diversified translation systems from a 
single translation engine in a principled way. 
Addressing this issue, we propose a boosting-
based system combination method to learn a 
combined translation system from a single SMT 
engine. In this method, a sequence of weak trans-
lation systems is generated from a baseline sys-
tem in an iterative manner. In each iteration, a 
new weak translation system is learned, focusing 
more on the sentences that are relatively poorly 
translated by the previous weak translation sys-
tem. Finally, a strong translation system is built 
from the ensemble of the weak translation sys-
tems. 
Our experiments are conducted on Chinese-to-
English translation in three state-of-the-art SMT 
systems, including a phrase-based system, a hier-
archical phrase-based system and a syntax-based 
739
Input:   a model u, a sequence of (training) samples {(f1, r1), ..., (fm, rm)} where fi is the 
i-th source sentence, and ri is the set of reference translations for fi. 
Output: a new translation system 
Initialize: D1(i) = 1 / m for all i = 1, ..., m 
For t = 1, ..., T 
1. Train a translation system u(?*t) on {(fi, ri)} using distribution Dt 
2. Calculate the error rate t? of u(?*t) on {(fi, ri)} 
3. Set 
1 1ln( )
2
t
t
t
?? ?
+=                                                         (3)
4. Update weights 
1
( )( )
t il
t
t
t
D i eD i
Z
? ?
+ =                                                    (4)
            where li is the loss on the i-th training sample, and Zt is the normalization factor. 
Output the final system:  
v(u(?*1), ..., u (?*T)) 
Figure 1: Boosting-based System Combination 
system. All the systems are evaluated on three 
NIST MT evaluation test sets. Experimental re-
sults show that our method leads to significant 
improvements in translation accuracy over the 
baseline systems. 
2 Background 
Given a source string f, the goal of SMT is to 
find a target string e* by the following equation. 
* arg max(Pr( | ))
e
e e f=                (1) 
where Pr( | )e f is the probability that e is the 
translation of the given source string f. To model 
the posterior probability Pr( | )e f , most of the 
state-of-the-art SMT systems utilize the log-
linear model proposed by Och and Ney (2002), 
as follows, 
1
' 1
exp( ( , ))
Pr( | )
exp( ( , '))
M
m m
m
M
m m
e m
h f e
e f
h f e
?
?
=
=
?= ?
?
? ?      (2) 
where {hm( f, e ) | m = 1, ..., M} is a set of fea-
tures, and ?m is the feature weight corresponding 
to the m-th feature. hm( f, e ) can be regarded as a 
function that maps every pair of source string f 
and target string e into a non-negative value, and 
?m can be viewed as the contribution of hm( f, e ) 
to the overall score Pr( | )e f . 
In this paper, u denotes a log-linear model that 
has M fixed features {h1( f ,e ), ..., hM( f ,e )}, ? = 
{?1, ..., ?M} denotes the M parameters of u, and 
u(?) denotes a SMT system based on u with pa-
rameters ?. Generally, ? is trained on a training 
data set1 to obtain an optimized weight vector ?* 
and consequently an optimized system u(?*). 
3 Boosting-based System Combination 
for Single Translation Engine  
Suppose that there are T available SMT systems 
{u1(?*1), ..., uT(?*T)}, the task of system combina-
tion is to build a new translation system 
v(u1(?*1), ..., uT(?*T)) from {u1(?*1), ..., uT(?*T)}. 
Here v(u1(?*1), ..., uT(?*T)) denotes the combina-
tion system which combines translations from the 
ensemble of the output of each ui(?*i). We call 
ui(?*i) a member system of v(u1(?*1), ..., uT(?*T)). 
As discussed in Section 1, the diversity among 
the outputs of member systems is an important 
factor to the success of system combination. To 
obtain diversified member systems, traditional 
methods concentrate more on using structurally 
different member systems, that is u1? u2 ?...? 
uT. However, this constraint condition cannot be 
satisfied when multiple translation engines are 
not available.  
In this paper, we argue that the diversified 
member systems can also be generated from a 
single engine u(?*) by adjusting the weight vector 
?* in a principled way. In this work, we assume 
that u1 = u2 =...= uT  = u. Our goal is to find a se-
ries of ?*i and build a combined system from 
{u(?*i)}. To achieve this goal, we propose a 
                                                 
1 The data set used for weight training is generally called 
development set or tuning set in the SMT field. In this paper, 
we use the term training set to emphasize the training of 
log-linear model. 
740
boosting-based system combination method (Fig-
ure 1). 
Like other boosting algorithms, such as 
AdaBoost (Freund and Schapire, 1997; Schapire, 
2001), the basic idea of this method is to use 
weak systems (member systems) to form a strong 
system (combined system) by repeatedly calling 
weak system trainer on different distributions 
over the training samples. However, since most 
of the boosting algorithms are designed for the 
classification problem that is very different from 
the translation problem in natural language proc-
essing, several key components have to be redes-
igned when boosting is adapted to SMT system 
combination. 
3.1 Training 
In this work, Minimum Error Rate Training 
(MERT) proposed by Och (2003) is used to es-
timate feature weights ? over a series of training 
samples. As in other state-of-the-art SMT sys-
tems, BLEU is selected as the accuracy measure 
to define the error function used in MERT. Since 
the weights of training samples are not taken into 
account in BLEU2, we modify the original defi-
nition of BLEU to make it sensitive to the distri-
bution Dt(i) over the training samples. The modi-
fied version of BLEU is called weighted BLEU 
(WBLEU) in this paper. 
Let E = e1 ... em be the translations produced 
by the system, R = r1 ... rm be the reference trans-
lations where ri = {ri1, ..., riN}, and Dt(i) be the 
weight of the i-th training sample (fi, ri). The 
weighted BLEU metric has the following form: 
{ }
( )
11 1
11
1/ 4
m
4 1 1
m
1
1
  WBLEU( , )
( ) min | ( ) |
exp 1 max 1,
( ) | ( ) |
( ) ( ) ( )
     (5)
( ) ( )
m
ijti j N
m
iti
N
i ijt n ni j
in t ni
E R
D i g r
D i g e
D i g e g r
D i g e
= ? ?
=
= =
= =
? ?? ?? ?? ?= ? ?? ?? ?? ?? ?? ?? ?
? ?? ?? ?? ?? ?
?
?
?? ?
I U
 
where ( )ng s  is the multi-set of all n-grams in a 
string s. In this definition, n-grams in ei and {rij} 
are weighted by Dt(i). If the i-th training sample 
has a larger weight, the corresponding n-grams 
will have more contributions to the overall score 
WBLEU( , )E R . As a result, the i-th training 
sample gains more importance in MERT. Obvi-
                                                 
2 In this paper, we use the NIST definition of BLEU where 
the effective reference length is the length of the shortest 
reference translation. 
ously the original BLEU is just a special case of 
WBLEU when all the training samples are 
equally weighted. 
As the weighted BLEU is used to measure the 
translation accuracy on the training set, the error 
rate is defined to be: 
1 WBLEU( , )t E R? = ?               (6) 
3.2 Re-weighting 
Another key point is the maintaining of the dis-
tribution Dt(i) over the training set. Initially all 
the weights of training samples are set equally. 
On each round, we increase the weights of the 
samples that are relatively poorly translated by 
the current weak system so that the MERT-based 
trainer can focus on the hard samples in next 
round. The update rule is given in Equation 4 
with two parameters t?  and li in it. 
t?  can be regarded as a measure of the im-
portance that the t-th weak system gains in boost-
ing. The definition of t?  guarantees that t?  al-
ways has a positive value3. A main effect of t?  
is to scale the weight updating (e.g. a larger t?  
means a greater update). 
li is the loss on the i-th sample. For each i, let 
{ei1, ..., ein} be the n-best translation candidates 
produced by the system. The loss function is de-
fined to be: 
*
1
1BLEU( , ) BLEU( , )ki i i ij i
j
l e e
k =
= ? ?r r  (7) 
where BLEU(eij, ri) is the smoothed sentence-level 
BLEU score (Liang et al, 2006) of the transla-
tion e with respect to the reference translations ri, 
and ei* is the oracle translation which is selected 
from {ei1, ..., ein} in terms of BLEU(eij, ri). li can 
be viewed as a measure of the average cost that 
we guess the top-k translation candidates instead 
of the oracle translation. The value of li counts 
for the magnitude of weight update, that is, a lar-
ger li means a larger weight update on Dt(i). The 
definition of the loss function here is similar to 
the one used in (Chiang et al, 2008) where only 
the top-1 translation candidate (i.e. k = 1) is 
taken into account. 
3.3 System Combination Scheme 
In the last step of our method, a strong transla-
tion system v(u(?*1), ..., u(?*T)) is built from the 
                                                 
3 Note that the definition of t?  here is different from that in 
the original AdaBoost algorithm (Freund and Schapire, 
1997; Schapire, 2001) where t?  is a negative number when 
0.5t? > . 
741
ensemble of member systems {u(?*1), ..., u(?*T)}. 
In this work, a sentence-level combination 
method is used to select the best translation from 
the pool of the n-best outputs of all the member 
systems.  
Let H(u(?*t)) (or Ht for short) be the set of the 
n-best translation candidates produced by the t-th 
member system u(?*t), and H(v) be the union set 
of all Ht (i.e. ( ) tH v H=U ). The final translation 
is generated from H(v) based on the following 
scoring function: 
*
1
( )
arg max ( ) ( , ( ))T t tt
e H v
e e e H v? ? ?=?= ? +?    (8) 
where ( )t e?  is the log-scaled model score of e in 
the t-th member system, and t?  is the corre-
sponding feature weight. It should be noted that 
ie H?  may not exist in any 'i iH ? . In this case, 
we can still calculate the model score of e in any 
other member systems, since all the member sys-
tems are based on the same model and share the 
same feature space. ( , ( ))e H v?  is a consensus-
based scoring function which has been success-
fully adopted in SMT system combination (Duan 
et al, 2009; Hildebrand and Vogel, 2008; Li et 
al., 2009). The computation of ( , ( ))e H v?  is 
based on a linear combination of a set of n-gram 
consensuses-based features.  
( , ( )) ( , ( ))n n
n
e H v h e H v? ? + += ? +?  
( , ( ))n n
n
h e H v? ? ???            (9) 
For each order of n-gram, ( , ( ))nh e H v
+ and 
( , ( ))nh e H v
?  are defined to measure the n-gram 
agreement and disagreement between e and other 
translation candidates in H(v), respectively. n? +  
and n? ? are the feature weights corresponding to 
( , ( ))nh e H v
+ and ( , ( ))nh e H v
? . As ( , ( ))nh e H v
+ and 
( , ( ))nh e H v
?  used in our work are exactly the 
same as the features used in (Duan et al, 2009) 
and similar to the features used in (Hildebrand 
and Vogel, 2008; Li et al, 2009), we do not pre-
sent the detailed description of them in this paper. 
If p orders of n-gram are used in computing 
( , ( ))e H v? , the total number of features in the 
system combination will be 2T p+ ? (T model-
score-based features defined in Equation 8 and 
2 p?  consensus-based features defined in Equa-
tion 9). Since all these features are combined 
linearly, we use MERT to optimize them for the 
combination model. 
4 Optimization 
If implemented naively, the translation speed of 
the final translation system will be very slow. 
For a given input sentence, each member system 
has to encode it individually, and the translation 
speed is inversely proportional to the number of 
member systems generated by our method. For-
tunately, with the thought of computation, there 
are a number of optimizations that can make the 
system much more efficient in practice. 
A simple solution is to run member systems in 
parallel when translating a new sentence. Since 
all the member systems share the same data re-
sources, such as language model and translation 
table, we only need to keep one copy of the re-
quired resources in memory. The translation 
speed just depends on the computing power of 
parallel computation environment, such as the 
number of CPUs. 
Furthermore, we can use joint decoding tech-
niques to save the computation of the equivalent 
translation hypotheses among member systems. 
In joint decoding of member systems, the search 
space is structured as a translation hypergraph 
where the member systems can share their trans-
lation hypotheses. If more than one member sys-
tems share the same translation hypothesis, we 
just need to compute the corresponding feature 
values only once, instead of repeating the com-
putation in individual decoders. In our experi-
ments, we find that over 60% translation hy-
potheses can be shared among member systems 
when the number of member systems is over 4. 
This result indicates that promising speed im-
provement can be achieved by using the joint 
decoding and hypothesis sharing techniques. 
Another method to speed up the system is to 
accelerate n-gram language model with n-gram 
caching techniques. In this method, a n-gram 
cache is used to store the most frequently and 
recently accessed n-grams. When a new n-gram 
is accessed during decoding, the cache is 
checked first. If the required n-gram hits the 
cache, the corresponding n-gram probability is 
returned by the cached copy rather than re-
fetching the original data in language model. As 
the translation speed of SMT system depends 
heavily on the computation of n-gram language 
model, the acceleration of n-gram language 
model generally leads to substantial speed-up of 
SMT system. In our implementation, the n-gram 
caching in general brings us over 30% speed im-
provement of the system. 
742
5 Experiments  
Our experiments are conducted on Chinese-to-
English translation in three SMT systems. 
5.1 Baseline Systems 
The first SMT system is a phrase-based system 
with two reordering models including the maxi-
mum entropy-based lexicalized reordering model 
proposed by Xiong et al (2006) and the hierar-
chical phrase reordering model proposed by Gal-
ley and Manning (2008). In this system all 
phrase pairs are limited to have source length of 
at most 3, and the reordering limit is set to 8 by 
default4. 
The second SMT system is an in-house reim-
plementation of the Hiero system which is based 
on the hierarchical phrase-based model proposed 
by Chiang (2005).  
The third SMT system is a syntax-based sys-
tem based on the string-to-tree model (Galley et 
al., 2006; Marcu et al, 2006), where both the 
minimal GHKM and SPMT rules are extracted 
from the bilingual text, and the composed rules 
are generated by combining two or three minimal 
GHKM and SPMT rules. Synchronous binariza-
tion (Zhang et al, 2006; Xiao et al, 2009) is per-
formed on each translation rule for the CKY-
style decoding. 
In this work, baseline system refers to the sys-
tem produced by the boosting-based system 
combination when the number of iterations (i.e. 
T ) is set to 1. To obtain satisfactory baseline per-
formance, we train each SMT system for 5 times 
using MERT with different initial values of fea-
ture weights to generate a group of baseline can-
didates, and then select the best-performing one 
from this group as the final baseline system (i.e. 
the starting point in the boosting process) for the 
following experiments. 
5.2 Experimental Setup 
Our bilingual data consists of 140K sentence 
pairs in the FBIS data set5. GIZA++ is employed 
to perform the bi-directional word alignment be-
tween the source and target sentences, and the 
final word alignment is generated using the inter-
sect-diag-grow method. All the word-aligned 
bilingual sentence pairs are used to extract 
phrases and rules for the baseline systems. A 5-
gram language model is trained on the target-side 
                                                 
4 Our in-house experimental results show that this system 
performs slightly better than Moses on Chinese-to-English 
translation tasks. 
5 LDC catalog number: LDC2003E14 
of the bilingual data and the Xinhua portion of 
English Gigaword corpus. Berkeley Parser is 
used to generate the English parse trees for the 
rule extraction of the syntax-based system. The 
data set used for weight training in boosting-
based system combination comes from NIST 
MT03 evaluation set. To speed up MERT, all the 
sentences with more than 20 Chinese words are 
removed. The test sets are the NIST evaluation 
sets of MT04, MT05 and MT06. The translation 
quality is evaluated in terms of case-insensitive 
NIST version BLEU metric. Statistical signifi-
cant test is conducted using the bootstrap re-
sampling method proposed by Koehn (2004). 
Beam search and cube pruning (Huang and 
Chiang, 2007) are used to prune the search space 
in all the three baseline systems. By default, both 
of the beam size and the size of n-best list are set 
to 20. 
In the settings of boosting-based system com-
bination, the maximum number of iterations is 
set to 30, and k (in Equation 7) is set to 5. The n-
gram consensuses-based features (in Equation 9) 
used in system combination ranges from unigram 
to 4-gram. 
5.3 Evaluation of Translations 
First we investigate the effectiveness of the 
boosting-based system combination on the three 
systems.  
Figures 2-5 show the BLEU curves on the de-
velopment and test sets, where the X-axis is the 
iteration number, and the Y-axis is the BLEU 
score of the system generated by the boosting-
based system combination. The points at itera-
tion 1 stand for the performance of the baseline 
systems. We see, first of all, that all the three 
systems are improved during iterations on the 
development set. This trend also holds on the test 
sets. After 5, 7 and 8 iterations, relatively stable 
improvements are achieved by the phrase-based 
system, the Hiero system and the syntax-based 
system, respectively. The BLEU scores tend to 
converge to the stable values after 20 iterations 
for all the systems. Figures 2-5 also show that the 
boosting-based system combination seems to be 
more helpful to the phrase-based system than to 
the Hiero system and the syntax-based system. 
For the phrase-based system, it yields over 0.6 
BLEU point gains just after the 3rd iteration on 
all the data sets.  
Table 1 summarizes the evaluation results, 
where the BLEU scores at iteration 5, 10, 15, 20 
and 30 are reported for the comparison. We see 
that the boosting-based system method stably ac- 
743
 33
 34
 35
 36
 37
 38
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT03 (dev.)
phrase-based
hiero
syntax-based
Figure 2: BLEU scores on the development set 
 33
 34
 35
 36
 37
 38
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT04 (test)
phrase-based
hiero
syntax-based
Figure 3: BLEU scores on the test  set of MT04 
 32
 33
 34
 35
 36
 37
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT05 (test)
phrase-based
hiero
syntax-based
Figure 4: BLEU scores on the test set of MT05 
 30
 31
 32
 33
 34
 35
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT06 (test)
phrase-based
hiero
syntax-based
Figure 5: BLEU scores on the test set of MT06 
 
Phrase-based Hiero Syntax-based 
Dev. MT04 MT05 MT06 Dev. MT04 MT05 MT06 Dev. MT04 MT05 MT06 
Baseline 33.21 33.68 32.68 30.59 33.42 34.30 33.24 30.62 35.84 35.71 35.11 32.43 
Baseline+600best 33.32 33.93 32.84 30.76 33.48 34.46 33.39 30.75 35.95 35.88 35.23 32.58 
Boosting-5Iterations 33.95* 34.32* 33.33* 31.33* 33.73 34.48 33.44 30.83 36.03 35.92 35.27 33.09 
Boosting-10Iterations 34.14* 34.68* 33.42* 31.35* 33.75 34.65 33.75* 31.02 36.14 36.39* 35.47 33.15*
Boosting-15Iterations 33.99* 34.78* 33.46* 31.45* 34.03* 34.88* 33.98* 31.20* 36.36* 36.46* 35.53* 33.43*
Boosting-20Iterations 34.09* 35.11* 33.56* 31.45* 34.17* 35.00* 34.04* 31.29* 36.44* 36.79* 35.77* 33.36*
Boosting-30Iterations 34.12* 35.16* 33.76* 31.59* 34.05* 34.99* 34.05* 31.30* 36.52* 36.81* 35.71* 33.46*
Table 1: Summary of the results (BLEU4[%]) on the development and test sets. * = significantly better 
than baseline (p < 0.05). 
  
hieves significant BLEU improvements after 15 
iterations, and the highest BLEU scores are gen-
erally yielded after 20 iterations.  
Also as shown in Table 1, over 0.7 BLEU 
point gains are obtained on the phrase-based sys-
tem after 10 iterations. The largest BLEU im-
provement on the phrase-based system is over 1 
BLEU point in most cases. These results reflect 
that our method is relatively more effective for 
the phrase-based system than for the other two 
systems, and thus confirms the fact we observed 
in Figures 2-5. 
We also investigate the impact of n-best list 
size on the performance of baseline systems. For 
the comparison, we show the performance of the 
baseline systems with the n-best list size of 600 
(Baseline+600best in Table 1) which equals to 
the maximum number of translation candidates 
accessed in the final combination system (combi- 
ne 30 member systems, i.e. Boosing-30Iterations). 
744
 15
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT03 (dev.)
phrase-based
hiero
syntax-based
Figure 6: Diversity on the development set 
 10
 15
 20
 25
 30
 35
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT04 (test)
phrase-based
hiero
syntax-based
Figure 7: Diversity on the test set of MT04 
 15
 20
 25
 30
 35
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT05 (test)
phrase-based
hiero
syntax-based
Figure 8: Diversity on the test set of MT05 
 15
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT06 (test)
phrase-based
hiero
syntax-based
Figure 9: Diversity on the test set of MT06 
 
As shown in Table 1, Baseline+600best obtains 
stable improvements over Baseline. It indicates 
that the access to larger n-best lists is helpful to 
improve the performance of baseline systems. 
However, the improvements achieved by Base-
line+600best are modest compared to the im-
provements achieved by Boosting-30Iterations. 
These results indicate that the SMT systems can 
benefit more from the diversified outputs of 
member systems rather than from larger n-best 
lists produced by a single system. 
5.4 Diversity among Member Systems 
We also study the change of diversity among the 
outputs of member systems during iterations. 
The diversity is measured in terms of the Trans-
lation Error Rate (TER) metric proposed in 
(Snover et al, 2006). A higher TER score means 
that more edit operations are performed if we 
transform one translation output into another 
translation output, and thus reflects a larger di-
versity between the two outputs. In this work, the 
TER score for a given group of member systems 
is calculated by averaging the TER scores be-
tween the outputs of each pair of member sys-
tems in this group. 
Figures 6-9 show the curves of diversity on 
the development and test sets, where the X-axis 
is the iteration number, and the Y-axis is the di-
versity. The points at iteration 1 stand for the 
diversities of baseline systems. In this work, the 
baseline?s diversity is the TER score of the group 
of baseline candidates that are generated in ad-
vance (Section 5.1). 
We see that the diversities of all the systems 
increase during iterations in most cases, though a 
few drops occur at a few points. It indicates that 
our method is very effective to generate diversi-
fied member systems. In addition, the diversities 
of baseline systems (iteration 1) are much lower 
745
than those of the systems generated by boosting 
(iterations 2-30). Together with the results shown 
in Figures 2-5, it confirms our motivation that 
the diversified translation outputs can lead to 
performance improvements over the baseline 
systems. 
Also as shown in Figures 6-9, the diversity of 
the Hiero system is much lower than that of the 
phrase-based and syntax-based systems at each 
individual setting of iteration number. This inter-
esting finding supports the observation that the 
performance of the Hiero system is relatively 
more stable than the other two systems as shown 
in Figures 2-5. The relative lack of diversity in 
the Hiero system might be due to the spurious 
ambiguity in Hiero derivations which generally 
results in very few different translations in trans-
lation outputs (Chiang, 2007). 
5.5 Evaluation of Oracle Translations 
In this set of experiments, we evaluate the oracle 
performance on the n-best lists of the baseline 
systems and the combined systems generated by 
boosting-based system combination. Our primary 
goal here is to study the impact of our method on 
the upper-bound performance.  
Table 2 shows the results, where Base-
line+600best stands for the top-600 translation 
candidates generated by the baseline systems, 
and Boosting-30iterations stands for the ensem-
ble of 30 member systems? top-20 translation 
candidates. As expected, the oracle performance 
of Boosting-30Iterations is significantly higher 
than that of Baseline+600best. This result indi-
cates that our method can provide much ?better? 
translation candidates for system combination 
than enlarging the size of n-best list naively. It 
also gives us a rational explanation for the sig-
nificant improvements achieved by our method 
as shown in Section 5.3. 
 
Data 
Set 
Method Phrase-
based 
Hiero Syntax-
based 
Baseline+600best 46.36 46.51 46.92 Dev. 
Boosting-30Iterations 47.78* 47.44* 48.70* 
Baseline+600best 43.94 44.52 46.88 MT04 
Boosting-30Iterations 45.97* 45.47* 49.40* 
Baseline+600best 42.32 42.47 45.21 MT05 
Boosting-30Iterations 44.82* 43.44* 47.02* 
Baseline+600best 39.47 39.39 40.52 MT06 
Boosting-30Iterations 41.51* 40.10* 41.88* 
Table 2: Oracle performance of various systems. 
* = significantly better than baseline (p < 0.05). 
6 Related Work 
Boosting is a machine learning (ML) method that 
has been well studied in the ML community 
(Freund, 1995; Freund and Schapire, 1997; 
Collins et al, 2002; Rudin et al, 2007), and has 
been successfully adopted in natural language 
processing (NLP) applications, such as document 
classification (Schapire and Singer, 2000) and 
named entity classification (Collins and Singer, 
1999). However, most of the previous work did 
not study the issue of how to improve a single 
SMT engine using boosting algorithms. To our 
knowledge, the only work addressing this issue is 
(Lagarda and Casacuberta, 2008) in which the 
boosting algorithm was adopted in phrase-based 
SMT. However, Lagarda and Casacuberta 
(2008)?s method calculated errors over the 
phrases that were chosen by phrase-based sys-
tems, and could not be applied to many other 
SMT systems, such as hierarchical phrase-based 
systems and syntax-based systems. Differing 
from Lagarda and Casacuberta?s work, we are 
concerned more with proposing a general 
framework which can work with most of the cur-
rent SMT models and empirically demonstrating 
its effectiveness on various SMT systems. 
There are also some other studies on building 
diverse translation systems from a single transla-
tion engine for system combination. The first 
attempt is (Macherey and Och, 2007). They em-
pirically showed that diverse translation systems 
could be generated by changing parameters at 
early-stages of the training procedure. Following 
Macherey and Och (2007)?s work, Duan et al 
(2009) proposed a feature subspace method to 
build a group of translation systems from various 
different sub-models of an existing SMT system. 
However, Duan et al (2009)?s method relied on 
the heuristics used in feature sub-space selection. 
For example, they used the remove-one-feature 
strategy and varied the order of n-gram language 
model to obtain a satisfactory group of diverse 
systems. Compared to Duan et al (2009)?s 
method, a main advantage of our method is that 
it can be applied to most of the SMT systems 
without designing any heuristics to adapt it to the 
specified systems. 
7 Discussion and Future Work 
Actually the method presented in this paper is 
doing something rather similar to Minimum 
Bayes Risk (MBR) methods. A main difference 
lies in that the consensus-based combination 
method here does not model the posterior prob-
ability of each hypothesis (i.e. all the hypotheses 
are assigned an equal posterior probability when 
we calculate the consensus-based features). 
746
Greater improvements are expected if MBR 
methods are used and consensus-based combina-
tion techniques smooth over noise in the MERT 
pipeline. 
In this work, we use a sentence-level system 
combination method to generate final transla-
tions. It is worth studying other more sophisti-
cated alternatives, such as word-level and 
phrase-level system combination, to further im-
prove the system performance. 
Another issue is how to determine an appro-
priate number of iterations for boosting-based 
system combination. It is especially important 
when our method is applied in the real-world 
applications. Our empirical study shows that the 
stable and satisfactory improvements can be 
achieved after 6-8 iterations, while the largest 
improvements can be achieved after 20 iterations. 
In our future work, we will study in-depth prin-
cipled ways to determine the appropriate number 
of iterations for boosting-based system combina-
tion. 
8 Conclusions 
We have proposed a boosting-based system com-
bination method to address the issue of building 
a strong translation system from a group of weak 
translation systems generated from a single SMT 
engine. We apply our method to three state-of-
the-art SMT systems, and conduct experiments 
on three NIST Chinese-to-English MT evalua-
tions test sets. The experimental results show that 
our method is very effective to improve the 
translation accuracy of the SMT systems. 
Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and the 
Fundamental Research Funds for the Central 
Universities (N090604008). The authors would 
like to thank the anonymous reviewers for their 
pertinent comments, Tongran Liu, Chunliang 
Zhang and Shujie Yao for their valuable sugges-
tions for improving this paper, and Tianning Li 
and Rushan Chen for developing parts of the 
baseline systems. 
References  
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. 
of ACL 2005, Ann Arbor, Michigan, pages 263-
270. 
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201-228. 
David Chiang, Yuval Marton and Philip Resnik. 2008. 
Online Large-Margin Training of Syntactic and 
Structural Translation Features. In Proc. of 
EMNLP 2008, Honolulu, pages 224-233. 
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In 
Proc. of EMNLP/VLC 1999, pages 100-110. 
Michael Collins, Robert Schapire and Yoram Singer. 
2002. Logistic Regression, AdaBoost and Bregman 
Distances. Machine Learning, 48(3): 253-285. 
Brooke Cowan, Ivona Ku?erov? and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. of EMNLP 2006, pages 232-241. 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. In Proc. of ACL 2005, Ann 
Arbor, Michigan, pages 541-548. 
Nan Duan, Mu Li, Tong Xiao and Ming Zhou. 2009. 
The Feature Subspace Method for SMT System 
Combination. In Proc. of EMNLP 2009, pages 
1096-1104. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Yoav Freund. 1995. Boosting a weak learning algo-
rithm by majority. Information and Computation, 
121(2): 256-285. 
Yoav Freund and Robert Schapire. 1997. A decision-
theoretic generalization of on-line learning and an 
application to boosting. Journal of Computer and 
System Sciences, 55(1):119-139. 
Michel Galley, Jonathan Graehl, Kevin Knight, 
Daniel Marcu, Steve DeNeefe, Wei Wang and 
Ignacio Thayer. 2006. Scalable inferences and 
training of context-rich syntax translation models. 
In Proc. of ACL 2006, Sydney, Australia, pages 
961-968. 
Michel Galley and Christopher D. Manning. 2008. A 
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP 2008, Hawaii, 
pages 848-856. 
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
Proc. of the 8th AMTA conference, pages 254-261. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. In Proc. of ACL 2007, Prague, Czech Re-
public, pages 144-151. 
747
Philipp Koehn, Franz Och and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In Proc. of 
HLT-NAACL 2003, Edmonton, USA, pages 48-54. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of 
EMNLP 2004, Barcelona, Spain, pages 388-395. 
Antonio Lagarda and Francisco Casacuberta. 2008. 
Applying Boosting to Statistical Machine Transla-
tion. In Proc. of the 12th EAMT conference, pages 
88-96. 
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li and 
Ming Zhou. 2009. Collaborative Decoding: Partial 
Hypothesis Re-Ranking Using Translation Consen-
sus between Decoders. In Proc. of ACL-IJCNLP 
2009, Singapore, pages 585-592. 
Percy Liang, Alexandre Bouchard-C?t?, Dan Klein 
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proc. of 
COLING/ACL 2006, pages 104-111. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. In Proc. of ACL 2006, pages 609-616. 
Wolfgang Macherey and Franz Och. 2007. An Em-
pirical Study on Computing Consensus Transla-
tions from Multiple Machine Translation Systems. 
In Proc. of EMNLP 2007, pages 986-995. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, Sydney, Aus-
tralia, pages 44-52. 
Evgeny Matusov, Nicola Ueffing and Hermann Ney. 
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced 
hypotheses alignment. In Proc. of EACL 2006, 
pages 33-40. 
Franz Och and Hermann Ney. 2002. Discriminative 
Training and Maximum Entropy Models for Statis-
tical Machine Translation. In Proc. of ACL 2002, 
Philadelphia, pages 295-302. 
Franz Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of ACL 
2003, Japan, pages 160-167. 
Antti-Veikko Rosti, Spyros Matsoukas and Richard 
Schwartz. 2007. Improved Word-Level System 
Combination for Machine Translation. In Proc. of 
ACL 2007, pages 312-319. 
Cynthia Rudin, Robert Schapire and Ingrid Daube-
chies. 2007. Analysis of boosting algorithms using 
the smooth margin function.  The Annals of Statis-
tics, 35(6): 2723-2768. 
Robert Schapire and Yoram Singer. 2000. BoosTexter: 
A boosting-based system for text categorization. 
Machine Learning, 39(2/3):135-168. 
Robert Schapire. The boosting approach to machine 
learning: an overview. 2001. In Proc. of MSRI 
Workshop on Nonlinear Estimation and Classifica-
tion, Berkeley, CA, USA, pages 1-23. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proc. of the 7th AMTA confer-
ence, pages 223-231. 
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and 
Ming Zhou. 2009. Better Synchronous Binarization 
for Machine Translation. In Proc. of EMNLP 2009, 
Singapore, pages 362-370. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proc. of ACL 
2006, Sydney, pages 521-528. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006, 
New York, USA, pages 256- 263. 
748
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 418?423,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Decoding Generalization for Tree-to-String Translation 
 
 
Jingbo Zhu 
 
Tong Xiao 
Natural Language Processing Laboratory Natural Language Processing Laboratory 
Northeastern University, Shenyang, China Northeastern University, Shenyang, China 
zhujingbo@mail.neu.edu.cn xiaotong@mail.neu.edu.cn 
 
 
 
 
Abstract 
To address the parse error issue for tree-to-
string translation, this paper proposes a 
similarity-based decoding generation (SDG) 
solution by reconstructing similar source 
parse trees for decoding at the decoding 
time instead of taking multiple source parse 
trees as input for decoding. Experiments on 
Chinese-English translation demonstrated 
that our approach can achieve a significant 
improvement over the standard method, 
and has little impact on decoding speed in 
practice. Our approach is very easy to im-
plement, and can be applied to other para-
digms such as tree-to-tree models.  
1 Introduction 
Among linguistically syntax-based statistical ma-
chine translation (SMT) approaches, the tree-to-
string model (Huang et al 2006; Liu et al 2006) is 
the simplest and fastest, in which parse trees on 
source side are used for grammar extraction and 
decoding. Formally, given a source (e.g., Chinese) 
string c and its auto-parsed tree T1-best, the goal of 
typical tree-to-string SMT is to find a target (e.g., 
English) string e* by the following equation as 
),|Pr(maxarg 1
*
best
e
Tcee ?=                  (1) 
where Pr(e|c,T1-best) is the probability that e is the 
translation of the given source string c and its T1-best. 
A typical tree-to-string decoder aims to search for 
the best derivation among all consistent derivations 
that convert source tree into a target-language 
string. We call this set of consistent derivations the 
tree-to-string search space. Each derivation in the 
search space respects the source parse tree.  
Parsing errors on source parse trees would cause 
negative effects on tree-to-string translation due to 
decoding on incorrect source parse trees. To ad-
dress the parse error issue in tree-to-string transla-
tion, a natural solution is to use n-best parse trees 
instead of 1-best parse tree as input for decoding, 
which can be expressed by 
),|Pr(maxarg* bestn
e
Tcee ?=              (2) 
where <Tn-best> denotes a set of n-best parse trees 
of c produced by a state-of-the-art syntactic parser. 
A simple alternative (Xiao et al 2010) to generate 
<Tn-best> is to utilize multiple parsers, which can 
improve the diversity among source parse trees in 
<Tn-best>. In this solution, the most representative 
work is the forest-based translation method (Mi et 
al. 2008; Mi and Huang 2008; Zhang et al 2009) 
in which a packed forest (forest for short) structure 
is used to effectively represent <Tn-best> for decod-
ing. Forest-based approaches can increase the tree-
to-string search space for decoding, but face a non-
trivial problem of high decoding time complexity 
in practice. 
In this paper, we propose a new solution by re-
constructing new similar source parse trees for de-
coding, referred to as similarity-based decoding 
generation (SDG), which is expressed as 
}),{,|Pr(maxarg
),|Pr(maxarg
1
1
*
simbest
e
best
e
TTce
Tcee
?
?
?
=
       (3) 
where <Tsim> denotes a set of similar parse trees of 
T1-best that are dynamically reconstructed at the de-
418
coding time. Roughly speaking, <Tn-best> is a sub-
set of {T1-best, <Tsim>}. Along this line of thinking, 
Equation (2) can be considered as a special case of 
Equation (3).  
In our SDG solution, given a source parse tree 
T1-best, the key is how to generate its <Tsim> at the 
decoding time. In practice, it is almost intractable 
to directly reconstructing <Tsim> in advance as in-
put for decoding due to too high computation com-
plexity. To address this crucial challenge, this 
paper presents a simple and effective technique 
based on similarity-based matching constraints to 
construct new similar source parse trees for decod-
ing at the decoding time. Our SDG approach can 
explicitly increase the tree-to-string search space 
for decoding without changing any grammar ex-
traction and pruning settings, and has little impact 
on decoding speed in practice.  
2 Tree-to-String Derivation 
We choose the tree-to-string paradigm in our study 
because this is the simplest and fastest among syn-
tax-based models, and has been shown to be one of 
the state-of-the-art syntax-based models. Typically, 
by using the GHKM algorithm (Galley et al 2004), 
translation rules are learned from word-aligned 
bilingual texts whose source side has been parsed 
by using a syntactic parser. Each rule consists of a 
syntax tree in the source language having some 
words (terminals) or variables (nonterminals) at 
leaves, and sequence words or variables in the tar-
get language. With the help of these learned trans-
lation rules, the goal of tree-to-string decoding is to 
search for the best derivation that converts the 
source tree into a target-language string. A deriva-
tion is a sequence of translation steps (i.e., the use 
of translation rules).  
Figure 1 shows an example derivation d that 
performs translation over a Chinese source parse 
tree, and how this process works. In the first step, 
we can apply rule r1 at the root node that matches a 
subtree {IP[1] (NP[2] VP[3])}. The corresponding 
target side {x1 x2} means to preserve the top-level 
word-order in the translation, and results in two 
unfinished subtrees with root labels NP[2] and VP[3], 
respectively. The rule r2 finishes the translation on 
the subtree of NP[2], in which the Chinese word 
???? is translated into an English string ?the 
Chinese side?. The rule r3 is applied to perform 
translation on the subtree of VP[3], and results in an  
 
An example tree-to-string derivation d consisting of five 
translation rules is given as follows: 
r1: IP[1] (x1:NP[2] x2:VP[3]) ? x1 x2 
r2: NP[2] (NN (??)) ? the Chinese side 
r3: VP[3] (ADVP(AD(??)) VP(VV(??) AS(?) 
x1:NP[4])) ? highly appreciated x1 
r4: NP[4] (DP(DT(?) CLP(M(?))) x1:NP[5]) ? this x1 
r5: NP[5] (NN(??)) ? talk 
Translation results: The Chinese side highly appreciated 
this talk. 
 
Figure 1. An example derivation performs translation 
over the Chinese parse tree T.  
 
unfinished subtree of NP[4]. Similarly, rules r4 and 
r5 are sequentially used to finish the translation on 
the remaining. This process is a depth-first search 
over the whole source tree, and visits every node 
only once. 
3 Decoding Generalization 
3.1 Similarity-based Matching Constraints 
In typical tree-to-string decoding, an ordered se-
quence of rules can be reassembled to form a deri-
vation d whose source side matches the given 
source parse tree T. The source side of each rule in 
d should match one of subtrees of T, referred to as 
matching constraint. Before discussing how to ap-
ply our similarity-based matching constraints to 
reconstruct new similar source parse trees for de-
coding at the decoding time, we first define the 
similarity between two tree-to-string rules. 
 
Definition 1 Given two tree-to-string rules t and u, 
we say that t and u are similar such that their 
source sides ts and us have the same root label and 
frontier nodes, written as ut ? , otherwise not.  
419
 
Figure 2: Two similar tree-to-string rules. (a) rule r3 
used by the example derivation d in Figure 1, and (b) a 
similar rule ?3 of r3.  
 
Here we use an example figure to explain our 
similarity-based matching constraint scheme (simi-
larity-based scheme for short). 
 
 
Figure 3: (a) a typical tree-to-string derivation d using 
rule t, and (b) a new derivation d* is generated by the 
similarity-based matching constraint scheme by using 
rule t* instead of rule t, where t* t? . 
 
Given a source-language parse tree T, in typical 
tree-to-string matching constraint scheme shown in 
Figure 3(a), rule t used by the derivation d should 
match a substree ABC of T. In our similarity-based 
scheme, the similar rule t* ( t? ) is used to form a 
new derivation d* that performs translation over 
the same source sentence {w1 ... wn}. In such a case, 
this new derivation d* can yield a new similar 
parse tree T* of T. 
Since an incorrect source parse tree might filter 
out good derivations during tree-to-string decoding, 
our similarity-based scheme is much more likely to 
recover the correct tree for decoding at the decod-
ing time, and does not rule out good (potentially 
correct) translation choices. In our method, many 
new source-language trees T* that are similar to but 
different from the original source tree T can be re-
constructed at the decoding time. In theory our 
similarity-based scheme can increase the search 
space of the tree-to-string decoder, but we did not 
change any rule extraction and pruning settings.  
In practice, our similarity-based scheme can ef-
fectively keep the advantage of fast decoding for 
tree-to-string translation because its implementa-
tion is very simple. Let?s revisit the example deri-
vation d in Figure 1, i.e., d=r1?r2?r3?r4?r51. In 
such a case, the decoder can effectively produce a 
new derivation d* by simply replacing rule r3 with 
its similar rule ?3 ( 3r? ) shown in Figure 2, that is, 
d*=r1?r2??3?r4?r5.  
With beam search, typical tree-to-string decod-
ing with an integrated language model can run in 
time2 O(ncb2) in practice (Huang 2007). For our 
decoding time complexity computation, only the 
parameter c value can be affected by our similar-
ity-based scheme. In other words, our similarity-
based scheme would result in a larger c value at 
decoding time as many similar translation rules 
might be matched at each node. In practice, there 
are two feasible optimization techniques to allevi-
ate this problem. The first technique is to limit the 
maximum number of similar translation rules 
matched at each node. The second one is to prede-
fine a similarity threshold to filter out less similar 
translation rules in advance.  
In the implementation, we add a new feature 
into the model: similarity-based matching counting 
feature. This feature counts the number of similar 
rules used to form the derivation. The weight ?sim 
of this feature is tuned via minimal error rate train-
ing (MERT) (Och 2003) with other feature weights. 
3.2 Pseudo-rule Generation 
In the implementation of tree-to-string decoding, 
the first step is to load all translation rules matched 
at each node of the source tree T. It is possible that 
some nonterminal nodes do not have any matched 
rules when decoding some new sentences. If the 
root node of the source tree has no any matched 
rules, it would cause decoding failure. To tackle 
this problem, motivated by ?glue? rules (Chiang 
2005), for some node S without any matched rules, 
we introduce a special pseudo-rule which reassem-
bles all child nodes with local reordering to form 
new translation rules for S to complete decoding. 
                                                          
1 The symbol?denotes the composition (leftmost substitution) 
operation of two tree-to-string rules. 
2 Where n is the number of words, b is the size of the beam, 
and c is the number of translation rules matched at each node.   
420
               S                S(x1:A x2:B x3:C x4:D)?x1 x2 x3 x4               
                                 S(x1:A x2:B x3:C x4:D)?x2 x1 x3 x4           
                                 S(x1:A x2:B x3:C x4:D)?x1 x3 x2 x4 
    A     B     C     D   S(x1:A x2:B x3:C x4:D)?x1 x2 x4 x3 
              (a)                                         (b) 
Figure 4: (a) An example unseen substree, and (b) its 
four pseudo-rules. 
 
Figure 4 (a) depicts an example unseen substree 
where no any rules is matched at its root node S.  
Its simplest pseudo-rule is to simply combine a 
sequence of S?s child nodes. To give the model 
more options to build partial translations, we util-
ize a local reordering technique in which any two 
adjacent frontier (child) nodes are reordered during 
decoding. Figure 4(b) shows four pseudo-rules in 
total generated from this example unseen substree.   
In the implementation, we add a new feature to 
the model: pseudo-rule counting feature. This fea-
ture counts the number of pseudo-rules used to 
form the derivation. The weight ?pseudo of this fea-
ture is tuned via MERT with other feature weights.   
4 Evaluation 
4.1 Setup 
Our bilingual training data consists of 140K Chi-
nese-English sentence pairs in the FBIS data set. 
For rule extraction, the minimal GHKM rules (Gal-
ley et al 2004) were extracted from the bitext, and 
the composed rules were generated by combining 
two or three minimal GHKM rules. A 5-gram lan-
guage model was trained on the target-side of the 
bilingual data and the Xinhua portion of English 
Gigaword corpus. The beam size for beam search 
was set to 20. The base feature set used for all sys-
tems is similar to that used in (Marcu et al 2006), 
including 14 base features in total such as 5-gram 
language model, bidirectional lexical and phrase-
based translation probabilities. All features were 
linearly combined and their weights are optimized 
by using MERT. The development data set used 
for weight training in our approaches comes from 
NIST MT03 evaluation set. To speed up MERT, 
sentences with more than 20 words were removed 
from the development set (Dev set). The test sets 
are the NIST MT04 and MT05 evaluation sets. The 
translation quality was evaluated in terms of case-
insensitive NIST version BLEU metric. Statistical 
significance test was conducted by using the boot-
strap re-sampling method (Koehn 2004). 
4.2 Results 
MT04 MT05  DEV
MT03 <=20 ALL <=20 ALL 
Baseline 32.99 36.54 32.70 34.61 30.60 
This 
work 
34.67*
(+1.68)
36.99+
(+0.45)
35.03* 
(+2.33) 
35.16+ 
(+0.55) 
33.12*
(+2.52)
Table 1. BLEU4 (%) scores of various methods on Dev 
set (MT03) and two test sets (MT04 and MT05). Each 
small test set (<=20) was built by removing the sen-
tences with more than 20 words from the full set (ALL). 
+ and * indicate significantly better on performance 
comparison at p < .05 and p < .01, respectively. 
 
Table 1 depicts the BLEU scores of various meth-
ods on the Dev set and four test sets. Compared to 
typical tree-to-string decoding (the baseline), our 
method can achieve significant improvements on 
all datasets. It is noteworthy that the improvement 
achieved by our approach on full test sets is bigger 
than that on small test sets. For example, our 
method results in an improvement of 2.52 BLEU 
points over the baseline on the MT05 full test set, 
but only 0.55 points on the MT05 small test set. As 
mentioned before, tree-to-string approaches are 
more vulnerable to parsing errors. In practice, the 
Berkeley parser (Petrov et al 2006) we used yields 
unsatisfactory parsing performance on some long 
sentences in the full test sets. In such a case, it 
would result in negative effects on the performance 
of the baseline method on the full test sets. Ex-
perimental results show that our SDG approach 
can effectively alleviate this problem, and signifi-
cantly improve tree-to-string translation.  
 
 Another issue we are interested in is the decod-
ing speed of our method in practice. To investigate 
this issue, we evaluate the average decoding speed 
of our SDG method and the baseline on the Dev set 
and all test sets.  
 
Decoding Time 
(seconds per sentence) 
  
<=20 ALL 
Baseline 0.43s 1.1s 
This work 0.50s 1.3s 
Table 2. Average decoding speed of various methods on 
small (<=20) and full (ALL) datasets in terms of sec-
onds per sentence. The parsing time of each sentence is 
not included. The decoders were implemented in C++ 
codes on an X86-based PC with two processors of 
2.4GHZ and 4GB physical memory.  
 
421
Table 2 shows that our approach only has little 
impact on decoding speed in practice, compared to 
the typical tree-to-string decoding (baseline). No-
tice that in these comparisons our method did not 
adopt any optimization techniques mentioned in 
Section 3.1, e.g., to limit the maximum number of 
similar rules matched at each node. It is obviously 
that the use of such an optimization technique can 
effectively increase the decoding speed of our 
method, but might hurt the performance in practice.  
Besides, to speed up decoding long sentences, it 
seems a feasible solution to first divide a long sen-
tence into multiple short sub-sentences for decod-
ing, e.g., based on comma. In other words, we can 
segment a complex source-language parse tree into 
multiple smaller subtrees for decoding, and com-
bine the translations of these small subtrees to form 
the final translation. This practical solution can 
speed up the decoding on long sentences in real-
world MT applications, but might hurt the transla-
tion performance. 
For convenience, here we call the rule ?3 in Fig-
ure 2(b) similar-rules. It is worth investigating how 
many similar-rules and pseudo-rules are used to 
form the best derivations in our similarity-based 
scheme. To do it, we count the number of similar-
rules and pseudo-rules used to form the best deri-
vations when decoding on the MT05 full set. Ex-
perimental results show that on average 13.97% of 
rules used to form the best derivations are similar-
rules, and one pseudo-rule per sentence is used. 
Roughly speaking, average five similar-rules per 
sentence are utilized for decoding generalization.  
5 Related Work 
String-to-tree SMT approaches also utilize the 
similarity-based matching constraint on target side 
to generate target translation. This paper applies it 
on source side to reconstruct new similar source 
parse trees for decoding at the decoding time, 
which aims to increase the tree-to-string search 
space for decoding, and improve decoding gener-
alization for tree-to-string translation.  
The most related work is the forest-based trans-
lation method (Mi et al 2008; Mi and Huang 2008; 
Zhang et al 2009) in which rule extraction and 
decoding are implemented over k-best parse trees 
(e.g., in the form of packed forest) instead of one 
best tree as translation input. Liu and Liu (2010) 
proposed a joint parsing and translation model by 
casting tree-based translation as parsing (Eisner 
2003), in which the decoder does not respect the 
source tree. These methods can increase the tree-
to-string search space. However, the decoding time 
complexity of their methods is high, i.e., more than 
ten or several dozen times slower than typical tree-
to-string decoding (Liu and Liu 2010).  
Some previous efforts utilized the techniques of 
soft syntactic constraints to increase the search 
space in hierarchical phrase-based models (Marton 
and Resnik 2008; Chiang et al 2009; Huang et al 
2010), string-to-tree models (Venugopal et al 
2009) or tree-to-tree (Chiang 2010) systems. These 
methods focus on softening matching constraints 
on the root label of each rule regardless of its in-
ternal tree structure, and often generate many new 
syntactic categories3. It makes them more difficult 
to satisfy syntactic constraints for the tree-to-string 
decoding.  
6 Conclusion and Future Work 
This paper addresses the parse error issue for tree-
to-string translation, and proposes a similarity-
based decoding generation solution by reconstruct-
ing new similar source parse trees for decoding at 
the decoding time. It is noteworthy that our SDG 
approach is very easy to implement. In principle, 
forest-based and tree sequence-based approaches 
improve rule coverage by changing the rule extrac-
tion settings, and use exact tree-to-string matching 
constraints for decoding. Since our SDG approach 
is independent of any rule extraction and pruning 
techniques, it is also applicable to forest-based ap-
proaches or other tree-based translation models, 
e.g., in the case of casting tree-to-tree translation as 
tree parsing (Eisner 2003). 
Acknowledgments 
We would like to thank Feiliang Ren, Muhua Zhu 
and Hao Zhang for discussions and the anonymous 
reviewers for comments. This research was sup-
ported in part by the National Science Foundation 
of China (60873091; 61073140), the Specialized 
Research Fund for the Doctoral Program of Higher 
Education (20100042110031) and the Fundamental 
Research Funds for the Central Universities in 
China. 
                                                          
3 Latent syntactic categories were introduced in the method of 
Huang et al (2010). 
422
References  
Chiang David. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proc. of 
ACL2005, pp263-270 
Chiang David. 2010. Learning to translate with source 
and target syntax. In Proc. of ACL2010, pp1443-
1452 
Chiang David, Kevin Knight and Wei Wang. 2009. 
11,001 new features for statistical machine transla-
tion. In Proc. of NAACL2009, pp218-226  
Eisner Jason. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. of ACL 2003, 
pp205-208. 
Galley Michel, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What's in a translation rule? In Proc. of 
HLT-NAACL 2004, pp273-280. 
Huang Liang. 2007. Binarization, synchronous binariza-
tion and target-side binarization. In Proc. of NAACL 
Workshop on Syntax and Structure in Statistical 
Translation.  
Huang Liang and David Chiang. 2007. Forest rescoring: 
Faster decoding with integrated language models. In 
Proc. of ACL 2007, pp144-151. 
Huang Liang, Kevin Knight and Aravind Joshi. 2006. 
Statistical syntax-directed translation with extended 
domain of locality. In Proc. of AMTA 2006, pp66-73. 
Huang Zhongqiang, Martin Cmejrek and Bowen Zhou. 
2010. Soft syntactic constraints for hierarchical 
phrase-based translation using latent syntactic distri-
bution. In Proc. of EMNLP2010, pp138-147 
Koehn Philipp. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of EMNLP 
2004, pp388-395. 
Liu Yang and Qun Liu. 2010. Joint parsing and transla-
tion. In Proc. of Coling2010, pp707-715 
Liu Yang, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of COLING/ACL 2006, pp609-
616. 
Marcu Daniel, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phrases. 
In Proc. of EMNLP 2006, pp44-52. 
Marton Yuval and Philip Resnik. 2008. Soft syntactic 
constraints for hierarchical phrase-based translation. 
In Proc. of ACL08, pp1003-1011 
Mi Haitao and Liang Huang. 2008. Forest-based Trans-
lation Rule Extraction. In Proc. of EMNLP 2008, 
pp206-214. 
Mi Haitao, Liang Huang and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL2008.  
Och Franz Josef. 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL2003. 
Petrov Slav, Leon Barrett, Roman Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL2006, 
pp433-440. 
Xiao Tong, Jingbo Zhu, Hao Zhang and Muhua Zhu. 
2010. An empirical study of translation rule extrac-
tion with multiple parsers. In Proc. of Coling2010, 
pp1345-1353 
Venugopal Ashish, Andreas Zollmann, Noah A. Smith 
and Stephan Vogel. 2009. Preference grammars: sof-
tening syntactic constraints to improve statistical ma-
chine translation. In Proc. of NAACL2009, pp236-
244 
Zhang Hui, Min Zhang, Haizhou Li, Aiti Aw and Chew 
Lim Tan. 2009. Forest-based tree sequence to string 
translation model. In Proc. of ACL-IJCNLP2009, 
pp172-180 
 
423
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 715?719,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Better Automatic Treebank Conversion Using A Feature-Based Approach
Muhua Zhu Jingbo Zhu Minghan Hu
Natural Language Processing Lab.
Northeastern University, China
zhumuhua@gmail.com
zhujingbo@mail.neu.edu.cn huminghan@ise.neu.edu.cn
Abstract
For the task of automatic treebank conversion,
this paper presents a feature-based approach
which encodes bracketing structures in a tree-
bank into features to guide the conversion of
this treebank to a different standard. Exper-
iments on two Chinese treebanks show that
our approach improves conversion accuracy
by 1.31% over a strong baseline.
1 Introduction
In the field of syntactic parsing, research efforts have
been put onto the task of automatic conversion of
a treebank (source treebank) to fit a different stan-
dard which is exhibited by another treebank (tar-
get treebank). Treebank conversion is desirable pri-
marily because source-style and target-style annota-
tions exist for non-overlapping text samples so that a
larger target-style treebank can be obtained through
such conversion. Hereafter, source and target tree-
banks are named as heterogenous treebanks due to
their different annotation standards. In this paper,
we focus on the scenario of conversion between
phrase-structure heterogeneous treebanks (Wang et
al., 1994; Zhu and Zhu, 2010).
Due to the availability of annotation in a source
treebank, it is natural to use such annotation to
guide treebank conversion. The motivating idea is
illustrated in Fig. 1 which depicts a sentence anno-
tated with standards of Tsinghua Chinese Treebank
(TCT) (Zhou, 1996) and Penn Chinese Treebank
(CTB) (Xue et al, 2002), respectively. Suppose
that the conversion is in the direction from the TCT-
style parse (left side) to the CTB-style parse (right
side). The constituents vp:[?/will??/surrender],
dj:[??/enemy?/will??/surrender], and np:[?
?/intelligence??/experts] in the TCT-style parse
strongly suggest a resulting CTB-style parse also
bracket the words as constituents. Zhu and
Zhu (2010) show the effectiveness of using brack-
eting structures in a source treebank (source-side
bracketing structures in short) as parsing constraints
during the decoding phase of a target treebank-based
parser.
However, using source-side bracketing structures
as parsing constraints is problematic in some cases.
As illustrated in the shadow part of Fig. 1, the TCT-
style parse takes ???/deems? as the right bound-
ary of a constituent while in the CTB-style parse,
???? is the left boundary of a constituent. Ac-
cording to the criteria used in Zhu and Zhu (2010),
any CTB-style constituents with ???? being the
left boundary are thought to be inconsistent with the
bracketing structure of the TCT-style parse and will
be pruned. However, if we prune such ?inconsistent?
constituents, the correct conversion result (right side
of Fig. 1) has no chance to be generated.
The problem comes from binary distinctions used
in the approach of Zhu and Zhu (2010). With bi-
nary distinctions, constituents generated by a target
treebank-based parser are judged to be either con-
sistent or inconsistent with source-side bracketing
structures. That approach prunes inconsistent con-
stituents which instead might be correct conversion
results1. In this paper, we insist on using source-
side bracketing structures as guiding information.
Meanwhile, we aim to avoid using binary distinc-
tions. To achieve such a goal, we propose to use a
feature-based approach to treebank conversion and
to encode source-side bracketing structures as a set
1To show how severe this problem might be, Section 3.1
presents statistics on inconsistence between TCT and CTB.
715
zj
dj
np
n
??
n
??
v
??
,
,
dj
n
??
vp
d
?
v
??
IP
NP
NN
??
NN
??
VP
VV
??
PU
,
IP
NP
NN
??
VP
AD
?
VV
??
?? ?? ?? , ?? ? ??
qingbao zhuanjia renwei , diren jiang touxiang
intelligence experts deem , enemy will surrender
Figure 1: An example sentence with TCT-style annotation (left) and CTB-style annotation (right).
of features. The advantage is that inconsistent con-
stituents can be scored with a function based on the
features rather than ruled out as impossible.
To test the efficacy of our approach, we conduct
experiments on conversion from TCT to CTB. The
results show that our approach achieves a 1.31% ab-
solute improvement in conversion accuracy over the
approach used in Zhu and Zhu (2010).
2 Our Approach
2.1 Generic System Architecture
To conduct treebank conversion, our approach, over-
all speaking, proceeds in the following steps.
Step 1: Build a parser (named source parser) on a
source treebank, and use it to parse sentences
in the training data of a target treebank.
Step 2: Build a parser on pairs of golden target-
style and auto-assigned (in Step 1) source-style
parses in the training data of the target tree-
bank. Such a parser is named heterogeneous
parser since it incorporates information derived
from both source and target treebanks, which
follow different annotation standards.
Step 3: In the testing phase, the heterogeneous
parser takes golden source-style parses as input
and conducts treebank conversion. This will be
explained in detail in Section 2.2.
To instantiate the generic framework described
above, we need to decide the following three factors:
(1) a parsing model for building a source parser, (2)
a parsing model for building a heterogeneous parser,
and (3) features for building a heterogeneous parser.
In principle, any off-the-shelf parsers can be used
to build a source parser, so we focus only on the
latter two factors. To build a heterogeneous parser,
we use feature-based parsing algorithms in order to
easily incorporate features that encode source-side
bracketing structures. Theoretically, any feature-
based approaches are applicable, such as Finkel et
al. (2008) and Tsuruoka et al (2009). In this pa-
per, we use the shift-reduce parsing algorithm for its
simplicity and competitive performance.
2.2 Shift-Reduce-Based Heterogeneous Parser
The heterogeneous parser used in this paper is based
on the shift-reduce parsing algorithm described in
Sagae and Lavie (2006a) and Wang et al (2006).
Shift-reduce parsing is a state transition process,
where a state is defined to be a tuple ?S,Q?. Here, S
is a stack containing partial parses, and Q is a queue
containing word-POS pairs to be processed. At each
state transition, a shift-reduce parser either shifts the
top item of Q onto S, or reduces the top one (or two)
items on S.
A shift-reduce-based heterogeneous parser pro-
ceeds similarly as the standard shift-reduce parsing
algorithm. In the training phase, each target-style
parse tree in the training data is transformed into
a binary tree (Charniak et al, 1998) and then de-
composed into a (golden) action-state sequence. A
classifier can be trained on the set of action-states,
716
where each state is represented as a feature vector.
In the testing phase, the trained classifier is used
to choose actions for state transition. Moreover,
beam search strategies can be used to expand the
search space of a shift-reduce-based heterogeneous
parser (Sagae and Lavie, 2006a). To incorporate in-
formation on source-side bracketing structures, in
both training and testing phases, feature vectors rep-
resenting states ?S,Q? are augmented with features
that bridge the current state and the corresponding
source-style parse.
2.3 Features
This section describes the feature functions used to
build a heterogeneous parser on the training data
of a target treebank. The features can be divided
into two groups. The first group of features are
derived solely from target-style parse trees so they
are referred to as target side features. This group
of features are completely identical to those used in
Sagae and Lavie (2006a).
In addition, we have features extracted jointly
from target-style and source-style parse trees. These
features are generated by consulting a source-style
parse (referred to as ts) while we decompose a
target-style parse into an action-state sequence.
Here, si denote the ith item from the top of the
stack, and qi denote the ith item from the front
end of the queue. We refer to these features as
heterogeneous features.
Constituent features Fc(si, ts)
This feature schema covers three feature functions:
Fc(s1, ts), Fc(s2, ts), and Fc(s1 ? s2, ts), which
decide whether partial parses on stack S correspond
to a constituent in the source-style parse ts. That is,
Fc(si, ts)=+ if si has a bracketing match (ignoring
grammar labels) with any constituent in ts. s1?s2
represents a concatenation of spans of s1 and s2.
Relation feature Fr(Ns(s1), Ns(s2))
We first position the lowest node Ns(si) in ts,
which dominates the span of si. Then a feature
function Fr(Ns(s1), Ns(s2)) is defined to indicate
the relationship of Ns(s1) and Ns(s2). If Ns(s1)
is identical to or a sibling of Ns(s2), we say
Fr(Ns(s1), Ns(s2)) =+.
Features Bridging Source and Target Parses
Fc(s1, ts)=?
Fc(s2, ts)=+
Fc(s1?s2, ts)=+
Fr(Ns(s1), Ns(s2))=?
Ff (RF (s1), q1)=?
Fp(RF (s1), q1)= ?v ? dj ? zj ?,?
Table 1: An example of new features. Suppose we are
considering the sentence depicted in Fig. 1.
Frontier-words feature Ff (RF (s1), q1)
A feature function which decides whether the right
frontier word of s1 and q1 are in the same base
phrase in ts. Here, a base phrase is defined to be
any phrase which dominates no other phrases.
Path feature Fp(RF (s1), q1)
Syntactic path features are widely used in the litera-
ture of semantic role labeling (Gildea and Jurafsky,
2002) to encode information of both structures and
grammar labels. We define a string-valued feature
function Fp(RF (s1), q1) which connects the right
frontier word of s1 to q1 in ts.
To better understand the above feature func-
tions, we re-examine the example depicted in
Fig. 1. Suppose that we use a shift-reduce-based
heterogeneous parser to convert the TCT-style parse
to the CTB-style parse and that stack S currently
contains two partial parses: s2:[NP (NN??) (NN
??)] and s1: (VV ??). In such a state, we can
see that spans of both s2 and s1 ?s2 correspond to
constituents in ts but that of s1 does not. Moreover,
Ns(s1) is dj and Ns(s2) is np, so Ns(s1) and
Ns(s2) are neither identical nor sisters in ts. The
values of these features are collected in Table 1.
3 Experiments
3.1 Data Preparation and Performance Metric
In the experiments, we use two heterogeneous tree-
banks: CTB 5.1 and the TCT corpus released by
the CIPS-SIGHAN-2010 syntactic parsing competi-
tion2. We actually only use the training data of these
two corpora, that is, articles 001-270 and 400-1151
(18,100 sentences, 493,869 words) of CTB 5.1 and
2http://www.cipsc.org.cn/clp2010/task2 en.htm
717
the training data (17,529 sentences, 481,061 words)
of TCT.
To evaluate conversion accuracy, we use the
same test set (named Sample-TCT) as in Zhu and
Zhu (2010), which is a set of 150 sentences with
manually assigned CTB-style and TCT-style parse
trees. In Sample-TCT, 6.19% (215/3473) CTB-
style constituents are inconsistent with respect to the
TCT standard and 8.87% (231/2602) TCT-style con-
stituents are inconsistent with respect to the CTB
standard.
For all experiments, bracketing F1 is used as the
performance metric, provided by EVALB 3.
3.2 Implementation Issues
To implement a heterogeneous parser, we first build
a Berkeley parser (Petrov et al, 2006) on the TCT
training data and then use it to assign TCT-style
parses to sentences in the CTB training data. On
the ?updated? CTB training data, we build two shift-
reduce-based heterogeneous parsers by using max-
imum entropy classification model, without/with
beam search. Hereafter, the two heterogeneous
parsers are referred to as Basic-SR and Beam-SR, re-
spectively.
In the testing phase, Basic-SR and Beam-SR con-
vert TCT-style parse trees in Sample-TCT to the
CTB standard. The conversion results are evalu-
ated against corresponding CTB-style parse trees in
Sample-TCT. Before conducting treebank conver-
sion, we apply the POS adaptation method proposed
in Jiang et al (2009) to convert TCT-style POS tags
in the input to the CTB standard. The POS conver-
sion accuracy is 96.2% on Sample-TCT.
3.3 Results
Table 2 shows the results achieved by Basic-SR and
Beam-SR with heterogeneous features being added
incrementally. Here, baseline represents the systems
which use only target side features. From the table
we can see that heterogeneous features improve con-
version accuracy significantly. Specifically, adding
the constituent (Fc) features to Basic-SR (Beam-
SR) achieves a 2.79% (3%) improvement, adding
the relation (Fr) and frontier-word (Ff ) features
yields a 0.79% (0.98%) improvement, and adding
3http://nlp.cs.nyu.edu/evalb
System Features <= 40 words Unlimited
Basic-SR baseline 83.34 80.33
+Fc 85.89 83.12
+Fr, +Ff 85.47 83.91
+Fp 86.01 84.05
Beam-SR baseline 84.40 81.27
+Fc 86.30 84.27
+Fr, + Ff 87.00 85.25
+Fp 87.27 85.38
Table 2: Adding new features to baselines improve tree-
bank conversion accuracy significantly on Sample-TCT.
the path (Fp) feature achieves a 0.14% (0.13%) im-
provement. The path feature is not so effective as
expected, although it manages to achieve improve-
ments. One possible reason lies on the data sparse-
ness problem incurred by this feature.
Since we use the same training and testing data
as in Zhu and Zhu (2010), we can compare our
approach directly with the informed decoding ap-
proach used in that work. We find that Basic-SR
achieves very close conversion results (84.05% vs.
84.07%) and Beam-SR even outperforms the in-
formed decoding approach (85.38% vs. 84.07%)
with a 1.31% absolute improvement.
4 Related Work
For phrase-structure treebank conversion, Wang et
al. (1994) suggest to use source-side bracketing
structures to select conversion results from k-best
lists. The approach is quite generic in the sense that
it can be used for conversion between treebanks of
different grammar formalisms, such as from a de-
pendency treebank to a constituency treebank (Niu
et al, 2009). However, it suffers from limited
variations in k-best lists (Huang, 2008). Zhu and
Zhu (2010) propose to incorporate bracketing struc-
tures as parsing constraints in the decoding phase of
a CKY-style parser. Their approach shows signifi-
cant improvements over Wang et al (1994). How-
ever, it suffers from binary distinctions (consistent
or inconsistent), as discussed in Section 1.
The approach in this paper is reminiscent of
co-training (Blum and Mitchell, 1998; Sagae and
Lavie, 2006b) and up-training (Petrov et al, 2010).
Moreover, it coincides with the stacking method
used for dependency parser combination (Martins
718
et al, 2008; Nivre and McDonald, 2008), the
Pred method for domain adaptation (Daume? III and
Marcu, 2006), and the method for annotation adap-
tation of word segmentation and POS tagging (Jiang
et al, 2009). As one of the most related works,
Jiang and Liu (2009) present a similar approach to
conversion between dependency treebanks. In con-
trast to Jiang and Liu (2009), the task studied in this
paper, phrase-structure treebank conversion, is rel-
atively complicated and more efforts should be put
into feature engineering.
5 Conclusion
To avoid binary distinctions used in previous ap-
proaches to automatic treebank conversion, we pro-
posed in this paper a feature-based approach. Exper-
iments on two Chinese treebanks showed that our
approach outperformed the baseline system (Zhu
and Zhu, 2010) by 1.31%.
Acknowledgments
We thank Kenji Sagae for helpful discussions on the
implementation of shift-reduce parser and the three
anonymous reviewers for comments. This work was
supported in part by the National Science Founda-
tion of China (60873091; 61073140), Specialized
Research Fund for the Doctoral Program of Higher
Education (20100042110031), the Fundamental Re-
search Funds for the Central Universities and Nat-
ural Science Foundation of Liaoning Province of
China.
References
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of COLT 1998.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-Based Best-First Chart Parsing. In Pro-
ceedings of the Six Workshop on Very Large Corpora,
pages 127-133.
Hal Daume? III and Daniel Marcu. 2006. Domain Adap-
tation for Statistical Classifiers. Journal of Artifical
Intelligence Research, 26:101-166.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, Feature-Based Conditional
Random Fileds Parsing. In Proceedings of ACL 2008,
pages 959-967.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic La-
beling for Semantic Roles. Computational Linguis-
tics, 28(3):245-288.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-local Features. In Proceedings of
ACL, pages 824-831.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chinese
Word Segmentation and POS Tagging - A Case Study.
In Proceedings of ACL 2009, pages 522-530.
Wenbin Jiang and Qun Liu. 2009. Automatic Adapta-
tion of Annotation Standards for Dependency Parsing
? Using Projected Treebank As Source Corpus. In
Proceedings of IWPT 2009, pages 25-28.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stack Dependency Parsers. In
Proceedings of EMNLP 2008, pages 157-166.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting Heterogeneous Treebanks for Parsing. In Pro-
ceedings of ACL 2009, pages 46-54.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing Graph-Based and Transition-Based Dependency
Parsers. In Proceedings of ACL 2008, pages 950-958.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of ACL
2006, pages 433-440.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for Accurate Deter-
ministic Question Parsing. In Proceedings of EMNLP
2010, pages 705-713.
Kenji Sagae and Alon Lavie. 2006. A Best-First Prob-
abilistic Shift-Reduce Parser. In Proceedings of ACL-
COLING 2006, pages 691-698.
Kenji Sagae and Alon Lavie. 2006. Parser Combination
by Reparsing. In Proceedings of NAACL 2006, pages
129-132.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Anani-
adou. 2009. Fast Full Parsing by Linear-Chain Condi-
tional Random Fields. In Proceedings of EACL 2009,
pages 790-798.
Jong-Nae Wang, Jing-Shin Chang, and Keh-Yih Su.
1994. An Automatic Treebank Conversion Algorithm
for Corpus Sharing. In Proceedings of ACL 1994,
pages 248-254.
Mengqiu Wang, Kenji Sagae, and Teruk Mitamura. 2006.
A Fast, Deterministic Parser for Chinese. In Proceed-
ings of ACL-COLING 2006, pages 425-432.
Nianwen Xue, Fu dong Chiou, and Martha Palmer. 2002.
Building a Large-Scale Annotated Chinese Corpus. In
Proceedings of COLING 2002, pages 1-8.
Qiang Zhou. 1996. Phrase Bracketing and Annotation on
Chinese Language Corpus (in Chinese). Ph.D. thesis,
Peking University.
Muhua Zhu, and Jingbo Zhu. 2010. Automatic Treebank
Conversion via Informed Decoding. In Porceedings of
COLING 2010, pages 1541-1549.
719
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 280?284,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning Better Rule Extraction with Translation Span Alignment 
Jingbo Zhu    Tong Xiao   Chunliang Zhang 
Natural Language Processing Laboratory 
Northeastern University, Shenyang, China 
{zhujingbo,xiaotong,zhangcl}@mail.neu.edu.cn 
 
 
 
 
Abstract 
This paper presents an unsupervised ap-
proach to learning translation span align-
ments from parallel data that improves 
syntactic rule extraction by deleting spuri-
ous word alignment links and adding new 
valuable links based on bilingual transla-
tion span correspondences. Experiments on 
Chinese-English translation demonstrate 
improvements over standard methods for 
tree-to-string and tree-to-tree translation.  
1 Introduction 
Most syntax-based statistical machine translation 
(SMT) systems typically utilize word alignments 
and parse trees on the source/target side to learn 
syntactic transformation rules from parallel data. 
The approach suffers from a practical problem that 
even one spurious (word alignment) link can pre-
vent some desirable syntactic translation rules from 
extraction, which can in turn affect the quality of 
translation rules and translation performance (May 
and Knight 2007; Fossum et al 2008). To address 
this challenge, a considerable amount of previous 
research has been done to improve alignment qual-
ity by incorporating some statistics and linguistic 
heuristics or syntactic information into word 
alignments (Cherry and Lin 2006; DeNero and 
Klein 2007; May and Knight 2007; Fossum et al 
2008; Hermjakob 2009; Liu et al 2010).  
Unlike their efforts, this paper presents a simple 
approach that automatically builds the translation 
span alignment (TSA) of a sentence pair by utiliz-
ing a phrase-based forced decoding technique, and 
then improves syntactic rule extraction by deleting 
spurious links and adding new valuable links based 
on bilingual translation span correspondences. The 
proposed approach has two promising properties.  
S
VP
ADVPNNS
imports
VBZ
have
DT
VBNRB
fallendrasticallythe
??
jianshao
???
dafudu
??
jinkou
?
le
NN
VV AS
AD VP
VP
S
NP Frontier node
Word alignment
 
Figure 1. A real example of Chinese-English sentence 
pair with word alignment and both-side parse trees.  
 
Some blocked Tree-to-string Rules: 
r1: AS(?) ? have 
r2: NN(??) ? the imports 
r3: S (NN:x1 VP:x2) ? x1 x2
Some blocked Tree-to-tree Rules: 
r4: AS(?) ? VBZ(have) 
r5: NN(??) ? NP(DT(the) NNS(imports)) 
r6: S(NN:x1 VP:x2) ? S(NP:x1 VP:x2) 
r7: VP(AD:x1 VP(VV:x2 AS:x3)) 
            ? VP(VBZ:x3 ADVP(RB:x1 VBN:x2)) 
Table 1. Some useful syntactic rules are blocked due to 
the spurious link between ??? and ?the?.  
 
Firstly, The TSAs are constructed in an unsuper-
vised learning manner, and optimized by the trans-
lation model during the forced decoding process, 
without using any statistics and linguistic heuristics 
or syntactic constraints. Secondly, our approach is 
independent of the word alignment-based algo-
rithm used to extract translation rules, and easy to 
implement. 
2 Translation Span Alignment Model 
Different from word alignment, TSA is a process 
of identifying span-to-span alignments between 
parallel sentences. For each translation span pair,  
280
1. Extract phrase translation rules R from the parallel 
corpus with word alignment, and construct a phrase-
based translation model M.  
2. Apply M to implement phrase-based forced decoding 
on each training sentence pair (c, e), and output its 
best derivation d* that can transform c into e.  
3. Build a TSA of each sentence pair (c, e) from its best 
derivation d*, in which each rule r in d* is used to 
form a translation span pair {src(r)<=>tgt(r)}.  
Figure 2. TSA generation algorithm. src(r) and tgt(r) 
indicate the source and target side of rule r.  
 
its source (or target) span is a sequence of source 
(or target) words. Given a source sentence c=c1...cn, 
a target sentence e=e1...em, and its word alignment 
A, a translation span pair ? is a pair of source span 
(ci...cj) and target span (ep...eq)  
)( qp
j
i ec ?=?  
where ? indicates that the source span (ci...cj) and 
the target span (ep...eq) are translational equivalent. 
We do not require that ? must be consistent with 
the associated word alignment A in a TSA model.  
Figure 2 depicts the TSA generation algorithm 
in which a phrase-based forced decoding tech-
nique is adopted to produce the TSA of each sen-
tence pair. In this work, we do not apply syntax-
based forced decoding (e.g., tree-to-string) because 
phrase-based models can achieve the state-of-the-
art translation quality with a large amount of train-
ing data, and are not limited by any constituent 
boundary based constraints for decoding.  
Formally, given a sentence pair (c, e), the 
phrase-based forced decoding technique aims to 
search for the best derivation d* among all consis-
tent derivations that convert the given source sen-
tence c into the given target sentence e with respect 
to the current translation model induced from the 
training data, which can be expressed by 
)|)((Prmaxarg
)(),(
* cdTGTd
edTGTecDd
?=??
=          (1) 
where D(c,e) is the set of candidate derivations that 
transform c to e, and TGT(d) is a function that out-
puts the yield of a derivation d. ? indicates parame-
ters of the phrase-based translation model learned 
from the parallel corpus.  
The best derivation d* produced by forced de-
coding can be viewed as a sequence of translation 
steps (i.e., phrase translation rules), expressed by 
krrrd ???= ...* 21 , 
c = ?? ??? ?? ? 
e =  the imports have drastically fallen 
The best derivation d* produced by forced decoding: 
r1: ?? ? the imports 
r2: ??? ?? ? drastically fallen 
r3: ? ? have 
Generating TSA from d*: 
[??]<=>[the imports]  
[??? ??]<=>[drastically fallen]   
[?]<=>[have] 
Table 2. Forced decoding based TSA generation on the 
example sentence pair in Fig. 1. 
 
where ri indicates a phrase rule used to form d*. 
?is a composition operation that combines rules 
{r1...rk} together to produce the target translation.  
As mentioned above, the best derivation d* re-
spects the input sentence pair (c, e). It means that 
for each phrase translation rule ri used by d*, its 
source (or target) side exactly matches a span of 
the given source (or target) sentence. The source 
side src(ri) and the target side tgt(ri) of each phrase 
translation rule ri in d* form a translation span pair 
{src(ri)<=>tgt(ri)} of (c,e). In other words, the 
TSA of (c,e) is a set of translation span pairs gen-
erated from phrase translation rules used by the 
best derivation d*. The forced decoding based TSA 
generation on the example sentence pair in Figure 
1 can be shown in Table 2. 
3 Better Rule Extraction with TSAs 
To better understand the particular task that we 
will address in this section, we first introduce a 
definition of inconsistent with a translation span 
alignment. Given a sentence pair (c, e) with the 
word alignment A and the translation span align-
ment P, we call a link (ci, ej)?A inconsistent with 
P, if  ci and ej are covered respectively by two dif-
ferent translation span pairs in P and vice versa. 
(ci, ej)?A inconsistent with P  ?
)()(:  
)()(:       
???
???
tgtesrccPOR
tgtesrccP
ji
ji
?????
?????
 
where src(?) and tgt(?) indicate the source and tar-
get span of a translation span pair ?.  
By this, we will say that a link (ci, ej)?A is a 
spurious link if it is inconsistent with the given 
TSA. Table 3 shows that an original link (4?1) 
are covered by two different translation span pairs  
281
Source Target WA TSA 
1: ?? 1: the 1?2 [1,1]<=>[1,2] 
2: ??? 2: imports 2?4 [2,3]<=>[4,5] 
3: ?? 3: have 3?5 [4,4]<=>[3,3] 
4: ? 4: drastically 4?1  
 5: fallen (null)?3  
Table 3. A sentence pair with the original word align-
ment (WA) and the translation span alignment (TSA).  
 
([4,4]<=>[3,3]) and ([1,1] <=>[1,2]), respectively. 
In such a case, we think that this link (4?1) is a 
spurious link according to this TSA, and should be 
removed for rule extraction.   
Given a resulting TSA P, there are four different 
types of translation span pairs, such as one-to-one, 
one-to-many, many-to-one, and many-to-many 
cases. For example, the TSA shown in Table 3 
contains a one-to-one span pair ([4,4]<=>[3,3]), a 
one-to-many span pair ([1,1]<=>[1,2]) and a 
many-many span pair ([2,3]<=>[4,5]). In such a 
case, we can learn a confident link from a one-to-
one translation span pair that is preferred by the 
translation model in the forced decoding based 
TSA generation approach. If such a confident link 
does not exist in the original word alignment, we 
consider it as a new valuable link.  
Until now, a natural way is to use TSAs to di-
rectly improve word alignment quality by deleting 
some spurious links and adding some new confi-
dent links, which in turn improves rule quality and 
translation quality. In other words, if a desirable 
translation rule was blocked due to some spurious 
links, we will output this translation rule. Let?s 
revisit the example in Figure 1 again. The blocked 
tree-to-string r3 can be extracted successfully after 
deleting the spurious link (?, the), and a new tree-
to-string rule r1 can be extracted after adding a new 
confident link (?, have) that is inferred from a 
one-to-one translation span pair [4,4]<=>[3,3].  
4 Experiments 
4.1 Setup 
We utilized a state-of-the-art open-source SMT 
system NiuTrans (Xiao et al 2012) to implement 
syntax-based models in the following experiments. 
We begin with a training parallel corpus of Chi-
nese-English bitexts that consists of 8.8M Chinese 
words and 10.1M English words in 350K sentence 
pairs. The GIZA++ tool was used to perform the  
Method Prec% Rec% F1% Del/Sent Add/Sent
Baseline 83.07 75.75 79.25 - - 
TSA 84.01 75.46 79.51 1.5 1.1 
Table 4. Word alignment precision, recall and F1-score 
of various methods on 200 sentence pairs of Chinese-
English data. 
 
bi-directional word alignment between the source 
and the target sentences, referred to as the baseline 
method. For syntactic translation rule extraction, 
minimal GHKM (Galley et al, 2004) rules are first 
extracted from the bilingual corpus whose source 
and target sides are parsed using the Berkeley 
parser (Petrov et al 2006). The composed rules are 
then generated by composing two or three minimal 
rules. A 5-gram language model was trained on the 
Xinhua portion of English Gigaword corpus. Beam 
search and cube pruning techniques (Huang and 
Chiang 2007) were used to prune the search space 
for all the systems. The base feature set used for all 
systems is similar to that used in (Marcu et al 
2006), including 14 base features in total such as 5-
gram language model, bidirectional lexical and 
phrase-based translation probabilities. All features 
were log-linearly combined and their weights were 
optimized by performing minimum error rate train-
ing (MERT) (Och 2003). The development data set 
used for weight training comes from NIST MT03 
evaluation set, consisting of 326 sentence pairs of 
less than 20 words in each Chinese sentence. Two 
test sets are NIST MT04 (1788 sentence pairs) and 
MT05 (1082 sentence pairs) evaluation sets. The 
translation quality is evaluated in terms of the case-
insensitive IBM-BLEU4 metric.  
4.2 Effect on Word Alignment 
To investigate the effect of the TSA method on 
word alignment, we designed an experiment to 
evaluate alignment quality against gold standard 
annotations. There are 200 random chosen and 
manually aligned Chinese-English sentence pairs 
used to assert the word alignment quality. For 
word alignment evaluation, we calculated precision, 
recall and F1-score over gold word alignment.  
Table 4 depicts word alignment performance of 
the baseline and TSA methods. We apply the TSAs 
to refine the baseline word alignments, involving 
spurious link deletion and new link insertion op-
erations. Table 4 shows our method can yield im-
provements on precision and F1-score, only 
causing a little negative effect on recall.  
282
4.3 Translation Quality 
Method # of Rules MT03 MT04 MT05 
Baseline (T2S) 33,769,071 34.10 32.55 30.15 
TSA (T2S) 32,652,261 
34.61+
(+0.51) 
33.01+
(+0.46)
30.66+
(+0.51)
     
Baseline (T2T) 24,287,206 34.51 32.20 31.78 
TSA (T2T) 24,119,719 
34.85 
(+0.34) 
32.92*
(+0.72)
32.22+ 
(+0.44)
Table 5. Rule sizes and IBM-BLEU4 (%) scores of 
baseline and our method (TSA) in tree-to-string (T2S) 
and tree-to-tree (T2T) translation on Dev set (MT03) 
and two test sets (MT04 and MT05). + and * indicate 
significantly better on performance comparison at p<.05 
and p<.01, respectively.  
 
Table 5 depicts effectiveness of our TSA method 
on translation quality in tree-to-string and tree-to-
tree translation tasks. Table 5 shows that our TSA 
method can improve both syntax-based translation 
systems. As mentioned before, the resulting TSAs 
are essentially optimized by the translation model. 
Based on such TSAs, experiments show that spuri-
ous link deletion and new valuable link insertion 
can improve translation quality for tree-to-string 
and tree-to-tree systems.  
5 Related Work 
Previous studies have made great efforts to incor-
porate statistics and linguistic heuristics or syntac-
tic information into word alignments (Ittycheriah 
and Roukos 2005; Taskar et al 2005; Moore et al 
2006; Cherry and Lin 2006; DeNero and Klein 
2007; May and Knight 2007; Fossum et al 2008; 
Hermjakob 2009; Liu et al 2010). For example, 
Fossum et al (2008) used a discriminatively 
trained model to identify and delete incorrect links 
from original word alignments to improve string-
to-tree transformation rule extraction, which incor-
porates four types of features such as lexical and 
syntactic features. This paper presents an approach 
to incorporating translation span alignments into 
word alignments to delete spurious links and add 
new valuable links.  
Some previous work directly models the syntac-
tic correspondence in the training data for syntactic 
rule extraction (Imamura 2001; Groves et al 2004; 
Tinsley et al 2007; Sun et al 2010a, 2010b; Pauls 
et al 2010). Some previous methods infer syntac-
tic correspondences between the source and the 
target languages through word alignments and con-
stituent boundary based syntactic constraints. Such 
a syntactic alignment method is sensitive to word 
alignment behavior. To combat this, Pauls et al 
(2010) presented an unsupervised ITG alignment 
model that directly aligns syntactic structures for 
string-to-tree transformation rule extraction. One 
major problem with syntactic structure alignment 
is that syntactic divergence between languages can 
prevent accurate syntactic alignments between the 
source and target languages.  
May and Knight (2007) presented a syntactic re-
alignment model for syntax-based MT that uses 
syntactic constraints to re-align a parallel corpus 
with word alignments. The motivation behind their 
methods is similar to ours. Our work differs from 
(May and Knight 2007) in two major respects. 
First, the approach proposed by May and Knight 
(2007) first utilizes the EM algorithm to obtain 
Viterbi derivation trees from derivation forests of 
each (tree, string) pair, and then produces Viterbi 
alignments based on obtained derivation trees. Our 
forced decoding based approach searches for the 
best derivation to produce translation span align-
ments that are used to improve the extraction of 
translation rules. Translation span alignments are 
optimized by the translation model. Secondly, their 
models are only applicable for syntax-based sys-
tems while our method can be applied to both 
phrase-based and syntax-based translation tasks.  
6 Conclusion 
This paper presents an unsupervised approach to 
improving syntactic transformation rule extraction 
by deleting spurious links and adding new valuable 
links with the help of bilingual translation span 
alignments that are built by using a phrase-based 
forced decoding technique. In our future work, it is 
worth studying how to combine the best of our ap-
proach and discriminative word alignment models 
to improve rule extraction for SMT models.  
Acknowledgments 
This research was supported in part by the National 
Science Foundation of China (61073140), the Spe-
cialized Research Fund for the Doctoral Program 
of Higher Education (20100042110031) and the 
Fundamental Research Funds for the Central Uni-
versities in China. 
283
References  
Colin Cherry and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative 
training. In Proc. of ACL. 
John DeNero and Dan Klein. 2007. Tailoring word 
alignments to syntactic machine translation. In Proc. 
of ACL. 
Victoria Fossum, Kevin Knight and Steven Abney. 
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. In Proc. 
of the Third Workshop on Statistical Machine Trans-
lation, pages 44-52. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What's in a translation rule? In Proc. of 
HLT-NAACL 2004, pp273-280. 
Declan Groves, Mary Hearne and Andy Way. 2004. 
Robust sub-sentential alignment of phrase-structure 
trees. In Proc. of COLING, pp1072-1078. 
Ulf Hermjakob. 2009. Improved word alignment with 
statistics and linguistic heuristics. In Proc. of EMNLP, 
pp229-237 
Liang Huang and David Chiang. 2007. Forest rescoring: 
Faster decoding with integrated language models. In 
Proc. of ACL, pp144-151. 
Kenji Imamura. 2001. Hierarchical Phrase Alignment 
Harmonized with Parsing. In Proc. of NLPRS, 
pp377-384. 
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for Arabic-English ma-
chine translation. In Proc. of HLT/EMNLP. 
Yang Liu, Qun Liu and Shouxun Lin. 2010. Discrimina-
tive word alignment by linear modeling. Computa-
tional Linguistics, 36(3):303-339 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phrases. 
In Proc. of EMNLP, pp44-52. 
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Proc. of 
EMNLP-CoNLL.  
Robert C. Moore, Wen-tau Yih and Andreas Bode. 2006. 
Improved discriminative bilingual word alignment. 
In Proc. of ACL 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL. 
Adam Pauls, Dan Klein, David Chiang and Kevin 
Knight. 2010. Unsupervised syntactic alignment with 
inversion transduction grammars. In Proc. of NAACL, 
pp118-126 
Slav Petrov, Leon Barrett, Roman Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL, pp433-440. 
Jun Sun, Min Zhang and Chew Lim Tan. 2010a. Explor-
ing Syntactic Structural Features for Sub-Tree 
Alignment Using Bilingual Tree Kernels. In Proc. of 
ACL, pp306-315. 
Jun Sun, Min Zhang and Chew Lim Tan. 2010b. Dis-
criminative Induction of Sub-Tree Alignment using 
Limited Labeled Data. In Proc. of COLING, pp1047-
1055. 
Ben Taskar, Simon Lacoste-Julien and Dan Klein. 2005. 
A discriminative matching approach to word align-
ment. In Proc. of HLT/EMNLP 
John Tinsley, Ventsislav Zhechev, Mary Hearne and 
Andy Way. 2007. Robust language pair-independent 
sub-tree alignment. In Proc. of MT Summit XI. 
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li. 2012. 
NiuTrans: An Open Source Toolkit for Phrase-based 
and Syntax-based Machine Translation. In Proceed-
ings of ACL, demonstration session 
284
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19?24,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
NiuTrans: An Open Source Toolkit for  
Phrase-based and Syntax-based Machine Translation  
 
Tong Xiao? ? , Jingbo Zhu? ? , Hao Zhang?  and Qiang Li?  
?Natural Language Processing Lab, Northeastern University 
?Key Laboratory of Medical Image Computing, Ministry of Education 
{xiaotong,zhujingbo}@mail.neu.edu.cn 
{zhanghao1216,liqiangneu}@gmail.com 
 
 
 
Abstract 
We present a new open source toolkit for 
phrase-based and syntax-based machine 
translation. The toolkit supports several 
state-of-the-art models developed in 
statistical machine translation, including 
the phrase-based model, the hierachical 
phrase-based model, and various syntax-
based models. The key innovation provided 
by the toolkit is that the decoder can work 
with various grammars and offers different 
choices of decoding algrithms, such as 
phrase-based decoding, decoding as 
parsing/tree-parsing and forest-based 
decoding. Moreover, several useful utilities 
were distributed with the toolkit, including 
a discriminative reordering model, a simple 
and fast language model, and an 
implementation of minimum error rate 
training  for weight tuning. 
1 Introduction 
We present NiuTrans, a new open source machine 
translation toolkit, which was developed for 
constructing high quality machine translation 
systems. The NiuTrans toolkit supports most 
statistical machine translation (SMT) paradigms 
developed over the past decade, and allows for 
training and decoding with several state-of-the-art 
models, including: the phrase-based model (Koehn 
et al, 2003), the hierarchical phrase-based model 
(Chiang, 2007), and various syntax-based models 
(Galley et al, 2004; Liu et al, 2006). In particular, 
a unified framework was adopted to decode with 
different models and ease the implementation of 
decoding algorithms. Moreover, some useful 
utilities were distributed with the toolkit, such as: a 
discriminative reordering model, a simple and fast 
language model, and an implementation of 
minimum error rate training that allows for various 
evaluation metrics for tuning the system. In 
addition, the toolkit provides easy-to-use APIs for 
the development of new features. The toolkit has 
been used to build translation systems that have 
placed well at recent MT evaluations, such as the 
NTCIR-9 Chinese-to-English PatentMT task (Goto 
et al, 2011). 
We implemented the toolkit in C++ language, 
with special consideration of extensibility and 
efficiency. C++ enables us to develop efficient 
translation engines which have high running speed 
for both training and decoding stages. This 
property is especially important when the programs 
are used for large scale translation. While the 
development of C++ program is slower than that of 
the similar programs written in other popular 
languages such as Java, the modern compliers 
generally result in C++ programs being 
consistently faster than the Java-based counterparts. 
The toolkit is available under the GNU general 
public license 1 . The website of NiuTrans is   
http://www.nlplab.com/NiuPlan/NiuTrans.html. 
2 Motivation 
As in current approaches to statistical machine 
translation, NiuTrans is based on a log-linear 
                                                          
1 http://www.gnu.org/licenses/gpl-2.0.html 
19
model where a number of features are defined to 
model the translation process. Actually NiuTrans is 
not the first system of this kind. To date, several 
open-source SMT systems (based on either phrase-
based models or syntax-based models) have been 
developed, such as Moses (Koehn et al, 2007), 
Joshua (Li et al, 2009), SAMT (Zollmann and 
Venugopal, 2006), Phrasal (Cer et al, 2010), cdec 
(Dyer et al, 2010), Jane (Vilar et al, 2010) and 
SilkRoad 2 , and offer good references for the 
development of the NiuTrans toolkit. While our 
toolkit includes all necessary components as 
provided within the above systems, we have 
additional goals for this project, as follows: 
z It fully supports most state-of-the-art SMT 
models. Among these are: the phrase-based 
model, the hierarchical phrase-based model, 
and the syntax-based models that explicitly 
use syntactic information on either (both) 
source and (or) target language side(s). 
z It offers a wide choice of decoding 
algorithms. For example, the toolkit has 
several useful decoding options, including: 
standard phrase-based decoding, decoding 
as parsing, decoding as tree-parsing, and 
forest-based decoding. 
z It is easy-to-use and fast. A new system can 
be built using only a few commands. To 
control the system, users only need to 
modify a configuration file. In addition to 
the special attention to usability, the 
running speed of the system is also 
improved in several ways. For example, we 
used several pruning and multithreading 
techniques to speed-up the system. 
3 Toolkit 
The toolkit serves as an end-to-end platform for 
training and evaluating statistical machine 
translation models. To build new translation 
systems, all you need is a collection of word-
aligned sentences 3 , and a set of additional 
sentences with one or more reference translations 
for weight tuning and test. Once the data is 
prepared, the MT system can be created using a 
                                                          
2 http://www.nlp.org.cn/project/project.php?proj_id=14 
3 To obtain word-to-word alignments, several easy-to-use 
toolkits are available, such as GIZA++ and Berkeley Aligner. 
sequence of commands. Given a number of 
sentence-pairs and the word alignments between 
them, the toolkit first extracts a phrase table and 
two reordering models for the phrase-based system, 
or a Synchronous Context-free/Tree-substitution 
Grammar (SCFG/STSG) for the hierarchical 
phrase-based and syntax-based systems. Then, an 
n-gram language model is built on the target-
language corpus. Finally, the resulting models are 
incorporated into the decoder which can 
automatically tune feature weights on the 
development set using minimum error rate training 
(Och, 2003) and translate new sentences with the 
optimized weights. 
In the following, we will give a brief review of 
the above components and the main features 
provided by the toolkit. 
3.1 Phrase Extraction and Reordering Model 
We use a standard way to implement the phrase 
extraction module for the phrase-based model. 
That is, we extract all phrase-pairs that are 
consistent with word alignments. Five features are 
associated with each phrase-pair. They are two 
phrase translation probabilities, two lexical weights, 
and a feature of phrase penalty. We follow the 
method proposed in (Koehn et al, 2003) to 
estimate the values of these features. 
Unlike previous systems that adopt only one 
reordering model, our toolkit supports two 
different reordering models which are trained 
independently but jointly used during decoding. 
z The first of these is a discriminative 
reordering model. This model is based on 
the standard framework of maximum 
entropy. Thus the reordering problem is 
modeled as a classification problem, and 
the reordering probability can be efficiently 
computed using a (log-)linear combination 
of features. In our implementation, we use 
all boundary words as features which are 
similar to those used in (Xiong et al, 2006). 
z The second model is the MSD reordering 
model4 which has been successfully used in 
the Moses system. Unlike Moses, our 
toolkit supports both the word-based and 
phrase-based methods for estimating the 
                                                          
4 Term MSD refers to the three orientations (reordering types), 
including Monotone (M), Swap (S), and Discontinuous (D). 
20
probabilities of the three orientations 
(Galley and Manning, 2008). 
3.2 Translation Rule Extraction 
For the hierarchical phrase-based model, we follow 
the general framework of SCFG where a grammar 
rule has three parts ? a source-side, a target-side 
and alignments between source and target non-
terminals. To learn SCFG rules from word-aligned 
sentences, we choose the algorithm proposed in 
(Chiang, 2007) and estimate the associated feature 
values as in the phrase-based system. 
For the syntax-based models, all non-terminals 
in translation rules are annotated with syntactic 
labels. We use the GHKM algorithm to extract 
(minimal) translation rules from bilingual 
sentences with parse trees on source-language side 
and/or target-language side5 . Also, two or more 
minimal rules can be composed together to obtain 
larger rules and involve more contextual 
information. For unaligned words, we attach them 
to all nearby rules, instead of using the most likely 
attachment as in (Galley et al, 2006). 
3.3 N-gram Language Modeling 
The toolkit includes a simple but effective n-gram 
language model (LM). The LM builder is basically 
a ?sorted? trie structure (Pauls and Klein, 2011), 
where a map is developed to implement an array of 
key/value pairs, guaranteeing that the keys can be 
accessed in sorted order. To reduce the size of 
resulting language model, low-frequency n-grams 
are filtered out by some thresholds. Moreover, an 
n-gram cache is implemented to speed up n-gram 
probability requests for decoding. 
3.4 Weight Tuning 
We implement the weight tuning component 
according to the minimum error rate training 
(MERT) method (Och, 2003). As MERT suffers 
from local optimums, we added a small program 
into the MERT system to let it jump out from the 
coverage area. When MERT converges to a (local) 
optimum, our program automatically conducts the 
MERT run again from a random starting point near 
the newly-obtained optimal point. This procedure 
                                                          
5 For tree-to-tree models, we use a natural extension of the 
GHKM algorithm which defines admissible nodes on tree-
pairs and obtains tree-to-tree rules on all pairs of source and 
target tree-fragments. 
is repeated for several times until no better weights 
(i.e., weights with a higher BLEU score) are found. 
In this way, our program can introduce some 
randomness into weight training. Hence users do 
not need to repeat MERT for obtaining stable and 
optimized weights using different starting points.  
3.5 Decoding 
Chart-parsing is employed to decode sentences in 
development and test sets. Given a source sentence, 
the decoder generates 1-best or k-best translations 
in a bottom-up fashion using a CKY-style parsing 
algorithm. The basic data structure used in the 
decoder is a chart, where an array of cells is 
organized in topological order. Each cell maintains 
a list of hypotheses (or items). The decoding 
process starts with the minimal cells, and proceeds 
by repeatedly applying translation rules or 
composing items in adjunct cells to obtain new 
items. Once a new item is created, the associated 
scores are computed (with an integrated n-gram 
language model). Then, the item is added into the 
list of the corresponding cell. This procedure stops 
when we reach the final state (i.e., the cell 
associates with the entire source span). 
The decoder can work with all (hierarchical) 
phrase-based and syntax-based models. In 
particular, our toolkit provides the following 
decoding modes. 
z Phrase-based decoding. To fit the phrase-
based model into the CKY paring 
framework, we restrict the phrase-based 
decoding with the ITG constraint (Wu, 
1996). In this way, each pair of items in 
adjunct cells can be composed in either 
monotone order or inverted order. Hence 
the decoding can be trivially implemented 
by a three-loop structure as in standard 
CKY parsing. This algorithm is actually the 
same as that used in parsing with 
bracketing transduction grammars. 
z Decoding as parsing (or string-based 
decoding). This mode is designed for 
decoding with SCFGs/STSGs which are 
used in the hierarchical phrase-based and 
syntax-based systems. In the general 
framework of synchronous grammars and 
tree transducers, decoding can be regarded 
as a parsing problem. Therefore, the above 
chart-based decoder is directly applicable to 
21
the hierarchical phrase-based and syntax-
based models. For efficient integration of n-
gram language model into decoding, rules 
containing more than two variables are 
binarized into binary rules. In addition to 
the rules learned from bilingual data, glue 
rules are employed to glue the translations 
of a sequence of chunks.  
z Decoding as tree-parsing (or tree-based 
decoding). If the parse tree of source 
sentence is provided, decoding (for tree-to-
string and tree-to-tree models) can also be 
cast as a tree-parsing problem (Eisner, 
2003). In tree-parsing, translation rules are 
first mapped onto the nodes of input parse 
tree. This results in a translation tree/forest 
(or a hypergraph) where each edge 
represents a rule application. Then 
decoding can proceed on the hypergraph as 
usual. That is, we visit in bottom-up order 
each node in the parse tree, and calculate 
the model score for each edge rooting at the 
node. The final output is the 1-best/k-best 
translations maintained by the root node of 
the parse tree. Since tree-parsing restricts 
its search space to the derivations that 
exactly match with the input parse tree, it in 
general has a much higher decoding speed 
than a normal parsing procedure. But it in 
turn results in lower translation quality due 
to more search errors. 
z Forest-based decoding. Forest-based 
decoding (Mi et al, 2008) is a natural 
extension of tree-based decoding. In 
principle, forest is a data structure that can 
encode exponential number of trees 
efficiently. This structure has been proved 
to be helpful in reducing the effects caused 
by parser errors. Since our internal 
representation is already in a hypergraph 
structure, it is easy to extend the decoder to 
handle the input forest, with little 
modification of the code. 
4 Other Features 
In addition to the basic components described 
above, several additional features are introduced to 
ease the use of the toolkit. 
4.1 Multithreading 
The decoder supports multithreading to make full 
advantage of the modern computers where more 
than one CPUs (or cores) are provided. In general, 
the decoding speed can be improved when multiple 
threads are involved. However, modern MT 
decoders do not run faster when too many threads 
are used (Cer et al, 2010). 
4.2 Pruning 
To make decoding computational feasible, beam 
pruning is used to aggressively prune the search 
space. In our implementation, we maintain a beam 
for each cell. Once all the items of the cell are 
proved, only the top-k best items according to 
model score are kept and the rest are discarded. 
Also, we re-implemented the cube pruning method 
described in (Chiang, 2007) to further speed-up the 
system. 
In addition, we develop another method that 
prunes the search space using punctuations. The 
idea is to divide the input sentence into a sequence 
of segments according to punctuations. Then, each 
segment is translated individually. The MT outputs 
are finally generated by composing the translations 
of those segments. 
4.3 APIs for Feature Engineering 
To ease the implementation and test of new 
features, the toolkit offers APIs for experimenting 
with the features developed by users. For example, 
users can develop new features that are associated 
with each phrase-pair. The system can 
automatically recognize them and incorporate them 
into decoding. Also, more complex features can be 
activated during decoding. When an item is created 
during decoding, new features can be introduced 
into an internal object which returns feature values 
for computing the model score. 
5 Experiments 
5.1 Experimental Setup 
We evaluated our systems on NIST Chinese-
English MT tasks. Our training corpus consists of 
1.9M bilingual sentences. We used GIZA++ and 
the ?grow-diag-final-and? heuristics to generate 
word alignment for the bilingual data. The parse 
trees on both the Chinese and English sides were 
22
BLEU4[%] Entry 
 Dev  Test 
Moses: phrase  36.51  34.93
Moses: hierarchical phrase  36.65  34.79
 phrase  36.99  35.29
 hierarchical phrase  37.41  35.35
 parsing  36.48  34.71
 tree-parsing  35.54  33.99
 t2s 
 forest-based  36.14  34.25
 parsing  35.99  34.01
 tree-parsing  35.04  33.21
 t2t 
 forest-based  35.56  33.45
   
   
   
 N
iu
Tr
an
s 
 s2t  parsing  37.63  35.65
Table 1: BLEU scores of various systems. t2s, t2t, 
and s2t represent the tree-to-string, tree-to-tree, and 
string-to-tree systems, respectively. 
 
generated using the Berkeley Parser, which were 
then binarized in a head-out fashion 6. A 5-gram 
language model was trained on the Xinhua portion 
of the Gigaword corpus in addition to the English 
part of the LDC bilingual training data. We used 
the NIST 2003 MT evaluation set as our 
development set (919 sentences) and the NIST 
2005 MT evaluation set as our test set (1,082 
sentences). The translation quality was evaluated 
with the case-insensitive IBM-version BLEU4. 
For the phrase-based system, phrases are of at 
most 7 words on either source or target-side. For 
the hierarchical phrase-based system, all SCFG 
rules have at most two variables. For the syntax-
based systems, minimal rules were extracted from 
the binarized trees on both (either) language-
side(s). Larger rules were then generated by 
composing two or three minimal rules. By default, 
all these systems used a beam of size 30 for 
decoding. 
5.2 Evaluation of Translations 
Table 1 shows the BLEU scores of different MT 
systems built using our toolkit. For comparison, 
the result of the Moses system is also reported. We 
see, first of all, that our phrase-based and 
hierarchical phrase-based systems achieve 
competitive performance, even outperforms the 
Moses system over 0.3 BLEU points in some cases. 
Also, the syntax-based systems obtain very  
                                                          
6 The parse trees follow the nested bracketing format, as 
defined in the Penn Treebank. Also, the NiuTrans package 
includes a tool for tree binarization. 
BLEU4[%] Entry 
Dev Test 
Speed
(sent/sec)
Moses: phrase  36.69  34.99    0.11
+ cube pruning   36.51  34.93    0.47
NiuTrans: phrase  37.14  35.47    0.14
+ cube pruning  36.98  35.39    0.60
+ cube & punct pruning  36.99  35.29    3.71
+ all pruning & 8 threads  36.99  35.29  21.89
+ all pruning & 16 threads  36.99  35.29  22.36
Table 2: Effects of pruning and multithreading 
techniques. 
 
promising results. For example, the string-to-tree 
system significantly outperforms the phrase-based 
and hierarchical phrase-based counterparts. In 
addition, Table 1 gives a test of different decoding 
methods (for syntax-based systems). We see that 
the parsing-based method achieves the best BLEU 
score. On the other hand, as expected, it runs 
slowest due to its large search space. For example, 
it is 5-8 times slower than the tree-parsing-based 
method in our experiments. The forest-based 
decoding further improves the BLEU scores on top 
of tree-parsing. In most cases, it obtains a +0.6 
BLEU improvement but is 2-3 times slower than 
the tree-parsing-based method. 
5.3 System Speed-up 
We also study the effectiveness of pruning and 
multithreading techniques. Table 2 shows that all 
the pruning methods implemented in the toolkit is 
helpful in speeding up the (phrase-based) system, 
while does not result in significant decrease in 
BLEU score. On top of a straightforward baseline 
(only beam pruning is used), cube pruning and 
pruning with punctuations give a speed 
improvement of 25 times together7. Moreover, the 
decoding process can be further accelerated by 
using multithreading technique. However, more 
than 8 threads do not help in our experiments. 
6 Conclusion and Future Work 
We have presented a new open-source toolkit for 
phrase-based and syntax-based machine translation. 
It is implemented in C++ and runs fast. Moreover, 
it supports several state-of-the-art models ranging 
from phrase-based models to syntax-based models, 
                                                          
7 The translation speed is tested on Intel Core Due 2 E8500 
processors running at 3.16 GHz. 
23
and provides a wide choice of decoding methods. 
The experimental results on NIST MT tasks show 
that the MT systems built with our toolkit achieve 
state-of-the-art translation performance. 
The next version of NiuTrans will support 
ARPA-format LMs, MIRA for weight tuning and a 
beam-stack decoder which removes the ITG 
constraint for phrase decoding. In addition, a 
Hadoop-based MapReduce-parallelized version is 
underway and will be released in near future.  
Acknowledgments  
This research was supported in part by the National 
Science Foundation of China (61073140), the 
Specialized Research Fund for the Doctoral 
Program of Higher Education (20100042110031) 
and the Fundamental Research Funds for the 
Central Universities in China. 
References  
Daniel Cer, Michel Galley, Daniel Jurafsky and 
Christopher D. Manning. 2010. Phrasal: A Toolkit 
for Statistical Machine Translation with Facilities for 
Extraction and Incorporation of Arbitrary Model 
Features. In Proc. of HLT/NAACL 2010 
demonstration Session, pages 9-12. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201?
228. 
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan 
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, 
Vladimir Eidelman, Philip Resnik. 2010. cdec: A 
Decoder, Alignment, and Learning Framework for 
Finite-State and Context-Free Translation Models. In 
Proc. of ACL 2010 System Demonstrations, pages 7-
12. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What's in a translation rule? In Proc. of 
HLT-NAACL 2004, pages 273-280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable inferences and training of 
context-rich syntax translation models. In Proc. of 
COLING/ACL 2006, pages 961-968. 
Michel Galley and Christopher D. Manning. 2008. A 
Simple and Effective Hierarchical Phrase Reordering 
Model. In Proc. of EMNLP2008, pages 848-856. 
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita and 
Benjamin K. Tsou. 2011. Overview of the Patent 
Machine Translation Task at the NTCIR-9 Workshop. 
In Proc. of NTCIR-9 Workshop Meeting, pages 559-
578. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proc. of 
HLT/NAACL 2003, pages 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. In 
Proc. of ACL 2007, pages 177?180. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev 
Khudanpur, Lane Schwartz, Wren Thornton, 
Jonathan Weese, and Omar Zaidan. 2009. Joshua: An 
Open Source Toolkit for Parsing-Based Machine 
Translation. In Proc. of the Workshop on Statistical 
Machine Translation, pages 135?139. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. In Proc. of ACL 2006, pages 609-616. 
Haitao Mi, Liang Huang and Qun Liu. 2008. Forest-
Based Translation. In Proc. of ACL 2008, pages 192-
199. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 2003, 
pages 160-167. 
Adam Pauls and Dan Klein. 2011. Faster and Smaller 
N-Gram Language Models. In Proc. of ACL 2011, 
pages 258?267. 
David Vilar, Daniel Stein, Matthias Huck and Hermann 
Ney. 2010. Jane: Open Source Hierarchical 
Translation, Extended with Reordering and Lexicon 
Models. In Proc. of the Joint 5th Workshop on 
Statistical Machine Translation and MetricsMATR, 
pages 262-270. 
Dekai Wu. 1996. A polynomial-time algorithm for 
statistical machine translation. In Proc. of ACL1996, 
pages 152?158. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In Proc. of ACL 
2006, pages 521-528. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax 
Augmented Machine Translation via Chart Parsing. 
In Proc. of HLT/NAACL 2006, pages 138-141. 
24
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434?443,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fast and Accurate Shift-Reduce Constituent Parsing
Muhua Zhu?, Yue Zhang?, Wenliang Chen?, Min Zhang? and Jingbo Zhu?
?Natural Language Processing Lab., Northeastern University, China
?Singapore University of Technology and Design, Singapore
? Soochow University, China and Institute for Infocomm Research, Singapore
zhumuhua@gmail.com yue zhang@sutd.edu.sg
chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
Shift-reduce dependency parsers give
comparable accuracies to their chart-
based counterparts, yet the best shift-
reduce constituent parsers still lag behind
the state-of-the-art. One important reason
is the existence of unary nodes in phrase
structure trees, which leads to different
numbers of shift-reduce actions between
different outputs for the same input. This
turns out to have a large empirical impact
on the framework of global training and
beam search. We propose a simple yet
effective extension to the shift-reduce
process, which eliminates size differences
between action sequences in beam-search.
Our parser gives comparable accuracies
to the state-of-the-art chart parsers. With
linear run-time complexity, our parser is
over an order of magnitude faster than the
fastest chart parser.
1 Introduction
Transition-based parsers employ a set of shift-
reduce actions and perform parsing using a se-
quence of state transitions. The pioneering mod-
els rely on a classifier to make local decisions, and
search greedily for a transition sequence to build a
parse tree. Greedy, classifier-based parsers have
been developed for both dependency grammars
(Yamada and Matsumoto, 2003; Nivre et al, 2006)
and phrase-structure grammars (Sagae and Lavie,
2005). With linear run-time complexity, they were
commonly regarded as a faster but less accurate
alternative to graph-based chart parsers (Collins,
1997; Charniak, 2000; McDonald et al, 2005).
Various methods have been proposed to address
the disadvantages of greedy local parsing, among
which a framework of beam-search and global
discriminative training have been shown effective
for dependency parsing (Zhang and Clark, 2008;
Huang and Sagae, 2010). While beam-search
reduces error propagation compared with greedy
search, a discriminative model that is globally op-
timized for whole sequences of transition actions
can avoid local score biases (Lafferty et al, 2001).
This framework preserves the most important ad-
vantage of greedy local parsers, including linear
run-time complexity and the freedom to define ar-
bitrary features. With the use of rich non-local fea-
tures, transition-based dependency parsers achieve
state-of-the-art accuracies that are comparable to
the best-graph-based parsers (Zhang and Nivre,
2011; Bohnet and Nivre, 2012). In addition, pro-
cessing tens of sentences per second (Zhang and
Nivre, 2011), these transition-based parsers can be
a favorable choice for dependency parsing.
The above global-learning and beam-search
framework can be applied to transition-based
phrase-structure (constituent) parsing also (Zhang
and Clark, 2009), maintaining all the afore-
mentioned benefits. However, the effects were
not as significant as for transition-based depen-
dency parsing. The best reported accuracies of
transition-based constituent parsers still lag behind
the state-of-the-art (Sagae and Lavie, 2006; Zhang
and Clark, 2009). One difference between phrase-
structure parsing and dependency parsing is that
for the former, parse trees with different numbers
of unary rules require different numbers of actions
to build. Hence the scoring model needs to disam-
biguate between transitions sequences with differ-
ent sizes. For the same sentence, the largest output
can take twice as many as actions to build as the
434
smallest one. This turns out to have a significant
empirical impact on parsing with beam-search.
We propose an extension to the shift-reduce pro-
cess to address this problem, which gives signifi-
cant improvements to the parsing accuracies. Our
method is conceptually simple, requiring only one
additional transition action to eliminate size dif-
ferences between different candidate outputs. On
standard evaluations using both the Penn Tree-
bank and the Penn Chinese Treebank, our parser
gave higher accuracies than the Berkeley parser
(Petrov and Klein, 2007), a state-of-the-art chart
parser. In addition, our parser runs with over 89
sentences per second, which is 14 times faster than
the Berkeley parser, and is the fastest that we are
aware of for phrase-structure parsing. An open
source release of our parser (version 0.6) is freely
available on the Web. 1
In addition to the above contributions, we apply
a variety of semi-supervised learning techniques to
our transition-based parser. These techniques have
been shown useful to improve chart-based pars-
ing (Koo et al, 2008; Chen et al, 2012), but little
work has been done for transition-based parsers.
We therefore fill a gap in the literature by report-
ing empirical results using these methods. Experi-
mental results show that semi-supervised methods
give a further improvement of 0.9% in F-score on
the English data and 2.4% on the Chinese data.
Our Chinese results are the best that we are aware
of on the standard CTB data.
2 Baseline parser
We adopt the parser of Zhang and Clark (2009) for
our baseline, which is based on the shift-reduce
process of Sagae and Lavie (2005), and employs
global perceptron training and beam search.
2.1 Vanilla Shift-Reduce
Shift-reduce parsing is based on a left-to-right
scan of the input sentence. At each step, a tran-
sition action is applied to consume an input word
or construct a new phrase-structure. A stack
is used to maintain partially constructed phrase-
structures, while the input words are stored in a
buffer. The set of transition actions are
? SHIFT: pop the front word from the buffer,
and push it onto the stack.
1http://sourceforge.net/projects/zpar/
Axioms [?, 0, false,0]
Goal [S, n, true, C]
Inference Rules:
[S, i, false, c]
SHIFT [S|w, i + 1, false, c + cs]
[S|s1s0, i, false, c]
REDUCE-L/R-X [S|X, i, false, c + cr]
[S|s0, i, false, c]
UNARY-X [S|X, i, false, c + cu]
[S, n, false, c]
FINISH [S, n, true, c + cf ]
Figure 1: Deduction system of the baseline shift-
reduce parsing process.
? REDUCE-L/R-X: pop the top two con-
stituents off the stack, combine them into a
new constituent with label X, and push the
new constituent onto the stack.
? UNARY-X: pop the top constituent off the
stack, raise it to a new constituent with la-
bel X, and push the new constituent onto the
stack.
? FINISH: pop the root node off the stack and
ends parsing.
The deduction system for the process is shown
in Figure 1, where the item is formed as ?stack,
buffer front index, completion mark, score?, and
cs, cr , and cu represent the incremental score of
the SHIFT, REDUCE, and UNARY parsing steps,
respectively; these scores are calculated according
to the context features of the parser state item. n
is the number of words in the input.
2.2 Global Discriminative Training and
Beam-Search
For a given input sentence, the initial state has an
empty stack and a buffer that contains all the input
words. An agenda is used to keep the k best state
items at each step. At initialization, the agenda
contains only the initial state. At each step, every
state item in the agenda is popped and expanded
by applying a valid transition action, and the top
k from the newly constructed state items are put
back onto the agenda. The process repeats until
the agenda is empty, and the best completed state
item (recorded as candidate output) is taken for
435
Description Templates
unigrams s0tc, s0wc, s1tc, s1wc, s2tc
s2wc, s3tc, s3wc, q0wt, q1wt
q2wt, q3wt, s0lwc, s0rwc
s0uwc, s1lwc, s1rwc, s1uwc
bigrams s0ws1w, s0ws1c, s0cs1w, s0cs1c,
s0wq0w, s0wq0t, s0cq0w, s0cq0t,
q0wq1w, q0wq1t, q0tq1w, q0tq1t,
s1wq0w, s1wq0t, s1cq0w, s1cq0t
trigrams s0cs1cs2c, s0ws1cs2c, s0cs1wq0t
s0cs1cs2w, s0cs1cq0t, s0ws1cq0t
s0cs1wq0t, s0cs1cq0w
Table 1: A summary of baseline feature templates,
where si represents the ith item on the stack S and
qi denotes the ith item in the queue Q. w refers to
the head lexicon, t refers to the head POS, and c
refers to the constituent label.
the output.
The score of a state item is the total score of the
transition actions that have been applied to build
the item:
C(?) =
N?
i=1
?(ai) ? ~?
Here ?(ai) represents the feature vector for the ith
action ai in state item ?. It is computed by apply-
ing the feature templates in Table 1 to the context
of ?. N is the total number of actions in ?.
The model parameter ~? is trained with the aver-
aged perceptron algorithm, applied to state items
(sequence of actions) globally. We apply the early
update strategy (Collins and Roark, 2004), stop-
ping parsing for parameter updates when the gold-
standard state item falls off the agenda.
2.3 Baseline Features
Our baseline features are adopted from Zhang and
Clark (2009), and are shown in Table 1 Here si
represents the ith item on the top of the stack S
and qi denotes the ith item in the front end of the
queue Q. The symbol w denotes the lexical head
of an item; the symbol c denotes the constituent
label of an item; the symbol t is the POS of a lex-
ical head. These features are adapted from Zhang
and Clark (2009). We remove Chinese specific
features and make the baseline parser language-
independent.
3 Improved hypotheses comparison
Unlike dependency parsing, constituent parse
trees for the same sentence can have different
numbers of nodes, mainly due to the existence
of unary nodes. As a result, completed state
NP
NN
address
NNS
issues
VP
VB
address
NP
NNS
issues
Figure 2: Example parse trees of the same sen-
tence with different numbers of actions.
items for the same sentence can have different
numbers of unary actions. Take the phrase ?ad-
dress issues? for example, two possible parses
are shown in Figure 2 (a) and (b), respectively.
The first parse corresponds to the action sequence
[SHIFT, SHIFT, REDUCE-R-NP, FINISH], while
the second parse corresponds to the action se-
quence [SHIFT, SHIFT, UNARY-NP, REDUCE-L-
VP, FINISH], which consists of one more action
than the first case. In practice, variances between
state items can be much larger than the chosen ex-
ample. In the extreme case where a state item does
not contain any unary action, the number of ac-
tions is 2n, where n is the number of words in
the sentence. On the other hand, if the maximum
number of consequent unary actions is 2 (Sagae
and Lavie, 2005; Zhang and Clark, 2009), then the
maximum number of actions a state item can have
is 4n.
The significant variance in the number of ac-
tions N can have an impact on the linear sepa-
rability of state items, for which the feature vec-
tors are
?N
i=1 ? (ai). This turns out to have a sig-
nificant empirical influence on perceptron training
with early-update, where the training of the model
interacts with search (Daume III, 2006).
One way of improving the comparability of
state items is to reduce the differences in their
sizes, and we use a padding method to achieve
this. The idea is to extend the set of actions by
adding an IDLE action, so that completed state
items can be further expanded using the IDLE ac-
tion. The action does not change the state itself,
but simply adds to the number of actions in the
sequence. A feature vector is extracted for the
IDLE action according to the final state context,
in the same way as other actions. Using the IDLE
action, the transition sequence for the two parses
in Figure 2 can be [SHIFT, SHIFT, REDUCE-
NP, FINISH, IDLE] and [SHIFT, SHIFT, UNARY-
NP, REDUCE-L-VP, FINISH], respectively. Their
436
Axioms [?, 0, false, 0, 0]
Goal [S, n, true, m : 2n ? m ? 4n, C]
Inference Rules:
[S, i, false, k,c]
SHIFT [S|w, i + 1, false, k + 1, c + cs]
[S|s1s0, i, false, k, c]
REDUCE-L/R-X [S|X, i, false, k + 1, c + cr]
[S|s0, i, false, k, c]
UNARY-X [S|X, i, false, k + 1, c + cu]
[S, n, false, k, c]
FINISH [S, n, true, k + 1, c + cf ]
[S,n, true, k, c]
IDLE [S, n, true, k + 1, c + ci]
Figure 3: Deductive system of the extended tran-
sition system.
corresponding feature vectors have about the same
sizes, and are more linearly separable. Note that
there can be more than one action that are padded
to a sequence of actions, and the number of IDLE
actions depends on the size difference between the
current action sequence and the largest action se-
quence without IDLE actions.
Given this extension, the deduction system is
shown in Figure 3. We add the number of actions
k to an item. The initial item (Axioms) has k = 0,
while the goal item has 2n ? k ? 4n. Given this
process, beam-search decoding can be made sim-
pler than that of Zhang and Clark (2009). While
they used a candidate output to record the best
completed state item, and finish decoding when
the agenda contains no more items, we can sim-
ply finish decoding when all items in the agenda
are completed, and output the best state item in
the agenda. With this new transition process, we
experimented with several extended features,and
found that the templates in Table 2 are useful to
improve the accuracies further. Here sill denotes
the left child of si?s left child. Other notations can
be explained in a similar way.
4 Semi-supervised Parsing with Large
Data
This section discusses how to extract informa-
tion from unlabeled data or auto-parsed data to
further improve shift-reduce parsing accuracies.
We consider three types of information, including
s0llwc, s0lrwc, s0luwc
s0rlwc, s0rrwc, s0ruwc
s0ulwc, s0urwc, s0uuwc
s1llwc, s1lrwc, s1luwc
s1rlwc, s1rrwc, s1ruwc
Table 2: New features for the extended parser.
paradigmatic relations, dependency relations, and
structural relations. These relations are captured
by word clustering, lexical dependencies, and a
dependency language model, respectively. Based
on the information, we propose a set of novel fea-
tures specifically designed for shift-reduce con-
stituent parsing.
4.1 Paradigmatic Relations: Word
Clustering
Word clusters are regarded as lexical intermedi-
aries for dependency parsing (Koo et al, 2008)
and POS tagging (Sun and Uszkoreit, 2012). We
employ the Brown clustering algorithm (Liang,
2005) on unannotated data (word segmentation is
performed if necessary). In the initial state of clus-
tering, each word in the input corpus is regarded
as a cluster, then the algorithm repeatedly merges
pairs of clusters that cause the least decrease in
the likelihood of the input corpus. The clustering
results are a binary tree with words appearing as
leaves. Each cluster is represented as a bit-string
from the root to the tree node that represents the
cluster. We define a function CLU(w) to return the
cluster ID (a bit string) of an input word w.
4.2 Dependency Relations: Lexical
Dependencies
Lexical dependencies represent linguistic relations
between words: whether a word modifies another
word. The idea of exploiting lexical dependency
information from auto-parsed data has been ex-
plored before for dependency parsing (Chen et al,
2009) and constituent parsing (Zhu et al, 2012).
To extract lexical dependencies, we first run the
baseline parser on unlabeled data. To simplify
the extraction process, we can convert auto-parsed
constituency trees into dependency trees by using
Penn2Malt. 2 From the dependency trees, we ex-
tract bigram lexical dependencies ?w1, w2, L/R?
where the symbol L (R) means that w1 (w2) is the
head of w2 (w1). We also extract trigram lexical
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
437
dependencies ?w1, w2, w3, L/R?, where L means
that w1 is the head of w2 and w3, meanwhile w2
and w3 are required to be siblings.
Following the strategy of Chen et al (2009),
we assign categories to bigram and trigram items
separately according to their frequency counts.
Specifically, top-10% most frequent items are as-
signed to the category of High Frequency (HF);
otherwise if an item is among top 20%, we assign
it to the category of Middle Frequency (MF); oth-
erwise the category of Low Frequency (LF). Here-
after, we refer to the bigram and trigram lexical
dependency lists as BLD and TLD, respectively.
4.3 Structural Relations: Dependency
Language Model
The dependency language model is proposed by
Shen et al (2008) and is used as additional in-
formation for graph-based dependency parsing in
Chen et al (2012). Formally, given a depen-
dency tree y of an input sentence x, we can
denote by H(y) the set of words that have at
least one dependent. For each xh ? H(y), we
have a corresponding dependency structure Dh =
(xLk, . . . xL1, xh, xR1, . . . , xRm). The probability
P (Dh) is defined to be
P (Dh) = PL(Dh) ? PR(Dh)
where PL(Dh) can be in turn defined as:
PL(Dh) ? P (xL1|xh)
?P (xL2|xL1, xh)
? . . .
?P (xLk|xLk?1, . . . , xLk?N+1, xh)
PR(Dh) can be defined in a similar way.
We build dependency language models on auto-
parsed data. Again, we convert constituency trees
into dependency trees for the purpose of simplic-
ity. From the dependency trees, we build a bigram
and a trigram language model, which are denoted
by BLM and TLM, respectively. The following
are the templates of the records of the dependency
language models.
(1) ?xLi, xh, P (xLi|xh)?
(2) ?xRi, xh, P (xRi|xh)?
(3) ?xLi, xLi?1, xh, P (xLi|xLi?1, xh)?
(4) ?xRi, xRi?1, xh, P (xRi|xRi?1, xh)?
Here the templates (1) and (2) belong to BLM
and the templates (3) and (4) belong to TLM. To
Stat Train Dev Test Unlabeled
EN # sent 39.8k 1.7k 2.4k 3,139.1k# word 950.0k 40.1k 56.7k 76,041.4k
CH # sent 18.1k 350 348 11,810.7k# word 493.8k 8.0k 6.8k 269,057.2k
Table 4: Statistics on sentence and word numbers
of the experimental data.
use the dependency language models, we employ
a map function ?(r) to assign a category to each
record r according to its probability, as in Chen et
al. (2012). The following is the map function.
?(r) =
?
??
??
HP if P (r) ? top?10%
MP else if P (r) ? top?30%
LP otherwise
4.4 Semi-supervised Features
We design a set of features based on the infor-
mation extracted from auto-parsed data or unan-
notated data. The features are summarized in Ta-
ble 3. Here CLU returns a cluster ID for a word.
The functions BLDl/r(?), TLDl/r(?), BLMl/r(?),
and TLMl/r(?) check whether a given word com-
bination can be found in the corresponding lists.
For example, BLDl(s1w, s0w) returns a category
tag (HF, MF, or LF) if ?s1w, s0w,L? exits in the
list BLD, else it returns NONE.
5 Experiments
5.1 Set-up
Labeled English data employed in this paper were
derived from the Wall Street Journal (WSJ) corpus
of the Penn Treebank (Marcus et al, 1993). We
used sections 2-21 as labeled training data, section
24 for system development, and section 23 for fi-
nal performance evaluation. For labeled Chinese
data, we used the version 5.1 of the Penn Chinese
Treebank (CTB) (Xue et al, 2005). Articles 001-
270 and 440-1151 were used for training, articles
301-325 were used as development data, and arti-
cles 271-300 were used for evaluation.
For both English and Chinese data, we used ten-
fold jackknifing (Collins, 2000) to automatically
assign POS tags to the training data. We found that
this simple technique could achieve an improve-
ment of 0.4% on English and an improvement of
2.0% on Chinese. For English POS tagging, we
adopted SVMTool, 3 and for Chinese POS tagging
3http://www.lsi.upc.edu/?nlp/SVMTool/
438
Word Cluster Features
CLU(s1w) CLU(s0w) CLU(q0w)
CLU(s1w)s1t CLU(s0w)s0t CLU(q0w)q0w
Lexical Dependency Features
BLDl(s1w, s0w) BLDl(s1w, s0w)?s1t?s0t BLDr(s1w, s0w)
BLDr(s1w, s0w)?s1t?s0t BLDl(s1w, q0w)?s1t?q0t BLDl(s1w, q0w)
BLDr(s1w, q0w) BLDr(s1w, q0w)?s1t?q0t BLDl(s0w, q0w)
BLDl(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)
TLDl(s1w, s1rdw, s0w) TLDl(s1w, s1rdw, s0w)?s1t?s0t TLDr(s1w, s0ldw, s0w)
TLDr(s1w, s0ldw, s0w)?s1t?s0t TLDl(s0w, s0rdw, q0w)?s0t?q0t TLDl(s0w, s0rdw, q0w)
TLDr(s0w,NONE, q0w) TLDr(s0w,NONE, q0w)?s0t?q0t
Dependency Language Model Features
BLMl(s1w, s0w) BLMl(s1w, s0w)?s1t?s0t BLMr(s1w, s0w)
BLMr(s1w, s0w)?s1t?s0t BLMl(s0w, q0w) BLMl(s0w, q0w)?s0t?q0t
BLMr(s0w, q0w)?s0t?q0t BLMr(s0w, q0w) TLMl(s1w, s1rdw, s0w)
TLMl(s1w, s1rdw, s0w)?s1t?s0t TLMr(s1w, s0ldw, s0w) TLMr(s1w, s0ldw, s0w)?s1t?s0t
Table 3: Semi-supervised features designed on the base of word clusters, lexical dependencies, and
dependency language models. Here the symbol si denotes a stack item, qi denotes a queue item, w
represents a word, and t represents a POS tag.
Lan. System LR LP F1
EN
G Baseline 88.4 88.7 88.6
+padding 88.8 89.5 89.1
+features 89.0 89.7 89.3
CH
N Baseline 85.6 86.3 86.0
+padding 85.5 87.2 86.4
+features 85.5 87.6 86.5
Table 5: Experimental results on the English and
Chinese development sets with the padding tech-
nique and new supervised features added incre-
mentally.
we employed the Stanford POS tagger. 4
We took the WSJ articles from the TIPSTER
corpus (LDC93T3A) as unlabeled English data. In
addition, we removed from the unlabeled English
data the sentences that appear in the WSJ corpus
of the Penn Treebank. For unlabeled Chinese data,
we used Chinese Gigaword (LDC2003T09), on
which we conducted Chinese word segmentation
by using a CRF-based segmenter. Table 4 summa-
rizes data statistics on sentence and word numbers
of the data sets listed above.
We used EVALB to evaluate parser perfor-
mances, including labeled precision (LP), labeled
recall (LR), and bracketing F1. 5 For significance
tests, we employed the randomized permutation-
based tool provided by Daniel Bikel. 6
In both training and decoding, we set the beam
size to 16, which achieves a good tradeoff be-
tween efficiency and accuracy. The optimal iter-
ation number of perceptron learning is determined
4http://nlp.stanford.edu/software/tagger.shtml
5http://nlp.cs.nyu.edu/evalb
6http://www.cis.upenn.edu/?dbikel/software.html#comparator
Lan. Features LR LP F1
EN
G +word cluster 89.3 90.0 89.7
+lexical dependencies 89.7 90.3 90.0
+dependency LM 90.0 90.6 90.3
CH
N +word cluster 85.7 87.5 86.6
+lexical dependencies 87.2 88.6 87.9
+dependency LM 87.2 88.7 88.0
Table 6: Experimental results on the English and
Chinese development sets with different types of
semi-supervised features added incrementally to
the extended parser.
on the development sets. For word clustering, we
set the cluster number to 50 for both the English
and Chinese experiments.
5.2 Results on Development Sets
Table 5 reports the results of the extended parser
(baseline + padding + supervised features) on the
English and Chinese development sets. We inte-
grated the padding method into the baseline parser,
based on which we further incorporated the super-
vised features in Table 2. From the results we find
that the padding method improves the parser accu-
racies by 0.5% and 0.4% on English and Chinese,
respectively. Incorporating the supervised features
in Table 2 gives further improvements of 0.2% on
English and 0.1% on Chinese.
Based on the extended parser, we experimented
different types of semi-supervised features by
adding the features incrementally. The results are
shown in Table 6. By comparing the results in Ta-
ble 5 and the results in Table 6 we can see that the
semi-supervised features achieve an overall im-
provement of 1.0% on the English data and an im-
439
Type Parser LR LP F1
SI
Ratnaparkhi (1997) 86.3 87.5 86.9
Collins (1999) 88.1 88.3 88.2
Charniak (2000) 89.5 89.9 89.5
Sagae & Lavie (2005)? 86.1 86.0 86.0
Sagae & Lavie (2006)? 87.8 88.1 87.9
Baseline 90.0 89.9 89.9
Petrov & Klein (2007) 90.1 90.2 90.1
Baseline+Padding 90.2 90.7 90.4
Carreras et al (2008) 90.7 91.4 91.1
RE Charniak & Johnson (2005) 91.2 91.8 91.5Huang (2008) 92.2 91.2 91.7
SE
Zhu et al (2012)? 90.4 90.5 90.4
Baseline+Padding+Semi 91.1 91.5 91.3
Huang & Harper (2009) 91.1 91.6 91.3
Huang et al (2010)? 91.4 91.8 91.6
McClosky et al (2006) 92.1 92.5 92.3
Table 7: Comparison of our parsers and related
work on the English test set. ? Shift-reduce
parsers. ? The results of self-training with a sin-
gle latent annotation grammar.
Type Parser LR LP F1
SI
Charniak (2000)? 79.6 82.1 80.8
Bikel (2004)? 79.3 82.0 80.6
Baseline 82.1 83.1 82.6
Baseline+Padding 82.1 84.3 83.2
Petrov & Klein (2007) 81.9 84.8 83.3
RE Charniak & Johnson (2005)? 80.8 83.8 82.3
SE Zhu et al (2012) 80.6 81.9 81.2Baseline+Padding+Semi 84.4 86.8 85.6
Table 8: Comparison of our parsers and related
work on the test set of CTB5.1.? Huang (2009)
adapted the parsers to Chinese parsing on CTB5.1.
? We run the parser on CTB5.1 to get the results.
provement of 1.5% on the Chinese data.
5.3 Final Results
Here we report the final results on the English and
Chinese test sets. We compared the final results
with a large body of related work. We grouped the
parsers into three categories: single parsers (SI),
discriminative reranking parsers (RE), and semi-
supervised parsers (SE). Table 7 shows the com-
parative results on the English test set and Table 8
reports the comparison on the Chinese test set.
From the results we can see that our extended
parser (baseline + padding + supervised features)
outperforms the Berkeley parser by 0.3% on En-
glish, and is comparable with the Berkeley parser
on Chinese (?0.1% less). Here +padding means
the padding technique and the features in Table 2.
After integrating semi-supervised features, the
parsing accuracy on English is improved to 91.3%.
We note that the performance is on the same level
Parser #Sent/Second
Ratnaparkhi (1997) Unk
Collins (1999) 3.5
Charniak (2000) 5.7
Sagae & Lavie (2005)? 3.7?
Sagae & Lavie (2006)? 2.2?
Petrov & Klein (2007) 6.2
Carreras et al (2008) Unk
This Paper
Baseline 100.7
Baseline+Padding 89.5
Baseline+Padding+Semi 46.8
Table 9: Comparison of running times on the En-
glish test set, where the time for loading models
is excluded. ? The results of SVM-based shift-
reduce parsing with greedy search. ? The results of
MaxEnt-based shift-reduce parser with best-first
search. ? Times reported by authors running on
different hardware.
as the performance of self-trained parsers, except
for McClosky et al (2006), which is based on the
combination of reranking and self-training. On
Chinese, the final parsing accuracy is 85.6%. To
our knowledge, this is by far the best reported per-
formance on this data set.
The padding technique, supervised features,
and semi-supervised features achieve an overall
improvement of 1.4% over the baseline on En-
glish, which is significant on the level of p <
10?5. The overall improvement on Chinese is
3.0%, which is also significant on the level of
p < 10?5.
5.4 Comparison of Running Time
We also compared the running times of our parsers
with the related single parsers. We ran timing tests
on an Intel 2.3GHz processor with 8GB mem-
ory. The comparison is shown in Table 9. From
the table, we can see that incorporating semi-
supervised features decreases parsing speed, but
the semi-supervised parser still has the advantage
of efficiency over other parsers. Specifically, the
semi-supervised parser is 7 times faster than the
Berkeley parser. Note that Sagae & Lavie (2005)
and Sagae & Lavie (2006) are also shift-reduce
parsers, and their running times were evaluated on
different hardwares. In practice, the running times
of the shift-reduce parsers should be much shorter
than the reported times in the table.
5.5 Error Analysis
We conducted error analysis for the three sys-
tems: the baseline parser, the extended parser with
440
 86
 88
 90
 92
 94
1 2 3 4 5 6 7 8
F 
Sc
or
e
Span Length
Baseline
Extended
Semi-supervised
Figure 5: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parsers on spans of different lengths.
the padding technique, and the semi-supervised
parser, focusing on the English test set. The analy-
sis was performed in four dimensions: parsing ac-
curacies on different phrase types, on constituents
of different span lengths, on different sentence
lengths, and on sentences with different numbers
of unknown words.
5.5.1 Different Phrase Types
Table 10 shows the parsing accuracies of the base-
line, extended parser, and semi-supervised parser
on different phrase types. Here we only consider
the nine most frequent phrase types in the English
test set. In the table, the phrase types are ordered
from left to right in the descending order of their
frequencies. We also show the improvements of
the semi-supervised parser over the baseline parser
(the last row in the table). As the results show, the
extended parser achieves improvements on most
of the phrase types with two exceptions: Preposi-
tion Prase (PP) and Quantifier Phrase (QP). Semi-
supervised features further improve parsing accu-
racies over the extended parser (QP is an excep-
tion). From the last row, we can see that improve-
ments of the semi-supervised parser over the base-
line on VP, S, SBAR, ADVP, and ADJP are above
the average improvement (1.4%).
5.5.2 Different Span Lengths
Figure 5 shows a comparison of the three parsers
on spans of different lengths. Here we consider
span lengths up to 8. As the results show, both
the padding extension and semi-supervised fea-
tures are more helpful on relatively large spans:
the performance gaps between the three parsers
are enlarged with increasing span lengths.
 82
 84
 86
 88
 90
 92
 94
10 20 30 40 50 60 70
F 
Sc
or
e
Sentence Length
Baseline
Extended
Semi-supervised
Figure 6: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parser on sentences of different lengths.
5.5.3 Different Sentence Lengths
Figure 6 shows a comparison of parsing accura-
cies of the three parsers on sentences of different
lengths. Each number on the horizontal axis repre-
sents the sentences whose lengths are between the
number and its previous number. For example, the
number 30 refers to the sentences whose lengths
are between 20 and 30. From the results we can
see that semi-supervised features improve parsing
accuracy on both short and long sentences. The
points at 70 are exceptions. In fact, sentences with
lengths between 60 and 70 have only 8 instances,
and the statistics on such a small number of sen-
tences are not reliable.
5.5.4 Different Numbers of Unknown Words
Figure 4 shows a comparison of parsing accura-
cies of the baseline, extended parser, and semi-
supervised parser on sentences with different num-
bers of unknown words. As the results show,
the padding method is not very helpful on sen-
tences with large numbers of unknown words,
while semi-supervised features help significantly
on this aspect. This conforms to the intuition that
semi-supervised methods reduce data sparseness
and improve the performance on unknown words.
6 Conclusion
In this paper, we addressed the problem of dif-
ferent action-sequence lengths for shift-reduce
phrase-structure parsing, and designed a set of
novel non-local features to further improve pars-
ing. The resulting supervised parser outperforms
the Berkeley parser, a state-of-the-art chart parser,
in both accuracies and speeds. In addition, we in-
corporated a set of semi-supervised features. The
441
System NP VP S PP SBAR ADVP ADJP WHNP QP
Baseline 91.9 90.1 89.8 88.1 85.7 84.6 72.1 94.8 89.3
Extended 92.1 90.7 90.2 87.9 86.6 84.5 73.6 95.5 88.6
Semi-supervised 93.2 92.0 91.5 89.3 88.2 86.8 75.1 95.7 89.1
Improvements +1.3 +1.9 +1.7 +1.2 +2.5 +2.2 +3.0 +0.9 -0.2
Table 10: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers
on different phrase types.
0 1 2 3 4 5 6 7
70
80
90
100
91
.9
8
89
.7
3
88
.8
7
87
.9
6
85
.9
5
83
.7
81
.4
2
82
.7
4
92
.1
7
90
.5
3
89
.5
1
87
.9
9
88
.6
6
87
.3
3
83
.8
9
80
.4
9
92
.8
8
91
.2
6
90
.4
3
89
.8
8
90
.3
5
86
.3
9 90
.6
8
90
.2
4
F-
sc
o
re
(%
)
Baseline Extended Semi-supervised
Figure 4: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parser
on sentences of different unknown words.
final parser reaches an accuracy of 91.3% on En-
glish and 85.6% on Chinese, by far the best re-
ported accuracies on the CTB data.
Acknowledgements
We thank the anonymous reviewers for their valu-
able comments. Yue Zhang and Muhua Zhu
were supported partially by SRG-ISTD-2012-038
from Singapore University of Technology and De-
sign. Muhua Zhu and Jingbo Zhu were funded
in part by the National Science Foundation of
China (61073140; 61272376), Specialized Re-
search Fund for the Doctoral Program of Higher
Education (20100042110031), and the Fundamen-
tal Research Funds for the Central Universities
(N100204002). Wenliang Chen was funded par-
tially by the National Science Foundation of China
(61203314).
References
Daniel M. Bikel. 2004. On the parameter space
of generative lexicalized statistical parsing models.
Ph.D. thesis, University of Pennsylvania.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP, pages 12?14, Jeju Island, Ko-
rea.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL, pages 9?16, Manchester, England.
Eugune Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugune Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, Seattle, Washington, USA.
Wenliang Chen, Junichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of EMNLP, pages 570?579, Singa-
pore.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012.
Utilizing dependency language models for graph-
based dependency. In Proceedings of ACL, pages
213?222, Jeju, Republic of Korea.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, Stroudsburg, PA, USA.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
ACL, Madrid, Spain.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking
for natural language processing. In Proceedings of
ICML, pages 175?182, Stanford, CA, USA.
Hal Daume III. 2006. Practical Structured Learn-
ing for Natural Language Processing. Ph.D. thesis,
USC.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
442
across languages. In Proceedings of EMNLP, pages
832?841, Singapore.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL, pages 1077?1086, Uppsala,
Sweden.
Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent variable
grammars. In Proceedings of EMNLP, pages 12?22,
Massachusetts, USA.
Liang Huang. 2008. Forest reranking: discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586?594, Ohio, USA.
Liang-Ya Huang. 2009. Improve Chinese parsing with
Max-Ent reranking parser. In Master Project Re-
port, Brown University.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of ICML, pages 282?289, Massachusetts, USA,
June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts Insti-
tute of Technology.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewiz. 1993. Building a large anno-
tated corpus of English. Computational Linguistics,
19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT/NAACL, Main Conference,
pages 152?159, New York City, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98, Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: a data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216?2219.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York,
April.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP, Rhode Island, USA.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125?132, Vancouver, Canada.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of HLT/NAACL,
Companion Volume: Short Papers, pages 129?132,
New York, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL, pages 577?585, Ohio, USA.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations: to-
wards accurate Chinese part-of-speech tagging. In
Proceedings of ACL, Jeju, Republic of Korea.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195?206,
Nancy, France.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL/HLT, pages 888?896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193, Portland, Ore-
gon, USA.
Muhua Zhu, Jingbo Zhu, and Huizhen Wang. 2012.
Exploiting lexical dependencies from large-scale
data for better shift-reduce constituency parsing. In
Proceedings of COLING, pages 3171?3186, Mum-
bai, India.
443
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110?114,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Easy-First POS Tagging and Dependency Parsing with Beam Search 
Ji Ma?   JingboZhu?  Tong Xiao?   Nan Yang? 
?Natrual Language Processing Lab., Northeastern University, Shenyang, China 
?MOE-MS Key Lab of MCC, University of Science and Technology of China, 
Hefei, China 
majineu@outlook.com 
{zhujingbo, xiaotong}@mail.neu.edu.cn 
nyang.ustc@gmail.com 
 
Abstract 
In this paper, we combine easy-first de-
pendency parsing and POS tagging algo-
rithms with beam search and structured 
perceptron. We propose a simple variant 
of ?early-update? to ensure valid update 
in the training process. The proposed so-
lution can also be applied to combine 
beam search and structured perceptron 
with other systems that exhibit spurious 
ambiguity. On CTB, we achieve 94.01% 
tagging accuracy and 86.33% unlabeled 
attachment score with a relatively small 
beam width. On PTB, we also achieve 
state-of-the-art performance. 
1 Introduction 
The easy-first dependency parsing algorithm 
(Goldberg and Elhadad, 2010) is attractive due to 
its good accuracy, fast speed and simplicity. The 
easy-first parser has been applied to many appli-
cations (Seeker et al, 2012; S?ggard and Wulff, 
2012). By processing the input tokens in an easy-
to-hard order, the algorithm could make use of 
structured information on both sides of the hard 
token thus making more indicative predictions. 
However, rich structured information also causes 
exhaustive inference intractable. As an alterna-
tive, greedy search which only explores a tiny 
fraction of the search space is adopted (Goldberg 
and Elhadad, 2010). 
 To enlarge the search space, a natural exten-
sion to greedy search is beam search. Recent 
work also shows that beam search together with 
perceptron-based global learning (Collins, 2002) 
enable the use of non-local features that are help-
ful to improve parsing performance without 
overfitting (Zhang and Nivre, 2012). Due to the-
se advantages, beam search and global learning 
has been applied to many NLP tasks (Collins and 
Roark 2004; Zhang and Clark, 2007). However, 
to the best of our knowledge, no work in the lit-
erature has ever applied the two techniques to 
easy-first dependency parsing.  
While applying beam-search is relatively 
straightforward, the main difficulty comes from 
combining easy-first dependency parsing with 
perceptron-based global learning. In particular, 
one needs to guarantee that each parameter up-
date is valid, i.e., the correct action sequence has 
lower model score than the predicted one1. The 
difficulty in ensuring validity of parameter up-
date for the easy-first algorithm is caused by its 
spurious ambiguity, i.e., the same result might be 
derived by more than one action sequences.  
For algorithms which do not exhibit spurious 
ambiguity, ?early update? (Collins and Roark 
2004) is always valid: at the k-th step when the 
single correct action sequence falls off the beam, 
                                                 
1 As shown by (Huang et al, 2012), only valid update guar-
antees the convergence of any perceptron-based training. 
Invalid update may lead to bad learning or even make the 
learning not converge at all. 
Figure 1: Example of cases without/with spurious 
ambiguity. The 3 ? 1 table denotes a beam. ?C/P? 
denotes correct/predicted action sequence. The 
numbers following C/P are model scores. 
 
110
its model score must be lower than those still in 
the beam (as illustrated in figure 1, also see the 
proof in (Huang et al, 2012)). While for easy-
first dependency parsing, there could be multiple 
action sequences that yield the gold result (C1 and 
C2 in figure 1). When all correct sequences fall 
off the beam, some may indeed have higher 
model score than those still in the beam (C2 in 
figure 1), causing invalid update. 
For the purpose of valid update, we present a 
simple solution which is based on early update. 
The basic idea is to use one of the correct action 
sequences that were pruned right at the k-th step 
(C1 in figure 1) for parameter update.  
The proposed solution is general and can also 
be applied to other algorithms that exhibit spuri-
ous ambiguity, such as easy-first POS tagging 
(Ma et al, 2012) and transition-based dependen-
cy parsing with dynamic oracle (Goldberg and 
Nivre, 2012). In this paper, we report experi-
mental results on both easy-first dependency 
parsing and POS tagging (Ma et al, 2012). We 
show that both easy-first POS tagging and de-
pendency parsing can be improved significantly 
from beam search and global learning. Specifi-
cally, on CTB we achieve 94.01% tagging accu-
racy which is the best result to date2 for a single 
tagging model. With a relatively small beam, we 
achieve 86.33% unlabeled score (assume gold 
tags), better than state-of-the-art transition-based 
parsers (Huang and Sagae, 2010; Zhang and 
Nivre, 2011). On PTB, we also achieve good 
results that are comparable to the state-of-the-art. 
2 Easy-first dependency parsing 
The easy-first dependency parsing algorithm 
(Goldberg and Elhadad, 2010) builds a depend-
ency tree by performing two types of actions 
LEFT(i) and RIGHT(i) to a list of sub-tree struc-
tures p1,?, pr. pi is initialized with the i-th word  
                                                 
2 Joint tagging-parsing models achieve higher accuracy, but 
those models are not directly comparable to ours.  
Algorithm 1: Easy-first with beam search 
Input:     sentence   of n words,  beam width s 
Output:  one best dependency tree 
     (     )        
         ( )   
    (  ) 
            // top s extensions from the beam 
1                     // initially, empty beam 
2 for    1   1 do 
3             (        ) 
4 return        ( )   // tree built by the best sequence  
 
of the input sentence. Action LEFT(i)/RIGHT(i) 
attaches pi to its left/right neighbor and then re-
moves pi from the sub-tree list. The algorithm 
proceeds until only one sub-tree left which is the 
dependency tree of the input sentence (see the 
example in figure 2). Each step, the algorithm 
chooses the highest score action to perform ac-
cording to the linear model: 
     ( )     ( ) 
Here,  is the weight vector and  is the feature 
representation. In particular,  (    ( ) 
     ( )) denotes features extracted from pi. 
The parsing algorithm is greedy which ex-
plores a tiny fraction of the search space. Once 
an incorrect action is selected, it can never yield 
the correct dependency tree. To enlarge the 
search space, we introduce the beam-search ex-
tension in the next section. 
3 Easy-first with beam search  
In this section, we introduce easy-first with beam 
search in our own notations that will be used 
throughout the rest of this paper.  
For a sentence x of n words, let   be the action 
(sub-)sequence that can be applied, in sequence, 
to x and the result sub-tree list is denoted by 
 ( )  For example, suppose x is ?I am valid? and 
y is [RIGHT(1)], then y(x) yields figure 2(b). Let 
   to be LEFT(i)/RIGHT(i) actions where    1   . 
Thus, the set of all possible one-action extension 
of   is: 
     ( )            ( )   
Here, ? ? means insert   to the end of  . Follow-
ing (Huang et al, 2012), in order to formalize 
beam search, we also use the          
    ( ) 
operation which returns the top s action sequenc-
es in   according to   ( ). Here,  denotes a 
set of action sequences,   ( ) denotes the sum of 
feature vectors of each action in    
Pseudo-code of easy-first with beam search is 
shown in algorithm 1. Beam search grows s 
(beam width) action sequences in parallel using a  
Figure 2: An example of parsing ?I am valid?. Spu-
rious ambiguity: (d) can be derived by both 
[RIGHT(1), LEFT(2)] and [LEFT(3), RIGHT(1)]. 
111
Algorithm 2: Perceptron-based training over one 
training sample (   ) 
Input:    (   ), s, parameter   
Output: new parameter    
    (       )        
     (      ( ))   
   (  ) 
 // top correct extension from the beam 
1         
2 for    1   1 do 
3     ?      (          ) 
4            (        ) 
5    if           // all correct seq. falls off the beam 
6             ( ?)   (     ) 
7         break 
8 if        ( )      // full update 
9         ( ?)   (       )  
10 return  
 
beam  , (sequences in   are sorted in terms of 
model score, i.e.,   (    )     (  1 ) ). 
At each step, the sequences in   are expanded in 
all possible ways and then   is filled up with the 
top s newly expanded sequences (line 2 ~ line 3). 
Finally, it returns the dependency tree built by 
the top action sequence in      . 
4 Training  
To learn the weight vector , we use the percep-
tron-based global learning3 (Collins, 2002) which 
updates  by rewarding the feature weights fired 
in the correct action sequence and punish those 
fired in the predicted incorrect action sequence. 
Current work (Huang et al, 2012) rigorously 
explained that only valid update ensures conver-
gence of any perceptron variants. They also justi-
fied that the popular ?early update? (Collins and  
Roark, 2004) is valid for the systems that do not 
exhibit spurious ambiguity4.  
However, for the easy-first algorithm or more 
generally, systems that exhibit spurious ambigui-
ty, even ?early update? could fail to ensure valid-
ity of update (see the example in figure 1). For 
validity of update, we propose a simple solution 
which is based on ?early update? and which can 
accommodate spurious ambiguity. The basic idea 
is to use the correct action sequence which was  
                                                 
3 Following (Zhang and Nivre, 2012), we say the training 
algorithm is global if it optimizes the score of an entire ac-
tion sequence. A local learner trains a classifier which dis-
tinguishes between single actions. 
4 As shown in (Goldberg and Nivre 2012), most transition-
based dependency parsers (Nivre et al, 2003; Huang and 
Sagae 2010;Zhang and Clark 2008) ignores spurious ambi-
guity by using a static oracle which maps a dependency tree 
to a single action sequence.  
Features of (Goldberg and Elhadad, 2010) 
for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp,  
tp-vrp, tlcp, trcp, wlcp, wlcp 
for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp,  tp-trcp, tp-tlcp-trcp 
for p, q, r in (pi-2, pi-1, pi), (pi-
1, pi+1, pi), (pi+1, pi+2 ,pi) 
tp-tq-tr, tp-tq-wr 
for p, q in (pi-1, pi) tp-tlcp-tq,   tp-trcp-tq,   ,tp-tlcp-wq,, 
 tp-trcp-wq,   tp-wq-tlcq,  tp-wq-trcq 
 
Table 1: Feature templates for English dependency 
parsing. wp denotes the head word of p, tp denotes the 
POS tag of wp. vlp/vrp denotes the number p?s of 
left/right child. lcp/rcp denotes p?s leftmost/rightmost 
child. pi denotes partial tree being considered. 
 
pruned right at the step when all correct sequence 
falls off the beam (as C1 in figure 1).  
Algorithm 2 shows the pseudo-code of the 
training procedure over one training sample 
(   ), a sentence-tree pair. Here we assume   to 
be the set of all correct action sequences/sub-
sequences. At step k, the algorithm constructs a 
correct action sequence  ? of length k by extend-
ing those in      (line 3). It also checks whether 
   no longer contains any correct sequence. If so, 
 ? together with       are used for parameter up-
date (line 5 ~ line 6). It can be easily verified that 
each update in line 6 is valid. Note that both 
?TOPC? and the operation in line 5 use   to check 
whether an action sequence y is correct or not. 
This  can  be  efficiently  implemented   (without 
explicitly enumerating  ) by checking if each 
LEFT(i)/RIGHT(i) in y are compatible with (   ): 
pi already collected all its dependents according 
to t; pi is attached to the correct neighbor sug-
gested by t.  
5 Experiments 
For English, we use PTB as our data set. We use 
the standard split for dependency parsing and the 
split used by (Ratnaparkhi, 1996) for POS tag-
ging. Penn2Malt5 is used to convert the bracket-
ed structure into dependencies. For dependency 
parsing, POS tags of the training set are generat-
ed using 10-fold jack-knifing.  
For Chinese, we use CTB 5.1 and the split 
suggested by (Duan et al, 2007) for both tagging 
and dependency parsing. We also use Penn2Malt 
and the head-finding rules of (Zhang and Clark 
2008) to convert constituency trees into depend-
encies. For dependency parsing, we assume gold 
segmentation and POS tags for the input.  
                                                 
5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 
112
Features used in English dependency parsing 
are listed in table 1. Besides the features in 
(Goldberg and Elhadad, 2010), we also include 
some trigram features and valency features 
which are useful for transition-based dependency 
parsing (Zhang and Nivre, 2011). For English 
POS tagging, we use the same features as in 
(Shen et al, 2007). For Chinese POS tagging and 
dependency parsing, we use the same features as 
(Ma et al, 2012). All of our experiments are 
conducted on a Core i7 (2.93GHz) machine, both 
the tagger and parser are implemented using C++.  
5.1 Effect of beam width 
Tagging/parsing performances with different 
beam widths on the development set are listed in 
table 2 and table 3. We can see that Chinese POS  
tagging, dependency parsing as well as English 
dependency parsing greatly benefit from beam 
search. While tagging accuracy on English only 
slightly improved. This may because that the 
accuracy of the greedy baseline tagger is already 
very high and it is hard to get further improve-
ment. Table 2 and table 3 also show that the 
speed of both tagging and dependency parsing 
drops linearly with the growth of beam width. 
5.2 Final results 
Tagging results on the test set together with some 
previous results are listed in table 4. Dependency 
parsing results on CTB and PTB are listed in ta-
ble 5 and table 6, respectively. 
On CTB, tagging accuracy of our greedy base-
line is already comparable to the state-of-the-art. 
As the beam size grows to 5, tagging accuracy 
increases to 94.01% which is 2.3% error reduc-
tion. This is also the best tagging accuracy com-
paring with previous single tagging models (For 
limited space, we do not list the performance of 
joint tagging-parsing models).  
Parsing performances on both PTB and CTB 
are significantly improved with a relatively small 
beam width (s = 8). In particular, we achieve 
86.33% uas on CTB which is 1.54% uas im-
provement over the greedy baseline parser. 
Moreover, the performance is better than the best 
transition-based parser (Zhang and Nivre, 2011) 
which adopts a much larger beam width (s = 64).  
6 Conclusion and related work 
This work directly extends (Goldberg and El-
hadad, 2010) with beam search and global learn-
ing. We show that both the easy-first POS tagger 
and dependency parser can be significantly impr- 
s PTB CTB speed  
1 97.17 93.91 1350 
3 97.20 94.15 560 
5 97.22 94.17 385 
 
Table 2: Tagging accuracy vs beam width vs. Speed is 
evaluated using the number of sentences that can be 
processed in one second 
 
s 
PTB CTB 
speed 
uas compl uas compl 
1 91.77 45.29 84.54 33.75 221 
2 92.29 46.28 85.11 34.62 124 
4 92.50 46.82 85.62 37.11 71 
8 92.74 48.12 86.00 35.87 39 
 
Table 3: Parsing accuracy vs beam width. ?uas? and 
?compl? denote unlabeled score and complete match 
rate respectively (all excluding punctuations). 
 
PTB CTB 
(Collins, 2002) 97.11 (Hatori et al, 2012) 93.82 
(Shen et al, 2007) 97.33 (Li et al, 2012) 93.88 
(Huang et al, 2012) 97.35 (Ma et al, 2012) 93.84 
this work   1 97.22 this work   1 93.87 
this work     97.28 this work     94.01? 
 
Table 4: Tagging results on the test set. ??? denotes 
statistically significant over the greedy baseline by 
McNemar?s test (      ) 
 
Systems s uas compl 
(Huang and Sagae, 2010) 8 85.20 33.72 
(Zhang and Nivre, 2011) 64 86.00 36.90 
(Li et al, 2012) ? 86.55 ? 
this work 1 84.79 32.98 
this work 8 86.33
?
 36.13 
 
Table 5: Parsing results on CTB test set. 
  
Systems s uas compl 
(Huang and Sagae, 2010) 8 92.10 ? 
(Zhang and Nivre, 2011) 64 92.90 48.50 
(Koo and Collins, 2010) ? 93.04 ? 
this work 1 91.72 44.04 
this work 8 92.47
?
 46.07 
 
Table 6: Parsing results on PTB test set.  
 
oved using beam search and global learning. 
This work can also be considered as applying 
(Huang et al, 2012) to the systems that exhibit 
spurious ambiguity. One future direction might 
be to apply the training method to transition-
based parsers with dynamic oracle (Goldberg and 
Nivre, 2012) and potentially further advance per-
formances of state-of-the-art transition-based 
parsers. 
113
Shen et al, (2007) and (Shen and Joshi, 2008) 
also proposed bi-directional sequential classifica-
tion with beam search for POS tagging and 
LTAG dependency parsing, respectively. The 
main difference is that their training method aims 
to learn a classifier which distinguishes between 
each local action while our training method aims 
to distinguish between action sequences. Our 
method can also be applied to their framework. 
Acknowledgments 
We would like to thank Yue Zhang, Yoav Gold-
berg and Zhenghua Li for discussions and sug-
gestions on earlier drift of this paper. We would 
also like to thank the three anonymous reviewers 
for their suggestions. This work was supported in 
part by the National Science Foundation of Chi-
na (61073140; 61272376), Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion (20100042110031) and the Fundamental 
Research Funds for the Central Universities 
(N100204002). 
References  
Collins, M. 2002. Discriminative training methods for 
hidden markov models: Theory and experiments 
with perceptron algorithms. In Proceedings of 
EMNLP. 
Duan, X., Zhao, J., , and Xu, B. 2007. Probabilistic 
models for action-based Chinese dependency pars-
ing. In Proceedings of ECML/ECPPKDD. 
Goldberg, Y. and Elhadad, M. 2010 An Efficient Al-
gorithm for Eash-First Non-Directional Dependen-
cy Parsing. In Proceedings of NAACL 
Huang, L. and Sagae, K. 2010. Dynamic program-
ming for linear-time incremental parsing. In Pro-
ceedings of ACL. 
Huang, L. Fayong, S. and Guo, Y. 2012. Structured 
Perceptron with Inexact Search. In Proceedings of 
NAACL. 
Koo, T. and Collins, M. 2010. Efficient third-order 
dependency parsers. In Proceedings of ACL. 
Li, Z., Zhang, M., Che, W., Liu, T. and Chen, W. 
2012. A Separately Passive-Aggressive Training 
Algorithm for Joint POS Tagging and Dependency 
Parsing. In Proceedings of COLING 
Ma, J., Xiao, T., Zhu, J. and Ren, F. 2012. Easy-First 
Chinese POS Tagging and Dependency Parsing. In 
Proceedings of COLING 
Rataparkhi, A. (1996) A Maximum Entropy Part-Of-
Speech Tagger. In Proceedings of EMNLP 
Shen, L., Satt, G. and Joshi, A. K. (2007) Guided 
Learning for Bidirectional Sequence Classification. 
In Proceedings of ACL. 
Shen, L. and  Josh, A. K. 2008. LTAG Dependency 
Parsing with Bidirectional Incremental Construc-
tion. In Proceedings of  EMNLP. 
Seeker, W., Farkas, R. and Bohnet, B. 2012 Data-
driven Dependency Parsing With Empty Heads. In 
Proceedings of COLING 
S?ggard, A. and Wulff, J. 2012. An Empirical Study 
of Non-lexical Extensions to Delexicalized Trans-
fer. In Proceedings of COLING 
Yue Zhang and Stephen Clark. 2007 Chinese Seg-
mentation Using a Word-based Perceptron Algo-
rithm. In Proceedings of ACL.  
Zhang, Y. and Clark, S. 2008. Joint word segmenta-
tion and POS tagging using a single perceptron. In 
Proceedings of ACL. 
Zhang, Y. and Nivre, J. 2011. Transition-based de-
pendency parsing with rich non-local features. In 
Proceedings of ACL. 
Zhang, Y. and Nivre, J. 2012. Analyzing the Effect of 
Global Learning and Beam-Search for Transition-
Based Dependency Parsing. In Proceedings of 
COLING. 
114
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 144?154,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Tagging The Web: Building A Robust Web Tagger with Neural Network
Ji Ma
?
, Yue Zhang
?
and Jingbo Zhu
?
?
Northeastern University, China
?
Singapore University of Technology and Design
majineu@gmail.com
yue zhang@sutd.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
In this paper, we address the problem of
web-domain POS tagging using a two-
phase approach. The first phase learns rep-
resentations that capture regularities un-
derlying web text. The representation is
integrated as features into a neural network
that serves as a scorer for an easy-first POS
tagger. Parameters of the neural network
are trained using guided learning in the
second phase. Experiment on the SANCL
2012 shared task show that our approach
achieves 93.15% average tagging accu-
racy, which is the best accuracy reported
so far on this data set, higher than those
given by ensembled syntactic parsers.
1 Introduction
Analysing and extracting useful information from
the web has become an increasingly important re-
search direction for the NLP community, where
many tasks require part-of-speech (POS) tag-
ging as a fundamental preprocessing step. How-
ever, state-of-the-art POS taggers in the literature
(Collins, 2002; Shen et al, 2007) are mainly opti-
mized on the the Penn Treebank (PTB), and when
shifted to web data, tagging accuracies drop sig-
nificantly (Petrov and McDonald, 2012).
The problem we face here can be considered
as a special case of domain adaptation, where we
have access to labelled data on the source domain
(PTB) and unlabelled data on the target domain
(web data). Exploiting useful information from
the web data can be the key to improving web
domain tagging. Towards this end, we adopt the
idea of learning representations which has been
demonstrated useful in capturing hidden regular-
ities underlying the raw input data (web text, in
our case).
Our approach consists of two phrases. In the
pre-training phase, we learn an encoder that con-
verts the web text into an intermediate represen-
tation, which acts as useful features for prediction
tasks. We integrate the learned encoder with a set
of well-established features for POS tagging (Rat-
naparkhi, 1996; Collins, 2002) in a single neural
network, which is applied as a scorer to an easy-
first POS tagger. We choose the easy-first tagging
approach since it has been demonstrated to give
higher accuracies than the standard left-to-right
POS tagger (Shen et al, 2007; Ma et al, 2013).
In the fine-tuning phase, the parameters of the
network are optimized on a set of labelled train-
ing data using guided learning. The learned model
preserves the property of preferring to tag easy
words first. To our knowledge, we are the first to
investigate guided learning for neural networks.
The idea of learning representations from un-
labelled data and then fine-tuning a model with
such representations according to some supervised
criterion has been studied before (Turian et al,
2010; Collobert et al, 2011; Glorot et al, 2011).
While most previous work focus on in-domain se-
quential labelling or cross-domain classification
tasks, we are the first to learn representations for
web-domain structured prediction. Previous work
treats the learned representations either as model
parameters that are further optimized in super-
vised fine-tuning (Collobert et al, 2011) or as
fixed features that are kept unchanged (Turian et
al., 2010; Glorot et al, 2011). In this work,
we investigate both strategies and give empirical
comparisons in the cross-domain setting. Our re-
sults suggest that while both strategies improve
in-domain tagging accuracies, keeping the learned
representation unchanged consistently results in
better cross-domain accuracies.
We conduct experiments on the official data set
provided by the SANCL 2012 shared task (Petrov
and McDonald, 2012). Our method achieves a
93.15% average accuracy across the web-domain,
which is the best result reported so far on this data
144
set, higher than those given by ensembled syntac-
tic parsers. Our code will be publicly available at
https://github.com/majineu/TWeb.
2 Learning from Web Text
Unsupervised learning is often used for training
encoders that convert the input data to abstract rep-
resentations (i.e. encoding vectors). Such repre-
sentations capture hidden properties of the input,
and can be used as features for supervised tasks
(Bengio, 2009; Ranzato et al, 2007). Among the
many proposed encoders, we choose the restricted
Boltzmann machine (RBM), which has been suc-
cessfully used in many tasks (Lee et al, 2009b;
Hinton et al, 2006). In this section, we give some
background on RBMs and then show how they can
be used to learn representations of the web text.
2.1 Restricted Boltzmann Machine
The RBM is a type of graphical model that con-
tains two layers of binary stochastic units v ?
{0, 1}
V
and h ? {0, 1}
H
, corresponding to a set
of visible and hidden variables, respectively. The
RBM defines the joint probability distribution over
v and h by an energy function
E(v,h) = ?c
?
h? b
?
v ? h
?
Wv, (1)
which is factorized by a visible bias b ? R
V
, a
hidden bias c ? R
H
and a weight matrix W ?
R
H?V
. The joint distribution P (v,h) is given by
P (v,h) =
1
Z
exp(E(v,h)), (2)
where Z is the partition function.
The affine form of E with respect to v and h
implies that the visible variables are conditionally
independent with each other given the hidden layer
units, and vice versa. This yields the conditional
distribution:
P (v|h) =
V
?
j=1
P (v
j
|h) P (h|v) =
H
?
i=1
P (h
i
|v)
P (v
j
= 1|h) = ?(b
j
+W
?j
h) (3)
P (h
i
= 1|v) = ?(c
j
+W
i?
v) (4)
Here ? denotes the sigmoid function. Parameters
of RBMs ? = {b, c,W} can be trained efficiently
using contrastive divergence learning (CD), see
(Hinton, 2002) for detailed descriptions of CD.
2.2 Encoding Web Text with RBM
Most of the indicative features for POS disam-
biguation can be found from the words and word
combinations within a local context (Ratnaparkhi,
1996; Collins, 2002). Inspired by this observa-
tion, we apply the RBM to learn feature repre-
sentations from word n-grams. More specifically,
given the i
th
word w
i
of a sentence, we apply
RBMs to model the joint distribution of the n-gram
(w
i?l
, ? ? ? , w
i+r
), where l and r denote the left
and right window, respectively. Note that the vis-
ible units of RBMs are binary. While in our case,
each visible variable corresponds to a word, which
may take on tens-of-thousands of different values.
Therefore, the RBM need to be re-factorized to
make inference tractable.
We utilize the Word Representation RBM (WR-
RBM) factorization proposed by Dahl et al
(2012). The basic idea is to share word representa-
tions across different positions in the input n-gram
while using position-dependent weights to distin-
guish between different word orders.
Let w
k
be the k-th entry of lexicon L, and w
k
be its one-hot representation (i.e., only the k-th
component of w
k
is 1, and all the others are 0).
Let v
(j)
represents the j-th visible variable of the
WRRBM, which is a vector of length |L|. Then
v
(j)
= w
k
means that the j-th word in the n-gram
is w
k
. Let D ? R
D?|L|
be a projection matrix,
then Dw
k
projects w
k
into a D-dimensional real
value vector (embedding). For each position j,
there is a weight matrix W
(j)
? R
H?D
, which
is used to model the interaction between the hid-
den layer and the word projection in position j.
The visible biases are also shared across different
positions (b
(j)
= b ?j) and the energy function is:
E(v,h) = ?c
?
h?
n
?
j=1
(b
?
v
(j)
+ h
?
W
(j)
Dv
(j)
),
(5)
which yields the conditional distributions:
P (v|h) =
n
?
j=1
P (v
(j)
|h) P (h|v) =
?
i=1
P (h
i
|v)
P (h
i
= 1|v) = ?(c
i
+
n
?
j=1
W
(j)
i?
Dv
(j)
) (6)
P (v
(j)
= w
k
|h) =
1
Z
exp(b
?
w
k
+ h
?
W
(j)
Dw
k
)
(7)
145
Again Z is the partition function.
The parameters {b, c,D,W
(1)
, . . . ,W
(n)
}
can be trained using a Metropolis-Hastings-based
CD variant and the learned word representations
also capture certain syntactic information; see
Dahl et al (2012) for more details.
Note that one can stack standard RBMs on top
of a WRRBM to construct a Deep Belief Network
(DBN). By adopting greedy layer-wise training
(Hinton et al, 2006; Bengio et al, 2007), DBNs
are capable of modelling higher order non-linear
relations between the input, and has been demon-
strated to improve performance for many com-
puter vision tasks (Hinton et al, 2006; Bengio et
al., 2007; Lee et al, 2009a). However, in this work
we do not observe further improvement by em-
ploying DBNs. This may partly be due to the fact
that unlike computer vision tasks, the input struc-
ture of POS tagging or other sequential labelling
tasks is relatively simple, and a single non-linear
layer is enough to model the interactions within
the input (Wang and Manning, 2013).
3 Neural Network for POS
Disambiguation
We integrate the learned WRRBM into a neural
network, which serves as a scorer for POS dis-
ambiguation. The main challenge to designing
the neural network structure is: on the one hand,
we hope that the model can take the advantage
of information provided by the learned WRRBM,
which reflects general properties of web texts, so
that the model generalizes well in the web domain;
on the other hand, we also hope to improve the
model?s discriminative power by utilizing well-
established POS tagging features, such as those of
Ratnaparkhi (1996).
Our approach is to leverage the two sources of
information in one neural network by combining
them though a shared output layer, as shown in
Figure 1. Under the output layer, the network
consists of two modules: the web-feature mod-
ule, which incorporates knowledge from the pre-
trained WRRBM, and the sparse-feature module,
which makes use of other POS tagging features.
3.1 The Web-Feature Module
The web-feature module, shown in the lower left
part of Figure 1, consists of a input layer and two
hidden layers. The input for the this module is the
word n-gram (w
i?l
, . . . , w
i+r
), the form of which
Figure 1: The proposed neural network. The web-
feature module (lower left) and sparse-feature
module (lower right) are combined by a shared
output layer (upper).
is identical to the training data of the pre-trained
WRRBM.
The first layer is a linear projection layer, where
each word in the input is projected into a D-
dimensional real value vector using the projection
operation described in Section 2.2. The output of
this layer o
1
w
is the concatenation of the projec-
tions of w
i?l
, . . . , w
i+r
:
o
1
w
=
?
?
?
M
1
w
w
i?l
.
.
.
M
1
w
w
i+r
?
?
?
(8)
Here M
1
w
denotes the parameters of the first layer
of the web-feature module, which is a D ? |L|
projection matrix.
The second layer is a sigmoid layer to model
non-linear relations between the word projections:
o
2
w
= ?(M
2
w
o
1
w
+ b
2
w
) (9)
Parameters of this layer include: a bias vector
b
2
w
? R
H
and a weight matrix M
2
w
? R
H?nD
.
The web-feature module enables us to explore
the learned WRRBM in various ways. First, it al-
lows us to investigate knowledge from the WR-
RBM incrementally. We can choose to use only
the word representations of the learned WRRBM.
This can be achieved by initializing only the first
layer of the web module with the projection matrix
D of the learned WRRBM:
M
1
w
? D. (10)
Alternatively, we can choose to use the hidden
states of the WRRBM, which can be treated as the
146
representations of the input n-gram. This can be
achieved by also initializing the parameters of the
second layer of the web-feature module using the
position-dependent weight matrix and hidden bias
of the learned WRRBM:
b
2
w
? c (11)
M
2
w
? (W
(1)
, . . . ,W
(n)
) (12)
Second, the web-feature module also allows us
to make a comparison between whether or not to
further adjust the pre-trained representation in the
supervised fine-tuning phase, which corresponds
to the supervised learning strategies of Turian et al
(2010) and Collobert et al (2011), respectively. To
our knowledge, no investigations have been pre-
sented in the literature on this issue.
3.2 The Sparse-Feature Module
The sparse-feature module, as shown in the lower
right part of Figure 1, is designed to incorporate
commonly-used tagging features. The input for
this module is a vector of boolean values ?(x) =
(f
1
(x), . . . , f
k
(x)), where x denotes the partially
tagged input sentence and f
i
(x) denotes a fea-
ture function, which returns 1 if the correspond-
ing feature fires and 0 otherwise. The first layer of
this module is a linear transformation layer, which
converts the high dimensional sparse vector into a
fixed-dimensional real value vector:
o
s
= M
s
?(x) + b
s
(13)
Depending on the specific task being considered,
the output of this layer can be further fed to other
non-linear layers, such as a sigmoid or hyperbolic
tangent layer, to model more complex relations.
For POS tagging, we found that a simple linear
layer yields satisfactory accuracies.
The web-feature and sparse-feature modules are
combined by a linear output layer, as shown in the
upper part of Figure 1. The value of each unit in
this layer denotes the score of the corresponding
POS tag.
o
o
= M
o
(
o
w
o
s
)
+ b
o
(14)
In some circumstances, probability distribution
over POS tags might be a more preferable form
of output. Such distribution can be easily obtained
by adding a soft-max layer on top of the output
layer to perform a local normalization, as done by
Collobert et al (2011).
Algorithm 1 Easy-first POS tagging
Input: x a sentence of m words w
1
, . . . , w
m
Output: tag sequence of x
1: U? [w
1
, . . . , w
m
] // untagged words
2: while U 6= [] do
3: (w?,
?
t)? arg max
(w,t)?U?T
S(w, t)
4: w?.t?
?
t
5: U? U/[w?] // remove w? from U
6: end while
7: return [w
1
.t, . . . , w
m
.t]
4 Easy-first POS tagging with Neural
Network
The neural network proposed in Section 3 is used
for POS disambiguation by the easy-first POS tag-
ger. Parameters of the network are trained using
guided learning, where learning and search inter-
act with each other.
4.1 Easy-first POS tagging
Pseudo-code of easy-first tagging is shown in Al-
gorithm 1. Rather than tagging a sentence from
left to right, easy-first tagging is based on a deter-
ministic process, repeatedly selecting the easiest
word to tag. Here ?easiness? is evaluated based
on a statistical model. At each step, the algorithm
adopts a scorer, the neural network in our case,
to assign a score to each possible word-tag pair
(w, t), and then selects the highest score one (w?,
?
t)
to tag (i.e., tag w? with
?
t). The algorithm repeats
until all words are tagged.
4.2 Training
The training algorithm repeats for several itera-
tions over the training data, which is a set of sen-
tences labelled with gold standard POS tags. In
each iteration, the procedure shown in Algorithm
2 is applied to each sentence in the training set.
At each step during the processing of a training
example, the algorithm calculates a margin loss
based on two word-tag pairs (w, t) and (w?,
?
t) (line
4 ? line 6). (w, t) denotes the word-tag pair that
has the highest model score among those that are
inconsistent with the gold standard, while (w?,
?
t)
denotes the one that has the highest model score
among those that are consistent with the gold stan-
dard. If the loss is zero, the algorithm continues to
process the next untagged word. Otherwise, pa-
rameters are updated using back-propagation.
The standard back-propagation algorithm
147
(Rumelhart et al, 1988) cannot be applied
directly. This is because the standard loss is
calculated based on a unique input vector. This
condition does not hold in our case, because w?
and w may refer to different words, which means
that the margin loss in line 6 of Algorithm 2 is
calculated based on two different input vectors,
denoted by ?w?? and ?w?, respectively.
We solve this problem by decomposing the mar-
gin loss in line 6 into two parts:
? 1 + nn(w, t), which is associated with ?w?;
? ?nn(w?,
?
t), which is associated with ?w??.
In this way, two separate back-propagation up-
dates can be used to update the model?s parameters
(line 8 ? line 11). For the special case where w?
and w do refer to the same word w, it can be easily
verified that the two separate back-propagation up-
dates equal to the standard back-propagation with
a loss 1 + nn(w, t)? nn(w,
?
t) on the input ?w?.
The algorithm proposed here belongs to a gen-
eral framework named guided learning, where
search and learning interact with each other. The
algorithm learns not only a local classifier, but also
the inference order. While previous work (Shen et
al., 2007; Zhang and Clark, 2011; Goldberg and
Elhadad, 2010) apply guided learning to train a
linear classifier by using variants of the percep-
tron algorithm, we are the first to combine guided
learning with a neural network, by using a margin
loss and a modified back-propagation algorithm.
5 Experiments
5.1 Setup
Our experiments are conducted on the data set
provided by the SANCL 2012 shared task, which
aims at building a single robust syntactic anal-
ysis system across the web-domain. The data
set consists of labelled data for both the source
(Wall Street Journal portion of the Penn Treebank)
and target (web) domains. The web domain data
can be further classified into five sub-domains, in-
cluding emails, weblogs, business reviews, news
groups and Yahoo!Answers. While emails and
weblogs are used as the development sets, reviews,
news groups and Yahoo!Answers are used as the
final test sets. Participants are not allowed to use
web-domain labelled data for training. In addi-
tion to labelled data, a large amount of unlabelled
data on the web domain is also provided. Statistics
Algorithm 2 Training over one sentence
Input: (x, t) a tagged sentence, neural net nn
Output: updated neural net nn
?
1: U? [w
1
, . . . , w
m
] // untagged words
2: R? [(w
1
, t
1
), . . . , (w
m
, t
m
)] // reference
3: while U 6= [] do
4: (w, t)? arg max
(w,t)?(U?T/R)
nn(w, t)
5: (w?,
?
t)? arg max
(w,t)?R
nn(w, t)
6: loss? max(0, 1 + nn(w, t)? nn(w?,
?
t))
7: if loss > 0 then
8: e?? nn.BackPropErr(?w??,?nn(w?,
?
t))
9: e? nn.BackPropErr(?w?, 1+nn(w, t))
10: nn.Update(?w??, e?)
11: nn.Update(?w?, e)
12: else
13: U? U/{w?}, R? R/(w?,
?
t)
14: end if
15: end while
16: return nn
about labelled and unlabelled data are summarized
in Table 1 and Table 2, respectively.
The raw web domain data contains much noise,
including spelling error, emotions and inconsis-
tent capitalization. Following some participants
(Le Roux et al, 2012), we conduct simple prepro-
cessing steps to the input of the development and
the test sets
1
? Neutral quotes are transformed to opening or
closing quotes.
? Tokens starting with ?www.?, ?http.? or end-
ing with ?.org?, ?.com? are converted to a
?#URL? symbol
? Repeated punctuations such as ?!!!!? are col-
lapsed into one.
? Left brackets such as ?<?,?{? and ?[? are
converted to ?-LRB-?. Similarly, right brack-
ets are converted to ?-RRB-?
? Upper cased words that contain more than 4
letters are lowercased.
? Consecutive occurrences of one or more dig-
its within a word are replaced with ?#DIG?
We apply the same preprocessing steps to all the
unlabelled data. In addition, following Dahl et
1
The preprocessing steps make use of no POS knowledge,
and does not bring any unfair advantages to the participants.
148
Training set Dev set Test set
WSJ-Train Emails Weblogs WSJ-dev Answers Newsgroups Reviews WSJ-test
#Sen 30060 2,450 1,016 1,336 1,744 1,195 1,906 1,640
#Words 731,678 29,131 24,025 32,092 28,823 20,651 28,086 35,590
#Types 35,933 5,478 4,747 5,889 4,370 4,924 4,797 6,685
Table 1: Statistics of the labelled data. #Sen denotes number of sentences. #Words and #Types denote
number of words and unique word types, respectively.
Emails Weblogs Answers Newsgroups Reviews
#Sen 1,194,173 524,834 27,274 1,000,000 1,965,350
#Words 17,047,731 10,365,284 424,299 18,424,657 29,289,169
#Types 221,576 166,515 33,325 357,090 287,575
Table 2: Statistics of the raw unlabelled data.
features templates
unigram H(w
i
), C(w
i
), L(w
i
), L(w
i?1
), L(w
i+1
), t
i?2
, t
i?1
, t
i+1
, t
i+2
bigram L(w
i
) L(w
i?1
), L(w
i
) L(w
i+1
), t
i?2
 t
i?1
, t
i?1
 t
i+1
, t
i+1
 t
i+2
,
L(w
i
) t
i?2
, L(w
i
) t
i?1
, L(w
i
) t
i+1
, L(w
i
) t
i+2
trigram L(w
i
) t
i?2
 t
i?1
, L(w
i
) t
i?1
 t
i+1
, L(w
i
) t
i+1
 t
i+2
Table 3: Feature templates, where w
i
denotes the current word. H(w) and C(w) indicates whether w
contains hyphen and upper case letters, respectively. L(w) denotes a lowercased w.
al. (2012) and Turian et al (2010), we also low-
ercased all the unlabelled data and removed those
sentences that contain less than 90% a-z letters.
The tagging performance is evaluated accord-
ing to the official evaluation metrics of SANCL
2012. The tagging accuracy is defined as the per-
centage of words (punctuations included) that are
correctly tagged. The averaged accuracies are cal-
culated across the web domain data.
We trained the WRRBM on web-domain data
of different sizes (number of sentences). The data
sets are generated by first concatenating all the
cleaned unlabelled data, then selecting sentences
evenly across the concatenated file.
For each data set, we investigate an extensive set
of combinations of hyper-parameters: the n-gram
window (l, r) in {(1, 1), (2, 1), (1, 2), (2, 2)}; the
hidden layer size in {200, 300, 400}; the learning
rate in {0.1, 0.01, 0.001}. All these parameters are
selected according to the averaged accuracy on the
development set.
5.2 Baseline
We reimplemented the greedy easy-first POS tag-
ger of Ma et al (2013), which is used for all the
experiments. While the tagger of Ma et al (2013)
utilizes a linear scorer, our tagger adopts the neural
network as its scorer. The neural network of our
baseline tagger only contains the sparse-feature
module. We use this baseline to examine the per-
formance of a tagger trained purely on the source
domain. Feature templates are shown in Table 3,
which are based on those of Ratnaparkhi (1996)
and Shen et al (2007).
Accuracies of the baseline tagger are shown in
the upper part of Table 6. Compared with the
performance of the official baseline (row 4 of Ta-
ble 6), which is evaluated based on the output of
BerkeleyParser (Petrov et al, 2006; Petrov and
Klein, 2007), our baseline tagger achieves com-
parable accuracies on both the source and target
domain data. With data preprocessing, the aver-
age accuracy boosts to about 92.02 on the test set
of the target domain. This is consistent with pre-
vious work (Le Roux et al, 2011), which found
that for noisy data such as web domain text, data
cleaning is a effective and necessary step.
5.3 Exploring the Learned Knowledge
As mentioned in Section 3.1, the knowledge
learned from the WRRBM can be investigated
incrementally, using word representation, which
corresponds to initializing only the projection
layer of web-feature module with the projection
matrix of the learned WRRBM, or ngram-level
representation, which corresponds to initializing
both the projection and sigmoid layers of the web-
feature module by the learned WRRBM. In each
case, there can be two different training strate-
gies depending on whether the learned representa-
tions are further adjusted or kept unchanged dur-
ing the fine-turning phrase. Experimental results
under the 4 combined settings on the development
sets are illustrated in Figure 2, 3 and 4, where the
149
96.5
96.6
96.7
96.8
96.9
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
WSJ
word-fixedword-adjustngram-fixedngram-adjust
Figure 2: Tagging accuracies on the source-
domain data. ?word? and ?ngram? denote using
word representations and n-gram representations,
respectively. ?fixed? and ?adjust? denote that the
learned representation are kept unchanged or fur-
ther adjusted in supervised learning, respectively.
89.8
90
90.2
90.4
90.6
90.8
91
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
Email
word-fixedword-adjustngram-fixedngram-adjust
Figure 3: Accuracies on the email domain.
94.8
95
95.2
95.4
95.6
95.8
200 400 600 800 1000
Acc
ura
cy
Number of unlabelled sentences (k)
Weblog
word-fixedword-adjustngram-fixedngram-adjust
Figure 4: Accuracies on the weblog domain.
x-axis denotes the size of the training data and y-
axis denotes tagging accuracy.
5.3.1 Effect of the Training Strategy
From Figure 2 we can see that when knowl-
edge from the pre-trained WRRBM is incorpo-
method all non-oov oov
baseline 89.81 92.42 65.64
word-adjust +0.09 ?0.05 +1.38
word-fix +0.11 +0.13 +1.73
ngram-adjust +0.53 +0.52 +0.53
ngram-fix +0.69 +0.60 +2.30
Table 4: Performance on the email domain.
rated, both the training strategies (?word-fixed?
vs ?word-adjusted?, ?ngram-fixed? vs ?ngram-
adjusted?) improve accuracies on the source do-
main, which is consistent with previous findings
(Turian et al, 2010; Collobert et al, 2011). In
addition, adjusting the learned representation or
keeping them fixed does not result in too much dif-
ference in tagging accuracies.
On the web-domain data, shown in Figure 3 and
4, we found that leaving the learned representation
unchanged (?word-fixed?, ?ngram-fixed?) yields
consistently higher performance gains. This re-
sult is to some degree expected. Intuitively, unsu-
pervised pre-training moves the parameters of the
WRRBM towards the region where properties of
the web domain data are properly modelled. How-
ever, since fine-tuning is conducted with respect
to the source domain, adjusting the parameters
of the pre-trained representation towards optimiz-
ing source domain tagging accuracies would dis-
rupt its ability in modelling the web domain data.
Therefore, a better idea is to keep the representa-
tion unchanged so that we can learn a function that
maps the general web-text properties to its syntac-
tic categories.
5.3.2 Word and N-gram Representation
From Figures 2, 3 and 4, we can see that
adopting the ngram-level representation consis-
tently achieves better performance compared with
using word representations only (?word-fixed?
vs ?ngram-fixed?, ?word-adjusted? vs ?ngram-
adjusted?). This result illustrates that the ngram-
level knowledge captures more complex interac-
tions of the web text, which cannot be recovered
by using only word embeddings. Similar result
was reported by Dahl et al (2012), who found
that using both the word embeddings and the hid-
den units of a tri-gram WRRBM as additional fea-
tures for a CRF chunker yields larger improve-
ments than using word embeddings only.
Finally, more detailed accuracies under the 4
settings on the email domain are shown in Table
4. We can see that the improvement of using word
150
RBM-E RBM-W RBM-M
+acc%
Emails +0.73 +0.37 +0.69
Weblog +0.31 +0.52 +0.54
cov%
Emails 95.24 92.79 93.88
Weblog 90.21 97.74 94.77
Table 5: Effect of unlabelled data. ?+acc? denotes
improvement in tagging accuracy and ?cov? de-
notes the lexicon coverages.
representations mainly comes from better accu-
racy of out-of-vocabulary (oov) words. By con-
trast, using n-gram representations improves the
performance on both oov and non-oov.
5.4 Effect of Unlabelled Domain Data
In some circumstances, we may know beforehand
that the target domain data belongs to a certain
sub-domain, such as the email domain. In such
cases, it might be desirable to train WRRBM using
data only on that domain. We conduct experiments
to test whether using the target domain data to
train the WRRBM yields better performance com-
pared with using mixed data from all sub-domains.
We trained 3 WRRBMs using the email do-
main data (RBM-E), weblog domain data (RBM-
W) and mixed domain data (RBM-M), respec-
tively, with each data set consisting of 300k sen-
tences. Tagging performance and lexicon cover-
ages of each data set on the development sets are
shown in Table 5. We can see that using the target
domain data achieves similar improvements com-
pared with using the mixed data. However, for the
email domain, RBM-W yields much smaller im-
provement compared with RBM-E, and vice versa.
From the lexicon coverages, we can see that the
sub-domains varies significantly. The results sug-
gest that using mixed data can achieve almost as
good performance as using the target sub-domain
data, while using mixed data yields a much more
robust tagger across all sub-domains.
5.5 Final Results
The best result achieved by using a 4-gram WR-
RBM, (w
i?2
, . . . , w
i+1
), with 300 hidden units
learned on 1,000k web domain sentences are
shown in row 3 of Table 6. Performance of the
top 2 systems of the SANCL 2012 task are also
shown in Table 6. Our greedy tagger achieves 93%
tagging accuracy, which is significantly better than
the baseline?s 92.02% accuracy (p < 0.05 by Mc-
Nemar?s test). Moreover, we achieve the high-
est tagging accuracy reported so far on this data
set, surpassing those achieved using parser combi-
nations based on self-training (Tang et al, 2012;
Le Roux et al, 2012). In addition, different from
Le Roux et al (2012), we do not use any external
resources in data cleaning.
6 Related Work
Learning representations has been intensively
studied in computer vision tasks (Bengio et al,
2007; Lee et al, 2009a). In NLP, there is also
much work along this line. In particular, Col-
lobert et al (2011) and Turian et al (2010) learn
word embeddings to improve the performance of
in-domain POS tagging, named entity recogni-
tion, chunking and semantic role labelling. Yang
et al (2013) induce bi-lingual word embeddings
for word alignment. Zheng et al (2013) investi-
gate Chinese character embeddings for joint word
segmentation and POS tagging. While those ap-
proaches mainly explore token-level representa-
tions (word or character embeddings), using WR-
RBM is able to utilize both word and n-gram rep-
resentations.
Titov (2011) and Glorot et al (2011) propose
to learn representations from the mixture of both
source and target domain unlabelled data to im-
prove cross-domain sentiment classification. Titov
(2011) also propose a regularizer to constrain the
inter-domain variability. In particular, their reg-
ularizer aims to minimize the Kullback-Leibler
(KL) distance between the marginal distributions
of the learned representations on the source and
target domains.
Their work differs from ours in that their ap-
proaches learn representations from the feature
vectors for sentiment classification, which might
be of thousands of dimensions. Such high di-
mensional input gives rise to high computational
cost and it is not clear whether those approaches
can be applied to large scale unlabelled data, with
hundreds of millions of training examples. Our
method learns representations from only word n-
grams with n ranging from 3 to 5, which can
be easily applied to large scale-data. In addition,
while Titov (2011) and Glorot et al (2011) use the
learned representation to improve cross-domain
classification tasks, we are the first to apply it to
cross-domain structured prediction.
Blitzer et al (2006) propose to induce shared
representations for domain adaptation, which is
based on the alternating structure optimization
151
System Answer Newsgroup Review WSJ-t Avg
baseline-raw 89.79 91.36 89.96 97.09 90.31
baseline-clean 91.35 92.06 92.92 97.09 92.02
best-clean 92.37 93.59 93.62 97.44 93.15
baseline-offical 90.20 91.24 89.33 97.08 90.26
Le Roux et al(2011) 91.79 93.81 93.11 97.29 92.90
Tang et al (2012) 91.76 92.91 91.94 97.49 92.20
Table 6: Main results. ?baseline-raw? and ?baseline-clean? denote performance of our baseline tagger
on the raw and cleaned data, respectively. ?best-clean? is best performance achieved using a 4-gram
WRRBM. The lower part shows accuracies of the official baseline and that of the top 2 participants.
(ASO) method of Ando and Zhang (2005). The
idea is to project the original feature representa-
tions into low dimensional representations, which
yields a high-accuracy classifier on the target do-
main. The new representations are induced based
on the auxiliary tasks defined on unlabelled data
together with a dimensionality reduction tech-
nique. Such auxiliary tasks can be specific to the
supervised task. As pointed out by Plank (2009),
for many NLP tasks, defining the auxiliary tasks is
a non-trivial engineering problem. Compared with
Blitzer et al (2006), the advantage of using RBMs
is that it learns representations in a pure unsuper-
vised manner, which is much simpler.
Besides learning representations, another line
of research addresses domain-adaptation by in-
stance re-weighting (Bickel et al, 2007; Jiang
and Zhai, 2007) or feature re-weighting (Satpal
and Sarawagi, 2007). Those methods assume that
each example x that has a non-zero probability on
the source domain must have a non-zero proba-
bility on the target domain, and vice-versa. As
pointed out by Titov (2011), such an assumption
is likely to be too restrictive since most NLP tasks
adopt word-based or lexicon-based features that
vary significantly across different domains.
Regarding using neural networks for sequential
labelling, our approach shares similarity with that
of Collobert et al (2011). In particular, we both
use a non-linear layer to model complex relations
underling word embeddings. However, our net-
work differs from theirs in the following aspects.
Collobert et al (2011) model the dependency be-
tween neighbouring tags in a generative manner,
by employing a transition score A
ij
. Training the
score involves a forward process of complexity
O(nT
2
), where T denotes the number of tags. Our
model captures such a dependency in a discrimina-
tive manner, by just adding tag-related features to
the sparse-feature module. In addition, Collobert
et al (2011) train their network by maximizing the
training set likelihood, while our approach is to
minimize the margin loss using guided learning.
7 Conclusion
We built a web-domain POS tagger using a
two-phase approach. We used a WRRBM to
learn the representation of the web text and
incorporate the representation in a neural net-
work, which is trained using guided learning
for easy-first POS tagging. Experiment showed
that our approach achieved significant improve-
ment in tagging the web domain text. In ad-
dition, we found that keeping the learned repre-
sentations unchanged yields better performance
compared with further optimizing them on the
source domain data. We release our tools at
https://github.com/majineu/TWeb.
For future work, we would like to investigate
the two-phase approach to more challenging tasks,
such as web domain syntactic parsing. We be-
lieve that high-accuracy web domain taggers and
parsers would benefit a wide range of downstream
tasks such as machine translation
2
.
8 Acknowledgements
We would like to thank Hugo Larochelle for his
advices on re-implementing WRRBM. We also
thank Nan Yang, Shujie Liu and Tong Xiao for
the fruitful discussions, and three anonymous re-
viewers for their insightful suggestions. This re-
search was supported by the National Science
Foundation of China (61272376; 61300097), the
research grant T2MOE1301 from Singapore Min-
istry of Education (MOE) and the start-up grant
SRG ISTD2012038 from SUTD.
References
Rie Ando and Tong Zhang. 2005. A high-performance
semi-supervised learning method for text chunk-
2
This work is done while the first author is visiting SUTD.
152
ing. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 1?9, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise train-
ing of deep networks. In B. Sch?olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Informa-
tion Processing Systems 19, pages 153?160. MIT
Press, Cambridge, MA.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1?127. Also published as a book. Now Pub-
lishers, 2009.
Steffen Bickel, Michael Brckner, and Tobias Scheffer.
2007. Discriminative learning for differing training
and test distributions. In Proc of ICML 2007, pages
81?88. ACM Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128, Sydney, Australia, July.
Association for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
George E. Dahl, Ryan P. Adams, and Hugo Larochelle.
2012. Training restricted boltzmann machines on
word observations. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ?12, pages 679?686, New York, NY, USA,
July. Omnipress.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc of
ICML 2011, pages 513?520.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527?1554, July.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Comput., 14(8):1771?1800, August.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 264?271,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 Systems for the SANCL 2012 Shared
Task. In Proceedings of the NAACL 2012 First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL), pages 1?4, Montr?eal, Canada,
June.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and
Andrew Y. Ng. 2009a. Convolutional deep belief
networks for scalable unsupervised learning of hi-
erarchical representations. In Proc of ICML 2009,
pages 609?616.
Honglak Lee, Peter Pham, Yan Largman, and Andrew
Ng. 2009b. Unsupervised feature learning for audio
classification using convolutional deep belief net-
works. In Y. Bengio, D. Schuurmans, J. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances
in Neural Information Processing Systems 22, pages
1096?1104.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-first pos tagging and dependency parsing with
beam search. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 110?114,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. Notes of
the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Barbara Plank. 2009. Structural correspondence learn-
ing for parse disambiguation. In Alex Lascarides,
153
Claire Gardent, and Joakim Nivre, editors, EACL
(Student Research Workshop), pages 37?45. The As-
sociation for Computer Linguistics.
Marc?Aurelio Ranzato, Christopher Poultney, Sumit
Chopra, and Yann LeCun. 2007. Efficient learn-
ing of sparse representations with an energy-based
model. In B. Sch?olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Process-
ing Systems 19, pages 1137?1144. MIT Press, Cam-
bridge, MA.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: Foundations
of research. chapter Learning Representations
by Back-propagating Errors, pages 696?699. MIT
Press, Cambridge, MA, USA.
Sandeepkumar Satpal and Sunita Sarawagi. 2007. Do-
main adaptation of conditional probability models
via feature subsetting. In PKDD, volume 4702 of
Lecture Notes in Computer Science, pages 224?235.
Springer.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Buzhou Tang, Min Jiang, and Hua Xu. 2012.
Varderlibt?s systems for sancl2012 shared task. In
Proceedings of the NAACL 2012 First Workshop
on Syntactic Analysis of Non-Canonical Language
(SANCL), Montr?eal, Canada, June.
Ivan Titov. 2011. Domain adaptation by constraining
inter-domain variability of latent feature representa-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 62?71, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence la-
beling. In Proceedings of the 6th International Joint
Conference on Natural Language Processing (IJC-
NLP).
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 166?175, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntax-based
grammaticality improvement using ccg and guided
search. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1147?1157, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmenta-
tion and POS tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 647?657, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
154
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 563?568,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Hybrid Approach to Skeleton-based Translation
Tong Xiao??, Jingbo Zhu??, Chunliang Zhang??
? Northeastern University, Shenyang 110819, China
? Hangzhou YaTuo Company, 358 Wener Rd., Hangzhou 310012, China
{xiaotong,zhujingbo,zhangcl}@mail.neu.edu.cn
Abstract
In this paper we explicitly consider sen-
tence skeleton information for Machine
Translation (MT). The basic idea is that
we translate the key elements of the input
sentence using a skeleton translation mod-
el, and then cover the remain segments us-
ing a full translation model. We apply our
approach to a state-of-the-art phrase-based
system and demonstrate very promising
BLEU improvements and TER reductions
on the NIST Chinese-English MT evalua-
tion data.
1 Introduction
Current Statistical Machine Translation (SMT) ap-
proaches model the translation problem as a pro-
cess of generating a derivation of atomic transla-
tion units, assuming that every unit is drawn out
of the same model. The simplest of these is the
phrase-based approach (Och et al, 1999; Koehn
et al, 2003) which employs a global model to
process any sub-strings of the input sentence. In
this way, all we need is to increasingly translate
a sequence of source words each time until the
entire sentence is covered. Despite good result-
s in many tasks, such a method ignores the roles
of each source word and is somewhat differen-
t from the way used by translators. For exam-
ple, an important-first strategy is generally adopt-
ed in human translation - we translate the key ele-
ments/structures (or skeleton) of the sentence first,
and then translate the remaining parts. This es-
pecially makes sense for some languages, such as
Chinese, where complex structures are usually in-
volved.
Note that the source-language structural infor-
mation has been intensively investigated in recent
studies of syntactic translation models. Some of
them developed syntax-based models on complete
syntactic trees with Treebank annotations (Liu et
al., 2006; Huang et al, 2006; Zhang et al, 2008),
and others used source-language syntax as soft
constraints (Marton and Resnik, 2008; Chiang,
2010). However, these approaches suffer from
the same problem as the phrase-based counterpart
and use the single global model to handle differ-
ent translation units, no matter they are from the
skeleton of the input tree/sentence or other not-so-
important sub-structures.
In this paper we instead explicitly model the
translation problem with sentence skeleton infor-
mation. In particular,
? We develop a skeleton-based model which
divides translation into two sub-models: a
skeleton translation model (i.e., translating
the key elements) and a full translation model
(i.e., translating the remaining source words
and generating the complete translation).
? We develop a skeletal language model to de-
scribe the possibility of translation skeleton
and handle some of the long-distance word
dependencies.
? We apply the proposed model to Chinese-
English phrase-based MT and demonstrate
promising BLEU improvements and TER re-
ductions on the NIST evaluation data.
2 A Skeleton-based Approach to MT
2.1 Skeleton Identification
The first issue that arises is how to identify the
skeleton for a given source sentence. Many ways
are available. E.g., we can start with a full syntac-
tic tree and transform it into a simpler form (e.g.,
removing a sub-tree). Here we choose a simple
and straightforward method: a skeleton is obtained
by dropping all unimportant words in the origi-
nal sentence, while preserving the grammaticali-
ty. See the following for an example skeleton of a
Chinese sentence.
563
Original Sentence (subscripts represent indices):
z
[1]
per
?
[2]
ton
?Yz
[3]
seawater desalination
?n
[4]
treatment

[5]
of
?
[6]
the cost
3
[7]
5
[8]
5

[9]
yuan

[10]
of
?:
[11]
from
?
[12]
???
[13]
has been further
e?
[14]
reduced
"
[15]
.
(The cost of seawater desalination treatment has
been further reduced from 5 yuan per ton.)
Sentence Skeleton (subscripts represent indices):
?
[6]
the cost
???
[13]
has been further
e?
[14]
reduced
"
[15]
.
(The cost has been further reduced.)
Obviously the skeleton used in this work can be
viewed as a simplified sentence. Thus the prob-
lem is in principle the same as sentence simpli-
fication/compression. The motivations of defin-
ing the problem in this way are two-fold. First,
as the skeleton is a well-formed (but simple) sen-
tence, all current MT approaches are applicable
to the skeleton translation problem. Second, ob-
taining simplified sentences by word deletion is
a well-studied issue (Knight and Marcu, 2000;
Clarke and Lapata, 2006; Galley and McKeown,
2007; Cohn and Lapata, 2008; Yamangil and
Shieber, 2010; Yoshikawa et al, 2012). Many
good sentence simpliciation/compression methods
are available to our work. Due to the lack of space,
we do not go deep into this problem. In Section
3.1 we describe the corpus and system employed
for automatic generation of sentence skeletons.
2.2 Base Model
Next we describe our approach to integrating
skeleton information into MT models. We start
with an assumption that the 1-best skeleton is pro-
vided by the skeleton identification system. Then
we define skeleton-based translation as a task of
searching for the best target string
?
t given the
source string and its skeleton ? :
?
t = argmax
t
P(t|?, s) (1)
As is standard in SMT, we further assume that
1) the translation process can be decomposed in-
to a derivation of phrase-pairs (for phrase-based
models) or translation rules (for syntax-based
models); 2) and a linear function g(?) is used to
assign a model score to each derivation. Let d
s,?,t
(or d for short) denote a translation derivation. The
above problem can be redefined in a Viterbi fash-
ion - we find the derivation
?
dwith the highest mod-
el score given s and ? :
?
d = argmax
d
g(d) (2)
In this way, the MT output can be regarded as the
target-string encoded in
?
d.
To compute g(d), we use a linear combination
of a skeleton translation model g
skel
(d) and a full
translation model g
full
(d):
g(d) = g
skel
(d) + g
full
(d) (3)
where the skeleton translation model handles the
translation of the sentence skeleton, while the full
translation model is the baseline model and han-
dles the original problem of translating the whole
sentence. The motivation here is straightforward:
we use an additional score g
skel
(d) to model the
problem of skeleton translation and interpolate it
with the baseline model. See Figure 1 for an exam-
ple of applying the above model to phrase-based
MT. In the figure, each source phrase is translated
into a target phrase, which is represented by linked
rectangles. The skeleton translation model focus-
es on the translation of the sentence skeleton, i.e.,
the solid (red) rectangles; while the full transla-
tion model computes the model score for all those
phrase-pairs, i.e., all solid and dashed rectangles.
Another note on the model. Eq. (3) provides a
very flexible way for model selection. While we
will restrict ourself to phrase-based translation in
the following description and experiments, we can
choose different models/features for g
skel
(d) and
g
full
(d). E.g., one may introduce syntactic fea-
tures into g
skel
(d) due to their good ability in cap-
turing structural information; and employ a stan-
dard phrase-based model for g
full
(d) in which not
all segments of the sentence need to respect syn-
tactic constraints.
2.3 Model Score Computation
In this work both the skeleton translation model
g
skel
(d) and full translation model g
full
(d) resem-
ble the usual forms used in phrase-based MT, i.e.,
the model score is computed by a linear combina-
tion of a group of phrase-based features and lan-
guage models. In phrase-based MT, the transla-
tion problem is modeled by a derivation of phrase-
pairs. Given a translation model m, a language
model lm and a vector of feature weights w, the
model score of a derivation d is computed by
564
z? ?Yz ?n  ? 3 5   ?: ? ??? e? "
the cost
p
h
r
a
s
e
1
p
1
Skeleton:
Full:
g(d
?
;w
?
,m, lm
?
) = w
?
m
? f
m
(p
1
) + w
?
lm
? lm
?
(?the cost?)
g(d;w,m, lm) = w
m
? f
m
(p
1
) + w
lm
? lm(?the cost?)
z? ?Yz ?n  ? 3 5   ?: ? ??? e? "
the cost of seawater desalination treatment
p
h
r
a
s
e
s
2
&
3
p
1
p
2
p
3
Skeleton:
Full:
g(d
?
;w
?
,m, lm
?
) = w
?
m
? f
m
(p
1
) + w
?
lm
? lm
?
(?the cost X?)
g(d;w,m, lm) = w
m
? f
m
(p
1
? p
2
? p
3
) + w
lm
? lm(?the cost of seawater desalination treatment?)
z? ?Yz ?n  ? 3 5   ?: ? ??? e? "
the cost of seawater desalination treatment has been further reduced
p
h
r
a
s
e
s
4
&
5
p
1
p
2
p
3
p
4
p
5
Skeleton:
Full:
g(d
?
;w
?
,m, lm
?
) = w
?
m
? f
m
(p
1
? p
4
? p
5
)+
w
?
lm
? lm
?
(?the cost X has been further reduced?)
g(d;w,m, lm) = w
m
? f
m
(p
1
? p
2
? ... ? p
5
) + w
lm
? lm(?the cost of seawater ... further reduced?)
z? ?Yz ?n  ? 3 5   ?: ? ??? e? "
the cost of seawater desalination treatment has been further reduced from
5 yuan
per ton
.
p
h
r
a
s
e
s
6
-
9
p
1
p
2
p
3
p
4
p
5
p
6
p
7
p
8
p
9
Skeleton:
Full:
g(d
?
;w
?
,m, lm
?
) = w
?
m
? f
m
(p
1
? p
4
? p
5
? p
9
)+
w
?
lm
? lm
?
(?the cost X has been further reduced X .?)
g(d;w,m, lm) = w
m
? f
m
(p
1
? p
2
? ... ? p
9
) + w
lm
? lm(?the cost of seawater ... per ton .?)
Figure 1: Example derivation and model scores for a sentence in LDC2006E38. The solid (red) rect-
angles represent the sentence skeleton, and the dashed (blue) rectangles represent the non-skeleton seg-
ments. X represents a slot in the translation skeleton. ? represents composition of phrase-pairs.
g(d;w,m, lm) = w
m
? f
m
(d)+w
lm
? lm(d) (4)
where f
m
(d) is a vector of feature values defined
on d, and w
m
is the corresponding weight vector.
lm(d) andw
lm
are the score and weight of the lan-
guage model, respectively.
To ease modeling, we only consider skeleton-
consistent derivations in this work. A deriva-
tion d is skeleton-consistent if no phrases in d
cross skeleton boundaries (e.g., a phrase where t-
wo of the source words are in the skeleton and
one is outside). Obviously, from any skeleton-
consistent derivation d we can extract a skeleton
derivation d
?
which covers the sentence skeleton
exactly. For example, in Figure 1, the deriva-
tion of phrase-pairs {p
1
, p
2
, ..., p
9
} is skeleton-
consistent, and the skeleton derivation is formed
by {p
1
, p
4
, p
5
, p
9
}.
Then, we can simply define g
skel
(d) and
g
full
(d) as the model scores of d
?
and d:
g
skel
(d) , g(d
?
;w
?
,m, lm
?
) (5)
g
full
(d) , g(d;w,m, lm) (6)
This model makes the skeleton translation and
full translation much simpler because they per-
form in the same way of string translation in
phrase-based MT. Both g
skel
(d) and g
full
(d) share
the same translation model m which can easily
learned from the bilingual data
1
. On the other
hand, it has different feature weight vectors for in-
dividual models (i.e., w and w
?
).
For language modeling, lm is the standard n-
gram language model adopted in the baseline sys-
tem. lm
?
is a skeletal language for estimating the
well-formedness of the translation skeleton. Here
a translation skeleton is a target string where all
segments of non-skeleton translation are general-
ized to a symbol X. E.g., in Figure 1, the trans-
1
In g
skel
(d), we compute the reordering model score on
the skeleton though it is learned from the full sentences. In
this way the reordering problems in skeleton translation and
full translation are distinguished and handled separately.
565
lation skeleton is ?the cost X has been further re-
duced X .?, where two Xs represent non-skeleton
segments in the translation. In such a way of string
representation, the skeletal language model can be
implemented as a standard n-gram language mod-
el, that is, a string probability is calculated by a
product of a sequence of n-gram probabilities (in-
volving normal words and X). To learn the skele-
tal language model, we replace non-skeleton parts
of the target sentences in the bilingual corpus to
Xs using the source sentence skeletons and word
alignments. The skeletal language model is then
trained on these generalized strings in a standard
way of n-gram language modeling.
By substituting Eq. (4) into Eqs. (5) and (6),
and then Eqs. (3) and (2), we have the final model
used in this work:
?
d = argmax
d
(
w
m
? f
m
(d) + w
lm
? lm(d) +
w
?
m
? f
m
(d
?
) + w
?
lm
? lm
?
(d
?
)
)
(7)
Figure 1 shows the translation process and as-
sociated model scores for the example sentence.
Note that this method does not require any new
translation models for implementation. Given a
baseline phrase-based system, all we need is to
learn the feature weights w and w
?
on the devel-
opment set (with source-language skeleton anno-
tation) and the skeletal language model lm
?
on
the target-language side of the bilingual corpus.
To implement Eq. (7), we can perform standard
decoding while ?doubly weighting? the phrases
which cover a skeletal section of the sentence, and
combining the two language models and the trans-
lation model in a linear fashion.
3 Evaluation
3.1 Experimental Setup
We experimented with our approach on Chinese-
English translation using the NiuTrans open-
source MT toolkit (Xiao et al, 2012). Our bilin-
gual corpus consists of 2.7M sentence pairs. Al-
l these sentences were aligned in word level us-
ing the GIZA++ system and the ?grow-diag-final-
and? heuristics. A 5-gram language model was
trained on the Xinhua portion of the English Gi-
gaword corpus in addition to the target-side of the
bilingual data. This language model was used
in both the baseline and our improved system-
s. For our skeletal language model, we trained a
5-gram language model on the target-side of the
bilingual data by generalizing non-skeleton seg-
ments to Xs. We used the newswire portion of the
NIST MT06 evaluation data as our developmen-
t set, and used the evaluation data of MT04 and
MT05 as our test sets. We chose the default fea-
ture set of the NiuTrans.Phrase engine for building
the baseline, including phrase translation proba-
bilities, lexical weights, a 5-gram language mod-
el, word and phrase bonuses, a ME-based lexical-
ized reordering model. All feature weights were
learned using minimum error rate training (Och,
2003).
Our skeleton identification system was built
using the t3 toolkit
2
which implements a state-
of-the-art sentence simplification system. We
used the NEU Chinese sentence simplification
(NEUCSS) corpus as our training data (Zhang
et al, 2013). It contains the annotation of sen-
tence skeleton on the Chinese-language side of
the Penn Parallel Chinese-English Treebank (LD-
C2003E07). We trained our system using the Parts
1-8 of the NEUCSS corpus and obtained a 65.2%
relational F1 score and 63.1% compression rate in
held-out test (Part 10). For comparison, we also
manually annotated the MT development and test
data with skeleton information according to the
annotation standard provided within NEUCSS.
3.2 Results
Table 1 shows the case-insensitive IBM-version
BLEU and TER scores of different systems. We
see, first of all, that the MT system benefits from
our approach in most cases. In both the manual
and automatic identification of sentence skeleton
(rows 2 and 4), there is a significant improvemen-
t on the ?All? data set. However, using different
skeleton identification results for training and in-
ference (row 3) does not show big improvements
due to the data inconsistency problem.
Another interesting question is whether the
skeletal language model really contributes to the
improvements. To investigate it, we removed the
skeletal language model from our skeleton-based
translation system (with automatic skeleton iden-
tification on both the development and test sets).
Seen from row ?lm
?
of Table 1, the removal of
the skeletal language model results in a significan-
t drop in both BLEU and TER performance. It
indicates that this language model is very benefi-
cial to our system. For comparison, we removed
2
http://staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/
566
Entry MT06 (Dev) MT04 MT05 All
system dev-skel test-skel BLEU TER BLEU TER BLEU TER BLEU TER
baseline - - 35.06 60.54 38.53 61.15 34.32 62.82 36.64 61.54
SBMT manual manual 35.71 59.60 38.99 60.67 35.35 61.60 37.30 60.73
SBMT manual auto 35.72 59.62 38.75 61.16 35.02 62.20 37.03 61.19
SBMT auto auto 35.57 59.66 39.21 60.59 35.29 61.89 37.33 60.80
?lm
?
auto auto 35.23 60.17 38.86 60.78 34.82 62.46 36.99 61.16
?m
?
auto auto 35.50 59.69 39.00 60.69 35.10 62.03 37.12 60.90
s-space - - 35.00 60.50 38.39 61.20 34.33 62.90 36.57 61.58
s-feat. - - 35.16 60.50 38.60 61.17 34.25 62.88 36.70 61.58
Table 1: BLEU4[%] and TER[%] scores of different systems. Boldface means a significant improvement
(p < 0.05). SBMT means our skeleton-based MT system. ?lm
?
(or ?m
?
) means that we remove the
skeletal language model (or translation model) from our proposed approach. s-space means that we
restrict the baseline system to the search space of skeleton-consistent derivations. s-feat. means that we
introduce an indicator feature for skeleton-consistent derivations into the baseline system.
the skeleton-based translation model from our sys-
tem as well. Row ?m
?
of Table 1 shows that the
skeleton-based translation model can contribute to
the overall improvement but there is no big differ-
ences between baseline and ?m
?
.
Apart from showing the effects of the skeleton-
based model, we also studied the behavior of the
MT system under the different settings of search
space. Row s-space of Table 1 shows the BLEU
and TER results of restricting the baseline sys-
tem to the space of skeleton-consistent derivation-
s, i.e., we remove both the skeleton-based trans-
lation model and language model from the SBMT
system. We see that the limited search space is a
little harmful to the baseline system. Further, we
regarded skeleton-consistent derivations as an in-
dicator feature and introduced it into the baseline
system. Seen from row s-feat., this feature does
not show promising improvements. These results
indicate that the real improvements are due to the
skeleton-based model/features used in this work,
rather than the ?well-formed? derivations.
4 Related Work
Skeleton is a concept that has been used in several
sub-areas in MT for years. For example, in confu-
sion network-based system combination it refer-
s to the backbone hypothesis for building confu-
sion networks (Rosti et al, 2007; Rosti et al,
2008); Liu et al (2011) regard skeleton as a short-
ened sentence after removing some of the function
words for better word deletion. In contrast, we de-
fine sentence skeleton as the key segments of a
sentence and develop a new MT approach based
on this information.
There are some previous studies on the use of
sentence skeleton or related information in MT
(Mellebeek et al, 2006a; Mellebeek et al, 2006b;
Owczarzak et al, 2006). In spite of their good
ideas of using skeleton skeleton information, they
did not model the skeleton-based translation prob-
lem in modern SMT pipelines. Our work is a fur-
ther step towards the use of sentence skeleton in
MT. More importantly, we develop a complete ap-
proach to this issue and show its effectiveness in a
state-of-the-art MT system.
5 Conclusion and Future Work
We have presented a simple but effective approach
to integrating the sentence skeleton information
into a phrase-based system. The experimental re-
sults show that the proposed approach achieves
very promising BLEU improvements and TER re-
ductions on the NIST evaluation data. In our fu-
ture work we plan to investigate methods of inte-
grating both syntactic models (for skeleton trans-
lation) and phrasal models (for full translation) in
our system. We also plan to study sophisticated
reordering models for skeleton translation, rather
than reusing the baseline reordering model which
is learned on the full sentences.
Acknowledgements
This work was supported in part by the Nation-
al Science Foundation of China (Grants 61272376
and 61300097), and the China Postdoctoral Sci-
ence Foundation (Grant 2013M530131). The au-
thors would like to thank the anonymous reviewers
for their pertinent and insightful comments.
567
References
David Chiang. 2010. Learning to Translate with
Source and Target Syntax. In Proc. of ACL 2010,
pages 1443-1452.
James Clarke and Mirella Lapata. 2006. Models for
Sentence Compression: A Comparison across Do-
mains, Training Requirements and Evaluation Mea-
sures. In Proc. of ACL/COLING 2006, pages 377-
384.
Trevor Cohn and Mirella Lapata. 2008. Sentence
Compression Beyond Word Deletion. In Proc. of
COLING 2008, pages 137-144.
Jason Eisner. 2003. Learning Non-Isomorphic Tree
Mappings for Machine Translation. In Proc. of ACL
2003, pages 205-208.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov Grammars for Sentence Compres-
sion. In Proc. of HLT:NAACL 2007, pages 180-187.
Liang Huang, Kevin Knight and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006, pages
66-73.
Kevin Knight and Daniel Marcu. 2000. Statistical-
based summarization-step one: sentence compres-
sion. In Proc. of AAAI 2000, pages 703-710.
Philipp Koehn, Franz J. Och and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of
NAACL 2003, pages 48-54.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proc. of ACL/COLING 2006, pages
609-616.
Shujie Liu, Chi-Ho Li and Ming Zhou. 2011. Statistic
Machine Translation Boosted with Spurious Word
Deletion. In Proc. of Machine Translation Summit
XIII, pages 72-79.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrased-Based Transla-
tion. In Proc. of ACL:HLT 2008, pages 1003-1011.
Bart Mellebeek, Karolina Owczarzak, Josef van Gen-
abith and Andy Way. 2006. Multi-Engine Machine
Translation by Recursive Sentence Decomposition.
In Proc. of AMTA 2006, pages 110-118.
Bart Mellebeek, Karolina Owczarzak, Declan Groves,
Josef Van Genabith and Andy Way. 2006. A Syn-
tactic Skeleton for Statistical Machine Translation.
In Proc. of EAMT 2006, pages 195-202.
Franz J. Och, Christoph Tillmann and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. of EMNLP/VLC
1999, pages 20-28.
Franz J. Och. 2003. Minimum error rate training in s-
tatistical machine translation. In Proc. of ACL 2003,
pages 160-167.
Karolina Owczarzak, Bart Mellebeek, Declan Groves,
Josef van Genabith and Andy Way. 2006. Wrapper
Syntax for Example-Based Machine Translation. In
Proc. of AMTA2006, pages 148-155.
Antti-Veikko I. Rosti, Spyros Matsoukas and Richard
Schwartz. 2007. Improved Word-Level System
Combination for Machine Translation. In Proc. of
ACL 2007, pages 312-319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proc. of Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li
2012. NiuTrans: An Open Source Toolkit for
Phrase-based and Syntax-based Machine Transla-
tion. In Proc. of ACL 2012, system demonstrations,
pages 19-24.
Elif Yamangil and Stuart M. Shieber. 2010. Bayesian
Synchronous Tree-Substitution Grammar Induction
and Its Application to Sentence Compression. In
Proc. of ACL 2010, pages 937-947.
Katsumasa Yoshikawa, Ryu Iida, Tsutomu Hirao and
Manabu Okumura. 2012. Sentence Compression
with Semantic Role Constraints. In Proc. of ACL
2012, pages 349-353.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008. A Tree Sequence
Alignment-based Tree-to-Tree Translation Model.
In Proc. of ACL:HLT 2008, pages 559-567.
Chunliang Zhang, Minghan Hu, Tong Xiao, Xue Jiang,
Lixin Shi and Jingbo Zhu. 2013. Chinese Sentence
Compression: Corpus and Evaluation. In Proc.
of Chinese Computational Linguistics and Natural
Language Processing Based on Naturally Annotated
Big Data, pages 257-267.
568
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 791?796,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Punctuation Processing for Projective Dependency Parsing
?
Ji Ma
?
, Yue Zhang
?
and Jingbo Zhu
?
?
?
Northeastern University, Shenyang, China
?
Singapore University of Technology and Design, Singapore
?
Hangzhou YaTuo Company, 358 Wener Rd., Hangzhou, China, 310012
majineu@gmail.com
yue zhang@sutd.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
Modern statistical dependency parsers as-
sign lexical heads to punctuations as well
as words. Punctuation parsing errors lead
to low parsing accuracy on words. In this
work, we propose an alternative approach
to addressing punctuation in dependency
parsing. Rather than assigning lexical
heads to punctuations, we treat punctu-
ations as properties of their neighbour-
ing words, used as features to guide the
parser to build the dependency graph. In-
tegrating our method with an arc-standard
parser yields a 93.06% unlabelled attach-
ment score, which is the best accuracy by
a single-model transition-based parser re-
ported so far.
1 Introduction
The task of dependency parsing is to identify the
lexical head of each of the tokens in a string.
Modern statistical parsers (McDonald et al, 2005;
Nivre et al, 2007; Huang and Sagae, 2010; Zhang
and Nivre, 2011) treat all the tokens equally, as-
signing lexical heads to punctuations as well as
words. Punctuations arguably play an important
role in syntactic analysis. However, there are a
number of reasons that it is not necessary to parse
punctuations:
First, the lexical heads of punctuations are not
as well defined as those of words. Consequently,
punctuations are not as consistently annotated in
treebanks as words, making it harder to parse
punctuations. For example, modern statistical
parsers achieve above 90% unlabelled attachment
score (UAS) on words. However, the UAS on
punctuations are generally below 85%.
?
This work was done while the first author was visiting
SUTD
Moreover, experimental results showed that
parsing accuracy of content words drops on sen-
tences which contain higher ratios of punctuations.
One reason for this result is that projective de-
pendency parsers satisfy the ?no crossing links?
constraint, and errors in punctuations may pre-
vent correct word-word dependencies from being
created (see section 2). In addition, punctuations
cause certain type of features inaccurate. Take va-
lency features for example, previous work (Zhang
and Nivre, 2011) has shown that such features are
important to parsing accuracy, e.g., it may inform
the parser that a verb already has two objects at-
tached to it. However, such information might
be inaccurate when the verb?s modifiers contain
punctuations.
Ultimately, it is the dependencies between
words that provide useful information for real
world applications. Take machine translation or
information extraction for example, most systems
take advantage of the head-modifier relationships
between word pairs rather than word-punctuation
pairs to make better predictions. The fact that most
previous work evaluates parsing accuracies with-
out taking punctuations into account is also largely
due to this reason.
Given the above reasons, we propose an alterna-
tive approach to punctuation processing for depen-
dency parsing. In this method, punctuations are
not associated with lexical heads, but are treated
as properties of their neighbouring words.
Our method is simple and can be easily incor-
porated into state-of-the-art parsers. In this work,
we report results on an arc-standard transition-
based parser. Experiments show that our method
achieves about 0.90% UAS improvement over the
greedy baseline parser on the standard Penn Tree-
bank test set. Although the improvement becomes
smaller as the beam width grows larger, we still
achieved 93.06% UAS with a beam of width 64,
which is the best result for transition-based parsers
791
Length 1 ? 20 21? 40 41? 60
Punc % 0 ? 15 15 ? 30 > 30 0 ? 15 15 ? 30 > 30 0 ? 15 15 ? 30 > 30
E-F 94.56 92.88 87.67 91.84 91.82 83.87 89.83 88.01 ?
A-S 93.87 92.00 90.05 90.81 90.15 75.00 88.06 88.89 ?
A-S-64 95.28 94.43 88.15 92.96 92.63 76.61 90.78 88.76 ?
MST 94.90 93.55 88.15 92.45 93.11 77.42 90.89 89.77 ?
Table 2: Parsing accuracies vs punctuation ratios, on the development set
System E-F A-S A-S-64 MST
Dev UAS 91.83 90.71 93.02 92.56
Test UAS 91.75 90.34 92.84 92.10
Dev UAS-p 83.20 79.69 84.80 84.42
Test UAS-p 84.67 79.64 87.80 85.67
Dev
?
UAS 90.64 89.55 91.87 90.11
Test
?
UAS 90.40 89.33 91.75 89.82
Table 1: Parsing accuracies. ?E-F? and ?MST? de-
note easy-first parser and MSTparser, respectively.
?A-S? and ?A-S 64? denote our arc-standard parser
with beam width 1 and 64, respectively. ?UAS?
and ?UAS-p? denote word and punctuation unla-
belled attachment score, respectively. ?
?
? denotes
the data set with punctuations removed.
reported so far. Our code will be available at
https://github.com/majineu/Parser/Punc/A-STD.
2 Influence of Punctuations on Parsing
In this section, we conduct a set of experiments to
show the influence of punctuations on dependency
parsing accuracies.
2.1 Setup
We use the Wall Street Journal portion of the Penn
Treebank with the standard splits: sections 02-21
are used as the training set; section 22 and sec-
tion 23 are used as the development and test set,
respectively. Penn2Malt is used to convert brack-
eted structures into dependencies. We use our own
implementation of the Part-Of-Speech (POS) tag-
ger proposed by Collins (2002) to tag the devel-
opment and test sets. Training set POS tags are
generated using 10-fold jack-knifing. Parsing ac-
curacy is evaluated using unlabelled attachment
score (UAS), which is the percentage of words that
are assigned the correct lexical heads.
To show that the influence of punctuations
on parsing is independent of specific pars-
ing algorithms, we conduct experiments us-
ing three parsers, each representing a different
parsing methodology: the open source MST-
Parser
1
(McDonald and Pereira, 2006), our own
re-implementation of an arc-standard transition-
based parser (Nivre, 2008), which is trained us-
ing global learning and beam-search (Zhang and
Clark, 2008) with a rich feature set (Zhang and
Nivre, 2011)
2
, and our own re-implementation of
the easy-first parser (Goldberg and Elhadad, 2010)
with an extended feature set (Ma et al, 2013).
2.2 Punctuations and Parsing Accuracy
Our first experiment is to show that, compared
with words, punctuations are more difficult to
parse and to learn. To see this, we evaluate the
parsing accuracies of the selected parsers on words
and punctuations, separately. Results are listed
in Table 1, where row 2 and row 3 list the UAS
of words (all excluding punctuations) on the de-
velopment and test set, respectively. Row 4 and
row 5 list accuracies of punctuations (all excluding
words) on the development and test set, respec-
tively. We can see that although all the parsers
achieve above 90% UAS on words, the UAS on
punctuations are mostly below 85%.
As for learning, we calculate the percentage of
parameter updates that are caused by associating
punctuations with incorrect heads during training
of the easy-first parser
3
. The result is that more
than 31% of the parameter updates are caused due
to punctuations, though punctuations account for
only 11.6% of the total tokens in the training set.
The fact that parsers achieve low accuracies on
punctuations is to some degree expected, because
the head of a punctuation mark is linguistically
less well-defined. However, a related problem is
1
We trained a second order labelled parser with all the
configurations set to the default value. The code is publicly
available at http://sourceforge.net/projects/mstparser/
2
Some feature templates in Zhang and Nivre (2011) in-
volve head word and head POS tags which are not avail-
able for an arc-standard parser. Interestingly, without those
features our arc-standard parser still achieves 92.84% UAS
which is comparable to the 92.90% UAS obtained by the arc-
eager parser of Zhang and Nivre (2011)
3
For the greedy easy-first parser, whether a parameter up-
date is caused by punctuation error can be determined with
no ambiguity.
792
Figure 1: Illustration of processing paired punctuation. The property of a word is denoted by the punc-
tuation below that word.
that parsing accuracy on words tends to drop on
the sentences which contain high ratio of punc-
tuations. To see this, we divide the sentences in
the development set into sub-sets according the
punctuation ratio (percentage of punctuations that
a sentence contains), and then evaluate parsing ac-
curacies on the sub-sets separately.
The results are listed in Table 2. Since long
sentences are inherently more difficult to parse,
to make a fair comparison, we further divide the
development set according to sentence lengths as
shown in the first row
4
. We can see that most of the
cases, parsing accuracies drop on sentences with
higher punctuation ratios. Note that this negative
effect on parsing accuracy might be overlooked
since most previous work evaluates parsing accu-
racy without taking punctuations into account.
By inspecting the parser outputs, we found that
error propagation caused by assigning incorrect
head to punctuations is one of the main reason that
leads to this result. Take the sentence shown in
Figure 1 (a) for example, the word Mechanisms
is a modifier of entitled according to the gold ref-
erence. However, if the quotation mark, ?, is in-
correctly recognized as a modifier of was, due to
the ?no crossing links? constraint, the arc between
Mechanisms and entitled can never be created.
A natural question is whether it is possible to
reduce such error propagation by simply remov-
ing all punctuations from parsing. Our next ex-
periment aims at answering this question. In this
experiment, we first remove all punctuations from
the original data and then modify the dependency
arcs accordingly in order to maintain word-word
dependencies in the original data. We re-train the
parsers on the modified training set and evaluate
4
1694 out of 1700 sentences on the development set with
length no larger than 60 tokens
parsing accuracies on the modified data.
Results are listed in row 6 and row 7 of Table 1.
We can see that parsing accuracies on the modified
data drop significantly compared with that on the
original data. The result indicates that by remov-
ing punctuations, we lose some information that is
important for dependency parsing.
3 Punctuation as Properties
In our method, punctuations are treated as prop-
erties of its neighbouring words. Such properties
are used as additional features to guide the parser
to construct the dependency graph.
3.1 Paired Punctuation
Our method distinguishes paired punctuations
from other punctuations. Here paired punctuations
include brackets and quotations marks, whose
Penn Treebank POS tags are the following four:
-LRB- -RRB- ? ?
The characteristics of paired punctuations include:
(1) they typically exist in pairs; (2) they serve as
boundaries that there is only one dependency arc
between the words inside the boundaries and the
words outside. Take the sentence in Figure 1 (a)
for example, the only arc cross the boundary is
(Mechanisms, entitled) where entitled is the head.
To utilize such boundary information, we fur-
ther classify paired punctuations into two cate-
gories: those that serve as the beginning of the
boundary, whose POS tags are either -LRB- or ?,
denoted by BPUNC; and those that serve as the end
of the boundary, denoted by EPUNC.
Before parsing starts, a preprocessing step is
used to first attach the paired punctuations as
properties of their neighbouring words, and then
remove them from the sentence. In particular,
793
unigram for p in ?
0
, ?
1
, ?
2
, ?
3
, ?
0
, ?
1
, ?
2
p
punc
for p in ?
0
, ?
1
, ?
2
, ?
0
, ?
1
p
punc
 p
w
, p
punc
 p
t
bigram for p, q in (?
0
, ?
0
), (?
0
, ?
1
), (?
0
, ?
2
), (?
0
, ?
1
), (?
0
, ?
2
) p
punc
 q
punc
, p
punc
 q
t
, p
punc
 q
w
for p, q in (?
2
, ?
0
), (?
1
, ?
0
), (?
2
, ?
0
) p
punc
 q
t
, p
punc
 p
t
 q
t
for p, q in (?
2
, ?
0
), (?
1
, ?
0
), (?
0
, ?
0
) d
pq
 p
punc
 p
t
 q
t
Table 3: Feature templates. For an element p either on ? or ? of an arc-standard parser, we use p
punc
,
p
w
and p
t
to denote the punctuation property, head word and head tag of p, respectively. d
pq
denotes the
distance between the two elements p and q.
we attach BPUNCs to their right neighbours and
EPUNCs to their left neighbours, as shown in Fig-
ure 1 (b). Note that in Figure 1 (a), the left neigh-
bour of ? is also a punctuation. In such cases, we
simply remove these punctuations since the exis-
tence of paired punctuations already indicates that
there should be a boundary.
During parsing, when a dependency arc with
lexical head w
h
is created, the property of w
h
is
updated by the property of its left (or right) most
child to keep track whether there is a BPUNC (or
EPUNC) to the left (or right) side of the sub-tree
rooted at w
h
, as shown in Figure 1 (c). When
BPUNCs and EPUNCs meet each other at w
h
, a
PAIRED property is assigned to w
h
to capture that
the words within the paired punctuations form a
sub-tree, rooted at w
h
. See Figure 1 (d).
3.2 Practical Issues
It is not uncommon that two BPUNCS appear ad-
jacent to each other. For example,
(?Congress?s Environmental Buccaneers,?
Sept. 18).
In our implementation, BPUNC or EPUNC prop-
erties are implemented using flags. In the exam-
ple, we set two flags ? and ( on the word Con-
grees?s. When BPUNC and EPUNC meet each
other, the corresponding flags are turned off. In
the example, when Congrees?s is identified as a
modifier of Buccaneers, the ? flag of Buccaneers
is turned off. However, we do not assign a PAIRED
property to Buccaneers since its ( flag is still on.
The PAIRED property is assigned only when all
the flags are turned off.
3.3 Non-Paired Punctuations
Though some types of non-paired punctuations
may capture certain syntactic patterns, we do not
make further distinctions between them, and treat
these punctuations uniformly for simplicity.
Before parsing starts and after the preprocessing
step for paired punctuations, our method employs
a second preprocessing step to attach non-paired
punctuations to their left neighbouring words. It
is guaranteed that the property of the left neigh-
bouring words of non-paired punctuations must be
empty. Otherwise, it means the non-paired punc-
tuation is adjacent to a paired punctuation. In
such cases, the non-paired punctuation would be
removed in the first processing step.
During parsing, non-paired punctuations are
also passed bottom-up: the property of w
h
is up-
dated by its right-most dependent to keep track
whether there is a punctuation to the right side
of the tree rooted at w
h
. The only special case is
that ifw
h
already contains a BPUNC property, then
our method simply ignores the non-paired prop-
erty since we maintain the boundary information
with the highest priority.
3.4 Features
We incorporate our method into the arc-standard
transition-based parser, which uses a stack ? to
maintain partially constructed trees and a buffer ?
for the incoming words (Nivre, 2008). We design
a set of features to exploit the potential of using
punctuation properties for the arc-standard parser.
The feature templates are listed in Table 3.
In addition to the features designed for paired
punctuations, such as bigram punctuation features
listed in line 3 of Table 3, we also design features
for non-paired punctuations. For example, the dis-
tance features in line 5 of Table 3 is used to capture
the pattern that if a word w with comma property
is the left modifier of a noun or a verb, the distance
between w and its lexical head is often larger than
1. In other words, they are not adjacent.
4 Results
Our first experiment is to investigate the effect of
processing paired punctuations on parsing accu-
racy. In this experiment, the method introduced
in Section 3.1 is used to process paired punctua-
tions, and the non-paired punctuations are left un-
794
s Baseline Paired All
1 90.76 91.25 91.47
2 91.88 92.06 92.34
4 92.50 92.61 92.70
8 92.73 92.76 92.82
16 92.90 92.94 92.99
64 92.99 93.04 93.10
Table 4: Parsing accuracies on the development
set. s denotes the beam width.
touched. Feature templates used in this experi-
ment are those listed in the top three rows of Ta-
ble 3 together with those used for the baseline arc-
standard parser.
Results on the development set are shown in the
second column of Table 4. We can see that when
the beam width is set to 1, our method achieves an
0.49 UAS improvement. By comparing the out-
puts of the two parsers, two types of errors made
by the baseline parser are effectively corrected.
The first is that our method is able to cap-
ture the pattern that there is only one depen-
dency arc between the words within the paired-
punctuations and the words outside, while the
baseline parser sometimes creates more depen-
dency arcs that cross the boundary.
The second is more interesting. Our method is
able to capture that the root, w
h
, of the sub-tree
within the paired-punctuation, such as ?Mecha-
nisms? in Figure 1, generally serves as a modifier
of the words outside, while the baseline parser oc-
casionally make w
h
as the head of the sentence.
As we increase the beam width, the improve-
ment of our method over the baseline becomes
smaller. This is as expected, since beam search
also has the effect of reducing error propagation
(Zhang and Nivre, 2012), thereby alleviating the
errors caused by punctuations.
In the last experiment, we examine the effect
of incorporating all punctuations using the method
introduced in Section 2. In this experiment, we
use all the feature templates in Table 3 and those
in the baseline parser. Results are listed in the
fourth column of Table 4, which shows that pars-
ing accuracies can be further improved by also
processing non-paired punctuations. The overall
accuracy improvement when the beam width is 1
reaches 0.91%. The extra improvements mainly
come from better accuracies on the sentences with
comma. However, the exact type of errors that
are corrected by using non-paired punctuations is
more difficult to summarize.
system UAS Comp Root
Baseline 90.38 37.71 89.45
All-Punc 91.32 41.35 92.43
Baseline-64 92.84 46.90 95.57
All-Punc-64 93.06 48.55 95.53
Huang 10 92.10 ? ?
Zhang 11 92.90 48.00 91.80
Choi 13 92.96 ? ?
Bohnet 12 93.03 ? ?
Table 5: Final result on the test set.
The final results on the test set are listed in Ta-
ble 5
5
. Table 5 also lists the accuracies of state-
of-the-art transition-based parsers. In particular,
?Huang 10? and ?Zhang 11? denote Huang and
Sagae (2010) and Zhang and Nivre (2011), re-
spectively. ?Bohnet 12? and ?Choi 13? denote
Bohnet and Nivre (2012) and Choi and Mccal-
lum (2013), respectively. We can see that our
method achieves the best accuracy for single-
model transition-based parsers.
5 Conclusion and Related Work
In this work, we proposed to treat punctuations
as properties of context words for dependency
parsing. Experiments with an arc-standard parser
showed that our method effectively improves pars-
ing performance and we achieved the best accu-
racy for single-model transition-based parser.
Regarding punctuation processing for depen-
dency parsing, Li et al (2010) proposed to uti-
lize punctuations to segment sentences into small
fragments and then parse the fragments separately.
A similar approach is proposed by Spitkovsky et
al. (2011) which also designed a set of constraints
on the fragments to improve unsupervised depen-
dency parsing.
Acknowledgements
We highly appreciate the anonymous reviewers
for their insightful suggestions. This research
was supported by the National Science Founda-
tion of China (61272376; 61300097; 61100089),
the Fundamental Research Funds for the Cen-
tral Universities (N110404012), the research grant
T2MOE1301 from Singapore Ministry of Ed-
ucation (MOE) and the start-up grant SRG
ISTD2012038 from SUTD.
5
The number of training iteration is determined using the
development set.
795
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ?12, pages 1455?1465, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jinho D. Choi and Andrew Mccallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Jan Hajic, Sandra Carberry, and Stephen Clark, ed-
itors, ACL, pages 1077?1086. The Association for
Computer Linguistics.
Zhenghua Li, Wanxiang Che, and Ting Liu. 2010. Im-
proving dependency parsing using punctuation. In
Minghui Dong, Guodong Zhou, Haoliang Qi, and
Min Zhang, editors, IALP, pages 53?56. IEEE Com-
puter Society.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-first pos tagging and dependency parsing with
beam search. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 110?114,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006)), volume 6, pages
81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 91?98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011. Punctuation: Making a point in un-
supervised dependency parsing. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL-2011).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391?1400,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
796
Mining Large-scale Parallel Corpora from Multilingual Patents: 
An English-Chinese example and its application to SMT 
Bin Lu?, Benjamin K. Tsou??, Tao Jiang?, Oi Yee Kwong?, and Jingbo Zhu? 
?Department of Chinese, Translation & Linguistics, City University of Hong Kong 
?Research Centre on Linguistics and Language Information Sciences,  
Hong Kong Institute of Education 
?ChiLin Star Corp., Southern Software Park, Zhuhai, China 
?Natural Language Processing Lab, Northeastern University, Shenyang, China 
{lubin2010, rlbtsou, jiangtaoster}@gmail.com, 
rlolivia@cityu.edu.hk, zhujingbo@mail.neu.edu.cn 
 
Abstract 
In this paper, we demonstrate how to 
mine large-scale parallel corpora with 
multilingual patents, which have not 
been thoroughly explored before. We 
show how a large-scale English-Chinese 
parallel corpus containing over 14 
million sentence pairs with only 1-5% 
wrong can be mined from a large amount 
of English-Chinese bilingual patents. To 
our knowledge, this is the largest single 
parallel corpus in terms of sentence pairs. 
Moreover, we estimate the potential for 
mining multilingual parallel corpora 
involving English, Chinese, Japanese, 
Korean, German, etc., which would to 
some extent reduce the parallel data 
acquisition bottleneck in multilingual 
information processing. 
1 Introduction 
Multilingual data are critical resources for 
building many applications, such as machine 
translation (MT) and cross-lingual information 
retrieval. Many parallel corpora have been built, 
such as the Canadian Hansards (Gale and 
Church, 1991), the Europarl corpus (Koehn, 
2005), the Arabic-English and English-Chinese 
parallel corpora used in the NIST Open MT 
Evaluation.  
However, few parallel corpora exist for many 
language pairs, such as Chinese-Japanese, 
Japanese-Korean, Chinese- French or 
Japanese-German. Even for language pairs with 
several parallel corpora, such as Chinese-English 
and Arabic-English, the size of parallel corpora 
is still a major limitation for SMT systems to 
achieve higher performance. 
In this paper, we present a way which could, to 
some extent, reduce the parallel data acquisition 
bottleneck in multilingual language processing.  
Based on multilingual patents, we show how an 
enlarged English-Chinese parallel corpus 
containing over 14 million high-quality sentence 
pairs can be mined from a large number of 
comparable patents harvested from the Web. To 
our knowledge, this is the largest single parallel 
corpus in terms of parallel sentences. Some SMT 
experiments are also reported. Moreover, we 
investigate the potential to get large-scale 
parallel corpora for languages beyond the 
Canadian Hansards, Europarl and UN news used 
in NIST MT Evaluation by estimating the 
quantity of multilingual patents involving 
English, Chinese, Japanese, Korean, German, 
etc.  
Related work is introduced in Section 2. 
Patents, PCT patents, multilingual patents are 
described in Section 3. Then an English-Chinese 
parallel corpus, its mining process and 
application to SMT are introduced in Section 4, 
followed by the quantity estimation of 
multilingual patents involving other language 
pairs in Section 5. We discuss the results in 
Section 6, and conclude in Section 7. 
2 Related Work 
Parallel sentences could be extracted from 
parallel documents or comparable corpora. 
Different approaches have been proposed to 
align sentences in parallel documents consisting 
of the same content in different languages based 
on the following information: a) the sentence 
length in bilingual sentences (Brown et al 1991; 
Gale and Church, 1991); b) lexical information 
in bilingual dictionaries (Ma, 2006); c) statistical 
translation model (Chen, 1993), or the composite 
of more than one approach (Simard and 
Plamondon, 1998; Moore, 2002).  
To overcome the lack of parallel documents, 
comparable corpora are also used to mine 
parallel sentences, which raises further 
challenges since the bilingual contents are not 
strictly parallel. For instance, Zhao and Vogel 
(2002) investigated the mining of parallel 
sentences for Web bilingual news. Munteanu and 
Marcu (2005) presented a method for 
discovering parallel sentences in large Chinese, 
Arabic, and English comparable, non-parallel 
corpora based on a maximum entropy classifier. 
Cao et al, (2007) and Lin et al, (2008) proposed 
two different methods utilizing the parenthesis 
pattern to extract term translations from bilingual 
web pages. Jiang et al (2009) presented an 
adaptive pattern-based method which produced 
Chinese-English bilingual sentences and terms  
with over 80% accuracy. 
Only a few papers were found on the related 
work in the patent domain. Higuchi et al (2001) 
used the titles and abstracts of 32,000 
Japanese-English bilingual patents to extract 
bilingual terms. Utiyama and Isahara (2007) 
mined about 2 million parallel sentences by 
using two parts in the description section of 
Japanese-English comparable patents. Lu et al 
(2009) derived about 160K parallel sentences 
from Chinese-English comparable patents by 
aligning sentences and filtering alignments with 
the combination of different quality measures. 
Another closely related work is the 
English-Chinese parallel corpus (Lu et al, 
2010), which is largely extended by this work, in 
which both the number of patents and that of 
parallel sentences are augmented by about 
100%, and more SMT experiments are given. 
Moreover, we show the potential for mining 
parallel corpora from multilingual patents 
involving other languages. 
For statistical machine translation (SMT), 
tremendous strides have been made in two 
decades, including Brown et al (1993), Och and 
Ney (2004) and Chiang (2007). For the MT 
evaluation, NIST (Fujii et al, 2008; 2010) has 
been organizing open evaluations for years, and 
the performance of the participants has been 
improved rapidly.  
3 Patents and Multilingual Patents 
A patent is a legal document representing ?an 
official document granting the exclusive right to 
make, use, and sell an invention for a limited 
period? (Collins English Dictionary1). A patent 
application consists of different sections, and we 
focus on the text, i.e. only title, abstract, claims 
and description.  
3.1 PCT Patents 
Since the invention in a patent is only protected 
in the filing countries, a patent applicant who 
wishes to protect his invention outside the 
original country should file patents in other 
countries, which may involve other languages. 
The Patent Cooperation Treaty (PCT) system 
offers inventors and industry an advantageous 
route for obtaining patent protection 
internationally. By filing one ?international? 
patent application under the PCT via the World 
Intellectually Property Organization (WIPO), 
protection of an invention can be sought 
simultaneously (i.e. the priority date) in each of a 
large number of countries. 
The number of PCT international applications 
                                                          
1 Retrieved March 2010, from 
http://www.collinslanguage.com/ 
filed is more than 1.7 million 2 . A PCT 
international application may be filed in any 
language accepted by the relevant receiving 
office, but must be published in one of the 
official publication languages (Arabic, Chinese, 
English, French, German, Japanese, Korean, 
Russian and Spanish). Other highly used 
languages for filing include Italian, Dutch, 
Finnish, Swedish, etc. Table 1 3  shows the 
number of PCT applications for the most used 
languages of filing and publication.  
 Lang. of Filing 
Share 
(%) 
Lang. of 
Publication 
Share 
(%) 
English 895K 52.1 943K 54.9 
Japanese 198K 11.5 196K 11.4 
German 185K 10.8 184K 10.7 
French 55K 3.2 55K 3.2 
Korean 24K 1.4 3K4 0.2 
Chinese 24K 1.4 24K 1.4 
Other 336K 19.6 313K 18.2 
Total 1.72M 100 1.72M 100 
Table 1. PCT Application Numbers for Languages of 
Publication and Filing 
From Table 1, we can observe that English, 
Japanese and German are the top 3 languages in 
terms of PCT applications, and English accounts 
for over 50% of applications in terms of 
language of both publication and filing.  
3.2 Multilingual Patents 
A PCT application does not necessarily mean a 
multilingual patent. An applicant who has 
decided to proceed further with his PCT 
international application must fulfill the 
requirements for entry into the PCT national 
phase at the patent offices of countries where he 
seeks protection. For example, a Chinese 
company may first file a Chinese patent in China 
                                                          
2 Retrieved Apr., 2010 from 
http://www.wipo.int/pctdb/en/. The data below involving 
PCT patents comes from the website of WIPO. 
3 The data in this and other tables in the following sections 
involving PCT patents comes from the website of WIPO. 
4  Korean just became one of the official publication 
languages for the PCT system since 2009, and thus the 
number of PCT patents with Korean as language of 
publication is small. 
patent office and then file its international 
application also in Chinese under the PCT. Later 
on, it may have the patent translated into English 
and file it in USA patent office, which means the 
patent becomes bilingual. If the applicant 
continues to file it in Japan with Japanese, it 
would be trilingual. Even more, it would be 
quadrilingual or involve more languages when it 
is filed in other countries with more languages. 
Such multilingual patents are considered 
comparable (or noisy parallel) because they are 
not parallel in the strict sense but still closely 
related in terms of information conveyed 
(Higuchi et al, 2001; Lu et al, 2009). 
4 A Large English-Chinese Parallel 
Corpus Mined from Bilingual Patents 
In this section, we introduce the English-Chinese 
bilingual patents harvested from the Web and the 
method to mine parallel sentences from them. 
SMT experiments on the final parallel corpus are 
also described. 
4.1 Harvesting English-Chinese Bilingual 
Patents 
The official patent office in China is the State 
Intellectual Property Office (SIPO). In early 
2009,  by searching on its website, we found 
about 200K Chinese patents previously filed as 
PCT applications in English and crawled their 
bibliographical data, titles, abstracts and the 
major claim from the Web, and then other claims 
and descriptions were also added. Since some 
contents are in the image format, the images 
were OCRed and the texts recognized were 
manually verified. 
All PCT patent applications are filed through 
WIPO. With the Chinese patents mentioned 
above, the corresponding English patents were 
searched from the website of WIPO by the PCT 
publication numbers to obtain relevant sections 
of the English PCT applications, including 
bibliographical data, title, abstract, claims and 
description. About 80% (160K) out of the 
Chinese patents found their corresponding 
English ones. Some contents of the English 
patents were OCRed by WIPO. 
We automatically split the patents into 
individual sections according to the respective 
tags inside the patents, and segmented each 
section sentences according to punctuations. The 
statistics of each section for Chinese and 
English patents are shown in Table 2. 
Chinese English 
Sections 
#Char #Sent #Word #Sent 
Title 2.7M 157K 1.6M 157K 
Abstract 33M 596K 20M 784K 
Claim 367M 6.8M 217M 7.4M 
Desc. 2,467M 48.8M 1,353M 54.0M 
Total 2,870M 56.2M 1,591M 62.3M 
Table 2. Statistics of Comparable Patents 
4.2 Mining Parallel Sentences from 
Bilingual Patents 
The sentences in each section of Chinese patents 
were aligned with those in the corresponding 
section of the corresponding English patents to 
find parallel sentences after the Chinese 
sentences were segmented into words. 
Since the comparable patents are not strictly 
parallel, the individual alignment methods 
mentioned in Section 2 would be not effective: 1) 
the length-based method is not accurate since it 
does not consider content similarity; 2) the 
bilingual dictionary-based method cannot deal 
with new technical terms in the patents; 3) the 
translation model-based method would need 
training data to get a translation model. Thus, in 
this study we combine these three methods to 
mine high-quality parallel sentences from 
comparable patents. 
We first use a bilingual dictionary to 
preliminarily align the sentences in each section 
of the comparable patents. The dictionary-based 
similarity score dP  of a sentence pair is 
computed based on a bilingual dictionary as 
follows (Utiyama and Isahara, 2003):  
2/??
)deg()deg(
),(
),(
ce
Sw Sw ec
ec
ecd ll
ww
ww
SSp cc ee
+
=
? ?
? ?
?
 
where cw  and ew  are respectively the 
word types in Chinese sentence cS  and 
English sentence eS ; cl  and el  respectively 
denote the lengths of cS  and eS  in terms of 
the number of words; and ),( ec ww?  = 1 if 
cw  and ew  is a translation pair in the 
bilingual dictionary or are the same string, 
otherwise 0; and 
?
?
=
ee Sw
ecc www ),()deg( ?
?
?
=
ce Sw
ece www ),()deg( ? . 
For the bilingual dictionary, we combine three 
ones: namely, LDC_CE_DIC2.0 5  constructed 
by LDC, bilingual terms in HowNet and the 
bilingual lexicon in Champollion (Ma, 2006). 
We then remove sentence pairs using length 
filtering and ratio filtering: 1) for length filtering, 
if a sentence pair has more than 100 words in the 
English sentence or more than 333 characters in 
the Chinese one, it is removed; 2) for length ratio 
filtering, we discard the sentence pairs with 
Chinese-English length ratio outside the range of 
0.8 to 1.8. The parameters here are set 
empirically. 
We further filter the parallel sentence 
candidates by learning an IBM Model-1 on the 
remaining aligned sentences and compute the 
translation similarity score tP  of sentence 
pairs by combining the translation probability 
value of both directions (i.e. Chinese->English 
and English->Chinese) based on the trained 
IBM-1 model (Moore, 2002; Chen, 2003; Lu et 
al, 2009). It is computed as follows: 
ec
ecce
ect ll
)S(SPlog)S(SPlog
SSp
+
+
=
)|()|(
),(
 
where )SS(P ce | denotes the probability 
that a translator will produce eS  in English 
when presented with cS  in Chinese, and vice 
versa for )|(S ec SP . Sentence pairs with 
                                                          
5 http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 
similarity score tP  lower than a predefined 
threshold are filtered out as wrong aligned 
sentences. 
Table 3 shows the sentence numbers and the 
percentages of sentences kept in each step above 
with respect to all sentence pairs. In the first row 
of Table 3, 1.DICT denotes the first step of using 
the bilingual dictionary to align sentences; 2. FL 
denotes the length and ratio filtering; 3. TM 
refers to the third and final step of using 
translation models to filter sentence pairs. 
 1. DICT 2.FL 3. TM (final) 
Abstr. 503K 352K  (70%) 
166K  
(33%) 
Claims 6.0M 4.3M (72.1%) 
2.0M 
(33.4%) 
Desc. 38.6M 26.8M (69.4%) 
12.1M 
(31.3%) 
Total6 45.1M 31.5M (69.8%) 
14.3M 
(31.7%) 
Table 3. Numbers of Sentence Pairs 
Both the 31.5M parallel sentences after the 
second step FL and the final 14.3M after the third 
step TM are manually evaluated by randomly 
sampling 100 sentence pairs for each section. 
The evaluation metric follows the one in Lu et al 
(2009), which classifies each sentence pair into 
Correct, Partially Correct or Wrong. The results 
of manual evaluation are shown in Table 4. 
 Section Correct Partially Correct Wrong 
Abstr. 85% 7% 8% 
Claims 83% 10% 7% 2. FL 
Desc. 69% 15% 15% 
Abstr. 97% 2% 1% 
Claims 92% 3% 5% 3. TM  (final) 
Desc. 89% 8% 3% 
Table 4. Manual Evaluation of the Corpus 
From Table 4, we can see that: 1) In the final 
corpus, the percentages of correct parallel 
sentences are quite high, and the wrong 
percentages are no higher than 5%; 2) Without 
                                                          
6 Here the total number does not include the number of 
titles, which are directly treated as parallel. 
the final step of TM, the accuracies of 31.5M 
sentence pairs are between 69%-85%, and the 
percentages of wrong pairs are between 
7%-15%; 3) The abstract section shows the 
highest correct percentage, while the description 
section shows the lowest. 
Thus, we could conclude that the mined 14M 
parallel sentences are of high quality with only 
1%-5% wrong pairs, and our combination of 
bilingual dictionaries and translation models for 
mining parallel sentences are quite effective. 
4.3 Chinese-English Statistical Machine 
Translation 
A Chinese-English SMT system is setup using 
Moses (Koehn, 2007). We train models  based 
on different numbers of parallel sentences mined 
above. The test set contains 548 sentence pairs 
which are randomly selected and different from 
the training data. The sizes of the training data 
and BLEU scores for the models are shown in 
Table 5. 
System BLEU (%) #Sentence Pairs for training 
Model-A 17.94 300K 
Model-B 19.96 750K 
Model-C 20.09 1.5M 
Model-D 20.98 3M 
Model-E 22.60 6M 
Table 5. SMT Experimental Results 
From Table 5, we can see that the BLEU 
scores are improving steadily when the training 
data increases. When the training data is 
enlarged by 20 times from 300K to 6M, the 
BLEU score increases to 22.60 from 17.94, 
which is quite a significant improvement. We 
show the translations of one Chinese sample 
sentence in Table 6 below. 
CN  
Sent. 
?? ?? ?? ??? ?? ? ? ?? ? 
? ? 
Ref. 
the main shaft of the electric motor 
extends into the working cavity of the 
compressor shell , 
Model-A the motor main shaft into the compressor the chamber 
Model-B motor shaft into the compressor housing . the working chamber 
Model-C motor shaft into the compressor housing . the working chamber 
Model-D 
motor spindle extends into the 
compressor housing . the working 
chamber 
Model-E motor spindle extends into the working chamber of the compressor housing , 
Table 6. Translations of One Chinese Sentence 
From Table 6, we can see the translations 
given by Model-A to Model-C are lack of the 
main verb, the one given by Model-D has an 
ordering problem for the head noun and the 
modifier, and the one given by Model-E seems 
better than the others and its content is already 
quite similar to the reference despite the lexical 
difference. 
5 Multilingual Corpora for More 
Languages 
In this section, we describe the potential of 
building large-scale parallel corpora for more 
languages, especially Asian languages by using 
the 1.7 million PCT patent applications and their 
national correspondents. By using PCT 
applications as the pivot, we can build 
multilingual parallel corpora from multilingual 
patents, which would greatly enlarge parallel 
data we could obtain. 
The patent applications filed in one country 
should be in the official language(s) of the 
country, e.g. the applications filed in China 
should be in Chinese, those in Japan be in 
Japanese, and so on. In Table 7, the second 
column shows the total numbers of patent 
applications in different countries which were 
previously filed as PCT ones; and the third 
column shows the total numbers of applications 
in different countries, which were previously 
filed as PCT ones with English as language of 
publication. 
National Phase 
Country7 ALL 
English as Lang. 
of Publication 
                                                          
7 For the national phase of the PCT System, the statistics 
are based on data supplied to WIPO by national and 
Japan 424K 269K 
China 307K 188K 
Germany 32K 10K 
R. Korea 236K 134K 
China & Japan 189K 130K 
China & R. Korea 154K 91K 
Japan & R. Korea 158K 103K 
China & Japan  
& R. Korea 106K 73K 
Table 7. Estimated Numbers of Multilingual 
Patents 
The number of the Chinese-English bilingual 
patents (CE) in Table 7 is about 188K, which is 
consistent with the number of 160K found in 
Section 4.1 since the latter contains only the 
applications up to early 2009. Based on Table 7, 
we estimate below the rough sizes of bilingual 
corpora, trilingual corpora, and even 
quadrilingual corpora for different languages. 
1) Bilingual Corpora with English as one 
language 
Compared to CE (188K), the 
Japanese-English bilingual corpus (269K) could 
be 50% larger in terms of bilingual patents, the 
Korean-English one (134K) could be about 30% 
smaller, and the German-English one (10K) 
would be much smaller. 
2) Bilingual Corpora for Asian Languages  
The Japanese-Chinese bilingual corpus 
(189K) could be comparable to CE (188K) in 
terms of bilingual applications, the Chinese- 
Korean one (154K) could be about 20% smaller, 
and the Japanese-Korean one (158K) is quite 
similar to the Chinese-Korean one. 
3)  Trilingual Corpora 
In addition to bilingual corpora, we can also 
build trilingual corpora from trilingual patents. It 
is quite interesting to note that the trilingual 
corpora  could be quite large even compared to 
the bilingual corpora.  
The trilingual corpora for Chinese, Japanese 
and English (130K) could be only 30% smaller 
than CE in terms of patents. The trilingual corpus 
                                                                                      
regional patent Offices, received at WIPO often 6 months or 
more after the end of the year concerned, i.e. the numbers 
are not up-to-date . 
for Chinese, Korean and English (91K) and that 
for Japanese, Korean and English (103K) are 
also quite large. The number of the trilingual 
patents for the Asian languages of Chinese, 
Japanese and Korean (106K) is about 54% of 
that of CE. 
4) Quadrilingual Corpora 
The number of the quadrilingual patents for 
Chinese, Japanese, Korean and English (73K) is 
about 38% of that of CE. From these figures, we 
could say that a large proportion of the PCT 
applications published in English later have been 
filed in all the three Asian countries: China, 
Japan, and R. Korea. 
6 Discussion 
The websites from which the Chinese and 
English patents were downloaded were quite 
slow to access, and were occasionally down 
during access. To avoid too much workload for 
the websites, the downloading speed had been 
limited. Some large patents would cost much 
time for the websites to respond and had be 
specifically handled. It took considerable efforts 
to obtain these comparable patents.  
In addition our English-Chinese corpus mined 
in this study is at least one order of magnitude 
larger, we give some other differences between 
ours and those introduced in Section 2 (Higuchi 
et al, 2001; Utiyama and Isahara, 2007; Lu et al 
2009)  
1) Their bilingual patents were identified by 
the priority information in the US patents, and 
could not be easily extended to language pairs 
without English; while our method using PCT 
applications as the pivot could be easily 
extended to other language pairs as illustrated in 
Section 5. 
2) The translation process is different: their 
patents were filed in USA Patent Office in 
English by translating from Japanese or Chinese, 
while our patents were first filed in English as a 
PCT application, and later translated into 
Chinese. The different translation processes may 
have different characteristics. 
Since the PCT and multilingual patent 
applications increase rapidly in recent years as 
discussed in Section 3, we could expect more 
multilingual patents to enlarge the large-scale 
parallel corpora with the new applications and 
keep them up-to-date with new technical terms. 
On the other hand, patents are usually translated 
by patent agents or professionals, we could 
expect high quality translations from 
multilingual patents. We have been planning to 
build trilingual and quadrilingual corpora from 
multilingual patents. 
One possible limitation of patent corpora is 
that the sentences are all from technical domains 
and written in formal style, and thus it is 
interesting to know if the parallel sentences 
could improve the performance of SMT systems  
on NIST MT evaluation corpus containing news 
sentences and web sentences.  
7 Conclusion 
In this paper, we show how a large high-quality 
English-Chinese parallel corpus can be mined 
from a large amount of comparable patents 
harvested from the Web, which is the largest 
single parallel corpus in terms of the  number of 
parallel sentences. Some sampled parallel 
sentences are available at 
http://www.livac.org/smt/parpat.html, and more 
parallel sentences would be publicly available to 
the research community. 
With 1.7 million PCT patent applications and 
their corresponding national ones, there are 
considerable potentials of constructing 
large-scale high-quality parallel corpora for 
languages. We give an estimation on the sizes of 
multilingual parallel corpora which could be 
obtained from multilingual patents involving 
English, Chinese, Japanese, Korean, German, 
etc., which would to some extent reduce the 
parallel data acquisition bottleneck in 
multilingual information processing. 
Acknowledgements 
We wish to thank Mr. Long Jiang from 
Microsoft Research Asia and anonymous 
reviewers for their valuable comments. 
References 
Brown, Peter F., Jennifer C. Lai, and Robert L. 
Mercer. 1991. Aligning sentences in parallel 
corpora. In Proceedings of ACL. pp.169-176. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. 
Mathematics of statistical machine translation: 
Parameter estimation. Computational Linguistics, 
19(2), 263-311. 
Cao, Guihong, Jianfeng Gao and Jianyun Nie. 2007. 
A System to Mine Large-scale Bilingual 
Dictionaries from  Monolingual Web Pages. In 
Proceedings of MT Summit. pp. 57-64. 
Chen, Stanley F. 1993. Aligning sentences in 
bilingual corpora using lexical information. In 
Proceedings of ACL. pp. 9-16. 
Chiang, David. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2), 
201?228. 
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto, 
and Takehito Utsuro. 2008. Overview of the patent 
translation task at the NTCIR-7 workshop. In 
Proceedings of the NTCIR-7 Workshop. pp. 
389-400. Tokyo, Japan. 
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto, 
Takehito Utsuro, Terumasa Ehara, Hiroshi 
Echizen-ya and Sayori Shimohata. 2010. 
Overview of the patent translation task at the 
NTCIR-8 workshop. In Proceedings of the 
NTCIR-8 Workshop. Tokyo, Japan. 
Gale, William A., and Kenneth W. Church. 1991. A 
program for aligning sentences in bilingual 
corpora. In Proceedings of ACL. pp.79-85. 
Higuchi, Shigeto, Masatoshi Fukui, Atsushi Fujii, and 
Tetsuya Ishikawa. PRIME: A System for 
Multi-lingual Patent Retrieval. In Proceedings of 
MT Summit VIII, pp.163-167, 2001. 
Koehn, Philipp. 2005. Europarl: A parallel corpus for 
statistical machine translation. In Proceedings of 
MT Summit X. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, et al 2007. Moses: Open source 
toolkit for statistical machine translation. In 
Proceedings of ACL Demo Session. pp. 177-180. 
Lin, Dekang, Shaojun Zhao, Benjamin V. Durme and 
Marius Pasca. 2008. Mining Parenthetical 
Translations from the Web by Word Alignment. In 
Proceedings of ACL-08. pp. 994-1002. 
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu, 
and Qingsheng Zhu. 2009. Mining Bilingual Data 
from the Web with Adaptively Learnt Patterns. In 
Proceedings of ACL-IJCNLP. pp. 870-878. 
Lu, Bin, Benjamin K. Tsou, Jingbo Zhu, Tao Jiang, 
and Olivia Y. Kwong. 2009. The Construction of 
an English-Chinese Patent Parallel Corpus. MT 
Summit XII 3rd Workshop on Patent Translation. 
Lu, Bin, Tao Jiang, Kapo Chow and Benjamin K. 
Tsou. 2010. Building a Large English-Chinese 
Parallel Corpus from Comparable Patents and its 
Experimental Application to SMT. LREC 
Workshop on Building and Using Comparable 
Corpora. Malta. May, 2010. 
Ma, Xiaoyi. 2006. Champollion: A Robust Parallel 
Text Sentence Aligner. In Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation (LREC). Genova, Italy. 
Moore, Robert C. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Proceedings of 
AMTA. pp.135-144. 
Munteanu, Dragos S., and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-parallel Corpora. Computational 
Linguistics, 31(4), 477?504. 
Och, Franz J., and Hermann Ney. 2004. The 
Alignment Template Approach to Machine 
Translation. Computational Linguistics, 30(4), 
417-449. 
Simard, Michel, and Pierre Plamondon. 1998. 
Bilingual Sentence Alignment: Balancing 
Robustness and Accuracy. Machine Translation, 
13(1), 59-80. 
Utiyama, Masao, and Hitoshi Isahara. 2007. A 
Japanese-English patent parallel corpus. In 
Proceeding of MT Summit XI. pp. 475?482. 
Zhao, Bing, and Stephen Vogel. 2002. Adaptive 
Parallel Sentences Mining from Web Bilingual 
News Collection. In Proceedings of Second IEEE 
International Conference on Data Mining 
(ICDM?02). 
High OOV-Recall Chinese Word Segmenter
Xiaoming Xu, Muhua Zhu, Xiaoxu Fei, and Jingbo Zhu
School of
Information Science and Engineering
Northeastern University
{xuxm, zhumh, feixx}@ics.neu.edu.cn
zhujingbo@mail.neu.edu.cn
Abstract
For the competition of Chinese word seg-
mentation held in the first CIPS-SIGHNA
joint conference. We applied a subword-
based word segmenter using CRFs and ex-
tended the segmenter with OOV words
recognized by Accessor Variety. More-
over, we proposed several post-processing
rules to improve the performance. Our
system achieved promising OOV recall
among all the participants.
1 Introduction
Chinese word segmentation is deemed to be a pre-
requisite for Chinese language processing. The
competition in the first CIPS-SIGHAN joint con-
ference put the task of Chinese word segmenta-
tion in a more challengeable setting, where train-
ing and test data are obtained from different do-
mains. This setting is widely known as domain
adaptation.
For domain adaptation, either a large-scale un-
labeled target domain data or a small size of la-
beled target domain data is required to adapt a
system built on source domain data to the tar-
get domain. In this word segmentation competi-
tion, unfortunately, only a small size of unlabeled
target domain data is available. Thus we focus
on handling out-of-vocabulary (OOV) words. For
this purpose, our system is based on a combina-
tion of subword-based tagging method (Zhang et
al., 2006) and accessor variety-based new word
recognition method (Feng et al, 2004). In more
detail, we adopted and extended subword-based
method. Subword list is augmented with new-
word list recognized by accessor variety method.
Feature Template Description
a) cn(?2,?1, 0, 1, 2) unigram of characters
b) cncn+1(?2,?1, 0, 1) bigram of characters
c) cn?1cncn+1(?1, 0, 1) trigram of characters
d) Pu(C0) whether punctuation
e) T (C?1)T (C0)T (C+1) type of characters
Table 1: Basic Features for CRF-based Segmenter
We participated in the close track of the word
segmentation competition, on all the four test
datasets, in two of which our system is ranked at
the 1st position with respect to the metric of OOV
recall.
2 System Description
2.1 Subword-based Tagging with CRFs
The backbone of our system is a character-based
segmenter with the application of Conditional
Random Fields (CRFs) (Zhao and Kit, 2008). In
detail, we apply a six-tag tagging scheme, as in
(Zhao et al, 2006). That is , each Chinese char-
acter can be assigned to one of the tags in {B,
B2, B3, M , E, S }. Refer to (Zhao et al, 2006)
for detailed meaning of the tags. Table 1 shows
basic feature templates used in our system, where
feature templates a, b, d, e are also used in (Zhu et
al., 2006) for SVM-based word segmentation.
In order to extend basic CRF-based segmenter,
we first collect 2k most frequent words from train-
ing data. Hereafter, the list of such words is
referred to as subword list. Moreover, single-
character words 1, if they are not contained in
the subword list, are also added. Such proce-
1By single-character word, we refer to words that consist
solely of a Chinese character.
Feature Template Description
f) in(str, subword-list) is str in subword list
g) in(str, confident-word-list) is str in confident-word
list
Table 2: Subword Features for CRF-based Seg-
menter
dure for constructing a subword list is similar to
the one used in (Zhang et al, 2006). To en-
hance the effect of subwords, we go one step
further to build a list, named confident-word list
here and below, which contains words that are
not a portion of other words and are never seg-
mented in the training data. In the competition,
400 most frequent words in the confident-word list
are used. With subword list and confident-word
list, both training and test data are segmented
with forward maximum match method by using
the union of subword list and confident-word list.
Each segmentation unit (single-character or multi-
character unit) in the segmentation results are re-
garded as ?pseudo character? and thus can be rep-
resented with the basic features in Table 1 and
two additional features as shown in Table 2. See
the details of subword-based Chinese word seg-
mentation in (Zhang et al, 2006)
2.2 OOV Recognition with Accessor Variety
Accessor variety (AV) (Feng et al, 2004) is a sim-
ple and effective unsupervised method for extrac-
tion of new Chinese words. Given a unsegmented
text, each substring (candidate word) in the text
can be assigned a value according to the follow-
ing equation:
AV (s) = min{Lav(s), Rav(s)} (1)
where the left and right AV values, Lav(s) and
Rav(s) are defined to be the number of distinct
character types appearing on the left and right,
respectively. Candidate words are sorted in the
descending order of AV values and most highly
ranked ones can be chosen as new words. In
practical applications, heuristic filtering rules are
generally needed (Feng et al, 2004). We re-
implemented the AV method and filtering rules,
as in (Feng et al, 2004). Moreover, we filter out
candidate words that have AV values less than 3.
Unfortunately, candidate word list generated this
way still contains many noisy words (substrings
that are not words). One possible reason is that
unlabeled data (test data) used in the competition
is extremely small in size. In order to refine the
results derived from the AV method, we make use
of the training data to filter the results from two
different perspectives.
? Segment test data with the CRF-based seg-
menter described above. Then we collect
(candidate) words that are in the CRF-based
segmentation results, but not appear in the
training data. Such words are called CRF-
OOV words hereafter. We retain the intersec-
tion of CRF-OOV words and AV-based re-
sults as the set of candidate words to be pro-
cessed by the following step.
? Any candidate word in the intersection of
CRF-based and AV-based results will be fil-
tered out if they satisfy one of the following
conditions: 1) the candidate word is a part of
some word in the training data; 2) the candi-
date word is formed by connection of consec-
utive words in the training data; 3) the candi-
date word contains position words, such as
? (up), ? (down),? (left),? (right), etc.
Moreover, we take all English words in test data
as OOV words. A simple heuristic rule is defined
for the purpose of English word recognition: an
English word is a consecutive sequence of English
characters and punctuations between two English
characters (including these two characters).
We finally add all the OOV words into subword
list and confident-word list.
3 Post-Processing Rules
In the results of subword-based word segmenta-
tion with CRFs, we found some errors could be
corrected with heuristic rules. For this purpose,
we propose following post-processing rules, for
handling OOV and in-vocabulary (IV) words, re-
spectively.
3.1 OOV Rules
3.1.1 Annotation-Standard Independent
Rules
We assume the phenomena discussed in the fol-
lowing are general across all kinds of annotation
standards. Thus corresponding rules can be ap-
plied without considering annotation standards of
training data.
? A punctuation tends to be a single-character
word. If a punctation?s previous character
and next character are both Chinese charac-
ters, i.e. not punctuation, digit, or English
character, we always regard the punctuation
as a word.
? Consecutive and identical punctuations tend
to be joined together as a word. For exam-
ple, ??? represents a Chinese hyphen which
consists of three ?-?, and ?!!!? is used to
show emphasizing. Inspired by this obser-
vations, we would like to unite consecutive
and identical punctuations as a single word.
? When the character ??? appears in the train-
ing data, it is generally used as a connec-
tions symbol in a foreign person name, such
as ????? (Saint John)?. Taking this ob-
servation into consideration, we always unite
the character ??? and its previous and next
segment units into a single word. A similar
rule is designed to unite consecutive digits on
the sides of the symbol ?.?, ex. ?1.11?.
? We notice that four consecutive characters
which are in the pattern of AABB generally
form a single word in Chinese, for example
????? (dull)?. Taking this observation
into account, we always unite consecutive
characters in the AABB into a single word.
3.1.2 Templates with Generalized Digits
Words containing digits generally belong to a
open class, for example, the word ?2012? (AD
2012?? means a date. Thus CRF-based seg-
menter has difficulties in recognizing such words
since they are frequently OOV words. To attack
this challenge, we first generalize digits in the
training data. In detail, we replaced consecutive
digits with ?*?. For example, the word ?2012??
will be transformed into ?*??. Second, we col-
lect word templates which consist of three con-
secutive words on condition that at least one of
the words in a template contains the character ?*?
and that the template appears in the training data
more than 4 times. For example, we can get a
template like ?*?(month) *?(day)?(publish)?.
With such templates, we are able to correct errors,
say ?10? 17??? into ?10? 17???.
3.2 IV Rules
We notice that long words have less ambiguity
than short words in the sense of being words.
For example, characters in ????? ?full
of talents)? always form a word in the training
data, whereas ???? have two plausible split-
ting forms, as ??? (talent)? or ?? (people) ?
(only)?. In our system, we collect words that have
at least four characters and filter out words which
belong to one of following cases: 1) the word is
a part of other words; 2) the word consists solely
of punctation and/or digit. For example, ???
?? (materialism)? and ????? (120)? are
discarded, since the former is a substring of the
word ?????? (materialist)? and the latter is
a word of digits. Finally we get a list containing
about 6k words. If a character sequence in the test
data is a member in the list, it is retained as a word
in the final segmentation results.
Another group of IV rules concern character
sequences that have unique splitting in the train-
ing data. For example, ???? (women)? is al-
ways split as ??? (woman) ? (s)?. Hereafter,
we refer to such character sequences as unique-
split-sequence (USS). In our system, we are con-
cerned with UUSs which are composed of less
than 5 words. In order to apply UUSs for post-
processing, we first collect word sequence of vari-
able length (word number) from training data. In
detail, we collect word sequences of two words,
three words, and four words. Second, word se-
quences that have more than one splitting cases
in the training data are filtered out. Third, spaces
between words are removed to form USSs. For
example, the words ??? (woman) ? (s)? will
form the USS ???? ?. Finally, we search the
test data for each USS. If the searching succeeds,
the USS will be replaced with the corresponding
word sequence.
4 Evaluation Results
We evaluated our Chinese word segmenter in the
close track, in four domain: literature (Lit), com-
Domain Basic +OOV +OOV+IV
ROV RIV F ROV RIV F ROV RIV F
Lit .643 .946 .927 .652 .947 .929 .648 .952 .934
Com .839 .961 .938 .850 .961 .941 .852 .965 .947
Med .725 .938 .912 .754 .939 .917 .756 .944 .923
Fin .761 .956 .932 .854 .958 .950 .871 .961 .955
Table 3: Effectiveness of post-processing rules
puter (Com), medicine (Med) and finance (Fin).
The results are depicted in Table 4, where R,
P and F refer to Recall, Precision, F measure
respectively, and ROOV and RIV refer to recall
of OOV and IV words respectively. Since OOV
words are the obstacle for practical Chinese word
segmenters to achieve high accuracy, we have spe-
cial interest in the metric of OOV recall. We
found that our system achieved high OOV recall
2
. Actually, OOV recall of our system in the do-
mains of computer and finance are both ranked at
the 1st position among all the participants. Com-
pared with the systems ranked second in these
two domains, our system achieved OOV recall
.853 vs. .827 and .871 vs. .857 respectively.
We also examined the effectiveness of post-
processing rules, as shown in Table 3, where
Basic represents the performance achieved be-
fore post-processing, +OOV represents the results
achieved after applying OOV post-processing
rules, and +OOV+IV denotes the results achieved
after using all the post-processing rules, including
both OOV and IV rules. As the table shows, de-
signed post-processing rules can improve both IV
and OOV recall significantly.
Domain R P F ROOV RIV
Lit .931 .936 .934 .648 .952
Com .948 .945 .947 .853 .965
Med .924 .922 .923 .756 .944
Fin .953 .956 .955 .871 .961
Table 4: Performance of our system in the compe-
tition
2For the test data from the domain of literature, we actu-
ally use combination of our system and forward maximum
match, so we will omit the results on this test dataset in our
discussion.
5 Conclusions and Future Work
We proposed an approach to refine new words rec-
ognized with the accessor variety method, and in-
corporated such words into a subword-based word
segmenter. We found that such method could
achieve high OOV recall. Moreover, we designed
effective post-processing rules to further enhance
the performance of our systems. Our system fi-
nally achieved satisfactory results in the competi-
tion.
Acknowledgments
This work was supported in part by the National
Science Foundation of China (60873091).
References
Feng, Haodi, Kang Chen, Xiaotie Deng, and Weimin
zhang. 2004. Accessor Variety Criteriafor Chinese
Word Extraction. Computational Linguistics 2004,
30(1), pages 75-93.
Zhang, Ruiqiang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based Tagging by Condi-
tional Random Fileds for Chinese Word Segmenta-
tion. In Proceedings of HLT-NAACL 2006, pages
193-196.
Zhao, Hai, Chang-Ning Huang, and Mu Li. 2006.
Improved Chinese Word Segmentation System with
Conditional Random Field. In Proceedings of
SIGHAN-5 2006, pages 162-165.
Zhao, Hai and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character
Tagging for Word Segmentation and Named Entity
Recognition. In Proceedings of SIGHAN-6 2008,
pages 106-111.
Zhu, Muhua, Yiling Wang, Zhenxing Wang, Huizhen
Wang, and Jingbo Zhu. 2006. Designing Spe-
cial Post-Processing Rules for SVM-based Chinese
Word Segmentation. In Proceedigns of SIGHAN-5
2006, pages 217-220.
Chinese Syntactic Parsing Evaluation 
Qiang Zhou 
Center for Speech and Language Tech.
Research Institute of Information Tech.
Tsinghua University 
zq-lxd@tsinghua.edu.cn 
Jingbo Zhu 
Natural Language Processing Lab.
Northeastern University 
 zhujingbo@mail.neu.edu.cn 
Abstract 
The paper introduced the task designing 
ideas, data preparation methods, evalua-
tion metrics and results of the second 
Chinese syntactic parsing evaluation 
(CIPS-Bakeoff-ParsEval-2010) jointed 
with SIGHAN Bakeoff tasks. 
1 Introduction 
Syntactic parsing is an important technique in 
the research area of natural language processing. 
The evaluation-driven methodology is a good 
way to spur the its development. Two main parts 
of the method are a benchmark database and 
several well-designed evaluation metrics. Its fea-
sibility has been proven in the English language. 
After the release of the Penn Treebank (PTB) 
(Marcus et al, 1993) and the PARSEVAL me-
trics (Black et al, 1991), some new corpus-
based syntactic parsing techniques were ex-
plored in the English language. Based on them, 
many state-of-art English parser were built, in-
cluding the well-known Collins parser (Collins, 
2003), Charniak parser (Charniak and Johnson, 
2005) and Berkeley parser (Petrov and Klein, 
2007). By automatically transforming the consti-
tuent structure trees annotated in PTB to other 
linguistic formalisms, such as dependency 
grammar, and combinatory categorical grammar 
(Hockenmaier and Steedman, 2007), many syn-
tactic parser other than the CFG formalism were 
also developed. These include Malt Parser (Ni-
vre et al, 2007), MSTParser (McDonald et al, 
2005), Stanford Parser (Klein and Manning, 
2003) and C&C Parser (Clark and Curran, 2007).  
Based on the Penn Chinese Treebank (CTB) 
(Xue et al, 2002) developed on the similar anno-
tation scheme of PTB, these parsing techniques 
were also transferred to the Chinese language. 
(Levy and Manning, 2003) explored the feasibil-
ity of applying lexicalized PCFG in Chinese. (Li 
et al, 2010) proposed a joint syntactic and se-
mantic model for parsing Chinese. But till now, 
there is not a good Chinese parser whose per-
formance can approach the state-of-art English 
parser. It is still an open challenge for parsing 
Chinese sentences due to some special characte-
ristics of the Chinese language. We need to find 
a suitable benchmark database and evaluation 
metrics for the Chinese language.  
Last year, we organized the first Chinese syn-
tactic parsing evaluation --- CIPS-ParsEval-2009 
(Zhou and Zhu, 2009). Five Chinese parsing 
tasks were designed as follows: 
z Task 1: Part-of-speech (POS) tagging; 
z Task 2: Base chunk (BC) parsing 
z Task 3: Functional chunk (FC) parsing 
z Task 4: Event description clause (EDC) 
recognition 
z Task 5: Constituent parsing in EDCs 
They cover different levels of Chinese syntac-
tic parsing, including POS tagging (Task 1), 
shallow parsing (Task 2 & 3), complex sentence 
splitting (Task 4) and constituent tree parsing 
(Task 5). The news and academic articles anno-
tated in the Tsinghua Chinese Treebank (TCT 
ver1.0) were used to build different gold-
standard data for them. Some detailed informa-
tion about CIPS-ParsEval-2009 can be found in 
(Zhou and Li, 2009). 
This evaluation found the following difficult 
points for Chinese syntactic parsing. 
1) There are two difficulties in Chinese POS 
tagging. One is the nominal verbs. The POS ac-
curacy of them is about 17% lower than the 
overall accuracy. The other is the unknown 
words. The POS accuracy of them is about 40-
10% lower than the overall accuracy. 
2) The chunks with complex internal struc-
tures show poor performance in two chunking 
tasks. How to recognize them correctly needs 
more lexical semantic knowledge.  
3) The joint recognition of constituent tag and 
head position show poor performance in the 
constituent parsing task of EDCs. 
Therefore, the second Chinese syntactic pars-
ing evaluation (CIPS-Bakeoff-ParsEval-2010) 
jointed with SIGHAN Bakeoff tasks was pro-
posed to deal with these problems. Some new 
designing ideas are as follows: 
1) We use the segments sentences as the input 
of the syntactic parser to test the effects of POS 
tagging for Chinese parsing. 
2) We design a new metric to evaluate per-
formance of event construction recognition in a 
constituent parser of EDCs. 
3) We try to evaluate the performance of 
event relation recognition in Chinese complex 
sentence. 
In the following sections, we will introduce 
the task designing ideas, data preparation me-
thods, evaluation metrics and results of the eval-
uation.  
2 Task description 
For the syntactic parsing task (Task 2) of the 
CIPS-Bakeoff-2010, we designed two sub-tasks: 
Task 2-1: Parsing the syntactic trees in Chi-
nese event description clauses  
Task 2-2: Parsing the syntactic trees in Chi-
nese sentences. 
Each subtask is separated as close and open 
track. In the close track, only the provided train-
ing data can be used to build the parsing model. 
In the open track, other outside language re-
sources can be freely used. 
We will give two examples to show the de-
tailed goals of these two sub-tasks: 
1) Task 2-1 
Input:  a Chinese event description clause 
with correct word segmentation annotations 
? ?? ??? ?? ?? ? ?? ? ?
? ? ?? ? ? ? ? ? ?? ? ?? 
Ouput: a syntactic parsing tree of the EDC 
with appropriate constitutent tag, head position 
and POS tag annotations. 
? [dj-2 ??/s ?/wP  [dj-1 ??/rNP  [vp-
1 ??/d  [vp-0 ??/v  [np-0-2 [np-2 
[vp-1 [pp-1 ?/p  ??/v  ] [vp-1 ?/cC  
??/v  ] ] ?/uJDE  ??/n  ] ?/wP  
[np-2 [vp-1 [pp-1 ?/p  [vp-0 ?/v  ?
/n  ] ] [vp-1 ?/vM  ??/v  ] ] ?/uJDE  
??/n  ] ] ] ] ] ]1 
2) Task 2-2 
Input:  a Chinese sentence with correct word 
segmentation annotations 
? ??  ? ?? ?? ?? ? ?? ? ?
? ? ?? ? ? ? ? ? ?? ? ?
? ? ? ? ?? ? ?  ? ?? ??  
?  ?? ? ??? ? ????  ?   
Output: a syntactic parsing tree of the sen-
tence with appropriate constitute tag and POS 
tag annotations. 
? [zj [fj [fj [dj??/s  ?/wP  [dj ??/rNP  
[vp ??/d  [vp ??/v  [np [np [vp [pp 
?/p  ??/v  ] [vp ?/cC  ??/v  ] ] ?
/uJDE  ??/n  ] ?/wP  [np [vp [pp ?/p  
[vp ?/v  ?/n  ] ] [vp ?/vM  ??/v  ] ] 
?/uJDE  ??/n  ] ] ] ] ]] ?/wP  [vp ?
/d  [vp ?/v  [np ??/a  ?/uJDE  ?
/n  ] ] ] ]  ?/wP  [dj [np [vp ??/v  ??
/n  ] ?/uJDE  [np ??/n  ?/wD  ??
?/n  ] ] ?/wP  ????/v  ] ]?/wE  ] 
We define a Chinese sentence as the Chinese 
word serials ending with period, question mark 
or exclamation mark in the Chinese text. Usually, 
a Chinese sentence can describe a complex sit-
uation with several inter-related events.  It con-
sists of several clauses separated by commas or 
semicolons to describe one or more detailed 
event content. We call these clauses as event 
description clauses.  
We use the following example to explain the 
relationship between a Chinese sentence and 
                                                 
1 Each bracketed constituent is annotated with consti-
tuent tag and head positions separated by ?-?.   
Constituent tags used in the sentence are: dj-simple 
sentence, vp-verb phrase, np-noun phrase, pp-
preposition phrase. 
POS tags used are: s-space noun, wP-comma, rNP-
personal pronoun, d-adverb, v-verb, p-preposition,  
cC-conjunction, uJDE-particle,  n-noun, vM-
modality verb; 
 
event description clauses.  
? [ ???????????????
????????????? ]?[ ?
????? ] ?[ ?????????
??????? ]?                             (1) 
? [ Along the way, we see the trees have 
been cut down for regeneration, and the 
trees needed to be cut for building ]. [ All 
of them are useful building material ]. 
[ We also see several freight trucks and 
tractors for carry away trees going south 
and north ]. 
The sentence gives us several sequential situa-
tions through the vision changing along the au-
thor?s journey way: Firstly, we see the trees that 
have been cut down. They are useful building 
material. Then, we see several trucks and trac-
tors to carry away these trees. They are going 
south and north busily.  All the above situations 
are described through three EDCs annotated 
with bracket pairs in the sentence.  
Interestingly, in the corresponding English 
translation, the same situation is described 
through three English sentences with complete 
subject and predicate structures.  They show dif-
ference event description characteristics of these 
two languages. 
The Chinese author tends to describe a com-
plex situation through a sentence. Many com-
plex event relations are implicit in the structural 
sequences or semantic connections among the 
EDCs of the sentence. So many subjects or ob-
jects of an EDC can be easily omitted based on 
the adjacent contexts. 
The English author tends to describe a com-
plex situation through several sentences. Each 
sentence can give a complete description of an 
event through the subject and predicate structure. 
The event relations are directly set through the 
paragraph structures and conjunctions. 
The distinction between Chinese sentence and 
EDC can make us focus on different evaluation 
emphasis in the CIPS-Bakeoff-2010 section. 
For an EDC, we can focus on the parsing per-
formance of event content recognition. So we 
design a special metric to evaluate the recall of 
the event recognition based on the syntactic 
parsing results. 
For a sentence, we can focus on the parsing 
performance of event relation recognition. So we 
separate the simple and complex sentence con-
stitutes and give different evaluation metrics for 
them.  
Some detailed designations of the evaluation 
metrics can be found in section 4.  
3 Data preparation 
The evaluation data were extracted from Tsing-
hua Chinese Treebank (TCT) and PKU Chinese 
Treebank (PKU-CTB).  
TCT (Zhou, 2004) adopted a new annotation 
scheme for Chinese Treebank. Under this 
scheme, every Chinese sentence will be anno-
tated with a complete parse tree, where each 
non-terminal constituent is assigned with two 
tags. One is the syntactic constituent tag, such as 
noun phrase(np), verb phrase(vp), simple sen-
tence(dj), complex sentence(fj), etc., which de-
scribes basic syntactic characteristics of a consti-
tuent in the parse tree. The other is the grammat-
ical relation tag, which describes the internal 
structural relation of its sub-components, includ-
ing the grammatical relations among different 
phrases and the event relations among different 
clauses. These two tag sets consist of 16 and 27 
tags respectively.  
Now we have two Chinese treebanks anno-
tated under above scheme: (1) TCT version 1.0, 
which is a 1M words Chinese treebank covering 
a balanced collection of journalistic, literary, 
academic, and other documents; (2) TCT-2010, 
which consists of 100 journalistic annotated ar-
ticles. The following is an annotated sentence 
under TCT scheme: 
? [zj-XX [fj-LS [dj-ZW ??/rN [vp-PO ?/v 
[dj-ZW [np-DZ ?/rN ??/rN ] [vp-PO ?
/v  ??/m  ] ] ] ] ?/? [dj-ZW ?/rN  [vp-
LW [vp-PO ??/v  [sp-DZ ??/n  ??
/s  ] ] [vp-PO ??/v  [np-DZ [mp-DZ ?/m  
?/qN  ] ??/n ] ] ] ] ] ?/? ] 2               (2) 
PKU-CTB (Zhan et al, 2006) adopted a tradi-
tional syntactic annotation scheme. They anno-
tated Chinese sentences with syntactic constitu-
                                                 
2 Some grammatical relation tags used in the sentence 
are as follows: LS?complex timing event relation, 
ZW?subject-predicate relation, DZ?modifier-head 
relation,  PO?predicate-object relation. 
ent and head position tags in a complete parse 
tree. The tag set consists of 22 constituent tags.  
Because every content word is directly annotated 
with suitable constituent tag, there are many un-
ary phrases in PKU-CTB annotated sentences. 
Its current annotation scale is 881,771 Chinese 
words, 55264 sentences. The following is an 
annotated sentence under PKU-CTB scheme: 
? ( zj ( !fj ( !fj ( !dj ( np ( vp ( !v ( ?? ) ) !np 
( !n ( ?? ) ) ) !vp ( !vp ( !v ( ? ) ) np ( !n 
( ? ) ) ) ) wco ( ? ) dj ( np ( ap ( !b ( ?
? ) ) !np ( !n ( ?? ) ) ) !vp ( dp ( !d 
( ? ) ) !vp ( !vp ( !vp ( !v ( ? ) ) v ( ? ) ) 
np ( qp ( mp ( !rm ( ? ) ) !q ( ? ) ) !np ( np 
( !n ( ??? ) ) !np ( !n ( ?? ) ) ) ) ) ) ) ) 
wco ( ? ) vp ( c ( ?? ) !vp ( !v ( ? ) np 
( ap ( !b ( ?? ) ) !np ( !n ( ?? ) ) ) vp 
( !vp ( !v ( ?? ) ) vp ( !vp ( !v ( ?? ) ) 
vp ( !v ( ?? ) ) ) ) ) ) ) wfs ( ? ) ) )3      (3) 
Due to the different annotation schemes and 
formats used in these two treebanks, we pro-
posed the following strategies to build the gold-
standard data set for Task 2-1 and Task 2-2: 
1) Unify POS tag set 
The PKU-CTB has 97 POS tags, and TCT has 
70 POS tags. After analyzing these POS tags, we 
found most of them have same meanings. So we 
designed a unified POS tag set with 58 inter-
sected tags. All the POS tags used in PKU-CTB 
and TCT can be automatically mapped to this 
unified tag set. 
2) Transform PKU-CTB annotations 
Firstly, we mapped the POS tags into the uni-
fied tag set, and transformed the word and POS 
tag format into TCT?s format. Then, we deleted 
all unary constituents in PKU-CTB parse trees 
and transferred the constituent structures and 
tags into TCT?s constituent tags. Finally, we 
manually proofread the transformed parse trees 
to modify some constituent structures that are 
inconsistent with TCT annotation scheme. About 
5% constituents are modified. 
                                                 
3 The PKU-CTB uses the similar POS and constituent 
tags with TCT scheme. The exclamation symbol ?!? is 
used to annotate the head of each constituent in the 
parse tree. 
3) Extract EDCs and event annotations from 
TCT 
Based on the detailed grammatical relation 
tags annotated in TCT, we can easily extract 
each EDC for a TCT sentence (Zhou and Zhu, 
2009). Then, we proposed an algorithm to ex-
tract different event constructions in each EDC 
and build a large scale Chinese event bank. It 
can be used as a gold-standard data to evaluation 
the event recognition performance of an auto-
matic syntactic parser in Task 2-1. 
An event construction is an event chunk serial 
controlled by an event target verb. It is a basic 
unit to describe event content. For example, for 
the first EDC extracted from the above sentence 
(1), we can obtain the follow four event con-
structions for the event target verb ????, ??
??, ???, and ???? . 
? [D-sp ??/s-@] ?/wP [S-np ??/rNP-
@  ] [D-dp ??/d-@  ] [P-vp-Tgt ??/v-
@  ] [O-np ?/p  ??/v  ?/cC  ??/v  ?
/uJDE  ??/n-@  ?/wP  ?/p  ?/v  ?/n  
?/vM  ??/v  ?/uJDE  ??/n-@  ]4 
? [D-pp ?/p  ??/v-@  ] [P-vp-Tgt ?/vM  
??/v-@ ] ?/uJDE  [H-np ??/n-@  ] ? 
? ?  ?/p  [P-vp-Tgt ?/v-@  ] [O-np ?/n-
@  ] ?/vM  ??/v  ?/uJDE  ??/n   
? ? [D-pp ?/p ?/v-@ ?/n ] [P-vp-Tgt ?
/vM ??/v-@ ]?/uJDE  [H-np ??/n-@  ] 
4) Obtain TCT constituent structure trees 
We can easily select all syntactic constituent 
tags annotated in TCT sentences to build the 
gold-standard parsing trees for Task 2-2. 
We mainly used the journalistic and academic 
texts annotated in TCT and PKU-CTB to build 
different training and test set for task 2-1 and 2-2. 
Table 1 summarizes current building status of 
these gold-standard data sets. 
 
 
 
                                                 
4 Each event chunk is annotated with bracket pairs 
with functional and constituent tags. Some functional 
tags used in the EDCs are as follows: D?adverbial, 
S?subject, P?predicate, O?object. The constituent 
tags are same with that ones used in above parse tree. 
The head of each chunk is indicated through ?-@?. 
Data 
set 
Source Genre Methods 
2-1, 
TR 
TCT 
ver1.0 
News, 
Academy 
POS unification, 
EDC and event 
extraction 
2-1, 
TS 
TCT-
2010 
News POS unification, 
EDC and event 
extraction 
2-2, 
TR 
TCT 
ver1.0 
News, 
Academy 
POS unification, 
Parse tree extrac-
tion 
2-2, 
TS 
PKU-
CTB 
Academy POS unification, 
annotation trans-
formation 
Table 1 Gold-standard data building status 
(TR=Training data, TS=Test data) 
We selected all news and academic texts an-
notated in TCT ver1.0 to form the training set of 
Task 2-1 and 2-2. 1000 EDCs extracted from 
TCT-2010 were selected as the test set of Task 
2-1. These sentences are extracted from the 
People?s Daily corpus with the same source of 
TCT ver1.0. 1000 sentences extracted from 
PKU-CTB were selected as the test set of Task 
2-2. Most of them are extracted from the tech-
nical reports or popular science articles. They 
have much more technical terms than the encyc-
lopedic articles used in TCT ver1.0. Table 2 
shows the basic statistics of all the training and 
test sets in Task 2. 
Data set Word 
Sum 
Sent.  
Sum 
Average
Length
2-1, TR 425619 37219 11.44 
2-1, TS 9182 1000 9.18 
2-2, TR 481061 17529 27.44 
2-2, TS 26381 1000 26.38 
Table 2 Basic statistics of Task 2 
4 Evaluation metrics 
For Task 2-1, we designed three kinds of evalua-
tion metrics: 
1) POS accuracy (POS-A) 
This metri is used to evaluate the performance 
of automatic POS tagging. Its computation for-
mula is as follows: 
? POS accuracy = (sum of words with cor-
rect POS tags) / (sum of words in gold-
standard sentences) * 100% 
The correctness criteria of POS tagging is as 
follows:  
? The automatically assigned POS tag is 
same with the gold-standard one. 
2) Constituent parsing evaluation 
We selected three commonly-used metrics to 
evaluation the performance of constituent pars-
ing: labeled precision, recall, and F1-score. 
Their computation formulas are as follows: 
? Precision = (sum of correctly labeled 
constituents ) / (sum of parsed constitu-
ents) * 100% 
? Recall = (sum of correctly labeled consti-
tuents) / (sum of gold-standard constitu-
ents) *100% 
? F1-score = 2*P*R / (P+R) 
Two correctness criteria are used for constitu-
ent parsing evaluation: 
? ?B+C? criteria: the boundaries and syn-
tactic tags of the automatically parsed 
constituents must be same with the gold-
standard ones. 
? ?B+C+H? criteria: the boundaries, syntac-
tic tags and head positions of the auto-
matically parsed constituents must be 
same with the gold-standard ones. 
3) Event recognition evaluation 
We only considered the recognition recall of 
each event construction annotated in the event 
bank, due to the current parsing status of Task 2-
1 output.  For each event target verb annotated in 
the event bank, we computed their Micro and 
Macro average recognition recall. The computa-
tion formulas are as follows: 
? Micro Recall = (sum of all correctly rec-
ognized event constructions) / (sum of all 
gold standard event constructions) * 
100% 
? Macro Recall = (sum of Micro-R of each 
event target verb ) / (sum of event target 
verbs in gold-standard set )  
The correctness criteria of event recognition 
should consider following two matching condi-
tions: 
Condition 1: Each event chunk in a gold-
standard event construction should have a cor-
responding constituent in the automatic parse 
tree. For the single-word chunk, the automatical-
ly assigned POS tag should be same with the 
gold standard one. For the multiword chunk, the 
boundary, syntactic tag and head positions of the 
automatically parsed constituent should be same 
with the gold-standard ones. Meanwhile, the cor-
responding constituents should have the same 
layout sequences with the gold standard event 
construction.  
Condition 2: All event-chunk-corresponding 
constituents should have a common ancestor 
node in the parse tree. One of the left and right 
boundaries of the ancestor node should be same 
with the left and right boundaries of the corres-
ponding event construction. 
For Task 2-2, we design two kinds of evalua-
tion metrics: 
1) POS accuracy (POS-A) 
This index is used to evaluate the performance 
of automatic POS tagging. Its formula and cor-
rectness criteria are same with the above defini-
tions of Task 2-1. 
2) Constituent parsing evaluation 
To evaluate the parsing performance of event 
relation recognition in complex Chinese sen-
tences, we firstly divided all parsed constituents 
into following two parts: 
? Constituent of complex sentence (C_S), 
whose tag is ?fj?; 
? Constituents in simple sentence (S_S), 
whose tags are belong to the tag set {dj, 
vp, ap, np, sp, tp, mp, mbar, dp, pp, bp}. 
Then we computed the labeled precision, re-
call and F1-socre of these two parts and obtain 
the arithmetic mean of these two F1-score as the 
final ranking index. Their computation formulas 
of each part are as follows: 
? Precision = (sum of correctly labeled 
constituents in one part) / (sum of parsed 
constituents in the part) * 100% 
? Recall = (sum of correctly labeled consti-
tuents in one part) / (sum of gold-
standard constituents in the part) *100% 
? F1-score = 2*P*R / (P+R) 
? Total F1-Score = (C_S F1 + S_S F1) / 2 
We use the above ?B+C? correctness criteria 
for constituent evaluation in Task 2-2. 
 
ID Participants Task 2-1 Task 2-2 
TPI Open close TPI open Close
01 School of Computer Sci. and Tech., 
Harbin Institute of Technology 
Y   Y  1 
02 Knowledge Engineering Research 
Center, Shenyang Aerospace Univ. 
Y  3 Y  2 
03 Dalian University of Technology Y  1 Y  1 
04 National Laboratory of Pattern Rec-
ognition Institute of Automation, 
Chinese Academy of Science 
Y 2 2 Y 4 2 
05 Beijing University of Posts and Tele-
communications 
Y  2 Y   
06 University of Science and Technolo-
gy of China 
Y   Y   
07 Dept. of Computer Science and 
Technology, Shanghai Jiao Tong 
University, 
Y  3 Y  3 
08 Soochow University Y   Y   
09 Harbin Institute of Technology Y  1 Y   
10 German Research Center for Artifi-
cial Intelligence 
Y 1 1 Y 1  
11 China Center for Information Indus-
try Development 
N   Y 1  
12 City University of Hong Kong Y   Y   
13 National Central University Y   Y   
Total  12 3 13 13 6 9 
Table 3  Result submission data of all participants in Task 2. (TPI=Take Part In)
5 Evaluation results 
The Task 2 of CIPS-Bakeoff-2010 attracted 13 
participants. Almost all of them took part in the 
two subtasks: Task 2-1 and 2-2. Only one partic-
ipant took part in the Task 2-2 subtask alone.  
Among them, 9 participants submitted parsing 
results.  In Task 2-1, we received 16 parsing re-
sults, including 13 close track systems and 3 
open track systems. In Task 2-2, we received 15 
parsing results, including 9 close track systems 
and 6 open track systems. Table 3  shows the 
submission information of all participants of 
Task 2. 
5.1 Task 2-1 analysis 
We evaluated the parsing performance of EDC 
on the constituent and event level respectively. 
The constituent parsing evaluation only consid-
ers the parsing performance of one single consti-
tuent. The event recognition evaluation will con-
sider the recognition performance of a complete 
event construction. So it can provide more useful 
reference information for event extraction appli-
cation. 
Table 5 and Table 6 show the evaluation re-
sults of constituent parsing in the close and open 
tracks respectively. In the close track, the best 
F1-score under ?B+C? criteria is 85.39%, while 
the best F1 score under ?B+C+H? criteria is 
83.66%.  Compared with the evaluation results 
of the task 5 in CIPS-ParEval-2009 under the 
similar training and test conditions (Zhou and Li, 
2009), the performance of head identification is 
improved about 2%. Table 4 shows the detailed 
comparison data. 
Rank ID ?B+C? ?B+C+H? POS-A
09-1 08 87.22 83.70 Gold 
09-2 15 86.25 81.75 Gold 
10-1 02 85.39 83.66 93.96
10-2 04 84.36 82.51 91.84
Table 4 F1 scores of the Top-2 single-model 
close-track systems in the ParsEval-2009 and 
ParsEval-2010. 
Table 7 and Table 8 show the evaluation re-
sults of event recognition in the close and open 
tracks respectively. When we consider the com-
plete event constructions contained in a parse 
tree, the best Macro-Recall is only about 71%. 
There are still lots of room to improve in the fu-
ture.  
 
ID Sys-ID Model ?B+C? ?B+C+H? POS-A Rank
P R F1 P R F1 
02 SAU01 Single 85.42 85.35 85.39 83.69 83.63 83.66 93.96 1 
02 SAU02 Single 85.02 85.11 85.06 83.21 83.31 83.26 93.96 2 
04 a Single 84.40 84.32 84.36 82.55 82.47 82.51 91.84 3 
04 b Single 83.79 83.74 83.76 81.82 81.78 81.80 91.67 4 
10 DFKI_C Single 82.93 82.85 82.89 80.54 80.46 80.50 81.99 5 
02 SAU03 Single 80.28 79.31 79.79 78.55 77.61 78.08 93.93 6 
07 b Single 78.61 78.76 78.69 76.61 76.75 76.68 92.77 7 
07 c Single 77.78 78.13 77.96 75.78 76.13 75.95 92.77 8 
05 BUPT Single 74.86 76.05 75.45 71.06 72.20 71.63 87.00 9 
05 BUPT Multiple 74.48 75.64 75.05 70.72 71.81 71.26 87.00 10 
03 DLUT Single 71.42 71.19 71.30 69.22 69.00 69.11 86.69 11 
09 InsunP Single 70.69 70.48 70.58 67.07 66.87 66.97 77.87 12 
07 a Single 9.09 12.51 10.53 7.17 9.88 8.31 7.02 13 
Table 5 Constituent parsing evaluation results of Task 2-1 (Close Track), ranked with ?B+C+H?- F1 
ID Sys-ID Model ?B+C? ?B+C+H? POS-A Rank
P R F1 P R F1 
04 a Single 86.07 86.08 86.08 84.27 84.28 84.27 92.51 1 
04 b Single 83.79 83.74 83.76 81.82 81.78 81.80 91.67 2 
10 DFKI_C Single 82.37 83.05 82.71 79.99 80.65 80.32 81.87 3 
Table 6 Constituent parsing evaluation results of Task 2-1 (Open Track), ranked with ?B+C+H?- F1 
 
ID Sys-ID Model Micro-R Macro-R POS-A Rank 
02 SAU01 Single 72.47 71.53 93.96 1 
02 SAU02 Single 72.93 70.71 93.96 2 
04 a Single 67.37 65.05 91.84 3 
04 b Single 67.17 64.23 91.67 4 
02 SAU03 Single 63.73 63.54 93.93 5 
07 c Single 63.14 62.48 92.77 6 
07 b Single 62.74 62.47 92.77 7 
10 DFKI_C Single 55.99 53.58 81.99 8 
03 DLUT Single 51.75 53.33 86.69 9 
05 BUPT Single 53.08 48.82 87.00 10 
05 BUPT Multiple 52.88 48.75 87.00 11 
09 InsunP Single 43.15 43.14 77.87 12 
07 a Single 1.13 0.79 7.02 13 
Table 7  Event recognition evaluation results of Task 2-1 (Close Track), ranked with Macro-R 
ID Sys-ID Model Micro-R Macro-R POS-A Rank 
04 a Single 70.62 69.33 92.51 1 
04 b Single 67.17 64.23 91.67 2 
10 DFKI_C Single 54.47 52.25 81.87 3 
Table 8 Event recognition evaluation results of Task 2-1 (Open Track), ranked with Macro-R 
5.2 Task 2-2 analysis 
Table 9 and Table 10 show the evaluation results 
of constituent parsing in the close and open 
tracks of Task 2-2 respectively. In each track, 
the F1-score of the complex sentence recogni-
tion is about 5-6% lower than that of the consti-
tuents in simple sentences. It indicates the diffi-
cultness of event relation recognition in real 
world Chinese sentences. Some new features 
need to be explored for them.  
Almost all the parsing performances of the 
systems in the open track are better than that 
ones in the close track. It indicates some outside 
language resources may useful for parsing per-
formance improvement. Compared with the 
commonly-used English Treebank PTB with 
about 1M words, our current annotated data may 
be not enough to train a good Chinese parser. 
We may need to collect more useful treebank 
data in the future evaluation tasks.  
The F1-scores of constituent parsing in simple 
sentences of Task 2-2 are still about 5-6% lower 
than that of EDC constituents under ?B+C? crite-
ria in Task 2-1. It indicates some lower level 
errors may be propagated to up-level constitu-
ents during complex sentence parsing. How to 
restrict the error propagation chains is an inter-
esting issue need to be explored.  
5.3 POS tagging analysis 
The best POS accuracy in Task 2-1 is 93.96%, 
approaching to the state-of-art performance of 
the Task 1 in CIPS-ParsEval-2009, under similar 
training and test conditions. But the POS accura-
cy in Task 2-2 is about 3-4% lower than it. A 
possible reason is that there are lots of unknown 
words in the test data of Task 2-2. Most of them 
are technical terms outside the training data lex-
icon. How to deal with the unknown words is 
still an open challenge for POS tagging.  
6 Conclusions 
The paper introduced the task designing ideas, 
data preparation methods, evaluation metrics and 
results of the second Chinese syntactic parsing 
evaluation jointed with SIGHAN Bakeoff tasks. 
Some new contributions of the evaluation are 
as follows: 
1) Set a new metric to evaluate the event 
construction recognition performance in 
the constituent parsing tree; 
 
 
 
ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A Rank
P R F1 P R F1 F1 
04 b Single 77.79 77.47 77.63 69.55 76.50 72.86 75.24 88.79 1 
04 a Single 77.91 77.54 77.73 68.47 76.90 72.44 75.08 88.95 2 
O2 SAU01 Single 78.64 78.73 78.69 70.22 71.62 70.91 74.80 91.05 3 
O2 SAU02 Single 78.46 78.34 78.40 69.48 72.42 70.92 74.66 91.03 4 
03 DLUT Single 61.67 59.75 60.69 65.27 67.31 66.27 63.48 79.67 5 
01 CHP Single 70.20 69.64 69.92 53.95 59.47 56.58 63.25 89.62 6 
07 b Single 55.33 59.57 57.37 6.25 0.64 1.16 29.26 89.01 7 
07 c Single 52.57 57.69 55.01 7.47 1.68 2.74 28.88 89.01 8 
07 a Single 0.71 1.00 0.83 0.00 0.00 0.00 0.42 1.39 9 
Table 9  Constituent parsing evaluation results of Task 2-2 (Close Track), ranked with Tot-F1 
(S_S=simple sentence, C_S=complex sentence) 
ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A Rank
P R F1 P R F1 F1 
04 d Single 80.04 79.68 79.86 70.11 76.50 73.17 76.51 89.59 1 
04 a Single 80.27 79.99 80.13 70.36 75.54 72.86 76.50 89.69 2 
04 c Single 80.25 79.95 80.10 70.40 75.30 72.77 76.44 89.78 3 
04 b Single 80.02 79.68 79.85 69.82 75.62 72.60 76.22 89.75 4 
10 DFKI_C Single 79.37 79.27 79.32 71.06 73.22 72.13 75.72 81.23 5 
11* CCID Single / / / / / / / / / 
Table 10 Constituent parsing evaluation results of Task 2-2 (Open Track), ranked with Tot-F1 
(S_S=simple sentence, C_S=complex sentence) There are some data format errors in the submitted 
results of CCID system (ID=11) 
 
2) Set a separated metric to evaluate the 
event relation recognition performance in 
complex Chinese sentence. 
Through this evaluation, we found: 
1) The event construction recognition in a 
Chinese EDC is still a challenge. Some 
new techniques and machine learning 
models need to be explored for this task. 
2) Compared with about 90% F1-score of 
the state-of-art English parser, the 75% 
F1-score of current Chinese parser is still 
on its primitive stage. There is a long way 
to go in the future. 
3) The event relation recognition in real 
world complex Chinese sentences is a dif-
ficult problem. Some new features and 
methods need to be explored for it. 
They lay good foundations for the new task 
designation in the future evaluation round. 
Acknowledgements 
Thanks Li Yemei for her hard work to organize 
the evaluation. Thanks Li Yanjiao and Li Yumei 
for their hard work to prepare the test data for 
the evaluation.  Thanks Zhu Muhua for making 
the evaluation tools and processing all the sub-
mitted data. Thanks all participants of the evalu-
ation. 
The work was also supported by the research 
projects of National Science Foundation of Chi-
na (Grant No. 60573185, 60873173) and National 
863 High-Tech research projects (Grant No. 
2007AA01Z173). 
References 
E. Black, S. Abney, et al 1991. A Procedure for 
Quantitatively Comparing the Syntactic Coverage 
of English Grammars. In Speech and natural lan-
guage: proceedings of a workshop, held at Pacific 
Grove, California, page 306.  
E. Charniak and M. Johnson. 2005. Coarse-to-fine 
nbest parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, page 180. 
S. Clark and J.R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear 
models. Computational Linguistics, 33(4):493?552. 
D. Klein and C. Manning. 2003. Accurate Unlexica-
lized Parsing. In Proc. of ACL-03. 
M. Collins. 2003. Head-driven statistical models for 
natural language parsing. Computational linguis-
tics, 29(4):589?637. 
J. Hockenmaier and M. Steedman. 2007. CCGbank: a 
corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396. 
R. Levy and C. Manning. (2003). Is it harder to parse 
Chinese, or the Chinese Treebank? In Proc. of 
ACL-03. 
J. Li, G. Zhou, and H.T. Ng. 2010. Joint Syntactic 
and Semantic Parsing of Chinese. In Proc. of the 
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1108?1117. 
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 
2005. Non-projective dependency parsing using 
spanning tree algorithms. In Proc. of HLT/EMNLP, 
pages 523?530. 
Mitchell P.Marcus, Mary Ann Marcinkiewicz, and 
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank, 
Computational Linguistics, 19(2): 313-330 
J. Nivre, J. Hall, J. Nilsson, el.al. 2007. Malt-Parser: 
A language-independent system for data driven 
dependency parsing. Natural Language Engineer-
ing, 13(02):95?135. 
S. Petrov and D. Klein. 2007. Improved inference for 
unlexicalized parsing. In Proc. of NAACL HLT 
2007, pages 404?411. 
N. Xue, F. Chiou, and M. Palmer. 2002. Building a 
large-scale annotated Chinese corpus. In Proc. of 
COLING-2002. 
Zhan Weidong, Chang Baobao, Dui Huiming, Zhang 
Huarui. 2006. Recent Developments in Chinese 
Corpus Research. Presented in The 13th NIJL In-
ternational Symposium, Language Corpora: Their 
Compliation and Application. Tokyo, Japan.  
Zhou Qiang, 2004. Chinese Treebank Annotation 
Scheme. Journal of Chinese Information, 18(4):1-
8.  
Zhou Qiang, Li Yuemei. 2009. Evaluation report of 
CIPS-ParsEval-2009.  In Proc. of First Workshop 
on Chinese Syntactic Parsing Evaluation, Beijing 
China.  
Zhou Qiang, Zhu Jingbo. 2009.  Evaluation tasks and 
data preparation of CIPS-ParsEval-2009, 
http://www.ncmmsc.org/ CIPS-ParsEval-20 
 
A Multi-stage Clustering Framework for Chinese Personal 
Name Disambiguation 
    Huizhen Wang, Haibo Ding, Yingchao Shi, Ji Ma,  Xiao Zhou, Jingbo Zhu 
Natural Language Processing Laboratory, 
Northeatern University 
 Shenyang, Liaoning, China 
{wanghuizhen|zhujingbo@mail.neu.edu.cn 
{dinghb|shiyc|maji}@mail.neu.edu.cn 
 
Abstract 
This paper presents our systems for the 
participation of Chinese Personal Name 
Disambiguation task in the CIPS-
SIGHAN 2010. We submitted two dif-
ferent systems for this task, and both of 
them all achieve the best performance. 
This paper introduces the multi-stage 
clustering framework and some key 
techniques used in our systems, and 
demonstrates experimental results on 
evaluation data. Finally, we further dis-
cuss some interesting issues found dur-
ing the development of the system. 
1 Introduction 
Personal name disambiguation (PND) is very 
important for web search and potentially other 
natural language applications such as question 
answering. CIPS-SIGHAN bakeoffs provide a 
platform to evaluate the effectiveness of various 
methods on Chinese PND task.  
Different from English PND, word segmenta-
tion techniques are needed for Chinese PND 
tasks. In practice, person names are highly am-
biguous because different people may have the 
same name, and the same name can be written 
in different ways. It?s an n-to-n mapping of per-
son names to the specific people. There are two 
main challenges on Chinese PND: the first one 
is how to correctly recognize personal names in 
the text, and the other is how to distinguish dif-
ferent persons who have the same name. For 
address these challenges, we designed a rule-
based combination technique to improve NER 
performance and propose a multi-stage cluster-
ing framework for Chinese PND. We partici-
pated in the bakeoff of the Chinese PND task, 
on the test set and the diagnosis test set, our two 
systems are ranked at the 1st and 2nd position. 
The rest of this paper is organized as follows. 
In Section 2, we first give the key features and 
techniques used in our two systems. In Section 
3, experimental results on the evaluation test 
data demonstrated that our methods are effec-
tive to disambiguate the personal name, and 
discussions on some issues we found during the 
development of the system are given. In Section 
4, we conclude our work. 
2 System Description 
In this section, we describe the framework of 
our systems in more detail, involving data pre-
processing, discard-class document identifica-
tion, feature definition, clustering algorithms, 
and sub-system combination. 
2.1 Data Preprocessing 
There are around 100-300 news articles per per-
sonal name in the evaluation corpus. Each arti-
cle is stored in the form of XML and encoded in 
UTF-8. At first, each news article should be 
preprocessed as follows: 
 Use a publicly available Chinese encoding 
Converter tool to convert each news article 
from UTF-8 coding into GB1; 
 Remove all XML tags; 
 Process Chinese word segmentation, part-
of-speech (POS) tagging and name entity 
recognition (NER); 
The performance of word segmentation and 
NER tools generally affect the effectiveness of 
our Chinese PND systems. During system de-
                                                 
1
 http://www.mandarintools.com/ 
veloping process, we found that the publicly 
available NER systems obtain unsatisfactory 
performance on evaluation data. To address this 
challenge, we propose a new rule-based combi-
nation technique to improve NER performance. 
In our combination framework, two different 
NER systems are utilized, including a CRF-
based NER system and our laboratory?s NER 
system (Yao et al,2002). The latter was imple-
mented based on the maximum matching prin-
ciple and some linguistic post-preprocessing 
rules. Since both two NER systems adopt dif-
ferent technical frameworks, it is possible to 
achieve a better performance by means of sys-
tem combination techniques.  
The basic idea of our combination method is 
to first simply combine the results produced by 
both NER systems, and further utilize some 
heuristic post-processing rules to refine NE 
identification results. To achieve this goal, we 
first investigate error types caused by both NER 
systems, and design some post-preprocessing 
rules to correct errors or select the appropriate 
NER results from disagreements. Notice that 
such rules are learned from sample data (i.e., 
training set), not from test set. Experimental 
results demonstrate satisfactory NER perform-
ance by introducing these heuristic refinement 
rules as follows:  
 Conjunction Rules. Two NEs separated 
by a conjunction (such as ???,???,???, 
??? ) belong to the same type, e.g., ???
/adj.?/???/person?. Such a conjunc-
tion rule can help NER systems make a 
consistent prediction on both NEs, e.g., ??
?/person? and ???/person?.  
 Professional Title Rules. Professional title 
words such as ???? are strong indicators 
of person names, e.g., ???/???. Such a 
rule can be written in the form of ?profes-
sional_title+person_name?.  
 Suffix Rules. If an identified person name 
is followed by a suffix of another type of 
named entities such as location, it is not a 
true person name, for example, ?????
???/person ?/?/???. Since ??? is 
a suffix of a location name. ??????
??/person ?/location-suffix? should be 
revised to be a new location name, namely 
?????????/location?. 
 Foreign Person Name Rules. Two identi-
fied person names connected by a dot are 
merged into a single foreign person name, 
e.g., ??/./???? => ??.???? 
 Chinese Surname Rules. Surnames are 
very important for Chinese person name 
identification. However, some common 
surnames can be single words depending 
upon the context, for example, the Chinese 
word ??? can be either a surname or a 
quantifier. To tackle this problem, some 
post-processing rules for ??, ?, ?, ?, 
?? are designed in our system. 
 Query-Dependent Rules. Given a query 
person name A, if the string AB occurring 
in the current document has been identified 
as a single person name many times in 
other documents, our system would tend to 
segment AB as a single person name rather 
than as A/B. For example, if ????? was 
identified as a true person name more than 
one time in other documents, in such a case, 
???/??/?/??/?=> ???/???
/person??/? 
Incorporating these above post-processing 
rules, our NER system based on heuristic post-
processing rules shows 98.89% precision of 
NER on training set.  
2.2 Discard-Class Document Identification 
Seen from evaluation data, there are a lot of 
documents belonging to a specific class, re-
ferred to as discard-class. In the discard-class, 
the query person name occurring in the docu-
ment is not a true person name. For example, a 
query word ???? is a famous ocean name not 
a person name in the sentence ???????
???????????????. In such a 
case, the corresponding document is considered 
as discard-class. Along this line, actually the 
discard-class document identification is very 
simple task. If a document does not contain a 
true person name that is the same as the query 
or contains the query, it is a discard-class 
document.  
2.3 Feature Definition 
To identify different types of person name and 
for the PND purpose, some effective binary fea-
tures are defined to construct the document rep-
resentation as feature vectors as follows: 
 Personal attributes: involving profes-
sional title, affiliation, location, co-
occurrence person name and organization 
related to the given query.   
 NE-type Features: collecting all NEs oc-
curring in the context of the given query. 
There are two kinds of NE-type features 
used in our systems, local features and 
global features. The global features are de-
fined with respect to the whole document 
while the local features are extracted only 
from the two or three adjacent sentences 
for the given query.  
 BOW-type features: constructing the con-
text feature vector based on bag-of-word 
model. Similarly, there are local and global 
BOW-type features with respect to the con-
text considered.  
2.4 A Multi-stage Clustering Framework  
Seen from the training set, 36% of person 
names indicate journalists, 10% are sportsmen, 
and the remaining are common person names. 
Based on such observations, it is necessary to 
utilize different methodology to PND on differ-
ent types of person names, for example, because 
the most effective information to distinguish 
different journalists are the reports? location and 
colleagues, instead of the whole document con-
tent. To achieve a satisfactory PND perform-
ance, in our system we design three different 
modules for analyzing journalist, sportsman and 
common person name, respectively.  
2.4.1 PND on the Journalist Class 
In our system, some regular expressions are 
designed to determine whether a person name is 
a journalist or not. For example: 
 ??? /ni */ns */t */t ?? |? /n (/w .* 
[?/w */ni ?/w ]* query name/nh .*)/w 
 (/w .*query name/nh .*)/w 
 [*/nh]* query name/nh [*/nh] 
 ? ? | ? ? /n [*/nh]* query name/nh 
[*/nh]* 
To disambiguate on the journalist class, our 
system utilizes a rule-based clustering technique 
distinguish different journalists. For each 
document containing the query person name as 
journalists, we first extract the organization and 
the location occurring in the local context of the 
query. Two such documents can be put into the 
same cluster if they contain the same organiza-
tion or location names, otherwise not. In our 
system, a location dictionary containing prov-
ince-city information extracted from Wikipedia 
is used to identify location name. For example: 
??? (?? ?? ?? ?? ?), ??(??
? ?? ??? ?? ???). Based on this 
dictionary, it is very easy to map a city to its 
corresponding province.  
2.4.2 PND on the Sportsman Class 
Like done in PND on the journalist class, we 
also use rule-based clustering techniques for 
disambiguating sportsman class. The major dif-
ference is to utilize topic features for PND on 
the sportsman class. If the topic of the given 
document is sports, this document can be con-
sidered as sportsman class. The key is to how to 
automatically identify the topic of the document 
containing the query. To address this challenge, 
we adopt a domain knowledge based technique 
for document topic identification. The basic 
idea is to utilize a domain knowledge dictionary 
NEUKD developed by our lab, which contains 
more than 600,000 domain associated terms and 
the corresponding domain features. Some do-
main associated terms defined in NEUKD are 
shown in Table 1.  
 
Domain associated term Domain feature concept 
???(football team) Football, Sports 
???? 
(cycling team) Traffic, Sports, cycling 
???? 
(Chinese chess) Sports, Chinese chess 
??(white side) Sports, the game of go 
????? 
(Chicago bulls) Sports, basketball 
 
Table 1: Six examples defined in the NEUKD 
 
In the domain knowledge based topic identi-
fication algorithm, all domain associated terms 
occurring in the given document are first 
mapped into domain features such as football, 
basketball or cycling. The most frequent do-
main feature is considered as the most likely 
topic. See Zhu and Chen (2005) for details. 
Two documents with the same topic can be 
grouped into the same cluster.  
 
 
Table 2: Examples of PND on Sportsman Class 
 
2.4.3 Multi-Stage Clustering Framework 
We proposed a multi-stage clustering frame-
work for PND on common person name class, 
as shown In Figure 1.  
In the multi-stage clustering framework, the 
first-stage is to adopt strict rule-based hard clus-
tering algorithm using the feature set of per-
sonal attributes. The second-stage is to imple-
ment constrained hierarchical agglomerative 
clustering using NE-type local features. The 
third-stage is to design hierarchical agglomera-
tive clustering using BOW-type global features. 
By combining those above techniques, we sub-
mitted the first system named NEU_1. 
2.4.4 The second system 
Besides, we also submitted another PND system 
named NEU_2 by using the single-link hierar-
chical agglomerative clustering algorithm in 
which the distance of two clusters is the cosine 
similarity of their most similar members (Ma-
saki et al, 2009, Duda et al, 2004). The differ-
ence between our two submission systems 
NEU_1 and NEU_2 is the feature weighting 
method. The motivation of feature weighting 
method used in NEU_2 is to assume that words 
surrounding the query person name in the given 
document are more important features than 
those far away from it, and person name and 
location names occurring in the context are 
more discriminative features than common 
words for PND purpose. Along this line, in the 
feature weighting scheme used in NEU_2, for 
each feature extracted from the sentence con-
taining the query person name, the weight of a 
word-type feature with the POS of ?ns?, ?ni? 
or ?nh ? is assigned  as 3, Otherwise 1.5; For 
the features extracted from other sentences, the 
weight of a word with the POS of ?ns?or ?nh ? 
is set to be 2, the ones of ?ni? POS is set to 1.5, 
otherwise 1.0. 
 
Algorithm 1: Multi-stage Clustering Framework 
Input: a person name pn, and its related document 
set D={d1, d2, ?, dm} in which each document di 
contains the person name pn; 
Output: clustering results C={C1,C2, ?,Cn}, where 
CCi =?i
 and ?=? ji CC  
For each di?D do 
 Si = {s|pn?s, s?di}; 
ORGi={t|t?s, s?Si, POS(t)= ni}; 
PERi={t|t?s, s?Si, POS(t)=nh} ; 
Ldi = {t|t?s, s?Si }; //local feature set 
Gdi = {t|t?di}; //global feature set 
Ci = {di} ; 
End for 
Stage 1: Strict rules-based clustering 
 Begin 
 For each Ci ? C do 
If ??? ji ORGORG or 
2?? ji PERPER  
Then Ci = Ci ?Cj;  
ORGi = ORGi?ORGj ; 
PERi = PERi?PERj ; 
Remove Cj from C ; 
End for 
End  
Stage 2: Constrained hierarchical agglomerative 
clustering algorithm using local features 
Begin  
         Set each c ?C as an initial cluster; 
 do  
),(maxarg],[
,
ji
CCC
ji CCsimCC
ji ?
=  
),cos(max
),(max),(
,
,
yxjyix
jyix
ddCdCd
yxCdCdji
LL
ddsimCCsim
??
??
=
=
 
Ci = Ci ?Cj; 
Remove Cj from C ; 
        until  sim(Ci,Cj) < ?. 
End 
Stage 3: Constrained hierarchical agglomerative 
clustering algorithm using global features, i.e., util-
ize the same algorithm used in stage 2 by consider-
ing the global feature set G for cosine-based similar-
ity calculation instead of the local feature set L. 
 
Figure 1: Multi-stage Clustering Framework 
Person name Document no. sports 
?? 081 ?? 
?? 094 ?? 
?? 098 ?? 
?? 100 ?? 
2.5 Final Result Generation 
As discussed above, there are many modules for 
PND on Chinese person name. In our NEU_1, 
the final results are produced by combining 
outputs of discard-class document clustering, 
journalist-class clustering, sportsman-class 
clustering and multi-stage clustering modules. 
In NEU-2 system, the outputs of discard-class 
document clustering, journalist-class clustering, 
sportsman-class clustering and single-link 
clustering modules are combined to generate 
the final results.  
3 Evaluation 
3.1 Experimental Settings 
 Training data: containing about 30 Chinese 
person names, and a set of about 100-300 
news articles are provided for each person 
name.  
 Test data: similar to the training data, and 
containing 26 unseen Chinese personal 
names, provided by the SIGHAN organizer.  
 Performance evaluation metrics (Artiles et 
al., 2009): B_Cubed and P_IP metrics. 
3.2 Results 
Table 3 shows the performance of our two 
submission systems NEU_1 and NEU_2 on the 
test set of Sighan2010 Chinese personal name 
disambiguation task. 
  
B_Cubed P_IP System 
No. P R F P IP F 
NEU_1 95.76 88.37 91.47 96.99 92.58 94.56 
NEU_2 95.08 88.62 91.15 96.73 92.73 94.46 
 
Table 3: Results on the test data 
 
NEU-1 system was implemented by the 
multi-stage clustering framework that uses sin-
gle-link clustering method. In this framework, 
there are two threshold parameters ? and ?. 
Both threshold parameters are tuned from train-
ing data sets.  
After the formal evaluation, the organizer 
provided a diagnosis test designed to explore 
the relationship between Chinese word segmen-
tation and personal name disambiguation. In the 
diagnosis test, the personal name disambigua-
tion task was simplified and limited to the 
documents in which the personal name is 
tagged correctly. The performance of our two 
systems on the diagnosis test set of Sighan2010 
Chinese personal name disambiguation task are 
shown in Table 4. 
 
B_Cubed P_IP System 
no. P R F  P IP F  
NEU_1 95.6 89.74 92.14 96.83 93.62 95.03 
NEU_2 94.53 89.99 91.66 96.41 93.8 94.9 
 
Table 4: Results of the diagnosis test on test 
data 
 
As shown in the Table 3 and Table 4, NEU-1 
system achieves the highest precision and F 
values on the test data and the diagnosis test 
data. 
3.3 Discussion 
We propose a multi-stage clustering framework 
for Chinese personal name disambiguation. The 
evaluation results demonstrate that the features 
and key techniques our systems adopt are effec-
tive. Our systems achieve the best performance 
in this competition. However, our recall values 
are not unsatisfactory. In such a case, there is 
still much room for improvement. Observed 
from experimental results, some interesting is-
sues are worth being discussed and addressed in 
our future work as follows: 
(1) For PND on some personal names, the 
document topic information seems not effective. 
For example, the personal name "?? (Guo 
Hua)" in training set represent one shooter and 
one billiards player. The PND system based on 
traditional clustering method can not effectively 
work in such a case due to the same sports topic. 
To solve this problem, one solution is to suffi-
ciently combine the personal attributes and 
document topic information for PND on this 
person name. 
(2) For the journalist-class personal names, 
global BOW-type features are not effective in 
this case as different persons can report on the 
same or similar events. For example, there are 
four different journalists named ????(Zhu 
Jianjun)? in the training set, involving different 
locations such as Beijing, Zhengzhou, Xining or 
Guangzhou. We can distinguish them in terms 
of the location they are working in.  
(3) We found that some documents in the 
training set only contain lists of news title and 
the news reporter. In this case, we can not dis-
criminate the persons with respect to the loca-
tion of entire news. It?s worth studying some 
effective solution to address this challenge in 
our future work.  
(4) Seen from the experimental results, some 
personal names such as ???(Li gang)? are 
wrong identified because this person is associ-
ated with multiple professional titles and affili-
ates. In this case, the use of exact matching 
methods can not yield satisfactory results. For 
example, the query name ???(Li gang)? in 
the documents 274 and 275 is the president of  
???????????(China International 
Culture Association)? while in the documents 
202, 225 and 228, he is the director of ????
???????(Bureau of External Cultural 
Relations of Chinese Ministry of Culture)?. To 
group both cases into the same cluster, it?s 
worth mining the relations and underlying se-
mantic relations between entities to achieve this 
goal.  
 
4 Conclusion 
This paper presents our two Chinese personal 
name disambiguation systems in which various 
constrained hierarchical agglomerative cluster-
ing algorithms using local or global features are 
adopted. The bakeoff results show that our sys-
tems achieve the best performance. In the future, 
we will pay more attention on the personal at-
tribute extraction and unsupervised learning 
approaches for Chinese personal name disam-
biguation.  
5 Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and 
the Fundamental Research Funds for the Cen-
tral Universities. 
 
References 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 
2009. ?WePS 2 Evaluation Campaign: overview of 
the Web People Search Clustering Task,? In 2nd 
Web People Search Evaluation Workshop (WePS 
2009), 18th WWW Conference. 
Duda, Richard O., Peter E.Hart, and David G.Stork. 
2004. Pattern Classification. China Machine Press. 
Masaki, Ikeda, Shingo Ono, Issei Sato, Minoru Yo-
shida, and Hiroshi Nakagawa. 2009. Person Name 
Disambiguation on the Web by TwoStage Clustering. 
In 2nd Web People Search Evaluation Workshop 
(WePS 2009), 18th WWW Conference. 
Yao, Tianshun, Zhu Jingbo , Zhang Li, Yang Ying. 
Nov. 2002. Natural Language Processing , Second 
Edition, Tsinghua press. 
Zhu, Jingbo and Wenliang Chen. 2005. Some Stud-
ies on Chinese Domain Knowledge Dictionary and 
Its Application to Text Classification. In Proc. of 
SIGHAN4. 
 
 
 
NEUNLPLab Chinese Word Sense Induction System for 
SIGHAN Bakeoff 2010 
Hao Zhang Tong Xiao Jingbo Zhu 
1. Key Laboratory of Medical Image Computing (Northeastern University), Ministry 
of Education 
2. Natural Language Processing Laboratory, Northeastern University 
zhanghao1216@gmail.com 
{xiaotong, zhujingbo}@mail.neu.edu.cn 
 
Abstract 
This paper describes a character-based 
Chinese word sense induction (WSI) sys-
tem for the International Chinese Lan-
guage Processing Bakeoff 2010. By 
computing the longest common sub-
strings between any two contexts of the 
ambiguous word, our system extracts 
collocations as features and does not de-
pend on any extra tools, such as Chinese 
word segmenters. We also design a con-
strained clustering algorithm for this task. 
Experiemental results show that our sys-
tem could achieve 69.88 scores of 
FScore on the development data set of 
SIGHAN Bakeoff 2010. 
1 Introduction 
The goal of word sense induction (WSI) is to 
group occurrences containing a given ambiguous 
word into clusters with respect to sense. Most 
researchers take the problem of word sense in-
duction as a clustering problem. Pantel & Lin 
(2002) clustered words on the basis of the dis-
tances of their co-occurrence vectors, and used 
global clustering as a solution. Neill (2002) used 
local clustering, and determined the senses of a 
given word by clustering its close associations. 
In this paper, we propose a simple but effec-
tive method to extract collocations as features 
from texts without pre-segmentations, and de-
sign a constrained clustering algorithm to ad-
dress the issue of Chinese word sense induction. 
By using our collocation extraction method, our 
Chinese WSI system is independent of any extra 
natural language processing tools, such as Chi-
nese word segmenters. On the development set 
of SIGHAN 2010 WSI task, the experimental 
results show that our system could achieve 69.88 
scores of FScore. In addition, the official results 
show that the performance of our system is 
67.15 scores of FScore on the test set of 
SIGHAN Bakeoff 2010. 
The rest of this paper is organized as follows. 
In Section 2, we present the task description of 
Chinese word sense induction. In Section 3, we 
first give an overview of our Chinese WSI sys-
tem, and then propose our feature extraction 
method and constrained clustering algorithm. In 
Section 4, we describe the evaluation method 
and show the experimental results on the devel-
opment and test data sets of the Bakeoff 2010. In 
Section 5, we conclude our work. 
2 Task Description 
Given the number of senses S and occurrences of 
the ambiguous word w, a word sense induction 
system is supposed to cluster the occurrences 
into S clusters, with each cluster representing a 
sense of the ambiguous word w. For example, 
suppose that there are some sentences containing 
the ambiguous word ???? (gloomy), and the 
sense number S is 2, the job of WSI system is to 
cluster these sentences into 2 clusters, with each 
cluster representing a sense of ????. Based on 
this task description, it is obvious to regard the 
problem of WSI as a clustering problem. 
Figures 1-2 shows example input and output 
of our WSI system , where there are 6 sentences 
and 2 resulting clusters. In Figure 1, the first 
column are the identifiers of sentences contain-
ing the word ????, and the second column are 
part of the sentences. In Figure 2, the first col-
umn represents the identifiers of sentences, and 
the second column represents the identifiers of 
clusters generated by our Chinese WSI system. 
 
Figure 1 Part of input of word ???? for our 
WSI system 
 
Figure 2 Output of our WSI system for word 
???? 
3 NEU Chinese WSI System 
3.1 System overview 
Our Chinese word sense induction system is 
built based on clustering work-frame. There are 
four major modules in the system, including 
data pre-processing, feature extraction, cluster-
ing and data post-processing modules. The ar-
chitecture of our Chinese WSI system is illus-
trated in Figure 3. 
3.2 Feature extraction 
Since there is no separators in Chinese like 
?space? in English to mark word boundaries, 
most Chinese natural language processing appli-
cations need to first apply a Chinese word seg-
menter to segment Chinese sentences. In our 
Chinese word sense induction system, we extract 
collocations from sentences containing the am-
biguous word as features. To extract collocations, 
we might first segment the sentences into word 
sequences, and then conduct feature extraction 
on the word-segmented corpus. However, errors 
might be induced in the procedure due to un-
avoidable incorrect segmentation results. Ad-
dressing this issue, we propose a method to di-
rectly extract collocations from sentences with-
out pre-segmentations. 
In our method, we extract two kinds of collo-
cations, namely ?global collocation? and ?local 
collocation?. Here global collocations are de-
fined to be the words (or character sequences) 
that frequently co-occur with the ambiguous 
word, and local collocations are defined to be 
the characters adjacent to the ambiguous word1. 
 
Figure 3 Architecture of our system 
To extract global collocations, we first com-
pute all the longest common substrings between 
any two of the sentences containing the ambigu-
ous word to form the set of candidate global col-
locations. For each candidate global collocation, 
we count the number of sentences containing it. 
We then reduce the size of the candidate set by 
eliminating candidates which contain only one 
character or functional words. We also remove 
the candidate with other candidates as its sub-
strings. Finally, we eliminate the candidates 
whose count of the number of sentences is below 
a certain threshold. The threshold equals to two 
in our experiments. We regard the candidates 
after the above processing as global collocations 
for WSI. 
To extract local collocations, we simply ex-
tract one character on both left and right sides of 
the ambiguous word to form the set of candidate 
local collocations. We then refine the candidate 
set by eliminating candidates which are func-
tional words or whose frequency is below a cer-
tain threshold. The threshold is set to two in our 
experiments. 
After extracting global collocations and local 
collocations, we put them together to form the 
                                                 
1
 Definitions of global collocation and local collocation 
might be different from those in other papers. 
start 
data pre-processing 
feature extraction 
clustering 
data post-processing 
end 
final set of collocations and use them as features 
of our system. For each collocation (or feature), 
we compute the list of indices of sentences that 
containing the collocation. Thus, every element 
of the set of collocations has the data structure of 
pair of ?key? and ?value?, where ?key? is the 
collocation itself, and the ?value? is the list of 
indices. 
3.3 Clustering algorithm 
We find that the high-confidence collocation is a 
very good indicator to distinguish the senses of 
an ambiguous word. However, the traditional 
clustering methods are based on the vector rep-
resentations of features, which probably de-
creases the effect of dominant features (i.e. high-
confidence collocations). To alleviate the prob-
lem, a nice way is to incorporate collocations 
into the clustering process as constraints. Moti-
vated by this idea, we design a constrained clus-
tering algorithm. In this algorithm, we could en-
sure that some occurrences of the ambiguous 
word must be in one cluster and some must not 
be in one cluster. The input for our constrained 
clustering algorithm is the set of collocations 
described in the previous section and the process 
of our clustering algorithm is shown in Table 1. 
Here the notation starting with character ?C? 
represents a collocation, and the notations of 
?Sin? and ?Srlt? represent the collocation set and 
the result set, respectively. 
Every element in the result set Srlt is regarded 
as one cluster for a given ambigous word, and 
the list of the element records the indices of the 
sentences belonging to the cluster. 
4 Evaluation of Our System 
The evaluation method is F-score which is pro-
vided within the Bakeoff 2010 (Zhao and 
Karypis, 2005). Suppose Cr is a class of the gold 
standard, and Si is a cluster of our system gener-
ated. FScore is computed with the formulas be-
low. 
( , ) 2 * * / ( )F score Cr Si P R P R? = +          (1) 
( ) max( ( , ))
Si
FScore Cr F score Cr Si= ?         (2) 
1
( )
c
r
nr
FScore FScore Cr
n
=
=?                (3) 
We evaluate our Chinese word sense induc-
tion system on the development data set and the 
test data set of the Bakeoff 2010. The details of 
the development data set and the test data set are 
summarized in Table 2. 
For comparison, we develop a baseline system 
that also uses the collocations as features and 
clustering based on the vector representations of 
features. On the development data set, we test 
our system and compare it with the baseline sys-
tem. The performance of our Chinese WSI sys-
tem and the baseline system are shown in Table 
3. From Table 3, we see that using our con-
strained clustering algorithm is better than using 
the traditional hierarchical clustering methods by 
7.06 scores of FScore for our Chinese WSI sys-
tem. It indicates that our constrained clustering 
algorithm could avoid reducing the effect of  
Input: collocation set Sin 
while there is available collocation Ci in the 
input set Sin 
 for each collocation Ct in the set Sin 
 if Ct not equals to Ci, and Ct is avail-
able 
 if list of Ct has intersection with 
that of Ci, or Ct and Ci have a 
meaningful substring (word or 
character), compose list of Ct into 
list of Ci, and mark Ct to be un-
available 
 end if 
 end if 
 end for 
 store Ci and its list into result set Srlt, and 
mark Ci to be unavailable 
end while 
if there are available collocations in the input 
set Sin 
 if the size of result set Srlt does not sat-
isfy the given cluster number, devide the 
rest collocations in Sin evenly into the 
rest clusters, and append their lists to 
their own clusters? lists respectively 
 else add the rest collocations into the last 
cluster, and append their list to the list of 
the last cluster 
 end if 
end if 
return the result set Srlt 
Output: result set Srlt 
Table 1 Constrained clustering algorithm 
high-confidence features (i.e. high-confidence 
collocations) and lead to better clustering results. 
This conclusion is also ensured by the compari-
son between our constrained clustering algo-
rithm and the traditional K-means clustering al-
gorithm. 
In addition, our system achieves 67.15 scores 
of FScore on the test data set reported by the 
SIGHAN Bakeoff 2010. 
data descriptions 
Dev set 
containing 50 ambiguous words, 
about 50 sentences for each am-
biguous word 
Test set 
containing 100 ambiguous words, 
about 50 sentences for each am-
biguous word 
Table 2 Data sets of SIGHAN Bakeoff 2010 
clustering methods 
FScore of 
our system 
(%) 
traditional hierarchical cluster-
ing 62.82 
traditional K-means clustering 62.48 
our constrained clustering 69.88 
Table 3 System performance on dev set of 
Bakeoff 2010 using different clustering methods 
5 Conclusions 
In this paper, we propose a collocation extrac-
tion method and a constrained clustering algo-
rithm for Chinese WSI task. By using the collo-
cation extraction method and the clustering algo-
rithm, our Chinese word sense induction system 
is independent of any extra tools. When tested 
on the test data set of the Bakeoff 2010, our sys-
tem achieves 67.15 scores of FScore. 
References 
Vickrey, David, Luke Biewald, Marc Teyssler, and 
Daphne Koller. 2005. Word-sense disambiguation 
for machine translation. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, 
Morristown, NJ, USA, pages 771-778. 
Yarowsky, David. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of 33rd Meeting of the Association for 
Computational Linguistics, Cambridge, MA, 189-
196. 
Schutze, Hinrich. 1998. Automatic word sense dis-
crimination. Computational Linguistics, Montreal, 
Canada, 24(1):97?123. 
Ng, Hwee Tou, Hian Beng Lee. 1996. Integrating 
Multiple Knowledge Sources to Disambiguate 
Word Sense: An Exemplar-Based Approach. In 
Proceedings of the 34th Meeting of the Association 
for Computational Linguistics, California, USA, 
pages 40-47. 
Daniel, Neill. 2002. Fully Automatic Word Sense 
Induction by Semantic Clustering. In Computer 
Speech, Cambridge University, Master?s Thesis. 
Pantel, Patrick, Dekang Lin. 2002. Discovering word 
senses from text. In Proceedings of ACM SIGKDD, 
Edmonton, 613-619. 
Rapp, Reinhard. 2004. A Practical Solution to the 
Problem of Automatic Word Sense Induction. In 
Proceedings of the 42nd Meeting of the Association 
for Computational Linguistics, Barcelona, Spain. 
Zhao, Ying, George Karypis. 2005. Hierarchical 
Clustering Algorithms for Document Datasets. 
Data Mining and Knowledge Discovery, 10:141-
168. 
