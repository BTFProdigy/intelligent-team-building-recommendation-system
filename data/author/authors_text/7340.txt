Determining Term Subjectivity and Term Orientation for Opinion Mining
Andrea Esuli1 and Fabrizio Sebastiani2
(1) Istituto di Scienza e Tecnologie dell?Informazione ? Consiglio Nazionale delle Ricerche
Via G Moruzzi, 1 ? 56124 Pisa, Italy
andrea.esuli@isti.cnr.it
(2) Dipartimento di Matematica Pura e Applicata ? Universita` di Padova
Via GB Belzoni, 7 ? 35131 Padova, Italy
fabrizio.sebastiani@unipd.it
Abstract
Opinion mining is a recent subdiscipline
of computational linguistics which is con-
cerned not with the topic a document is
about, but with the opinion it expresses.
To aid the extraction of opinions from
text, recent work has tackled the issue
of determining the orientation of ?subjec-
tive? terms contained in text, i.e. decid-
ing whether a term that carries opinion-
ated content has a positive or a negative
connotation. This is believed to be of key
importance for identifying the orientation
of documents, i.e. determining whether a
document expresses a positive or negative
opinion about its subject matter.
We contend that the plain determination
of the orientation of terms is not a realis-
tic problem, since it starts from the non-
realistic assumption that we already know
whether a term is subjective or not; this
would imply that a linguistic resource that
marks terms as ?subjective? or ?objective?
is available, which is usually not the case.
In this paper we confront the task of de-
ciding whether a given term has a positive
connotation, or a negative connotation, or
has no subjective connotation at all; this
problem thus subsumes the problem of de-
termining subjectivity and the problem of
determining orientation. We tackle this
problem by testing three different variants
of a semi-supervised method previously
proposed for orientation detection. Our
results show that determining subjectivity
and orientation is a much harder problem
than determining orientation alone.
1 Introduction
Opinion mining is a recent subdiscipline of com-
putational linguistics which is concerned not with
the topic a document is about, but with the opinion
it expresses. Opinion-driven content management
has several important applications, such as deter-
mining critics? opinions about a given product by
classifying online product reviews, or tracking the
shifting attitudes of the general public toward a po-
litical candidate by mining online forums.
Within opinion mining, several subtasks can be
identified, all of them having to do with tagging a
given document according to expressed opinion:
1. determining document subjectivity, as in de-
ciding whether a given text has a factual na-
ture (i.e. describes a given situation or event,
without expressing a positive or a negative
opinion on it) or expresses an opinion on its
subject matter. This amounts to performing
binary text categorization under categories
Objective and Subjective (Pang and Lee,
2004; Yu and Hatzivassiloglou, 2003);
2. determining document orientation (or polar-
ity), as in deciding if a given Subjective text
expresses a Positive or a Negative opinion
on its subject matter (Pang and Lee, 2004;
Turney, 2002);
3. determining the strength of document orien-
tation, as in deciding e.g. whether the Posi-
tive opinion expressed by a text on its subject
matter is Weakly Positive, Mildly Positive,
or Strongly Positive (Wilson et al, 2004).
To aid these tasks, recent work (Esuli and Se-
bastiani, 2005; Hatzivassiloglou and McKeown,
1997; Kamps et al, 2004; Kim and Hovy, 2004;
Takamura et al, 2005; Turney and Littman, 2003)
has tackled the issue of identifying the orientation
of subjective terms contained in text, i.e. determin-
ing whether a term that carries opinionated content
has a positive or a negative connotation (e.g. de-
ciding that ? using Turney and Littman?s (2003)
examples ? honest and intrepid have a
positive connotation while disturbing and
superfluous have a negative connotation).
193
This is believed to be of key importance for iden-
tifying the orientation of documents, since it is
by considering the combined contribution of these
terms that one may hope to solve Tasks 1, 2 and 3
above. The conceptually simplest approach to this
latter problem is probably Turney?s (2002), who
has obtained interesting results on Task 2 by con-
sidering the algebraic sum of the orientations of
terms as representative of the orientation of the
document they belong to; but more sophisticated
approaches are also possible (Hatzivassiloglou and
Wiebe, 2000; Riloff et al, 2003; Wilson et al,
2004).
Implicit in most works dealing with term orien-
tation is the assumption that, for many languages
for which one would like to perform opinion min-
ing, there is no available lexical resource where
terms are tagged as having either a Positive or a
Negative connotation, and that in the absence of
such a resource the only available route is to gen-
erate such a resource automatically.
However, we think this approach lacks real-
ism, since it is also true that, for the very same
languages, there is no available lexical resource
where terms are tagged as having either a Subjec-
tive or an Objective connotation. Thus, the avail-
ability of an algorithm that tags Subjective terms
as being either Positive or Negative is of little
help, since determining if a term is Subjective is
itself non-trivial.
In this paper we confront the task of de-
termining whether a given term has a Pos-
itive connotation (e.g. honest, intrepid),
or a Negative connotation (e.g. disturbing,
superfluous), or has instead no Subjective
connotation at all (e.g. white, triangular);
this problem thus subsumes the problem of decid-
ing between Subjective and Objective and the
problem of deciding between Positive and Neg-
ative. We tackle this problem by testing three dif-
ferent variants of the semi-supervised method for
orientation detection proposed in (Esuli and Se-
bastiani, 2005). Our results show that determining
subjectivity and orientation is a much harder prob-
lem than determining orientation alone.
1.1 Outline of the paper
The rest of the paper is structured as follows. Sec-
tion 2 reviews related work dealing with term ori-
entation and/or subjectivity detection. Section 3
briefly reviews the semi-supervised method for
orientation detection presented in (Esuli and Se-
bastiani, 2005). Section 4 describes in detail three
different variants of it we propose for determining,
at the same time, subjectivity and orientation, and
describes the general setup of our experiments. In
Section 5 we discuss the results we have obtained.
Section 6 concludes.
2 Related work
2.1 Determining term orientation
Most previous works dealing with the properties
of terms within an opinion mining perspective
have focused on determining term orientation.
Hatzivassiloglou and McKeown (1997) attempt
to predict the orientation of subjective adjectives
by analysing pairs of adjectives (conjoined by
and, or, but, either-or, or neither-nor)
extracted from a large unlabelled document set.
The underlying intuition is that the act of conjoin-
ing adjectives is subject to linguistic constraints
on the orientation of the adjectives involved; e.g.
and usually conjoins adjectives of equal orienta-
tion, while but conjoins adjectives of opposite
orientation. The authors generate a graph where
terms are nodes connected by ?equal-orientation?
or ?opposite-orientation? edges, depending on the
conjunctions extracted from the document set. A
clustering algorithm then partitions the graph into
a Positive cluster and a Negative cluster, based
on a relation of similarity induced by the edges.
Turney and Littman (2003) determine term ori-
entation by bootstrapping from two small sets of
subjective ?seed? terms (with the seed set for Pos-
itive containing terms such as good and nice,
and the seed set for Negative containing terms
such as bad and nasty). Their method is based
on computing the pointwise mutual information
(PMI) of the target term t with each seed term
ti as a measure of their semantic association.
Given a target term t, its orientation value O(t)
(where positive value means positive orientation,
and higher absolute value means stronger orien-
tation) is given by the sum of the weights of its
semantic association with the seed positive terms
minus the sum of the weights of its semantic as-
sociation with the seed negative terms. For com-
puting PMI, term frequencies and co-occurrence
frequencies are measured by querying a document
set by means of the AltaVista search engine1 with
a ?t? query, a ?ti? query, and a ?t NEAR ti? query,
and using the number of matching documents re-
turned by the search engine as estimates of the
probabilities needed for the computation of PMI.
Kamps et al (2004) consider instead the graph
defined on adjectives by the WordNet2 synonymy
relation, and determine the orientation of a target
1http://www.altavista.com/
2http://wordnet.princeton.edu/
194
adjective t contained in the graph by comparing
the lengths of (i) the shortest path between t and
the seed term good, and (ii) the shortest path be-
tween t and the seed term bad: if the former is
shorter than the latter, than t is deemed to be Pos-
itive, otherwise it is deemed to be Negative.
Takamura et al (2005) determine term orienta-
tion (for Japanese) according to a ?spin model?,
i.e. a physical model of a set of electrons each
endowed with one between two possible spin di-
rections, and where electrons propagate their spin
direction to neighbouring electrons until the sys-
tem reaches a stable configuration. The authors
equate terms with electrons and term orientation
to spin direction. They build a neighbourhood ma-
trix connecting each pair of terms if one appears in
the gloss of the other, and iteratively apply the spin
model on the matrix until a ?minimum energy?
configuration is reached. The orientation assigned
to a term then corresponds to the spin direction as-
signed to electrons.
The system of Kim and Hovy (2004) tackles ori-
entation detection by attributing, to each term, a
positivity score and a negativity score; interest-
ingly, terms may thus be deemed to have both a
positive and a negative correlation, maybe with
different degrees, and some terms may be deemed
to carry a stronger positive (or negative) orienta-
tion than others. Their system starts from a set
of positive and negative seed terms, and expands
the positive (resp. negative) seed set by adding to
it the synonyms of positive (resp. negative) seed
terms and the antonyms of negative (resp. positive)
seed terms. The system classifies then a target
term t into either Positive or Negative by means
of two alternative learning-free methods based on
the probabilities that synonyms of t also appear in
the respective expanded seed sets. A problem with
this method is that it can classify only terms that
share some synonyms with the expanded seed sets.
Kim and Hovy also report an evaluation of human
inter-coder agreement. We compare this evalua-
tion with our results in Section 5.
The approach we have proposed for determin-
ing term orientation (Esuli and Sebastiani, 2005)
is described in more detail in Section 3, since it
will be extensively used in this paper.
All these works evaluate the performance of
the proposed algorithms by checking them against
precompiled sets of Positive and Negative terms,
i.e. checking how good the algorithms are at clas-
sifying a term known to be subjective into either
Positive or Negative. When tested on the same
benchmarks, the methods of (Esuli and Sebastiani,
2005; Turney and Littman, 2003) have performed
with comparable accuracies (however, the method
of (Esuli and Sebastiani, 2005) is much more effi-
cient than the one of (Turney and Littman, 2003)),
and have outperformed the method of (Hatzivas-
siloglou and McKeown, 1997) by a wide margin
and the one by (Kamps et al, 2004) by a very
wide margin. The methods described in (Hatzi-
vassiloglou and McKeown, 1997) is also limited
by the fact that it can only decide the orientation
of adjectives, while the method of (Kamps et al,
2004) is further limited in that it can only work
on adjectives that are present in WordNet. The
methods of (Kim and Hovy, 2004; Takamura et
al., 2005) are instead difficult to compare with the
other ones since they were not evaluated on pub-
licly available datasets.
2.2 Determining term subjectivity
Riloff et al (2003) develop a method to determine
whether a term has a Subjective or an Objective
connotation, based on bootstrapping algorithms.
The method identifies patterns for the extraction
of subjective nouns from text, bootstrapping from
a seed set of 20 terms that the authors judge to be
strongly subjective and have found to have high
frequency in the text collection from which the
subjective nouns must be extracted. The results
of this method are not easy to compare with the
ones we present in this paper because of the dif-
ferent evaluation methodologies. While we adopt
the evaluation methodology used in all of the pa-
pers reviewed so far (i.e. checking how good our
system is at replicating an existing, independently
motivated lexical resource), the authors do not test
their method on an independently identified set of
labelled terms, but on the set of terms that the algo-
rithm itself extracts. This evaluation methodology
only allows to test precision, and not accuracy tout
court, since no quantification can be made of false
negatives (i.e. the subjective terms that the algo-
rithm should have spotted but has not spotted). In
Section 5 this will prevent us from drawing com-
parisons between this method and our own.
Baroni and Vegnaduzzo (2004) apply the PMI
method, first used by Turney and Littman (2003)
to determine term orientation, to determine term
subjectivity. Their method uses a small set Ss
of 35 adjectives, marked as subjective by human
judges, to assign a subjectivity score to each adjec-
tive to be classified. Therefore, their method, un-
like our own, does not classify terms (i.e. take firm
classification decisions), but ranks them according
to a subjectivity score, on which they evaluate pre-
cision at various level of recall.
195
3 Determining term subjectivity and
term orientation by semi-supervised
learning
The method we use in this paper for determining
term subjectivity and term orientation is a variant
of the method proposed in (Esuli and Sebastiani,
2005) for determining term orientation alone.
This latter method relies on training, in a semi-
supervised way, a binary classifier that labels
terms as either Positive or Negative. A semi-
supervised method is a learning process whereby
only a small subset L ? Tr of the training data
Tr are human-labelled. In origin the training
data in U = Tr ? L are instead unlabelled; it
is the process itself that labels them, automati-
cally, by using L (with the possible addition of
other publicly available resources) as input. The
method of (Esuli and Sebastiani, 2005) starts from
two small seed (i.e. training) sets Lp and Ln of
known Positive and Negative terms, respectively,
and expands them into the two final training sets
Trp ? Lp and Trn ? Ln by adding them new sets
of terms Up and Un found by navigating the Word-
Net graph along the synonymy and antonymy re-
lations3. This process is based on the hypothesis
that synonymy and antonymy, in addition to defin-
ing a relation of meaning, also define a relation of
orientation, i.e. that two synonyms typically have
the same orientation and two antonyms typically
have opposite orientation. The method is iterative,
generating two sets Trkp and Trkn at each iteration
k, where Trkp ? Trk?1p ? . . . ? Tr1p = Lp
and Trkn ? Trk?1n ? . . . ? Tr1n = Ln. At
iteration k, Trkp is obtained by adding to Trk?1p
all synonyms of terms in Trk?1p and all antonyms
of terms in Trk?1n ; similarly, Trkn is obtained by
adding to Trk?1n all synonyms of terms in Trk?1n
and all antonyms of terms in Trk?1p . If a total ofK
iterations are performed, then Tr = TrKp ? TrKn .
The second main feature of the method pre-
sented in (Esuli and Sebastiani, 2005) is that terms
are given vectorial representations based on their
WordNet glosses (i.e. textual definitions). For
each term ti in Tr ? Te (Te being the test set, i.e.
the set of terms to be classified), a textual represen-
tation of ti is generated by collating all the glosses
of ti as found in WordNet4. Each such represen-
3Several other WordNet lexical relations, and several
combinations of them, are tested in (Esuli and Sebastiani,
2005). In the present paper we only use the best-performing
such combination, as described in detail in Section 4.2. The
version of WordNet used here and in (Esuli and Sebastiani,
2005) is 2.0.
4In general a term ti may have more than one gloss, since
tation is converted into vectorial form by standard
text indexing techniques (in (Esuli and Sebastiani,
2005) and in the present work, stop words are
removed and the remaining words are weighted
by cosine-normalized tf idf ; no stemming is per-
formed)5. This representation method is based on
the assumption that terms with a similar orienta-
tion tend to have ?similar? glosses: for instance,
that the glosses of honest and intrepid will
both contain appreciative expressions, while the
glosses of disturbing and superfluous
will both contain derogative expressions. Note
that this method allows to classify any term, in-
dependently of its POS, provided there is a gloss
for it in the lexical resource.
Once the vectorial representations for all terms
in Tr?Te have been generated, those for the terms
in Tr are fed to a supervised learner, which thus
generates a binary classifier. This latter, once fed
with the vectorial representations of the terms in
Te, classifies each of them as either Positive or
Negative.
4 Experiments
In this paper we extend the method of (Esuli and
Sebastiani, 2005) to the determination of term sub-
jectivity and term orientation altogether.
4.1 Test sets
The benchmark (i.e. test set) we use for our exper-
iments is the General Inquirer (GI) lexicon (Stone
et al, 1966). This is a lexicon of terms labelled
according to a large set of categories6, each one
denoting the presence of a specific trait in the
term. The two main categories, and the ones we
will be concerned with, are Positive/Negative,
which contain 1,915/2,291 terms having a posi-
tive/negative orientation (in what follows we will
also refer to the category Subjective, which we
define as the union of the two categories Positive
and Negative). In opinion mining research the GI
was first used by Turney and Littman (2003), who
reduced the list of terms to 1,614/1,982 entries af-
it may have more than one sense; dictionaries normally asso-
ciate one gloss to each sense.
5Several combinations of subparts of a WordNet gloss are
tested as textual representations of terms in (Esuli and Sebas-
tiani, 2005). Of all those combinations, in the present paper
we always use the DGS? combination, since this is the one
that has been shown to perform best in (Esuli and Sebastiani,
2005). DGS? corresponds to using the entire gloss and per-
forming negation propagation on its text, i.e. replacing all the
terms that occur after a negation in a sentence with negated
versions of the term (see (Esuli and Sebastiani, 2005) for de-
tails).
6The definitions of all such categories are available at
http://www.webuse.umd.edu:9090/
196
ter removing 17 terms appearing in both categories
(e.g. deal) and reducing all the multiple entries
of the same term in a category, caused by multi-
ple senses, to a single entry. Likewise, we take
all the 7,582 GI terms that are not labelled as ei-
ther Positive or Negative, as being (implicitly)
labelled as Objective, and reduce them to 5,009
terms after combining multiple entries of the same
term, caused by multiple senses, to a single entry.
The effectiveness of our classifiers will thus be
evaluated in terms of their ability to assign the to-
tal 8,605 GI terms to the correct category among
Positive, Negative, and Objective7.
4.2 Seed sets and training sets
Similarly to (Esuli and Sebastiani, 2005), our
training set is obtained by expanding initial seed
sets by means of WordNet lexical relations. The
main difference is that our training set is now
the union of three sets of training terms Tr =
TrKp ?TrKn ?TrKo obtained by expanding, through
K iterations, three seed sets Tr1p, T r1n, T r1o , one
for each of the categories Positive, Negative, and
Objective, respectively.
Concerning categories Positive and Negative,
we have used the seed sets, expansion policy, and
number of iterations, that have performed best in
the experiments of (Esuli and Sebastiani, 2005),
i.e. the seed sets Tr1p = {good} and Tr1n =
{bad} expanded by using the union of synonymy
and indirect antonymy, restricting the relations
only to terms with the same POS of the original
terms (i.e. adjectives), for a total of K = 4 itera-
tions. The final expanded sets contain 6,053 Pos-
itive terms and 6,874 Negative terms.
Concerning the category Objective, the pro-
cess we have followed is similar, but with a few
key differences. These are motivated by the fact
that the Objective category coincides with the
complement of the union of Positive and Neg-
ative; therefore, Objective terms are more var-
ied and diverse in meaning than the terms in the
other two categories. To obtain a representative
expanded set TrKo , we have chosen the seed set
Tr1o = {entity} and we have expanded it by
using, along with synonymy and antonymy, the
WordNet relation of hyponymy (e.g. vehicle /
car), and without imposing the restriction that the
two related terms must have the same POS. These
choices are strictly related to each other: the term
entity is the root term of the largest generaliza-
tion hierarchy in WordNet, with more than 40,000
7We make this labelled term set available for download at
http://patty.isti.cnr.it/?esuli/software/
SentiGI.tgz.
terms (Devitt and Vogel, 2004), thus allowing to
reach a very large number of terms by using the
hyponymy relation8. Moreover, it seems reason-
able to assume that terms that refer to entities are
likely to have an ?objective? nature, and that hy-
ponyms (and also synonyms and antonyms) of an
objective term are also objective. Note that, at
each iteration k, a given term t is added to Trko
only if it does not already belong to either Trp or
Trn. We experiment with two different choices
for the Tro set, corresponding to the sets gener-
ated in K = 3 and K = 4 iterations, respectively;
this yields sets Tr3o and Tr4o consisting of 8,353
and 33,870 training terms, respectively.
4.3 Learning approaches and evaluation
measures
We experiment with three ?philosophically? dif-
ferent learning approaches to the problem of dis-
tinguishing between Positive, Negative, and Ob-
jective terms.
Approach I is a two-stage method which con-
sists in learning two binary classifiers: the first
classifier places terms into either Subjective or
Objective, while the second classifier places
terms that have been classified as Subjective by
the first classifier into either Positive or Negative.
In the training phase, the terms in TrKp ? TrKn are
used as training examples of category Subjective.
Approach II is again based on learning two bi-
nary classifiers. Here, one of them must discrim-
inate between terms that belong to the Positive
category and ones that belong to its complement
(not Positive), while the other must discriminate
between terms that belong to the Negative cate-
gory and ones that belong to its complement (not
Negative). Terms that have been classified both
into Positive by the former classifier and into (not
Negative) by the latter are deemed to be positive,
and terms that have been classified both into (not
Positive) by the former classifier and into Nega-
tive by the latter are deemed to be negative. The
terms that have been classified (i) into both (not
Positive) and (not Negative), or (ii) into both
Positive and Negative, are taken to be Objec-
tive. In the training phase of Approach II, the
terms in TrKn ? TrKo are used as training exam-
ples of category (not Positive), and the terms in
TrKp ? TrKo are used as training examples of cat-
egory (not Negative).
Approach III consists instead in viewing Posi-
tive, Negative, and Objective as three categories
8The synonymy relation connects instead only 10,992
terms at most (Kamps et al, 2004).
197
with equal status, and in learning a ternary clas-
sifier that classifies each term into exactly one
among the three categories.
There are several differences among these three
approaches. A first difference, of a conceptual
nature, is that only Approaches I and III view
Objective as a category, or concept, in its own
right, while Approach II views objectivity as a
nonexistent entity, i.e. as the ?absence of subjec-
tivity? (in fact, in Approach II the training exam-
ples of Objective are only used as training exam-
ples of the complements of Positive and Nega-
tive). A second difference is that Approaches I and
II are based on standard binary classification tech-
nology, while Approach III requires ?multiclass?
(i.e. 1-of-m) classification. As a consequence,
while for the former we use well-known learn-
ers for binary classification (the naive Bayesian
learner using the multinomial model (McCallum
and Nigam, 1998), support vector machines us-
ing linear kernels (Joachims, 1998), the Roc-
chio learner, and its PrTFIDF probabilistic version
(Joachims, 1997)), for Approach III we use their
multiclass versions9.
Before running our learners we make a pass of
feature selection, with the intent of retaining only
those features that are good at discriminating our
categories, while discarding those which are not.
Feature selection is implemented by scoring each
feature fk (i.e. each term that occurs in the glosses
of at least one training term) by means of the mu-
tual information (MI) function, defined as
MI(fk) =
?
c?{c1,...,cm},
f?{fk,fk}
Pr(f, c) ? log Pr(f, c)Pr(f) Pr(c) (1)
and discarding the x% features fk that minimize
it. We will call x% the reduction factor. Note that
the set {c1, . . . , cm} from Equation 1 is interpreted
differently in Approaches I to III, and always con-
sistently with who the categories at stake are.
Since the task we aim to solve is manifold, we
will evaluate our classifiers according to two eval-
uation measures:
? SO-accuracy, i.e. the accuracy of a classifier
in separating Subjective from Objective, i.e.
in deciding term subjectivity alone;
? PNO-accuracy, the accuracy of a classifier
in discriminating among Positive, Negative,
9The naive Bayesian, Rocchio, and PrTFIDF learners
we have used are from Andrew McCallum?s Bow package
(http://www-2.cs.cmu.edu/?mccallum/bow/),
while the SVMs learner we have used is Thorsten Joachims?
SV M light (http://svmlight.joachims.org/),
version 6.01. Both packages allow the respective learners to
be run in ?multiclass? fashion.
Table 1: Average and best accuracy values over
the four dimensions analysed in the experiments.
Dimension SO-accuracy PNO-accuracy
Avg (?) Best Avg (?) Best
Approach
I .635 (.020) .668 .595 (.029) .635
II .636 (.033) .676 .614 (.037) .660
III .635 (.036) .674 .600 (.039) .648
Learner
NB .653 (.014) .674 .619 (.022) .647
SVMs .627 (.033) .671 .601 (.037) .658
Rocchio .624 (.030) .654 .585 (.033) .616
PrTFIDF .637 (.031) .676 .606 (.042) .660
TSR
0% .649 (.025) .676 .619 (.027) .660
50% .650 (.022) .670 .622 (.022) .657
80% .646 (.023) .674 .621 (.021) .647
90% .642 (.024) .667 .616 (.024) .651
95% .635 (.027) .671 .606 (.031) .658
99% .612 (.036) .661 .570 (.049) .647
TrKo set
Tr3o .645 (.006) .676 .608 (.007) .658
Tr4o .633 (.013) .674 .610 (.018) .660
and Objective, i.e. in deciding both term ori-
entation and subjectivity.
5 Results
We present results obtained from running every
combination of (i) the three approaches to classifi-
cation described in Section 4.3, (ii) the four learn-
ers mentioned in the same section, (iii) five dif-
ferent reduction factors for feature selection (0%,
50%, 90%, 95%, 99%), and (iv) the two different
training sets (Tr3o and Tr4o) for Objective men-
tioned in Section 4.2. We discuss each of these
four dimensions of the problem individually, for
each one reporting results averaged across all the
experiments we have run (see Table 1).
The first and most important observation is that,
with respect to a pure term orientation task, ac-
curacy drops significantly. In fact, the best SO-
accuracy and the best PNO-accuracy results ob-
tained across the 120 different experiments are
.676 and .660, respectively (these were obtained
by using Approach II with the PrTFIDF learner
and no feature selection, with Tro = Tr3o for the
.676 SO-accuracy result and Tro = Tr4o for the
.660 PNO-accuracy result); this contrasts sharply
with the accuracy obtained in (Esuli and Sebas-
tiani, 2005) on discriminating Positive from Neg-
ative (where the best run obtained .830 accuracy),
on the same benchmarks and essentially the same
algorithms. This suggests that good performance
at orientation detection (as e.g. in (Esuli and Se-
bastiani, 2005; Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003)) may not be a
198
Table 2: Human inter-coder agreement values re-
ported by Kim and Hovy (2004).
Agreement Adjectives (462) Verbs (502)
measure Hum1 vs Hum2 Hum2 vs Hum3
Strict .762 .623
Lenient .890 .851
guarantee of good performance at subjectivity de-
tection, quite evidently a harder (and, as we have
suggested, more realistic) task.
This hypothesis is confirmed by an experiment
performed by Kim and Hovy (2004) on testing
the agreement of two human coders at tagging
words with the Positive, Negative, and Objec-
tive labels. The authors define two measures of
such agreement: strict agreement, equivalent to
our PNO-accuracy, and lenient agreement, which
measures the accuracy at telling Negative against
the rest. For any experiment, strict agreement val-
ues are then going to be, by definition, lower or
equal than the corresponding lenient ones. The au-
thors use two sets of 462 adjectives and 502 verbs,
respectively, randomly extracted from the basic
English word list of the TOEFL test. The inter-
coder agreement results (see Table 2) show a de-
terioration in agreement (from lenient to strict) of
16.77% for adjectives and 36.42% for verbs. Fol-
lowing this, we evaluated our best experiment ac-
cording to these measures, and obtained a ?strict?
accuracy value of .660 and a ?lenient? accuracy
value of .821, with a relative deterioration of
24.39%, in line with Kim and Hovy?s observa-
tion10. This confirms that determining subjectivity
and orientation is a much harder task than deter-
mining orientation alone.
The second important observation is that there
is very little variance in the results: across all 120
experiments, average SO-accuracy and PNO-
accuracy results were .635 (with standard devia-
tion ? = .030) and .603 (? = .036), a mere
6.06% and 8.64% deterioration from the best re-
sults reported above. This seems to indicate that
the levels of performance obtained may be hard to
improve upon, especially if working in a similar
framework.
Let us analyse the individual dimensions of the
problem. Concerning the three approaches to clas-
sification described in Section 4.3, Approach II
outperforms the other two, but by an extremely
narrow margin. As for the choice of learners, on
average the best performer is NB, but again by a
very small margin wrt the others. On average, the
10We observed this trend in all of our experiments.
best reduction factor for feature selection turns out
to be 50%, but the performance drop we witness
in approaching 99% (a dramatic reduction factor)
is extremely graceful. As for the choice of TrKo ,
we note that Tr3o and Tr4o elicit comparable levels
of performance, with the former performing best
at SO-accuracy and the latter performing best at
PNO-accuracy.
An interesting observation on the learners we
have used is that NB, PrTFIDF and SVMs, un-
like Rocchio, generate classifiers that depend on
P (ci), the prior probabilities of the classes, which
are normally estimated as the proportion of train-
ing documents that belong to ci. In many classi-
fication applications this is reasonable, as we may
assume that the training data are sampled from the
same distribution from which the test data are sam-
pled, and that these proportions are thus indica-
tive of the proportions that we are going to en-
counter in the test data. However, in our appli-
cation this is not the case, since we do not have a
?natural? sample of training terms. What we have
is one human-labelled training term for each cat-
egory in {Positive,Negative,Objective}, and as
many machine-labelled terms as we deem reason-
able to include, in possibly different numbers for
the different categories; and we have no indica-
tion whatsoever as to what the ?natural? propor-
tions among the three might be. This means that
the proportions of Positive, Negative, and Ob-
jective terms we decide to include in the train-
ing set will strongly bias the classification results
if the learner is one of NB, PrTFIDF and SVMs.
We may notice this by looking at Table 3, which
shows the average proportion of test terms classi-
fied as Objective by each learner, depending on
whether we have chosen Tro to coincide with Tr3o
or Tr4o ; note that the former (resp. latter) choice
means having roughly as many (resp. roughly five
times as many) Objective training terms as there
are Positive and Negative ones. Table 3 shows
that, the more Objective training terms there are,
the more test terms NB, PrTFIDF and (in partic-
ular) SVMs will classify as Objective; this is not
true for Rocchio, which is basically unaffected by
the variation in size of Tro.
6 Conclusions
We have presented a method for determining both
term subjectivity and term orientation for opinion
mining applications. This is a valuable advance
with respect to the state of the art, since past work
in this area had mostly confined to determining
term orientation alone, a task that (as we have ar-
199
Table 3: Average proportion of test terms classi-
fied as Objective, for each learner and for each
choice of the TrKo set.
Learner Tr3o Tr4o Variation
NB .564 (? = .069) .693 (.069) +23.0%
SVMs .601 (.108) .814 (.083) +35.4%
Rocchio .572 (.043) .544 (.061) -4.8%
PrTFIDF .636 (.059) .763 (.085) +20.0%
gued) has limited practical significance in itself,
given the generalized absence of lexical resources
that tag terms as being either Subjective or Ob-
jective. Our algorithms have tagged by orienta-
tion and subjectivity the entire General Inquirer
lexicon, a complete general-purpose lexicon that
is the de facto standard benchmark for researchers
in this field. Our results thus constitute, for this
task, the first baseline for other researchers to im-
prove upon.
Unfortunately, our results have shown that
an algorithm that had shown excellent, state-
of-the-art performance in deciding term orienta-
tion (Esuli and Sebastiani, 2005), once modified
for the purposes of deciding term subjectivity, per-
forms more poorly. This has been shown by test-
ing several variants of the basic algorithm, some
of them involving radically different supervised
learning policies. The results suggest that decid-
ing term subjectivity is a substantially harder task
that deciding term orientation alone.
References
M. Baroni and S. Vegnaduzzo. 2004. Identifying subjec-
tive adjectives through Web-based mutual information. In
Proceedings of KONVENS-04, 7th Konferenz zur Verar-
beitung Natu?rlicher Sprache (German Conference on Nat-
ural Language Processing), pages 17?24, Vienna, AU.
Ann Devitt and Carl Vogel. 2004. The topology of WordNet:
Some metrics. In Proceedings of GWC-04, 2nd Global
WordNet Conference, pages 106?111, Brno, CZ.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms through gloss analy-
sis. In Proceedings of CIKM-05, 14th ACM International
Conference on Information and Knowledge Management,
pages 617?624, Bremen, DE.
Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of ACL-97, 35th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 174?181,
Madrid, ES.
Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sentence
subjectivity. In Proceedings of COLING-00, 18th Inter-
national Conference on Computational Linguistics, pages
174?181, Saarbru?cken, DE.
Thorsten Joachims. 1997. A probabilistic analysis of the
Rocchio algorithm with TFIDF for text categorization. In
Proceedings of ICML-97, 14th International Conference
on Machine Learning, pages 143?151, Nashville, US.
Thorsten Joachims. 1998. Text categorization with support
vector machines: learning with many relevant features. In
Proceedings of ECML-98, 10th European Conference on
Machine Learning, pages 137?142, Chemnitz, DE.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten
De Rijke. 2004. Using WordNet to measure semantic ori-
entation of adjectives. In Proceedings of LREC-04, 4th In-
ternational Conference on Language Resources and Eval-
uation, volume IV, pages 1115?1118, Lisbon, PT.
Soo-Min Kim and Eduard Hovy. 2004. Determining the sen-
timent of opinions. In Proceedings of COLING-04, 20th
International Conference on Computational Linguistics,
pages 1367?1373, Geneva, CH.
Andrew K. McCallum and Kamal Nigam. 1998. A compari-
son of event models for naive Bayes text classification. In
Proceedings of the AAAI Workshop on Learning for Text
Categorization, pages 41?48, Madison, US.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL-04, 42nd
Meeting of the Association for Computational Linguistics,
pages 271?278, Barcelona, ES.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern boot-
strapping. In Proceedings of CONLL-03, 7th Conference
on Natural Language Learning, pages 25?32, Edmonton,
CA.
P. J. Stone, D. C. Dunphy, M. S. Smith, and D. M. Ogilvie.
1966. The General Inquirer: A Computer Approach to
Content Analysis. MIT Press, Cambridge, US.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting emotional polarity of words using spin
model. In Proceedings of ACL-05, 43rd Annual Meeting
of the Association for Computational Linguistics, pages
133?140, Ann Arbor, US.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Sys-
tems, 21(4):315?346.
Peter Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of re-
views. In Proceedings of ACL-02, 40th Annual Meeting
of the Association for Computational Linguistics, pages
417?424, Philadelphia, US.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? Finding strong and weak opinion
clauses. In Proceedings of AAAI-04, 21st Conference of
the American Association for Artificial Intelligence, pages
761?769, San Jose, US.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from opin-
ions and identifying the polarity of opinion sentences. In
Proceedings of EMNLP-03, 8th Conference on Empiri-
cal Methods in Natural Language Processing, pages 129?
136, Sapporo, JP.
200
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 424?431,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
PageRanking WordNet Synsets:
An Application to Opinion Mining?
Andrea Esuli and Fabrizio Sebastiani
Istituto di Scienza e Tecnologie dell?Informazione
Consiglio Nazionale delle Ricerche
Via Giuseppe Moruzzi, 1 ? 56124 Pisa, Italy
{andrea.esuli,fabrizio.sebastiani}@isti.cnr.it
Abstract
This paper presents an application of PageR-
ank, a random-walk model originally de-
vised for ranking Web search results, to
ranking WordNet synsets in terms of how
strongly they possess a given semantic prop-
erty. The semantic properties we use for ex-
emplifying the approach are positivity and
negativity, two properties of central impor-
tance in sentiment analysis. The idea derives
from the observation that WordNet may be
seen as a graph in which synsets are con-
nected through the binary relation ?a term
belonging to synset sk occurs in the gloss
of synset si?, and on the hypothesis that
this relation may be viewed as a transmit-
ter of such semantic properties. The data
for this relation can be obtained from eX-
tended WordNet, a publicly available sense-
disambiguated version of WordNet. We ar-
gue that this relation is structurally akin to
the relation between hyperlinked Web pages,
and thus lends itself to PageRank analysis.
We report experimental results supporting
our intuitions.
1 Introduction
Recent years have witnessed an explosion of work
on opinion mining (aka sentiment analysis), the dis-
?This work was partially supported by Project ONTOTEXT
?From Text to Knowledge for the Semantic Web?, funded by
the Provincia Autonoma di Trento under the 2004?2006 ?Fondo
Unico per la Ricerca? funding scheme.
cipline that deals with the quantitative and qualita-
tive analysis of text for the purpose of determining
its opinion-related properties (ORPs). An important
part of this research has been the work on the auto-
matic determination of the ORPs of terms, as e.g.,
in determining whether an adjective tends to give a
positive, a negative, or a neutral nature to the noun
phrase it appears in. While many works (Esuli and
Sebastiani, 2005; Hatzivassiloglou and McKeown,
1997; Kamps et al, 2004; Takamura et al, 2005;
Turney and Littman, 2003) view the properties of
positivity and negativity as categorical (i.e., a term is
either positive or it is not), others (Andreevskaia and
Bergler, 2006b; Grefenstette et al, 2006; Kim and
Hovy, 2004; Subasic and Huettner, 2001) view them
as graded (i.e., a term may be positive to a certain
degree), with the underlying interpretation varying
from fuzzy to probabilistic.
Some authors go a step further and attach these
properties not to terms but to term senses (typ-
ically: WordNet synsets), on the assumption that
different senses of the same term may have dif-
ferent opinion-related properties (Andreevskaia and
Bergler, 2006a; Esuli and Sebastiani, 2006b; Ide,
2006; Wiebe and Mihalcea, 2006).
In this paper we contribute to this latter literature
with a novel method for ranking the entire set of
WordNet synsets, irrespectively of POS, according
to their ORPs. Two rankings are produced, one ac-
cording to positivity and one according to negativity.
The two rankings are independent, i.e., it is not the
case that one is the inverse of the other, since e.g.,
the least positive synsets may be negative or neutral
synsets alike.
424
The main hypothesis underlying our method is
that the positivity and negativity of WordNet synsets
can be determined by mining their glosses. It
crucially relies on the observation that the gloss
of a WordNet synset contains terms that them-
selves belong to synsets, and on the hypothesis that
the glosses of positive (resp. negative) synsets will
mostly contain terms belonging to positive (nega-
tive) synsets. This means that the binary relation
si I sk (?the gloss of synset si contains a term
belonging to synset sk?), which induces a directed
graph on the set of WordNet synsets, may be thought
of as a channel through which positivity and nega-
tivity flow, from the definiendum (the synset si be-
ing defined) to the definiens (a synset sk that con-
tributes to the definition of si by virtue of its member
terms occurring in the gloss of si). In other words,
if a synset si is known to be positive (negative), this
can be viewed as an indication that the synsets sk to
which the terms occurring in the gloss of si belong,
are themselves positive (negative).
We obtain the data of the I relation from eX-
tended WordNet (Harabagiu et al, 1999), an auto-
matically sense-disambiguated version of WordNet
in which every term occurrence in every gloss is
linked to the synset it is deemed to belong to.
In order to compute how polarity flows in the
graph of WordNet synsets we use the well known
PageRank algorithm (Brin and Page, 1998). PageR-
ank, a random-walk model for ranking Web search
results which lies at the basis of the Google search
engine, is probably the most important single contri-
bution to the fields of information retrieval and Web
search of the last ten years, and was originally de-
vised in order to detect how authoritativeness flows
in the Web graph and how it is conferred onto Web
sites. The advantages of PageRank are its strong
theoretical foundations, its fast convergence proper-
ties, and the effectiveness of its results. The reason
why PageRank, among all random-walk algorithms,
is particularly suited to our application will be dis-
cussed in the rest of the paper.
Note however that our method is not limited to
ranking synsets by positivity and negativity, and
could in principle be applied to the determination of
other semantic properties of synsets, such as mem-
bership in a domain, since for many other properties
we may hypothesize the existence of a similar ?hy-
draulics? between synsets. We thus see positivity
and negativity only as proofs-of-concept for the po-
tential of the method.
The rest of the paper is organized as follows. Sec-
tion 2 reports on related work on the ORPs of lex-
ical items, highlighting the similarities and differ-
ences between the discussed methods and our own.
In Section 3 we turn to discussing our method; in or-
der to make the paper self-contained, we start with
a brief introduction of PageRank (Section 3.1) and
of the structure of eXtended WordNet (Section 3.2).
Section 4 describes the structure of our experiments,
while Section 5 discusses the results we have ob-
tained, comparing them with other results from the
literature. Section 6 concludes.
2 Related work
Several works have recently tackled the automated
determination of term polarity. Hatzivassiloglou and
McKeown (1997) determine the polarity of adjec-
tives by mining pairs of conjoined adjectives from
text, and observing that conjunctions such as and
tend to conjoin adjectives of the same polarity while
conjunctions such as but tend to conjoin adjectives
of opposite polarity. Turney and Littman (2003) de-
termine the polarity of generic terms by computing
the pointwise mutual information (PMI) between the
target term and each of a set of ?seed? terms of
known positivity or negativity, where the marginal
and joint probabilities needed for PMI computation
are equated to the fractions of documents from a
given corpus that contain the terms, individually or
jointly. Kamps et al (2004) determine the polarity
of adjectives by checking whether the target adjec-
tive is closer to the term good or to the term bad
in the graph induced on WordNet by the synonymy
relation. Kim and Hovy (2004) determine the po-
larity of generic terms by means of two alternative
learning-free methods that use two sets of seed terms
of known positivity and negativity, and are based
on the frequency with which synonyms of the target
term also appear in the respective seed sets. Among
these works, (Turney and Littman, 2003) has proven
by far the most effective, but it is also by far the most
computationally intensive.
Some recent works have employed, as in the
present paper, the glosses from online dictionar-
425
ies for term polarity detection. Andreevskaia and
Berger (2006a) extend a set of terms of known pos-
itivity/negativity by adding to them all the terms
whose glosses contain them; this algorithm does not
view glosses as a source for a graph of terms, and
is based on a different intuition than ours. Esuli
and Sebastiani (2005; 2006a) determine the ORPs of
generic terms by learning, in a semi-supervised way,
a binary term classifier from a set of training terms
that have been given vectorial representations by in-
dexing their WordNet glosses. The same authors
later extend their work to determining the ORPs
of WordNet synsets (Esuli and Sebastiani, 2006b).
However, there is a substantial difference between
these works and the present one, in that the former
simply view the glosses as sources of textual repre-
sentations for the terms/synsets, and not as inducing
a graph of synsets as we instead view them here.
The work closest in spirit to the present one is
probably that by Takamura et al (2005), who de-
termine the polarity of terms by applying intuitions
from the theory of electron spins: two terms that ap-
pear one in the gloss of the other are viewed as akin
to two neighbouring electrons, which tend to acquire
the same ?spin? (a notion viewed as akin to polarity)
due to their being neighbours. This work is simi-
lar to ours since a graph between terms is generated
from dictionary glosses, and since an iterative algo-
rithm that converges to a stable state is used, but the
algorithm is very different, and based on intuitions
from very different walks of life.
Some recent works have tackled the attribution
of opinion-related properties to word senses or
synsets (Ide, 2006; Wiebe and Mihalcea, 2006)1;
however, they do not use glosses in any significant
way, and are thus very different from our method.
The interested reader may also consult (Mihalcea,
2006) for other applications of random-walk models
to computational linguistics.
3 Ranking WordNet synsets by PageRank
3.1 The PageRank algorithm
Let G = ?N,L? be a directed graph, with N its set
of nodes and L its set of directed links; let W0 be
1Andreevskaia and Berger (2006a) also work on term
senses, rather than terms, but they evaluate their work on terms
only. This is the reason why they are listed in the preceding
paragraph and not here.
the |N | ? |N | adjacency matrix of G, i.e., the ma-
trix such that W0[i, j] = 1 iff there is a link from
node ni to node nj . We will denote by B(i) =
{nj |W0[j, i] = 1} the set of the backward neigh-
bours of ni, and by F (i) = {nj |W0[i, j] = 1}
the set of the forward neighbours of ni. Let W be
the row-normalized adjacency matrix of G, i.e., the
matrix such that W[i, j] = 1|F (i)| iff W0[i, j] = 1
and W[i, j] = 0 otherwise.
The input to PageRank is the row-normalized ad-
jacency matrix W, and its output is a vector a =
?a1, . . . , a|N |?, where ai represents the ?score? of
node ni. When using PageRank for search results
ranking, ni is a Web site and ai measures its com-
puted authoritativeness; in our application ni is in-
stead a synset and ai measures the degree to which
ni has the semantic property of interest. PageRank
iteratively computes vector a based on the formula
a(k)i ? ?
?
j?B(i)
a(k?1)j
|F (j)|
+ (1? ?)ei (1)
where a(k)i denotes the value of the i-th entry of vec-
tor a at the k-th iteration, ei is a constant such that?
i e
|N |
i=1 = 1, and 0 ? ? ? 1 is a control parameter.
In vectorial form, Equation 1 can be written as
a(k) = ?a(k?1)W + (1? ?)e (2)
The underlying intuition is that a node ni has a high
score when (recursively) it has many high-scoring
backward neighbours with few forward neighbours
each; a node nj thus passes its score aj along to
its forward neighbours F (j), but this score is sub-
divided equally among the members of F (j). This
mechanism (that is represented by the summation in
Equation 1) is then ?smoothed? by the ei constants,
whose role is (see (Bianchini et al, 2005) for de-
tails) to avoid that scores flow and get trapped into
so-called ?rank sinks? (i.e., cliques with backward
neighbours but no forward neighbours).
The computational properties of the PageRank al-
gorithm, and how to compute it efficiently, have
been widely studied; the interested reader may con-
sult (Bianchini et al, 2005).
In the original application of PageRank for rank-
ing Web search results the elements of e are usually
taken to be all equal to 1|N | . However, it is possible
426
to give different values to different elements in e. In
fact, the value of ei amounts to an internal source
of score for ni that is constant across the iterations
and independent from its backward neighbours. For
instance, attributing a null ei value to all but a few
Web pages that are about a given topic can be used
in order to bias the ranking of Web pages in favour
of this topic (Haveliwala, 2003).
In this work we use the ei values as internal
sources of a given ORP (positivity or negativity),
by attributing a null ei value to all but a few ?seed?
synsets known to possess that ORP. PageRank will
thus make the ORP flow from the seed synsets, at
a rate constant throughout the iterations, into other
synsets along the I relation, until a stable state is
reached; the final ai values can be used to rank the
synsets in terms of that ORP. Our method thus re-
quires two runs of PageRank; in the first e has non-
null scores for the positive seed synsets, while in the
second the same happens for the negative ones.
3.2 eXtended WordNet
The transformation of WordNet into a graph based
on the I relation would of course be non-
trivial, but is luckily provided by eXtended Word-
Net (Harabagiu et al, 1999), a publicly available
version of WordNet in which (among other things)
each term sk occurring in a WordNet gloss (ex-
cept those in example phrases) is lemmatized and
mapped to the synset in which it belongs2. We
use eXtended WordNet version 2.0-1.1, which refers
to WordNet version 2.0. The eXtended WordNet
resource has been automatically generated, which
means that the associations between terms and
synsets are likely to be sometimes incorrect, and this
of course introduces noise in our method.
3.3 PageRank, (eXtended) WordNet, and ORP
flow
We now discuss the application of PageRank to
ranking WordNet synsets by positivity and negativ-
ity. Our algorithm consists in the following steps:
1. The graph G = ?N,L? on which PageRank
will be applied is generated. We define N to
be the set of all WordNet synsets; in WordNet
2.0 there are 115,424 of them. We define L to
2http://xwn.hlt.utdallas.edu/
contain a link from synset si to synset sk iff the
gloss of si contains at least a term belonging
to sk (terms occurring in the examples phrases
and terms occurring after a term that expresses
negation are not considered). Numbers, articles
and prepositions occurring in the glosses are
discarded, since they can be assumed to carry
no positivity and negativity, and since they do
not belong to a synset of their own. This leaves
only nouns, adjectives, verbs, and adverbs.
2. The graph G = ?N,L? is ?pruned? by remov-
ing ?self-loops?, i.e., links going from a synset
si into itself (since we assume that there is no
flow of semantics from a concept unto itself).
The row-normalized adjacency matrix W of G
is derived.
3. The ei values are loaded into the e vector; all
synsets other than the seed synsets of renowned
positivity (negativity) are given a value of 0.
The ? control parameter is set to a fixed value.
We experiment with several different versions
of the e vector and several different values of
?; see Section 4.3 for details.
4. PageRank is executed using W and e, iter-
ating until a predefined termination condition
is reached. The termination condition we use
in this work consists in the fact that the co-
sine of the angle between a(k) and a(k+1) is
above a predefined threshold ? (here we have
set ? = 1? 10?9).
5. We rank all the synsets of WordNet in descend-
ing order of their ai score.
The process is run twice, once for positivity and
once for negativity.
The last question to be answered is: ?why PageR-
ank?? Are the characteristics of PageRank more
suitable to the problem of ranking synsets than other
random-walk algorithms? The answer is yes, since
it seems reasonable that:
1. If terms contained in synset sk occur in the
glosses of many positive synsets, and if the pos-
itivity scores of these synsets are high, then it
is likely that sk is itself positive (the same hap-
pens for negativity). This justifies the summa-
tion of Equation 1.
427
2. If the gloss of a positive synset that contains
a term in synset sk also contains many other
terms, then this is a weaker indication that sk is
itself positive (this justifies dividing by |F (j)|
in Equation 1).
3. The ranking resulting from the algorithm needs
to be biased in favour of a specific ORP; this
justifies the presence of the (1 ? ?)ei factor in
Equation 1).
The fact that PageRank is the ?right? random-walk
algorithm for our application is also confirmed by
some experiments (not reported here for reasons of
space) we have run with slightly different variants of
the model (e.g., one in which we challenge intuition
2 above and thus avoid dividing by |F (j)| in Equa-
tion 1). These experiments have always returned
inferior results with respect to standard PageRank,
thereby confirming the correctness of our intuitions.
4 Experiments
4.1 The benchmark
To evaluate the quality of the rankings produced
by our experiments we have used the Micro-WNOp
corpus (Cerini et al, 2007) as a benchmark3. Micro-
WNOp consists in a set of 1,105 WordNet synsets,
each of which was manually assigned a triplet of
scores, one of positivity, one of negativity, one
of neutrality. The evaluation was performed by
five MSc students of linguistics, proficient second-
language speakers of English. Micro-WNOp is rep-
resentative of WordNet with respect to the different
parts of speech, in the sense that it contains synsets
of the different parts of speech in the same propor-
tions as in the entire WordNet. However, it is not
representative of WordNet with respect to ORPs,
since this would have brought about a corpus largely
composed of neutral synsets, which would be pretty
useless as a benchmark for testing automatically de-
rived lexical resources for opinion mining. It was
thus generated by randomly selecting 100 positive +
100 negative + 100 neutral terms from the General
Inquirer lexicon (see (Turney and Littman, 2003) for
details) and including all the synsets that contained
3http://www.unipv.it/wnop/
at least one such term, without paying attention to
POS. See (Cerini et al, 2007) for more details.
The corpus is divided into three parts:
? Common: 110 synsets which all the evaluators
evaluated by working together, so as to align
their evaluation criteria.
? Group1: 496 synsets which were each inde-
pendently evaluated by three evaluators.
? Group2: 499 synsets which were each inde-
pendently evaluated by the other two evalua-
tors.
Each of these three parts has the same balance, in
terms of both parts of speech and ORPs, of Micro-
WNOp as a whole. We obtain the positivity (nega-
tivity) ranking from Micro-WNOp by averaging the
positivity (negativity) scores assigned by the evalua-
tors of each group into a single score, and by sorting
the synsets according to the resulting score. We use
Group1 as a validation set, i.e., in order to fine-tune
our method, and Group2 as a test set, i.e., in order
to evaluate our method once all the parameters have
been optimized on the validation set.
The result of applying PageRank to the graph G
induced by the I relation, given a vector e of in-
ternal sources of positivity (negativity) score and a
value for the ? parameter, is a ranking of all the
WordNet synsets in terms of positivity (negativity).
By using different e vectors and different values of
? we obtain different rankings, whose quality we
evaluate by comparing them against the ranking ob-
tained from Micro-WNOp.
4.2 The effectiveness measure
A ranking  is a partial order on a set of objects
N = {o1 . . . o|N |}. Given a pair (oi, oj) of objects,
oi may precede oj (oi  oj), it may follow oi (oi 
oj), or it may be tied with oj (oi ? oj).
To evaluate the rankings produced by PageRank
we have used the p-normalized Kendall ? distance
(noted ?p ? see e.g., (Fagin et al, 2004)) between
the Micro-WNOp rankings and those predicted by
PageRank. A standard function for the evaluation of
rankings with ties, ?p is defined as
?p =
nd + p ? nu
Z
(3)
428
where nd is the number of discordant pairs, i.e.,
pairs of objects ordered one way in the gold stan-
dard and the other way in the prediction; nu is the
number of pairs ordered (i.e., not tied) in the gold
standard and tied in the prediction, and p is a penal-
ization to be attributed to each such pair; and Z is
a normalization factor (equal to the number of pairs
that are ordered in the gold standard) whose aim is
to make the range of ?p coincide with the [0, 1] in-
terval. Note that pairs tied in the gold standard are
not considered in the evaluation.
The penalization factor is set to p = 12 , which
is equal to the probability that a ranking algorithm
correctly orders the pair by random guessing; there
is thus no advantage to be gained from either ran-
dom guessing or assigning ties between objects. For
a prediction which perfectly coincides with the gold
standard ?p equals 0; for a prediction which is ex-
actly the inverse of the gold standard ?p equals 1.
4.3 Setup
In order to produce a ranking by positivity (nega-
tivity) we need to provide an e vector as input to
PageRank. We have experimented with several dif-
ferent definitions of e, each for both positivity and
negativity. For reasons of space, we only report re-
sults from the five most significant ones.
We have first tested a vector (hereafter dubbed
e1) with all values uniformly set to 1|N | . This is the
e vector originally used in (Brin and Page, 1998)
for Web page ranking, and brings about an unbiased
(that is, with respect to particular properties) rank-
ing of WordNet. Of course, it is not meant to be
used for ranking by positivity or negativity; we have
used it as a baseline in order to evaluate the impact
of property-biased vectors.
The first sensible, albeit minimalistic, definition
of e we have used (dubbed e2) is that of a vec-
tor with uniform non-null ei scores assigned to the
synsets that contain the adjective good (bad), and
null scores for all other synsets. A further, still fairly
minimalistic definition we have used (dubbed e3) is
that of a vector with uniform non-null ei scores as-
signed to the synsets that contain at least one of the
seven ?paradigmatic? positive (negative) adjectives
used as seeds in (Turney and Littman, 2003)4, and
4The seven positive adjectives are good, nice, excellent,
null scores for all other synsets.
We have also tested a more complex version of
e, with ei scores obtained from release 1.0 of Senti-
WordNet (Esuli and Sebastiani, 2006b)5. This latter
is a lexical resource in which each WordNet synset
is given a positivity score, a negativity score, and a
neutrality score. We produced an e vector (dubbed
e4) in which the score assigned to a synset is propor-
tional to the positivity (negativity) score assigned to
it by SentiWordNet, and in which all entries sum up
to 1. In a similar way we also produced a further e
vector (dubbed e5) through the scores of a newer re-
lease of SentiWordNet (release 1.1), resulting from a
slight modification of the approach that had brought
about release 1.0 (Esuli and Sebastiani, 2007b).
PageRank is parametric on ?, which determines
the balance between the contributions of the a(k?1)
vector and the e vector. A value of ? = 0 makes
the a(k) vector coincide with e, and corresponds to
discarding the contribution of the random-walk al-
gorithm. Conversely, setting ? = 1 corresponds
to discarding the contribution of e, and makes a(k)
uniquely depend on the topology of the graph; the
result is an ?unbiased? ranking. The desirable cases
are, of course, in between. As first hinted in Sec-
tion 4.1, we thus optimize the ? parameter on the
synsets in Group1, and then test the algorithm with
the optimal value of ? on the synsets in Group2.
All the 101 values of ? from 0.0 to 1.0 with a step of
.01 have been tested in the optimization phase. Op-
timization is performed anew for each experiment,
which means that different values of ? may be even-
tually selected for different e vectors.
5 Results
The results show that the use of PageRank in com-
bination with suitable vectors e almost always im-
proves the ranking, sometimes significantly so, with
respect to the original ranking embodied by the e
vector.
For positivity, the rankings produced using
PageRank and any of the vectors from e2 to e5 all
improve on the original rankings, with a relative im-
provement, measured as the relative decrease in ?p,
positive, fortunate, correct, superior, and the seven negative
ones are bad, nasty, poor, negative, unfortunate, wrong, in-
ferior.
5http://sentiwordnet.isti.cnr.it/
429
ranging from ?4.88% (e5) to ?6.75% (e4). These
rankings are also all better than the rankings pro-
duced by using PageRank and the uniform-valued
vector e1, with a minimum relative improvement
of ?5.04% (e3) and a maximum of ?34.47% (e4).
This suggests that the key to good performance is
indeed a combination of positivity flow and internal
source of score.
For the negativity rankings, the performance of
both SentiWordNet-based vectors is still good, pro-
ducing a ?4.31% (e4) and a ?3.45% (e5) improve-
ment with respect to the original rankings. The
?minimalistic? vectors (i.e., e2 and e3) are not as
good as their positive counterparts. The reason
seems to be that the generation of a ranking by neg-
ativity seems a somehow harder task than the gen-
eration of a ranking by positivity; this is also shown
by the results obtained with the uniform-valued vec-
tor e1, in which the application of PageRank im-
proves with respect to e1 for positivity but deteri-
orates for negativity. However, against the baseline
constituted by the results obtained with the uniform-
valued vector e1 for negativity, our rankings show
a relevant improvement, ranging from ?8.56% (e2)
to ?48.27% (e4).
Our results are particularly significant for the e4
vectors, derived by SentiWordNet 1.0, for a num-
ber of reasons. First, e4 brings about the best value
of ?p obtained in all our experiments (.325 for pos-
itivity, .284 for negativity). Second, the relative im-
provement with respect to e4 is the most marked
among the various choices for e (6.75% for positiv-
ity, 4.31% for negativity). Third, the improvement
is obtained with respect to an already high-quality
resource, obtained by the same techniques that, at
the term level, are still the best performers for po-
larity detection on the widely used General Inquirer
benchmark (Esuli and Sebastiani, 2005).
Finally, observe that the fact that e4 outperforms
all other choices for e (and e2 in particular) was not
necessarily to be expected. In fact, SentiWordNet
1.0 was built by a semi-supervised learning method
that uses vectors e2 as its only initial training data.
This paper thus shows that, starting from e2 as the
only manually annotated data, the best results are
obtained neither by the semi-supervised method that
generated SentiWordNet 1.0, nor by PageRank, but
by the concatenation of the former with the latter.
Positivity Negativity
e PageRank? ?p ? ?p ?
e1 before .500 .500
after .496 (-0.81%) .549 (9.83%)
e2 before .500 .500
after .467 (-6.65%) .502 (0.31%)
e3 before .500 .500
after .471 (-5.79%) .495 (-0.92%)
e4 before .349 .296
after .325 (-6.75%) .284 (-4.31%)
e5 before .400 .407
after .380 (-4.88%) .393 (-3.45%)
Table 1: Values of ?p between predicted rankings
and gold standard rankings (smaller is better). For
each experiment the first line indicates the ranking
obtained from the original e vector (before the ap-
plication of PageRank), while the second line indi-
cates the ranking obtained after the application of
PageRank, with the relative improvement (a nega-
tive percentage indicates improvement).
6 Conclusions
We have investigated the applicability of a random-
walk model to the problem of ranking synsets ac-
cording to positivity and negativity. However, we
conjecture that this model can be of more general
use, i.e., for the determination of other properties of
term senses, such as membership in a domain. This
paper thus presents a proof-of-concept of the model,
and the results of experiments support our intuitions.
Also, we see this work as a proof of concept
for the applicability of general random-walk algo-
rithms (and not just PageRank) to the determination
of the semantic properties of synsets. In a more re-
cent paper (Esuli and Sebastiani, 2007a) we have
investigated a related random-walk model, one in
which, symmetrically to the intuitions of the model
presented in this paper, semantics flows from the
definiens to the definiendum; a metaphor that proves
no less powerful than the one we have championed
in this paper.
References
Alina Andreevskaia and Sabine Bergler. 2006a. Mining Word-
Net for fuzzy sentiment: Sentiment tag extraction from
WordNet glosses. In Proceedings of the 11th Conference of
the European Chapter of the Association for Computational
Linguistics (EACL?06), pages 209?216, Trento, IT.
Alina Andreevskaia and Sabine Bergler. 2006b. Sentiment
tag extraction from WordNet glosses. In Proceedings of
430
the 5th Conference on Language Resources and Evaluation
(LREC?06), Genova, IT.
Monica Bianchini, Marco Gori, and Franco Scarselli. 2005. In-
side PageRank. ACM Transactions on Internet Technology,
5(1):92?128.
Sergey Brin and Lawrence Page. 1998. The anatomy of a large-
scale hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1-7):107?117.
Sabrina Cerini, Valentina Compagnoni, Alice Demontis,
Maicol Formentelli, and Caterina Gandini. 2007. Micro-
WNOp: A gold standard for the evaluation of automati-
cally compiled lexical resources for opinion mining. In An-
drea Sanso`, editor, Language resources and linguistic the-
ory: Typology, second language acquisition, English linguis-
tics. Franco Angeli Editore, Milano, IT. Forthcoming.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the
semantic orientation of terms through gloss analysis. In Pro-
ceedings of the 14th ACM International Conference on In-
formation and Knowledge Management (CIKM?05), pages
617?624, Bremen, DE.
Andrea Esuli and Fabrizio Sebastiani. 2006a. Determining
term subjectivity and term orientation for opinion mining. In
Proceedings of the 11th Conference of the European Chapter
of the Association for Computational Linguistics (EACL?06),
pages 193?200, Trento, IT.
Andrea Esuli and Fabrizio Sebastiani. 2006b. SENTIWORD-
NET: A publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Language Re-
sources and Evaluation (LREC?06), pages 417?422, Gen-
ova, IT.
Andrea Esuli and Fabrizio Sebastiani. 2007a. Random-
walk models of term semantics: An application to opinion-
related properties. Technical Report ISTI-009/2007, Isti-
tuto di Scienza e Tecnologie dell?Informazione, Consiglio
Nazionale dellle Ricerche, Pisa, IT.
Andrea Esuli and Fabrizio Sebastiani. 2007b. SENTIWORD-
NET: A high-coverage lexical resource for opinion mining.
Technical Report 2007-TR-02, Istituto di Scienza e Tecnolo-
gie dell?Informazione, Consiglio Nazionale delle Ricerche,
Pisa, IT.
Ronald Fagin, Ravi Kumar, Mohammad Mahdiany, D. Sivaku-
mar, and Erik Veez. 2004. Comparing and aggregating rank-
ings with ties. In Proceedings of ACM International Confer-
ence on Principles of Database Systems (PODS?04), pages
47?58, Paris, FR.
Gregory Grefenstette, Yan Qu, David A. Evans, and James G.
Shanahan. 2006. Validating the coverage of lexical re-
sources for affect analysis and automatically classifying new
words along semantic axes. In James G. Shanahan, Yan Qu,
and Janyce Wiebe, editors, Computing Attitude and Affect
in Text: Theories and Applications, pages 93?107. Springer,
Heidelberg, DE.
Sanda H. Harabagiu, George A. Miller, and Dan I. Moldovan.
1999. WordNet 2: A morphologically and semantically en-
hanced resource. In Proceedings of the ACL SIGLEX Work-
shop on Standardizing Lexical Resources, pages 1?8, Col-
lege Park, US.
Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL?97), pages 174?181,
Madrid, ES.
Taher H. Haveliwala. 2003. Topic-sensitive PageRank:
A context-sensitive ranking algorithm for Web search.
IEEE Transactions on Knowledge and Data Engineering,
15(4):784?796.
Nancy Ide. 2006. Making senses: Bootstrapping sense-tagged
lists of semantically-related words. In Proceedings of the
7th International Conference on Computational Linguistics
and Intelligent Text Processing (CICLING?06), pages 13?27,
Mexico City, MX.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten
De Rijke. 2004. Using WordNet to measure semantic ori-
entation of adjectives. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Evaluation
(LREC?04), volume IV, pages 1115?1118, Lisbon, PT.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics (COL-
ING?04), pages 1367?1373, Geneva, CH.
Rada Mihalcea. 2006. Random walks on text structures. In
Proceedings of the 7th International Conference on Com-
putational Linguistics and Intelligent Text Processing (CI-
CLING?06), pages 249?262, Mexico City, MX.
Pero Subasic and Alison Huettner. 2001. Affect analysis of text
using fuzzy semantic typing. IEEE Transactions on Fuzzy
Systems, 9(4):483?496.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005.
Extracting emotional polarity of words using spin model.
In Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?05), pages 133?
140, Ann Arbor, US.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Sys-
tems, 21(4):315?346.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of the 44th Annual Meeting of the
Association for Computational Linguistics (ACL?06), pages
1065?1072, Sydney, AU.
431
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 218?221,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
ISTI@SemEval-2 Task #8:
Boosting-Based Multiway Relation Classification
Andrea Esuli, Diego Marcheggiani, Fabrizio Sebastiani
Istituto di Scienza e Tecnologie dell?Informazione
Consiglio Nazionale delle Ricerche
56124 Pisa, Italy
firstname.lastname@isti.cnr.it
Abstract
We describe a boosting-based supervised
learning approach to the ?Multi-Way Clas-
sification of Semantic Relations between
Pairs of Nominals? task #8 of SemEval-
2. Participants were asked to determine
which relation, from a set of nine relations
plus ?Other?, exists between two nomi-
nals, and also to determine the roles of the
two nominals in the relation.
Our participation has focused, rather than
on the choice of a rich set of features,
on the classification model adopted to de-
termine the correct assignment of relation
and roles.
1 Introduction
The ?Multi-Way Classification of Semantic Rela-
tions between Pairs of Nominals? (Hendrickx et
al., 2010) we faced can be seen as the composition
of two sub-tasks:
1. Determining which relation r, from a set of
relations R (see Table 1), exists between two
entities e
1
and e
2
.
2. Determining the direction of the relation, i.e.,
determining which of r(e
1
, e
2
) or r(e
2
, e
1
)
holds.
The set R is composed by nine ?semantically
determined? relations, plus a special Other rela-
tion which includes all the pairs which do not be-
long to any of the nine previously mentioned rela-
tions.
The two novel aspects of this task with respect to
the similar task # 4 of SemEval-2007 (Girju et al,
2007) (?Classification of Semantic Relations be-
tween Nominals?) are (i) the definition of the task
as a ?single-label? classification task and (ii) the
1 Cause-Effect
2 Instrument-Agency
3 Product-Producer
4 Content-Container
5 Entity-Origin
6 Entity-Destination
7 Component-Whole
8 Member-Collection
9 Message-Topic
Table 1: The nine relations defined for the task.
need of determining the direction of the relation
(i.e., Item 2 above).
The classification task described can be formal-
ized as a single-label (aka ?multiclass?) text clas-
sification (SLTC) task, i.e., as one in which exactly
one class must be picked for a given object out of
a set of m available classes.
Given a set of objects D (ordered pairs of nom-
inals, in our case) and a predefined set of classes
(aka labels, or categories) C = {c
1
, . . . , c
m
},
SLTC can be defined as the task of estimating
an unknown target function ? : D ? C, that
describes how objects ought to be classified, by
means of a function
?
? : D ? C called the classi-
fier
1
.
In the relation classification task which is the
object of this evaluation, the set C of classes is
composed of 19 elements, i.e., the nine relations
of Table 1, each one considered twice because it
may take two possible directions, plus Other.
2 The learner
As the learner for our experiments we have used a
boosting-based learner called MP-BOOST (Esuli
et al, 2006). Boosting is among the classes of su-
pervised learning devices that have obtained the
best performance in several learning tasks and,
at the same time, have strong justifications from
computational learning theory. MP-BOOST is a
1
Consistently with most mathematical literature we use
the caret symbol (?) to indicate estimation.
218
variant of ADABOOST.MH (Schapire and Singer,
2000), which has been shown in (Esuli et al,
2006) to obtain considerable effectiveness im-
provements with respect to ADABOOST.MH.
MP-BOOST works by iteratively generating, for
each class c
j
, a sequence
?
?
j
1
, . . . ,
?
?
j
S
of classifiers
(called weak hypotheses). A weak hypothesis is a
function
?
?
j
s
: D ? R, where D is the set of doc-
uments and R is the set of real numbers. The sign
of
?
?
j
s
(d
i
) (denoted by sgn(
?
?
j
s
(d
i
))) represents the
binary decision of
?
?
j
s
on whether d
i
belongs to c
j
,
i.e. sgn(
?
?
j
s
(d
i
)) = +1 (resp.,?1) means that d
i
is
believed to belong (resp., not to belong) to c
j
. The
absolute value of
?
?
j
s
(d
i
) (denoted by |
?
?
j
s
(d
i
)|)
represents instead the confidence that
?
?
j
s
has in
this decision, with higher values indicating higher
confidence.
At each iteration s MP-BOOST tests the effec-
tiveness of the most recently generated weak hy-
pothesis
?
?
j
s
on the training set, and uses the results
to update a distributionD
j
s
of weights on the train-
ing examples. The initial distribution D
j
1
is uni-
form by default. At each iteration s all the weights
D
j
s
(d
i
) are updated, yieldingD
j
s+1
(d
i
), so that the
weight assigned to an example correctly (resp., in-
correctly) classified by
?
?
j
s
is decreased (resp., in-
creased). The weight D
j
s+1
(d
i
) is thus meant to
capture how ineffective
?
?
j
1
, . . . ,
?
?
j
s
have been in
guessing the correct c
j
-assignment of d
i
(denoted
by ?
j
(d
i
)), i.e., in guessing whether training doc-
ument d
i
belongs to class c
j
or not. By using this
distribution, MP-BOOST generates a new weak
hypothesis
?
?
j
s+1
that concentrates on the exam-
ples with the highest weights, i.e. those that had
proven harder to classify for the previous weak hy-
potheses.
The overall prediction on whether d
i
belongs to
c
j
is obtained as a sum
?
?
j
(d
i
) =
?
S
s=1
?
?
j
s
(d
i
) of
the predictions made by the weak hypotheses. The
final classifier
?
?
j
is thus a committee of S clas-
sifiers, a committee whose S members each cast
a weighted vote (the vote being the binary deci-
sion sgn(
?
?
j
s
(d
i
)), the weight being the confidence
|
?
?
j
s
(d
i
)|) on whether d
i
belongs to c
j
. For the final
classifier
?
?
j
too, sgn(
?
?
j
(d
i
)) represents the bi-
nary decision as to whether d
i
belongs to c
j
, while
|
?
?
j
(d
i
)| represents the confidence in this decision.
MP-BOOST produces a multi-label classifier,
i.e., a classifier which independently classifies a
document against each class, possibly assigning
a document to multiple classes or no class at
?<e1>People</e1> have been moving back into
<e2>downtown</e2>.?
Entity-Destination(e1,e2)
F People FS Peopl FH group FP NNP
FS1 have FS1S have FS1H have FS1P VBP
FS2 been FS2S been FS2H be FS2P VBN
FP3 moving FP3S move FP3H travel FP3P VBG
SP3 moving SP3S move SP3H travel SP3P VBG
SP2 back SP2S back SP2H O SP2P RB
SP1 into SP1S into SP1H O SP1P IN
S downtown SS downtown SH city district SP NN
SS1 . SS1S . SS1H O SS1P .
Table 2: A training sentence and the features ex-
tracted from it.
all. In order to obtain a single-label classifier,
we compare the outcome of the |C| binary clas-
sifiers, and the class which has obtained the high-
est
?
?
j
(d
i
) value is assigned to d
i
, i.e.,
?
?(d
i
) =
arg max
j
?
?
j
(d
i
).
3 Vectorial representation
We have generated the vectorial representations of
the training and test objects by extracting a number
of contextual features from the text surrounding
the two nominals whose relation is to be identified.
An important choice we have made is to ?nor-
malize? the representation of the two nominals
with respect to the order in which they appear in
the relation, and not in the sentence. Thus, if e
2
appears in a relation r(e
2
, e
1
), then e
2
is consid-
ered to be the first (F) entity in the feature genera-
tion process and e
1
is the second (S) entity.
We have generated a number of features for
each term denoting an entity and also for the three
terms preceding each nominal (P1, P2, P3) and for
the three terms following it (S1, S2, S3):
T : the term itself;
S : the stemmed version of the term, obtained
using a Porter stemmer;
P : the POS of the term, obtained using the Brill
Tagger;
H : the hypernym of the term, taken from Word-
Net (?O? if not available).
Features are prefixed with a proper composition
of the above labels in order to identify their role
in the sentence. Table 2 illustrates a sentence from
the training set and its extracted features.
219
If an entity is composed by k > 1 terms, entity-
specific features are generated for all the term n-
grams contained in the entity, for all n ? [1, ..., k].
E.g., for ?phone call? features are generated for
the n-grams: ?phone?, ?call?, ?phone call?.
In all the experiments described in this paper,
MP-BOOST has been run for S = 1000 iterations.
No feature weighting has been performed, since
MP-BOOST requires binary input only.
4 Classification model
The classification model we adopted in our exper-
iments splits the two tasks of recognizing the rela-
tion type and the one of determining the direction
of the relation in two well distinct phases.
4.1 Relation type determination
Given the training set Tr of all the sentences for
which the classifier outcome is known, vectorial
representations (see Section 3) are built in a way
that ?normalizes? the direction of the relation, i.e.:
? if the training object belongs to one of the
nine relevant relations, the features extracted
from the documents are given proper identi-
fiers in order to mark their role in the relation,
not the order of appearance in the sentence;
? if the training object belongs to Other the
two distinct vectorial representations are gen-
erated, one for relation Other(e
1
, e
2
) and one
for Other(e
2
, e
1
).
The produced training set has thus a larger num-
ber of examples than the one actually provided.
The training set provided for the task yielded 9410
training examples from the original 8000 sen-
tences. A 10-way classifier is then trained on the
vectorial representation.
4.2 Relation direction determination
The 10-way classifier is thus able to assign a rela-
tion, or the Other relation, to a sentence, but not to
return the direction of the relation. The direction
of the relation is determined at test time, by classi-
fying two instances of each test sentence, and then
combining the outcome of the two classifications
in order to produce the final classification result.
More formally, given a test sentence d belong-
ing to an unknown relation r, two vectorial repre-
sentations are built: one, d
1,2
, under the hypoth-
esis that r(e
1
, e
2
) holds, and one, d
2,1
, under the
hypothesis that r(e
2
, e
1
) holds.
Both d
1,2
and d
2,1
are classified by
?
?:
? if both classifications return Other, then d is
assigned to Other;
? if one classification returns Other and the
other returns a relation r, then r, with the
proper direction determined by which vec-
torial representation determined the assign-
ment, is assigned to d;
? if the two classifications return two relations
r
1,2
and r
2,1
different from Other (of the
same or of different relation type), then the
one that obtains the highest
?
? value deter-
mines the relation and the direction to be as-
signed to d.
5 Experiments
We have produced two official runs.
The ISTI-2 run uses the learner, vectorial rep-
resentation, and classification model described in
the previous sections.
The ISTI-1 run uses the same configuration of
ISTI-2, with the only difference being how the
initial distribution D
j
1
of the boosting method is
defined. Concerning this, we followed the ob-
servations of (Schapire et al, 1998, Section 3.2)
on boosting with general utility functions; the ini-
tial distribution in the ISTI-1 run is thus set to be
equidistributed between the portion Tr
+
j
of pos-
itive examples of the training set and the portion
Tr
?
j
of negative examples, for each class j, i.e.,
D
j
1
(d
i
) =
1
2|Tr
+
j
|
iff d
i
? Tr
+
j
(1)
D
j
1
(d
i
) =
1
2|Tr
?
j
|
iff d
i
? Tr
?
j
(2)
This choice of initial distribution, which gives
more relevance to the less frequent type of ele-
ments of the training set (namely, the positive ex-
amples), is meant to improve the performance on
highly imbalanced classes, thus improving effec-
tiveness at the the macro-averaged level.
We have also defined a third method for an addi-
tional run, ISTI-3; unfortunately we were not able
to produce it in time, and there is thus no offi-
cial evaluation for this run on the test data. The
method upon which the ISTI-3 run is based re-
lies on a more ?traditional? approach to the clas-
sification task, i.e., a single-label classifier trained
220
Run pi
?
?
?
F
?
1
pi
M
?
M
F
M
1
Official results
ISTI-1 72.01% 67.08% 69.46% 71.12% 66.24% 68.42%
ISTI-2 73.55% 63.54% 68.18% 72.38% 62.34% 66.65%
10-fold cross-validation
ISTI-1 73.60% 69.34% 71.41% 72.44% 68.17% 69.95%
ISTI-2 75.34% 65.92% 70.32% 73.96% 64.65% 68.52%
ISTI-3 68.52% 61.58% 64.86% 66.19% 59.75% 62.31%
Table 3: Official results (upper part), and results of the three relation classification methods when used in
a 10-fold cross-validation experiment on training data (lower part). Precision, recall, and F
1
are reported
as percentages for more convenience.
on the nine relations plus Other, not considering
the direction, coupled with nine binary classifiers
trained to determined the direction of each rela-
tion. We consider this configuration as a reason-
able baseline to evaluate the impact of the original
classification model adopted in the other two runs.
Table 3 summarizes the experimental results.
The upper part of the table reports the official re-
sults for the two official runs. The lower part
reports the results obtained by the three rela-
tion classification methods when used in a 10-
fold cross-validation experiment on the training
data. The evaluation measures are precison (pi),
recall (?), and the F
1
score, computed both in
a microaveraged (?
?
) and a macroaveraged
(?
M
) way (Yang, 1999).
The results for ISTI-1 and ISTI-2 in the 10-fold
validation experiment are similar both in trend and
in absolute value to the official results, allowing
us to consider the ISTI-3 results in the 10-fold
validation experiment as a good prediction of the
efficacy of the ISTI-3 method on the test data.
The classification model of ISTI-2, which uses
an initial uniform distribution for the MP-BOOST
learner as ISTI-3, improves F
M
1
over ISTI-3 by
9.97%, and F
?
1
by 8.42%.
The use of aF
1
-customized distribution in ISTI-
1 results in a F
1
improvement with respect to
ISTI-2 (F
M
1
improves by 2.66% in official re-
sults, 2.09% in 10-fold validation results), which
is mainly due to a relevant improvement in recall.
Comparing ISTI-1 with ISTI-3 the total im-
provement is 12.26% for F
M
1
and 10.10% for F
?
1
.
6 Conclusion and future work
The original relation classification model we have
adopted has produced a relevant improvement in
efficacy with respect to a ?traditional? approach.
We have not focused on the development of a
rich set of features. In the future we would like to
apply our classification model to the vectorial rep-
resentations generated by the other participants, in
order to evaluate the distinct contributions of the
feature set and the classification model.
The use of a F
1
-customized initial distribution
for the MP-BOOST learner has also produced a
relevant improvement, and it will be further inves-
tigated on more traditional text classification tasks.
References
Andrea Esuli, Tiziano Fagni, and Fabrizio Sebastiani.
2006. MP-Boost: A multiple-pivot boosting al-
gorithm and its application to text categorization.
In Proceedings of the 13th International Sympo-
sium on String Processing and Information Retrieval
(SPIRE?06), pages 1?12, Glasgow, UK.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic
relations between nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 13?18, Prague, CZ.
Association for Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid
?
O S?eaghdha, Sebastian
Pad?o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, Upp-
sala, Sweden.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Robert E. Schapire, Yoram Singer, and Amit Singhal.
1998. Boosting and rocchio applied to text filtering.
In SIGIR ?98: Proceedings of the 21st annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 215?
223, New York, NY, USA. ACM.
Yiming Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. Information Re-
trieval, 1(1/2):69?90.
221
