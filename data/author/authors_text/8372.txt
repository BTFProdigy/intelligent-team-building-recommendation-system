Toward the "At-a-glance" Summary: 
Phrase-representation Summarization Method 
YoshihiroUEDA, MamikoOKA, TakahiroKOYAMA and TadanobuMIYAUCHl 
Industry Solutions Company, Fuji Xerox~ Co., Ltd. 
430 Sakai, Nakai-machi, Kanagawa 259-0157, JAPAN 
{Ueda.goshihiro, oka.mamiko, Koyama.Takahiro, Miyauchi.Tadanobu}@fujixerox.co.jp 
Abstract 
We have developed a summarization method that creates a snmmary suitable for tim p,'ocess of  
sifting information retrieval results. Unlike conventional methods that extract ilnportant sen- 
tences, this method constructs short phrases to reduce the burden of reading long sentences. We 
have developed a prototype summarization system tbr Japanese. Through a rather large-scale task~ 
based experiment, he sumnmry this system creates proved to be effective to sift IR results. This 
summarization method is also applicable to other languages such as English. 
Introduction 
Sulnmaries are used to select relevant information 
from information retrieval results. The goal of  
sunmmrization for such "indicative" use is to 
provide fast and accurate judgement. 
Most automatic summarization systems adopt 
the "sentence selection" metho& which gives a 
score to eve~ sentence on the basis of its charac-. 
teristics, such as word frequency, the position in 
which it appears, etc. and selects sentences with 
high scores. 
Tim sentences collected in such a way tend to 
be so long and complex that the reader must 
reconstruct im structure while reading them. 
Reading such sentences involves some annoy- 
ante .  
Our aim is to reduce this burden by provkling 
an "at-a-glance" summary. 
Phrase-representation summarization is a 
method to create the "at-a-glance" summary for 
the Japanese language, t tere we present the 
concept, the algorithm, and ewihiation of the 
efficacy of the summary produced by a prototype 
based on this method. Extension to English is 
also discussed. 
1 The Concept 
Examples of an "'at-a-glance'" summary are the 
headlines of news articles. The headline 
provides intbrmation tbr judging whether the 
article is to be read or not an& in this sense, it is 
really ??indicative." The characteristics are: 
? Brevity (short in length) 
? Simplicity (less embedded sentences) 
We use "'pllrases" to represent he simplicity 
characteristic I and set our goal to create phrase- 
represented summaries, which provide the reader 
with an outline of  the document, avoiding reading 
stress by enumerating short phrases containing the 
important words and concepts composed from 
these words. 
The lnethod we adopted to achieve this goal is 
to construct such phrases from the relations 
between words rather than extracting important 
sentences fl'om the original document. 
2 Summarization Method 
2.1 Outline of the Algorithm 
Here we give a short description of the ot,tline of 
tiffs method using the example shown in Fig. 1. 2 
i The word "'phrase" used here is not of tile linguistic 
sense but an expression tbr "'short" and "sinaple." In 
Japanese, there is no rigid distinction between "phrase" 
and "'clause." 
-~ In tiffs paper, Japanese words are represented in 
English as much as possible. The words left in 
Japanese arc shown in italics, such as -~a" (a particle 
for AGENT),  "jidai'" ("era"), etc. Each relation name 
is constructed from a Japanese pailicle and its function 
(shown as a case name or an equivalent l-;nglish 
878 
(a) original \[ "(9:.r~gl j sl~ t rans lat ion)  At the Green Fair  held on 24th, a venture company PICORP ap.nounced it l i censes  its env i ronment  pro tect ion  techno logy  to B_MICO, the 
U.S. top compa:!v. P ICORP's  CEO Ken Ono said that.. .  
(b) analysis graph & (1) analysis of relations 
....... ~ ~ - - ~  (;,',,enF~,ir } (2) selection of 
. / ,  ? . . . . . . .  "nioite"-AT ~ core relation 
m -FQ \[ - "1 "'ha'-IHEM\[= ~ ' ~  \] 
venture ~'\[ PICORP j 
' - -  I 
. . . . . .  e    ro m--71ent 4, I 
, , "no -ur  ! protection I / ' -  . ' ... " - " - I ,  , , , ~t" -~\ ]  license \[ , .- larmouncedl 
\[ t_ecnnomgy I".,o"-OBJ' ~ ' ' - I 
, I I  ~ (3) Addition of relations 
. . . . . . .  ni '-DAT 
(c) obtained phrase 
.~  (4) generation 
PICORP l icenses env i ronment  pro tect ion  techno logy  to AMICO I 
Fig. 1 : Outline of phrase-representation summarization 
The method consists of the Efllowing tbur major 
steps: 
(1) Syntactic analysis to extract he relations 
between words 
(2) Selection of the core relation 
(3) Adding relations necessa D' for the unity 
of the phrase's meaning 
(4)Generating the surface phrase from the 
constructed graph 
First, the sentences in the given document are 
analyzed to produce directed acyclic graphs 
(DAGs) constructed fi'om relation units, each of 
which consists of  two nodes (words) and an arc 
(relation between tim words). Each node is not 
only a single word but also can be a word 
sequence (noun group). 
Then an important relation is selected as a 
"core" relation. In F'ig. l, the arc connecting the 
two shaded nodes is selected as the "core." 
The core relation alone carries insufficient 
information to convey the content of the original 
docunaent. Additional arcs (represented by 
preposition). 
double lines) are attached to narrow the infornm-. 
tion the phrase supplies. 
The tbllowing short phrase can be generated 
fi'om the selected nodes and arcs in the graph: 
P ICORP l i censes  (its) env i ronment  
pro tect ion  techno logy  to AMICO.  3 
Phrase-representation summarization enutner- 
ates such short phrases to give the readers enough 
infornmtion to grasp the outline of a document. 
This algorithm is explained in the next section. 
2.2  Fur ther  descr ip t ion  o f  each  s tep  
The steps shown in the previous section consists 
of  a cycle that produces a single phrase. The 
cycles are repeated until the generated phrases 
satisfy a predefined condition (e.g. the length of 
the summary). The scores of the words used in 
the cycle are reduced by a predefined cut-down 
; This short sentence can be expressed as a phrase in 
tt~e linguistic sense in \[.;nglish: 
I~IC()RI)'s licensing (its) environment protection 
technology to AMIC(). 
879 
ratio to avoid fi'equent use of the same words in 
the summaiT. 
The basic algorithm is shown in El,, "~ 
Relation AnM|,.~'is 
Syntactic analysis is applied to each sentence ill 
the document to produce a DAG of the relations 
of words. We use a simple parser based on 
pattern matching (Miyauchi, et al 1995), one of 
whose rules always judges each case dependent 
on its nea,'est verb. Some of the misanalysis will 
be hidden by "ambiguity packing" ill the "addi- 
tional relation attachment" step. 
Relation Scoring 
All importance score is provided for each relation 
unit (two nodes and an arc connecting them). 
First, every word is scored by its importance. 
This score is calculated based on tile tf*IDF wdue 
(Salton, 1989) 4. 
Then, the relation score is calculated as fol- 
lows: 
Score = Srel * (Wl*S1 + W2"S2)  
Here, SI and $2 are tile scores of the two words 
connected by relations. The score of  a word 
sequence is calculated by decreasing the sum of 
the scores of its constituent words according to 
tile length of the word sequence. 
Wl and W2 are the weights given to each word. 
Currently, all words are equally treated (WI --- 
W2 = 1). 
Srel is the importance factor of  tile relation. 
The relations that play central roles ill the 
meaning, such as verb cases, are given high 
scores, and the surrounding relations, such as 
"'AND" relations, are scored low. Tile relation 
scores for modifier-modified relations such as 
adverbs are set to 0 to avoid selecting them as the 
core relations. 
Core relation selection 
The relation unit with tile highest score among all 
relations is selected as the "core relation." 
Additional relation attachment 
The inlbrmation that the core relation carries is 
usually insufficient. Additional relations arc 
attached to make the information tile phrase 
? ~ ll)F is calculated from I million WW~,V documcnts 
gathered by a Web search engine. 
Doctlnlent 
_ _ .  ~ Input 
Relation Analysis 1 
Relation Scoring \] 
I \[ Core relation 
1 \[ selection \[ Relation\[ 
\[ Generation of I 
\[_.surface ~hrases I
Output 
\[ Snlnnlary \] 
Fig.2 Basic flow of the algorithm 
supplies rnore specific and to give the reader 
sufficient information to infer the content of the 
original doculnent. "File following relations are a 
part of the relations to be attached. 
@ Mandatory cases 
Relations that correspond to mandatory cases 
are attached to verbs. Mandatory case lists 
are defined for verbs except for those that 
share tile common mandatory case list, which 
includes ?'ga'-AGENT, %vo"-OBJ and "ni"- 
DATIVE. "Ha"- ' f t fEME, "mo'-.ALSO, and 
null-marker elations are also treated as man- 
datory, because they can appear in place of 
the mandatory relations. 
Ex.) AMICe "ga"-AGENT release 
-+ AMICe "ga'-AGENT 
PDA "wo'-OBJ release 
(AMICe releases PDA.) 
@ Noun modified by a verb 
In Japanese, the "verb - noun" structure repre-. 
sents an embedded sentence, and the noun 
usually fills some gap in the embedded sen- 
tence, l('the verb in the core relation (noun 
-- verb) consists ot'sucll a verb -noun relation, 
the modified noun is also assumed to carry 
important information, even if it does not t511 
the mandatory case (fllough the case is not 
880 
arialyzed in tlic ctlrrent algorithm)? Tim.<; the 
verb - llOtlll relation is attached to tile core. 
Ex.) PDA "wo".-OBJ release 
PDA "wo"-OBJ release 
0-THAT 5 AMICe 
(AMICe that releases I:>DA) 
PI)A "wo"-OBJ release 
-~ PDA"wo"-OBJ release 
0.-Tt4AT pDs! 
(a plan to release PDA) 
@ Anlbiguity packing 
The analysi.s trees often contain error.<; be-- 
cause the pattern-base parser doesn't resolve 
ambiguities. For exarnple, the strtlCttlre 
V 0-.TI-IAT N1 "no'-OV N2 (Ving Nl's N2) 
i,q ambiguous in Japanese (V can rnodil~,/ 
either N1 or N2 but the parser always aim-. 
lyzes N2 as modified)? lf'the V-.NI rehltion 
iv; selected as the cole, the N1-N2 rehition is 
always attached to the core to include the pos-. 
sible V-N2 relation. 
il Modifiers of generic llOUllS 
Tile concepts brought by generic rloun,; such 
as <~momf" (thing), +~koto" (<~that"' of that- 
clause), ~baai" (case), ~Tidai" (era) are not so 
specific that they usually acconlpany lnodifi-. 
ers to be infbrmative, tlere such modifiers 
are attached to make them intbrmatiw e. 
l';x.) era "ni".TIME emerge 
' ~ U ~  "no"-.OF era 
"ni".-TIME emerge 
(emerged irl the era of confi,isiorl) 
77,rmimgtian comlitio~, 
Judges whether tim surnnlarics created so far arc 
sufi-icient. Curreritly the termination coriditiori s 
defined by either the number of produced phrases 
or the total summary length. 
Re-scoring ojrelationu 
I f  the condition is not fll lfi l led, thes;e steps from 
selection of the core relation Must I.)e repeated to 
create another phrase, t}efi)re selecting a new 
core, the scores of  the words used in this cycle are 
reduced to increase the possibility for other words 
to be used in the next phrase. Score reduction is 
achieved by multiplying tile predefined Ctll-dowll 
ratio R (0 < It < 1) by the scores of the words 
used. l,>,ehition scores are re-calculated usin.~, the 
nov, word scores. 
Generation o.f sur~we phrases 
Tiffs process produces I)AGs each of ~laich 
consists of one core relation and several attached 
iclations. In ,latmnesc, the surface phrases can 
be ea.,;il) obtained by connecthlg the still'ace 
string of the nodes in their original order. See 
Chapter 5 for the generatioil method for \]\[:,nglish. 
3 The  Pro to type  
Wc developed a prototype of the summarization 
system based on this algorithm. The development 
language is Java and the system is working on 
Windows 95/c)8/NT and Solaris 2.6 a. 
The time consumed by summarization process 
is in proportion to the text length and it takes 
about 700 rnsec to generate a surnmal T for an Ad 
sized document (2000 Japanese characters) using 
a PC with a Celeron processor (500 Mtlz). Over 
95% of the time is consumed in the relation 
analysis tep. 
4 Eva luat ion  
We have conducted an experiment to evahiate the 
system. This section is a short sumrna W of the 
expei+iment reported iri (()ka and Uedar, 2000). 
The aim of a phrase--represented summary is to 
give fast and accurate sifting of lit results. To 
evahiate whether the aim was achieved? we 
adopted a task-based evahlation (Jing, et al 1998, 
Mani, et al 1998). One of the problems of those 
experiments using human subjects as assessors is 
inaccuracy caused by the diversity of assessment. 
To reduce the diversity, first we assign 10 
sub.iects (experiment participants) fbr each 
sulnnlary sample. The nunlber o f  subjects was 
just I or 2 in the previous task-based experiments. 
Second, we gave the subjects a detailed instruc-- 
tion including the situation that led them to search 
the WWW. 
4,1 Exper iment  Method  
The outline of the evahiation is as follows: 
5 '0'" shows that there ~ll'e i1() particle~; ur any other 
\~,ol'ds Collnccting two ;~,old:-;. ,lapttrics;e dticSll't 
require anything like relative pi+onoun+< 
~' .lava and Solaris are the tra(temarks of Sun 
Microsvstems. Windows and Ccleron tll'O the 
mldcmark!; of Microsoft and lntel, respedively. 
881 
? Assume an inlbrmation need and make a 
queIw for the information eed 
? Prepare simulated WWW search results 
with different ypes of summaries: (A) first 
80 characters, (B) important sentence se- 
lection (Zechner, 1996), (C) phrase- 
represented summary, (I)) keyword enu- 
meration. The documents in the simulated 
search result set are selected so that the set 
includes an appropriate number of relevant 
documents and irrelevant documents. 
? Have subjects judge from the summaries 
the relevance between the search results 
and the given int'ormation need. The 
judgement is expressed in t'our levels (from 
higher to lower: L3, L2, LI, and L0, which 
is judged to be irrelevant). 
? Compare the relevance with the one that we 
assumed. 
The documents the user judges to be relevant 
compose a subset of the IR results and it should 
be more relevant o the information eed than the 
IR results themselves. Because we have 
introduced three relevance levels, we can assume 
three kinds of the subsets; L3 only, L3+L2, and 
L3+L2+LI. The subset composed only from the 
documents with L3 judgement should have a high 
precision score and the subset including L1 
documents should get a high recall score. 
4.2 Result 
Because recall and precision are in a trade~off 
relation, here we show the result using f-measure, 
the balanced score of the two indexes. 
2 * precision* recall 
f - -  meaX l l l ' e  = 
precision + recall 
The fmeasure averages of the experiment 
result of three different asks are shown in Fig. 3. 
It shows that the phrase-represented summaries 
(C) are more suitable tbr sifting search results 
than any other summaries in all cases. 
4.3 Discussion 
The result can be explained using the number of 
summaries that contain clues to the information 
need. Summaries consistin,, of short units 
(phrases (C) and keywoMs (D)) are gathered from 
the wide range of the original text and accord- 
in,.zlv have many chances to include the clues. 
The actual average numbers of summaries that 
phrase-represented Stltl lnlal-~ 
E1A FIB EIC l iD 
,? 
:::J 
i f )  
t'u 
i:11 
E 
ut. 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
0 
T 
Only L3 L2 L3 L1 L2 + L3 
Fig.3 Experiment result 
contain the clues are 2.0, 4.3 and 4.7 for (B) 
sentence, (C) phrases and (D) keywords, respeco 
tively, in spite that (D) keywords include more 
clues than any other samples, they don't get a 
good t-score. The reason is considered to be due 
to the lack of information about the relations 
among keywords. 
5 Applicability to Other Languages 
Although this algoritlun was first developed for 
the Japanese language, the concept of phrase~ 
representation stmunarization is also applicable to 
other languages. Here we show the direction 
toward its extension to t'nglish. 
English has a clear concept of ~'phrase," and 
simply connected words do not produce well- 
formed phrases. I'his requires emantic analysis 
and generation from the semantic structure. 
We will consider the following example again. 
Ex.) A venture company PICORP announced 
to license their environment protection tech- 
nology to AMICO, a U.S. top company. 
l f"PICORP" and "license" must be included in 
the summary and "announce" is not so important, 
"PlCORP license(s)" is the core of the desired 
phrase. Generating it requires ub.iect resolution 
o\[" "license" and thus semantic level analysis is 
required. Moreover, predicate-argument struc- 
tures arc preferable to syntactic trees because the 
sub.iect and the object are represented in the same 
level, thlification gramtnar flameworks uch as 
I,FG (Kaplan and P, restmn. 1082) and tlPSG 
(Pollard and Sag, t994) fulfill these requirements. 
Fig. 4 is a part of the analysis rcsuh represented in
I.FG. 
882 
PRED 
SUBJ 
VCOMP 
'announce( 1" SUB J) ( ? VCOMP)' 
\[1\]\[PRED "PICORP"\] 
PRED 'license(i" SUB J) ( \[ OBJ) ( t  TO OBJ)' 
SUBJ \[11 
OBJ \[PRED 'environment protection technology'\] 
TO E PP TO 1 OBJ \[PRED 'AMICO' \] 
2$ 
SUBJ \[PRED "PICORP"\] 
OBJ \[PRED 'environment protection technology'\] 
TO ~ PP 10 7\] 
OBJ \[PRED 'AMICO' \] 
PICORP licenses erlvironment protection technology to AMICe. 
PICORP's licensing of environment protection technology to AMICe. 
PICORP to license environment protection technology to AMICe (headline style) 
Fig. 4: Analysis and generation of summary 
A score is calculated for each feature structure 
and the core feature structure will be selected by 
its score instead of  selecting a core relation and 
attaching malldatory relations. In the core 
l~mture structure, index \[1\] is replaced by %I, JBJ of 
the top l\]eature structure. 
(}eneratin<,. > phrases t'rOlll the t\:ature structure 
requires templates ?. Several pattern,<; c, an be 
selected io generate phrases: 
V- ing (gerund) tbrm 
ARGI'  s PRED--Ang ARG2 'co ARG3 
notin |'orm 
ARGI ' s  noun (PRF, D) o? ARG2 to  ARG3 
to--infinitive l~}l-nl 
For  ARGI  to  PRED ARG2 to  ARG3 
In this case, tile herin fOFlll ~" lqC()RP 's  license 
c,f the protection technology to AMIC()" is 
avoided because tile noun "qicense" lacks the 
meaning of "action" or "'event. '" ()tiler rules 
specific to headlines such as ~'to-infinitive 
represents |'uture" Call alSO be hltroduced. 
6 Re lated Work  
bllOSt sumnmrization studies ( inc luding Zcchnero 
1996) arc based on inq3oitant sentence selection 
and seek belier selection methods. We have 
+' Generation el" articles is h.'ft to be considered. 
pointed out that sumnmries made by this method 
tend to be btndensome to read, and have proposed 
phrase-representation summarization as an 
alternative. The following studies bear some 
relation to our study. 
The summarization method by Boguraev and 
Kctmedy (1997) adopts ~phrasal expression" 
rather than sentences or paragraphs. However, it 
begins to create a phrase not from a core relation 
but a core word (in their words, "'topic stamp") 
and produces multipk; phrases containing the 
same core word; it is therefore not suitable for 
summaries for sifting IR results. In addition, 
because it does not consider the roles and 
importance of thc attaching arcs when enriching 
the core, less important words are often attached 
to the core. They aimed at supporting fast 
reading rather than sifting IR restllts. 
Some studies are similar to ours in that they 
make sentences short. Wakao, et al (1998) and 
Mikami, ct al. (1998) aim to create closed 
captioning fl-om an announcer's manuscript by 
paraf~hrasing and renlovhlg nlodifiers. This 
method doesnh ronlove \[he "'{l'tlllk ~" o1" the 
analxsis tree and the sunlll~aries canilot be made 
as short as in phrase-representation. 
Na{~ao, el al. (1998) also proposed a ineti~od to 
create summarization based on the i'ehlthms 
883 
between words. They utilize GDA (Global 
Document Annotation), a tag set that the docu- 
ment author inserts into the document and that 
contains linguistic information such as sentence 
structures and reference infimnation. Althot@a 
this method is similar to ours in some points, the 
stlmmaw consists of sentences and thus does not 
have "at-a-glalme" capability. Most of  all, the 
expectation that every doctmlent is tagged 
linguistically will not be fulfilled until special 
editors with automatic linguistic tagging beconte 
popular. 
Conclusion 
We introduced the concept of  "at-a-glance" 
summary and showed an algorithm of phrase- 
representation SUlnmarization as a realization of 
the concept. An experiment shows that the 
summaries are effective for sifting IR results. 
We continue to fine-trine the prototype for 
timber efficacy. 
Acknowledgement  
We would like to thank our laboratory members 
who give us valuable suggestions and participated 
in the experiment. 
References  
Bougraev, t3. and Kennedy, C. (1997): "Salience-based 
Content Characterisation f Text Documents," Proc. 
Intelligent Scalable Text Summarization, pp. 2-9. 
Jing, H., Barzilay, R., McKeown, K. and Elhadad, M. 
(1998): "Summarization Evaluation Methods: 
Experiments and Analysis." In Intelligent Text 
Summarization. pp. 51-59. AAAI Press. 
Kaplan, R. M. and Bresnan, J. (1982): "'Lexical- 
Functional Grammar: A Forlnal System for Gram- 
matical Representation," in Bresnan, J. (ed.) The 
Mental Representation oJ" Gramnzatical Relalions, 
MIT Press. 
Mani, 1., House, D., Klein, G., ttirschman, L., Obrst, 
L., Firmin, T., Chizanowski, M., and Sundheim, B. 
(1998): "'The 77PSTER SL/MM:tC T~:vt Summariza- 
tion Evaluation." Technical P, eport MTR 
98W0000138, MITRE Technical Report. 
Mikami, M., Yamazaki, K., Masuyama, S. and 
Nakagawa, S. (1998): "Summarization of News 
Sentences for Closed Caption Generation," t'roc. 
llq>rksh~q) l)rogram The 4th Anmzal Meeting (71 Tim 
.-l.s'xociation . /br Natural Language l)roce.ssiny, pp. 
14-21 (in Japanese). 
Miyauchi, T., Ol<a, M. and Ueda. Y. (1995):-Key- 
relation technology for text rett+ieval. "" /'roe. the 
SDAIR '95, pp. 469-483. 
Nagao, K. and tlasida, K. (1998): "Autotnatic Text 
Sununarization P, ased on the Global Doctnnent 
Annotation," Proc. COLING-g& pp. 917-92 I. 
Oka, M. and Ueda, Y. (2000): "'Evaluation of Phrase- 
representation Summarization based on Information 
Retrieval Task," Proc. ANLP, NAACL 2000 Work- 
shop o,'z ,4zzlomatic Sumnzarization, pp. 59 -- 68. 
Pollard, C. and Sag, 1. A. (1994): ttead-Driven t'hrase 
Strltctm'e Grammar, The University of Chicago 
Press. 
Salton, G. (1989): :tulomalic 7Z, x/ l'rocessing: The 
7)'an.~/brmation, A alysis, and Retrieval of InJbrma- 
tion by Compttter, Addison-Wesley. 
Wakao, "F., Ehara, T. and Shirai, K. (1998): "Auto? 
matic Summarization for Closed Caption for TV 
News," Proc. Workshop Program The 4t\]l Annual 
Meeting (?/ The Association for Natural La~Nuage 
Processing, 7-13 (in Japanese). 
Zechner, K. (1996): "'Fast Generation of Abstracts 
from General Domain Text Corpora by Extracting 
Relevant Sentences." l'roc. COLING-96, pp. 986.,, 
989. 
884 
Evaluation of Phrase-Representation Summarization 
based on Information Retrieval Task 
Mamiko OKA Yoshihiro UEDA 
Industry Solutions Company, 
Fuji Xerox Co., Ltd. 
430 Sakai, Nakai-machi, Ashigarakami-gun, Kanagawa, Japan, 259-0157 
oka.mamiko@fujixerox.co.jp Ueda.Yoshihiro@fujixerox.co.jp 
Abstract 
We have developed an improved task-based 
evaluation method of summarization, the 
accuracy of which is increased by specifying 
the details of the task including background 
stories, and by assigning ten subjects per 
summary sample. The method also serves 
precision/recall pairs for a variety of situa- 
tions by introducing multiple levels of 
relevance assessment. The method is applied 
to prove phrase-represented summary is 
most effective to select relevant documents 
from information retrieval results. 
Introduction 
Summaries are often used to select relevant 
documents from information retrieval results. 
The goal of summarization for such "indicative" 
use is to serve fast and accurate judgement. We 
have developed the concept of the "at-a-glance" 
summary, and its realization in the Japanese 
language - "phrase-representation summariza- 
tiola" - to achieve this goal (Ueda, et al 2000). 
We have conducted an evaluation experiment to
verify the effectiveness of this summarization 
method. 
There are two strategies for evaluating 
summarization systems: intrinsic and extrinsic 
(Jing, et al 1998). Intrinsic methods measure a
system's quality mainly by comparing the 
system's output with an "ideal" summary. 
Extrinsic methods measure a system's perfor- 
mance in a particular task. The aim of the 
phrase-representation summarization method is 
fast and accurate judgement in selecting 
documents in information retrieval. Thus, we 
adopted a task-based method to evaluate 
whether the goal was achieved. Task-based 
evaluation has recently drawn the attention in 
thq summarization field, because the assumption 
that there is only one "ideal" summary is 
considered to be incorrect, and some experi- 
ments on information retrieval were reported 
(Jing, et al 1998) (Mani, et al 1998) (Mochizu- 
ki and Okunura 1999). However, there is no 
standard evaluation method, and we consider 
that there are some shortcomings in the existing 
methods. Thus, we have developed an improved 
evaluation method and carried out a relatively 
large experiment. 
In this paper, we first give an overview of the 
phrase-representation summarization method. 
We then consider the evaluation method and 
show the result of an experiment based on the 
improved method to demonstrate he effective- 
ness of phrase-representation summarization. 
1 Phrase-Representation Summarization 
Most automatic summarization systems adopt 
the "sentence xtraction" method, which gives a 
score to every sentence based on such charac- 
teristics as the frequency of a word or the 
position where it appears, and selects entences 
with high scores. In such a way, long and 
complex sentences tend to be extracted. 
However, a long and complex sentence is 
difficult to read and understand, and therefore it
is not a suitable unit to compose a summary for 
use in selecting documents. 
To avoid the burden of reading such long and 
complex sentences, we have developed the 
phrase-representation summarization method, 
which represents he outline of a document by a 
series of short and simple expressions 
("phrases") that contain key concepts. We use 
the word "phrase" to representthe simplicity 
59 
characteristic I in a word. 
The phrase-represented summary has the 
following characteristics. 
(1) At-a-glance comprehension 
Because ach unit is short and simple, the user 
is able to grasp the meaning at a glance. 
(2) Adequate informativeness 
Unlike extracted sentences, phrases created by 
this method are not accompanied by informa- 
tion unnecessary for relevance judgement. 
(3) Wide coverage of topics 
Units composing a summary are relatively . 
short, and point various positions of the 
original text. Therefore, even a generic 
summary includes various topics written in a 
J 
document. 
,~ phrase-represented summary is generated 
as follows. 
1. Syntactic analysis to extract he relation- 
ships between words 
2. Selection of an important relation (two 
word sequences connected by an arc) as a 
"core" 
3. Addition of relations necessary for the 
unity of the phrase's meaning (e.g., essen- 
tial cases) 
4. Generation of the surface phrase from the 
selected relations 
An important relation is selected by 
considering both the importance of a word and 
that of  a relation between words. For example, 
predicate-argument relations are considered 
important and noun-modifier relations are given 
low importance scores. Steps \[2\] to \[4\] are 
repeated until specified amount of phrases are 
obtained. Before selecting a new "core," the 
stores for the already selected words are 
decreased to suppress overuse of the same 
words. 
Fig. 1 shows a sample summary created from 
a news article 2 put on WWW. The underlined 
words constitute the core relation of each phrase. 
The word "phrase" as used here is not used in the 
linguistic sense, but an expression for "short" and 
"simple." In Japanese, there is no rigid linguistic 
distinction between a "phrase" mad a "clause." 
2 The original text in Japanese and its outline in 
English can be seen in the following URL. 
http://www, fuiixerox.eo.ip/release/2000/0224..purcha 
.se.html (in Japanese) 
... acauire chemical toner business 3 
Fuji Xerox ... acouires chemical toner 
business of Nippon Carbide Industries Co., 
Inc . . . .  
... new chemical toner that contributes to 
reduce cost in  laser nrinters and to lower 
energy consumption ... 
... strengthen...supplies bu iness ... 
manufacturing facilities of Hayatsuki Plant, ... 
... uniform...each particle ... 
Fig. 1" A sample summary 
2 Evaluation Method 
2.1 Summar izat ion  Methods  to be 
Compared  
In this experiment, we compare the effectiveness 
of phrase-represented summaries to summaries 
created by other commonly used summarization 
methods. From the viewpoint of  the phrase- 
represented summary, we focus the comparison 
of the units that constitute summaries. The units 
to be compared with phrases are sentences 
(created by the sentence xtraction method) and 
words (by the keyword enumeration method). 
We also compare "leading fixed-length 
characters," which are often used as substitutes 
for summaries by WWW search engines. The 
generation method for each summary is 
described as follows. 
(A) Leading fixed-length characters: extract 
the first 80 characters of  the document 
body. 
(B) Sentence xtraction summarization: select 
important sentences from a document. 
The importance score of each sentence is 
calculated from the simple sum of the im- 
portance scores of the words in a sentence 
(Zechner 1996). 
(C) Phrase-representation summarization: 
described in Chapter 1. 
(D) Keyword enumeration summarization: list 
up important words or compound nouns. 
http://www, fujixerox.co.jp/headlineJ2000/0308__nton 
e,r_biz,hlml (in English) 
s This phrase lacks the subject because the original 
sentence lacks it. Cases are usually omitted in 
Japanese if they can be easily inferred. 
60 
I 
I 
I 
I 
I In (B), (C), and (D), the same method of 
calcuiating the importance scores of words is 18ilm~'-,~t~tmm\]m ~~~.~. .~ 
used in common, and lengths of summaries are Ill 
..I kept o be 60 to80 characters. ~ ~  . , , ,~~e~ 
As you can see each summary is generic, i.e. . /  not created for any specific queries. Because the ~ . ~  
I phrase-representation summarization method is 
applied to Japanese, we examine the effective- Relevanti ) 
ness of these four methods in Japanese. =__.- .1 
Relevant i~  2 
I 2.2 Previous Work 
The best-known example of task-based Irrelevant--- .... 3 -~  ! I ~  
I evaluation on information retrieval is the ad hoc ~ ~ ~  I task in the TIPSTER Text Summarization Evaluation Conference (SUMMAC) (Mani, et al ' ') Accuracy 1998). Hand (1997) details the proposed task- 
based evaluation under TIPSTER. Jing, et al 
(1998) describe how various parameters affect 
the evaluation result through a relatively large 
task-based experiment. Evaluation conferences 
like SUMMAC are not yet held for Japanese 
summarization systems 4. Mochizuki and 
Okumura (1999) applied the SUMMAC 
methodology to Japanese summarization 
methods for the first time. Most previous 
experiments are concerned with SUMMAC, 
accordingly the methods resemble each other. 
2.3 Framework of Evaluation 
~Fhe framework of task-based evaluation on 
information retrieval is shown in Fig. 2. 
Task-based evaluation in general consists of 
the following three steps: 
(l) Data preparation: Assume an information 
need, create a query for the information 
need, and prepare simulated search results 
with different types of summaries. 
(2) Relevance assessment: Using the summa- 
des, human subjects assess the relevance 
of the search results to the assumed in- 
formation eeds. 
(3) Measuring performance: Measure the 
accuracy of the subjects' assessment by 
comparing the subjects' judgement with 
the correct relevance. The assessment 
process is also timed. 
4 It is planning to be held in 2000. Further 
information is in the following URL. 
http://www.rd.nacsis.acAp/-ntcadm/workshop/ann2p- 
en.html 
Fig.2: Framework of Task-Based Evaluation 
We designed our evaluation method through 
detailed examination of previous work. The 
consideration points are compared to the 
SUMMAC ad hoc task (Table l). A section 
number will be found in the "*" column if we 
made an improvement. Details will be discussed 
in the section indicated by the number in the 
next chapter. 
3 Improvements 
3.1 Description of Questions 
To assess the relevance accurately, the situation 
of information retrieval should be realistic 
enough for the subjects to feel as if they really 
want to know about a given question. The 
previous experiments gave only a short 
description of a topic. We consider it is not 
sufficiently specific and the interpretation of a 
question must varied with the subjects. 
We selected two topics ("moon cake" and 
"journey in Malay. Peninsula") and assumed 
three questions. To indicate to the subjects, we 
set detailed situation including the motivation to 
know about that or the use of the information 
obtained for each question. This method satisfies 
the restriction "to limit the variation in 
assessment between readers" in the MLUCE 
Protocol (Minel, et ai. 1997). 
61 
For each topic, ten documents are selected 
from search results by major WWW search 
engines, so that more than five relevant 
documents are included for each question. The 
topics, the outline of the questions, the queries 
for WWW search, and the number of relevant 
documents are shown in Table 2. The descrip- 
tion of Question-a2 that was given to the 
subjects is shown in Fig. 3. 
One day just after the mid-autumn festival, my 
colleague Mr. A brought some moon cakes to 
the office. He said that one of his Chinese 
friends had given them to him. They rooked so 
new to us that we shared and ate them at a 
coffee break. Chinese eat moon cakes at the 
mid-autumn festival while Japanese have 
dumplings then. Someone asked a question 
why Chinese ate moon cakes, to-which nobody 
gave the answer. Some cakes tasted sweet as 
we  expected; some were stuffed with salty 
fillings like roasted pork. Ms. B said that there 
were over fifty kinds of filling. Her story made 
me think of a question: 
What kinds of filling are there for moon 
cakes sold at the mid-autumn festival in 
Chinese society? 
Fig. 3: An example of question (Question-a2) 
3.2 Number  of Subjects per  Summary  
Sample 
In the previous experiments, one to three 
subjects were assigned to each summary sample. 
Because the judgement must vary with the 
subjects even if a detailed situation is given, we 
assigned ten subjects per summary sample to 
reduce the influence of each person's assessment. 
The only requirement for subjects is that they 
should be familiar with WWW search process. 
3.3 Relevance Levels 
In the previous experiments, a subject reads a 
summary and judges whether it is relevant or 
irrelevant. However, a summary sometimes does 
not give enough information for relevance 
judgement. In actual information retrieval 
situations, selecting criteria vary depending on 
the question, the motivation, and other 
circumstances. We will not examine dubious 
documents if sufficient information is obtained 
or we do not have sufficient ime, and we will 
examine dubious documents when an exhaustive 
survey is required. Thus, here we introduce four 
relevance levels L0 to L3 to simulate various 
cases in the experiment. L3, L2, and L1 are 
considered relevant, the confidence becomes 
lower in order. To reduce the variance of 
interpretation by subjects, we define each level 
as follows. 
L3: The answer to the given question is found 
.in a summary. 
L2: A clue to the answer is found in a sum- 
mary. 
L l :Apparent clues are not found, but it is 
probable that the answer is contained in the 
whole document. 
L0: A summary is not relevant o the question 
at all. 
If  these are applied to the case of the fare of  
the Malay Railway, the criteria will be 
interpreted as follows. 
L3:An expression like "the berth charge of 
the second class is about RMI5"  is in a 
summary. 
L2: An expression like "I looked into the fare 
of the train" is in a summary. 
L I :A  summary describes about a trip by the 
Malay Railway, but the fare is not referred in 
it. 
3.4 Measures of Accuracy 
In the previous experiments, precision and recal l  
are used to measure accuracy. There are two 
drawbacks to these measurements: (1) the 
variance of the subjects' assessment makes the 
measure inaccurate, and (2) performance of each 
summary sample is not measured. 
Precision and recall are widely used to 
measure information retrieval performance. In 
the evaluation of summarization, they are 
calculated as follows. 
62 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
Documents hat are actually 
relevant in S 
Precision = Documents hat are assessed 
relevant by a subject (S) 
Documents hat are assessed 
relevant by a subject 
Reca l l  = Relevant documents 
In the previous experiments, the assessment 
standard was not fixed, and some subjects 
tended to make the relevant set broader and 
others narrower. The variance reduces the 
significance of the average precision and recall 
value. Because we introduced four relevance 
levels and showed the assessment criteria to the 
subjects, we can assume three kinds of relevance 
document sets: L3 only, L3 + L2, and L3 + L2 + 
L1. The set composed only of the documents 
with L3 assessment should have a high precision 
score. This case represents a user wants to know 
only high-probability information, for example, 
the user is hurried, or just one answer is 
sufficient. The set including L1 documents 
should get a high recall score. This case 
represents a user wants to know any information 
concerned with a specific question. 
Precision and recall represent he perfor- 
mance of a summarization method for certain 
question, however they do not indicate the 
reason why the method presents higher or lower 
performance. To find the reasons and improve a 
summarization method based on them, it is 
useful to analyze quality and performance 
connected together for each summary sample. 
Measuring each summary's performance is 
necessary for such analysis. Therefore, we 
introduce the relevance score, which represents 
the correspondence between the  subject 
judgement and the correct document relevance. 
The score of each pair of subject judgement and 
document relevance is shown in Table 3. 
By averaging scores of all subjects for every 
sample, summary's performances are compared. 
By averaging scores of all summary samples for 
every summarization method, method's 
performances are compared. 
Table 1 : Experimental Method 
Document source Newspaper 
Question 
Number of questions 
Number of documents 
question 
Summary type 
Summarization 
methods 
per 
systems or 
(TREC collection) 
Selected from TREC topics 
20 
50 
User-focused summary 
11 systems 
WWW 
Newly created, including 3.1 
the detailed situation 
3 
10 
Generic summary 
4 methods that utilize 
different units 
qm n ~&b~n~m~ ~ ~ ~ ~ 
Subject 21 information analysts 40 persons who usually 
use WWW search 
Number of subjects assigned to 1 or 2 10 3.2 
each summary sample 
Relevance l vels 2 levels 4 levels 3.3 
(Relevant or irrelevant) (L0, LI, L2, L3) 
~:0)~:. ~ff6rmance.measurmgph.ase.: . ' : ,  :...- . ? :~". . i .,. ... : .  ...... :.:.'.~:. '.::2:7::~,::,~ '~ :?~.~:~:~ 
Measure  of accuracy Precision and recall Precision and recall \] 3.4 
Relevance score I 
63 
Q-a 1 Moon 
cake 
Table 2: Topics and Questions 
5 
Q-a2 
Q-b Journey in 
Malay 
Peninsula 
What is the origin of the Chinese custom to have moon 
cakes inthe mid-autumn? 
moon cake 
& 
mid-autumn What kinds of fillings are there in moon cakes? 
About the train between Singapore and Bankok: Singapore 
How much does it cost? & 
How long does it take? Bankok 
What is the difference in the equipment by the class? & 
(A document containing one of these information is railway 
regarded as relevant.) 
6 
z 
Table 3: Relevance Score 
: ? . ~ . ~ , ~ % _ , ~ % ~  . .~  
~'~ ~ u ~  Relevant Relevant Relevant 
~ $ ~ ~  L3 L2 LI 
~ 10 8 5 
Relevant Irrelevant 
L0 L0 
-2 2 
Irrelevant 
L1 
-5 
Irrelevant 
L2 
-8 
Irrelevant 
L3 
-10 
4 Exper iment Results 
4.1 Accuracy 
4.1.1 Precision and Recall 
The precision and recall are shown in Fig. 4, and 
the F-measure is shown in Fig. 5. The F-measure 
is the balanced score of precision and recall, 
calculated as follows: 
2 * precision * recall 
"- F-measure = precision + recall 
Figures 4 and 5 show that the phrase- 
represented summary (C) presents the highest 
performance. It satisfies both the high precision 
and the high recall requirements. Because there 
are various situations in WWW searches, 
phrase-representation sumtnarization is 
considered suitable in any cases. 
4.1.2 Relevance Score 
The relevance score for each question is shown 
in Fig. 6. The phrase-represented summary (C) 
gets the highest score on average, and the best in 
Question-a2 and Question-b. For Question-al, 
though all summaries get poor scores, the 
sentence xtraction summary (B) is the best 
among them. 
4.2 Time 
The time required to assess relevance is shown 
in Fig. 7. The time for Question-a isa sum of the 
times for Questions al and a2. In the Question-a 
case, phrase-represented summary (C) requires 
the shortest time. For Question-b, leading fixed- 
length characters (A) requires the shortest time, 
and this result is different from the intuition. 
This requires further examination. 
64 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
c -  
o .u  
t~ "5 
t~ 
Only L3 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
L3 + L2 
--O.-- ............... - ] "  ......... 
,0  ~ ~- .  - ,  
- "+"* - - : ' i  . . . . . . . . . . . .  
? 1 ,3 .~+_L I . _ J  
0 0.5 1 
Recall 
Fig. 4: Precision.& Recall 
- . -  A I 
? --A- B 
~ C  
--o-- D' 
E, 
0.81 
0.77 -" . . . . .  
O.6.-.: ~ -- A 
0.5~- 
0.4" --5 ? B 
0.3 ? C 
0.11~ . ND.  
..1 ,...1 + 
,-" _.1 
O + ,a 
High presicion High recall 
Fig. 5: F-measure 
t,., 
O 
O 
O 
O 
t -  
5~ 
i: 
4 
3 
2 
1 
0 
A B C D 
Fig. 6: Relevance Score 
-_3 Q-a 1: 
[] Q-a2i 
UQ-b i 
i 
? Aver l 
age :  
"C 
.m. 
350~ 
30 
25, 
20, 
15, 
10v 
A B C 
Fig. 7: Time 
? Q-a 
iQ-b  
i Ave 
rage 
D 
Table 4: Summaries Containing Clues 
Q-al 
A[B IC ID  
5 
A[  B~'ac  [ D 
6 
Q-b 
A t B I c I  D 
7 
o l  3 151  6 
- 17.73 I 8 I 6.43 
5 Discussion 
Here we analyze the experiment result from 
multiple viewpoint,s: the constituent unit of 
summaries and .the characteristics of questions 
and documents in Section 5.1 and 5.2. We then 
discuss advantages of our experimental method 
in Section 5.3, and language dependency of the 
experiment result in Section 5.4. 
65 
5.1 Comparison of Constituent Unit 
The units that constitute a summary may affect 
the judging process; if the unit is long, the 
number of units appeared in a summary may 
decrease and the summary contains fewer key 
concepts in the original document. We counted 
the number of the summaries that contain the 
clues to the given questions (see Table 4). The 
average numbers are 0.3, 2.0, 4.3 and 4.7 for (A) 
fixed-length characters, (B) sentences, (C) 
phrases and (D) words, respectively. The phrase- 
represented summary (C) and the keyword 
enumeration summary (D) widely cover the 
topics, and they are about twice as wide as the 
sentence xtraction summary (B). The leading 
fixed-length characters (A) contain very few 
clues and this fact supports that this summary 
presents the worst performance (see Section 
4.1). 
In order to compare a summary's perfor- 
mance with a summary's quality, we calculate 
the average relevance score of summaries that 
contain clues. These scores are also shown in 
Table 4. The average score represents the 
informativeness of each summary. Table 4 
shows that the sentence xtraction summary (B) 
and the phrase-represented summary (C) get 
, relatively high scores, but vary with the question. 
Th is  is because sentences and phrases are 
sufficiently informative in most cases, but 
sentences tend to contain unnecessary informa- 
tion, and phrases tend to lack necessary 
information. The keyword enumeration 
summary (D) gets a relatively low score. This is 
because a word is not sufficiently informative to  
enable judgement of whether it is clue to the 
answer, and relations among words are lacked. 
These analyses upport he two characteris- 
tics of the phrase-represented summaries 
described in Chapter 1, that is, adequate 
informativeness and wide coverage of topics. 
5.2 Influence of Question and Document 
The most suitable summarization method may 
depend on the type of question and/or document. 
In the experiment results (see Section 4.1.2), the 
sentence extraction summary (B) and the phrase- 
represented summary (C) get the highest 
relevance score. Therefore, here we focus on 
those two summarization methods and consider 
the influence of questions and documents. 
In selecting questions, we consider two 
factors may affect performance. One is which 
unit an answer is expressed in. Another is 
whether clue words easily come to mind. 
If an answer is expressed as a relation of a 
predicate to its arguments, the phrase- 
representation summarization may be suitable. 
Question-a2 and Question-b are of this case. If 
an answer is expressed as compound relations, 
e.g., reason-consequence relations or cause- 
result relations, the sentence extraction 
summarization may be required. And, if an 
answer is expressed in complex relations of 
sentences, any summarization method of the 
four is not suitable. Questions that sk historical 
background or complicated procedures are 
examples of this kind, e.g., Question-al. 
As for another factor, if clue words easily 
come to mind, the phrase-represented summary 
is suitable for any unit in which an answer is 
expressed. This is because the clues are found 
more easily in short phrases than in long 
sentences. 
In selecting documents, whether a question is 
relevant to the main topic of a document affects 
the performance, because we use generic 
summaries. By sentence xtraction summariza- 
tion, the answer is extracted as a summary only 
when the question is relevant o the main topic. 
Phrase-represented summary is able to cover 
topics more widely, for example, one of the 
main topics or detailed escription of each topic 
(see Section 5.1). Because the characteristic of 
the document is independent of the question, 
which summaries cannot be predicted, and thus 
the phrase-represented summary will give better 
results. 
Through these discussions, we conclude that 
the phrase-representation summarization is 
suitable for various cases, while the sentence 
extraction summarization is for only some 
restricted cases. Though the samples of 
questions and documents are relatively few in 
our experiment, it is sufficient to show the 
effectiveness of the phrase-representation 
summarization. 
66 
5.3 Advantages of our Experimental 
Method 
Our experimental method has the following 
advantages. 
(1) More exact assessment 
(2)Serves precision/recall pairs for a variety of 
situations 
(3)Helps further analysis of problems of a 
summarization method 
5.3.1 More Exact Assessment 
Our experimental method provides more exact 
relevance assessment in the following ways. 
(a) More detailed escription of a question 
We asked the subjects to assess the relevance of 
full documents to each question after the 
experiment. Result shows that 93% of the 
subject judgements match the assumed relevance, 
while only 69% match in the same kind of 
assessment in SUMMAC. The percentage that 
all judgements per document agreed the 
assumed relevance is 33%, while only 17% in 
SUMMAC. This is because the subjects 
comprehended the questions correctly by given 
detailed information about he situation. 
(b) More subjects assigned per summary sample 
We assigned ten subjects to each summary 
sample, while only one or two subjects were 
used in SUMMAC. We examined the difference 
of judgement between the average of ten 
subjects and the first subject of the ten. Result 
"shows that 47% of the first subject's judgement 
differ more than one level from the average. 
This proves that the assessment varies from one 
subject o another, even if a detailed situation is 
given. 
(c) Finer levels of relevance 
We introduced four levels of relevance, by 
which ambiguity of relevance can be expressed 
better. 
5.3.2 Serves precision~recall pairs for a varie(F 
of situations 
According to the four levels of relevance, we 
assume three kinds of relevance document sets. 
This enables to plot the PR curve. 
In evaluation conferences like SUMMAC, 
various summarization methods that are 
developed for different purposes must be 
compared. Using such a PR curve, each method 
can be compared in a criterion that matches its 
purpose. 
5.3.3 Helps further analysis of problems of a 
summarization method 
We have introduced the relevance score, which 
allows each summary to be evaluated. Using this 
score, we can analyze the extrinsic evaluation 
result and the intrinsic evaluation result 
connected together, for example, an evaluation 
result based on information retrieval task and 
that based on Q & A task using the same 
questions. Through such analyses, the text 
quality of summaries or the adequate informa- 
tiveness can be xamined. We ourselves got a lot 
of benefit from the analysis to find problems and 
improve the quality of the summary. 
5.4 Language dependency 
Though experiment method may be applied to 
any other languages, we must consider the  
possibility that our result depends on the 
language characteristics. Japanese text is written 
by mixing several kinds of characters; Kana 
characters (Hiragana and Katakana) and Kanji 
(Chinese) characters, and alphabetic haracters 
are also used. Kanji characters are mainly used 
to represent concept words and Hiragana 
characters are used for function words. The fact 
that they play the different roles makes it easy to 
find the full words. Also Kanji is a kind of 
ideogram and each character has its own 
meaning. Thus, most words can be expressed by 
1 to 3 Kanji characters tomake short phrases (15 
- 20 characters) ufficiently informative. 
Though the basic algorithm to create phrase- 
represented summary itself can be applied to 
other languages by replacing its analysis 
component and generation component, similar 
experiment in that language is required to prove 
the effectiveness of the phrase-represented 
summary. 
Conclusion 
We proposed an improved method of task-based 
evaluation on information retrieval. This method 
can be used to evaluate the performance of 
summarization methods more accurately than is 
possible by the methods used in previous work. 
We carried out a relatively large experiment 
using this method, the results of which show that 
67 
phrase-representation summarization is effective 
to select relevant documents from information 
retrieval results. 
Acknowledgements  
We would like to thank our company members 
who gave valuable suggestions and participated 
in the experiment. 
References  
Hand, T. F. (1997). "A Proposal for Task-based 
Evaluation of Text Summarization Systems." In 
Proceedings of the ACL/EACL'97 Workshop on 
Intelligent Scalable Text Summarization, pp31-38. 
Jing, H., Barzilay, R., McKeown, K. and Elhadad, M. 
(1998). "Summarization Evaluation Methods: 
Experiments and Analysis." In Intelligent Text 
Summarization. pp51-59. AAAI Press. 
Mani, I., House, D., Klein,G., Hirschman, L. Obrst, 
L., Firmin, T., Chizanowski, M., and Sundheim, B. 
(1998). "'The TIPSTER SUMMAC Text Summari- 
zation Evaluation.'" Technical Report MTR 
98W0000138, MITRE Technical Report. 
Minel, J.-L., Nugier, S. and Piat, G. (1997). "'How to 
Appreciate the Quality of Automatic Text Summa- 
rization? Examples of FAN and MLUCE Protocols 
and their Results on SERAPHIN." In Proc. of the 
ACL/EACL'97 Workshop on Intelligent Scalable 
Text Summarization, pp.25-30. 
Mochizuki, H and Okumura, M. (1999). "Evaluation 
of Summarization Methods based on Information 
Retrieval Task." In Notes of SIGNL of the 
Information Processing Society of Japan, 99-NL- 
132, pp41-48. (In Japanese) 
Salton, G. (1989). Automatic Text Processing: The 
Transformation, Analysis, and Retrieval ot" 
"- Information by Computer. Addison-Wesley 
Publishing Company, Inc. 
Ueda, Y., Oka, M., Koyama, T. and Miyauchi, T. 
(2000). "Toward the "At-a-glance" Summary: 
Phrase-representation Summarization Method." 
submitted to COLING2000. 
Zechner, K. (1996). "Fast Generation of Abstracts 
from General Domain Text Corpora by Extracting 
Relevant Sentences." In Proc. of COLING-96, pp. 
986-989. 
68 
