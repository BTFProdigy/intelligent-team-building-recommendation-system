Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 503?507,
Dublin, Ireland, August 23-24, 2014.
SA-UZH: Verb-based Sentiment Analysis
Nora Hollenstein, Michi Amsler, Martina Bachmann, Manfred Klenner
Institute of Computational Linguistics, University of Zurich
Binzmuehlestrasse 14, CH-8050 Zurich, Switzerland
{hollenstein,mamsler,bachmann,klenner}@ifi.uzh.ch
Abstract
This paper describes the details of our
system submitted to the SemEval-2014
shared task about aspect-based sentiment
analysis on review texts. We participated
in subtask 2 (prediction of the polarity
of aspect terms) and 4 (prediction of the
polarity of aspect categories). Our ap-
proach to determine the sentiment of as-
pect terms and categories is based on lin-
guistic preprocessing, including a com-
positional analysis and a verb resource,
task-specific feature engineering and su-
pervised machine learning techniques. We
used a Logistic Regression classifier to
make predictions, which were ranked
above-average in the shared task.
1 Introduction
Aspect-based sentiment analysis refers to the
problem of predicting the polarity of an explicit
or implicit mention of a target in a sentence or
text. The SemEval-2014 shared task required sen-
timent analysis of laptop and restaurant reviews
on sentence level and comprised four subtasks
(Pontiki et al., 2014). The organizers created and
shared manually labelled domain-specific training
and test data sets. Two of the four subtasks dealt
with determining the sentiment of a given aspect
term (explicitly mentioned) or aspect category (ex-
plicitly or implicitly mentioned) in a sentence.
The subtasks we participated in do not include the
recognition of aspects. Given the sentence ?The
sushi rolls were perfect, but overall it was too ex-
pensive.?, ?sushi rolls? is an aspect term, and the
corresponding aspect categories are ?food? and
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
?price?. The correct predictions would be the fol-
lowing:
? Subtask 2 (aspect terms): {sushi rolls? pos-
itive}
? Subtask 4 (aspect categories): {food ? posi-
tive, price ? negative}
To solve these tasks, we introduce a Logis-
tic Regression Model for target-specific sentiment
analysis. Features are derived from a fine-grained
polarity lexicon, a verb resource specifying expec-
tations and effects of the verbs functional roles,
and a compositional analysis. In our experiments
on the restaurant and laptop reviews data for the
SemEval-2014 shared task, we found that im-
provements over the baseline are possible for all
classes except ?conflict?.
2 Related Work
We focus on the question whether fine-grained lin-
guistic sentiment analysis improves target-specific
polarity classification. Existing approaches to
aspect-based sentiment detection have focused on
different aspects of this task, e.g. the identifi-
cation of targets and their components (Popescu
and Etzioni, 2005) and sentence-level composition
(Moilanen and Pulman, 2007). Ding et al. (2008)
and Hu and Liu (2004) produced lexicon-based
approaches, which perform quite well in a large
number of domains, and Blair-Goldensohn et al.
(2008) combined lexicon-based methods and su-
pervised learning. Jiang et al. (2011) used a depen-
dency parser to generate a set of aspect dependent
features for classification. For our system we built
a sentiment composition resembling the one of
L?aubli et al. (2012), which was developed for Ger-
man. Moreover, our verb resource has some simi-
larity with the one of Neviarouskaya et al. (2009):
both rely on verb classes and utilize verb-specific
503
behavior. However, only we specify the individ-
ual verb?s (default) perspective on each role (and
are, thus, able to count polar propagations). See
also Reschke and Anand (2011), who describe in
detail how polar (verb) complements combine to
verb frame polarity (again without recording and
using role perspectives as we do).
3 System Description
In this section we present the details of our senti-
ment analysis system. We used the same prepro-
cessing and learning algorithm for both subtasks
(2 & 4). Only the feature extraction was expanded
in subtask 4 for determining the polarities of as-
pect categories (see section 3.3). The data sets
consisted of restaurant and laptop reviews, which
comprise about 3?000 manually classified target-
specific sentences for each domain.
3.1 Sentiment Composition
The fundamental steps of our sentiment analysis
system are parsing the sentences, rule-based sen-
timent analysis using a polarity lexicon and a verb
resource, feature extraction and training a machine
learning algorithm. In this section we will de-
scribe the composition of the lexicon as well as the
structure of the sentiment composition pipeline.
Category Example
POS strong ?awesome?
POS weak ?adequate?
NEG strong ?catastrophe?
NEG weak ?demotivated?
POS active ?generous?
POS passive ?noteworthy?
NEG active ?rebellion?
NEG passive ?orphaned?
Table 1: Additional categories in our fine-grained
polarity lexicon
The same polarity lexicon was used for both
domains. After mapping the polarities from the
lexicon to the words and multi-word expressions,
we calculated the polarity of nominal (NPs) and
prepositional phrases (PPs) by means of lexical
marking and the syntactic analysis of a depen-
dency parser (Choi and Palmer, 2011). We did not
implement any rules for neutral phrases, all words
and phrases not marked as positive or negative are
considered as neutral. In general, the polarities are
propagated bottom-up to their respective heads of
the NPs/PPs in composition with the subordinates.
Shifters and negation words are also taken into ac-
count. The parser output is converted into a con-
straint grammar (CG) format for the subsequent
analysis of words and phrases. To conduct this
composition of polarity for the phrases we imple-
mented a CG with the vislcg3 tools (VISL-group,
2013). The next stage of our sentiment detection
is the verb resource, which was also implemented
with the vislcg3 tools and will be explained in the
next section.
3.2 Verb-based Sentiment Analysis
In order to combine the composition of the po-
lar phrases with verb information, we encoded the
impact of the verbs on polarity using three di-
mensions: effects, expectations and verb polarity.
While effects should be understood as the outcome
instantiated through the verb, expectations can be
understood as anticipated polarities induced by the
verb. Effects and expectations are assigned to sub-
jects or objects, not to the verb itself. A positive
or negative verb effect propagates from the verb to
a subject or object if the latter receives the polar-
ity of the verb. For a verb expectation, the subject
or object is expected to be polar and thus receives
a polarity even if the sentiment composition re-
sulted neutral (see examples below). The verb po-
larity as such is the evaluation of the whole verbal
phrase. Moreover, we process predicative and pas-
sive verbs, adapting the effects and expectations to
the syntactic structure.
Since these effects and expectations match di-
rectly to the subject and objects of a sentence,
they are of great use detecting the polarity of as-
pect terms (which are predominantly subjects or
objects). We present the following examples ex-
tracted from the training data to illustrate three di-
mensions annotated by the verb analysis:
? Example of a positive effect on the direct ob-
ject of a sentence induced by the verb: ?I
love (verb POS) the operating system and the
preloaded software (POS EFF).?
? Example for a negative expectation on a
prepositional object induced by the verb:
?[...] the guy, who constantly com-
plains (verb NEG) about the noise level
(NEG EXP).?
? Example of positive predicative effects
with an auxiliary, non-polar verb: ?Ser-
504
vice (POS predicative) is (verb PRED) great,
takeout (POS predicative) is (verb PRED)
good too.?
Furthermore, we make a distinction between the
different prepositions a verb can invoke and the
succeeding semantic changes. For example, the
verb ?to die? can be annotated in three different
manners, depending on the prepositional object:
1. ?My phone died (verb NEG).?
2. ?Their pizza (POS EFF) is to die (verb POS)
for.?
3. ?He died (verb NEG) of cancer
(NEG EXP).?
To summarize, in addition to verb polarity, we
introduce effects and expectations to verb frames,
which are determined through the syntactic pattern
found, the bottom-up calculated phrase polarities
and the meaning of the verb itself. We manually
categorized approx. 300 of the most frequent pos-
itive and negative English verbs and their respec-
tive verb frames.
Laptop reviews
Feature Occurrences in %
Verbs effects 367 12.05
Verb expectations 6 0.02
Predicatives 298 9.78
Polar verbs 530 17.39
Restaurant reviews
Feature Occurrences in %
Verbs effects 246 8.09
Verb expectations 12 0.04
Predicatives 378 12.43
Polar verbs 521 17.13
Table 2: Occurrences and percentage of sentences
of annotated polar verb features in the training data
of the shared task
In table 2, we illustrate the relevance of the lin-
guistic features of this verb resource by showing in
how many sentences of the training set these anno-
tations appear. Since we merely annotated the verb
frames of the most frequent English verbs, it is
conceivable that this resource may have a consid-
erably greater effect if more domain-specific verbs
are modelled.
After this final sentiment composition step, all
derived polarity chunks are converted into a set of
features for machine learning algorithms.
3.3 Feature Extraction
In a first step of our system, the sentences are
parsed, phrase polarities are calculated and verb
effects and expectations are assigned. Subse-
quently, a feature extractor, which extracts and ag-
gregates polar information, operates on the out-
put. The Simple Logistic Regression classifier
from weka Hall et al. (2009) is then trained on
these features.
We developed a feature extraction pipeline that
retrieves information about various polarity levels
in words, syntactic functions and phrases of the
sentences in the data set. In order to use our senti-
ment composition approach for machine learning,
we extract three different sets of features, result-
ing in a total of 32 features for subtask 2 and 39
features for subtask 4.
In short, the feature sets are constructed as fol-
lows:
? Lexicon-based features: These features com-
prise simple frequency counts of positive and
negative words in the sentences and binary
features showing whether any positive or
negative, strong or active tokens are present
at all. Furthermore, these features not only
include absolute counts but also token ratios.
? Composition-based features: This feature set
describes the information found in nomi-
nal, prepositional and verbal phrases, such
as the number of positive/negative phrase
heads or predicative verb effects found. It
is also possible to distinguish between fea-
tures which represent frequency counts and
features which represent polarity ratios.
? Target-specific features: This set includes
features from the previous two sets in con-
nection with the aspect terms, e.g. whether
the aspect term has a verb expectation or
whether the aspect term is the head of a neg-
ative/positive phrase, the subject or direct ob-
ject, etc. In this set we also include accu-
mulative features that represent the complete
amount of polar information in connection
with an aspect term.
? (only for subtask 4) Category-specific fea-
tures: These features are based on a co-
occurrence analysis of the most frequent
words used in each category. That is to
505
say, we calculated the frequencies of all po-
lar nouns, verbs and adjectives that appear in
sentences of the same category in order to
find category-specific words which have an
influence on the polarity. This set includes
features such as the number of category-
specific words occurring in the sentence, etc.
For the classification of the aspect terms and
categories of the sentences into the four classes
(positive, negative, neutral and conflict), we
trained a Simple Logistic Regression classifier on
the features described above. We also explored
other machine learning algorithms such as SVMs
and artificial neural networks, however, the Logis-
tic Regression proved to yield the best results.
4 Results & Discussion
In this section we present and discuss the results
of our system in the SemEval 2014 shared task.
The results of our submission for subtasks 2 and 4,
compared to the majority baselines, can be found
in table 3. Our system performs significantly bet-
ter on restaurant reviews than on laptop reviews,
probably due to the fact that our polarity lexi-
con comprises more restaurant-specific vocabu-
lary than computer-specific vocabulary.
Subtask Data Baseline Acc.
(2) Laptops 47.06 58.30
(2) Restaurants 57.8 70.98
(4) Restaurants 59.84 73.10
Table 3: Shared-Task results for subtask 2 (aspect
term polarity) and subtask 4 (aspect category po-
larity)
In both subtasks, calculating the polarity of the
aspect terms and the aspect categories, the class
positive scores better than the three other classes.
In all data sets and all subtasks positive was the
majority class of the four-partite classification:
42% in the aspect terms of the laptop reviews, 59%
in the aspect terms and aspect categories of the
laptop reviews equally (measured in the training
data). Thus, it is not surprising that the most fre-
quent error of our system is to categorize neutral
aspect terms and categories as positive.
We do not achieve any improvements for the
class conflict. The latter is very hard to detect, not
only because this class is difficult to define but also
because of the lack of training data given for this
class. This could not be improved even though
we included lexical features to address this par-
ticular class, for example, Boolean features show-
ing whether an adversative conjunction is present
in the sentence or whether the count of positive
chunks equals the count of negative chunks in the
same sentence. These features are in line with
the theory that aspects are considered controver-
sial if positive and negative occurrences are bal-
anced and no polarity clearly prevails. Further-
more, the conflictive facet of a sentence is fre-
quently not represented in the words (e.g. ?It has
no camera, but I can always buy and install one
easy.?; camera = conflict). Thus, it becomes chal-
lenging to generate features for this class conflict
with a lexicon-based approach.
Furthermore, since our verb resource was newly
implemented, there are still many verbs (espe-
cially domain-specific verbs) which will have to
be modelled in addition to the most frequent En-
glish verbs included in the analysis by now. An-
other limitation of our current system is the fact
that verb negation is not yet implemented: We
process negation occurring in noun phrases (e.g.
?a not so tasty chicken curry?), but not when the
negation word relates to the verb (e.g. ?we didn?t
complain?).
In summary, our aspect-based sentiment anal-
ysis pipeline takes into consideration many lin-
guistic characteristics relevant for detecting opin-
ion, and still provides the possibility to expand our
compositional resources.
5 Conclusion
Given the above-average results obtained in the
shared task system ranking, we conclude that the
method for aspect-based sentiment analysis in re-
view texts presented in this paper yields competi-
tive results. We showed that the performance for
this task can be improved by using linguistically
motivated features for all classes except conflict.
We presented a supervised aspect-based senti-
ment analysis system to detect target-specific po-
larity with features derived from a fine-grained po-
larity lexicon, a verb resource and compositional
analysis based on a dependency parser. Our results
have shown that deeper linguistic analysis can pos-
itively influence the detection of target-specific
polarities on sentence level in review texts.
506
Acknowledgements
We would like to thank the organizers of the
shared task for their effort, as well as the reviewers
for their helpful comments on the paper.
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan
McDonald, Tyler Neylon, George A. Reis, , and
Jeff Reynar. Building a sentiment summarizer
for local service reviews. In WWW Workshop on
NLP in the Information Explosion Era, 2008.
Jinho D. Choi and Martha Palmer. Getting the
most out of transition-based dependency pars-
ing. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguis-
tics: Human Language Technologies, HLT ?11,
pages 687?692, Stroudsburg, PA, USA, 2011.
ACL.
Xiaowen Ding, Bing Liu, and Philip S. Yu. A
holistic lexicon-based approach to opinion min-
ing. In Proceedings of the 2008 International
Conference on Web Search and Data Mining,
WSDM ?08, pages 231?240, New York, NY,
USA, 2008. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H.
Witten. The weka data mining software: An
update. SIGKDD Explor. Newsl., 11(1):10?18,
November 2009.
Minqing Hu and Bing Liu. Mining and sum-
marizing customer reviews. In Proceedings of
the Tenth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Min-
ing, KDD ?04, pages 168?177, New York, NY,
USA, 2004. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu,
and Tiejun Zhao. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 151?160. Associa-
tion for Computational Linguistics, 2011.
Samuel L?aubli, Mario Schranz, Urs Christen, and
Manfred Klenner. Sentiment Analysis for Me-
dia Reputation Research. In Proceedings of
KONVENS 2012 (PATHOS 2012 workshop),
pages 274?281, Vienna, Austria, 2012.
Karo Moilanen and Stephen Pulman. Sentiment
composition. In Proceedings of RANLP-2007,
pages 378?382, Borovets, Bulgaria, 2007.
Alena Neviarouskaya, Helmut Prendinger, and
Mitsuru Ishizuka. Semantically distinct verb
classes involved in sentiment analysis. IADIS
AC (1), 2009.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos,
and Suresh Manandhar. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. In Proceed-
ings of the 8th International Workshop on Se-
mantic Evaluation (SemEval 2014), Dublin, Ire-
land, 2014.
Ana-Maria Popescu and Oren Etzioni. Extraction
of product features and opinions from reviews.
In Proceedings of HLT-EMNLP-05, pages 339?
349, Vancouver, Canada, 2005.
Kevin Reschke and Pranav Anand. Extracting
contextual evaluativity. In Proceedings of the
Ninth International Conference on Computa-
tional Semantics, pages 370?374, 2011.
VISL-group. http://beta.visl.sdu.dk/cg3.html. In-
stitute of Language and Communication (ISK),
University of Southern Denmark, 2013.
507
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 38?43,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Measuring the Public Accountability of
New Modes of Governance
Bruno Wueest
Institute of Political Science
University of Zurich
wueest@ipz.uzh.ch
Gerold Schneider
Institute of Computational Linguistics
University of Zurich
gschneid@ifi.uzh.ch
Michael Amsler
Institute of Computational Linguistics
University of Zurich
mamsler@ifi.uzh.ch
Abstract
We present an encompassing research en-
deavour on the public accountability of
new modes of governance in Europe. The
aim of this project is to measure the
salience, tonality and framing of regula-
tory bodies and public interest organisa-
tions in newspaper coverage and parlia-
mentary debates over the last 15 years.
In order to achieve this, we use language
technology which is still underused in po-
litical science text analyses. Institution-
ally, the project has emerged from a col-
laboration between a computational lin-
guistics and a political science department.
1 Introduction
The institutionalization of the regulatory state in
Europe entailed new modes of governance such
as transgovernmental networks between officials
and non-state authorities or the involvement of pri-
vate corporations (e.g. rating agencies) in the pol-
icy processes (Gilardi, 2005; Abbott and Snidal,
2008). At the subnational level, the emergence
of regulatory agencies and public-private partner-
ships spreading across metropolitan regions have
come to challenge traditional state institutions
(Kelleher and Lowery, 2009). Since these new
modes of governance organize political authority
along functional rather than territorial lines, many
observers are worried about their potential ?demo-
cratic deficit? (Dahl, 1994; Follesdal and Hix,
2006; Keohane et al., 2009). In response to these
considerations, scholars usually point to the ad-
ministrative and professional accountability mech-
anisms of governmental and parliamentary over-
sight as well as judicial review (Majone, 2000;
Lodge, 2002; Busuioc, 2009). Other, more in-
formal accountability mechanisms such as media
coverage and public involvement, in contrast, have
been either neglected, dismissed as scarcely rele-
vant or dealt with only in comparative case stud-
ies (Maggetti, 2012). This is surprising, given that
public communication plays an ever more deci-
sive role for setting the political agenda and estab-
lishing transparency of policy making in modern
democratic societies (Walgrave et al., 2008; Koop-
mans and Statham, 2010; M?uller, forthcoming).
With respect to the public accountability of new
modes of governance, the media can thus be ex-
pected to constitute a key intermediary variable for
the progressive formalization and institutionaliza-
tion of voluntary private rules through reputational
mechanisms (Gentzkow and Shapiro, 2006).
This paper is structured as follows. In section 2
we present our core research question, in section
3 we summarize our research methods, and in sec-
tion 4 we briefly present a pilot study.
2 Research Question
It is important to ask whether and to what ex-
tent public communication systematically exposes
new modes of governance to public accountabil-
ity. More precisely, the project?s ambition is to
determine how much attention the media and par-
liamentary debates dedicate to survey the regula-
tory bodies and public interest organizations un-
der scrutiny, whether they watch these actors crit-
ically, and whether they report on these actors in
terms of frames which are conductive to their pub-
lic accountability, e.g. norm and rule compliance,
transparency, efficiency or responsiveness to pub-
lic demands.
38
3 Methodology
To answer these questions, the project implements
approaches developed in computational linguistics
and web automation in order to collect and classify
big text data at the European level (European and
internationally relevant newspapers), the domes-
tic level in four countries (newspapers in the U.K.,
France, Germany and Switzerland), and the sub-
national level in eight metropolitan areas (parlia-
mentary debates and newspapers relevant for Lon-
don, Birmingham, Paris, Lyon, Berlin, Stuttgart,
Berne and Zurich). The project (1) starts from an
encompassing gazetteer of actors involved in the
new modes of governance in the areas and coun-
tries mentioned above, (2) uses application pro-
gramming interfaces (API) and webscraping tech-
niques to establish a large representative text cor-
pus in English, French and German, (3) calculates
the salience of the actors of interest by means of
named entity recognition, coreference resolution
and keyword detection, (4) applies sentiment de-
tection and opinion mining to estimate the tonality
of these actors, (5) uses relation mining methods
(Schneider et al., 2009) to detect interactions and
types of interactions between the entities of inter-
est, and (6) intends to automate the recognition of
media frames used in the context of these actors by
identifying hidden topics via latent semantic anal-
ysis (LSA) (McFarlane, 2011; Odijk et al., 2014).
As points 3-6 provide key research challenges,
we will discuss them in more detail in the fol-
lowing subsections. Before that, we present an
overview of our current pipeline.
3.1 Pipeline
The pipeline consists of several components
chained together in a modular way (see Figure 1).
This provides us with the possibility to exchange
components on demand. First, data acquisition is
done via the use of an API to the media content
database (e.g. LexisNexis). This allows us to fully
automate the retrieval and storage of the media
documents.
At a second stage, we employ a full natural lan-
guage processing chain which includes morpho-
logical analysis, tagging, lemmatizing, and depen-
dency parsing. On this basis, we then conduct sev-
eral more layers of analysis. On the one hand, we
use the result of the preprocessing chain for coref-
erence resolution and sentiment analysis as well
as relation mining. On the other hand, we also
integrate further tools such as named entity recog-
nition and LSA which can be applied on the full
text or corpus level. The thus enriched data is then
aggregated and stored in a database.
Figure 1: Scheme of pipeline
Finally, the actual data analysis can be con-
ducted by querying the database, based on the al-
ready available information or an adapted setting
suitable to the requirements of the media content
analysis.
3.2 Salience, Named Entities and
Coreferences
One of the main metrics of interest is the salience
of the entities. Therefore, a reliable detection of
the entities in the articles is a pivotal task. Fur-
thermore it is crucial to find those occurences of
entities in the text which are not directly detectable
by using a gazetter, since journalists often use ref-
erences to the entities in the same article. Hence,
we will integrate coreference resolution (Klenner
and Tuggener, 2011) into our pipeline. In addi-
tion, we will also create a resource which will
allow us to integrate external information on the
entities, thus increasing the performance of the
coreference resolution. For example, politicians
are often mentioned with their name, their func-
tion (e.g. National Council), their party affiliation,
their age, or a combination of such attributes. To-
gether with the metadata of the media documents
(media source, and time of publication) it is then
possible to calculate these attributes and possible
combinations and include them in the coreference
resolution module in order to increase both preci-
sion and recall.
3.3 From Sentiment Detection to Opinion
Mining
Sentiment analysis and opinion mining are re-
search areas in computational linguistics which
have received growing attention in the last decade
(Pang and Lee, 2008; Liu and Zhang, 2012). In or-
der to detect the tonality in the media coverage to-
39
wards the actors under scrutiny, we use a lexicon-
based compositional sentiment analysis system
component similar to Taboada et al. (2011). How-
ever, our approach is additionally based on the out-
put of the full dependency parse and the results of
the named entity recognition and coreference res-
olution. This will provide us with the ability to
perform target-specific tonality calculation.
In addition to the mere calculation of sentiment
or tonality over a whole article, our task includes
the detection of sentiment on the sentence level
and in respect to certain targets (i.e. entities). An
additional challenge is to detect quotations includ-
ing their sources and targets, since they may reveal
the actors? most opinionated stances towards each
other (Balahur et al., 2009). From this perspec-
tive, opinion mining can be seen as a sister disci-
pline to sentiment analysis, which we can employ
to map utterances of actors towards other actors,
or towards specific political topics, stepping from
classical sentiment detection to relation and opin-
ion mining. We will focus on high precision as-
signment of the source of the statement.
It is important to note that the detection and
determination of sentiment and opinion in me-
dia documents is a challenging endeavour since
it differs in many ways from the task of previ-
ous research which has mostly considered reviews
and other clearly opinionated text (Balahur et al.,
2010). It will therefore also be necessary to adapt
the sentiment analysis system to the domain of
(political) news text and to use advanced tech-
niques to match fine-grained targets and the en-
tity to which they belong. For example, it should
be possible to assign statements of a spokesperson
to the institution he or she represents. However,
we can build on existing research, since such a
mapping can be considered similar to aspect-based
opinion mining (Zhang and Liu, 2014).
3.4 Relation Mining
In well-resourced areas such as biomedical rela-
tion mining, the detection of interactions between
entities such as genes and proteins or drugs and
diseases is an established research focus. Training
resources are abundant, and several systems have
been evaluated in competitive challenges. Polit-
ical science texts are typically less richly anno-
tated. However, it is also possible to learn pat-
terns expressing interactions from lean document-
level annotation, by using distance-learning meth-
ods. If a document is annotated as containing the
key actors A and B, then all syntactic connec-
tions found in that document between A and B can
be assumed to provide patterns typically express-
ing interactions. Such approaches have been used
in biomedicine (Rinaldi et al., 2012) and can be
ported to the political domain.
3.5 Media Frames
Associative Framing (van Atteveldt et al., 2008)
is based on measuring co-occurrence in large con-
text windows. His suggested association measure
is also different, he uses the conditional probabil-
ity of seeing concept 1 (c1) in the context of con-
cept 2 (c2), p(c1|c2). Sahlgren (2006) describes
how short context windows tend to detect syntag-
matic relations like collocations, while large con-
text windows detect paradigmatic relations. In van
Atteveldt et al. (2008), concepts are basically key-
words, while we will use vector space models,
which allow one to automatically detect concepts.
In vector space model approaches, each word is
defined by the sum of its contexts, and words
which have very similar contexts are clustered into
a concept. There are many variants of this ap-
proach: in singular-value decomposition (SVD) or
latent semantic analysis (LSA) approaches (Deer-
wester et al., 1990), the original very high dimen-
sional space is reduced to fewer dimensions. In
Word Space (Sch?utze, 1998) each word is defined
recursively, by the contexts of its contexts, using
an observation window of up to 100 words be-
fore and after the target word. Rothenh?ausler and
Sch?utze (2009) have shown that approaches using
syntactic relations instead of large context win-
dows can even perform better.
In the political communication literature, the
definition of frames is contested. Matthes and
Kohring (2008) thus suggest a bottom-up, data-
driven and interactive method which on the one
hand offers the possibility to correct and guide au-
tomatic approaches as has been exemplified by Hu
et al. (2011), on the other hand the rigid consis-
tency of automatic approaches can also add new
insights for data interpretation.
4 Pilot Study
As a short glimpse at the potential of our research
we present first data from a small pilot study. The
depth of the analysis is still limited due to the not
yet fully functional pipeline. In a first step, we col-
40
lected 4445 articles from the last ten years in three
large German print and online news sources. The
institutions under scrutiny are (private) associa-
tions for technical inspection in Germany. In this
area, the T
?
UV (Technischer
?
Uberwachungsverein,
i.e., Technical Inspection Association) and its sub-
companies almost exert a regulatory monopoly.
As a first goal, we want to investigate the differ-
ence in the tonality in the media coverage towards
the institutions in this area. We therefore chose to
investigate a public scandal revolving on defective
breast implants that have been tested and certified
by a T
?
UV subcompany. Table 1 reports the results.
Institution Articles Tonality
Name n negative ambivalent neutral positive
T
?
UV 57 47 5 3 2
T
?
UV subcompanies 45 39 3 2 1
Other institutions 10 6 2 0 2
Table 1: Absolute counts of articles about breast
implants and tonality per institution
A first interesting finding is that we only found
articles about breast implants in the last 3 years.
Considering the sentiment analysis results for
these articles, we see a clearly negative aggre-
gated result. 82.1% of the articles were of negative
tonality, compared to only 4.5% positive tonal-
ity. The remaining articles were of neutral (4.5%)
or ambivalent (8.9%) tonality. The percentage of
negative articles is even larger if only articles con-
taining mentions of T
?
UV and its subcompanies are
considered (84.3%), while the percentage of posi-
tive articles drops to 2.9%.
Furthermore, these findings are in line with the
increase in negative articles on T
?
UV subcompa-
nies during these years (see Figure 2). In fact,
from all negative articles about the T
?
UV subcom-
panies, 28.8% in 2012 and even 38.2% in 2013
contained mentions of breast implants. The scan-
dal itself was therefore responsible for the increase
in negative articles in this period.
This development can be interpreted as an in-
dication for the accountability of such institutions
in the public media, although it remains an open
question which aspects were dominant in the pub-
lic discourse considering the scandal about the
breast implants.
In sum, this pilot study increases our confidence
to be able to successfully collect the necessary
data for our main purpose, i.e. to answer the ques-
tion whether new forms of governance are held
accountable in the media. In the near future, we
Figure 2: Percentage and raw counts of negative
(breast implant) articles for T
?
UV subcompanies
plan to implement approaches that allow us to in-
ductively detect the issues brought forward in the
context of an actor in a selection of texts. More
precisely, we are planning to describe and detect
the dynamics of the debate in articles as well as
the tonality inside them.
5 Conclusions
We have introduced a project measuring media
coverage and applying opinion and relation min-
ing to the question of accountability of new modes
of governance in Europe. To answer how public
communication exposes them to public account-
ability, we apply computational linguistics meth-
ods ranging from named entity recognition, de-
pendency parsing and coreference resolution to
opinion and relation mining and ultimately fram-
ing.
We have given a pilot study on a public scandal
involving defective breast implants that have been
tested and certified by a T
?
UV subcompany in Ger-
many. We find, on the one hand, that most of the
articles on breast implants during the period are of
negative tonality, and on the other hand, that a cor-
responding proportion of negative articles on T
?
UV
mentions breast implants, explaining the spike in
negativity. In future research, we will detect such
spikes in a data-driven fashion and with the help of
targeted opinion and relation mining approaches.
Acknowledgments
This research is supported by the Swiss National
Science Foundation project NCCR democracy
1
.
1
http://www.nccr-democracy.uzh.ch
41
References
Kenneth W. Abbott and Duncan Snidal. 2008. The
governance triangle: regulatory standards institu-
tions and the shadow of the state. In Walter Mattli
and Ngaire Woods, editors, The Politics of Global
Regulation. Princeton University Press, Princeton,
NJ.
Alexandra Balahur, Ralf Steinberger, Erik van der
Goot, Bruno Pouliquen, and Mijail Kabadjov. 2009.
Opinion mining on newspaper quotations. In Pro-
ceedings of the 2009 IEEE/WIC/ACM International
Joint Conference on Web Intelligence and Intelligent
Agent Technology-Volume 03, pages 523?526. IEEE
Computer Society.
Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov,
Vanni Zavarella, Erik van der Goot, Matina Halkia,
Bruno Pouliquen, and Jenya Belyaeva. 2010. Sen-
timent analysis in the news. In Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation (LREC?10), Valletta, Malta,
may.
Madalina Busuioc. 2009. Accountability, control and
independence: the case of European agencies. Eu-
ropean Law Journal, 15:599?615.
Robert A. Dahl. 1994. A democratic dilemma: System
effectiveness versus citizen participation. Political
Science Quarterly, 109(1):23?34.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Andreas Follesdal and Simon Hix. 2006. Why there is
a democratic deficit in the EU: A response to Majone
and Moravcsik. JCMS: Journal of Common Market
Studies, 44:533?562.
Matthiew Gentzkow and Jesse M. Shapiro. 2006. Me-
dia bias and reputation. Journal of Political Econ-
omy, 114(2):280?316.
Fabrizio Gilardi. 2005. The institutional foundations
of regulatory capitalism: The diffusion of indepen-
dent regulatory agencies in Western Europe. Annals
of the American Academy of Political and Social Sci-
ence, 598:84?101.
Yuening Hu, Jordan Boyd-Graber, and Brianna Sati-
noff. 2011. Interactive topic modeling. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 248?257, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Christine A. Kelleher and David Lowery. 2009. Cen-
tral city size, metropolitan institutions and political
participation. British Journal of Political Science,
39(1):59?92.
Robert O. Keohane, Stephen Macedo, and Andrew
Moravcsik. 2009. Democracy-enhancing multilat-
eralism. International Organization, 63(1):1?31.
Manfred Klenner and Don Tuggener. 2011. An
incremental entity-mention model for coreference
resolution with restrictive antecedent accessibility.
In G Angelova, K Bontcheva, R Mitkov, and
N Nikolov, editors, Recent Advances in Natural Lan-
guage Processing (RANLP 2011), Proceedings of
Recent Advances in Natural Language Processing,
pages 178?185, September.
Ruud Koopmans and Paul Statham. 2010. The Making
of a European Public Sphere. Media Discourse and
Political Contention. Cambridge University Press,
Cambridge, MA.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text Data,
pages 415?463. Springer.
Martin Lodge. 2002. The wrong type of regulation?
regulatory failure and the railways in Britain and
Germany. Journal of Public Policy, 22:271?297.
Martino Maggetti. 2012. The media accountability of
independent regulatory agencies. European Politi-
cal Science Review, 4(3):385?408.
Giandomenico Majone. 2000. The credibility crisis of
community regulation. Journal of Common Market
Studies, 38:273?302.
J?org Matthes and Matthias Kohring. 2008. The con-
tent analysis of media frames: toward improving re-
liability and validity. Journal of Communication,
58:258?279.
Delano J. McFarlane, 2011. Computational Methods
for Analyzing Health News Coverage. PhD disserta-
tion, Columbia University.
Lisa M?uller. forthcoming. Patterns of Media Perfor-
mance: Comparing the Contribution of Mass Me-
dia to Democratic Quality Worldwide. Palgrave
Macmillan, Houndmills, UK.
Daan Odijk, Bjorn Burscher, Rens Vliegenthart, and
Maarten de Rijke, 2014. Automatic Thematic Con-
tent Analysis: Finding Frames in News. unpub. Ms.,
Amsterdam, NL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Fabio Rinaldi, Gerold Schneider, and Simon
Clematide. 2012. Relation mining experi-
ments in the pharmacogenomics domain. Journal
of Biomedical Informatics.
Klaus Rothenh?ausler and Hinrich Sch?utze. 2009.
Unsupervised classification with dependency based
word spaces. In Proceedings of the Workshop on
Geometrical Models of Natural Language Seman-
tics, pages 17?24, Athens, Greece, March. Associ-
ation for Computational Linguistics.
42
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional Analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Gerold Schneider, Kaarel Kaljurand, and Fabio Ri-
naldi. 2009. Detecting protein-protein interactions
in biomedical texts using a parser and linguistic re-
sources. In Computational Linguistics and Intelli-
gent Text Processing, volume 5449, pages 406?417,
Berlin, DE. CICLing, Springer.
Hinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Wouter van Atteveldt, Jan Kleinnijenhuis, and Nel
Ruigrok. 2008. Parsing, semantic networks, and
political authority: Using syntactic analysis to ex-
tract semantic relations from dutch newspaper arti-
cles. Political Analysis, 16(4):428?446.
Stefaan Walgrave, Stuart Soroka, and Michiel Nuyte-
mans. 2008. The mass media?s political agenda-
setting power: A longitudinal analysis of media, par-
liament, and government in Belgium (1993 to 2000).
Comparative Political Studies, 41:814?836.
Lei Zhang and Bing Liu. 2014. Aspect and entity ex-
traction for opinion mining. In Data Mining and
Knowledge Discovery for Big Data, pages 1?40.
Springer.
43
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 18?23,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Inducing Domain-specific Noun Polarity Guided by Domain-independent
Polarity Preferences of Adjectives
Manfred Klenner
Computational Linguistics
University of Zurich
Switzerland
klenner@cl.uzh.ch
Michael Amsler
Computational Linguistics
University of Zurich
Switzerland
mamsler@ifi.uzh.ch
Nora Hollenstein
Computational Linguistics
University of Zurich
Switzerland
hollenstein@ifi.uzh.ch
Abstract
In this paper, we discuss how domain-
specific noun polarity lexicons can be in-
duced. We focus on the generation of
good candidates and compare two ma-
chine learning scenarios in order to estab-
lish an approach that produces high pre-
cision. Candidates are generated on the
basis of polarity preferences of adjectives
derived from a large domain-independent
corpus. The polarity preference of a word,
here an adjective, reflects the distribution
of positive, negative and neutral arguments
the word takes (here: its nominal head).
Given a noun modified by some adjectives,
a vote among the polarity preferences of
these adjectives establishes a good indica-
tor of the polarity of the noun. In our ex-
periments with five domains, we achieved
f-measure of 59% up to 88% on the basis
of two machine learning approaches car-
ried out on top of the preference votes.
1 Introduction
Polarity lexicons are crucial for fine-grained sen-
timent analysis. For instance, in approaches
carrying out sentiment composition (Moilanen
and Pulman, 2007), where phrase-level polar-
ity is composed out of word level polarity (e.g.
disappointed
?
hope
+
? NP
?
). However, often
freely available lexicons are domain-independent,
which is a problem with domain-specific texts,
since lexical gaps reduce composition anchors.
But how many domain-specific words do we have
to expect? Is it a real or rather a marginal problem?
In our experiments, we found that domain-specific
nouns do occur quite often - so they do matter. In
one of our domains, we identified about 1000 neg-
ative nouns, 409 were domain-specific. In that do-
main, the finance sector, more than 13?000 noun
types exist that do not occur at all in the DeWac
corpus - a large Web corpus (in German) with over
90 Million sentences. Thus, most of them must
be regarded as domain-specific. It would be quite
time-consuming to go through all of them in order
to identify and annotate the polar ones. Could we,
rather, predict good candidates? We would need
polarity predictors - words that take other, polar
words e.g. as their heads. If they, moreover, had
a clear-cut preference, i.e. they mostly took one
kind of polar words, say negative, then they were
perfect predictors of the polarity of nouns. We
found that adjectives (e.g. acute) can be used as
such polarity predictors (e.g. acute mostly takes
negative nouns, denoted n
?
, e.g. acute pain)).
Our hypothesis is that the polarity prefer-
ences of adjectives are (more or less) domain-
independent. We can learn the preferences from
domain-independent texts and apply it to domain-
specific texts and get good candidates of domain-
specific polar nouns. Clearly, if the polarity pref-
erences of an adjective are balanced (0.33 for each
polarity), than the predictions could not help at all.
But if one polarity clearly prevails, we might even
get a good performance by just classifying the po-
larity of unknown nouns in a domain according to
the dominant polarity preference of the adjectives
they co-occur with.
In this paper, we show how to generate
such a preference model on the basis of a
large, domain-independent German corpus and
a domain-independent German polarity lexicon.
We use this model to generate candidate nouns
from five domain-specific text collections - rang-
ing from 3?200 up to 37?000 texts per domain.
In order to see how far an automatic induction
of a domain-specific noun lexicon could go, we
also experimented with machine learning scenar-
ios on the output of the baseline system. We ex-
perimented with a distributional feature setting on
the basis of unigrams and used the Maximum En-
18
tropy learner, Megam (Daum?e III, 2004), to learn
a classifier. We also worked with Weka (Frank et
al., 2010) and features derived from the German
polarity lexicon. Both approaches yield significant
gains in terms of precision - so they realize a high-
precision scenario.
2 Inducing the Preference Model
We seek to identify adjectives which impose a
clear-cut polar preference on their head nouns.
The polarity preference of an adjective reflects the
distribution of positive, negative and neutral nouns
the adjective modifies given to some text corpus.
We used the domain-independent DeWac corpus
(Baroni M., 2009) comprising about 90 million
German sentences. We selected those adjectives
that frequently co-occurred with polar nouns from
PoLex, a freely available German polarity lexicon
(Clematide and Klenner, 2010). Since the original
polarity lexicon contained no neutral nouns, we
first identified 2100 neutral nouns and expanded
the lexicon
1
. Altogether 5?500 nouns were avail-
able, 2100 neutral, 2100 negative and 1250 pos-
itive. For each adjective, we counted how often
it took (i.e. modified) positive, negative or neu-
tral nouns in the DeWac corpus and determined
their polarity preferences for each class (positive,
negative and neutral). This way, 28?500 adjec-
tives got a probability distribution, most of them,
however, with a dominating neutral polarity pref-
erence. Two lexicons were derived from it: a pos-
itive and a negative polarity preference lexicon.
An adjective obeys a polar polarity preference if
the sum of its positive and negative polarity pref-
erences is higher than its neutral preference. If the
positive preference is higher than the negative, the
adjective is a positive polarity predictor, otherwise
it is a negative polarity predictor. This procedure
leaves us with 506 adjectives, 401 negative polar-
ity predictors and 105 positive polarity predictors.
Figure 1 shows some examples of negative polar-
ity predictors. It reveals that, for instance, the ad-
jective akut (acute) is mostly coupled with neg-
ative nouns (61.50%). Nouns not in PoLex that
co-occur with an adjective are not considered. We
assume that these unknown nouns of an adjective
follow the same distribution that we are sampling
from the known co-occurring nouns. Note that po-
1
We searched for nouns that frequently co-occurred with
the same adjectives the polar nouns from the polarity lexicon
did and stopped annotating when we reached 2?100 neutral
nouns.
larity predictors not necessarily must have a prior
polarity itself. Actually, only 3 of the 12 adjectives
from Figure 1 do have a prior polarity (indicated as
n
?
). For instance, the adjective pl?otzlich (immedi-
ate) is not polar but has a negative polarity pref-
erence. The polarity preference of a word is not
useful in composition, it just reveals the empirical
(polar) context of the word. If, however, the polar-
ity of the context word is unknown, the preference
might license an informed polarity guess.
adjective English POS NEG #n
?
arg
?
bad/very 02.65 55.14 301
heftig intensive 07.73 48.77 814
v?ollig total 25.79 42.43 787
akut acute 06.27 61.50 478
latent latent 07.96 47.76 402
ziemlich rather 14.16 52.36 233
drohend
?
threatening 35.1 52.54 824
pl?otzlich immediate 17.78 41.82 703
gravierend grave 04.5 48.5 400
chronisch chronic 03.26 72.11 398
schleichend subtle 03.76 52.97 319
hemmunglos
?
unscrupulous 15.49 43.19 213
Figure 1: Negative Polarity Predictors
Here is the formula for the estimation of the
negative polarity preference as given in Figure 1
(n
?
denotes a negative noun from PoLex, a
j
an
adjective modifying an instance of n
?
)
2
:
prefn
?
(a
j
) =
#a
j
n
?
#a
+,?,=
j
Note that we count the number of adj-noun
types (#a
j
n
?
), not tokens. #a
+,?,=
j
is the num-
ber of adj-noun types of the adjective a
j
for all
classes: positive (+), negative(-) and neutral (=).
Figure 2 gives examples of positive polarity pre-
dictors with some of their nouns.
German English POS
ungetr?ubt unclouded joy
unbeirrbar unerring hope
?uberstr?omend overwhelming happiness
bewunderswert mirable competence
falschverstanden falsely-understood tolerance
wiedergewonnen regained freedom
Figure 2: Positive Polarity Predictors
3 Applying the Preference Model
We applied the preference model to texts from five
domains: banks (37?346 texts), transport (3221),
2
This could be interpreted as the conditional probability
of a negative noun given the adjective.
19
insurance (4768), politics (3208) and pharma
(4790). These texts have been manually classified
over the last 15 years by an institute carrying out
media monitoring
3
, not only wrt. their domain,
but also wrt. target-specific polarity (we just use
the domain annotation, currently).
The polarity of a noun is predicted by the vote
of the adjectives it occurred with. The following
formula shows the polarity prediction pol
+,?,=
for
the class negative (pol
?
):
pol
?
(n
i
) = A
i
?
?
a
j
?PM
?
??(a
j
,n
i
)
prefn
?
(a
j
)
A
i
is the number of adjectives that modify the
noun n
i
in the domain-specific texts. PM
?
is the
set of adjectives from the polarity model (PM )
with a negative polarity preference and (a
j
, n
i
) is
true, if the adjective a
j
modifies the noun n
i
ac-
cording to the domain-specific documents.
4 Improving the Predictions
The preference model serves two purposes: it gen-
erates a list of candidates for polar nouns and it
establishes a baseline. We experimented with two
feature settings in order to find out whether we
could improve on these results.
In the first setting, the WK setting, we wanted to
exploit the fact that for some adjectives that mod-
ify a noun, we know their prior polarity (from the
polarity lexicon). These adjectives do not nec-
essarily have a clear positive or negative polarity
preference. If not, then they are not used in the
prediction of the noun polarity.
But could the co-occurrence of a noun with ad-
jectives bearing a prior polarity also be indicative
of the noun polarity? For instance, if a noun is
coupled frequently and exclusively with negative
adjectives. Does this indicate something? Ones
intuition might mislead, but a machine learning
approach could reveal correlations. We used Sim-
ple Logistic Regression (SRL) from Weka and the
following features:
1. the number of positive adjectives with a prior
polarity that modify the noun
2. the number of negative adjectives with a prior
polarity that modify the noun
3
We would like to thank the f?og institute (cf.
www.foeg.uzh.ch/) for these data (mainly newspaper texts in
German).
3. the difference between 1) and 2): absolute
and ratio
4. the ratio of positive and negative adjectives
5. two binary features indicating the majority
class
6. three features for the output of the prefer-
ence model: the positive, negative and neu-
tral scores: pol
?
, pol
+
, pol
=
, respectively.
In the second setting, the MG setting, we trained
Megam, a Maximum Entropy learner, among the
following lines: we took all polar nouns from
PoLex and extracted from the DeWac corpus all
sentences containing these nouns. For each noun,
all (context) words (nouns, adjectives, verbs) co-
occurring with it in these sentences are used as
bag of words training vectors. In other words, we
learned a tri-partite classifier to predict the polarity
class (positive, negative or neutral) given a target
noun and its context, i.e. those nouns co-occurring
with it in a text collection.
5 Experiments
The goal of our experiments were the prediction
of positive and negative domain-specific nouns in
five domains. We used our preference model to
generate candidates. Then we manually annotated
the results in order to obtain a domain-specific
gold standard. We evaluated the output of the
preference model relative to the new gold stan-
dards and we run our experiments with Megam
and Weka?s Simple Logistic Regression (SRL).
Megam and Weka?s SLR were trained on the basis
of the positive, negative and neutral nouns from
PoLex and the DeWac corpus.
Figure 3 shows the results. #PM gives the num-
ber of nouns predicted by the preference model
to be negative (e.g. 220 in the politics domain).
These are the nouns we annotated for polarity
and that formed our gold standard afterwards (e.g.
75.90 out of 110 predicted are true negative nouns
and are kept as the gold standard). Since the gen-
eration of the gold standard is based on the prefer-
ence model?s output, its recall is 1. We cannot fix
the real recall since this would require to manu-
ally classify all nouns occurring in those texts (e.g.
13?000 in the banks domain). However, since we
wanted to compare the machine learning perfor-
mance with the preference model, we had to mea-
20
ID domain texts #PM prec f #WK prec rec f #MG prec rec f
D1 politics 3208 220 75.90 86.29 195 78.97 92.22 83.26 130 81.54 63.48 69.13
D2 transport 3221 141 71.63 83.47 127 73.22 92.07 80.57 64 78.12 49.50 58.54
D3 insurance 4768 255 76.86 86.91 238 78.57 95.40 85.13 155 79.35 62.75 69.09
D4 pharma 4790 257 71.59 83.44 228 76.75 95.11 81.69 137 87.83 65.40 68.35
D5 banks 37346 1013 70.38 88.02 825 77.84 90.07 79.02 437 81.23 49.78 58.32
Figure 3: Prediction of Negative Nouns
sure recall, otherwise we could not determine the
overall performance.
From Figure 3 we can see that the preference
model (PM) performs best in terms of f-measure
(in bold). Of course, recall (i.e. 1, not shown) is
idealized, since we took the output of the prefer-
ence model to generate the gold standard. Note
however that this was our premise, that we needed
an approach that delivers good candidates, other-
wise we were lost given the vast amount of can-
didate nouns (e.g. remember the 13?000 nouns in
the finance sector).
German English
Wertverminderung impairment of assets
Stagflation stagflation
Geldschwemme money glut
?
Uberhitzungssymptom overheating symptom
Hyperinflation hyperinflation
Euroschw?ache weakness of the euro
Werterosion erosion in value
Nachfrage?uberhang surplus in demand
Margendruck pressure on margins
Klumpenrisiko cluster risk
Virus virus
Handekzem hand eczema
Schweinegrippe swine flu
Geb?armutterriss ruptured uterus
Alzheimer Alzheimer
Sehst?orung defective eye sight
Tinnitus tinnitus
Figure 4: Domain-specific Negative Nouns
Figure 4 shows examples of negative nouns
from two domains: banks and pharma. But: are all
found nouns domain-specific negative nouns? In
the bank domain, we have manually annotated for
domain specificity: out of 1013 nouns predicted
to be negative by the model, 409 actually were
domain-specific (40.3 %)
4
. The other nouns could
also be in a domain-independent polarity lexicon.
Now, we turn to the prediction of positive
domain-specific nouns. It is not really surpris-
ing that the preference model is unbalanced - that
there are far more negative than positive polarity
predictors: 401 compared to 105. PoLex, the pool
4
51 of the 131 (38.93%) as positive classified nouns actu-
ally were domain-specific.
of nouns used for learning of the polarity prefer-
ences already is unbalanced (2100 negative com-
pared to 1250 positive nouns). Also, the major-
ity of the texts in our five domains are negative
(all texts are annotated for document-level polar-
ity). It is obvious then that our model is better
in the prediction of negative than positive polarity.
Actually, our base model comprising 105 positive
polarity predictors does not trigger often within
the whole corpus. For instance, only 10 predic-
tions were made in the banks domain, despite the
37?346 texts. Clearly, newspaper texts often are
critical and thus more negative than positive vo-
cabulary is used. This explains the very low recall.
However, what if we relaxed our model? If we,
for example, keep those adjectives in our model
that have a positive polarity preference > 0.35, at
least 35 out of 100 nouns co-occurring with those
adjectives should be positive.
ID #1 prec #2 prec #3 prec #4 prec
D1 18 66.6 25 60.0 25 60.0 8 50
D2 14 85.7 16 75.0 0 0 3 33.3
D3 13 69.2 15 60.0 5 100 1 100
D4 13 84.6 15 80.0 9 55.5 2 100
D5 135 76.2 174 71.2 58 87.9 40 82.5
Figure 5: Prediction of Positive Nouns
We report the results of two runs. The first one,
labelled #1, where adjectives are used to predict a
positive noun polarity if they have a positive po-
larity preference > 0.35 and where the negative
polarity preference is < 0.1. In the second run, la-
belled #2, we only require the positive preference
to be > 0.35. Table 5 shows the results. We also
show the results of Weka (label #3) and Megam
(label #4) for the candidates generated by #2.
Compared to the negative settings, the number
of found positive nouns is rather low. For instance,
in the banks domain, 174 nouns were suggested
compared to 1013 negative ones. However, pre-
cision has not dropped and it is especially higher
than the threshold value of 0.35 seemed to indi-
cate (as discussed previously). Weka (#3) and
Megam (#4) again show better precision, however
21
the number of found nouns is too low (in a setting
that suffers already from low numbers). Figure 6
shows a couple of found positive nouns.
German English
Versammlungsfreiheit freedom of assembly
Ausl?anderintegration integration of foreigners
Einlagesicherung deposit protection
Lohntransparenz wage transparency
Haushaltsdisziplin budgetary discipline
Vertriebsst?arke marketing strength
Anlegervertrauen confidence of investors
Kritikf?ahigkeit ability for criticism
F?uhrungskompetenz leadership competencies
Figure 6: Predicted Positive Nouns
So far, we have discussed a binary approach
where each class (positive, negative) was predicted
and classified independently and where especially
no adjectives with a neutral preference where con-
sidered. What happens if we include these adjec-
tives? The results are given in Figure 7.
domain #neg prec #pos prec
banks 288 80.16 3 66.66
pharma 141 70.92 32 68.75
transport 78 67.94 0 0
politics 115 76.52 0 0
insurance 132 66.66 0 0
Figure 7: Unrestricted Prediction of Noun Polarity
Although precision is good, the results are very
conservative, e.g. in the banks domain, only 288
nouns were found compared to 1013 nouns given
the binary mode. Recall and f-measure are lower
compared to the binary setting. The huge amount
of neutral preference adjectives (about 28?000)
seems to neutralize polar tendencies. But even
then, some predictions survive - so these contexts
seem to be strong.
6 Related Work
The expansion or creation of sentiment lexicons
has been investigated in many variations from dif-
ferent perspectives and for various goals. Liu
and Zhang (2012) subdivide the work in this field
into three groups: manual approaches, dictionary-
based approaches and corpus-based approaches.
While the manual approach is time-consuming, it
is still often used to create core lexicons which are
not domain-specific, e.g. (Taboada et al., 2011).
The dictionary-based approaches which are also
called thesaurus-based approaches (Huang et al.,
2014) try to make use of existing dictionaries or
thesauri like WordNet (e.g. (Esuli and Sebastiani,
2006; Baccianella et al., 2010; Neviarouskaya et
al., 2011)) while the corpus-based approaches rely
on statistical measures based on different con-
cepts, for example, sentiment consistency (Hatzi-
vassiloglou and McKeown, 1997), pointwise mu-
tual information (Turney, 2002), context co-
herency (Kanayama and Nasukawa, 2006), double
propagation (Qiu et al., 2011) or label propagation
(Huang et al., 2014). Our approach is based on the
use of an existing dictionary and of an domain-
independent corpus. But rather than using the cor-
pus to directly detect new entries for the lexicon,
we use it to derive the polarity preference of adjec-
tives which in turn is used to generate candidates
from the domain-specific corpus.
The model most similar to our approach is
(Klenner and Petrakis, 2014), where the contex-
tual and prior polarity of nouns is learned from the
polarity preference of verbs for the verb?s direct
object. However, no attempt is made to induce
domain-specific polarity as we do. We also fo-
cus on the polarity preference of adjectives and we
also try to improve precision by machine learning.
7 Conclusions
We have introduced a plain model for the in-
duction of domain-specific noun lexicons. First,
the polarity preferences of adjectives are learned
from domain-independent text and from a gen-
eral polarity lexicon. A voting approach then pre-
dicts noun polarity from adjective noun pairings
sampled from domain-specific texts. The predic-
tions based only on adjectives acting as positive
or negative polarity predictors perform astonish-
ingly well. Machine Learning can be used to im-
prove precision at the cost of recall. Our approach
thus even might be useful for fully automatic gen-
eration of a high precision, domain-specific prior
noun polarity lexicons.
In future work, we will apply our approach to
other languages than German. We then will also
have to cope with multiword expressions as well,
since compounds not longer - as in German - come
as single words. We also would like to carry out
an extrinsic evaluation in order to see how big the
impact of an induced domain-specific lexicon on
polarity text classification actually is.
22
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proc. of LREC 2010, volume 10, pages 2200?
2204.
Ferraresi A. Zanchetta E. Baroni M., Bernardini S.
2009. The WaCky Wide Web: A collection of very
large linguistically processed Web-crawled corpora.
Language Resources and Evaluation, 43(3):209?
226.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the First Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 7?13.
Hal Daum?e III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper avail-
able at http://pub.hal3.name#daume04cg-bfgs, im-
plementation available at http://hal3.name/megam.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proc. of LREC 2006, volume 6,
pages 417?422.
Eibe Frank, Mark Hall, Geoffrey Holmes, Richard
Kirkby, Bernhard Pfahringer, Ian H. Witten, and Len
Trigg. 2010. Weka-A Machine Learning Work-
bench for Data Mining. In Oded Maimon and Lior
Rokach, editors, Data Mining and Knowledge Dis-
covery Handbook, chapter 66, pages 1269?1277.
Springer US, Boston, MA.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proc. of the ACL 1997, pages 174?181.
Association for Computational Linguistics.
Sheng Huang, Zhendong Niu, and Chongyang Shi.
2014. Automatic construction of domain-specific
sentiment lexicon based on constrained label prop-
agation. Knowledge-Based Systems, 56:191?200.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proc. of EMNLP
2006, pages 355?363. Association for Computa-
tional Linguistics.
Manfred Klenner and Stefanos Petrakis. 2014. Induc-
ing the contextual and prior polarity of nouns from
the induced polarity preference of verbs. Data &
Knowledge Engineering, 90:13?21.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text Data,
pages 415?463. Springer.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proc. of RANLP 2007, pages 378?
382, Borovets, Bulgaria, September 27-29.
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru
Ishizuka. 2011. Sentiful: A lexicon for sentiment
analysis. Affective Computing, IEEE Transactions
on, 2(1):22?36.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9?27.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised clas-
sification of reviews. In Proc. of the ACL 2002,
pages 417?424. Association for Computational Lin-
guistics.
23
