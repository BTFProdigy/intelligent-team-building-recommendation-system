Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 182?189,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Domain Adaptation for Statistical Machine Translation
with Monolingual Resources
Nicola Bertoldi Marcello Federico
FBK-irst - Ricerca Scientifica e Tecnologica
Via Sommarive 18, Povo (TN), Italy
{bertoldi, federico}@fbk.eu
Abstract
Domain adaptation has recently gained
interest in statistical machine translation
to cope with the performance drop ob-
served when testing conditions deviate
from training conditions. The basic idea
is that in-domain training data can be ex-
ploited to adapt all components of an al-
ready developed system. Previous work
showed small performance gains by adapt-
ing from limited in-domain bilingual data.
Here, we aim instead at significant per-
formance gains by exploiting large but
cheap monolingual in-domain data, either
in the source or in the target language.
We propose to synthesize a bilingual cor-
pus by translating the monolingual adap-
tation data into the counterpart language.
Investigations were conducted on a state-
of-the-art phrase-based system trained on
the Spanish?English part of the UN cor-
pus, and adapted on the corresponding
Europarl data. Translation, re-ordering,
and language models were estimated after
translating in-domain texts with the base-
line. By optimizing the interpolation of
these models on a development set the
BLEU score was improved from 22.60%
to 28.10% on a test set.
1 Introduction
A well-known problem of Statistical Machine
Translation (SMT) is that performance quickly de-
grades as soon as testing conditions deviate from
training conditions. The very simple reason is that
the underlying statistical models always tend to
closely approximate the empirical distributions of
the training data, which typically consist of bilin-
gual texts and monolingual target-language texts.
The former provide a means to learn likely trans-
lations pairs, the latter to form correct sentences
with translated words. Besides the general diffi-
culties of language translation, which we do not
consider here, there are two aspects that make
machine learning of this task particularly hard.
First, human language has intrinsically very sparse
statistics at the surface level, hence gaining com-
plete knowledge on translation phrase pairs or tar-
get language n-grams is almost impractical. Sec-
ond, language is highly variable with respect to
several dimensions, style, genre, domain, topics,
etc. Even apparently small differences in domain
might result in significant deviations in the un-
derlying statistical models. While data sparseness
corroborates the need of large language samples in
SMT, linguistic variability would indeed suggest
to consider many alternative data sources as well.
By rephrasing a famous saying we could say that
?no data is better than more and assorted data?.
The availability of language resources for SMT
has dramatically increased over the last decade,
at least for a subset of relevant languages and es-
pecially for what concerns monolingual corpora.
Unfortunately, the increase in quantity has not
gone in parallel with an increase in assortment, es-
pecially for what concerns the most valuable re-
source, that is bilingual corpora. Large parallel
data available to the research community are for
the moment limited to texts produced by interna-
tional organizations (European Parliament, United
Nations, Canadian Hansard), press agencies, and
technical manuals.
The limited availability of parallel data poses
challenging questions regarding the portability of
SMT across different application domains and lan-
guage pairs, and its adaptability with respect to
language variability within the same application
domain.
This work focused on the second issue, namely
the adaptation of a Spanish-to-English phrase-
based SMT system across two apparently close
domains: the United Nation corpus and the Euro-
182
pean Parliament corpus. Cross-domain adaptation
is faced under the assumption that only monolin-
gual texts are available, either in the source lan-
guage or in the target language.
The paper is organized as follows. Section 2
presents previous work on the problem of adap-
tation in SMT; Section 3 introduces the exemplar
task and research questions we addressed; Sec-
tion 4 describes the SMT system and the adapta-
tion techniques that were investigated; Section 5
presents and discusses experimental results; and
Section 6 provides conclusions.
2 Previous Work
Domain adaptation in SMT has been investigated
only recently. In (Eck et al, 2004) adaptation is
limited to the target language model (LM). The
background LM is combined with one estimated
on documents retrieved from the WEB by using
the input sentence as query and applying cross-
language information retrieval techniques. Refine-
ments of this approach are described in (Zhao et
al., 2004).
In (Hildebrand et al, 2005) information retrieval
techniques are applied to retrieve sentence pairs
from the training corpus that are relevant to the test
sentences. Both the language and the translation
models are retrained on the extracted data.
In (Foster and Kuhn, 2007) two basic settings are
investigated: cross-domain adaptation, in which
a small sample of parallel in-domain text is as-
sumed, and dynamic adaptation, in which only
the current input source text is considered. Adap-
tation relies on mixture models estimated on the
training data through some unsupervised cluster-
ing method. Given available adaptation data, mix-
ture weights are re-estimated ad-hoc. A varia-
tion of this approach was also recently proposed
in (Finch and Sumita, 2008). In (Civera and Juan,
2007) mixture models are instead employed to
adapt a word alignment model to in-domain par-
allel data.
In (Koehn and Schroeder, 2007) cross-domain
adaptation techniques were applied on a phrase-
based SMT trained on the Europarl task, in or-
der to translate news commentaries, from French
to English. In particular, a small portion of in-
domain bilingual data was exploited to adapt the
Europarl language model and translation models
by means of linear interpolation techniques. Ueff-
ing et al (2007) proposed several elaborate adap-
tation methods relying on additional bilingual data
synthesized from the development or test set.
Our work is mostly related to (Koehn and
Schroeder, 2007) but explores different assump-
tions about available adaptation data: i.e. only
monolingual in-domain texts are available. The
adaptation of the translation and re-ordering mod-
els is performed by generating synthetic bilingual
data from monolingual texts, similarly to what
proposed in (Schwenk, 2008). Interpolation of
multiple phrase tables is applied in a more prin-
cipled way than in (Koehn and Schroeder, 2007):
all entries are merged into one single table, cor-
responding feature functions are concatenated and
smoothing is applied when observations are miss-
ing. The approach proposed in this paper has
many similarities with the simplest technique in
(Ueffing et al, 2007), but it is applied to a much
larger monolingual corpus.
Finally, with respect to previous work we also
investigate the behavior of the minimum error
training procedure to optimize the combination of
feature functions on a small in-domain bilingual
sample.
3 Task description
This paper addresses the issue of adapting an al-
ready developed phrase-based translation system
in order to work properly on a different domain,
for which almost no parallel data are available but
only monolingual texts.1
The main components of the SMT system are
the translation model, which aims at porting the
content from the source to the target language, and
the language model, which aims at building fluent
sentences in the target language. While the former
is trained with bilingual data, the latter just needs
monolingual target texts. In this work, a lexical-
ized re-ordering model is also exploited to control
re-ordering of target words. This model is also
learnable from parallel data.
Assuming some large monolingual in-domain
texts are available, two basic adaptation ap-
proaches are pursued here: (i) generating syn-
thetic bilingual data with an available SMT sys-
tem and use this data to adapt its translation and
re-ordering models; (ii) using synthetic or pro-
vided target texts to also, or only, adapt its lan-
guage model. The following research questions
1We assume only availability of a development set and an
evaluation set.
183
summarize our basic interest in this work:
? Is automatic generation of bilingual data ef-
fective to tackle the lack of parallel data?
? Is it more effective to use source language
adaptation data or target language adaptation
data?
? Is it convenient to combine models learned
from adaptation data with models learned
from training data?
? How can interpolation of models be effec-
tively learned from small amounts of in-
domain parallel data?
4 System description
The investigation presented in this paper was car-
ried out with the Moses toolkit (Koehn et al,
2007), a state-of-the-art open-source phrase-based
SMT system. We trained Moses in a standard con-
figuration, including a 4-feature translation model,
a 7-feature lexicalized re-ordering model, one LM,
word and phrase penalties.
The translation and the re-ordering model re-
lied on ?grow-diag-final? symmetrized word-to-
word alignments built using GIZA++ (Och and
Ney, 2003) and the training script of Moses. A
5-gram language model was trained on the tar-
get side of the training parallel corpus using the
IRSTLM toolkit (Federico et al, 2008), exploiting
Modified Kneser-Ney smoothing, and quantizing
both probabilities and backoff weights. Decoding
was performed applying cube-pruning with a pop-
limit of 6000 hypotheses.
Log-linear interpolations of feature functions
were estimated with the parallel version of mini-
mum error rate training procedure distributed with
Moses.
4.1 Fast Training from Synthetic Data
The standard procedure of Moses for the estima-
tion of the translation and re-ordering models from
a bilingual corpus consists in three main steps:
1. A word-to-word alignment is generated with
GIZA++.
2. Phrase pairs are extracted from the word-to-
word alignment using the method proposed
by (Och and Ney, 2003); countings and re-
ordering statistics of all pairs are stored. A
word-to-word lexicon is built as well.
3. Frequency-based and lexicon-based direct
and inverted probabilities, and re-ordering
probabilities are computed using statistics
from step 2.
Recently, we enhanced Moses decoder to also
output the word-to-word alignment between the
input sentence and its translation, given that they
have been added to the phrase table at training
time. Notice that the additional information intro-
duces an overhead in disk usage of about 70%, but
practically no overhead at decoding time. How-
ever, when training translation and re-ordering
models from synthetic data generated by the de-
coder, this feature allows to completely skip the
time-expensive step 1.2
We tested the efficiency of this solution for
training a translation model on a synthesized cor-
pus of about 300K Spanish sentences and 8.8M
running words, extracted from the EuroParl cor-
pus. With respect to the standard procedure, the
total training time was reduced by almost 50%,
phrase extraction produced 10% more phrase
pairs, and the final translation system showed a
loss in translation performance (BLEU score) be-
low 1% relative. Given this outcome we decided
to apply the faster procedure in all experiments.
4.2 Model combination
Once monolingual adaptation data is automati-
cally translated, we can use the synthetic parallel
corpus to estimate new language, translation, and
re-ordering models. Such models can either re-
place or be combined with the original models of
the SMT system. There is another simple option
which is to concatenate the synthetic parallel data
with the original training data and re-build the sys-
tem. We did not investigate this approach because
it does not allow to properly balance the contribu-
tion of different data sources, and also showed to
underperform in preliminary work.
Concerning the combination of models, in the
following we explain how Moses was extended
to manage multiple translation models (TMs) and
multiple re-ordering models (RMs).
4.3 Using multiple models in Moses
In Moses, a TM is provided as a phrase table,
which is a set S = {(f? , e?)} of phrase pairs as-
sociated with a given number of features values
2Authors are aware of an enhanced version of GIZA++,
which allows parallel computation, but it was not taken into
account in this work.
184
h(f? , e?;S). In our configuration, 5 features for the
TM (the phrase penalty is included) are taken into
account.
In the first phase of the decoding process, Moses
generates translation options for all possible in-
put phrases f? through a lookup into S; it simply
extracts alternative phrase pairs (f? , e?) for a spe-
cific f? and optionally applies pruning (based on
the feature values and weights) to limit the num-
ber of such pairs. In the second phase of decod-
ing, it creates translation hypotheses of the full
input sentence by combining in all possible ways
(satisfying given re-ordering constraints) the pre-
fetched translation options. In this phase the hy-
potheses are scored, according to all features func-
tions, ranked, and possibly pruned.
When more TMs Sj are available, Moses can
behave in two different ways in pre-fetching the
translation options. It searches a given f? in all sets
and keeps a phrase pair (f? , e?) if it belongs to either
i) their intersection or ii) their union. The former
method corresponds to building one new TM SI ,
whose set is the intersection of all given sets:
SI = {(f? , e?) | ?j (f? , e?) ? Sj}
The set of features of the new TM is the union of
the features of all single TMs. Straightforwardly,
all feature values are well-defined.
The second method corresponds to building one
new TM SU , whose set is the union of all given
sets:
SU = {(f? , e?) | ?j (f? , e?) ? Sj}
Again, the set of features of the new TM is the
union of the features of all single TMs; but for a
phrase pair (f? , e?) belonging to SU \Sj , the feature
values h(f? , e?;Sj) are undefined. In these unde-
fined situations, Moses provides a default value of
0, which is the highest available score, as the fea-
ture values come from probabilistic distributions
and are expressed as logarithms. Henceforth, a
phrase pair belonging to all original sets is penal-
ized with respect to phrase pairs belonging to few
of them only.
To address this drawback, we proposed a
new method3 to compute a more reliable and
smoothed score in the undefined case, based on
the IBM model 1 (Brown et al, 1993). If (f? =
f1, . . . , fl, e? = e1, . . . , el) ? SU \ Sj for any j the
3Authors are not aware of any work addressing this issue.
phrase-based and lexical-based direct features are
defined as follows:
h(f? , e?;Sj) =

(l + 1)m
m?
k=1
l?
h=0
?(ek | fh)
Here, ?(ek | fh) is the probability of ek given fh
provided by the word-to-word lexicon computed
on Sj . The inverted features are defined simi-
larly. The phrase penalty is trivially set to 1. The
same approach has been applied to build the union
of re-ordering models. In this case, however, the
smoothing value is constant and set to 0.001.
As concerns as the use of multiple LMs, Moses
has a very easy policy, consisting of querying each
of them to get the likelihood of a translation hy-
potheses, and uses all these scores as features.
It is worth noting that the exploitation of mul-
tiple models increases the number of features of
the whole system, because each model adds its
set of features. Furthermore, the first approach of
Moses for model combination shrinks the size of
the phrase table, while the second one enlarges it.
5 Evaluation
5.1 Data Description
In this work, the background domain is given by
the Spanish-English portion of the UN parallel
corpus,4 composed by documents coming from
the Office of Conference Services at the UN in
New York spanning the period between 1988 and
1993. The adaptation data come from the Eu-
ropean Parliament corpus (Koehn, 2002) (EP) as
provided for the shared translation task of the
2008 Workshop on Statistical Machine Transla-
tion.5 Development and test sets for this task,
namely dev2006 and test2008, are supplied as
well, and belong to the European Parliament do-
main.
We use the symbol S? (E?) to denote synthetic
Spanish (English) data. Spanish-to-English and
English-to-Spanish systems trained on UN data
were exploited to generate English and Spanish
synthetic portions of the original EP corpus, re-
spectively. In this way, we created two synthetic
versions of the EP corpus, named SE?-EP and S?E-
EP, respectively. All presented translation systems
were optimized on the dev2006 set with respect to
4Distributed by the Linguistic Data Consortium, cata-
logue # LDC94T4A.
5http://www.statmt.org/wmt08
185
the BLEU score (Papineni et al, 2002), and tested
on test2008. (Notice that one reference translation
is available for both sets.) Table 1 reports statistics
of original and synthetic parallel corpora, as well
of the employed development and evaluation data
sets. All the texts were just tokenized and mixed
case was kept. Hence, all systems were developed
to produce case-sensitive translations.
corpus sent Spanish English
word dict word dict
UN 2.5M 50.5M 253K 45.2M 224K
EP 1.3M 36.4M 164K 35.0M 109K
SE?-EP 1.3M 36.4M 164K 35.4M 133K
S?E-EP 1.3M 36.2M 120K 35.0M 109K
dev 2,000 60,438 8,173 58,653 6,548
test 2,000 61,756 8,331 60,058 6,497
Table 1: Statistics of bilingual training corpora,
development and test data (after tokenization).
5.2 Baseline systems
Three Spanish-to-English baseline systems were
trained by exploiting different parallel or mono-
lingual corpora summarized in the first three lines
in Table 2. For each system, the table reports the
perplexity and out-of-vocabulary (OOV) percent-
age of their LM, and its translation performance
achieved on the test set in terms of BLEU score,
NIST score, WER (word error rate) and PER (po-
sition independent error rate).
The distance in style, genre, jargon, etc. be-
tween the UN and the EP corpora is made evident
by the gap in perplexity (Federico and De Mori,
1998) and OOV percentage between their English
LMs: 286 vs 74 and 1.12% vs 0.15%, respectively.
Performance of the system trained on the EP
corpus (third row) can be taken as an upper bound
for any adaptation strategy trying to exploit parts
of the EP corpus, while those of the first line
clearly provide the corresponding lower-bound.
The system in the second row can instead be con-
sider as the lower bound when only monolingual
English adaptation data are assumed.
The synthesis of the SE?-EP corpus was per-
formed with the system trained just on the UN
training data (first row of Table 2), because we had
assumed that the in-domain data were only mono-
lingual Spanish and thus not useful for neither the
TM, RM nor target LM estimation.
Similarly, the system in the last row of Table 2
was developed on the UN corpus to translate the
English part of the EP data to generate the syn-
thetic S?E-EP corpus. Again, any in-domain data
were exploited to train this sytem. Of course, this
system cannot be compared with any other be-
cause of the different translation direction.
In order to compare reported performance with
the state-of-the-art, Table 2 also reports results
of the best system published in the EuroMatrix
project website6 and of the Google online trans-
lation engine.7
5.3 Analysis of the tuning process
It is well-known that tuning the SMT system is
fundamental to achieve good performance. The
standard tuning procedure consists of a minimum
error rate training (mert) (Och and Ney, 2003)
which relies on the availability of a development
data set. On the other hand, the most important
assumption we make is that almost no parallel in-
domain data are available.
conf sent n-best time (min) BLEU (?)
? ? ? ? 22.28
a 2000 1000 2034 23.68 (1.40)
b 2000 200 391 23.67 (1.39)
c 200 1000 866 23.13 (0.85)
d 200 200 551 23.54 (1.26)
Table 3: Global time, not including decoding, of
the tuning process and BLEU score achieved on
the test set by the uniform interpolation weights
(first row), and by the optimal weights with differ-
ent configurations of the tuning parameters.
In a preliminary phase, we investigated different
settings of the tuning process in order to under-
stand how much development data is required to
perform a reliable weight optimization. Our mod-
els were trained on the SE?-EP parallel corpus and
by using uniform interpolation weights the system
achieved a BLEU score of 22.28% on the test set
(see Table 3).
We assumed to dispose of either a regular
in-domain development set of 2,000 sentences
(dev2006), or a small portion of it of just 200 sen-
6http://www.euromatrix.net. Translations of the best sys-
tem were downloaded on November 7th, 2008. Published
results differ because we performed a case-sensitive evalua-
tion.
7Google was queried on November 3rd, 2008.
186
language pair training data PP OOV (%) BLEU NIST WER PER
TM/RM LM
Spanish-English UN UN 286 1.12 22.60 6.51 64.60 48.52
? UN EP 74 0.15 27.83 7.12 60.93 45.19
? EP EP ? ? 32.80 7.84 56.47 41.15
? UN SE?-EP 89 0.21 23.52 6.64 63.86 47.68
? SE?-EP SE?-EP ? ? 23.68 6.65 63.64 47.56
? S?E-EP EP 74 0.15 28.10 7.18 60.86 44.85
? Google na na 28.60 7.55 57.38 57.38
? Euromatrix na na 32.99 7.86 56.36 41.12
English-Spanish UN UN 281 1.39 23.24 6.44 65.81 49.61
Table 2: Description and performance on the test set of compared systems in terms of perplexity, out-of-
vocabulary percentage of their language model, and four translation scores: BLEU, NIST, word-error-
rate, and position-independent error rate. Systems were optimized on the dev2006 development set.
 0
 500
 1000
 1500
 2000
 2500
 5  10  15  20  25  30  35
tim
e (
min
ute
s)
iteration
a) large dev, 1000 bestb) large dev, 200 bestc) small dev, 1000 bestd) small dev, 200 best
 19
 20
 21
 22
 23
 24
 5  10  15  20  25  30  35
BL
EU
 (%
)
iteration
a) large dev, 1000 bestb) large dev, 200 bestc) small dev, 1000 bestd) small dev, 200 best
Figure 1: Incremental time of the tuning process (not including decoding phase) (left) and BLEU score on
the test set using weights produced at each iteration of the tuning process. Four different configurations
of the tuning parameters are considered.
tences. Moreover, we tried to employ either 1,000-
best or 200-best translation candidates during the
mert process.
From a theoretical point of view, computational
effort of the tuning process is proportional to the
square of the number of translation alternatives
generated at each iteration times the number of it-
erations until convergence.
Figure 1 reports incremental tuning time and
translation performance on the test set at each it-
eration. Notice that the four tuning configurations
are ranked in order of complexity. Table 3 sum-
maries the final performance of each tuning pro-
cess, after convergence was reached.
Notice that decoding time is not included in this
plot, as Moses allows to perform this step in par-
allel on a computer cluster. Hence, to our view
the real bottleneck of the tuning process is actu-
ally related to the strictly serial part of the mert
implementation of Moses.
As already observed in previous literature
(Macherey et al, 2008), first iterations of the tun-
ing process produces very bad weights (even close
to 0); this exceptional performance drop is at-
tributed to an over-fitting on the candidate reposi-
tory.
Configurations exploiting the small develop-
ment set (c,d) show a slower and more unstable
convergence; however, their final performance in
Table 3 result only slightly lower than that ob-
tained with the standard dev sets (a, b). Due to the
larger number of iterations they needed, both con-
figurations are indeed more time consuming than
the intermediate configuration (b), which seems
the best one. In conclusion, we found that the size
of the n-best list has essentially no effect on the
quality of the final weights, but it impacts signif-
icantly on the computational time. Moreover, us-
ing the regular development set with few transla-
tion alternatives ends up to be the most efficient
187
configuration in terms of computational effort, ro-
bustness, and performance.
Our analysis suggests that it is important to dis-
pose of a sufficiently large development set al
though reasonably good weights can be obtained
even if such data are very few.
5.4 LM adaptation
A set of experiments was devoted to the adapta-
tion of the LM only. We trained three different
LMs on increasing portions of the EP and we em-
ployed them either alone or in combination with
the background LM trained on the UN corpus.
2 LMs (+UN)1 LM
  20
  22
  24
  26
  30
10050250
B
L
E
U
 
(
%
)
Percentage of monolingual English adaptation data
  28
Figure 2: BLEU scores achieved by systems ex-
ploiting one or two LMs trained on increasing per-
centages of English in-domain data.
Figure 2 reports BLEU score achieved by these
systems. The absolute gain with respect to the
baseline is fairly high, even with the smallest
amount of adaptation data (+4.02). The benefit
of using the background data together with in-
domain data is very small, and rapidly vanishes
as the amount of such data increases.
If English synthetic texts are employed to adapt
the LM component, the increase in performance
is significantly lower but still remarkable (see Ta-
ble 2). By employing all the available data, the
gain in BLEU% score was of 4% relative, that is
from 22.60 to 23.52.
5.5 TM and RM adaptation
Another set of experiments relates to the adapta-
tion of the TM and the RM. In-domain TMs and
RMs were estimated on three different versions of
the full parallel EP corpus, namely EP, SE?-EP, and
S?E-EP. In-domain LMs were trained on the cor-
responding English side. All in-domain models
were either used alone or combined with the base-
line models according to multiple-model paradigm
explained in Section 4.3. Tuning of the interpola-
tion weights was performed on the standard devel-
opment set as usual. Results of these experiments
are reported in Figure 3.
Results suggest that regardless of the used bilin-
gual corpora the in-domain TMs and RMs work
better alone than combined with the original mod-
els. We think that this behavior can be explained
by a limited disciminative power of the result-
ing combined model. The background translation
model could contain phrases which either do or
do not fit the adaptation domain. As the weights
are optimized to balance the contribution of all
phrases, the system is not able to well separate the
positive examples from the negative ones. In ad-
dition to it, system tuning is much more complex
because the number of features increases from 14
to 26.
Finally, TMs and RMs estimated from synthetic
data show to provide smaller, but consistent, con-
tributions than the corresponding LMs. When En-
glish in-domain data is provided, BLEU% score
increases from 22.60 to 28.10; TM and RM con-
tribute by about 5% relative, by covering the gap
from 27.83 to 28.10. When Spanish in-domain
data is provided BLEU% score increases from
22.60 to 23.68; TM and RM contribute by about
15% relative, by covering the gap from 23.52 to
23.68 .
Summarizing, the most important role in the do-
main adaptation is played by the LM; nevertheless
the adaptation of the TM and RM gives a small
further improvement..
2 TMs, RMs, LMs (+UN)1 TM, RM, LM
  20
  22
  24
  26
  28
  32
  34
bilingualEnglishSpanishnothing
B
L
E
U
 
(
%
)
Type of adaptation data
  30
Figure 3: BLEU scores achieved by system ex-
ploiting both TM, RM and LM trained on different
corpora.
6 Conclusion
This paper investigated cross-domain adaptation
of a state-of-the-art SMT system (Moses), by ex-
ploiting large but cheap monolingual data. We
proposed to generate synthetic parallel data by
188
translating monolingual adaptation data with a
background system and to train statistical models
from the synthetic corpus.
We found that the largest gain (25% relative) is
achieved when in-domain data are available for the
target language. A smaller performance improve-
ment is still observed (5% relative) if source adap-
tation data are available. We also observed that the
most important role is played by the LM adapta-
tion, while the adaptation of the TM and RM gives
consistent but small improvement.
We also showed that a very tiny development set
of only 200 parallel sentences is adequate enough
to get comparable performance as a 2000-sentence
set.
Finally, we described how to reduce the time
for training models from a synthetic corpus gen-
erated through Moses by 50% at least, by exploit-
ing word-alignment information provided during
decoding.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?312.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 177?180,
Prague, Czech Republic.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In Pro-
ceedings of the International Conference on Lan-
guage Resources and Evaluation (LREC), pages
327?330, Lisbon, Portugal.
Marcello Federico and Renato De Mori. 1998. Lan-
guage modelling. In Renato DeMori, editor, Spoken
Dialogues with Computers, chapter 7, pages 199?
230. Academy Press, London, UK.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
dling large scale language models. In Proceedings
of Interspeech, pages 1618?1621, Melbourne, Aus-
tralia.
Andrew Finch and Eiichiro Sumita. 2008. Dy-
namic model interpolation for statistical machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 208?215,
Columbus, Ohio.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel, and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In Proceedings of
the 10th Conference of the European Association for
Machine Translation (EAMT), pages 133?142, Bu-
dapest.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished,
http://www.isi.edu/?koehn/europarl/.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
725?734, Honolulu, Hawaii.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association of
Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA.
Holger Schwenk. 2008. Investigations on Large-Scale
Lightly-Supervised Training for Statistical Machine
Translation. In Proc. of the International Workshop
on Spoken Language Translation, pages 182?189,
Hawaii, USA.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Semi-supervised model adaptation
for statistical machine translation. Machine Trans-
lation, 21(2):77?94.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation via structured query models. In Pro-
ceedings of Coling 2004, pages 411?417, Geneva,
Switzerland.
189
A Web-based Demonstrator of a Multi-lingual Phrase-based
Translation System
Roldano Cattoni, Nicola Bertoldi, Mauro Cettolo, Boxing Chen and Marcello Federico
ITC-irst - Centro per la Ricerca Scientifica e Tecnologica
38050 Povo - Trento, Italy
{surname}@itc.it
Abstract
This paper describes a multi-lingual
phrase-based Statistical Machine Transla-
tion system accessible by means of a Web
page. The user can issue translation re-
quests from Arabic, Chinese or Spanish
into English. The same phrase-based sta-
tistical technology is employed to realize
the three supported language-pairs. New
language-pairs can be easily added to the
demonstrator. The Web-based interface al-
lows the use of the translation system to
any computer connected to the Internet.
1 Introduction
At this time, Statistical Machine Translation
(SMT) has empirically proven to be the most
competitive approach in international competi-
tions like the NIST Evaluation Campaigns1 and
the International Workshops on Spoken Language
Translation (IWSLT-20042 and IWSLT-20053).
In this paper we describe our multi-lingual
phrase-based Statistical Machine Translation sys-
tem which can be accessed by means of a Web
page. Section 2 presents the general log-linear
framework to SMT and gives an overview of
our phrase-based SMT system. In section 3
the software architecture of the demo is out-
lined. Section 4 focuses on the currently supported
language-pairs: Arabic-to-English, Chinese-to-
English and Spanish-to-English. In section 5 the
Web-based interface of the demo is described.
1http://www.nist.gov/speech/tests/mt/
2http://www.slt.atr.jp/IWSLT2004/
3http://www.is.cs.cmu.edu/iwslt2005/
2 SMT System Description
2.1 Log-Linear Model
Given a string f in the source language, the goal of
the statistical machine translation is to select the
string e in the target language which maximizes
the posterior distribution Pr(e | f). By introduc-
ing the hidden word alignment variable a, the fol-
lowing approximate optimization criterion can be
applied for that purpose:
e? = arg max
e
Pr(e | f)
= arg max
e
?
a
Pr(e,a | f)
? arg max
e,a
Pr(e,a | f)
Exploiting the maximum entropy (Berger et
al., 1996) framework, the conditional distribu-
tion Pr(e,a | f) can be determined through
suitable real valued functions (called features)
hr(e, f ,a), r = 1 . . . R, and takes the parametric
form:
p?(e,a | f) ? exp{
R
?
r=1
?rhr(e, f ,a)}
The ITC-irst system (Chen et al, 2005) is
based on a log-linear model which extends the
original IBM Model 4 (Brown et al, 1993)
to phrases (Koehn et al, 2003; Federico and
Bertoldi, 2005). In particular, target strings e are
built from sequences of phrases e?1 . . . e?l. For each
target phrase e? the corresponding source phrase
within the source string is identified through three
random quantities: the fertility ?, which estab-
lishes its length; the permutation pii, which sets
its first position; the tablet f? , which tells its word
string. Notice that target phrases might have fer-
tility equal to zero, hence they do not translate any
91
source word. Moreover, uncovered source posi-
tions are associated to a special target word (null)
according to specific fertility and permutation ran-
dom variables.
The resulting log-linear model applies eight fea-
ture functions whose parameters are either esti-
mated from data (e.g. target language models,
phrase-based lexicon models) or empirically fixed
(e.g. permutation models). While feature func-
tions exploit statistics extracted from monolingual
or word-aligned texts from the training data, the
scaling factors ? of the log-linear model are esti-
mated on the development data by applying a min-
imum error training procedure (Och, 2004).
2.2 Decoding Strategy
The translation of an input string is performed by
the SMT system in two steps. In the first pass a
beam search algorithm (decoder) computes a word
graph of translation hypotheses. Hence, either
the best translation hypothesis is directly extracted
from the word graph and output, or an N-best list
of translations is computed (Tran et al, 1996). The
N-best translations are then re-ranked by applying
additional features and the top ranking translation
is finally output.
The decoder exploits dynamic programming,
that is the optimal solution is computed by expand-
ing and recombining previously computed partial
theories. A theory is described by its state which is
the only information needed for its expansion. Ex-
panded theories sharing the same state are recom-
bined, that is only the best scoring one is stored
for further expansions. In order to output a word
graph of translations, backpointers to all expanded
theories are mantained, too.
To cope with the large number of generated the-
ories some approximations are introduced during
the search: less promising theories are pruned off
(beam search) and a new source position is se-
lected by limiting the number of vacant positions
on the left-hand and the distance from the left most
vacant position (re-ordering constraints).
2.3 Phrase extraction and model training
Training of the phrase-based translation model
requires a parallel corpus provided with word-
alignments in both directions, i.e. from source
to target positions, and viceversa. This pre-
processing step can be accomplished by applying
the GIZA++ toolkit (Och and Ney, 2003) that pro-
vides Viterbi alignments based on IBM Model-4.
Starting from the parallel training corpus, pro-
vided with direct and inverted alignments, the so-
called union alignment (Och and Ney, 2003) is
computed.
Phrase-pairs are extracted from each sentence pair
which correspond to sub-intervals of the source
and target positions, J and I , such that the union
alignment links all positions of J into I and all
positions of I into J . In general, phrases are ex-
tracted with maximum length in the source and tar-
get defined by the parameters Jmax and Imax. All
such phrase-pairs are efficiently computed by an
algorithm with complexity O(lImaxJ2max) (Cet-
tolo et al, 2005).
Given all phrase-pairs extracted from the train-
ing corpus, lexicon probabilities and fertility prob-
abilities are estimated.
Target language models (LMs) used by the de-
coder and rescoring modules are, respectively,
estimated from 3-gram and 4-gram statistics
by applying the modified Kneser-Ney smoothing
method (Goodman and Chen, 1998). LMs are es-
timated with an in-house software toolkit which
also provides a compact binary representation of
the LM which is used by the decoder.
3 Demo Architecture
Figure 1 shows the two-layer architecture of the
demo. At the bottom lie the programs that provide
the actual translation services: for each language-
pair a wrapper coordinates the activity of a special-
ized pre-processing tool and a MT decoder. The
translation programs run on a grid-based cluster
of high-end PCs to optimize the processing speed.
All the wrappers communicate with the MT front-
end whose main task is to forward translation re-
quests to the appropriate language-pair wrapper
and to report an error in case of wrong requests
(e.g. unsupported language-pair). It is worth
noticing here that a new language-pair can be eas-
ily added to the system with a minimal interven-
tion on the code of the MT front-end.
At the top of the architecture are the programs
that provide the interface with the user. This layer
is separated from the translation layer (hosted by
internal machines only) by means of a firewall.
The user interface is implemented as a Web page
in which a translation request (a source sentence
and a language-pair) is input by means of an
HTML form. The cgi script invocated by the form
manages the interaction with the MT front-end.
92
Web Page
(form)
script
CGI
lang 1
wrapper
prepro?
cessing
MT
decoder
prepro?
cessing
MT
decoder
wrapper
lang 2
prepro?
cessing
MT
decoder
wrapper
lang N
...
MT
front?end
firewall
external host
internal hosts
fast machines
Figure 1: Architecture of the demo. For each
language-pair a set of programs (in particular the
MT decoder) provides the translation service. The
request issued by the user on the Web page is
sent by the cgi script to the MT front-end. The
translation is then performed on the appropriate
language-pair service and the output sent back to
the Web browser.
When a user issues a translation request after
filling the form fields, the cgi script sends the re-
quest to the MT front-end and waits for its reply.
The input sentence is then forwarded to the wrap-
per of the appropriate language-pair. After a pre-
processing step, the actual translation is performed
by the specific MT decoder. The output in the tar-
get language is then sent back to the user?s Web
browser through the chain in the reverse order.
From a technical point of view, the inter-process
communication is realized by means of standard
TCP-IP sockets. As far as the encoding of texts is
concerned, all the languages are encoded in UTF-
8: this allows to manage the processing phase in
an uniform way and to render graphically different
character sets.
4 The supported language-pairs
Although there is no theoretical limit to the num-
ber of supported language-pairs, the current ver-
sion of the demo provides translations to English
from three source languages: Arabic, Chinese and
Spanish. For demonstration purpose, three differ-
ent application domains are covered too.
Arabic-to-English (Tourism)
The Arabic-to-English system has been trained
with the data provided by the International Work-
shop on Spoken Language Translation 2005 The
context is that of the Basic Traveling Expres-
sion Corpus (BTEC) task (Takezawa et al, 2002).
BTEC is a multilingual speech corpus which con-
tains sentences coming from phrase books for
tourists. Training set includes 20k sentences con-
taining 159K Arabic and 182K English running
words; vocabulary size is 18K for Arabic, 7K for
English.
Chinese-to-English (Newswire)
The Chinese-to-English system has been trained
with the data provided by the NIST MT Evaluation
Campaign 2005 , large-data condition. In this case
parallel data are mainly news-wires provided by
news agencies. Training set includes 71M Chinese
and 77M English running words; vocabulary size
is 157K for Chinese, 214K for English.
Spanish-to-English (European Parliament)
The Spanish-to-English system has been trained
with the data provided by the Evaluation Cam-
paign 2005 of the European integrated project TC-
STAR4. The context is that of the speeches of
the European Parliament Plenary sessions (EPPS)
from April 1996 to October 2004. Training set for
the Final Text Edition transcriptions includes 31M
Spanish and 30M English running words; vocabu-
lary size is 140K for Spanish, 94K for English.
5 The Web-based Interface
Figure 2 shows a snapshot of the Web-based in-
terface of the demo ? the URL has been removed
to make this submission anonymous. In the upper
part of the page the user provides the two informa-
tion required for the translation: the source sen-
tence can be input in a 80x5 textarea html struc-
ture, while the language-pair can be selected by
means of a set a radio-buttons. The user can reset
the input area or send the translation request by
means of standard reset and submit buttons. Some
examples of bilingual sentences are provided in
the lower part of the page.
4http://www.tc-star.org
93
Figure 2: A snapshot of the Web-based interface.
The user provides the sentence to be translated
in the desired language-pair. Some examples of
bilingual sentences are also available to the user.
The output of a translation request is simple: the
requested source sentence, the translation in the
target language and the selected language-pair are
presented to the user. Figure 3 shows an example
of an Arabic sentence translated into English.
We plan to extend the interface with the pos-
sibility for the user to ask additional information
about the translation ? e.g. the number of explored
theories or the score of the first-best translation.
6 Acknowledgements
This work has been funded by the European Union
under the integrated project TC-STAR - Technol-
ogy and Corpora for Speech to Speech Translation
- (IST-2002-FP6-506738, http://www.tc-star.org).
References
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22(1):39?71.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?313.
Figure 3: Example of an Arabic sentence trans-
lated into English.
Mauro Cettolo, Marcello Federico, Nicola Bertoldi,
Roldano Cattoni, and Boxing Chen. 2005. A look
inside the itc-irst smt system. In Proceedings of the
10th Machine Translation Summit, pages 451?457,
Phuket, Thailand, September.
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and
M. Federico. 2005. The ITC-irst SMT System for
IWSLT-2005. In Proceedings of the IWSLT 2005,
Pittsburgh, USA.
M. Federico and N. Bertoldi. 2005. A Word-to-Phrase
Statistical Translation Model. ACM Transactions on
Speech and Language Processing. to appear.
J. Goodman and S. Chen. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University, August.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACl 2003, pages 127?133, Edmonton, Canada.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F.J. Och. 2004. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, Sapporo, Japan.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a Broad-Coverage
Bilingual Corpus for Speech Translation of Travel
Conversations in the Real World. In Proceedings of
3rd LREC, pages 147?152, Las Palmas, Spain.
B. H. Tran, F. Seide, and V. Steinbiss. 1996. A Word
Graph based N-Best Search in Continuous Speech
Recognition. In Proceedings of ICLSP, Philadel-
phia, PA, USA.
94
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
 	

Proceedings of the Workshop on Statistical Machine Translation, pages 94?101,
New York City, June 2006. c?2006 Association for Computational Linguistics
How Many Bits Are Needed To Store Probabilities
for Phrase-Based Translation?
Marcello Federico and Nicola Bertoldi
ITC-irst - Centro per la Ricerca Scientica e Tecnologica
38050 Povo - Trento, Italy
{federico,bertoldi}@itc.it
Abstract
State of the art in statistical machine trans-
lation is currently represented by phrase-
based models, which typically incorpo-
rate a large number of probabilities of
phrase-pairs and word n-grams. In this
work, we investigate data compression
methods for efficiently encoding n-gram
and phrase-pair probabilities, that are usu-
ally encoded in 32-bit floating point num-
bers. We measured the impact of com-
pression on translation quality through a
phrase-based decoder trained on two dis-
tinct tasks: the translation of European
Parliament speeches from Spanish to En-
glish, and the translation of news agencies
from Chinese to English. We show that
with a very simple quantization scheme all
probabilities can be encoded in just 4 bits
with a relative loss in BLEU score on the
two tasks by 1.0% and 1.6%, respectively.
1 Introduction
In several natural language processing tasks, such as
automatic speech recognition and machine transla-
tion, state-of-the-art systems rely on the statistical
approach.
Statistical machine translation (SMT) is based
on parametric models incorporating a large num-
ber of observations and probabilities estimated from
monolingual and parallel texts. The current state of
the art is represented by the so-called phrase-based
translation approach (Och and Ney, 2004; Koehn et
al., 2003). Its core components are a translation
model that contains probabilities of phrase-pairs,
and a language model that incorporates probabilities
of word n-grams.
Due to the intrinsic data-sparseness of language
corpora, the set of observations increases almost lin-
early with the size of the training data. Hence, to
efficiently store observations and probabilities in a
computer memory the following approaches can be
tackled: designing compact data-structures, pruning
rare or unreliable observations, and applying data
compression.
In this paper we only focus on the last approach.
We investigate two different quantization methods
to encode probabilities and analyze their impact on
translation performance. In particular, we address
the following questions:
? How does probability quantization impact on
the components of the translation system,
namely the language model and the translation
model?
? Which is the optimal trade-off between data
compression and translation performance?
? How do quantized models perform under dif-
ferent data-sparseness conditions?
? Is the impact of quantization consistent across
different translation tasks?
Experiments were performed with our phrase-
based SMT system (Federico and Bertoldi, 2005) on
two large-vocabulary tasks: the translation of Euro-
pean Parliament Plenary Sessions from Spanish to
94
English, and the translation of news agencies from
Chinese to English, according to the set up defined
by the 2005 NIST MT Evaluation Workshop.
The paper is organized as follows. Section 2 re-
views previous work addressing efficiency in speech
recognition and information retrieval. Section 3 in-
troduces the two quantization methods considered
in this paper, namely the Lloyd?s algorithm and the
Binning method. Section 4 briefly describes our
phrase-based SMT system. Sections 5 reports and
discusses experimental results addressing the ques-
tions in the introduction. Finally, Section 6 draws
some conclusions.
2 Previous work
Most related work can be found in the area of speech
recognition, where n-gram language models have
been used for a while.
Efforts targeting efficiency have been mainly fo-
cused on pruning techniques (Seymore and Rosen-
feld, 1996; Gao and Zhang, 2002), which permit
to significantly reduce the amount of n-grams to be
stored at a negligible cost in performance. More-
over, very compact data-structures for storing back-
off n-gram models have been recently proposed by
Raj and Whittaker (2003).
Whittaker and Raj (2001) discuss probability en-
coding as a means to reduce memory requirements
of an n-gram language model. Quantization of a
3-gram back-off model was performed by applying
the k-means Lloyd-Max algorithm at each n-gram
level. Experiments were performed on several large-
vocabulary speech recognition tasks by considering
different levels of compression. By encoded proba-
bilities in 4 bits, the increase in word-error-rate was
only around 2% relative with respect to a baseline
using 32-bit floating point probabilities.
Similar work was carried out in the field of in-
formation retrieval, where memory efficiency is in-
stead related to the indexing data structure, which
contains information about frequencies of terms in
all the individual documents. Franz and McCarley
(2002) investigated quantization of term frequencies
by applying a binning method. The impact on re-
trieval performance was analyzed against different
quantization levels. Results showed that 2 bits are
sufficient to encode term frequencies at the cost of a
negligible loss in performance.
In our work, we investigate both data compres-
sion methods, namely the Lloyd?s algorithm and the
binning method, in a SMT framework.
3 Quantization
Quantization provides an effective way of reducing
the number of bits needed to store floating point
variables. The quantization process consists in par-
titioning the real space into a finite set of k quantiza-
tion levels and identifying a center ci for each level,
i = 1, . . . , k. A function q(x) maps any real-valued
point x onto its unique center ci. Cost of quantiza-tion is the approximation error between x and ci.
If k = 2h, h bits are enough to represent a floating
point variable; as a floating point is usually encoded
in 32 bits (4 byte), the compression ratio is equal
to 32/h1 . Hence, the compression ratio also gives
an upper bound for the relative reduction of mem-
ory use, because it assumes an optimal implemen-
tation of data structures without any memory waste.
Notice that memory consumption for storing the k-
entry codebook is negligible (k ? 32 bits).
As we will apply quantization on probabilistic
distribution, we can restrict the range of real val-
ues between 0 and 1. Most quantization algorithms
require a fixed (although huge) amount of points
in order to define the quantization levels and their
centers. Probabilistic models used in SMT satisfy
this requirement because the set of parameters larger
than 0 is always limited.
Quantization algorithms differ in the way parti-
tion of data points is computed and centers are iden-
tified. In this paper we investigate two different
quantization algorithms.
Lloyd?s Algorithm
Quantization of a finite set of real-valued data points
can be seen as a clustering problem. A large fam-
ily of clustering algorithms, called k-means algo-
rithms (Kanungo et al, 2002), look for optimal cen-
ters ci which minimize the mean squared distancefrom each data point to its nearest center. The map
between points and centers is trivially derived.
1In the computation of the compression ratio we take into
account only the memory needed to store the probabilities of the
observations, and not the memory needed to store the observa-
tions themselves which depends on the adopted data structures.
95
As no efficient exact solution to this problem
is known, either polynomial-time approximation or
heuristic algorithms have been proposed to tackle
the problem. In particular, Lloyd?s algorithm starts
from a feasible set of centers and iteratively moves
them until some convergence criterion is satisfied.
Finally, the algorithm finds a local optimal solution.
In this work we applied the version of the algorithm
available in the K-MEANS package2 .
Binning Method
The binning method partitions data points into uni-
formly populated intervals or bins. The center of
each bin corresponds to the mean value of all points
falling into it. If Ni is the number of points of the
i-th bin, and xi the smallest point in the i-th bin, apartition [xi, xi+1] results such that Ni is constantfor each i = 0, . . . , k ? 1, where xk = 1 by default.The following map is thus defined:
q(x) = ci if xi <= x < xi+1.
Our implementation uses the following greedy
strategy: bins are build by uniformly partition all
different points of the data set.
4 Phrase-based Translation System
Given a string f in the source language, our SMT
system (Federico and Bertoldi, 2005; Cettolo et al,
2005), looks for the target string e maximizing the
posterior probability Pr(e,a | f) over all possible
word alignments a. The conditional distribution is
computed with the log-linear model:
p?(e,a | f) ? exp
{ R
?
r=1
?rhr(e, f ,a)
}
,
where hr(e, f ,a), r = 1 . . . R are real valued featurefunctions.
The log-linear model is used to score translation
hypotheses (e,a) built in terms of strings of phrases,
which are simple sequences of words. The transla-
tion process works as follows. At each step, a target
phrase is added to the translation whose correspond-
ing source phrase within f is identified through three
random quantities: the fertility which establishes its
length; the permutation which sets its first position;
2www.cs.umd.edu/?mount/Projects/KMeans.
the tablet which tells its word string. Notice that tar-
get phrases might have fertility equal to zero, hence
they do not translate any source word. Moreover,
untranslated words in f are also modeled through
some random variables.
The choice of permutation and tablets can be
constrained in order to limit the search space un-
til performing a monotone phrase-based translation.
In any case, local word reordering is permitted by
phrases.
The above process is performed by a beam-search
decoder and is modeled with twelve feature func-
tions (Cettolo et al, 2005) which are either esti-
mated from data, e.g. the target n-gram language
models and the phrase-based translation model, or
empirically fixed, e.g. the permutation models.
While feature functions exploit statistics extracted
from monolingual or word-aligned texts from the
training data, the scaling factors ? of the log-linear
model are empirically estimated on development
data.
The two most memory consuming feature func-
tions are the phrase-based Translation Model (TM)
and the n-gram Language Model (LM).
Translation Model
The TM contains phrase-pairs statistics computed
on a parallel corpus provided with word-alignments
in both directions. Phrase-pairs up to length 8 are
extracted and singleton observations are pruned off.
For each extracted phrase-pair (f? , e?), four transla-
tion probabilities are estimated:
? a smoothed frequency of f? given e?
? a smoothed frequency of e? given f?
? an IBM model 1 based probability of e? given f?
? an IBM model 1 based probability of f? given e?
Hence, the number of parameters of the transla-
tion models corresponds to 4 times the number of
extracted phrase-pairs. From the point of view of
quantization, the four types of probabilities are con-
sidered separately and a specific codebook is gener-
ated for each type.
Language Model
The LM is a 4-gram back-off model estimated with
the modied Kneser-Ney smoothing method (Chen
and Goodman, 1998). Singleton pruning is applied
on 3-gram and 4-gram statistics. In terms of num-
96
task parallel resources mono resources LM TM
src trg words 1-gram 2-gram 3-gram 4-gram phrase pairs
NIST 82,168 88,159 463,855 1,408 20,475 29,182 46,326 10,410
EPPS 34,460 32,951 3,2951 110 2,252 2,191 2,677 3,877
EPPS-800 23,611 22,520 22,520 90 1,778 1,586 1,834 2,499
EPPS-400 11,816 11,181 11,181 65 1,143 859 897 1,326
EPPS-200 5,954 5,639 5,639 47 738 464 439 712
EPPS-100 2,994 2,845 2,845 35 469 246 213 387
Table 1: Figures (in thousand) regarding the training data of each translation task.
ber of parameters, each n-gram, with n < 4, has
two probabilities associated with: the probability of
the n-gram itself, and the back-off probability of the
corresponding n + 1-gram extensions. Finally, 4-
grams have only one probability associated with.
For the sake of quantization, two separate code-
books are generated for each of the first three lev-
els, and one codebook is generated for the last level.
Hence, a total of 7 codebooks are generated. In all
discussed quantized LMs, unigram probabilities are
always encoded with 8 bits. The reason is that uni-
gram probabilities have indeed the largest variability
and do not contribute significantly to the total num-
ber of parameters.
5 Experiments
Data and Experimental Framework
We performed experiments on two large vocabulary
translation tasks: the translation of European Parlia-
mentary Plenary Sessions (EPPS) (Vilar et al, 2005)
from Spanish to English, and the translation of doc-
uments from Chinese to English as proposed by the
NIST MT Evaluation Workshops3 .
Translation of EPPS is performed on the so-called
final text editions, which are prepared by the trans-
lation office of the European Parliament. Both the
training and testing data were collected by the TC-
STAR4 project and were made freely available to
participants in the 2006 TC-STAR Evaluation Cam-
paign. In order to perform experiments under differ-
ent data sparseness conditions, four subsamples of
the training data with different sizes were generated,
too.
Training and test data used for the NIST task are
3www.nist.gov/speech/tests/mt/.
4www.tc-star.org
task sentences src words ref words
EPPS 840 22725 23066
NIST 919 25586 29155
Table 2: Statistics of test data for each task.
available through the Linguistic Data Consortium5.
Employed training data meet the requirements set
for the Chinese-English large-data track of the 2005
NIST MT Evaluation Workshop. For testing we
used instead the NIST 2003 test set.
Table 1 reports statistics about the training data of
each task and the models estimated on them. That
is, the number of running words of source and target
languages, the number of n-grams in the language
model and the number phrase-pairs in the transla-
tion model. Table 2 reports instead statistics about
the test sets, namely, the number of source sentences
and running words in the source part and in the gold
reference translations.
Translation performance was measured in terms
of BLEU score, NIST score, word-error rate (WER),
and position independent error rate (PER). Score
computation relied on two and four reference trans-
lations per sentence, respectively, for the EPPS
and NIST tasks. Scores were computed in case-
insensitive modality with punctuation. In general,
none of the above measures is alone sufficiently in-
formative about translation quality, however, in the
community there seems to be a preference toward
reporting results with BLEU. Here, to be on the safe
side and to better support our findings we will report
results with all measures, but will limit discussion
on performance to the BLEU score.
In order to just focus on the effect of quantiza-
5www.ldc.upenn.edu
97
LM-h
32 8 6 5 4 3 2
32 54.78 54.75 54.73 54.65 54.49 54.24 53.82
8 54.78 54.69 54.69 54.79 54.55 54.18 53.65
6 54.57 54.49 54.76 54.57 54.63 54.26 53.60
TM-h 5 54.68 54.68 54.56 54.61 54.60 54.10 53.39
4 54.37 54.36 54.47 54.44 54.23 54.06 53.26
3 54.28 54.03 54.22 53.96 53.75 53.69 53.03
2 53.58 53.51 53.47 53.35 53.39 53.41 52.41
Table 3: BLEU scores in the EPPS task with different quantization levels of the LM and TM.
tion, all reported experiments were performed with
a plain configuration of the ITC-irst SMT system.
That is, we used a single decoding step, no phrase
re-ordering, and task-dependent weights of the log-
linear model.
Henceforth, LMs and TM quantized with h bits
are denoted with LM-h and TM-h, respectively.
Non quantized models are indicated with LM-32
and TM-32.
Impact of Quantization on LM and TM
A first set of experiments was performed on the
EPPS task by applying probability quantization ei-
ther on the LM or on the TMs. Figures 1 and 2
compare the two proposed quantization algorithms
(LLOYD and BINNING) against different levels of
quantization, namely 2, 3, 4, 5, 6, and 8 bits.
The scores achieved by the non quantized models
(LM-32 and TM-32) are reported as reference.
The following considerations can be drawn from
these results. The Binning method works slightly,
but not significantly, better than the Lloyd?s algo-
rithm, especially with the highest compression ra-
tios.
In general, the LM seems less affected by data
compression than the TM. By comparing quantiza-
tion with the binning method against no quantiza-
tion, the BLEU score with LM-4 is only 0.42% rel-
ative worse (54.78 vs 54.55). Degradation of BLEU
score by TM-4 is 0.77% (54.78 vs 54.36). For all the
models, encoding with 8 bits does not affect transla-
tion quality at all.
In following experiments, binning quantization
was applied to both LM and TM. Figure 3 plots
all scores against different levels of quantization.
As references, the curves corresponding to only
LM-h TM-h BLEU NIST WER PER
32 32 28.82 8.769 62.41 42.30
8 8 28.87 8.772 62.39 42.19
4 4 28.36 8.742 62.94 42.45
2 2 25.95 8.491 65.87 44.04
Table 4: Translation scores on the NIST task with
different quantization levels of the LM and TM.
LM quantization (LM-h) and only TM quantization
(TM-h) are shown. Independent levels of quantiza-
tion of the LM and TM were also considered. BLEU
scores related to several combinations are reported
in Table 3.
Results show that the joint impact of LM and TM
quantization is almost additive. Degradation with
4 bits quantization is only about 1% relative (from
54.78 to 54.23). Quantization with 2 bits is sur-
prisingly robust: the BLEU score just decreases by
4.33% relative (from 54.78 to 52.41).
Quantization vs. Data Sparseness
Quantization of LM and TM was evaluated with re-
spect to data-sparseness. Quantized and not quan-
tized models were trained on four subset of the EPPS
corpus with decreasing size. Statistics about these
sub-corpora are reported in Table 1. Quantization
was performed with the binning method using 2,
4, and 8 bit encodings. Results in terms of BLEU
score are plotted in Figure 4. It is evident that the
gap in BLEU score between the quantized and not
quantized models is almost constant under different
training conditions. This result suggests that perfor-
mance of quantized models is not affected by data
sparseness.
98
Consistency Across Different Tasks
A subset of quantization settings tested with the
EPPS tasks was also evaluated on the NIST task.
Results are reported in Table 4.
Quantization with 8 bits does not affect perfor-
mance, and gives even slightly better scores. Also
quantization with 4 bits produces scores very close
to those of non quantized models, with a loss in
BLEU score of only 1.60% relative. However, push-
ing quantization to 2 bits significantly deteriorates
performance, with a drop in BLEU score of 9.96%
relative.
In comparison to the EPPS task, performance
degradation due to quantization seems to be twice as
large. In conclusion, consistent behavior is observed
among different degrees of compression. Absolute
loss in performance, though quite different from the
EPPS task, remains nevertheless very reasonable.
Performance vs. Compression
From the results of single versus combined com-
pression, we can reasonably assume that perfor-
mance degradation due to quantization of LM and
TM probabilities is additive. Hence, as memory sav-
ings on the two models are also independent we can
look at the optimal trade-off between performance
and compression separately. Experiments on the
NIST and EPPS tasks seem to show that encoding
of LM and TM probabilities with 4 bits provides the
best trade-off, that is a compression ratio of 8 with a
relative loss in BLEU score of 1% and 1.6%. It can
be seen that score degradation below 4 bits grows
generally faster than the corresponding memory sav-
ings.
6 Conclusion
In this paper we investigated the application of data
compression methods to the probabilities stored by
a phrase-based translation model. In particular,
probability quantization was applied on the n-gram
language model and on the phrase-pair translation
model. Experimental results confirm previous find-
ings in speech recognition: language model proba-
bilities can be encoded in just 4 bits at the cost of
a very little loss in performance. The same resolu-
tion level seems to be a good compromise even for
the translation model. Remarkably, the impact of
quantization on the language model and translation
model seems to be additive with respect to perfor-
mance. Finally, quantization does not seems to be
affected by data sparseness and behaves similarly on
different translation tasks.
References
M Cettolo, M. Federico, N. Bertoldi, R. Cattoni, and B.Chen. 2005. A Look Inside the ITC-irst SMT Sys-
tem. In Proc. of MT Summit X, pp. 451?457, Pukhet,Thailand.
S. F. Chen and J. Goodman. 1998. An Empirical
Study of Smoothing Techniques for Language Mod-eling. Technical Report TR-10-98, Computer ScienceGroup, Harvard University, Cambridge, MA, USA.
M. Federico and N. Bertoldi. 2005. A Word-to-PhraseStatistical Translation Model. ACM Transaction onSpeech Language Processing, 2(2):1?24.
M. Franz and J. S. McCarley. 2002. How Many Bits areNeeded to Store Term Frequencies. In Proc. of ACMSIGIR, pp. 377?378, Tampere, Finland.
J. Gao and M. Zhang. 2002. Improving Language ModelSize Reduction using Better Pruning Criteria. In Proc.of ACL, pp. 176?182, Philadelphia, PA.
T. Kanungo, D. M. Mount, N. Netanyahu, C. Piatko,
R. Silverman, , and A. Y. Wu. 2002. An EfficientK-Means Clustering Algorithm: Analysis and Imple-mentation. IEEE Transaction on Pattern Analysis andMachine Intelligence, 24(7):881?892.
P. Koehn, F. J. Och, and D. Marcu. 2003. StatisticalPhrase-Based Translation. In Proc. of HLT/NAACL2003, pp. 127?133, Edmonton, Canada.
F. J. Och and H. Ney. 2004. The Alignment Template
Approach to Statistical Machine Translation. Compu-tational Linguistics, 30(4):417?449.
B. Raj and E. W. D. Whittaker. 2003. Lossless Compres-
sion of Language Model Structure and Word Identi-fiers. In Proc. of ICASSP, pp. 388?391, Honk Kong.
K. Seymore and R. Rosenfeld. 1996. Scalable Backoff
Language Models. In Proc. of ICSLP, vol. 1, pp. 232?235, Philadelphia, PA.
D. Vilar, E. Matusov, S. Hasan, R . Zens, , and H. Ney.2005. Statistical Machine Translation of EuropeanParliamentary Speeches. In Proc. of MT Summit X,
pp. 259?266, Pukhet, Thailand.
E. W. D. Whittaker and B. Raj. 2001. Quantization-based Language Model Compression. In Proc. of Eu-rospeech, pp. 33?36, Aalborg, Denmark.
99
 51
 51.5
 52
 52.5
 53
 53.5
 54
 54.5
 55
32865432
B
L
E
U
 
S
C
O
R
E
BITS
BINNING
LLOYD
 10.3
 10.4
 10.5
 10.6
 10.7
 10.8
32865432
N
I
S
T
 
S
C
O
R
E
BITS
BINNING
LLOYD
 34.5
 35
 35.5
 36
 36.5
32865432
W
E
R
BITS
BINNING
LLOYD
 25
 25.5
 26
 26.5
 27
 27.5
 28
32865432
P
E
R
BITS
BINNING
LLOYD
Figure 1: EPPS task: translation scores vs. quantization level of LM. TM is not quantized.
 51
 51.5
 52
 52.5
 53
 53.5
 54
 54.5
 55
32865432
B
L
E
U
 
S
C
O
R
E
BITS
BINNING
LLOYD
 10.3
 10.4
 10.5
 10.6
 10.7
 10.8
32865432
N
I
S
T
 
S
C
O
R
E
BITS
BINNING
LLOYD
 34.5
 35
 35.5
 36
 36.5
32865432
W
E
R
BITS
BINNING
LLOYD
 25
 25.5
 26
 26.5
 27
 27.5
 28
32865432
P
E
R
BITS
BINNING
LLOYD
Figure 2: EPPS task: translation scores vs. quantization level of TM. LM is not quantized.
100
 52
 52.5
 53
 53.5
 54
 54.5
 55
32865432
B
L
E
U
 
S
C
O
R
E
BITS
LM-h+TM-h
LM-h
TM-h
 10.4
 10.45
 10.5
 10.55
 10.6
 10.65
 10.7
 10.75
 10.8
32865432
N
I
S
T
 
S
C
O
R
E
BITS
LM-h+TM-h
LM-h
TM-h
 34.6
 34.8
 35
 35.2
 35.4
 35.6
 35.8
 36
32865432
W
E
R
BITS
LM-h+TM-h
LM-h
TM-h
 25
 25.5
 26
 26.5
 27
32865432
P
E
R
BITS
LM-h+TM-h
LM-h
TM-h
Figure 3: EPPS task: translation scores vs. quantization level of LM and TM. Quantization was performed
with the Binning algorithm.
 44
 46
 48
 50
 52
 54
EPPSEPPS-800EPPS-400EPPS-200EPPS-100
B
L
E
U
 
S
C
O
R
E
LM-32+TM32
LM-8+TM-8
LM-4+TM-4
LM-2+TM-2
 9.6
 9.8
 10
 10.2
 10.4
 10.6
 10.8
EPPSEPPS-800EPPS-400EPPS-200EPPS-100
N
I
S
T
 
S
C
O
R
E
LM-32+TM-32
LM-8+TM-8
LM-4+TM-4
LM-2+TM-2
 34
 35
 36
 37
 38
 39
 40
 41
EPPSEPPS-800EPPS-400EPPS-200EPPS-100
W
E
R
 
S
C
O
R
E
LM-32+TM-32
LM-8+TM-8
LM-4+TM-4
LM-2+TM-2
 25
 26
 27
 28
 29
 30
EPPSEPPS-800EPPS-400EPPS-200EPPS-100
P
E
R
 
S
C
O
R
E
LM-32+TM-32
LM-8+TM-8
LM-4+TM-4
LM-2+TM-2
Figure 4: EPPS task: translation scores vs. amount of training data. Different levels of quantization were
generated with the Binning algorithm.
101
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 129?132, Dublin, Ireland, August 23-29 2014.
THE MATECAT TOOL
M. Federico and N. Bertoldi and M. Cettolo and M. Negri and M. Turchi
Fondazione Bruno Kessler, Trento (Italy)
M. Trombetti and A. Cattelan and A. Farina and
D. Lupinetti and A. Martines and A. Massidda
Translated Srl, Roma (Italy)
H. Schwenk and L. Barrault and F. Blain
Universit?e du Maine, Le Mans (France)
P. Koehn and C. Buck and U. Germann
The University of Edinburgh (United Kingdom)
www.matecat.com
Abstract
We present a new web-based CAT tool providing translators with a professional work environ-
ment, integrating translation memories, terminology bases, concordancers, and machine transla-
tion. The tool is completely developed as open source software and has been already successfully
deployed for business, research and education. The MateCat Tool represents today probably the
best available open source platform for investigating, integrating, and evaluating under realistic
conditions the impact of new machine translation technology on human post-editing.
1 Introduction
The objective of MateCat
1
is to improve the integration of machine translation (MT) and human transla-
tion within the so-called computer aided translation (CAT) framework. CAT tools represent nowadays the
dominant technology in the translation industry. They provide translators with text editors that can man-
age several document formats and suitably arrange their content into text segments ready to be translated.
Most importantly, CAT tools provide access to translation memories (TMs), terminology databases, con-
cordance tools and, more recently, to machine translation (MT) engines. A TM is basically a repository
of translated segments. During translation, the CAT tool queries the TM to search for exact or fuzzy
matches of the current source segment. These matches are proposed to the user as translation sugges-
tions. Once a segment is translated, its source and target texts are added to the TM for future queries. The
integration of suggestions from an MT engine as a complement to TM matches is motivated by recent
studies (Federico et al., 2012; Green et al., 2013; L?aubli et al., 2013), which have shown that post-editing
MT suggestions can substantially improve the productivity of professional translators. MateCat lever-
ages the growing interest and expectations in statistical MT by advancing the state-of-the-art along three
directions:
? Self-tuning MT, i.e. methods to train statistical MT for specific domains or translation projects;
? User adaptive MT, i.e. methods to quickly adapt statistical MT from user corrections and feedback;
? Informative MT, i.e. supply more information to enhance users? productivity and work experience.
Research along these three directions has converged into a new generation CAT software, which is
both an enterprise level translation workbench (currently used by several hundreds of professional trans-
lators) as well as an advanced research platform for integrating new MT functions, running post-editing
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/.
1
MateCat, acronym of Machine Translation Enhanced Computer Assisted Translation, is a 3-year research project (11/2011-
10/2014) funded by the European Commission under FP7 (grant agreement no 287688). The project consortium is led by FBK
(Trento, Italy) and includes the University of Edinburgh (United Kingdom), Universit?e du Maine (Le Mans, France), and
Translated Srl (Rome, Italy).
129
Figure 1: The MateCat Tool editing page.
experiments and measuring user productivity. The MateCat Tool, which is distributed under the LGPL
open source license, combines features of the most advanced systems (either commercial, like the pop-
ular SDL Trados Workbench,
2
or free like OmegaT
3
) with new functionalities. These include: i) an
advanced API for the Moses Toolkit,
4
customizable to languages and domains, ii) ease of use through a
clean and intuitive web interface that enables the collaboration of multiple users on the same project, iii)
concordancers, terminology databases and support for customizable quality estimation components and
iv) advanced logging functionalities.
2 The MateCat Tool in a Nutshell
Overview. The MateCat Tool runs as a web-server accessible through Chrome, Firefox and Safari. The
CAT web-server connects with other services via open APIs: the TM server MyMemory
5
, the commer-
cial Google Translate (GT) MT server, and a list of Moses-based servers specified in a configuration file.
While MyMemory?s and GT?s servers are always running and available, customized Moses servers have
to be first installed and set-up. Communication with the Moses servers extends the GT API in order to
support self-tuning, user-adaptive and informative MT functions. The natively supported document for-
mat of MateCat Tool is XLIFF,
6
although its configuration file makes it possible to specify external file
converters. The tool supports Unicode (UTF-8) encoding, including non latin alphabets and right-to-left
languages, and handles texts embedding mark-up tags.
How it works. The tool is intended both for individual translators or managers of translation projects
involving one or more translators. A translation project starts by uploading one or more documents and
specifying the desired translation direction. Then the user can optionally select a MT engine from an
available list and/or a new or existing private TM in MyMemory, by specifying its private key. Notice
that the public MyMemory TM and the GT MT services are assumed by default. The following step is
the volume analysis of the document, which reports statistics about the words to be actually translated
based on the coverage provided by the TM. At this stage, long documents can be also split into smaller
portions to be for instance assigned to different translators or translated at different times. The following
step starts the actual translation process by opening the editing window. All source segments of the
2
http://www.translationzone.com/
3
http://www.omegat.org/
4
http://www.statmt.org/moses/
5
http://mymemory.translated.net
6
http://docs.oasis-open.org/xliff/v1.2/os/xliff-core.html
130
document and their corresponding target segments are arranged side-by-side on the screen. By selecting
one segment, an editing pane opens (Figure 1) including an editable field that is initialized with the best
available suggestion or with the last post-edit. Translation hints are shown right below together with
their origin (MT or TM). Their ranking is based on the TM match score or the MT confidence score. MT
hints with no confidence score are assigned a default score. Tag consistency is automatically checked
during translation and warnings are possibly shown in the editing window. An interesting feature of the
MateCat Tool is that each translation project is uniquely identified by its URL page which also includes
the currently edited segment. This permits for instance more users to simultaneously access and work on
the same project. Moreover, to support simultaneous team work on the same project, translators can mark
the status (draft, translated, approved, rejected) of each segment with a corresponding color (see Figure
1, right blue bar). The user interface is enriched with search and replace functions, a progress report at
the bottom of the page, and several shortcut commands for the skilled users. Finally, the tool embeds a
concordance tool to search for terms in the TM, and a glossary where each user can upload, query and
update her terminology base. Users with a Google account can access a project management page which
permits then to manage all their projects, including storage, deletion, and access to the editing page.
MT support. The tool supports Moses-based servers able to provide an enhanced CAT-MT commu-
nication. In particular, the GT API is augmented with feedback information provided to the MT engine
every time a segment is post-edited as well as enriched MT output, including confidence scores, word
lattices, etc. The developed MT server supports multi-threading to serve multiple translators, properly
handles text segments including tags, and instantly adapts from the post-edits performed by each user
(Bertoldi et al., 2013).
Edit Log. During post-editing the tool collects timing information for each segment, which is updated
every time the segment is opened and closed. Moreover, for each segment, information is collected about
the generated suggestions and the one that has actually been post-edited. This information is accessible at
any time through a link in the Editing Page, named Editing Log. The Editing Log page (Figure 2) shows
a summary of the overall editing performed so far on the project, such as the average translation speed
and post-editing effort and the percentage of top suggestions coming from MT or the TM. Moreover,
for each segment, sorted from the slowest to the fastest in terms of translation speed, detailed statistics
about the performed edit operations are reported. This information, with even more details, can be also
downloaded as a CSV file to perform a more detailed post-editing analysis. While the information shown
in the Edit Log page is very useful to monitor progress of a translation project in real time, the CSV file
is a fundamental source of information for detailed productivity analyses once the project is ended.
3 Applications.
The MateCat Tool has been exploited by the MateCat project to investigate new MT functions (Bertoldi
et al., 2013; Cettolo et al., 2013; Turchi et al., 2013; Turchi et al., 2014) and to evaluate them in a real
professional setting, in which translators have at disposal all the sources of information they are used
to work with. Moreover, taking advantage of its flexibility and ease of use, the tool has been recently
exploited for data collection and education purposes (a course on CAT technology for students in trans-
lation studies). An initial version of the tool has also been leveraged by the Casmacat project
7
to create
a workbench (Alabau et al., 2013), particularly suitable for investigating advanced interaction modalities
such as interactive MT, eye tracking, and handwritten input. Currently the tool is employed by Trans-
lated for their internal translation projects and is being tested by several international companies, both
language service providers and IT companies. This has made possible to collect continuous feedback
from hundreds of translators, which besides helping us to improve the robustness of the tool is also
influencing the way new MT functions will be integrated to supply the best help to the final user.
7
http://www.casmacat.eu
131
Figure 2: The MateCat Tool edit log page.
References
Vicent Alabau, Ragnar Bonk, Christian Buck, Michael Carl, Francisco Casacuberta, Mercedes Garca-Mart??nez,
Jes?us Gonz?alez, Philipp Koehn, Luis Leiva, Bartolom?e Mesa-Lao, Daniel Oriz, Herv?e Saint-Amand, Germ?an
Sanchis, and Chara Tsiukala. 2013. Advanced computer aided translation with a web-based workbench. In
Proceedings of Workshop on Post-editing Technology and Practice, pages 55?62.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico. 2013. Cache-based Online Adaptation for Machine
Translation Enhanced Computer Assisted Translation. In Proceedings of the MT Summit XIV, pages 35?42,
Nice, France, September.
Mauro Cettolo, Christophe Servan, Nicola Bertoldi, Marcello Federico, Lo??c Barrault, and Holger Schwenk. 2013.
Issues in Incremental Adaptation of Statistical MT from Human Post-edits. In Proceedings of the MT Summit
XIV Workshop on Post-editing Technology and Practice (WPTP-2), pages 111?118, Nice, France, September.
Marcello Federico, Alessandro Cattelan, and Marco Trombetti. 2012. Measuring user productivity in machine
translation enhanced computer assisted translation. In Proceedings of the Tenth Conference of the Association
for Machine Translation in the Americas (AMTA).
Spence Green, Jeffrey Heer, and Christopher D Manning. 2013. The efficacy of human post-editing for language
translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439?
448. ACM.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. Assessing Post-
Editing Efficiency in a Realistic Translation Environment. In Michel Simard Sharon O?Brien and Lucia Specia
(eds.), editors, Proceedings of MT Summit XIVWorkshop on Post-editing Technology and Practice, pages 83?91,
Nice, France.
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity of human judgements
in MT quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
240?251, Sofia, Bulgaria, August. Association for Computational Linguistics.
Marco Turchi, Antonios Anastasopoulos, Jos?e G.C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (ACL ?14). Association for Computational Linguistics.
132
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 412?419,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Machine Translation of Texts with Misspelled Words
Nicola Bertoldi Mauro Cettolo Marcello Federico
FBK - Fondazione Bruno Kessler
via Sommarive 18 - 38123 Povo, Trento, Italy
{bertoldi,cettolo,federico}@fbk.eu
Abstract
This paper investigates the impact of mis-
spelled words in statistical machine transla-
tion and proposes an extension of the transla-
tion engine for handling misspellings. The en-
hanced system decodes a word-based confu-
sion network representing spelling variations
of the input text.
We present extensive experimental results on
two translation tasks of increasing complex-
ity which show how misspellings of different
types do affect performance of a statistical ma-
chine translation decoder and to what extent
our enhanced system is able to recover from
such errors.
1 Introduction
With the widespread adoption of the Internet, of
modern communication, multimedia and mobile de-
vice technologies, the amount of multilingual in-
formation distributed and available to anyone, any-
where, has exploded. So called social media have
rapidly reshaped information exchange among Inter-
net users, providing new means of communication
(blogs, tweets, etc.), collaboration (e.g. wikis), and
sharing of multimedia content, and entertainment.
In particular, social media have today become also
an important market for advertisement as well as a
global forum for consumer opinions (Kushal et al,
2003).
The growing spread of user-generated content is
scaling-up the potential demand for on-line machine
translation (MT) but also setting new challenges to
the field of natural language processing (NLP) in
general. The language written and spoken in the
social media presents an impressive variety of con-
tent and styles (Schler et al, 2006), and writing con-
ventions that rapidly evolve over time. Moreover,
much of the content is expressed in informal style,
that more or less violates the standard grammar, con-
tains many abbreviations and acronyms, and finally
many misspelled words. From the point of view of
MT, language of social media is hence very different
from the one represented in the text corpora nowa-
days available to train statistical MT systems.
Facing all these challenges, we pragmatically
scaled down our ambition and decided to investigate
a basic, somehow preliminary, well defined prob-
lem: the impact of misspelled words in statistical
MT. Unintentional typing errors are indeed remark-
ably frequent in online chats, blogs, wikis, reviews,
and hence constitute a major source of noise (Subra-
maniam et al, 2009).
In this paper we aim at studying performance
degradation of statistical MT under different levels
and kinds of noise, and at analyzing to what extent
statistical MT is able to recover from errors by en-
riching its input with spelling variations.
After a brief overview of NLP literature related
to noisy texts, in Section 3 we consider different
types of misspellings and derive simple but realistic
models that are able to reproduce them. Such mod-
els are then used to generate errors in texts passed
to a phrase-based statistical MT system. Next, in
Section 4 we introduce an extension of a statistical
MT system able to handle misspellings by exploiting
confusion network decoding (Bertoldi et al, 2008).
Experiments are reported in Section 5 that in-
412
vestigate the trade-off between complexity of the
extended MT decoder versus translation accuracy.
Moreover, as the proposed model for handling mis-
spellings embeds specific assumptions on how er-
rors are generated, we also measure the robustness
of the enhanced MT decoder with respect to differ-
ent noise sources. Experiments are reported on two
tasks of different complexity, the translation of Eu-
roparl texts and weather bulletins, involving English
and Italian languages.
2 Previous Work
Most contributions addressing NLP of noisy user-
generated content are from the text mining commu-
nity. A survey about the different types of noise that
might affect text mining is in (Subramaniam et al,
2009), while an analysis of how noise phenomena,
commonly occurring in blogs, affect an opinion min-
ing application is in (Dey and Haque, 2009).
Concerning spelling correction literature, many
works apply the noisy channel model which con-
sists of two components: a source model (prior
of word probabilities) and a channel (error) model,
that accounts for spelling transformations on let-
ter sequences. Several approaches have been
proposed under this framework, that mainly dif-
fer in the employed error model; see for exam-
ple: (Church and Gale, 1991), (Brill and Moore,
2000) and (Toutanova and Moore, 2002).
Comprehensive surveys on methods to model and
recover spelling errors can be found in (Kukich,
1992) and (Pedler, 2007); in particular, the latter
work is specifically centered on methods for cor-
recting so-called real-word errors (cf. Section 3).
The detection of errors and the suggestion of cor-
rections typically rely on the availability of text cor-
pora or human-made lexical resources. Search for
correct alternatives can be based on word similarity
measures, such as the edit distance (Mitton, 1995),
anagram hashing (Reynaert, 2006), and semantic
distance based on WordNet (Hirst and Budanitsky,
2005). More sophisticated approaches have been
proposed by (Fossati and Di Eugenio, 2008), that
mixes surface and Part-Of-Speech Information, and
(Schaback and Li, 2007), which combines similarity
measures at the character, phonetic, word, syntax,
and semantic levels into one global feature-based
framework.
a) *W *w had just come in from Australia [Australia]
b) good service we *staid one week. [Tahiti]
c) The room was *exellent but the hallway was *filty .
[NJ]
d) is a good place to stay, if you are looking for a hotel
*arround LAX airport. [Tahiti]
e) The staff was *freindly ... I was *conerned about
the noise [CT]
Table 1: Examples of misspellings found in on-line re-
views of an hotel close to Los Angeles Int?l Airport. Cor-
responding corrections are: a) We, , b) stayed, c) excel-
lent, filthy, d) around, e) friendly, concerned.
Concerning the literature of statistical MT, inter-
est in noisy data has been so far considering is-
sues different from misspelled words. For instance,
(Davis et al, 1995) and (Vogel, 2003) address train-
ing methods coping with noisy parallel data, in the
sense that translations do not perfectly match. Work
on speech translation (Casacuberta et al, 2008) fo-
cused instead on efficient methods to couple speech
recognition and MT in order to avoid error propaga-
tion. Very recently, (Carrera et al, 2009) conducted
a qualitative study on the impact of noisy social me-
dia content on statistical and rule-based MT. Unfor-
tunately, this work does not report any quantitative
result, it is only based on a small selection of exam-
ples that are manually evaluated, and finally it does
not address the problem of integrating error correc-
tion with MT.
3 Types of Misspellings
In general, a misspelled word is a sequence of let-
ters that corresponds to no correctly spelled word of
the same language (non-word error), or to a correct
spelling of another word (real-word error). In the
examples shown in Table 1, all marked errors are
non-word errors, but the one in sentence b), which
indeed is likely a misspelling of the word stayed.
Causes of a misspelling may be an unintentional
typing error (e.g. *freindly for friendly), or lack of
knowledge about the proper spelling. Typing errors
can originate from six different typing operations
(Kukich, 1992): substitution, insertion, deletion,
transposition, run-on, and split.1 Lack of knowledge
could be the cause of the misspelled *exellent in sen-
tence c).
1 Run-on and split are the special cases of deleting and in-
serting blank spaces, respectively.
413
1. your - you?re
2. then - than
3. its - it?s
4. to - too - two
5. were - where - we?re
6. there - their - they?re
7. a - an - and
8. off - of
9. here - hear
10. lose - loose
Table 2: List of frequent real-word errors found in blogs.
Source: http://www.theprobabilist.com.
An interesting combination of cause and effect is
when lack of linguistic competence results in con-
fusing the spelling of a word with the spelling of
another word that sounds similarly (Hirst and Bu-
danitsky, 2005). This could be likely the case of the
Polynesian tourist that authored sentence b).
A short list of words frequently confused in blogs
is reported in Table 2 while a longer list can be found
in the Wikipedia.2 Real-word errors typically fool
spell checkers because their identification requires
analyzing the context in which they occur.
In this paper, we automatically corrupt clean text
with three types of noise described below. This pro-
cedure permits us to analyze the MT performance
against different sources and levels of noise and to
systematically evaluate our error-recovering strat-
egy.
Non-word Noise We randomly replace words in
the text according to a list of 4,100 frequently non-
word errors provided in the Wikipedia. A qualitative
analysis of these errors reveals that all of them origi-
nate by one or two keyboard typing errors of the kind
described beforehand. Practically, non-word noise is
introduced by defining a desired level of corruption
of the source text.
Real-word Noise Similarly to the previous case,
real-word errors are automatically introduced by
another list of frequently misused words in the
Wikipedia. This list contains about 300 pairs of con-
fusable words to which we also added the 10 fre-
quent real-word errors occurring in blogs reported
in Table 2.
2See Wikipedia?s ?list of frequently misused English
words?.
Random Noise Finally, we may corrupt the origi-
nal text by randomly replacing, inserting, and delet-
ing characters in it up to a desired percentage.
4 Error-recovering Statistical MT
An enhancement of a statistical MT system is pro-
posed with the goal of improving robustness to mis-
spellings in the input. Rrror recovery is realized
by performing a sequence of actions before the ac-
tual translation, which create reliable spelling alter-
natives of the input and store them into a compact
word-based Confusion Network (CN).
Starting from the possibly noisy input text,
spelling variations are generated by assuming that
each character is a potential typing error, indepen-
dent from other characters.
The variants are represented as a character-based
CN that models possible substitutions, insertion,
deletions of each character, with an empirically de-
termined weight assigned to each alternative. The
network is then searched by a non-monotonic search
process that scores possible character sequences
through a character n-gram language model, and
outputs a set of multiple spelling variants that is fi-
nally converted into a word-based CN. The result-
ing word-based network is finally passed to the MT
engine. In the following, more details are provided
on the augmented MT system with the help of Fig-
ure 1, which shows how the system acts on the cor-
rupted example ?all off ame?, supposed to be ?hall
of fame?.
Step 1 The input text (a) is split into a sequence
of characters (b) including punctuation marks and
blank spaces ( ), which are here considered as stan-
dard characters. Moreover, single characters inter-
leaved with the conventional empty character .
Step 2 A CN (c) is built by adding all alternative
characters of the keyboard to each input character,
including the space character and the empty char-
acter. When the string character is , the only ad-
mitted alternative is . Possible alternative spellings
of the original string correspond to paths in the CN.
Notice that each CN column beginning with a stan-
dard character permits to manage insertion, substi-
tution and split errors, while each column beginning
with the empty character permits to handle deletion
and run-on errors.
414
...
d
e
?
a
c
b
j
?
e
g
...
d
c
?
b
...
a
e
y
m
?
...
rb
c
d
?
e
a
...
...
s
z
?
w
a
f
...
...
b
?
a
e
?
_
e
d
a
...
c
?
b
f
d
...
c
?
b
?
d
c
b
...
a
e
?
...
r
c
f
d
c
a
...
?
e
b
dk
p
i
?
o
...
e
c
d
...
a
b
?
_
?
a
c
...
b
?
e
d
k
p
l
...
?
o
?
...
b
e
d
a
c
?
o
p
...
l
k
...
i
h ...
z
...
s
a
w
?
d
e
b
?
c
a
...
a
g
c
?
b
........
ah emaf
_
fo
_
ll
la em
_
fo
_
l
a lh emaf
_
ol
uh em
_
ffo
_
ll
ela maf
_
fo
_
l
ema_ffo_lla
arca della gloria
...
hull
...
...
?
hallo
ofhall me
fameoffall
(a)
(b) ?
e
?
m?
a
?
_
?f?
f
?
o
?
_
?
l
?
la
??
(c)
(d)
(e)
(f)
p(w|a) ? 0.91
1
2
3
5
4
Figure 1: The whole process to translate the mistaken
input ?all off ame [hall of fame]? into ?arca della gloria?.
A probability distribution of confusable
keystrokes is generated based on the distance
between the keys on a standard QWERTY key-
board. This distribution is intended to model how a
spelling error is actually produced. Hence, character
alternatives in the CN are associated to a probability
given by:
p (x|y) ? ?
1
k ? d(x, y) + 1
(1)
where d(x, y) is the physical distance between the
key of x and the key of y on the keyboard layout;
for example, the character a has a distance of 3 from
the character c on the considered keyboard layout.
The free parameter k tunes the discriminative power
of the model between correct and wrong typing. In
this paper, k was empirically set to 0.1. The  and
characters are assigned a default distance of 9 and
999 from any other character, respectively.
For the sake of clarity, the probability p(w|a) of
just one entry is reported in Figure 1.
Step 3 The generation of spelling variations (d) is
operated by means of the same decoder employed
for translation (see below), but in a much simplified
configuration which does not exploit any translation
model. It is designed to search the input character-
based CN for the n-best character sequences which
better ?correct? the mistaken input. In Figure 1 the
best sequence is marked by bold boxes (c), and the
empty character  is removed for the sake of clarity
(d). This process relies only on the character-based
6-gram language model trained on monolingual data
in the source language. It is worth noticing that the
generated spelling alternatives may in principle still
contain non-words, just because they are selected by
a character-based language model, which does not
explicitly embed the notion of word.
Transposition errors are modeled both (i) indi-
rectly through consecutive substitutions with appro-
priate characters and (ii) directly by permitting some
re-orderings of adjacent characters. Moreover, pre-
liminary experiments revealed that the explicit han-
dling of deletion and run-on errors by interleaving
input characters with the empty character  (Step 1)
is crucial to achieve good performance. Although
the size of the character-based CN doubles, its de-
coding time increases only by a small factor.
Step 4 The n-best character sequences (d) are
transformed into a word-based CN (e) (Mangu et
al., 2000). First, each character-based sequence is
transformed into a unifilar word-based lattice, whose
edges correspond to words and timestamps to the
character positions. Then, the unifilar lattices are put
in parallel to create one lattice with all spelling vari-
ations of the input text (a). Finally, a word-based CN
is generated by means of the lattice-tool available in
the SRILM Toolkit (Stolcke, 2002).
Step 5 Translation of the CN (e) is performed
with the Moses decoder (Koehn et al, 2007), that
has been successfully applied mainly to text trans-
lation, but also to process multiple input hypothe-
ses (Bertoldi et al, 2008), representing, for exam-
ple, speech transcriptions, word segmentations, texts
with possible punctuation marks, etc. In general,
415
set #sent. English Italian
#wrd dict. #wrd dict.
EP train 1.2M 36M 106K 35M 146K
test 2K 60K 6.5K 60K 8.3K
WF train 42K 996K 2641 994K 2843
test 328 8789 606 8704 697
Table 3: Statistics of train/test data of the Europarl (EP)
and the Weather Forecast (WF) tasks.
Moses looks for the best translation exploring the
search space defined by a set of feature functions
(models), which are log-linearly interpolated with
weights estimated during a tuning stage.
The rationale of storing the spelling alternatives
into a word-based CN instead of n-best list is two-
fold: (i) the CN contains a significantly larger num-
ber of variations, and (ii) the translation system is
much more efficient to translate CNs instead of n-
best lists.
5 Experiments
Extensive experiments have been conducted on the
Europarl shared task, from English to Italian, as
specified by the Workshop on Statistical Machine
Translation of the ACL 2008.3 Additional experi-
ments were conducted on a smaller task, namely the
translation of weather forecast bulletins between the
same language pair. Statistics on texts employed in
experiments are reported in Table 3.
For both tasks, we created evaluation data by ar-
tificially corrupting input text with the noise sources
described in Section 3. The module for generating
spelling variations (Step 3) was trained on additional
4M and 16M running words in English and Italian,
respectively.
We empirically investigated the following issues:
(a) performance of the standard MT engine versus
nature and level of the input noise; (b) performance
of the error-recovering MT engine versus number of
provided spelling variations; (c) portability of the
approach to another task and translation direction;
(d) computational requirements of the approach.
5.1 Impact of Noise
The first set of experiments involved the translation
of corrupted versions of the Europarl test set. Fig-
3http://www.statmt.org/wmt08/
 10
 15
 20
 25
20105210.50
 10
 15
 20
 25
B
L
E
U
Noise Level (%)
baselinerandom, no-recoverynon-word, no-recoveryreal-word, no-recovery
Figure 2: Translation performance as function of the
noise level (in log-scale) for different types of noise.
ure 2 plots three curves of BLEU(%) scores, corre-
sponding to different noise sources and noise ratios,
given in terms of percentage of word error rate. It
also shows the BLEU score on the original clean
text. Notice that this baseline performance (25.16)
represents the state-of-the-art4 for this task.
The major outcome of these experiments is that
the different types of errors seem to affect MT per-
formance in a very similar manner. Quantitatively,
performance degradation begins even for low noise
levels ? about 0.5 absolute BLEU loss at 1% of
noise level ? and reaches 50% when text corruption
reaches the level of 30%. The similar impact of non-
word and random errors is somehow expected. The
plain reason is that both types of errors very likely5
generate Out-Of-Vocabulary (OOV) words.
We find instead less predictable that the impact of
real-word errors is indistinguishable from that of the
other two noise sources. Notice also that most of the
real-word errors produce indeed words known to the
MT system. Hence, the question regards the behav-
ior of the MT system when the sentence includes on
OOV word or an out-of-context known word. Em-
pirically it seems that in both cases the decoder pro-
duces translations with the same amount of errors.
In some sense, the good news is that real-word er-
rors do not induce more translation errors than OOV
words do.
4http://matrix.statmt.org/matrix
5Modulo noise in the parallel data and the chance that a ran-
dom error generates a true word.
416
 15
 20
 25
5020105210.50
 15
 20
 25
B
L
E
U
Noise Level (%)
baselineno-recoverysinglemultiple, 200  20
 25
105210.50
 20
 25
B
L
E
U
Noise Level (%)
baselineno-recoverysinglemultiple, 200
Figure 3: Performance of error-recovering method with random (left) and real-word (right) noise.
5.2 Impact of Multiple Corrections
Experiments presented here address evaluation of
our enhanced MT system. In addition to nature and
level of noise, translation performance is also an-
alyzed with respect to the number (1 and 200) of
spelling alternatives generated at Step 3. Figure 3
plots BLEU scores for random (left plot) and real-
word (right plot) noises. For comparison purposes,
the curves with no error recovery are also shown.
Results with non-word noise are not provided since
they are pretty similar to those with random noise.
It is worth noticing that real-word errors are re-
covered in a different way than random errors; in
fact, for the latter a single spelling alternative seems
sufficient to guarantee a substantial error recovery,
whereas for real-word errors this is not the case.
Concerning the use of spelling variations, it is
worth remarking that our system is able to fully re-
cover from both random and non-word errors up to
noise levels of 10%, which remains high even for
noise levels up to 20%, where the BLEU degrada-
tion is limited to around 5% relative.
Real-word errors are optimally recovered in the
case of multiple spelling variations until they do not
exceed 2% of the words in the input text; after that,
the decrement of the MT quality becomes signif-
icant but still limited to about 5% BLEU relative
for a noise level of 10%. So the question arises
about what could be a realistic real-word noise level.
Clearly this question is not easy to address. How-
ever, to get a rough idea we can look at the exam-
ples reported in Table 1. These five sentences were
extracted from a text of about 100 words (of which
Table 1 only shows the sentences containing errors)
that contain in total 8 errors: 7 of which are non-
words and 1 is a real-word. Although from these
figures reliable statistics cannot be estimated, a rea-
sonable assumption could be that a global noise level
of 10%6 might contain a 1/10 ratio for real-word vs.
non-word errors. Thus, looking at the real-word er-
ror curve of Figure 3, the inability to recover errors
for noise levels greater than 2-5% should actually be
acceptable given this empirical observation.
Another relevant remark from Figure 3 is that
for low noise levels (less than 1%) the use of the
error-recovering module is counterproductive, since
it introduces more errors than those actually affect-
ing the original input text, causing a slight degra-
dation of the translation performance. If the com-
putational cost to generate variants, which will be
analyzed in the next paragraph, is also taken into ac-
count, it results evident the importance of design-
ing a good strategy for enabling or disabling on de-
mand the error-recovering stage. A starting point for
defining an effective activation strategy is the esti-
mation of the noise rate. For doing this, non-words
can be counted by exploiting proper dictionaries or
spell checkers; concerning real-word noise, its rate
can be inferred either from the non-word rate, or by
means of the perplexity, which is expected to be-
come higher as the real-word error rate increases
(Subramaniam et al, 2009). Once the noise level
of the input text is known, the decision of activat-
ing the correction module can be easily taken on a
6By the way, at this noise rate, an error-recovering strategy
would be highly recommended.
417
 0 10 20
 30 40 50
 60
501010.10  0 10
 20 30 40
 50 60
B
L
E
U
Noise Level (%)
English-Italian
baselineno-recoverymultiple, 200
 0 10 20
 30 40 50
501010.10  0 10
 20 30 40
 50
Noise Level (%)
Italian-English
baselineno-recoverymultiple, 200
Figure 4: Effects of random noise and noise correction
on translation performance for the WF task.
threshold basis. Alternatively, the proper working
point, in terms of precision and recall, of the correc-
tion model could be dynamically chosen as a func-
tion of the actual noise level.
5.3 Computational Costs
Although our investigation does not address explic-
itly computational aspects of translating noisy in-
put, nevertheless some general considerations can be
drawn.
The effectiveness of our recovering approach re-
lies on the compact representation of many spelling
alternatives in a word-based CN. The CN decod-
ing has been shown to be efficient, just minimally
larger than the single string decoding (Bertoldi et
al., 2008). On the contrary, in the current enhanced
MT setting, the sequence of Steps 1 to 4 for build-
ing the CN from the noisy input text is quite costly.
Rather than to an intrinsic complexity, this is due to
our choice of creating a rich character-based CN in
Step 3 for the sake of flexibility and to a naive im-
plementation of Step 4.
5.4 Portability
So far we have analyzed in detail our approach
on the medium-large sized Europarl task, for the
English-to-Italian translation direction. For assess-
ing portability, we also considered a simpler task
?the translation of weather forecast bulletins? where
the translation quality is definitely higher, for the
same language pair but in both translation directions.
The choice of the weather forecast task is not by
chance. In fact, as the automatically translated bul-
letins are published on the Web, a very high transla-
tion quality is required, and then the presence of any
typing error in the original text could be a concern.
(By the way, for this task the presence of real-word
errors is very marginal.)
Figure 4 plots curves of MT performance under
random noise conditions against multiple spelling
variations, for two translation directions. It can
be noticed that the error-recovering system behaves
qualitatively as for the Europarl task but even better
from a quantitative viewpoint. Again, the recovering
model introduces spurious errors which affect trans-
lation quality for low levels of noisy input, but in
this case the break-even point is less than 0.1% noise
level. On the other side, errors corrupting the input
text are fully recovered up to 30-40% of noise lev-
els, for which the BLEU score would be more than
halved for non-corrected texts.
6 Future Work
There are a number of important issues that this
work has still left open. First of all, we focused
on a specific way of generating spelling varia-
tions, based on single characters, but other possible
choices should be investigated and compared to our
approach, like the use of n-grams of words.
An important open question regards efficiency of
the proposed recovering strategy, since the problem
has been only sketched in Section 5.3. It is our in-
tention to analyze the intrinsic complexity of our
model, possibly discover its bottlenecks and imple-
ment a more efficient solution.
Another topic, mentioned in Section 5.2, is the ac-
tivation strategy of the misspelling recovery. Some
further investigation is required on how its working
point can be effectively selected; in fact, since the
enhanced system necessarily introduces spurious er-
rors, it would be desirable to increase its precision
for low-corrupted input texts.
7 Conclusions
This paper addressed the issue of automatically
translating written texts that are corrupted by mis-
spelling errors. An enhancement of a state-of-the-art
statistical MT system is proposed which efficiently
performs the translation of multiple spelling variants
of noisy input. These alternatives are generated by a
character-based error recovery system under the as-
sumption that misspellings are due to typing errors.
The enhanced MT system has been tested on texts
corrupted with increasing noise levels of three dif-
ferent sources: random, non-word, and real-word er-
rors.
418
Analysis of experimental results has led us to
draw the following conclusions:
? The impact of misspelling errors on MT perfor-
mance depends on the noise rate, but not on the
noise source.
? The capability of the enhanced MT system to
recover from errors differs according to the
noise source: real-word noise is significantly
harder to remove than random and non-word
noise, which behave substantially the same.
? The exploitation of several spelling alternatives
permits to almost fully recover from errors if
the noise rate does not exceed 10% for non-
word noise and 2% for real-word noise, which
are likely above the corruption level observed
in many social media.
? Finally, performance slightly decreases when
input text is correct or just mistaken at a negli-
gible level, because the error recovery module
rewards recall rather than precision and hence
tends to overgenerate correction alternatives,
even if not needed.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720), which is funded by the EC un-
der the 7th Framework Programme for Research and
Technological Development.
References
N. Bertoldi, et al 2008. Efficient speech translation
through confusion network decoding. IEEE Trans-
actions on Audio, Speech, and Language Processing,
16(8):1696?1705.
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. In Pro-
ceedings of ACL. Hong Kong.
J. Carrera, et al 2009. Machine trans-
lation for cross-language social media.
http://www.promt.com/company/technology/pdf/mach
ine translation for cross language social media.pdf.
F. Casacuberta, et al 2008. Recent efforts in spoken lan-
guage processing. IEEE Signal Processing Magazine,
25(3):80?88.
K. W. Church and W. A. Gale. 1991. Probability scor-
ing for spelling correction. Statistics and Computing,
1(2):93?103.
M. W. Davis, et al 1995. Text alignment in the real
world: Improving alignments of noisy translations us-
ing common lexical features, string matching strate-
gies and n-gram comparisons. In Proceedings of
EACL, Dublin, Ireland.
L. Dey and S. M. Haque. 2009. Studying the effects of
noisy text on text mining applications. In Proceedings
of AND, pages 107?114, Barcelona, Spain.
D. Fossati and B. Di Eugenio. 2008. I saw tree trees in
the park: How to correct real-word spelling mistakes.
In Proceedings of LREC, Marrakech, Morocco.
G. Hirst and A. Budanitsky. 2005. Correcting real-word
spelling errors by restoring lexical cohesion. Natural
Language Engineering, 11(01):87?111.
P. Koehn, et al 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL
- Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
K. Kukich. 1992. Spelling correction for the telecom-
munications network for the deaf. Communications of
the ACM, 35(5):80?90.
D. Kushal, et al 2003. Mining the peanut gallery:
opinion extraction and semantic classification of prod-
uct reviews. In Proceedings of the WWW conference,
pages 519?528, Budapest, Hungary.
L. Mangu, et al 2000. Finding consensus in speech
recognition: Word error minimization and other appli-
cations of confusion networks. Computer, Speech and
Language, 14(4):373?400.
R. Mitton. 1995. English Spelling and the Computer
(Studies in Language and Linguistics). Addison Wes-
ley Publishing Company.
J. Pedler. 2007. Computer correction of real-word
spelling errors in dyslexic text. Ph.D. thesis, Univer-
sity of London.
M. Reynaert. 2006. Corpus-induced corpus cleanup. In
Proceedings of LREC, Genoa, Italy.
J. Schaback and F. Li. 2007. Multi-level feature extrac-
tion for spelling correction. In IJCAI - Workshop on
Analytics for Noisy Unstructured Text Data, pages 79?
86, Hyderabad, India.
J. Schler, et al 2006. Effects of age and gender on blog-
ging. In Proceedings of AAAI-CAAW, Palo Alto, CA.
A. Stolcke. 2002. Srilm - an extensible language model-
ing toolkit. In Proceedings of ICSLP, Denver, CO.
L. V. Subramaniam, et al 2009. A survey of types of text
noise and techniques to handle noisy text. In Proceed-
ings of AND, pages 115?122, Barcelona, Spain.
K. Toutanova and R. C. Moore. 2002. Pronunciation
modeling for improved spelling correction. In Pro-
ceedings of ACL, pages 144?151, Philadelphia, PA
S. Vogel. 2003. Using noisy biligual data for statisti-
cal machine translation. In Proceedings of EACL, Bu-
dapest, Hungary.
419
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 433?441,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Evaluating the Learning Curve of Domain Adaptive
Statistical Machine Translation Systems
Nicola Bertoldi Mauro Cettolo Marcello Federico
Fondazione Bruno Kessler
via Sommarive 18
38123 Trento, Italy
<surename>@fbk.eu
Christian Buck
University of Edinburgh
10 Crichton Street
EH8 9AB Edinburgh, UK
christian.buck@ed.ac.uk
Abstract
The new frontier of computer assisted transla-
tion technology is the effective integration of
statistical MT within the translation workflow.
In this respect, the SMT ability of incremen-
tally learning from the translations produced
by users plays a central role. A still open
problem is the evaluation of SMT systems that
evolve over time. In this paper, we propose
a new metric for assessing the quality of an
adaptive MT component that is derived from
the theory of learning curves: the percentage
slope.
1 Introduction
Translation memories and computer assisted trans-
lation (CAT) tools are currently the dominant tech-
nologies in the translation and localization market,
but recent achievements in statistical MT have raised
new expectations in the translation industry. So far,
statistical MT has focused on providing ready-to-use
translations, rather than outputs that minimize the
effort of a human translator. The MateCAT project1
aims at pushing what can be considered the new
frontier of CAT technology: how to effectively inte-
grate statistical MT within the translation workflow.
One pursued research direction is developing do-
main adaptive SMT models, i.e. models that dynam-
ically adapt to the translations that are continuously
added to the translation memory by the user dur-
ing her/his work. The ideal goal is to progressively
reduce the mismatch between training and testing
1http://www.matecat.com/
data, in such a way that the adapted SMT engine will
be able to provide the user with useful suggestions
? i.e. perfect or worth being post-edited ? when the
translation memory fails to retrieve perfect or almost
perfect matches. Among the well known machine
learning paradigms that fit with this scenario are on-
line learning and incremental learning, which basi-
cally differ in the amount of data that is employed
to dynamically adapt the system: a single piece of
data in the first case and a batch of data in the lat-
ter. Notice that in both cases one assumes that do-
main adaptation is performed efficiently, i.e. by only
processing the newly received data. Moreover, al-
though the quantity of acquired in-domain data is
generally limited, their high quality and relevance to
the translation task justify their exploitation by all
means possible.
Domain adaptive SMT embeds two challenges:
(1) the design of effective adaptation algorithms, and
(2) the evaluation of MT systems evolving over time.
Since the ultimate goal of our efforts is to increase
the productivity of human translators, the most ac-
curate assessment methodology would be of course
to run a field test. This way, we could compare pro-
ductivity of human translators receiving suggestions
from an MT engine featuring dynamic domain adap-
tation against the productivity of human translators
working with a static MT engine. As this evaluation
is infeasible during daily MT development, we can
resort to the several automatic MT metrics, which
however, as we will see later, are unsuitable to track
the dynamic behaviors we are interested to inves-
tigate. Metrics for measuring performance in the
case of interactive MT, see for example (Khadivi,
433
2008), like Key-Stroke Ratio (KSR), Mouse-Action
Ratio (MAR), Key-Stroke and Mouse-Action Ratio
(KSMR) are known to correlate well with the pro-
ductivity of human translators, but their computation
requires the actual use of an interactive MT system,
i.e. a field test.
In the SMART project,2 the evaluation of adap-
tive interactive MT is explored (Cesa-Bianchi et al,
2008). While no specific metric is proposed, the
analysis is based on a plot of cumulative differences
of BLEU scores between a baseline and an adaptive
system. These differences are computed sentence by
sentence and present an interesting view of the dy-
namic change of the MT system. We are going to
further elaborate on this idea.
Other metrics like Character Error Rate (CER)
and Translation Edit Rate (TER) would accurately
predict the translators? productivity if references
were generated by using the CAT system; on the
contrary, references are usually, as in this paper, gen-
erated from scratch based only on the source text
and can thus be quite far from CAT-based transla-
tions, both lexically and syntactically. The Human-
targeted variant of TER, HTER (Snover et al, 2006),
needs human intervention and is therefore unfit to
meet our requirements.
The main goal of this paper is to design an objec-
tive automatic evaluation methodology for an MT
system adapting over time. We propose to use the
percentage slope from the theory on learning curves
to measure the learning ability of adaptive MT sys-
tems.
To assess the proposed metric, we have imple-
mented a simple but effective adaptation strategy
suitable for an MT system integrated in a CAT tool.
We show that the percentage slope is able to expose
different dynamic behaviors, such as learning, no
learning, and forgetting.
2 Dynamic Adaptation Framework
In the MateCAT project scenario, the MT system,
which is embedded in the CAT tool to increase the
translators? productivity, adapts over time by ex-
ploiting translations generated by the user. The
adapted system is then used to provide the user
with translation suggestions for the next sentences.
2http://www.smart-project.eu
We refer to this process as dynamic (or incremen-
tal) adaptation to emphasize that adaptation hap-
pens continuously based on a stream of data.
2.1 Abstract View of the Adaptation Process
From an abstract point of view, the framework of in-
cremental adaptation can be summarized as follows:
i) before the process starts, an initial system is
built on available data including a parallel cor-
pus;
ii) a stream of parallel data becomes available that
is split into blocks of (not necessarily) similar
size;
iii) the first/next block is considered, but only the
source is available yet;
iv) the latest instance of the adapting system trans-
lates the source text of the current block;
v) the target part of the current block becomes
available for use;3
vi) the system is adapted using the current parallel
block and possibly all the previous ones;
vii) the loop continues from step iii) until all blocks
are processed.
In each adaptation step, all of the data available
so far can be used, but no look ahead is possible.
Note that, in principle, each block is translated with
a different instance of the adapting system; hence,
the same text occurring in two different blocks can
be translated differently.
2.2 Evaluation Goals and Requirements
Although dynamic adaptation is closely related to
static domain adaptation (Foster and Kuhn, 2007),
in this scenario we are not interested in the quality
of the final model. In fact, this model is only avail-
able once the stream is depleted and therefore is not
used anymore.
What we are interested in, and what we want to
compare among different approaches, is the systems
evolvement over time.
Consider a translator who uses such an incremen-
tally adapting system and performs post-editing on
its suggested translations. The highest productivity
3In the CAT framework, the target part of a block is the
translation post-edited by the user.
434
gain is achieved when the adaptation is quick and
persistent.
Even though in this paper we are concerned with
an automatic metric, it is important to keep the use
case of CAT in mind, in particular the presence of
a human translator. The TransType2 project4 has
found that repeated correction of the same error is
strongly disliked by editors (Macklovitch, 2006) and
may lead to rejection of the entire system. Similarly,
segments that were translated correctly by previous,
less adapted systems, should not be negatively af-
fected by updates. We will refer to these particular
aspects of adaptation as backward reliability.
Automatic measures, which are aimed at static
MT modules, can not take the evolution of the sys-
tem into account and are therefore unable to pinpoint
such problems. Thus, they are not suitable for the
dynamic adaptation scenario.
A new evaluation methodology should satisfy the
following requirements:
? ability to compare different strategies
? show behavior over time and reward early im-
provements and consistent adaptation
? expose possible overfitting, i.e. check whether
generalization is lost due to overly aggressive
adaptation
? strong correlation to human productivity
? estimate benefit over a static baseline model
without adaptation
? check backward reliability.
2.3 Evaluation Protocol
The performance of adaptive systems as sketched
in Section 2.1 is evaluated on different parts of the
stream as opposed to the global evaluation used for
static systems. We distinguish between two proto-
cols which differ in their use of historic data.
For block-wise evaluation only the translations of
the most recent block are evaluated with respect to
the correct translations once these become available.
Any static automatic MT score, e.g. TER (Snover
et al, 2006), BLEU (Papineni et al, 2001), can be
used, provided that it is reliable on a block of usually
relatively small size.
In contrast, in incremental evaluation the scores
are computed on all blocks available so far. The
4http://tt2.atosorigin.es
translations of previous blocks are kept fixed, i.e.
blocks are not translated again once a newly adapted
system becomes available as this new system has al-
ready seen this data.
Both the block-wise and incremental protocols
yield a sequence of scores that reflects the adaptation
behavior over time. The former is useful to expose
potential weaknesses as discussed above: we expect
to see improvement at first and after a while, when
enough adaptation data is available, a level curve. If
this is not the case, this indicates a problem:
i) should the scores deteriorate over time we
might be facing overfitting, possibly due to un-
expected heterogeneity in our corpus;
ii) if the scores continue to improve, then the adap-
tation method is not aggressive enough and the
system underfits.
The incremental evaluation on the other hand allows
for easy comparison of different adaptation strate-
gies. While the performance on the most recent
block becomes less important over time, the perfor-
mance on all the blocks processed so far nicely re-
flects the utility of the system in the application set-
ting.
The metric we are going to propose in the next
section processes such sequences of partial scores.
It accumulates the trend into a single number and
offers an interpretation that relates adaptive behavior
to productivity gains.
3 The Percentage Slope
Learning curves (see (Stump P.E., 2002) for a de-
tailed introduction) are mathematical models used
to estimate the efficiency gain when an activity is
repeated. The learning effect was noted in indus-
trial environment: the underlying notion is that when
people repeat an activity, there tends to be a gain in
efficiency. That is exactly the expected behavior of
our dynamically adapting MT system: it should im-
prove its performance on texts including terms and
expressions whose proper translation has been pre-
viously provided. Thus we decided to exploit ele-
ments from learning theory to measure the evolution
of translation capability.
Several learning curve models have been pro-
posed, but only two are in widespread use, the unit
435
(U) model due to Crawford and the cumulative av-
erage (CA) model due to Wright. Both models are
based on a common mathematical form:
y = axb (1)
where:
a represents the theoretical labor hours required
to build the first unit produced (a positive num-
ber)
b represents the rate of learning (negative value,
except for ?forgetting?)
x represents the number of an item in the produc-
tion sequence (unit #1,#2,#3, . . .)
The models differ in the interpretation of y:
U: y is the labor hours required to build unit #x
CA: y is the average labor hours per unit required
to build the first x units
Since b is a mathematically appropriate but
counter-intuitive number for describing the slope,
the percentage slope S is typically used:
S = 10b log10(2)+2 (2)
S provides the rate of learning on a scale of 0 to 100,
as a percentage. A 100% slope represents no learn-
ing at all, zero percentage reflects a theoretically in-
finite rate of learning. In practice, human operations
hardly ever achieve a rate of learning faster than 70%
as measured on this scale.
The correspondence between our block-wise eval-
uation (Section 2.3) with the U model, and the incre-
mental evaluation with the CA model is straightfor-
ward. In the first case, y is the number of errors
done in the translation of the block #x; in the sec-
ond case, y is the average number of errors (that is
the TER score or the 100-BLEU score) made on the
first x blocks.
From a practical point of view, the sequence of
scores can be provided while the adapting system is
being used; the learning curve which best matches
the sequence is then found5 and eventually the per-
centage slope S is computed.
5Notice that the best fitting learning curve can be estimated
in the log scale with a simple linear regression analysis.
set #sent. #src words #tgt words
train 1.2M 18.9M 19.4M
test 3.4k 57.0k 61.4k
Table 1: Overall statistics on parallel data of the IT
domain used for training and testing the SMT system.
Counts of (English) source words and (Italian) target
words refer to tokenized texts.
4 Experiments
In order to test-drive the evaluation metric intro-
duced in Section 3, several SMT systems showing
effective, weak, poor or absent adaptation capabil-
ity have been developed. Moreover, a preliminary
investigation on backward reliability has been car-
ried out. The next paragraphs detail and discuss the
experiments performed.
4.1 Data
The task considered in this work involves the trans-
lation from English into Italian of documents in the
Information Technology (IT) domain.
The training set consists of a large Translation
Memory in the IT domain and several OPUS6 sub-
corpora, namely KDE4, KDEdoc and PHP. The test
set includes the human generated translation of 6
documents, disjoint from the training set. Although
in the same domain, the test set is quite different
from the training data as shown by comparing val-
ues of perplexity (650 vs. 40) and OOV rate (2.4%
vs. 0.4%) computed on the source side.7 Further-
more, the 6 documents significantly differ among
each other: perplexity and OOV rate range from 465
to 880 and from 0.8 to 3.3, respectively. Table 1 col-
lects overall statistics on training and test sets.
To simulate the stream of fresh data, the IT test
set has been split into blocks of about a thousand8
words each. Before splitting, sentences have been
scrambled, with the rationale of generating a large
number of homogeneous blocks, simulating a test
set consisting of a single document.
6http://opus.lingfil.uu.se
7Figures for the training data were measured through a
cross-validation technique.
8Different sizes have been also considered (three and five
thousands) to test different adaptation rates, but results were
qualitatively similar to those on shorter blocks and then are not
reported.
436
4.2 Baseline System
The SMT baseline system is built upon the open-
source MT toolkit Moses9 (Koehn et al, 2007).
The translation and the lexicalized reordering mod-
els are estimated on parallel training data with the
default setting; a 5-gram LM smoothed through the
improved Kneser-Ney technique (Chen and Good-
man, 1999) is estimated on monolingual texts via
the IRSTLM toolkit (Federico et al, 2008). Here-
inafter, these models are referred to as background
(BG) models. The log-linear interpolation weights
are optimized by means of the standard MERT pro-
cedure provided within the Moses toolkit.
4.3 Adaptive System
The adapting SMT system is built on Moses as well.
Besides the BG models of the baseline system, trans-
lation, reordering and language models estimated on
the stream of fresh data are employed as additional
features. Hereinafter, these models are referred to
as foreground (FG) models. Unless differently spec-
ified, the FG models employed to translate a given
block are trained on all preceding blocks. Note that
the first instance of the adapting system (i.e. that
translating the first block) is exactly the baseline sys-
tem, because no adaptation data is available to train
FG models yet. FG translation and reordering mod-
els are trained in the same way as the BG models.
Due to the limited amount of adaptation data, the FG
LM is a 3-gram LM smoothed through the more ro-
bust Witten-Bell technique (Witten and Bell, 1991).
The interpolation weights are inherited from a
companion system trained and tuned on a different
domain ? official documents of the European Union
organization ? and are kept fixed.
4.4 Experiments on Adaptive SMT
First of all, the baseline and adapting systems were
run on the scrambled test set and compared at both
block-wise and incremental mode (see Section 2.3).
Figure 1 plots block-wise TER and BLEU scores
of the baseline and adapting systems as functions of
the amount (number of words) of adaptation data.
On one hand, it can be guessed that the adapting
system performs gradually better and better than the
baseline; on the other hand, it is evident that such
9http://www.statmt.org/moses
plots are not the most effective way to show the evo-
lution of the adapting system. In fact, the transla-
tion difficulty of contiguous blocks can differ a lot.
Hence, scores computed on them are not comparable
and the corresponding curves are jagged.
The block-wise differences of TER and BLEU
scores between the adapting and the baseline sys-
tems are plotted in Figure 2: the plots are now
cleaner and more readable and vaguely suggest a
positive trend, but still remain too jagged and do not
provide any information about the absolute perfor-
mance of the systems.
Figure 3 plots the incremental TER and BLEU
scores of the baseline and adapting systems as func-
tions of the amount of adaptation data. First of all,
it is worth noting that the right-most values are the
scores computed on the whole test set. In standard
evaluation, those would be the only scores provided
to show how the adapting system outperforms the
baseline system; in particular, the relative improve-
ment is larger for TER (9.3%) than for BLEU (3.9%)
supposedly because tuning was performed to opti-
mize BLEU score which thus is harder to improve.
However, the overall scores obscure the way they
are reached, that is the evolution over time of the
systems, which is especially important for adaptive
systems.
Secondly, the incremental evaluation yields much
smoother plots clearly showing that after initial fluc-
tuations: (i) performance of the baseline stabilizes
around an average which does not change over time;
(ii) scores of the adapting system tend to get increas-
ingly better as more adaptation data is available for
updating FG models.
The evaluation metric we are proposing, the per-
centage slope introduced in Section 3, is indeed able
to spot such kind of paradigmatic behaviors as we
will see in the next section. But before going on
with the assessment of the metric, some further com-
ments on Figure 3:
? in early stages, the adaptation is not effective,
likely because of the scarcity of data. This
raises two issues: design of more effective
adaptation strategies and, in the CAT frame-
work, identifying the appropriate time to re-
place the baseline with the adapting system;
? the adaptive system outperforms the baseline in
437
 40
 45
 50
 55
 60
 65
 0  10000  20000  30000  40000  50000  60000
T
E
R
 
(
%
)
# Number of Words
adabsln
 16
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 0  10000  20000  30000  40000  50000  60000
B
L
E
U
 
(
%
)
# Number of Words
adabsln
Figure 1: Block-wise TER (on the left) and BLEU (right) scores of the baseline and the dynamically adapting systems.
-12
-10
-8
-6
-4
-2
 0
 2
 4
 0  10000  20000  30000  40000  50000  60000
T
E
R
 
(
%
)
# Number of Words
ada -- bsln
-8
-6
-4
-2
 0
 2
 4
 6
 8
 0  10000  20000  30000  40000  50000  60000
B
L
E
U
 
(
%
)
# Number of Words
ada -- bsln
Figure 2: Block-wise TER (left) and BLEU (right) differences between the baseline and the dynamically adapting
systems.
terms of TER very soon, while the overtaking
with regard to BLEU is observed much later.
This is because the baseline SMT system was
tuned with respect to the BLEU score on in-
domain data, differently to the adapting system.
Both these issues are out of the scope of this paper
and will be subject of future investigations.
4.5 Assessment of the Percentage Slope
To assess its effectiveness, the percentage slope has
been computed on errors committed by the baseline
system, the adapting system and an adapting system
featuring only FG models (that is without BG mod-
els). The FG-only system was used to translate each
block either fairly and unfairly: the former mode fits
the adaptation process sketched in Section 2.1; in the
latter mode, the FG model is adapted on the block
before its translation starts.
Figure 4 shows the TER and BLEU scores of such
systems in the incremental evaluation. The four dif-
ferent behaviors are expected to correspond to dif-
ferent percentage slopes. In fact, the S values col-
lected in Table 2 confirm the expectations:
? the baseline, completely unable to learn, has in
fact an S of 100%
? the adapting system, that learns through a dy-
namic adaptation of FG models and generalizes
thanks to BG models, has an S of 96-98%
? the FG-only adapting system tested in unfair
mode worsens its performance as the models
become larger, i.e. less focused on the block to
be translated: this is evidenced by an S greater
than 100%
438
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 0  10000  20000  30000  40000  50000  60000
T
E
R
 
(
%
)
# Number of Words
adabsln
 21
 21.5
 22
 22.5
 23
 23.5
 24
 24.5
 25
 25.5
 0  10000  20000  30000  40000  50000  60000
B
L
E
U
 
(
%
)
# Number of Words
adabsln
Figure 3: Incremental TER (left) and BLEU (right) scores of the baseline and the dynamically adapting systems.
model
system
baseline adapting
FG-only adapting
fair unfair
U 100.4 96.9 96.2 107.2
CA 100.3 97.7 96.5 107.4
Table 2: S values of 4 SMT systems (see text) for
the block-wise TER evaluation, corresponding to the U
model, and the incremental evaluation, corresponding to
the CA model.
? the FG-only adapting system tested in fair
mode increases its performance as the models
become larger, i.e. more general, as evidenced
by an S similar to that of our original adapting
system (96%).
Therefore, we can state that S exposes common
behaviors of evolving SMT systems; however, stan-
dard metrics like TER and BLEU are still in charge
of providing absolute performance measures.
In order to give a hint for properly interpret-
ing the values reported, we summarize the discus-
sion in (Stump P.E., 2002) about ?typical learning
slopes?. Operations that are fully automated tend
to have slopes of 100%, 70% if entirely manual, an
intermediate value if mixed. In real industrial envi-
ronments, the average slope depends on the type of
manufacturing activity: for example, in aircraft in-
dustry it is about 85%, it ranges in 90-95% in elec-
tronics and in machining. Hence, a 96-98% slope
as we measured in our experiments must be con-
sidered a significant learning ability of a fully au-
tomated system.
4.6 Experiments on Backward Reliability
A proper assessment of the backward reliability of
an evolving system as defined in Section 2.2 would
require the identification of patterns translated dif-
ferently by the system during its life. We will inves-
tigate this issue in the future. For the moment, we
try to attack the problem from a global point of view:
we simply check that the adaptive system does ?re-
member? its previous translation capabilities ?on av-
erage?, while it learns to better translate novel texts.
To this end, a cross-validation policy was fol-
lowed: the first two thirds of each test set document
are used for dynamically training the FG models,
while the remaining portions are used as held-out
test sets.
Figure 5 reports the TER and BLEU scores on
the 6 test sets of three systems: the baseline sys-
tem (bsln), the adapting system (ada) fed by in-
crementally merging the available reduced adapta-
tion sets, and the system adapted on all adaptation
data sets (final).
The final system achieves performance close
to ada system on each held-out set; this reveals that
our adaptation process is effective both in learning
and in remembering.
We think that the monitoring of the backward re-
liability of adapting systems is a good practice. A
cross validation scheme like ours allows not only to
reveal the backward reliability as shown before, but
also to discover the forgetting trend of, for example,
an MT system featuring an overly aggressive learn-
439
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 0  10000  20000  30000  40000  50000  60000
T
E
R
 
(
%
)
# Number of Words
bslnadaFGonly fair adaFGonly unfair ada
 10
 20
 30
 40
 50
 60
 70
 0  10000  20000  30000  40000  50000  60000
B
L
E
U
 
(
%
)
# Number of Words
bslnadaFGonly fair adaFGonly unfair ada
Figure 4: Incremental TER (left) and BLEU (right) of 4 systems showing different learning slopes.
ing method. On the other hand, it only provides cues
about the average behavior and it is not as quickly
informative as a single score could be. Hence, the
design of a proper metric for measuring the back-
ward reliability of MT systems is a challenging task
that should be faced by the research community.
5 Summary and Future Work
The evaluation of a dynamically adapting system is
an open issue. Metrics used in interactive MT such
as HTER or field tests, are infeasible in the daily de-
velopment as they involve human translators/judges.
On the other hand, standard MT evaluation met-
rics either do not expose changes over time (BLEU,
TER) or cannot be applied (CER).
The main contribution of this paper is to propose
the use of the percentage slope for the evaluation of
adapting MT systems, a metric borrowed from the
theory on learning curves. For assessing its effec-
tiveness, we have developed a simple but effective
adapting SMT system suitable to work in the context
of a CAT tool supported by MT. We have compared
several ways to plot the change in error rate over
time for different systems and identified the most
suitable for computing the percentage slope. Finally,
we have shown that the percentage slope well ex-
poses the paradigmatic behaviors of evolving SMT
systems.
The MateCAT project has scheduled field tests
for the near future which will allow for inclusion
of human productivity in the assessment of the per-
centage slope. Moreover, efforts will be devoted to
the design of adaptation techniques which are more
sophisticated than the simple approach used in this
work.
We have also identified the issue of backward re-
liability of an adapting system, that is the ability to
learn without forgetting the past, and the importance
of monitoring it. A best practice based on a cross
validation scheme has been proposed. Future inves-
tigations will concern finding an effective metric to
measure backward reliability.
Acknowledgments
This work was supported by the MateCAT project,
which is funded by the EC under the 7th Framework
Programme.
References
N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008.
Online learning algorithms for computer-assisted
translation. Deliverable 4.2, SMART project (FP6).
http://www.smart-project.eu/files/D4
2.pdf.
S. F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Com-
puter Speech and Language, 4(13):359?393.
M. Federico, N. Bertoldi, and M. Cettolo. 2008.
IRSTLM: an Open Source Toolkit for Handling Large
Scale Language Models. In Proc. of Interspeech, pp.
1618?1621, Melbourne, Australia.
G. Foster and R. Kuhn. 2007. Mixture-Model Adapta-
tion for SMT. In Proc. of WMT, pp. 128?135, Prague,
Czech Republic.
S. Khadivi. 2008. Statistical Computer-Assisted Trans-
lation. Ph.D. thesis, RWTH Aachen University,
440
 45
 50
 55
 60
 65
 70
 75
 1  2  3  4  5  6
T
E
R
 
(
%
)
Document
bslnadafinal
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 1  2  3  4  5  6
B
L
E
U
 
(
%
)
Document
bslnadafinal
Figure 5: TER (left) and BLEU (right) scores of the baseline system, the evolving system and the final adapted system
on the document-specific held-out test sets.
Aachen, Germany. Advisors: Hermann Ney and En-
rique Vidal.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc. of
ACL: Demo and Poster Sessions, pp. 177?180, Prague,
Czech Republic.
E. Macklovitch. 2006. Transtype2: The last word. In
Proc. of LREC 2006, Genoa, Italy.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Research Report RC22176, IBM Research
Division, Thomas J. Watson Research Center.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.
Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. of AMTA, Boston,
US-MA.
E. Stump P.E. 2002. All about learning curves. In Proc.
of SCEA. http://www.galorath.com/im
ages/uploads/LearningCurves1.pdf.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Trans. Inform.
Theory, IT-37(4):1085?1094.
441
Workshop on Humans and Computer-assisted Translation, pages 84?92,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Online Word Alignment for Online Adaptive Machine Translation
M. Amin Farajian
FBK-irst,
University of Trento
Trento, Italy
farajian@fbk.eu
Nicola Bertoldi
FBK-irst
Trento, Italy
bertoldi@fbk.eu
Marcello Federico
FBK-irst
Trento, Italy
federico@fbk.eu
Abstract
A hot task in the Computer Assisted
Translation scenario is the integration of
Machine Translation (MT) systems that
adapt sentence after sentence to the post-
edits made by the translators. A main
role in the MT online adaptation process is
played by the information extracted from
source and post-edited sentences, which
in turn depends on the quality of the
word alignment between them. In fact,
this step is particularly crucial when the
user corrects the MT output with words
for which the system has no prior infor-
mation. In this paper, we first discuss
the application of popular state-of-the-art
word aligners to this scenario and reveal
their poor performance in aligning un-
known words. Then, we propose a fast
procedure to refine their outputs and to
get more reliable and accurate alignments
for unknown words. We evaluate our
enhanced word-aligner on three language
pairs, namely English-Italian, English-
French, and English-Spanish, showing a
consistent improvement in aligning un-
known words up to 10% absolute F-
measure.
1 Introduction
In the adaptive MT the goal is to let the MT system
take as soon and as much as possible advantage of
user feedback, in order to learn from corrections
and to hence avoid repeating the same mistakes in
future sentences.
A typical application scenario is the usage by
a professional translator of a Computer Assisted
Translation (CAT) tool enhanced with a SMT sys-
tem. For each input sentence, first the translator
receives one or more translation suggestions from
either a Translation Memory or a SMT system,
then (s)he chooses which suggestion is more use-
ful, and finally (s)he creates an approved transla-
tion by post-editing. The pair of input sentence
and post-edit is a valuable feedback to improve the
quality of next suggestions. While the sentence
pair is trivially added to the Translation Memory,
how to exploit it for improving the SMT system is
far to be a solved problem, but rather is a hot and
quite recent topic in the MT community.
In online MT adaptation specific issues have to
be addressed, which distinguish it from the more
standard and investigated task of domain adapta-
tion. First of all, the SMT system should adapt
very quickly, because the time between two con-
secutive requests are usually short, and very pre-
cisely, because the translator is annoyed by cor-
recting the same error several time. Then, a crucial
point is which and how information is extracted
from the feedback, and how it is exploited to up-
date the SMT system. Finally, model updating re-
lies on a little feedback consisting of just one sen-
tence pair.
In this work we focus on the word alignment
task which is the first and most important step in
extracting information from the given source and
its corresponding post-edit. In particular, we are
interested in the cases where the given sentence
pairs contain new words, for which no prior infor-
mation is available. This is an important and chal-
lenging problem in the online scenario, in which
the user interacts with the system and expects that
it learns from the previous corrections and does
not repeat the same errors again and again.
Unfortunately, state-of-the-art word-aligners
show poor generalization capability and are prone
to errors when infrequent or new words occur in
the sentence pair. Word alignment errors at this
stage could cause the extraction of wrong phrase
pairs, i.e. wrong translation alternatives, which
can lead in producing wrong translations for those
84
words, if they appear in the following sentences.
Our investigation focuses on how to quickly
build a highly precise word alignment from a
source sentence and its translation. Moreover, we
are interested in improving the word alignment of
unknown terms, i.e. not present in the training
data, because they are one of the most important
source of errors in model updating.
Although we are working in the online MT
adaptation framework, our proposal is worthwhile
per se; indeed, having an improved and fast word
aligner can be useful for other interesting tasks,
like for instance terminology extraction, transla-
tion error detection, and pivot translation.
In Section 2 we report on some recent ap-
proaches aiming at improving word alignment. In
Section 3, we describe three widely used toolk-
its, highlight their pros and cons in the online
MT adaptation scenario, and compare their per-
formance in aligning unknown terms. In Section 4
we propose a standalone module which refines the
word alignment of unknown words; moreover, we
present an enhanced faster implementation of the
best performing word aligner, to make it usable in
the online scenario. In Section 5 we show exper-
imental results of this module on three different
languages. Finally, we draw some final comments
in Section 6.
2 Related works
Hardt et al. (2010) presented an incremental re-
training method which simulates the procedure
of learning from post-edited MT outputs (refer-
ences), in a real time fashion. By dividing the
learning task into word alignment and phrase ex-
traction tasks, and replacing the standard word-
alignment module, which is a variation of EM
algorithm (Och and Ney, 2003), with a greedy
search algorithm, they attempt to find a quick ap-
proximation of the word alignments of the newly
translated sentence. They also use some heuris-
tics to improve the obtained alignments, without
supporting it with some proofs or even providing
some experimental results. Furthermore, the run-
ning time of this approach is not discussed, and it
is not clear how effective this approach is in online
scenarios.
Blain et al. (2012) have recently studied the
problem of incremental learning from post-editing
data, with minimum computational complexity
and acceptable quality. They use the MT out-
put (hypothesis) as a pivot to find the word align-
ments between the source sentence and its corre-
sponding reference. Similarly to (Hardt and Elm-
ing, 2010), once the word alignment between the
source and post-edit sentence pair is generated,
they use the standard phrase extraction method
to extract the parallel phrase pairs. This work
is based on an implicit assumption that MT out-
put is reliable enough to make a bridge between
source and reference. However, in the real world
this is not always true. The post-editor sometimes
makes a lot of changes in the MT output, or even
translates the entire sentence from scratch, which
makes the post-edit very different from the auto-
matic translation. Moreover, in the presence of
new words in the source sentence, the MT system
either does not produce any translation for the new
word, or directly copies it in the output. Due to
the above two reasons, there will be missing align-
ments between the automatic translation and post-
edit, which ultimately results in incomplete paths
from source to post-edit. But, the goal here is to
accurately align the known words, as well as learn-
ing the alignments of the new words, which is not
feasible by this approach.
In order to improve the quality of the word
alignments McCarley et al. (2011) proposed a
trainable correction model which given a sentence
pair and their corresponding automatically pro-
duced word alignment, it tries to fix the wrong
alignment links. Similar to the hill-climbing ap-
proach used in IBM models 3-5 (Brown et al.,
1993), this approach iteratively performs small
modifications in each step, based on the changes
of the previous step. However, the use of addi-
tional sources of knowledge, such as POS tags of
the words and their neighbours, helps the system
to take more accurate decisions. But, requiring
manual word alignments for learning the align-
ment moves makes this approach only applicable
for a limited number of language pairs for which
manual aligned gold references are available.
Tomeh et al. (2010) introduced a supervised
discriminative word alignment model for produc-
ing higher quality word alignments, which is
trained on a manually aligned training corpus. To
reduce the search space of the word aligner, they
propose to provide the system with a set of au-
tomatic word alignments and consider the union
of these alignments as the possible search space.
This transforms the word alignment process into
85
the alignment refinement task in which given a set
of automatic word alignments, the system tries to
find the best word alignment points. Similar to
(McCarley et al., 2011), this approach relies on the
manually annotated training corpora which is not
available for most of the language pairs.
3 Word Alignment
Word alignment is the task of finding the corre-
spondence among the words of a sentence pair
(Figure 1). From a mathematical point of view,
it is a relation among the words, because any word
in a sentence can be mapped into zero, one or
more words of the other, and vice-versa; in other
words, any kind of link is allowed, namely one-to-
one, many-to-one, many-to-many, as well as leav-
ing words unaligned. So called IBM models 1-5
(Brown et al., 1993) as well as the HMM-based
alignment models (Vogel et al., 1996), and their
variations are extensively studied and widely used
for this task. They are directional alignment mod-
els, because permit only many-to-one links; but
often the alignments in the two opposite directions
are combined in a so-called symmetrized align-
ment, which is obtained by intersection, union or
other smart combination.
Nowadays, word-aligners are mostly employed
in an intermediate step of the training procedure
of a SMT system; In this step, the training cor-
pus is word aligned as a side effect of the es-
timation of the alignment models by means of
the Expectation-Maximization algorithm. For this
task, they perform sufficiently well, because the
training data are often very large, and the limited
amount of alignment errors do not have strong im-
pact on the estimation of the translation model.
Instead, the already trained word-aligners are
rarely applied for aligning new sentence pairs. In
this task their performance are often not satisfac-
tory, due to their poor generalization capability;
they are especially prone to errors when infrequent
or new words occur in the sentence pair.
This is the actual task to be accomplished in the
online adaptive scenario: as soon as a new source
and post-edited sentence pair is available, it has
to be word aligned quickly and precisely. In this
scenario, the sentence pair likely does not belong
to the training corpus, hence might contain infre-
quent or new words, for which the aligner has little
or no prior information.
3.1 Evaluation Measures
A word aligner is usually evaluated in terms of
Precision, Recall, and F-measure (or shortly F ),
which are defined as follows (Fraser and Marcu,
2007):
Precision =
|A
?
P |
|A|
, Recall =
|A
?
S|
|S|
F ?measure =
1
?
Precision
+
1??
Recall
where A is the set of automatically computed
alignments, and S and P refer to the sure (un-
ambiguous) and possible (ambiguous) manual
alignments; note that S ? P . In this paper, ? is
set to 0.5 for all the experiments, in order to have
a balance between Precision and Recall.
In this paper we are mainly interested how the
word-aligner performs on the unknown words;
hence, we define a version of Precision, Recall,
and F metrics focused on the oov-alignment only,
i.e. the alignments for which either the source or
the target word is not included in the training cor-
pus. The subscript all identifies the standard met-
rics; the subscript oov identifies their oov-based
versions.
In Figure 1 we show manual and automatic
word alignments between an English-Italian sen-
tence pair. A sure alignment, like are-sono, is rep-
resented by a solid line, and a possible alignment,
like than-ai, by a dash line. An oov-alignment,
like that linking the unknown English word de-
ployable to the Italian word attivabili, is identi-
fied by a dotted line. According to this example,
Precision and Recall will be about 0.85 (=11/13)
and 0.91 (=10/11), respectively, and the corre-
sponding F is hence about 0.88. Focusing on the
oov-alignment only, Precision
oov
is 1.00 (=1/1),
Recall
oov
is 0.50 (=1/2), and F
oov
is 0.67.
3.2 Evaluation Benchmark
In this paper, we compare word-alignment perfor-
mance of three word-aligners introduced in Sec-
tion 3.3 on three distinct tasks, namely English-
Italian, English-French, and English-Spanish; the
training corpora, common to all word-aligners, are
subset of the JRC-legal corpus
1
(Steinberger et
al., ), of the Europarl corpus V7.0 (Koehn, 2005),
and of the Hansard parallel corpus
2
, respectively.
1
langtech.jrc.it/JRC-Acquis.html
2
www.isi.edu/natural-language/
download/hansard/index.html
86
financial assistance mechanisms are less rapidly deployable than conventional budgetary mechanisms
i meccanismi diassistenza finanziaria sono attivabili meno rapidamente rispetto ai meccanismi bilancio convenzionalidi
financial assistance mechanisms are less rapidly deployable than conventional budgetary mechanisms
i meccanismi diassistenza finanziaria sono attivabili meno rapidamente rispetto ai meccanismi bilancio convenzionalidi
Figure 1: Example of manual (above) and automatic (below) word alignments between an English-Italian
sentence pair. Sure and possible alignments are identified by solid and dash lines, respectively, and the
oov-alignments by a dotted line. The OOV words, like deployable (English) and finanziaria (Italian), are
printed in italics.
Statistics of the three training corpora are reported
in Table 1.
En-It En-Fr En-Es
Segments 940K 1.1M 713K
Tokens
src
19.8M 19.8M 19.8M
Tokens
trg
20.3M 23.3M 20.4M
Table 1: Statistics of the training corpora
for English-Italian, English-French, and English-
Spanish tasks.
Three evaluation data sets are also available,
which belong to the same domains of the cor-
responding training corpora. The English-Italian
test set was built by two professional translators
by correcting an automatically produced word-
alignment. The English-French test set is the man-
ually aligned parallel corpus introduced in (Och
and Ney, 2000)
3
. The English-Spanish test set was
provided by (Lambert et al., 2005)
4
. Statistics of
the three test sets are reported in Table 2.
To have a better understanding of the behavior
of the word aligners on the unknown words, we
created new test sets with an increasing ratio of the
unknown words (oov-rate), for each task. Starting
from each of the original test set, we replaced an
increasing portion of randomly chosen words by
strings which do not exist in the training corpus;
the oov-noise artificially introduced ranges from
3
www.cse.unt.edu/
?
rada/wpt/data/
English-French.test.tar.gz
4
www.computing.dcu.ie/
?
plambert/data/
epps-alignref.html
En-It En-Fr En-Es
Segments 200 484 500
Tokens
src
6,773 7,681 14,652
Tokens
trg
7,430 8,482 15,516
oov-rate
src
0.90 0.27 0.35
oov-rate
trg
0.84 0.34 0.32
#alignment 7,380 19,220 21,442
Table 2: Staticts of the test corpora for English-
Italian, English-French, and English-Spanish
tasks. oov-rate
src
and oov-rate
trg
are the ratio of
the new words in the source and target side of the
test corpus, respectively.
1% to 50%. For each value of the artificial oov-
noise (m = 1, ..., 50), we randomly selected m%
words in both the source and target side indepen-
dently, and replaced them by artificially created
strings. For selecting the words to be replaced
by artificially created strings, we do not differenti-
ate between the known and unknown words; hence
the actual oov-rate in the test corpus, used in the
plots, might be slightly larger.
To further make sure that the random selection
of the words does not affect the systems, for each
oov-noise we created 10 different test corpora and
reported the averaged results. One might think of
other approaches for introducing oov-noise, such
as replacing singletons or low-frequency words
which have more potential to be unknown, instead
of randomly selection of the words. But in this pa-
per we decided to follow the random selection of
the words.
87
3.3 State-of-the-art Word Aligners
We consider three widely-used word aligners,
namely berkeley, fast-align, and mgiza++. We
analyze their performance in aligning an held-out
test corpora; in particular, we compare their capa-
bility in handling the unknown words. For a fair
comparison, all aligners are trained on the same
training corpora described in Section 3.2.
berkeley aligner (Liang et al., 2006) applies the
co-training approach for training the IBM model
1 and HMM. We trained berkeley aligner using
5 iterations of model 1 followed by 5 iterations
of HMM. When applied to new sentence pairs,
the system produces bi-directional symmetrized
alignment.
fast-align is a recently developed unsuper-
vised word aligner that uses a log-linear re-
parametrization of IBM model 2 for training the
word alignment models (Dyer et al., 2013). We
exploited the default configuration with 5 itera-
tions for training. As the system is directional, we
trained two systems (source-to-target and target-
to-source). When applied to new sentence pairs,
we first produced the two directional alignments,
and then combined them into a symmetrized align-
ment by using the grow-diag-final-and heuristic
(Och and Ney, 2003).
mgiza++ (Gao and Vogel, 2008) and its an-
cestors, i.e. giza, and giza++, implement all the
IBM models and HMM based alignment models.
mgiza++ is a multithreaded version of giza++,
which enables an efficient use of multi-core plat-
forms. We trained the system using the follow-
ing configuration for model iterations: 1
5
h
5
3
3
4
3
.
mgiza++ also produces directional alignment;
hence, we followed the same protocol to create a
symmetrize alignment of sentence pairs as we did
for fast-align.
Differently from berkeley and fast-align,
mgiza++ somehow adapts its models when
applied to new sentence pairs. According to
the so-called ?forced alignment?, it essentially
proceeds with the training procedure on these
new data starting from pre-trained and pre-loaded
models, and produces the alignment as a by-
product. In preliminary experiments, we observed
that performing 3 iterations of model 4 is the
best configuration for mgiza++ to align the new
sentence pairs.
These word aligners are designed to work in of-
fline mode; they load the models and align the
whole set of available input data in one shot. How-
ever, in the online scenario where a single sen-
tence pair is provided at a time, they need to reload
the models every time which is very expensive in
terms of I/O operations. In this paper we first
were interested in measuring the quality of the
word aligners to select the best one. Therefore,
we mimic the online modality by forcing them to
align one sentence pair at a time.
Precision Recall F-measure
all oov all oov all oov
English-Italian
fast-align 82.6 33.3 82.8 19.6 82.7 24.7
berkeley 91.9 ? 81.0 ? 86.1 ?
mgiza++ 86.2 84.6 89.4 30.8 87.8 45.2
English-French
fast-align 81.5 47.2 91.8 19.5 86.3 27.6
berkeley 87.9 ? 92.9 ? 90.3 ?
mgiza++ 89.0 88.2 96.0 17.2 92.4 28.8
English-Spanish
fast-align 81.5 31.3 71.8 12.7 76.3 18.1
berkeley 88.7 ? 71.2 ? 79.0 ?
mgiza++ 89.2 95.5 80.6 35.6 84.7 51.9
Table 3: Comparison of different widely-used
word aligners in terms of precision, recall, and F-
measure on English-Italian, English-French, and
English-Spanish language pairs. Columns all re-
port the evaluation performed on all alignments,
while columns oov the evaluation performed on
the oov-alignments.
The three word aligners were evaluated on the
three tasks introduced in Section 3.2. Table 3
shows their performance on the full set of align-
ments (all) and on the subset of oov-alignments
(oov) in terms of Precision, Recall, and F-measure.
The figures show that all aligners perform well on
the whole test corpus. mgiza++ is definitely su-
perior to fast-align; it also outperforms berkeley
in terms of F-measure, but they are comparable in
terms of Precision.
Unfortunately, the quality of the word align-
ments produced for the new words is quite poor for
all systems. mgiza++ outperforms the other align-
ers in all the language pairs on oov-alignments,
and in particular it achieves a very high preci-
sion. On the contrary, berkeley aligner always fails
to detect out-of-vocabulary words; its precision is
hence undefined, and consequently its F-measure.
To our knowledge of the system, this behavior is
expected because of the joint alignment approach
used in berkeley which produces an alignment be-
tween two terms if both the directional models
88
 30 40
 50 60
 70 80
 90
 1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
OOV rate
English-Italian
mgizaberkeleyfast-align  30 40
 50 60
 70 80
 90
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
OOV rate
English-French
mgizaberkeleyfast-align  30 40
 50 60
 70 80
 90
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
OOV rate
English-Spanish
mgizaberkeleyfast-align
 10 15 20
 25 30 35
 40 45 50
 1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Italianmgizafast-align
 10 15 20
 25 30 35
 40 45 50
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Frenchmgizafast-align
 10 15 20
 25 30 35
 40 45 50
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Spanishmgizafast-align
Figure 2: Performance in terms of standard F-measure (above) and oov-based F-measure (below) of the
word aligners on test sets with increasing oov-rate, for all language pairs. The oov-based F-measure for
berkeley is not reported because it is undefined.
agree, and this hardly occurs for unknown words.
To further investigate the behavior of the word
aligners on the unknown words, we evaluated their
performance on the artificially created test sets,
described in Section 3.2. The performance of the
word aligners in terms of standard and oov-based
F-measure is shown in Figure 2. As expected, the
overall F-measure decreases by introducing un-
known words. mgiza++ is more accurate than the
other aligners up to oov-rate of 16%.
We observe that mgiza++ outperforms the oth-
ers in terms of the oov-based F-measure on
the English-Italian and English-Spanish language
pairs up to oov-noise of 32% and 16%, respec-
tively. fast-align instead performs better in the
English-French task. fast-align always show a
better quality when the oov-rate is very high.
oov-based F-measure is not reported for berke-
ley because this aligner is not able to detect oov-
alignments as explained above.
4 Enhancement to Word Alignment
4.1 Refinement of oov-alignments
To address the problem of unaligned new words,
we present a novel approach, in which the word
alignments of the source and target segment pair
are induced in two-steps. First, a standard word
aligner is applied; most of the words in the source
and target sentence pair will be aligned, but most
of the unknown words will not. It is worth men-
tioning that aligning unknown words in this step
depends on the quality of the employed word
aligner. Once the alignments are computed and
symmetrized (if required), phrase extraction pro-
cedure is applied to extract all valid phrase-pairs.
Note that un-aligned words are included in the ex-
tracted phrase pairs, if their surrounding words are
aligned.
It has been shown that inclusion of un-aligned
words in the phrase-pairs, generally, has neg-
ative effects on the translation quality and can
produce errors in the translation output (Zhang
et al., 2009). Nevertheless, the overlap among
phrase-pairs, which contain un-aligned unknown
words, can be considered as a valuable source
of knowledge for inducing the correct alignment
of these words. To get their alignments from
the extracted phrase-pairs we follow an approach
similar to (Espl?a-Gomis et al., 2012) in which
the word alignment probabilities are determined
by the alignment strength measure. Given the
source and target segments (S = {s
1
, . . . , s
l
}
and T = {t
1
, . . . , s
m
}), and the set of extracted
parallel phrase-pairs (?), the alignment strength
A
i,j
(S, T,?) of the s
i
and t
j
can be calculated as
follows:
A
i,j
(S, T,?) =
?
(?,?)??
cover(i, j, ?, ?)
|?|.|? |
cover(i, j, ?, ?) =
{
1 if s
i
? ? and t
j
? ?
0 otherwise
where |?| and |? | are the source and target
lengths (in words) of the phrase pair (?, ?).
89
cover(i, j, ?, ?) simply spots whether the word-
pair (s
i
, t
j
) is covered by the phrase pair (?, ?).
The alignment strengths are then used to pro-
duce the a directional source-to-target word align-
ments; s
i
is aligned to t
j
if A
i,j
> 0 and A
i,j
?
A
i,k
, ?k ? [1, |T |]. One-to-many alignment is
allowed in cases that multiple target words have
equal probabilities to be aligned to i-th source
word (A
i,j
= A
i,k
). The directional word align-
ments are then symmetrized.
The new set of symmetrized alignments can be
used in different ways: (i) as a replacement of the
initial word alignments as in (Espl?a-Gomis et al.,
2012), or (ii) as additional alignment points to be
added to the initial set. According to a prelim-
inary investigation, we choose the latter option:
only a subset of the new word alignments is used
for updating the initial alignments. More specifi-
cally, we add only the alignments of the new words
which are not already aligned.
Moreover, our approach differs from that pro-
posed by Espl?a-Gomis et al. (2012) in the proce-
dure to collect the original set of phrase pairs from
the source and target sentence pair. They rely on
the external sources of information such as online
machine translation systems (e.g. Google Trans-
late, and Microsoft Translator). Communicating
with external MT systems imposes some delays
to the pipeline, which is not desired for the on-
line scenario. Furthermore, the words that are not
known by the machine translation systems are not
covered by any phrase-pair, hence the refinement
module is not able to align them.
We instead employ the phrase-extract software
5
provided by the Moses toolkit, which relies on the
alignment information of the given sentence pair,
and allows the inclusion of un-aligned unknown
words in the extracted phrase pairs; hence, the re-
finement module has the potential to find the cor-
rect alignment for those words.
Note that there is no constraint on the word
alignment and phrase extraction modules used in
the first step, hence, any word aligner and phrase
extractor can be used for computing the initial
alignments and extracting the parallel phrase pairs
from the given sentence pairs. But, since the out-
puts of the first aligner make the ground for obtain-
ing the alignments of the second level, they need
to be highly accurate and precise.
5
The ?grow-diag-final-and? heuristic was set for the sym-
metrization.
4.2 onlineMgiza++
The experiments to compare state-of-the-art word
aligners, reported and discussed in Section 3, are
carried out offline. This is because the aforemen-
tioned word aligners are not designed to work on-
line, and need to load the models every time re-
ceives a new sentence pair. Loading the models is
very time consuming, and depending on the size
of the models might take several minutes, which
is not desired for the online scenario.
To overcome this problem, we decided to im-
plement an online version of mgiza++ which
provides the best performance as shown in Sec-
tion 3.3. This new version, called onlineM-
giza++, works in client-server mode. It con-
sists of two main modules mgizaServer and mgiza-
Client. mgizaServer is responsible for computing
the alignment of the given sentence pairs. To avoid
unnecessary I/O operations, mgizaServer loads all
the required models once at the beginning of the
alignment session, and releases them at the end.
mgizaClient communicates with the client appli-
cations through the standard I/O channel.
In our final experiments we observed some
unexpected differences between the results of
mgiza++ and onlineMgiza++. Therefore, we do
not present the results of onlineMgiza++ in this
paper. However, we expect the two systems pro-
duce the same results.
5 Experimental Results
In this section we evaluate the effectiveness of
the proposed refinement module. Each consid-
ered word aligner was equipped by our refinement
module, and compared to its corresponding base-
line. Figure 3 shows the oov-based F-measure
achieved by the baseline and enhanced word align-
ers on all test sets and all tasks. We observe that
the refinement module consistently improves the
F-measure of all aligners on all language pairs;
The improvement for mgiza++ are big (up to
10%) for very low oov-rates and decreases when
the oov-rate increases; the same but smaller be-
havior is observed for fast-align. This is due to the
fact that by inserting more oov words into the test
sets the systems are able to produce less accurate
alignment points, which leads in lower contextual
information (i.e. smaller number of overlapping
phrase-pairs) for aligning the unknown words. In-
terestingly, the refinement module applied to the
berkeley output permits the correct detection of
90
 0 10 20
 30 40 50
 60 70 80
 1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Italianmgiza + enhberkeley + enhfast-align + enhmgizafast-align
 0 10 20
 30 40 50
 60 70 80
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Frenchmgiza + enhberkeley + enhfast-align + enhmgizafast-align
 0 10 20
 30 40 50
 60 70 80
 0.5  1  2  4  8  16  32
F
-
m
e
a
s
u
r
e
-
o
o
v
OOV rate
English-Spanishmgiza + enhberkeley + enhfast-align + enhmgizafast-align
Figure 3: Performance in terms of oov-based F-measure of the baseline and enhanced word aligners on
test sets with increasing oov rate, for all language pairs. The oov-based F-measure for berkeley is not
reported because it is undefined.
 0 0.5
 1 1.5
 2
 1  2  4  8  16  32
D
e
l
t
a
 
F
-
m
e
a
s
u
r
e
OOV rate
English-Italianmgiza + enhberkeley + enhfast-align + enh
 0 0.5
 1 1.5
 2
 0.5  1  2  4  8  16  32
D
e
l
t
a
 
F
-
m
e
a
s
u
r
e
OOV rate
English-Frenchmgiza + enhberkeley + enhfast-align + enh
 0 0.5
 1 1.5
 2
 0.5  1  2  4  8  16  32
D
e
l
t
a
 
F
-
m
e
a
s
u
r
e
OOV rate
English-Spanishmgiza + enhberkeley + enhfast-align + enh
Figure 4: Difference of performance in terms of standard F-measure of the enhanced word aligners from
their corresponding baselines on test sets with increasing OOV rate, for all language pairs.
many oov-alignments, which the baseline system
can not find most of them.
Furthermore, Figure 4 reports the F-measure
differences achieved by the enhanced word-
aligners from their corresponding baselines on the
full data sets. The refinement module slightly
but consistently improves the overall F-measure as
well, especially for high oov-rates. The highest
improvement is achieved by the enhanced berke-
ley aligner, mainly because its baseline performs
worse in this condition.
6 Conclusion
In this paper we discussed the need of having a fast
and reliable online word aligner in the online adap-
tive MT scenario that is able to accurately align
the new words. The quality of three state-of-the-
art word aligners, namely berkeley, mgiza++, and
fast-align, were evaluated on this task in terms of
Precision, Recall, and F-measure. For this purpose
we created a benchmark in which an increasing
amount of the words of the test corpus are ran-
domly replaced by new words in order to augment
the oov-rate. The results show that the quality of
the aligners on new words is quite low, and sug-
gest that new models are required to effectively ad-
dress this task. As a first step, we proposed a fast
and language independent procedure for aligning
the unknown words which refines any given au-
tomatic word alignment. The results show that
the proposed approach significantly increases the
word alignment quality of the new words.
In future we plan to evaluate our approach in an
end-to-end evaluation to measure its effect on the
final translation. We also plan to investigate the
exploitation of additional features such as linguis-
tic and syntactic information in order to further
improve the quality of the word alignment mod-
els as well as the proposed refinement procedure.
However, this requires other policies of introduc-
ing new words, rather than just randomly selecting
the words and replacing them by artificial strings.
Acknowledgments
This work was supported by the MateCat project,
which is funded by the EC under the 7
th
Frame-
work Programme.
References
F. Blain, H. Schwenk, and J. Senellart. 2012. In-
cremental adaptation using translation information
and post-editing analysis. In International Work-
shop on Spoken Language Translation, pages 234?
241, Hong-Kong (China).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
91
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?312.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644?648, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Miquel Espl?a-Gomis, Felipe S?anchez-Mart??nez, and
Mikel L. Forcada. 2012. A simple approach to use
bilingual information sources for word alignment.
Procesamiento del Lenguaje Natural, (49):93?100.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Comput. Linguist., 33(3):293?303.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Daniel Hardt and Jakob Elming. 2010. Incremental
re-training for post-editing smt. In 9th Conference
of the Association for Machine Translation in the
Americas (AMTA), Denver, United States.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Tenth Machine Translation Summit (MT Summit X),
pages 79?86, Phuket, Thailand.
Patrik Lambert, Adri`a de Gispert, Rafael E. Banchs,
and Jos?e B. Mari?no. 2005. Guidelines for word
alignment evaluation and manual alignment. Lan-
guage Resources and Evaluation, 39(4):267?285.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreent. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim
Roukos, Bing Xiang, and Jian-ming Xu. 2011. A
correction model for word alignments. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?11, pages 889?
898, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma?z Erjavec, Dan Tufis?, and
D?aniel Varga. The jrc-acquis: A multilingual
aligned parallel corpus with 20+ languages. In In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), pages
2142?2147, Genoa, Italy.
Nadi Tomeh, Alexandre Allauzen, Guillaume Wis-
niewski, and Franois Yvon. 2010. Refining word
alignment with discriminative training. In Proceed-
ings of the ninth Conference of the Association for
Machine Translation in the America (AMTA).
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Pro-
ceedings of COLING, pages 836?841, Copenhagen,
Denmark.
Yuqi Zhang, Evgeny Matusov, and Hermann Ney.
2009. Are unaligned words important for machine
translation? In Conference of the European As-
sociation for Machine Translation, pages 226?233,
Barcelona, Spain.
92
