Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 649?657, Prague, June 2007. c?2007 Association for Computational Linguistics
A Graph-based Approach to Named Entity Categorization in Wikipedia
Using Conditional Random Fields
Yotaro Watanabe, Masayuki Asahara and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{yotaro-w,masayu-a,matsu}@is.naist.jp
Abstract
This paper presents a method for catego-
rizing named entities in Wikipedia. In
Wikipedia, an anchor text is glossed in a
linked HTML text. We formalize named en-
tity categorization as a task of categorizing
anchor texts with linked HTML texts which
glosses a named entity. Using this repre-
sentation, we introduce a graph structure in
which anchor texts are regarded as nodes.
In order to incorporate HTML structure on
the graph, three types of cliques are defined
based on the HTML tree structure. We pro-
pose a method with Conditional Random
Fields (CRFs) to categorize the nodes on
the graph. Since the defined graph may in-
clude cycles, the exact inference of CRFs is
computationally expensive. We introduce an
approximate inference method using Tree-
based Reparameterization (TRP) to reduce
computational cost. In experiments, our pro-
posed model obtained significant improve-
ments compare to baseline models that use
Support Vector Machines.
1 Introduction
Named and Numeric Entities (NEs) refer to proper
nouns (e.g. PERSON, LOCATION and ORGANI-
ZATION), time expressions, date expressions and so
on. Since a large number of NEs exist in the world,
unknown expressions appear frequently in texts, and
they become hindrance to real-world text analysis.
To cope with the problem, one effective ways to add
a large number of NEs to gazetteers.
In recent years, NE extraction has been performed
with machine learning based methods. However,
such methods cannot cover all of NEs in texts.
Therefore, it is necessary to extract NEs from ex-
isting resources and use them to identify more NEs.
There are many useful resources on the Web. We fo-
cus on Wikipedia1 as the resource for acquiring NEs.
Wikipedia is a free multilingual online encyclope-
dia and a rapidly growing resource. In Wikipedia,
a large number of NEs are described in titles of ar-
ticles with useful information such as HTML tree
structures and categories. Each article links to other
related articles. According to these characteristics,
they could be an appropriate resource for extracting
NEs.
Since a specific entity or concept is glossed in a
Wikipedia article, we can regard the NE extraction
problem as a document classification problem of the
Wikipedia article. In traditional approaches for doc-
ument classification, in many cases, documents are
classified independently. However, the Wikipedia
articles are hypertexts and they have a rich structure
that is useful for categorization. For example, hyper-
linked mentions (we call them anchor texts) which
are enumerated in a list tend to refer to the articles
that describe other NEs belonging to the same class.
It is expected that improved NE categorization is ac-
complished by capturing such dependencies.
We structure anchor texts and dependencies be-
tween them into a graph, and train graph-based
CRFs to obtain probabilistic models to estimate cat-
egories for NEs in Wikipedia.
So far, several statistical models that can cap-
1http://wikipedia.org/
649
ture dependencies between examples have been pro-
posed. There are two types of classification meth-
ods that can capture dependencies: iterative classi-
fication methods (Neville and Jensen, 2000; Lu and
Getoor, 2003b) and collective classification methods
(Getoor et al, 2001; Taskar et al, 2002). In this
paper, we use Conditional Random Fields (CRFs)
(Lafferty et al, 2001) for NE categorization in
Wikipedia.
The rest of the paper is structured as follows. Sec-
tion 2 describes the general framework of CRFs.
Section 3 describes a graph-based CRFs for NE cat-
egorization in Wikipedia. In section 4, we show
the experimental results. Section 5 describes related
work. We conclude in section 6.
2 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are undirected graphical models that give a
conditional probability distribution p(y|x) in a form
of exponential model.
CRFs are formalized as follows. Let G = {V,E}
be an undirected graph over random variables y and
x, where V is a set of vertices, and E is a set
of edges in the graph G. When a set of cliques
C = {{yc,xc}} are given, CRFs define the con-
ditional probability of a state assignment given an
observation set.
p(y|x) = 1Z(x)
?
c?C
?(xc,yc) (1)
where ?(xc,yc) is a potential function defined
over cliques, and Z(x) =
?
y
?
c?C ?(xc,yc) is
the partition function.
The potentials are factorized according to the set
of features {fk}.
?(xc,yc) = exp
(
?
k
?kfk(xc,yc)
)
(2)
where F = {f
1
, ..., fK} are feature functions on
the cliques, ? = {?
1
, ..., ?K ? R} are the model
parameters. The parameters ? are estimated itera-
tive scaling or quasi-Newton method from labeled
data.
The original paper (Lafferty et al, 2001) fo-
cused on linear-chain CRFs, and applied them to
part-of-speech tagging problem. McCallum et al
(2003), Sutton et al(2004) proposed Dynamic Con-
ditional Random Fields (DCRFs), the generaliza-
tion of linear-chain CRFs, that have complex graph
structure (include cycles). Since DCRFs model
structure contains cycles, it is necessary to use ap-
proximate inference methods to calculate marginal
probability. Tree-based Reparameterization (TRP)
(Wainwright et al, 2003), a schedule for loopy be-
lief propagation, is used for approximate inference
in these papers.
3 Graph-based CRFs for NE
Categorization in Wikipedia
In this section we describe how to apply CRFs for
NE categorization in Wikipedia.
Each Wikipedia article describes a specific entity
or concept by a heading word, a definition, and one
or more categories. One possible approach is to clas-
sify each NE described in an article into an appropri-
ate category by exploiting the definition of the arti-
cle. This process can be done one by one without
considering the relationship with other articles.
On the other hand, articles in Wikipedia are
semi-structured texts. Especially lists (<UL> or
<OL>) and tables (<TABLE>) have an important
characteristics, that is, occurrence of elements in
them have some sort of dependencies. Structural
characteristics, such as lists (<UL> or <OL>) or
tables (<TABLE>), are useful becase their ele-
ments have some sort of dependencies.
Figure 2 shows an example of an HTML segment
and the corresponding tree structure. The first an-
chor texts in each list tag (<LI>) tend to be in the
same NE category. Such characteristics are useful
feature for the categorization task. In this paper we
focus on lists which appear frequently in Wikipedia.
Furthermore, there are anchor texts in articles.
Anchor texts are glossed entity or concept described
with links to other pages. With this in mind, our NE
categorization problem can be regarded as NE cat-
egory labeling problem for anchor texts in articles.
Exploiting dependencies of anchor texts that are in-
duced by the HTML structure is expected to improve
categorization performance.
We use CRFs for categorization in which anchor
texts correspond to random variables V in G and de-
650
Sibling ES = {(vTi , vTj )|vTi , vTj ? V T , d(vTi , ca(vTi , vTj )) = d(vTj , ca(vTi , vTj )) = 1, vTj = ch(pa(vTj , 1), k),
vTi = ch(pa(vTi , 1), max{l|l < k})}
Cousin EC = {(vTi , vTj )|vTi , vTj ? V T , d(vTi , ca(vTi , vTj )) = d(vTj , ca(vTi , vTj )) ? 2, vTi = ch(pa(vTi ), k),
vTj = ch(pa(vTj ), k), pa(vTj , d(vTj , ca(vTi , vTj ))? 1) = ch(pa(vTj , d(vTj , ca(vTi , vTj ))), k),
pa(vTi , d(vTi , ca(vTi , vTj ))? 1) = ch(pa(vTi , ca(vTi , vTj )),max{l|l < k})}
Relative ER = {(vTi , vTj )|vTi , vTj ? V T , d(vTi , ca(vTi , vTj )) = 1, d(vTj , ca(vTi , vTj )) = 3,
pa(vTj , 2) = ch(pa(vTj , 3), k), vTi = ch(pa(vTi , 1),max{l|l < k})}
Figure 1: The definitions of sibling, cousin and relative cliques, where ES , EC , ER correspond to sets which
consist of anchor text pairs that have sibling, cousin and relative relations respectively.
pendencies between anchor texts are treated as edges
E in G. In the next section, we describe the concrete
way to construct graphs.
3.1 Constructing a graph from an HTML tree
An HTML document is an ordered tree. We de-
fine a graph G = (V G , EG) on an HTML tree
T HTML = (V T , ET ): the vertices V G are anchor
texts in the HTML text; the edges E are limited to
cliques of Sibling, Cousin, and Relative, which we
will describe later in the section. These cliques are
intended to encode a NE label dependency between
anchor texts where the two NEs tend to be in the
same or related class, or one NE affects the other
NE label.
Let us consider dependent anchor text pairs in
Figure 2. First, ?Dillard & Clark? and ?country
rock? have a sibling relation over the tree structure,
and appearing the same element of the list. The latter
element in this relation tends to be an attribute or a
concept of the other element in the relation. Second,
?Dillard & Clark? and ?Carpenters? have a cousin
relation over the tree structure, and they tend to have
a common attribute such as ?Artist?. The elements in
this relation tend to belong to the same class. Third,
?Carpenters? and ?Karen Carpenter? have a relation
in which ?Karen Carpenter? is a sibling?s grandchild
in relation to ?Carpenters? over the tree structure.
The latter elements in this relation tends to be a con-
stituent part of the other element in the relation. We
can say that the model can capture dependencies by
dealing with anchor texts that depend on each other
as cliques. Based on the observations as above, we
treat a pair of anchor texts as cliques which satisfy
the condtions in Figure 1.
<UL>
<LI>
<A>
<LI> <LI>
<A>
<A>
<UL><A>
Dillard & 
Clark
country 
rock
Carpenters
Karen 
Carpenter
Sibling
Cousin
Relative
 Dillard & Clark ??
?country rock?
 Carpenters
 Karen Carpenter
Figure 2: Correspondence between tree structure
and defined cliques.
Now, we define the three sorts of edges given an
HTML tree. Consider an HTML tree T HTML =
(V T , ET ), where V T and ET are nodes and edges
over the tree. Let d(vTi , vTj ) be the number of edges
between vTi and vTj where vTi , vTj ? V T , pa(vTi , k)
be k-th generation ancestor of vTi , ch(vTi , k) be
vTi ?s k-th child, ca(vTi , vTj ) be a common ances-
tor of vTi , vTj ? V T . Precise definitions of cliques,
namely Sibling, Cousin, and Relative, are given in
Figure 1. A set of cliques used in our graph-based
CRFs are edges defined in Figure 1 and vertices, i.e.
C = ES ? EC ? ER ? V . Note that they are re-
stricted to pairs of the nearest vertices to keep the
graph simple.
3.2 Model
We introduce potential functions for cliques to de-
fine conditional probability distribution over CRFs.
Conditional distribution over label set y given ob-
651
servation set x is defined as:
p(y|x) = 1Z(x)
?
?
?
(vi,vj)?ES?EC?ER
?SCR(yi, yj)
?
?
?
?
?
vi?V
?V (yi,x)
?
? (3)
where ?SCR(yi, yj) is the potential over sibling,
cousin and relative edges, ?V (yi,x) is the potential
over the nodes, and Z(x) is the partition function.
The potentials ?SCR(yi, yj) and ?V (yi,x) factor-
ize according to the features fk and weights ?k as:
?SCR (yi, yj) = exp
(
?
k
?kfk(yi, yj)
)
(4)
?V (yi,x) = exp
(
?
k?
?k?fk?(yi,x)
)
(5)
fk(yi, yj) captures co-occurrences between labels,
where k ? {(yi, yj)|Y ? Y} corresponds to the par-
ticular element of the Cartesian product of the label
set Y . fk?(yi,x) captures co-occurrences between
label yi ? Y and observation features, where k? cor-
responds to the particular element of the label set
and observed features.
The weights of a CRF, ? = {?k, . . . , ?k? , . . .}
are estimated to maximize the conditional log-
likelihood of the graph in a training dataset
D = {?x(1), y(1)?, ?x(2), y(2)?, . . . , ?x(N), y(N)?}
The log-likelihood function can be defined as fol-
lows:
L? =
N
?
d=1
[
?
(vi,vj)?E(d)S ?E
(d)
C ?E
(d)
R
?
k
?kfk(yi, yj)
+
?
vi?V (d)
?
k?
?k?fk?(yi,x(d)) ? logZ(x(d))]
?
?
k
?2k
2?2 ?
?
k?
?2k?
2?2 (6)
where the last two terms are due to the Gaussian
prior (Chen and Rosenfeld, 1999) used to reduce
overfitting. Quasi-Newton methods, such as L-
BFGS (Liu and Nocedal, 1989) can be used for max-
imizing the function.
3.3 Tree-based Reparameterization
Since the proposed model may include loops, it is
necessary to introduce an approximation to calculate
mariginal probabilities. For this, we use Tree-based
Reparameterization (TRP) (Wainwright et al, 2003)
for approximate inference. TRP enumerates a set of
spanning trees from the graph. Then, inference is
performed by applying an exact inference algorithm
such as Belief Propagation to each of the spanning
trees, and updates of marginal probabilities are con-
tinued until they converge.
4 Experiments
4.1 Dataset
Our dataset is a random selection of 2300 articles
from the Japanese version of Wikipedia as of Octo-
ber 2005. All anchor texts appearing under HTML
<LI> tags are hand-annotated with NE class la-
bel. We use the Extended Named Entity Hierar-
chy (Sekine et al, 2002) as the NE class labeling
guideline, but reduce the number of classes to 13
from the original 200+ by ignoring fine-grained cat-
egories and nearby categories in order to avoid data
sparseness. We eliminate examples that consist of
less than two nodes in the SCR model. There are
16136 anchor texts with 14285 NEs. The number
of Sibling, Cousin and Relative edges in the dataset
are |ES | = 4925, |EC | = 13134 and |ER| = 746
respectively.
4.2 Experimental settings
The aims of experiments are the two-fold. Firstly,
we investigate the effect of each cliques. The sev-
eral graphs are composed with the three sorts of
edges. We also compare the graph-based models
with a node-wise method ? just MaxEnt method not
using any edge dependency. Secondly, we com-
pare the proposed method by CRFs with a baseline
method by Support Vector Machines (SVMs) (Vap-
nik, 1998).
The experimental settings of CRFs and SVMs are
as follows.
CRFs In order to investigate which type of clique
boosts classification performance, we perform ex-
periments on several CRFs models that are con-
structed from combinations of defined cliques. Re-
652
SCR SC SR CR
# of loopy examples 318 (36%) 324 (32%) 101 (1%) 42 (2%)
# of linear chain or tree examples 555 (64%) 631 (62%) 2883 (27%) 1464 (54%)
# of one node examples 0 (0%) 60 (6%) 7800 (72%) 1176 (44%)
# of total examples 873 1015 10784 2682
average # of nodes per example 18.5 15.8 1.5 6.0
S C R I
# of loopy examples 0 (0%) 0 (0%) 0 (0%) 0 (0%)
# of linear chain or tree examples 2913 (26%) 1631 (54%) 237 (2%) 0 (0%)
# of one node examples 8298 (74%) 1380 (46%) 15153 (98%) 16136 (100%)
# of total examples 11211 3011 15390 16136
average # of nodes per example 1.4 5.4 1.05 1
Table 1: The dataset details constructed from each model.
sulting models of CRFs evaluated on this experi-
ments are SCR, SC, SR, CR, S, C, R and I (indepen-
dent). Figure 3 shows representative graphs of the
eight models. When the graph are disconnected by
reducing the edges, the classification is performed
on each connected subgraph. We call it an example.
We name the examples according the graph struc-
ture: ?loopy examples? are subgraphs including at
least one cycle; ?linear chain or tree examples? are
subgraphs including not a cycle but at least an edge;
?one node examples? are subgraphs without edges.
Table 1 shows the distribution of the examples of
each model. Since SCR, SC, SR and CR model have
loopy examples, TRP approximate inference is nec-
essary. To perform training and testing with CRFs,
we use GRMM (Sutton, 2006) with TRP. We set the
Gaussian Prior variances for weights as ?2 = 10 in
all models.
SC model
C
C
C C
S S
SSC
SCR model
C
C
C C
S S
SS
R
R
C
SR model
S S
SS
R
R
CR model
C
C
C C
R
R
C
S model
S S
SS
C model
C
C
C C
C
R model
R
R
I model
Figure 3: An example of graphs constructed by
combination of defined cliques. S, C, R in the
model names mean that corresponding model has
Sibling, Cousin, Relative cliques respectively. In
each model, classification is performed on each con-
nected subgraph.
SVMs We introduce two models by SVMs (model
I and model P). In model I, each anchor text is clas-
sified independently. In model P, we ordered the
anchor texts in a linear-chain sequence. Then, we
perform a history-based classification along the se-
quence, in which j ? 1-th classification result is
used in j-th classification. We use TinySVM with
a linear-kernel. One-versus-rest method is used for
multi-class classification. To perform training and
testing with SVMs, we use TinySVM 2 with a linear-
kernel, and one-versus-rest is used for multi-class
classification. We used the cost of constraint vio-
lation C = 1.
Features for CRFs and SVMs The features used
in the classification with CRFs and SVMs are shown
in Table 2. Japanese morphological analyzer MeCab
3 is used to obtain morphemes.
4.3 Evaluation
We evaluate the models by 5 fold cross-validation.
Since the number of examples are different in each
model, the datasets are divided taking the examples
? namely, connected subgraphs ? in SCR model.
The size of divided five sub-data are roughly equal.
We evaluate per-class and total extraction perfor-
mance by F1-value.
4.4 Results and discussion
Table 3 shows the classification accuracy of each
model. The second column ?N? stands for the num-
ber of nodes in the gold data. The second last row
?ALL? stands for the F1-value of all NE classes.
2http://www.chasen.org/?taku/software/
TinySVM/
3http://mecab.sourceforge.net/
653
types feature SVMs CRFs
observation definition (bag-of-words) ? ? (V )
features heading of articles
? ?
(V )
heading of articles (morphemes) ? ? (V )
categories articles
? ?
(V )
categories articles (morphemes) ? ? (V )
anchor texts
? ?
(V )
anchor texts (morphemes) ? ? (V )
parent tags of anchor texts
? ?
(V )
text included in the last header of anchor texts
? ?
(V )
text included in the last header of anchor texts(morphemes) ? ? (V )
label features between-label feature
?
(S,C,R)
previous label
?
Table 2: Features used in experiments. ?
?
? means that the corresponding features are used in classification.
The V , S, C and R in CRFs column corresponds to the node, sibling edges, cousin edges and relative edges
respectively.
CRFs SVMs
NE CLASS N C CR I R S SC SCR SR I P
PERSON 3315 .7419 .7429 .7453 .7458 .7507 .7533 .7981 .7515 .7383 .7386
TIMEX/NUMEX 2749 .9936 .9944 .9940 .9936 .9938 .9931 .9933 .9940 .9933 .9935
FACILITY 2449 .8546 .8541 .8540 .8516 .8500 .8530 .8495 .8495 .8504 .8560
PRODUCT 1664 .7414 .7540 .7164 .7208 .7130 .7371 .7418 .7187 .7154 .7135
LOCATION 1480 .7265 .7239 .6989 .7048 .6974 .7210 .7232 .7033 .7022 .7132
NATURAL OBJECTS 1132 .3333 .3422 .3476 .3513 .3547 .3294 .3304 .3316 .3670 .3326
ORGANIZATION 991 .7122 .7160 .7100 .7073 .7122 .6961 .5580 .7109 .7141 .7180
VOCATION 303 .9088 .9050 .9075 .9059 .9150 .9122 .9100 .9186 .9091 .9069
EVENT 121 .2740 .2345 .2533 .2667 .2800 .2740 .2759 .2667 .3418 .3500
TITLE 42 .1702 .0889 .2800 .2800 .3462 .2083 .1277 .3462 .2593 .2642
NAME OTHER 24 .0000 .0000 .0000 .0000 .0000 .0000 .0000 .0000 .0690 .0000
UNIT 15 .2353 .1250 .2353 .2353 .2353 .1250 .1250 .2353 .3333 .3158
ALL 14285 .7846 .7862 .7806 .7814 .7817 .7856 .7854 .7823 .7790 .7798
ALL (no articles) 3898 .5476 .5495 .5249 .5274 .5272 .5484 .5465 .5224 .5278 .5386
Table 3: Comparison of F1-values of CRFs and SVMs.
654
The last row ?ALL (no article)? stands for the F1-
value of all NE classes which have no gloss texts in
Wikipedia.
Relational vs. Independent Among the models
constructed by combination of defined cliques, the
best F1-value is achieved by CR model, followed by
SC, SCR, C, SR, S, R and I. We performed McNe-
mar paired test on labeling disagreements between
CR model of CRFs and I model of CRFs. The
difference was significant (p < 0.01). These re-
sults show that considering dependencies work pos-
itively in obtaining better accuracy than classify-
ing independently. The Cousin cliques provide the
highest accuracy improvement among the three de-
fined cliques. The reason may be that the Cousin
cliques appear frequently in comparison with the
other cliques, and also possess strong dependencies
among anchor texts. As for PERSON, better accu-
racy is achieved in SC and SCR models. In fact,
the PERSON-PERSON pairs frequently appear in
Sibling cliques (435 out of 4925) and in Cousin
cliques (2557 out of 13125) in the dataset. Also, as
for PRODUCT and LOCATION, better accuracy is
achieved in the models that contain Cousin cliques
(C, CR, SC and SCR model). 1072 PRODUCT-
PRODUCT pairs and 738 LOCATION-LOCATION
pairs appear in Cousin cliques. ?All (no article)?
row in Table 3 shows the F1-value of nodes which
have no gloss texts. The F1-value difference be-
tween CR and I model of CRF in ?ALL (no article)?
row is larger than the difference in ?All? row. The
fact means that the dependency information helps to
extract NEs without gloss texts in Wikipedia. We
attempted a different parameter tying in which the
SCR potential functions are tied with a particular ob-
servation feature. This parameter tying is introduced
by Ghamrawi and McCallum (2005). However, we
did not get any improved accuracy.
CRFs vs. SVMs The best model of CRFs (CR
model) outperforms the best model of SVMs (P
model). We performed McNemar paired test on la-
beling disagreements between CR model of CRFs
and P model of SVMs. The difference was signifi-
cant (p < 0.01). In the classes having larger num-
ber of examples, models of CRFs achieve better F1-
values than models of SVMs. However, in several
classes having smaller number of examples such as
0.4 0.5 0.6 0.7 0.8
0.
80
0.
85
0.
90
0.
95
Recall
Pr
ec
is
io
n
CR model of CRFs
Figure 4: Precision-Recall curve obtained by vary-
ing the threshold ? of marginal probability from 1.0
to 0.0.
EVENT and UNIT, models of SVMs achieve signif-
icantly better F1-values than models of CRFs.
Filtering NE Candidates using Marginal Prob-
ability The precision-recall curve obtained by
thresholding the marginal probability of the MAP
estimation in the CR models is shown in Figure 4.
The curve reaches a peak at 0.57 in recall, and the
precision value at that point is 0.97. This preci-
sion and recall values mean that 57% of all NEs can
be classified with approximately 97% accuracy on a
particular thresholding of marginal probability. This
results suggest that the extracted NE candidates can
be filtered with fewer cost by exploiting the marginal
probability.
Training Time The total training times of all
CRFs and SVMs models are shown in Table 4. The
training time tends to increase in case models have
complicated graph structure. For instance, model
SCR has complex graph structure compare to model
I, therefore the SCR?s training time is three times
longer than model I. Training the models by SVMs
are faster than training the models by CRFs. The dif-
ference comes from the implementation issues: C++
655
CRFs SVMs
C CR I R S SC SCR SR I P
Training Time (minutes) 207 255 97 90 138 305 316 157 28 29
Table 4: Training Time (minutes)
vs. Java, differences of feature extraction modules,
and so on. So, the comparing these two is not the
important issue in this experiment.
5 Related Work
Wikipedia has become a popular resource for NLP.
Bunescu and Pasca used Wikipedia for detecting and
disambiguating NEs in open domain texts (2006).
Strube and Ponzetto explored the use of Wikipedia
for measuring Semantic Relatedness between two
concepts (2006), and for Coreference Resolution
(2006).
Several CRFs have been explored for informa-
tion extraction from the web. Tang et al pro-
posed Tree-structured Conditional Random Fields
(TCRFs) (2006) that capture hierarchical structure
of web documents. Zhu et al proposed Hierar-
chical Conditional Random Fields (HCRFs) (2006)
for product information extraction from Web docu-
ments. TCRFs and HCRFs are similar to our ap-
proach described in section 4 in that the model struc-
ture is induced by page structure. However, the
model structures of these models are different from
our model.
There are statistical models that capture depen-
dencies between examples. There are two types of
classification approaches: iterative (Lu and Getoor,
2003b; Lu and Getoor, 2003a) or collective (Getoor
et al, 2001; Taskar et al, 2002). Lu et al (2003a;
2003b) proposed link-based classification method
based on logistic regression. This model iterates lo-
cal classification until label assignments converge.
The results vary from the ordering strategy of lo-
cal classification. In contrast to iterative classifica-
tion methods, collective classification methods di-
rectly estimate most likely assignments. Getoor
et al proposed Probabilistic Relational Models
(PRMs) (2001) which are built upon Bayesian Net-
works. Since Bayesian Networks are directed graph-
ical models, PRMs cannot model directly the cases
where instantiated graph contains cycles. Taskar et
al. proposed Relational Markov Networks (RMNs)
(2002). RMNs are the special case of Conditional
Markov Networks (or Conditional Random Fields)
in which graph structure and parameter tying are de-
termined by SQL-like form.
As for the marginal probability to use as a confi-
dence measure shown in Figure 4, Peng et al (2004)
has applied linear-chain CRFs to Chinese word seg-
mentation. It is calculated by constrained forward-
backward algorithm (Culotta and McCallum, 2004),
and confident segments are added to the dictionary
in order to improve segmentation accuracy.
6 Conclusion
In this paper, we proposed a method for categorizing
NEs in Wikipedia. We defined three types of cliques
that are constitute dependent anchor texts in con-
struct CRFs graph structure, and introduced poten-
tial functions for them to reflect classification. The
experimental results show that the effectiveness of
capturing dependencies, and proposed CRFs model
can achieve significant improvements compare to
baseline methods with SVMs. The results also show
that the dependency information from the HTML
tree helps to categorize entities without gloss texts
in Wikipedia. The marginal probability of MAP as-
signments can be used as confidence measure of the
entity categorization. We can control the precision
by filtering the confidence measure as PR curve in
Figure 4. The measure can be also used as a con-
fidence estimator in active learning in CRFs (Kim
et al, 2006), where examples with the most uncer-
tainty are selected for presentation to human anno-
tators.
In future research, we plan to explore NE catego-
rization with more fine-grained label set. For NLP
applications such as QA, NE dictionary with fine-
grained label sets will be a useful resource. How-
ever, generally, classification with statistical meth-
ods becomes difficult in case that the label set is
large, because of the insufficient positive examples.
It is an issue to be resolved in the future.
656
References
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University.
Aron Culotta and Andrew McCallum. 2004. Confi-
dence estimation for information extraction. In Pro-
ceedings of Human Language Technology Conference
and North American Chapter of the Association for
Computational Linguistics (HLT-NAACL).
Lise Getoor, Eran Segal, Ben Taskar, and Daphne Koller.
2001. Probabilistic models of text and link structure
for hypertext classification. In IJCAI Workshop on
Text Learning: Beyond Supervision, 2001.
Nadia Ghamrawi and Andrew McCallum. 2005. Col-
lective multi-label classification. In Fourteenth Con-
ference on Information and Knowledge Management
(CIKM).
Juanzi Li Jie Tang, Mingcai Hong and Bangyong Liang.
2006. Tree-structured conditional random fields for
semantic annotation. In Proceedings of 5th Interna-
tional Conference of Semantic Web (ISWC-06).
Seokhwan Kim, Yu Song, Kyungduk Kim, Jeong-Won
Cha, and Gary Geunbae Lee. 2006. MMR-based ac-
tive machine learning for bio named entity recogni-
tion. In Proceedings of the Human Language Technol-
ogy Conference/North American chapter of the Asso-
ciation for Computational Linguistics annual meeting
(HLT-NAACL06).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282?289. Morgan Kaufmann,
San Francisco, CA.
Dong C. Liu and Jorge Nocedal. 1989. The limited mem-
ory BFGS methods for large scale optimization. In
Mathematical Programming 45.
Qing Lu and Lise Getoor. 2003a. Link-based classifica-
tion using labeled and unlabeled data. In Proceedings
of the International Conference On Machine Learning,
Washington DC, August.
Qing Lu and Lise Getoor. 2003b. Link-based text clas-
sification. In Proceedings of the International Joint
Conference on Artificial Intelligence.
Andrew McCallum, Khashayar Rohanimanesh, and
Charles Sutton. 2003. Dynamic conditional random
fields for jointly labeling multiple sequences. In NIPS
Workshop on Syntax, Semantics, and Statistics, De-
cember.
J. Neville and D. Jensen. 2000. Iterative classification
in relational data. In Proceedings of AAAI-2000 Work-
shop on Learning Statistical Models from Relational
Data, pages 13?20. AAAI Press.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
The 20th International Conference on Computational
Linguistics (COLING).
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of the LREC-2002.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence (AAAI-06).
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. In Proceedings of the 21th
International Conference on Machine Learning.
Charles Sutton. 2006. GRMM: A graphical models
toolkit. http://mallet.cs.umass.edu.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Proceedings of the 18th Conference on Uncertainty
in Artificial Intelligence. Morgan Kaufmann.
Vladimir Vapnik. 1998. Statistical Learning Theory.
Wiley Interscience.
Martin Wainwright, Tommi Jaakkola, and Alan Will-
sky. 2003. Tree-based reparameterization frame-
work for analysis of sum-product and related algo-
rithms. IEEE Transactions on Information Theory,
45(9):1120?1146.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-
Ying Ma. 2006. Simultaneous record detection and
attribute labeling in web data extraction. In Proceed-
ings of the 12th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining.
657
Combination of Machine Learning Methods
for Optimum Chinese Word Segmentation
Masayuki Asahara
Chooi-Ling Goh
Kenta Fukuoka
Yotaro Watanabe
Nara Institute of Science and Technology, Japan
E-mail: cje@is.naist.jp
Ai Azuma
Yuji Matsumoto
Takashi Tsuzuki
Matsushita Electric
Industrial Co., Ltd.
Abstract
This article presents our recent work for par-
ticipation in the Second International Chi-
nese Word Segmentation Bakeoff. Our
system performs two procedures: Out-of-
vocabulary extraction and word segmenta-
tion. We compose three out-of-vocabulary
extraction modules: Character-based tag-
ging with different classifiers ? maximum
entropy, support vector machines, and con-
ditional random fields. We also com-
pose three word segmentation modules ?
character-based tagging by maximum en-
tropy classifier, maximum entropy markov
model, and conditional random fields. All
modules are based on previously proposed
methods. We submitted three systems which
are different combination of the modules.
1 Overview
We compose three systems: Models a, b and c for the
closed test tracks on all four data sets.
For Models a and c, three out-of-vocabulary (OOV)
word extraction modules are composed: 1. Maximum
Entropy (MaxEnt) classifier-based tagging; 2. Max-
imum Entropy Markov Model (MEMM)-based word
segmenter with Conditional Random Fields (CRF)-
based chunking; 3. MEMM-based word segmenter
with Support Vector Machines (SVM)-based chunk-
ing. Two lists of OOV word candidates are constructed
either by voting or merging the three OOV word ex-
traction modules. Finally, a CRFs-based word seg-
menter produces the final results using either of the
voted list (Model a) or the merged list (Model c).
Most of the classifiers use surrounding words and
characters as the contextual features. Since word and
character features may cause data sparse problem, we
utilize a hard clustering algorithm (K-means) to define
word classes and character classes in order to over-
come the data sparse problem. The word classes are
used as the hidden states in MEMM and CRF-based
word segmenters. The character classes are used as the
features in character-based tagging, character-based
chunking and word segmentation.
Model b is our previous method proposed in (Goh
et al, 2004b): First, a MaxEnt classifier is used to per-
form character-based tagging to identify OOV words
in the test data. In-vocabulary (IV) word list together
with the extracted OOV word candidates is used in
Maximum Matching algorithm. Overlapping ambi-
guity is denoted by the different outputs from For-
ward and Backward Maximum Matching algorithm.
Finally, character-based tagging by MaxEnt classifier
resolves the ambiguity.
Section 2 describes Models a and c. Section 3 de-
scribes Model b. Section 4 discusses the differences
among the three models.
2 Models a and c
Models a and c use several modules. First, a hard
clustering algorithm is used to define word classes and
character classes. Second, three OOV extraction mod-
ules are trained with the training data. These modules,
then, extract the OOV words in the test data. Third,
the OOV word candidates produced by the three OOV
extraction modules are refined by voting (Model a) or
merging (Model c) them. The final word list is com-
posed by appending the OOV word candidates to the
IV word list. Finally, a CRF-based word segmenter
analyzes the sentence based on the new word list.
2.1 Clustering for word/character classes
We perform hard clustering for all words
and characters in the training data. K-
means algorithm is utilized. We use R 2.2.1
(http://www.r-project.org/) to perform
k-means clustering.
134
Since the word types are too large, we cannot run k-
means clustering on the whole data. Therefore, we di-
vide the word types into 4 groups randomly. K-means
clustering is performed for each group. Words in each
group are divided into 5 disjoint classes, producing 20
classes in total. Preceding and succeeding words in the
top 2000 rank are used as the features for the cluster-
ing. We define the set of the OOV words as the 21st
class. We also define two other classes for the begin-
of-sentence (BOS) and end-of-sentence (EOS). So, we
define 23 classes in total.
20 classes are defined for characters. K-means clus-
tering is performed for all characters in the training
data. Preceding and succeeding characters and BIES
position tags are used as features for the clustering:
?B? stands for ?the first character of a word?; ?I? stands
for ?an intermediate character of a word?; ?E? stands
for ?the last character of a word?; ?S? stands for ?the
single character word?. Characters only in the test data
are not assigned with any character class.
2.2 Three OOV extraction modules
In Models a and c, we use three OOV extraction mod-
ules.
First and second OOV extraction modules use
the output of a Maximam Entropy Markov Model
(MEMM)-based word segmenter (McCallum et al,
2000) (Uchimoto et al, 2001). Word list is composed
by the words appeared in 80% of the training data.
The words occured only in the remaining 20% of the
training data are regarded as OOV words. All word
candidates in a sentence are extracted to form a trel-
lis. Each word is assigned with a word class. The
word classes are used as the hidden states in the trellis.
In encoding, MaxEnt estimates state transition proba-
bilities based on the preceding word class (state) and
observed features such as the first character, last char-
acter, first character class, last character class of the
current word. In decoding, a simple Viterbi algorithm
is used.
The output of the MEMM-based word segmenter is
splitted character by character. Next, character-based
chunking is performed to extract OOV words. We use
two chunkers: based on SVM (Kudo and Matsumoto,
2001) and CRF (Lafferty et al, 2001). The chunker
annotates BIO position tags: ?B? stands for ?the first
character of an OOV word?; ?I? stands for ?other char-
acters in an OOV word?; ?O? stands for ?a character
outside an OOV word?.
The features used in the two chunkers are the char-
acters, the character classes and the information of
other characters in five-character window size. The
word sequence output by the MEMM-based word seg-
menter is converted into character sequence with BIES
position tags and the word classes. The position tags
with the word classes are also introduced as the fea-
tures.
The third one is a variation of the OOV module in
section 3 which is character-based tagging by MaxEnt
classifier. The difference is that we newly introduce
character classes in section 2.1 as the features.
In summary, we introduce three OOV word extrac-
tion modules: ?MEMM+SVM?, ?MEMM+CRF? and
?MaxEnt classifier?.
2.3 Voting/Merging the OOV words
The word list for the final word segmenter are com-
posed by voting or merging. Voting means the OOV
words which are extracted by two or more OOV word
extraction modules. Merging means the OOV words
which are extracted by any of the OOV word extrac-
tion modules. The model with the former (voting)
OOV word list is used in Model a, and the model with
the latter (merging) OOV word list is used in Model c.
2.4 CRF-based word segmenter
Final word segmentation is carried out by a CRF-based
word segmenter (Kudo and Matsumoto, 2004) (Peng
and McCallum, 2004). The word trellis is composed
by the similar method with MEMM-based word seg-
menter. Though state transition probabilities are esti-
mated in the case of MaxEnt framework, the proba-
bilities are normalized in the whole sentence in CRF-
based method. CRF-based word segmenter is robust to
length-bias problem (Kudo and Matsumoto, 2004) by
the global normalization. We will discuss the length-
bias problem in section 4.
2.5 Note on MSR data
Unfortunately, we could not complete Models a and
c for the MSR data due to time constraints. There-
fore, we submitted the following 2 fragmented mod-
els: Model a for MSR data is MEMM-based word
segmenter with OOV word list by voting; Model c for
MSR data is CRF-based word segmenter with no OOV
word candidate.
3 Model b
Model b uses a different approach. First, we extract the
OOV words using a MaxEnt classifier with only the
character as the features. We did not use the character
classes as the features. Each character is assigned with
BIES position tags. Word segmentation by character-
based tagging is firstly introduced by (Xue and Con-
verse, 2002). In encoding, we extract characters within
five-character window size for each character position
in the training data as the features for the classifier.
In decoding, the BIES position tag is deterministically
annotated character by character in the test data. The
135
words that appear only in the test data are treated as
OOV word candidates.
We can obtain quite high unknown word recall with
this model but the precision is a bit low. However,
the following segmentation model will try to elimi-
nate some false unknown words. In the next step, we
append OOV word candidates into the IV word list
extracted from the training data. The segmentation
model is similar to the OOV extraction method, except
that the features include the output from the Maximum
Matching (MaxMatch) algorithm. The algorithm runs
in both forward (FMaxMatch) and backward (BMax-
Match) directions using the final word list as the ref-
erences. The outputs of FMaxMatch and BMaxMatch
are also assigned with BIES tags. The differences be-
tween the FMaxMatch and BMaxMatch outputs indi-
cate the positions where the overlapping ambiguities
occur. The final word segmentation is carried out by
MaxEnt classifier again.
Note, both procedures in Model b use whole train-
ing data in the training phase. The dictionary used in
the MaxMatch algorithm is extracted from the training
data only during the training phase. So, the training of
segmentation model does not explicitly consider OOV
words. We did not use the word and character classes
as features in Model b unlike in the case of Models a
and c. The details of the model can be found in (Goh
et al, 2004b). The difference is that we do not pro-
vide character types here because it is forbidden in
this round. Besides, we also did not prune the OOV
words because this step involve the intervention of hu-
man knowledge.
4 Discussions and Conclusions
Table 1 summarizes the results of the three models.
The proposed systems employ purely corpus-based
statistical/machine learning method. Now, we discuss
what we observe in the three models. We remark two
problems in word segmentation: OOV word problem
and length-bias problem.
OOV word problem is that simple word-based
Markov Model family cannot analyze the words not
included in the word list. One of the solutions is
character-based tagging (Xue and Converse, 2002)
(Goh et al, 2004a). The simple character-based tag-
ging (Model b) achieved high ROOV but the precision
is low. We tried to refine OOV extraction by voting
and merging (Model a and c). However, the ROOV
of Models a and c are not as good as that of Model
b. Figure 1 shows type-precision and type-recall of
each OOV extraction modules. While voting helps to
make the precision higher, voting deteriorates the re-
call. Defining some hand written rules to prune false
OOV words will help to improve the IV word segmen-
tation (Goh et al, 2004b), because the precision of
OOV word extraction becomes higher. Other types of
OOV word extraction methods should be introduced.
For example, (Uchimoto et al, 2001) embeded OOV
models in MEMM-based word segmenter (with POS
tagging). Less than six-character substrings are ex-
tracted as the OOV word candidates in the word trel-
lis. (Peng and McCallum, 2004) proposed OOV word
extraction methods based on CRF-based word seg-
menter. Their CRF-based word segmenter can com-
pute a confidence in each segment. The high confi-
dent segments that are not in the IV word list are re-
garded as OOV word candidates. (Nakagawa, 2004)
proposed integration of word and OOV word position
tag in a trellis. These three OOV extraction method are
different from our methods ? character-based tagging.
Future work will include implementation of these dif-
ferent sorts of OOV word extraction modules.
Length bias problem means the tendency that the lo-
cally normalized Markov Model family prefers longer
words. Since choosing the longer words reduces the
number of words in a sentence, the state-transitions are
reduced. The less the state-transitions, the larger the
likelihood of the whole sentence. Actually, the length-
bias reflects the real distribution in the corpus. Still,
the length-bias problem is nonnegligible to achieve
high accuracy due to small exceptional cases. We used
CRF-based word segmenter which relaxes the prob-
lem (Kudo and Matsumoto, 2004). Actually, the CRF-
based word segmenter achieved high RIV .
We could not complete Model a and c for MSR.
After the deadline, we managed to complete Model
a (CRF + Voted Unk.) and c (CRF + Merged Unk.)
The result of Model a was precesion 0.976, recall
0.966, F-measure 0.971, OOV recall 0.570 and IV re-
call 0.988. The result of Model c was precesion 0.969,
recall 0.963, F-measure 0.966, OOV recall 0.571 and
IV recall 0.974. While the results are quite good, un-
fortunately, we could not submit the outputs in time.
While our results for the three data sets (AS,
CITYU, MSR) are fairly good, the result for the PKU
data is not as good. There is no correlation between
scores and OOV word rates. We investigate unseen
character distributions in the data set. There is no cor-
relation between scores and unseen character distribu-
tions.
We expected Model c (merging) to achieve higher
recall for OOV words than Model a (voting). How-
ever, the result was opposite. The noises in OOV
word candidates should have deteriorated the F-value
of overall word segmentation. One reason might be
that our CRF-based segmenter could not encode the
occurence of OOV words. We defined the 21st word
class for OOV words. However, the training data for
CRF-based segmenter did not contain the 21st class.
We should include the 21st class in the training data
136
Table 1: Our Three Models and Results: F-value/ROOV /RIV (Rank of F-value)
AS CITYU MSR PKU
Model a CRF + Voted Unk. CRF + Voted Unk. MEMM + Voted Unk. CRF + Voted Unk.
0.947/0.606/0.971 0.942/0.629/0.967 0.949/0.378/0.971 0.934/0.521/0.955
(2/11) (2/15) (16/29) (10/23)
Model b Char.-based tagging Char.-based tagging Char.-based tagging Char.-based tagging
0.952/0.696/0.963 0.941/0.736/0.953 0.958/0.718/0.958 0.941/0.760/0.941
(1/11) (3/15) (6/29) (7/23)
Model c CRF + Merged Unk. CRF + Merged Unk. CRF + No Unk. CRF + Merged Unk.
0.939/0.445/0.967 0.928/0.598/0.940 0.943/0.025/0.990 0.917/0.325/0.940
(7/11) (8/15) (21/29) (14/23)
150/764
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1727/2504=0.689Voted Recall = 1727/3226=0.535Merged Precision = 2532/6003=0.421Merged Recall = 2532/3226=0.784
69/599 586/2136
165/480 420/579
184/304
958/1141
AS
51/406
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1068/1714=0.623Voted Recall = 1068/1670=0.639Merged Precision = 1367/3531=0.387Merged Recall = 1367/1670=0.818
42/352 206/1059
87/439 114/188
109/196
758/891
CITYU
correctly extracted types(left side)
extracted types(right side)
57/555
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1196/1659=0.720Voted Recall = 1196/1991=0.600Merged Precision = 1628/4454=0.365Merged Recall = 1628/1991=0.817
40/330 335/1910
93/293 149/243
245/333
709/790
MSR
67/882
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1528/2827=0.540Voted Recall = 1528/2863=0.533Merged Precision = 2184/7064=0.309Merged Recall = 2184/2863=0.762
87/720 502/2635
181/727 217/424
201/407
929/1269
PKU
Figure 1: OOV Extraction Precision and Recall by Type
by regarding some words as pseudo OOV words.
We also found a bug in the CRF-based OOV word
extration module. The accuracy of the module might
be slightly better than the reported results. However,
the effect of the bug on overall F-value might be lim-
ited, since the module was only part of the OOV ex-
traction module combination ? voting and merging.
Acknowledgement
We would like to express our appreciation to Dr. Taku
Kudo who developed SVM-based chunker and gave us
several fruitful comments.
References
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2004a. Chinese Word Segmentation by
Classification of Characters. In Proc. of Third
SIGHAN Workshop, pages 57?64.
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2004b. Pruning False Unknown Words to
Improve Chinese Word Segmentation. In Proc. of
PACLIC-18, pages 139?149.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with Support Vector Machines. In Proc. of NAACL-
2001, pages 192?199.
Taku Kudo and Yuji Matsumoto. 2004. Applying
Conditional Random Fields to Japanese Morpho-
logical Analysis. In Proc. of EMNLP-2004, pages
230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proc. of ICML-2001, pages 282?
289.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Mod-
els for Information Extraction and Segmentation. In
Proc. of ICML-2000, pages 591?598.
Tetsuji Nakagawa. 2004. Chinese and Japanese Word
Segmentation Using Word-Level and Character-
Level Information. In Proc. of COLING-2004,
pages 466?472.
Fuchun Peng and Andrew McCallum. 2004. Chinese
Segmentation and New Word Detection using Con-
ditional Random Fields. In Proc. of COLING-2004,
pages 562?568.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The Unknown Word Problem: a
Morphological Analysis of Japanese Using Maxi-
mum Entropy Aided by a Dictionary. In Proc. of
EMNLP-2001, pages 91?99.
Nianwen Xue and Susan P. Converse. 2002. Combin-
ing Classifiers for Chinese Word Segmentation. In
Proc. of First SIGHAN Workshop, pages 63?70.
137
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 114?119,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage
Approximate Max-Margin Linear Models
Yotaro Watanabe, Masayuki Asahara and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, Japan, 630-0192
{yotaro-w, masayu-a, matsu}@is.naist.jp
Abstract
This paper describes a system for syntactic-
semantic dependency parsing for multiple lan-
guages. The system consists of three parts: a
state-of-the-art higher-order projective depen-
dency parser for syntactic dependency pars-
ing, a predicate classifier, and an argument
classifier for semantic dependency parsing.
For semantic dependency parsing, we ex-
plore use of global features. All components
are trained with an approximate max-margin
learning algorithm.
In the closed challenge of the CoNLL-2009
Shared Task (Hajic? et al, 2009), our system
achieved the 3rd best performances for En-
glish and Czech, and the 4th best performance
for Japanese.
1 Introduction
In recent years, joint inference of syntactic and se-
mantic dependencies has attracted attention in NLP
communities. Ideally, we would like to choose the
most plausible syntactic-semantic structure among
all possible structures in that syntactic dependencies
and semantic dependencies are correlated. How-
ever, solving this problem is too difficult because
the search space of the problem is extremely large.
Therefore we focus on improving performance for
each subproblem: dependency parsing and semantic
role labeling.
In the past few years, research investigating
higher-order dependency parsing algorithms has
found its superiority to first-order parsing algo-
rithms. To reap the benefits of these advances, we
use a higher-order projective dependency parsing al-
gorithm (Carreras, 2007) which is an extension of
the span-based parsing algorithm (Eisner, 1996), for
syntactic dependency parsing.
In terms of semantic role labeling, we would
like to capture global information about predicate-
argument structures in order to accurately predict the
correct predicate-argument structure. Previous re-
search dealt with such information using re-ranking
(Toutanova et al, 2005; Johansson and Nugues,
2008). We explore a different approach to deal
with such information using global features. Use
of global features for structured prediction problem
has been explored by several NLP applications such
as sequential labeling (Finkel et al, 2005; Krishnan
and Manning, 2006; Kazama and Torisawa, 2007)
and dependency parsing (Nakagawa, 2007) with a
great deal of success. We attempt to use global fea-
tures for argument classification in which the most
plausible semantic role assignment is selected using
both local and global information. We present an
approximate max-margin learning algorithm for ar-
gument classifiers with global features.
2 Dependency Parsing
As in previous work, we use a linear model for de-
pendency parsing. The score function used in our
dependency parser is defined as follows.
s(y) = ?
(h,m)?y
F (h,m,x) (1)
where h and m denote the head and the dependent
of the dependency edge in y, and F (h,m,x) is a
Factor that specifies dependency edge scores.
114
We used a second-order factorization as in (Car-
reras, 2007). The second-order factor F is defined
as follows.
F (h,m,x) = w ??(h,m,x)+w ??(h,m, ch,x)
+w ? ?(h,m, cmi,x) +w ? ?(h,m, cmo,x) (2)
where w is a parameter vector, ? is a feature vector,
ch is the child of h in the span [h...m] that is closest
to m, cmi is the child of m in the span [h...m] that is
farthest fromm and cmo is the child of m outside the
span [h...m] that is farthest fromm. For more details
of the second-order parsing algorithm, see (Carreras,
2007).
For parser training, we use the Passive Aggres-
sive Algorithm (Crammer et al, 2006), which is an
approximate max-margin variant of the perceptron
algorithm. Also, we apply an efficient parameter av-
eraging technique (Daume? III, 2006). The resulting
learning algorithm is shown in Algorithm 1.
Algorithm 1 A Passive Aggressive Algorithm with
parameter averaging
input Training set T = {xt,yt}Tt=1, Number of iterations
N and Parameter C
w ? 0, v ? 0, c ? 1
for i ? 0 to N do
for (xt,yt) ? T do
y? = argmaxy w ? ?(xt,y) + ?(yt, y?)
?t = min
?
C, w??(xt,y?)?w??(xt,yt)+?(yt,y?)||?(xt,yt)??(xt,y?)||2
?
w ? w + ?t(?(xt,yt)? ?(xt, y?))
v ? v + c?t(?(xt,yt)? ?(xt, y?))
c ? c + 1
end for
end for
return w ? v/c
We set ?(yt, y?) as the number of incorrect head
predictions in the y?, and C as 1.0.
Among the 7 languages of the task, 4 languages
(Czech, English, German and Japanese) contain
non-projective edges (13.94 %, 3.74 %, 25.79 %
and 0.91 % respectively), therefore we need to deal
with non-projectivity. In order to avoid losing the
benefits of higher-order parsing, we considered ap-
plying pseudo-projective transformation (Nivre and
Nilsson, 2005). However, growth of the number of
dependency labels by pseudo-projective transforma-
tion increases the dependency parser training time,
so we did not adopt transformations. Therefore, the
parser ignores the presence of non-projective edges
in the training and the testing phases.
The features used for our dependency parser are
based on those listed in (Johansson, 2008). In addi-
tion, distance features are used. We use shorthand
notations in order to simplify the feature represen-
tations: ?h?, ?d?, ?c?, ?l?, ?p?, ??1? and ?+1? cor-
respond to head, dependent, head?s or dependent?s
child, lemma , POS, left position and right position
respectively.
First-order Features
Token features: hl, hp, hl+hp, dl, dp and dl+dp.
Head-Dependent features: hp+dp, hl+dl, hl+dl,
hl+hp+dl, hl+hp+dp, hl+dl+dp, hp+dl+dp and
hl+hp+dl+dp.
Context features: hp+hp+1+dp?1+dp,
hp?1+hp+dp?1+dp, hp+hp+1+dp+dp+1 and
hp?1+hp+dp+dp+1.
Distance features: The number of tokens between the
head and the dependent.
Second-order Features
Head-Dependent-Head?s or Dependent?s Child:
hl+cl, hl+cl+cp, hp+cl, hp+cp, hp+dp+cp, dp+cp,
dp+cl+cp, dl+cp, dl+cp+cl
3 Semantic Role Labeling
Our SRL module consists of two parts: a predicate
classifier and an argument classifier. First, our sys-
tem determines the word sense for each predicate
with the predicate classifier, and then it detects the
highest scored argument assignment using the argu-
ment classifier with global features.
3.1 Predicate Classification
The first phase of SRL in our system is to detect
the word sense for each predicate. WSD can be for-
malized as a multi-class classification problem given
lemmas. We created a linear model for each lemma
and used the Passive Aggressive Algorithm with pa-
rameter averaging to train the models.
3.1.1 Features for Predicate Classification
Word features: Predicted lemma and the predicted POS
of the predicate, predicate?s head, and its conjunc-
tions.
Dependency label: The dependency label between the
predicate and the predicate?s head.
115
Dependency label sequence: The concatenation of the
dependency labels of the predicate dependents.
Since effective features for predicate classifica-
tion are different for each language, we performed
greedy forward feature selection.
3.2 Argument Classification
In order to capture global clues of predicate-
argument structures, we consider introducing global
features for linear models. Let A(p) be a joint
assignment of role labels for argument candidates
given the predicate p. Then we define a score func-
tion s(A(p)) for argument label assignments A(p).
s(A(p)) =?
k
Fk(x,A(p)) (3)
We introduce two factors: Local Factor FL and
Global Factor FG defined as follows.
FL(x, a(p)) = w ? ?L(x, a(p)) (4)
FG(x,A(p)) = w ? ?G(x,A(p)) (5)
where ?L, ?G denote feature vectors for the local
factor and the global factor respectively. FL scores a
particular role assignment for each argument candi-
date individually, and FG treats global features that
capture what structure the assignment A has. Re-
sulting scoring function for the assignment A(p) is
as follows.
s(A(p)) = ?
a(p)?A(p)
w??L(x, a(p))+w??G(x,A(p))
(6)
Use of global features is problematic, because it
becomes difficult to find the highest assignment ef-
ficiently. In order to deal with the problem, we use
a simple approach, n-best relaxation as in (Kazama
and Torisawa, 2007). At first we generate n-best as-
signments using only the local factor, and then add
the global factor score for each n-best assignment, fi-
nally select the best scoring assignment from them.
In order to generate n-best assignments, we used a
beam-search algorithm.
3.2.1 Learning the Model
As in dependency parser and predicate classifier,
we train the model using the PA algorithm with pa-
rameter averaging. The learning algorithm is shown
in Algorithm 2. In this algorithm, the weights cor-
respond to local factor features ?L and global factor
features ?G are updated simultaneously.
Algorithm 2 Learning with Global Features for Ar-
gument Classification
input Training set T = {xt,At}Tt=1, Number of iterations
N and Parameter C
w ? 0, v ? 0, c ? 1
for i ? 0 to N do
for (xt,At) ? T do
let ?(xt,A) = Pa?A ?L(xt, a) + ?G(xt,A)generate n-best assignments {An} using FL
A? = argmaxA?{An} w ? ?(xt,A) + ?(At,A)
?t = min
?
C, w??(xt,A?)?w??(xt,At)+?(At,A?)||?(xt,At)??(xt,A?)||2
?
w ? w + ?t(?(xt,At)? ?(xt, A?))
v ? v + c?t(?(xt,At)? ?(xt, A?))
c ? c + 1
end for
end for
return w ? v/c
We set the margin value ?(A, A?) as the number
of incorrect assignments plus ?(A, A?), and C as 1.0.
The delta function returns 1 if at least one assign-
ment is different from the correct assignment and 0
otherwise.
The model is similar to re-ranking (Toutanova et
al., 2005; Johansson and Nugues, 2008). However
in contrast to re-ranking, we only have to prepare
one model. The re-ranking approach requires other
training datasets that are different from the data used
in local model training.
3.2.2 Features for Argument Classification
The local features used in our system are the same
as our previous work (Watanabe et al, 2008) except
for language dependent features. The global features
that used in our system are based on (Johansson and
Nugues, 2008) that used for re-ranking.
Local Features
Word features: Predicted lemma and predicted POS of
the predicate, predicate?s head, argument candidate,
argument candidate?s head, leftmost/rightmost de-
pendent and leftmost/rightmost sibling.
Dependency label: The dependency label of predicate,
argument candidate and argument candidate?s de-
pendent.
Family: The position of the argument candicate with re-
spect to the predicate position in the dependency
tree (e.g. child, sibling).
116
Average Catalan Chinese Czech English German Japanese Spanish
Macro F1 Score 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
(78.00*) (74.83*) (73.43*) (81.38*) (86.40*) (68.39*) (84.84*) (76.74*)
Semantic Labeled F1 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
(75.17*) (71.05*) (74.17*) (84.66*) (84.26*) (61.94*) (77.91*) (72.25*)
Labeled Syntactic Accuracy 81.16 79.48 72.66 78.17 88.54 75.85 91.69 81.74
(80.77*) (78.62*) (72.66*) (78.10*) (88.54*) (74.60*) (91.66*) (81.23*)
Macro F1 Score 84.30 84.79 81.63 83.08 87.93 83.25 85.54 83.94
Semantic Labeled F1 81.58 80.99 79.99 86.67 85.09 79.46 79.03 79.85
Labeled Syntactic Accuracy 87.02 88.59 83.27 79.48 90.77 87.03 91.96 88.04
Table 1: Scores of our system.
Position: The position of the head of the dependency re-
lation with respect to the predicate position in the
sentence.
Pattern: The left-to-right chain of the predicted
POS/dependency labels of the predicate?s children.
Path features: Predicted lemma, predicted POS and de-
pendency label paths between the predicate and the
argument candidate.
Distance: The number of dependency edges between the
predicate and the argument candidate.
Global Features
Predicate-argument label sequence: The sequence of
the predicate sense and argument labels in the
predicate-argument strucuture.
Presence of labels defined in frame files: Whether the
semantic roles defined in the frame present in the
predicate-argument structure (e.g. MISSING:A1 or
CONTAINS:A1.)
3.2.3 Argument Pruning
We observe that most arguments tend to be not far
from its predicate, so we can prune argument candi-
dates to reduce search space. Since the characteris-
tics of the languages are slightly different, we apply
two types of pruning algorithms.
Pruning Algorithm 1: Let S be an argument candi-
date set. Initially set S ? ? and start at predicate node.
Add dependents of the node to S, and move current node
to its parent. Repeat until current node reaches to ROOT.
Pruning Algorithm 2: Same as the Algorithm 1 ex-
cept that added nodes are its grandchildren as well as its
dependents.
The pruning results are shown in Table 2. Since
we could not prune arguments in Japanese accu-
rately using the two algorithms, we pruned argument
candidates simply by POS.
algorithm coverage (%) reduction (%)
Catalan 1 100 69.1
Chinese 1 98.9 69.1
Czech 2 98.5 49.1
English 1 97.3 63.1
German 1 98.3 64.3
Japanese POS 99.9 41.0
Spanish 1 100 69.7
Table 2: Pruning results.
4 Results
The submitted results on the test data are shown in
the upper part of Table 1. Due to a bug, we mistak-
enly used the gold lemmas in the dependency parser.
Corrected results are shown in the part marked with
*. The lower part shows the post evaluation results
with the gold lemmas and POSs.
For some of the 7 languages, since the global
model described in Section 3.2 degraded perfor-
mance compare to a model trained with only FL,
we did NOT use the model for all languages. We
used the global model for only three languages: Chi-
nese, English and Japanese. The remaining lan-
guages (Catalan, Czech, German and Spanish) used
a model trained with only FL.
4.1 Dependency Parsing Results
The parser achieved relatively high accuracies for
Czech, English and Japanese, and for each language,
the difference between the performance with correct
POS and predicted POS is not so large. However, in
Catalan, Chinese German and Spanish, the parsing
accuracies was seriously degraded by replacing cor-
rect POSs with predicted POSs (6.3 - 11.2 %). This
is likely because these languages have relatively low
predicted POS accuracies (92.3 - 95.5 %) ; Chinese
117
FL FL+FG (?P, ?R)
Catalan 85.80 85.68 (+0.01, -0.26)
Chinese 86.58 87.39 (+0.24, +1.36)
Czech 89.63 89.05 (-0.87, -0.28)
English 85.66 85.74 (-0.87, +0.98)
German 80.82 77.30 (-7.27, +0.40)
Japanese 79.87 81.01 (+0.17, +1.88)
Spanish 84.38 83.89 (-0.42, -0.57)
Table 3: Effect of global features (semantic labeled F1).
?P and ?R denote the differentials of labeled precision
and labeled recall between FL and FL+FG respectively.
has especially low accuracy (92.3%). The POS ac-
curacy may affect the parsing performances.
4.2 SRL Results
In order to highlight the effect of the global fea-
tures, we compared two models. The first model
is trained with only the local factor FL. The sec-
ond model is trained with both the local factor FL
and the global factor FG. The results are shown in
Table 3. In the experiments, we used the develop-
ment data with gold parse trees. For Chinese and
Japanese, significant improvements are obtained us-
ing the global features (over +1.0% in labeled re-
call and the slightly better labeled precision). How-
ever, for Catalan, Czech, German and Spanish, the
global features degraded the performance in labeled
F1. Especially, in German, the precision is substan-
tially degraded (-7.27% in labeled F1). These results
indicate that it is necessary to introduce language de-
pendent features.
4.3 Training, Evaluation Time and Memory
Requirements
Table 4 and 5 shows the training/evaluation times
and the memory consumption of the second-order
dependency parsers and the global argument classi-
fiers respectively. The training times of the predi-
cate classifier were less than one day, and the testing
times were mere seconds.
As reported in (Carreras, 2007; Johansson and
Nugues, 2008), training and inference of the second-
order parser are very expensive. For Chinese, we
could only complete 2 iterations.
In terms of the argument classifier, since N-best
generation time account for a substantial proportion
of the training time (in this work N = 100), chang-
iter hrs./iter sent./min. mem.
Catalan 9 14.6 9.0 9.6 GB
Chinese 2 56.5 3.7 16.2 GB
Czech 8 14.6 20.5 12.6 GB
English 7 22.0 13.4 15.1 GB
German 4 12.3 59.1 13.1 GB
Japanese 7 11.2 21.8 13.0 GB
Spanish 7 19.5 7.3 17.9 GB
Table 4: Training, evaluation time and memory require-
ments of the second-order dependency parsers. The ?iter?
column denote the number of iterations of the model
used for the evaluations. Catalan, Czech and English
are trained on Xeon 3.0GHz, Chinese and Japanese are
trained on Xeon 2.66GHz, German and Spanish are
trained on Opteron 2.3GHz machines.
train (hrs.) sent./min. mem.
Chinese 6.5 453.7 2.0 GB
English 13.5 449.8 3.2 GB
Japanese 3.5 137.6 1.1 GB
Table 5: Training, evaluation time and memory require-
ments of the global argument classifiers. The classifiers
are all trained on Opteron 2.3GHz machines.
ing N affects the training and evaluation times sig-
nificantly.
All modules of our system are implemented in
Java. The required memory spaces shown in Table
4 and 5 are calculated by subtracting free memory
size from the total memory size of the Java VM.
Note that we observed that the value fluctuated dras-
tically while measuring memory usage, so the value
may not indicate precise memory requirements of
our system.
5 Conclusion
In this paper, we have described our system for syn-
tactic and semantic dependency analysis in multilin-
gual. Although our system is not a joint approach
but a pipeline approach, the system is comparable to
the top system for some of the 7 languages.
A further research direction we are investigating
is the application of various types of global features.
We believe that there is still room for improvements
since we used only two types of global features for
the argument classifier.
Another research direction is investigating joint
approaches. To the best of our knowledge, three
118
types of joint approaches have been proposed:
N-best based approach (Johansson and Nugues,
2008), synchronous joint approach (Henderson et
al., 2008), and a joint approach where parsing
and SRL are performed simultaneously (Llu??s and
Ma`rquez, 2008). We attempted to perform N-
best based joint approach, however, the expen-
sive computational cost of the 2nd-order projective
parser discouraged it. We would like to investigate
syntactic-semantic joint approaches with reasonable
time complexities.
Acknowledgments
We would like to thank Richard Johansson for his
advice on parser implementation, and the CoNLL-
2009 organizers (Hajic? et al, 2009; Taule? et al,
2008; Palmer and Xue, 2009; Hajic? et al, 2006; Sur-
deanu et al, 2008; Burchardt et al, 2006; Kawahara
et al, 2002; Taule? et al, 2008).
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proc. of LREC-2006, Genoa,
Italy.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of EMNLP-
CoNLL 2007.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7:551?585.
Hal Daume? III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Ange-
les, CA, August.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing. In Proc. of ICCL 1996.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proc. of ACL 2005.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proc. of CoNLL-2009,
Boulder, Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic dependen-
cies. In Proc. of CoNLL 2008.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In Proc. of CoNLL
2008.
Richard Johansson. 2008. Dependency-based Semantic
Analysis of Natural-language Text. Ph.D. thesis, Lund
University.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proc. of LREC-2002, pages 2008?2013,
Las Palmas, Canary Islands.
Jun?Ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proc. of EMNLP-CoNLL 2007.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In Proc. of
ACL-COLING 2006.
Xavier Llu??s and Llu??s Ma`rquez. 2008. A joint model for
parsing syntactic and semantic dependencies. In Proc.
of CoNLL 2008.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. of ACL 2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proc. of CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proc. of LREC-2008, Mar-
rakesh, Morroco.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proc. of ACL 2005.
Yotaro Watanabe, Masakazu Iwatate, Masayuki Asahara,
and Yuji Matsumoto. 2008. A pipeline approach for
syntactic and semantic dependency parsing. In Proc.
of CoNLL 2008.
119
Proceedings of the ACL 2010 Conference Short Papers, pages 98?102,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Structured Model for Joint Learning of
Argument Roles and Predicate Senses
Yotaro Watanabe
Graduate School of Information Sciences
Tohoku University
6-6-05, Aramaki Aza Aoba, Aoba-ku,
Sendai 980-8579, Japan
yotaro-w@ecei.tohoku.ac.jp
Masayuki Asahara Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma,
Nara, 630-0192, Japan
{masayu-a, matsu}@is.naist.jp
Abstract
In predicate-argument structure analysis,
it is important to capture non-local de-
pendencies among arguments and inter-
dependencies between the sense of a pred-
icate and the semantic roles of its argu-
ments. However, no existing approach ex-
plicitly handles both non-local dependen-
cies and semantic dependencies between
predicates and arguments. In this pa-
per we propose a structured model that
overcomes the limitation of existing ap-
proaches; the model captures both types of
dependencies simultaneously by introduc-
ing four types of factors including a global
factor type capturing non-local dependen-
cies among arguments and a pairwise fac-
tor type capturing local dependencies be-
tween a predicate and an argument. In
experiments the proposed model achieved
competitive results compared to the state-
of-the-art systems without applying any
feature selection procedure.
1 Introduction
Predicate-argument structure analysis is a process
of assigning who does what to whom, where,
when, etc. for each predicate. Arguments of a
predicate are assigned particular semantic roles,
such as Agent, Theme, Patient, etc. Lately,
predicate-argument structure analysis has been re-
garded as a task of assigning semantic roles of
arguments as well as word senses of a predicate
(Surdeanu et al, 2008; Hajic? et al, 2009).
Several researchers have paid much attention to
predicate-argument structure analysis, and the fol-
lowing two important factors have been shown.
Toutanova et al (2008), Johansson and Nugues
(2008), and Bjo?rkelund et al (2009) presented
importance of capturing non-local dependencies
of core arguments in predicate-argument structure
analysis. They used argument sequences tied with
a predicate sense (e.g. AGENT-buy.01/Active-
PATIENT) as a feature for the re-ranker of the
system where predicate sense and argument role
candidates are generated by their pipelined archi-
tecture. They reported that incorporating this type
of features provides substantial gain of the system
performance.
The other factor is inter-dependencies between
a predicate sense and argument roles, which re-
late to selectional preference, and motivated us
to jointly identify a predicate sense and its argu-
ment roles. This type of dependencies has been
explored by Riedel and Meza-Ruiz (2008; 2009b;
2009a), all of which use Markov Logic Networks
(MLN). The work uses the global formulae that
have atoms in terms of both a predicate sense and
each of its argument roles, and the system identi-
fies predicate senses and argument roles simulta-
neously.
Ideally, we want to capture both types of depen-
dencies simultaneously. The former approaches
can not explicitly include features that capture
inter-dependencies between a predicate sense and
its argument roles. Though these are implicitly in-
corporated by re-ranking where the most plausi-
ble assignment is selected from a small subset of
predicate and argument candidates, which are gen-
erated independently. On the other hand, it is dif-
ficult to deal with core argument features in MLN.
Because the number of core arguments varies with
the role assignments, this type of features cannot
be expressed by a single formula.
Thompson et al (2010) proposed a gener-
ative model that captures both predicate senses
and its argument roles. However, the first-order
markov assumption of the model eliminates abil-
ity to capture non-local dependencies among ar-
guments. Also, generative models are in general
inferior to discriminatively trained linear or log-
98
!!!
"#!
"$%!
"%!
"$!
&'!&(! &)!&*!&+!
,!
Figure 1: Undirected graphical model representa-
tion of the structured model
linear models.
In this paper we propose a structured model
that overcomes limitations of the previous ap-
proaches. For the model, we introduce several
types of features including those that capture both
non-local dependencies of core arguments, and
inter-dependencies between a predicate sense and
its argument roles. By doing this, both tasks are
mutually influenced, and the model determines
the most plausible set of assignments of a predi-
cate sense and its argument roles simultaneously.
We present an exact inference algorithm for the
model, and a large-margin learning algorithm that
can handle both local and global features.
2 Model
Figure 1 shows the graphical representation of our
proposed model. The node p corresponds to a
predicate, and the nodes a1, ..., aN to arguments
of the predicate. Each node is assigned a particu-
lar predicate sense or an argument role label. The
black squares are factors which provide scores of
label assignments. In the model, the nodes for ar-
guments depend on the predicate sense, and by in-
fluencing labels of a predicate sense and its argu-
ment roles, the most plausible label assignment of
the nodes is determined considering all factors.
In this work, we use linear models. Let x be
words in a sentence, p be a sense of a predicate in
x, and A = {an}N1 be a set of possible role label
assignments for x. A predicate-argument structure
is represented by a pair of p and A. We define
the score function for predicate-argument struc-
tures as s(p,A) =
?
Fk?F Fk(x, p,A). F is a
set of all the factors, Fk(x, p,A) corresponds to a
particular factor in Figure 1, and gives a score to a
predicate or argument label assignments. Since we
use linear models, Fk(x, p,A) = w ??k(x, p,A).
2.1 Factors of the Model
We define four types of factors for the model.
Predicate Factor FP scores a sense of p, and
does not depend on any arguments. The score
function is defined byFP (x, p,A) = w??P (x, p).
Argument Factor FA scores a label assignment
of a particular argument a ? A. The score is deter-
mined independently from a predicate sense, and
is given by FA(x, p, a) = w ? ?A(x, a).
Predicate-Argument Pairwise Factor
FPA captures inter-dependencies between
a predicate sense and one of its argument
roles. The score function is defined as
FPA(x, p, a) = w ? ?PA(x, p, a). The dif-
ference from FA is that FPA influences both
the predicate sense and the argument role. By
introducing this factor, the role label can be
influenced by the predicate sense, and vise versa.
Global Factor FG is introduced to capture plau-
sibility of the whole predicate-argument structure.
Like the other factors, the score function is de-
fined as FG(x, p,A) = w ? ?G(x, p,A). A pos-
sible feature that can be considered by this fac-
tor is the mutual dependencies among core argu-
ments. For instance, if a predicate-argument struc-
ture has an agent (A0) followed by the predicate
and a patient (A1), we encode the structure as a
string A0-PRED-A1 and use it as a feature. This
type of features provide plausibility of predicate-
argument structures. Even if the highest scoring
predicate-argument structure with the other factors
misses some core arguments, the global feature
demands the model to fill the missing arguments.
The numbers of factors for each factor type are:
FP and FG are 1, FA and FPA are |A|. By inte-
grating the all factors, the score function becomes
s(p,A) = w ? ?P (x, p) +w ? ?G(x, p,A) +w ?
?
a?A{?A(x, a) + ?PA(x, p, a)}.
2.2 Inference
The crucial point of the model is how to deal
with the global factor FG, because enumerating
possible assignments is too costly. A number of
methods have been proposed for the use of global
features for linear models such as (Daume? III
and Marcu, 2005; Kazama and Torisawa, 2007).
In this work, we use the approach proposed in
(Kazama and Torisawa, 2007). Although the ap-
proach is proposed for sequence labeling tasks, it
99
can be easily extended to our structured model.
That is, for each possible predicate sense p of the
predicate, we provide N-best argument role as-
signments using three local factors FP , FA and
FPA, and then add scores of the global factor FG,
finally select the argmax from them. In this case,
the argmax is selected from |Pl|N candidates.
2.3 Learning the Model
For learning of the model, we borrow a funda-
mental idea of Kazama and Torisawa?s perceptron
learning algorithm. However, we use a more so-
phisticated online-learning algorithm based on the
Passive-Aggressive Algorithm (PA) (Crammer et
al., 2006).
For the sake of simplicity, we introduce some
notations. We denote a predicate-argument struc-
ture y = ?p,A?, a local feature vector as
?L(x,y) = ?P (x, p) +
?
a?A{?A(x, a) +
?PA(x, p, a)}?a feature vector coupling both
local and global features as ?L+G(x,y) =
?L(x,y) + ?G(x, p,A), the argmax using ?L+G
as y?L+G, the argmax using ?L as y?L. Also, we
use a loss function ?(y,y?), which is a cost func-
tion associated with y and y?.
The margin perceptron learning proposed by
Kazama and Torisawa can be seen as an optimiza-
tion with the following two constrains.
(A) w??L+G(x,y)?w??L+G(x, y?L+G) ? ?(y, y?L+G)
(B) w ? ?L(x,y) ?w ? ?L(x, y?L) ? ?(y, y?L)
(A) is the constraint that ensures a sufficient
margin ?(y, y?L+G) between y and y?L+G. (B)
is the constraint that ensures a sufficient margin
?(y, y?L) between y and y?L. The necessity of
this constraint is that if we apply only (A), the al-
gorithm does not guarantee a sufficient margin in
terms of local features, and it leads to poor quality
in the N-best assignments. The Kazama and Tori-
sawa?s perceptron algorithm uses constant values
for the cost function ?(y, y?L+G) and ?(y, y?L).
The proposed model is trained using the follow-
ing optimization problem.
wnew = arg min
w??<n
1
2 ||w
? ?w||2 + C?
(
s.t. lL+G ? ?, ? ? 0 if y?L+G 6= y
s.t. lL ? ?, ? ? 0 if y?L+G = y 6= y?L
(1)
lL+G = w ? ?L+G(x, y?L+G)
?w ? ?L+G(x,y) + ?(y, y?L+G) (2)
lL = w ? ?L(x, y?L) ?w ? ?L(x,y) + ?(y, y?L) (3)
lL+G is the loss function for the case of using
both local and global features, corresponding to
the constraint (A), and lL is the loss function for
the case of using only local features, correspond-
ing to the constraints (B) provided that (A) is sat-
isfied.
2.4 The Role-less Argument Bias Problem
The fact that an argument candidate is not as-
signed any role (namely it is assigned the la-
bel ?NONE?) is unlikely to contribute pred-
icate sense disambiguation. However, it re-
mains possible that ?NONE? arguments is bi-
ased toward a particular predicate sense by FPA
(i.e. w ? ?PA(x, sensei, ak= ?NONE??) > w ?
?PA(x, sensej , ak= ?NONE??).
In order to avoid this bias, we define a spe-
cial sense label, senseany, that is used to cal-
culate the score for a predicate and a roll-less
argument, regardless of the predicate?s sense.
We use the feature vector ?PA(x, senseany, ak)
if ak= ?NONE?? and ?PA(x, sensei, ak) other-
wise.
3 Experiment
3.1 Experimental Settings
We use the CoNLL-2009 Shared Task dataset
(Hajic? et al, 2009) for experiments. It is a
dataset for multi-lingual syntactic and semantic
dependency parsing 1. In the SRL-only challenge
of the task, participants are required to identify
predicate-argument structures of only the specified
predicates. Therefore the problems to be solved
are predicate sense disambiguation and argument
role labeling. We use Semantic Labeled F1 for
evaluation.
For generating N-bests, we used the beam-
search algorithm, and the number of N-bests was
set to N = 64. For learning of the joint model, the
loss function ?(yt,y?) of the Passive-Aggressive
Algorithm was set to the number of incorrect as-
signments of a predicate sense and its argument
roles. Also, the number of iterations of the model
used for testing was selected based on the perfor-
mance on the development data.
Table 1 shows the features used for the struc-
tured model. The global features used for FG are
based on those used in (Toutanova et al, 2008;
Johansson and Nugues, 2008), and the features
1The dataset consists of seven languages: Catalan, Chi-
nese, Czech, English, German, Japanese and Spanish.
100
FP Plemma of the predicate and predicate?s head, and ppos of the predicate
Dependency label between the predicate and predicate?s head
The concatenation of the dependency labels of the predicate?s dependents
FA Plemma and ppos of the predicate, the predicate?s head, the argument candidate, and the argument?s head
Plemma and ppos of the leftmost/rightmost dependent and leftmost/rightmost sibling
The dependency label of predicate, argument candidate and argument candidate?s dependent
The position of the argument candidate with respect to the predicate position in the dep. tree (e.g. CHILD)
The position of the head of the dependency relation with respect to the predicate position in the sentence
The left-to-right chain of the deplabels of the predicate?s dependents
Plemma, ppos and dependency label paths between the predicate and the argument candidates
The number of dependency edges between the predicate and the argument candidate
FPA Plemma and plemma&ppos of the argument candidate
Dependency label path between the predicate and the argument candidates
FG The sequence of the predicate and the argument labels in the predicate-argument structure (e.g. A0-PRED-A1?
Whether the semantic roles defined in frames exist in the structure, (e.g. CONTAINS:A1)
The conjunction of the predicate sense and the frame information (e.g. wear.01&CONTAINS:A1)
Table 1: Features for the Structured Model
Avg. Ca Ch Cz En Ge Jp Sp
FP+FA 79.17 78.00 76.02 85.24 83.09 76.76 77.27 77.83
FP+FA+FPA 79.58 78.38 76.23 85.14 83.36 78.31 77.72 77.92
FP+FA+FG 80.42 79.50 76.96 85.88 84.49 78.64 78.32 79.21
ALL 80.75 79.55 77.20 85.94 84.97 79.62 78.69 79.29
Bjo?rkelund 80.80 80.01 78.60 85.41 85.63 79.71 76.30 79.91
Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
SENSE ARG
FP+FA 89.65 72.20
FP+FA+FPA 89.78 72.74
FP+FA+FG 89.83 74.11
ALL 90.15 74.46
Table 3: Predicate sense disambiguation and argu-
ment role labeling results (average).
used for FPA are inspired by formulae used in
the MLN-based SRL systems, such as (Meza-Ruiz
and Riedel, 2009b). We used the same feature
templates for all languages.
3.2 Results
Table 2 shows the results of the experiments, and
also shows the results of the top 3 systems in the
CoNLL-2009 Shared Task participants of the SRL-
only system.
By incorporating FPA, we achieved perfor-
mance improvement for all languages. This results
suggest that it is effective to capture local inter-
dependencies between a predicate sense and one
of its argument roles. Comparing the results with
FP+FA and FP+FA+FG, incorporating FG also
contributed performance improvements for all lan-
guages, especially the substantial F1 improvement
of +1.88 is obtained in German.
Next, we compare our system with top 3 sys-
tems in the CoNLL-2009 Shared Task. By in-
corporating both FPA and FG, our joint model
achieved competitive results compared to the top 2
systems (Bjo?rkelund and Zhao), and achieved the
better results than the Meza-Ruiz?s system 2. The
systems by Bjo?rkelund and Zhao applied feature
selection algorithms in order to select the best set
of feature templates for each language, requiring
about 1 to 2 months to obtain the best feature set.
On the other hand, our system achieved the com-
petitive results with the top two systems, despite
the fact that we used the same feature templates
for all languages without applying any feature en-
gineering procedure.
Table 3 shows the performances of predicate
sense disambiguation and argument role labeling
separately. In terms of sense disambiguation re-
sults, incorporating FPA and FG worked well. Al-
though incorporating either of FPA and FG pro-
vided improvements of +0.13 and +0.18 on av-
erage, adding both factors provided improvements
of +0.50. We compared the predicate sense dis-
2The result of Meza-Ruiz for Czech is substantially worse
than the other systems because of inappropriate preprocess-
ing for predicate sense disambiguation. Excepting Czech, the
average F1 value of the Meza-Ruiz is 77.75, where as our
system is 79.89.
101
ambiguation results of FP +FA and ALL with the
McNemar test, and the difference was statistically
significant (p < 0.01). This result suggests that
combination of these factors is effective for sense
disambiguation.
As for argument role labeling results, incorpo-
rating FPA and FG contributed positively for all
languages. Especially, we obtained a substan-
tial gain (+4.18) in German. By incorporating
FPA, the system achieved the F1 improvements
of +0.54 on average. This result shows that cap-
turing inter-dependencies between a predicate and
its arguments contributes to argument role label-
ing. By incorporating FG, the system achieved the
substantial improvement of F1 (+1.91).
Since both tasks improved by using all factors,
we can say that the proposed joint model suc-
ceeded in joint learning of predicate senses and
its argument roles.
4 Conclusion
In this paper, we proposed a structured model that
captures both non-local dependencies between ar-
guments, and inter-dependencies between a pred-
icate sense and its argument roles. We designed
a linear model-based structured model, and de-
fined four types of factors: predicate factor, ar-
gument factor, predicate-argument pairwise fac-
tor and global factor for the model. In the ex-
periments, the proposed model achieved compet-
itive results compared to the state-of-the-art sys-
tems without any feature engineering.
A further research direction we are investi-
gating is exploitation of unlabeled texts. Semi-
supervised semantic role labeling methods have
been explored by (Collobert and Weston, 2008;
Deschacht and Moens, 2009; Fu?rstenau and La-
pata, 2009), and they have achieved successful
outcomes. However, we believe that there is still
room for further improvement.
References
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In
CoNLL-2009.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML
2008.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551?585.
Hal Daume? III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In ICML-2005.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In EMNLP-2009.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In EMNLP-2009.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL-2009, Boul-
der, Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In CoNLL-2008.
Jun?Ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with
non-local features. In EMNLP-CoNLL 2007.
Ivan Meza-Ruiz and Sebastian Riedel. 2009a. Jointly
identifying predicates, arguments and senses using
markov logic. In HLT/NAACL-2009.
Ivan Meza-Ruiz and Sebastian Riedel. 2009b. Multi-
lingual semantic role labelling with markov logic.
In CoNLL-2009.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collec-
tive semantic role labelling with markov logic. In
CoNLL-2008.
Mihai Surdeanu, Richard Johansson, Adam Mey-
ers, Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In CoNLL-2008.
Synthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2010. A generative model for semantic
role labeling. In Proceedings of the 48th Annual
Meeting of the Association of Computational Lin-
guistics (to appear).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2).
102
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382?391,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Is a 204 cm Man Tall or Small ?
Acquisition of Numerical Common Sense from the Web
Katsuma Narisawa1 Yotaro Watanabe1 Junta Mizuno2
Naoaki Okazaki1,3 Kentaro Inui1
1Graduate School of Information Sciences, Tohoku University
2National Institute of Information and Communications Technology (NICT)
3Japan Science and Technology Agency (JST)
{katsuma,yotaro-w,junta-m,okazaki,inui}@ecei.tohoku.ac.jp
Abstract
This paper presents novel methods for
modeling numerical common sense: the
ability to infer whether a given number
(e.g., three billion) is large, small, or nor-
mal for a given context (e.g., number of
people facing a water shortage). We first
discuss the necessity of numerical com-
mon sense in solving textual entailment
problems. We explore two approaches for
acquiring numerical common sense. Both
approaches start with extracting numeri-
cal expressions and their context from the
Web. One approach estimates the distribu-
tion of numbers co-occurring within a con-
text and examines whether a given value is
large, small, or normal, based on the distri-
bution. Another approach utilizes textual
patterns with which speakers explicitly ex-
presses their judgment about the value of
a numerical expression. Experimental re-
sults demonstrate the effectiveness of both
approaches.
1 Introduction
Textual entailment recognition (RTE) involves a
wide range of semantic inferences to determine
whether the meaning of a hypothesis sentence (h)
can be inferred from another text (t) (Dagan et
al., 2006). Although several evaluation campaigns
(e.g., PASCAL/TAC RTE challenges) have made
significant progress, the RTE community recog-
nizes the necessity of a deeper understanding of
the core phenomena involved in textual inference.
Such recognition comes from the ideas that cru-
cial progress may derive from decomposing the
complex RTE task into basic phenomena and from
solving each basic phenomenon separately (Ben-
tivogli et al, 2010; Sammons et al, 2010; Cabrio
and Magnini, 2011; Toledo et al, 2012).
Given this background, we focus on solving one
of the basic phenomena in RTE: semantic infer-
ence related to numerical expressions. The spe-
cific problem we address is acquisition of numeri-
cal common sense. For example,
(1) t : Before long, 3b people will face a water
shortage in the world.
h : Before long, a serious water shortage
will occur in the world.
Although recognizing the entailment relation be-
tween t and h is frustratingly difficult, we assume
this inference is decomposable into three phases:
3b people face a water shortage.
? 3,000,000,000 people face a water shortage.
|= many people face a water shortage.
|= a serious water shortage.
In the first phase, it is necessary to recognize 3b
as a numerical expression and to resolve the ex-
pression 3b into the exact amount 3,000,000,000.
The second phase is much more difficult because
we need subjective but common-sense knowledge
that 3,000,000,000 people is a large number.
In this paper, we address the first and sec-
ond phases of inference as an initial step towards
semantic processing with numerical expressions.
The contributions of this paper are four-fold.
1. We examine instances in existing RTE cor-
pora, categorize them into groups in terms of
the necessary semantic inferences, and dis-
cuss the impact of this study for solving RTE
problems with numerical expressions.
2. We describe a method of normalizing numer-
ical expressions referring to the same amount
in text into a unified semantic representation.
3. We present approaches for aggregating nu-
merical common sense from examples of nu-
merical expressions and for judging whether
a given amount is large, small, or normal.
382
4. We demonstrate the effectiveness of this ap-
proach, reporting experimental results and
analyses in detail. Although it would be ideal
to evaluate the impact of this study on the
overall RTE task, we evaluate each phase sep-
arately. We do this because the existing RTE
data sets tend to exhibit very diverse linguis-
tic phenomena, and it is difficult to employ
such data for evaluating the real impact of
this study.
2 Related work
Surprisingly, NLP research has paid little atten-
tion to semantic processing of numerical expres-
sions. This is evident when we compare with tem-
poral expressions, for which corpora (e.g., ACE-
20051, TimeBank2) were developed with annota-
tion schemes (e.g., TIMEX3, TimeML4).
Several studies deal with numerical expressions
in the context of information extraction (Bakalov
et al, 2011), information retrieval (Fontoura et al,
2006; Yoshida et al, 2010), and question answer-
ing (Moriceau, 2006). Numbers such as prod-
uct prices and weights have been common targets
of information extraction. Fontoura et al (2006)
and Yoshida et al (2010) presented algorithms and
data structures that allow number-range queries
for searching documents. However, these studies
do not interpret the quantity (e.g., 3,000,000,000)
of a numerical expression (e.g., 3b people), but
rather treat numerical expressions as strings.
Banerjee et al (2009) focused on quantity con-
sensus queries, in which there is uncertainty about
the quantity (e.g., weight airbus A380 pounds).
Given a query, their approach retrieves documents
relevant to the query and identifies the quantities
of numerical expressions in the retrieved docu-
ments. They also proposed methods for enumer-
ating and ranking the candidates for the consen-
sus quantity intervals. Even though our study
shares a similar spirit (modeling of consensus for
quantities) with Banerjee et al (2009), their goal
is different: to determine ground-truth values for
queries.
In question answering, to help ?sanity check?
answers with numerical values that were
1http://www.itl.nist.gov/iad/mig/
tests/ace/ace05/
2http://www.timeml.org/site/timebank/
timebank.html
3http://timex2.mitre.org/
4http://timeml.org/site/index.html
way out of common-sense ranges, IBM?s PI-
QUANT (Prager et al, 2003; Chu-Carroll et al,
2003) used information in Cyc (Lenat, 1995).
For example, their question-answering system
rejects 200 miles as a candidate answer for the
height of Mt. Everest, since Cyc knows mountains
are between 1,000 and 30,000 ft. high. They
also consider the problem of variations in the
precision of numbers (e.g., 5 million, 5.1 million,
5,200,390) and unit conversions (e.g., square
kilometers and acres).
Some recent studies delve deeper into the se-
mantic interpretation of numerical expressions.
Aramaki et al (2007) focused on the physical size
of an entity to predict the semantic relation be-
tween entities. For example, knowing that a book
has a physical size of 20 cm ? 25 cm and that a li-
brary has a size of 10 m ? 10 m, we can estimate
that a library contains a book (content-container
relation). Their method acquires knowledge about
entity size from the Web (by issuing queries like
?book (*cm x *cm)?), and integrates the knowl-
edge as features for the classification of relations.
Davidov and Rappoport (2010) presented a
method for the extraction from the Web and ap-
proximation of numerical object attributes such as
height and weight. Given an object-attribute pair,
the study expands the object into a set of compa-
rable objects and then approximates the numerical
values even when no exact value can be found in a
text. Aramaki et al (2007) and Davidov and Rap-
poport (2010) rely on hand-crafted patterns (e.g.,
?Object is * [unit] tall?), focusing on a specific set
of numerical attributes (e.g., height, weight, size).
In contrast, this study can handle any kind of target
and situation that is quantified by numbers, e.g.,
number of people facing a water shortage.
Recently, the RTE community has started to
pay some attention to the appropriate processing
of numerical expressions. Iftene (2010) presented
an approach for matching numerical ranges ex-
pressed by a set of phrases (e.g., more than and at
least). Tsuboi et al (2011) designed hand-crafted
rules for matching intervals expressed by temporal
expressions. However, these studies do not nec-
essarily focus on semantic processing of numeri-
cal expressions; thus, these studies do not normal-
ize units of numerical expressions nor make infer-
ences with numerical common sense.
Sammons et al (2010) reported that most sys-
tems submitted to RTE-5 failed on examples
383
where numeric reasoning was necessary. They ar-
gued the importance of aligning numerical quanti-
ties and performing numerical reasoning in RTE.
LoBue and Yates (2011) identified 20 categories
of common-sense knowledge that are prevalent in
RTE. One of the categories comprises arithmetic
knowledge (including computations, comparisons,
and rounding). They concluded that many kinds
of the common-sense knowledge have received
scarce attention from researchers even though the
knowledge is essential to RTE. These studies pro-
vided a closer look at the phenomena involved in
RTE, but they did not propose a solution for han-
dling numerical expressions.
3 Investigation of textual-entailment
pairs with numerical expressions
In this section, we investigate textual entailment
(TE) pairs in existing corpora in order to study
the core phenomena that establish an entailment
relation. We used two Japanese TE corpora:
RITE (Shima et al, 2011) and Odani et al (2008).
RITE is an evaluation workshop of textual entail-
ment organized by NTCIR-9, and it targets the
English, Japanese, and Chinese languages. We
used the Japanese portions of the development
and training data. Odani et al (2008) is another
Japanese corpus that was manually created. The
total numbers of text-hypothesis (T -H) pairs are
1,880 (RITE) and 2,471 (Odani).
We manually selected sentence pairs in which
one or both of the sentences contained a numerical
expression. Here, we define the term numerical
expression as an expression containing a number
or quantity represented by a numeral and a unit.
For example, 3 kilometers is a numerical expres-
sion with the numeral 3 and the unit kilometer.
Note that intensity of 4 is not a numerical expres-
sion because intensity is not a unit.
We obtained 371 pairs from the 4,351 T -H
pairs. We determined the inferences needed to
prove ENTAILMENT or CONTRADICTION of the
hypotheses, and classified the 371 pairs into 11
categories. Note that we ignored T -H pairs in
which numerical expressions were unnecessary
to prove the entailment relation (e.g., Socrates
was sentenced to death by 500 jury members and
Socrates was sentenced to death). Out of 371
pairs, we identified 114 pairs in which numerical
expressions played a central role in the entailment
relation.
Table 1 summarizes the categories of TE phe-
nomena we found in the data set. The largest cate-
gory is numerical matching (32 pairs). We can in-
fer an entailment relation in this category by align-
ing two numerical expressions, e.g., 2.2 million
|= over 800 thousand. This is the most funda-
mental task in numerical reasoning, interpreting
the amount (number, unit, and range) in a numer-
ical expression. We address this task in Section
4.1. The second largest category requires com-
mon sense about numerical amounts. In order to
recognize textual entailment of pairs in this cat-
egory, we need common-sense knowledge about
humans? subjective judgment of numbers. We
consider this problem in Section 5.
To summarize, this study covers 37.9% of the
instances in Table 1, focusing on the first and sec-
ond categories. Due to space limitations, we omit
the explanations for the other phenomena, which
require such things as lexical knowledge, arith-
metic operations, and counting. The coverage of
this study might seem small, but it is difficult to
handle varied phenomena with a unified approach.
We believe that this study forms the basis for in-
vestigating other phenomena of numerical expres-
sions in the future.
4 Collecting numerical expressions from
the Web
In this paper, we explore two approaches to acquir-
ing numerical common sense. Both approaches
start with extracting numerical expressions and
their context from the Web. We define a context
as the verb and its arguments that appear around a
numerical expression.
For instance, the context of 3b people in the sen-
tence 3b people face a water shortage is ?face?
and ?water shortage.? In order to extract and
aggregate numerical expressions in various doc-
uments, we converted the numerical expressions
into semantic representations (to be described in
Section 4.1), and extracted their context (to be de-
scribed in Section 4.2).
The first approach for acquiring numerical com-
mon sense estimates the distribution of numbers
that co-occur within a context, and examines
whether a given value is large, small, or normal
based on that distribution (to be described in Sec-
tion 5.1). The second approach utilizes textual
patterns with which speakers explicitly expresses
their judgment about the value of a numerical ex-
384
Category Definition Example #
Numerical matching
Aligning numerical expres-
sions in T and H, considering
differences in unit, range, etc.
t: It is said that there are about 2.2 million alcoholics in the whole country.
h: It is estimated that there are over 800 thousand people who are alcoholics. 32
Numerical common sense
Inferring by interpreting the
numerical amount (large or
small).
t: In the middle of the 21st century, 7 billion people, corresponding to 70% of the
global population, will face a water shortage.
h: It is concerning that a serious water shortage will spread around the world in the
near future.
12
Lexical knowledge Inferring by using numericalaspects of word meanings.
t: Mr. and Ms. Sato celebrated their 25th wedding anniversary.
h: Mr. and Ms. Sato celebrated their silver wedding anniversary. 12
Arithmetic Arithmetic operations includ-ing addition and subtraction.
t: The number of 2,000-yen bills in circulation has increased to 450 million, in
contrast with 440 million 5,000-yen bills.
h: The number of 2,000-yen bills in circulation exceeds the number of 5,000-yen
bills by 10 million bills.
11
Numeric-range expression
of verbs
Numerical ranges expressed by
verbs (e.g., exceed).
t: It is recorded that the maximum wave height reached 13.8 meters during the Sea
of Japan Earthquake Tsunami in May 1983.
h: During the Sea of Japan Earthquake, the height of the tsunami exceeded 10meters.
9
Simple Rewrite Rule This includes various simplerules for rewriting.
t: The strength of Taro?s grip is No. 1 in his class.
h: Taro?s grip is the strongest in his class. 7
State change Expressing the change of avalue by a multiplier or ratio.
t: Consumption of pickled plums is 1.5 times the rate of 20 years ago.
h: Consumption of pickled plums has increased. 6
Ordinal numbers Inference by interpreting ordi-nal numbers.
t: Many precious lives were sacrificed in the Third World War.
h: So far, there have been at least three World Wars. 6
Temporal expression
Inference by interpreting tem-
poral expressions such as an-
niversary, age, and ordinal
numbers.
t: Mr. and Ms. Sato celebrate their 25th wedding anniversary.
h: Mr. and Ms. Sato got married 25 years ago. 3
Count Counting up the number of var-ious entities.
t: In Japan, there are the Asian Triopsidae, the American Triopsidae, and the Euro-
pean Triopsidae.
h: In Japan, there are 3 types of Triopsidae.
3
Others 15
All 116
Table 1: Frequency and simple definitions for each category of the entailment phenomena in the survey.
Numerical Semantic representation
Expression Value Unit Mod.
about seven grams 7 g about
roughly 7 kg 7000 g about
as heavy as 7 tons 7? 106 g large
as cheap as $1 1 $ small
30?40 people [30, 40] nin (people)
more than 30 cars 30 dai (cars) over
7 km per hour 7000 m/h
Table 2: Normalized representation examples
pression (to be explained in Section 5.2).
In this study, we acquired numerical common
sense from a collection of 8 billion sentences in
100 million Japanese Web pages (Shinzato et al,
2012). For this reason, we originally designed
text patterns specialized for Japanese dependency
trees. For the sake of the readers? understand-
ing, this paper uses examples with English trans-
lations for explaining language-independent con-
cepts, and both Japanese and English translations
for explaining language-dependent concepts.
4.1 Extracting and normalizing numerical
expressions
The first step for collecting numerical expres-
sions is to recognize when a numerical expression
is mentioned and then to normalize it into a seman-
tic representation. This is the most fundamental
String Operation
gram(s) set-unit: ?g?
kilogram(s) set-unit: ?g?; multiply-value: 1,000
kg set-unit: ?g?; multiply-value: 1,000
ton(s) set-unit: ?g?; multiply-value: 1,000,000
nin (people) set-unit: ?nin? (person)
about set-modifier: ?about?
as many as set-modifier: ?large?
as little as set-modifier: ?small?
Table 3: An example of unit/modifier dictionary
step in numerical reasoning and has a number of
applications. For example, this step handles cases
of numerical matching, as in Table 1.
The semantic representation of a numerical ex-
pression consists of three fields: the value or range
of the real number(s)5, the unit (a string), and the
optional modifiers. Table 2 shows some exam-
ples of numerical expressions and their semantic
representations. During normalization, we identi-
fied spelling variants (e.g., kilometer and km) and
transformed auxiliary units into their correspond-
ing canonical units (e.g., 2 tons and 2,000 kg to
2,000,000 grams). When a numerical expression
is accompanied by a modifier such as over, about,
or more than, we updated the value and modifier
fields appropriately.
5Internally, all values are represented by ranges (e.g., 75
is represented by the range [75, 75]).
385
We developed an extractor and a normalizer for
Japanese numerical expressions6. We will outline
the algorithm used in the normalizer with an exam-
ple sentence: ?Roughly three thousand kilograms
of meats have been provided every day.?
1. Find numbers in the text by using regular ex-
pressions and convert the non-Arabic num-
bers into their corresponding Arabic num-
bers. For example, we find three thousand7
and represent it as 3, 000.
2. Check whether the words that precede or fol-
low the number are units that are registered in
the dictionary. Transform any auxiliary units.
In the example, we find that kilograms8 is a
unit. We multiply the value 3, 000 by 1, 000,
and obtain the value 3, 000, 000 with the unit
g.
3. Check whether the words that precede or fol-
low the number have a modifier that is regis-
tered in the dictionary. Update the value and
modifier fields if necessary. In the example,
we find roughly and set about in the modifier
field.
We used a dictionary9 to perform procedures 2
and 3 (Table 3). If the words that precede or fol-
low an extracted number match an entry in the dic-
tionary, we change the semantic representation as
described in the operation.
The modifiers ?large? and ?small? require elab-
oration because the method in Section 5.2 relies
heavily on these modifiers. We activated the mod-
ifier ?large? when a numerical expression occurred
with the Japanese word mo, which roughly cor-
responds to as many as, as large as, or as heavy
as in English10. Similarly, we activated the modi-
fier ?small? when a numerical expression occurred
with the word shika, which roughly corresponds
to as little as, as small as, or as light as11. These
modifiers are important for this study, reflecting
the writer?s judgment about the amount.
6The software is available at http://www.cl.
ecei.tohoku.ac.jp/?katsuma/software/
normalizeNumexp/
7In Japanese 3, 000 is denoted by the Chinese symbols ?
???.
8We write kilograms as ??????? in Japanese.
9The dictionary is bundled with the tool. See Footnote 6.
10In Japanese, we can use the word mo with a numerical
expression to state that the amount is ?large? regardless of
how large it is (e.g., large, big, many, heavy).
11Similarly, we can use the word shika with any adjective.
?? ??? ??? ????? ????
He gave to a friend$300 at the bank.
Japanese:
English:
nsubj dobj prep_to
prep_at
Number: {value: 300; unit: ?$? }
Context: {verb: ?give? ; nsubj: ?he? ; 
 prep_to: ?friend? ; prep_at: ?bank? }
Figure 1: Example of context extraction
4.2 Extraction of context
The next step in acquiring numerical common
sense is to capture the context of numerical ex-
pressions. Later, we will aggregate numbers that
share the same context (see Section 5). The con-
text of a numerical expression should provide suf-
ficient information to determine what it measures.
For example, given the sentence, ?He gave $300 to
a friend at the bank,? it would be better if we could
generalize the context to someone gives money to
a friend for the numerical expression $300. How-
ever, it is a nontrivial task to design an appropriate
representation of varying contexts. For this rea-
son, we employ a simple rule to capture the con-
text of numerical expressions: we represent the
context with the verb that governs the numerical
expression and its typed arguments.
Figure 1 illustrates the procedure for extracting
the context of a numerical expression12. The com-
ponent in Section 4.1 recognizes $300 as a numer-
ical expression, then normalizes it into a semantic
representation. Because the numerical expression
is a dependent of the verb gave, we extract the verb
and its arguments (except for the numerical ex-
pression itself) as the context. After removing in-
flections and function words from the arguments,
we obtain the context representation of Figure 1.
5 Acquiring numerical common sense
In this section, we present two approaches for ac-
quiring numerical common sense from a collec-
tion of numerical expressions and their contexts.
Both approaches start with collecting the numbers
(in semantic representation) and contexts of nu-
merical expressions from a large number of sen-
tences (Shinzato et al, 2012), and storing them
12The English dependency tree might look peculiar be-
cause it is translated from the Japanese dependency tree.
386
in a database. When a context and a value are
given for a prediction (hereinafter called the query
context and query value, respectively), these ap-
proaches judge whether the query value is large,
small, or normal.
5.1 Distribution-based approach
Given a query context and query value, this
approach retrieves numbers associated with the
query context and draws a distribution of normal-
ized numbers. This approach considers the dis-
tribution estimated for the query context and de-
termines if the value is within the top 5 percent
(large), within the bottom 5 percent (small), or is
located in between these regions (normal).
The underlying assumption of this approach is
that the real distribution of a query (e.g., money
given to a friend) can be approximated by the dis-
tribution of numbers co-occurring with the context
(e.g., give and friend) on the Web. However, the
context space generated in Section 4.2 may be too
sparse to find numbers in the database, especially
when a query context is fine-grained. Therefore,
when no item is retrieved for the query context,
we employ a backoff strategy to drop some of the
uninformative elements in the query context: ele-
ments are dropped from the context based on the
type of argument, in this order: he (prep to), kara
(prep from), ha (nsubj), yori (prep from), made
(prep to), nite (prep at), de (prep at, prep by), ni
(prep at), wo (dobj), ga (nsubj), and verb.
5.2 Clue-based approach
This approach utilizes textual clues with which a
speaker explicitly expresses his or her judgment
about the amount of a numerical expression. We
utilize large and small modifiers (described in Sec-
tion 4.1), which correspond to textual clues mo
(as many as, as large as) and shika (only, as
few as), respectively, for detecting humans? judg-
ments. For example, we can guess that $300 is
large if we find an evidential sentence13, He gave
as much as $100 to a friend.
Similarly to the distribution-based approach,
this approach retrieves numbers associated with
the query context. This approach computes the
13Although the sentence states a judgment about $100, we
can infer that $300 is also large because $300 > $100.
largeness L(x) of a value x:
L(x) = pl(x)ps(x) + pl(x)
, (1)
pl(x) =
??{r|rv < x ? rm 3 large}
??
??{r|rm 3 large}
?? , (2)
ps(x) =
??{r|rv > x ? rm 3 small}
??
??{r|rm 3 small}
?? . (3)
In these equations, r denotes a retrieved item for
the query context, and rv and rm represent the nor-
malized value and modifier flags, respectively, of
the item r. The numerator of Equation 2 counts
the number of numerical expressions that support
the judgment that x is large14, and its denominator
counts the total number of numerical expressions
with large as a modifier. Therefore, pl(x) com-
putes the ratio of times there is textual evidence
that says that x is large, to the total number of
times there is evidences with large as a modifier.
In an analogous way, ps(x) is defined to be the ra-
tio for evidence that says x is small. Hence, L(x)
approaches 1 if everyone on the Web claims that
x is large, and approaches 0 if everyone claims
that x is small. This approach predicts large if
L(x) > 0.95, small if L(x) < 0.05, and normal
otherwise.
6 Experiments
6.1 Normalizing numerical expressions
We evaluated the method that we described in Sec-
tion 4.1 for extracting and normalizing numerical
expressions. In order to prepare a gold-standard
data set, we obtained 1,041 sentences by randomly
sampling about 1% of the sentences containing
numbers (Arabic digits and/or Chinese numerical
characters) in a Japanese Web corpus (100 million
pages) (Shinzato et al, 2012). For every numer-
ical expression in these sentences, we manually
determined a tuple of the normalized value, unit,
and modifier. Here, non-numerical expressions
such as temporal expressions, telephone numbers,
and postal addresses, which were very common,
were beyond the scope of the project15. We ob-
tained 329 numerical expressions from the 1,041
sentences.
We evaluated the correctness of the extraction
and normalization by measuring the precision and
14This corresponds to the events where we find an evidence
expression ?as many as rv?, where rv < x.
15If a tuple was extracted from a non-numerical expres-
sion, we regarded this as a false positive
387
recall using the gold-standard data set16. Our
method performed with a precision of 0.78 and a
recall of 0.92. Most of the false negatives were
caused by the incompleteness of the unit dictio-
nary. For example, the proposed method could not
identify 1Ghz as a numerical expression because
the unit dictionary did not register Ghz but GHz.
It is trivial to improve the recall of the method by
enriching the unit dictionary.
The major cause of false positives was the se-
mantic ambiguity of expressions. For example, the
proposed method identified Seven Hills as a nu-
merical expression although it denotes a location
name. In order to reduce false positives, it may
be necessary to utilize broader contexts when lo-
cating numerical expressions; this could be done
by using, for example, a named entity recognizer.
This is the next step to pursue in future work.
However, these errors do not have a large effect
on the estimation of the distribution of the numer-
ical values that occur with specific named entities
and idiomatic phrases. Moreover, as explained in
Section 5, we draw distributions for fine-grained
contexts of numerical expressions. For these rea-
sons, we think that the current performance is suf-
ficient for acquiring numerical common sense.
6.2 Acquisition of numerical common sense
6.2.1 Preparing an evaluation set
We built a gold-standard data set for numerical
common sense. We applied the method in Sec-
tion 4.1 to sentences sampled at random from the
Japanese Web corpus (Shinzato et al, 2012), and
we extracted 2,000 numerical expressions. We
asked three human judges to annotate every nu-
merical expression with one of six labels, small,
relatively small, normal, relatively large, large,
and unsure. The label relatively small could be
applied to a numerical expression when the judge
felt that the amount was rather small (below the
normal) but hesitated to label it small. The la-
bel relatively large was defined analogously. We
gave the following criteria for labeling an item as
unsure: when the judgment was highly dependent
on the context; when the sentence was incompre-
hensible; and when it was a non-numerical expres-
sions (false positives of the method are discussed
in Section 4.1).
Table 4 reports the inter-annotator agreement.
16All fields (value, unit, modifier) of the extracted tuple
must match the gold-standard data set.
Agreement # expressions
3 annotators 735 (36.7%)
2 annotators 963 (48.2%)
no agreement 302 (15.1%)
Total 2000 (100.0%)
Table 4: Inter-annotator agreement
0"
100"
200"
300"
400"
500"
0"
100"
200"
130" 140" 150" 160" 170" 180" 190" 200" 210"[cm]
distribu7on:based"clue:based(large)"clue:based(small)"
[#"extrac7on]"(distribu7on:based)[#"extrac7on]"(clue:based)
Figure 2: Distributions of numbers with large and
small modifiers for the context human?s height.
For the evaluation of numerical expressions in the
data set, we used those for which at least two anno-
tators assigned the same label. After removing the
unsure instances, we obtained 640 numerical ex-
pressions (20 small, 35 relatively small, 152 nor-
mal, 263 relatively large, and 170 large) as the
evaluation set.
6.2.2 Results
The proposed method extracted about 23 million
pairs of numerical expressions and their context
from the corpus (with 100 million Web pages).
About 15% of the extracted pairs were accom-
panied by either a large or small modifier. Fig-
ure 2 depicts the distributions of the context hu-
man?s height produced by the distribution-based
and clue-based approaches. These distributions
are quite reasonable as common-sense knowledge:
we can interpret that numbers under 150 cm are
perceived as small and those above 180 cm as
large.
We measured the correctness of the proposed
methods on the gold-standard data. For this
evaluation, we employed two criteria for correct-
ness: strict and lenient. With the strict crite-
rion, the method must predict a label identical to
that in the gold-standard. With the lenient crite-
rion, the method was also allowed to predict either
large/small or normal when the gold-standard la-
bel was relatively large/small.
Table 5 reports the precision (P), recall (R), F1
(F1), and accuracy (Acc) of the proposed methods.
388
No. System Gold Sentence Remark
1 small small
I think that three men can
create such a great thing in
the world.
Correct
2 normal normal I have two cats. Correct
3 large large It?s above 32 centigrade. Correct
4 large large I earned 10 million yen fromhorse racing. Correct
5 small normal There are 2 reasons. Difficulty in judging small. Since a few people say, ?There areonly 2 reasons,? our approach predicted a small label.
6 small large
Ten or more people came,
and my eight-mat room was
packed.
Difficulty in modeling the context because this sentence omits
the locational argument for the verb came. We should extract
the context as the number of people who came to my eight-mat
room instead of the number of people who came.
7 small normal
I have two friends who
have broken up with their
boyfriends recently.
Difficulty in modeling the context. We should extract context as
the number of friends who have broken up with their boyfriends
recently instead of the number of friends.
8 small large
Lack of knowledge. We extract the context as the number of
heads of a turtle, but no corresponding information was found
on the Web.
Table 6: Output example and error analysis. We present translations of the sentences, which were origi-
nally in Japanese.
Approach Label P R F1 Acc
large+ 0.892 0.498 0.695
Distribution normal+ 0.753 0.935 0.844 0.760
small+ 0.273 0.250 0.262
large 0.861 0.365 0.613
Distribution normal 0.529 0.908 0.719 0.590
small 0.222 0.100 0.161
large+ 0.923 0.778 0.851
Clue normal+ 0.814 0.765 0.790 0.770
small+ 0.228 0.700 0.464
large 0.896 0.659 0.778
Clue normal 0.593 0.586 0.590 0.620
small 0.164 0.550 0.357
Table 5: Precision (P), recall (R), F1 score (F1),
and accuracy (Acc) of the acquisition of numerical
common sense.
Labels with the suffix ?+? correspond to the lenient
criterion. The clue-based approach achieved 0.851
F1 (for large), 0.790 F1 (for normal), and 0.464
(for small) with the lenient criterion. The perfor-
mance is surprisingly good, considering the sub-
jective nature of this task.
The clue-based approach was slightly better
than the distribution-based approach. In particu-
lar, the clue-based approach is good at predicting
large and small labels, whereas the distribution-
based approach is good at predicting normal la-
bels. We found some targets for which the distri-
bution on the Web is skewed from the ?real? dis-
tribution. For example, let us consider the distri-
bution of the context ?the amount of money that a
person wins in a lottery?. We can find a number
of sentences like if you won the 10-million-dollar
lottery, .... In other words, people talk about a
large amount of money even if they did not win
any money at all. In order to remedy this problem,
we may need to enrich the context representation
by introducing, for example, the factuality of an
event.
6.2.3 Discussion
Table 6 shows some examples of predictions from
the clue-based approach. Because of space limita-
tions, we mention only the false instances of this
approach.
The clue-based approach tends to predict small
even if the gold-standard label is normal. About
half of the errors of the clue-based approach were
of this type; this is why the precision for small and
the recall for normal are low. The cause of this er-
ror is exemplified by the sentence, ?there are two
reasons.? Human judges label normal to the nu-
merical expression two reasons, but the method
predicts small. This is because a few people say
there are only two reasons, but no one says there
are as many as two reasons. In order to handle
these cases, we may need to incorporate the distri-
bution information with the clue-based approach.
We found a number of examples for which
modeling the context is difficult. Our approach
represents the context of a numerical expression
with the verb that governs the numerical expres-
sion and its typed arguments. However, this ap-
proach sometimes misses important information,
especially when an argument of the verb is omit-
ted (Example 6). The approach also suffers from
the relative clause in Example 7, which conveys an
essential context of the number. These are similar
to the scope-ambiguity problem such as encoun-
389
tered with negation and quantification; it is diffi-
cult to model the scope when a numerical expres-
sion refers to a situation.
Furthermore, we encountered some false exam-
ples even when we were able to precisely model
the context. In Example 8, the proposed method
was unable to predict the label correctly because
no corresponding information was found on the
Web. The proposed method might more easily pre-
dict a label if we could generalize the word turtle
as animal. It may be worth considering using lan-
guage resources (e.g., WordNet) to generalize the
context.
7 Conclusions
We proposed novel approaches for acquiring nu-
merical common sense from a collection of texts.
The approaches collect numerical expressions and
their contexts from the Web, and acquire numeri-
cal common sense by considering the distributions
of normalized numbers and textual clues such as
mo (as many as) and shika (only, as few as). The
experimental results showed that our approaches
can successfully judge whether a given amount
is large, small, or normal. The implementations
and data sets used in this study are available on
the Web17. We believe that acquisition of numer-
ical common sense is an important step towards a
deeper understanding of inferences with numbers.
There are three important future directions for
this research. One is to explore a more sophis-
ticated approach for precisely modeling the con-
texts of numbers. Because we confirmed in this
paper that these two approaches have different
characteristics, it would be interesting to incorpo-
rate textual clues into the distribution-based ap-
proach by using, for example, machine learning
techniques. Finally, we are planning to address the
?third phase? of the example explained in Section
1: associating many people face a water shortage
with a serious water shortage.
Acknowledgments
This research was partly supported by JST,
PRESTO. This research was partly supported by
JSPS KAKENHI Grant Numbers 23240018 and
23700159.
17http://www.cl.ecei.tohoku.ac.jp/
?katsuma/resource/numerical common sense/
References
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and
Kazuhiko Ohe. 2007. Uth: Svm-based semantic
relation classification using physical sizes. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 464?467.
Anton Bakalov, Ariel Fuxman, Partha Pratim Talukdar,
and Soumen Chakrabarti. 2011. SCAD: collective
discovery of attribute values. In Proceedings of the
20th international conference on World wide web,
WWW ?11, pages 447?456.
Somnath Banerjee, Soumen Chakrabarti, and Ganesh
Ramakrishnan. 2009. Learning to rank for quantity
consensus queries. In Proceedings of the 32nd inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?09,
pages 243?250.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2010. Building textual entailment special-
ized data sets: a methodology for isolating linguis-
tic phenomena relevant to inference. Proceedings of
the Seventh International Conference on Language
Resources and Evaluation, pages 3542?3549.
Elena Cabrio and Bernardo Magnini. 2011. Towards
component-based textual entailment. In Proceed-
ings of the Ninth International Conference on Com-
putational Semantics, IWCS ?11, pages 320?324.
Jennifer Chu-Carroll, David A. Ferrucci, John M.
Prager, and Christopher A. Welty. 2003. Hybridiza-
tion in question answering systems. In New Direc-
tions in Question Answering?03, pages 116?121.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine Learning Challenges. Eval-
uating Predictive Uncertainty, Visual Object Classi-
fication, and Recognising Tectual Entailment, pages
177?190.
Dmitry Davidov and Ari Rappoport. 2010. Extrac-
tion and approximation of numerical attributes from
the web. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1308?1317.
Marcus Fontoura, Ronny Lempel, Runping Qi, and Ja-
son Zien. 2006. Inverted index support for numeric
search. Internet Mathematics, 3(2):153?185.
Adrian Iftene and Mihai-Alex Moruz. 2010. UAIC
participation at RTE-6. In Proceedings of the Third
Text Analysis Conference (TAC 2010) November.
Douglas B Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
Peter LoBue and Alexander. Yates. 2011. Types of
common-sense knowledge needed for recognizing
390
textual entailment. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers-Volume 2, pages 329?334.
Ve?ronique Moriceau. 2006. Generating intelligent
numerical answers in a question-answering system.
In Proceedings of the Fourth International Natural
Language Generation Conference, INLG ?06, pages
103?110.
Michitaka Odani, Tomohide Shibata, Sadao Kurohashi,
and Takayuki Nakata. 2008. Building data of
japanese text entailment and recognition of infer-
encing relation based on automatic achieved similar
expression. In Proceeding of 14th Annual Meeting
of the Association for ?atural Language Processing,
pages 1140?1143.
John M. Prager, Jennifer Chu-Carroll, Krzysztof
Czuba, Christopher A. Welty, Abraham Ittycheriah,
and Ruchi Mahindru. 2003. IBM?s PIQUANT in
TREC2003. In TREC, pages 283?292.
Mark Sammons, Vinod V.G. Vydiswaran, and Dan
Roth. 2010. Ask not what textual entailment can do
for you... In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1199?1208.
Hideki Shima, Hiroshi Kanayama, Cheng-Wei Lee,
Chuan-Jie Lin, Teruko Mitamura, Yusuke Miyao,
Shuming Shi, and Koichi Takeda. 2011. Overview
of ntcir-9 rite: Recognizing inference in text. In Pro-
ceeding of NTCIR-9 Workshop Meeting, pages 291?
301.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
and Sadao Kurohashi. 2012. Tsubaki: An open
search engine infrastructure for developing informa-
tion access methodology. Journal of Information
Processing, 20(1):216?227.
Assaf Toledo, Sophia Katrenko, Stavroula Alexan-
dropoulou, Heidi Klockmann, Asher Stern, Ido Da-
gan, and Yoad Winter. 2012. Semantic annotation
for textual entailment recognition. In Proceedings of
the 11th Mexican International Conference on Arti-
ficial Intelligence, MICAI ?12.
Yuta Tsuboi, Hiroshi Kanayama, Masaki Ohno, and
Yuya Unno. 2011. Syntactic difference based ap-
proach for ntcir-9 rite task. In Proceedings of the
9th NTCIR Workshop, pages 404?411.
Minoru Yoshida, Issei Sato, Hiroshi Nakagawa, and
Akira Terada. 2010. Mining numbers in text using
suffix arrays and clustering based on dirichlet pro-
cess mixture models. Advances in Knowledge Dis-
covery and Data Mining, pages 230?237.
391
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 228?232
Manchester, August 2008
A Pipeline Approach for Syntactic and Semantic Dependency Parsing
Yotaro Watanabe and Masakazu Iwatate and Masayuki Asahara and Yuji Matsumoto
Nara Institute of Science and Technology, Japan
8916-5, Takayama, Ikoma, Nara, Japan, 630-0192
{yotaro-w, masakazu-i, masayu-a, matsu}@is.naist.jp
Abstract
This paper describes our system for syn-
tactic and semantic dependency parsing
to participate the shared task of CoNLL-
2008. We use a pipeline approach, in
which syntactic dependency parsing, word
sense disambiguation, and semantic role
labeling are performed separately: Syn-
tactic dependency parsing is performed
by a tournament model with a support
vector machine; word sense disambigua-
tion is performed by a nearest neighbour
method in a compressed feature space by
probabilistic latent semantic indexing; and
semantic role labeling is performed by
a an online passive-aggressive algorithm.
The submitted result was 79.10 macro-
average F1 for the joint task, 87.18% syn-
tactic dependencies LAS, and 70.84 se-
mantic dependencies F1. After the dead-
line, we constructed the other configura-
tion, which achieved 80.89 F1 for the joint
task, and 74.53 semantic dependencies F1.
The result shows that the configuration of
pipeline is a crucial issue in the task.
1 Introduction
This paper presents the description of our system
in CoNLL-2008 shared task. We split the shared
task into five sub-problems ? syntactic dependency
parsing, syntactic dependency label classification,
predicate identification, word sense disambigua-
tion, and semantic role labeling. The overview
of our system is illustrated in Figure 1. Our de-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Figure 1: Overview of the System
pendency parsing module is based on a tourna-
ment model (Iida et al, 2003), in which a depen-
dency attachment is estimated in step-ladder tour-
nament matches. The relative preference of the at-
tachment is modeled by one-on-one match in the
tournament. Iwatate et al (Iwatate et al, 2008)
initially proposed the method for Japanese depen-
dency parsing, and we applied it to other languages
by relaxing some constraints (Section 2.1). Depen-
dency label classification is performed by a linear-
chain sequential labeling on the dependency sib-
lings like McDonald?s schemata (McDonald et al,
2006). We use an online passive-aggressive al-
gorithm (Crammer et al, 2006) for linear-chain
sequential labeling (Section 2.2). We also use
the other linear-chain sequential labeling method
to annotate whether each word is a predicate or
not (Section 2.3). If an identified predicate has
more than one sense, a nearest neighbour classifier
disambiguates the word sense candidates (Section
2.4). We use an online passive-aggressive algo-
rithm again for the semantic role labeling (Section
2.5). The machine learning algorithms used in sep-
arated modules are diverse due to role sharing.
1
1
Unlabeled dependency parsing was done by Iwatate, de-
pendency label classification and semantic role labeling was
done by Watanabe, predicate identification and word sense
228
We attempt to construct a framework in which
each module passes k-best solutions and the last
semantic role labeling module performs rerank-
ing of the k-best solutions using the overall infor-
mation. Unfortunately, we couldn?t complete the
framework before the deadline of the test run. Our
method is not a ?joint learning? approach but a
pipeline approach.
2 Methods
2.1 Unlabeled Dependency Parsing
The detailed description of the tournament model-
based Japanese dependency parsing is found in
(Iwatate et al, 2008). The original Iwatate?s pars-
ing algorithm was for Japanese, which is for a
strictly head-final language. We adapt the algo-
rithm to English in this shared task. The tour-
nament model chooses the most likely candidate
head of each of the focused words in a step-
ladder tournament. For a given word, the al-
gorithm repeats to compare two candidate heads
and finds the most plausible head in the series
of a tournament. On each comparison, the win-
ner is chosen by an SVM binary classifier with
a quadratic polynomial kernel
2
. The model uses
different algorithms for training example gener-
ation and parsing. Figures 2 and 3 show train-
ing example generation and parsing algorithm, re-
spectively. Time complexity of both algorithms is
O(n
2
) for the number of words in an input sen-
tence. Below, we present the features for SVM
// N: # of tokens in input sentence
// true_head[j]: token j?s head at
// training data
// gen(j,i1,i2,LEFT): generate an example
// where token j is dependent of i1
// gen(j,i1,i2,RIGHT): generate an example
// where token j is dependent of i2
// Token 0 is the virtual ROOT.
for j = 1 to N-1 do
h = true_head[j];
for i = 0 to h-1 do
if i!=j then gen(j,i,h,RIGHT);
for i = h+1 to N do
if i!=j then gen(j,h,i,LEFT);
end-for;
Figure 2: Pseudo Code of Training Example Gen-
eration
disambiguation was done by Asahara, and all tasks were su-
pervised by Matsumoto.
2
We use TinySVM as an SVM classifier. chasen.org/
?
taku/software/TinySVM/
// N: # of tokens in input sentence
// head[]: (analyzed-) head of tokens
// classify(j,i1,i2): ask SVM
// which candidate (i1 or i2) is
// more likely for head of j.
// return LEFT if i1 wins.
// return RIGHT if i2 wins.
// cands.push_back(k): add token index k
// to the end of cands.
// cands.erase(i): remove i-th element
// from cands.
for j = 1 to N do
cands = [];
for i = 0 to N do
if i!=j then cands.push_back(i);
end-for;
while cands.size() > 1 do
if classify(j,cands[0],
cands[1]) = LEFT then
cands.erase(1);
else
cands.erase(0);
end-if;
end-while;
head[j] = cands[0];
end-for;
Figure 3: Pseudo Code of Parsing Algorithm
in our tournament model. The FORM, LEMMA,
GPOS(for training), PPOS(for testing, instead of
GPOS), SPLIT FORM, SPLIT LEMMA, PPOSS
in the following tokens were used as the features:
? Dependent, candidate1, candidate2
? Immediately-adjacent tokens of dependent, candidate1,
candidate2, respectively
? All tokens between dependent-candidate1, dependent-
candidate2, candidate1-candidate2, respectively
We also used the distance feature: distance (1 or
2-5 or 6+ tokens) between dependent-candidate1,
dependent-candidate2, and candidate1-candidate2.
Features corresponding to the candidates, includ-
ing the distance feature, have a prefix that indicates
its side: ?L-?(the candidate appears on left-hand-
side of the dependent) or ?R-?(appears on right-
hand-side of the dependent). Training an SVM
model with all examples is time-consuming, and
split the examples by the dependent GPOS for
training (PPOS for testing, instead of GPOS
3
) to
run SVM training in parallel. Since the number of
examples with the dependent PPOS:IN, NN, NNP
3
We cannot use GPOS for testing due to the shared task
regulation.
229
is still large, we used only first 1.5 million exam-
ples for the dependent GPOS. Note that, the algo-
rithm does not check the well-formedness of de-
pendency trees
4
.
2.2 Dependency Label Classification
This phase labels a dependency relation label to
each word in a parse tree produced in the preced-
ing phase. (McDonald et al, 2006) suggests that
edges of head x
i
and its dependents x
j1
, ..., x
jM
are highly correlated, and capturing these corre-
lation improves classification accuracy. In their
approach, edges of a head and its dependents
e
i,j1
, ..., e
i,jM
are classified sequentially, and then
Viterbi algorithm is performed to find the highest
scoring label sequence. We take a similar approach
with some simplification. In our system, each edge
is classified deterministically, and the previous de-
cision is used as a feature for the subsequent clas-
sification.
We use an online passive aggressive algorithm
(Crammer et al, 2006)
5
for dependency label clas-
sification since it converges fast, gives good per-
formance and can be implemented easily. The fea-
tures used in this phase are primarily similar to that
of (McDonald et al, 2006).
Word features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the head and the dependent.
Position: Position relation between the head and the depen-
dent (Is the head anterior to dependent?). Is the word
top of the sentence? Is the word last of the sentence?
Context features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the nearest left/right word. SPLIT LEMMA
and PPOS bigram (ww, wp, pw, pp) of the head and the
dependent (window size 5).
Sibling features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the dependent?s nearest left and right siblings
in the dependency tree.
Other features: The number of dependent?s children.
Whether the dependent and the dependent?s grand
parent SPLIT LEMMA/PPOS are the same. The
previous classification result (previous label).
2.3 Predicate Identification
This phase solves which word can be a predi-
cate. In the predicate spotting, the linear-chain
4
We tried to make a k-best cascaded model among the
modules. The latter module can check the well-formedness
of the tree. The current implementation skips this well-
formedness checking.
5
We use PA algorithm among PA, PA-I and PA-II in
(Crammer et al, 2006).
CRF (Lafferty et al, 2001) annotates whether the
word is a predicate or not. The FORM, LEMMA
(itself, and whether the LEMMA is registered in
the PropBank/NomBank frames), SPLIT FORM,
SPLIT LEMMA, PPOSS within 5 token window
size are used as the features. We also use bigram
features within 3 token window size and trigram
features within 5 token window size for FORM,
LEMMA, SPLIT FORM, SPLIT LEMMA, and
PPOSS. The main reason why we use a sequence
labeling method for predicate identification was
to relax the effect of the tagging error of PPOS
and PPOSS. However, we will show later that this
module aggravates the total performance.
2.4 Word Sense Disambiguation
For the word sense disambiguation, we use 1-
nearest neighbour method in a compressed fea-
ture space by probabilistic latent semantic index-
ing (PLSI). We trained the word sense disambigua-
tion model from the example sentences in the train-
ing/development data and PropBank/NomBank
frames. The metric in the nearest neighbour
method is based on the occurrence of LEMMA
in the example sentences. However, the exam-
ples in the PropBank/NomBank do not contain the
lemma information. To lemmatize the words in
the PropBank/NomBank, we compose a lemma-
tizer from the FORM-LEMMA table in the train-
ing and development.
6
Since the metric space
is very sparse, PLSI (Hofmann, 1999) is used to
reduce the metric space dimensions. We used
KL-divergence between two examples of P (d
i
|z
k
)
of P (d
i
, w
j
) =
?
k
P (d
i
|z
k
)P (w
j
|z
k
)P (z
k
) as
hemi-metric for the nearest neighbour method
7
,
in which d
i
? D is an example sentence in the
training/devel/test data and PropBank/NomBank
frames; w
j
? W is LEMMA; and z
k
? Z is a
latent class. We use |Z| = 100, which gave the
best performance in the development data. Note,
we transductively used the test data for the PLSI
modeling within the test run period.
2.5 Semantic Role Labeling
While semantic role labeling task is generally per-
formed by two phases: argument identification and
argument classification, we did not divide the task
6
We are not violating the closed track regulation to build
the lemmatizer. If a word in the PropBank/NomBank is not in
the training/development data, we give up lemmatization.
7
We useD
KL
=
?
k
P (d
input data
|z
k
)log
P (d
input data
|z
k
)
P (d
1-nearest data
|z
k
)
as hemi-metric. It is a non-commutative measure.
230
into the two phases. That is, argument candidates
are directly assigned a particular semantic role la-
bel. We did not employ any candidate filtering pro-
cedure, so argument candidates consist of words in
any predicate-word pair. The argument candidates
that have no roles are assigned ?NONE? label. For
the reason that described in Section 2.2 (fast con-
vergence and good performance), we use an on-
line passive aggressive algorithm for learning the
semantic role classifiers.
Useful features for argument classification of
verb and noun predicates are different. For exam-
ple, voice (active or passive) is essential for verb
predicate?s argument classification. On the other
hand, presence of a genitive word is useful for
noun predicate?s argument classification. For this
reason, we created twomodels: argument classifier
for verb predicates and that for noun predicates.
Semantic frames are useful information for se-
mantic role classification. Generally, obligatory
arguments not included in semantic frames do not
appear in actual texts. For this reason, we use
PropBank/NomBank semantic frames for seman-
tic role pruning. Suppose semantic roles in the se-
mantic frame are F
i
= {A0, A1, A2, A3}. Since
obligatory arguments are {A0...AA}, the remain-
ing arguments {A4, A5, AA} are removed from
label candidates.
For verb predicates, the features used in our sys-
tem are based on (Hacioglu, 2004). We also em-
ployed some other features proposed in (Gildea
and Jurafsky, 2002; Pradhan et al, 2004b). For
noun predicates, the features are primarily based
on (Pradhan et al, 2004a). The features that we
defined for semantic role labeling are as follows:
Word features: SPLIT LEMMA and PPOS of the predicate,
dependent and dependent?s head, and its conjunctions.
Dependency label: The dependency label between the argu-
ment candidate and the its head.
Family: The position of the argument candidate with respect
to the predicate position over the dependency tree (e.g.,
child, sibling).
Position: The position of the head of the dependency relation
with respect to the predicate position in the sentence.
Pattern: The left-to-right chain of the PPOS/dependency la-
bels of the predicate?s children.
Context features: PPOS of the nearest left/right word.
Path features: SPLIT LEMMA, PPOS and dependency la-
bel paths between the predicate and the argument can-
didate, and its path bi-gram.
Distance: The number of paths between the predicate and
the argument candidate.
Voice: Voice of the predicate (active or passive) and voice-
position conjunction (for verb predicates).
Is predicate plural: Whether the predicate is singular or
plural (for noun predicates).
Genitives between the predicate and the argument: Is
there a genitive word between the predicate and the
argument? (for noun predicates)
3 Results
Table 1 shows the result of our system. The pro-
posed method was effective in dependency pars-
ing (rank 3rd), but was not good in semantic role
labeling (rank 9th). One reason of the result of
semantic role labeling could be usages of Prop-
Bank/NomBank frames. We did not achieve the
maximum use of the resources, hence the design of
features and the choice of learning algorithm may
not be optimal.
Figure 4: Overview of the Modified System
The other reason is the design of the pipeline.
We changed the design of the pipeline after the
test run. The overview of the modified system
is illustrated in Figure 4. After the syntactic de-
pendency parsing, we limited the predicate can-
didates as verbs and nouns by PPOSS, and fil-
tered the argument candidates by Xue?s method
(Xue and Palmer, 2004). Next, the candidate pair
of predicate-argument was classified by an online
passive-aggressive algorithm as shown in Section
2.5. Finally, the word sense of the predicate is de-
termined by the module in Section 2.4. The new
result is scores with ? in Table 1. The result means
that the first design was not the best for the task.
Acknowledgements
We would like to thank the CoNLL-2008 shared
task organizers and the data providers (Surdeanu
et al, 2008).
231
Problem All WSJ Brown Rank
Complete Problem 79.10 (80.89
?
) 80.30 (82.06
?
) 69.29 (71.32
?
) 9th
Semantic Dependency 70.84 (74.53
?
) 72.37 (76.01
?
) 58.21 (62.41
?
) 9th
Semantic Role Labeling 67.92 (72.31
?
) 69.31 (73.62
?
) 56.42 (61.64
?
) -
Predicate Identification & Word Sense Disambiguation 77.20 (79.17
?
) 79.02 (80.99
?
) 62.10 (64.03
?
) -
Syntactic Dependency (Labeled) 87.18 88.06 80.17 3rd
Syntactic Label Accuracy 91.63 92.31 86.26 -
Unlabeled Syntactic Dependency Unlabeled 90.20 90.73 85.94 -
The scores with ? mark are our post-evaluation results.
Table 1: The Results ? Closed Challenge
References
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In CoNLL-2006: Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 149?164.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwarz, and Yoram Singer. 2006. Online
Passive-Agressive Algorithms. Journal of Machine
Learning Research, 7:551?585.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Hacioglu, Kadri. 2004. Semantic role labeling using
dependency trees. In COLING-2004: Proceedings
of the 20th International Conference on Computa-
tional Linguistics, pages 1273?1276.
Hofmann, Thomas. 1999. Probabilistic Latent Seman-
tic Indexing. In SIGIR-1999: Proceedings of the
22nd Annual International ACM SIGIR Conference
on Research and Development in Informatino Re-
trieval, pages 50?57.
Iida, Ryu, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003. Incorporating Contextual Cues
in Trainable Models for Coreference Resolution. In
EACL Workshop ?The Computational Treatment of
Anaphora?, pages 23?30.
Iwatate, Masakazu, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese Dependency Parsing Using
a Tournament Model. In COLING-2008: Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (To Appear).
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML-1001: Proceedings of
the Eighteenth International Conference on Machine
Learning, pages 282?289.
McDonald, Ryan, Kevin Lerman, and Fernando
Pereira. 2006. Multilingual Dependency Analysis
with a Two-Stage Discriminative Parser. In CoNLL-
2006: Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, pages 216?
220.
Nivre, Joakim, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In CoNLL-2007: Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL-
2007, pages 915?932.
Pradhan, Sameer, Honglin Sun, Wayne Ward, James H.
Martin, and Dan Jurafsky. 2004a. Parsing Argu-
ments of Nominalizations in English and Chinese. In
HLT-NAACL-2004: Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 141?144.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004b. Shal-
low Semantic Parsing Using Support Vector Ma-
chines. In HLT-NAACL-2004: Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 233?240.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL-2008:
Proceedings of the 12th Conference on Computa-
tional Natural Language Learning.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In EMNLP-
2004: Proceedings of 2004 Conference on Empirical
Methods in Natural Language Processing, pages 88?
94.
232
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 21?30,
Beijing, August 2010
Automatic Classification of Semantic Relations
between Facts and Opinions
Koji Murakami? Eric Nichols? Junta Mizuno?? Yotaro Watanabe?
Hayato Goto? Megumi Ohki? Suguru Matsuyoshi? Kentaro Inui? Yuji Matsumoto?
?Nara Institute of Science and Technology
?Tohoku University
{kmurakami,matuyosi,hayato-g,megumi-o,matsu}@is.naist.jp
{eric-n,junta-m,inui}@ecei.tohoku.ac.jp
Abstract
Classifying and identifying semantic re-
lations between facts and opinions on
the Web is of utmost importance for or-
ganizing information on the Web, how-
ever, this requires consideration of a
broader set of semantic relations than are
typically handled in Recognizing Tex-
tual Entailment (RTE), Cross-document
Structure Theory (CST), and similar
tasks. In this paper, we describe the con-
struction and evaluation of a system that
identifies and classifies semantic rela-
tions in Internet data. Our system targets
a set of semantic relations that have been
inspired by CST but that have been gen-
eralized and broadened to facilitate ap-
plication to mixed fact and opinion data
from the Internet. Our system identi-
fies these semantic relations in Japanese
Web texts using a combination of lexical,
syntactic, and semantic information and
evaluate our system against gold stan-
dard data that was manually constructed
for this task. We will release all gold
standard data used in training and eval-
uation of our system this summer.
1 Introduction
The task of organizing the information on the In-
ternet to help users find facts and opinions on
their topics of interest is increasingly important
as more people turn to the Web as a source of
important information. The vast amounts of re-
search conducted in NLP on automatic summa-
rization, opinion mining, and question answer-
ing are illustrative of the great interest in mak-
ing relevant information easier to find. Provid-
ing Internet users with thorough information re-
quires recognizing semantic relations between
both facts and opinions, however the assump-
tions made by current approaches are often in-
compatible with this goal. For example, the
existing semantic relations considered in Rec-
ognizing Textual Entailment (RTE) (Dagan et
al., 2005) are often too narrow in scope to be
directly applicable to text on the Internet, and
theories like Cross-document Structure Theory
(CST) (Radev, 2000) are only applicable to facts
or second-hand reporting of opinions rather than
relations between both.
As part of the STATEMENT MAP project we
proposed the development of a system to sup-
port information credibility analysis on the Web
(Murakami et al, 2009b) by automatically sum-
marizing facts and opinions on topics of inter-
est to users and showing them the evidence and
conflicts for each viewpoint. To facilitate the de-
tection of semantic relations in Internet data, we
defined a sentence-like unit of information called
the statement that encompasses both facts and
opinions, started compiling a corpus of state-
ments annotated with semantic relations (Mu-
rakami et al, 2009a), and begin constructing a
system to automatically identify semantic rela-
tions between statements.
In this paper, we describe the construction and
evaluation of a prototype semantic relation iden-
tification system. We build on the semantic rela-
tions proposed in RTE and CST and in our pre-
vious work, refining them into a streamlined set
of semantic relations that apply across facts and
opinions, but that are simple enough to make
automatic recognition of semantic relations be-
tween statements in Internet text possible.Our
semantic relations are [AGREEMENT], [CON-
FLICT], [CONFINEMENT], and [EVIDENCE].
[AGREEMENT] and [CONFLICT] are expansions
of the [EQUIVALENCE] and [CONTRADICTION]
21
relations used in RTE. [CONFINEMENT] and
[EVIDENCE] are new relations between facts
and opinions that are essential for understanding
how statements on a topic are inter-related.
Our task differs from opinion mining and sen-
timent analysis which largely focus on identify-
ing the polarity of an opinion for defined param-
eters rather than identify how facts and opinions
relate to each other, and it differs from web doc-
ument summarization tasks which focus on ex-
tracting information fromweb page structure and
contextual information from hyperlinks rather
than analyzing the semantics of the language on
the webpage itself.
We present a system that automatically iden-
tifies semantic relations between statements in
Japanese Internet texts. Our system uses struc-
tural alignment to identify statement pairs that
are likely to be related, then classifies seman-
tic relations using a combination of lexical, syn-
tactic, and semantic information. We evaluate
cross-statement semantic relation classification
on sentence pairs that were taken from Japanese
Internet texts on several topics and manually an-
notated with a semantic relation where one is
present. In our evaluation, we look closely at the
impact that each of the resources has on semantic
relation classification quality.
The rest of this paper is organized as follows.
In Section 2, we discuss related work in summa-
rization, semantic relation classification, opinion
mining, and sentiment analysis, showing how
existing classification schemes are insufficient
for our task. In Section 3, we introduce a set of
cross-sentential semantic relations for use in the
opinion classification needed to support informa-
tion credibility analysis on the Web. In Section
4, we present our cross-sentential semantic re-
lation recognition system, and discuss the algo-
rithms and resources that are employed. In Sec-
tion 5, we evaluate our system in a semantic rela-
tion classification task. In Section 6, we discuss
our findings and conduct error analysis. Finally,
we conclude the paper in Section 7.
2 Related Work
2.1 Recognizing Textual Entailment
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment, the
task of deciding whether the meaning of one
text is entailed from another text. A major
task in the RTE Challenge (Recognizing Tex-
tual Entailment Challenge) is classifying the se-
mantic relation between a Text (T) and a Hy-
pothesis (H) into [ENTAILMENT], [CONTRA-
DICTION], or [UNKNOWN]. Over the last sev-
eral years, several corpora annotated with thou-
sands of (T,H) pairs have been constructed for
this task. In these corpora, each pair was tagged
indicating its related task (e.g. Information Ex-
traction, Question Answering, Information Re-
trieval or Summarization).
The RTE Challenge has successfully em-
ployed a variety of techniques in order to rec-
ognize instances of textual entailment, including
methods based on: measuring the degree of lex-
ical overlap between bag of words (Glickman
et al, 2005; Jijkoun and de Rijke, 2005), the
alignment of graphs created from syntactic or se-
mantic dependencies (Marsi and Krahmer, 2005;
MacCartney et al, 2006), statistical classifiers
which leverage a wide range of features (Hickl
et al, 2005), or reference rule generation (Szpek-
tor et al, 2007). These approaches have shown
great promise in RTE for entailment pairs in the
corpus, but more robust models of recognizing
logical relations are still desirable.
The definition of contradiction in RTE is that
T contradicts H if it is very unlikely that both T
and H can be true at the same time. However, in
real documents on the Web, there are many pairs
of examples which are contradictory in part, or
where one statement confines the applicability of
another, as shown in the examples in Table 1.
2.2 Cross-document Structure Theory
Cross-document Structure Theory (CST), devel-
oped by Radev (2000), is another task of rec-
ognizing semantic relations between sentences.
CST is an expanded rhetorical structure analy-
sis based on Rhetorical Structure Theory (RST:
(William and Thompson, 1988)), and attempts
to describe the semantic relations that exist
between two or more sentences from differ-
ent source documents that are related to the
same topic, as well as those that come from
a single source document. A corpus of cross-
document sentences annotated with CST rela-
tions has also been constructed (The CSTBank
Corpus: (Radev et al, 2003)). CSTBank is
organized into clusters of topically-related ar-
ticles. There are 18 kinds of semantic rela-
tions in this corpus, not limited to [EQUIVA-
LENCE] or [CONTRADICTION], but also includ-
ing [JUDGEMENT], [ELABORATION], and [RE-
22
Query Matching sentences Output
??????????????
???
?????????????????????????????????? ??
The cavity-prevention effects are greater the more Xylitol is included. [AGREEMENT].
????????????????????????????? ??
Xylitol is effective at preventing
cavities.
Xylitol shows effectiveness at maintaining good oral hygiene and preventing cavities. [AGREEMENT]
???????????????????????????????????
?????????????
??
There are many opinions about the cavity-prevention effectiveness of Xylitol, but it
is not really effective.
[CONFLICT]
?????????
???????????????????????????????? ??
Reduced water, which has weak alkaline ions, supports the health of you and your
family.
[AGREEMENT]
?????????????????????????????????? ??
Reduced water is good for the
health.
Reduced water is said to remove active oxygen from the body, making it effective at
promoting good health.
[AGREEMENT]
??????????????????????? ??
Even if oxidized water tastes good, it does not help one?s health. [CONFLICT]
??????????????
???
???????????????????????????????????
?????????
??
Isoflavone is effective at
maintaining good health.
Taking too much soy isoflavone as a supplement will have a negative effect on one?s
health
[CONFINEMENT]
Table 1: Example semantic relation classification.
FINEMENT]. Etoh et al (Etoh and Okumura,
2005) constructed a Japanese Cross-document
Relation Corpus, and they redefined 14 kinds of
semantic relations in their corpus.
CST was designed for objective expressions
because its target data is newspaper articles re-
lated to the same topic. Facts, which can be ex-
tracted from newspaper articles, have been used
in conventional NLP research, such as Informa-
tion Extraction or Factoid Question Answering.
However, there are a lot of opinions on the Web,
and it is important to survey opinions in addition
to facts to give Internet users a comprehensive
view of the discussions on topics of interest.
2.3 Cross-document Summarization Based
on CST Relations between Sentences
Zhang and Radev (2004) attempted to classify
CST relations between sentence pairs extracted
from topically related documents. However, they
used a vector space model and tried multi-class
classification. The results were not satisfactory.
This observation may indicate that the recog-
nition methods for each relation should be de-
veloped separately. Miyabe et al (2008) at-
tempted to recognize relations that were defined
in a Japanese cross-document relation corpus
(Etoh and Okumura, 2005). However, their tar-
get relations were limited to [EQUIVALENCE]
and [TRANSITION]; other relations were not tar-
geted. Recognizing [EVIDENCE] is indispens-
able for organizing information on the Internet.
We need to develop satisfactory methods of [EV-
IDENCE] recognition.
2.4 Opinion Mining and Sentiment Analysis
Subjective statements, such as opinions, have
recently been the focus of much NLP re-
search including review analysis, opinion ex-
traction, opinion question answering, and senti-
ment analysis. In the corpus constructed in the
Multi-Perspective Question Answering (MPQA)
Project (Wiebe et al, 2005), individual expres-
sions are tagged that correspond to explicit men-
tions of private states, speech event, and expres-
sive subjective elements.
The goal of opinion mining to extract expres-
sions with polarity from texts, not to recognize
semantic relations between sentences. Sentiment
analysis also focus classifying subjective expres-
sions in texts into positive/negative classes. In
comparison, although we deal with sentiment in-
formation in text, our objective is to recognize
semantic relations between sentences. If a user?s
query requires positive/negative information, we
will also need to extract sentences including sen-
timent expression like in opinion mining, how-
ever, our semantic relation, [CONFINEMENT], is
more precise because it identifies the condition
or scope of the polarity. Queries do not neces-
sarily include sentiment information; we also ac-
cept queries that are intended to be a statement
of fact. For example, for the query ?Xylitol is
effective at preventing cavities.? in Table 1, we
extract a variety of sentences from the Web and
recognize semantic relations between the query
and many kinds of sentences.
23
3 Semantic Relations between
Statements
In this section, we define the semantic relations
that we will classify in Japanese Internet texts as
well as their corresponding relations in RTE and
CST. Our goal is to define semantic relations that
are applicable over both fact and opinions, mak-
ing them more appropriate for handling Internet
texts. See Table 1 for real examples.
3.1 [AGREEMENT]
A bi-directional relation where statements have
equivalent semantic content on a shared topic.
Here we use topic in a narrow sense to mean that
the semantic contents of both statements are rel-
evant to each other.
The following is an example of [AGREE-
MENT] on the topic of bio-ethanol environmental
impact.
(1) a. Bio-ethanol is good for the environment.
b. Bio-ethanol is a high-quality fuel, and it
has the power to deal with the environ-
ment problems that we are facing.
Once relevance has been established,
[AGREEMENT] can range from strict logi-
cal entailment or identical polarity of opinions.
Here is an example of two statements that
share a broad topic, but that are not classified as
[AGREEMENT] because preventing cavities and
tooth calcification are not intuitively relevant.
(2) a. Xylitol is effective at preventing cavities.
b. Xylitol advances tooth calcification.
3.2 [CONFLICT]
A bi-directional relation where statements have
negative or contradicting semantic content on a
shared topic. This can range from strict logical
contradiction to opposite polarity of opinions.
The next pair is a [CONFLICT] example.
(3) a. Bio-ethanol is good for our earth.
b. There is a fact that bio-ethanol further the
destruction of the environment.
3.3 [EVIDENCE]
A uni-directional relation where one statement
provides justification or supporting evidence for
the other. Both statements can be either facts or
opinions. The following is a typical example:
(4) a. I believe that applying the technology of
cloning must be controlled by law.
b. There is a need to regulate cloning, be-
cause it can be open to abuse.
The statement containing the evidence con-
sists of two parts: one part has a [AGREEMENT]
or [CONFLICT] with the other statement, the
other part provides support or justification for it.
3.4 [CONFINEMENT]
A uni-directional relation where one statement
provides more specific information about the
other or quantifies the situations in which it ap-
plies. The pair below is an example, in which
one statement gives a condition under which the
other can be true.
(5) a. Steroids have side-effects.
b. There is almost no need to worry about
side-effects when steroids are used for lo-
cal treatment.
4 Recognizing Semantic Relations
In order to organize the information on the
Internet, we need to identify the [AGREE-
MENT], [CONFLICT], [CONFINEMENT], and
[EVIDENCE] semantic relations. Because iden-
tification of [AGREEMENT] and [CONFLICT] is
a problem of measuring semantic similarity be-
tween two statements, it can be cast as a sen-
tence alignment problem and solved using an
RTE framework. The two sentences do not need
to be from the same source.
However, the identification of [CONFINE-
MENT] and [EVIDENCE] relations depend on
contextual information in the sentence. For ex-
ample, conditional statements or specific dis-
course markers like ?because? act as important
cues for their identification. Thus, to identify
these two relations across documents, we must
first identify [AGREEMENT] or [CONFLICT] be-
tween sentences in different documents and then
determine if there is a [CONFINEMENT] or [EV-
IDENCE] relation in one of the documents.
Furthermore, the surrounding text often con-
tains contextual information that is important for
identifying these two relations. Proper handling
of surrounding context requires discourse analy-
sis and is an area of future work, but our basic
detection strategy is as follows:
1. Identify a [AGREEMENT] or [CONFLICT] re-
lation between the Query and Text
2. Search the Text sentence for cues that iden-
tify [CONFINEMENT] or [EVIDENCE]
24
3. Infer the applicability of the [CONFINE-
MENT] or [EVIDENCE] relations in the Text
to the Query
4.1 System Overview
We have finished constructing a prototype sys-
tem that detects semantic relation between state-
ments. It has a three-stage architecture similar to
the RTE system of MacCartney et al (2006):
1. Linguistic analysis
2. Structural alignment
3. Feature extraction for detecting [EVIDENCE]
and [CONFINEMENT]
4. Semantic relation classification
However, we differ in the following respects.
First, our relation classification is broader than
RTE?s simple distinction between [ENTAIL-
MENT], [CONTRADICTION], and [UNKNOWN];
in place of [ENTAILMENT] and [CONTRA-
DICTION, we use broader [AGREEMENT] and
[CONFLICT] relations. We also consider cover
gradations of applicability of statements with the
[CONFINEMENT] relation.
Second, we conduct structural alignment with
the goal of aligning semantic structures. We do
this by directly incorporating dependency align-
ments and predicate-argument structure informa-
tion for both the user query and the Web text
into the alignment scoring process. This allows
us to effectively capture many long-distance
alignments that cannot be represented as lexical
alignments. This contrasts with MacCartney et
al. (2006), who uses dependency structures for
the Hypothesis to reduce the lexical alignment
search space but do not produce structural align-
ments and do not use the dependencies in detect-
ing entailment.
Finally, we apply several rich semantic re-
sources in alignment and classification: extended
modality information that helps align and clas-
sify structures that are semantically similar but
divergent in tense or polarity; and lexical simi-
larity through ontologies like WordNet.
4.2 Linguistic Analysis
In order to identify semantic relations between
the user query (Q) and the sentence extracted
from Web text (T), we first conduct syntactic and
semantic linguistic analysis to provide a basis for
alignment and relation classification.
For syntactic analysis, we use the Japanese
dependency parser CaboCha (Kudo and Mat-
!"#$%&'!"#$!%&'()*'+$! ()*!,'-!.-'/'0+1!#$)23#!+,-!$4$50*$!./!%&!6! 7!!"#$%&'!"#$!%&'()*'+$!01234!'&3$'.'-'&%&!
567!*)-%'8&!
897:;!&85#!)&!9!3-$)3/$+3!
<=>?@A/!#)&!:$$+!&#';+!
()BC-! B! C!?!
)!
5!:!J!3'!#)*$!!#$)23#!)..2%5)0'+&!
C!J!
J!
B!
B!C!
Figure 1: An example of structural alignment
sumoto, 2002) and the predicate-argument struc-
ture analyzer ChaPAS (Watanabe et al, 2010).
CaboCha splits the Japanese text into phrase-like
chunks and represents syntactic dependencies
between the chunks as edges in a graph. Cha-
PAS identifies predicate-argument structures in
the dependency graph produced by CaboCha.
We also conduct extended modality analysis
using the resources provided by Matsuyoshi et
al. (2010), focusing on tense, modality, and po-
larity, because such information provides impor-
tant clues for the recognition of semantic rela-
tions between statements.
4.3 Structural Alignment
In this section, we describe our approach to
structural alignment. The structural alignment
process is shown in Figure 1. It consists of the
following two phases:
1. lexical alignment
2. structural alignment
We developed a heuristic-based algorithm to
align chunk based on lexical similarity infor-
mation. We incorporate the following informa-
tion into an alignment confidence score that has
a range of 0.0-1.0 and align chunk whose scores
cross an empirically-determined threshold.
? surface level similarity: identical content
words or cosine similarity of chunk contents
? semantic similarity of predicate-argument
structures
predicates we check for matches in predi-
cate entailment databases (Hashimoto et
al., 2009; Matsuyoshi et al, 2008) consid-
ering the default case frames reported by
ChaPAS
arguments we check for synonym or hy-
pernym matches in the Japanese WordNet
(2008) or the Japanese hypernym collec-
tion of Sumida et al (2008)
25
>?@???????????????????AB?C????DEF)!
>?'???????????????????AB?C????GHF)!I!
T :!
H :!
(field) (in)!(agricultural chemicals) (ACC)! (use)!
(field) (on)!(agricultural chemicals) (ACC)! (spray)!
Figure 2: Determining the compatibility of se-
mantic structures
We compare the predicate-argument structure
of the query to that of the text and determine
if the argument structures are compatible. This
process is illustrated in Figure 2 where the T(ext)
?Agricultural chemicals are used in the field.? is
aligned with the H(ypothesis) ?Over the field,
agricultural chemicals are sprayed.? Although
the verbs used and sprayed are not directly se-
mantically related, they are aligned because they
share the same argument structures. This lets up
align predicates for which we lack semantic re-
sources. We use the following information to de-
termine predicate-argument alignment:
? the number of aligned children
? the number of aligned case frame arguments
? the number of possible alignments in a win-
dow of n chunk
? predicates indicating existence or quantity.
E.g. many, few, to exist, etc.
? polarity of both parent and child chunks us-
ing the resources in (Higashiyama et al,
2008; Kobayashi et al, 2005)
We treat structural alignment as a machine
learning problem and train a Support Vector Ma-
chine (SVM) model to decide if lexically aligned
chunks are semantically aligned.
We train on gold-standard labeled alignment
of 370 sentence pairs. This data set is described
in more detail in Section 5.1. As features for our
SVM model, we use the following information:
? the distance in edges in the dependency graph
between parent and child for both sentences
? the distance in chunks between parent and
child in both sentences
? binary features indicating whether each
chunk is a predicate or argument according
to ChaPAS
? the parts-of-speech of first and last word in
each chunk
? when the chunk ends with a case marker, the
case of the chunk , otherwise none
? the lexical alignment score of each chunk
pair
4.4 Feature Extraction for Detecting
Evidence and Confinement
Once the structural alignment system has iden-
tified potential [AGREEMENT] or [CONFLICT]
relations, we need to extract contextual cues in
the Text as features for detecting [CONFINE-
MENT] and [EVIDENCE] relations. Conditional
statements, degree adverbs, and partial negation,
which play a role in limiting the scope or degree
of a query?s contents in the statement, can be im-
portant cues for detecting the these two semantic
relations. We currently use a set of heuristics to
extract a set of expressions to use as features for
classifying these relations using SVM models.
4.5 Relation Classification
Once the structural alignment is successfully
identified, the task of semantic relation classi-
fication is straightforward. We also solve this
problem with machine learning by training an
SVM classifier. As features, we draw on a com-
bination of lexical, syntactic, and semantic infor-
mation including the syntactic alignments from
the previous section. The feature set is:
alignments We define two binary function,
ALIGNword(qi, tm) for the lexical align-
ment and ALIGNstruct((qi, qj), (tm, tk))
for the structural alignment to be true if and
only if the node qi, qj ? Q has been seman-
tically and structurally aligned to the node
tm, tk ? T . Q and T are the (Q)uery and the
(T)ext, respectively. We also use a separate
feature for a score representing the likelihood
of the alignment.
modality We have a feature that encodes all of
the possible polarities of a predicate node
from modality analysis, which indicates the
utterance type, and can be assertive, voli-
tional, wish, imperative, permissive, or in-
terrogative. Modalities that do not repre-
sent opinions (i.e. imperative, permissive and
interrogative) often indicate [OTHER] rela-
tions.
antonym We define a binary function
ANTONYM(qi, tm) that indicates if
the pair is identified as an antonym. This
information helps identify [CONFLICT].
26
Relation Measure 3-class Cascaded 3-class ?
[AGREEMENT] precision 0.79 (128 / 162) 0.80 (126 / 157) +0.01
[AGREEMENT] recall 0.86 (128 / 149) 0.85 (126 / 149) -0.01
[AGREEMENT] f-score 0.82 0.82 -
[CONFLICT] precision 0 (0 / 5) 0.36 (5 / 14) +0.36
[CONFLICT] recall 0 (0 / 12) 0.42 (5 / 12) +0.42
[CONFLICT] f-score 0 0.38 +0.38
[CONFINEMENT] precision 0.4 (4 / 10) 0.8 (4 / 5) +0.4
[CONFINEMENT] recall 0.17 (4 / 23) 0.17 (4 / 23) -
[CONFINEMENT] f-score 0.24 0.29 +0.05
Table 2: Semantic relation classification results comparing 3-class and cascaded 3-class approaches
negation To identify negations, we primar-
ily rely on a predicate?s Actuality value,
which represents epistemic modality and
existential negation. If a predicate pair
ALIGNword(qi, tm) has mismatching actu-
ality labels, the pair is likely a [OTHER].
contextual cues This set of features is used to
mark the presence of any contextual cues
that identify of [CONFINEMENT] or [EVI-
DENCE] relations in a chunk . For example,
??? (because)? or ??? (due to)? are typ-
ical contextual cues for [EVIDENCE], and ?
?? (when)? or ???? (if)? are typical for
[CONFINEMENT].
5 Evaluation
5.1 Data Preparation
In order to evaluate our semantic relation clas-
sification system on realistic Web data, we con-
structed a corpus of sentence pairs gathered from
a vast collection of webpages (2009a). Our basic
approach is as follows:
1. Retrieve documents related to a set number
of topics using the Tsubaki1 search engine
2. Extract real sentences that include major sub-
topic words which are detected based on
TF/IDF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate
5. Annotate corresponding sentences with
[AGREEMENT], [CONFLICT], [CONFINE-
MENT], or [OTHER]
1http://tsubaki.ixnlp.nii.ac.jp/
Although our target semantic relations in-
clude [EVIDENCE], they difficult annotate con-
sistently, so we do not annotate them at this
time. Expanding our corpus and semantic re-
lation classifier to handle [EVIDENCE] remains
and area of future work.
The data that composes our corpus comes
from a diverse number of sources. A hand sur-
vey of a random sample of the types of domains
of 100 document URLs is given below. Half of
the URL domains were not readily identifiable,
but the known URL domains included govern-
mental, corporate, and personal webpages. We
believe this distribution is representative of in-
formation sources on the Internet.
type count
academic 2
blogs 23
corporate 10
governmental 4
news 5
press releases 4
q&a site 1
reference 1
other 50
We have made a partial release of our corpus
of sentence pairs manually annotated with the
correct semantic relations2. We will fully release
all the data annotated semantic relations and with
gold standard alignments at a future date.
5.2 Experiment Settings
In this section, we present results of empiri-
cal evaluation of our proposed semantic rela-
tion classification system on the dataset we con-
structed in the previous section. For this experi-
ment, we use SVMs as described in Section 4.5
2http://stmap.naist.jp/corpus/ja/
index.html (in Japanese)
27
to classify semantic relations into one of the four
classes: [AGREEMENT], [CONFLICT], [CON-
FINEMENT], or [OTHER] in the case of no re-
lation. As data we use 370 sentence pairs that
have been manually annotated both with the cor-
rect semantic relation and with gold standard
alignments. Annotations are checked by two na-
tive speakers of Japanese, and any sentence pair
where annotation agreement is not reached is
discarded. Because we have limited data that is
annotated with correct alignments and semantic
relations, we perform five-fold cross validation,
training both the structural aligner and semantic
relation classifier on 296 sentence pairs and eval-
uating on the held out 74 sentence pairs. The
figures presented in the next section are for the
combined results on all 370 sentence pairs.
5.3 Results
We compare two different approaches to classi-
fication using SVMs:
3-class semantic relations are directly classified
into one of [AGREEMENT], [CONFLICT],
and [CONFINEMENT] with all features de-
scribed in 4.5
cascaded 3-class semantic relations are first
classified into one of [AGREEMENT], [CON-
FLICT] without contextual cue features. Then
an additional judgement with all features de-
termines if [AGREEMENT] and [CONFLICT]
should be reclassified as [CONFINEMENT]
Initial results using the 3-class classifica-
tion model produced high f-scores for [AGREE-
MENT] but unfavorable results for [CONFLICT]
and [CONFINEMENT]. We significantly im-
proved classification of [CONFLICT] and [CON-
FINEMENT] by adopting the cascaded 3-class
model. We present these results in Table 2 and
successfully recognized examples in Table 1.
6 Discussion and Error Analysis
We constructed a prototype semantic relation
classification system by combining the compo-
nents described in the previous section. While
the system developed is not domain-specific and
capable of accepting queries on any topic, we
evaluate its semantic relation classification on
several queries that are representative of our
training data.
Figure 3 shows a snapshot of the semantic re-
lation classification system and the various se-
mantic relations it recognized for the query.
Baseline Structural Upper-boundAlignment
Precision 0.44 0.52 0.74(56/126) (96/186) (135/183)
Recall 0.30 0.52 0.73(56/184) (96/184) (135/184)
F1-score 0.36 0.52 0.74
Table 3: Comparison of lexical, structural, and
upper-bound alignments on semantic relation
classification
In the example (6), recognized as [CONFINE-
MENT] in Figure 3, our system correctly identi-
fied negation and analyzed the description ?Xyl-
itol alone can not completely? as playing a role
of requirement.
(6) a. ?????????????????
(Xylitol is effective at preventing cavi-
ties.)
b. ?????????????????
????
(Xylitol alone can not completely prevent
cavities.)
Our system correctly identifies [AGREE-
MENT] relations in other examples about re-
duced water from Table 1 by structurally align-
ing phrases like ?promoting good health? and
?supports the health? to ?good for the health.?
These examples show how resources like
(Matsuyoshi et al, 2010) and WordNet (Bond et
al., 2008) have contributed to the relation clas-
sification improvement of structural alignment
over them baseline in Table 3. Focusing on sim-
ilarity of syntactic and semantic structures gives
our alignment method greater flexibility.
However, there are still various examples
which the system cannot recognized correctly.
In examples on cavity prevention, the phrase
?effective at preventing cavities? could not be
aligned with ?can prevent cavities? or ?good for
cavity prevention,? nor can ?cavity prevention?
and ?cavity-causing bacteria control.?
The above examples illustrate the importance
of the role played by the alignment phase in the
whole system?s performance.
Table 3 compares the semantic relation classi-
fication performance of using lexical alignment
only (as the baseline), lexical alignment and
structural alignment, and, to calculate the maxi-
mum possible precision, classification using cor-
rect alignment data (the upper-bound). We can
28
Figure 3: Alignment and classification example for the query ?Xylitol is effective at preventing
cavities.?
see that structural alignment makes it possible to
align more words than lexical alignment alone,
leading to an improvement in semantic relation
classification. However, there is still a large gap
between the performance of structural alignment
and the maximum possible precision. Error anal-
ysis shows that a big cause of incorrect classifi-
cation is incorrect lexical alignment. Improving
lexical alignment is a serious problem that must
be addressed. This entails expanding our cur-
rent lexical resources and finding more effective
methods of apply them in alignment.
The most serious problem we currently face is
the feature engineering necessary to find the op-
timal way of applying structural alignments or
other semantic information to semantic relation
classification. We need to conduct a quantita-
tive evaluation of our current classification mod-
els and find ways to improve them.
7 Conclusion
Classifying and identifying semantic relations
between facts and opinions on the Web is of ut-
most importance to organizing information on
the Web, however, this requires consideration of
a broader set of semantic relations than are typi-
cally handled in RTE, CST, and similar tasks. In
this paper, we introduced a set of cross-sentential
semantic relations specifically designed for this
task that apply over both facts and opinions. We
presented a system that identifies these semantic
relations in Japanese Web texts using a combina-
tion of lexical, syntactic, and semantic informa-
tion and evaluated our system against data that
was manually constructed for this task. Prelimi-
nary evaluation showed that we are able to detect
[AGREEMENT] with high levels of confidence.
Our method also shows promise in [CONFLICT]
and [CONFINEMENT] detection. We also dis-
cussed some of the technical issues that need to
be solved in order to identify [CONFLICT] and
[CONFINEMENT].
Acknowledgments
This work is supported by the National Institute
of Information and Communications Technology
Japan.
References
Bond, Francis, Hitoshi Isahara, Kyoko Kanzaki, and
Kiyotaka Uchimoto. 2008. Boot-strapping a
wordnet using multiple existing wordnets. In Proc.
of the 6th International Language Resources and
Evaluation (LREC?08).
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proc. of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Etoh, Junji and Manabu Okumura. 2005. Cross-
document relationship between sentences corpus.
29
In Proc. of the 14th Annual Meeting of the Associa-
tion for Natural Language Processing, pages 482?
485. (in Japanese).
Glickman, Oren, Ido Dagan, and Moshe Koppel.
2005. Web based textual entailment. In Proc. of
the First PASCAL Recognizing Textual Entailment
Workshop.
Hashimoto, Chikara, Kentaro Torisawa, Kow
Kuroda, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition
from the web. In Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP2009), pages 1172?1181.
Hickl, Andrew, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2005. Recog-
nizing textual entailment with lcc?s groundhog sys-
tem. In Proc. of the Second PASCAL Challenges
Workshop.
Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring noun polarity knowl-
edge using selectional preferences. In Proc. of the
14th Annual Meeting of the Association for Natu-
ral Language Processing.
Jijkoun, Valentin and Maarten de Rijke. 2005. Rec-
ognizing textual entailment using lexical similar-
ity. In Proc. of the First PASCAL Challenges Work-
shop.
Kobayashi, Nozomi, Kentaro Inui, Yuji Matsumoto,
Kenji Tateishi, and Toshikazu Fukushima. 2005.
Collecting evaluative expressions for opinion ex-
traction. Journal of natural language processing,
12(3):203?222.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc of CoNLL 2002, pages 63?69.
MacCartney, Bill, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D.
Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proc. of
HLT/NAACL 2006.
Marsi, Erwin and Emiel Krahmer. 2005. Classifi-
cation of semantic relations by humans and ma-
chines. In Proc. of ACL-05 Workshop on Empiri-
cal Modeling of Semantic Equivalence and Entail-
ment, pages 1?6.
Matsuyoshi, Suguru, Koji Murakami, Yuji Mat-
sumoto, and Kentaro Inui. 2008. A database of re-
lations between predicate argument structures for
recognizing textual entailment and contradiction.
In Proc. of the Second International Symposium
on Universal Communication, pages 366?373, De-
cember.
Matsuyoshi, Suguru, Megumi Eguchi, Chitose Sao,
Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2010. Annotating event mentions in text
with modality, focus, and source information. In
Proc. of the 7th International Language Resources
and Evaluation (LREC?10), pages 1456?1463.
Miyabe, Yasunari, Hiroya Takamura, and Manabu
Okumura. 2008. Identifying cross-document re-
lations between sentences. In Proc. of the 3rd In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-08), pages 141?148.
Murakami, Koji, Shouko Masuda, Suguru Mat-
suyoshi, Eric Nichols, Kentaro Inui, and Yuji Mat-
sumoto. 2009a. Annotating semantic relations
combining facts and opinions. In Proceedings of
the Third Linguistic Annotation Workshop, pages
150?153, Suntec, Singapore, August. Association
for Computational Linguistics.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui, and
Yuji Matsumoto. 2009b. Statement map: Assist-
ing information credibility analysis by visualizing
arguments. In Proc. of the 3rd ACM Workshop
on Information Credibility on the Web (WICOW
2009), pages 43?50.
Radev, Dragomir, Jahna Otterbacher,
and Zhu Zhang. 2003. CSTBank:
Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Radev, Dragomir R. 2000. Common theory of infor-
mation fusion from multiple text sources step one:
Cross-document structure. In Proc. of the 1st SIG-
dial workshop on Discourse and dialogue, pages
74?83.
Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in wikipedia. In Proc. of the 6th International
Language Resources and Evaluation (LREC?08).
Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proc. of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
456?463.
Watanabe, Yotaro, Masayuki Asahara, and Yuji Mat-
sumoto. 2010. A structured model for joint learn-
ing of argument roles and predicate senses. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (to appear).
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2-3):165?210.
William, Mann and Sandra Thompson. 1988.
Rhetorical structure theory: towards a functional
theory of text organization. Text, 8(3):243?281.
Zhang, Zhu and Dragomir Radev. 2004. Combin-
ing labeled and unlabeled data for learning cross-
document structural relationships. In Proc. of the
Proceedings of IJC-NLP.
30
