Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 343?350, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Statistical Machine Translation Performance by         
Training Data Selection and Optimization 
Yajuan L?, Jin Huang and Qun Liu 
Key Laboratory of Intelligent Information Processing 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100080, China 
{lvyajuan, huangjin,liuqun}@ict.ac.cn 
 
Abstract 
Parallel corpus is an indispensable resource 
for translation model training in statistical 
machine translation (SMT). Instead of col-
lecting more and more parallel training 
corpora, this paper aims to improve SMT 
performance by exploiting full potential of 
the existing parallel corpora. Two kinds of 
methods are proposed: offline data optimi-
zation and online model optimization. The 
offline method adapts the training data by 
redistributing the weight of each training 
sentence pairs. The online method adapts 
the translation model by redistributing the 
weight of each predefined submodels. In-
formation retrieval model is used for the 
weighting scheme in both  methods. Ex-
perimental results show that without using 
any additional resource, both methods can 
improve SMT performance significantly.   
1 Introduction 
Statistical machine translation relies heavily on the 
available training data.  Typically, the more data is 
used to estimate the parameters of the translation 
model, the better it can approximate the true trans-
lation probabilities, which will obviously lead to a 
higher translation performance. However, large 
corpora are not easily available. The collected cor-
pora are usually from very different areas. For 
example, the parallel corpora provided by LDC 
come from quite different domains, such as 
Hongkong laws, Hangkong Hansards and 
Hongkong news. This results in the problem that a 
translation system trained on data from a particular 
domain(e.g. Hongkong Hansards) will perform 
poorly when translating text from a different 
domain(e.g. news articles). Our experiments also 
show that simply putting all these domain specific 
corpora together will not always improve 
translation quality. From another aspect, larger 
amount of training data also requires larger 
computational resources. With the increasing of 
training data, the improvement of translation 
quality will become smaller and smaller. Therefore, 
while keeping collecting more and more parallel 
corpora, it is also important to seek effective ways 
of making better use of available parallel training 
data.  
There are two cases when we train a SMT 
system. In one case, we know the target test set or 
target test domain, for example, when building a 
specific domain SMT system or when participating 
the NIST MT evaluation1. In the other case, we are 
unaware of any information of the testing data. 
This paper presents two methods to exploit full 
potential of the available parallel corpora in the 
two cases. For the first case, we try to optimize the 
training data offline to make it match the test data 
better in domain, topic and style, thus improving 
the translation performance. For the second case, 
we first divide the training data into several do-
mains and train submodels for each domain. Then, 
in the translation process, we try to optimize the 
predefined models according to the online input 
source sentence. Information retrieval model is 
used for similar sentences retrieval in both meth-
ods. Our preliminary experiments show that both 
methods can improve SMT performance without 
using any additional data.  
                                                 
1 http://www.nist.gov/speech/tests/mt/ 
343
The remainder of this paper is organized as fol-
lows: Section 2 describes the offline data selection 
and optimization method. Section 3 describes the 
online model optimization method. The evaluation 
and discussion are given in section 4. Related work 
is introduced before concluding.  
2 Offline training data optimization 
In offline training data optimization, we assume 
that the target test data or target test domain is 
known before building the translation model. We 
first select sentences similar to the test text using 
information retrieval method to construct a small 
and adapted training data. Then the extracted simi-
lar subset is used to optimize the distribution of the 
whole training data. The adapted and the optimized 
training data will be used to train new translation 
models.  
2.1 Similar data selection using TF-IDF 
We use information retrieval method for similar 
data retrieval. The standard TF-IDF (Term Fre-
quency and Inverse Document Frequency) term 
weighting scheme is used to measure the similarity 
between the test sentence and the training sentence. 
TF-IDF is a similarity measure widely used in in-
formation retrieval. Each document i is represented 
as a vector  ,  is the size of the 
vocabulary.  is calculate as follows: 
D
),...,,( 21 inii www n
ijw
)log( jijij idftfw ?=  
where,  
ij  is the term frequency(TF) of the j-th word 
in the vocabulary in the document , i.e. the 
number of occurrences;  
tf
iD
j  is the inverse document frequency(IDF) 
of the j-th word calculated as below: 
idf
    
th term-j  #
#
containingdocuments
documentsidf j = . 
The similarity between two documents is then 
defined as the cosine of the angle between the two 
vectors. 
We perform information retrieval using the Le-
mur toolkit2. The source language part of the par-
allel training data is used as the document collec-
tion. Each sentence represents one document. Each 
sentence from the test data or test domain is used 
as one separate query. In the sentence retrieval 
                                                 
2 http://www.cs.cmu.edu/~lemur/
process, both the query and the document are con-
verted into vectors by assigning a term weight to 
each word. Then the cosine similarity is calculated 
proportional to the inner product of the two vectors. 
All retrieved sentences are ranked according to 
their similarity with the query. We pair each of the 
retrieved sentences with the corresponding target 
part and the top N most similar sentences pairs are 
put together to form an adapted parallel data. N 
ranges from one to several thousand in our experi-
ments. Since Lemur toolkit gives the similarity 
score for each retrieved sentences, it is also possi-
ble to select the most similar sentences according 
to the similarity score. 
Note that the selected similar data can contain 
duplicate sentences as the top N retrieval results 
for different test sentences can contain the same 
training sentences. The duplicate sentences will 
force the translation probability towards the more 
often seen words. Intuitively, this could help. In 
experiment section, we will compare experimental 
results by keeping or removing duplicates to see 
how the duplicate sentences affect the translations.  
The selected subset contains the similar sen-
tences with the test data or test domain. It matches 
the test data better in domain, topic and style. 
Hopefully, training translation model using this 
adapted parallel data may helpful for improving 
translation performance. In addition, the translation 
model trained using the selected subset is usually 
much smaller than that trained using the whole 
translation data. Limiting the size of translation 
model is very important for some real applications. 
Since SMT systems usually require large computa-
tion resource. The complexity of standard training 
and decoding algorithm depends mainly on the size 
of the parallel training data and the size of the 
translation model. Limiting the size of the training 
data with the similar translation performance 
would also reduce the memories and speed up the 
translations.  
In the information retrieval process, we only use 
the source language part for document indexing 
and query generating. It is easy to get source part 
of the test data. This is different from the common 
language model adaptation methods, which have to 
do at lease one pass machine translation to get the 
candidate English translation as query(Zhao 2004, 
Zhang 2006). So our method has the advantage 
that it is independent from the quality of baseline 
translation system.  
344
2.2 Training data optimization  
There are two factors on training data that influ-
ence the translation performance of SMT system: 
the scale and the quality. In some sense, we im-
prove the quality of the training data by selecting 
the similar sentence to form an adapted training set. 
However, we also reduce the scale of the training 
data at the same time. Although this is helpful for 
some small device applications, it is also possible 
to induce the data sparseness problem.  Here, we 
introduce a method to optimize between the scale 
and the quality of the training data.  
The basic idea is that we still use all the avail-
able training data; by redistributing the weight of 
each sentence pairs we adapt the whole training 
data to the test domain. In our experiments, we 
simply combine the selected small similar subset 
and the whole training data. The weights of each 
sentence pairs are changed accordingly. Figure 1 
shows the procedure of the optimization.  
 
Figure 1. Training data optimization 
As can be seen, through the optimization, the 
weight of the similar sentence pairs are increased, 
while the general sentence pairs still have an ordi-
nary weight. This make the translation model in-
clined to give higher probabilities to the adapted 
words, and at the same time avoid the data sparse-
ness problem. Since we only change the weight of 
the sentence pairs, and no new training data is in-
troduced, the translation model size trained on the 
optimized data will keep as the same as the origi-
nal one. We use GIZA++ toolkit3 for word align-
                                                 
3 http://www.fjoch.com/GIZA++.html
ment training in the training process. The input 
training file formats for GIZA++ is as follows: 
Each training sentence pair is stored in three lines. 
The first line is the number of times this sentence 
pair occurred. The second line is the source sen-
tence where each token is replaced by its unique 
integer id and the third is the target sentence in the 
same format. To deal with our optimized training 
data, we only need to change the number of sen-
tence pairs in the first line accordingly. This will 
not call for extra training time and memory for the 
whole training process.  
It might be beneficial to investigate other so-
phisticated weighting schemes under the similar 
idea, such as to give more precise fractional 
weights to the sentences according the retrieval 
similarity scores. 
3 Online model optimization  
In most circumstances, we don?t know exactly the 
test data or the test domain when we train a ma-
chine translation system. This results in the fact 
that the performance of the translation system 
highly depends on the training data and the test 
data it is used in. To alleviate this blindfold status 
and maximize the potential of the available train-
ing corpora, we propose a novel online model op-
timization method.  
     The basic idea is that: several candidate transla-
tion models are prepared in training stage. In par-
ticularly, a general model is also prepared. Then, in 
the translation process, the similarity between the 
input sentence and the predefined models is calcu-
lated online to get the weights of each model. The 
optimized model is used to translate the input sen-
tence.  
There are two problems in the method: how to 
prepare submodels in training process and how to 
optimize the model weight online in translation 
process.  
3.1 Prepare the submodels  
There are several ways to prepare submodels in 
training process. If the training data comes from 
very different sources, we can divide the data ac-
cording to its origins. Otherwise, we can use clus-
tering method to separate the training corpus into 
several classes. In addition, our offline data adapta-
tion method can also be used for submodel prepa-
ration. For each candidate domain, we can use the 
345
source side of a small corpus as queries to extract a 
domain specific training set. In this case, a sen-
tence pair in the training data may occur in several 
sub training data, but this doesn?t matter. The gen-
eral model is used when the online input is not 
similar to any prepared submodels. We can use all 
available training data to train the general model 
since generally larger data can get better model 
even there are some noises.   
3.2 Online model weighting 
We also use TF-IDF information retrieval method 
for online model weighting. The procedure is as 
follows: 
For each input sentence: 
 1. Do IR on training data collection, using the 
input sentence as query.  
 2. Determine the weights of submodels accord-
ing to the retrieved sentences.  
 3. Use the optimized model to translate the sen-
tence.  
The information retrieval process is the same as 
the offline data selection except that each retrieved 
sentence is attached with the sub-corpus informa-
tion, i.e. it belongs to which sub-models in the 
training process.   
With the sub-corpus information, we can calcu-
late the weights of submodels. We get the top N 
most similar sentences, and then calculate propor-
tions of each submodel?s sentences. The proportion 
can be calculated use the count of the sentences or 
the similarity score of the sentences. The weight of 
each submodel can be determined according to the 
proportions.  
Our optimized model is the log linear interpola-
tion of the sub-models as follows: 
?
=
?=
M
i
i
icepcepcep
1
0 )|()|()|(? 0
??  
?
=
+=
M
i
ii
e
cepcepe
1
00 )))|(log())|(log((maxarg? ??
 
where, 0 is the probability of general model, ip is 
the probability of submodel i. 0
p
? is the weight of 
general model. i? is the weight of submodel i. Each 
model i is also implemented using log linear model in 
our SMT system. So after the log operation, the sub-
models are interpolated linearly.  
In our experiments, the interpolation factor i?  is 
determined using the following four simple weight-
ing schemes:   
Weighting scheme 1:  
  ;0     ;1     ;00 === ?max_modelimax_model ???  
Weighting scheme 2:  
      if  Proportion(max_model) > 0.5 
          Use weighting scheme1; 
     else 
  ;0    ;1     0 == i??  
Weighting scheme 3:  
   
);(Proportion 
;00
ii model=
=
?
?
 
Weighting scheme 4:  
       if  Proportion(max_model) > 0.5 
           Use weighting scheme3; 
       else 
 );( Proportion5.0     
  ;5.0     0
ii model?=
=
?
?  
where, modeli is the i-th submodel, . 
Proportion (model
)...1( Mi =
i) is the proportion of modeli in 
the retrieved results. We use count for proportion 
calculation. max_model is the submodel with the 
max proportion score. 
The training and translation procedure of online 
model optimization is illustrated in Figure 2. 
Figure 2. Online model optimization 
346
      The online model optimization method makes 
it possible to select suitable models for each indi-
vidual test sentence. Since the IR process is done 
on a fixed training data, the size of the index data 
is quite small compared with the web IR. The IR 
process will not take much time in the translation.  
4 Experiments and evaluation 
4.1 Experimental setting 
We conduct our experiments on Chinese-to-
English translation tasks. The baseline system is a 
variant of the phrase-base SMT system, imple-
mented using log-linear translation model (He et al 
2006). The baseline SMT system is used in all ex-
periments. The only difference between them is 
that they are trained on different parallel training 
data.  
In training process, we use GIZA++4 toolkit for 
word alignment in both translation directions, and 
apply ?grow-diag-final? method to refine it (Koehn 
et al, 2003). We change the preprocess part of 
GIZA++ toolkit to make it accept the weighted 
training data. Then we use the same criterion as 
suggested in (Zens et al, 2002) to do phrase ex-
traction. For the log-linear model training, we take 
minimum-error-rate training method as described 
in (Och, 2003). The language model is trained us-
ing Xinhua portion of Gigaword with about 190M 
words. SRI Language Modeling toolkit5 is used to 
train a 4-gram model with modified Kneser-Ney 
smoothing(Chen and Goodman, 1998). All ex-
periments use the same language model. This en-
sures that any differences in performance are 
caused only by differences in the parallel training 
data. 
Our training data are from three LDC corpora as 
shown in Table 1. We random select 200,000 sen-
tence pairs from each corpus and combine them 
together as the baseline corpus, which includes 
16M Chinese words and 19M English words in 
total. This is the usual case when we train a SMT 
system, i.e. we simply combine all corpora from 
different origins to get a larger training corpus. 
We use the 2002 NIST MT evaluation test data 
as our development set, and the 2005 NIST MT 
test data as the test set in offline data optimization 
experiments. In both data, each sentence has four 
                                                 
4 http://www.fjoch.com/GIZA++.html
5 http://www.speech.sri.com/projects/srilm/
human translations as references. The translation 
quality is evaluated by BLEU metric (Papineni et 
al., 2002), as calculated by mteval-v11b.pl6 with 
case-sensitive matching of n-grams. 
Corpus LDC No. Description # sent. pairs
FBIS LDC2003E14 FBIS Multilanguage Texts 200000 
HK_Hansards LDC2004T08 Hong Kong Hansards Text 200000 
HK_News LDC2004T08 Hong Kong News Text 200000 
Baseline - All above data 600000 
Table 1. Training corpora 
4.2 Baseline experiments 
We first train translation models on each sub train-
ing corpus and the baseline corpus. The develop-
ment set is used to tune the feature weights. The 
results on test set are shown in Table 2.   
System BLEU on dev set BLEU on test set 
FBIS 0.2614 0.2331 
HK_Hansards 0.1679 0.1624 
HK_News 0.1748 0.1608  
Baseline 0.2565 0.2363 
Table 2. Baseline results 
From the results we can see that although the 
size of each sub training corpus is similar, the 
translation results from the corresponding system 
are quite different on the same test set. It seems 
that the FBIS corpus is much similar to the test set 
than the other two corpora.  In fact, it is the case. 
The FBIS contains text mainly from China 
mainland news stories, while the 2005 NIST test 
set alo include lots of China news text. The results 
illustrate the importance of selecting suitable train-
ing data.  
When combining all the sub corpora together, 
the baseline system gets a little better result than 
the sub systems. This indicates that larger data is 
useful even it includes some noise data. However, 
compared with the FBIS corpus, the baseline cor-
pus contains three times larger data, while the im-
provement of translation result is not significant. 
This indicates that simply putting different corpora 
together is not a good way to make use of the 
available corpora.  
                                                 
6http://www.nist.gov/speech/tests/mt/resources/scoring.htm  
347
4.3 Offline data optimization experiments 
We use baseline corpus as initial training corpus, 
and take Lemur toolkit to build document index on 
Chinese part of the corpus. The Chinese sentences 
in development set and test set are used as queries. 
For each query, N = 100, 200, 500, 1000, 2000 
similar sentences are retrieved from the indexed 
collection. The extracted similar sentence pairs are 
used to train the new adapted translation models. 
Table 3 illustrates the results. We give the distinct 
pair numbers for each adapted set and compare the 
size of the translation models. To illustrate the ef-
fect of duplicate sentences, we also give the results 
with duplicates and without duplicates (distinct). 
System Distinct pairs 
Size of 
trans model 
BLEU on 
duplicates
BLEU on 
distinct
Baseline 600000 2.41G 0.2363 0.2363 
Top100 91804 0.43G 0.2306 0.2346 
Top200 150619 0.73G 0.2360 0.2345 
Top500 261003 1.28G 0.2415 0.2370 
Top1000 357337 1.74G 0.2463 0.2376 
Top2000 445890 2.11G 0.2351 0.2346 
Table 3. Offline data adaptation results 
The results show that: 
1. By using similar data selection, it is possible 
to use much smaller training data to get compara-
ble or even better results than the baseline system. 
When N=200, using only 1/4 of the training data 
and 1/3 of the model size, the adapted translation 
model achieves comparable result with the baseline 
model. When N=500, the adapted model outper-
forms the baseline model with much less training 
data. The results indicate that relevant data is better 
data. The method is particular useful for SMT ap-
plications on small device.  
2. In general, using duplicate data achieves bet-
ter results than using distinct data. This justifies 
our idea that give a higher weight to more similar 
data will benefit.  
3. With the increase of training data size,   the 
translation performance tends to improve also. 
However, when the size of corpus achieves a cer-
tain scale, the performance may drop. This maybe 
because that with the increase of the data, noisy 
data may also be included. More and more in-
cluded noises may destroy the data. It is necessary 
to use a development set to determine an optimal 
size of N. 
We combine each adapted data with the baseline 
corpus to get the optimized models. The results are 
shown in Table 4. We also compare the adapted 
models (TopN) and the optimized models (TopN+) 
in the table.   
Without using any additional data, the optimized 
models achieve significant better results than the 
baseline model by redistributing the weight of 
training sentences. The optimized models also out-
perform adapted models when the size of the 
adapted data is small since they make use of all the 
available data which decrease the influence of data 
sparseness. However, with the increase of the 
adapted data, the performance of optimized models 
is similar to that of the adapted models.  
System Distinct pairs 
BLEU on 
TopN 
BLEU on 
TopN+ 
Baseline 600000 0.2363 0.2363 
Top100+ 600000 0.2306 0.2387 
Top200+ 600000 0.2360 0.2443 
Top500+ 600000 0.2415 0.2461 
Top1000+ 600000 0.2463 0.2431 
Top2000+ 600000 0.2351 0.2355 
Table 4. Offline data optimization results 
4.4 Online model optimization experiments 
Since 2005 NIST MT test data tends bias to FBIS 
corpus too much, we build a new test set to evalu-
ate the online model optimization method. We ran-
domly select 500 sentences from extra part of FBIS, 
HK_Hansards and HK_News corpus respectively 
(i.e the selected 1500 test sentences are not in-
cluded in any of the training set). The correspond-
ing English part is used as translation reference. 
Note that there is only one reference for each test 
sentence. We also include top 500 sentence and 
their first reference translation of 2005 NIST MT 
test data in the new test set. So in total, the new test 
contains 2000 test sentences with one translation 
reference for each sentence.  The test set is used to 
simulate SMT system?s online inputs which may 
come from various domains.   
The baseline translation results are shown in Ta-
ble 5. We also give results on each sub test set (de-
notes as Xcorpus_part). Please note that the abso-
lute BLEU scores are not comparable to the previ-
ous experiments since there is only one reference 
in this test set.  
348
As expected, using the same domain data for 
training and testing achieves the best results as in-
dicate by bold fonts.  The results demonstrate 
again that relevant data is better data.  
To test our online model optimization method, 
we divide the baseline corpus according to the ori-
gins of sub corpus. That is, the FBIS, HK_ Han-
sards and HK_News models are used as three sub-
models and the baseline model is used as general 
model. The four weighting schemes described in 
section 3.2 are used as online weighting schemes 
individually. The experimental results are shown in 
Table 6. S_i indicates the system using weighting 
scheme i.   
      System 
Test data FBIS 
HK_ 
Hansards 
HK_
News Baseline
FBIS-part 0.1096 0.0687 0.0622 0.1030
HK_Hans_part 0.0726 0.0918 0.0846 0.0897
HK_News_part 0.0664 0.0801 0.0936 0.0870
MT05_part 0.1130 0.0805 0.0776 0.1116
Whole test set 0.0937 0.0799 0.0781 0.0993
Table 5. Baseline results on new test set 
      System 
Test data S_1 S_2 S_3 S_4 
FBIS-part 0.1090 0.1090 0.1089 0.1089
HK_Hans_part 0.0906 0.0903 0.0902 0.0902
HK_News_part 0.0952 0.0950 0.0933 0.0934
MT05_part 0.1119 0.1123 0.1149 0.1151
Whole test set 0.1034 0.1034 0.1038 0.1038
Table 6. Online model optimization results 
Different weighting schemes don?t show signifi-
cant improvements from each other. However, all 
the four weighting schemes achieve better results 
than the baseline system. The improvements are 
shown not only on the whole test set but also on 
each part of the sub test set. The results justify the 
effectiveness of our online model optimization 
method.  
5 Related work 
Most previous research on SMT training data is 
focused on parallel data collection. Some work 
tries to acquire parallel sentences from web (Nie et 
al. 1999; Resnik and Smith 2003; Chen et al 2004). 
Others extract parallel sentences from comparable 
or non-parallel corpora (Munteanu and Marcu 
2005, 2006). These work aims to collect more 
parallel training corpora, while our work aims to 
make better use of existing parallel corpora.  
Some research has been conducted on parallel 
data selection and adaptation. Eck et al (2005) 
propose a method to select more informative sen-
tences based on n-gram coverage. They use n-
grams to estimate the importance of a sentence. 
The more previously unseen n-grams in the sen-
tence the more important the sentence is. TF-IDF 
weighting scheme is also tried in their method, but 
didn?t show improvements over n-grams. This 
method is independent of test data. Their goal is to 
decrease the amount of training data to make SMT 
system adaptable to small devices. Similar to our 
work, Hildebrand et al (2005) also use information 
retrieval method for translation model adaptation.  
They select sentences similar to the test set from 
available in-of-domain and out-of-domain training 
data to form an adapted translation model. Differ-
ent from their work, our method further use the 
small adapted data to optimize the distribution of 
the whole training data. It takes the full advantage 
of larger data and adapted data. In addition, we 
also propose an online translation model optimiza-
tion method, which make it possible to select 
adapted translation model for each individual sen-
tence. 
Since large scale monolingual corpora are easier 
to obtain than parallel corpora. There has some 
research on language model adaptation recent 
years. Zhao et al (2004) and Eck et al(2004) in-
troduce information retrieval method for language 
model adaptation. Zhang et al(2006)  and  Mauser 
et al(2006) use adapted language model for SMT 
re-ranking. Since language model is built for target 
language in SMT, one pass translation is usually 
needed to generate n-best translation candidates in 
language model adaptation. Translation model ad-
aptation doesn?t need a pre-translation procedure. 
Comparatively, it is more direct. Language model 
adaptation and translation model adaptation are 
good complement to each other. It is possible that 
combine these two adaptation approaches could 
further improve machine translation performance. 
6 Conclusion and future work 
This paper presents two new methods to im-
prove statistical machine translation performance 
by making better use of the available parallel train-
ing corpora. The offline data selection method 
349
adapts the training corpora to the test domain by 
retrieving similar sentence pairs and redistributing 
their weight in the training data. Experimental re-
sults show that the selected small subset achieves 
comparable or even better performance than the 
baseline system with much less training data. The 
optimized training data can further improve trans-
lation performance without using any additional 
resource. The online model optimization method 
adapts the translation model to the online test 
sentence by redistributing the weight of each 
predefined submodels. Preliminary results show 
the effectiveness of the method. Our work also 
demonstrates that in addition to larger training data, 
more relevant training data is also important for 
SMT model training. 
In future work, we will improve our methods in 
several aspects. Currently, the similar sentence re-
trieval model and the weighting schemes are very 
simple. It might work better by trying other sophis-
ticated similarity measure models or using some 
optimization algorithms to determine submodel?s 
weights. Introducing language model optimization 
into our system might further improve translation 
performance.  
Acknowledgement 
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095 
and 60573188. 
References 
Jisong Chen, Rowena Chau, Chung-Hsing Yeh 2004. 
Discovering Parallel Text from the World Wide Web. 
ACSW Frontiers 2004: 157-161  
Stanley F. Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language 
Modeling. Technical Report TR-10-98, Harvard Uni-
versity Center for Research in Computing Technol-
ogy. 
Matthias Eck, Stephan Vogel, and Alex Waibel 2004. 
Language Model Adaptation for Statistical Machine 
Translation Based on Information Retrieval. Proceed-
ings of Fourth International Conference on Language 
Resources and Evaluation:327-330 
Matthias Eck, Stephan Vogel,  Alex Waibel 2005. Low 
cost portability for statistical machine translation 
based on n-gram coverage. MT Summit X: 227-234. 
Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, and 
Qun Liu 2006. ICT System Description for the 2006 
TC-STAR Run#2 SLT Evaluation. Proceedings of TC-
STAR Workshop on Speech-to-Speech Translation: 
63-68 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. Proceedings of 
HLT-NAACL 2003: 127?133. 
Arne Mauser, Richard Zens, Evgeny Matusov, Sasa 
Hasan, Hermann Ney 2006. The RWTH Statistical 
Machine Translation System for the IWSLT 2006 
Evaluation. Proceedings of International Workshop 
on Spoken Language Translation.:103-110 
Dragos Stefan Munteanu and Daniel Marcu 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Comparable Corpora. Computational Lin-
guistics, 31 (4): 477-504  
Dragos Stefan Munteanu and Daniel Marcu 2006. Ex-
tracting Parallel Sub-Sentential Fragments from 
Comparable Corpora. ACL-2006: 81-88 
Jian-Yun Nie, Michel Simard, Pierre Isabelle, Richard 
Durand 1999. Cross-Language Information Retrieval 
based on Parallel Texts and Automatic Mining of 
Parallel Texts in the Web. SIGIR-1999: 74-81 
Franz Josef Och 2003. Minimum Error Rate Training in 
Statistical Machine Translation. ACL-2003:160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic 
Evaluation of Machine Translation. ACL-2002: 311?
318 
Philip Resnik and Noah A. Smith 2003. The Web as a 
Parallel Corpus. Computational Linguistics 29(3): 
349-380 
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, 
and Alex Waibel 2005. Adaptation of the Translation 
Model for Statistical Machine Translation based on 
Information Retrieval. Proceedings of EAMT 2005: 
133-142. 
Richard Zens, Franz Josef Och, Hermann Ney 2002. 
Phrase-Based Statistical Machine Translation. An-
nual German Conference on AI, KI 2002, Vol. LNAI 
2479: 18-32 
Ying Zhang, Almut Silja Hildebrand, Stephan Vogel 
2006. Distributed Language Modeling for N-best List 
Re-ranking.  EMNLP-2006:216-223 
Bing Zhao, Matthias Eck, Stephan Vogel 2004. Lan-
guage Model Adaptation for Statistical Machine 
Translation with structured query models. COLING- 
2004 
350
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1105?1113,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Lattice-based System Combination for Statistical Machine Translation
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{fengyang, yliu, htmi, liuqun, lvyajuan}@ict.ac.cn
Abstract
Current system combination methods usu-
ally use confusion networks to find consensus
translations among different systems. Requir-
ing one-to-one mappings between the words
in candidate translations, confusion networks
have difficulty in handling more general situa-
tions in which several words are connected to
another several words. Instead, we propose a
lattice-based system combination model that
allows for such phrase alignments and uses
lattices to encode all candidate translations.
Experiments show that our approach achieves
significant improvements over the state-of-
the-art baseline system on Chinese-to-English
translation test sets.
1 Introduction
System combination aims to find consensus transla-
tions among different machine translation systems.
It has been proven that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
In recent several years, the system combination
methods based on confusion networks developed
rapidly (Bangalore et al, 2001; Matusov et al, 2006;
Sim et al, 2007; Rosti et al, 2007a; Rosti et al,
2007b; Rosti et al, 2008; He et al, 2008), which
show state-of-the-art performance in benchmarks. A
confusion network consists of a sequence of sets of
candidate words. Each candidate word is associated
with a score. The optimal consensus translation can
be obtained by selecting one word from each set to
maximizing the overall score.
To construct a confusion network, one first need
to choose one of the hypotheses (i.e., candidate
translations) as the backbone (also called ?skeleton?
in the literature) and then decide the word align-
ments of other hypotheses to the backbone. Hy-
pothesis alignment plays a crucial role in confusion-
network-based system combination because it has a
direct effect on selecting consensus translations.
However, a confusion network is restricted in
such a way that only 1-to-1 mappings are allowed
in hypothesis alignment. This is not the fact even
for word alignments between the same languages. It
is more common that several words are connected
to another several words. For example, ?be capa-
ble of? and ?be able to? have the same meaning.
Although confusion-network-based approaches re-
sort to inserting null words to alleviate this problem,
they face the risk of producing degenerate transla-
tions such as ?be capable to? and ?be able of?.
In this paper, we propose a new system combina-
tion method based on lattices. As a more general
form of confusion network, a lattice is capable of
describing arbitrary mappings in hypothesis align-
ment. In a lattice, each edge is associated with a
sequence of words rather than a single word. There-
fore, we select phrases instead of words in each
candidate set and minimize the chance to produce
unexpected translations such as ?be capable to?.
We compared our approach with the state-of-the-art
confusion-network-based system (He et al, 2008)
and achieved a significant absolute improvement of
1.23 BLEU points on the NIST 2005 Chinese-to-
English test set and 0.93 BLEU point on the NIST
2008 Chinese-to-English test set.
1105
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(a) unidirectional alignments
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(b) bidirectional alignments
He feels like ? apples
? prefer of
is fond
(c) confusion network
he feels like apples
? prefer
is fond of
(d) lattice
Figure 1: Comparison of a confusion network and a lat-
tice.
2 Background
2.1 Confusion Network and Lattice
We use an example shown in Figure 1 to illustrate
our idea. Suppose that there are three hypotheses:
He feels like apples
He prefer apples
He is fond of apples
We choose the first sentence as the backbone.
Then, we perform hypothesis alignment to build a
confusion network, as shown in Figure 1(a). Note
that although ?feels like? has the same meaning with
?is fond of?, a confusion network only allows for
one-to-one mappings. In the confusion network
shown in Figure 1(c), several null words ? are in-
serted to ensure that each hypothesis has the same
length. As each edge in the confusion network only
has a single word, it is possible to produce inappro-
priate translations such as ?He is like of apples?.
In contrast, we allow many-to-many mappings
in the hypothesis alignment shown in Figure 2(b).
For example, ?like? is aligned to three words: ?is?,
?fond?, and ?of?. Then, we use a lattice shown in
Figure 1(d) to represent all possible candidate trans-
lations. Note that the phrase ?is fond of? is attached
to an edge. Now, it is unlikely to obtain a translation
like ?He is like of apples?.
A lattice G = ?V,E? is a directed acyclic graph,
formally a weighted finite state automation (FSA),
where V is the set of nodes and E is the set of edges.
The nodes in a lattice are usually labeled according
to an appropriate numbering to reflect how to pro-
duce a translation. Each edge in a lattice is attached
with a sequence of words as well as the associated
probability.
As lattice is a more general form of confusion
network (Dyer et al, 2008), we expect that replac-
ing confusion networks with lattices will further im-
prove system combination.
2.2 IHMM-based Alignment Method
Since the candidate hypotheses are aligned us-
ing Indirect-HMM-based (IHMM-based) alignment
method (He et al, 2008) in both direction, we briefly
review the IHMM-based alignment method first.
Take the direction that the hypothesis is aligned to
the backbone as an example. The conditional prob-
ability that the hypothesis is generated by the back-
bone is given by
p(e
?
1
J
|e
I
1
) =
?
a
J
1
J
?
j=1
[p(a
j
|a
j?1
, I)p(e
?
j
|e
a
j
)]l (1)
Where eI
1
= (e
1
, ..., e
I
) is the backbone, e?J
1
=
(e
?
1
, ..., e
?
J
) is a hypothesis aligned to eI
1
, and aJ
1
=
(a
1
, .., a
J
) is the alignment that specifies the posi-
tion of backbone word that each hypothesis word is
aligned to.
The translation probability p(e?
j
|e
i
) is a linear in-
terpolation of semantic similarity p
sem
(e
?
j
|e
i
) and
surface similarity p
sur
(e
?
j
|e
i
) and ? is the interpo-
lation factor:
p(e
?
j
|e
i
) = ??p
sem
(e
?
j
|e
i
)+(1??)?p
sur
(e
?
j
|e
i
) (2)
The semantic similarity model is derived by using
the source word sequence as a hidden layer, so the
bilingual dictionary is necessary. The semantic sim-
1106
ilarity model is given by
p
sem
(e
?
j
|e
i
) =
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
, e
i
)
?
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
)
(3)
The surface similarity model is estimated by calcu-
lating the literal matching rate:
p
sur
(e
?
j
|e
i
) = exp{? ? [s(e
?
j
, e
i
)? 1]} (4)
where s(e?
j
, e
i
) is given by
s(e
?
j
, e
i
) =
M(e
?
j
, e
i
)
max(|e
?
j
|, |e
i
|)
(5)
where M(e?
j
, e
i
) is the length of the longest matched
prefix (LMP) and ? is a smoothing factor that speci-
fies the mapping.
The distortion probability p(a
j
= i|a
j?1
= i
?
, I)
is estimated by only considering the jump distance:
p(i|i
?
, I) =
c(i? i
?
)
?
I
i=1
c(l ? i
?
)
(6)
The distortion parameters c(d) are grouped into 11
buckets, c(? ?4), c(?3), ..., c(0), ..., c(5), c(? 6).
Since the alignments are in the same language, the
distortion model favor monotonic alignments and
penalize non-monotonic alignments. It is given in
a intuitive way
c(d) = (1 + |d? 1|)
?K
, d = ?4, ..., 6 (7)
where K is tuned on held-out data.
Also the probability p
0
of jumping to a null word
state is tuned on held-out data. So the overall distor-
tion model becomes
p(i|i
?
, I) =
{
p
0
if i = null state
(1? p
0
) ? p(i|i
?
, I) otherwise
3 Lattice-based System Combination
Model
Lattice-based system combination involves the fol-
lowing steps:
(1) Collect the hypotheses from the candidate sys-
tems.
(2) Choose the backbone from the hypotheses.
This is performed using a sentence-level Minimum
Bayes Risk (MBR) method. The hypothesis with the
minimum cost of edits against all hypotheses is se-
lected. The backbone is significant for it influences
not only the word order, but also the following align-
ments. The backbone is selected as follows:
E
B
= argmin
E
?
?E
?
E?E
TER(E
?
, E) (8)
(3) Get the alignments of the backbone and hy-
pothesis pairs. First, each pair is aligned in both di-
rections using the IHMM-based alignment method.
In the IHMM alignment model, bilingual dictionar-
ies in both directions are indispensable. Then, we
apply a grow-diag-final algorithm which is widely
used in bilingual phrase extraction (Koehn et al,
2003) to monolingual alignments. The bidirec-
tional alignments are combined to one resorting to
the grow-diag-final algorithm, allowing n-to-n map-
pings.
(4)Normalize the alignment pairs. The word or-
der of the backbone determines the word order of
consensus outputs, so the word order of hypotheses
must be consistent with that of the backbone. All
words of a hypotheses are reordered according to
the alignment to the backbone. For a word aligned
to null, an actual null word may be inserted to the
proper position. The alignment units are extracted
first and then the hypothesis words in each unit are
shifted as a whole.
(5) Construct the lattice in the light of phrase
pairs extracted on the normalized alignment pairs.
The expression ability of the lattice depends on the
phrase pairs.
(6) Decode the lattice using a model similar to the
log-linear model.
The confusion-network-based system combina-
tion model goes in a similar way. The first two steps
are the same as the lattice-based model. The differ-
ence is that the hypothesis pairs are aligned just in
one direction due to the expression limit of the con-
fusion network. As a result, the normalized align-
ments only contain 1-to-1 mappings (Actual null
words are also needed in the case of null alignment).
In the following, we will give more details about the
steps which are different in the two models.
1107
4 Lattice Construction
Unlike a confusion network that operates words
only, a lattice allows for phrase pairs. So phrase
pairs must be extracted before constructing a lat-
tice. A major difficulty in extracting phrase pairs
is that the word order of hypotheses is not consistent
with that of the backbone. As a result, hypothesis
words belonging to a phrase pair may be discon-
tinuous. Before phrase pairs are extracted, the hy-
pothesis words should be normalized to make sure
the words in a phrase pair is continuous. We call a
phrase pair before normalization a alignment unit.
The problem mentioned above is shown in Fig-
ure 2. In Figure 2 (a), although (e?
1
e
?
3
, e
2
) should be
a phrase pair, but /e?
1
0 and /e?
3
0 are discontin-
uous, so the phrase pair can not be extracted. Only
after the words of the hypothesis are reordered ac-
cording to the corresponding words in the backbone
as shown in Figure 2 (b), /e?
1
0 and /e?
3
0 be-
come continuous and the phrase pair (e?
1
e
?
3
, e
2
) can
be extracted. The procedure of reordering is called
alignment normalization
E
h
: e?
1
e
?
2
e
?
3
E
B
:
e
1
e
2
e
3
(a)
E
h
: e?
2
e
?
1
e
?
3
E
B
:
e
1
e
2
e
3
(b)
Figure 2: An example of alignment units
4.1 Alignment Normalization
After the final alignments are generated in the grow-
diag-final algorithm, minimum alignment units are
extracted. The hypothesis words of an alignment
unit are packed as a whole in shift operations.
See the example in Figure 2 (a) first. All mini-
mum alignment units are as follows: (e?
2
, e
1
), (e?
1
e
?
3
,
e
2
) and (?, e
3
). (e?
1
e
?
2
e
?
3
, e
1
e
2
) is an alignment unit,
but not a minimum alignment unit.
Let a?
i
= (e?
?
i
, e?
i
) denote a minimum alignment
unit, and assume that the word string e??
i
covers words
e
?
i
1
,..., e
?
i
m
on the hypothesis side, and the word
string e?
i
covers the consecutive words e
i
1
,..., e
i
n
on
the backbone side. In an alignment unit, the word
string on the hypothesis side can be discontinuous.
The minimum unit a?
i
= (e?
?
i
, e?
i
) must observe the
following rules:
E
B
: e
1
e
2
e
3
E
h
:
e
?
1
e
?
2 (a)
e
1
e
2
e
3
e
?
2
?
e
?
1
E
B
: e
1
e
2
E
h
: e
?
1
e
?
2
e
?
3
e
1
e
2
e
?
1
e
?
3
e
?
1
e
?
2
e
?
3
(b)
E
B
: e
1
e
2
E
h
:
e
?
1
e
?
2
e
?
3
e
1
?
e
2
e
?
1
e
?
2
e
?
3
(c)
Figure 3: Different cases of null insertion
? ? e
?
i
k
? e?
?
i
, e
a
?
i
k
? e?
i
? ? e
i
k
? e?
i
, e
?
a
i
k
= null or e?
a
i
k
? e?
?
i
? ? a?
j
= (e?
?
j
, e?
j
), e?
j
= e
i
1
, ..., e
i
k
or e?
j
=
e
i
k
, ..., e
i
n
, k ? [1, n]
Where a?
i
k
denotes the position of the word in the
backbone that e?
i
k
is aligned to, and a
i
k
denotes the
position of the word in the hypothesis that e
i
k
is
aligned to.
An actual null word may be inserted to a proper
position if a word, either from the hypothesis or from
the backbone, is aligned to null. In this way, the
minimum alignment set is extended to an alignment
unit set, which includes not only minimum align-
ment units but also alignment units which are gener-
ated by adding null words to minimum alignment
units. In general, the following three conditions
should be taken into consideration:
? A backbone word is aligned to null. A null
word is inserted to the hypothesis as shown in
Figure 3 (a).
? A hypothesis word is aligned to null and it is
between the span of a minimum alignment unit.
A new alignment unit is generated by insert-
ing the hypothesis word aligned to null to the
minimum alignment unit. The new hypothesis
string must remain the original word order of
the hypothesis. It is illustrated in Figure 3 (b).
? A hypothesis word is aligned to null and it is
not between the hypothesis span of any mini-
mum alignment unit. In this case, a null word
1108
e1
e
2
?
e
3
e?
?
4
e?
?
5
e?
?
6
(a)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
(b)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
(c)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
(d)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
e?
?
6
(e)
Figure 4: A toy instance of lattice construction
are inserted to the backbone. This is shown in
Figure 3 (c).
4.2 Lattice Construction Algorithm
The lattice is constructed by adding the normalized
alignment pairs incrementally. One backbone arc in
a lattice can only span one backbone word. In con-
trast, all hypothesis words in an alignment unit must
be packed into one hypothesis arc. First the lattice is
initialized with a normalized alignment pair. Then
given all other alignment pairs one by one, the lat-
tice is modified dynamically by adding the hypothe-
sis words of an alignment pair in a left-to-right fash-
ion.
A toy instance is given in Figure 4 to illustrate the
procedure of lattice construction. Assume the cur-
rent inputs are: an alignment pair as in Figure 4 (a),
and a lattice as in Figure 4 (b). The backbone words
of the alignment pair are compared to the backbone
words of the lattice one by one. The procedure is as
follows:
? e
1
is compared with e
1
. Since they are the
same, the hypothesis arc e??
4
, which comes from
the same node with e
1
in the alignment pair,
is compared with the hypothesis arc e??
1
, which
comes from the same node with e
1
in the lat-
tice. The two hypothesis arcs are not the same,
so e??
4
is added to the lattice as shown in Figure
4(c). Both go to the next backbone words.
? e
2
is compared with ?. The lattice remains the
same. The lattice goes to the next backbone
word e
2
.
? e
2
is compared with e
2
. There is no hypothesis
arc coming from the same node with the bone
arc e
2
in the alignment pair, so the lattice re-
mains the same. Both go to the next backbone
words.
? ? is compared with e
3
. A null backbone arc is
inserted into the lattice between e
2
and e
3
. The
hypothesis arc e??
5
is inserted to the lattice, too.
The modified lattice is shown in Figure 4(d).
The alignment pair goes to the next backbone
word e
3
.
? e
3
is compared with e
3
. For they are the same
and there is no hypothesis arc e??
6
in the lattice,
e?
?
6
is inserted to the lattice as in Figure 4(e).
? Both arrive at the end and it is the turn of the
next alignment pair.
When comparing a backbone word of the given
alignment pair with a backbone word of the lattice,
the following three cases should be handled:
? The current backbone word of the given align-
ment pair is a null word while the current back-
bone word of the lattice is not. A null back-
bone word is inserted to the lattice.
? The current backbone word of the lattice is a
null word while the current word of the given
alignment pair is not. The current null back-
bone word of the lattice is skipped with nothing
to do. The next backbone word of the lattice is
compared with the current backbone word of
the given alignment pair.
1109
Algorithm 1 Lattice construction algorithm.
1: Input: alignment pairs {p
n
}
N
n=1
2: L? p
1
3: Unique(L)
4: for n? 2 .. N do
5: pnode = p
n
? first
6: lnode = L ? first
7: while pnode ? barcnext 6= NULL do
8: if lnode ? barcnext = NULL or pnode ?
bword = null and lnode ? bword 6= null then
9: INSERTBARC(lnode, null)
10: pnode = pnode ? barcnext
11: else
12: if pnode ? bword 6= null and lnode ?
bword = null then
13: lnode = lnode ? barcnext
14: else
15: for each harc of pnode do
16: if NotExist(lnode, pnode ? harc)
then
17: INSERTHARC(lnode, pnode ?
harc)
18: pnode = pnode ? barcnext
19: lnode = lnode ? barcnext
20: Output: lattice L
? The current backbone words of the given align-
ment pair and the lattice are the same. Let
{harc
l
} denotes the set of hypothesis arcs,
which come from the same node with the cur-
rent backbone arc in the lattice, and harc
h
de-
notes one of the corresponding hypothesis arcs
in the given alignment pair. In the {harc
l
},
if there is no arc which is the same with the
harc
h
, a hypothesis arc projecting to harc
h
is
added to the lattice.
The algorithm of constructing a lattice is illus-
trated in Algorithm 1. The backbone words of the
alignment pair and the lattice are processed one by
one in a left-to-right manner. Line 2 initializes the
lattice with the first alignment pair, and Line 3 re-
moves the hypothesis arc which contains the same
words with the backbone arc. barc denotes the back-
bone arc, storing one backbone word only, and harc
denotes the hypothesis arc, storing the hypothesis
words. For there may be many alignment units span
the same backbone word range, there may be more
than one harc coming from one node. Line 8 ? 10
consider the condition 1 and function InsertBarc in
Line 9 inserts a null bone arc to the position right
before the current node. Line 12?13 deal with con-
dition 2 and jump to the next backbone word of the
lattice. Line 15?19 handle condition 3 and function
InsertHarc inserts to the lattice a harc with the same
hypothesis words and the same backbone word span
with the current hypothesis arc.
5 Decoding
In confusion network decoding, a translation is gen-
erated by traveling all the nodes from left to right.
So a translation path contains all the nodes. While
in lattice decoding, a translation path may skip some
nodes as some hypothesis arcs may cross more than
one backbone arc.
Similar to the features in Rosti et al (2007a), the
features adopted by lattice-based model are arc pos-
terior probability, language model probability, the
number of null arcs, the number of hypothesis arcs
possessing more than one non-null word and the
number of all non-null words. The features are com-
bined in a log-linear model with the arc posterior
probabilities being processed specially as follows:
log p(e/f) =
N
arc
?
i=1
log (
N
s
?
s=1
?
s
p
s
(arc))
+ ?L(e) + ?N
nullarc
(e)
+ ?N
longarc
(e) + ?N
word
(e)
(9)
where f denotes the source sentence, e denotes a
translation generated by the lattice-based system,
N
arc
is the number of arcs the path of e covers,
N
s
is the number of candidate systems and ?
s
is the
weight of system s. ? is the language model weight
and L(e) is the LM log-probability. N
nullarcs
(e) is
the number of the arcs which only contain a null
word, and N
longarc
(e) is the number of the arcs
which store more than one non-null word. The
above two numbers are gotten by counting both
backbone arcs and hypothesis arcs. ? and ? are the
corresponding weights of the numbers, respectively.
N
word
(e) is the non-null word number and ? is its
weight.
Each arc has different confidences concerned with
different systems, and the confidence of system s
is denoted by p
s
(arc). p
s
(arc) is increased by
1110
1/(k+1) if the hypothesis ranking k in the system s
contains the arc (Rosti et al, 2007a; He et al, 2008).
Cube pruning algorithm with beam search is em-
ployed to search for the consensus output (Huang
and Chiang, 2005). The nodes in the lattice are
searched in a topological order and each node re-
tains a list of N best candidate partial translations.
6 Experiments
The candidate systems participating in the system
combination are as listed in Table 1: System A is a
BTG-based system using a MaxEnt-based reorder-
ing model; System B is a hierarchical phrase-based
system; System C is the Moses decoder (Koehn et
al., 2007); System D is a syntax-based system. 10-
best hypotheses from each candidate system on the
dev and test sets were collected as the input of the
system combination.
In our experiments, the weights were all tuned on
the NIST MT02 Chinese-to-English test set, includ-
ing 878 sentences, and the test data was the NIST
MT05 Chinese-to-English test set, including 1082
sentences, except the experiments in Table 2. A 5-
gram language model was used which was trained
on the XinHua portion of Gigaword corpus. The re-
sults were all reported in case sensitive BLEU score
and the weights were tuned in Powell?s method to
maximum BLEU score. The IHMM-based align-
ment module was implemented according to He et
al. (2008), He (2007) and Vogel et al (1996). In all
experiments, the parameters for IHMM-based align-
ment module were set to: the smoothing factor for
the surface similarity model, ? = 3; the controlling
factor for the distortion model, K = 2.
6.1 Comparison with
Confusion-network-based model
In order to compare the lattice-based system with
the confusion-network-based system fairly, we used
IHMM-based system combination model on behalf
of the confusion-network-based model described in
He et al (2008). In both lattice-based and IHMM-
based systems, the bilingual dictionaries were ex-
tracted on the FBIS data set which included 289K
sentence pairs. The interpolation factor of the simi-
larity model was set to ? = 0.1.
The results are shown in Table 1. IHMM stands
for the IHMM-based model and Lattice stands for
the lattice-based model. On the dev set, the lattice-
based system was 3.92 BLEU points higher than the
best single system and 0.36 BLEU point higher than
the IHMM-based system. On the test set, the lattice-
based system got an absolute improvement by 3.73
BLEU points over the best single system and 1.23
BLEU points over the IHMM-based system.
System MT02 MT05
BLEU% BLEU%
SystemA 31.93 30.68
SystemB 32.16 32.07
SystemC 32.09 31.64
SystemD 33.37 31.26
IHMM 36.93 34.57
Lattice 37.29 35.80
Table 1: Results on the MT02 and MT05 test sets
The results on another test sets are reported in Ta-
ble 2. The parameters were tuned on the newswire
part of NIST MT06 Chinese-to-English test set, in-
cluding 616 sentences, and the test set was NIST
MT08 Chinese-to-English test set, including 1357
sentences. The BLEU score of the lattice-based sys-
tem is 0.93 BLEU point higher than the IHMM-
based system and 3.0 BLEU points higher than the
best single system.
System MT06 MT08
BLEU% BLEU%
SystemA 32.51 25.63
SystemB 31.43 26.32
SystemC 31.50 23.43
SystemD 32.41 26.28
IHMM 36.05 28.39
Lattice 36.53 29.32
Table 2: Results on the MT06 and MT08 test sets
We take a real example from the output of the
two systems (in Table 3) to show that higher BLEU
scores correspond to better alignments and better
translations. The translation of System C is selected
as the backbone. From Table 3, we can see that
because of 1-to-1 mappings, ?Russia? is aligned to
?Russian? and ??s? to ?null? in the IHMM-based
model, which leads to the error translation ?Russian
1111
Source: ?dIE?h?i??dIEd?i?1??
SystemA: Russia merger of state-owned oil company and the state-run gas company in Russia
SystemB: Russia ?s state-owned oil company is working with Russia ?s state-run gas company mergers
SystemC: Russian state-run oil company is combined with the Russian state-run gas company
SystemD: Russia ?s state-owned oil companies are combined with Russia ?s state-run gas company
IHMM: Russian ?s state-owned oil company working with Russia ?s state-run gas company
Lattice: Russia ?s state-owned oil company is combined with the Russian state-run gas company
Table 3: A real translation example
?s?. Instead, ?Russia ?s? is together aligned to ?Rus-
sian? in the lattice-based model. Also due to 1-to-
1 mappings, null word aligned to ?is? is inserted.
As a result, ?is? is missed in the output of IHMM-
based model. In contrast, in the lattice-based sys-
tem, ?is working with? are aligned to ?is combined
with?, forming a phrase pair.
6.2 Effect of Dictionary Scale
The dictionary is important to the semantic similar-
ity model in IHMM-based alignment method. We
evaluated the effect of the dictionary scale by using
dictionaries extracted on different data sets. The dic-
tionaries were respectively extracted on similar data
sets: 30K sentence pairs, 60K sentence pairs, 289K
sentence pairs (FBIS corpus) and 2500K sentence
pairs. The results are illustrated in Table 4. In or-
der to demonstrate the effect of the dictionary size
clearly, the interpolation factor of similarity model
was all set to ? = 0.1.
From Table 4, we can see that when the cor-
pus size rise from 30k to 60k, the improvements
were not obvious both on the dev set and on the
test set. As the corpus was expanded to 289K, al-
though on the dev set, the result was only 0.2 BLEU
point higher, on the test set, it was 0.63 BLEU point
higher. As the corpus size was up to 2500K, the
BLEU scores both on the dev and test sets declined.
The reason is that, on one hand, there are more noise
on the 2500K sentence pairs; on the other hand, the
289K sentence pairs cover most of the words appear-
ing on the test set. So we can conclude that in or-
der to get better results, the dictionary scale must be
up to some certain scale. If the dictionary is much
smaller, the result will be impacted dramatically.
MT02 MT05
BLEU% BLEU%
30k 36.94 35.14
60k 37.09 35.17
289k 37.29 35.80
2500k 37.14 35.62
Table 4: Effect of dictionary scale
6.3 Effect of Semantic Alignments
For the IHMM-based alignment method, the transla-
tion probability of an English word pair is computed
using a linear interpolation of the semantic similar-
ity and the surface similarity. So the two similarity
models decide the translation probability together
and the proportion is controlled by the interpolation
factor. We evaluated the effect of the two similarity
models by varying the interpolation factor ?.
We used the dictionaries extracted on the FBIS
data set. The result is shown in Table 5. We got the
best result with ? = 0.1. When we excluded the
semantic similarity model (? = 0.0) or excluded the
surface similarity model (? = 1.0), the performance
became worse.
7 Conclusion
The alignment model plays an important role in
system combination. Because of the expression
limitation of confusion networks, only 1-to-1 map-
pings are employed in the confusion-network-based
model. This paper proposes a lattice-based system
combination model. As a general form of confusion
networks, lattices can express n-to-n mappings. So
a lattice-based model processes phrase pairs while
1112
MT02 MT05
BLEU% BLEU%
? = 1.0 36.41 34.92
? = 0.7 37.21 35.65
? = 0.5 36.43 35.02
? = 0.4 37.14 35.55
? = 0.3 36.75 35.66
? = 0.2 36.81 35.55
? = 0.1 37.29 35.80
? = 0.0 36.45 35.14
Table 5: Effect of semantic alignments
a confusion-network-based model processes words
only. As a result, phrase pairs must be extracted be-
fore constructing a lattice.
On NIST MT05 test set, the lattice-based sys-
tem gave better results with an absolute improve-
ment of 1.23 BLEU points over the confusion-
network-based system (He et al, 2008) and 3.73
BLEU points over the best single system. On
NIST MT08 test set, the lattice-based system out-
performed the confusion-network-based system by
0.93 BLEU point and outperformed the best single
system by 3.0 BLEU points.
8 Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China Contract 60736014, Na-
tional Natural Science Foundation of China Con-
tract 60873167 and High Technology R&D Program
Project No. 2006AA010108. Thank Wenbin Jiang,
Tian Xia and Shu Cai for their help. We are also
grateful to the anonymous reviewers for their valu-
able comments.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. of
IEEE ASRU, pages 351?354.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL/HLT 2008, pages 1012?1020, Colum-
bus, Ohio, June.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP, pages
95?100.
Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for computing outputs from ma-
chine translation systems. In Proc. of EMNLP, pages
98?107.
Xiaodong He. 2007. Using word-dependent translation
models in hmm based word alignment for statistical
machine translation. In Proc. of COLING-ACL, pages
961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages 53?
64.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstration
Session.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. of IEEE EACL, pages 33?40.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. of ACL,
pages 312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple ma-
chine translation systems. In Proc. of NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translaiton system combination. In
Proc. of the Third ACL WorkShop on Statistical Ma-
chine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. of ICASSP, pages
105?108.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proc. of COLING, pages 836?841.
1113
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558?566,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Improving Tree-to-Tree Translation with Packed Forests
Yang Liu and Yajuan Lu? and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,lvyajuan,liuqun}@ict.ac.cn
Abstract
Current tree-to-tree models suffer from
parsing errors as they usually use only 1-
best parses for rule extraction and decod-
ing. We instead propose a forest-based
tree-to-tree model that uses packed forests.
The model is based on a probabilis-
tic synchronous tree substitution gram-
mar (STSG), which can be learned from
aligned forest pairs automatically. The de-
coder finds ways of decomposing trees in
the source forest into elementary trees us-
ing the source projection of STSG while
building target forest in parallel. Compa-
rable to the state-of-the-art phrase-based
system Moses, using packed forests in
tree-to-tree translation results in a signif-
icant absolute improvement of 3.6 BLEU
points over using 1-best trees.
1 Introduction
Approaches to syntax-based statistical machine
translation make use of parallel data with syntactic
annotations, either in the form of phrase structure
trees or dependency trees. They can be roughly
divided into three categories: string-to-tree mod-
els (e.g., (Galley et al, 2006; Marcu et al, 2006;
Shen et al, 2008)), tree-to-string models (e.g.,
(Liu et al, 2006; Huang et al, 2006)), and tree-to-
tree models (e.g., (Eisner, 2003; Ding and Palmer,
2005; Cowan et al, 2006; Zhang et al, 2008)).
By modeling the syntax of both source and tar-
get languages, tree-to-tree approaches have the po-
tential benefit of providing rules linguistically bet-
ter motivated. However, while string-to-tree and
tree-to-string models demonstrate promising re-
sults in empirical evaluations, tree-to-tree models
have still been underachieving.
We believe that tree-to-tree models face two
major challenges. First, tree-to-tree models are
more vulnerable to parsing errors. Obtaining
syntactic annotations in quantity usually entails
running automatic parsers on a parallel corpus.
As the amount and domain of the data used to
train parsers are relatively limited, parsers will
inevitably output ill-formed trees when handling
real-world text. Guided by such noisy syntactic in-
formation, syntax-based models that rely on 1-best
parses are prone to learn noisy translation rules
in training phase and produce degenerate trans-
lations in decoding phase (Quirk and Corston-
Oliver, 2006). This situation aggravates for tree-
to-tree models that use syntax on both sides.
Second, tree-to-tree rules provide poorer rule
coverage. As a tree-to-tree rule requires that there
must be trees on both sides, tree-to-tree mod-
els lose a larger amount of linguistically unmoti-
vated mappings. Studies reveal that the absence of
such non-syntactic mappings will impair transla-
tion quality dramatically (Marcu et al, 2006; Liu
et al, 2007; DeNeefe et al, 2007; Zhang et al,
2008).
Compactly encoding exponentially many
parses, packed forests prove to be an excellent
fit for alleviating the above two problems (Mi et
al., 2008; Mi and Huang, 2008). In this paper,
we propose a forest-based tree-to-tree model. To
learn STSG rules from aligned forest pairs, we in-
troduce a series of notions for identifying minimal
tree-to-tree rules. Our decoder first converts the
source forest to a translation forest and then finds
the best derivation that has the source yield of one
source tree in the forest. Comparable to Moses,
our forest-based tree-to-tree model achieves an
absolute improvement of 3.6 BLEU points over
conventional tree-based model.
558
IP1
NP2 VP3
PP4 VP-B5
NP-B6 NP-B7 NP-B8
NR9 CC10P11 NR12 VV13 AS14 NN15
bushi yu shalong juxing le huitan
Bush held a talk with Sharon
NNP16 VBD17 DT18 NN19 IN20 NNP21
NP22 NP23 NP24
NP25 PP26
NP27
VP28
S 29
Figure 1: An aligned packed forest pair. Each
node is assigned a unique identity for reference.
The solid lines denote hyperedges and the dashed
lines denote word alignments. Shaded nodes are
frontier nodes.
2 Model
Figure 1 shows an aligned forest pair for a Chinese
sentence and an English sentence. The solid lines
denote hyperedges and the dashed lines denote
word alignments between the two forests. Each
node is assigned a unique identity for reference.
Each hyperedge is associated with a probability,
which we omit in Figure 1 for clarity. In a forest,
a node usually has multiple incoming hyperedges.
We use IN(v) to denote the set of incoming hy-
peredges of node v. For example, the source node
?IP1? has following two incoming hyperedges: 1
e1 = ?(NP-B6,VP3), IP1?
e2 = ?(NP2,VP-B5), IP1?
1As there are both source and target forests, it might be
confusing by just using a span to refer to a node. In addition,
some nodes will often have the same labels and spans. There-
fore, it is more convenient to use an identity for referring to a
node. The notation ?IP1? denotes the node that has a label of
?IP? and has an identity of ?1?.
Formally, a packed parse forest is a compact
representation of all the derivations (i.e., parse
trees) for a given sentence under a context-free
grammar. Huang and Chiang (2005) define a for-
est as a tuple ?V,E, v?,R?, where V is a finite set
of nodes, E is a finite set of hyperedges, v? ? V is
a distinguished node that denotes the goal item in
parsing, and R is the set of weights. For a given
sentence w1:l = w1 . . . wl, each node v ? V is in
the form of Xi,j , which denotes the recognition of
non-terminal X spanning the substring from posi-
tions i through j (that is, wi+1 . . . wj). Each hy-
peredge e ? E is a triple e = ?T (e), h(e), f(e)?,
where h(e) ? V is its head, T (e) ? V ? is a vector
of tail nodes, and f(e) is a weight function from
R|T (e)| to R.
Our forest-based tree-to-tree model is based on
a probabilistic STSG (Eisner, 2003). Formally,
an STSG can be defined as a quintuple G =
?Fs,Ft,Ss,St, P ?, where
? Fs andFt are the source and target alhabets,
respectively,
? Ss and St are the source and target start sym-
bols, and
? P is a set of production rules. A rule r is a
triple ?ts, tt,?? that describes the correspon-
dence ? between a source tree ts and a target
tree tt.
To integrate packed forests into tree-to-tree
translation, we model the process of synchronous
generation of a source forest Fs and a target forest
Ft using a probabilistic STSG grammar:
Pr(Fs, Ft) =
?
Ts?Fs
?
Tt?Ft
Pr(Ts, Tt)
=
?
Ts?Fs
?
Tt?Ft
?
d?D
Pr(d)
=
?
Ts?Fs
?
Tt?Ft
?
d?D
?
r?d
p(r) (1)
where Ts is a source tree, Tt is a target tree, D is
the set of all possible derivations that transform Ts
into Tt, d is one such derivation, and r is a tree-to-
tree rule.
Table 1 shows a derivation of the forest pair in
Figure 1. A derivation is a sequence of tree-to-tree
rules. Note that we use x to represent a nontermi-
nal.
559
(1) IP(x1:NP-B, x2:VP)? S(x1:NP, x2:VP)
(2) NP-B(x1:NR)? NP(x1:NNP)
(3) NR(bushi)? NNP(Bush)
(4) VP(x1:PP, VP-B(x2:VV, AS(le), x3:NP-B))? VP(x2:VBD, NP(DT(a), x3:NP), x1:PP)
(5) PP(x1:P, x2:NP-B)? PP(x1:IN, x2:NP)
(6) P(yu)? IN(with)
(7) NP-B(x1:NR)? NP(x1:NP)
(8) NR(shalong) ? NNP(Sharon)
(9) VV(juxing) ? VBD(held)
(10) NP-B(x1:NN)? NP(x1:NN)
(11) NN(huitan) ? NN(talk)
Table 1: A minimal derivation of the forest pair in Figure 1.
id span cspan complement consistent frontier counterparts
1 1-6 1-2, 4-6 1 1 29
2 1-3 1, 5-6 2, 4 0 0
3 2-6 2, 4-6 1 1 1 28
4 2-3 5-6 1-2, 4 1 1 25, 26
5 4-6 2, 4 1, 5-6 1 0
6 1-1 1 2, 4-6 1 1 16, 22
7 3-3 6 1-2, 4-5 1 1 21, 24
8 6-6 4 1-2, 5-6 1 1 19, 23
9 1-1 1 2, 4-6 1 1 16, 22
10 2-2 5 1-2, 4, 6 1 1 20
11 2-2 5 1-2, 4, 6 1 1 20
12 3-3 6 1-2, 4-5 1 1 21, 24
13 4-4 2 1, 4-6 1 1 17
14 5-5 1-2, 4-6 1 0
15 6-6 4 1-2, 5-6 1 1 19, 23
16 1-1 1 2-4, 6 1 1 6, 9
17 2-2 4 1-3, 6 1 1 13
18 3-3 1-4, 6 1 0
19 4-4 6 1-4 1 1 8, 15
20 5-5 2 1, 3-4, 6 1 1 10, 11
21 6-6 3 1-2, 4, 6 1 1 7, 12
22 1-1 1 2-4, 6 1 1 6, 9
23 3-4 6 1-4 1 1 8, 15
24 6-6 3 1-2, 4, 6 1 1 7, 12
25 5-6 2-3 1, 4, 6 1 1 4
26 5-6 2-3 1, 4, 6 1 1 4
27 3-6 2-3, 6 1, 4 0 0
28 2-6 2-4, 6 1 1 1 3
29 1-6 1-4, 6 1 1 1
Table 2: Node attributes of the example forest pair.
3 Rule Extraction
Given an aligned forest pair as shown in Figure
1, how to extract all valid tree-to-tree rules that
explain its synchronous generation process? By
constructing a theory that gives formal seman-
tics to word alignments, Galley et al (2004)
give principled answers to these questions for ex-
tracting tree-to-string rules. Their GHKM proce-
dure draws connections among word alignments,
derivations, and rules. They first identify the
tree nodes that subsume tree-string pairs consis-
tent with word alignments and then extract rules
from these nodes. By this means, GHKM proves
to be able to extract all valid tree-to-string rules
from training instances. Although originally de-
veloped for the tree-to-string case, it is possible to
extend GHKM to extract all valid tree-to-tree rules
from aligned packed forests.
In this section, we introduce our tree-to-tree rule
extraction method adapted from GHKM, which
involves four steps: (1) identifying the correspon-
dence between the nodes in forest pairs, (2) iden-
tifying minimum rules, (3) inferring composed
rules, and (4) estimating rule probabilities.
3.1 Identifying Correspondence Between
Nodes
To learn tree-to-tree rules, we need to find aligned
tree pairs in the forest pairs. To do this, the start-
ing point is to identify the correspondence be-
tween nodes. We propose a number of attributes
for nodes, most of which derive from GHKM, to
facilitate the identification.
Definition 1 Given a node v, its span ?(v) is an
index set of the words it covers.
For example, the span of the source node
?VP-B5? is {4, 5, 6} as it covers three source
words: ?juxing?, ?le?, and ?huitan?. For conve-
nience, we use {4-6} to denotes a contiguous span
{4, 5, 6}.
Definition 2 Given a node v, its corresponding
span ?(v) is the index set of aligned words on an-
other side.
For example, the corresponding span of the
source node ?VP-B5? is {2, 4}, corresponding to
the target words ?held? and ?talk?.
Definition 3 Given a node v, its complement span
?(v) is the union of corresponding spans of nodes
that are neither antecedents nor descendants of v.
For example, the complement span of the source
node ?VP-B5? is {1, 5-6}, corresponding to target
words ?Bush?, ?with?, and ?Sharon?.
Definition 4 A node v is said to be consistent with
alignment if and only if closure(?(v))??(v) = ?.
For example, the closure of the corresponding
span of the source node ?VP-B5? is {2-4} and
its complement span is {1, 5-6}. As the intersec-
tion of the closure and the complement span is an
empty set, the source node ?VP-B5? is consistent
with the alignment.
560
PP4
NP-B7
P11 NR12
PP4
P11 NP-B7
PP4
NP-B7
P11 NR12
PP26
IN20
NP24
NNP21
PP4
P11 NP-B7
PP26
IN 20 NP24
(a) (b) (c) (d)
Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier
tree pair. All trees are taken from the example forest pair in Figure 1. Shaded nodes are frontier nodes.
Each node is assigned an identity for reference.
Definition 5 A node v is said to be a frontier node
if and only if:
1. v is consistent;
2. There exists at least one consistent node v? on
another side satisfying:
? closure(?(v?)) ? ?(v);
? closure(?(v)) ? ?(v?).
v? is said to be a counterpart of v. We use ?(v) to
denote the set of counterparts of v.
A frontier node often has multiple counter-
parts on another side due to the usage of unary
rules in parsers. For example, the source node
?NP-B6? has two counterparts on the target side:
?NNP16? and ?NP22?. Conversely, the target node
?NNP16? also has two counterparts counterparts
on the source side: ?NR9? and ?NP-B6?.
The node attributes of the example forest pair
are listed in Table 2. We use identities to refer to
nodes. ?cspan? denotes corresponding span and
?complement? denotes complement span. In Fig-
ure 1, there are 12 frontier nodes (highlighted by
shading) on the source side and 12 frontier nodes
on the target side. Note that while a consistent
node is equal to a frontier node in GHKM, this is
not the case in our method because we have a tree
on the target side. Frontier nodes play a critical
role in forest-based rule extraction because they
indicate where to cut the forest pairs to obtain tree-
to-tree rules.
3.2 Identifying Minimum Rules
Given the frontier nodes, the next step is to iden-
tify aligned tree pairs, from which tree-to-tree
rules derive. Following Galley et al (2006), we
distinguish between minimal and composed rules.
As a composed rule can be decomposed as a se-
quence of minimal rules, we are particularly inter-
ested in how to extract minimal rules. Also, we in-
troduce a number of notions to help identify mini-
mal rules.
Definition 6 A frontier tree is a subtree in a forest
satisfying:
1. Its root is a frontier node;
2. If the tree contains only one node, it must be
a lexicalized frontier node;
3. If the tree contains more than one nodes,
its leaves are either non-lexicalized frontier
nodes or lexicalized non-frontier nodes.
For example, Figure 2(a) shows a frontier tree
in which all nodes are frontier nodes.
Definition 7 A minimal frontier tree is a frontier
tree such that all nodes other than the root and
leaves are non-frontier nodes.
For example, Figure 2(b) shows a minimal fron-
tier tree.
Definition 8 A frontier tree pair is a triple
?ts, tt,?? satisfying:
1. ts is a source frontier tree;
561
2. tt is a target frontier tree;
3. The root of ts is a counterpart of that of tt;
4. There is a one-to-one correspondence ? be-
tween the frontier leaves of ts and tt.
For example, Figure 2(c) shows a frontier tree
pair.
Definition 9 A frontier tree pair ?ts, tt,?? is said
to be a subgraph of another frontier tree pair
?ts?, tt?,??? if and only if:
1. root(ts) = root(ts?);
2. root(tt) = root(tt?);
3. ts is a subgraph of ts?;
4. tt is a subgraph of tt?.
For example, the frontier tree pair shown in Fig-
ure 2(d) is a subgraph of that in Figure 2(c).
Definition 10 A frontier tree pair is said to be
minimal if and only if it is not a subgraph of any
other frontier tree pair that shares with the same
root.
For example, Figure 2(d) shows a minimal fron-
tier tree pair.
Our goal is to find the minimal frontier tree
pairs, which correspond to minimal tree-to-tree
rules. For example, the tree pair shown in Figure
2(d) denotes a minimal rule as follows:
PP(x1:P,x2:NP-B)? PP(x1:IN, x2:NP)
Figure 3 shows the algorithm for identifying
minimal frontier tree pairs. The input is a source
forest Fs, a target forest Ft, and a source frontier
node v (line 1). We use a set P to store collected
minimal frontier tree pairs (line 2). We first call
the procedure FINDTREES(Fs , v) to identify a set
of frontier trees rooted at v in Fs (line 3). For ex-
ample, for the source frontier node ?PP4? in Figure
1, we obtain two frontier trees:
(PP4(P11)(NP-B7))
(PP4(P11)(NP-B7(NR12)))
Then, we try to find the set of corresponding
target frontier trees (i.e., Tt). For each counter-
part v? of v (line 5), we call the procedure FIND-
TREES(Ft, v?) to identify a set of frontier trees
rooted at v? in Ft (line 6). For example, the source
1: procedure FINDTREEPAIRS(Fs , Ft, v)
2: P = ?
3: Ts ? FINDTREES(Fs , v)
4: Tt ? ?
5: for v? ? ?(v) do
6: Tt ? Tt? FINDTREES(Ft , v?)
7: end for
8: for ?ts, tt? ? Ts ? Tt do
9: if ts ? tt then
10: P ? P ? {?ts, tt,??}
11: end if
12: end for
13: for ?ts, tt,?? ? P do
14: if ??ts?, tt?,??? ? P : ?ts?, tt?,??? ?
?ts, tt,?? then
15: P ? P ? {?ts, tt,??}
16: end if
17: end for
18: end procedure
Figure 3: Algorithm for identifying minimal fron-
tier tree pairs.
frontier node ?PP4? has two counterparts on the
target side: ?NP25? and ?PP26?. There are four
target frontier trees rooted at the two nodes:
(NP25(IN20)(NP24))
(NP25(IN20)(NP24(NNP21)))
(PP26(IN20)(NP24))
(PP26(IN20)(NP24(NNP21)))
Therefore, there are 2 ? 4 = 8 pairs of trees.
We examine each tree pair ?ts, tt? (line 8) to see
whether it is a frontier tree pair (line 9) and then
update P (line 10). In the above example, all the
eight tree pairs are frontier tree pairs.
Finally, we keep only minimal frontier tree pairs
in P (lines 13-15). As a result, we obtain the
following two minimal frontier tree pairs for the
source frontier node ?PP4?:
(PP4(P11)(NP-B7))? (NP25(IN20)(NP24))
(PP4(P11)(NP-B7))? (PP26(IN20)(NP24))
To maintain a reasonable rule table size, we re-
strict that the number of nodes in a tree of an STSG
rule is no greater than n, which we refer to as max-
imal node count.
It seems more efficient to let the procedure
FINDTREES(F, v) to search for minimal frontier
562
trees rather than frontier trees. However, a min-
imal frontier tree pair is not necessarily a pair of
minimal frontier trees. On our Chinese-English
corpus, we find that 38% of minimal frontier tree
pairs are not pairs of minimal frontier trees. As a
result, we have to first collect all frontier tree pairs
and then decide on the minimal ones.
Table 1 shows some minimal rules extracted
from the forest pair shown in Figure 1.
3.3 Inferring Composed Rules
After minimal rules are learned, composed rules
can be obtained by composing two or more min-
imal rules. For example, the composition of the
second rule and the third rule in Table 1 produces
a new rule:
NP-B(NR(shalong))? NP(NNP(Sharon))
While minimal rules derive from minimal fron-
tier tree pairs, composed rules correspond to non-
minimal frontier tree pairs.
3.4 Estimating Rule Probabilities
We follow Mi and Huang (2008) to estimate the
fractional count of a rule extracted from an aligned
forest pair. Intuitively, the relative frequency of a
subtree that occurs in a forest is the sum of all the
trees that traverse the subtree divided by the sum
of all trees in the forest. Instead of enumerating
all trees explicitly and computing the sum of tree
probabilities, we resort to inside and outside prob-
abilities for efficient calculation:
c(r) =
p(ts)? ?(root(ts))?
?
v?leaves(ts) ?(v)
?(v?s)
?
p(tt)? ?(root(tt))?
?
v?leaves(tt) ?(v)
?(v?t)
where c(r) is the fractional count of a rule, ts is the
source tree in r, tt is the target tree in r, root(?) a
function that gets tree root, leaves(?) is a function
that gets tree leaves, and ?(v) and ?(v) are outside
and inside probabilities, respectively.
4 Decoding
Given a source packed forest Fs, our decoder finds
the target yield of the single best derivation d that
has source yield of Ts(d) ? Fs:
e? = e
(
argmax
d s.t. Ts(d)?Fs
p(d)
)
(2)
We extend the model in Eq. 1 to a log-linear
model (Och and Ney, 2002) that uses the follow-
ing eight features: relative frequencies in two di-
rections, lexical weights in two directions, num-
ber of rules used, language model score, number
of target words produced, and the probability of
matched source tree (Mi et al, 2008).
Given a source parse forest and an STSG gram-
mar G, we first apply the conversion algorithm
proposed by Mi et al (2008) to produce a trans-
lation forest. The translation forest has a simi-
lar hypergraph structure. While the nodes are the
same as those of the parse forest, each hyperedge
is associated with an STSG rule. Then, the de-
coder runs on the translation forest. We use the
cube pruning method (Chiang, 2007) to approxi-
mately intersect the translation forest with the lan-
guage model. Traversing the translation forest in
a bottom-up order, the decoder tries to build tar-
get parses at each node. After the first pass, we
use lazy Algorithm 3 (Huang and Chiang, 2005)
to generate k-best translations for minimum error
rate training.
5 Experiments
5.1 Data Preparation
We evaluated our model on Chinese-to-English
translation. The training corpus contains 840K
Chinese words and 950K English words. A tri-
gram language model was trained on the English
sentences of the training corpus. We used the 2002
NIST MT Evaluation test set as our development
set, and used the 2005 NIST MT Evaluation test
set as our test set. We evaluated the translation
quality using the BLEU metric, as calculated by
mteval-v11b.pl with its default setting except that
we used case-insensitive matching of n-grams.
To obtain packed forests, we used the Chinese
parser (Xiong et al, 2005) modified by Haitao
Mi and the English parser (Charniak and Johnson,
2005) modified by Liang Huang to produce en-
tire parse forests. Then, we ran the Python scripts
(Huang, 2008) provided by Liang Huang to out-
put packed forests. To prune the packed forests,
Huang (2008) uses inside and outside probabili-
ties to compute the distance of the best derivation
that traverses a hyperedge away from the glob-
ally best derivation. A hyperedge will be pruned
away if the difference is greater than a threshold
p. Nodes with all incoming hyperedges pruned
are also pruned. The greater the threshold p is,
563
p avg trees # of rules BLEU
0 1 73, 614 0.2021 ? 0.0089
2 238.94 105, 214 0.2165 ? 0.0081
5 5.78 ? 106 347, 526 0.2336 ? 0.0078
8 6.59 ? 107 573, 738 0.2373 ? 0.0082
10 1.05 ? 108 743, 211 0.2385 ? 0.0084
Table 3: Comparison of BLEU scores for tree-
based and forest-based tree-to-tree models.
0.04
0.05
0.06
0.07
0.08
0.09
0.10
 0  1  2  3  4  5  6  7  8  9  10  11
co
ve
ra
ge
maximal node count
p=0
p=2
p=5
p=8
p=10
Figure 4: Coverage of lexicalized STSG rules on
bilingual phrases.
the more parses are encoded in a packed forest.
We obtained word alignments of the training
data by first running GIZA++ (Och and Ney, 2003)
and then applying the refinement rule ?grow-diag-
final-and? (Koehn et al, 2003).
5.2 Forests Vs. 1-best Trees
Table 3 shows the BLEU scores of tree-based and
forest-based tree-to-tree models achieved on the
test set over different pruning thresholds. p is the
threshold for pruning packed forests, ?avg trees?
is the average number of trees encoded in one for-
est on the test set, and ?# of rules? is the number
of STSG rules used on the test set. We restrict that
both source and target trees in a tree-to-tree rule
can contain at most 10 nodes (i.e., the maximal
node count n = 10). The 95% confidence inter-
vals were computed using Zhang ?s significance
tester (Zhang et al, 2004).
We chose five different pruning thresholds in
our experiments: p = 0, 2, 5, 8, 10. The forests
pruned by p = 0 contained only 1-best tree per
sentence. With the increase of p, the average num-
ber of trees encoded in one forest rose dramati-
cally. When p was set to 10, there were over 100M
parses encoded in one forest on average.
p extraction decoding
0 1.26 6.76
2 2.35 8.52
5 6.34 14.87
8 8.51 19.78
10 10.21 25.81
Table 4: Comparison of rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence)
Moreover, the more trees are encoded in packed
forests, the more rules are made available to
forest-based models. The number of rules when
p = 10 was almost 10 times of p = 0. With the
increase of the number of rules used, the BLEU
score increased accordingly. This suggests that
packed forests enable tree-to-tree model to learn
more useful rules on the training data. However,
when a pack forest encodes over 1M parses per
sentence, the improvements are less significant,
which echoes the results in (Mi et al, 2008).
The forest-based tree-to-tree model outper-
forms the original model that uses 1-best trees
dramatically. The absolute improvement of 3.6
BLEU points (from 0.2021 to 0.2385) is statis-
tically significant at p < 0.01 using the sign-
test as described by Collins et al (2005), with
700(+1), 360(-1), and 15(0). We also ran Moses
(Koehn et al, 2007) with its default setting us-
ing the same data and obtained a BLEU score of
0.2366, slightly lower than our best result (i.e.,
0.2385). But this difference is not statistically sig-
nificant.
5.3 Effect on Rule Coverage
Figure 4 demonstrates the effect of pruning thresh-
old and maximal node count on rule coverage.
We extracted phrase pairs from the training data
to investigate how many phrase pairs can be cap-
tured by lexicalized tree-to-tree rules that con-
tain only terminals. We set the maximal length
of phrase pairs to 10. For tree-based tree-to-tree
model, the coverage was below 8% even the max-
imal node count was set to 10. This suggests that
conventional tree-to-tree models lose over 92%
linguistically unmotivated mappings due to hard
syntactic constraints. The absence of such non-
syntactic mappings prevents tree-based tree-to-
tree models from achieving comparable results to
phrase-based models. With more parses included
564
0.09
0.10
0.11
0.12
0.13
0.14
0.15
0.16
0.17
0.18
0.19
0.20
 0  1  2  3  4  5  6  7  8  9  10  11
BL
EU
maximal node count
Figure 5: Effect of maximal node count on BLEU
scores.
in packed forests, the rule coverage increased ac-
cordingly. When p = 10 and n = 10, the cov-
erage was 9.7%, higher than that of p = 0. As
a result, packed forests enable tree-to-tree models
to capture more useful source-target mappings and
therefore improve translation quality. 2
5.4 Training and Decoding Time
Table 4 gives the rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence) with varying pruning thresholds.
We found that the extraction time grew faster than
decoding time with the increase of p. One possi-
ble reason is that the number of frontier tree pairs
(see Figure 3) rose dramatically when more parses
were included in packed forests.
5.5 Effect of Maximal Node Count
Figure 5 shows the effect of maximal node count
on BLEU scores. With the increase of maximal
node count, the BLEU score increased dramati-
cally. This implies that allowing tree-to-tree rules
to capture larger contexts will strengthen the ex-
pressive power of tree-to-tree model.
5.6 Results on Larger Data
We also conducted an experiment on larger data
to further examine the effectiveness of our ap-
proach. We concatenated the small corpus we
used above and the FBIS corpus. After remov-
ing the sentences that we failed to obtain forests,
2Note that even we used packed forests, the rule coverage
was still very low. One reason is that we set the maximal
phrase length to 10 words, while an STSG rule with 10 nodes
in each tree usually cannot subsume 10 words.
the new training corpus contained about 260K sen-
tence pairs with 7.39M Chinese words and 9.41M
English words. We set the forest pruning threshold
p = 5. Moses obtained a BLEU score of 0.3043
and our forest-based tree-to-tree system achieved
a BLEU score of 0.3059. The difference is still not
significant statistically.
6 Related Work
In machine translation, the concept of packed for-
est is first used by Huang and Chiang (2007) to
characterize the search space of decoding with lan-
guage models. The first direct use of packed for-
est is proposed by Mi et al (2008). They replace
1-best trees with packed forests both in training
and decoding and show superior translation qual-
ity over the state-of-the-art hierarchical phrase-
based system. We follow the same direction and
apply packed forests to tree-to-tree translation.
Zhang et al (2008) present a tree-to-tree model
that uses STSG. To capture non-syntactic phrases,
they apply tree-sequence rules (Liu et al, 2007)
to tree-to-tree models. Their extraction algorithm
first identifies initial rules and then obtains abstract
rules. While this method works for 1-best tree
pairs, it cannot be applied to packed forest pairs
because it is impractical to enumerate all tree pairs
over a phrase pair.
While Galley (2004) describes extracting tree-
to-string rules from 1-best trees, Mi and Huang et
al. (2008) go further by proposing a method for
extracting tree-to-string rules from aligned forest-
string pairs. We follow their work and focus on
identifying tree-tree pairs in a forest pair, which is
more difficult than the tree-to-string case.
7 Conclusion
We have shown how to improve tree-to-tree trans-
lation with packed forests, which compactly en-
code exponentially many parses. To learn STSG
rules from aligned forest pairs, we first identify
minimal rules and then get composed rules. The
decoder finds the best derivation that have the
source yield of one source tree in the forest. Ex-
periments show that using packed forests in tree-
to-tree translation results in dramatic improve-
ments over using 1-best trees. Our system also
achieves comparable performance with the state-
of-the-art phrase-based system Moses.
565
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done
while Yang Liu was visiting the SMT group led
by Stephan Vogel at CMU. We thank the anony-
mous reviewers for their insightful comments.
Many thanks go to Liang Huang, Haitao Mi, and
Hao Xiong for their invaluable help in producing
packed forests. We are also grateful to Andreas
Zollmann, Vamshi Ambati, and Kevin Gimpel for
their helpful feedback.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL 2005.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. of EMNLP 2006.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. of EMNLP 2007.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proc. of ACL 2005.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003 (Companion Volume).
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of NAACL/HLT 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of COLING/ACL 2006.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT 2005.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL 2007.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proc. of
ACL/HLT 2008.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL 2007 (demonstration session).
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of COLING/ACL 2006.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. of ACL 2007.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP 2006.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. of EMNLP 2008.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL/HLT 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Chris Quirk and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proc. of EMNLP
2006.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL/HLT 2008.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proc. of IJCNLP 2005.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores how much improve-
ment do we need to have a better system? In Proc.
of LREC 2004.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL/HLT 2008.
566
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 121?124,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Reducing SMT Rule Table with Monolingual Key Phrase
Zhongjun He? Yao Meng? Yajuan Lj ? Hao Yu? Qun Liu?
? Fujitsu R&D Center CO., LTD, Beijing, China
{hezhongjun, mengyao, yu}@cn.fujitsu.com
? Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
{lvyajuan, liuqun}@ict.ac.cn
Abstract
This paper presents an effective approach
to discard most entries of the rule table for
statistical machine translation. The rule ta-
ble is filtered by monolingual key phrases,
which are extracted from source text us-
ing a technique based on term extraction.
Experiments show that 78% of the rule ta-
ble is reduced without worsening trans-
lation performance. In most cases, our
approach results in measurable improve-
ments in BLEU score.
1 Introduction
In statistical machine translation (SMT) commu-
nity, the state-of-the-art method is to use rules that
contain hierarchical structures to model transla-
tion, such as the hierarchical phrase-based model
(Chiang, 2005). Rules are more powerful than
conventional phrase pairs because they contain
structural information for capturing long distance
reorderings. However, hierarchical translation
systems often suffer from a large rule table (the
collection of rules), which makes decoding slow
and memory-consuming.
In the training procedure of SMT systems, nu-
merous rules are extracted from the bilingual cor-
pus. During decoding, however, many of them are
rarely used. One of the reasons is that these rules
have low quality. The rule quality are usually eval-
uated by the conditional translation probabilities,
which focus on the correspondence between the
source and target phrases, while ignore the quality
of phrases in a monolingual corpus.
In this paper, we address the problem of reduc-
ing the rule table with the information of mono-
lingual corpus. We use C-value, a measurement
of automatic term recognition, to score source
phrases. A source phrase is regarded as a key
phrase if its score greater than a threshold. Note
that a source phrase is either a flat phrase consists
of words, or a hierarchical phrase consists of both
words and variables. For rule table reduction, the
rule whose source-side is not key phrase is dis-
carded.
Our approach is different from the previous re-
search. Johnson et al (2007) reduced the phrase
table based on the significance testing of phrase
pair co-occurrence in bilingual corpus. The ba-
sic difference is that they used statistical infor-
mation of bilingual corpus while we use that of
monolingual corpus. Shen et al (2008) pro-
posed a string-to-dependency model, which re-
stricted the target-side of a rule by dependency
structures. Their approach greatly reduced the rule
table, however, caused a slight decrease of trans-
lation quality. They obtained improvements by
incorporating an additional dependency language
model. Different from their research, we restrict
rules on the source-side. Furthermore, the system
complexity is not increased because no additional
model is introduced.
The hierarchical phrase-based model (Chiang,
2005) is used to build a translation system. Exper-
iments show that our approach discards 78% of the
rule table without worsening the translation qual-
ity.
2 Monolingual Phrase Scoring
2.1 Frequency
The basic metrics for phrase scoring is the fre-
quency that a phrase appears in a monolingual cor-
pus. The more frequent a source phrase appears in
a corpus, the greater possibility the rule that con-
tains the source phrase may be used.
However, one limitation of this metrics is that if
we filter the rule table by the source phrase with
lower frequency, most long phrase pairs will be
discarded. Because the longer the phrase is, the
less possibility it appears. However, long phrases
121
are very helpful for reducing ambiguity since they
contains more information than short phrases.
Another limitation is that the frequency metrics
focuses on a phrase appearing by itself while ig-
nores it appears as a substring of longer phrases.
It is therefore inadequate for hierarchical phrases.
We use an example for illustration. Considering
the following three rules (the subscripts indicate
word alignments):
R
1
:
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 47?54,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Improving Statistical Machine Translation Using
Domain Bilingual Multiword Expressions
Zhixiang Ren1 Yajuan Lu?1 Jie Cao1 Qun Liu1 Yun Huang2
1Key Lab. of Intelligent Info. Processing 2Department of Computer Science
Institute of Computing Technology School of Computing
Chinese Academy of Sciences National University of Singapore
P.O. Box 2704, Beijing 100190, China Computing 1, Law Link, Singapore 117590
{renzhixiang,lvyajuan huangyun@comp.nus.edu.sg
caojie,liuqun}@ict.ac.cn
Abstract
Multiword expressions (MWEs) have
been proved useful for many natural lan-
guage processing tasks. However, how to
use them to improve performance of statis-
tical machine translation (SMT) is not well
studied. This paper presents a simple yet
effective strategy to extract domain bilin-
gual multiword expressions. In addition,
we implement three methods to integrate
bilingual MWEs to Moses, the state-of-
the-art phrase-based machine translation
system. Experiments show that bilingual
MWEs could improve translation perfor-
mance significantly.
1 Introduction
Phrase-based machine translation model has been
proved a great improvement over the initial word-
based approaches (Brown et al, 1993). Recent
syntax-based models perform even better than
phrase-based models. However, when syntax-
based models are applied to new domain with few
syntax-annotated corpus, the translation perfor-
mance would decrease. To utilize the robustness
of phrases and make up the lack of syntax or se-
mantic information in phrase-based model for do-
main translation, we study domain bilingual mul-
tiword expressions and integrate them to the exist-
ing phrase-based model.
A multiword expression (MWE) can be consid-
ered as word sequence with relatively fixed struc-
ture representing special meanings. There is no
uniform definition of MWE, and many researchers
give different properties of MWE. Sag et al (2002)
roughly defined MWE as ?idiosyncratic interpre-
tations that cross word boundaries (or spaces)?.
Cruys and Moiro?n (2007) focused on the non-
compositional property of MWE, i.e. the property
that whole expression cannot be derived from their
component words. Stanford university launched
a MWE project1, in which different qualities of
MWE were presented. For bilingual multiword
expression (BiMWE), we define a bilingual phrase
as a bilingual MWE if (1) the source phrase is a
MWE in source language; (2) the source phrase
and the target phrase must be translated to each
other exactly, i.e. there is no additional (boundary)
word in target phrase which cannot find the corre-
sponding word in source phrase, and vice versa.
In recent years, many useful methods have been
proposed to extract MWEs or BiMWEs automati-
cally (Piao et al, 2005; Bannard, 2007; Fazly and
Stevenson, 2006). Since MWE usually constrains
possible senses of a polysemous word in context,
they can be used in many NLP applications such
as information retrieval, question answering, word
sense disambiguation and so on.
For machine translation, Piao et al (2005) have
noted that the issue of MWE identification and
accurate interpretation from source to target lan-
guage remained an unsolved problem for existing
MT systems. This problem is more severe when
MT systems are used to translate domain-specific
texts, since they may include technical terminol-
ogy as well as more general fixed expressions and
idioms. Although some MT systems may employ
a machine-readable bilingual dictionary of MWE,
it is time-consuming and inefficient to obtain this
resource manually. Therefore, some researchers
have tried to use automatically extracted bilingual
MWEs in SMT. Tanaka and Baldwin (2003) de-
scribed an approach of noun-noun compound ma-
chine translation, but no significant comparison
was presented. Lambert and Banchs (2005) pre-
sented a method in which bilingual MWEs were
used to modify the word alignment so as to im-
prove the SMT quality. In their work, a bilin-
gual MWE in training corpus was grouped as
1http://mwe.stanford.edu/
47
one unique token before training alignment mod-
els. They reported that both alignment quality
and translation accuracy were improved on a small
corpus. However, in their further study, they re-
ported even lower BLEU scores after grouping
MWEs according to part-of-speech on a large cor-
pus (Lambert and Banchs, 2006). Nonetheless,
since MWE represents liguistic knowledge, the
role and usefulness of MWE in full-scale SMT
is intuitively positive. The difficulty lies in how
to integrate bilingual MWEs into existing SMT
system to improve SMT performance, especially
when translating domain texts.
In this paper, we implement three methods that
integrate domain bilingual MWEs into a phrase-
based SMT system, and show that these ap-
proaches improve translation quality significantly.
The main difference between our methods and
Lambert and Banchs? work is that we directly aim
at improving the SMT performance rather than im-
proving the word alignment quality. In detail, dif-
ferences are listed as follows:
? Instead of using the bilingual n-gram trans-
lation model, we choose the phrase-based
SMT system, Moses2, which achieves sig-
nificantly better translation performance than
many other SMT systems and is a state-of-
the-art SMT system.
? Instead of improving translation indirectly
by improving the word alignment quality,
we directly target at the quality of transla-
tion. Some researchers have argued that large
gains of alignment performance under many
metrics only led to small gains in translation
performance (Ayan and Dorr, 2006; Fraser
and Marcu, 2007).
Besides the above differences, there are some
advantages of our approaches:
? In our method, automatically extracted
MWEs are used as additional resources rather
than as phrase-table filter. Since bilingual
MWEs are extracted according to noisy au-
tomatic word alignment, errors in word align-
ment would further propagate to the SMT and
hurt SMT performance.
? We conduct experiments on domain-specific
corpus. For one thing, domain-specific
2http://www.statmt.org/moses/
corpus potentially includes a large number
of technical terminologies as well as more
general fixed expressions and idioms, i.e.
domain-specific corpus has high MWE cov-
erage. For another, after the investigation,
current SMT system could not effectively
deal with these domain-specific MWEs es-
pecially for Chinese, since these MWEs are
more flexible and concise. Take the Chi-
nese term ?^ j ? (? for example. The
meaning of this term is ?soften hard mass
and dispel pathogenic accumulation?. Ev-
ery word of this term represents a special
meaning and cannot be understood literally
or without this context. These terms are dif-
ficult to be translated even for humans, let
alone machine translation. So, treating these
terms as MWEs and applying them in SMT
system have practical significance.
? In our approach, no additional corpus is intro-
duced. We attempt to extract useful MWEs
from the training corpus and adopt suitable
methods to apply them. Thus, it benefits
for the full exploitation of available resources
without increasing great time and space com-
plexities of SMT system.
The remainder of the paper is organized as fol-
lows. Section 2 describes the bilingual MWE ex-
traction technique. Section 3 proposes three meth-
ods to apply bilingual MWEs in SMT system.
Section 4 presents the experimental results. Sec-
tion 5 draws conclusions and describes the future
work. Since this paper mainly focuses on the ap-
plication of BiMWE in SMT, we only give a brief
introduction on monolingual and bilingual MWE
extraction.
2 Bilingual Multiword Expression
Extraction
In this section we describe our approach of bilin-
gual MWE extraction. In the first step, we obtain
monolingual MWEs from the Chinese part of par-
allel corpus. After that, we look for the translation
of the extracted MWEs from parallel corpus.
2.1 Automatic Extraction of MWEs
In the past two decades, many different ap-
proaches on automatic MWE identification were
reported. In general, those approaches can be
classified into three main trends: (1) statisti-
cal approaches (Pantel and Lin, 2001; Piao et
48
al., 2005), (2) syntactic approaches (Fazly and
Stevenson, 2006; Bannard, 2007), and (3) seman-
tic approaches (Baldwin et al, 2003; Cruys and
Moiro?n, 2007). Syntax-based and semantic-based
methods achieve high precision, but syntax or se-
mantic analysis has to be introduced as preparing
step, so it is difficult to apply them to domains with
few syntactical or semantic annotation. Statistical
approaches only consider frequency information,
so they can be used to obtain MWEs from bilin-
gual corpora without deeper syntactic or semantic
analysis. Most statistical measures only take two
words into account, so it not easy to extract MWEs
containing three or more than three words.
Log Likelihood Ratio (LLR) has been proved a
good statistical measurement of the association of
two random variables (Chang et al, 2002). We
adopt the idea of statistical approaches, and pro-
pose a new algorithm named LLR-based Hierar-
chical Reducing Algorithm (HRA for short) to ex-
tract MWEs with arbitrary lengths. To illustrate
our algorithm, firstly we define some useful items.
In the following definitions, we assume the given
sentence is ?A B C D E?.
Definition 1 Unit: A unit is any sub-string of the
given sentence. For example, ?A B?, ?C?, ?C D E?
are all units, but ?A B D? is not a unit.
Definition 2 List: A list is an ordered sequence of
units which exactly cover the given sentence. For
example, {?A?,?B C D?,?E?} forms a list.
Definition 3 Score: The score function only de-
fines on two adjacent units and return the LLR
between the last word of first unit and the first
word of the second unit3. For example, the score
of adjacent unit ?B C? and ?D E? is defined as
LLR(?C?,?D?).
Definition 4 Select: The selecting operator is to
find the two adjacent units with maximum score
in a list.
Definition 5 Reduce: The reducing operator is to
remove two specific adjacent units, concatenate
them, and put back the result unit to the removed
position. For example, if we want to reduce unit
?B C? and unit ?D? in list {?A?,?B C?,?D?,?E?},
we will get the list {?A?,?B C D?,?E?}.
Initially, every word in the sentence is consid-
ered as one unit and all these units form a initial
list L. If the sentence is of length N , then the
3we use a stoplist to eliminate the units containing func-
tion words by setting their score to 0
list contains N units, of course. The final set of
MWEs, S, is initialized to empty set. After initial-
ization, the algorithm will enter an iterating loop
with two steps: (1) select the two adjacent units
with maximum score in L, naming U1 and U2; and
(2) reduce U1 and U2 in L, and insert the reducing
result into the final set S. Our algorithm termi-
nates on two conditions: (1) if the maximum score
after selection is less than a given threshold; or (2)
if L contains only one unit.
c1(?? p ?? w  ? )
c1(?? p ?? w )
c1(?? p ?? w)
c1(??)
c2(p ?? w)
c2(p ??)
c2(p) c3(??) c4(w) c5()
c6(? )
c6(?) c7()
?? p ?? w  ? 147.1 6755.2 1059.6 0 0 809.6
Figure 1: Example of Hierarchical Reducing Al-
gorithm
Let us make the algorithm clearer with an ex-
ample. Assume the threshold of score is 20, the
given sentence is ??? p ?? w  ? ?4.
Figure 1 shows the hierarchical structure of given
sentence (based on LLR of adjacent words). In
this example, four MWEs (?p ???, ?p ??
w?, ?? ?, ??? p ?? w?) are extracted
in the order, and sub-strings over dotted line in fig-
ure 1 are not extracted.
From the above example, we can see that the
extracted MWEs correspond to human intuition.
In general, the basic idea of HRA is to reflect
the hierarchical structure pattern of natural lan-
guage. Furthermore, in the HRA, MWEs are mea-
sured with the minimum LLR of adjacent words
in them, which gives lexical confidence of ex-
tracted MWEs. Finally, suppose given sentence
has length N , HRA would definitely terminate
within N ? 1 iterations, which is very efficient.
However, HRA has a problem that it would ex-
tract substrings before extracting the whole string,
even if the substrings only appear in the particu-
lar whole string, which we consider useless. To
solve this problem, we use contextual features,
4The whole sentence means ?healthy tea for preventing
hyperlipidemia?, and we give the meaning for each Chi-
nese word: ??(preventing), p(hyper-), ??(-lipid-), w(-
emia), (for),?(healthy),(tea).
49
contextual entropy (Luo and Sun, 2003) and C-
value (Frantzi and Ananiadou, 1996), to filter out
those substrings which exist only in few MWEs.
2.2 Automatic Extraction of MWE?s
Translation
In subsection 2.1, we described the algorithm to
obtain MWEs, and we would like to introduce the
procedure to find their translations from parallel
corpus in this subsection.
For mining the English translations of Chinese
MWEs, we first obtain the candidate translations
of a given MWE from the parallel corpus. Steps
are listed as follows:
1. Run GIZA++5 to align words in the training
parallel corpus.
2. For a given MWE, find the bilingual sentence
pairs where the source language sentences in-
clude the MWE.
3. Extract the candidate translations of the
MWE from the above sentence pairs accord-
ing to the algorithm described by Och (2002).
After the above procedure, we have already
extracted all possible candidate translations of a
given MWE. The next step is to distinguish right
candidates from wrong candidates. We construct
perceptron-based classification model (Collins,
2002) to solve the problem. We design two
groups of features: translation features, which
describe the mutual translating chance between
source phrase and target phrase, and the language
features, which refer to how well a candidate
is a reasonable translation. The translation fea-
tures include: (1) the logarithm of source-target
translation probability; (2) the logarithm of target-
source translation probability; (3) the logarithm
of source-target lexical weighting; (4) the loga-
rithm of target-source lexical weighting; and (5)
the logarithm of the phrase pair?s LLR (Dunning,
1993). The first four features are exactly the same
as the four translation probabilities used in tradi-
tional phrase-based system (Koehn et al, 2003).
The language features include: (1) the left entropy
of the target phrase (Luo and Sun, 2003); (2) the
right entropy of the target phrase; (3) the first word
of the target phrase; (4) the last word of the target
phrase; and (5) all words in the target phrase.
5http://www.fjoch.com/GIZA++.html
We select and annotate 33000 phrase pairs ran-
domly, of which 30000 pairs are used as training
set and 3000 pairs are used as test set. We use the
perceptron training algorithm to train the model.
As the experiments reveal, the classification preci-
sion of this model is 91.67%.
3 Application of Bilingual MWEs
Intuitively, bilingual MWE is useful to improve
the performance of SMT. However, as we de-
scribed in section 1, it still needs further research
on how to integrate bilingual MWEs into SMT
system. In this section, we propose three methods
to utilize bilingual MWEs, and we will compare
their performance in section 4.
3.1 Model Retraining with Bilingual MWEs
Bilingual phrase table is very important for
phrase-based MT system. However, due to the er-
rors in automatic word alignment and unaligned
word extension in phrase extraction (Och, 2002),
many meaningless phrases would be extracted,
which results in inaccuracy of phrase probability
estimation. To alleviate this problem, we take the
automatically extracted bilingual MWEs as paral-
lel sentence pairs, add them into the training cor-
pus, and retrain the model using GIZA++. By
increasing the occurrences of bilingual MWEs,
which are good phrases, we expect that the align-
ment would be modified and the probability es-
timation would be more reasonable. Wu et al
(2008) also used this method to perform domain
adaption for SMT. Different from their approach,
in which bilingual MWEs are extracted from ad-
ditional corpus, we extract bilingual MWEs from
the original training set. The fact that additional
resources can improve the domain-specific SMT
performance was proved by many researchers
(Wu et al, 2008; Eck et al, 2004). However,
our method shows that making better use of the
resources in hand could also enhance the quality
of SMT system. We use ?Baseline+BiMWE? to
represent this method.
3.2 New Feature for Bilingual MWEs
Lopez and Resnik (2006) once pointed out that
better feature mining can lead to substantial gain
in translation quality. Inspired by this idea, we
append one feature into bilingual phrase table to
indicate that whether a bilingual phrase contains
bilingual MWEs. In other words, if the source lan-
guage phrase contains a MWE (as substring) and
50
the target language phrase contains the translation
of the MWE (as substring), the feature value is 1,
otherwise the feature value is set to 0. Due to the
high reliability of bilingual MWEs, we expect that
this feature could help SMT system to select bet-
ter and reasonable phrase pairs during translation.
We use ?Baseline+Feat? to represent this method.
3.3 Additional Phrase Table of bilingual
MWEs
Wu et al (2008) proposed a method to construct a
phrase table by a manually-made translation dic-
tionary. Instead of manually constructing transla-
tion dictionary, we construct an additional phrase
table containing automatically extracted bilingual
MWEs. As to probability assignment, we just as-
sign 1 to the four translation probabilities for sim-
plicity. Since Moses supports multiple bilingual
phrase tables, we combine the original phrase ta-
ble and new constructed bilingual MWE table. For
each phrase in input sentence during translation,
the decoder would search all candidate transla-
tion phrases in both phrase tables. We use ?Base-
line+NewBP? to represent this method.
4 Experiments
4.1 Data
We run experiments on two domain-specific patent
corpora: one is for traditional medicine domain,
and the other is for chemical industry domain. Our
translation tasks are Chinese-to-English.
In the traditional medicine domain, table 1
shows the data statistics. For language model, we
use SRI Language Modeling Toolkit6 to train a tri-
gram model with modified Kneser-Ney smoothing
(Chen and Goodman, 1998) on the target side of
training corpus. Using our bilingual MWE ex-
tracting algorithm, 80287 bilingual MWEs are ex-
tracted from the training set.
Chinese English
Training Sentences 120,355
Words 4,688,873 4,737,843
Dev Sentences 1,000
Words 31,722 32,390
Test Sentences 1,000
Words 41,643 40,551
Table 1: Traditional medicine corpus
6http://www.speech.sri.com/projects/srilm/
In the chemical industry domain, table 2 gives
the detail information of the data. In this experi-
ment, 59466 bilingual MWEs are extracted.
Chinese English
Training Sentences 120,856
Words 4,532,503 4,311,682
Dev Sentences 1,099
Words 42,122 40,521
Test Sentences 1,099
Words 41,069 39,210
Table 2: Chemical industry corpus
We test translation quality on test set and use the
open source tool mteval-vllb.pl7 to calculate case-
sensitive BLEU 4 score (Papineni et al, 2002) as
our evaluation criteria. For this evaluation, there is
only one reference per test sentence. We also per-
form statistical significant test between two trans-
lation results (Collins et al, 2005). The mean of
all scores and relative standard deviation are calcu-
lated with a 99% confidence interval of the mean.
4.2 MT Systems
We use the state-of-the-art phrase-based SMT sys-
tem, Moses, as our baseline system. The features
used in baseline system include: (1) four transla-
tion probability features; (2) one language model
feature; (3) distance-based and lexicalized distor-
tion model feature; (4) word penalty; (5) phrase
penalty. For ?Baseline+BiMWE? method, bilin-
gual MWEs are added into training corpus, as a
result, new alignment and new phrase table are
obtained. For ?Baseline+Feat? method, one ad-
ditional 0/1 feature are introduced to each entry in
phrase table. For ?Baseline+NewBP?, additional
phrase table constructed by bilingual MWEs is
used.
Features are combined in the log-linear model.
To obtain the best translation e? of the source sen-
tence f , log-linear model uses following equation:
e? = arg max
e
p(e|f)
= arg max
e
M
?
m=1
?mhm(e, f) (1)
in which hm and ?m denote the mth feature and
weight. The weights are automatically turned by
minimum error rate training (Och, 2002) on devel-
opment set.
7http://www.nist.gov/speech/tests/mt/resources/scoring.htm
51
4.3 Results
Methods BLEU
Baseline 0.2658
Baseline+BiMWE 0.2661
Baseline+Feat 0.2675
Baseline+NewBP 0.2719
Table 3: Translation results of using bilingual
MWEs in traditional medicine domain
Table 3 gives our experiment results. From
this table, we can see that, bilingual MWEs
improve translation quality in all cases. The
Baseline+NewBP method achieves the most im-
provement of 0.61% BLEU score compared
with the baseline system. The Baseline+Feat
method comes next with 0.17% BLEU score im-
provement. And the Baseline+BiMWE achieves
slightly higher translation quality than the baseline
system.
To our disappointment, however, none of these
improvements are statistical significant. We
manually examine the extracted bilingual MWEs
which are labeled positive by perceptron algorithm
and find that although the classification precision
is high (91.67%), the proportion of positive exam-
ple is relatively lower (76.69%). The low positive
proportion means that many negative instances
have been wrongly classified to positive, which in-
troduce noises. To remove noisy bilingual MWEs,
we use the length ratio x of the source phrase over
the target phrase to rank the bilingual MWEs la-
beled positive. Assume x follows Gaussian distri-
butions, then the ranking score of phrase pair (s, t)
is defined as the following formula:
Score(s, t) = log(LLR(s, t))? 1?
2pi?
?e?
(x??)2
2?2
(2)
Here the mean ? and variance ?2 are estimated
from the training set. After ranking by score, we
select the top 50000, 60000 and 70000 bilingual
MWEs to perform the three methods mentioned in
section 3. The results are showed in table 4.
From this table, we can conclude that: (1) All
the three methods on all settings improve BLEU
score; (2) Except the Baseline+BiMWE method,
the other two methods obtain significant improve-
ment of BLEU score (0.2728, 0.2734, 0.2724)
over baseline system (0.2658); (3) When the scale
of bilingual MWEs is relatively small (50000,
60000), the Baseline+Feat method performs better
Methods 50000 60000 70000
Baseline 0.2658
Baseline+BiMWE 0.2671 0.2686 0.2715
Baseline+Feat 0.2728 0.2734 0.2712
Baseline+NewBP 0.2662 0.2706 0.2724
Table 4: Translation results of using bilingual
MWEs in traditional medicine domain
than others; (4) As the number of bilingual MWEs
increasing, the Baseline+NewBP method outper-
forms the Baseline+Feat method; (5) Comparing
table 4 and 3, we can see it is not true that the
more bilingual MWEs, the better performance of
phrase-based SMT. This conclusion is the same as
(Lambert and Banchs, 2005).
To verify the assumption that bilingual MWEs
do indeed improve the SMT performance not only
on particular domain, we also perform some ex-
periments on chemical industry domain. Table 5
shows the results. From this table, we can see that
these three methods can improve the translation
performance on chemical industry domain as well
as on the traditional medicine domain.
Methods BLEU
Baseline 0.1882
Baseline+BiMWE 0.1928
Baseline+Feat 0.1917
Baseline+Newbp 0.1914
Table 5: Translation results of using bilingual
MWEs in chemical industry domain
4.4 Discussion
In order to know in what respects our methods im-
prove performance of translation, we manually an-
alyze some test sentences and gives some exam-
ples in this subsection.
(1) For the first example in table 6, ?? ??
is aligned to other words and not correctly trans-
lated in baseline system, while it is aligned to cor-
rect target phrase ?dredging meridians? in Base-
line+BiMWE, since the bilingual MWE (?? ??,
?dredging meridians?) has been added into train-
ing corpus and then aligned by GIZA++.
(2) For the second example in table 6, ??
? has two candidate translation in phrase table:
?tea? and ?medicated tea?. The baseline system
chooses the ?tea? as the translation of ?? ?,
while the Baseline+Feat system chooses the ?med-
52
Src T ?? ? k ?? ! ? ? ! ? ? ! ) 9 ! | Y ! S  _? ?
 , ? ?  ?E 8 "
Ref the obtained product is effective in tonifying blood , expelling cold , dredging meridians
, promoting production of body fluid , promoting urination , and tranquilizing mind ;
and can be used for supplementing nutrition and protecting health .
Baseline the food has effects in tonifying blood , dispelling cold , promoting salivation and water
, and tranquilizing , and tonic effects , and making nutritious health .
+Bimwe the food has effects in tonifying blood , dispelling cold , dredging meridians , promoting
salivation , promoting urination , and tranquilizing tonic , nutritious pulverizing .
Src ? ? ???J ! J ! ?J !?! 5J "
Ref the product can also be made into tablet , pill , powder , medicated tea , or injection .
Baseline may also be made into tablet , pill , powder , tea , or injection .
+Feat may also be made into tablet , pill , powder , medicated tea , or injection .
Table 6: Translation example
icated tea? because the additional feature gives
high probability of the correct translation ?medi-
cated tea?.
5 Conclusion and Future Works
This paper presents the LLR-based hierarchical
reducing algorithm to automatically extract bilin-
gual MWEs and investigates the performance of
three different application strategies in applying
bilingual MWEs for SMT system. The translation
results show that using an additional feature to rep-
resent whether a bilingual phrase contains bilin-
gual MWEs performs the best in most cases. The
other two strategies can also improve the quality
of SMT system, although not as much as the first
one. These results are encouraging and motivated
to do further research in this area.
The strategies of bilingual MWE application is
roughly simply and coarse in this paper. Com-
plicated approaches should be taken into account
during applying bilingual MWEs. For example,
we may consider other features of the bilingual
MWEs and examine their effect on the SMT per-
formance. Besides application in phrase-based
SMT system, bilingual MWEs may also be inte-
grated into other MT models such as hierarchical
phrase-based models or syntax-based translation
models. We will do further studies on improving
statistical machine translation using domain bilin-
gual MWEs.
Acknowledgments
This work is supported by National Natural Sci-
ence Foundation of China, Contracts 60603095
and 60873167. We would like to thank the anony-
mous reviewers for their insightful comments on
an earlier draft of this paper.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond aer: an extensive analysis of word align-
ments and their impact on mt. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 9?16.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisiton and Treatment,
pages 89?96.
Colin Bannard. 2007. A measure of syntactic flex-
ibility for automatically identifying multiword ex-
pressions in corpora. In Proceedings of the ACL
Workshop on A Broader Perspective on Multiword
Expressions, pages 1?8.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Baobao Chang, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation unit from
chinese-english parallel corpora. In Proceedings of
the first SIGHAN workshop on Chinese language
processing, pages 1?5.
Stanley F. Chen and Joshua Goodman. 1998. Am em-
pirical study of smoothing techniques for language
modeling. Technical report.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
53
Meeting on Association for Computational Linguis-
tics, pages 531?540.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions, pages 25?32.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Improving statistical machine translation in the med-
ical domain using the unified medical language sys-
tem. In Proceedings of the 20th international con-
ference on Computational Linguistics table of con-
tents, pages 792?798.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of the EACL,
pages 337?344.
Katerina T. Frantzi and Sophia Ananiadou. 1996. Ex-
tracting nested collocations. In Proceedings of the
16th conference on Computational linguistics, pages
41?46.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48?54.
Patrik Lambert and Rafael Banchs. 2005. Data in-
ferred multi-word expressions for statistical machine
translation. In Proceedings of Machine Translation
Summit X, pages 396?403.
Patrik Lambert and Rafael Banchs. 2006. Grouping
multi-word expressions according to part-of-speech
in statistical machine translation. In Proceedings of
the Workshop on Multi-word-expressions in a multi-
lingual context, pages 9?16.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What?s the
link? In proceedings of the 7th conference of the as-
sociation for machine translation in the Americas:
visions for the future of machine translation, pages
90?99.
Shengfen Luo and Maosong Sun. 2003. Two-character
chinese word extraction based on hybrid of internal
and contextual measures. In Proceedings of the sec-
ond SIGHAN workshop on Chinese language pro-
cessing, pages 24?30.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.d. thesis, Computer Science Department,
RWTH Aachen, Germany.
Patrick Pantel and Dekang Lin. 2001. A statistical cor-
pus based term extractor. In AI ?01: Proceedings of
the 14th Biennial Conference of the Canadian Soci-
ety on Computational Studies of Intelligence, pages
36?46.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics, pages 311?318.
Scott Songlin Piao, Paul Rayson, Dawn Archer, and
Tony McEnery. 2005. Comparing and combining a
semantic tagger and a statistical tool for mwe extrac-
tion. Computer Speech and Language, 19(4):378?
397.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for nlp. In
Proceedings of the 3th International Conference
on Intelligent Text Processing and Computational
Linguistics(CICLing-2002), pages 1?15.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of
the ACL-2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment, pages 17?24.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of Conference on Computa-
tional Linguistics (COLING), pages 993?1000.
54
Proceedings of ACL-08: HLT, pages 897?904,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Cascaded Linear Model for Joint Chinese Word Segmentation and
Part-of-Speech Tagging
Wenbin Jiang ? Liang Huang ? Qun Liu ? Yajuan Lu? ?
?Key Lab. of Intelligent Information Processing ?Department of Computer & Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
jiangwenbin@ict.ac.cn lhuang3@cis.upenn.edu
Abstract
We propose a cascaded linear model for
joint Chinese word segmentation and part-
of-speech tagging. With a character-based
perceptron as the core, combined with real-
valued features such as language models, the
cascaded model is able to efficiently uti-
lize knowledge sources that are inconvenient
to incorporate into the perceptron directly.
Experiments show that the cascaded model
achieves improved accuracies on both seg-
mentation only and joint segmentation and
part-of-speech tagging. On the Penn Chinese
Treebank 5.0, we obtain an error reduction of
18.5% on segmentation and 12% on joint seg-
mentation and part-of-speech tagging over the
perceptron-only baseline.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are important tasks in computer processing of
Chinese and other Asian languages. Several mod-
els were introduced for these problems, for example,
the Hidden Markov Model (HMM) (Rabiner, 1989),
Maximum Entropy Model (ME) (Ratnaparkhi and
Adwait, 1996), and Conditional Random Fields
(CRFs) (Lafferty et al, 2001). CRFs have the ad-
vantage of flexibility in representing features com-
pared to generative ones such as HMM, and usually
behaves the best in the two tasks. Another widely
used discriminative method is the perceptron algo-
rithm (Collins, 2002), which achieves comparable
performance to CRFs with much faster training, so
we base this work on the perceptron.
To segment and tag a character sequence, there
are two strategies to choose: performing POS tag-
ging following segmentation; or joint segmentation
and POS tagging (Joint S&T). Since the typical ap-
proach of discriminative models treats segmentation
as a labelling problem by assigning each character
a boundary tag (Xue and Shen, 2003), Joint S&T
can be conducted in a labelling fashion by expand-
ing boundary tags to include POS information (Ng
and Low, 2004). Compared to performing segmen-
tation and POS tagging one at a time, Joint S&T can
achieve higher accuracy not only on segmentation
but also on POS tagging (Ng and Low, 2004). Be-
sides the usual character-based features, additional
features dependent on POS?s or words can also be
employed to improve the performance. However, as
such features are generated dynamically during the
decoding procedure, two limitation arise: on the one
hand, the amount of parameters increases rapidly,
which is apt to overfit on training corpus; on the
other hand, exact inference by dynamic program-
ming is intractable because the current predication
relies on the results of prior predications. As a result,
many theoretically useful features such as higher-
order word or POS n-grams are difficult to be in-
corporated in the model efficiently.
To cope with this problem, we propose a cascaded
linear model inspired by the log-linear model (Och
and Ney, 2004) widely used in statistical machine
translation to incorporate different kinds of knowl-
edge sources. Shown in Figure 1, the cascaded
model has a two-layer architecture, with a character-
based perceptron as the core combined with other
real-valued features such as language models. We
897
Core
Linear Model
(Perceptron)
g1 =
?
i ?i ? fi
~?
Outside-layer
Linear Model
S = ?j wj ? gj
~w
f1
f2
f|R|
g1
Word LM: g2 = Pwlm(W ) g2
POS LM: g3 = Ptlm(T ) g3
Labelling: g4 = P (T |W ) g4
Generating: g5 = P (W |T ) g5
Length: g6 = |W | g6
S
Figure 1: Structure of Cascaded Linear Model. |R| denotes the scale of the feature space of the core perceptron.
will describe it in detail in Section 4. In this ar-
chitecture, knowledge sources that are intractable to
incorporate into the perceptron, can be easily incor-
porated into the outside linear model. In addition,
as these knowledge sources are regarded as separate
features, we can train their corresponding models in-
dependently with each other. This is an interesting
approach when the training corpus is large as it re-
duces the time and space consumption. Experiments
show that our cascaded model can utilize different
knowledge sources effectively and obtain accuracy
improvements on both segmentation and Joint S&T.
2 Segmentation and POS Tagging
Given a Chinese character sequence:
C1:n = C1 C2 .. Cn
the segmentation result can be depicted as:
C1:e1 Ce1+1:e2 .. Cem?1+1:em
while the segmentation and POS tagging result can
be depicted as:
C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tm
Here, Ci (i = 1..n) denotes Chinese character,
ti (i = 1..m) denotes POS tag, and Cl:r (l ? r)
denotes character sequence ranges from Cl to Cr.
We can see that segmentation and POS tagging task
is to divide a character sequence into several subse-
quences and label each of them a POS tag.
It is a better idea to perform segmentation and
POS tagging jointly in a uniform framework. Ac-
cording to Ng and Low (2004), the segmentation
task can be transformed to a tagging problem by as-
signing each character a boundary tag of the follow-
ing four types:
? b: the begin of the word
? m: the middle of the word
? e: the end of the word
? s: a single-character word
We can extract segmentation result by splitting
the labelled result into subsequences of pattern s or
bm?e which denote single-character word and multi-
character word respectively. In order to perform
POS tagging at the same time, we expand boundary
tags to include POS information by attaching a POS
to the tail of a boundary tag as a postfix following
Ng and Low (2004). As each tag is now composed
of a boundary part and a POS part, the joint S&T
problem is transformed to a uniform boundary-POS
labelling problem. A subsequence of boundary-POS
labelling result indicates a word with POS t only if
the boundary tag sequence composed of its bound-
ary part conforms to s or bm?e style, and all POS
tags in its POS part equal to t. For example, a tag
sequence b NN m NN e NN represents a three-
character word with POS tag NN .
3 The Perceptron
The perceptron algorithm introduced into NLP by
Collins (2002), is a simple but effective discrimina-
tive training method. It has comparable performance
898
Non-lexical-target Instances
Cn (n = ?2..2) C?2=e, C?1=?, C0=U, C1=/, C2=?
CnCn+1 (n = ?2..1) C?2C?1=e?, C?1C0=?U, C0C1=U/, C1C2=/?
C?1C1 C?1C1=?/
Lexical-target Instances
C0Cn (n = ?2..2) C0C?2=Ue, C0C?1=U?, C0C0=UU, C0C1=U/, C0C2=U?
C0CnCn+1 (n = ?2..1) C0C?2C?1=Ue?, C0C?1C0=U?U, C0C0C1=UU/, C0C1C2=U/?
C0C?1C1 C0C?1C1 =U?/
Table 1: Feature templates and instances. Suppose we are considering the third character ?U? in ?e? U /??.
to CRFs, while with much faster training. The per-
ceptron has been used in many NLP tasks, such as
POS tagging (Collins, 2002), Chinese word seg-
mentation (Ng and Low, 2004; Zhang and Clark,
2007) and so on. We trained a character-based per-
ceptron for Chinese Joint S&T, and found that the
perceptron itself could achieve considerably high ac-
curacy on segmentation and Joint S&T. In following
subsections, we describe the feature templates and
the perceptron training algorithm.
3.1 Feature Templates
The feature templates we adopted are selected from
those of Ng and Low (2004). To compare with oth-
ers conveniently, we excluded the ones forbidden by
the close test regulation of SIGHAN, for example,
Pu(C0), indicating whether character C0 is a punc-
tuation.
All feature templates and their instances are
shown in Table 1. C represents a Chinese char-
acter while the subscript of C indicates its posi-
tion in the sentence relative to the current charac-
ter (it has the subscript 0). Templates immediately
borrowed from Ng and Low (2004) are listed in
the upper column named non-lexical-target. We
called them non-lexical-target because predications
derived from them can predicate without consider-
ing the current character C0. Templates in the col-
umn below are expanded from the upper ones. We
add a field C0 to each template in the upper col-
umn, so that it can carry out predication according
to not only the context but also the current char-
acter itself. As predications generated from such
templates depend on the current character, we name
these templates lexical-target. Note that the tem-
plates of Ng and Low (2004) have already con-
tained some lexical-target ones. With the two kinds
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi)?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? +?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
of predications, the perceptron model will do exact
predicating to the best of its ability, and can back
off to approximately predicating if exact predicating
fails.
3.2 Training Algorithm
We adopt the perceptron training algorithm of
Collins (2002) to learn a discriminative model map-
ping from inputs x ? X to outputs y ? Y , where X
is the set of sentences in the training corpus and Y
is the set of corresponding labelled results. Follow-
ing Collins, we use a function GEN(x) generating
all candidate results of an input x , a representation
? mapping each training example (x, y) ? X ? Y
to a feature vector ?(x, y) ? Rd, and a parameter
vector ~? ? Rd corresponding to the feature vector.
d means the dimension of the vector space, it equals
to the amount of features in the model. For an input
character sequence x, we aim to find an output F (x)
satisfying:
F (x) = argmax
y?GEN(x)
?(x, y) ? ~? (1)
?(x, y) ? ~? represents the inner product of feature
vector ?(x, y) and the parameter vector ~?. We used
the algorithm depicted in Algorithm 1 to tune the
parameter vector ~?.
899
To alleviate overfitting on the training examples,
we use the refinement strategy called ?averaged pa-
rameters? (Collins, 2002) to the algorithm in Algo-
rithm 1.
4 Cascaded Linear Model
In theory, any useful knowledge can be incorporated
into the perceptron directly, besides the character-
based features already adopted. Additional features
most widely used are related to word or POS n-
grams. However, such features are generated dy-
namically during the decoding procedure so that
the feature space enlarges much more rapidly. Fig-
ure 2 shows the growing tendency of feature space
with the introduction of these features as well as the
character-based ones. We noticed that the templates
related to word unigrams and bigrams bring to the
feature space an enlargement much rapider than the
character-base ones, not to mention the higher-order
grams such as trigrams or 4-grams. In addition, even
though these higher grams were managed to be used,
there still remains another problem: as the current
predication relies on the results of prior ones, the
decoding procedure has to resort to approximate in-
ference by maintaining a list of N -best candidates at
each predication position, which evokes a potential
risk to depress the training.
To alleviate the drawbacks, we propose a cas-
caded linear model. It has a two-layer architec-
ture, with a perceptron as the core and another linear
model as the outside-layer. Instead of incorporat-
ing all features into the perceptron directly, we first
trained the perceptron using character-based fea-
tures, and several other sub-models using additional
ones such as word or POS n-grams, then trained the
outside-layer linear model using the outputs of these
sub-models, including the perceptron. Since the per-
ceptron is fixed during the second training step, the
whole training procedure need relative small time
and memory cost.
The outside-layer linear model, similar to those
in SMT, can synthetically utilize different knowl-
edge sources to conduct more accurate comparison
between candidates. In this layer, each knowledge
source is treated as a feature with a corresponding
weight denoting its relative importance. Suppose we
have n features gj (j = 1..n) coupled with n corre-
 0
 300000
 600000
 900000
 1.2e+006
 1.5e+006
 1.8e+006
 2.1e+006
 2.4e+006
 2.7e+006
 3e+006
 3.3e+006
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22
Fe
atu
re 
sp
ac
e
Introduction of features
growing curve
Figure 2: Feature space growing curve. The horizontal
scope X[i:j] denotes the introduction of different tem-
plates. X[0:5]: Cn (n = ?2..2); X[5:9]: CnCn+1 (n =
?2..1); X[9:10]: C?1C1; X[10:15]: C0Cn (n =
?2..2); X[15:19]: C0CnCn+1 (n = ?2..1); X[19:20]:
C0C?1C1; X[20:21]: W0; X[21:22]: W?1W0. W0 de-
notes the current considering word, while W?1 denotes
the word in front of W0. All the data are collected from
the training procedure on MSR corpus of SIGHAN bake-
off 2.
sponding weights wj (j = 1..n), each feature gj
gives a score gj(r) to a candidate r, then the total
score of r is given by:
S(r) =
?
j=1..n
wj ? gj(r) (2)
The decoding procedure aims to find the candidate
r? with the highest score:
r? = argmax
r
S(r) (3)
While the mission of the training procedure is to
tune the weights wj(j = 1..n) to guarantee that the
candidate r with the highest score happens to be the
best result with a high probability.
As all the sub-models, including the perceptron,
are regarded as separate features of the outside-layer
linear model, we can train them respectively with
special algorithms. In our experiments we trained
a 3-gram word language model measuring the flu-
ency of the segmentation result, a 4-gram POS lan-
guage model functioning as the product of state-
transition probabilities in HMM, and a word-POS
co-occurrence model describing how much probably
a word sequence coexists with a POS sequence. As
shown in Figure 1, the character-based perceptron is
used as the inside-layer linear model and sends its
output to the outside-layer. Besides the output of the
perceptron, the outside-layer also receive the outputs
900
of the word LM, the POS LM, the co-occurrence
model and a word count penalty which is similar to
the translation length penalty in SMT.
4.1 Language Model
Language model (LM) provides linguistic probabil-
ities of a word sequence. It is an important measure
of fluency of the translation in SMT. Formally, an
n-gram word LM approximates the probability of a
word sequence W = w1:m with the following prod-
uct:
Pwlm(W ) =
m
?
i=1
Pr(wi|wmax(0,i?n+1):i?1) (4)
Similarly, the n-gram POS LM of a POS sequence
T = t1:m is:
Ptlm(T ) =
m
?
i=1
Pr(ti|tmax(0,i?n+1):i?1) (5)
Notice that a bi-gram POS LM functions as the prod-
uct of transition probabilities in HMM.
4.2 Word-POS Co-occurrence Model
Given a training corpus with POS tags, we can train
a word-POS co-occurrence model to approximate
the probability that the word sequence of the la-
belled result co-exists with its corresponding POS
sequence. Using W = w1:m to denote the word se-
quence, T = t1:m to denote the corresponding POS
sequence, P (T |W ) to denote the probability that W
is labelled as T , and P (W |T ) to denote the prob-
ability that T generates W , we can define the co-
occurrence model as follows:
Co(W,T ) = P (T |W )?wt ? P (W |T )?tw (6)
?wt and ?tw denote the corresponding weights of the
two components.
Suppose the conditional probability Pr(t|w) de-
scribes the probability that the word w is labelled as
the POS t, while Pr(w|t) describes the probability
that the POS t generates the word w, then P (T |W )
can be approximated by:
P (T |W ) ?
m
?
k=1
Pr(tk|wk) (7)
And P (W |T ) can be approximated by:
P (W |T ) ?
m
?
k=1
Pr(wk|tk) (8)
Pr(w|t) and Pr(t|w) can be easily acquired by
Maximum Likelihood Estimates (MLE) over the
corpus. For instance, if the word w appears N times
in training corpus and is labelled as POS t for n
times, the probability Pr(t|w) can be estimated by
the formula below:
Pr(t|w) ? nN (9)
The probability Pr(w|t) could be estimated through
the same approach.
To facilitate tuning the weights, we use two com-
ponents of the co-occurrence model Co(W,T ) to
represent the co-occurrence probability of W and T ,
rather than use Co(W,T ) itself. In the rest of the
paper, we will call them labelling model and gener-
ating model respectively.
5 Decoder
Sequence segmentation and labelling problem can
be solved through a viterbi style decoding proce-
dure. In Chinese Joint S&T, the mission of the de-
coder is to find the boundary-POS labelled sequence
with the highest score. Given a Chinese character
sequence C1:n, the decoding procedure can proceed
in a left-right fashion with a dynamic programming
approach. By maintaining a stack of size N at each
position i of the sequence, we can preserve the top N
best candidate labelled results of subsequence C1:i
during decoding. At each position i, we enumer-
ate all possible word-POS pairs by assigning each
POS to each possible word formed from the charac-
ter subsequence spanning length l = 1..min(i,K)
(K is assigned 20 in all our experiments) and ending
at position i, then we derive all candidate results by
attaching each word-POS pair p (of length l) to the
tail of each candidate result at the prior position of p
(position i? l), and select for position i a N -best list
of candidate results from all these candidates. When
we derive a candidate result from a word-POS pair
p and a candidate q at prior position of p, we cal-
culate the scores of the word LM, the POS LM, the
labelling probability and the generating probability,
901
Algorithm 2 Decoding algorithm.
1: Input: character sequence C1:n
2: for i? 1 .. n do
3: L ? ?
4: for l? 1 .. min(i, K) do
5: w ? Ci?l+1:i
6: for t ? POS do
7: p? label w as t
8: for q ? V[i? l] do
9: append D(q, p) to L
10: sort L
11: V[i]? L[1 : N ]
12: Output: n-best results V[n]
as well as the score of the perceptron model. In ad-
dition, we add the score of the word count penalty as
another feature to alleviate the tendency of LMs to
favor shorter candidates. By equation 2, we can syn-
thetically evaluate all these scores to perform more
accurately comparing between candidates.
Algorithm 2 shows the decoding algorithm.
Lines 3 ? 11 generate a N -best list for each char-
acter position i. Line 4 scans words of all possible
lengths l (l = 1..min(i,K), where i points to the
current considering character). Line 6 enumerates
all POS?s for the word w spanning length l and end-
ing at position i. Line 8 considers each candidate
result in N -best list at prior position of the current
word. Function D derives the candidate result from
the word-POS pair p and the candidate q at prior po-
sition of p.
6 Experiments
We reported results from two set of experiments.
The first was conducted to test the performance of
the perceptron on segmentation on the corpus from
SIGHAN Bakeoff 2, including the Academia Sinica
Corpus (AS), the Hong Kong City University Cor-
pus (CityU), the Peking University Corpus (PKU)
and the Microsoft Research Corpus (MSR). The sec-
ond was conducted on the Penn Chinese Treebank
5.0 (CTB5.0) to test the performance of the cascaded
model on segmentation and Joint S&T. In all ex-
periments, we use the averaged parameters for the
perceptrons, and F-measure as the accuracy mea-
sure. With precision P and recall R, the balance
F-measure is defined as: F = 2PR/(P + R).
 0.966
 0.968
 0.97
 0.972
 0.974
 0.976
 0.978
 0.98
 0.982
 0.984
 0  1  2  3  4  5  6  7  8  9  10
F-
me
as
su
re
number of iterations
Perceptron Learning Curve
Non-lex + avg
Lex + avg
Figure 3: Averaged perceptron learning curves with Non-
lexical-target and Lexical-target feature templates.
AS CityU PKU MSR
SIGHAN best 0.952 0.943 0.950 0.964
Zhang & Clark 0.946 0.951 0.945 0.972
our model 0.954 0.958 0.940 0.975
Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN
best: best scores SIGHAN reported on the four corpus,
cited from Zhang and Clark (2007).
6.1 Experiments on SIGHAN Bakeoff
For convenience of comparing with others, we focus
only on the close test, which means that any extra
resource is forbidden except the designated train-
ing corpus. In order to test the performance of the
lexical-target templates and meanwhile determine
the best iterations over the training corpus, we ran-
domly chosen 2, 000 shorter sentences (less than 50
words) as the development set and the rest as the
training set (84, 294 sentences), then trained a per-
ceptron model named NON-LEX using only non-
lexical-target features and another named LEX us-
ing both the two kinds of features. Figure 3 shows
their learning curves depicting the F-measure on the
development set after 1 to 10 training iterations. We
found that LEX outperforms NON-LEX with a mar-
gin of about 0.002 at each iteration, and its learn-
ing curve reaches a tableland at iteration 7. Then
we trained LEX on each of the four corpora for 7
iterations. Test results listed in Table 2 shows that
this model obtains higher accuracy than the best of
SIGHAN Bakeoff 2 in three corpora (AS, CityU
and MSR). On the three corpora, it also outper-
formed the word-based perceptron model of Zhang
and Clark (2007). However, the accuracy on PKU
corpus is obvious lower than the best score SIGHAN
902
Training setting Test task F-measure
POS- Segmentation 0.971
POS+ Segmentation 0.973
POS+ Joint S&T 0.925
Table 3: F-measure on segmentation and Joint S&T of
perceptrons. POS-: perceptron trained without POS,
POS+: perceptron trained with POS.
reported, we need to conduct further research on this
problem.
6.2 Experiments on CTB5.0
We turned to experiments on CTB 5.0 to test the per-
formance of the cascaded model. According to the
usual practice in syntactic analysis, we choose chap-
ters 1? 260 (18074 sentences) as training set, chap-
ter 271? 300 (348 sentences) as test set and chapter
301? 325 (350 sentences) as development set.
At the first step, we conducted a group of contrast-
ing experiments on the core perceptron, the first con-
centrated on the segmentation regardless of the POS
information and reported the F-measure on segmen-
tation only, while the second performed Joint S&T
using POS information and reported the F-measure
both on segmentation and on Joint S&T. Note that
the accuracy of Joint S&T means that a word-POS
pair is recognized only if both the boundary tags and
the POS?s are correctly labelled.
The evaluation results are shown in Table 3. We
find that Joint S&T can also improve the segmen-
tation accuracy. However, the F-measure on Joint
S&T is obvious lower, about a rate of 95% to the
F-measure on segmentation. Similar trend appeared
in experiments of Ng and Low (2004), where they
conducted experiments on CTB 3.0 and achieved F-
measure 0.919 on Joint S&T, a ratio of 96% to the
F-measure 0.952 on segmentation.
As the next step, a group of experiments were
conducted to investigate how well the cascaded lin-
ear model performs. Here the core perceptron was
just the POS+ model in experiments above. Be-
sides this perceptron, other sub-models are trained
and used as additional features of the outside-layer
linear model. We used SRI Language Modelling
Toolkit (Stolcke and Andreas, 2002) to train a 3-
gram word LM with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), and a 4-gram POS
Features Segmentation F1 Joint S&T F1
All 0.9785 0.9341
All - PER 0.9049 0.8432
All - WLM 0.9785 0.9340
All - PLM 0.9752 0.9270
All - GPR 0.9774 0.9329
All - LPR 0.9765 0.9321
All - LEN 0.9772 0.9325
Table 4: Contribution of each feture. ALL: all features,
PER: perceptron model, WLM: word language model,
PLM: POS language model, GPR: generating model,
LPR: labelling model, LEN: word count penalty.
LM with Witten-Bell smoothing, and we trained
a word-POS co-occurrence model simply by MLE
without smoothing. To obtain their corresponding
weights, we adapted the minimum-error-rate train-
ing algorithm (Och, 2003) to train the outside-layer
model. In order to inspect how much improvement
each feature brings into the cascaded model, every
time we removed a feature while retaining others,
then retrained the model and tested its performance
on the test set.
Table 4 shows experiments results. We find that
the cascaded model achieves a F-measure increment
of about 0.5 points on segmentation and about 0.9
points on Joint S&T, over the perceptron-only model
POS+. We also find that the perceptron model func-
tions as the kernel of the outside-layer linear model.
Without the perceptron, the cascaded model (if we
can still call it ?cascaded?) performs poorly on both
segmentation and Joint S&T. Among other features,
the 4-gram POS LM plays the most important role,
removing this feature causes F-measure decrement
of 0.33 points on segmentation and 0.71 points on
Joint S&T. Another important feature is the labelling
model. Without it, the F-measure on segmentation
and Joint S&T both suffer a decrement of 0.2 points.
The generating model, which functions as that in
HMM, brings an improvement of about 0.1 points
to each test item. However unlike the three fea-
tures, the word LM brings very tiny improvement.
We suppose that the character-based features used
in the perceptron play a similar role as the lower-
order word LM, and it would be helpful if we train
a higher-order word LM on a larger scale corpus.
Finally, the word count penalty gives improvement
to the cascaded model, 0.13 points on segmentation
903
and 0.16 points on Joint S&T.
In summary, the cascaded model can utilize these
knowledge sources effectively, without causing the
feature space of the percptron becoming even larger.
Experimental results show that, it achieves obvious
improvement over the perceptron-only model, about
from 0.973 to 0.978 on segmentation, and from
0.925 to 0.934 on Joint S&T, with error reductions
of 18.5% and 12% respectively.
7 Conclusions
We proposed a cascaded linear model for Chinese
Joint S&T. Under this model, many knowledge
sources that may be intractable to be incorporated
into the perceptron directly, can be utilized effec-
tively in the outside-layer linear model. This is a
substitute method to use both local and non-local
features, and it would be especially useful when the
training corpus is very large.
However, can the perceptron incorporate all the
knowledge used in the outside-layer linear model?
If this cascaded linear model were chosen, could
more accurate generative models (LMs, word-POS
co-occurrence model) be obtained by training on
large scale corpus even if the corpus is not correctly
labelled entirely, or by self-training on raw corpus in
a similar approach to that of McClosky (2006)? In
addition, all knowledge sources we used in the core
perceptron and the outside-layer linear model come
from the training corpus, whereas many open knowl-
edge sources (lexicon etc.) can be used to improve
performance (Ng and Low, 2004). How can we uti-
lize these knowledge sources effectively? We will
investigate these problems in the following work.
Acknowledgement
This work was done while L. H. was visiting
CAS/ICT. The authors were supported by National
Natural Science Foundation of China, Contracts
60736014 and 60573188, and 863 State Key Project
No. 2006AA010108 (W. J., Q. L., and Y. L.), and by
NSF ITR EIA-0205456 (L. H.). We would also like
to Hwee-Tou Ng for sharing his code, and Yang Liu
and Yun Huang for suggestions.
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the 18th ICML, pages 282?289, Mas-
sachusetts, USA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of ACL 2006.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Lawrence. R. Rabiner. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of IEEE, pages 257?286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Conference.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 311?318.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL 2007.
904
Collocation Translation Acquisition Using Monolingual Corpora 
Yajuan L? 
Microsoft Research Asia 
5F Sigma Center,  
No. 49 Zhichun Road, Haidian District, 
Beijing, China, 100080 
t-yjlv@microsoft.com 
Ming ZHOU 
Microsoft Research Asia 
5F Sigma Center,  
No. 49 Zhichun Road, Haidian District, 
Beijing, China, 100080 
mingzhou@microsoft.com 
 
Abstract 
Collocation translation is important for 
machine translation and many other NLP tasks. 
Unlike previous methods using bilingual 
parallel corpora, this paper presents a new 
method for acquiring collocation translations 
by making use of monolingual corpora and 
linguistic knowledge. First, dependency triples 
are extracted from Chinese and English 
corpora with dependency parsers. Then, a 
dependency triple translation model is 
estimated using the EM algorithm based on a 
dependency correspondence assumption. The 
generated triple translation model is used to 
extract collocation translations from two 
monolingual corpora. Experiments show that 
our approach outperforms the existing 
monolingual corpus based methods in 
dependency triple translation and achieves 
promising results in collocation translation 
extraction. 
1 Introduction 
A collocation is an arbitrary and recurrent word 
combination (Benson, 1990). Previous work in 
collocation acquisition varies in the kinds of 
collocations they detect. These range from two-
word to multi-word, with or without syntactic 
structure (Smadja 1993; Lin, 1998; Pearce, 2001; 
Seretan et al 2003). In this paper, a collocation 
refers to a recurrent word pair linked with a certain 
syntactic relation. For instance, <solve, verb-object, 
problem> is a collocation with a syntactic relation 
verb-object. 
Translation of collocations is difficult for non-
native speakers. Many collocation translations are 
idiosyncratic in the sense that they are 
unpredictable by syntactic or semantic features. 
Consider Chinese to English translation. The 
translations of ???? can be ?solve? or ?resolve?. 
The translations of ???? can be ?problem? or 
?issue?. However, translations of the collocation 
??? ~ ??? as ?solve~problem? or ?resolve~ 
issue? is preferred over ?solve~issue? or ?resolve 
~problem?. Automatically acquiring these 
collocation translations will be very useful for 
machine translation, cross language information 
retrieval, second language learning and many other 
NLP applications. (Smadja et al, 1996; Gao et al, 
2002; Wu and Zhou, 2003).  
Some studies have been done for acquiring 
collocation translations using parallel corpora 
(Smadja et al 1996; Kupiec, 1993; Echizen-ya et 
al., 2003). These works implicitly assume that a 
bilingual corpus on a large scale can be obtained 
easily. However, despite efforts in compiling 
parallel corpora, sufficient amounts of such 
corpora are still unavailable. Instead of heavily 
relying on bilingual corpora, this paper aims to 
solve the bottleneck in a different way: to mine 
bilingual knowledge from structured monolingual 
corpora, which can be more easily obtained in a 
large volume. 
Our method is based on the observation that 
despite the great differences between Chinese and 
English, the main dependency relations tend to 
have a strong direct correspondence (Zhou et al, 
2001). Based on this assumption, a new translation 
model based on dependency triples is proposed. 
The translation probabilities are estimated from 
two monolingual corpora using the EM algorithm 
with the help of a bilingual translation dictionary. 
Experimental results show that the proposed triple 
translation model outperforms the other three 
models in comparison. The obtained triple 
translation model is also used for collocation 
translation extraction. Evaluation results 
demonstrate the effectiveness of our method.  
The remainder of this paper is organized as 
follows. Section 2 provides a brief description on 
the related work. Section 3 describes our triple 
translation model and training algorithm. Section 4 
extracts collocation translations from two 
independent monolingual corpora. Section 5 
evaluates the proposed method, and the last section 
draws conclusions and presents the future work. 
2 Related work 
There has been much previous work done on 
monolingual collocation extraction. They can in 
general be classified into two types: window-based 
and syntax-based methods. The former extracts 
collocations within a fixed window (Church and 
Hanks 1990; Smadja, 1993). The latter extracts 
collocations which have a syntactic relationship 
(Lin, 1998; Seretan et al, 2003). The syntax-based 
method becomes more favorable with recent 
significant increases in parsing efficiency and 
accuracy. Several metrics have been adopted to 
measure the association strength in collocation 
extraction. Thanopoulos et al (2002) give 
comparative evaluations on these metrics. 
Most previous research in translation knowledge 
acquisition is based on parallel corpora (Brown et 
al., 1993). As for collocation translation, Smadja et 
al. (1996) implement a system to extract 
collocation translations from a parallel English-
French corpus. English collocations are first 
extracted using the Xtract system, then 
corresponding French translations are sought based 
on the Dice coefficient. Echizen-ya et al (2003) 
propose a method to extract bilingual collocations 
using recursive chain-link-type learning. In 
addition to collocation translation, there is also 
some related work in acquiring phrase or term 
translations from parallel corpus (Kupiec, 1993; 
Yamamoto and Matsumoto 2000).   
Since large aligned bilingual corpora are hard to 
obtain, some research has been conducted to 
exploit translation knowledge from non-parallel 
corpora. Their work is mainly on word level. 
Koehn and Knight (2000) presents an approach to 
estimating word translation probabilities using 
unrelated monolingual corpora with the EM 
algorithm. The method exhibits promising results 
in selecting the right translation among several 
options provided by bilingual dictionary. Zhou et 
al.(2001) proposes a method to simulate translation 
probability with a cross language similarity score, 
which is estimated from monolingual corpora 
based on mutual information. The method achieves 
good results in word translation selection. In 
addition, (Dagan and Itai, 1994) and (Li, 2002) 
propose using two monolingual corpora for word 
sense disambiguation. (Fung, 1998) uses an IR 
approach to induce new word translations from 
comparable corpora. (Rapp, 1999) and (Koehn and 
Knight, 2002) extract new word translations from 
non-parallel corpus. (Cao and Li, 2002) acquire 
noun phrase translations by making use of web 
data. (Wu and Zhou, 2003) also make full use of 
large scale monolingual corpora and limited 
bilingual corpora for synonymous collocation 
extraction. 
3 Training a triple translation model from 
monolingual corpora  
In this section, we first describe the dependency 
correspondence assumption underlying our 
approach. Then a dependency triple translation 
model and the monolingual corpus based training 
algorithm are proposed. The obtained triple 
translation model will be used for collocation 
translation extraction in next section. 
3.1 Dependency correspondence between 
Chinese and English 
A dependency triple consists of a head, a 
dependant, and a dependency relation. Using a 
dependency parser, a sentence can be analyzed into 
dependency triples. We represent a triple as 
(w1,r,w2), where w1 and w2 are words and r is the 
dependency relation. It means that w2 has a 
dependency relation r with w1. For example, a 
triple (overcome, verb-object, difficulty) means that 
?difficulty? is the object of the verb ?overcome?. 
Among all the dependency relations, we only 
consider the following three key types that we 
think, are the most important in text analysis and 
machine translation: verb-object (VO), noun-
adj(AN), and verb- adv(AV).  
It is our observation that there is a strong 
correspondence in major dependency relations in 
the translation between English and Chinese. For 
example, an object-verb relation in Chinese 
(e.g.(??, VO, ??)) is usually translated into 
the same verb-object relation in English(e.g. 
(overcome, VO, difficulty)). 
This assumption has been experimentally 
justified based on a large and balanced bilingual 
corpus in our previous work (Zhou et al, 2001).  
We come to the conclusion that more than 80% of 
the above dependency relations have a one-one 
mapping between Chinese and English. We can 
conclude that there is indeed a very strong 
correspondence between Chinese and English in 
the three considered dependency relations. This 
fact will be used to estimate triple translation 
model using two monolingual corpora. 
3.2 Triple translation model 
According to Bayes?s theorem, given a Chinese 
triple ),,( 21 crcc ctri = , and the set of its candidate 
English triple translations ),,( 21 eree etri = , the 
best English triple )?,,?(? 21 eree etri = is the one that 
maximizes the Equation (1): 
)|()(maxarg      
)(/)|()(maxarg      
)|(maxarg?
tritritri
e
tritritritri
e
tritri
e
tri
ecpep
cpecpep
cepe
tri
tri
tri
=
=
=
   (1) 
where )( triep is usually called the language model 
and )|( tritri ecp is usually called the translation 
model. 
Language Model 
The language model )( triep  is calculated with 
English triples database. In order to tackle with the 
data sparseness problem, we smooth the language 
model with an interpolation method, as described 
below. 
When the given English triple occurs in the 
corpus, we can calculate it as in Equation (2). 
N
erefreq
ep etri
),,(
)( 21=                   (2) 
where ),,( 21 erefreq e  represents the frequency  of 
triple trie . N represents the total counts of all the 
English triples in the training corpus. 
For an English triple ),,( 21 eree etri = , if we 
assume that two words 1e and 2e are conditionally 
independent given the relation er , Equation (2) can 
be rewritten as in (3)(Lin, 1998). 
)|()|()()( 21 eeetri repreprpep =        (3) 
where      
N
rfreqrp ee
,*)(*,)( = ,  
,*)(*,
,*),(
)|( 11
e
e
e rfreq
refreq
rep = , 
,*)(*,
),(*,
)|( 222
e
e
rfreq
erfreq
rep = . 
The wildcard symbol * means it can be any word 
or relation. With Equations (2) and (3), we get the 
interpolated language model as shown in (4). 
)|()|()()1(
)(
)( 21 eee
tri
tri repreprpN
efreq
ep ?? ?+=  (4) 
where 10 << ? . ?  is calculated as below: 
)(1
11
triefreq+
?=?                       (5) 
Translation Model 
We simplify the translation model according the 
following two assumptions. 
Assumption 1: Given an English triple trie , and 
the corresponding Chinese dependency relation cr , 
1c and 2c are conditionally independent. We have:   
)|(),|(),|(                 
)|,,()|(
21
21
trictrictric
trictritri
erpercpercp
ecrcpecp
=
=    (6) 
Assumption 2: For an English triple trie , 
assume that ic  only depends on {1,2})  (i ?ie , 
and cr  only depends on er  . Equation (6) is 
rewritten as: 
)|()|()|(                
)|(),|(),|()|(
2211
21
ec
trietrictrictritri
rrpecpecp
erpercpercpecp
=
=       (7) 
Notice that )|( 11 ecp and )|( 22 ecp  are 
translation probabilities within triples, they are 
different from the unrestricted probabilities such as 
the ones in IBM models (Brown et al, 1993). We 
distinguish translation probability between head 
( )|( 11 ecp ) and dependant ( )|( 22 ecp ). In the 
rest of the paper, we use )|( ecphead and 
)|( ecpdep to denote the head translation 
probability and dependant translation probability 
respectively. 
As the correspondence between the same 
dependency relation across English and Chinese is 
strong, we simply assume 1)|( =ec rrp for the 
corresponding er  and cr , and 0)|( =ec rrp for the 
other cases. 
)|( 11 ecphead   and )|( 22 ecpdep cannot be 
estimated directly because there is no triple-aligned 
corpus available. Here, we present an approach to 
estimating these probabilities from two 
monolingual corpora based on the EM algorithm. 
3.3 Estimation of word translation 
probability using the EM algorithm 
Chinese and English corpora are first parsed 
using a dependency parser, and two dependency 
triple databases are generated. The candidate 
English translation set of Chinese triples is 
generated through a bilingual dictionary and the 
assumption of strong correspondence of 
dependency relations. There is a risk that unrelated 
triples in Chinese and English can be connected 
with this method. However, as the conditions that 
are used to make the connection are quite strong 
(i.e. possible word translations in the same triple 
structure), we believe that this risk, is not very 
severe. Then, the expectation maximization (EM) 
algorithm is introduced to iteratively strengthen the 
correct connections and weaken the incorrect 
connections.  
EM Algorithm 
According to section 3.2, the translation 
probabilities from a Chinese triple tric  to an 
English triple trie can be computed using the 
English triple language model )( triep and a 
translation model from English to Chinese 
)|( tritri ecp . The English language model can be 
estimated using Equation (4) and the translation 
model can be calculated using Equation (7). The 
translation probabilities )|( ecphead and 
)|( ecpdep are initially set to a uniform distribution 
as follows: 
??
??
? ??
?==
otherwise
cif
ecpecp eedephead
       ,0
)(    ,1
)|()|(      (8) 
Where e? represents the translation set of the 
English word e.  
Then, the word translation probabilities are 
estimated iteratively using the EM algorithm. 
Figure 1 gives a formal description of the EM 
algorithm.  
 
Figure 1:  EM algorithm 
The basic idea is that under the restriction of the 
English triple language model )( triep  and 
translation dictionary, we wish to estimate the 
translation probabilities )|( ecphead  and 
)|( ecpdep that best explain the Chinese triple 
database as a translation from the English triple 
database. In each iteration, the normalized triple 
translation probabilities are used to update the 
word translation probabilities. Intuitively, after 
finding the most probable translation of the 
Chinese triple, we can collect counts for the word 
translation it contains. Since the English triple 
language model provides context information for 
the disambiguation of the Chinese words, only the 
appropriate occurrences are counted. 
Now, with the language model estimated using 
Equation (4) and the translation probabilities 
estimated using EM algorithm, we can compute the 
best triple translation for a given Chinese triple 
using Equations (1) and (7).  
4 Collocation translation extraction from two 
monolingual corpora 
This section describes how to extract collocation 
translation from independent monolingual corpora. 
First, collocations are extracted from a 
monolingual triples database. Then, collocation 
translations are acquired using the triple translation 
model obtained in section 3. 
4.1 Monolingual collocation extraction 
As introduced in section 2, much work has been 
done to extract collocations. Among all the 
measure metrics, log likelihood ratio (LLR) has 
proved to give better results (Duning, 1993; 
Thanopoulos et al, 2002). In this paper, we take 
LLR as the metric to extract collocations from a 
dependency triple database.  
For a given Chinese triple ),,( 21 crcc ctri = , the 
LLR score is calculated as follows:  
NN
dcdcdbdb
cacababa
ddccbbaaLogl
log         
)log()()log()(         
)log()()log()(         
loglogloglog
+
++?++?
++?++?
+++=
  (9) 
where,  
.
),,,(),(*,
),,,(,*),(
),,,(
212
211
21
cbaNd
crcfreqcrfreqc
crcfreqrcfreqb
crcfreqa
cc
cc
c
???=
?=
?=
=
 
N is the total counts of all Chinese triples. 
Those triples whose LLR values are larger than a 
given threshold are taken as a collocation. This 
syntax-based collocation has the advantage that it 
can represent both adjacent and long distance word 
association. Here, we only extract the three main 
types of collocation that have been mentioned in 
section 3.1.  
4.2 Collocation translation extraction  
For the acquired collocations, we try to extract 
their translations from the other monolingual 
Train language model for English triple )( triep ; 
Initialize word translation probabilities )|( ecphead  
and )|( ecpdep uniformly as in Equation (8); 
Iterate 
  Set )|( ecscorehead and )|( ecscoredep to 0 for all 
dictionary entries (c,e); 
   for all Chinese triples ),,( 21 crcc ctri =  
        for all candidate English triple translations  
),,( 21 eree etri =  
           compute triple translation probability 
)|( tritri cep by
)|()|()|()( 2211 ecdepheadtri rrpecpecpep  
end for 
        normalize )|( tritri cep , so that their sum is 1; 
        for all triple translation ),,( 21 eree etri =  
             add )|( tritri cep to )|( 11 ecscorehead  
             add )|( tritri cep to )|( 22 ecscoredep  
        endfor 
    endfor 
    for all translation pairs (c,e) 
       set )|( ecphead  to normalized )|( ecscorehead ; 
       set )|( ecpdep  to normalized )|( ecscoredep ; 
    endfor 
enditerate 
corpus using the triple translation model trained 
with the method proposed in section 3.  
Our objective is to acquire collocation 
translations as translation knowledge for a machine 
translation system, so only highly reliable 
collocation translations are extracted. Figure 2 
describes the algorithm for Chinese-English 
collocation translation extraction. It can be seen 
that the best English triple candidate is extracted as 
the translation of the given Chinese collocation 
only if the Chinese collocation is also the best 
translation candidate of the English triple. But the 
English triple is not necessarily a collocation. 
English collocation translations can be extracted in 
a similar way. 
 
Figure 2: Collocation translation extraction 
4.3 Implementation of our approach 
Our English corpus is from Wall Street Journal 
(1987-1992) and Associated Press (1988-1990), 
and the Chinese corpus is from People?s Daily 
(1980-1998). The two corpora are parsed using the 
NLPWin parser1 (Heidorn, 2000). The statistics for 
three main types of dependency triples are shown 
in tables 1 and 2. Token refers to the total number 
of triple occurrences and Type refers to the number 
of unique triples in the corpus. Statistic for the 
extracted Chinese collocations and the collocation 
translations is shown in Table 3. 
 
Class #Type #Token 
VO 1,579,783 19,168,229
AN 311,560 5,383,200 
AV 546,054 9,467,103 
Table 1:  Chinese dependency triples 
                                                     
1  The NLPWin parser is a rule-based parser 
developed at Microsoft research, which parses several 
languages including Chinese and English. Its output can 
be a phrase structure parse tree or a logical form which 
is represented with dependency triples. 
Class #Type #Token 
VO 1,526,747 8,943,903 
AN 1,163,440 6,386,097 
AV 215,110 1,034,410 
Table 2:  English dependency triples 
Class #Type #Translated
VO 99,609 28,841 
AN 35,951 12,615 
AV 46,515 6,176 
Table 3:  Extracted Chinese collocations 
and E-C translation pairs 
The translation dictionaries we used in training 
and translation are combined from two dictionaries: 
HITDic and NLPWinDic 2 . The final E-C 
dictionary contains 126,135 entries, and C-E 
dictionary contains 91,275 entries. 
5 Experiments and evaluation   
To evaluate the effectiveness of our methods, 
two experiments have been conducted. The first 
one compares our method with three other 
monolingual corpus based methods in triple 
translation. The second one evaluates the accuracy 
of the acquired collocation translation. 
5.1 Dependency triple translation 
Triple translation experiments are conducted 
from Chinese to English. We randomly selected 
2000 Chinese triples (whose frequency is larger 
than 2) from the dependency triple database. The 
standard translation answer sets were built 
manually by three linguistic experts. For each 
Chinese triple, its English translation set contain 
English triples provided by anyone of the three 
linguists. Among 2000 candidate triples, there are 
101 triples that can?t be translated into English 
triples with same relation. For example, the 
Chinese triple (?, VO, ??) should be translated 
into ?bargain?. The two words in triple cannot be 
translated separately. We call this kind of 
collocation translation no-compositional 
translations. Our current model cannot deal with 
this kind of translation. In addition, there are also 
157 error dependency triples, which result from 
parsing mistakes. We filtered out these two kinds 
of triples and got a standard test set with 1,742 
Chinese triples and 4,645 translations in total.   
We compare our triple translation model with 
three other models on the same standard test set 
with the same translation dictionary. As the 
                                                     
2 These two dictionaries are built by Harbin Institute 
of Technology and Microsoft Research respectively.  
For each Chinese collocation colc : 
a. Acquire the best English triple translation 
trie?  using C-E triple translation model: 
          )|()(maxarg? tritritri
e
tri ecpepe
tri
=  
b. For the acquired trie? , calculate  the best 
Chinese triple translation tric? using E-C 
triple translation model: 
)|?()(maxarg? tritritri
c
tri cepcpc
tri
=  
c. If colc = tric?  , add colc ? trie? to collocation 
translation database. 
baseline experiment, Model A selects the highest-
frequency translation for each word in triple; 
Model B selects translation with the maximal 
target triple probability, as proposed in (Dagan 
1994); Model C selects translation using both 
language model and translation model, but the 
translation probability is simulated by a similarity 
score which is estimated from monolingual corpus 
using mutual information measure (Zhou et al, 
2001). And our triple translation model is model D.  
Suppose ),,( 21 crcc ctri = is the Chinese triple to 
be translated. The four compared models can be 
formally expressed as follows: 
Model A: 
))((maxarg,)),((maxarg( 2
)(
1
)(
max
2211
efreqrefreqe
cTranse
e
cTranse ??
=  
Model B:  
),,(maxarg)(maxarg 21
)(
)(
max
22
11
erepepe e
cTranse
cTranse
tri
etri
?
?
==  
Model C:  
)),Sim(),Sim()((maxarg       
))|(likelyhood)((maxarg
2211
)(
)(
max
22
11
ceceep
ecepe
tri
cTranse
cTranse
tritritri
etri
??=
?=
?
?
 
where, ),Sim( ce is similarity score between e 
and c (Zhou et al, 2001).  
Model D (our model):  
))|()|()|()((maxarg      
))|()((maxarg
2211
)(
)(
max
22
11
ecdepheadtri
cTranse
cTranse
tritritri
e
rrpecpecpep
ecpepe
tri
?
?
=
=
 
Accuracy(%)  Cove- Rage(%) Top 1 Top 3 
Oracle
(%)
Model A 17.21 ---- 
Model B 33.56 53.79 
Model C 35.88 57.74 
Model D 
83.98 
36.91 58.58 
66.30
Table 4:  Translation results comparison 
The evaluation results on the standard test set are 
shown in Table 4, where coverage is the 
percentages of triples which can be translated. 
Some triples can?t be translated by Model B, C and 
D because of the lack of dictionary translations or 
data sparseness in triples.  In fact, the coverage of 
Model A is 100%. It was set to the same as others 
in order to compare accuracy using the same test 
set. The oracle score is the upper bound accuracy 
under the conditions of current translation 
dictionary and standard test set. Top N accuracy is 
defined as the percentage of triples whose selected 
top N translations include correct translations. 
We can see that both Model C and Model D 
achieve better results than Model B. This shows 
that the translation model trained from 
monolingual corpora really helps to improve the 
performance of translation. Our model also 
outperforms Model C, which demonstrates the 
probabilities trained by our EM algorithm achieve 
better performance than heuristic similarity scores.  
In fact, our evaluation method is very rigorous. 
To avoid bias in evaluation, we take human 
translation results as standard. The real translation 
accuracy is reasonably better than the evaluation 
results. But as we can see, compared to the oracle 
score, the current models still have much room for 
improvement. And coverage is also not high due to 
the limitations of the translation dictionary and the 
sparse data problem. 
5.2 Collocation translation extraction 
47,632 Chinese collocation translations are 
extracted with the method proposed in section 4. 
We randomly selected 1000 translations for 
evaluation. Three linguistic experts tag the 
acceptability of the translation. Those translations 
that are tagged as acceptable by at least two 
experts are evaluated as correct. The evaluation 
results are shown in Table 5.  
 Total Acceptance Accuracy (%)
VO 590 373 63.22 
AN 292 199 68.15 
AV 118 60 50.85 
All 1000 632 63.20 
ColTrans 334 241 72.16 
Table 5:  Extracted collocation translation results 
We can see that the extracted collocation 
translations achieve a much better result than triple 
translation. The average accuracy is 63.20% and 
the collocations with relation AN achieve the 
highest accuracy of 68.15%. If we only consider 
those Chinese collocations whose translations are 
also English collocations, we obtain an even better 
accuracy of 72.16% as shown in the last row of 
Table 5. The results justify our idea that we can 
acquire reliable translation for collocation by 
making use of triple translation model in two 
directions. 
These acquired collocation translations are very 
valuable for translation knowledge building. 
Manually crafting collocation translations can be 
time-consuming and cannot ensure high quality in 
a consistent way. Our work will certainly improve 
the quality and efficiency of collocation translation 
acquisition.  
5.3 Discussion 
Although our approach achieves promising 
results, it still has some limitations to be remedied 
in future work. 
(1) Translation dictionary extension 
Due to the limited coverage of the dictionary, a 
correct translation may not be stored in the 
dictionary. This naturally limits the coverage of 
triple translations. Some research has been done to 
expand translation dictionary using a non-parallel 
corpus (Rapp, 1999; Keohn and Knight, 2002). It 
can be used to improve our work. 
(2) Noise filtering of parsers 
Since we use parsers to generate dependency 
triple databases, this inevitably introduces some 
parsing mistakes. From our triple translation test 
data, we can see that 7.85% (157/2000) types of 
triples are error triples. These errors will certainly 
influence the translation probability estimation in 
the training process. We need to find an effective 
way to filter out mistakes and perform necessary 
automatic correction. 
(3) Non-compositional collocation translation. 
Our model is based on the dependency 
correspondence assumption, which assumes that a 
triple?s translation is also a triple. But there are still 
some collocations that can?t be translated word by 
word. For example, the Chinese triple (??, VO, 
??) usually be translated into ?be effective?; the 
English triple (take, VO, place) usually be 
translated into ????. The two words in triple 
cannot be translated separately. Our current model 
cannot deal with this kind of non-compositional 
collocation translation. Melamed (1997) and Lin 
(1999) have done some research on non-
compositional phrases discovery. We will consider 
taking their work as a complement to our model. 
6 Conclusion and future work  
This paper proposes a novel method to train a 
triple translation model and extract collocation 
translations from two independent monolingual 
corpora. Evaluation results show that it 
outperforms the existing monolingual corpus based 
methods in triple translation, mainly due to the 
employment of EM algorithm in cross language 
translation probability estimation. By making use 
of the acquired triple translation model in two 
directions, promising results are achieved in 
collocation translation extraction.  
Our work also demonstrates the possibility of 
making full use of monolingual resources, such as 
corpora and parsers for bilingual tasks. This can 
help overcome the bottleneck of the lack of a 
large-scale bilingual corpus. This approach is also 
applicable to comparable corpora, which are also 
easier to access than bilingual corpora. 
In future work, we are interested in extending 
our method to solving the problem of non-
compositional collocation translation. We are also 
interested in incorporating our triple translation 
model for sentence level translation. 
7 Acknowledgements 
The authors would like to thank John Chen, 
Jianfeng Gao and Yunbo Cao for their valuable 
suggestions and comments on a preliminary draft 
of this paper. 
References  
Morton Benson. 1990. Collocations and general-
purpose dictionaries. International Journal of 
Lexicography. 3(1):23?35 
Yunbo Cao, Hang Li. 2002. Base noun phrase 
translation using Web data and the EM algorithm. 
The 19th International Conference on 
Computational Linguistics. pp.127-133 
Kenneth W. Church and Patrick Hanks. 1990. 
Word association norms, mutural information, 
and lexicography. Computational Linguistics, 
16(1):22-29 
Ido Dagan  and Alon Itai. 1994. Word sense 
disambiguation using a second language 
monolingual corpus. Computational Linguistics, 
20(4):563-596 
Ted Dunning. 1993. Accurate methods for the 
statistics of surprise and coincidence. 
Computational  Linguistics. 19(1):61-74 
Hiroshi Echizen-ya, Kenji Araki, Yoshi Momouchi, 
Koji Tochinai. 2003. Effectiveness of automatic 
extraction of bilingual collocations using 
recursive chain-link-type learning. The 9th 
Machine Translation Summit. pp.102-109 
Pascale Fung, and Yee Lo Yuen. 1998. An IR 
approach for translating new words from 
nonparallel, comparable Texts. The 36th  annual 
conference of the Association for Computational 
Linguistics. pp. 414-420 
Jianfeng Gao, Jianyun Nie, Hongzhao He, Weijun 
Chen, Ming Zhou. 2002. Resolving query 
translation ambiguity using a decaying co-
occurrence model and syntactic dependence 
relations. The 25th Annual International ACM 
SIGIR Conference on Research and 
Development in Information Retrieval. pp.183 - 
190  
G. Heidorn. 2000. Intelligent writing assistant. In 
R. Dale, H. Moisl, and H. Somers, editors, A 
Handbook of Natural Language Processing: 
Techniques and Applications for the Processing 
of Language as Text. Marcel Dekker. 
Philipp Koehn and Kevin Knight. 2000. Estimating 
word translation probabilities from unrelated 
monolingual corpora using the EM algorithm. 
National Conference on Artificial Intelligence.  
pp.711-715 
Philipp Koehn and Kevin Knight. 2002. Learning a 
translation lexicon from monolingual corpora. 
Unsupervised Lexical Acquisition: Workshop of 
the ACL Special Interest Group on the Lexicon. 
pp. 9-16 
Julian Kupiec. 1993. An algorithm for finding 
noun phrase correspondences in bilingual 
corpora. The 31st Annual Meeting of the 
Association for Computational Linguistics, pp. 
23-30 
Cong Li, Hang Li. 2002. Word translation 
disambiguation using bilingual bootstrapping. 
The 40th annual conference of the Association 
for Computational Linguistics. pp: 343-351 
Dekang Lin. 1998. Extracting collocation from 
Text corpora. First Workshop on Computational 
Terminology. pp. 57-63 
Dekang Lin 1999. Automatic identification of non-
compositional phrases. The 37th Annual Meeting 
of the Association for Computational Linguistics. 
pp.317--324 
Ilya Dan Melamed. 1997. Automatic discovery of 
non-compositional compounds in parallel data. 
The 2nd Conference on Empirical Methods in 
Natural Language Processing. pp. 97~108 
Brown P.F., Pietra, S.A.D., Pietra, V. J. D., and 
Mercer R. L. 1993. The mathematics of machine 
translation: parameter estimation. Computational 
Linguistics, 19(2):263-313 
Reinhard Rapp. 1999. Automatic identification of 
word translations from unrelated English and 
German corpora. The 37th annual conference of 
the Association for Computational Linguistics. 
pp. 519-526 
Violeta Seretan, Luka Nerima, Eric Wehrli. 2003. 
Extraction of Multi-Word collocations using 
syntactic bigram composition. International 
Conference on Recent Advances in NLP.  pp. 
424-431 
Frank Smadja. 1993. Retrieving collocations from 
text: Xtract. Computational Linguistics, 
19(1):143-177 
Frank Smadja, Kathleen R. Mckeown, Vasileios 
Hatzivassiloglou. 1996. Translation collocations 
for bilingual lexicons: a statistical approach. 
Computational Linguistics, 22:1-38 
Aristomenis Thanopoulos, Nikos Fakotakis, 
George Kokkinakis. 2002. Comparative 
evaluation of collocation extraction metrics. The 
3rd International Conference on Language 
Resource and Evaluation. pp.620-625 
Hua Wu, Ming Zhou. 2003. Synonymous 
collocation extraction using translation 
Information. The 41th annual conference of the 
Association for Computational Linguistics. pp. 
120-127 
Kaoru Yamamoto, Yuji Matsumoto. 2000. 
Acquisition of phrase-level bilingual 
correspondence using dependency structure. The 
18th International Conference on Computational 
Linguistics. pp. 933-939 
Ming Zhou, Ding Yuan and Changning Huang. 
2001. Improving translation selection with a new 
translation model trained by independent 
monolingual corpora. Computaional Linguistics 
& Chinese Language Processing. 6(1): 1-26 
 
 
A New Approach for English-Chinese Named Entity Alignment 
Donghui Feng? 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way, Suite 1001 
Marina Del Rey, CA, U.S.A, 90292 
donghui@isi.edu  
Yajuan Lv?                             Ming Zhou? 
?Microsoft Research Asia 
5F Sigma Center, No.49 Zhichun Road, Haidian 
Beijing, China, 100080 
{t-yjlv, mingzhou}@microsoft.com 
 
Abstract? 
Traditional word alignment approaches cannot 
come up with satisfactory results for Named 
Entities. In this paper, we propose a novel 
approach using a maximum entropy model for 
named entity alignment. To ease the training 
of the maximum entropy model, bootstrapping 
is used to help supervised learning. Unlike 
previous work reported in the literature, our 
work conducts bilingual Named Entity 
alignment without word segmentation for 
Chinese and its performance is much better 
than that with word segmentation. When 
compared with IBM and HMM alignment 
models, experimental results show that our 
approach outperforms IBM Model 4 and 
HMM significantly. 
1 Introduction 
This paper addresses the Named Entity (NE) 
alignment of a bilingual corpus, which means 
building an alignment between each source NE and 
its translation NE in the target language. Research 
has shown that Named Entities (NE) carry 
essential information in human language (Hobbs et 
al., 1996). Aligning bilingual Named Entities is an 
effective way to extract an NE translation list and 
translation templates. For example, in the 
following sentence pair, aligning the NEs, [Zhi 
Chun road] and [???] can produce a translation 
template correctly. 
? Can I get to [LN Zhi Chun road] by eight 
o?clock? 
? ????? [LN ???]?? 
In addition, NE alignment can be very useful for 
Statistical Machine Translation (SMT) and Cross-
Language Information Retrieval (CLIR). 
A Named Entity alignment, however, is not easy 
to obtain. It requires both Named Entity 
Recognition (NER) and alignment be handled 
correctly. NEs may not be well recognized, or only 
                                                     
? The work was done while the first author was 
visiting Microsoft Research Asia. 
parts of them may be recognized during NER. 
When aligning bilingual NEs in different 
languages, we need to handle many-to-many 
alignments. And the inconsistency of NE 
translation and NER in different languages is also a 
big problem. Specifically, in Chinese NE 
processing, since Chinese is not a tokenized 
language, previous work (Huang et al, 2003) 
normally conducts word segmentation and 
identifies Named Entities in turn. This involves 
several problems for Chinese NEs, such as word 
segmentation error, the identification of Chinese 
NE boundaries, and the mis-tagging of Chinese 
NEs. For example, ?????? in Chinese is really 
one unit and should not be segmented as [ON ??
?]/? . The errors from word segmentation and 
NER will propagate into NE alignment. 
In this paper, we propose a novel approach using 
a maximum entropy model to carry out English-
Chinese Named Entity1 alignment. NEs in English 
are first recognized by NER tools. We then 
investigate NE translation features to identify NEs 
in Chinese and determine the most probable 
alignment. To ease the training of the maximum 
entropy model, bootstrapping is used to help 
supervised learning. 
On the other hand, to avoid error propagations 
from word segmentation and NER, we directly 
extract Chinese NEs and make the alignment from 
plain text without word segmentation. It is unlike 
previous work reported in the literature. Although 
this makes the task more difficult, it greatly 
reduces the chance of errors introduced by 
previous steps and therefore produces much better 
performance on our task. 
To justify our approach, we adopt traditional 
alignment approaches, in particular IBM Model 4 
(Brown et al, 1993) and HMM (Vogel et al, 
1996), to carry out NE alignment as our baseline 
systems. Experimental results show that in this task 
our approach outperforms IBM Model 4 and HMM 
significantly. Furthermore, the performance 
                                                     
1 We only discuss NEs of three categories: Person 
Name (PN), Location Name (LN), and Organization 
Name (ON). 
without word segmentation is much better than that 
with word segmentation. 
The rest of this paper is organized as follows: In 
section 2, we discuss related work on NE 
alignment. Section 3 gives the overall framework 
of NE alignment with our maximum entropy 
model. Feature functions and bootstrapping 
procedures are also explained in this section. We 
show experimental results and compare them with 
baseline systems in Section 4. Section 5 concludes 
the paper and discusses ongoing future work. 
2 Related Work 
Translation knowledge can be acquired via word 
and phrase alignment. So far a lot of research has 
been conducted in the field of machine translation 
and knowledge acquisition, including both 
statistical approaches (Cherry and Lin, 2003; 
Probst and Brown, 2002; Wang et al, 2002; Och 
and Ney, 2000; Melamed, 2000; Vogel et al, 1996) 
and symbolic approaches (Huang and Choi, 2000; 
Ker and Chang, 1997). 
However, these approaches do not work well on 
the task of NE alignment. Traditional approaches 
following IBM Models (Brown et al, 1993) are not 
able to produce satisfactory results due to their 
inherent inability to handle many-to-many 
alignments. They only carry out the alignment 
between words and do not consider the case of 
complex phrases like some multi-word NEs. On 
the other hand, IBM Models allow at most one 
word in the source language to correspond to a 
word in the target language (Koehn et al, 2003; 
Marcu, 2001). Therefore they can not handle 
many-to-many word alignments within NEs well. 
Another well-known word alignment approach, 
HMM (Vogel et al, 1996), makes the alignment 
probabilities depend on the alignment position of 
the previous word. It does not explicitly consider 
many-to-many alignment either. 
Huang et al (2003) proposed to extract Named 
Entity translingual equivalences based on the 
minimization of a linearly combined multi-feature 
cost. But they require Named Entity Recognition 
on both the source side and the target side. 
Moore?s (2003) approach is based on a sequence of 
cost models. However, this approach greatly relies 
on linguistic information, such as a string repeated 
on both sides, and clues from capital letters that are 
not suitable for language pairs not belonging to the 
same family. Also, there are already complete 
lexical compounds identified on the target side, 
which represent a big part of the final results. 
During the alignment, Moore does not hypothesize 
that translations of phrases would require splitting 
predetermined lexical compounds on the target set. 
These methods are not suitable for our task, 
since we only have NEs identified on the source 
side, and there is no extra knowledge from the 
target side. Considering the inherent characteristics 
of NE translation, we can find several features that 
can help NE alignment; therefore, we use a 
maximum entropy model to integrate these features 
and carry out NE alignment. 
3 NE Alignment with a Maximum Entropy 
Model  
Without relying on syntactic knowledge from 
either the English side or the Chinese side, we find 
there are several valuable features that can be used 
for Named Entity alignment. Considering the 
advantages of the maximum entropy model 
(Berger et al, 1996) to integrate different kinds of 
features, we use this framework to handle our 
problem. 
Suppose the source English NE 
,ene },...,{ 21 ne eeene = consists of n English 
words and the candidate Chinese NE 
,cne },...,{ 21 mc cccne = is composed of m 
Chinese characters.  Suppose also that we have M 
feature functions .,...,1),,( Mmneneh ecm = For 
each feature function, we have a model parameter 
.,...,1, Mmm =? The alignment probability can 
be defined as follows (Och and Ney, 2002): 
? ?
?
=
=
=
=
'
1
]),(exp[
]),(exp[
)|()|(
1
'
1
c
M
ne
M
m
ecmm
M
m
ecmm
ecec
neneh
neneh
nenepneneP
?
?
?
(3.1) 
The decision rule to choose the most probable 
aligned target NE of the English NE is (Och and 
Ney, 2002): 
{ }
??
?
??
?
=
=
?
=
M
m
ecmm
ne
ec
ne
c
neneh
nenePen
c
c
1
),(maxarg
)|(maxarg?
?
   (3.2) 
In our approach, considering the characteristics 
of NE translation, we adopt 4 features: translation 
score, transliteration score, the source NE and 
target NE?s co-occurrence score, and distortion 
score for distinguishing identical NEs in the same 
sentence. Next, we discuss these four features in 
detail. 
3.1 Feature Functions 
3.1.1 Translation Score 
It is important to consider the translation 
probability between words in English NE and 
characters in Chinese NE. When processing 
Chinese sentence without segmentation, word here 
refers to single Chinese character. 
The translation score here is used to represent 
how close an NE pair is based on translation 
probabilities. Supposing the source English NE 
ene consists of n English words, 
}...,{ 21 ne eeene = and the candidate Chinese NE 
cne is composed of m Chinese 
characters, }...,{ 21 mc cccne = , we can get the 
translation score of these two bilingual NEs based 
on the translation probability between ei and cj: 
??
= =
=
m
j
n
i
ijce ecpneneS
1 1
)|(),(      (3.3) 
Given a parallel corpus aligned at the sentence 
level, we can achieve the translation probability 
between each English word and each Chinese 
character )|( ij ecp via word alignments with IBM 
Model 1 (Brown et al, 1993). Without word 
segmentation, we have to calculate every possible 
candidate to determine the most probable 
alignment, which will make the search space very 
large. Therefore, we conduct pruning upon the 
whole search space. If there is a score jump 
between two adjacent characters, the candidate will 
be discarded. The scores between the candidate 
Chinese NEs and the source English NE are 
calculated via this formula as the value of this 
feature. 
3.1.2 Transliteration Score 
Although in theory, translation scores can build 
up relations within correct NE alignments, in 
practice this is not always the case, due to the 
characteristics of the corpus. This is more obvious 
when we have sparse data. For example, most of 
the person names in Named Entities are sparsely 
distributed in the corpus and not repeated regularly. 
Besides that, some English NEs are translated via 
transliteration (Lee and Chang, 2003; Al-Onaizan 
and Knight, 2002; Knight and Graehl, 1997) 
instead of semantic translation. Therefore, it is 
fairly important to make transliteration models. 
Given an English Named Entity e, 
}...,{ 21 neeee = , the procedure of transliterating e 
into a Chinese Named Entity c, }...,{ 21 mcccc = , 
can be described with Formula (3.4) (For 
simplicity of denotation, we here use e and c to 
represent English NE and Chinese NE instead of 
ene and cne ). 
)|(maxarg ecPc
c
=
)        (3.4) 
According to Bayes? Rule, it can be transformed 
to: 
)|(*)(maxarg cePcPc
c
=
)    (3.5) 
Since there are more than 6k common-used 
Chinese characters, we need a very large training 
corpus to build the mapping directly between 
English words and Chinese characters. We adopt a 
romanization system, Chinese PinYin, to ease the 
transformation. Each Chinese character 
corresponds to a Chinese PinYin string. And the 
probability from a Chinese character to PinYin 
string is 1)|( ?crP , except for polyphonous 
characters. Thus we have: 
)|(*)|(*)(maxarg rePcrPcPc
c
=
)   (3.6) 
Our problem is: Given both English NE and 
candidate Chinese NEs, finding the most probable 
alignment, instead of finding the most probable 
Chinese translation of the English NE. Therefore 
unlike previous work (Lee and Chang, 2003; 
Huang et al, 2003) in English-Chinese 
transliteration models, we transform each 
candidate Chinese NE to Chinese PinYin strings 
and directly train a PinYin-based language model 
with a separate English-Chinese name list 
consisting of 1258 name pairs to decode the most 
probable PinYin string from English NE. 
To find the most probable PinYin string from 
English NE, we rewrite Formula (3.5) as the 
following: 
)|(*)(maxarg rePrPr
r
=
)      (3.7) 
where r represents the romanization (PinYin 
string), }...,{ 21 mrrrr = . For each of the factor, we 
have 
)|()|(
1
?
=
=
m
i
ii rePreP      (3.8) 
)|()|()()( 1
3
2121 ?
=
??= i
m
i
ii rrrPrrPrPrP   (3.9) 
where ie  is an English syllable and ir  is a 
Chinese PinYin substring. 
For example, we have English NE ?Richard? and 
its candidate Chinese NE ?????. Since both the 
channel model and language model are PinYin 
based, the result of Viterbi decoding is from ?Ri 
char d? to ?Li Cha De?. We transform ????? to 
the PinYin string ?Li Cha De?. Then we compare 
the similarity based on the PinYin string instead of 
with Chinese characters directly. This is because 
when transliterating English NEs into Chinese, it is 
very flexible to choose which character to simulate 
the pronunciation, but the PinYin string is 
relatively fixed. 
For every English word, there exist several ways 
to partition it into syllables, so here we adopt a 
dynamic programming algorithm to decode the 
English word into a Chinese PinYin sequence. 
Based on the transliteration string of the English 
NE and the PinYin string of the original candidate 
Chinese NE, we can calculate their similarity with 
the XDice coefficient (Brew and McKelvie, 1996). 
This is a variant of Dice coefficient which allows 
?extended bigrams?. An extended bigram (xbig) is 
formed by deleting the middle letter from any 
three-letter substring of the word in addition to the 
original bigrams. 
Suppose the transliteration string of the English 
NE and the PinYin string of the candidate Chinese 
NE are tle  and pyc , respectively. The XDice 
coefficient is calculated via the following formula: 
)()(
)()(2
),(
pytl
pytl
pytl
cxbigsexbigs
cxbigsexbigs
ceXDice
+
?
=
I   (3.10) 
Another point to note is that foreign person 
names and Chinese person names have different 
translation strategies. The transliteration 
framework above is only applied on foreign names. 
For Chinese person name translation, the surface 
English strings are exactly Chinese person names? 
PinYin strings. To deal with the two situations, let 
sure  denote the surface English string, the final 
transliteration score is defined by taking the 
maximum value of the two XDice coefficients: 
)),(),,(max(
),(
surpytlpy ecXDiceecXDice
ecTl =
  (3.11) 
This formula does not differentiate foreign 
person names and Chinese person names, and 
foreign person names? transliteration strings or 
Chinese person names? PinYin strings can be 
handled appropriately. Besides this, since the 
English string and the PinYin string share the same 
character set, our approach can also work as an 
alternative if the transliteration decoding fails. 
For example, for the English name ?Cuba?, the 
alignment to a Chinese NE should be ????. If 
the transliteration decoding fails, its PinYin string, 
?Guba?, still has a very strong relation with the 
surface string ?Cuba? via the XDice coefficient. 
This can make the system more powerful. 
3.1.3 Co-occurrence Score 
Another approach is to find the co-occurrences 
of source and target NEs in the whole corpus. If 
both NEs co-occur very often, there exists a big 
chance that they align to each other. The 
knowledge acquired from the whole corpus is an 
extra and valuable feature for NE alignment. We 
calculate the co-occurrence score of the source 
English NE and the candidate Chinese NE with the 
following formula: 
?= )(*,
),(
)|(
e
ec
ecco necount
nenecount
neneP     (3.12) 
where ),( ec nenecount  is the number of times 
cne  and ene  appear together and )(*, enecount  
is the number of times that ene  appears. This 
probability is a good indication for determining 
bilingual NE alignment. 
3.1.4 Distortion Score 
When translating NEs across languages, we 
notice that the difference of their positions is also a 
good indication for determining their relation, and 
this is a must when there are identical candidates in 
the target language. The bigger the difference is, 
the less probable they can be translations of each 
other. Therefore, we define the distortion score 
between the source English NE and the candidate 
Chinese NE as another feature. 
Suppose the index of the start position of the 
English NE is i, and the length of the English 
sentence is m. We then have the relative position of 
the source English NE
m
ipose = , and the 
candidate Chinese NE?s relative 
position ,cpos 1,0 ?? ce pospos . The distortion 
score is defined with the following formula: 
)(1),( ceec posposABSneneDist ??= (3.13) 
where ABS means the absolute value. If there 
are multiple identical candidate Chinese NEs at 
different positions in the target language, the one 
with the largest distortion score will win. 
3.2 Bootstrapping with the MaxEnt Model 
To apply the maximum entropy model for NE 
alignment, we process in two steps: selecting the 
NE candidates and training the maximum entropy 
model parameters. 
3.2.1 NE Candidate Selection 
To get an NE alignment with our maximum 
entropy model, we first use NLPWIN (Heidorn, 
2000) to identify Named Entities in English. For 
each word in the recognized NE, we find all the 
possible translation characters in Chinese through 
the translation table acquired from IBM Model 1. 
Finally, we have all the selected characters as the 
?seed? data. With an open-ended window for each 
seed, all the possible sequences located within the 
window are considered as possible candidates for 
NE alignment. Their lengths range from 1 to the 
empirically determined length of the window. 
During the candidate selection, the pruning 
strategy discussed above is applied to reduce the 
search space. 
For example, in Figure 1, if ?China? only has a 
translation probability over the threshold value 
with ???, the two seed data are located with the 
index of 0 and 4. Supposing the length of the 
window to be 3, all the candidates around the seed 
data including ????, with the length ranging 
from 1 to 3, are selected. 
 
 
 
 
 
 
Figure 1. Example of Seed Data 
3.2.2 MaxEnt Parameter Training 
With the four feature functions defined in 
Section 3.1, for each identified NE in English, we 
calculate the feature scores of all the selected 
Chinese NE candidates. 
To achieve the most probable aligned Chinese 
NE, we use the published package YASMET2 to 
conduct parameter training and re-ranking of all 
the NE candidates. YASMET requires supervised 
learning for the training of the maximum entropy 
model. However, it is not easy to acquire a large 
annotated training set. Here bootstrapping is used 
to help the process. Figure 2 gives the whole 
procedure for parameter training. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Parameter Training 
4 Experimental Results 
4.1 Experimental Setup 
We perform experiments to investigate the 
performance of the above framework. We take the 
LDC Xinhua News with aligned English-Chinese 
sentence pairs as our corpus. 
The incremental testing strategy is to investigate 
the system?s performance as more and more data 
are added into the data set. Initially, we take 300 
                                                     
2 http://www.isi.edu/~och/YASMET.html 
sentences as the standard testing set, and we 
repeatedly add 5k more sentences into the data set 
and process the new data. After iterative re-ranking, 
the performance of alignment models over the 300 
sentence pairs is calculated. The learning curves 
are drawn from 5k through 30k sentences with the 
step as 5k every time. 
4.2 Baseline System 
A translated Chinese NE may appear at a 
different position from the corresponding English 
NE in the sentence. IBM Model 4 (Brown et al, 
1993) integrates a distortion probability, which is 
complete enough to account for this tendency. The 
HMM model (Vogel et al, 1996) conducts word 
alignment with a strong tendency to preserve 
localization from one language to another. 
Therefore we extract NE alignments based on the 
results of these two models as our baseline systems. 
For the alignments of IBM Model 4 and HMM, we 
use the published software package, GIZA++ 3 
(Och and Ney, 2003) for processing. 
Some recent research has proposed to extract 
phrase translations based on the results from IBM 
Model (Koehn et al, 2003). We extract English-
Chinese NE alignments based on the results from 
IBM Model 4 and HMM. The extraction strategy 
takes each of the continuous aligned segments as 
one possible candidate, and finally the one with the 
highest frequency in the whole corpus wins. 
 
 
 
 
 
 
 
 
Figure 3. Example of Extraction Strategy 
Figure 3 gives an example of the extraction 
strategy. ?China? here is aligned to either ???? 
or ???. Finally the one with a higher frequency in 
the whole corpus, say, ????, will be viewed as 
the final alignment for ?China?. 
4.3 Results Analysis 
Our approach first uses NLPWIN to conduct 
NER. Suppose S? is the set of identified NE with 
NLPWIN. S is the alignment set we compute with 
our models based on S?, and T is the set consisting 
of all the true alignments based on S?. We define 
the evaluation metrics of precision, recall, and F-
score as follows: 
                                                     
3 http://www.isi.edu/~och/GIZA++.html 
[China] hopes to further economic ? [EU]. 
 
 
? ? ? ? ? ? ? ? ? ?? 
1. Set the coefficients i? as uniform 
distribution; 
2. Calculate all the feature scores to get the 
N-best list of the Chinese NE candidates; 
3. Candidates with their values over a given 
threshold are considered to be correct and 
put into the re-ranking training set; 
4. Retrain the parameters i?  with YASMET;
5. Repeat from Step 2 until i?  converge, and 
take the current ranking as the final result. 
[China] hopes to further economic ? [EU]. 
 
 
? ? ? ? ? ? ? ? ? ?? 
Aligned Candidates:  China ??? 
                               China ?? 
S
TS
precision
I
=           (4.1) 
T
TS
recall
I
=           (4.2) 
recallprecision
recallprecisionscoreF
+
??
=?
2  (4.3) 
4.3.1 Results without Word Segmentation 
Based on the testing strategies discussed in 
Section 4.1, we perform all the experiments on 
data without word segmentation and get the 
performance for NE alignment with IBM Model 4, 
the HMM model, and the maximum entropy model. 
Figure 4, 5, and 6 give the learning curves for 
precision, recall, and F-score, respectively, with 
these experiments. 
Precision Without Word Segmentation
0
0.2
0.4
0.6
0.8
1
5k 10k 15k 20k 25k 30k data size
pr
ec
is
io
n IBM Model
HMM
MaxEnt
Upper Bound
 
Figure 4. Learning Curve with Precision 
Recall Without Word Segmentation
0
0.2
0.4
0.6
0.8
1
5k 10k 15k 20k 25k 30k data size
re
ca
ll IBM Model
HMM
MaxEnt
 
Figure 5. Learning Curve with Recall 
F-score Without Word Segmentation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
5k 10k 15k 20k 25k 30k data size
F-
sc
or
e IBM Model
HMM
MaxEnt
 
Figure 6. Learning Curve with F-score 
From these curves, we see that HMM generally 
works a little better than IBM Model 4, both for 
precision and for recall. NE alignment with the 
maximum entropy model greatly outperforms IBM 
Model 4 and HMM in precision, recall, and F-
Score. Since with this framework, we first use 
NLPWIN to recognize NEs in English, we have 
NE identification error. The precision of NLPWIN 
on our task is about 77%. Taking this into account, 
we know our precision score has actually been 
reduced by this rate. In Figure 4, this causes the 
upper bound of precision to be 77%. 
4.3.2 Comparison with Results with Word 
Segmentation 
To justify that our approach of NE alignment 
without word segmentation really reduces the error 
propagations from word segmentation and 
thereafter NER, we also perform all the 
experiments upon the data set with word 
segmentation. The segmented data is directly taken 
from published LDC Xinhua News corpus. 
 
 precision recall F-score 
MaxEnt 
(Seg) 
0.56705 0.734491 0.64 
MaxEnt 
(Unseg) 
0.636015 0.823821 0.717838 
HMM 
(Seg) 
0.281955 0.372208 0.320856 
HMM 
(Unseg) 
0.291859 0.471464 0.360531 
IBM 4 
(Seg) 
0.223062 0.292804 0.253219 
IBM 4 
(Unseg) 
0.251185 0.394541 0.30695 
Table 1. Results Comparison 
Table 1 gives the comparison of precision, recall, 
and F-score for the experiments with word 
segmentation and without word segmentation 
when the size of the data set is 30k sentences. 
For HMM and IBM Model 4, performance 
without word segmentation is always better than 
with word segmentation. For maximum entropy 
model, the scores without word segmentation are 
always 6 to 9 percent better than those with word 
segmentation. This owes to the reduction of error 
propagation from word segmentation and NER. 
For example, in the following sentence pair with 
word segmentation, the English NE ?United 
States? can no longer be correctly aligned to ??
??. Since in the Chinese sentence, the incorrect 
segmentation takes ?????? as one unit. But if 
we conduct alignment without word segmentation, 
???? can be correctly aligned. 
? Greek Prime Minister Costas Simitis visits 
[United States] .  
? ?? ?? ?? ? ? ???? . 
Similar situations exist when HMM and IBM 
Model 4 are used for NE alignment. When 
compared with IBM Model 4 and HMM with word 
segmentation, our approach with word 
segmentation also has a much better performance 
than them. This demonstrates that in any case our 
approach outperforms IBM Model 4 and HMM 
significantly. 
4.3.3 Discussion 
Huang et al?s (2003) approach investigated 
transliteration cost and translation cost, based on 
IBM Model 1, and NE tagging cost by an NE 
identifier. In our approach, we do not have an NE 
tagging cost. We use a different type of translation 
and transliteration score, and add a distortion score 
that is important to distinguish identical NEs in the 
same sentence. 
Experimental results prove that in our approach 
the selected features that characterize NE 
translations from English to Chinese help much for 
NE alignment. The co-occurrence score uses the 
knowledge from the whole corpus to help NE 
alignment. And the transliteration score addresses 
the problem of data sparseness. For example, 
English person name ?Mostafizur Rahman? only 
appears once in the data set. But with the 
transliteration score, we get it aligned to the 
Chinese NE ?????????? correctly. 
Since in ME training we use iterative 
bootstrapping to help supervised learning, the 
training data is not completely clean and brings 
some errors into the final results. But it avoids the 
acquisition of large annotated training set and the 
performance is still much better than traditional 
alignment models. The performance is also 
impaired by the English NER tool. Another 
possible reason for alignment errors is the 
inconsistency of NE translation in English and 
Chinese. For example, usually only the last name 
of foreigners is translated into Chinese and the first 
name is ignored. This brings some trouble for the 
alignment of person names. 
5 Conclusions 
Traditional word alignment approaches cannot 
come up with satisfactory results for Named Entity 
alignment. In this paper, we propose a novel 
approach using a maximum entropy model for NE 
alignment. To ease the training of the MaxEnt 
model, bootstrapping is used to help supervised 
learning. Unlike previous work reported in the 
literature, our work conducts bilingual Named 
Entity alignment without word segmentation for 
Chinese, and its performance is much better than 
with word segmentation. When compared with 
IBM and HMM alignment models, experimental 
results show that our approach outperforms IBM 
Model 4 and HMM significantly. 
Due to the inconsistency of NE translation, some 
NE pairs can not be aligned correctly. We may 
need some manually-generated rules to fix this. We 
also notice that NER performance over the source 
language can be improved using bilingual 
knowledge. These problems will be investigated in 
the future. 
6 Acknowledgements 
Thanks to Hang Li, Changning Huang, Yunbo 
Cao, and John Chen for their valuable comments 
on this work. Also thank Kevin Knight for his 
checking of the English of this paper. Special 
thanks go to Eduard Hovy for his continuous 
support and encouragement while the first author 
was visiting MSRA. 
References  
Al-Onaizan, Y. and Knight, K. 2002. Translating 
Named Entities Using Monolingual and 
Bilingual Resources. ACL 2002, pp. 400-408. 
Philadelphia. 
Berger, A. L.; Della Pietra, S. A.; and Della Pietra, 
V. J. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational 
Linguistics, vol. 22, no. 1, pp. 39-68. 
Brew, C. and McKelvie, D. 1996. Word-pair 
extraction for lexicography. The 2nd 
International Conference on New Methods in 
Language Processing, pp. 45?55. Ankara. 
Brown, P. F.; Della Pietra, S. A.; Della Pietra, V. J. 
;and Mercer, R. L. 1993. The Mathematics of 
Statistical Machine Translation: Parameter 
Estimation. Computational Linguistics, 
19(2):263-311. 
Cherry, C. and Lin, D. 2003. A Probability Model 
to Improve Word Alignment. ACL 2003. 
Sapporo, Japan. 
Darroch, J. N. and Ratcliff, D. 1972. Generalized 
Iterative Scaling for Log-linear Models. Annals 
of Mathematical Statistics, 43:1470-1480. 
Heidorn, G. 2000. Intelligent Writing Assistant. A 
Handbook of Natural Language Processing: 
Techniques and Applications for the Processing 
of Language as Text. Marcel Dekker. 
Hobbs, J. et al 1996. FASTUS: A Cascaded Finite-
State Transducer for Extracting Information 
from Natural Language Text, MIT Press. 
Cambridge, MA. 
Huang, F.; Vogel, S. and Waibel, A. 2003. 
Automatic Extraction of Named Entity 
Translingual Equivalence Based on Multi-
Feature Cost Minimization. ACL 2003 Workshop 
on Multilingual and Mixed-language NER. 
Sapporo, Japan. 
Huang, J. and Choi, K. 2000. Chinese-Korean 
Word Alignment Based on Linguistic 
Comparison. ACL-2000. Hongkong. 
Ker, S. J. and Chang, J. S. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, 23(2):313-343. 
Knight, K. and Graehl, J. 1997. Machine 
Transliteration. ACL 1997, pp. 128-135. 
Koehn, P.; Och, F. J. and Marcu, D. 2003. 
Statistical Phrase-Based Translation. 
HLT/NAACL 2003. Edmonton, Canada. 
Lee, C. and Chang, J. S. 2003. Acquisition of 
English-Chinese Transliterated Word Pairs from 
Parallel-Aligned Texts, HLT-NAACL 2003 
Workshop on Data Driven MT, pp. 96-103. 
Marcu, D. 2001. Towards a Unified Approach to 
Memory- and Statistical-Based Machine 
Translation. ACL 2001, pp. 378-385. Toulouse, 
France. 
Melamed, I. D. 2000. Models of Translation 
Equivalence among Words. Computational 
Linguistics, 26(2): 221-249. 
Moore, R. C. 2003. Learning Translations of 
Named-Entity Phrases from Parallel Corpora. 
EACL-2003. Budapest, Hungary. 
Och, F. J. and Ney, H. 2003. A Systematic 
Comparison of Various Statistical Alignment 
Models, Computational Linguistics, volume 29, 
number 1, pp. 19-51. 
Och, F. J. and Ney, H. 2002. Discriminative 
Training and Maximum Entropy Models for 
Statistical Machine Translation. ACL 2002, pp. 
295-302. 
Och, F. J. and Ney, H. 2000. Improved Statistical 
Alignment Models. ACL 2000, pp: 440-447. 
Probst, K. and Brown, R. 2002. Using Similarity 
Scoring to Improve the Bilingual Dictionary for 
Word Alignment. ACL-2002, pp: 409-416. 
Vogel, S.; Ney, H. and Tillmann, C. 1996. HMM-
Based Word Alignment in Statistical Translation. 
COLING?96, pp. 836-841. 
Wang, W.; Zhou, M.; Huang, J. and Huang, C. 
2002. Structural Alignment using Bilingual 
Chunking. COLING-2002. 
Learning Chinese Bracketing Knowledge Based on  
a Bilingual Language Model 
Yajuan L?, Sheng Li, Tiejun Zhao, Muyun Yang  
School of Computer Science & Engineering, Harbin Institute of Technology 
Harbin, China, 150001 
Email: {lyj,lish,tjzhao,ymy}@mtlab.hit.edu.cn 
 
Abstract  
This paper proposes a new method for 
automatic acquisition of Chinese bracketing 
knowledge from English-Chinese sentence- 
aligned bilingual corpora. Bilingual sentence 
pairs are first aligned in syntactic structure by 
combining English parse trees with a 
statistical bilingual language model. Chinese 
bracketing knowledge is then extracted 
automatically. The preliminary experiments 
show automatically learned knowledge 
accords well with manually annotated 
brackets. The proposed method is 
particularly useful to acquire bracketing 
knowledge for a less studied language that 
lacks tools and resources found in a second 
language more studied. Although this paper 
discusses experiments with Chinese and 
English, the method is also applicable to 
other language pairs. 
Introduction 
The past few years have seen a great success in 
automatic acquisition of monolingual parsing 
knowledge and grammars. The availability of 
large tagged and syntactically bracketed corpora, 
such as Penn Tree bank, makes it possible to 
extract syntactic structure and grammar rules 
automatically (Marcus 1993). Substantial 
improvements have been made to parse western 
language such as English, and many powerful 
models have been proposed (Brill 1993, Collins 
1997). However, very limited progress has been 
achieved in Chinese. 
      Knowledge acquisition is a bottleneck for 
real appication of Chinese parsing. While some 
methods have been proposed to learn syntactic 
knowledge from annotated Chinese corpus, most 
of the methods depended on the annotated or 
partial annotated data(Zhou 1997, Streiter 2000). 
Due to the limited availbility of Chinese 
annotated corpus, tests of these methods are still 
small in scale. Although some institutions and 
universities currently are engaged in building 
Chinese tree bank, no large scale annotated 
corpus has been published until now because the 
complexity in Chinese syntatic sturcture and the 
difficulty in corpus annotation (Chen 1996).  
This paper proposes a novel method to 
facilitate the Chinese tree bank construction. 
Based on English-Chinese bilingual corpora and 
better English parsing, this method obtains 
Chinese bracketing information automatically via 
a bilingual model and word alignment results. 
The main idea of the method is that we may 
acquire knowledge for a language lacking a rich 
collection of resources and tools from a second 
language that is full of them.  
The rest of this paper is organized as 
follows : In the next section, a bilingual language 
model is introduced. Then, a bilingual parsing 
method supervised by English parsing is 
proposed in section 2. Based on the bilingual 
parsing, Chinese bracketing knowlege is 
extracted in section 3. The evaluation and 
discussion are given in section 4. We conclude 
with discussion of future work. 
1 A bilingual language model ? ITG 
Wu (1997) has proposed a bilingual language 
model called Inversion Transduction Grammar 
(ITG), which can be used to parse bilingual 
sentence pairs simultaneously. We will give a 
brief description here. For details please refer to 
(Wu 1995, Wu 1997).  
The Inversion Transduction Grammar is a 
bilingual context-free grammar that generates 
two matched output languages (referred to as L1 
and L2). It also differs from standard context-free 
grammars in that the ITG allows right-hand side 
production in two directions: straight or inverted. 
The following examples are two ITG 
productions: 
C -> [A B] 
C -> <A B> 
Each nonterminal symbol stands for a pair of 
matched strings. For example, the nonterminal A 
stands for the string-pair (A1, A2). A1 is a 
sub-string in L1, and A2 is A1?s corresponding 
translation in L2. Similarly, (B1, B2) denotes the 
string-pair generated by B. The operator [ ] 
performs the usual concatenation, so that C -> [A 
B] yields the string-pair (C1, C2), where C1=A1B1 
and C2=A2B2. On the other hand, the operator <> 
performs the straight concatenation for language 
1 but the reversing concatenation for language 2, 
so that C -> <A B> yields C1=A1B1, but C2=B2A2. 
The inverted concatenation operator permits the 
extra flexibility needed to accommodate many 
kinds of word-order variation between source 
and target languages (Wu 1995). 
There are also lexical productions of the 
following form in ITG: 
A -> x/y 
This means that a symbol x in language L1 is 
translated by the symbol y in language L2.  x or y 
may be a null symbol e, which means there may 
be no counterpart string on other side of the 
bitext.  
ITG based parsing matches constituents for 
an input sentence-pair. For example, Figure 1 
shows an ITG parsing tree for an 
English-Chinese sentence-pair. The inverted 
production is indicated by a horizontal line in the 
parsing tree. The English text is read in the usual 
depth-first left to right order, but for the Chinese 
text, a horizontal line means the right sub-tree is 
traversed before the left. The generated parsing 
results are: 
(1) [[[Mr. Wu]BNP [[plays basketball]VP [on 
Sunday ]PP ]VP ]S . ]S  
(2) [[[ ] [ [ 	]]] 
] 
We can also represent the common structure 
of the two sentences more clearly and compactly 
with the aid of <> notation: 
(3)  [[<Mr./ Wu/>BNP < [plays/ basketball/	]VP 
[on/e Sunday/]PP >VP ]S ./
]S 
where the horizontal line from Figure 1 
corresponds to the <> level of bracketing. 
. 
S 
BNP 
VP 
PP 
VP 
Mr./ Wu/ 
plays/ basketball/

 
on/e Sunday/	 
S 
./ 
Figure 1  Inversion transduction Grammar parsing
      Any ITG can be converted to a normal form, 
where all productions are either lexical 
productions or binary-fanout nonterminal 
productions(Wu 1997). If probability is 
associated with each production, the ITG is 
called the Stochastic Inversion Transduction 
Grammar (SITG). 
2 English parsing supervised bilingual 
bracketing 
Because of the difficulty in finding a suitable 
bilingual syntactic grammar for Chinese and 
English, a practical ITG is the generic Bracketing 
Inversion Transduction Grammar (BTG)(Wu 
1995). BTG is a simplified ITG that has only one 
nonterminal and does not use any syntactic 
grammar. A Statistical BTG (SBTG) grammar is 
as follows: 
j
b
i
b
ji
baa
veAeuA
vuAAAAAAA
ejie
ij
/    ;/ 
   ; /    ;    ];[ 
??????
???><??????
       SBTG employs only one nonterminal 
symbol A that can be used recursively. Here, ?a? 
denotes the probability of syntactic rules. 
However, since those constituent categories are 
not differentiated in BTG, it has no practical 
effect here and can be set to an arbitrary constant. 
The remaining productions are all lexical. bij is 
the translation probability that source word ui 
translates into target word vj. bij can be obtained 
using a statistical word-translation model 
(Melamed 2000) or word alignment(L? 2001a). 
The last two productions denote that the word in 
one language has no counterpart on other side of 
the bitext. A small constant can be chosen for the 
probabilities bie and bej.   
In BTG, no language specific syntactic 
grammar is used. The maximum-likelihood 
parser selects the parse tree that best satisfies the 
combined lexical translation preferences, as 
expressed by the bij probabilities. Because the 
expressiveness characteristics of ITG naturally 
constrain the space of possible matching in a 
highly appropriate fashion, BTG achieves 
encouraging results for bilingual bracketing 
using a word-translation lexicon alone (Wu 
1997). 
Since no syntactic knowledge is used in 
SBTG, output grammaticality can not be 
guaranteed. In particular, if the corresponding 
constituents appear in the same order in both 
languages, both straight and inverted, then lexical 
matching does not provide the discriminative 
leverage needed to identify the sub-constituent 
boundaries. For example, consider an 
English-Chinese sentence pair: 
(4) English: That old teacher is our adviser. 
Chinese: 	
 
Using SBTG, the bilingual bracketing result is : 
(5) [[[[[[The/ old/] teacher/] is/] our/	] 
adviser/
] ./] 
The result is not consistent with the 
expected syntactic structure. In this case, 
grammatical information about one or both of the 
languages can be very helpful. For example, if we 
know the English parsing result shown in (6), 
then the bilingual bracketing can be determined 
easily; the result should be (7).  
(6) [[That old teacher]BNP [is [our adviser]BNP ]VP .]S 
(7) [[That/ old/ teacher/] [is/ [our/	 
adviser/
] ] ./] 
From the example, we can see that if one 
language parser is available, the induced 
bilingual bracketing result would be more 
accurate. English parsing methods have been 
well studied and many powerful models have 
been proposed. It will be helpful to make use of 
English parsing results. In the following, we will 
propose a method of bilingual bracketing 
supervised by English parsing.  
Here, English parsing supervised BTG 
means using an English parser?s bracketing 
information as a boundary restriction in the BTG 
language model. But this does not necessitate 
parsing Chinese completely according to the 
same parsing boundary of English. If the English 
parsing structure is totally fixed, it is possible that 
the structure is not linguistically valid for 
Chinese under the formalism of Inversion 
Transduction Grammar. To illustrate this, see the 
example shown in Figure 2.  
If you want to lose weight, you had better eat less bread . 
    	
 
 

 
  
 
eat 
less bread 
VP 
BNP 

   

     

 
 (a) 
VP 
eat/ less/ 
bread/

 
X 
 (b) 
Figure 2  A example of mismatch subtree 
VP 
eat less bread 
 (c) 
 
 
 
        The sub-tree for blacked underlined part of 
English and corresponding Chinese are shown in 
Figure 2(a). We can see that the Chinese 
constituents do not match the English 
counterparts in the English structure. In this case, 
our solution is that: the whole English constituent 
of ?VP? is aligned with the whole Chinese 
correspondence; i.e., ?eat less bread? is matched 
with ?? shown in Figure 2(b). At the 
same time, we give the inner structure matching 
according to ITG regardless of the English 
parsing constraint. An ?X? tag is introduced to 
indicate that the sub-bilingual-parsing-tree is not 
consistent with the given English sub-tree. Our 
result can also be understood as a flattened 
bilingual parsing tree as shown in Figure 2(c). 
This means that when the bilingual constituents 
couldn?t match in the small syntactic structure, 
we will match them in a larger structure. 
        The main idea is that the given English 
parser is only used as a boundary constraint for 
bilingual parsing. When the constraint is 
incompatible with the bilingual model ITG, we 
use ITG as the default result. This process 
enables parsing to go on regardless of some 
failures in matching. 
We heuristically define a constraint function 
Fe(s, t) to denote the English boundary constraint, 
where s is the beginning position and t is the end. 
There are three cases of structure matching: 
violate match, exact match and inside match. 
Violate match means the bilingual parsing 
conflicts with the given English bracketing 
boundary. For example, given the following 
English bracketing result (8), (1,2), (1,3), (2,3), 
(2,4) etc. are Violate matches. We assign a 
minimum Fe(s, t) (0.0001 at present) to prevent 
the structure match from being chosen when an 
alternative match is available. Exact match 
means the match falls exactly on the English 
parsing boundary, and we assign a high Fe(s, t) 
value (10 at present) to emphasize it. (1,6), (2,5), 
(3,5) are examples. (3,4), (4,5) are examples of 
inside match, and the value 1 is assigned to these 
Fe(s, t) functions. 
(8) [She/1 [is/2 [a/3 lovely/4 girl/5] ] ./6]    
Let the input English and Chinese sentences 
be Tee ,...1  and Vcc ,...1 . As an abbreviation we 
write tse ...  for the sequence of words 
tss eee ..., ,21 ++ , and similarly write vuc ... . The local 
optimization function =),,,( vuts?  
]/[max
.... vuts ceP denotes the maximum probability 
of sub-parsing-tree of node q and that both the 
sub-string tse ...  and vuc ...  derive from node q. 
Thus, the best parser has the 
probability ),0,,0( VT? . ),,,( vuts? is calculated as 
the maximum probability combination of all 
possible sub-tree combinations(Wu 1995). To 
insert English parsing constraints in bilingual 
parsing, we integrate the constraint function Fe(s, 
t) into the local optimization function.   
Computation of the local optimization function is 
then modified as given below:  
.),,,(),,,(),(max),,,(
,),,,(),,,(),(max),,,(
,)],,,(),,,,(max[),,,(
0))(())((
0))(())((
[]
[]
UutSvUSstsFvuts
vUtSUuSstsFvuts
vutsvutsvuts
e
UvuUStsS
vUu
tSs
e
UvuUStsS
vUu
tSs
???
???
???
???+??
??
??
<>
???+??
??
??
<>
=
=
=
 
    Initialization is as follows : 
V1,1),/(
V1,1),/(
V1,1),/(
,1,,
,,,1
,1,,1
????=
????=
????=
?
?
??
vTtceb
vTteeb
vTtceb
vvvtt
tvvtt
vtvvtt
?
?
?
     
where, T ,V is the length of English and Chinese 
sentence respectively. )/( vt ceb is the probability 
of translating English word te  into Chinese word 
vc . A minimal probability can be assigned to 
empty word alignment b( eet / ) and b( vce / ). 
The optimal bilingual parsing tree for a 
given sentence-pair can be computed using  
dynamic programming (DP) algorithm(Wu 1997). 
Using the standard SBTG local optimization 
fuction, the obtained bilingual parsing result for 
the given sentence-pair(4) is shown as example 
(5); when using the above modified local 
optimization function, the parsing result is that 
shown as example (7). Comparing the two results, 
we can see that by intergrating English parsing 
constraints into BTG, the bilingual parsing 
becomes more grammatical. Our experiments 
showed that this English parsing supervised BTG 
would improve the accuracy of bilingual 
bracketing by nearly 20% (L? 2001b). 
The obtained bilingual parsing tree is in the 
normal form of ITG, that is each node in the tree 
is either a lexical node or a binary-fanout 
nonterminal node. We can combine the subtree to 
restore the fanout flexibility using the production 
characters [[AA]A]=[A[AA]]=[AAA] and 
<<AA>A>= <A<AA>>=<AAA>. The combining 
operation could not cross the given English 
parisng boundary.  
3 Chinese bracketing knowledge extraction 
Table 1 shows some bilingual bracketing 
examples obtained using the above method. To 
understand easily, we give the tree form of the 
first example in Figure 3(a). The leaf node is the 
aligned words of the two languages and their 
POS tag categories. These POS tags are 
generated from an English and a Chinese POS 
tagger respectively. The English POS tag and 
phrase tag set are the same as those of the Penn 
Tree Bank (Marcus 1993) and the Chinse POS 
tag set please refer to the web site: 
http://mtlab.hit.edu.cn. The nonterminal node are 
labeled using English sub-tree tags. 
Based on the bilingual parsing result, it is 
easy to extract the Chinese bracketing structure 
according to the Inversion Transduction 
Grammar. For the normal node, the Chinese text 
is traversed in depth-first left to right order, but 
for an inverted node (indicated by a horizontal 
line in the parsing tree or indicated by a <> 
notation in bracketing expression), the right 
sub-tree is traversed before the left. Thus, the 
Chinese parsing tree corresponding to Figure 3(a) 
is shown in Figure 3(b). The nonterminal labels 
are derived from the English sub-tree. The 
extracted Chinese bracketing results from Table1  
Table 1  Bilingual bracketing examples 
1. [<Mr.(NNP)/(nc) Chen(NNP)/(nx) >BNP [is (VBZ) /(vx) < [the(ART)/e representative(NN)/(ng)]BNP 
<of (IN) /(usde) [our (PRP$)/	(r) company(NN)/
(ng)]BNP >PP >NP ]VP .(.)/(wj) ]S 
2. [Spring(NN)/(t) [is(VBZ)/(vx) <[the(ART)/e first(JJ)/(m) e/(q) season(NN)/(ng) ]BNP <in(IN)/
(f) [a(ART)/(m) year(NN)/(q) ]BNP >PP >X ]VP .(.)/(wj) ]S 
3. [[The(ART)/e window(NN)/(ng)]BNP [is/e/VBZ <[e/(d) narrower(JJR)/Coling 2010: Poster Volume, pages 516?524,
Beijing, August 2010
Effective Constituent Projection across Languages
Wenbin Jiang and Yajuan Lu? and Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, lvyajuan, yliu, liuqun}@ict.ac.cn
Abstract
We describe an effective constituent pro-
jection strategy, where constituent pro-
jection is performed on the basis of de-
pendency projection. Especially, a novel
measurement is proposed to evaluate the
candidate projected constituents for a tar-
get language sentence, and a PCFG-style
parsing procedure is then used to search
for the most probable projected con-
stituent tree. Experiments show that, the
parser trained on the projected treebank
can significantly boost a state-of-the-art
supervised parser. When integrated into a
tree-based machine translation system, the
projected parser leads to translation per-
formance comparable with using a super-
vised parser trained on thousands of anno-
tated trees.
1 Introduction
In recent years, supervised constituent parsing has
been well studied and achieves the state-of-the-art
for many resource-rich languages (Collins, 1999;
Charniak, 2000; Petrov et al, 2006). Because
of the cost and difficulty in treebank construc-
tion, researchers have also investigated the utiliza-
tion of unannotated text, including the unsuper-
vised parsing which totally uses unannotated data
(Klein and Manning, 2002; Klein and Manning,
2004; Bod, 2006; Seginer, 2007), and the semi-
supervised parsing which uses both annotated and
unannotated data (Sarkar, 2001; Steedman et al,
2003; McClosky et al, 2006).
Because of the higher complexity and lower
performance of unsupervised methods, as well as
the need of reliable priori knowledge in semi-
supervised methods, it seems promising to project
the syntax structures from a resource-rich lan-
guage to a resource-scarce one across a bilingual
corpus. Lots of researches have so far been de-
voted to dependency projection (Hwa et al, 2002;
Hwa et al, 2005; Ganchev et al, 2009; Smith
and Eisner, 2009). While for constituent projec-
tion there is few progress. This is due to the fact
that the constituent syntax describes the language
structure in a more detailed way, and the degree of
isomorphism between constituent structures ap-
pears much lower.
In this paper we propose for constituent pro-
jection a stepwise but totally automatic strategy,
which performs constituent projection on the ba-
sis of dependency projection, and then use a con-
straint EM optimization algorithm to optimized
the initially projected trees. Given a word-aligned
bilingual corpus with source sentences parsed, we
first project the dependency structures of these
constituent trees to the target sentences using a
dynamic programming algorithm, then we gener-
ate a set of candidate constituents for each target
sentence and design a novel evaluation function
to calculate the probability of each candidate con-
stituent, finally, we develop a PCFG-style parsing
procedure to search for the most probable pro-
jected constituent tree in the evaluated candidate
constituent set. In addition, we design a constraint
EM optimization procedure to decrease the noise
in the initially projected constituent treebank.
Experimental results validate the effectiveness
of our approach. On the Chinese-English FBIS
corpus, we project the English parses produced
by the Charniak parser across to the Chinese sen-
516
tences. A berkeley parser trained on this pro-
jected treebank can effectively boost the super-
vised parsers trained on bunches of CTB trees.
Especially, the supervised parser trained on the
smaller CTB 1.0 benefits a significant F-measure
increment of more than 1 point from the projected
parser. When using the projected parser in a tree-
based translation model (Liu et al, 2006), we
achieve translation performance comparable with
using a state-of-the-art supervised parser trained
on thousands of CTB trees. This surprising re-
sult gives us an inspiration that better translation
would be achieved by combining both projected
parsing and supervised parsing into a hybrid pars-
ing schema.
2 Stepwise Constituent Projection
We first introduce the dynamic programming pro-
cedure for dependency projection, then describe
the PCFG-style algorithm for constituent projec-
tion which is conducted on projected dependent
structures, and finally show the constraint EM
procedure for constituent optimization.
2.1 Dependency Projection
For dependency projection we adopt a dynamic
programming algorithm, which searches the most
probable projected target dependency structure
according to the source dependency structure and
the word alignment.
In order to mitigate the effect of word alignment
errors, multiple GIZA++ (Och and Ney, 2000) re-
sults are combined into a compact representation
called alignment matrix. Given a source sentence
with m words, represented as E1:m, and a target
sentence with n words, represented as F1:n, their
word alignment matrix A is an m ? n matrix,
where each element Ai,j denotes the probability
of the source word Ei aligned to the target word
Fj .
Using P (DF |DE , A) to denote the probability
of the projected target dependency structure DF
conditioned on the source dependency structure
DE and the alignment matrix A, the projection al-
gorithm aims to find
D?F = argmax
DF
P (DF |DE , A) (1)
Algorithm 1 Dependency projection.
1: Input: F , and Pe for all word pairs in F
2: for ?i, j? ? ?1, |F |? in topological order do
3: buf ? ?
4: for k? i..j ? 1 do ? all partitions
5: for l ? V[i, k] and r ? V[k + 1, j] do
6: insert DERIV(l, r, Pe) into buf
7: insert DERIV(r, l, Pe) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |F |]
10: function DERIV(p, c, Pe)
11: d? p ? c ? {p ? rooty c ? root} ? new derivation
12: d ? evl ? EVAL(d, Pe) ? evaluation function
13: return d
P (DF |DE , A) can be factorized into each depen-
dency edge xy y in DF
P (DF |DE , A) =
?
xyy?DF
Pe(xy y|DE , A)
Pe can then be obtained by simple accumulation
across all possible situations of correspondence
Pe(xy y|DE , A)
=
?
1?x?,y??|E|
Ax,x? ?Ay,y? ? ?(x?, y?|DE)
where ?(x?, y?|DE) is a 0-1 function that equals
1 only if the dependent relation x? y y? holds in
DE .
The search procedure needed by the argmax op-
eration in equation 1 can be effectively solved
by the Chu-Liu-Edmonds algorithm used in (Mc-
Donald et al, 2005). In this work, however, we
adopt a more general and simple dynamic pro-
gramming algorithm as shown in Algorithm 1,
in order to facilitate the possible expansions. In
practice, the cube-pruning strategy (Huang and
Chiang, 2005) is used to speed up the enumera-
tion of derivations (loops started by line 4 and 5).
2.2 Constituent Projection
The PCFG-style parsing procedure searches for
the most probable projected constituent tree in
a shrunken search space determined by the pro-
jected dependency structure and the target con-
stituent tree. The shrunken search space can be
built as following. First, we generates the candi-
date constituents of the source tree and the can-
didate spans of the target sentence, so as to enu-
merate the candidate constituents of the target sen-
tence. Then we compute the consistent degree for
517
each pair of candidate constituent and span, and
further estimate the probability of each candidate
constituent for the target sentence.
2.2.1 Candidate Constituents and Spans
For the candidate constituents of the source
tree, using only the original constituents imposes
a strong hypothesis of isomorphism on the con-
stituent projection between two languages, since
it requires that each couple of constituent and span
must be strictly matched. While for the candi-
date spans of the target sentences, using all sub-
sequences makes the search procedure suffer from
more perplexity. Therefore, we expand the candi-
date constituent set and restrict the candidate span
set:
? Candidate Constituent: Suppose a produc-
tion in the source constituent tree, denoted as
p ? c1c2..ch..c|p|, and ch is the head child
of the parent p. Each constituent, p or c, is a
triple ?lb, rb, nt?, where nt denotes its non-
terminal, while lb and rb represent its left-
and right bounds of the sub-sequence that the
constituent covers. The candidate constituent
set of this production consists the head of
the production itself, and a set of incomplete
constituents,
{?l, r, p ? nt??|c1 ? lb ? l ? ch ? lb?
ch ? rb ? r ? c|p| ? rb?
(l < ch ? lb ? r > ch ? rb)}
where the symbol ? indicates an incomplete
non-terminal. The candidate constituent set
of the entire source tree is the unification of
the sets extracted from all productions of the
tree.
? Candidate Span: A candidate span of the tar-
get sentence is a tuple ?lb, rb?, where lb and
rb indicate the same as in a constituent. We
define the candidate span set as the spans of
all regular dependent segments in the corre-
sponding projected dependency structure. A
regular dependency segment is a dependent
segment that every modifier of the root is a
complete dependency structure. Suppose a
dependency structure rooted at word p, de-
noted as clL..cl2cl1 x p y cr1cr2..crR, it
has L (L ? 0) modifiers on its left and R
(R ? 0) modifiers on its right, each of them
is a smaller complete dependency structure.
Then the word p itself is a regular depen-
dency segment without any modifier, and
{cli..cl1 x py cr1..crj |0 ? i ? L?
0 ? j ? R?
(i > 0 ? j > 0)}
is a set of regular dependency structures with
at least one modifier. The regular depen-
dency segments of the entire projected de-
pendency structure can simply be accumu-
lated across all dependency nodes.
2.2.2 Span-to-Constituent Correspondence
After determining the candidate constituent set
of the source tree, denoted as ?E , and the can-
didate span set of the target sentence, denoted as
?F , we then calculate the consistent degree for
each pair of candidate constituent and candidate
span.
Given a candidate constituent ? ? ?E and a
candidate span ? ? ?F , their consistent degree
C(?, ?|A) is the probability that they are aligned
to each other according to A.
We display the derivations from bottom to up.
First, we define the alignment probability from a
word i in the span ? to the constituent ? as
P (i 7? ?|A) =
?
??lb?j???rbAi,j?
j Ai,j
Then we define the alignment probability from the
span ? to the constituent ? as
P (? 7? ?|A) =
?
??lb?i???rb
P (i 7? ?|A)
Note that we use i to denote both a word and its in-
dex for simplicity without causing confusion. Fi-
nally, we define C(?,?|A) as
C(?, ?|A) = P (? 7? ?|A)? P (? 7? ?|AT ) (2)
Where P (? 7? ?|AT ) denotes the alignment
probability from the constituent ? to the span ?, it
can be calculated in the same manner.
518
2.2.3 Constituent Projection Algorithm
The purpose of constituent projection is to find
the most probable projected constituent tree for
the target sentence conditioned on the source con-
stituent tree and the word alignment
T?F = argmax
TF??F
P (TF |TE, A) (3)
Here, we use ?F to denote the set of candidate
constituents of the target sentence
?F = ?F ?NT (?E)
= {?F |?(?F ) ? ?F ? nt(?F ) ? NT (?E)}
where ?(?) and nt(?) represent the span and the
non-terminal of a constituent respectively, and
NT (?) represents the set of non-terminals ex-
tracted from a constituent set. Note that TF is a
subset of ?F if we treat a tree as a set of con-
stituents.
The probability of the projected tree TF can be
factorized into the probabilities of the projected
constituents that composes the tree
P (TF |TE , A) =
?
?F?TF
P?(?F |TE , A)
while the probability of the projected source con-
stituent can be defined as a statistics of span-to-
constituent- and constituent-to-constituent consis-
tent degrees
P?(?F |TE , A) =
?
?E??E C(?F , ?E |A)?
?E??E C(?(?F ), ?E |A)
where C(?F , ?E |A) in the numerator denotes the
consistent degree for each pair of constituents,
which can be calculated based on that of span and
constituent described in Formula 2
C(?F , ?E) =
{
0 if ?F ? nt 6= ?E ? nt
C(?(?F ), ?E) else
Algorithm 2 shows the pseudocode for con-
stituent projection. A PCFG-style parsing pro-
cedure searches for the best projected constituent
tree in the constrained space determined by ?F .
Note that the projected trees are binarized, and can
be easily recovered according to the asterisks at
the tails of non-terminals.
Algorithm 2 Constituent projection.
1: Input: ?F , ?F , and P? for all spans in ?F
2: for ?i, j? ? ? in topological order do
3: buf ? ?
4: for p ? ?F s.t. ?(p) = ?i, j? do
5: for k? i..j ? 1 do ? all partitions
6: for l ? V[i, k] and r ? V[k + 1, j] do
7: insert DERIV(l, r, p, P?) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |F |]
10: function DERIV(l, r, p, P?)
11: d? l ? r ? {p} ? new derivation
12: d ? evl ? EVAL(d, P?) ? evaluation function
13: return d
2.3 EM Optimization
Since the constituent projection is conducted on
each sentence pair separately, the projected tree-
bank is apt to suffer from more noise caused by
free translation and word alignment error. It can
be expected that an EM iteration over the whole
projected treebank will lead to trees with higher
consistence.
We adopt the inside-outside algorithm to im-
prove the quality of the initially projected tree-
bank. Different from previous works, all expecta-
tion and maximization operations for a single tree
are performed in a constrained space determined
by the candidate span set of the projected target
dependency structure. That is to say, all the sum-
mation operations, both for calculating ?/? values
and for re-estimating the rule probabilities, only
consider the spans in the candidate span set. This
means that the projected dependency structures
are supposed believable, and the noise is mainly
introduced in the following constituent projection
procedure.
Here we give an overall description of the tree-
bank optimization procedure. First, an initial
PCFG grammar G0F is estimated from the original
projected treebank. Then several iterations of ?/?
calculation and rule probability re-estimation are
performed. For example in the i-the iteration, ?/?
values are calculated based on the current gram-
mar Gi?1F , afterwards the optimized grammar GiF
is obtained based on these ?/? values. The itera-
tive procedure terminates when the likelihood of
whole treebank increases slowly. Finally, with the
optimized grammar, a constrained PCFG parsing
procedure is conducted on each of the initial pro-
519
jected trees, so as to obtain an optimized treebank.
3 Applications of Constituent Projection
The most direct contribution of constituent pro-
jection is pushing an initial step for the statis-
tical constituent parsing of resource-scarce lan-
guages. It also has some meaningful applica-
tions even for the resource-rich languages. For
instances, the projected treebank, due to its large
scale and high coverage, can used to boost an tra-
ditional supervised-trained parser. And, the parser
trained on the projected treebank can adopted to
conduct tree-to-string machine translation, since
it give parsing results with larger isomorphism
with the target language than a supervised-trained
parser dose.
3.1 Boost an Traditional Parser
We first establish a unified framework for the en-
hanced parser where a projected parser is adopted
to guide the parsing procedure of the baseline
parser.
For a given target sentence S, the enhanced
parser selected the best parse T? among the set
of candidates ?(S) according to two evaluation
functions, given by the baseline parser B and the
projected guide parser G, respectively.
T? = argmax
T??(S)
P (T |B)? P (T |G)? (4)
These two evaluation functions can be integrated
deeply into the decoding procedure (Carreras et
al., 2008; Zhang and Clark, 2008; Huang, 2008),
or can be integrated at a shallow level in a rerank-
ing manner (Collins, 2000; Charniak and John-
son, 2005). For simplicity and generability, we
adopt the reranking strategy. In k-best reranking,
?(S) is simply a set of candidate parses, denoted
as {T1, T2, ..., Tk}, and we use the single parse of
the guide parser, TG, to re-evaluate these candi-
dates. Formula 4 can be redefined as
T? (TG) = argmax
T??(S)
w ? f(T, TG) (5)
Here, f(T, TG) and w represent a high dimen-
sional feature representation and a correspond-
ing weight vector, respectively. The first feature
f1(T, TG) = logP (T |B) is the log probability
of the baseline parser, while the remaining fea-
tures are integer-valued guide features, and each
of them represents the guider parser?s predication
result for a particular configuration in candidate
parse T , so as to utilize the projected parser?s
knowledge to guide the parsing procedure of the
traditional parser.
In our work a guide feature is composed of two
parts, the non-terminal of a certain constituent ?
in the candidate parse T ,1 and the non-terminal
at the corresponding span ?(?) in the projected
parse TG. Note that in the projected parse this
span does not necessarily correspond to a con-
stituent. In such situations, we simply use the
non-terminal of the constituent that just be able
to cover this span, and attach a asterisk at the tail
of this non-terminal. Here is an example of the
guide features
f100(T, TG) = V P ? T ? PP? ? TG
It represents that a V P in the candidate parse cor-
responds to a segment of a PP in the projected
parse. The quantity of its weight w100 indicates
how probably a span can be predicated as V P if
the span corresponds to a partial PP in the pro-
jected parse.
We adopt the perceptron algorithm to train
the reranker. To reduce overfitting and pro-
duce a more stable weight vector, we also use
a refinement strategy called averaged parameters
(Collins, 2002).
3.2 Using in Machine Translation
Researchers have achieved promising improve-
ments in tree-based machine translation (Liu et
al., 2006; Huang et al, 2006). Such models use
a parsed tree as input and converts it into a target
tree or string. Given a source language sentence,
first we use a traditional source language parser
to parse the sentence to obtain the syntax tree T ,
and then use the translation decoder to search for
the best derivation d?, where a derivation d is a se-
quence of transformations that converts the source
tree into the target language string
d? = argmax
d?D
P (d|T ) (6)
1Using non-terminals as features brings no improvement
in the reranking experiments, so as to examine the impact of
the projected parser.
520
Here D is the candidate set of d, and it is deter-
mined by the source tree T and the transformation
rules.
Since the tree-based models are based on
the synchronous transformational grammars, they
suffer much from the isomerism between the
source syntax and the target sentence structure.
Considering that the parsed tree produced by a
projected parser may have larger isomorphism
with the target language, it would be a promis-
ing idea to adopt the projected parser to parse the
input sentence for the subsequent translation de-
coding procedure.
4 Experiments
In this section, we first invalidate the effect of con-
stituent projection by evaluating a parser trained
on the projected treebank. Then we investigate
two applications of the projected parser: boosting
an traditional supervised-trained parser, and inte-
gration in a tree-based machine translation sys-
tem. Following the previous works, we depict the
parsing performance by F-score on sentences with
no more than 40 words, and evaluate the transla-
tion quality by the case-sensitive BLEU-4 metric
(Papineni et al, 2002) with 4 references.
4.1 Constituent Projection
We perform constituent projection from English
to Chinese on the FBIS corpus, which contains
239K sentence pairs with about 6.9M/8.9M words
in Chinese/English. The English sentences are
parsed by the Charniak Parser and the dependency
structures are extracted from these parses accord-
ing to the head-finding rules of (Yamada and
Matsumoto, 2003). The word alignment matrixes
are obtained by combining the 10-best results of
GIZA++ according to (Liu et al, 2009).
We first project the dependency structures from
English to Chinese according to section 2.1, and
then project the constituent structures according
to section 2.2. We define an assessment criteria
to evaluate the confidence of the final projected
constituent tree
c = n
?
P (DF |DE , A) ? P (TF |TE , A)
where n is the word count of a Chinese sentence
in our experiments. A series of projected Chi-
Thres c #Resrv Cons-F1 Span-F1
0.5 12.6K 23.9 32.7
0.4 17.8K 23.9 33.4
0.3 27.2K 25.4 35.7
0.2 45.1K 26.6 38.0
0.1 87.0K 27.8 40.4
Table 1: Performances of the projected parsers
on the CTB test set. #Resrv denotes the amount
of reserved trees within threshold c. Cons-F1 is
the traditional F-measure, while Span-F1 is the F-
measure without consideration of non-terminals.
nese treebanks with different scales are obtained
by specifying different c as the filtering threshold.
The state-of-the-art Berkeley Parser is adopted to
train on these treebanks because of its high per-
formance and independence of head word infor-
mation.
Table 1 shows the performances of these pro-
jected parsers on the standard CTB test set, which
is composed of sentences in chapters 271-300.
We find that along with the decrease of the filter-
ing threshold c, more projected trees are reserved
and the performance of the projected parser con-
stantly increases. We also find that the traditional
F-value, Cons-F1, is obviously lower than the one
without considering non-terminals, Span-F1. This
indicates that the constituent projection procedure
introduces more noise because of the higher com-
plexity of constituent correspondence. In all the
rest experiments, however, we simply use the pro-
jected treebank filtered by threshold c = 0.1 and
do not try any smaller thresholds, since it already
takes more than one weak to train the Berkeley
Parser on the 87 thousands trees resulted by this
threshold.
The constrained EM optimization procedure
described in section 2.3 is used to alleviate the
noise in the projected treebank, which may be
caused by free translation, word alignment errors,
and projection on each single sentence pair. Fig-
ure 1 shows the log-likelihood on the projected
treebank after each EM iteration. It is obvious that
the log-likelihood increases very slowly after 10
iterations. We terminate the EM procedure after
40 iterations.
Finally we train the Berkeley Parser on the op-
timized projected treebank, and test its perfor-
521
-65
-64
-63
-62
-61
-60
-59
-58
 0  5  10  15  20  25  30  35  40
Lo
g-
lik
eli
ho
od
EM iteration
Figure 1: Log-likelihood of the 87K-projected
treebank after each EM interation.
Train Set Cons-F1 Span-F1
Original 87K 27.8 40.4
Optimized 87K 22.8 40.2
Table 2: Performance of the parser trained on the
optimized projected treebank, compared with that
of the original projected parser.
Train Set Baseline Bst-Ini Bst-Opt
CTB 1.0 75.6 76.4 76.9
CTB 5.0 85.2 85.5 85.7
Table 3: Performance improvement brought by
the projected parser to the baseline parsers trained
on CTB 1.0 and CTB 5.0, respectively. Bst-
Ini/Bst-Opt: boosted by the parser trained on the
initial/optimized projected treebank.
mance on the standard CTB test set. Table 2
shows the performance of the parser trained on
the optimized projected treebank. Unexpectedly,
we find that the constituent F1-value of the parser
trained on the optimized treebank drops sharply
from the baseline, although the span F1-value re-
mains nearly the same. We assume that the EM
procedure gives the original projected treebank
more consistency between each single tree while
the revised treebank deviates from the CTB anno-
tation standard, but it needs to be validated by the
following experiments.
4.2 Boost an Traditional Parser
The projected parser is used to help the reranking
of the k-best parses produced by another state-of-
the-art parser, which is called the baseline parser
for convenience. In our experiments we choose
the revised Chinese parser (Xiong et al, 2005)
 70
 72
 74
 76
 78
 80
 82
 84
 86
 88
 1000  10000
Pa
rs
ev
al 
F-
sc
or
e 
(%
)
Scale of treebank (log)
CTB 1.0
CTB 5.0
baseline
boosted parser
Figure 2: Boosting performance of the projected
parser on a series of baseline parsers that are
trained on treebanks of different scales.
based on Collins model 2 (Collins, 1999) as the
baseline parser.2
The baseline parser is respectively trained on
CTB 1.0 and CTB 5.0. For both corpora we
follow the traditional corpus splitting: chapters
271-300 for testing, chapters 301-325 for devel-
opment, and else for training. Experimental re-
sults are shown in Table 3. We find that both
projected parsers bring significant improvement to
the baseline parsers. Especially the later, although
performs worse on CTB standard test set, gives a
larger improvement than the former. This to some
degree confirms the previous assumption. How-
ever, more investigation must be conducted in the
future.
We also observe that for the baseline parser
trained on the much larger CTB 5.0, the boost-
ing performance of the projected parser is rela-
tively lower. To further investigate the regularity
that the boosting performance changes according
to the scale of training treebank of the baseline
parser, we train a series of baseline parsers with
different amounts of trees, then use the projected
parser trained on the optimized treebank to en-
hance these baseline parsers. Figure 2 shows the
experimental results. From the curves we can see
that the smaller the training corpus of the baseline
parser, the more significant improvement can be
obtained. This is a good news for the resource-
scarce languages that have no large treebanks.
2The Berkeley Parser fails to give k-best parses for some
sentences when trained on small treebanks, and these sen-
tences have to be deleted in the k-best reranking experiments.
522
4.3 Using in Machine Translation
We investigate the effect of the projected parser
in the tree-based translation model on Chinese-to-
English translation. A series of contrast transla-
tion systems are built, each of which uses a super-
vised Chinese parser (Xiong et al, 2005) trained
on a particular amount of CTB trees.
We use the FBIS Chinese-English bitext as the
training corpus, the 2002 NIST MT Evaluation
test set as our development set, and the 2005 NIST
MT Evaluation test set as our test set. We first ex-
tract the tree-to-string translation rules from the
training corpus by the algorithm of (Liu et al,
2006), and train a 4-gram language model on
the Xinhua portion of GIGAWORD corpus with
Kneser-Ney smoothing using the SRI Language
Modeling Toolkit (Stolcke and Andreas, 2002).
Then we use the standard minimum error-rate
training (Och, 2003) to tune the feature weights
to maximize the system.s BLEU score.
Figure 3 shows the experimental results. We
find that the translation system using the projected
parser achieves the performance comparable with
the one using the supervised parser trained on
CTB 1.0. Considering that the F-score of the pro-
jected parser is only 22.8%, which is far below of
the 75.6% F-score of the supervised parser trained
on CTB 1.0, we can give more confidence to the
assumption that the projected parser is apt to de-
scribe the syntax structure of the counterpart lan-
guage. This surprising result also gives us an in-
spiration that better translation would be achieved
by combining projected parsing and supervised
parsing into hybrid parsing schema.
5 Conclusion
This paper describes an effective strategy for con-
stituent projection, where dependency projection
and constituent projection are consequently con-
ducted to obtain the initial projected treebank,
and an constraint EM procedure is then per-
formed to optimized the projected trees. The
projected parser, trained on the projected tree-
bank, significantly boosts an existed state-of-the-
art supervised-trained parser, especially trained on
a smaller treebank. When using the projected
parser in tree-based translation, we achieve the
0.220
0.230
0.240
0.250
0.260
0.270
 1000  10000
BL
EU
 sc
or
e
Scale of treebank (log)
use projected parser
CTB 1.0
CTB 5.0
use supervised parsers
Figure 3: Performances of the translation systems,
which use the projected parser and a series of su-
pervised parsers trained CTB trees.
translation performance comparable with using a
supervised parser trained on thousands of human-
annotated trees.
As far as we know, this is the first time that
the experimental results are systematically re-
ported about the constituent projection and its ap-
plications. However, many future works need
to do. For example, more energy needs to be
devoted to the treebank optimization, and hy-
brid parsing schema that integrates the strengths
of both supervised-trained parser and projected
parser would be valuable to be investigated for
better translation.
Acknowledgments
The authors were supported by 863 State Key
Project No. 2006AA010108, National Natural
Science Foundation of China Contract 60873167,
Microsoft Research Asia Natural Language Pro-
cessing Theme Program grant (2009-2010), and
National Natural Science Foundation of China
Contract 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions.
References
Bod, Rens. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proceedings of the COLING-
ACL.
Carreras, Xavier, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the CoNLL.
523
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the ACL.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the NAACL.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. In Ph.D. Thesis.
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
ICML, pages 175?182.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1?8, Philadelphia, USA.
Ganchev, Kuzman, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53?64.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the AMTA.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping parsers via syntactic projection across paral-
lel texts. In Natural Language Engineering, vol-
ume 11, pages 311?325.
Klein, Dan and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proceedings of the ACL.
Klein, Dan and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models
of dependency and constituency. In Proceedings of
the ACL.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the ACL.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of the ACL.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Och, Franz J. and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL.
Och, Franz Joseph. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41th Annual Meeting of the Association
for Computational Linguistics, pages 160?167.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Weijing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the ACL.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
ACL.
Sarkar, Anoop. 2001. Applying co-training methods
to statistical parsing. In Proceedings of NAACL.
Seginer, Yoav. 2007. Fast unsupervised incremental
parsing. In Proceedings of the ACL.
Smith, David and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Steedman, Mark, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the EACL.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 311?318.
Xiong, Deyi, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Yamada, H and Y Matsumoto. 2003. Statistical de-
pendency analysis using support vector machines.
In Proceedings of IWPT.
Zhang, Yue and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
524
Coling 2010: Poster Volume, pages 1185?1193,
Beijing, August 2010
Dependency-Based Bracketing Transduction Grammar
for Statistical Machine Translation
Jinsong Su, Yang Liu, Haitao Mi, Hongmei Zhao, Yajuan Lu?, Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{sujinsong,yliu,htmi,zhaohongmei,lvyajuan,liuqun}@ict.ac.cn
Abstract
In this paper, we propose a novel
dependency-based bracketing transduc-
tion grammar for statistical machine
translation, which converts a source sen-
tence into a target dependency tree. Dif-
ferent from conventional bracketing trans-
duction grammar models, we encode tar-
get dependency information into our lex-
ical rules directly, and then we employ
two different maximum entropy models
to determine the reordering and combi-
nation of partial dependency structures,
when we merge two neighboring blocks.
By incorporating dependency language
model further, large-scale experiments on
Chinese-English task show that our sys-
tem achieves significant improvements
over the baseline system on various test
sets even with fewer phrases.
1 Introduction
Bracketing transduction grammar (BTG) (Wu,
1995) is an important subclass of synchronous
context free grammar, which employs a special
synchronous rewriting mechanism to parse paral-
lel sentence of both languages.
Due to the prominent advantages such as the
simplicity of grammar and the good coverage of
syntactic diversities in different language pairs,
BTG has attracted increasing attention in statis-
tical machine translation (SMT). In flat reorder-
ing model (Wu, 1996; Zens et al, 2004), which
assigns constant reordering probabilities depend-
ing on the language pairs, BTG constraint proves
to be very effective for reducing the search space
of phrase reordering. To pursue a better method
to predict the order between two neighboring
blocks1, Xiong et al (2006) present an enhanced
BTG with a maximum entropy (ME) based re-
ordering model. Along this line, source-side syn-
tactic knowledge is introduced into the reorder-
ing model to improve BTG-based translation (Se-
tiawan et al, 2007; Zhang et al, 2007; Xiong et
al., 2008; Zhang and Li, 2009). However, these
methods mainly focus on the utilization of source
syntactic knowledge, while ignoring the modeling
of the target-side syntax that directly influences
the translation quality. As a result, how to ob-
tain better translation by exploiting target syntac-
tic knowledge is somehow neglected. Thus, we
argue that it is important to model the target-side
syntax in BTG-based translation.
Recently, modeling syntactic information on
the target side has progressed significantly. De-
pending on the type of output, these models can
be divided into two categories: the constituent-
output systems (Galley et al, 2006; Zhang et
al., 2008; Liu et al, 2009) and dependency-
output systems (Eisner, 2003; Lin, 2004; Ding
and Palmer, 2005; Quirk et al, 2005; Shen et
al., 2008). Compared with the constituent-output
systems, the dependency-output systems provide a
simpler platform to capture the target-side syntac-
tic information, while also having the best inter-
lingual phrasal cohesion properties (Fox, 2002).
Typically, Shen et al (2008) propose a string-to-
dependency model, which integrates the target-
side well-formed dependency structure into trans-
lation rules. With the dependency structure, this
system employs a dependency language model
(LM) to exploit long distance word relations, and
achieves a significant improvement over the hier-
archical phrase-based system (Chiang, 2007). So
1A block is a bilingual phrase without maximum length
limitation.
1185
we think it will be a promising way to integrate the
target-side dependency structure into BTG-based
translation.
In this paper, we propose a novel dependency-
based BTG (DepBTG) for SMT, which represents
translation in the form of dependency tree. Ex-
tended from BTG, our grammars operate on two
neighboring blocks with target dependency struc-
ture. We integrate target syntax into bilingual
phrases and restrict target phrases to the well-
formed structures inspired by (Shen et al, 2008).
Then, we adopt two ME models to predict how to
reorder and combine partial structures into a target
dependency tree, which gives us access to captur-
ing the target-side syntactic information. To the
best of our knowledge, this is the first effort to
combine the translation generation with the mod-
eling of target syntactic structure in BTG-based
translation.
The remainder of this paper is structured as fol-
lows: In Section 2, we give brief introductions to
the bases of our research: BTG and dependency
tree. In Section 3, we introduce DepBTG in detail.
In Section 4, we further illustrate how to create
two ME models to predict the reordering and de-
pendency combination between two neighboring
blocks. Section 5 describes the implementation
of our decoder. Section 6 shows our experiments
on Chinese-English task. Finally, we end with a
summary and future research in Section 7.
2 Background
2.1 BTG
BTG is a special case of synchronous context free
grammar. There are three rules utilized in BTG:
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
where the reordering rules (1) and (2) are used
to merge two neighboring blocks A1 and A2 in
a straight or inverted order, respectively. The lex-
ical rule (3) is used to translate the source phrase
x into the target phrase y.
 



	




	
 
		

	
 


	


 








Figure 1: The dependency tree for sentence The
UN will provide abundant financial aid to Haiti
next week.
2.2 Dependency Tree
In a given sentence, each word depends on a par-
ent word, except for the root word. The depen-
dency tree for a given sentence reflects the long
distance dependency and grammar relations be-
tween words. Figure 1 shows an example of a de-
pendency tree, where a black arrow points from a
child word to its parent word.
Compared with constituent tree, dependency
tree directly models semantic structure of a sen-
tence in a simpler form. Thus, it provides a desir-
able platform for us to utilize the target-side syn-
tactic knowledge.
3 Dependency-based BTG
3.1 Grammars
In this section, we extend the original BTG into
DepBTG. The rules of DepBTG, which derive
from that of BTG, merge blocks with target de-
pendency structure into a larger one. These rules
take the following forms:
Ad ? [A1d, A2d]CC (4)
Ad ? [A1d, A2d]LA (5)
Ad ? [A1d, A2d]RA (6)
Ad ? ?A1d, A2d?CC (7)
Ad ? ?A1d, A2d?LA (8)
Ad ? ?A1d, A2d?RA (9)
Ad ? x/y (10)
where A1d and A2d represent two neighboring
blocks with target dependency structure. Rules
(4)?(9) are used to determine the reordering and
combination of two dependency structures, when
1186

Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 412?420, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Iterative Annotation Transformation with Predict-Self Reestimation
for Chinese Word Segmentation
Wenbin Jiang and Fandong Meng and Qun Liu and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, mengfandong, liuqun, lvyajuan}@ict.ac.cn
Abstract
In this paper we first describe the technol-
ogy of automatic annotation transformation,
which is based on the annotation adaptation
algorithm (Jiang et al2009). It can auto-
matically transform a human-annotated cor-
pus from one annotation guideline to another.
We then propose two optimization strategies,
iterative training and predict-self reestimation,
to further improve the accuracy of annota-
tion guideline transformation. Experiments on
Chinese word segmentation show that, the it-
erative training strategy together with predict-
self reestimation brings significant improve-
ment over the simple annotation transforma-
tion baseline, and leads to classifiers with sig-
nificantly higher accuracy and several times
faster processing than annotation adaptation
does. On the Penn Chinese Treebank 5.0,
it achieves an F-measure of 98.43%, signif-
icantly outperforms previous works although
using a single classifier with only local fea-
tures.
1 Introduction
Annotation guideline adaptation depicts a general
pipeline to integrate the knowledge of corpora with
different underling annotation guidelines (Jiang et
al., 2009). In annotation adaptation two classifiers
are cascaded together, where the classification re-
sults of the lower classifier are used as guiding fea-
tures of the upper classifier, in order to achieve more
accurate classification. This method can automat-
ically adapt the divergence between different an-
notation guidelines and bring improvement to Chi-
nese word segmentation. However, the need of cas-
caded classification decisions makes it less practical
for tasks of high computational complexity such as
parsing, and less efficient to incorporate more than
two annotated corpora.
In this paper, we first describe the algorithm of
automatic annotation transformation. It is based on
the annotation adaptation algorithm, and it focuses
on the automatic transformation (rather than adapta-
tion) of a human-annotated corpus from one annota-
tion guideline to another. First, a classifier is trained
on the corpus with an annotation guideline not de-
sired, it is used to classify the corpus with the an-
notation guideline we want, so as to obtain a corpus
with parallel annotation guidelines. Then a second
classifier is trained on the parallelly annotated cor-
pus to learn the statistical regularity of annotation
transformation, and it is used to process the previous
corpus to transform its annotation guideline to that
of the target corpus. Instead of the online knowl-
edge integration methodology of annotation adapta-
tion, annotation transformation can lead to improved
classification accuracy in an offline manner by using
the transformed corpora as additional training data
for the classifier. This method leads to an enhanced
classifier with much faster processing than the cas-
caded classifiers in annotation adaptation.
We then propose two optimization strategies, iter-
ative training and predict-self reestimation, to fur-
ther improve the accuracy of annotation transfor-
mation. Although the transformation classifiers
can only be trained on corpora with autogenerated
(rather than gold) parallel annotations, an iterative
training procedure can gradually improve the trans-
412
formation accuracy by iteratively optimizing the par-
allelly annotated corpora. Both source-to-target and
target-to-source annotation transformations are per-
formed in each training iteration, and the trans-
formed corpora are used to provide better annota-
tions for the parallelly annotated corpora of the next
iteration; then the better parallelly annotated corpora
will result in more accurate transformation classi-
fiers, which will generate better transformed corpora
in the new iteration. The predict-self reestimation
is based on the following hypothesis, a better trans-
formation result should be easier to be transformed
back to the original form. The predict-self heuristic
is also validated by Daume? III (2009) in unsuper-
vised dependency parsing.
Experiments in Chinese word segmentation show
that, the iterative training strategy together with
predict-self reestimation brings significant improve-
ment over the simple annotation transformation
baseline. We perform optimized annotation trans-
formation from the People?s Daily (Yu et al2001)
to the Penn Chinese Treebank 5.0 (CTB) (Xue et
al., 2005), in order to improve the word segmenter
with CTB annotation guideline. Compared to anno-
tation adaptation, the optimized annotation transfor-
mation strategy leads to classifiers with significantly
higher accuracy and several times faster processing
on the same data sets. On CTB 5.0, it achieves an F-
measure of 98.43%, significantly outperforms pre-
vious works although using a single classifier with
only local features.
The rest of the paper is organized as follows.
Section 2 describes the classification-based Chinese
word segmentation method. Section 3 details the
simple annotation transformation algorithm and the
two optimization methods. After the introduction of
related works in section 4, we give the experimental
results on Chinese word segmentation in section 5.
2 Classification-Based Chinese Word
Segmentation
Chinese word segmentation can be formalized as
the problem of sequence labeling (Xue and Shen,
2003), where each character in the sentence is given
a boundary tag denoting its position in a word. Fol-
lowing Ng and Low (2004), joint word segmenta-
tion and part-of-speech (POS) tagging can also be
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi) ?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? + ?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
solved in a character classification approach by ex-
tending the boundary tags to include POS informa-
tion. For word segmentation we adopt the 4 bound-
ary tags of Ng and Low (2004), b, m, e and s, where
b, m and e mean the beginning, the middle and the
end of a word, and s indicates a single-character
word. The word segmentation result can be gen-
erated by splitting the labeled character sequence
into subsequences of pattern s or bm?e, indicating
single-character words or multi-character words, re-
spectively.
We choose the perceptron algorithm (Collins,
2002) to train the character classifier. It is an online
training algorithm and has been successfully used in
many NLP tasks, including POS tagging (Collins,
2002), parsing (Collins and Roark, 2004) and word
segmentation (Zhang and Clark, 2007; Jiang et al
2008; Zhang and Clark, 2010).
The training procedure learns a discriminative
model mapping from the inputs x ? X to the outputs
y ? Y , where X is the set of sentences in the train-
ing corpus and Y is the set of corresponding labeled
results. We use the function GEN(x) to enumerate
the candidate results of an input x, and the function
? to map a training example (x, y) ? X ? Y to a
feature vector ?(x, y) ? Rd. Given the character
sequence x, the decoder finds the output F (x) that
maximizes the score function:
F (x) = argmax
y?GEN(x)
S(y|~?,?, x)
= argmax
y?GEN(x)
?(x, y) ? ~?
(1)
Where ~? ? Rd is the parameter vector (that is, the
discriminative model) and ?(x, y) ? ~? is the inner
product of ?(x, y) and ~?.
Algorithm 1 shows the perceptron algorithm for
tuning the parameter ~?. The ?averaged parameters?
413
Type Feature Templates
Unigram C?2 C?1 C0
C1 C2
Bigram C?2C?1 C?1C0 C0C1
C1C2 C?1C1
Property Pu(C0)
T (C?2)T (C?1)T (C0)T (C1)T (C2)
Table 1: Feature templates for classification-based Chi-
nese segmentation model.
technology (Collins, 2002) is used for better per-
formance. The feature templates for the classifier
is shown in Table 1. C0 denotes the current char-
acter, while C?i/Ci denote the ith character to the
left/right of C0. The function Pu(?) returns true
for a punctuation character and false for others, the
function T (?) classifies a character into four types:
number, date, English letter and others.
3 Iterative and Predict-Self Annotation
Transformation
This section first describes the technology of au-
tomatic annotation transformation, then introduces
the two optimization strategies, iterative training and
predict-self reestimation. Iterative training takes
a global view, it conducts several rounds of bidi-
rectional annotation transformations, and improve
the transformation performance round by round.
Predict-self reestimation takes a local view instead,
it considers each training sentence, and improves the
transformation performance by taking into account
the predication result of the reverse transformation.
The two strategies can be adopted jointly to obtain
better transformation performance.
3.1 Automatic Annotation Transformation
Annotation adaptation can integrate the knowledge
from two corpora with different underling annota-
tion guidelines. First, a classifier (source classi-
fier) is trained on the corpus (source corpus) with
an annotation standard (source annotation) not de-
sired, it is then used to classify the corpus (target
corpus) with the annotation standard (target annota-
tion) we want. Then a second classifier (transforma-
tion classifier 1) is trained on the target corpus with
1It is called target classifier in (Jiang et al2009). We
think that transformation classifier better reflects its role, the
Type Feature Templates
Baseline C?2 C?1 C0
C1 C2
C?2C?1 C?1C0 C0C1
C1C2 C?1C1
Pu(C0)
T (C?2)T (C?1)T (C0)T (C1)T (C2)
Guiding ?
C?2 ? ? C?1 ? ? C0 ? ?
C1 ? ? C2 ? ?
C?2C?1 ? ? C?1C0 ? ? C0C1 ? ?
C1C2 ? ? C?1C1 ? ?
Pu(C0) ? ?
T (C?2)T (C?1)T (C0)T (C1)T (C2) ? ?
Table 2: Feature templates for annotation transformation,
where ? is short for ?(C0), representing the source an-
notation of C0.
the source classifier?s classification result as guid-
ing features. In decoding, a raw sentence is first de-
coded by the source classifier, and then inputted into
the transformation classifier together with the anno-
tations given by the source classifier, so as to obtain
an improved classification result.
However, annotation adaptation has a drawback,
it has to cascade two classifiers in decoding to inte-
grate the knowledge in two corpora, thus seriously
degrades the processing speed. This paper describes
a variant of annotation adaptation, name annotation
transformation, aiming at automatic transformation
(rather than adaptation) between annotation stan-
dards of human-annotated corpora. In annotation
transformation, a source classifier and a transforma-
tion classifier are trained in the same way as in an-
notation adaptation. The transformation classifier is
used to process the source corpus, with the classi-
fication label derived from the segmented sentences
as the guiding features, so as to relabel the source
corpus with the target annotation guideline. By inte-
grating the target corpus and the transformed source
corpus for the training of the character classifier, im-
proved classification accuracy can be achieved.
Both the source classifier and the transforma-
tion classifier are trained with the perceptron algo-
rithm. The feature templates used for the source
classifier are the same with those for the baseline
renaming also avoids name confusion in the optimized annota-
tion transformation.
414
Algorithm 2 Baseline annotation transformation.
1: function ANNOTRANS(Cs, Ct)
2: Ms ? TRAIN(Cs)
3: Cst ? ANNOTATE(Ms, Ct)
4: Ms?t ? TRANSTRAIN(Cst , Ct)
5: Cts ? TRANSANNOTATE(Ms?t, Cs)
6: Ct? ? Cts ? Ct
7: return Ct?
8: function DECODE(M, ?, x)
9: return argmaxy?GEN(x) S(y|M,?, x)
character classifier. The feature templates for the
transformation classifier are the same with those in
annotation adaptation, as listed in Table 2. Al-
gorithm 2 shows the overall training algorithm
for annotation transformation. Cs and Ct denote
the source corpus and the target corpus; Ms and
Ms?t denote the source classifier and the trans-
formation classifier; Cqp denotes the p corpus re-
labeled in q annotation guideline, for example Cts
is the source corpus transformed to target annota-
tion guideline; Functions TRAIN and TRANSTRAIN
both invoke the perceptron algorithm, yet with
different feature sets; Functions ANNOTATE and
TRANSANNOTATE call the function DECODE with
different models (source/transformation classifiers),
feature functions (without/with guiding features),
and inputs (raw/source-annotated sentences).
The best training iterations for the functions
TRAIN and TRANSTRAIN are determined on the de-
veloping sets of the source corpus and the target
corpus, respectively. In the algorithm the param-
eters corresponding to developing sets are omitted
for simplicity. Compared to the online knowledge
integration methodology of annotation adaptation,
annotation transformation leads to improved perfor-
mance in an offline manner by integrating corpora
before the training procedure. This manner could
achieve processing several times as fast as the cas-
caded classifiers in annotation adaptation. In the fol-
lowing we will describe the two optimization strate-
gies in details.
3.2 Iterative Training for Annotation
Transformation
The training of annotation transformation is based
on an auto-generated (rather than gold) parallelly an-
notated corpus, where the source annotation is pro-
Algorithm 3 Iterative annotation transformation.
1: function ITERANNOTRANS(Cs, Ct)
2: Ms ? TRAIN(Cs)
3: Cst ? ANNOTATE(Ms, Ct)
4: Mt ? TRAIN(Ct)
5: Cts ? ANNOTATE(Mt, Cs)
6: repeat
7: Ms?t ? TRANSTRAIN(Cst , Ct)
8: Mt?s ? TRANSTRAIN(Cts, Cs)
9: Cts ? TRANSANNOTATE(Ms?t, Cs)
10: Cst ? TRANSANNOTATE(Mt?s, Ct)
11: Ct? ? Cts ? Ct
12: M? ? TRAIN(Ct?)
13: until EVAL(M?) converges
14: return Ct?
15: function DECODE(M, ?, x)
16: return argmaxy?GEN(x) S(y|M,?, x)
vided by the source classifier. Therefore, the perfor-
mance of transformation training is correspondingly
determined by the accuracy of the source classifier.
We propose an iterative training procedure to
gradually improve the transformation accuracy by
iteratively optimizing the parallelly annotated cor-
pora. In each training iteration, both source-to-target
and target-to-source annotation transformations are
performed, and the transformed corpora are used to
provide better annotations for the parallelly anno-
tated corpora of the next iteration. Then in the new
iteration, the better parallelly annotated corpora will
result in more accurate transformation classifiers, so
as to generate better transformed corpora.
Algorithm 3 shows the overall procedure of the
iterative training method. The loop of lines 6-13
iteratively performs source-to-target and target-to-
source annotation transformations. The source an-
notations of the parallelly annotated corpora, Cst and
Cts, are initialized by applying the source and tar-
get classifiers respectively on the target and source
corpora (lines 2-5). In each training iteration, the
transformation classifiers are trained on the current
parallelly annotated corpora (lines 7-8), they are
used to produce the transformed corpora (lines 9-10)
which provide better annotations for the parallelly
annotated corpora of the next iteration. The itera-
tive training terminates when the performance of the
classifier trained on the merged corpus Cts ? Ct con-
verges.
415
The discriminative training of TRANSTRAIN pre-
dicts the target annotations with the guidance of
source annotations. In the first iteration, the trans-
formed corpora generated by the transformation
classifiers are better than the initialized ones gener-
ated by the source and target classifiers, due to the
assistance of the guiding features. In the follow-
ing iterations, the transformed corpora provide bet-
ter annotations for the parallelly annotated corpora
of the subsequent iteration, the transformation ac-
curacy will improve gradually along with optimiza-
tion of the parallelly annotated corpora until conver-
gence.
3.3 Predict-Self Reestimation for Annotation
Transformation
The predict-self hypothesis is implicit in many unsu-
pervised learning approaches, such as Markov ran-
dom field. This methodology has also been success-
fully used by Daume? III (2009) in unsupervised de-
pendency parsing. The basic idea of predict-self is
that, if a prediction is a better candidate for an input,
it can be easier converted back to the original input
by a reverse procedure. If applied to the task of an-
notation transformation, predict-self indicates that a
better transformation candidate following the target
annotation guideline can be easier transformed back
to the original form following the source annotation
guideline.
The most intuitionistic strategy to introduce the
predict-self methodology into annotation transfor-
mation is using a reversed annotation transforma-
tion procedure to filter out unreliable predictions of
the previous transformation. In detail, a source-to-
target annotation transformation is performed on the
source annotated sentence to obtain a prediction that
follows the target annotation guideline, then a sec-
ond, target-to-source transformation is performed
on this prediction result to check whether it can
be transformed back to the previous source annota-
tion. Transformation results failing in this reversal
verification are discarded, so this strategy is named
predict-self filtration.
A more precious strategy can be called predict-
self reestimation. Instead of using the reversed
transformation procedure for filtration, the rees-
timation strategy integrates the scores given by
the source-to-target and target-to-source annotation
transformation models when evaluating the transfor-
mation candidates. By properly tuning the relative
weights of the two transformation directions, bet-
ter transformation performance would be achieved.
The scores of the two transformation models are
weighted integrated in a log-linear manner:
S+(y|Ms?t,Mt?s,?, x)
= (1? ?)? S(y|Ms?t,?, x)
+ ?? S(x|Mt?s,?, y)
(2)
The weight parameter ? is tuned on the develop-
ing set. To integrating the predict-self reestima-
tion into the iterative transformation training, a re-
versed transformation model is introduced and the
enhanced scoring function above is used when the
function TRANSANNOTATE invokes the function
DECODE.
4 Related Works
Researches focused on the automatic adaptation
between different corpora can be roughly clas-
sified into two kinds, adaptation between differ-
ent domains (with different statistical distribution)
(Blitzer et al2006; Daume? III, 2007), and adapta-
tion between different annotation guidelines (Jiang
et al2009; Zhu et al2011). There are also
some efforts that totally or partially resort to man-
ual transformation rules, to conduct treebank con-
version (Cahill and Mccarthy, 2002; Hockenmaier
and Steedman, 2007; Clark and Curran, 2009), and
word segmentation guideline transformation (Gao
et al2004; Mi et al2008). This work focuses
on the automatic transformation between annotation
guidelines, and proposes better annotation transfor-
mation technologies to improve the transformation
accuracy and the utilization rate of human-annotated
knowledge.
The iterative training procedure proposed in this
work shares some similarity with the co-training al-
gorithm in parsing (Sarkar, 2001), where the train-
ing procedure lets two different models learn from
each other during parsing the raw text. The key
idea of co-training is utilize the complementarity of
different parsing models to mine additional training
data from raw text, while iterative training for an-
notation transformation emphasizes the iterative op-
timization of the parellelly annotated corpora used
416
Partition Sections # of word
CTB
Training 1? 270 0.47M
400? 931
1001? 1151
Developing 301? 325 6.66K
Test 271? 300 7.82K
PD
Training 02? 06 5.86M
Test 01 1.07M
Table 3: Data partitioning for CTB and PD.
to train the transformation models. The predict-
self methodology is implicit in many unsupervised
learning approaches, it has been successfully used
by (Daume? III, 2009) in unsupervised dependency
parsing. We adapt this idea to the scenario of anno-
tation transformation to improve transformation ac-
curacy.
In recent years many works have been devoted to
the word segmentation task. For example, the in-
troduction of global training or complicated features
(Zhang and Clark, 2007; Zhang and Clark, 2010);
the investigation of word structures (Li, 2011);
the strategies of hybrid, joint or stacked modeling
(Nakagawa and Uchimoto, 2007; Kruengkrai et al
2009; Wang et al2010; Sun, 2011), and the semi-
supervised and unsupervised technologies utilizing
raw text (Zhao and Kit, 2008; Johnson and Gold-
water, 2009; Mochihashi et al2009; Hewlett and
Cohen, 2011). We estimate that the annotation trans-
formation technologies can be adopted jointly with
complicated features, system combination and semi-
supervised/unsupervised technologies to further im-
prove segmentation performance.
5 Experiments and Analysis
We perform annotation transformation from Peo-
ple?s Daily (PD) (Yu et al2001) to Penn Chi-
nese Treebank 5.0 (CTB) (Xue et al2005), follow-
ing the same experimental setting as the annotation
adaptation work (Jiang et al2009) for convenience
of comparison. The two corpora are segmented fol-
lowing different segmentation guidelines and differ
largely in quantity of data. CTB is smaller in size
with about 0.5M words, while PD is much larger,
containing nearly 6M words.
Test on (F1%)
Train on CTB SPD
CTB 97.35 86.65(? 10.70)
SPD 91.23(? 3.02) 94.25
Table 4: Performance of the perceptron classifiers for
Chinese word segmentation.
Model Time (s) Accuracy (F1%)
Merging 1.33 93.79
Anno. Adapt. 4.39 97.67
Anno. Trans. 1.33 97.69
Baseline 1.21 97.35
Table 5: Comparison of the baseline annotation transfor-
mation, annotation adaptation and a simple corpus merg-
ing strategy.
To approximate more general scenarios of anno-
tation adaptation problems, we extract from PD a
subset which is comparable to CTB in size. We ran-
domly select 20, 000 sentences (0.45M words) from
the PD training data as the new training set, and
1000/1000 sentences from the PD test data as the
new test/developing set. 2 We name the smaller ver-
sion of PD as SPD. The balanced source corpus and
target corpus also facilitate the investigation of an-
notation transformation.
5.1 Baseline Classifiers for Word Segmentation
We train the baseline perceptron classifiers de-
scribed in section 2 on the training sets of SPD
and CTB, using the developing sets to determine the
best training iterations. The performance measure-
ment indicators for word segmentation is balanced
F-measure, F = 2PR/(P + R), a function of Pre-
cision P and Recall R. where P is the percentage
of words in segmentation result that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Accuracies of the baseline classifiers are listed in
Table 4. We also report the performance of the clas-
sifiers on the test sets of the opposite corpora. Ex-
perimental results are in line with our expectations.
A classifier performs better in its corresponding test
set, and performs significantly worse on a test set
following a different annotation guideline.
2There are many extremely long sentences in original PD
corpus, we split them into normal sentences according to period
punctuations.
417
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
%)
Training iterations
Iterative training
Baseline annotation transformation
Figure 1: Learning curve of iterative training for annota-
tion transformation.
5.2 Annotation Transformation vs. Annotation
Adaptation
Experiments of annotation transformation are con-
ducted on the direction of SPD-to-CTB. The trans-
formed corpus can be merged into the regular cor-
pus, so as to train an enhanced classifier. As com-
parison, the cascaded model of annotation adapta-
tion (Jiang et al2009) is faithfully implemented
(yet using our feature representation) and tested on
the same adaptation direction.
Table 5 shows the performances of the classi-
fiers resulted by the baseline annotation transforma-
tion and annotation adaptation, as well as the clas-
sifier trained on the directly merged corpus. The
time costs for decoding are also listed to facilitate
the comparison of practicality. We find that the sim-
ple corpus merging strategy leads to dramatic de-
crease in accuracy, due to the different and incom-
patible annotation guidelines. The baseline annota-
tion transformation method leads to a classifier with
accuracy increment comparable to that of the anno-
tation adaptation strategy, while consuming only one
third of the decoding time.
5.3 Iterative Training with Predict-Self
Reestimation
We adopt the iterative training strategy to the base-
line annotation transformation model. The CTB de-
veloping set is used to determine the best training
iteration for annotation transformation from SPD to
CTB. After each iteration, we test the performance
of the classifier trained on the merged corpus. Fig-
ure 1 shows the performance curve, with iterations
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (F
%)
Predict-self ratio
Predict-self reestimation
Predict-self filtration
Baseline annotation transformation
Figure 2: Performance of predict-self filtration and
predict-self reestimation.
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
%)
Training iterations
Iterative training with predict-self reestimation
Iterative training
Figure 3: Learning curve of iterative training with
predict-self reestimation for annotation transformation.
ranging from 1 to 10. The performance of the base-
line annotation transformation model is naturally in-
cluded in the curve (located at iteration 1). The
curve shows that the performance of the classifier
trained on the merged corpus consistently improves
from iteration 2 to iteration 5.
Experimental results of predict-self filtration and
predict-self reestimation are shown in Figure 2.
The curve shows the performance of the predict-self
reestimation according to a series of weight param-
eters, ranging from 0 to 1 with step 0.05. The point
at ? = 0 shows the performance of the baseline
annotation transformation strategy. The upper hor-
izontal line shows the performance of predict-self
filtration. We find that predict-self filtration brings
slight improvement over the baseline, and predict-
self reestimation outperforms the filtration strategy
when ? falls in a proper range. An initial analysis
on the experimental results of predict-self filtration
418
Model Time (s) Accuracy (F1%)
SPD? CTB
Anno. Adapt. 4.39 97.67
Opt. Trans. 1.33 97.97
PD? CTB
Anno. Adapt. 4.76 98.15
Opt. Trans. 1.37 98.43
Previous Works
(Jiang et al2008) 97.85
(Kruengkrai et al2009) 97.87
(Zhang and Clark, 2010) 97.79
(Sun, 2011) 98.17
Table 6: The performance of the iterative annotation
transformation with predict-self reestimation compared
with annotation adaptation.
shows that, the filtration discards 5% of the train-
ing sentences and these discarded sentences contain
nearly 10% of training words. It can be confirmed
that the sentences discarded by predict-self filtra-
tion are much longer and more complicated. With a
properly tuned weight, predict-self reestimation can
make better use of the training data. The best F-
measure improvement achieved over the annotation
transformation baseline is 0.3 points, a little worse
than that brought by iterative training.
Figure 3 shows the performance curve of iterative
annotation transformation with predict-self reesti-
mation. We find that the predict-self reestimation
brings improvement to the iterative training at each
iteration. The maximum performance is achieved
at iteration 4. The corresponding model is evalu-
ated on the test set of CTB, table 6 shows the ex-
perimental results. Compared to annotation adapta-
tion, the optimized annotation transformation strat-
egy leads to a classifier with significantly higher ac-
curacy and several times faster processing. When
using the whole PD as the source corpus, the final
classifier 3 achieves an F-measure of 98.43%, sig-
nificantly outperforms previous works although us-
ing a single classifier with only local features. Of
course, the comparison between our system and pre-
vious works without using additional training data
is unfair. This work aim to find another way to im-
prove Chinese word segmentation, which focuses on
the collection of more training data instead of mak-
3The predict-self reestimation ratio ? is fixed after the first
training iteration for efficiency.
ing full use of a certain corpus. We believe that the
performance can be further improved by adopting
the advanced technologies of previous works, such
as complicated features and model combination.
Considering the fact that today some corpora for
word segmentation are really large (usually tens
of thousands of sentences), it is necessary to ob-
tain the latest CTB and investigate whether and
how much does annotation transformation bring im-
provement to a much higher baseline. On the other
hand, it is valuable to conduct experiments with
more source-annotated training data, such as the
SIGHAN dataset, to investigate the trend of im-
provement along with the increment of the addi-
tional annotated sentences. It is also valuable to
evaluate the improved word segmenter on the out-
of-domain datasets. However, currently most cor-
pora for Chinese word segmentation do not explic-
itly distinguish the domains of their data sections, it
makes such evaluations difficult to conduct.
6 Conclusion and Future Works
In this paper, we first describe an annotation trans-
formation algorithm to automatically transform a
human-annotated corpus from one annotation guide-
line to another. Then we propose two optimization
strategies, iterative training and predict-self reesti-
mation, to further improve the accuracy of anno-
tation guideline transformation. On Chinese word
segmentation, the optimized annotation transforma-
tion strategy leads to classifiers with obviously bet-
ter performance and several times faster processing
on the same datasets, compared to annotation adap-
tation. When adopting the whole PD as the source
corpus, the final classifier significantly outperforms
previous works on CTB 5.0, although using a single
classifier with only local features.
As future works, we will investigate the accel-
eration of the iterative training and the weight pa-
rameter tuning, and extend the optimized annotation
transformation strategy to joint Chinese word seg-
mentation and POS tagging, parsing and other NLP
tasks.
Acknowledgments
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004
419
and 61100082, and 863 State Key Project No.
2011AA01A207. We are grateful to the anonymous
reviewers for their thorough reviewing and valuable
suggestions.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
Aoife Cahill and Mairead Mccarthy. 2002. Automatic
annotation of the penn treebank with lfg f-structure in-
formation. In in Proceedings of the LREC Workshop.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of ccg and penn treebank parsers. In Pro-
ceedings of ACL-IJCNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL 2004.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of ACL.
Hal Daume? III. 2009. Unsupervised search-based struc-
tured prediction. In Proceedings of ICML.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceedings
of ACL.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with bve and mdl. In Pro-
ceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
a corpus of ccg derivations and dependency structures
extracted from the penn treebank. In Computational
Linguistics, volume 33(3), pages 355?396.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of NAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Junichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of ACL-IJCNLP.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chineseword segmenta-
tion. In Proceedings of ACL.
Haitao Mi, Deyi Xiong, and Qun Liu. 2008. Research
on strategy of integrating chinese lexical analysis and
parser. In Journal of Chinese Information Processing.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of ACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hy-
brid approach to word segmentation and pos tagging.
In Proceedings of ACL.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A
character-based joint model for chinese word segmen-
tation. In Proceedings of COLING.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan,
Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao,
and Weidong Zhan. 2001. Processing norms of mod-
ern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and pos-tagging using a sin-
gle discriminative model. In Proceedings of EMNLP.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
for word segmentation and named entity recognition.
In Proceedings of SIGHAN Workshop.
Muhua Zhu, Jingbo Zhu, and Minghan Hu. 2011. Better
automatic treebank conversion using a feature-based
approach. In Proceedings of ACL.
420
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066?1076,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Translation with Source Constituency and Dependency Trees
Fandong Meng?? Jun Xie? Linfeng Song?? Yajuan Lu?? Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?University of Chinese Academy of Sciences
{mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
We present a novel translation model, which
simultaneously exploits the constituency and
dependency trees on the source side, to com-
bine the advantages of two types of trees. We
take head-dependents relations of dependency
trees as backbone and incorporate phrasal n-
odes of constituency trees as the source side
of our translation rules, and the target side as
strings. Our rules hold the property of long
distance reorderings and the compatibility
with phrases. Large-scale experimental result-
s show that our model achieves significantly
improvements over the constituency-to-string
(+2.45 BLEU on average) and dependency-
to-string (+0.91 BLEU on average) model-
s, which only employ single type of trees,
and significantly outperforms the state-of-the-
art hierarchical phrase-based model (+1.12
BLEU on average), on three Chinese-English
NIST test sets.
1 Introduction
In recent years, syntax-based models have become a
hot topic in statistical machine translation. Accord-
ing to the linguistic structures, these models can be
broadly divided into two categories: constituency-
based models (Yamada and Knight, 2001; Graehl
and Knight, 2004; Liu et al, 2006; Huang et al,
2006), and dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Xiong
et al, 2007; Shen et al, 2008; Xie et al, 2011).
These two kinds of models have their own advan-
tages, as they capture different linguistic phenome-
na. Constituency trees describe how words and se-
quences of words combine to form constituents, and
constituency-based models show better compatibil-
ity with phrases. However, dependency trees de-
scribe the grammatical relation between words of
the sentence, and represent long distance dependen-
cies in a concise manner. Dependency-based mod-
els, such as dependency-to-string model (Xie et al,
2011), exhibit better capability of long distance re-
orderings.
In this paper, we propose to combine the advan-
tages of source side constituency and dependency
trees. Since the dependency tree is structurally sim-
pler and directly represents long distance depen-
dencies, we take dependency trees as the backbone
and incorporate constituents to them. Our mod-
el employs rules that represent the source side as
head-dependents relations which are incorporated
with constituency phrasal nodes, and the target side
as strings. A head-dependents relation (Xie et al,
2011) is composed of a head and all its dependents in
dependency trees, and it encodes phrase pattern and
sentence pattern (typically long distance reordering
relations). With the advantages of head-dependents
relations, the translation rules of our model hold the
property of long distance reorderings and the com-
patibility with phrases.
Our new model (Section 2) extracts rules from
word-aligned pairs of source trees (constituency
and dependency) and target strings (Section 3), and
translate source trees into target strings by employ-
ing a bottom-up chart-based algorithm (Section 4).
Compared with the constituency-to-string (Liu et al,
2006) and dependency-to-string (Xie et al, 2011)
models that only employ a single type of trees, our
1066
??/VV
???/NR ?/AD ???/NN
??/NR ?/M ??/JJ
??/OD
NP1
VP2
VP3
????? ? ????? ? ????
NR AD VV NR OD M JJ NN
NP1
CLP
QP
NP
NP
VP2
ADVP
VP3
NP
IP
(a)
(c)
Intel         will   launch  Asia     first              super     laptop
Chinese: ??? ? ?? ?? ?? ? ?? ???
English:  Intel will launch the first Ultrabook in Asia
ADVP NP
(b)
Figure 1: Illustration of phrases that can not be captured by a dependency tree (b) while captured by a constituency tree
(a), where the bold phrasal nodes NP1,VP2,VP3 indicate the phrases which can not be captured by dependency syn-
tactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing
the nodes with same phrasal categories.
approach yields encouraging results by exploiting t-
wo types of trees. Large-scale experiments (Sec-
tion 5) on Chinese-English translation show that
our model significantly outperforms the state-of-
the-art single constituency-to-string model by av-
eraged +2.45 BLEU points, dependency-to-string
model by averaged +0.91 BLEU points, and hierar-
chical phrase-based model (Chiang, 2005) by aver-
aged +1.12 BLEU points, on three Chinese-English
NIST test sets.
2 Grammar
We take head-dependents relations of dependency
trees as backbone and incorporate phrasal nodes of
constituency trees as the source side of our transla-
tion rules, and the target side as strings. A head-
dependents relation consists of a head and all its de-
pendents in dependency trees, and it can represent
long distance dependencies. Incorporating phrasal
nodes of constituency trees into head-dependents
relations further enhances the compatibility with
phrases of our rules. Figure 1 shows an example of
phrases which can not be captured by a dependen-
cy tree while captured by a constituency tree, such
as the bold phrasal nodes NP1,VP2 and VP3. The
phrasal node NP1 in the constituency tree indicates
that ??? )P? is a noun phrase and it should
be translated as a basic unit, while in the depen-
dency tree it is a non-syntactic phrase. The head-
dependents relation in the top level of the dependen-
cy tree presents long distance dependencies of the
words ?=A?, ???, ????, and ?)P? in a
concise manner, which is useful for long distance re-
ordering. We adopt this kind of rule representation
to hold the property of long distance reorderings and
the compatibility with phrases.
Figure 2 shows two examples of our translation
rules corresponding to the top level of Figure 1-(b).
We can see that r1 captures a head-dependents rela-
tion, while r2 extends r1 by incorporating a phrasal
node VP2 to replace the two nodes ???/VV? and
?)P/NN?. As shown in Figure 1-(b), VP2 con-
sists of two parts, a head node ???/VV? and a
subtree rooted at the dependent node ?)P/NN?.
Therefore, we use VP2 and the POS tags of the t-
wo nodes VV and NN to denote the part covered
by VP2 in r2, to indicate that the source sequence
covered by VP2 can be translated by a bilingual
phrase. Since VP2 covers a head node ???/VV?,
we represent r2 by constructing a new head node
1067
1
??
??? ? 1
1
2
1 2
??? ? 1
Figure 2: Two examples of our translation rules corre-
sponding to the top level of Figure 1-(b). r1 captures a
head-dependents relation, and r2 extends r1 by incorpo-
rating a phrasal node VP2. ?x1:NN? indicates a substitu-
tion site which can be replaced by a subtree whose root
has POS tag ?NN?. ?x1:VP2|||VV NN? indicates a sub-
stitution site which can be replaced by a source phrase
covered by a phrasal node VP (the phrasal node consist-
s of two dependency nodes with POS tag VV and NN,
respectively). The underline denotes a leaf node.
VP2|||VV NN. For simplicity, we use a shorten for-
m CHDR to represent the head-dependents relations
with/without constituency phrasal nodes.
Formally, our grammar G is defined as a 5-tuple
G = ??, Nc, Nd,?, R?, where ? is a set of source
language terminals, Nc is a set of constituency
phrasal categories, Nd is a set of categories (POS
tags) for the terminals in ?, ? is a set of target lan-
guage terminals, and R is a set of translation rules
that include bilingual phrases for translating source
language terminals and CHDR rules for translation
and reordering. A CHDR rule is represented as a
triple ?t, s,??, where:
? t is CHDR with each node labeled by a ter-
minal from ? or a variable from a set X =
{x1, x2, ? ? ? } constrained by a terminal from ?
or a category from Nd or a joint category (con-
structed by the categories from Nc and Nd);
? s ? (X ??) denotes the target side string;
? ? denotes one-to-one links between nontermi-
nals in t and variables in s.
We use the lexicon dependency grammar (Hellwig,
2006) which adopts a bracket representation to ex-
press the head-dependents relation and CHDR. For
example, the left-hand sides of r1 and r2 in Figure 2
can be respectively represented as follows:
(=A) (?)?? (x1:NN)
(=A) (?) x1:VP2|||VV NN
??/VV
???/NR ?/AD ???/NN
??/NR?/M ??/JJ
??/OD
NP1
VP2
VP3
??? ? ?? ?? ?? ? ?? ???
Parseing      Labelling
???/NR ?/AD launch
Intel will launch ????? in Asia
Intel will launch in Asia
(a)
(b)
(c)
(d)
(e)
NP1
?/M
??/OD
Intel will launch in Asiathe    first(f)
Ultrabook
Ultrabook
???/NN
??/NR?/M ??/JJ
??/OD
NP1
r3
r4 r5
r6
r7
?/M
??/OD
r8
(x1:NR) (x2:AD) ?? (x3:???) x1 x2 launch x3
Intel???
? will
(??)(x1:M)x2:NP1|||JJ_NN x1 x2 in Aisa 
????? Ultrabook
?? (?) the first
Translation Rules
r3
r4
r5
r6
r7
r8
(g)
Figure 3: An example derivation of translation. (g) lists
all the translation rules. r3, r6 and r8 are CHDR rules,
while r4, r5 and r7 are bilingual phrases, which are used
for translating source terminals. The dash lines indicate
the reordering when employing a translation rule.
The formalized presentation of r2 in Figure 2-(b):
t = (=A) (?) x1:VP2|||VV NN
s = Intel will x1
?= x1:VP2|||VV NN ? x1
where the underline indicates a leaf node.
Figure 3 gives an example of the translation
derivation in our model, with the translation rules
1068
listed in (g). r3, r6 and r8 are CHDR rules, while
r4, r5 and r7 are bilingual phrases, which are used
for translating source language terminals. Given a
sentence to translate in (a), we first parse it into a
constituency tree and a dependency tree, then label
the phrasal nodes from the constituency tree to the
dependency tree, and yield (b). Then, we translate
it into a target string by the following steps. At the
root node, we apply rule r3 to translate the top level
head-dependents relation and results in four unfin-
ished substructures and target strings in (c). From
(c) to (d), there are three steps (one rule for one step).
We use r4 to translate ?=A? to ?Intel?, r5 to
translate ??? to ?will?, and r6 to translate the right-
most unfinished part. Then, we apply r7 to translate
the phrase ???)P? to ?Ultrabook?, and yield
(e). Finally, we apply r8 to translate the last frag-
ment to ?the first?, and get the final result (f).
3 Rule Extraction
In this section, we describe how to extract rules from
a set of 4-tuples ?C, T, S,A?, where C is a source
constituency tree, T is a source dependency tree, S
is a target side sentence, and A is an word alignmen-
t relation between T /C and S. We extract CHDR
rules from each 4-tuple ?C, T, S,A? based on GHK-
M algorithm (Galley et al, 2004) with three steps:
1. Label the dependency tree with phrasal nodes
from the constituency tree, and annotate align-
ment information to the phrasal nodes labeled
dependency tree (Section 3.1).
2. Identify acceptable CHDR fragments from the
annotated dependency tree for rule induction
(Section 3.2).
3. Induce a set of lexicalized and generalized
CHDR rules from the acceptable fragments
(Section 3.3).
3.1 Annotation
Given a 4-tuple ?C, T, S,A?, we first label phrasal
nodes from the constituency tree C to the depen-
dency tree T , which can be easily accomplished by
phrases mapping according to the common covered
source sequences. As dependency trees can capture
some phrasal information by dependency syntactic
??/VV
{3-3}{1-8}
???/NR
{1-1}{1-1}
?/AD
{2-2}{2-2}
???/NN
{6-6}{4-8}
??/NR
{7-8}{7-8}
?/M
{null}{4-5}
??/JJ
{6-6}{6-6}
??/OD
{4-5}{4-5}
NP1
<6-6>
VP2
<3-8>
VP3
<2-8>
Figure 4: An annotated dependency tree. Each node is
annotated with two spans, the former is node span and
the latter subtree span. The fragments covered by phrasal
nodes are annotated with phrasal spans. The nodes de-
noted by the solid line box are not nsp consistent.
phrases, in order to complement the information that
dependency trees can not capture, we only label the
phrasal nodes that cover dependency non-syntactic
phrases.
Then, we annotate alignment information to the
phrasal nodes labeled dependency tree T , as shown
in Figure 4. For description convenience, we make
use of the notion of spans (Fox, 2002; Lin, 2004).
Given a node n in the source phrasal nodes labeled
T with word alignment information, the spans of n
induced by the word alignment are consecutive se-
quences of words in the target sentence. As shown
in Figure 4, we annotate each node n of phrasal n-
odes labeled T with two attributes: node span and
subtree span; besides, we annotate phrasal span to
the parts covered by phrasal nodes in each subtree
rooted at n. The three types of spans are defined as
follows:
Definition 1 Given a node n, its node span nsp(n)
is the consecutive target word sequence aligned with
the node n.
Take the node ???/NR? in Figure 4 for example,
nsp(??/NR)={7-8}, which corresponds to the tar-
get words ?in? and ?Asia?.
Definition 2 Given a subtree T ? rooted at n, the
subtree span tsp(n) of n is the consecutive target
word sequence from the lower bound of the nsp of
1069
all nodes in T ? to the upper bound of the same set of
spans.
For instance, tsp()P/NN)={4-8}, which corre-
sponds to the target words ?the first Ultrabook in A-
sia?, whose indexes are from 4 to 8.
Definition 3 Given a fragment f covered by a
phrasal node, the phrasal span psp(f) of f is
the consecutive target word sequence aligned with
source string covered by f .
For example, psp(VP2)=?3-8?, which corresponds
to the target word sequence ?launch the first Ultra-
book in Asia?.
We say nsp, tsp and psp are consistent according
to the notion in the phrase-based model (Koehn et
al., 2003). For example, nsp(??/NR), tsp()P
/NN) and psp(NP1) are consistent while nsp(?
?/JJ) and nsp()P/NN) are not consistent.
The annotation can be achieved by a single pos-
torder transversal of the phrasal nodes labeled de-
pendency tree. For simplicity, we call the annotat-
ed phrasal nodes labeled dependency tree annotated
dependency tree. The extraction of bilingual phrases
(including the translation of head node, dependen-
cy syntactic phrases and the fragment covered by a
phrasal node) can be readily achieved by the algo-
rithm described in Koehn et al, (2003). In the fol-
lowing, we focus on CHDR rules extraction.
3.2 Acceptable Fragments Identification
Before present the method of acceptable fragments
identification, we give a brief description of CHDR
fragments. A CHDR fragment is an annotated frag-
ment that consists of a source head-dependents rela-
tion with/without constituency phrasal nodes, a tar-
get string and the word alignment information be-
tween the source and target side. We identify the ac-
ceptable CHDR fragments that are suitable for rule
induction from the annotated dependency tree. We
divide the acceptable CHDR fragments into two cat-
egories depending on whether the fragments con-
tain phrasal nodes. If an acceptable CHDR frag-
ment does not contain phrasal nodes, we call it
CHDR-normal fragment, otherwise CHDR-phrasal
fragment. Given a CHDR fragment F rooted at n,
we say F is acceptable if it satisfies any one of the
following properties:
CHDR-phrasal Rules
r9: (???)(?)x1:VP2|||VV_NN Intel will x1
r10: (x1:NR)(x2:AD)x3:VP2|||VV_NN x1 x2 x3
r11: (???)x1:VP3|||AD_VV_NN Intel x1
r12: (x1:NR)x2:VP3|||AD_VV_NN x1 x2
CHDR-normal Rules
r4: (x1:NR) (x2:AD) ?? (x3:NN) x1 x2 launch x3
Intel will launch x1r3: (???) (?)?? (x1:NN)
r2: (x1:NR) (x2:AD) ?? (x3:???) x1 x2 launch x3
r1: (???) (?)?? (x1:???) Intel will launch x1
r5: (???) (?) x1:VV (x2:???) Intel will x1 x2
r8: (x1:NR) (x2:AD) x3:VV (x4:NN) x1 x2 x3 x4
r6: (x1:NR) (x2:AD) x3:VV (x4:???) x1 x2 x3 x4
Intel will x1 x2r7: (???) (?) x1:VV (x2:NN)
(d)
??/VV
???/NR ?/AD ???/NN
Intel
1
will
2
launch
3
the first Ultrabook in Asia
4-8
(a)
Intel
1
will
2
launch the first Ultrabook in Asia
3-8
VP2
??/VV
???/NR ?/AD ???/NN(b)
(c)
Intel
1
will launch the first Ultrabook in Asia
2-8
VP3
??/VV
???/NR ?/AD ???/NN
VP2|||VV_NN
VP3|||AD_VV_NN
Figure 5: Examples of a CHDR-normal fragment (a), two
CHDR-phrasal fragments (b) and (c) that are identified
from the top level of the annotated dependency tree in
Figure 4, and the corresponding CHDR rules (d) induced
from (a), (b) and (c). The underline denotes a leaf node.
1. Without phrasal nodes, the node span of the
root n is consistent and the subtree spans of
n?s all dependents are consistent. For example,
Figure 5-(a) shows a CHDR-normal fragmen-
t that identified from the top level of the an-
notated dependency tree in Figure 4, since the
nsp(??/VV), tsp(=A/NR), tsp(?/AD)
and tsp()P/NN) are consistent.
1070
2. With phrasal nodes, the phrasal spans of
phrasal nodes are consistent; and for the other
nodes, the node span of head (if it is not cov-
ered by any phrasal node) is consistent, and the
subtree spans of dependents are consistent. For
instance, Figure 5-(b) and (c) show two CHDR-
phrasal fragments identified from the top level
of Figure 4. In Figure 5-(b), psp(VP2), tsp(=
A/NR) and tsp(?/AD) are consistent. In
Figure 5-(c), psp(VP3) and tsp(=A/NR)
are consistent.
The identification of acceptable fragments can be
achieved by a single postorder transversal of the an-
notated dependency tree. Typically, each acceptable
fragment contains at most three types of nodes: head
node, head of the related CHDR; internal nodes, in-
ternal nodes of the related CHDR except head node;
leaf nodes, leaf nodes of the related CHDR.
3.3 Rule Induction
From each acceptable CHDR fragment, we induce
a set of lexicalized and generalized CHDR rules.
We induce CHDR-normal rules and CHDR-phrasal
rules from CHDR-normal fragments and CHDR-
phrasal fragments, respectively.
We first induce a lexicalized form of CHDR rule
from an acceptable CHDR fragment:
1. For a CHDR-normal fragment, we first mark
the internal nodes as substitution sites. This
forms the input of a CHDR-normal rule. Then
we generate the target string according to the
node span of the head and the subtree spans of
the dependents, and turn the word sequences
covered by the internal nodes into variables.
This forms the output of a lexicalized CHDR-
normal rule.
2. For a CHDR-phrasal fragment, we first mark
the internal nodes and the phrasal nodes as sub-
stitution sites. This forms the input of a CHDR-
phrasal rule. Then we construct the output of
the CHDR-phrasal rule in almost the same way
with constructing CHDR-normal rules, except
that we replace the target sequences covered by
the internal nodes and the phrasal nodes with
variables.
For example, rule r1 in Figure 5-(d) is a lexicalized
CHDR-normal rule induced from the CHDR-normal
fragment in Figure 5-(a). r9 and r11 are CHDR-
phrasal rules induced from the CHDR-phrasal frag-
ment in Figure 5-(b) and Figure 5-(c) respectively.
As we can see, these CHDR-phrasal rules are par-
tially unlexicalized.
To alleviate the sparseness problem, we gener-
alize the lexicalized CHDR-normal rules and par-
tially unlexicalized CHDR-phrasal rules with un-
lexicalized nodes by the method proposed in Xie
et al, (2011). As the modification relations be-
tween head and dependents are determined by the
edges, we can replace the lexical word of each n-
ode with its category (POS tag) and obtain new
head-dependents relations with unlexicalized nodes
keeping the same modification relations. We gen-
eralize the rule by simultaneously turn the nodes of
the same type (head, internal, leaf) into their cate-
gories. For example, CHDR-normal rules r2 ? r7
are generalized from r1 in Figure 5-(d). Besides, r10
and r12 are the corresponding generalized CHDR-
phrasal rules. Actually, our CHDR rules are the su-
perset of head-dependents relation rules in Xie et
al., (2011). CHDR-normal rules are equivalent with
the head-dependents relation rules and the CHDR-
phrasal rules are the extension of these rules. For
convenience of description, we use the subscript to
distinguish the phrasal nodes with the same catego-
ry, such as VP2 and VP3. In actual operation, we use
VP instead of VP2 and VP3.
We handle the unaligned words of the target side
by extending the node spans of the lexicalized head
and leaf nodes, and the subtree spans of the lexical-
ized dependents, on both left and right directions.
This procedure is similar with the method of Och
and Ney, (2004). During this process, we might ob-
tain m(m ? 1) CHDR rules from an acceptable
fragment. Each of these rules is assigned with a frac-
tional count 1/m. We take the extracted rule set as
observed data and make use of relative frequency es-
timator to obtain the translation probabilities P (t|s)
and P (s|t).
4 Decoding and the Model
Following Och and Ney, (2002), we adopt a general
loglinear model. Let d be a derivation that convert a
1071
source phrasal nodes labeled dependency tree into a
target string e. The probability of d is defined as:
P (d) ?
?
i
?i(d)?i (1)
where ?i are features defined on derivations and ?i
are feature weights. In our experiments of this paper,
the features are used as follows:
? CHDR rules translation probabilities P (t|s)
and P (s|t), and CHDR rules lexical translation
probabilities Plex(t|s) and Plex(s|t);
? bilingual phrases translation probabilities
Pbp(t|s) and Pbp(s|t), and bilingual phrases
lexical translation probabilities Pbplex(t|s) and
Pbplex(s|t);
? rule penalty exp(?1);
? pseudo translation rule penalty exp(?1);
? target word penalty exp(|e|);
? language model Plm(e).
We have twelve features in our model. The values of
the first four features are accumulated on the CHDR
rules and the next four features are accumulated on
the bilingual phrases. We also use a pseudo transla-
tion rule (constructed according to the word order of
head-dependents relation) as a feature to guarantee
the complete translation when no matched rules can
be found during decoding.
Our decoder is based on bottom-up chart-based
algorithm. It finds the best derivation that convert
the input phrasal nodes labeled dependency tree into
a target string among all possible derivations. Giv-
en the source constituency tree and dependency tree,
we first generate phrasal nodes labeled dependency
tree T as described in Section 3.1, then the decoder
transverses each node in T by postorder. For each
node n, it enumerates all instances of CHDR rooted
at n, and checks the rule set for matched translation
rules. A larger translation is generated by substitut-
ing the variables in the target side of a translation
rule with the translations of the corresponding de-
pendents. Cube pruning (Chiang, 2007; Huang and
Chiang, 2007) is used to find the k-best items with
integrated language model for each node.
To balance the performance and speed of the de-
coder, we limit the search space by reducing the
number of translation rules used for each node.
There are two ways to limit the rule table size: by
a fixed limit (rule-limit) of how many rules are re-
trieved for each input node, and by a threshold (rule-
threshold) to specify that the rule with a score low-
er than ? times of the best score should be discard-
ed. On the other hand, instead of keeping the full
list of candidates for a given node, we keep a top-
scoring subset of the candidates. This can also be
done by a fixed limit (stack-limit) and a threshold
(stack-threshold).
5 Experiments
We evaluated the performance of our model by com-
paring with hierarchical phrase-based model (Chi-
ang, 2007), constituency-to-string model (Liu et al,
2006) and dependency-to-string model (Xie et al,
2011) on Chinese-English translation. First, we de-
scribe data preparation (Section 5.1) and systems
(Section 5.2). Then, we validate that our model sig-
nificantly outperforms all the other baseline models
(Section 5.3). Finally, we give detail analysis (Sec-
tion 5.4).
5.1 Data Preparation
Our training data consists of 1.25M sentence pairs
extracted from LDC 1 data. We choose NIST MT
Evaluation test set 2002 as our development set,
NIST MT Evaluation test sets 2003 (MT03), 2004
(MT04) and 2005 (MT05) as our test sets. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric 2.
We parse the source sentences to constituency
trees (without binarization) and projective depen-
dency trees with Stanford Parser (Klein and Man-
ning, 2002). The word alignments are obtained by
running GIZA++ (Och and Ney, 2003) on the corpus
in both directions and using the ?grow-diag-final-
and? balance strategy (Koehn et al, 2003). We get
bilingual phrases from word-aligned data with algo-
rithm described in Koehn et al (2003) by running
Moses Toolkit 3. We apply SRI Language Modeling
Toolkit (Stolcke and others, 2002) to train a 4-gram
1Including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and LD-
C2005T06.
2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
3http://www.statmt.org/moses/
1072
System Rule # MT03 MT04 MT05 Average
Moses-chart 116.4M 34.65 36.47 34.39 35.17
cons2str 25.4M+32.5M 33.14 35.12 33.27 33.84
dep2str 19.6M+32.5M 34.85 36.57 34.72 35.38
consdep2str 23.3M+32.5M 35.57* 37.68* 35.62* 36.29
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.
The ?+? denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The ?*?
denotes that the results are significantly better than all the other systems (p<0.01).
language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the English Gigaword
corpus. We make use of the standard MERT (Och,
2003) to tune the feature weights in order to maxi-
mize the system?s BLEU score on the development
set. The statistical significance test is performed by
sign-test (Collins et al, 2005).
5.2 Systems
We take the open source hierarchical phrase-based
system Moses-chart (with default configuration),
our in-house constituency-to-string system cons2str
and dependency-to-string system dep2str as our
baseline systems.
For cons2str, we follow Liu et al, (Liu et al,
2006) to strict that the height of a rule tree is no
greater than 3 and phrase length is no greater than
7. To keep consistent with our proposed model,
we implement the dependency-to-string model (X-
ie et al, 2011) with GHKM (Galley et al, 2004)
rule extraction algorithm and utilize bilingual phras-
es to translate source head node and dependency
syntactic phrases. Our dep2str shows comparable
performance with Xie et al, (2011), which can be
seen by comparing with the results of hierarchical
phrase-based model in our experiments. For dep2str
and our proposed model consdep2str, we set rule-
threshold and stack-threshold to 10?3, rule-limit to
100, stack-limit to 300, and phrase length limit to 7.
5.3 Experimental Results
Table 1 illustrates the translation results of our ex-
periments. As we can see, our consdep2str sys-
tem has gained the best results on all test sets, with
+1.12 BLEU points higher than Moses-chart, +2.45
BLEU points higher than cons2str, and +0.91 BLEU
points higher than dep2str, averagely on MT03,
MT04 and MT05. Our model significantly outper-
forms all the other baseline models, with p<0.01
on statistical significance test sign-test (Collins et
al., 2005). By exploiting two types of trees on
source side, our model gains significant improve-
ments over constituency-to-string and dependency-
to-string models, which employ single type of trees.
Table 1 also lists the statistical results of rules ex-
tracted from training data by different systems. Ac-
cording to our statistics, the number of rules extract-
ed by our consdep2str system is about 18.88% larger
than dep2str, without regard to the 32.5M bilingual
phrases. The extra rules are CHDR-phrasal rules,
which can bring in BLEU improvements by enhanc-
ing the compatibility with phrases. We will conduct
a deep analysis in the next sub-section.
5.4 Analysis
In this section, we first illustrate the influence of
CHDR-phrasal rules in our consdep2str model. We
calculate the proportion of 1-best translations in test
sets that employ CHDR-phrasal rules, and we cal-
l this proportion ?CHDR-phrasal Sent.?. Besides,
the proportion of CHDR-phrasal rules in all CHDR
rules is calculated in these translations, and we cal-
l this proportion ?CHDR-phrasal Rule?. Table 2
lists the using of CHDR-phrasal rules on test sets,
showing that CHDR-phrasal Sent. on all test sets
are higher than 50%, and CHDR-phrasal Rule on al-
l three test sets are higher than 10%. These results
indicate that CHDR-phrasal rules do play a role in
decoding.
Furthermore, we compare some actual transla-
tions of our test sets generated by cons2str, de-
p2str and consdep2str systems, as shown in Fig-
ure 6. In the first example, the Chinese input hold-
s long distance dependencies ???I ?? ?
... \u ... L? '??, which correspond
to the sentence pattern ?noun+adverb+prepositional
1073
System MT03 MT04 MT05
CHDR-phrasal Sent. 50.71 61.80 56.19
CHDR-phrasal Rule 10.53 13.55 10.83
Table 2: The proportion (%) of 1-best translations that
employs CHDR-phrasal rules (CHDR-phrasal Sent.) and
the proportion (%) of CHDR-phrasal rules in all CHDR
rules in these translations (CHDR-phrasal Rule).
phrase+verb+noun?. Cons2str gives a bad result
with wrong global reordering, while our consdep2str
system gains an almost correct result since we cap-
ture this pattern by CHDR-normal rules. In the sec-
ond example, we can see that the Chinese phrase
?2g?y? is a non-syntactic phrase in the depen-
dency tree, and this phrase can not be captured by
head-dependents relation rules in Xie et al, (2011),
thus can not be translated as one unit. Since we en-
code constituency phrasal nodes to the dependency
tree, ?2g?y? is labeled by a phrasal node ?VP?
(means verb phrase), which can be captured by our
CHDR-phrasal rules and translated into the correct
result ?reemergence? with bilingual phrases.
By combining the merits of constituency and
dependency trees, our consdep2str model learns
CHDR-normal rules to acquire the property of long
distance reorderings and CHDR-phrasal rules to ob-
tain good compatibility with phrases.
6 Related Work
In recent years, syntax-based models have witnessed
promising improvements. Some researchers make
efforts on constituency-based models (Graehl and
Knight, 2004; Liu et al, 2006; Huang et al, 2006;
Zhang et al, 2007; Mi et al, 2008; Liu et al, 2009;
Liu et al, 2011; Zhai et al, 2012). Some works pay
attention to dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Xiong et
al., 2007; Shen et al, 2008; Xie et al, 2011). These
models are based on single type of trees.
There are also some approaches combining mer-
its of different structures. Marton and Resnik (2008)
took the source constituency tree into account and
added soft constraints to the hierarchical phrase-
based model (Chiang, 2005). Cherry (2008) u-
tilized dependency tree to add syntactic cohesion
to the phrased-based model. Mi and Liu, (2010)
proposed a constituency-to-dependency translation
model, which utilizes constituency forests on the
source side to direct the translation, and depen-
dency trees on the target side to ensure grammati-
cality. Feng et al (2012) presented a hierarchical
chunk-to-string translation model, which is a com-
promise between the hierarchical phrase-based mod-
el and the constituency-to-string model. Most work-
s make effort to introduce linguistic knowledge in-
to the phrase-based model and hierarchical phrase-
based model with constituency trees. Only the work
proposed by Mi and Liu, (2010) utilized constituen-
cy and dependency trees, while their work applied
two types of trees on two sides.
Instead, our model simultaneously utilizes con-
stituency and dependency trees on the source side to
direct the translation, which is concerned with com-
bining the advantages of two types of trees in trans-
lation rules to advance the state-of-the-art machine
translation.
7 Conclusion
In this paper, we present a novel model that si-
multaneously utilizes constituency and dependency
trees on the source side to direct the translation. To
combine the merits of constituency and dependen-
cy trees, our model employs head-dependents rela-
tions incorporating with constituency phrasal nodes.
Experimental results show that our model exhibits
good performance and significantly outperforms the
state-of-the-art constituency-to-string, dependency-
to-string and hierarchical phrase-based models. For
the first time, source side constituency and depen-
dency trees are simultaneously utilized to direct the
translation, and the model surpasses the state-of-the-
art translation models.
Since constituency tree binarization can lead
to more constituency-to-string rules and syntactic
phrases in rule extraction and decoding, which im-
prove the performance of constituency-to-string sys-
tems, for future work, we would like to do research
on encoding binarized constituency trees to depen-
dency trees to improve translation performance.
Acknowledgments
The authors were supported by National Natural Sci-
ence Foundation of China (Contracts 61202216),
1074
MT05 ---- segment 448
??? ?? ? ?? ?? ??? ?? ?? ? ?? ?? ???
cons2srt: united nations with the indonesian government have expressed concern over the time limit for foreign troops .
consdep2srt: the united nations has expressed concern over the deadline of the indonesian government on foreign troops .
reference: The United Nations has expressed concern over the deadline the Indonesian government imposed on foreign troops.
??? ?? ?? ?? ??? ?? ?? ? ??? ?? ?? ?
dobjpobj
prep
advmod
nsubj
pnuct
the united nations has the deadline of the indonesian government on foreign troopsexpressed concern over .
?? ?? ?? ? ?? ?? ??? ??? 6$56?? ??
dep2srt: ?? again severe acute respiratory syndrome ( SARS ) case ??
consdep2srt: ?? reemergence of a severe acute respiratory syndrome ( SARS ) case??
reference: ?? the reemergence of a severe acute respiratory syndrome (SARS) case ??
MT04 ---- segment 194
dep cons & dep
??/VV
??/AD ?/DEG
VP
reemergence
???/NN
??/JJ ??/JJ??/VV
??/AD ?/DEG
again
???/NN
??/JJ ??/JJ
Figure 6: Actual examples translated by the cons2str, dep2str and consdep2str systems.
863 State Key Project (No. 2011AA01A207),
and National Key Technology R&D Program (No.
2012BAH39B03), Key Project of Knowledge Inno-
vation Program of Chinese Academy of Sciences
(No. KGZD-EW-501). Qun Liu.s work was
partially supported by Science Foundation Ireland
(Grant No. 07/CE/I1142) as part of the CNGL
at Dublin City University. Sincere thanks to the
anonymous reviewers for their thorough reviewing
and valuable suggestions. We appreciate Haitao Mi,
Zhaopeng Tu and Anbang Zhao for insightful ad-
vices in writing.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In ACL, pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531?540.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 541?548.
Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou, and
Qun Liu. 2012. Hierarchical chunk-to-string transla-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers-Volume 1, pages 950?958.
Heidi J Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing-Volume 10, pages 304?3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule. In Pro-
1075
ceedings of HLT/NAACL, volume 4, pages 273?280.
Boston.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. An International Handbook of Contemporary
Research, 2:1081?1109.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Annual Meeting-Association For Computational Lin-
guistics, volume 45, pages 144?151.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. S-
tatistical syntax-directed translation with extended do-
main of locality. In Proceedings of AMTA, pages 66?
73.
Dan Klein and Christopher D Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In Advances in neural information processing
systems, volume 15, pages 3?10.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48?54.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, pages
625?630.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Join-
t Conference on Natural Language Processing of the
AFNLP: Volume 2-Volume 2, pages 558?566.
Yang Liu, Qun Liu, and Yajuan Lu?. 2011. Adjoining
tree-to-string translation. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1278?1287.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-08: HLT, pages 1003?1011.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1433?1442.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for s-
tatistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computation-
al Linguistics, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignmen-
t template approach to statistical machine translation.
Computational linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computation-
al Linguistics-Volume 1, pages 160?167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Andreas Stolcke et al 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A nov-
el dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 216?226.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A de-
pendency treelet string correspondence model for s-
tatistical machine translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 40?47.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting on Association for Computation-
al Linguistics, pages 523?530.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2012. Tree-based translation without using
parse trees. In Proceedings of COLING 2012, pages
3037?3054.
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. MT-
Summit-07, pages 535?542.
1076
Proceedings of the ACL 2010 Conference Short Papers, pages 12?16,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Lexicalized Reordering Models from Reordering Graphs
Jinsong Su, Yang Liu, Yajuan Lu?, Haitao Mi, Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{sujinsong,yliu,lvyajuan,htmi,liuqun}@ict.ac.cn
Abstract
Lexicalized reordering models play a crucial
role in phrase-based translation systems. They
are usually learned from the word-aligned
bilingual corpus by examining the reordering
relations of adjacent phrases. Instead of just
checking whether there is one phrase adjacent
to a given phrase, we argue that it is important
to take the number of adjacent phrases into
account for better estimations of reordering
models. We propose to use a structure named
reordering graph, which represents all phrase
segmentations of a sentence pair, to learn lex-
icalized reordering models efficiently. Exper-
imental results on the NIST Chinese-English
test sets show that our approach significantly
outperforms the baseline method.
1 Introduction
Phrase-based translation systems (Koehn et al,
2003; Och and Ney, 2004) prove to be the state-
of-the-art as they have delivered translation perfor-
mance in recent machine translation evaluations.
While excelling at memorizing local translation and
reordering, phrase-based systems have difficulties in
modeling permutations among phrases. As a result,
it is important to develop effective reordering mod-
els to capture such non-local reordering.
The early phrase-based paradigm (Koehn et al,
2003) applies a simple distance-based distortion
penalty to model the phrase movements. More re-
cently, many researchers have presented lexicalized
reordering models that take advantage of lexical
information to predict reordering (Tillmann, 2004;
Xiong et al, 2006; Zens and Ney, 2006; Koehn et
Figure 1: Occurrence of a swap with different numbers
of adjacent bilingual phrases: only one phrase in (a) and
three phrases in (b). Black squares denote word align-
ments and gray rectangles denote bilingual phrases. [s,t]
indicates the target-side span of bilingual phrase bp and
[u,v] represents the source-side span of bilingual phrase
bp.
al., 2007; Galley and Manning, 2008). These mod-
els are learned from a word-aligned corpus to pre-
dict three orientations of a phrase pair with respect
to the previous bilingual phrase: monotone (M ),
swap (S), and discontinuous (D). Take the bilingual
phrase bp in Figure 1(a) for example. The word-
based reordering model (Koehn et al, 2007) ana-
lyzes the word alignments at positions (s?1, u?1)
and (s ? 1, v + 1). The orientation of bp is set
to D because the position (s ? 1, v + 1) contains
no word alignment. The phrase-based reordering
model (Tillmann, 2004) determines the presence
of the adjacent bilingual phrase located in position
(s? 1, v+1) and then treats the orientation of bp as
S. Given no constraint on maximum phrase length,
the hierarchical phrase reordering model (Galley and
Manning, 2008) also analyzes the adjacent bilingual
phrases for bp and identifies its orientation as S.
However, given a bilingual phrase, the above-
mentioned models just consider the presence of an
adjacent bilingual phrase rather than the number of
adjacent bilingual phrases. See the examples in Fig-
12
Figure 2: (a) A parallel Chinese-English sentence pair and (b) its corresponding reordering graph. In (b), we denote
each bilingual phrase with a rectangle, where the upper and bottom numbers in the brackets represent the source
and target spans of this bilingual phrase respectively. M = monotone (solid lines), S = swap (dotted line), and D =
discontinuous (segmented lines). The bilingual phrases marked in the gray constitute a reordering example.
ure 1 for illustration. In Figure 1(a), bp is in a swap
order with only one bilingual phrase. In Figure 1(b),
bp swaps with three bilingual phrases. Lexicalized
reordering models do not distinguish different num-
bers of adjacent phrase pairs, and just give bp the
same count in the swap orientation.
In this paper, we propose a novel method to better
estimate the reordering probabilities with the con-
sideration of varying numbers of adjacent bilingual
phrases. Our method uses reordering graphs to rep-
resent all phrase segmentations of parallel sentence
pairs, and then gets the fractional counts of bilin-
gual phrases for orientations from reordering graphs
in an inside-outside fashion. Experimental results
indicate that our method achieves significant im-
provements over the traditional lexicalized reorder-
ing model (Koehn et al, 2007).
This paper is organized as follows: in Section 2,
we first give a brief introduction to the traditional
lexicalized reordering model. Then we introduce
our method to estimate the reordering probabilities
from reordering graphs. The experimental results
are reported in Section 3. Finally, we end with a
conclusion and future work in Section 4.
2 Estimation of Reordering Probabilities
Based on Reordering Graph
In this section, we first describe the traditional lexi-
calized reordering model, and then illustrate how to
construct reordering graphs to estimate the reorder-
ing probabilities.
2.1 Lexicalized Reordering Model
Given a phrase pair bp = (ei, fai), where ai de-
fines that the source phrase fai is aligned to the
target phrase ei, the traditional lexicalized reorder-
ing model computes the reordering count of bp in
the orientation o based on the word alignments of
boundary words. Specifically, the model collects
bilingual phrases and distinguishes their orientations
with respect to the previous bilingual phrase into
three categories:
o =
?
??
??
M ai ? ai?1 = 1
S ai ? ai?1 = ?1
D |ai ? ai?1| 6= 1
(1)
Using the relative-frequency approach, the re-
ordering probability regarding bp is
p(o|bp) = Count(o, bp)?
o? Count(o?, bp)
(2)
2.2 Reordering Graph
For a parallel sentence pair, its reordering graph in-
dicates all possible translation derivations consisting
of the extracted bilingual phrases. To construct a
reordering graph, we first extract bilingual phrases
using the way of (Och, 2003). Then, the adjacent
13
bilingual phrases are linked according to the target-
side order. Some bilingual phrases, which have
no adjacent bilingual phrases because of maximum
length limitation, are linked to the nearest bilingual
phrases in the target-side order.
Shown in Figure 2(b), the reordering graph for
the parallel sentence pair (Figure 2(a)) can be rep-
resented as an undirected graph, where each rect-
angle corresponds to a phrase pair, each link is the
orientation relationship between adjacent bilingual
phrases, and two distinguished rectangles bs and be
indicate the beginning and ending of the parallel sen-
tence pair, respectively. With the reordering graph,
we can obtain all reordering examples containing
the given bilingual phrase. For example, the bilin-
gual phrase ?zhengshi huitan, formal meetings? (see
Figure 2(a)), corresponding to the rectangle labeled
with the source span [6,7] and the target span [4,5],
is in a monotone order with one previous phrase
and in a discontinuous order with two subsequent
phrases (see Figure 2(b)).
2.3 Estimation of Reordering Probabilities
We estimate the reordering probabilities from re-
ordering graphs. Given a parallel sentence pair,
there are many translation derivations correspond-
ing to different paths in its reordering graph. As-
suming all derivations have a uniform probability,
the fractional counts of bilingual phrases for orien-
tations can be calculated by utilizing an algorithm in
the inside-outside fashion.
Given a phrase pair bp in the reordering graph,
we denote the number of paths from bs to bp with
?(bp). It can be computed in an iterative way
?(bp) = ?bp? ?(bp?), where bp? is one of the pre-
vious bilingual phrases of bp and ?(bs)=1. In a sim-
ilar way, the number of paths from be to bp, notated
as ?(bp), is simply ?(bp) = ?bp?? ?(bp??), where
bp?? is one of the subsequent bilingual phrases of bp
and ?(be)=1. Here, we show the ? and ? values of
all bilingual phrases of Figure 2 in Table 1. Espe-
cially, for the reordering example consisting of the
bilingual phrases bp1=?jiang juxing, will hold? and
bp2=?zhengshi huitan, formal meetings?, marked in
the gray color in Figure 2, the ? and ? values can be
calculated: ?(bp1) = 1, ?(bp2) = 1+1 = 2, ?(bs) =
8+1 = 9.
Inspired by the parsing literature on pruning
src span trg span ? ?
[0, 0] [0, 0] 1 9
[1, 1] [1, 1] 1 8
[1, 7] [1, 7] 1 1
[4, 4] [2, 2] 1 1
[4, 5] [2, 3] 1 3
[4, 6] [2, 4] 1 1
[4, 7] [2, 5] 1 2
[2, 7] [2, 7] 1 1
[5, 5] [3, 3] 1 1
[6, 6] [4, 4] 2 1
[6, 7] [4, 5] 1 2
[7, 7] [5, 5] 3 1
[2, 2] [6, 6] 5 1
[2, 3] [6, 7] 2 1
[3, 3] [7, 7] 5 1
[8, 8] [8, 8] 9 1
Table 1: The ? and ? values of the bilingual phrases
shown in Figure 2.
(Charniak and Johnson, 2005; Huang, 2008), the
fractional count of (o, bp?, bp) is
Count(o, bp?, bp) = ?(bp
?) ? ?(bp)
?(bs) (3)
where the numerator indicates the number of paths
containing the reordering example (o, bp?, bp) and
the denominator is the total number of paths in the
reordering graph. Continuing with the reordering
example described above, we obtain its fractional
count using the formula (3): Count(M, bp1, bp2) =
(1? 2)/9 = 2/9.
Then, the fractional count of bp in the orientation
o is calculated as described below:
Count(o, bp) =
?
bp?
Count(o, bp?, bp) (4)
For example, we compute the fractional count of
bp2 in the monotone orientation by the formula (4):
Count(M, bp2) = 2/9.
As described in the lexicalized reordering model
(Section 2.1), we apply the formula (2) to calculate
the final reordering probabilities.
3 Experiments
We conduct experiments to investigate the effec-
tiveness of our method on the msd-fe reorder-
ing model and the msd-bidirectional-fe reordering
model. These two models are widely applied in
14
phrase-based system (Koehn et al, 2007). The msd-
fe reordering model has three features, which rep-
resent the probabilities of bilingual phrases in three
orientations: monotone, swap, or discontinuous. If a
msd-bidirectional-fe model is used, then the number
of features doubles: one for each direction.
3.1 Experiment Setup
Two different sizes of training corpora are used in
our experiments: one is a small-scale corpus that
comes from FBIS corpus consisting of 239K bilin-
gual sentence pairs, the other is a large-scale corpus
that includes 1.55M bilingual sentence pairs from
LDC. The 2002 NIST MT evaluation test data is
used as the development set and the 2003, 2004,
2005 NIST MT test data are the test sets. We
choose the MOSES1 (Koehn et al, 2007) as the ex-
perimental decoder. GIZA++ (Och and Ney, 2003)
and the heuristics ?grow-diag-final-and? are used to
generate a word-aligned corpus, where we extract
bilingual phrases with maximum length 7. We use
SRILM Toolkits (Stolcke, 2002) to train a 4-gram
language model on the Xinhua portion of Gigaword
corpus.
In exception to the reordering probabilities, we
use the same features in the comparative experi-
ments. During decoding, we set ttable-limit = 20,
stack = 100, and perform minimum-error-rate train-
ing (Och, 2003) to tune various feature weights. The
translation quality is evaluated by case-insensitive
BLEU-4 metric (Papineni et al, 2002). Finally, we
conduct paired bootstrap sampling (Koehn, 2004) to
test the significance in BLEU scores differences.
3.2 Experimental Results
Table 2 shows the results of experiments with the
small training corpus. For the msd-fe model, the
BLEU scores by our method are 30.51 32.78 and
29.50, achieving absolute improvements of 0.89,
0.66 and 0.62 on the three test sets, respectively. For
the msd-bidirectional-fe model, our method obtains
BLEU scores of 30.49 32.73 and 29.24, with abso-
lute improvements of 1.11, 0.73 and 0.60 over the
baseline method.
1The phrase-based lexical reordering model (Tillmann,
2004) is also closely related to our model. However, due to
the limit of time and space, we only use Moses-style reordering
model (Koehn et al, 2007) as our baseline.
model method MT-03 MT-04 MT-05
baseline 29.62 32.12 28.88m-f RG 30.51?? 32.78?? 29.50?
baseline 29.38 32.00 28.64m-b-f RG 30.49?? 32.73?? 29.24?
Table 2: Experimental results with the small-scale cor-
pus. m-f: msd-fe reordering model. m-b-f: msd-
bidirectional-fe reordering model. RG: probabilities esti-
mation based on Reordering Graph. * or **: significantly
better than baseline (p < 0 .05 or p < 0 .01 ).
model method MT-03 MT-04 MT-05
baseline 31.58 32.39 31.49m-f RG 32.44?? 33.24?? 31.64
baseline 32.43 33.07 31.69m-b-f RG 33.29?? 34.49?? 32.79??
Table 3: Experimental results with the large-scale cor-
pus.
Table 3 shows the results of experiments with
the large training corpus. In the experiments of
the msd-fe model, in exception to the MT-05 test
set, our method is superior to the baseline method.
The BLEU scores by our method are 32.44, 33.24
and 31.64, which obtain 0.86, 0.85 and 0.15 gains
on three test set, respectively. For the msd-
bidirectional-fe model, the BLEU scores produced
by our approach are 33.29, 34.49 and 32.79 on the
three test sets, with 0.86, 1.42 and 1.1 points higher
than the baseline method, respectively.
4 Conclusion and Future Work
In this paper, we propose a method to improve the
reordering model by considering the effect of the
number of adjacent bilingual phrases on the reorder-
ing probabilities estimation. Experimental results on
NIST Chinese-to-English tasks demonstrate the ef-
fectiveness of our method.
Our method is also general to other lexicalized
reordering models. We plan to apply our method
to the complex lexicalized reordering models, for
example, the hierarchical reordering model (Galley
and Manning, 2008) and the MEBTG reordering
model (Xiong et al, 2006). In addition, how to fur-
ther improve the reordering model by distinguishing
the derivations with different probabilities will be-
come another study emphasis in further research.
15
Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China, Contracts 60873167 and
60903138. We thank the anonymous reviewers for
their insightful comments. We are also grateful to
Hongmei Zhao and Shu Cai for their helpful feed-
back.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL 2005, pages 173?180.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP 2008, pages 848?856.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL 2008,
pages 586?594.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388?395.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, pages 417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP 2002, pages 901?
904.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
ACL 2004, Short Papers, pages 101?104.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for statis-
tical machine translation. In Proc. of ACL 2006, pages
521?528.
Richard Zens and Hermann Ney. 2006. Discriminvative
reordering models for statistical machine translation.
In Proc. of Workshop on Statistical Machine Transla-
tion 2006, pages 521?528.
16
Proceedings of the ACL 2010 Conference Short Papers, pages 142?146,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Better Filtration and Augmentation for Hierarchical Phrase-Based
Translation Rules
Zhiyang Wang ? Yajuan Lu? ? Qun Liu ? Young-Sook Hwang ?
?Key Lab. of Intelligent Information Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
P.O. Box 2704, Beijing 100190, China 11, Euljiro2-ga, Jung-gu, Seoul 100-999, Korea
wangzhiyang@ict.ac.cn yshwang@sktelecom.com
Abstract
This paper presents a novel filtration cri-
terion to restrict the rule extraction for
the hierarchical phrase-based translation
model, where a bilingual but relaxed well-
formed dependency restriction is used to
filter out bad rules. Furthermore, a new
feature which describes the regularity that
the source/target dependency edge trig-
gers the target/source word is also pro-
posed. Experimental results show that, the
new criteria weeds out about 40% rules
while with translation performance im-
provement, and the new feature brings an-
other improvement to the baseline system,
especially on larger corpus.
1 Introduction
Hierarchical phrase-based (HPB) model (Chiang,
2005) is the state-of-the-art statistical machine
translation (SMT) model. By looking for phrases
that contain other phrases and replacing the sub-
phrases with nonterminal symbols, it gets hierar-
chical rules. Hierarchical rules are more powerful
than conventional phrases since they have better
generalization capability and could capture long
distance reordering. However, when the train-
ing corpus becomes larger, the number of rules
will grow exponentially, which inevitably results
in slow and memory-consuming decoding.
In this paper, we address the problem of reduc-
ing the hierarchical translation rule table resorting
to the dependency information of bilingual lan-
guages. We only keep rules that both sides are
relaxed-well-formed (RWF) dependency structure
(see the definition in Section 3), and discard others
which do not satisfy this constraint. In this way,
about 40% bad rules are weeded out from the orig-
inal rule table. However, the performance is even
better than the traditional HPB translation system.
Source
Target 
f f? 
e
Figure 1: Solid wire reveals the dependency rela-
tion pointing from the child to the parent. Target
word e is triggered by the source word f and it?s
head word f ?, p(e|f ? f ?).
Based on the relaxed-well-formed dependency
structure, we also introduce a new linguistic fea-
ture to enhance translation performance. In the
traditional phrase-based SMT model, there are
always lexical translation probabilities based on
IBM model 1 (Brown et al, 1993), i.e. p(e|f),
namely, the target word e is triggered by the source
word f . Intuitively, however, the generation of e
is not only involved with f , sometimes may also
be triggered by other context words in the source
side. Here we assume that the dependency edge
(f ? f ?) of word f generates target word e (we
call it head word trigger in Section 4). Therefore,
two words in one language trigger one word in
another, which provides a more sophisticated and
better choice for the target word, i.e. Figure 1.
Similarly, the dependency feature works well in
Chinese-to-English translation task, especially on
large corpus.
2 Related Work
In the past, a significant number of techniques
have been presented to reduce the hierarchical rule
table. He et al (2009) just used the key phrases
of source side to filter the rule table without taking
advantage of any linguistic information. Iglesias
et al (2009) put rules into syntactic classes based
on the number of non-terminals and patterns, and
applied various filtration strategies to improve the
rule table quality. Shen et al (2008) discarded
142
found
The
girl 
lovely 
house
a beautiful
Figure 2: An example of dependency tree. The
corresponding plain sentence is The lovely girl
found a beautiful house.
most entries of the rule table by using the con-
straint that rules of the target-side are well-formed
(WF) dependency structure, but this filtering led to
degradation in translation performance. They ob-
tained improvements by adding an additional de-
pendency language model. The basic difference
of our method from (Shen et al, 2008) is that we
keep rules that both sides should be relaxed-well-
formed dependency structure, not just the target
side. Besides, our system complexity is not in-
creased because no additional language model is
introduced.
The feature of head word trigger which we ap-
ply to the log-linear model is motivated by the
trigger-based approach (Hasan and Ney, 2009).
Hasan and Ney (2009) introduced a second word
to trigger the target word without considering any
linguistic information. Furthermore, since the sec-
ond word can come from any part of the sentence,
there may be a prohibitively large number of pa-
rameters involved. Besides, He et al (2008) built
a maximum entropy model which combines rich
context information for selecting translation rules
during decoding. However, as the size of the cor-
pus increases, the maximum entropy model will
become larger. Similarly, In (Shen et al, 2009),
context language model is proposed for better rule
selection. Taking the dependency edge as condi-
tion, our approach is very different from previous
approaches of exploring context information.
3 Relaxed-well-formed Dependency
Structure
Dependency models have recently gained consid-
erable interest in SMT (Ding and Palmer, 2005;
Quirk et al, 2005; Shen et al, 2008). Depen-
dency tree can represent richer structural infor-
mation. It reveals long-distance relation between
words and directly models the semantic structure
of a sentence without any constituent labels. Fig-
ure 2 shows an example of a dependency tree. In
this example, the word found is the root of the tree.
Shen et al (2008) propose the well-formed de-
pendency structure to filter the hierarchical rule ta-
ble. A well-formed dependency structure could be
either a single-rooted dependency tree or a set of
sibling trees. Although most rules are discarded
with the constraint that the target side should be
well-formed, this filtration leads to degradation in
translation performance.
As an extension of the work of (Shen et
al., 2008), we introduce the so-called relaxed-
well-formed dependency structure to filter the hi-
erarchical rule table. Given a sentence S =
w1w2...wn. Let d1d2...dn represent the position of
parent word for each word. For example, d3 = 4
means that w3 depends on w4. If wi is a root, we
define di = ?1.
Definition A dependency structure wi...wj is
a relaxed-well-formed structure, where there is
h /? [i, j], all the words wi...wj are directly or
indirectly depended on wh or -1 (here we define
h = ?1). If and only if it satisfies the following
conditions
? dh /? [i, j]
? ?k ? [i, j], dk ? [i, j] or dk = h
From the definition above, we can see that
the relaxed-well-formed structure obviously cov-
ers the well-formed one. In this structure, we
don?t constrain that all the children of the sub-root
should be complete. Let?s review the dependency
tree in Figure 2 as an example. Except for the well-
formed structure, we could also extract girl found
a beautiful house. Therefore, if the modifier The
lovely changes to The cute, this rule also works.
4 Head Word Trigger
(Koehn et al, 2003) introduced the concept of
lexical weighting to check how well words of
the phrase translate to each other. Source word
f aligns with target word e, according to the
IBM model 1, the lexical translation probability
is p(e|f). However, in the sense of dependency
relationship, we believe that the generation of the
target word e, is not only triggered by the aligned
source word f , but also associated with f ?s head
word f ?. Therefore, the lexical translation prob-
ability becomes p(e|f ? f ?), which of course
allows for a more fine-grained lexical choice of
143
the target word. More specifically, the probabil-
ity could be estimated by the maximum likelihood
(MLE) approach,
p(e|f ? f ?) = count(e, f ? f
?)
?
e? count(e?, f ? f ?)
(1)
Given a phrase pair f , e and word alignment
a, and the dependent relation of the source sen-
tence dJ1 (J is the length of the source sentence,
I is the length of the target sentence). Therefore,
given the lexical translation probability distribu-
tion p(e|f ? f ?), we compute the feature score of
a phrase pair (f , e) as
p(e|f, dJ1 , a)
= ?|e|i=1
1
|{j|(j, i) ? a}|
?
?(j,i)?a
p(ei|fj ? fdj) (2)
Now we get p(e|f, dJ1 , a), we could obtain
p(f |e, dI1, a) (dI1 represents dependent relation of
the target side) in the similar way. This new fea-
ture can be easily integrated into the log-linear
model as lexical weighting does.
5 Experiments
In this section, we describe the experimental set-
ting used in this work, and verify the effect of
the relaxed-well-formed structure filtering and the
new feature, head word trigger.
5.1 Experimental Setup
Experiments are carried out on the NIST1
Chinese-English translation task with two differ-
ent size of training corpora.
? FBIS: We use the FBIS corpus as the first
training corpus, which contains 239K sen-
tence pairs with 6.9M Chinese words and
8.9M English words.
? GQ: This is manually selected from the
LDC2 corpora. GQ contains 1.5M sentence
pairs with 41M Chinese words and 48M En-
glish words. In fact, FBIS is the subset of
GQ.
1www.nist.gov/speech/tests/mt
2It consists of six LDC corpora:
LDC2002E18, LDC2003E07, LDC2003E14, Hansards part
of LDC2004T07, LDC2004T08, LDC2005T06.
For language model, we use the SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram model on the first 1/3 of the Xinhua portion
of GIGAWORD corpus. And we use the NIST
2002 MT evaluation test set as our development
set, and NIST 2004, 2005 test sets as our blind
test sets. We evaluate the translation quality us-
ing case-insensitive BLEU metric (Papineni et
al., 2002) without dropping OOV words, and the
feature weights are tuned by minimum error rate
training (Och, 2003).
In order to get the dependency relation of the
training corpus, we re-implement a beam-search
style monolingual dependency parser according
to (Nivre and Scholz, 2004). Then we use the
same method suggested in (Chiang, 2005) to
extract SCFG grammar rules within dependency
constraint on both sides except that unaligned
words are allowed at the edge of phrases. Pa-
rameters of head word trigger are estimated as de-
scribed in Section 4. As a default, the maximum
initial phrase length is set to 10 and the maximum
rule length of the source side is set to 5. Besides,
we also re-implement the decoder of Hiero (Chi-
ang, 2007) as our baseline. In fact, we just exploit
the dependency structure during the rule extrac-
tion phase. Therefore, we don?t need to change
the main decoding algorithm of the SMT system.
5.2 Results on FBIS Corpus
A series of experiments was done on the FBIS cor-
pus. We first parse the bilingual languages with
monolingual dependency parser respectively, and
then only retain the rules that both sides are in line
with the constraint of dependency structure. In
Table 1, the relaxed-well-formed structure filtered
out 35% of the rule table and the well-formed dis-
carded 74%. RWF extracts additional 39% com-
pared to WF, which can be seen as some kind
of evidence that the rules we additional get seem
common in the sense of linguistics. Compared to
(Shen et al, 2008), we just use the dependency
structure to constrain rules, not to maintain the tree
structures to guide decoding.
Table 2 shows the translation result on FBIS.
We can see that the RWF structure constraint can
improve translation quality substantially both at
development set and different test sets. On the
Test04 task, it gains +0.86% BLEU, and +0.84%
on Test05. Besides, we also used Shen et al
(2008)?s WF structure to filter both sides. Al-
though it discard about 74% of the rule table, the
144
System Rule table size
HPB 30,152,090
RWF 19,610,255
WF 7,742,031
Table 1: Rule table size with different con-
straint on FBIS. Here HPB refers to the base-
line hierarchal phrase-based system, RWF means
relaxed-well-formed constraint and WF represents
the well-formed structure.
System Dev02 Test04 Test05
HPB 0.3285 0.3284 0.2965
WF 0.3125 0.3218 0.2887
RWF 0.3326 0.3370** 0.3050
RWF+Tri 0.3281 / 0.2965
Table 2: Results of FBIS corpus. Here Tri means
the feature of head word trigger on both sides. And
we don?t test the new feature on Test04 because of
the bad performance on development set. * or **
= significantly better than baseline (p < 0.05 or
0.01, respectively).
over-all BLEU is decreased by 0.66%-0.78% on
the test sets.
As for the feature of head word trigger, it seems
not work on the FBIS corpus. On Test05, it gets
the same score with the baseline, but lower than
RWF filtering. This may be caused by the data
sparseness problem, which results in inaccurate
parameter estimation of the new feature.
5.3 Result on GQ Corpus
In this part, we increased the size of the training
corpus to check whether the feature of head word
trigger works on large corpus.
We get 152M rule entries from the GQ corpus
according to (Chiang, 2007)?s extraction method.
If we use the RWF structure to constrain both
sides, the number of rules is 87M, about 43% of
rule entries are discarded. From Table 3, the new
System Dev02 Test04 Test05
HPB 0.3473 0.3386 0.3206
RWF 0.3539 0.3485** 0.3228
RWF+Tri 0.3540 0.3607** 0.3339*
Table 3: Results of GQ corpus. * or ** = sig-
nificantly better than baseline (p < 0.05 or 0.01,
respectively).
feature works well on two different test sets. The
gain is +2.21% BLEU on Test04, and +1.33% on
Test05. Compared to the result of the baseline,
only using the RWF structure to filter performs the
same as the baseline on Test05, and +0.99% gains
on Test04.
6 Conclusions
This paper proposes a simple strategy to filter the
hierarchal rule table, and introduces a new feature
to enhance the translation performance. We em-
ploy the relaxed-well-formed dependency struc-
ture to constrain both sides of the rule, and about
40% of rules are discarded with improvement of
the translation performance. In order to make full
use of the dependency information, we assume
that the target word e is triggered by dependency
edge of the corresponding source word f . And
this feature works well on large parallel training
corpus.
How to estimate the probability of head word
trigger is very important. Here we only get the pa-
rameters in a generative way. In the future, we we
are plan to exploit some discriminative approach
to train parameters of this feature, such as EM al-
gorithm (Hasan et al, 2008) or maximum entropy
(He et al, 2008).
Besides, the quality of the parser is another ef-
fect for this method. As the next step, we will
try to exploit bilingual knowledge to improve the
monolingual parser, i.e. (Huang et al, 2009).
Acknowledgments
This work was partly supported by National
Natural Science Foundation of China Contract
60873167. It was also funded by SK Telecom,
Korea under the contract 4360002953. We show
our special thanks to Wenbin Jiang and Shu Cai
for their valuable suggestions. We also thank
the anonymous reviewers for their insightful com-
ments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
145
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263?
270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 541?548.
Sas?a Hasan and Hermann Ney. 2009. Comparison of
extended lexicon models in search and rescoring for
smt. In NAACL ?09: Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 17?20.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet lexicon models
for statistical machine translation. In EMNLP ?08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 372?
381.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In COLING ?08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 321?328.
Zhongjun He, Yao Meng, Yajuan Lu?, Hao Yu, and Qun
Liu. 2009. Reducing smt rule table with monolin-
gual key phrase. In ACL-IJCNLP ?09: Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 121?124.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1222?1231.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 380?388.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In COLING
?04: Proceedings of the 20th international confer-
ence on Computational Linguistics, pages 64?70.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In ACL ?05: Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 72?80.
Andreas Stolcke. 2002. Srilman extensible language
modeling toolkit. In In Proceedings of the 7th Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), pages 901?904.
146
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1278?1287,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Adjoining Tree-to-String Translation
Yang Liu, Qun Liu, and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,liuqun,lvyajuan}@ict.ac.cn
Abstract
We introduce synchronous tree adjoining
grammars (TAG) into tree-to-string transla-
tion, which converts a source tree to a target
string. Without reconstructing TAG deriva-
tions explicitly, our rule extraction algo-
rithm directly learns tree-to-string rules from
aligned Treebank-style trees. As tree-to-string
translation casts decoding as a tree parsing
problem rather than parsing, the decoder still
runs fast when adjoining is included. Less
than 2 times slower, the adjoining tree-to-
string system improves translation quality by
+0.7 BLEU over the baseline system only al-
lowing for tree substitution on NIST Chinese-
English test sets.
1 Introduction
Syntax-based translation models, which exploit hi-
erarchical structures of natural languages to guide
machine translation, have become increasingly pop-
ular in recent years. So far, most of them have
been based on synchronous context-free grammars
(CFG) (Chiang, 2007), tree substitution grammars
(TSG) (Eisner, 2003; Galley et al, 2006; Liu et
al., 2006; Huang et al, 2006; Zhang et al, 2008),
and inversion transduction grammars (ITG) (Wu,
1997; Xiong et al, 2006). Although these for-
malisms present simple and precise mechanisms for
describing the basic recursive structure of sentences,
they are not powerful enough to model some impor-
tant features of natural language syntax. For ex-
ample, Chiang (2006) points out that the transla-
tion of languages that can stack an unbounded num-
ber of clauses in an ?inside-out? way (Wu, 1997)
provably goes beyond the expressive power of syn-
chronous CFG and TSG. Therefore, it is necessary
to find ways to take advantage of more powerful syn-
chronous grammars to improve machine translation.
Synchronous tree adjoining grammars (TAG)
(Shieber and Schabes, 1990) are a good candidate.
As a formal tree rewriting system, TAG (Joshi et al,
1975; Joshi, 1985) provides a larger domain of lo-
cality than CFG to state linguistic dependencies that
are far apart since the formalism treats trees as basic
building blocks. As a mildly context-sensitive gram-
mar, TAG is conjectured to be powerful enough to
model natural languages. Synchronous TAG gener-
alizes TAG by allowing the construction of a pair
of trees using the TAG operations of substitution
and adjoining on tree pairs. The idea of using syn-
chronous TAG in machine translation has been pur-
sued by several researchers (Abeille et al, 1990;
Prigent, 1994; Dras, 1999), but only recently in
its probabilistic form (Nesson et al, 2006; De-
Neefe and Knight, 2009). Shieber (2007) argues that
probabilistic synchronous TAG possesses appealing
properties such as expressivity and trainability for
building a machine translation system.
However, one major challenge for applying syn-
chronous TAG to machine translation is computa-
tional complexity. While TAG requires O(n6) time
for monolingual parsing, synchronous TAG requires
O(n12) for bilingual parsing. One solution is to use
tree insertion grammars (TIG) introduced by Sch-
abes and Waters (1995). As a restricted form of
TAG, TIG still allows for adjoining of unbounded
trees but only requires O(n3) time for monolingual
parsing. Nesson et al (2006) firstly demonstrate
1278
o?
zo?ngto?ng
NN
NP
President
X,?1
{I
me?iguo?
NR
NP
US
X,?2
NP? NP?
NP
X? X?
X
,?1
NP
NP? NP
NN
o?
zo?ngto?ng
X
X? X
President
,?2
NP
NP
NR
{I
me?iguo?
NP
NN
o?
zo?ngto?ng
X
X
US
X
President
,?3
Figure 1: Initial and auxiliary tree pairs. The source side (Chinese) is a Treebank-style linguistic tree. The target side
(English) is a purely structural tree using a single non-terminal (X). By convention, substitution and foot nodes are
marked with a down arrow (?) and an asterisk (?), respectively. The dashed lines link substitution sites (e.g., NP? and
X? in ?1) and adjoining sites (e.g., NP and X in ?2) in tree pairs. Substituting the initial tree pair ?1 at the NP?-X?
node pair in the auxiliary tree pair ?1 yields a derived tree pair ?2, which can be adjoined at NN-X in ?2 to generate
?3.
the use of synchronous TIG for machine translation
and report promising results. DeNeefe and Knight
(2009) prove that adjoining can improve translation
quality significantly over a state-of-the-art string-
to-tree system (Galley et al, 2006) that uses syn-
chronous TSG with tractable computational com-
plexity.
In this paper, we introduce synchronous TAG into
tree-to-string translation (Liu et al, 2006; Huang et
al., 2006), which is the simplest and fastest among
syntax-based approaches (Section 2). We propose
a new rule extraction algorithm based on GHKM
(Galley et al, 2004) that directly induces a syn-
chronous TAG from an aligned and parsed bilingual
corpus without converting Treebank-style trees to
TAG derivations explicitly (Section 3). As tree-to-
string translation takes a source parse tree as input,
the decoding can be cast as a tree parsing problem
(Eisner, 2003): reconstructing TAG derivations from
a derived tree using tree-to-string rules that allow for
both substitution and adjoining. We describe how to
convert TAG derivations to translation forest (Sec-
tion 4). We evaluated the new tree-to-string system
on NIST Chinese-English tests and obtained con-
sistent improvements (+0.7 BLEU) over the STSG-
based baseline system without significant loss in ef-
ficiency (1.6 times slower) (Section 5).
2 Model
A synchronous TAG consists of a set of linked ele-
mentary tree pairs: initial and auxiliary. An initial
tree is a tree of which the interior nodes are all la-
beled with non-terminal symbols, and the nodes on
the frontier are either words or non-terminal sym-
bols marked with a down arrow (?). An auxiliary
tree is defined as an initial tree, except that exactly
one of its frontier nodes must be marked as foot
node (?). The foot node must be labeled with a non-
terminal symbol that is the same as the label of the
root node.
Synchronous TAG defines two operations to build
derived tree pairs from elementary tree pairs: substi-
tution and adjoining. Nodes in initial and auxiliary
tree pairs are linked to indicate the correspondence
between substitution and adjoining sites. Figure 1
shows three initial tree pairs (i.e., ?1, ?2, and ?3)
and two auxiliary tree pairs (i.e., ?1 and ?2). The
dashed lines link substitution nodes (e.g., NP? and
X? in ?1) and adjoining sites (e.g., NP and X in ?2)
in tree pairs. Substituting the initial tree pair ?1 at
1279
{I
me?iguo?
o?
zo?ngto?ng
n?
a`oba?ma?
?
du`?
l?
qia?ngj??
??
sh`?jia`n
??
yu?y??
gI
qia?nze?
0 1 2 3 4 5 6 7 8
NR NN NR P NN NN VV NN
NP NP NP NP NP
NP PP VP
NP VP
IP
US President Obama has condemned the shooting incident
Figure 2: A training example. Tree-to-string rules can be extracted from shaded nodes.
node minimal initial rule minimal auxiliary rule
NR0,1 [1] ( NR me?iguo? ) ? US
NP0,1 [2] ( NP ( x1:NR? ) ) ? x1
NN1,2 [3] ( NN zo?ngto?ng ) ? President
NP1,2 [4] ( NP ( x1:NN? ) ) ? x1
[5] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[6] ( NP0:1 ( x1:NR? ) ) ? x1 [7] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NP0,2 [8] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[9] ( NP0:1 ( x1:NN? ) ) ? x1 [10] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[11] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NR2,3 [12] ( NR a`oba?ma? ) ? Obama
NP2,3 [13] ( NP ( x1:NR? ) ) ? x1
[14] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[15] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2 [16] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NP0,3 [17] ( NP0:1 ( x1:NR? ) ) ? x1 [18] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[19] ( NP0:1 ( x1:NN? ) ) ? x1
[20] ( NP0:1 ( x1:NR? ) ) ? x1
NN4,5 [21] ( NN qia?ngj?? ) ? shooting
NN5,6 [22] ( NN sh?`jia`n ) ? incident
NP4,6 [23] ( NP ( x1:NN? ) ( x2:NN? ) ) ? x1 x2
PP3,6 [24] ( PP ( du?` ) ( x1:NP? ) ) ? x1
NN7,8 [25] ( NN qia?nze? ) ? condemned
NP7,8 [26] ( NP ( x1:NN? ) ) ? x1
VP6,8 [27] ( VP ( VV yu?y?? ) ( x1:NP? ) ) ? x1
[28] ( VP ( x1:PP? ) ( x2:VP? ) ) ? x2 the x1VP3,8 [29] ( VP0:1 ( VV yu?y?? ) ( x1:NP? ) ) ? x1 [30] ( VP ( x1:PP? ) ( x2:VP? ) ) ? x2 the x1
IP0,8 [31] ( IP ( x1:NP? ) ( x2:VP? ) ) ? x1 has x2
Table 1: Minimal initial and auxiliary rules extracted from Figure 2. Note that an adjoining site has a span as subscript.
For example, NP0:1 in rule 6 indicates that the node is an adjoining site linked to a target node dominating the target
string spanning from position 0 to position 1 (i.e., x1). The target tree is hidden because tree-to-string translation only
considers the target surface string.
1280
the NP?-X? node pair in the auxiliary tree pair ?1
yields a derived tree pair ?2, which can be adjoined
at NN-X in ?2 to generate ?3.
For simplicity, we represent ?2 as a tree-to-string
rule:
( NP0:1 ( NR me?iguo? ) ) ? US
where NP0:1 indicates that the node is an adjoin-
ing site linked to a target node dominating the tar-
get string spanning from position 0 to position 1
(i.e., ?US?). The target tree is hidden because tree-
to-string translation only considers the target surface
string. Similarly, ?1 can be written as
( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
where x denotes a non-terminal and the subscripts
indicate the correspondence between source and tar-
get non-terminals.
The parameters of a probabilistic synchronous
TAG are
?
?
Pi(?) = 1 (1)
?
?
Ps(?|?) = 1 (2)
?
?
Pa(?|?) + Pa(NONE|?) = 1 (3)
where ? ranges over initial tree pairs, ? over aux-
iliary tree pairs, and ? over node pairs. Pi(?) is
the probability of beginning a derivation with ?;
Ps(?|?) is the probability of substituting ? at ?;
Pa(?|?) is the probability of adjoining ? at ?; fi-
nally, Pa(NONE|?) is the probability of nothing ad-
joining at ?.
For tree-to-string translation, these parameters
can be treated as feature functions of a discrimi-
native framework (Och, 2003) combined with other
conventional features such as relative frequency, lex-
ical weight, rule count, language model, and word
count (Liu et al, 2006).
3 Rule Extraction
Inducing a synchronous TAG from training data
often begins with converting Treebank-style parse
trees to TAG derivations (Xia, 1999; Chen and
Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and
Knight (2009) propose an algorithm to extract syn-
chronous TIG rules from an aligned and parsed
bilingual corpus. They first classify tree nodes
into heads, arguments, and adjuncts using heuristics
(Collins, 2003), then transform a Treebank-style tree
into a TIG derivation, and finally extract minimally-
sized rules from the derivation tree and the string on
the other side, constrained by the alignments. Proba-
bilistic models can be estimated by collecting counts
over the derivation trees.
However, one challenge is that there are many
TAG derivations that can yield the same derived tree,
even with respect to a single grammar. It is difficult
to choose appropriate single derivations that enable
the resulting grammar to translate unseen data well.
DeNeefe and Knight (2009) indicate that the way to
reconstruct TIG derivations has a direct effect on fi-
nal translation quality. They suggest that one possi-
ble solution is to use derivation forest rather than a
single derivation tree for rule extraction.
Alternatively, we extend the GHKM algorithm
(Galley et al, 2004) to directly extract tree-to-string
rules that allow for both substitution and adjoining
from aligned and parsed data. There is no need for
transforming a parse tree into a TAG derivation ex-
plicitly before rule extraction and all derivations can
be easily reconstructed using extracted rules. 1 Our
rule extraction algorithm involves two steps: (1) ex-
tracting minimal rules and (2) composition.
3.1 Extracting Minimal Rules
Figure 2 shows a training example, which consists of
a Chinese parse tree, an English string, and the word
alignment between them. By convention, shaded
nodes are called frontier nodes from which tree-to-
string rules can be extracted. Note that the source
phrase dominated by a frontier node and its corre-
sponding target phrase are consistent with the word
alignment: all words in the source phrase are aligned
to all words in the corresponding target phrase and
vice versa.
We distinguish between three categories of tree-
1Note that our algorithm does not take heads, complements,
and adjuncts into consideration and extracts all possible rules
with respect to word alignment. Our hope is that this treatment
would make our system more robust in the presence of noisy
data. It is possible to use the linguistic preferences as features.
We leave this for future work.
1281
to-string rules:
1. substitution rules, in which the source tree is
an initial tree without adjoining sites.
2. adjoining rules, in which the source tree is an
initial tree with at least one adjoining site.
3. auxiliary rules, in which the source tree is an
auxiliary tree.
For example, in Figure 1, ?1 is a substitution rule,
?2 is an adjoining rule, and ?1 is an auxiliary rule.
Minimal substitution rules are the same with those
in STSG (Galley et al, 2004; Liu et al, 2006) and
therefore can be extracted directly using GHKM. By
minimal, we mean that the interior nodes are not
frontier and cannot be decomposed. For example,
in Table 2, rule 1 (for short r1) is a minimal substi-
tution rule extracted from NR0,1.
Minimal adjoining rules are defined as minimal
substitution rules, except that each root node must
be an adjoining site. In Table 2, r2 is a minimal
substitution rule extracted from NP0,1. As NP0,1 is
a descendant of NP0,2 with the same label, NP0,1
is a possible adjoining site. Therefore, r6 can be
derived from r2 and licensed as a minimal adjoining
rule extracted from NP0,2. Similarly, four minimal
adjoining rules are extracted from NP0,3 because it
has four frontier descendants labeled with NP.
Minimal auxiliary rules are derived from minimal
substitution and adjoining rules. For example, in Ta-
ble 2, r7 and r10 are derived from the minimal sub-
stitution rule r5 while r8 and r11 are derived from
r15. Note that a minimal auxiliary rule can have ad-
joining sites (e.g., r8).
Table 1 lists 17 minimal substitution rules, 7 min-
imal adjoining rules, and 7 minimal auxiliary rules
extracted from Figure 2.
3.2 Composition
We can obtain composed rules that capture rich con-
texts by substituting and adjoining minimal initial
and auxiliary rules. For example, the composition
of r12, r17, r25, r26, r29, and r31 yields an initial
rule with two adjoining sites:
( IP ( NP0:1 ( NR a`oba?ma? ) ) ( VP2:3 ( VV yu?y?? )
( NP ( NN qia?nze? ) ) ) ) ? Obama has condemned
Note that the source phrase ?a`oba?ma? . . . yu?y?? qia?nze??
is discontinuous. Our model allows both the source
and target phrases of an initial rule with adjoining
sites to be discontinuous, which goes beyond the ex-
pressive power of synchronous CFG and TSG.
Similarly, the composition of two auxiliary rules
r8 and r16 yields a new auxiliary rule:
( NP ( NP ( x1:NP? ) ( x2:NP? ) ) ( x3:NP? ) ) ? x1x2x3
We first compose initial rules and then com-
pose auxiliary rules, both in a bottom-up way. To
maintain a reasonable grammar size, we follow Liu
(2006) to restrict that the tree height of a rule is no
greater than 3 and the source surface string is no
longer than 7.
To learn the probability models Pi(?), Ps(?|?),
Pa(?|?), and Pa(NONE|?), we collect and normal-
ize counts over these extracted rules following De-
Neefe and Knight (2009).
4 Decoding
Given a synchronous TAG and a derived source tree
pi, a tree-to-string decoder finds the English yield
of the best derivation of which the Chinese yield
matches pi:
e? = e
(
arg max
D s.t. f(D)=pi
P (D)
)
(4)
This is called tree parsing (Eisner, 2003) as the de-
coder finds ways of decomposing pi into elementary
trees.
Tree-to-string decoding with STSG is usually
treated as forest rescoring (Huang and Chiang,
2007) that involves two steps. The decoder first con-
verts the input tree into a translation forest using a
translation rule set by pattern matching. Huang et
al. (2006) show that this step is a depth-first search
with memorization in O(n) time. Then, the decoder
searches for the best derivation in the translation for-
est intersected with n-gram language models and
outputs the target string. 2
Decoding with STAG, however, poses one major
challenge to forest rescoring. As translation forest
only supports substitution, it is difficult to construct
a translation forest for STAG derivations because of
2Mi et al (2008) give a detailed description of the two-step
decoding process. Huang and Mi (2010) systematically analyze
the decoding complexity of tree-to-string translation.
1282
?1
IP0,8
NP2,3 VP3,8?
NR2,3?
?2
NR2,3
n?
a`oba?ma?
?1
NP0,3
NP1,2 NP2,3?
NN1,2?
?2
NP0,3
NP0,2? NP
2,3
?
?3
NP0,2
NP0,1 NP1,2?
NR0,1?
?3
NN2,3
o?
zo?ngto?ng
elementary tree translation rule
?1 r1 ( IP ( NP0:1 ( x1:NR? ) ) ( x2:VP? ) ) ? x1 x2
?2 r2 ( NR a`oba?ma? ) ? Obama
?1 r3 ( NP ( NP0:1 ( x1:NN? ) ) ( x2:NP? ) ) ? x1 x2
?2 r4 ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
?3 r5 ( NP ( NP ( x1:NR? ) ) ( x2:NP? ) ) ? x1 x2
?3 r6 ( NN zo?ngto?ng ) ? President
Figure 3: Matched trees and corresponding rules. Each node in a matched tree is annotated with a span as superscript
to facilitate identification. For example, IP0,8 in ?1 indicates that IP0,8 in Figure 2 is matched. Note that its left child
NP2,3 is not its direct descendant in Figure 2, suggesting that adjoining is required at this site.
?1
?2(1.1) ?1(1) ?2(1)
?3(1) ?3(1.1)
IP0,8
NP0,2 VP3,8
NR0,1 NN1,2 NR2,3
e1 e2
e3 e4
hyperedge translation rule
e1 r1 + r4 ( IP ( NP ( x1:NP? ) ( NP ( x2:NR? ) ) ) ( x3:VP? ) ? x1 x2 x3
e2 r1 + r3 + r5 ( IP ( NP ( NP ( x1:NP? ) ( x2:NP? ) ) ( NP ( x3:NR? ) ) ) ( x4:VP? ) ) ? x1 x2 x3 x4
e3 r6 ( NN zo?ngto?ng ) ? President
e4 r2 ( NR a`oba?ma? ) ? Obama
Figure 4: Converting a derivation forest to a translation forest. In a derivation forest, a node in a derivation forest is a
matched elementary tree. A hyperedge corresponds to operations on related trees: substitution (dashed) or adjoining
(solid). We use Gorn addresses as tree addresses. ?2(1.1) denotes that ?2 is substituted in the tree ?1 at the node NR2,3?
of address 1.1 (i.e., the first child of the first child of the root node). As translation forest only supports substitution, we
combine trees with adjoining sites to form an equivalent tree without adjoining sites. Rules are composed accordingly
(e.g., r1 + r4).
1283
adjoining. Therefore, we divide forest rescoring for
STAG into three steps:
1. matching, matching STAG rules against the in-
put tree to obtain a TAG derivation forest;
2. conversion, converting the TAG derivation for-
est into a translation forest;
3. intersection, intersecting the translation forest
with an n-gram language model.
Given a tree-to-string rule, rule matching is to find
a subtree of the input tree that is identical to the
source side of the rule. While matching STSG rules
against a derived tree is straightforward, it is some-
what non-trivial for STAG rules that move beyond
nodes of a local tree. We follow Liu et al (2006) to
enumerate all elementary subtrees and match STAG
rules against these subtrees. This can be done by first
enumerating all minimal initial and auxiliary trees
and then combining them to obtain composed trees,
assuming that every node in the input tree is fron-
tier (see Section 3). We impose the same restrictions
on the tree height and length as in rule extraction.
Figure 3 shows some matched trees and correspond-
ing rules. Each node in a matched tree is annotated
with a span as superscript to facilitate identification.
For example, IP0,8 in ?1 means that IP0,8 in Figure
2 is matched. Note that its left child NP2,3 is not
its direct descendant in Figure 2, suggesting that ad-
joining is required at this site.
A TAG derivation tree specifies uniquely how
a derived tree is constructed using elementary trees
(Joshi, 1985). A node in a derivation tree is an ele-
mentary tree and an edge corresponds to operations
on related elementary trees: substitution or adjoin-
ing. We introduce TAG derivation forest, a com-
pact representation of multiple TAG derivation trees,
to encodes all matched TAG derivation trees of the
input derived tree.
Figure 4 shows part of a TAG derivation forest.
The six matched elementary trees are nodes in the
derivation forest. Dashed and solid lines represent
substitution and adjoining, respectively. We use
Gorn addresses as tree addresses: 0 is the address
of the root node, p is the address of the pth child of
the root node, and p ? q is the address of the qth child
of the node at the address p. The derivation forest
should be interpreted as follows: ?2 is substituted in
the tree ?1 at the node NR2,3? of address 1.1 (i.e., the
first child of the first child of the root node) and ?1 is
adjoined in the tree ?1 at the node NP2,3 of address
1.
To take advantage of existing decoding tech-
niques, it is necessary to convert a derivation forest
to a translation forest. A hyperedge in a transla-
tion forest corresponds to a translation rule. Mi et
al. (2008) describe how to convert a derived tree
to a translation forest using tree-to-string rules only
allowing for substitution. Unfortunately, it is not
straightforward to convert a derivation forest includ-
ing adjoining to a translation forest. To alleviate this
problem, we combine initial rules with adjoining
sites and associated auxiliary rules to form equiv-
alent initial rules without adjoining sites on the fly
during decoding.
Consider ?1 in Figure 3. It has an adjoining site
NP2,3. Adjoining ?2 in ?1 at the node NP2,3 pro-
duces an equivalent initial tree with only substitution
sites:
( IP0,8 ( NP0,3 ( NP0,2? ) ( NP2,3 ( NR2,3? ) ) ) ( VP3,8? ) )
The corresponding composed rule r1 + r4 has no
adjoining sites and can be added to translation forest.
We define that the elementary trees needed to be
composed (e.g., ?1 and ?2) form a composition tree
in a derivation forest. A node in a composition tree is
a matched elementary tree and an edge corresponds
to adjoining operations. The root node must be an
initial tree with at least one adjoining site. The de-
scendants of the root node must all be auxiliary trees.
For example, ( ?1 ( ?2 ) ) and ( ?1 ( ?1 ( ?3 ) ) ) are
two composition trees in Figure 4. The number of
children of a node in a composition tree depends on
the number of adjoining sites in the node. We use
composition forest to encode all possible composi-
tion trees.
Often, a node in a composition tree may have mul-
tiple matched rules. As a large amount of composi-
tion trees and composed rules can be identified and
constructed on the fly during forest conversion, we
used cube pruning (Chiang, 2007; Huang and Chi-
ang, 2007) to achieve a balance between translation
quality and decoding efficiency.
1284
category description number
VP verb phrase 12.40
NP noun phrase 7.69
IP simple clause 7.26
QP quantifier phrase 0.14
CP clause headed by C 0.10
PP preposition phrase 0.09
CLP classifier phrase 0.02
ADJP adjective phrase 0.02
LCP phrase formed by ?XP+LC? 0.02
DNP phrase formed by ?XP+DEG? 0.01
Table 2: Top-10 phrase categories of foot nodes and their
average occurrences in training corpus.
5 Evaluation
We evaluated our adjoining tree-to-string translation
system on Chinese-English translation. The bilin-
gual corpus consists of 1.5M sentences with 42.1M
Chinese words and 48.3M English words. The Chi-
nese sentences in the bilingual corpus were parsed
by an in-house parser. To maintain a reasonable
grammar size, we follow Liu et al (2006) to re-
strict that the height of a rule tree is no greater than
3 and the surface string?s length is no greater than 7.
After running GIZA++ (Och and Ney, 2003) to ob-
tain word alignment, our rule extraction algorithm
extracted 23.0M initial rules without adjoining sites,
6.6M initial rules with adjoining sites, and 5.3M
auxiliary rules. We used the SRILM toolkit (Stol-
cke, 2002) to train a 4-gram language model on the
Xinhua portion of the GIGAWORD corpus, which
contains 238M English words. We used the 2002
NIST MT Chinese-English test set as the develop-
ment set and the 2003-2005 NIST test sets as the
test sets. We evaluated translation quality using the
BLEU metric, as calculated by mteval-v11b.pl with
case-insensitive matching of n-grams.
Table 2 shows top-10 phrase categories of foot
nodes and their average occurrences in training cor-
pus. We find that VP (verb phrase) is most likely
to be the label of a foot node in an auxiliary rule.
On average, there are 12.4 nodes labeled with VP
are identical to one of its ancestors per tree. NP and
IP are also found to be foot node labels frequently.
Figure 4 shows the average occurrences of foot node
labels VP, NP, and IP over various distances. A dis-
tance is the difference of levels between a foot node
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
 0  1  2  3  4  5  6  7  8  9  10  11
av
er
ag
e 
oc
cu
rre
nc
e
distance
VP
IP
NP
Figure 5: Average occurrences of foot node labels VP,
NP, and IP over various distances.
system grammar MT03 MT04 MT05
Moses - 33.10 33.96 32.17
hierarchical SCFG 33.40 34.65 32.88
STSG 33.13 34.55 31.94
tree-to-string STAG 33.64 35.28 32.71
Table 3: BLEU scores on NIST Chinese-English test sets.
Scores marked in bold are significantly better that those
of STSG at pl.01 level.
and the root node. For example, in Figure 2, the dis-
tance between NP0,1 and NP0,3 is 2 and the distance
between VP6,8 and VP3,8 is 1. As most foot nodes
are usually very close to the root nodes, we restrict
that a foot node must be the direct descendant of the
root node in our experiments.
Table 3 shows the BLEU scores on the NIST
Chinese-English test sets. Our baseline system is the
tree-to-string system using STSG (Liu et al, 2006;
Huang et al, 2006). The STAG system outper-
forms the STSG system significantly on the MT04
and MT05 test sets at pl.01 level. Table 3 also
gives the results of Moses (Koehn et al, 2007) and
an in-house hierarchical phrase-based system (Chi-
ang, 2007). Our STAG system achieves compara-
ble performance with the hierarchical system. The
absolute improvement of +0.7 BLEU over STSG is
close to the finding of DeNeefe and Knight (2009)
on string-to-tree translation. We feel that one major
obstacle for achieving further improvement is that
composed rules generated on the fly during decod-
ing (e.g., r1 + r3 + r5 in Figure 4) usually have too
many non-terminals, making cube pruning in the in-
1285
STSG STAG
matching 0.086 0.109
conversion 0.000 0.562
intersection 0.946 1.064
other 0.012 0.028
total 1.044 1.763
Table 4: Comparison of average decoding time.
tersection phase suffering from severe search errors
(only a tiny fraction of the search space can be ex-
plored). To produce the 1-best translations on the
MT05 test set that contains 1,082 sentences, while
the STSG system used 40,169 initial rules without
adjoining sites, the STAG system used 28,046 initial
rules without adjoining sites, 1,057 initial rules with
adjoining sites, and 1,527 auxiliary rules.
Table 4 shows the average decoding time on the
MT05 test set. While rule matching for STSG needs
0.086 second per sentence, the matching time for
STAG only increases to 0.109 second. For STAG,
the conversion of derivation forests to translation
forests takes 0.562 second when we restrict that at
most 200 rules can be generated on the fly for each
node. As we use cube pruning, although the trans-
lation forest of STAG is bigger than that of STSG,
the intersection time barely increases. In total, the
STAG system runs in 1.763 seconds per sentence,
only 1.6 times slower than the baseline system.
6 Conclusion
We have presented a new tree-to-string translation
system based on synchronous TAG. With translation
rules learned from Treebank-style trees, the adjoin-
ing tree-to-string system outperforms the baseline
system using STSG without significant loss in effi-
ciency. We plan to introduce left-to-right target gen-
eration (Huang and Mi, 2010) into the STAG tree-
to-string system. Our work can also be extended to
forest-based rule extraction and decoding (Mi et al,
2008; Mi and Huang, 2008). It is also interesting to
introduce STAG into tree-to-tree translation (Zhang
et al, 2008; Liu et al, 2009; Chiang, 2010).
Acknowledgements
The authors were supported by National Natural
Science Foundation of China Contracts 60736014,
60873167, and 60903138. We thank the anonymous
reviewers for their insightful comments.
References
Anne Abeille, Yves Schabes, and Aravind Joshi. 1990.
Using lexicalized tags for machine translation. In
Proc. of COLING 1990.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of tags from the penn treebank. In Proc. of
IWPT 2000.
David Chiang. 2003. Statistical parsing with an au-
tomatically extracted tree adjoining grammar. Data-
Oriented Parsing.
David Chiang. 2006. An introduction to synchronous
grammars. ACL Tutorial.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. of ACL 2010.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4).
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proc. of
EMNLP 2009.
Mark Dras. 1999. A meta-level grammar: Redefining
synchronous tag for translation and paraphrase. In
Proc. of ACL 1999.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. of ACL 2003.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of NAACL 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc. of
ACL 2006.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL 2007.
Liang Huang and Haitao Mi. 2010. Efficient incremen-
tal decoding for tree-to-string translation. In Proc. of
EMNLP 2010.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Aravind Joshi, L. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal of Computer and System
Sciences, 10(1).
Aravind Joshi. 1985. How much contextsensitiv-
ity is necessary for characterizing structural descrip-
tions)tree adjoining grammars. Natural Language
1286
Processing)Theoretical, Computational, and Psy-
chological Perspectives.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL 2007 (poster), pages 77?80, Prague,
Czech Republic, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of ACL 2006.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc. of
ACL 2009.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP 2008.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL/HLT 2008,
pages 192?199, Columbus, Ohio, USA, June.
Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Proc.
of AMTA 2006.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of ACL 2003.
Gilles Prigent. 1994. Synchronous tags and machine
translation. In Proc. of TAG+3.
Yves Schabes and Richard Waters. 1995. A cubic-time,
parsable formalism that lexicalizes context-free gram-
mar without changing the trees produced. Computa-
tional Linguistics, 21(4).
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proc. of COLING 1990.
Stuart M. Shieber. 2007. Probabilistic synchronous tree-
adjoining grammars for machine translation: The ar-
gument from bilingual dictionaries. In Proc. of SSST
2007.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of ICSLP 2002,
pages 901?904.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proc. of the Fifth Natural Lan-
guage Processing Pacific Rim Symposium.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL 2006.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. of ACL 2008.
1287
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761?769,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discriminative Learning with Natural Annotations:
Word Segmentation as a Case Study
Wenbin Jiang 1 Meng Sun 1 Yajuan Lu? 1 Yating Yang 2 Qun Liu 3, 1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{jiangwenbin, sunmeng, lvyajuan}@ict.ac.cn
2Multilingual Information Technology Research Center
The Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences
yangyt@ms.xjb.ac.cn
3Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Structural information in web text pro-
vides natural annotations for NLP prob-
lems such as word segmentation and pars-
ing. In this paper we propose a discrim-
inative learning algorithm to take advan-
tage of the linguistic knowledge in large
amounts of natural annotations on the In-
ternet. It utilizes the Internet as an external
corpus with massive (although slight and
sparse) natural annotations, and enables a
classifier to evolve on the large-scaled and
real-time updated web text. With Chinese
word segmentation as a case study, exper-
iments show that the segmenter enhanced
with the Chinese wikipedia achieves sig-
nificant improvement on a series of testing
sets from different domains, even with a
single classifier and local features.
1 Introduction
Problems related to information retrieval, machine
translation and social computing need fast and ac-
curate text processing, for example, word segmen-
tation and parsing. Taking Chinese word seg-
mentation for example, the state-of-the-art mod-
els (Xue and Shen, 2003; Ng and Low, 2004;
Gao et al, 2005; Nakagawa and Uchimoto, 2007;
Zhao and Kit, 2008; Jiang et al, 2009; Zhang and
Clark, 2010; Sun, 2011b; Li, 2011) are usually
trained on human-annotated corpora such as the
Penn Chinese Treebank (CTB) (Xue et al, 2005),
and perform quite well on corresponding test sets.
Since the text used for corpus annotating are usu-
ally drawn from specific fields (e.g. newswire or
finance), and the annotated corpora are limited in
 think that NLP                  has already ...
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(a) Natural annotation by hyperlink
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(b) Knowledge for word segmentation
(c) Knowledge for dependency parsing
Figure 1: Natural annotations for word segmenta-
tion and dependency parsing.
size (e.g. tens of thousands), the performance of
word segmentation tends to degrade sharply when
applied to new domains.
Internet provides large amounts of raw text, and
statistics collected from it have been used to im-
prove parsing performance (Nakov and Hearst,
2005; Pitler et al, 2010; Bansal and Klein, 2011;
Zhou et al, 2011). The Internet alo gives mas-
sive (although slight and sparse) natural annota-
tions in the forms of structural information includ-
ing hyperlinks, fonts, colors and layouts (Sun,
2011a). These annotations usually imply valuable
knowledge for problems such as word segmen-
tation and parsing, based on the hypothesis that
the subsequences marked by structural informa-
tion are meaningful fragments in sentences. Fig-
ure 1 shows an example. The hyperlink indicates
761
a Chinese phrase (meaning NLP), and it probably
corresponds to a connected sub-graph for depen-
dency parsing. Creators of web text give valuable
annotations during editing, the whole Internet can
be treated as a wide-coveraged and real-time up-
dated corpus.
Different from the dense and accurate annota-
tions in human-annotated corpora, natural annota-
tions in web text are sparse and slight, it makes
direct training of NLP models impracticable. In
this work we take for example a most important
problem, word segmentation, and propose a novel
discriminative learning algorithm to leverage the
knowledge in massive natural annotations of web
text. Character classification models for word seg-
mentation usually factorize the whole prediction
into atomic predictions on characters (Xue and
Shen, 2003; Ng and Low, 2004). Natural anno-
tations in web text can be used to get rid of im-
plausible predication candidates for related char-
acters, knowledge in the natural annotations is
therefore introduced in the manner of searching
space pruning. Since constraint decoding in the
pruned searching space integrates the knowledge
of the baseline model and natural annotations, it
gives predictions not worse than the normal decod-
ing does. Annotation differences between the out-
puts of constraint decoding and normal decoding
are used to train the enhanced classifier. This strat-
egy makes the usage of natural annotations simple
and universal, which facilitates the utilization of
massive web text and the extension to other NLP
problems.
Although there are lots of choices, we choose
the Chinese wikipedia as the knowledge source
due to its high quality. Structural information, in-
cluding hyperlinks, fonts and colors are used to de-
termine the boundaries of meaningful fragments.
Experimental results show that, the knowledge im-
plied in the natural annotations can significantly
improve the performance of a baseline segmenter
trained on CTB 5.0, an F-measure increment of
0.93 points on CTB test set, and an average incre-
ment of 1.53 points on 7 other domains. It is an ef-
fective and inexpensive strategy to build word seg-
menters adaptive to different domains. We hope to
extend this strategy to other NLP problems such
as named entity recognition and parsing.
In the rest of the paper, we first briefly intro-
duce the problems of Chinese word segmentation
and the character classification model in section
Type Templates Instances
n-gram C?2 C?2=@
C?1 C?1=?
C0 C0=g
C1 C1=,
C2 C2=?
C?2C?1 C?2C?1=@?
C?1C0 C?1C0=?g
C0C1 C0C1=g,
C1C2 C1C2=,?
C?1C1 C?1C1=?,
function Pu(C0) Pu(C0)=false
T (C?2:2) T (C?2:2)= 44444
Table 1: Feature templates and instances for
character classification-based word segmentation
model. Suppose we are considering the i-th char-
acter ?g? in ?...@? g ,???n??...?.
2, then describe the representation of the knowl-
edge in natural annotations of web text in section
3, and finally detail the strategy of discriminative
learning on natural annotations in section 4. Af-
ter giving the experimental results and analysis in
section 5, we briefly introduce the previous related
work and then give the conclusion and the expec-
tation of future research.
2 Character Classification Model
Character classification models for word segmen-
tation factorize the whole prediction into atomic
predictions on single characters (Xue and Shen,
2003; Ng and Low, 2004). Although natural anno-
tations in web text do not directly support the dis-
criminative training of segmentation models, they
do get rid of the implausible candidates for predic-
tions of related characters.
Given a sentence as a sequence of n charac-
ters, word segmentation splits the sequence into
m(? n) subsequences, each of which indicates a
meaningful word. Word segmentation can be for-
malized as a character classification problem (Xue
and Shen, 2003), where each character in the sen-
tence is given a boundary tag representing its posi-
tion in a word. We adopt the boundary tags of Ng
and Low (2004), b, m, e and s, where b, m and
e mean the beginning, the middle and the end of a
word, and s indicates a single-character word. the
decoding procedure searches for the labeled char-
acter sequence y that maximizes the score func-
762
Algorithm 1 Perceptron training algorithm.
1: Input: Training corpus C
2: ~?? 0
3: for t? 1 .. T do ? T iterations
4: for (x, y?) ? C do
5: y ? argmaxy ?(x, y) ? ~?
6: if y 6= y? then
7: ~?? ~?+?(x, y?)? ?(x, y)
8: Output: Parameters ~?
tion:
f(x) = argmax
y
S(y|~?,?, x)
= argmax
y
?(x, y) ? ~?
= argmax
y
?
(i,t)?y
?(i, t, x, y) ? ~?
(1)
The score of the whole sequence y is accumulated
across all its character-label pairs, (i, t) ? y (s.t.
1 ? i ? n and t ? {b,m, e, s}). The feature
function ? maps a labeled sequence or a character-
label pair into a feature vector, ~? is the parame-
ter vector and ?(x, y) ? ~? is the inner product of
?(x, y) and ~?.
Analogous to other sequence labeling prob-
lems, word segmentation can be solved through a
viterbi-style decoding procedure. We omit the de-
coding algorithm in this paper due to its simplicity
and popularity.
The feature templates for the classifier is shown
in Table 1. C0 denotes the current character, while
C?k/Ck denote the kth character to the left/right
of C0. The function Pu(?) returns true for a punc-
tuation character and false for others, the function
T (?) classifies a character into four types, 1, 2, 3
and 4, representing number, date, English letter
and others, respectively.
The classifier can be trained with online learn-
ing algorithms such as perceptron, or offline learn-
ing models such as support vector machines.
We choose the perceptron algorithm (Collins,
2002) to train the classifier for the character
classification-based word segmentation model. It
learns a discriminative model mapping from the
inputs x ? X to the outputs y? ? Y , where X is the
set of sentences in the training corpus and Y is the
set of corresponding labeled results. Algorithm 1
shows the perceptron algorithm for tuning the pa-
rameter ~?. The ?averaged parameters? technology
(Collins, 2002) is used for better performance.
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(a) Original searching space
n
n
n
n
n
n
n
n
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(b) Shrinked searching space
n
n
n
n
n
n
n
n
b
m
e
s
e
s
b
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
e
s
b
s
b
m
e
s
Figure 2: Shrink of searching space for the charac-
ter classification-based word segmentation model.
3 Knowledge in Natural Annotations
Web text gives massive natural annotations in the
form of structural informations, including hyper-
links, fonts, colors and layouts (Sun, 2011a). Al-
though slight and sparse, these annotations imply
valuable knowledge for problems such as word
segmentation and parsing.
As shown in Figure 1, the subsequence P =
i..j of sentence S is composed of bolded charac-
ters determined by a hyperlink. Such natural anno-
tations do not clearly give each character a bound-
ary tag, or define the head-modifier relationship
between two words. However, they do help to
shrink the set of plausible predication candidates
for each character or word. For word segmenta-
tion, it implies that characters i ? 1 and j are the
rightmost characters of words, while characters i
and j + 1 are the leftmost characters of words.
For i ? 1 or j, the plausible predication set ? be-
comes {e, s}; For i and j + 1, it becomes {b, s};
For other characters c except the two at sentence
boundaries, ?(c) is still {b,m, e, s}. For depen-
dency parsing, the subsequence P tends to form
a connected dependency graph if it contains more
than one word. Here we use ? to denote the set of
plausible head of a word (modifier). There must
be a single word w ? P as the root of subse-
quence P , whose plausible heads fall out of P ,
that is, ?(w) = {x|x ? S ? P}. For the words
in P except the root, the plausible heads for each
763
Algorithm 2 Perceptron learning with natural an-
notations.
1: ~?? TRAIN(C)
2: for x ? F do
3: y ? DECODE(x, ~?)
4: y? ? CONSTRAINTDECODE(x, ~?,?)
5: if y 6= y? then
6: C? ? C? ? {y?}
7: ~?? TRAIN(C ? C?)
word w are the words in P except w itself, that is,
?(w) = {x|x ? P ? {w}}.
Creators of web text give valuable structural
annotations during editing, these annotations re-
duce the predication uncertainty for atomic char-
acters or words, although not exactly defining
which predication is. Figure 2 shows an exam-
ple for word segmentation, depicting the shrink
of searching space for the character classification-
based model. Since the decrement of uncertainty
indicates the increment of knowledge, the whole
Internet can be treated as a wide-coveraged and
real-time updated corpus. We choose the Chinese
wikipedia as the external knowledge source, and
structural information including hyperlinks, fonts
and colors are used in the current work due to their
explicitness of representation.
4 Learning with Natural Annotations
Different from the dense and accurate annotations
in human-annotated corpora, natural annotations
are sparse and slight, which makes direct training
of NLP models impracticable. Annotations im-
plied by structural information do not give an ex-
act predication to a character, however, they help
to get rid of the implausible predication candidates
for related characters, as described in the previous
section.
Previous work on constituency parsing or ma-
chine translation usually resort to some kinds of
heuristic tricks, such as punctuation restrictions,
to eliminate some implausible candidates during
decoding. Here the natural annotations also bring
knowledge in the manner of searching space prun-
ing. Conditioned on the completeness of the de-
coding algorithm, a model trained on an exist-
ing corpus probably gives better or at least not
worse predications, by constraint decoding in the
pruned searching space. The constraint decoding
procedure integrates the knowledge of the baseline
Algorithm 3 Online version of perceptron learn-
ing with natural annotations.
1: ~?? TRAIN(C)
2: for x with natural annotations do
3: y ? DECODE(x, ~?)
4: y? ? CONSTRAINTDECODE(x, ~?,?)
5: if y 6= y? then
6: ~?? ~? +?(x, y?)??(x, y)
7: output ~? at regular time
model and natural annotations, the predication dif-
ferences between the outputs of constraint decod-
ing and normal decoding can be used to train the
enhanced classifier.
Restrictions of the searching space according to
natural annotations can be easily incorporated into
the decoder. If the completeness of the searching
algorithm can be guaranteed, the constraint decod-
ing in the pruned searching space will give predi-
cations not worse than those given by the normal
decoding. If a predication of constraint decoding
differs from that of normal decoding, it indicates
that the annotation precision is higher than the lat-
ter. Furthermore, the degree of difference between
the two predications represents the amount of new
knowledge introduced by the natural annotations
over the baseline.
The baseline model ~? is trained on an exist-
ing human-annotated corpus. A set of sentences
F with natural annotations are extracted from the
Chinese wikipedia, and we reserve the ones for
which constraint decoding and normal decoding
give different predications. The predictions of re-
served sentences by constraint decoding are used
as additional training data for the enhanced classi-
fier. The overall training pipeline is analogous to
self-training (McClosky et al, 2006), Algorithm
2 shows the pseudo-codes. Considering the online
characteristic of the perceptron algorithm, if we
are able to leverage much more (than the Chinese
wikipedia) data with natural annotations, an online
version of learning procedure shown in Algorithm
3 would be a better choice. The technology of ?av-
eraged parameters? (Collins, 2002) is easily to be
adapted here for better performance.
When constraint decoding and normal decod-
ing give different predications, we only know that
the former is probably better than the latter. Al-
though there is no explicit evidence for us to mea-
sure how much difference in accuracy between the
764
Partition Sections # of word
CTB
Training 1? 270 0.47M
400 ? 931
1001 ? 1151
Developing 301 ? 325 6.66K
Testing 271 ? 300 7.82K
Table 2: Data partitioning for CTB 5.0.
two predications, we can approximate how much
new knowledge that a naturally annotated sentence
brings. For a sentence x, given the predications of
constraint decoding and normal decoding, y? and
y, the difference of their scores ? = S(y) ? S(y?)
indicates the degree to which the current model
mistakes. This indicator helps us to select more
valuable training examples.
The strategy of learning with natural annota-
tions can be adapted to other situations. For ex-
ample, if we have a list of words or phrases (espe-
cially in a specific domain such as medicine and
chemical), we can generate annotated sentences
automatically by string matching in a large amount
of raw text. It probably provides a simple and
effective domain adaptation strategy for already
trained models.
5 Experiments
We use the Penn Chinese Treebank 5.0 (CTB)
(Xue et al, 2005) as the existing annotated cor-
pus for Chinese word segmentation. For conve-
nient of comparison with other work in word seg-
mentation, the whole corpus is split into three par-
titions as follows: chapters 271-300 for testing,
chapters 301-325 for developing, and others for
training. We choose the Chinese wikipedia 1 (ver-
sion 20120812) as the external knowledge source,
because it has high quality in contents and it is
much better than usual web text. Structural infor-
mations, including hyperlinks, fonts and colors are
used to derive the annotation information.
To further evaluate the improvement brought
by the fuzzy knowledge in Chinese wikipedia, a
series of testing sets from different domains are
adopted. The four testing sets from SIGHAN
Bakeoff 2010 (Zhao and Liu, 2010) are used, they
are drawn from the domains of literature, finance,
computer science and medicine. Although the ref-
erence sets are annotated according to a different
1http://download.wikimedia.org/backup-index.html.
 95.6
 95.8
 96
 96.2
 96.4
 96.6
 96.8
 97
 97.2
 97.4
 1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
1%
)
Training iterations
Figure 3: Learning curve of the averaged percep-
tron classifier on the CTB developing set.
word segmentation standard (Yu et al, 2001), the
quantity of accuracy improvement is still illustra-
tive since there are no vast diversities between the
two segmentation standards. We also annotated
another three testing sets 2, their texts are drawn
from the domains of chemistry, physics and ma-
chinery, and each contains 500 sentences.
5.1 Baseline Classifier for Word
Segmentation
We train the baseline perceptron classifier for
word segmentation on the training set of CTB
5.0, using the developing set to determine the
best training iterations. The performance mea-
surement for word segmentation is balanced F-
measure, F = 2PR/(P +R), a function of preci-
sion P and recall R, where P is the percentage of
words in segmentation results that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Figure 3 shows the learning curve of the aver-
aged perceptron on the developing set. The sec-
ond column of Table 3 lists the performance of
the baseline classifier on eight testing sets, where
newswire denotes the testing set of the CTB it-
self. The classifier performs much worse on the
domains of chemistry, physics and machinery, it
indicates the importance of domain adaptation for
word segmentation (Gao et al, 2004; Ma and
Way, 2009; Gao et al, 2010). The accuracy on the
testing sets from SIGHAN Bakeoff 2010 is even
lower due to the difference in both domains and
word segmentation standards.
2They are available at http://nlp.ict.ac.cn/ jiangwenbin/.
765
Dataset Baseline (F%) Enhanced (F%)
Newswire 97.35 98.28 +0.93
Out-of-Domain
Chemistry 93.61 95.68 +2.07
Physics 95.10 97.24 +2.14
Machinery 96.08 97.66 +1.58
Literature 92.42 93.53 +1.11
Finance 92.50 93.16 +0.66
Computer 89.46 91.19 +1.73
Medicine 91.88 93.34 +1.46
Average 93.01 94.54 +1.53
Table 3: Performance of the baseline classifier and
the classifier enhanced with natural annotations in
Chinese wikipedia.
5.2 Classifier Enhanced with Natural
Annotations
The Chinese wikipedia contains about 0.5 million
items. From their description text, about 3.9 mil-
lions of sentences with natural annotations are ex-
tracted. With the CTB training set as the exist-
ing corpus C, about 0.8 million sentences are re-
served according to Algorithm 2, the segmenta-
tions given by constraint decoding are used as ad-
ditional training data for the enhanced classifier.
According to the previous description, the dif-
ference of the scores of constraint decoding and
normal decoding, ? = S(y) ? S(y?), indicates
the importance of a constraint segmentation to the
improvement of the baseline classifier. The con-
straint segmentations of the reserved sentences are
sorted in descending order according to the dif-
ference of the scores of constraint decoding and
normal decoding, as described previously. From
the beginning of the sorted list, different amounts
of segmented sentences are used as the additional
training data for the enhanced character classifier.
Figure 4 shows the performance curve of the en-
hanced classifiers on the developing set of CTB.
We found that the highest accuracy was achieved
when 160, 000 sentences were used, while more
additional training data did not give continuous
improvement. A recent related work about self-
training for segmentation (Liu and Zhang, 2012)
also reported a very similar trend, that only a mod-
erate amount of raw data gave the most obvious
improvements.
The performance of the enhanced classifier is
listed in the third column of Table 3. On the
CTB testing set, training data from the Chinese
 97.1
 97.2
 97.3
 97.4
 97.5
 97.6
 97.7
 97.8
Ac
cu
ra
cy
 (F
1%
)
Count of selected sentences
10000 20000 40000 80000 160000 320000 640000
using selected sentences
using all sentences
Figure 4: Performance curve of the classifier en-
hanced with selected sentences of different scales.
Model Accuracy (F%)
(Jiang et al, 2008) 97.85
(Kruengkrai et al, 2009) 97.87
(Zhang and Clark, 2010) 97.79
(Wang et al, 2011) 98.11
(Sun, 2011b) 98.17
Our Work 98.28
Table 4: Comparison with state-of-the-art work in
Chinese word segmentation.
wikipedia brings an F-measure increment of 0.93
points. On out-of-domain testing sets, the im-
provements are much larger, an average increment
of 1.53 points is achieved on seven domains. It
is probably because the distribution of the knowl-
edge in the CTB training data is concentrated in
the domain of newswire, while the contents of
the Chinese wikipedia cover a broad range of do-
mains, it provides knowledge complementary to
that of CTB.
Table 4 shows the comparison with other
work in Chinese word segmentation. Our model
achieves an accuracy higher than that of the
state-of-the-art models trained on CTB only, al-
though using a single classifier with only local
features. From the viewpoint of resource uti-
lization, the comparison between our system and
previous work without using additional training
data is unfair. However, we believe this work
shows another interesting way to improve Chi-
nese word segmentation, it focuses on the utiliza-
tion of fuzzy and sparse knowledge on the Internet
rather than making full use of a specific human-
annotated corpus. On the other hand, since only
a single classifier and local features are used in
our method, better performance could be achieved
766
resorting to complicated features, system com-
bination and other semi-supervised technologies.
What is more, since the text on Internet is wide-
coveraged and real-time updated, our strategy also
helps a word segmenter be more domain adaptive
and up to date.
6 Related Work
Li and Sun (2009) extracted character classifi-
cation instances from raw text for Chinese word
segmentation, resorting to the indication of punc-
tuation marks between characters. Sun and Xu
(Sun and Xu, 2011) utilized the features derived
from large-scaled unlabeled text to improve Chi-
nese word segmentation. Although the two work
also made use of large-scaled raw text, our method
is essentially different from theirs in the aspects
of both the source of knowledge and the learning
strategy.
Lots of efforts have been devoted to semi-
supervised methods in sequence labeling and word
segmentation (Xu et al, 2008; Suzuki and Isozaki,
2008; Haffari and Sarkar, 2008; Tomanek and
Hahn, 2009; Wang et al, 2011). A semi-
supervised method tries to find an optimal hyper-
plane of both annotated data and raw data, thus to
result in a model with better coverage and higher
accuracy. Researchers have also investigated un-
supervised methods in word segmentation (Zhao
and Kit, 2008; Johnson and Goldwater, 2009;
Mochihashi et al, 2009; Hewlett and Cohen,
2011). An unsupervised method mines the latent
distribution regularity in the raw text, and auto-
matically induces word segmentation knowledge
from it. Our method also needs large amounts of
external data, but it aims to leverage the knowl-
edge in the fuzzy and sparse annotations. It is
fundamentally different from semi-supervised and
unsupervised methods in that we aimed to exca-
vate a totally different kind of knowledge, the nat-
ural annotations implied by the structural informa-
tion in web text.
In recent years, much work has been devoted to
the improvement of word segmentation in a vari-
ety of ways. Typical approaches include the in-
troduction of global training or complicated fea-
tures (Zhang and Clark, 2007; Zhang and Clark,
2010), the investigation of word internal structures
(Zhao, 2009; Li, 2011), the adjustment or adapta-
tion of word segmentation standards (Wu, 2003;
Gao et al, 2004; Jiang et al, 2009), the integrated
solution of segmentation and related tasks such as
part-of-speech tagging and parsing (Zhou and Su,
2003; Zhang et al, 2003; Fung et al, 2004; Gold-
berg and Tsarfaty, 2008), and the strategies of hy-
brid or stacked modeling (Nakagawa and Uchi-
moto, 2007; Kruengkrai et al, 2009; Wang et al,
2010; Sun, 2011b).
In parsing, Pereira and Schabes (1992) pro-
posed an extended inside-outside algorithm that
infers the parameters of a stochastic CFG from a
partially parsed treebank. It uses partial bracket-
ing information to improve parsing performance,
but it is specific to constituency parsing, and its
computational complexity makes it impractical for
massive natural annotations in web text. There
are also work making use of word co-occurrence
statistics collected in raw text or Internet n-grams
to improve parsing performance (Nakov and
Hearst, 2005; Pitler et al, 2010; Zhou et al, 2011;
Bansal and Klein, 2011). When enriching the re-
lated work during writing, we found a work on de-
pendency parsing (Spitkovsky et al, 2010) who
utilized parsing constraints derived from hypertext
annotations to improve the unsupervised depen-
dency grammar induction. Compared with their
method, the strategy we proposed is formal and
universal, the discriminative learning strategy and
the quantitative measurement of fuzzy knowledge
enable more effective utilization of the natural an-
notation on the Internet when adapted to parsing.
7 Conclusion and Future Work
This work presents a novel discriminative learning
algorithm to utilize the knowledge in the massive
natural annotations on the Internet. Natural anno-
tations implied by structural information are used
to decrease the searching space of the classifier,
then the constraint decoding in the pruned search-
ing space gives predictions not worse than the nor-
mal decoding does. Annotation differences be-
tween the outputs of constraint decoding and nor-
mal decoding are used to train the enhanced classi-
fier, linguistic knowledge in the human-annotated
corpus and the natural annotations of web text
are thus integrated together. Experiments on Chi-
nese word segmentation show that, the enhanced
word segmenter achieves significant improvement
on testing sets of different domains, although us-
ing a single classifier with only local features.
Since the contents of web text cover a broad
range of domains, it provides knowledge comple-
767
mentary to that of human-annotated corpora with
concentrated distribution of domains. The content
on the Internet is large-scaled and real-time up-
dated, it compensates for the drawback of expen-
sive building and updating of corpora. Our strat-
egy, therefore, enables us to build a classifier more
domain adaptive and up to date. In the future, we
will compare this method with self-training to bet-
ter illustrate the importance of boundary informa-
tion, and give error analysis on what types of er-
rors are reduced by the method to make this inves-
tigation more complete. We will also investigate
more efficient algorithms to leverage more mas-
sive web text with natural annotations, and further
extend the strategy to other NLP problems such as
named entity recognition and parsing.
Acknowledgments
The authors were supported by National
Natural Science Foundation of China (Con-
tracts 61202216), 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&D Program (No. 2012BAH39B03). Qun Liu?s
work was partially supported by Science Foun-
dation Ireland (Grant No.07/CE/I1142) as part
of the CNGL at Dublin City University. Sincere
thanks to the three anonymous reviewers for their
thorough reviewing and valuable suggestions!
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8, Philadelphia, USA.
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy chinese
parser augmented by transformation-based learning.
In Proceedings of TALIP.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceed-
ings of ACL.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Computational Linguistics.
Wenjun Gao, Xipeng Qiu, and Xuanjing Huang. 2010.
Adaptive chinese word segmentation with online
passive-aggressive algorithm. In Proceedings of
CIPS-SIGHAN Workshop.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings of ACL-HLT.
Gholamreza Haffari and Anoop Sarkar. 2008.
Homotopy-based semi-supervised hidden markov
models for sequence labeling. In Proceedings of
COLING.
Daniel Hewlett and Paul Cohen. 2011. Fully unsu-
pervised word segmentation with bve and mdl. In
Proceedings of ACL.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adaptor
grammars. In Proceedings of NAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun.ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of ACL-IJCNLP.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmenta-
tion. In Proceedings of ACL.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and pos-tagging.
In Proceedings of COLING.
Yanjun Ma and Andy Way. 2009. Bilingually moti-
vated domain-adapted word segmentation for statis-
tical machine translation. In Proceedings of EACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT-NAACL.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings of ACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL.
Preslav Nakov and Marti Hearst. 2005. Using the
web as an implicit training set: Application to struc-
tural ambiguity resolution. In Proceedings of HLT-
EMNLP.
768
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of ACL.
Emily Pitler, Shane Bergsma, Dekang Lin, and Ken-
neth Church. 2010. Using web-scale n-grams to
improve base np parsing performance. In Proceed-
ings of COLING.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
ACL.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP.
Maosong Sun. 2011a. Natural language processing
based on naturally annotated web resources. CHI-
NESE INFORMATION PROCESSING.
Weiwei Sun. 2011b. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
Proceedings of ACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of COLING.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of IJC-
NLP.
Andi Wu. 2003. Customizable segmentation of mor-
phologically derived words in chinese. Computa-
tional Linguistics and Chinese Language Process-
ing.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of ACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and pos-tagging using
a single discriminative model. In Proceedings of
EMNLP.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of SIGHAN Workshop.
Hai Zhao and Chunyu Kit. 2008. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named entity
recognition. In Proceedings of SIGHAN Workshop.
Hongmei Zhao and Qun Liu. 2010. The cips-sighan
clp 2010 chinese word segmentation bakeoff. In
Proceedings of CIPS-SIGHAN Workshop.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
EACL.
Guodong Zhou and Jian Su. 2003. A chinese effi-
cient analyser integrating word segmentation, part-
ofspeech tagging, partial parsing and full parsing. In
Proceedings of SIGHAN Workshop.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
Proceedings of ACL.
769
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063?1072,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bilingually-Guided Monolingual Dependency Grammar Induction
Kai Liu??, Yajuan Lu??, Wenbin Jiang?, Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{liukai,lvyajuan,jiangwenbin,liuqun}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
?University of Chinese Academy of Sciences
Abstract
This paper describes a novel strategy for
automatic induction of a monolingual de-
pendency grammar under the guidance
of bilingually-projected dependency. By
moderately leveraging the dependency in-
formation projected from the parsed coun-
terpart language, and simultaneously min-
ing the underlying syntactic structure of
the language considered, it effectively in-
tegrates the advantages of bilingual pro-
jection and unsupervised induction, so as
to induce a monolingual grammar much
better than previous models only using
bilingual projection or unsupervised in-
duction. We induced dependency gram-
mar for five different languages under the
guidance of dependency information pro-
jected from the parsed English translation,
experiments show that the bilingually-
guided method achieves a significant
improvement of 28.5% over the unsuper-
vised baseline and 3.0% over the best pro-
jection baseline on average.
1 Introduction
In past decades supervised methods achieved the
state-of-the-art in constituency parsing (Collins,
2003; Charniak and Johnson, 2005; Petrov et al,
2006) and dependency parsing (McDonald et al,
2005a; McDonald et al, 2006; Nivre et al, 2006;
Nivre et al, 2007; Koo and Collins, 2010). For
supervised models, the human-annotated corpora
on which models are trained, however, are expen-
sive and difficult to build. As alternative strate-
gies, methods which utilize raw texts have been in-
vestigated recently, including unsupervised meth-
ods which use only raw texts (Klein and Man-
ning, 2004; Smith and Eisner, 2005; William et
al., 2009), and semi-supervised methods (Koo et
al., 2008) which use both raw texts and annotat-
ed corpus. And there are a lot of efforts have also
been devoted to bilingual projection (Chen et al,
2010), which resorts to bilingual text with one lan-
guage parsed, and projects the syntactic informa-
tion from the parsed language to the unparsed one
(Hwa et al, 2005; Ganchev et al, 2009).
In dependency grammar induction, unsuper-
vised methods achieve continuous improvements
in recent years (Klein and Manning, 2004; Smith
and Eisner, 2005; Bod, 2006; William et al, 2009;
Spitkovsky et al, 2010). Relying on a predefined
distributional assumption and iteratively maximiz-
ing an approximate indicator (entropy, likelihood,
etc.), an unsupervised model usually suffers from
two drawbacks, i.e., lower performance and high-
er computational cost. On the contrary, bilin-
gual projection (Hwa et al, 2005; Smith and Eis-
ner, 2009; Jiang and Liu, 2010) seems a promis-
ing substitute for languages with a
large amount of bilingual sentences and an exist-
ing parser of the counterpart language. By project-
ing syntactic structures directly (Hwa et al, 2005;
Smith and Eisner, 2009; Jiang and Liu, 2010)
across bilingual texts or indirectly across multi-
lingual texts (Snyder et al, 2009; McDonald et
al., 2011; Naseem et al, 2012), a better depen-
dency grammar can be easily induced, if syntactic
isomorphism is largely maintained between target
and source languages.
Unsupervised induction and bilingual projec-
tion run according to totally different principles,
the former mines the underlying structure of the
monolingual language, while the latter leverages
the syntactic knowledge of the parsed counter-
1063
Bilingual corpus Joint Optimization
Bilingually-guided
Parsing model
Unsupervised
objective
Projection
objective
Random
Treebank
Evolved
treebank
Target
sentences
Source
sentences projection
Figure 1: Training the bilingually-guided parsing model by iteration.
part language. Considering this, we propose a
novel strategy for automatically inducing a mono-
lingual dependency grammar under the guidance
of bilingually-projected dependency information,
which integrates the advantage of bilingual pro-
jection into the unsupervised framework. A
randomly-initialized monolingual treebank
evolves in a self-training iterative procedure, and
the grammar parameters are tuned to simultane-
ously maximize both the monolingual likelihood
and bilingually-projected likelihood of the evolv-
ing treebank. The monolingual likelihood is sim-
ilar to the optimization objectives of convention-
al unsupervised models, while the bilingually-
projected likelihood is the product of the projected
probabilities of dependency trees. By moderately
leveraging the dependency information projected
from the parsed counterpart language, and simul-
taneously mining the underlying syntactic struc-
ture of the language considered, we can automat-
ically induce a monolingual dependency grammar
which is much better than previous models only
using bilingual projection or unsupervised induc-
tion. In addition, since both likelihoods are fun-
damentally factorized into dependency edges (of
the hypothesis tree), the computational complexi-
ty approaches to unsupervised models, while with
much faster convergence. We evaluate the final
automatically-induced dependency parsing mod-
el on 5 languages. Experimental results show
that our method significantly outperforms previ-
ous work based on unsupervised method or indi-
rect/direct dependency projection, where we see
an average improvement of 28.5% over unsuper-
vised baseline on all languages, and the improve-
ments are 3.9%/3.0% over indirect/direct base-
lines. And our model achieves the most signif-
icant gains on Chinese, where the improvements
are 12.0%, 4.5% over indirect and direct projec-
tion baselines respectively.
In the rest of the paper, we first describe the un-
supervised dependency grammar induction frame-
work in section 2 (where the unsupervised op-
timization objective is given), and introduce the
bilingual projection method for dependency pars-
ing in section 3 (where the projected optimiza-
tion objective is given); Then in section 4 we
present the bilingually-guided induction strategy
for dependency grammar (where the two objec-
tives above are jointly optimized, as shown in Fig-
ure 1). After giving a brief introduction of previ-
ous work in section 5, we finally give the experi-
mental results in section 6 and conclude our work
in section 7.
2 Unsupervised Dependency Grammar
Induction
In this section, we introduce the unsupervised ob-
jective and the unsupervised training algorithm
which is used as the framework of our bilingually-
guided method. Unlike previous unsupervised
work (Klein and Manning, 2004; Smith and Eis-
ner, 2005; Bod, 2006), we select a self-training
approach (similar to hard EM method) to train
the unsupervised model. And the framework of
our unsupervised model builds a random treebank
on the monolingual corpus firstly for initialization
and trains a discriminative parsing model on it.
Then we use the parser to build an evolved tree-
bank with the 1-best result for the next iteration
run. In this way, the parser and treebank evolve in
an iterative way until convergence. Let?s introduce
the parsing objective firstly:
Define ei as the ith word in monolingual sen-
tence E; deij denotes the word pair dependency re-
lationship (ei ? ej). Based on the features around
deij , we can calculate the probability Pr(y|deij )
that the word pair deij can form a dependency arc
1064
as:
Pr(y|deij ) =
1
Z(deij )
exp(
?
n
?n ? fn(deij , y)) (1)
where y is the category of the relationship of deij :
y = + means it is the probability that the word
pair deij can form a dependency arc and y = ?
means the contrary. ?n denotes the weight for fea-
ture function fn(deij , y), and the features we used
are presented in Table 1 (Section 6). Z(deij) is a
normalizing constant:
Z(deij ) =
?
y
exp(
?
n
?n ? fn(deij , y)) (2)
Given a sentence E, parsing a dependency tree
is to find a dependency tree DE with maximum
probability PE :
PE = argmax
DE
?
deij?DE
Pr(+|deij ) (3)
2.1 Unsupervised Objective
We select a simple classifier objective function as
the unsupervised objective function which is in-
stinctively in accordance with the parsing objec-
tive:
?(?) =
?
de?DE
Pr(+|de)
?
de?D?E
Pr(?|de) (4)
where E is the monolingual corpus and E ? E,
DE is the treebank that contains all DE in the cor-
pus, and D?E denotes all other possible dependen-
cy arcs which do not exist in the treebank.
Maximizing the Formula (4) is equivalent to
maximizing the following formula:
?1(?) =
?
de?DE
logPr(+|de)
+
?
de?D?E
logPr(?|de)
(5)
Since the size of edges between DE and D?E is
disproportionate, we use an empirical value to re-
duce the impact of the huge number of negative
instances:
?2(?) =
?
de?DE
logPr(+|de)
+ |DE |
|D?E |
?
de?D?E
logPr(?|de)
(6)
where |x| is the size of x.
Algorithm 1 Training unsupervised model
1: build random DE
2: ?? train(DE , D?E)
3: repeat
4: for each E ? E do ? E step
5: DE ? parse(E,?)
6: ?? train(DE , D?E) ? M step
7: until convergence
Bush held talk with Sharona
bushi yu juxingshalong huitanle
? ?
?? ? ???? ???
Figure 2: Projecting a Chinese dependency tree
to English side according to DPA. Solid arrows
are projected dependency arcs; dashed arrows are
missing dependency arcs.
2.2 Unsupervised Training Algorithm
Algorithm 1 outlines the unsupervised training in
its entirety, where the treebank DE and unsuper-
vised parsing model with ? are updated iteratively.
In line 1 we build a random treebank DE on
the monolingual corpus, and then train the parsing
model with it (line 2) through a training procedure
train(?, ?) which needs DE and D?E as classifica-
tion instances. From line 3-7, we train the unsu-
pervised model in self training iterative procedure,
where line 4-5 are similar to the E-step in EM al-
gorithm where calculates objective instead of ex-
pectation of 1-best tree (line 5) which is parsed
according to the parsing objective (Formula 3) by
parsing process parse(?, ?), and update the tree
bank with the tree. Similar to M-step in EM, the
algorithm maximizes the whole treebank?s unsu-
pervised objective (Formula 6) through the train-
ing procedure (line 6).
3 Bilingual Projection of Dependency
Grammar
In this section, we introduce our projection objec-
tive and training algorithm which trains the model
with arc instances.
Because of the heterogeneity between dif-
ferent languages and word alignment errors, pro-
jection methods may contain a lot of noises. Take
Figure 2 as an example, following the Direct
Projection Algorithm (DPA) (Hwa et al, 2005)
(Section 5), the dependency relationships between
words can be directly projected from the source
1065
Algorithm 2 Training projection model
1: DP , DN ? proj(F ,DF , A,E)
2: repeat ? train(DP , DN )
3: ??? grad(DP , DN , ?(?))
4: ?? climb(?,??, ?)
5: until maximization
language to the target language. Therefore, we
can hardly obtain a treebank with complete trees
through direct projection. So we extract projected
discrete dependency arc instances instead of tree-
bank as training set for the projected grammar in-
duction model.
3.1 Projection Objective
Correspondingly, we select an objective which has
the same form with the unsupervised one:
?(?) =
?
de?DP
log Pr(+|de)
+
?
de?DN
logPr(?|de)
(7)
where DP is the positive dependency arc instance
set, which is obtained by direct projection methods
(Hwa et al, 2005; Jiang and Liu, 2010) and DN is
the negative one.
3.2 Projection Algorithm
Basically, the training procedure in line 2,7 of Al-
gorithm 1 can be divided into smaller iterative
steps, and Algorithm 2 outlines the training step
of projection model with instances. F in Algo-
rithm 2 is source sentences in bilingual corpus,
and A is the alignments. Function grad(?, ?, ?)
gives the gradient (??) and the objective is op-
timized with a generic optimization step (such as
an LBFGS iteration (Zhu et al, 1997)) in the sub-
routine climb(?, ?, ?).
4 Bilingually-Guided Dependency
Grammar Induction
This section presents our bilingually-guided gram-
mar induction model, which incorporates unsuper-
vised framework and bilingual projection model
through a joint approach.
According to following observation: unsuper-
vised induction model mines underlying syntactic
structure of the monolingual language, however, it
is hard to find good grammar induction in the ex-
ponential parsing space; bilingual projection ob-
tains relatively reliable syntactic knowledge of the
parsed counterpart, but it possibly contains a lot
of noises (e.g. Figure 2). We believe that unsu-
pervised model and projection model can comple-
ment each other and a joint model which takes bet-
ter use of both unsupervised parse trees and pro-
jected dependency arcs can give us a better parser.
Based on the idea, we propose a nov-
el strategy for training monolingual grammar in-
duction model with the guidance of unsuper-
vised and bilingually-projected dependency infor-
mation. Figure 1 outlines our bilingual-guided
grammar induction process in its entirety. In our
method, we select compatible objectives for unsu-
pervised and projection models, in order to they
can share the same grammar parameters. Then
we incorporate projection model into our iterative
unsupervised framework, and jointly optimize un-
supervised and projection objectives with evolv-
ing treebank and constant projection information
respectively. In this way, our bilingually-guided
model?s parameters are tuned to simultaneous-
ly maximizing both monolingual likelihood and
bilingually-projected likelihood by 4 steps:
1. Randomly build treebank on target sentences
for initialization, and get the projected arc in-
stances through projection from bitext.
2. Train the bilingually-guided grammar induc-
tion model by multi-objective optimization
method with unsupervised objective and pro-
jection objective on treebank and projected
arc instances respectively.
3. Use the parsing model to build new treebank
on target language for next iteration.
4. Repeat steps 1, 2 and 3 until convergence.
The unsupervised objective is optimized by the
loop??tree bank?optimized model?new tree
bank?. The treebank is evolved for runs. The
unsupervised model gets projection constraint im-
plicitly from those parse trees which contain in-
formation from projection part. The projection ob-
jective is optimized by the circulation??projected
instances?optimized model?, these projected in-
stances will not change once we get them.
The iterative procedure proposed here is not a
co-training algorithm (Sarkar, 2001; Hwa et al,
2003), because the input of the projection objec-
tive is static.
1066
4.1 Joint Objective
For multi-objective optimization method, we em-
ploy the classical weighted-sum approach which
just calculates the weighted linear sum of the ob-
jectives:
OBJ =
?
m
weightmobjm (8)
We combine the unsupervised objective (For-
mula (6)) and projection objective (Formula (7))
together through the weighted-sum approach in
Formula (8):
?(?) = ??2(?) + (1 ? ?)?(?) (9)
where ?(?) is our weight-sum objective. And ?
is a mixing coefficient which reflects the relative
confidence between the unsupervised and projec-
tion objectives. Equally, ? and (1??) can be seen
as the weights in Formula (8). In that case, we can
use a single parameter ? to control both weights
for different objective functions. When ? = 1 it
is the unsupervised objective function in Formula
(6). Contrary, if ? = 0, it is the projection objec-
tive function (Formula (7)) for projected instances.
With this approach, we can optimize the mixed
parsing model by maximizing the objective in For-
mula (9). Though the function (Formula (9)) is
an interpolation function, we use it for training
instead of parsing. In the parsing procedure, our
method calculates the probability of a dependency
arc according to the Formula (2), while the inter-
polating method calculates it by:
Pr(y|deij) =?Pr1(y|deij )
+ (1 ? ?)Pr2(y|deij )
(10)
where Pr1(y|deij ) and Pr2(y|deij ) are the proba-
bilities provided by different models.
4.2 Training Algorithm
We optimize the objective (Formula (9)) via a
gradient-based search algorithm. And the gradi-
ent with respect to ?k takes the form:
??(?k) = ?
??2(?)
??k
+ (1 ? ?)??(?)??k
(11)
Algorithm 3 outlines our joint training proce-
dure, which tunes the grammar parameter ? simul-
taneously maximize both unsupervised objective
Algorithm 3 Training joint model
1: DP , DN ? proj(F,DF , A,E)
2: build random DE
3: ?? train(DP , DN )
4: repeat
5: for each E ? E do ? E step
6: DE ? parse(E,?)
7: ??(?)? grad(DE, D?E , DP , DN , ?(?))
8: ??climb(?(?),??(?), ?) ? M step
9: until convergence
and projection objective. And it incorporates un-
supervised framework and projection model algo-
rithm together. It is grounded on the work which
uses features in the unsupervised model (Berg-
Kirkpatrick et al, 2010).
In line 1, 2 we get projected dependency in-
stances from source side according to projec-
tion methods and build a random treebank (step
1). Then we train an initial model with projection
instances in line 3. From line 4-9, the objective is
optimized with a generic optimization step in the
subroutine climb(?, ?, ?, ?, ?). For each sentence we
parse its dependency tree, and update the tree into
the treebank (step 3). Then we calculate the gra-
dient and optimize the joint objective according to
the evolved treebank and projected instances (step
2). Lines 5-6 are equivalent to the E-step of the
EM algorithm, and lines 7-8 are equivalent to the
M-step.
5 Related work
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) which
is based on POS tags. And DMV learns the gram-
mar via inside-outside re-estimation (Baker, 1979)
without any smoothing, while Spitkovsky et al
(2010) utilizes smoothing and learning strategy
during grammar learning and William et al (2009)
improves DMV with richer context.
The dependency projection method DPA (H-
wa et al, 2005) based on Direct Correspondence
Assumption (Hwa et al, 2002) can be described
as: if there is a pair of source words with a de-
pendency relationship, the corresponding aligned
words in target sentence can be considered as hav-
ing the same dependency relationship equivalent-
ly (e.g. Figure 2). The Word Pair Classification
(WPC) method (Jiang and Liu, 2010) modifies the
DPA method and makes it more robust. Smith
and Eisner (2009) propose an adaptation method
founded on quasi-synchronous grammar features
1067
Type Feature Template
Unigram wordi posi wordi ? posi
wordj posj wordj ? posj
Bigram wordi ? posj wordj ? posi posi ? posj
wordi ? wordj wordi ? posi ? wordj wordi ? wordj ? posj
wordi ? posi ? posj posi ? wordj ? posj
wordi ? posi ? wordj ? posj
Surrounding posi?1 ? posi ? posj posi ? posi+1 ? posj posi ? posj?1 ? posjposi ? posj ? posj+1 posi?1 ? posi ? posj?1 posi ? posi+1 ? posj+1posi?1 ? posj?1 ? posj posi+1 ? posj ? posj+1 posi?1 ? posi ? posj+1posi ? posi+1 ? posj?1 posi?1 ? posj ? posj+1 posi+1 ? posj?1 ? posjposi?1 ? posi ? posj?1 ? posj posi ? posi+1 ? posj ? posj+1posi ? posi+1 ? posj?1 ? posj posi?1 ? posi ? posj ? posj+1
Table 1: Feature templates for dependency parsing. For edge deij : wordi is the parent word and wordj
is the child word, similar to ?pos?. ?+1? denotes the preceding token of the sentence, similar to ?-1?.
for dependency projection and annotation, which
requires a small set of dependency annotated cor-
pus of target language.
Similarly, using indirect information from mul-
tilingual (Cohen et al, 2011; Ta?ckstro?m et al,
2012) is an effective way to improve unsupervised
parsing. (Zeman and Resnik, 2008; McDonald et
al., 2011; S?gaard, 2011) employ non-lexicalized
parser trained on other languages to process a
target language. McDonald et al (2011) adapts
their multi-source parser according to DCA, while
Naseem et al (2012) selects a selective sharing
model to make better use of grammar information
in multi-sources.
Due to similar reasons, many works are devoted
to POS projection (Yarowsky et al, 2001; Shen et
al., 2007; Naseem et al, 2009), and they also suf-
fer from similar problems. Some seek for unsu-
pervised methods, e.g. Naseem et al (2009), and
some further improve the projection by a graph-
based projection (Das and Petrov, 2011).
Our model differs from the approaches above
in its emphasis on utilizing information from both
sides of bilingual corpus in an unsupervised train-
ing framework, while most of the work above only
utilize the information from a single side.
6 Experiments
In this section, we evaluate the performance of the
MST dependency parser (McDonald et al, 2005b)
which is trained by our bilingually-guided model
on 5 languages. And the features used in our ex-
periments are summarized in Table 1.
6.1 Experiment Setup
Datasets and Evaluation Our experiments are
run on five different languages: Chinese(ch),
Danish(da), Dutch(nl), Portuguese(pt) and
Swedish(sv) (da, nl, pt and sv are free data sets
distributed for the 2006 CoNLL Shared Tasks
(Buchholz and Marsi, 2006)). For all languages,
we only use English-target parallel data: we take
the FBIS English-Chinese bitext as bilingual cor-
pus for English-Chinese dependency projection
which contains 239K sentence pairs with about
8.9M/6.9M words in English/Chinese, and for
other languages we use the readily available data
in the Europarl corpus. Then we run tests on the
Penn Chinese Treebank (CTB) and CoNLL-X test
sets.
English sentences are tagged by the implemen-
tations of the POS tagger of Collins (2002), which
is trained on WSJ. The source sentences are then
parsed by an implementation of 2nd-ordered MST
model of McDonald and Pereira (2006), which is
trained on dependency trees extracted from Penn
Treebank.
As the evaluation metric, we use parsing accu-
racy which is the percentage of the words which
have found their correct parents. We evaluate on
sentences with all length for our method.
Training Regime In experiments, we use the
projection method proposed by Jiang and Liu
(2010) to provide the projection instances. And
we train the projection part ? = 0 first for initial-
ization, on which the whole model will be trained.
Availing of the initialization method, the model
can converge very fast (about 3 iterations is suffi-
cient) and the results are more stable than the ones
trained on random initialization.
Baselines We compare our method against
three kinds of different approaches: unsupervised
method (Klein and Manning, 2004); single-
source direct projection methods (Hwa et al,
2005; Jiang and Liu, 2010); multi-source in-
direct projection methods with multi-sources (M-
1068
60.0
61.5
          
 
 
ch
50.3
51.2
          
 
 
da
59.5
60.5
          
ac
cu
ra
cy
%
 
nl
70.5
74.5
          
 
 
pt
61.5
65.0
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
 
alpha
sv
Figure 3: The performance of our model with re-
spect to a series of ratio ?
cDonald et al, 2011; Naseem et al, 2012).
6.2 Results
We test our method on CTB and CoNLL-X free
test data sets respectively, and the performance is
summarized in Table 2. Figure 3 presents the per-
formance with different ? on different languages.
Compare against Unsupervised Baseline Ex-
perimental results show that our unsupervised
framework?s performance approaches to the DMV
method. And the bilingually-guided model can
promote the unsupervised method consisten-
cy over all languages. On the best results? aver-
age of four comparable languages (da, nl, pt, sv),
the promotion gained by our model is 28.5% over
the baseline method (DMV) (Klein and Manning,
2004).
Compare against Projection Baselines For
all languages, the model consistent-
ly outperforms on direct projection baseline.
On the average of each language?s best result, our
model outperforms all kinds of baselines, yielding
3.0% gain over the single-source direct-projection
method (Jiang and Liu, 2010) and 3.9% gain over
the multi-source indirect-projection method (Mc-
Donald et al, 2011). On the average of all results
with different parameters, our method also gain-
s more than 2.0% improvements on all baselines.
Particularly, our model achieves the most signif-
icant gains on Chinese, where the improvements
are 4.5%/12.0% on direct/indirect projection base-
Accuracy%
Model ch da nl pt sv avg
DMV 42.5? 33.4 38.5 20.1 44.0 ?.?
DPA 53.9 ?.? ?.? ?.? ?.? ?.?
WPC 56.8 50.1 58.4 70.5 60.8 59.3
Transfer 49.3 49.5 53.9 75.8 63.6 58.4
Selective 51.2 ?.? 55.9 73.5 61.5 ?.?
unsuper 22.6 41.6 15.2 45.7 42.4 33.5
avg 61.0 50.7 59.9 72.0 63.1 61.3
max 61.3 51.1 60.1 74.2 64.6 62.3
Table 2: The directed dependency accuracy with
different parameter of our model and the base-
lines. The first section of the table (row 3-7)
shows the results of the baselines: a unsupervised
method baseline (Klein and Manning, 2004)(D-
MV); a single-source projection method baseline
(Hwa et al, 2005) (DPA) and its improve-
ment (Jiang and Liu, 2010)(WPC); two multi-
source baselines (McDonald et al, 2011)(Trans-
fer) and (Naseem et al, 2012)(Selective). The
second section of the table (row 8) presents the
result of our unsupervised framework (unsuper).
The third section gives the mean value (avg) and
maximum value (max) of our model with different
? in Figure 3.
*: The result is based on sentences with 10
words or less after the removal of punctuation, it
is an incomparable result.
lines.
The results in Figure 3 prove that our unsuper-
vised framework ? = 1 can promote the grammar
induction if it has a good start (well initialization),
and it will be better once we incorporate the infor-
mation from the projection side (? = 0.9). And
the maximum points are not in ? = 1, which im-
plies that projection information is still available
for the unsupervised framework even if we employ
the projection model as the initialization. So we
suggest that a greater parameter is a better choice
for our model. And there are some random factors
in our model which make performance curves with
more fluctuation. And there is just a little improve-
ment shown in da, in which the same situation is
observed by (McDonald et al, 2011).
6.3 Effects of the Size of Training Corpus
To investigate how the size of the training corpus
influences the result, we train the model on ex-
tracted bilingual corpus with varying sizes: 10K,
50K, 100K, 150K and 200K sentences pairs.
As shown in Figure 4, our approach continu-
1069
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
10K 50K 100K 150K 200K
ac
cu
ra
cy
%
size of training set
our model
baseline
Figure 4: Performance on varying sizes (average
of 5 languages, ? = 0.9)
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
ac
cu
ra
cy
%
noise rate
our model
baseline
Figure 5: Performance on different projection
quality (average of 5 languages, ? = 0.9). The
noise rate is the percentage of the projected in-
stances being messed up.
ously outperforms the baseline with the increasing
size of training corpus. It is especially noteworthy
that the more training data is utilized the more su-
periority our model enjoys. That is, because our
method not only utilizes the projection informa-
tion but also avails itself of the monolingual cor-
pus.
6.4 Effect of Projection Quality
The projection quality can be influenced by the
quality of the source parsing, alignments, projec-
tion methods, corpus quality and many other fac-
tors. In order to detect the effects of varying pro-
jection qualities on our approach, we simulate the
complex projection procedure by messing up the
projected instances randomly with different noise
rates. The curves in Figure 5 show the perfor-
mance of WPC baseline and our bilingual-guided
method. For different noise rates, our model?s re-
sults consistently outperform the baselines. When
the noise rate is greater than 0.2, our improvement
49.5
...
54.6
...
58.2
58.6
59.0
59.4
59.8
60.2
0 0.02 0.04 0.06 0.08 0.1 ... 0.2 ... 0.3
ac
cu
ra
cy
%
alpha
our model
baseline(58.5)
Figure 6: The performance curve of our model
(random initialization) on Chinese, with respect to
a series of ratio ?. The baseline is the result of
WPC model.
increases with the growth of the noise rate. The re-
sult suggests that our method can solve some prob-
lems which are caused by projection noise.
6.5 Performance on Random Initialization
We test our model with random initialization on
different ?. The curve in Figure 6 shows the per-
formance of our model on Chinese.
The results seem supporting our unsupervised
optimization method when ? is in the range of
(0, 0.1). It implies that the unsupervised structure
information is useful, but it seems creating a nega-
tive effect on the model when ? is greater than 0.1.
Because the unsupervised part can gain constraints
from the projection part. But with the increase of
?, the strength of constraint dwindles, and the
unsupervised part will gradually lose control. And
bad unsupervised part pulls the full model down.
7 Conclusion and Future Work
This paper presents a bilingually-guided strate-
gy for automatic dependency grammar induction,
which adopts an unsupervised skeleton and lever-
ages the bilingually-projected dependency infor-
mation during optimization. By simultaneous-
ly maximizing the monolingual likelihood and
bilingually-projected likelihood in the EM proce-
dure, it effectively integrates the advantages of
bilingual projection and unsupervised induction.
Experiments on 5 languages show that the novel
strategy significantly outperforms previous unsu-
pervised or bilingually-projected models.
Since its computational complexity approaches to
the skeleton unsupervised model (with much few-
er iterations), and the bilingual text aligned to
1070
resource-rich languages is easy to obtain, such a
hybrid method seems to be a better choice for au-
tomatic grammar induction. It also indicates that
the combination of bilingual constraint and unsu-
pervised methodology has a promising prospect
for grammar induction. In the future work we will
investigate such kind of strategies, such as bilin-
gually unsupervised induction.
Acknowledgments
The authors were supported by National
Natural Science Foundation of China, Con-
tracts 61202216, 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&D Program (No. 2012BAH39B03), Key
Project of Knowledge Innovation Program of Chi-
nese Academy of Sciences (No. KGZD-EW-501).
Qun Liu?s work is partially supported by Science
Foundation Ireland (Grant No.07/CE/I1142) as
part of the CNGL at Dublin City University. We
would like to thank the anonymous reviewers for
their insightful comments and those who helped
to modify the paper.
References
H. Alshawi. 1996. Head automata for speech transla-
tion. In Proc. of ICSLP.
James K Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
of America, 65:S132.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In HLT: NAACL, pages 582?590.
Rens Bod. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proc. of the 21st ICCL and the
44th ACL, pages 865?872.
S. Buchholz and E. Marsi. 2006. Conll-x shared task
on multilingual dependency parsing. In Proc. of the
2002 Conference on EMNLP. Proc. CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative r-
eranking. In Proc. of the 43rd ACL, pages 173?180,
Ann Arbor, Michigan, June.
W. Chen, J. Kazama, and K. Torisawa. 2010. Bi-
text dependency parsing with bilingual subtree con-
straints. In Proc. of ACL, pages 21?29.
S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsu-
pervised structure prediction with non-parallel mul-
tilingual guidance. In Proc. of the Conference on
EMNLP, pages 50?61.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proc. of the
2002 Conference on EMNLP, pages 1?8, July.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. In Computational
Linguistics.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proc. of IJCNLP of the AFNLP: Vol-
ume 1-Volume 1, pages 369?377.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proc. of ACL, pages 392?399.
R. Hwa, M. Osborne, A. Sarkar, and M. Steedman.
2003. Corrected co-training for statistical parsers.
In ICML-03 Workshop on the Continuum from La-
beled to Unlabeled Data in Machine Learning and
Data Mining, Washington DC.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural language
engineering, 11(3):311?325.
W. Jiang and Q. Liu. 2010. Dependency parsing
and projection based on word-pair classification. In
Proc. of ACL, pages 12?20.
D. Klein and C.D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependen-
cy and constituency. In Proc. of ACL, page 478.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of the 48th ACL,
pages 1?11, July.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. pages 595?
603.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of the 11th Conf. of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b. Non-projective dependency parsing using s-
panning tree algorithms. In Proc. of EMNLP, pages
523?530.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proc. of CoNLL, pages 216?
220.
1071
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In Proc. of EMNLP, pages 62?72. ACL.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. Journal of Artificial Intelli-
gence Research, 36(1):341?385.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proc. of the 50th ACL, pages 629?637,
July.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006. Labeled pseudo-projective dependency
parsing with support vector machines. In Proc. of
CoNLL, pages 221?225.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95?135.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st ICCL
& 44th ACL, pages 433?440, July.
A. Sarkar. 2001. Applying co-training methods to sta-
tistical parsing. In Proc. of NAACL, pages 1?8.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learning
for bidirectional sequence classification. In Annual
Meeting-, volume 45, page 760.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proc. of ACL, pages 354?362.
D.A. Smith and J. Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous gram-
mar features. In Proc. of EMNLP: Volume 2-Volume
2, pages 822?831.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsu-
pervised multilingual grammar induction. In Proc.
of IJCNLP of the AFNLP: Volume 1-Volume 1, pages
73?81.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Proc.
of the 49th ACL: HLT, pages 682?686.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010. From baby steps to leapfrog: How
?less is more? in unsupervised dependency parsing.
In HLT: NAACL, pages 751?759, June.
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure.
William, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with rich-
er contexts and smoothing. In Proc. of NAACL,
pages 101?109.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proc. of HLT,
pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proc. of the IJCNLP-08. Proc. CoNLL.
Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. Algorithm 778: L-bfgs-b: Fortran
subroutines for large-scale bound-constrained opti-
mization. ACM Transactions on Mathematical Soft-
ware (TOMS), 23(4):550?560.
1072
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364?369,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Stem Translation with Affix-Based Rule Selection
for Agglutinative Languages
Zhiyang Wang?, Yajuan Lu??, Meng Sun?, Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{wangzhiyang,lvyajuan,sunmeng,liuqun}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Current translation models are mainly de-
signed for languages with limited mor-
phology, which are not readily applicable
to agglutinative languages as the differ-
ence in the way lexical forms are gener-
ated. In this paper, we propose a nov-
el approach for translating agglutinative
languages by treating stems and affixes
differently. We employ stem as the atomic
translation unit to alleviate data spare-
ness. In addition, we associate each stem-
granularity translation rule with a distri-
bution of related affixes, and select desir-
able rules according to the similarity of
their affix distributions with given spans to
be translated. Experimental results show
that our approach significantly improves
the translation performance on tasks of
translating from three Turkic languages to
Chinese.
1 Introduction
Currently, most methods on statistical machine
translation (SMT) are developed for translation
of languages with limited morphology (e.g., En-
glish, Chinese). They assumed that word was the
atomic translation unit (ATU), always ignoring the
internal morphological structure of word. This
assumption can be traced back to the original
IBM word-based models (Brown et al, 1993) and
several significantly improved models, including
phrase-based (Och and Ney, 2004; Koehn et al,
2003), hierarchical (Chiang, 2005) and syntac-
tic (Quirk et al, 2005; Galley et al, 2006; Liu et
al., 2006) models. These improved models worked
well for translating languages like English with
large scale parallel corpora available.
Different from languages with limited morphol-
ogy, words of agglutinative languages are formed
mainly by concatenation of stems and affixes.
Generally, a stem can attach with several affixes,
thus leading to tens of hundreds of possible inflect-
ed variants of lexicons for a single stem. Modeling
each lexical form as a separate word will generate
high out-of-vocabulary rate for SMT. Theoretical-
ly, ways like morphological analysis and increas-
ing bilingual corpora could alleviate the problem
of data sparsity, but most agglutinative languages
are less-studied and suffer from the problem of
resource-scarceness. Therefore, previous research
mainly focused on the different inflected variants
of the same stem and made various transformation
of input by morphological analysis, such as (Lee,
2004; Goldwater and McClosky, 2005; Yang and
Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza
and Federico, 2009; Wang et al, 2011). These
work still assume that the atomic translation unit
is word, stem or morpheme, without considering
the difference between stems and affixes.
In agglutinative languages, stem is the base
part of word not including inflectional affixes.
Affix, especially inflectional affix, indicates dif-
ferent grammatical categories such as tense, per-
son, number and case, etc., which is useful for
translation rule disambiguation. Therefore, we
employ stem as the atomic translation unit and
use affix information to guide translation rule
selection. Stem-granularity translation rules have
much larger coverage and can lower the OOV
rate. Affix based rule selection takes advantage
of auxiliary syntactic roles of affixes to make a
better rule selection. In this way, we can achieve
a balance between rule coverage and matching
accuracy, and ultimately improve the translation
performance.
364
zunyi
/STM
i
/SUF
yighin
/STM
gha
/SUF
zunyi yighin ||| ?? ??? ||| i gha
Original:zunyi yighin+i+gha
Meaning:of zunyi conference
(B)Translation rules with affix distribution
zunyi yighin ||| ????? ||| i:0 gha:0.09 zunyi yighin ||| ?????? ||| i:0 da:0.24
zunyi
/STM
i
/SUF
yighin
/STM
da
/SUF
zunyi yighin ||| ?????? ||| i da
(A) Instances of translation rule
(1) (2)
zunyi
/STM
i
/SUF
yighin
/STM
gha
/SUF
zunyi yighin ||| ?? ??? ||| i gha
(3)
Original:zunyi yighin+i+da
Meaning:on zunyi conference
Original:zunyi yighin+i+gha
Meaning:of zunyi conference
Figure 1: Translation rule extraction from Uyghur to Chinese. Here tag ?/STM? represents stem and
?/SUF? means suffix.
2 Affix Based Rule Selection Model
Figure 1 (B) shows two translation rules along
with affix distributions. Here a translation rule
contains three parts: the source part (on stem lev-
el), the target part, and the related affix distribution
(represented as a vector). We can see that, al-
though the source part of the two translation rules
are identical, their affix distributions are quite
different. Affix ?gha? in the first rule indicates
that something is affiliated to a subject, similar to
?of? in English. And ?da? in second rule implies
location information. Therefore, given a span
?zunyi/STM yighin/STM+i/SUF+da/SUF+...? to
be translated, we hope to encourage our model to
select the second translation rule. We can achieve
this by calculating similarity between the affix
distributions of the translation rule and the span.
The affix distribution can be obtained by keep-
ing the related affixes for each rule instance during
translation rule extraction ((A) in Figure 1). After
extracting and scoring stem-granularity rules in a
traditional way, we extract stem-granularity rules
again by keeping affix information and compute
the affix distribution with tf-idf (Salton and Buck-
ley, 1987). Finally, the affix distribution will be
added to the previous stem-granularity rules.
2.1 Affix Distribution Estimation
Formally, translation rule instances with the same
source part can be treated as a document collec-
tion1, so each rule instance in the collection is
1We employ concepts from text classification to illustrate
how to estimate affix distribution.
some kind of document. Our goal is to classify the
source parts into the target parts on the document
collection level with the help of affix distribu-
tion. Accordingly, we employ vector space model
(VSM) to represent affix distribution of each rule
instance. In this model, the feature weights are
represented by the classic tf-idf (Salton and Buck-
ley, 1987):
tf i,j =
ni,j?
k nk,j
idf i,j = log
|D|
|j : ai ? rj|
tfidf i,j = tf i,j ? idf i,j
(1)
where tfidf i,j is the weight of affix ai in transla-
tion rule instance rj . ni,j indicates the number of
occurrence of affix ai in rj . |D| is the number
of rule instance with the same source part, and
|j : ai ? rj| is the number of rule instance which
contains affix ai within |D|.
Let?s take the suffix ?gha? from (A1) in Figure
1 as an example. We assume that there are only
three instances of translation rules extracted from
parallel corpus ((A) in Figure 1). We can see that
?gha? only appear once in (A1) and also appear
once in whole instances. Therefore, tfgha,(A1) is
0.5 and idfgha,(A1) is log(3/2). tfidfgha,(A1) is
the product of tfgha,(A1) and idfgha,(A1) which
is 0.09.
Given a set of N translation rule instances with
the same source and target part, we define the
centroid vector dr according to the centroid-based
classification algorithm (Han and Karypis, 2000),
dr =
1
N
?
i?N
di (2)
365
Data set #Sent. #Type #Tokenword stem morph word stem morph
UY-CH-Train. 50K 69K 39K 42K 1.2M 1.2M 1.6M
UY-CH-Dev. 0.7K*4 5.9K 4.1K 4.6K 18K 18K 23.5K
UY-CH-Test. 0.7K*1 4.7K 3.3K 3.8K 14K 14K 17.8K
KA-CH-Train. 50K 62K 40K 42K 1.1M 1.1M 1.3M
KA-CH-Dev. 0.7K*4 5.3K 4.2K 4.5K 15K 15K 18K
KA-CH-Test. 0.2K*1 2.6K 2.0K 2.3K 8.6K 8.6K 10.8K
KI-CH-Train. 50K 53K 27K 31K 1.2M 1.2M 1.5M
KI-CH-Dev. 0.5K*4 4.1K 3.1K 3.5K 12K 12K 15K
KI-CH-Test. 0.2K*4 2.2K 1.8K 2.1K 4.7K 4.7K 5.8K
Table 1: Statistics of data sets. ?N means the number of reference, morph is short to morpheme. UY,
KA, KI, CH represent Uyghur, Kazakh, Kirghiz and Chinese respectively.
dr is the final affix distribution.
By comparing the similarity of affix distribu-
tions, we are able to decide whether a translation
rule is suitable for a span to be translated. In
this work, similarity is measured using the cosine
distance similarity metric, given by
sim(d1,d2) =
d1 ? d2
?d1? ? ?d2?
(3)
where di corresponds to a vector indicating affix
distribution, and ??? denotes the inner product of
the two vectors.
Therefore, for a specific span to be translated,
we first analyze it to get the corresponding stem
sequence and related affix distribution represented
as a vector. Then the stem sequence is used to
search the translation rule table. If the source part
is matched, the similarity will be calculated for
each candidate translation rule by cosine similarity
(as in equation 3). Therefore, in addition to the
traditional translation features on stem level, our
model also adds the affix similarity score as a
dynamic feature into the log-linear model (Och
and Ney, 2002).
3 Related Work
Most previous work on agglutinative language
translation mainly focus on Turkish and Finnish.
Bisazza and Federico (2009) and Mermer and
Saraclar (2011) optimized morphological analysis
as a pre-processing step to improve the translation
between Turkish and English. Yeniterzi and Oflaz-
er (2010) mapped the syntax of the English side
to the morphology of the Turkish side with the
factored model (Koehn and Hoang, 2007). Yang
and Kirchhoff (2006) backed off surface form to
stem when translating OOV words of Finnish.
Luong and Kan (2010) and Luong et al (2010)
focused on Finnish-English translation through
improving word alignment and enhancing phrase
table. These works still assumed that the atomic
translation unit is word, stem or morpheme, with-
out considering the difference between stems and
affixes.
There are also some work that employed the
context information to make a better choice of
translation rules (Carpuat and Wu, 2007; Chan et
al., 2007; He et al, 2008; Cui et al, 2010). all the
work employed rich context information, such as
POS, syntactic, etc., and experiments were mostly
done on less inflectional languages (i.e. Chinese,
English) and resourceful languages (i.e. Arabic).
4 Experiments
In this work, we conduct our experiments on
three different agglutinative languages, including
Uyghur, Kazakh and Kirghiz. All of them are
derived from Altaic language family, belonging to
Turkic languages, and mostly spoken by people in
Central Asia. There are about 24 million people
take these languages as mother tongue. All of
the tasks are derived from the evaluation of Chi-
na Workshop of Machine Translation (CWMT)2.
Table 1 shows the statistics of data sets.
For the language model, we use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train
a 5-gram model with the target side of training
corpus. And phrase-based Moses3 is used as our
2http://mt.xmu.edu.cn/cwmt2011/en/index.html.
3http://www.statmt.org/moses/
366
UY-CH KA-CH KI-CH
word 31.74+0.0 28.64+0.0 35.05+0.0
stem 33.74+2.0 30.14+1.5 35.52+0.47
morph 32.69+0.95 29.21+0.57 34.97?0.08
affix 34.34+2.6 30.19+2.27 35.96+0.91
Table 2: Translation results from Turkic languages
to Chinese. word: ATU is surface form,
stem: ATU is represented stem, morph: ATU
denotes morpheme, affix: stem translation with
affix distribution similarity. BLEU scores in
bold means significantly better than the baseline
according to (Koehn, 2004) for p-value less than
0.01.
baseline SMT system. The decoding weights are
optimized with MERT (Och, 2003) to maximum
word-level BLEU scores (Papineni et al, 2002).
4.1 Using Unsupervised Morphological
Analyzer
As most agglutinative languages are resource-
poor, we employ unsupervised learning method
to obtain the morphological structure. Follow-
ing the approach in (Virpioja et al, 2007), we
employ the Morfessor4 Categories-MAP algorith-
m (Creutz and Lagus, 2005). It applies a hierar-
chical model with three categories (prefix, stem,
and suffix) in an unsupervised way. From Table 1
we can see that vocabulary sizes of the three lan-
guages are reduced obviously after unsupervised
morphological analysis.
Table 2 shows the translation results. All the
three translation tasks achieve obvious improve-
ments with the proposed model, which always per-
forms better than only employ word, stem and
morph. For the Uyghur to Chinese translation
(UY-CH) task in Table 2, performances after unsu-
pervised morphological analysis are always better
than the baseline. And we gain up to +2.6 BLEU
points improvements with affix compared to the
baseline. For the Kazakh to Chinese translation
(KA-CH) task, the improvements are also signifi-
cant. We achieve +2.27 and +0.77 improvements
compared to the baseline and stem, respectively.
As for the Kirghiz to Chinese translation (KI-CH)
task, improvements seem relative small compared
to the other two language pairs. However, it also
gains +0.91 BLEU points over the baseline.
4http://www.cis.hut.fi/projects/morpho/
UY Unsup Sup
stem #Type 39K 21K#Token 1.2M 1.2M
affix #Type 3.0K 0.3K#Token 0.4M 0.7M
Table 3: Statistics of training corpus after unsuper-
vised(Unsup) and supervised(Sup) morphological
analysis.
 31.5 32
 32.5 33
 33.5 34
 34.5 35
 35.5 36
 36.5
word morph stem affix
B
L
E
U
 
s
c
o
r
e
(
%
)
UnsupervisedSupervised
Figure 2: Uyghur to Chinese translation results
after unsupervised and supervised analysis.
4.2 Using Supervised Morphological
Analyzer
Taking it further, we also want to see the effect of
supervised analysis on our model. A generative
statistical model of morphological analysis for
Uyghur was developed according to (Mairehaba
et al, 2012). Table 3 shows the difference of
statistics of training corpus after supervised and
unsupervised analysis. Supervised method gen-
erates fewer type of stems and affixes than the
unsupervised approach. As we can see from
Figure 2, except for the morph method, stem
and affix based approaches perform better after
supervised analysis. The results show that our
approach can obtain even better translation per-
formance if better morphological analyzers are
available. Supervised morphological analysis gen-
erates more meaningful morphemes, which lead to
better disambiguation of translation rules.
5 Conclusions and Future Work
In this paper we propose a novel framework for
agglutinative language translation by treating stem
and affix differently. We employ the stem se-
quence as the main part for training and decod-
ing. Besides, we associate each stem-granularity
translation rule with an affix distribution, which
could be used to make better translation decisions
by calculating the affix distribution similarity be-
367
tween the rule and the instance to be translated.
We conduct our model on three different language
pairs, all of which substantially improved the
translation performance. The procedure is totally
language-independent, and we expect that other
language pairs could benefit from our approach.
Acknowledgments
The authors were supported by 863 State
Key Project (No. 2011AA01A207), and
National Key Technology R&D Program (No.
2012BAH39B03), Key Project of Knowledge
Innovation Program of Chinese Academy of
Sciences (No. KGZD-EW-501). Qun Liu?s work
is partially supported by Science Foundation
Ireland (Grant No.07/CE/I1142) as part of the
CNGL at Dublin City University. We would
like to thank the anonymous reviewers for their
insightful comments and those who helped to
modify the paper.
References
Arianna Bisazza and Marcello Federico. 2009. Mor-
phological pre-processing for Turkish to English
statistical machine translation. In Proceedings of
IWSLT, pages 129?135.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
Marine Carpuat and Dekai Wu. 2007. Improving
statistical machine translation using word sense
disambiguation. In Proceedings of EMNLP-CoNLL,
pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves
statistical machine translation. In Proceedings of
ACL, pages 33?40.
David Chiang. 2005. A hierarchical phrase-
based model for statistical machine translation. In
Proceedings of ACL, pages 263?270.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of AKRR, pages
106?113.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou,
and Tiejun Zhao. 2010. A joint rule selection
model for hierarchical phrase-based translation. In
Proceedings of ACL, Short Papers, pages 6?11.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of COLING/ACL, pages 961?968.
Sharon Goldwater and David McClosky. 2005.
Improving statistical MT through morphological
analysis. In Proceedings of HLT-EMNLP, pages
676?683.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proceedings of NAACL, Short Papers, pages 49?
52.
Eui-Hong Sam Han and George Karypis. 2000.
Centroid-based document classification: analysis
experimental results. In Proceedings of PKDD,
pages 424?431.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008.
Improving statistical machine translation using
lexicalized rule selection. In Proceedings of
COLING, pages 321?328.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of EMNLP-
CoNLL, pages 868?876.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of NAACL, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In Proceedings of
HLT-NAACL, Short Papers, pages 57?60.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING-ACL, pages
609?616.
Minh-Thang Luong and Min-Yen Kan. 2010.
Enhancing morphological alignment for translating
highly inflected languages. In Proceedings of
COLING, pages 743?751.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich
languages. In Proceedings of EMNLP, pages 148?
157.
Aili Mairehaba, Wenbin Jiang, Zhiyang Wang, Yibu-
layin Tuergen, and Qun Liu. 2012. Directed graph
model of Uyghur morphological analysis. Journal
of Software, 23(12):3115?3129.
Coskun Mermer and Murat Saraclar. 2011. Un-
supervised Turkish morphological segmentation for
statistical machine translation. In Workshop of MT
and Morphologically-rich Languages.
368
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2004. The
alignment template approach to statistical machine
translation. Comput. Linguist., pages 417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency treelet translation: syntactically
informed phrasal SMT. In Proceedings of ACL,
pages 271?279.
Gerard Salton and Chris Buckley. 1987. Term
weighting approaches in automatic text retrieval.
Technical report.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Proceedings of
ICSLP, pages 311?318.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs
induced in an unsupervised manner. In Proceedings
of MT SUMMIT, pages 491?498.
Zhiyang Wang, Yajuan Lu?, and Qun Liu. 2011.
Multi-granularity word alignment and decoding for
agglutinative language translation. In Proceedings
of MT SUMMIT, pages 360?367.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly
inflected languages. In Proceedings of EACL, pages
1017?1020.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-
to-morphology mapping in factored phrase-based
statistical machine translation from English to
Turkish. In Proceedings of ACL, pages 454?464.
369
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382?386,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bilingual Lexical Cohesion Trigger Model for Document-Level
Machine Translation
Guosheng Ben? Deyi Xiong?? Zhiyang Teng? Yajuan Lu?? Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{benguosheng,tengzhiyang,lvyajuan,liuqun}@ict.ac.cn
?School of Computer Science and Technology,Soochow University
{dyxiong}@suda.edu.cn
?Centre for Next Generation Localisation, Dublin City University
{qliu}@computing.dcu.ie
Abstract
In this paper, we propose a bilingual lexi-
cal cohesion trigger model to capture lex-
ical cohesion for document-level machine
translation. We integrate the model into
hierarchical phrase-based machine trans-
lation and achieve an absolute improve-
ment of 0.85 BLEU points on average over
the baseline on NIST Chinese-English test
sets.
1 Introduction
Current statistical machine translation (SMT) sys-
tems are mostly sentence-based. The major draw-
back of such a sentence-based translation fash-
ion is the neglect of inter-sentential dependencies.
As a linguistic means to establish inter-sentential
links, lexical cohesion ties sentences together in-
to a meaningfully interwoven structure through
words with the same or related meanings (Wong
and Kit, 2012).
This paper studies lexical cohesion devices and
incorporate them into document-level machine
translation. We propose a bilingual lexical cohe-
sion trigger model to capture lexical cohesion for
document-level SMT. We consider a lexical co-
hesion item in the source language and its corre-
sponding counterpart in the target language as a
trigger pair, in which we treat the source language
lexical cohesion item as the trigger and its target
language counterpart as the triggered item. Then
we use mutual information to measure the strength
of the dependency between the trigger and trig-
gered item.
We integrate this model into a hierarchical
phrase-based SMT system. Experiment results
?Corresponding author
show that it is able to achieve substantial improve-
ments over the baseline.
The remainder of this paper proceeds as fol-
lows: Section 2 introduces the related work and
highlights the differences between previous meth-
ods and our model. Section 3 elaborates the pro-
posed bilingual lexical cohesion trigger model, in-
cluding the details of identifying lexical cohesion
devices, measuring dependency strength of bilin-
gual lexical cohesion triggers and integrating the
model into SMT. Section 4 presents experiments
to validate the effectiveness of our model. Finally,
Section 5 concludes with future work.
2 Related Work
As a linguistic means to establish inter-sentential
links, cohesion has been explored in the literature
of both linguistics and computational linguistics.
Cohesion is defined as relations of meaning that
exist within the text and divided into grammatical
cohesion that refers to the syntactic links between
text items and lexical cohesion that is achieved
through word choices in a text by Halliday and
Hasan (1976). In order to improve the quality of
machine translation output, cohesion has served as
a high level quality criterion in post-editing (Vas-
concellos, 1989). As a part of COMTIS project,
grammatical cohesion is integrated into machine
translation models to capture inter-sentential links
(Cartoni et al, 2011). Wong and Kit (2012) in-
corporate lexical cohesion to machine translation
evaluation metrics to evaluate document-level ma-
chine translation quality. Xiong et al (2013) inte-
grate various target-side lexical cohesion devices
into document-level machine translation. Lexical
cohesion is also partially explored in the cache-
based translation models of Gong et al (2011) and
translation consistency constraints of Xiao et al
382
(2011).
All previous methods on lexical cohesion for
document-level machine translation as mentioned
above have one thing in common, which is that
they do not use any source language information.
Our work is mostly related to the mutual infor-
mation trigger based lexical cohesion model pro-
posed by Xiong et al (2013). However, we sig-
nificantly extend their model to a bilingual lexical
cohesion trigger model that captures both source
and target-side lexical cohesion items to improve
target word selection in document-level machine
translation.
3 Bilingual Lexical Cohesion Trigger
Model
3.1 Identification of Lexical Cohesion Devices
Lexical cohesion can be divided into reiteration
and collocation (Wong and Kit, 2012). Reitera-
tion is a form of lexical cohesion which involves
the repetition of a lexical item. Collocation is a
pair of lexical items that have semantic relation-
s, such as synonym, near-synonym, superordinate,
subordinate, antonym, meronym and so on. In
the collocation, we focus on the synonym/near-
synonym and super-subordinate semantic relation-
s 1. We define lexical cohesion devices as content
words that have lexical cohesion relations, namely
the reiteration, synonym/near-synonym and super-
subordinate.
Reiteration is common in texts. Take the fol-
lowing two sentences extracted from a document
for example (Halliday and Hasan, 1976).
1. There is a boy climbing the old elm.
2. That elm is not very safe.
We see that word elm in the first sentence is re-
peated in the second sentence. Such reiteration de-
vices are easy to identify in texts. Synonym/near-
synonym is a semantic relationship set. We can
use WordNet (Fellbaum, 1998) to identify them.
WordNet is a lexical resource that clusters words
with the same sense into a semantic group called
synset. Synsets in WordNet are organized ac-
cording to their semantic relations. Let s(w) de-
note a function that defines all synonym words of
w grouped in the same synset in WordNet. We
can use the function to compute all synonyms and
near-synonyms for word w. In order to represen-
t conveniently, s0 denotes the set of synonyms in
1Other collocations are not used frequently, such as
antonyms. So we we do not consider them in our study.
s(w). Near-synonym set s1 is defined as the union
of all synsets that are defined by the function s(w)
where w? s0. It can be formulated as follows.
s1 =
?
w?s0
s(w) (1)
s2 =
?
w?s1
s(w) (2)
s3 =
?
w?s2
s(w) (3)
Similarly sm can be defined recursively as follows.
sm =
?
w?sm?1
s(w) (4)
Obviously, We can find synonyms and near-
synonyms for word w according to formula (4).
Superordinate and subordinate are formed by
words with an is-a semantic relation in WordNet.
As the super-subordinate relation is also encoded
in WordNet, we can define a function that is simi-
lar to s(w) identify hypernyms and hyponyms.
We use rep, syn and hyp to represent the lex-
ical cohesion device reiteration, synonym/near-
synonym and super-subordinate respectively here-
after for convenience.
3.2 Bilingual Lexical Cohesion Trigger
Model
In a bilingual text, lexical cohesion is present in
the source and target language in a synchronous
fashion. We use a trigger model capture such a
bilingual lexical cohesion relation. We define xRy
(R?{rep, syn, hyp}) as a trigger pair where x is
the trigger in the source language and y the trig-
gered item in the target language. In order to cap-
ture these synchronous relations between lexical
cohesion items in the source language and their
counterparts in the target language, we use word
alignments. First, we identify a monolingual lexi-
cal cohesion relation in the target language in the
form of tRy where t is the trigger, y the triggered
item that occurs in a sentence succeeding the sen-
tence of t, and R?{rep, syn, hyp}. Second, we
find word x in the source language that is aligned
to t in the target language. We may find multiple
words xk1 in the source language that are aligned
to t. We use all of them xiRt(1?i?k) to define
bilingual lexical cohesion relations. In this way,
we can create bilingual lexical cohesion relations
xRy (R?{rep, syn, hyp}): x being the trigger and
y the triggered item.
383
The possibility that y will occur given x is equal
to the chance that x triggers y. Therefore we mea-
sure the strength of dependency between the trig-
ger and triggered item according to pointwise mu-
tual information (PMI) (Church and Hanks, 1990;
Xiong et al, 2011).
The PMI for the trigger pair xRy where x is the
trigger, y the triggered item that occurs in a target
sentence succeeding the target sentence that aligns
to the source sentence of x, and R?{rep, syn, hyp}
is calculated as follows.
PMI(xRy) = log( p(x, y,R)p(x,R)p(y,R) ) (5)
The joint probability p(x, y,R) is:
p(x, y,R) = C(x, y,R)?
x,y C(x, y,R)
(6)
where C(x, y,R) is the number of aligned bilin-
gual documents where both x and y occur
with the relation R in different sentences, and?
x,y C(x, y,R) is the number of bilingual docu-
ments where this relation R occurs. The marginal
probabilities of p(x,R) and p(y,R) can be calcu-
lated as follows.
p(x,R) =
?
y
C(x, y,R) (7)
p(y,R) =
?
x
C(x, y,R) (8)
Given a target sentence ym1 , our bilingual lexical
cohesion trigger model is defined as follows.
MIR(ym1 ) =
?
yi
exp(PMI(?Ryi)) (9)
where yi are content words in the sentence ym1 and
PMI(?Ryi)is the maximum PMI value among all
trigger words xq1 from source sentences that have
been recently translated, where trigger words xq1
have an R relation with word yi.
PMI(?Ryi) = max1?j?qPMI(xjRyi) (10)
Three models MIrep(ym1 ), MIsyn(ym1 ),
MIhyp(ym1 ) for the reiteration device, the
synonym/near-synonym device and the super-
subordinate device can be formulated as above.
They are integrated into the log-linear model of
SMT as three different features.
3.3 Decoding
We incorporate our bilingual lexical cohesion trig-
ger model into a hierarchical phrase-based system
(Chiang, 2007). We add three features as follows.
? MIrep(ym1 )
? MIsyn(ym1 )
? MIhyp(ym1 )
In order to quickly calculate the score of each fea-
ture, we calculate PMI for each trigger pair be-
fore decoding. We translate document one by one.
During translation, we maintain a cache to store
source language sentences of recently translated
target sentences and three sets Srep, Ssyn, Shyp
to store source language words that have the re-
lation of {rep, syn, hyp} with content words gen-
erated in target language. During decoding, we
update scores according to formula (9). When one
sentence is translated, we store the corresponding
source sentence into the cache. When the whole
document is translated, we clear the cache for the
next document.
4 Experiments
4.1 Setup
Our experiments were conducted on the NIST
Chinese-English translation tasks with large-scale
training data. The bilingual training data contain-
s 3.8M sentence pairs with 96.9M Chinese word-
s and 109.5M English words from LDC2. The
monolingual data for training data English lan-
guage model includes the Xinhua portion of the
Gigaword corpus. The development set is the
NIST MT Evaluation test set of 2005 (MT05),
which contains 100 documents. We used the sets
of MT06 and MT08 as test sets. The numbers of
documents in MT06, MT08 are 79 and 109 respec-
tively. For the bilingual lexical cohesion trigger
model, we collected data with document bound-
aries explicitly provided. The corpora are select-
ed from our bilingual training data and the whole
Hong Kong parallel text corpus3, which contains
103,236 documents with 2.80M sentences.
2The corpora include LDC2002E18, LDC2003E07, LD-
C2003E14,LDC2004E12,LDC2004T07,LDC2004T08(Only
Hong Kong News), LDC2005T06 and LDC2005T10.
3They are LDC2003E14, LDC2004T07, LDC2005T06,
LDC2005T10 and LDC2004T08 (Hong Kong Hansard-
s/Laws/News).
384
We obtain the word alignments by running
GIZA++ (Och and Ney, 2003) in both direction-
s and applying ?grow-diag-final-and? refinemen-
t (Koehn et al, 2003). We apply SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram language model with Kneser-Ney smooth-
ing. Case-insensitive NIST BLEU (Papineni et
al., 2002) was used to measure translation per-
formance. We used minimum error rate training
MERT (Och, 2003) for tuning the feature weights.
4.2 Distribution of Lexical Cohesion Devices
in the Target Language
Cohesion Device Percentage(%)
rep 30.85
syn 17.58
hyp 18.04
Table 1: Distributions of lexical cohesion devices
in the target language.
In this section we want to study how these
lexical cohesion devices distribute in the train-
ing data before conducting our experiments on
the bilingual lexical cohesion model. Here
we study the distribution of lexical cohesion in
the target language (English). Table 1 shows
the distribution of percentages that are counted
based on the content words in the training da-
ta. From Table 1, we can see that the reitera-
tion cohesion device is nearly a third of all con-
tent words (30.85%), synonym/near-synonym and
super-subordinate devices account for 17.58% and
18.04%. Obviously, lexical cohesion devices are
frequently used in real-world texts. Therefore cap-
turing lexical cohesion devices is very useful for
document-level machine translation.
4.3 Results
System MT06 MT08 Avg
Base 30.43 23.32 26.88
rep 31.24 23.70 27.47
syn 30.92 23.71 27.32
hyp 30.97 23.48 27.23
rep+syn+hyp 31.47 23.98 27.73
Table 2: BLEU scores with various lexical co-
hesion devices on the test sets MT06 and MT08.
?Base? is the traditonal hierarchical system, ?Avg?
is the average BLEU score on the two test sets.
Results are shown in Table 2. From the table,
we can see that integrating a single lexical cohe-
sion device into SMT, the model gains an improve-
ment of up to 0.81 BLEU points on the MT06 test
set. Combining all three features rep+syn+hyp to-
gether, the model gains an improvement of up to
1.04 BLEU points on MT06 test set, and an av-
erage improvement of 0.85 BLEU points on the
two test sets of MT06 and MT08. These stable
improvements strongly suggest that our bilingual
lexical cohesion trigger model is able to substan-
tially improve the translation quality.
5 Conclusions
In this paper we have presented a bilingual lex-
ical cohesion trigger model to incorporate three
classes of lexical cohesion devices, namely the
reiteration, synonym/near-synonym and super-
subordinate devices into a hierarchical phrase-
based system. Our experimental results show
that our model achieves a substantial improvement
over the baseline. This displays the advantage of
exploiting bilingual lexical cohesion.
Grammatical and lexical cohesion have often
been studied together in discourse analysis. In
the future, we plan to extend our model to cap-
ture both grammatical and lexical cohesion in
document-level machine translation.
Acknowledgments
This work was supported by 863 State Key Project
(No.2011AA01A207) and National Key Technol-
ogy R&D Program(No.2012BAH39B03). Qun
Liu was also partially supported by Science Foun-
dation Ireland (Grant No.07/CE/I1142) as part of
the CNGL at Dublin City University. We would
like to thank the anonymous reviewers for their in-
sightful comments.
References
Bruno Cartoni, Andrea Gesmundo, James Hender-
son, Cristina Grisot, Paola Merlo, Thomas Mey-
er, Jacques Moeschler, Sandrine Zufferey, Andrei
Popescu-Belis, et al 2011. Improving mt coher-
ence through text-level processing of input texts:
the comtis project. http://webcast. in2p3. fr/videos-
the comtis project.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201?228.
Kenneth Ward Church and Patrick Hanks. 1990. Word
385
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Christine Fellbaum. 1998. Wordnet: An electronic
lexical database.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion
in english. English language series, 9.
Philipp Koehn, Franz Josef Och, and Daniel Mar-
cu. 2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, S-
apporo, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of the internation-
al conference on spoken language processing, vol-
ume 2, pages 901?904.
Muriel Vasconcellos. 1989. Cohesion and coherence
in the presentation of machine translation products.
Georgetown University Round Table on Languages
and Linguistics, pages 89?105.
Billy T. M. Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lex-
ical cohesion to document level. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1060?1068, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Machine Translation Sum-
mit, volume 13, pages 131?138.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011.
Enhancing language models in statistical machine
translation with backward n-grams and mutual in-
formation triggers. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1288?1297, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, Beijing,China.
386
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 591?596,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Iterative Transformation of Annotation Guidelines for
Constituency Parsing
Xiang Li 1, 2 Wenbin Jiang 1 Yajuan Lu? 1 Qun Liu 1, 3
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{lixiang, jiangwenbin, lvyajuan}@ict.ac.cn
2University of Chinese Academy of Sciences
3Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
This paper presents an effective algorith-
m of annotation adaptation for constituen-
cy treebanks, which transforms a treebank
from one annotation guideline to anoth-
er with an iterative optimization proce-
dure, thus to build a much larger treebank
to train an enhanced parser without in-
creasing model complexity. Experiments
show that the transformed Tsinghua Chi-
nese Treebank as additional training da-
ta brings significant improvement over the
baseline trained on Penn Chinese Tree-
bank only.
1 Introduction
Annotated data have become an indispensable
resource for many natural language processing
(NLP) applications. On one hand, the amount of
existing labeled data is not sufficient; on the other
hand, however there exists multiple annotated da-
ta with incompatible annotation guidelines for the
same NLP task. For example, the People?s Daily
corpus (Yu et al, 2001) and Chinese Penn Tree-
bank (CTB) (Xue et al, 2005) are publicly avail-
able for Chinese segmentation.
An available treebank is a major resource for
syntactic parsing. However, it is often a key bottle-
neck to acquire credible treebanks. Various tree-
banks have been constructed based on differen-
t annotation guidelines. In addition to the most
popular CTB, Tsinghua Chinese Treebank (TC-
T) (Zhou, 2004) is another real large-scale tree-
bank for Chinese constituent parsing. Figure 1 il-
lustrates some differences between CTB and TCT
in grammar category and syntactic structure. Un-
fortunately, these heterogeneous treebanks can not
be directly merged together for training a parsing
model. Such divergences cause a great waste of
human effort. Therefore, it is highly desirable to
transform a treebank into another compatible with
another annotation guideline.
In this paper, we focus on harmonizing het-
erogeneous treebanks to improve parsing perfor-
mance. We first propose an effective approach to
automatic treebank transformation from one an-
notation guideline to another. For convenience
of reference, a treebank with our desired anno-
tation guideline is named as target treebank, and
a treebank with a differtn annotation guideline is
named as source treebank. Our approach proceeds
in three steps. A parser is firstly trained on source
treebank. It is used to relabel the raw sentences
of target treebank, to acquire parallel training da-
ta with two heterogeneous annotation guidelines.
Then, an annotation transformer is trained on the
parallel training data to model the annotation in-
consistencies. In the last step, a parser trained on
target treebank is used to generate k-best parse
trees with target annotation for source sentences.
Then the optimal parse trees are selected by the an-
notation transformer. In this way, the source tree-
bank is transformed to another with our desired
annotation guideline. Then we propose an op-
timization strategy of iterative training to further
improve the transformation performance. At each
iteration, the annotation transformation of source-
to-target and target-to-source are both performed.
The transformed treebank is used to provide better
annotation guideline for the parallel training da-
ta of next iteration. As a result, the better paral-
lel training data will bring an improved annotation
transformer at next iteration.
We perform treebank transformation from TC-
591
zjXXXXXEE
djHHH
np
ZZProceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 76?80,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
ETS: An Error Tolerable System for Coreference Resolution
Hao Xiong , Linfeng Song , Fandong Meng , Yang Liu , Qun Liu and Yajuan Lu?
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao,songlinfeng,mengfandong,yliu,liuqun,lvyajuan}@ict.ac.cn
Abstract
This paper presents our error tolerable sys-
tem for coreference resolution in CoNLL-
2011(Pradhan et al, 2011) shared task (closed
track). Different from most previous reported
work, we detect mention candidates based on
packed forest instead of single parse tree, and
we use beam search algorithm based on the
Bell Tree to create entities. Experimental re-
sults show that our methods achieve promising
results on the development set.
1 Introduction
Over last decades, there has been increasing inter-
est on coreference resolution within NLP commu-
nity. The task of coreference resolution is to iden-
tify expressions in a text that refer to the same dis-
course entity. This year, CoNLL1 holds a shared
task aiming to model unrestricted coreference in
OntoNotes.2 The OntoNotes project has created a
large-scale, accurate corpus for general anaphoric
coreference that covers entities and events not lim-
ited to noun phrases or a limited set of entity types.
And Pradhan et al (2007) have ever used this corpus
for similar unrestricted coreference task.
Our approach to this year?s task could be divided
into two steps: mention identification and creation
of entities. The first stage is conducted on the anal-
ysis of parse trees produced by input data. The of-
ficial data have provided gold and automatic parse
trees for each sentences in training and development
1http://conll.bbn.com/
2http://www.bbn.com/ontonotes/
set. However, according to statistics, almost 3%
mentions have no corresponding constituents in au-
tomatic parse trees. Since only automatic parse trees
will be provided in the final test set, the effect of
parsing errors are inevitable. To alleviate this issue,
based on given automatic parse trees, we modify a
state-of-the-art parser (Charniak and Johnson, 2005)
to generate packed forest, and determine mention
candidates among all constituents from both given
parse tree and packed forest. The packed forest is a
compact representation of all parse trees for a given
sentence. Readers can refer to (Mi et al, 2008) for
detailed definitions.
Once the mentions are identified, the left step is
to group mentions referring to same object into sim-
ilar entity. This problem can be viewed as binary
classification problem of determining whether each
mention pairs corefer. We use a Maximum Entropy
classifier to predict the possibility that two mentions
refer to the similar entity. And mainly following the
work of Luo et al (2004), we use a beam search
algorithm based on Bell Tree to obtain the global
optimal classification.
As this is the first time we participate competi-
tion of coreference resolution, we mainly concen-
trate on developing fault tolerant capability of our
system while omitting feature engineering and other
helpful technologies.
2 Mention Detection
The first step of the coreference resolution tries to
recognize occurrences of mentions in documents.
Note that we recognize mention boundaries only on
development and test set while generating training
76
Figure 1: Left side is parse tree extracted from develop-
ment set, and right side is a forest. ?my daughter? is a
mention in this discourse, however it has no correspond-
ing constituent in parse tree, but it has a corresponding
constituent NP0 in forest.
instances using gold boundaries provided by official
data.
The first stage of our system consists of following
three successive steps:
? Extracting constituents annotated with NP,
NNP, PRP, PRP$ and VBD POS tags from sin-
gle parse tree.
? Extracting constituents with the same tags as
the last step from packed forest.
? Extracting Named Entity recognized by given
data.
It is worth mentioning that above three steps will
produce duplicated mentions, we hence collect all
mentions into a list and discard duplicated candi-
dates. The contribution of using packed forest is that
it extends the searching space of mention candidates.
Figure 1 presents an example to explain the advan-
tage of employing packed forest to enhance the men-
tion detection process. The left side of Figure 1 is
the automatic parse tree extracted from development
set, in which mention ?my daughter? has no corre-
sponding constituent in its parse tree. Under nor-
mal strategy, such mention will not be recognized
and be absent in the clustering stage. However, we
find that mention has its constituent NP0 in packed
forest. According to statistics, when using packed
forest, only 0.5% mentions could not be recognized
while the traditional method is 3%, that means the
theoretical upper bound of our system reaches 99%
compared to baseline?s 97%.
Since the requirement of this year?s task is
to model unrestricted coreference, intuitively, we
should not constraint in recognizing only noun
phrases but also adjective phrase, verb and so on.
However, we find that most mentions appeared in
corpus are noun phrases, and our experimental re-
sults indicate that considering constituents annotated
with above proposed POS tags achieve the best per-
formance.
3 Determining Coreference
This stage is to determine which mentions belong to
the same entity. We train a Maximum Entropy clas-
sifier (Le, 2004) to decide whether two mentions are
coreferent. We use the method proposed by Soon, et
al.?s to generate the training instances, where a posi-
tive instance is formed between current mention Mj
and its closest preceding antecedent Mi, and a neg-
ative instance is created by paring Mj with each of
the intervening mentions, Mi+1, Mi+2,...,Mj?1.
We use the following features to train our classi-
fier.
Features in Soon et al?s work (Soon et al, 2001)
Lexical features
IS PREFIX: whether the string of one mention is
prefix of the other;
IS SUFFIX: whether the string of one mention is
suffix of the other;
ACRONYM: whether one mention is the acronym
of the other;
Distance features
SENT DIST: distance between the sentences con-
taining the two mentions;
MEN DIST: number of mentions between two
mentions;
Grammatical features
IJ PRONOUN: whether both mentions are pro-
noun;
I NESTED: whether mention i is nested in an-
other mention;
J NESTED: whether mention j is nested in an-
other mention;
Syntax features
HEAD: whether the heads of two mentions have
the same string;
HEAD POS: whether the heads of two mentions
have the same POS;
HEA POS PAIRS: pairs of POS of the two men-
tions? heads;
77
Semantic features
WNDIST: distance between two mentions in
WordNet;
I ARG0: whether mention i has the semantic role
of Arg0;
J ARG0: whether mention j has the semantic role
of Arg0;
IJ ARGS: whether two mentions have the seman-
tic roles for similar predicate;
In the submitted results, we use the L-BFGS pa-
rameter estimation algorithm with gaussian prior
smoothing (Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 100 itera-
tions.
3.1 Creation of Entities
This stage aims to create the mentions detected in
the first stage into entities, according to the predic-
tion of classifier. One simple method is to use a
greedy algorithm, by comparing each mention to its
previous mentions and refer to the one that has the
highest probability. In principle, this algorithm is
too greedy and sometimes results in unreasonable
partition (Ng, 2010). To address this problem, we
follow the literature (Luo et al, 2004) and propose
to use beam search to find global optimal partition.
Intuitively, creation of entities can be casted as
partition problem. And the number of partitions
equals the Bell Number (Bell, 1934), which has a
?closed? formula B(n) = 1e
??
k=0
kn
k! . Clearly, this
number is very huge when n is large, enumeration of
all partitions is impossible, so we instead designing
a beam search algorithm to find the best partition.
Formally, the task is to optimize the following ob-
jective,
y? = argmax
??P
?
e??
Prob(e) (1)
where P is all partitions, Prob(e) is the cost of
entity e. And we can use the following formula to
calculate the Prob(e),
Prob(e) =
?
i?e,j?e
pos(mi,mj)
+
?
i?e,j /?e
neg(mi,mj)
(2)
where pos(mi,mj) is the score predicted by clas-
sifier that the possibility two mentions mi and mj
group into one entity, and neg(mi,mj) is the score
that two mentions are not coreferent.
Theoretically, we can design a dynamic algorithm
to obtain the best partition schema. Providing there
are four mentions from A to D, and we have ob-
tained the partitions of A, B and C. To incorporate
D, we should consider assigning D to each entity of
every partition, and generate the partitions of four
mentions. For detailed explanation, the partitions
of three mentions are [A][B][C], [AB][C], [A][BC]
and [ABC], when considering the forth mention D,
we generate the following partitions:
? [A][B][C][D], [AD][B][C], [A][BD][C],
[A][B][CD]
? [AB][C][D], [ABD][C],[AB][CD]
? [A][BC][D], [AD][BC], [A][BCD]
? [ABC][D], [ABCD]
The score of partition [AD][B][C] can be
calculated by score([A][B][C]) + pos(A,D) +
neg(B,D) + neg(C,D). Since we can computer
pos and neg score between any two mentions in
advance, this problem can be efficiently solved by
dynamic algorithm. However, in practice, enumer-
ating the whole partitions is intractable, we instead
exploiting a beam with size k to store the top k parti-
tions of current mention size, according to the score
the partition obtain. Due to the scope limitation, we
omit the detailed algorithm, readers can refer to Luo
et al (2004) for detailed description, since our ap-
proach is almost similar to theirs.
4 Experiments
4.1 Data Preparation
The shared task provided data includes information
of lemma, POS, parse tree, word sense, predicate
arguments, named entity and so on. In addition to
those information, we use a modified in house parser
to generate packed forest for each sentence in devel-
opment set, and prune the packed forest with thresh-
old p=3 (Huang, 2008). Since the OntoNotes in-
volves multiple genre data, we merge all files and
78
Mention MUC BCUBED CEAFM CEAFE BLANC
baseline 58.97% 44.17% 63.24% 45.08% 37.13% 62.44%
baseline gold 59.18% 44.48% 63.46% 45.37% 37.47% 62.36%
sys forest 59.07% 44.4% 63.39% 45.29% 37.41% 62.41%
sys btree 59.44% 44.66% 63.77% 45.62% 37.82% 62.47%
sys forest btree 59.71% 44.97% 63.95% 45.91% 37.96% 62.52%
Table 1: Experimental results on development set (F score).
Mention MUC BCUBED CEAFM CEAFE BLANC
sys1 54.5% 39.15% 63.91% 45.32% 37.16% 63.18%
sys2 53.06% 35.55% 59.68% 38.24% 32.03% 50.13%
Table 2: Experimental results on development set with different training division (F score).
take it as our training corpus. We use the sup-
plied score toolkit 3 to compute MUC, BCUBED,
CEAFM, CEAFE and BLANC metrics.
4.2 Experimental Results
We first implement a baseline system (baseline)
that use single parse tree for mention detection
and greedy algorithm for creation of entities. We
also run the baseline system using gold parse tree,
namely baseline gold. To investigate the contribu-
tion of packed forest, we design a reinforced sys-
tem, namely sys forest. And another system, named
as sys btree, is used to see the contribution of beam
search with beam size k=10. Lastly, we combine
two technologies and obtain system sys forest btree.
Table 1 shows the experimental results on devel-
opment data. We find that the system using beam
search achieve promising improvement over base-
line. The reason for that has been discussed in last
section. We also find that compared to baseline,
sys forest and baseline gold both achieve improve-
ment in term of some metrics. And we are glad to
find that using forest, the performance of our sys-
tem is approaching the system based on gold parse
tree. But even using the gold parse tree, the im-
provement is slight. 4 One reason is that we used
some lexical and grammar features which are dom-
3http://conll.bbn.com/download/scorer.v4.tar.gz
4Since under task requirement, singleton mentions are fil-
tered out, it is hard to recognize the contribution of packed for-
est to mention detection, while we may incorrectly resolve some
mentions into singletons that affects the score of mention detec-
tion.
inant during prediction, and another explanation is
that packed forest enlarges the size of mentions but
brings difficulty to resolve them.
To investigate the effect of different genres to de-
velop set, we also perform following compared ex-
periments:
? sys1: all training corpus + WSJ development
corpus
? sys2: WSJ training corpus + WSJ development
corpus
Table 2 indicates that knowledge from other genres
can help coreference resolution. Perhaps the reason
is the same as last experiments, where syntax diver-
sity affects the task not very seriously.
5 Conclusion
In this paper, we describe our system for CoNLL-
2011 shared task. We propose to use packed for-
est and beam search to improve the performance of
coreference resolution. Multiple experiments prove
that such improvements do help the task.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004.
We would like to thank the anonymous reviewers
for suggestions, and SHUGUANG COMPUTING
PLATFORM for supporting experimental platform.
79
References
E.T. Bell. 1934. Exponential numbers. The American
Mathematical Monthly, 41(7):411?419.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio, June.
Z. Le. 2004. Maximum entropy modeling toolkit for
Python and C++.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 135?es. As-
sociation for Computational Linguistics.
H. Mi, L. Huang, and Q. Liu. 2008. Forestbased transla-
tion. In Proceedings of ACL-08: HLT, pages 192?199.
Citeseer.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
80
