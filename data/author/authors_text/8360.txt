Combining Linguistic and Machine Learning Techniques for Email
Summarization
Smaranda Muresan
Dept. of Computer Science
Columbia University
500 West 120 Street
New York, NY, 10027
smara@cs.columbia.edu
Evelyne Tzoukermann
Bell Laboratories
Lucent Technologies
700 Mountain Avenue
Murray Hill, NJ, 07974
evelyne@lucent.com
Judith L. Klavans
Columbia University
Center for Research on
Information Access
535 West 114th Street
New York, NY 10027
klavans@cs.columbia.edu
Abstract
This paper shows that linguistic tech-
niques along with machine learning
can extract high quality noun phrases
for the purpose of providing the gist
or summary of email messages. We
describe a set of comparative experi-
ments using several machine learning
algorithms for the task of salient noun
phrase extraction. Three main conclu-
sions can be drawn from this study: (i)
the modifiers of a noun phrase can be
semantically as important as the head,
for the task of gisting, (ii) linguistic fil-
tering improves the performance of ma-
chine learning algorithms, (iii) a combi-
nation of classifiers improves accuracy.
1 Introduction
In this paper we present a comparative study of
symbolic machine learning models applied to nat-
ural language task of summarizing email mes-
sages through topic phrase extraction.
Email messages are domain-general text, they
are unstructured and not always syntactically well
formed. These characteristics raise challenges for
automatic text processing, especially for the sum-
marization task. Our approach to email summa-
rization, implemented in the GIST-IT system, is
to identify topic phrases, by first extracting noun
phrases as candidate units for representing doc-
ument meaning and then using machine learning
algorithms to select the most salient ones.
The comparative evaluation of several machine
learning models in the settings of our experiments
indicates that : (i) for the task of gisting the mod-
ifiers of the noun phrase are equally as important
as the head, (ii) noun phrases are better than n-
grams for the phrase-level representation of the
document, (iii) linguistic filtering enhances ma-
chine learning techniques, (iv) a combination of
classifiers improves accuracy.
Section 2 of the paper outlines the machine
learning aspect of extracting salient noun phrases,
emphasizing the features used for classifica-
tion and the symbolic machine learning models
used in the comparative experiments. Section
3 presents the linguistic filtering steps that im-
prove the accuracy of the machine learning algo-
rithms. Section 4 discusses in detail our conclu-
sions stated above.
2 Machine Learning for Content
Extraction
Symbolic machine learning has been applied suc-
cessfully in conjunction with many NLP applica-
tions (syntactic and semantic parsing, POS tag-
ging, text categorization, word sense disambigua-
tion) as reviewed by Mooney and Cardie (1999).
We used machine learning techniques for finding
salient noun phrases that can represent the sum-
mary of an email message. This section describes
the three steps involved in this classification task:
1) what representation is appropriate for the infor-
mation to be classified as relevant or non-relevant
(candidate phrases), 2) which features should be
associated with each candidate, 3) which classifi-
cation models should be used.
Case 1
CNP: scientific/JJ and/CC technical/JJ articles/NNS
SNP1: scientific/JJ articles/NNS
SNP2: technical/JJ articles/NNS
Case 2
CNP: scientific/JJ thesauri/NNS and databases/NNS
SNP1: scientific/JJ thesauri/NNS
SNP2: scientific/JJ databases/NNS
Case 3
CNP: physics/NN and/CC biology/NN skilled/JJ researchers/NNS
SNP1: physics/NN skilled/JJ researchers/NNS
SNP2: biology/NN skilled/JJ researchers/NNS
Table 1: Resolving Coordination of NPs
2.1 Candidate Phrases
Of the major syntactic constituents of a sentence,
e.g. noun phrases, verb phrases, and prepositional
phrases, we assume that noun phrases (NPs) carry
the most contentful information about the doc-
ument, a well-supported hypothesis (Smeaton,
1999; Wacholder, 1998).
As considered by Wacholder (1998), the sim-
ple NPs are the maximal NPs that contain pre-
modifiers but not post-nominal constituents such
as prepositions or clauses. We chose simple NPs
for content representation because they are se-
mantically and syntactically coherent and they are
less ambiguous than complex NPs. For extracting
simple noun phrases we first used Ramshaw and
Marcus?s base NP chunker (Ramshaw and Mar-
cus, 1995). The base NP is either a simple NP or
a coordination of simple NPs. We used heuristics
based on POS tags to automatically split the co-
ordinate NPs into simple ones, properly assigning
the premodifiers. Table 1 presents some coordi-
nate NPs (CNP) encountered in our data collec-
tion and the results of our algorithm which split
them into simple NPs (SNP1 and SNP2).
2.2 Features used for Classification
The choice of features used to represent the can-
didate phrases has a strong impact on the accu-
racy of the classifiers (e.g. the number of exam-
ples needed to obtain a given accuracy on the test
data, the cost of classification). For our classifica-
tion task of determining if a noun phrase is salient
or not to the document meaning, we chose a set of
nine features.
Several studies rely on the linguistic intuition
that the head of the noun phrase makes a greater
contribution to the semantics of the nominal
group than the modifiers. However, for some
specific tasks in NLP , the head is not necessar-
ily the most semantically important part of the
noun phrase. In analyzing email messages from
the perspective of finding salient NPs, we claim
that the modifier(s) of the noun phrase - usually
nominal modifiers(s), often have as much seman-
tic content as the head. This opinion is also sup-
ported in the work of Strzalkowski et al (1999),
where syntactic NPs are captured for the goal
of extracting their semantic content but are pro-
cessed as an ?ordered? string of words rather than
a syntactic unit. Thus we introduce as a sepa-
rate feature in the feature vector, a new TF*IDF
measure which consider the NP as a sequence of
equally weighted elements, counting individually
the modifier(s) and the head.
Consider the following list of simple NPs se-
lected as candidates:
1. conference workshop announcement
2. international conference
3. workshop description
4. conference deadline
In the case of the first noun phrase, for exam-
ple, its importance is found in the two noun mod-
ifiers: conference and workshop as much as in
the head announcement, due to their presence as
heads or modifiers in the candidate NPs 2-4. Our
new feature will be: TF  IDF
conference
+TF 
IDF
workshop
+ TF  IDF
announcement
. Giving
these linguistic observations we divided the set of
features into three groups, as we mentioned also
in (Tzoukermann et al, 2001): 1) one associated
with the head of the noun phrase; 2) one associ-
ated with the whole NP and 3) one that represents
the new TF*IDF measure discussed above.
2.2.1 Features associated with the Head
We choose two features to characterize the
head of the noun phrases:
 head tfidf: the TF*IDF measure of the
head of the candidate NP. For the NP in
example (1) this feature will be TF 
IDF
announcement
.
 head focc: The position of the first occur-
rence of the head in text (the number of
words that precede the first occurrence of the
head divided by the total number of words in
the document).
2.2.2 Features associated with the whole NP
We select six features that we consider relevant
in determining the relative importance of the noun
phrase:
 np tfidf: the TF*IDF measure of
the whole NP. For the NP in the
example (1) this feature will be
TF  IDF
conference workshop announcement
.
 np focc: The position of the first occurrence
of the noun phrase in the document.
 np length words: Noun phrase length mea-
sured in number of words, normalized by di-
viding it with the total number of words in
the candidate NP list.
 np length chars: Noun phrase length mea-
sured in number of characters, normalized
by dividing it with the total number of char-
acters in the candidate NPs list.
 sent pos: Position of the noun phrase in the
sentence: the number of words that precede
the noun phrase, divided by sentence length.
For noun phrases in the subject line (which
are usually short and will be affected by this
measure), we consider the maximum length
of sentence in document as the normalization
factor.
 par pos: Position of noun phrase in para-
graph, same as sent pos, but at the paragraph
level.
2.2.3 Feature that considers all constituents
of the NP equally weighted
One of the important hypotheses we tested in
this work is that both the modifiers and the head
of NP contribute equally to its salience. Thus we
consider mh tfidf as an additional feature in the
feature vector.
 mh tfidf: the new TF*IDF measure that
takes also into consideration the importance
of the modifiers. In our example the value of
this feature will be : TF  IDF
conference
+
TF IDF
workshop
+TF IDF
announcement
In computing the TF*IDF measures (head tfidf,
np tfidf, mh tfidf), specific weights, w
i
, were as-
signed to account for the presence in the email
subject line and/or headlines in the email body.
 w
i1
: presence in the subject line and head-
line
 w
i2
: presence in the subject line
 w
i3
: presence in headlines where w
i1
> w
i2
> w
i3
.
These weights were manually chosen after a set
of experiments, but we plan to use a regression
method to automatically learn them.
2.3 Symbolic Machine Learning Models
We compared three symbolic machine learning
paradigms (decision trees, rule induction and de-
cision forests) applied to the task of salient NP
extraction, evaluating five classifiers.
2.3.1 Decision Tree Classifiers
Decision trees classify instances represented as
feature vectors, where internal nodes of the tree
test one or several attributes of the instance and
where the leaves represent categories. Depending
on how the test is performed at each node, there
exists two types of decision tree classifiers: axis
parallel and oblique. The axis-parallel decision
trees check at each node the value of a single at-
tribute. If the attributes are numeric, the test has
the form x
i
> t, where x
i
is one of the attribute
of an instance and t is the threshold. Oblique de-
cision trees test a linear combination of attributes
at each internal node:
n
X
i=1
a
i
x
i
+ a
n+1
> 0
where a
i
; :::; a
n+1
are real-valued coefficients.
We compared the performance of C4.5, an axis-
parallel decision tree classifier (Quinlan, 1993)
and OC1, an oblique decision tree classifier
(Murthy et al, 1993).
2.3.2 Rule Induction Classifiers
In rule induction, the goal is to learn the small-
est set of rules that capture all the generalisable
knowledge within the data. Rule induction clas-
sification is based on firing rules on a new in-
stance, triggered by the matching feature values
to the left-hand side of the rules. Rules can be of
various normal forms and can be ordered. How-
ever, the appropriate ordering can be hard to find
and the key point of many rule induction algo-
rithms is to minimize the search strategy through
the space of possible rule sets and orderings. For
our task, we test the effectiveness of two rule
induction algorithms : C4.5rules that form pro-
duction rules from unpruned decision tree, and
a fast top-down propositional rule learning sys-
tem, RIPPER (Cohen, 1995). Both algorithms
first construct an initial model and then iteratively
improve it. C4.5rules improvement strategy is a
greedy search, thus potentially missing the best
rule set. Furthermore, as discussed in (Cohen,
1995), for large noisy datasets RIPPER starts with
an initial model of small size, while C4.5rules
starts with an over-large initial model. This means
that RIPPER?s search is more efficient for noisy
datasets and thus is more appropriate for our data
collection. It also allows the user to specify the
loss ratio, which indicates the ratio of the cost of
false positives to the cost of false negatives, thus
allowing a trade off between precision and recall.
This is crucial for our analysis since we deal with
sparse data due to the fact that in a document the
number of salient NPs is much smaller than the
number of irrelevant NPs.
2.3.3 Decision Forest Classifier
Decision forests are a collection of decision
trees together with a combination function. We
test the performance of DFC (Ho, 1998), a deci-
sion forest classifier that systematically constructs
decision trees by pseudo-randomly selecting sub-
sets of components of feature vectors. The advan-
tage of this classifier is that it combines a set of
different classifiers in order to improve accuracy.
It implements different splitting functions. In the
setting of our evaluation we tested the informa-
tion gain ratio (similar to the one used by Quinlan
in C4.5). An augmented feature vector (pairwise
sums, differences, and products of features) was
used for this classifier.
3 Linguistic Knowledge Enhances
Machine Learning
Not all simple noun phrases are equally
important to reflect document meaning.
Boguraev and Kennedy (1999) discuss the
issue that for the task of document gisting, topical
noun phrases are usually noun-noun compounds.
In our work, we rely on ML techniques to decide
which are the salient NPs, but we claim that a
shallow linguistic filtering applied before the
learning process improves the accuracy of the
classifiers. We performed four filtering steps:
1. Inflectional morphological processing:
Grouping inflectional variants together can
help especially in case of short documents
(which is sometimes the case for email
messages). English nouns have only two
kinds of regular inflection: a suffix for
the plural mark and another suffix for the
possessive one.
2. Removing unimportant modifiers: In this
second step we remove the determiners that
accompany the nouns and also the auxil-
iary words most and more that form the pe-
riphrastic forms of comparative and superla-
tive adjectives modifying the nouns (e.g.
?the most complex morphology? will be fil-
tered to ?complex morphology?).
3. Removing common words: We used a list
of 571 common words used in IR systems
in order to further filter the list of candi-
date NPs. Thus, words like even, following,
every, are eliminated from the noun phrase
structure.
4. Removing empty nouns: Words like lot,
group, set, bunch are considered empty
heads. For example the primary concept of
the noun phrases like ?group of students?,
?lots of students? or ?bunch of students?
is given by the noun ?students?. We ex-
tracted all the nouns that appear in front of
the preposition ?of? and then sorted them by
frequency of appearance. A threshold was
then used to select the final list (Klavans et
al., 1990). Three different data collections
were used: the Brown corpus, the Wall Street
Journal, and a set of 4000 email messages
(most of them related to a conference orga-
nization). We generated a set of 141 empty
nouns that we used in this forth step of the
filtering process.
4 Results and Discussion
One important step in summarization is the dis-
covery of the relevant information from the source
text. Our approach was to extract the salient NPs
using linguistic knowledge and machine learning
techniques. Our evaluation corpus consists of a
collection of email messages which is heteroge-
neous in genre, length, and topic. We used 2,500
NPs extracted from 51 email messages as a train-
ing set and 324 NPs from 8 messages for testing.
Each NP was manually tagged for saliency by one
human judge. We are planning to add more judges
in the future and measure the interuser agreement.
This section outlines a comparative evaluation
of five classifiers using two feature settings on the
task of extracting salient NPs from email mes-
sages. The evaluation shows the following im-
portant results:
Result 1. In the context of gisting, the head-
modifier relationship is an ordered relation be-
tween semantically equal elements.
We evaluate the impact of adding mh tfidf (see
section 2.2), as an additional feature in the feature
vector. This is shown in Table 2 in the different
feature vectors fv1 and fv2. The first feature vec-
tor, fv1, contains the features in sections 2.2.1 and
2.2.2, while fv2 includes as an additional feature
mh tfidf.
As can be seen from Table 3, the results of eval-
uating these two feature settings using five differ-
ent classifiers, show that fv2 performed better than
fv1. For example, the DFC classifier shows an in-
crease both in precision and recall. This allows us
to claim that in the context of gisting, the syntactic
head of the noun phrase is not always the seman-
tic head, and modifiers can have also an important
role.
One advantage of the rule-induction algorithms
is that their output is easily interpretable by hu-
mans. Analyzing C4.5rules output, we gain an
insight on the features that contribute most in the
classification process. In case of fv1, the most im-
portant features are: the first appearance of the
NP and its head (np focc, head focc), the length
of NP in number of words (np length words) and
the tf*idf measure of the whole NP and its head
(np tfidf, head tfidf ). For example:
 IF head focc <= 0.0262172 AND np tfidf
> 0.0435465 THEN Relevant
 IF np focc <= 0.912409 AND
np length words > 0.0242424 THEN
Relevant
 IF head tfidf <= 0.0243452 AND np tfidf
<= 0.0435465 AND np length words <=
0.0242424 then Not relevant
In case of fv2, the new feature m tfidf impacts
the rules for both Relevant and Not relevant cat-
egories. It supercedes the need for np tfidf and
head tfidf, as can be seen also from the rules be-
low:
 IF mh tfidf > 0.0502262 AND np focc <=
0.892585 THEN Relevant
 IF mh tfidf > 0.0180134 AND
np length words > 0.0260708 THEN
Relevant
 IF mh tfidf <= 0.0223546 AND
np length words <= 0.0260708 THEN
Not relevant
 IF mh tfidf <= 0.191205 AND np focc >
0.892585 THEN Not relevant
Feature vector 1 (fv1)
head focc head tfidf np focc np tfidf np length chars np length words par pos sent pos
Feature vector 2 (fv2)
head focc head tfidf mh tfidf np focc np tfidf np length chars np length words par pos sent pos
Table 2: Two feature settings to evaluate the impact of mh tfidf
C4.5 OC1 C4.5 rules Ripper DFC
p r p r p r p r p r
fv1 73.3% 78.6% 73.7% 93% 73.7% 88.5% 83.6% 71.4% 80.3% 83.5%
fv2 70% 88.9% 82.3% 88% 73.7% 95% 85.7% 78.8% 85.7% 87.9%
Table 3: Evaluation of two feature vectors using five classifiers
Result 2. Classifiers? performance depends
on the characteristics of the corpus, and com-
bining classifiers improves accuracy
This result was postulated by evaluating the
performance of five different classifiers in the task
of extracting salient noun phrases. As measures
of performance we use precision and recall . The
evaluation was performed according to what de-
gree the output of the classifiers corresponds to
the user judgments and the results are presented
in Table 3.
We first compare two decision tree classifiers:
one which uses as the splitting function only a sin-
gle feature (C4.5) and the other, the oblique tree
classifier (OC1) which at each internal node tests
a linear combination of features. Table 3 shows
that OC1 outperforms C4.5.
Columns 4 and 5 from Table 3 show the rela-
tive performance of RIPPER and C4.5rules. As
discussed in (Cohen, 1995), RIPPER is more ap-
propriate for noisy and sparse data collection than
C4.5rules. Table 3 shows that RIPPER performs
better than C4.5rules in terms of precision.
Finally, we investigate whether a combination
of classifiers will improve performance. Thus we
choose the Decision Forest Classifier, DFC, to
perform our test. DFC obtains the best results,
as can be seen from column 6 of Table 3.
Result 3. Linguistic filtering is an important
step in extracting salient NPs
As seen from Result 2, the DFC performed best
in our task, so we chose only this classifier to
present the impact of linguistic filtering. Table
4 shows that linguistic filtering improves preci-
sion and recall, having an important role espe-
cially on fv2, where the new feature, mh tfidf was
used (from 69.2% precision and 56.25% recall to
85.7% precision and 87.9% recall).
without filtering with filtering
precision recall precision recall
fv1 75% 75% 80.3% 83.5%
fv2 69.2% 56.25% 85.7% 87.9%
Table 4: Evaluation of linguistic filtering
This is explained by the fact that the filter-
ing presented in section 3 removed the noise in-
troduced by unimportant modifiers, common and
empty nouns.
Result 4. Noun phrases are better candi-
dates than n-grams
Presenting the gist of an email message by
phrase extraction addresses one obvious question:
are noun-phrases better than n-grams for repre-
senting the document content? To answer this
question we compared the results of our system,
GIST-IT, that extracts linguistically well moti-
vated phrasal units, with KEA output, that ex-
tracts bigrams and trigrams as key phrases using
a Na?ive Bayes model (Witten et al, 1999). Table
5 shows the results on one email message. The
n-gram approach of KEA system extracts phrases
like sort of batch, extracting lots, wn, and even
URLs that are unlikely to represent the gist of a
document. This is an indication that the linguis-
tically motivated GIST-IT phrases are more use-
ful for document gisting. In future work we will
perform also a task-based evaluation of these two
GIST-IT KEA
perl module wordnet interface module
?wn? command line program sort of batch
simple easy perl interface WordNet data
wordnet.pm module accesses the WordNet
wordnet system lots of WordNet
query perl module WordNet perl
wordnet QueryData
wordnet package wn
wordnet relation perl module
command line extracting
wordnet data use this module
included man page extracting lots
free software WordNet system
querydata www.cogsci.princeton.edu
Table 5: Salient phrase extraction with GIST-IT vs. KEA on one email message
approaches, to test usability.
5 Related Work
Machine learning has been successfully applied
to different natural language tasks, including text
summarization. A document summary is seen
as a succinct and coherent prose that captures
the meaning of the text. Prior work in docu-
ment summarization has been mostly based on
sentence extraction. Kupiec et al (1995) use ma-
chine learning for extracting the most impor-
tant sentences of the document. But extrac-
tive summarization relies on the properties of
source text that emails typically do not have:
coherence, grammaticality, well defined struc-
ture. Berger and Mittal (2000) present a summa-
rization system, named OCELOT that provides
the gist of the web documents based on proba-
bilistic models. Their approach is closed related
with statistical machine translation.
As discussed in (Boguraev and Kennedy,
1999), the meaning of ?summary? should be ad-
justed depending on the information management
task for which it is used. Key phrases, for ex-
ample, can be seen as semantic metadata that
summarize and characterize documents (Witten
et al, 1999; Turney, 2000). These approaches
select a set of candidate phrases (bigrams or tri-
grams) and then apply Na?ive Bayes learning to
classify them as key phrases or not. But deal-
ing only with n-grams does not always provide
good output in terms of a summary. In (Bogu-
raev and Kennedy, 1999) the ?gist? of a document
is seen as a sequence of salient objects, usually
topical noun phrases, presented in a highlighted
context. Their approach is similar to extracting
technical terms (Justeson and Katz, 1995). Noun
phrases are used also in IR task (Strzalkowski et
al., 1999; Smeaton, 1999; Sparck Jones, 1999).
The work of Strzalkowski et al (1999) supports
our hypothesis that for some NLP tasks (gisting,
IR) the head+modifier relation of a noun phrase is
in fact an ordered relation between semantically
equally important elements.
6 Conclusions and Future Work
In this paper we presented a novel technique for
document gisting suitable for domain and genre
independent collections such as email messages.
The method extracts simple noun phrases using
linguistic techniques and then uses machine learn-
ing to classify them as salient for the document
content. The contributions of this work are:
1. From a linguistic standpoint, we demon-
strated that the modifiers of a noun phrase
can be as semantically important as the head
for the task of gisting.
2. From a machine learning standpoint, we
evaluated the power and limitation of sev-
eral classifiers: decision trees, rule induc-
tion, and decision forests classifiers.
3. We proved that linguistic knowledge can en-
hance machine learning by evaluating the
impact of linguistic filtering before applying
the learning scheme.
The study, the evaluation, and the results pro-
vide experimental grounds for research not only
in summarization, but also in information extrac-
tion and topic detection.
References
A.L. Berger and V.O. Mittal. 2000. OCELOT:A sys-
tem for summarizing web pages. In Proceedings of
the 23rd Anual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 144?151, Athens, Greece.
B. Boguraev and C. Kennedy. 1999. Salience-based
content characterisation of text documents. In In-
terjit Mani and T. Maybury, Mark, editors, Ad-
vances in Automatic Text Summarization, pages 99?
111. The MIT Press.
W. Cohen. 1995. Fast effective rule induction. In
Machine-Learning: Proceedings of the Twelfth In-
ternational Conference.
T.K. Ho. 1998. The random subspace method
for constructing decision forests. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
20(8):832?844.
J. Justeson and S. Katz. 1995. Technical terminol-
ogy: Some linguistic properties and an algorithm
for identification in text. Natural Language Engi-
neering, (1):9?27.
J.L. Klavans, M.S. Chodorow, and N. Wacholder.
1990. From dictionary to knowledge base via
taxonomy. In Proceedings of the Sixth Confer-
ence of the University of Waterloo Centre for the
New Oxford English Dictionary and Text Research:
Electronic Text Research, University of Waterloo,
Canada.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings on the
18th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68?73, Seattle,WA.
R.J Mooney and C. Cardie. 1999. Symbolic ma-
chine learning for natural language processing. In
ACL?99 Tutorial.
S.K. Murthy, S. Kasif, S. Salzberg, and R. Beigel.
1993. OC1: Randomized induction of oblique de-
cision trees. In Proceedings of the Eleventh Na-
tional Conference on Artificial Intelligence, pages
322?327, Washington, D.C.
J.R Quinlan. 1993. C4.5: Program for Machine
Learning. Morgan Kaufmann Publisher, San Ma-
teo, California.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Pro-
ceedings of Third ACL Workshop on Very Large
Corpora.
A. Smeaton. 1999. Using NLP or NLP resources
for information retrieval tasks. In Tomek Strza-
lkowski, editor, Natural Language Information Re-
trieval. Kluwer, Boston, MA.
K. Sparck Jones. 1999. What is the role for NLP in
text retrieval. In Tomek Strzalkowski, editor, Nat-
ural Language Information Retrieval, pages 1?12.
Kluwer, Boston, MA.
T. Strzalkowski, F. Lin, J. Wang, and J. Perez-
Carballo. 1999. Evaluating natural language pro-
cessing techniques in information retrieval. In
Tomek Strzalkowski, editor, Natural Language In-
formation Retrieval. Kluwer, Boston, MA.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336,
May.
E Tzoukermann, S Muresan, and J.L. Klavans.
2001. GIST-IT: Summarizing email using linguis-
tic knowledge and machine learning. In Proceeding
of the HLT and KM Workshop, EACL/ACL 2001.
N. Wacholder. 1998. Simplex NPS sorted by head:
A method for identifying significant topics within
a document. In Proceedings of the COLING-ACL
Workshop on the Computational Treatment of Nom-
inals, Montreal, Canada.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
C.G. Nevill-Manning. 1999. KEA: Practical au-
tomatic keyphrase extraction. In Proceedings of
DL?99, pages 254?256.
GIST-IT: Summarizing Email Using Linguistic Knowledge and Machine 
Learning  
 
Evelyne Tzoukermann 
Bell Labs, Lucent 
Technologies 
700 Mountain Avenue 
Murray Hill, NJ, 07974, USA 
evelyne@lucent.com 
Smaranda Muresan 
Columbia University  
500 W 120th Street 
New York, NY, 10027, USA 
smara@cs.columbia.edu 
Judith L. Klavans 
Columbia University 
Center for Research on 
Information Access 
535 W 114th Street 
New York, NY, 10027, USA 
klavans@cs.columbia.edu 
 
Abstract  
We present a system for the automatic 
extraction of salient information from 
email messages, thus providing the gist of 
their meaning.   Dealing with email raises 
several challenges that we address in this 
paper:  heterogeneous data in terms of 
length and topic. Our method combines 
shallow linguistic processing with 
machine learning to extract phrasal units 
that are representative of email content. 
The GIST-IT application is fully 
implemented and embedded in an active 
mailbox platform.  Evaluation was 
performed over three machine learning 
paradigms.  
Introduction 
The volume of email messages is huge and 
growing.  A qualitative and quantitative study of 
email overload [Whittaker and Sidner (1996)] 
shows that people receive a large number of 
email messages each day (~ 49) and that 21% of 
their   inboxes (about 334 messages) are long 
messages (over 10 Kbytes).  Therefore 
summarization techniques adequate for real-
world applications are of great interest and need 
[Berger and Mittal (2000), McKeown and Radev 
(1995), Kupiec et al(1995), McKeown et al
(1999), Hovy (2000)].  
  
In this paper we present GIST-IT, an 
automatic email message summarizer that will 
convey to the user the gist of the document 
through topic phrase extraction, by combining 
linguistic and machine learning techniques. 
 Email messages and web documents raise 
several challenges to automatic text 
processing, and the summarization task 
addresses most of them: they are free-style 
text, not always syntactically or 
grammatically well-formed, domain and 
genre independent, of variable length and on 
multiple topics.  Furthermore, due to the lack 
of well-formed syntactic and grammatical 
structures, the granularity of document 
extracts presents another level of complexity.  
In our work, we address the extraction 
problem at phrase-level [Ueda et al(2000), 
Wacholder et al(2000)], identifying salient 
information that is spread across multiple 
sentences and paragraphs.           
Our novel approach first extracts simple 
noun phrases as candidate units for 
representing document meaning and then 
uses machine learning algorithms to select 
the most prominent ones.  This combined 
method allows us to generate an informative, 
generic, ?at-a-glance? summary.  
In this paper, we show: (a) the efficiency 
of the linguistic approach for phrase 
extraction in comparing results with and 
without filtering techniques,  (b) the 
usefulness of vector representation in 
determining proper features to identify 
contentful information, (c) the benefit of 
using a new measure of TF*IDF for the noun 
phrase and its constituents, (d) the power of 
machine learning systems in evaluating 
several classifiers in order to select the one 
performing the best for this task. 
1 Related work  
Traditionally a document summary is seen as a 
small, coherent prose that renders to the user the 
important meaning of the text. In this framework 
most of the research has focused on extractive 
summaries at sentence level. However, as 
discussed in [Boguraev and Kennedy (1999)], 
the meaning of ?summary? should be adjusted 
depending on the information management task 
for which it is used. Key phrases, for example, 
can be seen as semantic metadata that 
summarize and characterize documents [Witten 
et al(1999), Turney (1999)]. These approaches 
select a set of candidate phrases (sequence of 
one, two or three consecutive stemmed, non-stop 
words) and then apply machine learning 
techniques to classify them as key phrases or 
not. But dealing only with n-grams does not 
always provide good output in terms of a 
summary (see discussion in Section 5.4).      
 Wacholder (1998) proposes a linguistically-
motivated method for the representation of the 
document aboutness: ?head clustering?. A list of 
simple noun phrases is first extracted, clustered 
by head and then ranked by the frequency of the 
head. Klavans et al(2000) report on the 
evaluation of ?usefulness? of head clustering in 
the context of browsing applications, in terms of 
quality and coverage.  
  Other researchers have used noun-phrases 
quite successfully for information retrieval task  
[Strzalkowski et al(1999), Sparck-Jones 
(1999)]. Strzalkowski et al(1999) uses head + 
modifier pairs as part of a larger system 
which constitutes the ?stream model? that is 
used for information retrieval. They treat the 
head-modifier relationship as an ?ordered 
relation between otherwise equal elements?, 
emphasizing that for some tasks, the syntactic 
head of the NP is not necessarily a semantic 
head, and the modifier is not either 
necessarily a semantic modifier and that the 
opposite is often true. Using a machine 
learning approach, we proved this hypothesis 
for the task of gisting.  
 Berger and Mittal (2000) present a 
summarization system named OCELOT, 
based on probabilistic models, which 
provides the gist of web documents. Like 
email messages, web documents are also very 
heterogeneous and their unstructured nature 
pose equal difficulties.  
In this paper, we propose a novel 
technique for summarization that combines 
the linguistic approach of extracting simple 
noun phrases as possible candidates for 
document extracts, and the use of machine 
learning algorithms to automatically select 
the most salient ones. 
2 System architecture 
The input to GIST-IT is a single email 
message. The architecture, presented in 
Figure 1 consists of four distinct functional 
components.  The first module is an email 
preprocessor developed for Text-To-Speech 
 
HPDLO 
PHVVDJH 
(  0DLO 3UHS 
7RNHQL]DWLRQ 
6LPSOH 13  
([WUDFWLRQ  
13 ILOWHULQJ 
13 ([WUDFWLRQ DQG )LOWHULQJ 8QLW 
)HDWXUH  
VHOHFWLRQ 
)HDWXUH  
VHOHFWLRQ 
&ODVVLILFDWLRQ  
0RGHO 
13  
FODVVLILFDWLRQ 
JLVW RI HPDLO  
PHVVDJH 
SUHVHQWDWLRQ 
0/ 8QLW 
 
Figure 1 System Architecture 
applications. The second component is a shallow 
text processing unit, which is actually a pipeline 
of modules for extraction and filtering of simple 
NP candidates.  The third functional component 
is a machine learning unit, which consists of a 
feature selection module and a text classifier. 
This module uses a training set and a testing set 
that were devided from our email corpus.  In 
order to test the performance of GIST-IT on the 
task of summarization, we use a heterogeneous 
collection of email messages in genre, length, 
and topic.  We represent each email as a set of 
NP feature vectors.  We used 2,500 NPs 
extracted from 51 email messages as a training 
set and 324 NPs from 8 messages for testing. 
Each NP was manually tagged for saliency by 
one of the authors and we are planning to add 
more judges in the future. The final module 
deals with presentation of the gisted email 
message. 
2.1 The Email Preprocessor 
This module uses finite-state transducer 
technology in order to identify message content.  
Information at the top of the message related to 
?From/To/Date'' as well as the signature block 
are separated from the message content. 
2.2 Candidate Simple Noun Phrase Extraction and 
Filtering Unit 
This module performs shallow text processing 
for extraction and filtering of simple NP 
candidates, consisting of a pipeline of three 
modules: text tokenization, NP extraction, and 
NP filtering. Since the tool was created to 
preprocess email for speech output, some of the 
text tokenization suitable for speech is not 
accurate for text processing and some 
modifications needed to be implemented (e.g. 
email preprocessor splits acronyms like DLI2 
into DLI 2).  The noun phrase extraction module 
uses Brill's POS tagger [Brill (1992)]and a base 
NP chunker [Ramshaw and Marcus (1995)]. 
After analyzing some of these errors, we 
augmented the tagger lexicon from our training 
data and we added lexical and contextual rules 
to deal mainly with incorrect tagging of gerund 
endings. In order to improve the accuracy of 
classifiers we perform linguistic filtering, as 
discussed in detail in Section 3.1.2. 
2.3 Machine Learning Unit 
The first component of the ML unit is the 
feature selection module to compute NP 
vectors.  In the training phase, a model for 
identifying salient simple NPs is created.  
The training data consist of a list of feature 
vectors already classified as salient/non-
salient by the user.  Thus we rely on user-
relevance judgments to train the ML unit. In 
the extraction phase this unit will classify 
relevant NPs using the model generated 
during training.  We applied three machine 
learning paradigms (decision trees, rule 
induction algorithms, and decision forest) 
evaluating three different classifiers.  
2.4 Presentation 
The presentation of the message gist is a 
complex user interface issue with its 
independent set of problems.   Depending on 
the application and its use, one can think of 
different presentation techniques.  The gist of 
the message could be the set of NPs or the set 
of sentences in which these NPs occur so that 
the added context would make it more 
understandable to the user. We do not address 
in this work the disfluency that could occur in 
listing a set of extracted sentences, since the 
aim is to deliver to the user the very content 
of the message even in a raw fashion.   GIST-
IT is to be used in an application where the 
output is synthesized speech.   The focus of 
this paper is on extracting content with GIST-
IT, although presentation is a topic for future 
research.  
3 Combining Linguistic Knowledge and 
Machine Learning for Email Gisting 
We combine symbolic machine learning and 
linguistic processing in order to extract the 
salient phrases of a document.   Out of the 
large syntactic constituents of a sentence, e.g. 
noun phrases, verb phrases, and prepositional 
phrases, we assume that noun phrases (NPs) 
carry the most contentful information about 
the document, even if sometimes the verbs 
are important too, as reported in the work by 
[Klavans and Kan (1998)]. The problem is 
that no matter the size of a document, the 
number of informative noun phrases is very 
small comparing with the number of all noun 
phrases, making selection a necessity. Indeed, in 
the context of gisting, generating and presenting 
the list of all noun phrases, even with adequate 
linguistic filtering, may be overwhelming. Thus, 
we define the extraction of important noun 
phrases as a classification task, applying 
machine learning techniques to determine which 
features associated with the candidate NPs 
classify them as salient vs. non-salient.  We 
represent the document -- in this case an email 
message -- as a set of candidate NPs, each of 
them associated with a feature vector used in the 
classification model.  We use a number of 
linguistic methods both in the extraction and in 
the filtering of candidate noun phrases, and in 
the selection of the features.  
   
3.1 Candidate NPs 
Noun phrases were extracted using Ramshaw 
and Marcus's base NP chunker [Ramshaw and 
Marcus (1995)].  The base NP is either a simple 
NP as defined by Wacholder (1998) or a 
conjunction of two simple NPs.  Since the 
feature vectors used in the classifier scheme are 
simple NPs we used different heuristics to 
automatically split the conjoined NPs (CNP) 
into simple ones (SNP), properly assigning the 
premodifiers. Table 1 presents such an example: 
 
CNP: physics/NN and/CC biology/NN skilled/JJ  
researchers/NNS 
SNP1:  physics/NN skilled/JJ researchers/NNS 
SNP2: biology/NN skilled/JJ researchers/NNS 
Table 1 Splitting Complex NPs into Simple NPs  
3.1.2 Filtering simple NPs   
Since not all simple noun phrases are equally 
important to reflect the document meaning, we 
use well-defined linguistic properties to extract 
only those NPs (or parts of NPs) that have a 
greater chance to render the salient information. 
By introducing this level of linguistic filtering 
before applying the learning scheme, we 
improve the accuracy of the classifiers, thus 
obtaining better results (see discussion in 
sections 4.1.3 and 5.3). We performed four 
filtering steps: 
1. Inflectional morphological processing. 
English nouns have only two kinds of inflection: 
an affix that marks plural and an affix that 
marks possessive.
 
2. Removing unimportant modifiers. In this 
second step we remove the determiners that 
accompany the nouns and also the auxiliary 
words most and more that form the 
periphrastic forms of comparative and 
superlative adjectives modifying the nouns.
 
3. Remove common words. We used a list of 
571 common words used in IR systems in 
order to further filter the list of candidate 
NPs. Thus, words like even, following, every, 
are eliminated from the noun phrase 
structure. (i.e. ?even more detailed 
information? and ?detailed information? will 
also be grouped together). 
 
4. Remove ?empty? nouns. Words like lot, 
group, set, bunch are considered ?empty? 
nouns in the sense that they have no 
contribution to the noun phrase meaning. For 
example the meaning of the noun phrases like 
?group of students?,  ?lots of students? or 
?bunch of students? is given by the noun 
?students?. In order not to bias the extraction 
of empty nouns we used three different data 
collections: Brown corpus, Wall Street 
Journal, and a set of 4000 email messages 
(most of which were collected during a 
conference organization). Our algorithm was 
a simple one: we extracted all the nouns that 
appear in front of the preposition ?of? and 
then sorted them by frequency of appearance 
in all three corpora and used a threshold to 
select the final list. We generated a set of 141 
empty nouns that we used in this forth step of 
filtering process.   
3.2 Feature Selection 
We select a set of nine features that fall into 
three categories: linguistic, statistical 
(frequency-based) and positional. These 
features capture information about the 
relative importance of NPs to the document 
meaning.  
Several studies rely on linguistic intuition 
that the head of the noun phrase makes a 
greater contribution to the semantics of the 
nominal group than the modifiers. For some 
NLP tasks, the head is not necessarily the 
most important item of the noun phrase.  In 
analyzing email messages from the 
perspective of finding salient NPs, we claim 
that the constituents of the NP have often as 
much semantic content as the head.  This 
opinion is also supported in the work of 
[Strzalkowski et al(1999)]. In many cases, the 
meaning of the NP is given equally by 
modifier(s) -- usually nominal modifiers(s) -- 
and head.  Consider the following list of simple 
NPs selected as candidates: 
(1) ?conference workshop announcement? 
(2) ?international conference? 
(3) ?workshop description? 
(4) ?conference deadline? 
In the case of noun phrase (1) the importance of 
the noun phrase is found in the two noun 
modifiers: conference and   workshop as much 
as in the head announcement. We test this 
empirical observation by introducing as a 
separate feature in the feature vector, a new 
TF*IDF measure that counts for both the 
modifiers and the head of the noun phrase, thus 
seeing the NP as a sequence of equally weighted 
elements.  For the example above the new 
feature will be: 
TF*IDFconference + TF*IDFworkshop + TF*IDFannouncement 
We divided the set of features into three 
groups: one associated with the head of the noun 
phrase, one associated with the whole NP and 
one that represents the new TF*IDF measure 
discussed above.  Since we want to use this 
technique on other types of documents, all 
features are independent of the text type or 
genre.  For example, in the initial selection of 
our attributes we introduced as separate features 
the presence or the absence of NPs in the subject 
line of the email and in the headline of the body. 
Kilander (1996) pointed out that users estimate 
that ?subject lines can be useful, but also 
devastating if their importance is overly 
emphasized?.  Based on this study and also on 
our goal to provide a method that is domain and 
genre independent we decided not to consider 
the subject line and the headlines as separate 
features, but rather as weights included in the 
TF*IDF measures as presented below.  Another 
motivation for this decision is that in email 
processing the correct identification of headlines 
is not always clear. 
3.2.1 Features associated with the Head 
We choose two features to characterize the head 
of the noun phrases: 
head_tfidf ? the TF*IDF measure of the 
head of the candidate NP. 
head_focc - The first occurrence of the head 
in text (the numbers of words that precede the 
head divided by the total number of words in 
the document).  
3.2.2 Features associated with the whole 
NP 
We select six features that we consider 
relevant in association with the whole NP:  
np_tfidf ? the TF*IDF measure associated 
with the whole NP.  
np_focc - The first occurrence of the noun 
phrase in the document.  
np_length_words - Noun phrase length 
measured in number of words, normalized by 
dividing it with the total numbers of words in 
the candidate NPs list. 
np_length_chars - Noun phrase length 
measured in number of characters, 
normalized by dividing it with the total 
numbers of characters in the candidate NPs 
list. 
sent_pos - Position of the noun phrase in 
sentence: the number of words that precede 
the noun phrase, divided by the sentence 
length. For noun phrases in the subject line 
and headlines (which are usually short and 
will be affected by this measure), we consider 
the maximum length of sentence in document 
as the normalization factor.  
par_pos - Position of noun phrase in 
paragraph, same as sent_pos, but at the 
paragraph level. 
3.2.3 Feature that considers all constituents 
of the NP equally weighted 
m_htfidf - the new TF*IDF measure that 
take into consideration the importance of the 
modifiers.  
In computing the TF*IDF measures 
(head_tfidf, np_tfidf, m_tfidf), weights wi, 
were assigned to account for the presence in 
the subject line and/or headline.  
wi1 ? if the head appears both in the subject 
line and headline; 
wi2 ? if the head appears only in the subject 
line; 
wi3 ? if the head appears only in headlines 
 where wi1 > wi2 > wi3. 
These weights were manually chosen after 
a set of experiments, but we plan to use either 
a regression method or explore with genetic 
algorithms to automatically learn them. 
3.3 Three Paradigms of Supervised Machine 
Learning  
Symbolic machine learning is used in 
conjunction with many NLP applications 
(syntactic and semantic parsing, POS tagging, 
text categorization, word sense disambiguation).     
In this paper we compare three symbolic 
learning techniques applied to the task of salient 
NP extraction: decision tree, rule induction 
learning and decision forests.   
We tested the performance of an axis-parallel 
decision tree, C4.5 [Quinlan (1993)]; a rule 
learning system RIPPER [Cohen (1995)] and a 
decision forest classifier (DFC) [Ho (1998)]. 
RIPPER allows the user to specify the loss ratio, 
which indicates the ratio of the cost of a false 
positive to the cost of a false negative, thus 
allowing the trade off between precision and 
recall. This is crucial for our analysis since we 
deal with sparse data set (in a document the 
number of salient NPs is much smaller than the 
number of irrelevant NPs). Finally we tried to 
prove that a combination of classifiers might 
improve accuracy, increasing both precision and 
recall. The Decision Forest Classifier (DFC) 
uses an algorithm for systematically 
constructing decision trees by pseudo-randomly 
selecting subsets of components of feature 
vectors. It implements different splitting 
functions.  In the setting of our evaluation we 
tested the information gain ratio (similar to the 
one used by Quinlan in C4.5). An augmented 
feature vector (pairwise sums, differences, and 
products of features) was used for this classifier. 
4 Evaluation and Experimental Results 
Since there are many different summaries for 
each document, evaluating summaries is a 
difficult problem. Extracting the salient noun 
phrases is the first key step in the summarization 
method that we adopt in this paper. Thus, we 
focus on evaluating the performance of GIST-IT 
on this task, using three classification schemes 
and two different feature settings. 
4.1 Evaluation Scheme 
There are several questions that we address in 
this paper: 
4.1.1 What features or combination of 
features are important in determining the 
degree of salience of an NP?   
Following our assumption that each 
constituent of the noun phrase is equally 
meaningful, we evaluate the impact of adding 
m_htfidf
 (see section 3.2.3), as an additional 
feature in the feature vector.  This is shown in 
Table 2 in the different feature vectors fv1 
and fv2. 
 
fv1-  head_focc  head_tfidf np_focc np_tfidf   
        np_length_words  np_length_chars par_pos sent_pos 
fv2 - head_focc  head_tfidf  m_htfidf  np_focc np_tfidf  
         np_length_words np_length_chars par_pos sent_pos 
Table 2 Two feature settings to evaluate the 
impact of m_htfidf 
4.1.2 What classification scheme is more 
adequate to our task? 
We evaluate the performance of three 
different classifiers in the task of extracting 
salient noun phrases.  As measures of 
performance we use precision (p) and recall 
(r).  The evaluation was performed according 
to what degree the output of the classifiers 
corresponds to the user judgments.  
 
C4.5 Ripper     DFC  Feature 
vectors p  r p r p r 
fv1 73.3 78.6 83.6 71.4 80.3 83.5 
fv2 70 88.9 85.7 78.8 85.7 87.9 
Table 3 Evaluation of two feature vectors using 
three classifiers 
 
Table 3 shows our results that answer 
these two questions. The table rows represent 
the two feature vectors we are comparing, 
and the columns correspond to the three 
classifiers chosen for the evaluation.   
4.1.3 Is linguistic filtering an important step 
in extracting salient NPs? 
In the third evaluation we analyse the impact 
of linguistic filtering on the classifier?s 
performance. It turns out that results show 
major improvements, from 69.2% to 85.7% 
for precision of fv2, and from 56.25% to 
87.9% for recall of fv2.  For detailed results, 
see [Muresan et al (2001)]. 
4.1.4 After the filtering and classification, are 
noun phrases good candidates for representing 
the gist of an email message? 
In order to answer this question, we compare 
the output of GIST-IT on one email with the 
results of KEA system [Witten et al(1999)] that 
uses a 'bag-of-words' approach to key phrase 
extraction (see Table 4). 
 
module     
sort of batch 
WordNet data 
 accesses   
the WordNet     
lots of WordNet       
WordNet perl          
QueryData 
wn 
perl module 
extracting      
use this module 
extracting lots      
WordNet system  
www.cogsci.princeton.e
du 
Perl module wordne 
interface  
'wn' command line program   
simple easy perl interface      
included man page 
wordnet                 
wordnet.pm module 
wordnet system      
wordnet package 
query perl module     
command line  
wordnet relation    
wordnet data   
free software        
querydata        
Table 4 KEA (left)  vs GIST-IT output (right) 
5  Discussion of results 
The results shown indicate that best system 
performance reached 87.9% recall and 85.7% 
precision.  Although these results are very high, 
judging NP relevance is a complex and highly 
variable task.  In the future, we will extend the 
gold standard with more judges, more data, and 
thus a more precise standard for measurement. 
5.1 The right selection of features 
Feature selection has a decisive impact on 
overall performance. As seen in Table 2, fv2 has 
m_htfidf
 as an additional feature, and its 
performance shown in Table 3 is superior to fv1; 
the DFC classifier shows an increase both in 
precision and recall. These results support the 
original hypothesis that in the context of gisting, 
the syntactic head of the noun phrase is not 
always the semantic head, and modifiers can 
also have an important role.  
5.2 Different classification models 
The effectiveness of different classification 
schemes in the context of our task is discussed 
here. As shown in Table 3, C4.5 performs well 
especially in terms of recall. RIPPER, as 
discussed in [Cohen (1995)], is more appropriate 
for noisy and sparse data collection than 
C4.5, showing an improvement in precision. 
Finally, DFC which is a combination of 
classifiers, shows  improved performance. 
The classifier was run with an augumented 
feature vector that included pairwise sums, 
differences and products of the features.  
5.3 Impact of linguistic knowledge 
As shown in previous section, DFC 
performed best in our task, so we chose only 
this classifier to present the impact of 
linguistic knowledge. Linguistic filtering 
improved precision and recall, having an 
important role especially on fv2, where the 
new feature m_tfidf was used. This is 
explained by the fact that the filtering 
presented in section 3.1.2 removed the noise 
introduced by unimportant modifiers, 
common and empty nouns, thus giving this 
new feature a larger impact.   
5.4 Noun phrases are better than n-grams   
Presenting the gist of an email message by 
phrase extraction addresses one obvious 
question: can any phrasal extract represent 
the content of a document, or must a well 
defined linguistic phrasal structure be used? 
To answer this question we compare the 
results of our system that extract 
linguistically principled phrasal units, with 
KEA output, that extracts bigrams and 
trigrams as key phrases [Witten et al(1999)].   
Table 4 shows the results of the KEA system. 
Due to the n-gram approach, KEA output 
contains phrases like sort of batch, extracting 
lots, wn, and even urls that are unlikely to 
represent the gist of a document. 
 
Conclusion and future work 
In this paper we presented a novel technique 
for document gisting suitable for domain and 
genre independent collections such as email 
messages.  The method extracts simple noun 
phrases using linguistic techniques and then 
use machine learning to classify them as 
salient for the document content.  We 
evaluated the system in different 
experimental settings using three 
classification models. In analyzing the 
structure of NPs, we demonstrated that the 
modifiers of a noun phrase can be 
semantically as important as the head for the 
task of gisting. GIST-IT is fully implemented, 
evaluated, and embedded in an application, 
which allows user to access a set of information 
including email, finances, etc.  
  We plan to extend our work by taking 
advantage of structured email, by classifying 
messages into folders, and then by applying 
information extraction techniques.  Since NPs 
and machine learning techniques are domain and 
genre independent, we plan to test GIST-IT on 
different data collections (e.g. web pages), and 
for other knowledge management tasks, such as   
document indexing or query refinement. 
Additionally, we plan to test the significance of 
the output for the user, i.e. whether the system 
provide informative content and adequate gist of 
the message. 
References  
Berger, A.L and Mittal, V.O (2000). OCELOT:A system for 
summarizing web pages. In Proceedings of the 23rd 
Annual International ACM SIGIR, Athens, Greece, pp 
144-151.  
Brill, E. (1992).  A Simple Rule-based Part of Speech 
Tagger. In Proceedings of the Third Conference on 
ANLP. Trento, Italy; 1992 
Boguraev, B. and Kennedy, C. (1999). Salience-based 
content characterisation of text documents. In I. Mani 
and T. Maybury, M., editors, Advances in Automatic 
Text Summarization, pp 99-111. The MIT Press.  
Cohen, W. (1995). Fast Effective Rule Induction. Machine-
Learning: Proceedings of the Twelfth International 
Conference.  
Ho, T.K (1998). The random subspace method for 
constructing decision forests. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 20(8). 
Hovy, E.H (2000). Automated Text Summarization. In R. 
Mitkov, editor,  Oxford University Handbook of 
Computational Linguistics. Oxford Univ. Press. 
Kilander, F. (1996). Properties of electronic texts for 
classification purposes as suggested by users. 
Klavans, J.L., Wacholder, N. and Evans, D.K. (2000) 
Evaluation of computational linguistic techniques for 
identifying significant topics for browsing applications. 
In Proceedings (LREC-2000), Athens. Greece. 
Klavans, J.L. and Kan, M-Y. (1998).Role of verbs in 
document analysis. In proceedings of COLING/ACL  98. 
Kupiec, J., Pedersen, J. and Chen, F. (1995). A trainable 
document summarizer. In Proceedings of the 18th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, pp 
68-73, Seattle, WA. 
McKeown, K.R,  Klavans, J.L, Hatzivassiloglou, V.,  
Barzilay, R. and Eskin, E. (1999). Towards 
multidocument summarization by reformulation: 
Progress and prospects. In Proceedings of AAAI'99. 
McKeown, K.R and Radev, D.R (1995). Generating 
summaries of multiple news articles. In Proceedings 
of the 18th Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, pp 74-82, Seattle, WA. 
Muresan, S., Tzoukermann, E. and Klavans, J.L. 
(2001). Email Summarization Using Linguistic and 
Machine Learning Techniques. In Proceedings of 
CoNLL 2001 ACL Workshop, Toulouse, France. 
Murthy, S.K., Kasif, S., Salzberg, S. and Beigel, R. 
(1993). OC1: Randomized Induction of Oblique 
Decision Trees. Proceedings of the Eleventh National 
Conference on Artificial Intelligence, pp. 322--327, 
Washington, D.C. 
Quinlan, J.R (1993). C4.5: Program for Machine 
Learning. Morgan Kaufmann. 
Ramshaw, L.A. and Marcus, M.P. (1995). Text 
Chunking Using Transformation-Based Learning. In 
Proceedings of Third ACL Workshop on Very Large 
Corpora, MIT. 
Sparck-Jones, K. (1999). What Is The Role of NLP in 
Text Retrieval. In T. Strzalkowski, editor, Natural 
Language Information Retrieval. Kluwer, Boston, 
MA.    
Strzalkowski, T., Lin, F., Wang, J., and Perez-Carballo, 
J. (1999). Evaluating natural language  processing 
techniques for information retrieval. In T. 
Strzalkowski, editor, Natural Language Information 
Retrieval. Kluwer, Boston, MA.    
Turney, P.D. (2000). Learning algorithms for 
keyphrase exraction. Information Retrieval, 2(4): pp 
303-336. 
Ueda, Y., Oka M., Koyama T. and Miyauchi T (2000). 
Toward the "at-a-glance" summary: Phrase-
representation summarization method. In 
Proceedings of COLING 2000.   
Wacholder, N. (1998). Simplex NPS sorted by head: a 
method for identifying significant topics within a 
document, In Proceedings of the COLING-ACL 
Workshop on the Computational Treatment of 
Nominals. 
Whittaker, S. and Sidner, C. Email overload: Exploring 
personal information management of email. In 
Proceedings of CHI?96. p. 276-283. NY:ACM Press 
Witten, I.H, Paynter, G.W., Frank E., Gutwin C. and 
Nevill-Manning, C.G (1999). KEA: Practical 
automatic keyphrase extraction. In Proceedings of 
DL'99, pp 254-256. 
