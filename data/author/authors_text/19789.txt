Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1447?1456, Dublin, Ireland, August 23-29 2014.
A Markovian approach to distributional semantics
with application to semantic compositionality
?
Edouard Grave
EECS Department
UC Berkeley
grave@berkeley.edu
Guillaume Obozinski
LIGM ? Universit?e Paris-Est
?
Ecole des Ponts ? ParisTech
guillaume.obozinski
@imagine.enpc.fr
Francis Bach
Inria ? Sierra project-team
?
Ecole Normale Sup?erieure
francis.bach@ens.fr
Abstract
In this article, we describe a new approach to distributional semantics. This approach relies
on a generative model of sentences with latent variables, which takes the syntax into account
by using syntactic dependency trees. Words are then represented as posterior distributions over
those latent classes, and the model allows to naturally obtain in-context and out-of-context word
representations, which are comparable. We train our model on a large corpus and demonstrate
the compositionality capabilities of our approach on different datasets.
1 Introduction
It is often considered that words appearing in similar contexts tend to have similar meaning (Harris,
1954). This idea, known as the distributional hypothesis was famously summarized by Firth (1957)
as follow: ?you shall know a word by the company it keeps.? The distributional hypothesis has been
applied in computational linguistics in order to automatically build word representations that capture
their meaning. For example, simple distributional information about words, such as co-occurence counts,
can be extracted from a large text corpus, and used to build a vectorial representation of words (Lund
and Burgess, 1996; Landauer and Dumais, 1997). According to the distributional hypothesis, two words
having similar vectorial representations must have similar meanings. It is thus possible and easy to
compare words using their vectorial representations.
In natural languages, sentences are formed by the composition of simpler elements: words. It is
thus reasonable to assume that the meaning of a sentence is determined by combining the meanings
of its parts and the syntactic relations between them. This principle, often attributed to the German
logician Frege, is known as semantic compositionality. Recently, researchers in computational linguistics
started to investigate how the principle of compositionality could be applied to distributional models of
semantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008). Given the representations of individual
words, such as federal and agency, is it possible to combine them in order to obtain a representation
capturing the meaning of the noun phrase federal agency?
Most approaches to distributional semantics represent words as vectors in a high-dimensional space
and use linear algebra operations to combine individual word representations in order to obtain represen-
tations for complex units. In this article, we propose a probabilistic approach to distributional semantics.
This approach is based on the generative model of sentences with latent variables, which was introduced
by Grave et al. (2013). We make the following contributions:
? Given the model introduced by Grave et al. (2013), we describe how in-context and ouf-of-context
words can be represented by posterior distributions over latent variables (section 4).
? We evaluate out-of-context representations on human similarity judgements prediction tasks and
determine what kind of semantic relations are favored by our approach (section 5).
? Finally, we evaluate in-context representations on two similarity tasks for short phrases (section 6).
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1447
2 Related work
Most approaches to distributional semantics are based on vector space models (VSM), in which words
are represented as vectors in a high-dimensional space. These vectors are obtained from a large text
corpus, by extracting distributional information about words such as the contexts in which they appear.
A corpus is then represented as a word-by-context co-occurence matrix. Contexts can be defined as
documents in which the target word appear (Deerwester et al., 1990; Landauer and Dumais, 1997) or
as words that appear in the neighbourhood of the target word, for example in the same sentence or in a
fixed-size window around the target word (Schutze, 1992; Lund and Burgess, 1996).
Next to vector space models, other approaches to distributional semantics are based on probabilistic
models of documents, such as probabilistic latent semantic analysis (pLSA) introduced by Hofmann
(1999) and which is inspired by latent semantic analysis, or latent Dirichlet allocation (LDA), introduced
by Blei et al. (2003). In those models, each document is viewed as a mixture of k topics, where each
topic is a distribution over the words of the vocabulary.
The previous models do not take into account the linguistic structure of the sentences used to build
word representations. Several models have been proposed to address this limitation. In those models, the
contexts are defined by using the syntactic relations between words (Lin, 1998; Curran and Moens, 2002;
Turney, 2006; Pad?o and Lapata, 2007; Baroni and Lenci, 2010). For example, two words are considered
in the same context if there exists a syntactic relation between them, or if there is a path between them in
the dependency graph.
One of the first approaches to semantic compositionality using vector space models was proposed
by Mitchell and Lapata (2008). In this study, individual word representations are combined using linear
algebra operations such as addition, componentwise multiplication, tensor product or dilation. Those dif-
ferent composition operations are then used to disambiguate intransitive verbs given a subject (Mitchell
and Lapata, 2008) or to compute similarity scores between pairs of small phrases (Mitchell and Lapata,
2010).
Another approach to semantic compositionality is to learn the function used to compose individual
word representations. First, a semantic space containing representations for both individual words and
phrases is built. For example, the words federal, agency and the phrase federal agency all have a vectorial
representation. Then, a function mapping individual word representations to phrase representations can
be learnt in a supervised way. Guevara (2010) proposed to use partial least square regression to learn
this function. Similarly, Baroni and Zamparelli (2010) proposed to learn a matrix A for each adjective,
such that the vectorial representation p of the adjective-noun phrase can be obtained from the vectorial
representation b of the noun by the matrix-vector multiplication:
p = Ab.
Socher et al. (2012) later generalized this model by proposing to represent each node in a parse tree by a
vector capturing the meaning and a matrix capturing the compositional effects. A composition function,
inspired by artificial neural networks, is recursively applied in the tree to compute those representations.
Following the theoretical framework introduced by Coecke et al. (2010), Grefenstette and Sadrzadeh
(2011) proposed to represent relational words (such as verbs) by tensors and theirs arguments (such
as nouns) by vectors. Composing a relational word with its arguments is then performed by taking
the pointwise product between the tensor and the Kronecker product of the vectors representing the
arguments. Jenatton et al. (2012) and Van de Cruys et al. (2013) proposed two approaches to model
subject-verb-object triples based on tensor factorization.
Finally, research in computation of word meaning in context is closely related to distributional seman-
tic compositionality. Erk and Pad?o (2008) proposed a structured vector space model in which a word
is represented by multiple vectors, capturing its meaning but also the selectional restrictions it has for
the different arguments. Those different vectors can then be combined to compute a word representation
in context. This model was later generalized by Thater et al. (2010). Dinu and Lapata (2010) intro-
duced a probabilistic model for computing word representations in context. In their approach, words are
represented as probability distributions over latent senses.
1448
Computers can be designed to do anything with information
c
0
c
1
c
2
c
3
c
4
c
5
c
6
c
7
c
8
c
9
w
1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9
Figure 1: Example of a dependency tree and its corresponding graphical model.
3 Model of semantics
In this section we briefly review the generative model of sentences introduced by Grave et al. (2013), and
which serves as the basis of our approach to distributional semantics.
3.1 Generative model of sentences
We denote the tokens of a sentence of length K by the K-uple w = (w
1
, ..., w
K
) ? {1, ..., V }
K
, where
V is the size of the vocabulary and each integer represents a word. We suppose that each token w
k
is
associated to a corresponding semantic class c
k
? {1, ..., C}, where C is the number of semantic classes.
Finally, the syntactic dependency tree corresponding to the sentence is represented by the function pi :
{1, ...,K} 7? {0, ...,K}, where pi(k) represents the parent of word k and 0 is the root of the tree (which
is not associated to a word).
Given a tree pi, the semantic classes and the words of a sentence are generated as follows. The semantic
class of the root of the tree is set to a special start symbol, represented by the integer 0.
1
Then, the
semantic classes corresponding to words are recursively generated down the tree: each semantic class
c
k
is drawn from a multinomial distribution p
T
(c
k
| c
pi(k)
), conditioned on the semantic class c
pi(k)
of
its parent in the tree. Finally, each word w
k
is also drawn from a multinomial distribution p
O
(w
k
| c
k
),
conditioned on its corresponding semantic class c
k
. Thus, the joint probability distribution on words and
semantic classes can be factorized as
p(w, c) =
K
?
k=1
p
T
(c
k
| c
pi(k)
)p
O
(w
k
| c
k
),
where the variable c
0
= 0 represents the root of the tree. The initial class probability distribution
p
T
(c
k
| c
0
= 0) is parameterized by the probability vector q, while the transition probability distribution
between classes p
T
(c
k
| c
pi(k)
) and the emission probability distribution p
O
(w
k
| c
k
) are parameterized
by the stochastic matrices T and O (i.e., matrices with non-negative elements and unit-sum columns).
This model is a hidden Markov model on a tree (instead of a chain). See Fig. 1 for an example of a
sentence and its corresponding graphical model.
3.2 Corpus and learning
We train the generative model of sentences on the ukWac corpus (Baroni et al., 2009). This corpus, which
contains approximately 1.9 billions tokens, was POS-tagged and lemmatized using TreeTagger (Schmid,
1994) and parsed using MaltParser (Nivre et al., 2007). Each word of our vocabulary is a pair of lemma
and its part-of-speech. We perform smoothing by only keeping the V most frequent pairs, the infrequent
ones being replaced by a common token. The parameters ? = (q,T,O) of the model are learned
using the algorithm described by Grave et al. (2013). The number of latent states C and the number of
lemma/POS pairs V were set using the development set of Bruni et al. (2012).
1
We recall that the semantic classes corresponding to words are represented by integers between 1 and C.
1449
president
chief
chairman
director
executivemanagereyeface
shoulder
hand
leg foot
head head-2head-1
Figure 2: Comparison of out-of-context (black) and in-context (red) word representations. The two-
dimensional visualization is obtained by using multidimensional scaling (Borg, 2005). See text for de-
tails.
4 Word representations
Given a trained hidden Markov model, we now describe how to obtain word representations, for both in-
context and out-of-context words. In both cases, words will be represented as a probability distribution
over the latent semantic classes.
In-context word representation. Obtaining a representation of a word in the context of a sentence is
very natural using the model introduced in the previous section: we start by parsing the sentence in order
to obtain the syntactic dependency tree. We then compute the posterior distribution of semantic classes c
for that word, and use this probability distribution to represent the word. More formally, given a sentence
w = (w
1
, ..., w
K
), the kth word of the sentence is represented by the vector u
k
? R
C
defined by
u
k
i
= P(C
k
= i |W = w).
The vector u
k
is the posterior distribution of latent classes corresponding to the kth word of the sentence,
and thus, sums to one. It is efficiently computed using the message passing algorithm (a.k.a. forward-
backward algorithm for HMM).
Out-of-context representation. In order to obtain word representations that are independent of the
context, we compute the previously introduced in-context representations on a very large corpus, and for
each word type, we average all the in-context representations for all the occurrences of that word type
in the corpus. More formally, given a large set of pairs of tokens and their in-context representations
(w
k
,u
k
) ? N? R
C
, the representation of the word type a is the vector v
a
? R
C
, defined by
v
a
=
1
Z
a
?
k : w
k
=a
u
k
,
where Z
a
is the number of occurrences of the word type a. The vector v
a
is thus the posterior distribution
of semantic classes averaged over all the occurrences of word type a.
Comparing in-context and out-of-context representations. Since in-context and out-of-context
word representations are defined on the same space (the simplex of dimension C) it is possible to com-
pare in-context and out-of-context representations easily. As an example, we have plotted in Figure 2
the out-of-context representation for the words head, president, chief, chairman, director, executive, eye,
face, shoulder, hand, leg, etc. and the in-context representations for the word head in the context of the
two following sentences:
1. The nurse stuck her head in the room to announce that Dr. Reitz was on the phone.
2. A well-known Wall Street figure may join the Cabinet as head of the Treasury Department.
1450
Distance RG65 WS353
Cosine 0.68 0.50
Kullback-Leibler 0.69 0.47
Jensen-Shannon 0.72 0.50
Hellinger 0.73 0.51
Agirre et al. (BoW) 0.81 0.65
Distance SIM. REL.
Cosine 0.68 0.34
Kullback-Leibler 0.64 0.31
Jensen-Shannon 0.69 0.33
Hellinger 0.70 0.34
Agirre et al. (BoW) 0.70 0.62
Table 1: Left: Spearman?s rank correlation coefficient ? between human and distributional similarity, on
the RG65 and WORDSIM353 datasets. Right: Spearman?s rank correlation coefficient ? between human
and distributional similarity on two subsets (similarity v.s. relatedness) of the WORDSIM353 dataset.
The two-dimensional visualization is obtained by using multidimensional scaling (Borg, 2005). First of
all, we observe that the words are clustered in two groups, one containing words belonging to the body
part class, the other containing words belonging to the leader class, and the word head, appears between
those two groups. Second, we observe that the in-context representations are shifted toward the cluster
corresponding to the disambiguated sense of the ambiguous word head.
5 Out-of-context evaluation
In this section, we evaluate out-of-context word representations on a similarity prediction task and deter-
mine what kind of semantic relations are favored by our approach.
5.1 Similarity judgements prediction
In word similarity prediction tasks, pairs of words are presented to human subjects who are asked to
rate the relatedness between those two words. These human similarity scores are then compared to
distributional similarity scores induced by our models, by computing the correlation between them.
Methodology. We use the RG65 dataset, introduced by Rubenstein and Goodenough (1965) and the
WORDSIM353 dataset, collected by Finkelstein et al. (2001). These datasets comprise 65 and 353 word
pairs respectively. Human subjects rated the relatedness of those word pairs. We use the Spearman?s
rank correlation coefficient ? to compare human and distributional score distributions.
Comparison of similarity measures. Since words are represented by posterior distributions over la-
tent semantic classes, we have considered distances (or divergences) that are adapted to probability dis-
tributions to compute the similarity between word representations: the symmetrised Kullback-Leibler
divergence, the Jensen-Shannon divergence, and the Hellinger distance. We use the opposite of these
dissimilarity measures in order to obtain similarity scores. We also included the cosine similarity mea-
sure as a baseline, as it is widely used in the field of distributional semantics.
We report results on both datasets in Table 1. Unsurprisingly, we observe that the dissimilarity mea-
sures giving the best results are the one tailored for probability distribution, namely the Jensen-Shannon
divergence and the Hellinger distance. The Kullback-Leibler divergence is too sensitive to fluctuations
of small probabilities and thus does not perform as well as other similarity measures between probability
distributions. In the following, we will use the Hellinger distance. It should be noted that the results
reported by Agirre et al. (2009) were obtained using a corpus containing 1.6 terawords, making it 1,000
times larger than ours. They also report results for various corpus sizes, and when using a corpus whose
size is comparable to ours, their result on WORDSIM353 drops to 0.55.
Relatedness v.s. similarity. As noted by Agirre et al. (2009), words might be rated as related for
different reasons since different kinds of semantic relations exist between word senses. Some words,
such as telephone and communication might even be rated as related because they belong to the same
semantic field. Thus, they proposed to split the WORDSIM353 dataset into two subsets: the first one
comprising words that are similar, i.e., synonyms, antonyms and hyperonym-hyponym and the second
1451
cohyp hyper mero attri event randn randj randv2
1
0
1
2
3
cohyp hyper mero attri event randn randj randv
Figure 3: Similarity score distributions for various semantic relations on the BLESS dataset, without
using the transition matrix (left) and with using the transition matrix (right) for comparing adjectives and
verbs with nouns.
one comprising words that are related, i.e., meronym-holonym and topically related words. We report
results on these two subsets in Table 1. We observe that our model capture similarity (? = 0.70) much
better than relatedness (? = 0.34). This is not very surprising since our model takes the syntax into
account.
5.2 Semantic relations captured by our word representations
As we saw in the previous section, different semantic relations between words are not equally captured
by our word representations. In this section, we thus investigate which kind of semantic relations are
favored by our approach.
The BLESS dataset. The BLESS dataset (Baroni and Lenci, 2011) comprises 200 concrete concepts
and eight relations. For each pair of concept-relation, a list of related words, referred to as relatum, is
given. Five semantic relations are considered: co-hyponymy, hypernymy, meronymy, attribute and event.
The attribute relation means that the relatum is an adjective expressing an attribute of the concept, while
the event relation means that the relatum is a verb designing an activity or an event in which the concept
is involved. The dataset also contains three random relations (randn, randj ans randv), obtained by the
association of a random relatum, for different POS: noun, adjective and verb.
Methodology. We follow the evaluation proposed by the authors: for each pair of concept-relation, we
keep the score of the most similar relatum associated to that pair of concept-relation. Thus, for each
concept, we have eight scores, one for each relation. We normalize these eight scores (mean: 0, std: 1),
in order to reduce concept-specific effects. We then report the score distributions for each relation as box
plots in Figure 3 (left).
Results. We observe that the co-hyponymy relation is the best captured relation by a large margin.
It is followed by the hypernymy and meronymy relations. The random noun relation is prefered over
the attribute and the event relations. This happens because words with different part-of-speeches tend
to appear in different semantic classes. It is thus impossible to compare words with different parts-of-
speeches and thus to capture relation such as the event or the attribute relation as defined in the BLESS
dataset. It is however possible to make a more principled use of the model to overcome this issue.
Comparing adjectives with nouns and nouns with verbs. In syntactic relations between nouns and
adjectives, the noun is the head word and the adjective is the dependent. Similarly, in syntactic relations
between nouns and verbs, most often the verb is the head and the noun is the dependent. Given a vector
v
a
representing an adjective and a vector v
n
representing a noun, it is thus natural to left multiply them by
the transition matrix of the model to obtain a vector u
a
comparable to nouns and a vector u
n
comparable
to verbs:
u
a
= T
>
v
a
and u
n
= T
>
v
n
.
1452
small house
emphasise need
scholar write book
c
2
c
1
w
2
w
1
c
1
c
2
w
1
w
2
c
2
c
1
c
3
w
2
w
1
w
3
Figure 4: Graphical models used to compute in-context word representations for the compositional tasks.
We report in Figure 3 (right) the new score distributions obtained when adjective and noun representa-
tions are transformed before being compared to nouns and verbs. We observe that, when using these
transformations, the attribute and event relations are better captured than the random relations. This
demonstrates that the transition matrix T captures selectional preferences.
6 Compositional semantics
So far, we have only evaluated how well our representations are able to capture the meaning of words
taken as individual and independent units. However, natural languages are highly compositional, and it
is reasonable to assume that the meaning of a sentence or a phrase can be deduced from the meanings of
its parts and the syntactic relations between them. This assumption is known as the principle of semantic
compositionality.
In this section, we thus evaluate our representations on semantic composition tasks. More precisely, we
determine if using in-context word representations helps to compute the similarity between short phrases
such as adjective-noun, verb-object, compound-noun or subject-verb-object phrases. We use two datasets
of human similarity scores, introduced respectively by Mitchell and Lapata (2010) and Grefenstette and
Sadrzadeh (2011).
6.1 Methodology
We compare different ways to obtain a representation of a short phrase given our model. First, as a
baseline, we represent a phrase by the out-of-context representation of its head word. In that case, there
is no composition at all. Second, following Mitchell and Lapata (2008), we represent a phrase by the
sum of the out-of-context representations of the words forming that phrase. Third, we represent a phrase
by the in-context representation of its head word. Finally, we represent a phrase by the sum of the two
in-context representations of the words forming that phrase. The graphical models used to compute in-
context word representations are represented in Fig 4. The probability distribution p(c
1
) of the head?s
semantic class is set to the uniform distribution (and not to the initial class distribution p
T
(c
k
| c
0
= 0)).
6.2 Datasets
The first dataset we consider was introduced by Mitchell and Lapata (2010), and is composed of pairs of
adjective-noun, compound-noun and verb-object phrases, whose similarities were evaluated by human
subjects on a 1? 7 scale. We compare our results with the one reported by (Mitchell and Lapata, 2010).
The second dataset we consider was introduced by Grefenstette and Sadrzadeh (2011). Each example of
this dataset consists in a triple of subject-verb-object, forming a small transitive sentence, and a landmark
verb. Human subjects were asked to evaluate the similarity between the verb and its landmark in the
context of the small sentence. Following Van de Cruys et al. (2013), we compare the contextualized verb
with the non-contextualized landmark, meaning that the landmark is always represented by its out-of-
context representation. We do so because it is believed to better capture the compositional ability of our
model and it works better in practice. We compare our results with the one reported by Van de Cruys et
al. (2013).
1453
AN NN VN
head (out-of-context) 0.44 0.26 0.41
add (out-of-context) 0.50 0.45 0.42
head (in-context) 0.49 0.42 0.43
add (in-context) 0.51 0.46 0.41
M&L (vector space model) 0.46 0.49 0.38
Humans 0.52 0.49 0.55
SVO
head (out-of-context) 0.25
add (out-of-context) 0.25
head (in-context) 0.41
add (in-context) 0.40
Van de Cruys et al. 0.37
Humans 0.62
Table 2: Spearman?s rank correlation coefficients between human similarity judgements and similarity
computed by our models on the Mitchell and Lapata (2010) dataset (left) and on the Grefenstette and
Sadrzadeh (2011) dataset (right). AN stands for adjective-noun, NN stands for compoundnoun and VN
stands for verb-object.
6.3 Discussion
Before discussing the results, it is interesting to note that our approach provides a way to evaluate the
importance of disambiguation for compositional semantics. Indeed, the in-context representations pro-
posed in this paper are a way to disambiguate their out-of-context equivalents. It was previously noted by
Reddy et al. (2011) that disambiguating the vectorial representations of words improve the performance
on compositional tasks.
Mitchell and Lapata (2010) dataset. We report results on the Mitchell and Lapata (2010) dataset in
Table 2 (left). Overall, in-context representations achieves better performance than out-of-context ones.
For the adjective-noun pairs and the verb-noun pairs, using only the in-context representation of the head
word works almost as well (AN) or even better (VN) than adding the representations of the two words
forming a pair. This means that for those particular tasks, disambiguation plays an important role. On
the other hand, this is not the case for the noun-noun pairs. On that task, most improvement over the
baseline comes from the add operation.
Grefenstette and Sadrzadeh (2011) dataset. We report results in Table 2 (right). First, we observe
that in-context representations clearly outperform out-of-context ones. Second, we note that adding the
subject, object and verb representations does not improve the result over only using the representation of
the verb. These two conclusions are not really surprising since this task is mainly a disambiguation task,
and disambiguation is achieved by using the in-context representations. We also note that our approach
yields better results than those obtained by Van de Cruys et al. (2013), while their method was specifically
designed to model subject-verb-object triples.
7 Conclusion and future work
In this article, we introduced a new approach to distributional semantics, based on a generative model
of sentences. This model is somehow to latent Dirichlet allocation as structured vector space models are
to latent semantic analysis. Indeed, our approach is based on a probabilistic model of sentences, which
takes the syntax into account by using dependency trees. Similarly to LDA, our model can be viewed
as a topic model, the main difference being that the topics are generated using a Markov process on a
syntactic dependency tree instead of using a Dirichlet process.
The approach we propose seems quite competitive with other distributional models of semantics. In
particular, we match or outperform state-of-the-art methods on semantic compositionality tasks. Thanks
to its probabilistic nature, it is very easy to derive word representations for various tasks: the same model
can be used to compute in-context word representations for adjective-noun phrases, subject-verb-object
triples or even full sentences, which is not the case of the tensor based approach proposed by Van de
Cruys et al. (2013).
1454
Currently, the model of sentences does not use the dependency labels, which is the most significant
limitation that we would like to address in future work. We also plan to explore spectral methods (Anand-
kumar et al., 2012) to provide better initialization for learning the parameters of the model. Indeed, we
believe this could speed up learning and yields better results, since the expectation-maximization al-
gorithm is quite sensitive to bad initialization. Finally, the code corresponding to this article will be
available on the first author webpage.
Acknowledgments
Edouard Grave is supported by a grant from INRIA (Associated-team STATWEB). Francis Bach is
partially supported by the European Research Council (SIERRA Project)
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca, and A. Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. 2012. Tensor decompositions for learning latent
variable models. arXiv preprint arXiv:1210.7559.
M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Com-
putational Linguistics, 36(4):673?721.
M. Baroni and A. Lenci. 2011. How we blessed distributional semantic evaluation. In Proceedings of the GEMS
2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1?10. Association for Compu-
tational Linguistics.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun
constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. 2009. The WaCky wide web: a collection of very large
linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209?226.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning
Research.
I. Borg. 2005. Modern multidimensional scaling: Theory and applications. Springer.
E. Bruni, G. Boleda, M. Baroni, and N. K. Tran. 2012. Distributional semantics in technicolor. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages
136?145. Association for Computational Linguistics.
S. Clark and S. Pulman. 2007. Combining symbolic and distributional models of meaning. In AAAI Spring
Symposium: Quantum Interaction, pages 52?55.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Mathematical foundations for a compositional distributional model
of meaning. arXiv preprint arXiv:1003.4394.
J. R. Curran and M. Moens. 2002. Scaling context space. In Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American society for information science.
G. Dinu and M. Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Processing.
K. Erk and S. Pad?o. 2008. A structured vector space model for word meaning in context. In Proceedings of the
2008 Conference on Empirical Methods in Natural Language Processing.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search
in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web.
1455
J. R. Firth. 1957. A synopsis of linguistic theory, 1930-1955.
E. Grave, G. Obozinski, and F. Bach. 2013. Hidden Markov tree models for semantic class induction. In Proceed-
ings of the Seventeenth Conference on Computational Natural Language Learning.
E. Grefenstette and M. Sadrzadeh. 2011. Experimental support for a categorical compositional distributional
model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Pro-
cessing.
E. Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceed-
ings of the 2010 Workshop on GEometrical Models of Natural Language Semantics.
Z. S. Harris. 1954. Distributional structure. Springer.
T. Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncer-
tainty in artificial intelligence.
R. Jenatton, N. Le Roux, A. Bordes, and G. Obozinski. 2012. A latent factor model for highly multi-relational
data. In Advances in Neural Information Processing Systems 25.
T. K Landauer and S. T. Dumais. 1997. A solution to Plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological review.
D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-volume 2.
K. Lund and C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behav-
ior Research Methods, Instruments, & Computers.
J. Mitchell and M. Lapata. 2008. Vector-based models of semantic composition. In Proceedings of the 46th
Annual Meeting of the Association of Computational Linguistics.
J. Mitchell and M. Lapata. 2010. Composition in distributional models of semantics. Cognitive Science.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K?ubler, S. Marinov, and E. Marsi. 2007. Maltparser: A
language-independent system for data-driven dependency parsing. Natural Language Engineering.
S. Pad?o and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguis-
tics.
S. Reddy, I. P. Klapaftis, D. McCarthy, and S. Manandhar. 2011. Dynamic and static prototype vectors for
semantic composition. In IJCNLP, pages 705?713.
H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM,
8(10):627?633.
H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international
conference on new methods in language processing.
H. Schutze. 1992. Dimensions of meaning. In Supercomputing?92. Proceedings. IEEE.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. 2012. Semantic compositionality through recursive matrix-
vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning.
S. Thater, H. F?urstenau, and M. Pinkal. 2010. Contextualizing semantic representations using syntactically en-
riched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Lin-
guistics.
P. D. Turney. 2006. Similarity of semantic relations. Computational Linguistics.
T. Van de Cruys, T. Poibeau, and A. Korhonen. 2013. A tensor-based factorization model of semantic composi-
tionality. In Proceedings of NAACL-HLT.
1456
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1580?1590,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A convex relaxation for weakly supervised relation extraction
?
Edouard Grave
EECS Department
University of California, Berkeley
grave@berkeley.edu
Abstract
A promising approach to relation extrac-
tion, called weak or distant supervision,
exploits an existing database of facts as
training data, by aligning it to an unla-
beled collection of text documents. Using
this approach, the task of relation extrac-
tion can easily be scaled to hundreds of
different relationships. However, distant
supervision leads to a challenging multi-
ple instance, multiple label learning prob-
lem. Most of the proposed solutions to this
problem are based on non-convex formu-
lations, and are thus prone to local min-
ima. In this article, we propose a new
approach to the problem of weakly su-
pervised relation extraction, based on dis-
criminative clustering and leading to a
convex formulation. We demonstrate that
our approach outperforms state-of-the-art
methods on the challenging dataset intro-
duced by Riedel et al. (2010).
1 Introduction
Information extraction refers to the broad task
of automatically extracting structured information
from unstructured documents. An example is the
extraction of named entities and the relations be-
tween those entities from natural language texts.
In the age of the world wide web and big data,
information extraction is quickly becoming perva-
sive. For example, in 2013, more than 130, 000
scientific articles were published about cancer.
Keeping track with that quantity of information
is almost impossible, and it is thus of utmost im-
portance to transform the knowledge contained in
this massive amount of documents into structured
databases.
Traditional approaches to information extrac-
tion relies on supervised learning, yielding high
Knowledge base
r e
1
e
2
BornIn Lichtenstein New York City
DiedIn Lichtenstein New York City
Sentences Latent labels
Roy Lichtenstein was born in
New York City, into an upper-
middle-class family.
BornIn
In 1961, Leo Castelli started
displaying Lichtenstein?s work
at his gallery in New York.
None
Lichtenstein died of pneumonia
in 1997 in New York City.
DiedIn
Figure 1: An example of a knowledge database
comprising two facts and training sentences ob-
tained by aligning this database to unlabeled text.
precision and recall results (Zelenko et al.,
2003). Unfortunately, these approaches need large
amount of labeled data, and thus do not scale well
to the great number of different types of fact found
on the Web or in scientific articles. A promising
approach, called distant or weak supervision, is
to exploit an existing database of facts as training
data, by aligning it to an unlabeled collection of
text documents (Craven and Kumlien, 1999).
In this article, we are interested in weakly super-
vised extraction of binary relations. A challenge
pertaining to weak supervision is that the obtained
training data is noisy and ambiguous (Riedel et
al., 2010). Let us start with an example: if the
fact Attended(Turing, King
?
s College) exists
in the knowledge database and we observe the sen-
tence
Turing studied as an undergraduate from
1931 to 1934 at King?s College, Cambridge.
which contains mentions of both entities Turing
1580
and King
?
s College, then this sentence might ex-
press the fact that Alan Turing attended King?s
College, and thus, might be a useful example for
learning to extract the relation Attended. How-
ever, the sentence
Celebrations for the centenary of Alan Tur-
ing are being planned at King?s College.
also contains mentions of Turing and
King
?
s College, but do not express the re-
lation Attended. Thus, weak supervision lead
to noisy examples. As noted by Riedel et al.
(2010), such negative extracted sentences for
existing facts can represent more than 30% of
the data. Moreover, a given pair of entities,
such as (Roy Lichtenstein, New York City),
car verify multiple relations, such as BornIn
and DiedIn. Weak supervision thus lead to
ambiguous examples.
This challenge is illustrated in Fig. 1. A solution
to address it is to formulate the task of weakly su-
pervised relation extraction as a multiple instance,
multiple label learning problem (Hoffmann et al.,
2011; Surdeanu et al., 2012). However, these for-
mulations are often non-convex and thus suffer
from local minimum.
In this article, we make the following contribu-
tions:
? We propose a new convex relaxation for the
problem of weakly supervised relation ex-
traction, based on discriminative clustering,
? We propose an efficient algorithm to solve the
associated convex program,
? We demonstrate that our approach obtains
state-of-the-art results on the dataset intro-
duced by Riedel et al. (2010).
To our knowledge, this paper is the first to propose
a convex formulation for solving the problem of
weakly supervised relation extraction.
2 Related work
Supervised learning. Many approaches based
on supervised learning have been proposed to
solve the problem of relation extraction, and the
corresponding literature is to large to be summa-
rized here. One of the first supervised method for
relation extraction was inspired by syntactic pars-
ing: the system described by Miller et al. (1998)
combines syntactic and semantic knowledge, and
thus, part-of-speech tagging, parsing, named en-
tity recognition and relation extraction all happen
at the same time. The problem of relation ex-
traction was later formulated as a classification
problem: Kambhatla (2004) proposed to solve this
problem using maximum entropy models using
lexical, syntactic and semantic features. Kernel
methods for relation extraction, based on shallow
parse trees or dependency trees were introduced
by Zelenko et al. (2003), Culotta and Sorensen
(2004) and Bunescu and Mooney (2005).
Unsupervised learning. The open information
extraction paradigm, simultaneously proposed by
Shinyama and Sekine (2006) and Banko et al.
(2007), does not rely on any labeled data or even
existing relations. Instead, open information ex-
traction systems only use an unlabeled corpus, and
output a set of extracted relations. Such systems
are based on clustering (Shinyama and Sekine,
2006) or self-supervision (Banko et al., 2007).
One of the limitations of these systems is the fact
that they extract uncanonicalized relations.
Weakly supervised learning. Weakly super-
vised learning refers to a broad class of meth-
ods, in which the learning system only have ac-
cess to partial, ambiguous and noisy labeling.
Craven and Kumlien (1999) were the first to pro-
pose a weakly supervised relation extractor. They
aligned a knowledge database (the Yeast Protein
Database) with scientific articles mentioning a par-
ticular relation, and then used the extracted sen-
tences to learn a classifier for extracting that rela-
tion.
Later, many different sources of weak label-
ings have been considered. Bellare and McCallum
(2007) proposed a method to extract bibliographic
relations based on conditional random fields and
used a database of BibTex entries as weak super-
vision. Wu and Weld (2007) described a method
to learn relations based on Wikipedia infoboxes.
Knowledge databases, such as Freebase
1
(Mintz et
al., 2009; Sun et al., 2011) and YAGO
2
(Nguyen
and Moschitti, 2011) were also considered as a
source of weak supervision.
Multiple instance learning. The methods we
previously mentionned transform the weakly su-
pervised problem into a fully supervised one, lead-
ing to noisy training datasets (see Fig. 1). Mul-
1
www.freebase.com
2
www.mpi-inf.mpg.de/yago-naga/yago
1581
tiple instance learning (Dietterich et al., 1997) is
a paradigm in which the learner receives bags of
examples instead of individual examples. A pos-
itively labeled bag contains at least one positive
example, but might also contains negative exam-
ples. In the context of relation extraction, Bunescu
and Mooney (2007) introduced a kernel method
for multiple instance learning, while Riedel et al.
(2010) proposed a solution based on a graphical
model.
Both these methods allow only one label per
bag, which is an asumption that is not true for
relation extraction (see Fig. 1). Thus, Hoffmann
et al. (2011) proposed a multiple instance, multi-
ple label method, based on an undirected graphical
model, to solve the problem of weakly supervised
relation extraction. Finally, Surdeanu et al. (2012)
also proposed a graphical model to solve this prob-
lem. One of their main contributions is to cap-
ture dependencies between relation labels, such as
the fact that two labels cannot be generated jointly
(e.g. the relations SpouseOf and BornIn).
Discriminative clustering. Our approach is
based on the discriminative clustering framework,
introduced by Xu et al. (2004). The goal of dis-
criminative clustering is to find a labeling of the
data points leading to a classifier with low classifi-
cation error. Different formulations of discrimina-
tive clustering have been proposed, based on sup-
port vector machines (Xu et al., 2004), the squared
loss (Bach and Harchaoui, 2007) or the logistic
loss (Joulin et al., 2010). A big advantage of dis-
criminative clustering is that weak supervision or
prior information can easily be incorporated. Our
work is closely related to the method proposed by
Bojanowski et al. (2013) for learning the names of
characters in movies.
3 Weakly supervised relation extraction
In this article, our goal is to extract binary
relations between entities from natural lan-
guage text. Given a set of entities, a binary
relation r is a collection of ordered pairs of
entities. The statement that a pair of entities
(e
1
, e
2
) belongs to the relation r is denoted by
r(e
1
, e
2
) and this triple is called a fact or relation
instance. For example, the fact that Ernest
Hemingway was born in Oak Park is denoted
by BornIn(Ernest Hemingway, Oak Park).
A given pair of entities, such as
(Edouard Manet, Paris), can belong to
different relations, such as BornIn and DiedIn.
An entity mention is a contiguous sequence of
tokens refering to an entity, while a pair mention
or relation mention candidate is a sequence of text
in which a pair of entities is mentioned. In the
following, relation mention candidates will be re-
stricted to pair of entities that are mentioned in the
same sentence. For example, the sentence:
Ernest Hemingway was born in Oak Park.
contains two entity mentions, corresponding
to two relation mention candidates. In-
deed, the pairs (Hemingway, Oak Park) and
(Oak Park, Hemingway) are two distinct pairs of
entities, where only the first one verifies the rela-
tion BornIn.
Given a text corpus, aggregate extraction corre-
sponds to the task of extracting a set of facts, such
that each extracted fact is expressed at least once in
the corpus. On the other hand, the task of senten-
tial extraction corresponds to labeling each rela-
tion mention candidate by the relation it expresses,
or by a None label if it does not express any rela-
tion. Given a solution to the sentential extraction
problem, it is possible to construct a solution for
the aggregate extraction problem by returning all
the facts that were detected. We will follow this
approach, by building an instance level classifier,
and aggregating the results by extracting the facts
that were detected at least once in the corpus.
In the following, we will describe a method to
learn such a classifier using a database of facts in-
stead of a set of labeled sentences. This setting
is known as distant supervision or weak supervi-
sion, since we do not have access to labeled data
on which we could directly train a sentence level
relation extractor.
4 General approach
In this section, we propose a two step procedure to
solve the problem of weakly supervised relation
extraction:
1. First, we describe a method to infer the re-
lation labels corresponding to each relation
mention candidate of our training set,
2. Second, we train a supervised instance level
relation extractor, using the labels infered
during step 1.
In the second step of our approach, we will simply
use a multinomial logistic regression model. We
1582
(Lichtenstein, New York City)
Roy Lichtenstein was
born in New York City.
Lichtenstein left New
York to study in Ohio.
BornIn
DiedIn
N relation mention candidates
represented by vectors x
n
I pairs of entities p
i
K relations
E
in
R
ik
Figure 2: Instance of the weakly supervised relation extraction problem, with notations used in the text.
now describe the approach we propose for the first
step.
4.1 Notations
Let (p
i
)
1?i?I
be a collection of I pairs of entities.
We suppose that we have N relation mention can-
didates, represented by the vectors (x
n
)
1?n?N
.
LetE ? R
I?N
be a matrix such thatE
in
= 1 if the
relation mention candidate n corresponds to the
pair of entities i, and E
in
= 0 otherwise. The ma-
trix E thus indicates which relation mention can-
didate corresponds to which pair of entities. We
suppose that we have K relations, indexed by the
integers {1, ...,K}. Let R ? R
I?K
be a matrix
such that R
ik
= 1 if the pair of entities i verifies
the relation k, and R
ik
= 0 otherwise. The matrix
R thus represents the knowledge database. See
Fig. 2 for an illustration of these notations.
4.2 Problem formulation
Our goal is to infer a binary matrix
Y ? {0, 1}
N?(K+1)
, such that Y
nk
= 1 if
the relation mention candidate n express the
relation k and Y
nk
= 0 otherwise (and thus, the
integer K + 1 represents the relation None).
We take an approach inspired by the discrimi-
native clustering framework of Xu et al. (2004).
We are thus looking for a (K + 1)-class indicator
matrix Y, such that the classification error of an
optimal multiclass classifier f is minimum. Given
a multiclass loss function ` and a regularizer ?,
this problem can be formulated as:
min
Y
min
f
N
?
n=1
`(y
n
, f(x
n
)) + ?(f),
s.t. Y ? Y
where y
n
is the nth line of Y. The constraints
Y ? Y are added in order to take into account
the information from the weak supervision. We
will describe in the next section what kind of con-
straints are considered.
4.3 Weak supervision by constraining Y
In this section, we show how the information
from the knowledge base can be expressed as con-
straints on the matrix Y.
First, we suppose that each relation mention
candidate express exactly one relation (including
the None relation). This means that the matrix Y
contains exactly one 1 per line, which is equivalent
to the constraint:
?n ? {1, ..., N},
K
?
k=1
Y
nk
= 1.
Second, if the pair i of entities verifies the rela-
tion k we suppose that at least one relation men-
tion candidate indeed express that relation. Thus
we want to impose that for at least one relation
mention candidate n such that E
in
= 1, we have
Y
nk
= 1. This is equivalent to the constraint:
?(i, k) such that R
ik
= 1,
N
?
n=1
E
in
Y
nk
? 1.
Third, if the pair i of entities does not verify the re-
lation k, we suppose that no relation mention can-
didate express that relation. Thus, we impose that
for all mention candidate n such that E
in
= 1, we
have Y
nk
= 0. This is equivalent to the constraint:
?(i, k) such that R
ik
= 0,
N
?
n=1
E
in
Y
nk
= 0.
Finally, we do not want too many relation men-
tion candidates to be classified as None. We thus
impose
?i ? {1, ..., I},
N
?
n=1
E
in
Y
n(K+1)
? c
N
?
n=1
E
in
,
where c is the proportion of relation mention can-
didates that do not express a relation, for entity
pairs that appears in the knowledge database.
1583
We can rewrite these constraints using only ma-
trix operations in the following way:
Y1 = 1
(EY) ? S ?
?
R, (1)
where ? is the Hadamard product (a.k.a. the ele-
mentwise product), the matrix S ? R
I?(K+1)
is
defined by
S
ik
=
{
1 if R
ik
= 1
?1 if R
ik
= 0 or k = K + 1,
and the matrix
?
R ? R
I?(K+1)
is defined by
?
R = [R,?cE1].
The set Y is thus defined as the set of matrices
Y ? {0, 1}
N?(K+1)
that verifies those two linear
constraints. It is important to note that besides the
boolean constraints, the two other constraints are
convex.
5 Squared loss and convex relaxation
In this section, we describe the problem we ob-
tain when using the squared loss, and its associated
convex relaxation. We then introduce an efficient
algorithm to solve this problem, by computing its
dual.
5.1 Primal problem
Following Bach and Harchaoui (2007), we use lin-
ear classifiers W ? R
D?(K+1)
, the squared loss
and the squared `
2
-norm as the regularizer. In that
case, our formulation becomes:
min
Y,W
1
2
?Y ?XW?
2
F
+
?
2
?W?
2
F
,
s.t. Y ? {0, 1}
N?(K+1)
Y1 = 1,
(EY) ? S ? R.
where ? ? ?
F
is the Frobenius norm and the ma-
trix X = [x
1
, ...,x
N
]
>
? R
N?D
represents the
relation mention candidates. Thanks to using the
squared loss, we have a closed form solution for
the matrix W:
W = (X
>
X + ?I
D
)
?1
X
>
Y.
Replacing the matrix W by its optimal solution,
we obtain the following cost function:
min
Y
1
2
Y
>
(I
N
?X(X
>
X + ?I
D
)
?1
X
>
)Y.
Then, by applying the Woodbury matrix identity
and relaxing the constraint Y ? {0, 1}
N?(K+1)
into Y ? [0, 1]
N?(K+1)
, we obtain the following
convex quadratic problem in Y:
min
Y
1
2
tr
(
Y
>
(XX
>
+ ?I
N
)
?1
Y
)
,
s.t. Y ? 0,
Y1 = 1,
(EY) ? S ? R.
Since the inequality constraints might be in-
feasible, we add the penalized slack variables
? ? R
I?(K+1)
, finally obtaining:
min
Y,?
1
2
tr
(
Y
>
(XX
>
+ ?I
N
)
?1
Y
)
+ ????
1
s.t. Y ? 0, ? ? 0,
Y1 = 1,
(EY) ? S ? R? ?.
This convex problem is a quadratic program. In
the following section, we will describe how to
solve this problem efficiently, by exploiting the
structure of its dual problem.
5.2 Dual problem
The matrix Q = (XX
>
+ ?I
N
) appearing in the
quadratic program is an N by N matrix, where
N is the number of mention relation candidates.
Computing its inverse is thus expensive, since N
can be large. Instead, we propose to solve the
dual of this problem. Introducing dual variables
? ? R
I?(K+1)
, ? ? R
N?(K+1)
and ? ? R
N
,
the dual problem is equal to
min
?,?,?
1
2
tr
(
Z
>
QZ
)
? tr
(
?
>
R
)
? ?
>
1
s.t. 0 ? ?
ik
? ?, 0 ? ?
nk
,
where
Z = E
>
(S ? ?) + ? + ?1
>
.
The derivation of this dual problem is given in Ap-
pendix A.
Solving the dual problem instead of the primal
has two main advantages. First, the dual does not
depend on the inverse of the matrix Q, while the
primal does. Since traditional features used for re-
lation extraction are indicators of lexical, syntactic
and named entities properties of the relation men-
tion candidates, the matrix X is extremely sparse.
1584
Using the dual problem, we can thus exploit the
sparsity of the matrix X in the optimization pro-
cedure. Second, the constraints imposed on dual
variables are simpler than constraints imposed on
primal variables. Again, we will exploit this struc-
ture in the proposed optimization procedure.
Given a solution of the dual problem, the asso-
ciated primal variable Y is equal to:
Y = (XX
>
+ ?I
N
)Z.
Thus, we do not need to compute the inverse of the
matrix (XX
>
+ ?I
N
) to obtain a solution to the
primal problem once we have solved the dual.
5.3 Optimization of the dual problem
We propose to solve the dual problem using
the accelerated projected gradient descent algo-
rithm (Nesterov, 2007; Beck and Teboulle, 2009).
Indeed, computing the gradient of the dual cost
function is efficient, since the matrix X is sparse.
Moreover, the constraints on the dual variables are
simple and it is thus efficient to project onto this
set of constraints. See Appendix B for more de-
tails.
Complexity. The overall complexity of one step
of the accelerated projected gradient descent al-
gorithm is O(NFK), where F is the average
number of features per relation mention candi-
date. This means that the complexity of solving
the quadratic problem corresponding to our ap-
proach is linear with respect to the number N of
relation mention candidates, and thus our algo-
rithm can scale to large datasets.
5.4 Discussion
Before moving to the experimental sections of this
article, we would like to discuss some properties
of our approach.
Kernels. First of all, one should note that our
proposed formulation only depends on the (lin-
ear) kernel matrix XX
T
. It is thus possible to re-
place this matrix by any other kernel. However,
in the case of a general kernel, the optimization
algorithm presented in the previous section has a
quadratic complexity O(KN
2
) with respect to the
number N of relation mention candidates, and it
is thus not applicable as is. We plan to explore the
use of kernels in future work.
Rounding. Given a continuous solution Y ?
[0, 1]
N?(K+1)
of the relaxed problem, a very sim-
ple way to obtain a relation label for each relation
mention candidate of the training set is to com-
pute the orthogonal projection of the matrix Y on
the set of indicator matrices
{
M ? {0, 1}
N?(K+1)
|M1 = 1
}
.
This projection consists in taking the maximum
value along the rows of the matrix Y. It should
be noted that the obtained matrix does not neces-
sarily verify the inequality constraints defined in
Eq. 1. In the following, we will use this rounding,
refered to as argmax rounding, to obtain relation
labels for each relation mention candidate.
6 Dataset and features
In this section, we describe the dataset used in the
experimental section and the features used to rep-
resent the data.
6.1 Dataset
We consider the dataset introduced by Riedel et
al. (2010). This dataset consists of articles from
the New York Times corpus (Sandhaus, 2008),
from which named entities where extracted and
tagged using the Stanford named entity recog-
nizer (Finkel et al., 2005). Consecutive tokens
with the same category were treated as a single
mention. These named entity mentions were then
aligned with the Freebase knowledge database, by
using a string match between the mentions and the
canonical names of entities in Freebase.
6.2 Features
We use the features extracted by Riedel et al.
(2010), which were first introduced by Mintz et
al. (2009). These features capture how two en-
tity mentions are related in a given sentence, based
on syntactic and lexical properties. Lexical fea-
tures include: the sequence of words between the
two entities, a window of k words before the first
entity and after the second entity, the correspond-
ing part-of-speech tags, etc.. Syntactic features are
based on the dependency tree of the sentence, and
include: the path between the two entities, neigh-
bors of the two entities that do not belong to the
path. The OpenNLP
3
part-of-speech tagger and
the Malt parser (Nivre et al., 2007) were used to
extract those features.
3
opennlp.apache.org
1585
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Recall
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Pr
ec
isi
on
Mintz et al. (2009)
Hoffmann et al. (2011)
Surdeanu et al. (2012)
This work
Figure 3: Precision/recall curves for different methods on the Riedel et al. (2010) dataset, for the task of
aggregate extraction.
6.3 Implementation details
In this section, we discuss some important imple-
mentation details.
Kernel normalization. We normalized the ker-
nel matrix XX
>
, so that its diagonal coefficients
are equal to 1. This corresponds to normalizing
the vectors x
n
so that they have a unit `
2
-norm.
Choice of parameters. We kept 20% of the ex-
amples from the training set as a validation set, in
order to choose the parameters of our method. We
then re-train a model on the whole training set, us-
ing the chosen parameters.
7 Experimental evaluation
In this section, we evaluate our approach to weakly
supervised relation extraction by comparing it to
state-of-the art methods.
7.1 Baselines
We now briefly present the different methods we
compare to.
Mintz et al. This baseline corresponds to the
method described by Mintz et al. (2009). We
use the implementation of Surdeanu et al. (2012),
which slightly differs from the original method:
each relation mention candidate is treated inde-
pendently (and not collapsed across mentions for
a given entity pair). This strategy allows to predict
multiple labels for a given entity pair, by OR-ing
the predictions for the different mentions.
Hoffmann et al. This method, introduced by
Hoffmann et al. (2011), is based on probabilis-
tic graphical model of multi-instance multi-label
learning. They proposed a learning method
for this model, based on the perceptron algo-
rithm (Collins, 2002) and a greedy search for the
inference. We use the publicly available code of
Hoffmann et al.
4
.
Surdeanu et al. Finally, we compare our
method to the one described by Surdeanu et al.
(2012). This method is based on a two-layer
graphical model, the first layer corresponding to
4
www.cs.washington.edu/ai/raphaelh/mr/
1586
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Recall
0.0
0.2
0.4
0.6
0.8
1.0
Pr
ec
isi
on
/location/location/contains
/people/person/place_lived
/person/person/nationality
/people/person/place_of_birth
/business/person/company
Figure 4: Precision/recall curves per relation for our method, on the Riedel et al. (2010) dataset, for the
task of aggregate extraction.
a relation classifier at the mention level, while the
second layer is aggregating the different predic-
tion for a given entity pair. In particular, this sec-
ond layer capture dependencies between relation
labels, such as the fact that two labels cannot be
generated jointly (e.g. the relations SpouseOf and
BornIn). This model is trained by using hard
discriminative Expectation-Maximization. We use
the publicly available code of Surdeanu et al.
5
.
7.2 Precision / recall curves
Following standard practices in relation extrac-
tion, we report precision/recall curves for the dif-
ferent models. In order to rank aggregate extrac-
tions for our model, the score of an extracted fact
r(e
1
, e
2
) is set to the maximal score of the differ-
ent extractions of that fact. This is sometimes ref-
ered to as the soft-OR function.
7.3 Discussion
Comparison with the state-of-the-art. We re-
port results for the different methods on the dataset
5
nlp.stanford.edu/software/mimlre.shtml
introduced by Riedel et al. (2010) in Fig. 3. We
observe that our approach generally outperforms
the state of the art. Indeed, at equivalent recall,
our method achieves better (or similar) precision
than the other methods, except for very low re-
call (smaller than 0.05). The improvement over
the methods proposed by Hoffmann et al. (2011)
and Surdeanu et al. (2012), which are currently
the best published results on this dataset, can be
as high as 5 points in precision for the same recall
point. Moreover, our method achieves a higher re-
call (0.30) than these two methods (0.25).
Performance per relation. The dataset in-
troduced by Riedel et al. (2010) is highly
unbalanced: for example, the most common
relation, /location/location/contains, rep-
resents almost half of the positive relations, while
some relations are mentioned less than ten times.
We thus decided to also report precision/recall
curves for the five most common relations of
that dataset in Fig. 4. First, we observe that the
perfomances vary a lot from a relation to another.
The frequence of the different relations is not the
1587
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Recall
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Prec
ision
Hoffmann et al. (2011)This work
Figure 5: Precision/recall curves for the task
of sentential extraction, on the manually labeled
dataset of Hoffmann et al. (2011).
only factor in those discrepancies. Indeed, the
relation /people/person/place lived and the
relation /people/person/place of birth
are more frequent than the relation
/business/person/company, but the ex-
traction of the later works much better than the
extraction of the two first.
Upon examination of the data, this can
partly be explained by the fact that al-
most no sentences extracted for the relation
/people/person/place of birth in fact
express this relation. In other words, many
facts present in Freebase are not expressed in
the corpus, and are thus impossible to extract.
On the other hand, most facts for the relation
/people/person/place lived are missing in
Freebase. Therefore, many extractions produced
by our system are considered false, but are in
fact true positives. The problem of incomplete
knowledge base was studied by Min et al. (2013).
Sentential extraction. We finally report preci-
sion/recall curves for the task of sentential extrac-
tion, in Fig. 5, using the manually labeled dataset
of Hoffmann et al. (2011). We observe that for
most values of recall, our method achieves simi-
lar precision that the one proposed by Hoffmann
et al. (2011), while extending the highest recall
from 0.52 to 0.68. Thanks to this higher recall, our
method achieves a highest F1 score of 0.66, com-
pared to 0.61 obtained by the method proposed by
Hoffmann et al. (2011).
Method Runtime
Mintz et al. (2009) 7 min
Hoffmann et al. (2011) 2 min
Surdeanu et al. (2012) 3 hours
This work 3 hours
Table 1: Comparison of running times for the dif-
ferent methods compared in the experimental sec-
tion.
8 Conclusion
In this article, we introduced a new formulation
for weakly supervised relation extraction. Our
method is based on a constrained discriminative
formulation of the multiple instance, multiple la-
bel learning problem. Using the squared loss,
we obtained a convex relaxation of this formula-
tion, allowing us to obtain an approximate solu-
tion to the initial integer quadratic program. Thus,
our method is not sensitive to initialization. We
demonstrated the competitiveness of our approach
on the dataset introduced by Riedel et al. (2010),
on which our method outperforms the state of the
art methods for weakly supervised relation extrac-
tion, on both aggregate and sentential extraction.
As noted earlier, another advantage of our
method is the fact that it is easily kernelizable.
We would like to explore the use of kernels, such
as the ones introduced by Zelenko et al. (2003),
Culotta and Sorensen (2004) and Bunescu and
Mooney (2005), in future work. We believe that
such kernels could improve the relatively low re-
call obtained so far by weakly supervised method
for relation extraction.
Acknowledgments
The author is supported by a grant from Inria
(Associated-team STATWEB) and would like to
thank Armand Joulin for helpful discussions.
References
Francis Bach and Za??d Harchaoui. 2007. DIFFRAC: a
discriminative and flexible framework for clustering.
In Adv. NIPS.
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI.
Amir Beck and Marc Teboulle. 2009. A fast iterative
shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1).
1588
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Sixth international workshop on in-
formation integration on the web.
Piotr Bojanowski, Francis Bach, Ivan Laptev, Jean
Ponce, Cordelia Schmid, and Josef Sivic. 2013.
Finding actors and actions in movies. In Proceed-
ings of ICCV.
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of HLT-EMNLP.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
supervision. In Proceedings of the ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In ISMB, volume 1999.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the ACL.
Thomas G Dietterich, Richard H Lathrop, and Tom?as
Lozano-P?erez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial in-
telligence, 89(1).
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the ACL.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the ACL.
Armand Joulin, Jean Ponce, and Francis Bach. 2010.
Efficient optimization for discriminative latent class
models. In Adv. NIPS.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In Proceedings
of the ACL.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone, and
Ralph Weischedel. 1998. Algorithms that learn to
extract information. In Proceedings of MUC-7.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of HLT-NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
ACL-IJCNLP.
Yurii Nesterov. 2007. Gradient methods for minimiz-
ing composite objective function.
Truc-Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases.
Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the HLT-NAACL.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP-CoNLL.
Fei Wu and Daniel S Weld. 2007. Autonomously
semantifying wikipedia. In Proceedings of the six-
teenth ACM conference on Conference on informa-
tion and knowledge management.
Linli Xu, James Neufeld, Bryce Larson, and Dale
Schuurmans. 2004. Maximum margin clustering.
In Adv. NIPS.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3.
1589
Appendix A Derivation of the dual
In this section, we derive the dual problem of the
quadratic program of section 5. We introduce dual
variables ? ? R
I?(K+1)
, ? ? R
N?(K+1)
,
? ? R
I?(K+1)
and ? ? R
N
, such that ? ? 0,
? ? 0 and ? ? 0.
The Lagrangian of the problem is
1
2
tr
(
Y
>
(XX
>
+ ?I
N
)
?1
Y
)
+ ?
?
i,k
?
ik
? tr
(
?
>
((EY) ? S?R + ?)
)
? tr(?
>
Y)? tr(?
>
?)? ?
>
(Y1? 1).
To find the dual function g we minimize the La-
grangian over Y and ?. Minimizing over ?, we
find that the dual function is equal to ?? unless
???
ik
??
ik
= 0, in which case, we are left with
1
2
tr
(
Y
>
(XX
>
+ ?I
N
)
?1
Y
)
? tr((? ? S)
>
EY)? tr(?
>
Y)? tr(1?
>
Y)
+ tr(?
>
R) + ?
>
1.
Minimizing over Y, we then obtain
Y = (XX
>
+ ?I
N
)(E
>
(S ? ?) + ? + ?1
>
).
Replacing Y by its optimal value, we then obtain
the dual function
?
1
2
tr
(
Z
>
QZ
)
+ tr
(
?
>
R
)
+ ?
>
1.
where
Q = (XX
>
+ ?I
N
),
Z = E
>
(S ? ?) + ? + ?1
>
.
Thus, the dual problem is
max
?,?,?
?
1
2
tr
(
Z
>
QZ
)
+ tr
(
?
>
R
)
+ ?
>
1
s.t. 0 ? ?
ik
, 0 ? ?
nk
, 0 ? ?
ik
,
?? ?
ik
? ?
ik
= 0.
We can then eliminate the dual variable ?, since
the constraints ?
ik
= ? ? ?
ik
and ?
ik
? 0 are
equivalent to ? ? ?
ik
. We finally obtain
max
?,?,?
?
1
2
tr
(
Z
>
QZ
)
+ tr
(
?
>
R
)
+ ?
>
1
s.t. 0 ? ?
ik
? ?, 0 ? ?
nk
.
Appendix B Optimization details
Gradient of the dual cost function. The gradi-
ent of the dual cost function f with respect to the
dual variables ?, ? and ? is equal to
?
?
f = (XX
>
+ ?I
N
)Z,
?
?
f =
(
(XX
>
+ ?I
N
)ZE
>
)
? S?R,
?
?
f = (XX
>
+ ?I
N
)Z1? 1.
The most expensive step to compute those gra-
dients is to compute the matrix product XX
>
Z.
Since the matrix X is sparse, we efficiently com-
pute this product by first computing the product
X
>
Z, and then by left multiplying the result by
X. The complexity of these two operations is
O(NFK), where F is the average number of fea-
tures per relation mention candidate.
Projecting ? and ?. The componentwise pro-
jection operators associated to the constraints on
? and ? are defined by:
proj
?
(?
nk
) = max(0,?
nk
),
proj
?
(?
ik
) = max(0,min(?,?
ik
)).
The complexity of projecting ? and ? is O(NK).
Thus, the cost of those operations is ne gligible
compared to the cost of computing the gradients
of the dual cost function.
1590
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 94?103,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Hidden Markov tree models for semantic class induction
E?douard Grave
Inria - Sierra Project-Team
E?cole Normale Supe?rieure
Paris, France
Edouard.Grave
@inria.fr
Guillaume Obozinski
Universite? Paris-Est, LIGM
E?cole des Ponts - ParisTech
Marne-la-Valle?e, France
Guillaume.Obozinski
@imagine.enpc.fr
Francis Bach
Inria - Sierra Project-Team
E?cole Normale Supe?rieure
Paris, France
Francis.Bach
@ens.fr
Abstract
In this paper, we propose a new method
for semantic class induction. First, we in-
troduce a generative model of sentences,
based on dependency trees and which
takes into account homonymy. Our model
can thus be seen as a generalization of
Brown clustering. Second, we describe
an efficient algorithm to perform inference
and learning in this model. Third, we
apply our proposed method on two large
datasets (108 tokens, 105 words types),
and demonstrate that classes induced by
our algorithm improve performance over
Brown clustering on the task of semi-
supervised supersense tagging and named
entity recognition.
1 Introduction
Most competitive learning methods for compu-
tational linguistics are supervised, and thus re-
quire labeled examples, which are expensive to
obtain. Moreover, those techniques suffer from
data scarcity: many words only appear a small
number of time, or even not at all, in the training
data. It thus helps a lot to first learn word clus-
ters on a large amount of unlabeled data, which
are cheap to obtain, and then to use this clusters
as features for the supervised task. This scheme
has proven to be effective for various tasks such
as named entity recognition (Freitag, 2004; Miller
et al, 2004; Liang, 2005; Faruqui et al, 2010),
syntactic chunking (Turian et al, 2010) or syntac-
tic dependency parsing (Koo et al, 2008; Haffari
et al, 2011; Tratz and Hovy, 2011). It was also
successfully applied for transfer learning of multi-
lingual structure by Ta?ckstro?m et al (2012).
The most commonly used clustering method for
semi-supervised learning is the one proposed by
Brown et al (1992), and known as Brown clus-
tering. While still being one of the most efficient
word representation method (Turian et al, 2010),
Brown clustering has two limitations we want to
address in this work. First, since it is a hard clus-
tering method, homonymy is ignored. Second, it
does not take into account syntactic relations be-
tween words, which seems crucial to induce se-
mantic classes. Our goal is thus to propose a
method for semantic class induction which takes
into account both syntax and homonymy, and then
to study their effects on semantic class learning.
In this paper, we start by introducing a new un-
supervised method for semantic classes induction.
This is achieved by defining a generative model
of sentences with latent variables, which aims at
capturing semantic roles of words. We require our
method to be scalable, in order to learn models on
large datasets containing tens of millions of sen-
tences. More precisely, we make the following
contributions:
? We introduce a generative model of sen-
tences, based on dependency trees, which can
be seen as a generalization of Brown cluster-
ing,
? We describe a fast approximate inference al-
gorithm, based on message passing and on-
line EM for scaling to large datasets. It al-
lowed us to learn models with 512 latent
states on a dataset with hundreds of millions
of tokens in less than two days on a single
core,
? We learn models on two datasets, Wikipedia
articles about musicians and the NYT corpus,
94
and evaluate them on two semi-supervised
tasks, namely supersense tagging and named
entity recognition.
1.1 Related work
Brown clustering (Brown et al, 1992) is the most
commonly used method for word cluster induc-
tion for semi-supervised learning. The goal of this
algorithm is to discover a clustering function C
from words to clusters which maximizes the like-
lihood of the data, assuming the following sequen-
tial model of sentences:
?
k
p(wk | C(wk))p(C(wk) | C(wk?1)).
It can be shown that the best clustering is actually
maximizing the mutual information between adja-
cent clusters. A greedy agglomerative algorithm
was proposed by Brown et al (1992) in order to
find the clustering C, while Clark (2003) proposed
to use the exchange clustering algorithm (Kneser
and Ney, 1993) to maximize the previous likeli-
hood. One of the limitations of this model is the
fact that it neither takes into account homonymy
or syntax.
Another limitation of this method is the com-
plexity of the algorithms proposed to find the best
clustering. This led Uszkoreit and Brants (2008)
to consider a slightly different model, where the
class-to-class transitions are replaced by word-to-
class transitions:
?
k
p(wk | C(wk))p(C(wk) | wk?1).
Thanks to that modification, Uszkoreit and Brants
(2008) designed an efficient variant of the ex-
change algorithm, allowing them to train models
on very large datasets. This model was then ex-
tended to the multilingual setting by Ta?ckstro?m et
al. (2012).
Semantic space models are another family of
methods, besides clustering, that can be used as
features for semi-supervised learning. In those
techniques, words are represented as vectors in
a high-dimensional space. These vectors are ob-
tained by representing the unlabeled corpus as a
word-document co-occurrence matrix in the case
of latent semantic analysis (LSA) (Deerwester et
al., 1990), or word-word co-occurrence matrix in
the case of the hyperspace analog to language
model (HAL) (Lund and Burgess, 1996). Dimen-
sion reduction is then performed, by taking the
singular value decomposition of the co-occurrence
matrix, in order to obtained the so-called seman-
tic space. Hofmann (1999) proposed a variant of
LSA, which corresponds to a generative model of
document. More recently, Dhillon et al (2011)
proposed a method based on canonical correlation
analysis to obtained a such word embeddings.
A last approach to word representation is la-
tent Dirichlet alocation (LDA), proposed by Blei
et al (2003). LDA is a generative model where
each document is viewed as a mixture of topics.
The major difference between LDA and our model
is the fact that LDA treats documents as bags of
words, while we introduce a model of sentences,
taking into account the syntax. Griffiths et al
(2005) defined a composite model, using LDA for
topic modeling and an HMM for syntax model-
ing. This model, HMM-LDA, was used by Li
and McCallum (2005) for semi-supervised learn-
ing and applied to part-of-speech tagging and Chi-
nese word segmentation. Se?aghdha (2010) pro-
posed to use topic models, such as LDA, to per-
form selectional preference induction.
Finally, Boyd-Graber and Blei (2009) proposed
a variant of LDA, using parse trees to include the
syntax. Given that we aim for our classes to cap-
ture as much of the word semantics reflected by
the syntax, such as the semantic roles of words,
we believe that it is not necessarily useful or even
desirable that the latent variables should be deter-
mined, even in part, by topic parameters that are
sharing information at the document level. More-
over, our model being significantly simpler, we
were able to design fast and efficient algorithms,
making it possible to use our model on much
larger datasets, and with many more latent classes.
2 Model
In this section, we introduce our probabilistic gen-
erative model of sentences. We start by setting
up some notations. A sentence is represented
by a K-tuple w = (w1, ..., wK) where each
wk ? {1, ..., V } is an integer representing a word
and V is the size of the vocabulary. Our goal will
be to infer a K-tuple c = (c1, ..., cK) of seman-
tic classes, where each ck ? {1, ..., C} is an in-
teger representing a semantic class, corresponding
to the word wk.
The generation of a sentence can be decom-
posed in two steps: first, we generate the seman-
tic classes according to a Markov process, and
95
Opposition political parties have harshly criticized the pact
c0 c1 c2 c3 c4 c5 c6 c7 c8
w1 w2 w3 w4 w5 w6 w7 w8
Figure 1: Example of a dependency tree and its corresponding graphical model.
then, given each class ck, we generate the corre-
sponding word wk independently of other words.
The Markov process used to generate the seman-
tic classes will take into account selectional pref-
erence. Since we want to model homonymy, each
word can be generated by multiple classes.
We now describe the Markov process we pro-
pose to generate the semantic classes. We assume
that we are given a directed tree defined by the
function pi : {1, ...,K} 7? {0, ...,K}, where pi(k)
represents the unique parent of the node k and 0
is the root of the tree. Each node, except the root,
corresponds to a word of the sentence. First, we
generate the semantic class corresponding to the
root of the tree and then generate recursively the
class for the other nodes. The classes are condi-
tionally independent given the classes of their par-
ents. Using the language of probabilistic graphical
models, this means that the distribution of the se-
mantic classes factorizes in the tree defined by pi
(See Fig. 1 for an example). We obtain the fol-
lowing distribution on pairs (w, c) of words and
semantic classes:
p(w, c) =
K?
k=1
p(ck | cpi(k))p(wk | ck),
with c0 being equal to a special symbol denoting
the root of the tree.
In order to fully define our model, we now
need to specify the observation probability distri-
bution p(wk | ck) of a word given the correspond-
ing class and the transition probability distribution
p(ck | cpi(k)) of a class given the class of the par-
ent. Both these distributions will be categorical
(and thus multinomial with one trial). The cor-
responding parameters will be represented by the
stochastic matrices O and T (i.e. matrices with
non-negative elements and unit-sum columns):
p(wk = i | ck = j) = Oij ,
p(ck = i | cpi(k) = j) = Tij .
Finally, we introduce the trees that we consider to
define the distribution on semantic classes. (We
recall that the trees are assumed given, and not a
part of the model.)
2.1 Markov chain model
The simplest structure we consider on the seman-
tic classes is a Markov chain. In this special case,
our model reduces to a hidden Markov model.
Each semantic class only depends on the class of
the previous word in the sentence, thus failing to
capture selectional preference of semantic class.
But because of its simplicity, it may be more ro-
bust, and does not rely on external tools. It can be
seen as a generalization of the Brown clustering
algorithm (Brown et al, 1992) taking into account
homonymy.
2.2 Dependency tree model
The second kind of structure we consider to model
interactions between semantic classes is a syntac-
tic dependency tree corresponding to the sentence.
A dependency tree is a labeled tree in which nodes
correspond to the words of a sentence, and edges
represent the grammatical relations between those
words, such as nominal subject, direct object or
determiner. We use the Stanford typed dependen-
cies basic representations, which always form a
tree (De Marneffe and Manning, 2008).
96
We believe that a dependency tree is a better
structure than a Markov chain to learn semantic
classes, with no additional cost for inference and
learning compared to a chain. First, syntactic de-
pendencies can capture long distance interactions
between words. See Fig. 1 and the dependency
between parties and criticized for an ex-
ample. Second, the syntax is important to model
selectional preference. Third, we believe that syn-
tactic trees could help much for languages which
do not have a strict word order, such as Czech,
Finnish, or Russian. One drawback of this model
is that all the children of a particular node share
the same transition probability distribution. While
this is not a big issue for nouns, it is a bigger con-
cern for verbs: subject and object should not share
the same transition probability distribution.
A potential solution would be to introduce a dif-
ferent transition probability distribution for each
type of dependency. This possibility will be ex-
plored in future work.
2.3 Brown clustering on dependency trees
As for Brown clustering, we can assume that
words are generated by a single class. In that case,
our model reduces to finding a deterministic clus-
tering function C which maximizes the following
likelihood:
?
k
p(wk | C(wk))p(C(wk) | C(wpi(k))).
In that case, we can use the algorithm proposed
by Brown et al (1992) to greedily maximize the
likelihood of the data. This model can be seen as
a generalization of Brown clustering taking into
account the syntactic relations between words.
3 Inference and learning
In this section, we present the approach used to
perform learning and inference in our model. Our
goal here is to have efficient algorithms, in order
to apply our model to large datasets (108 tokens,
105 words types). The parameters T and O of the
model will be estimated with the maximum likeli-
hood estimator:
T?, O? = argmax
T,O
N?
n=1
p(w(n) | T,O),
where (w(n))n?{1,...,N} represents our training set
of N sentences.
First, we present an online variant of the well-
known expectation-maximization (EM) algorithm,
proposed by Cappe? and Moulines (2009), allowing
our method to be scalable in term of numbers of
examples. Then, we present an approximate mes-
sage passing algorithm which has a linear com-
plexity in the number of classes, instead of the
quadratic complexity of the exact inference algo-
rithm. Finally, we describe a state-splitting strat-
egy to speed up the learning.
3.1 Online EM
In the batch EM algorithm, the E-step consists in
computing the expected sufficient statistics ? and
? of the model, sometimes referred as pseudo-
counts, corresponding respectively to T and O:
?ij =
N?
n=1
Kn?
k=1
E
[
?(c(n)k = i, c
(n)
pi(k) = j)
]
,
?ij =
N?
n=1
Kn?
k=1
E
[
?(w(n)k = i, c
(n)
k = j)
]
.
On large datasets, N which is the number of sen-
tences can be very large, and so, EM is inefficient
because it requires that inference is performed on
the entire dataset at each iteration. We therefore
consider the online variant proposed by Cappe?
and Moulines (2009): instead of recomputing the
pseudocounts on the whole dataset at each itera-
tion t, those pseudocounts are updated using only
a small subset Bt of the data, to get
? (t)ij = (1? ?t)?
(t?1)
ij +
?t
?
n?Bt
Kn?
k=1
E
[
?(c(n)k = i, c
(n)
pi(k) = j)
]
,
and
?(t)ij = (1? ?t)?
(t?1)
ij +
?t
?
n?Bt
Kn?
k=1
E
[
?(w(n)k = i, c
(n)
k = j)
]
,
where the scalars ?t are defined by ?t = 1/(a +
t)? with 0.5 < ? ? 1. In the experiments,
we used a = 4. We chose ? in the set
{0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.
3.2 Approximate inference
Inference is performed on trees using the sum-
product message passing algorithm, a.k.a. belief
97
0 2000 4000 6000 8000 10000Iteration5.95
5.905.85
5.80
Normali
zed log-
likelihoo
d
k = 128k = 64k = 32k = 16 0 2000 4000 6000 8000 10000Iteration5.95
5.905.85
5.80
Normali
zed log-
likelihoo
d
epsilon = 0.0epsilon = 0.001epsilon = 0.01epsilon = 0.1 0 100 200 300 400 500Iteration010
203040
506070
80
Support
 size epsilon = 0.0001epsilon = 0.001epsilon = 0.01epsilon = 0.1
Figure 2: Comparison of the two projection methods for approximating vectors, for a model with 128
latent classes. The first two plots are the log-likelihood on a held-out set as a function of the iterates of
online EM. Green curves (k = 128 and ? = 0) correspond to learning without approximation.
propagation, which extends the classical ??? re-
cursions used for chains, see e.g. Wainwright and
Jordan (2008). We denote by N (k) the set con-
taining the children and the father of node k. In
the exact message-passing algorithm, the message
?k?pi(k) from node k to node pi(k) takes the form:
?k?pi(k) = T>u,
where u is the vector obtained by taking the ele-
mentwise product of all the messages received by
node k except the one from node pi(k), i.e.,
ui =
?
k??N (k)\{pi(k)}
?k??k(i).
Similarly, the pseudocounts can be written as
E
[
?(c(n)k = i, c
(n)
pi(k) = j)
]
? uiTijvj ,
where v is the vector obtained by taking the ele-
mentwise product of all the messages received by
node pi(k), except the one from node k, i.e.,
vj =
?
k??N (pi(k))\{k}
?k??pi(k)(j).
Both these operations thus have quadratic com-
plexity in the number of semantic classes. In or-
der to reduce the complexity of those operations,
we propose to start by projecting the vectors u
and v on a set of sparse vectors, and then, per-
form the operations with the sparse approximate
vectors. We consider two kinds of projections:
? k-best projection, where the approximate
vector is obtained by keeping the k largest
coefficients,
? ?-best projection, where the approximate
vector is obtained by keeping the smallest set
of larger coefficients such that their sum is
greater than (1? ?) times the `1-norm of the
original vector.
This method is similar to the one proposed by Pal
et al (2006). The advantage of the k-best projec-
tion is that we control the complexity of the op-
erations, but not the error, while the advantage of
the ?-best projection is that we control the error
but not the complexity. As shown in Fig. 2, good
choices for ? and k are respectively 0.01 and 16.
We use these values in the experiments. We also
note, on the right plot of Fig. 2, that during the
first iterations of EM, the sparse vectors obtained
with the ?-best projection have a large number of
non-zero elements. Thus, this projection is not
adequate to directly learn large latent class mod-
els. This issue is addressed in the next section,
where we present a state splitting strategy in or-
der to learn models with a large number of latent
classes.
3.3 State splitting
A common strategy to speed up the learning of
large latent state space models, such as ours, is
to start with a small number of latent states, and
split them during learning (Petrov, 2009). As far
as we know, there are still no good heuristics to
choose which states to split, or how to initialize the
parameters corresponding to the new states. We
thus apply the simple, yet effective method, con-
sisting in splitting all states into two and in break-
ing the symmetry by adding a bit of randomness
to the emission probabilities of the new states. As
noted by Petrov (2009), state splitting could also
improve the quality of learnt models.
3.4 Initialization
Because the negative log-likelihood function is not
convex, initialization can greatly change the qual-
ity of the final model. Initialization for online EM
is done by setting the initial pseudocounts, and
then performing an M-step. We have considered
98
the following strategies to initialize our model:
? random initialization: the initial pseudo-
counts ?ij and ?ij are sampled from a uni-
form distribution on [0, 1],
? Brown initialization: the model is initial-
ized using the (normalized) pseudocounts ob-
tained by the Brown clustering algorithm.
Because a parameter equal to zero remains
equal to zero when using the EM algorithm,
we replace null pseudocounts by a small
smoothing value, e.g., for observation i, we
use 10?5 ?maxj ?ij ,
4 Experiments
In this section, we present the datasets used for the
experiments, and the two semi-supervised tasks
on which we evaluate our models: named entity
recognition and supersense tagging.
4.1 Datasets
We considered two datasets: the first one, which
we refer to as the music dataset, corresponds to
all the Wikipedia articles refering to a musical
artist. They were extracted using the Freebase
database1. This dataset comprises 2.22 millions
sentences and 56 millions tokens. We choose this
dataset because it corresponds to a restricted do-
main.
The second dataset are the articles of the NYT
corpus (Sandhaus, 2008) corresponding to the pe-
riod 1987-1997 and labeled as news. This dataset
comprises 14.7 millions sentences and 310 mil-
lions tokens.
We parsed both datasets using the Stanford
parser, and converted parse trees to dependency
trees (De Marneffe et al, 2006). We decided to
discard sentences longer than 50 tokens, for pars-
ing time reasons, and then lemmatized tokens us-
ing Wordnet. Each word of our vocabulary is then
a pair of lemma and its associated part-of-speech.
This means that the noun attack and the verb at-
tack are two different words. Finally, we intro-
duced a special token, -*-, for infrequent (lemma,
part-of-speech) pairs, in order to perform smooth-
ing. For the music dataset, we kept the 25 000
most frequent words, while for the NYT corpus,
we kept the 100 000 most frequent words. For the
music dataset we set the number of latent states to
256, while we set it to 512 for the NYT corpus.
1www.freebase.com
4.2 Qualitative results
Before moving on to the quantitative evaluation of
our model, we discuss qualitatively the induced se-
mantic classes. Examples of semantic classes are
presented in Tables 1, 2 and 3. Tree models with
random initialization were used to obtain those se-
mantic classes. First we observe that most classes
can be easily given natural semantic interpretation.
For example class 196 of Table 1 contains musical
instruments, while class 116 contains musical gen-
res.
Table 2 presents groups of classes that contain a
given homonymous word; it seems that the differ-
ent classes capture rather well the different senses
of each word. For example, the word head belongs
to the class 116, which contains body parts and to
the class 127, which contains words referring to
leaders.
4.3 Semi-supervised learning
We propose to evaluate and compare the different
models in the following semi-supervised learning
setting: we start by learning a model on the NYT
corpus in an unsupervised way, and then use it to
define features for a supervised classifier. We now
introduce the tasks we considered.
4.3.1 Named entity recognition
The first supervised task on which we evaluate the
different models, is named entity recognition. We
cast it as a sequence tagging problem, and thus, we
use a linear conditional random field (CRF) (Laf-
ferty et al, 2001) as our supervised classifier. For
each sentence, we apply the Viterbi algorithm in
order to obtain the most probable sequence of se-
mantic classes, and use this as features for the
CRF. The only other feature we use is a binary
feature indicating if the word is capitalized or not.
Results of experiments performed on the MUC7
dataset are reported in table 4. The baseline for
this task is assigning named entity classes to word
sequences that occur in the training data.
4.3.2 Supersense tagging
Supersense tagging consists in identifying, for
each word of a sentence, its corresponding su-
persense, a.k.a. lexicographer class, as defined by
Wordnet (Ciaramita and Altun, 2006). Because
each Wordnet synset belongs to one lexicogra-
pher class, supersense tagging can be seen as a
coarse disambiguation task for nouns and verbs.
We decided to evaluate our models on this task to
99
# 54 radio BBC television station tv stations channel 1 MTV program network fm music
# 52 chart billboard uk top top singles 100 Hot album country 40 10 R&B 200 US song u.s.
# 78 bach mozart liszt beethoven wagner chopin brahms stravinsky haydn debussy tchaikovsky
# 69 sound style instrument elements influence genre theme form lyric audience direction
#215 tour show concert performance appearance gig date tours event debut session set night party
#116 rock pop jazz classical folk punk metal roll hip country traditional -*- blues dance
#123 win receive sell gain earn award achieve garner give enjoy have get attract bring include
#238 reach peak hit chart go debut make top platinum fail enter gold become with certify
#203 piano concerto -*- for violin symphony in works sonata string of quartet orchestra no.
#196 guitar bass vocal drum keyboard piano saxophone percussion violin player trumpet organ
#243 leave join go move form return sign tour begin decide continue start attend meet disband
#149 school university college hall conservatory academy center church institute cathedral
Table 1: Selected semantic classes corresponding to the music dataset. Like LDA, our model is a proba-
bilistic model which generates words from latent classes. Unlike LDA though, rather than treating words
as exchangeable, it accounts for syntax and semantic relations between words. As a consequence, instead
of grouping words with same topic but various semantic roles or grammatical functions, our model tends
to group words that tend to be syntactically and semantically equivalent.
#116 head hand hands foot face shoulder way knee eyes back body finger car arms arm
#127 president member director chairman executive head editor professor manager secretary
#360 company corporation group industry fund bank association institute trust system
#480 street avenue side bank square precinct coast broadway district strip bridge station
#87 pay base sell use available buy depend make provide receive get lose spend charge offer
#316 charge arrest convict speak tell found accuse release die indict ask responsible suspend
#263 system computer machine technology plant product program equipment line network
#387 plan agreement contract effort program proposal deal offer bill bid order campaign request
#91 have be win score play lead hit make run -*- lose finish pitch start miss come go shoot take
#198 kill shoot die wound injure found arrest fire report take dead attack beat leave strike carry
Table 2: Semantic classes containing homonymous words. Different classes capture different senses of
each word.
demonstrate the effect of homonymy. We cast su-
persense tagging as a classification problem and
use posterior distribution of semantic classes as
features for a support vector machine with the
Hellinger kernel, defined by
K(p,q) =
C?
c=1
?pcqc,
where p and q are posterior distributions. We train
and test the SVM classifier on the section A, B and
C of the Brown corpus, tagged with Wordnet su-
persenses (SemCor). All the considered methods
predict among the possible supersenses according
to Wordnet, or among all the supersenses if the
word does not appear in Wordnet. We report re-
sults in Table 5. The baseline predicts the most
common supersense of the training set.
4.4 Discussion of results
First, we observe that hidden Markov models im-
prove performances over Brown clustering, on
both chains and trees. This seems to indicate
that taking into account homonymy leads to richer
models which is beneficial for both tasks. We also
note that Brown clustering on dependency trees al-
ways outperforms Brown clustering on chains for
the two tasks we consider, confirming that syntac-
tic dependencies are a better structure to induce
semantic classes than a linear chain.
Hidden Markov tree models also outperform
hidden Markov chain models, except for super-
sense tagging on verbs. We believe that this drop
in performance on verbs can be explained because
in English the word order (Subject-Verb-Object)
is strict, and thus, the chain model is able to dif-
100
#484 rise fell be close offer drop gain trade price jump slip end decline unchanged sell total lose
#352 it have would But be not nt will get may too make So see might can always still probably
#115 coach manager bill Joe george don pat Jim bob Lou al general mike Dan tom owner ray
#131 San St. santa Notre s Francisco calif. green tampa Diego louis class AP bay &aaa Fla. Jose
#350 strong short score good better hit second leave fast close impressive easy high quick enough
#274 A Another an new second single free -*- special fair national strong long major political big
#47 gogh rushdie pan guardia vega freud Prensa miserable picasso jesus Armani Monde Niro
#489 health public medical right care human civil community private social research housing
#238 building house home store apartment area space restaurant site neighborhood town park
#38 more very too as so much less enough But seem even because if particularly relatively pretty
Table 3: Randomly selected semantic classes corresponding to the news dataset.
F1 score
Baseline 71.66
Brown clustering 82.57
tree Brown clustering 82.93
chain HMM, random init 84.66
chain HMM, Brown init 84.47
tree HMM, random init 84.07
tree HMM, Brown init 85.49
Table 4: Results of semi-supervised named entity
recognition.
ferentiate between subject and object, while the
tree model treats subject and object in the same
way (both are children of the verb). Moreover, in
the tree model, verbs have a lot of children, such
as adverbial clauses and auxiliary verbs, which
share their transition probability distribution with
the subject and the object. These two effects make
the disambiguation of verbs more noisy for trees
than for chains. Another possible explanation of
this drop of performance is that it is due to errors
made by the syntactic parser.
4.5 On optimization parameters
We briefly discuss the different choices that can
influence learning efficiency in the proposed mod-
els. In practice, we have not observed noticeable
differences between ?-best projection and k-best
projection for the approximate inference, and we
thus advise to use the latter as its complexity is
controled. By contrast, as illustrated by results in
tables 4 and 5, initialization can greatly change the
performance in semi-supervised learning, in par-
ticular for tree models. We thus advise to initialize
with Brown clusters. Finally, as noted by Liang
and Klein (2009), the step size of online EM also
nouns verbs
Baseline 61.9 (0.2) 43.1 (0.2)
Brown clustering 73.9 (0.1) 63.7 (0.2)
tree Brown clustering 75.0 (0.2) 65.2 (0.2)
HMM (random) 76.1 (0.1) 63.0 (0.2)
HMM (Brown) 76.8 (0.1) 66.6 (0.3)
tree HMM (random) 76.7 (0.1) 61.5 (0.2)
tree HMM (Brown) 77.9 (0.1) 66.0 (0.2)
Table 5: Results of semi-supervised supersense
tagging: prediction accuracies with confidence in-
tervals, obtained on 50 random splits of the data.
has a significant impact on performance.
5 Conclusion
In this paper, we considered an arguably natural
generative model of sentences for semantic class
induction. It can be seen as a generalization of
Brown clustering, taking into account homonymy
and syntax, and thus allowed us to study their im-
pact on semantic class induction. We developed an
efficient algorithm to perform inference and learn-
ing, which makes it possible to learn in this model
on large datasets, such as the New York Times
corpus. We showed that this model induces rel-
evant semantic classes and that it improves perfor-
mance over Brown clustering on semi-supervised
named entity recognition and supersense tagging.
We plan to explore in future work better ways to
model verbs, and in particular how to take into ac-
count the type of dependencies between words.
Acknowledgments
Francis Bach is supported in part by the European
Research Council (SIERRA ERC-239993).
101
References
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learn-
ing Research.
J. L. Boyd-Graber and D. Blei. 2009. Syntactic topic
models. In Advances in Neural Information Pro-
cessing Systems 21.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-
gram models of natural language. Computational
linguistics.
O. Cappe? and E. Moulines. 2009. On-line
expectation?maximization algorithm for latent data
models. Journal of the Royal Statistical Society: Se-
ries B (Statistical Methodology).
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings
of the 2006 Conference on Empirical Methods in
Natural Language Processing.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth conference of Eu-
ropean chapter of the Association for Computational
Linguistics.
M. C. De Marneffe and C. D. Manning. 2008. The
Stanford typed dependencies representation. In Col-
ing 2008: Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
M. C. De Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of
LREC.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American society
for information science.
P. S. Dhillon, D. Foster, and L. Ungar. 2011. Multi-
view learning of word embeddings via CCA. Ad-
vances in Neural Information Processing Systems.
M. Faruqui, S. Pado?, and M. Sprachverarbeitung.
2010. Training and evaluating a German named en-
tity recognizer with semantic generalization. Se-
mantic Approaches in Natural Language Process-
ing.
D. Freitag. 2004. Trained named entity recognition
using distributional clusters. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B.
Tenenbaum. 2005. Integrating topics and syn-
tax. Advances in Neural Information Processing
Systems.
G. Haffari, M. Razavi, and A. Sarkar. 2011. An en-
semble model that combines syntactic and semantic
clustering for discriminative dependency parsing. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics.
T. Hofmann. 1999. Probabilistic latent semantic anal-
ysis. In Proceedings of the Fifteenth conference on
Uncertainty in artificial intelligence.
R. Kneser and H. Ney. 1993. Improved clustering
techniques for class-based statistical language mod-
elling. In Third European Conference on Speech
Communication and Technology.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of ACL-08: HLT.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proceedings
of the 18th International Conference on Machine
Learning.
W. Li and A. McCallum. 2005. Semi-supervised se-
quence modeling with syntactic topic models. In
Proceedings of the National Conference on Artificial
Intelligence.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute
of Technology.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative train-
ing. In Proceedings of HLT-NAACL.
C. Pal, C. Sutton, and A. McCallum. 2006.
Sparse forward-backward using minimum diver-
gence beams for fast training of conditional random
fields. In ICASSP 2006 Proceedings.
S. Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California
at Bekeley.
E. Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia.
D. O. Se?aghdha. 2010. Latent variable models of se-
lectional preference. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics.
102
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
S. Tratz and E. Hovy. 2011. A fast, accurate, non-
projective, semantically-enriched parser. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics.
J. Uszkoreit and T. Brants. 2008. Distributed word
clustering for large scale class-based language mod-
eling in machine translation. Proceedings of ACL-
08: HLT.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
models, exponential families, and variational infer-
ence. Foundations and Trends R? in Machine Learn-
ing.
103
