Morphological Analysis of The Spontaneous Speech Corpus
Kiyotaka Uchimoto?, Chikashi Nobata?, Atsushi Yamada?,
Satoshi Sekine?, and Hitoshi Isahara?
?Communications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes a project tagging a sponta-
neous speech corpus with morphological infor-
mation such as word segmentation and parts-of-
speech. We use a morphological analysis system
based on a maximum entropy model, which is
independent of the domain of corpora. In this
paper we show the tagging accuracy achieved by
using the model and discuss problems in tagging
the spontaneous speech corpus. We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
1 Introduction
In recent years, systems developed for analyz-
ing written-language texts have become consid-
erably accurate. This accuracy is largely due
to the large amounts of tagged corpora and the
rapid progress in the study of corpus-based nat-
ural language processing. However, the accu-
racy of the systems developed for written lan-
guage is not always high when these same sys-
tems are used to analyze spoken-language texts.
The reason for this remaining inaccuracy is due
to several differences between the two types of
languages. For example, the expressions used
in written language are often quite different
from those in spoken language, and sentence
boundaries are frequently ambiguous in spoken
language. The ?Spontaneous Speech: Corpus
and Processing Technology? project was imple-
mented in 1999 to overcome this problem. Spo-
ken language includes both monologue and dia-
logue texts; the former (e.g. the text of a talk)
was selected as a target of the project because it
was considered to be appropriate to the current
level of study on spoken language.
Tagging the spontaneous speech corpus with
morphological information such as word seg-
mentation and parts-of-speech is one of the
goals of the project. The tagged corpus is help-
ful for us in making a language model in speech
recognition as well as for linguists investigat-
ing distribution of morphemes in spontaneous
speech. For tagging the corpus with morpholog-
ical information, a morphological analysis sys-
tem is needed. Morphological analysis is one of
the basic techniques used in Japanese sentence
analysis. A morpheme is a minimal grammat-
ical unit, such as a word or a suffix, and mor-
phological analysis is the process of segment-
ing a given sentence into a row of morphemes
and assigning to each morpheme grammatical
attributes such as part-of-speech (POS) and in-
flection type. One of the most important prob-
lems in morphological analysis is that posed by
unknown words, which are words found in nei-
ther a dictionary nor a training corpus. Two
statistical approaches have been applied to this
problem. One is to find unknown words from
corpora and put them into a dictionary (e.g.,
(Mori and Nagao, 1996)), and the other is to
estimate a model that can identify unknown
words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both
approaches. They proposed a morphological
analysis method based on a maximum entropy
(M.E.) model (Uchimoto et al, 2001). We used
their method to tag a spontaneous speech cor-
pus. Their method uses a model that can not
only consult a dictionary but can also identify
unknown words by learning certain characteris-
tics. To learn these characteristics, we focused
on such information as whether or not a string
is found in a dictionary and what types of char-
acters are used in a string. The model esti-
mates how likely a string is to be a morpheme.
This model is independent of the domain of cor-
pora; in this paper we demonstrate that this is
true by applying our model to the spontaneous
speech corpus, Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000). We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
2 A Morpheme Model
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem of
Japanese morphological analysis can be reduced
to the problem of assigning one of two tags to
each string in a sentence. A string is tagged
with a 1 or a 0 to indicate whether or not it is
a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. The 1
tag is thus divided into the number, n, of gram-
matical attributes assigned to morphemes, and
the problem is to assign an attribute (from 0
to n) to every string in a given sentence. The
(n + 1) tags form the space of ?futures? in the
M.E. formulation of our problem of morpholog-
ical analysis. The M.E. model enables the com-
putation of P (f |h) for any future f from the
space of possible futures, F , and for every his-
tory, h, from the space of possible histories, H.
The computation of P (f |h) in any M.E. model
is dependent on a set of ?features? which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
g(h, f) =
?
?
?
?
?
1 : if has(h, x) = true,
x = ?POS(?1)(Major) : verb,??
& f = 1
0 : otherwise.
(1)
Here ?has(h,x)? is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and what part-of-speech
the adjacent morpheme is.
Given a set of features and some training
data, the M.E. estimation process produces a
model, which is represented as follows (Berger
et al, 1996; Ristad, 1997; Ristad, 1998):
P (f |h) =
?
i
?
g
i
(h,f)
i
Z
?
(h)
(2)
Z
?
(h) =
?
f
?
i
?
g
i
(h,f)
i
. (3)
We define a model which estimates the like-
lihood that a given string is a morpheme and
has the grammatical attribute i(1 ? i ? n) as a
morpheme model. This model is represented by
Eq. (2), in which f can be one of (n + 1) tags
from 0 to n.
Given a sentence, it is divided into mor-
phemes, and a grammatical attribute is assigned
to each morpheme so as to maximize the sen-
tence probability estimated by our morpheme
model. Sentence probability is defined as the
product of the probabilities estimated for a par-
ticular division of morphemes in a sentence. We
use the Viterbi algorithm to find the optimal set
of morphemes in a sentence.
3 Experiments and Discussion
3.1 Experimental Conditions
We used the spontaneous speech corpus, CSJ,
which is a tagged corpus of transcriptions of
academic presentations and simulated public
speech. Simulated public speech is short speech
spoken specifically for the corpus by paid non-
professional speakers. For training, we used
805,954 morphemes from the corpus, and for
testing, we used 68,315 morphemes from the
corpus. Since there are no boundaries between
sentences in the corpus, we used two types of
boundaries, utterance boundaries, which are au-
tomatically detected at the place where a pause
of 200 ms or longer emerges in the CSJ, and
sentence boundaries assigned by the sentence
boundary identification system, which is based
on hand-crafted rules which use the pauses as
a clue. In the CSJ, fillers and disfluencies are
marked with tags (F) and (D). In the experi-
ments, we did not use those tags. Thus the in-
put sentences for testing are character strings
without any tags. The output is a sequence
of morphemes with grammatical attributes. As
the grammatical attributes, we define the part-
of-speech categories in the CSJ. There are 12
major categories. Therefore, the number of
grammatical attributes is 12, and f in Eq. (2)
can be one of 13 tags from 0 to 12.
Given a sentence, for every string consist-
ing of five or fewer characters and every string
appearing in a dictionary, whether or not the
string is a morpheme was determined and then
the grammatical attribute of each string deter-
mined to be a morpheme was identified and
assigned to that string. We collected all mor-
phemes from the training corpus except dis-
fluencies and used them as dictionary entries.
We denote the entries with a Corpus dictionary.
The maximum length for a morpheme was set
at five because morphemes consisting of six or
more characters are mostly compound words or
words consisting of katakana characters. We as-
sumed that compound words that do not appear
in the dictionary can be divided into strings con-
sisting of five or fewer characters because com-
pound words tend not to appear in dictionar-
ies. Katakana strings that are not found in the
dictionary were assumed to be included in the
dictionary as an entry having the part-of-speech
?Unknown(Major), Katakana(Minor).? An op-
timal set of morphemes in a sentence is searched
for by employing the Viterbi algorithm. The
assigned part-of-speech in the optimal set is se-
lected from all the categories of the M.E. model
except the one in which the string is not a mor-
pheme.
The features used in our experiments are
listed in Table 1. Each feature consists of a
type and a value, which are given in the rows of
the table. The features are basically some at-
tributes of the morpheme itself or attributes of
the morpheme to the left of it. We used the fea-
tures found three or more times in the training
corpus. The notations ?(0)? and ?(-1)? used in
the feature type column in Table 1 respectively
indicate a target string and the morpheme to
the left of it.
The terms used in the table are as follows:
String: Strings appearing as a morpheme three
or more times in the training corpus
Substring: Characters used in a string.
?(Left1)? and ?(Right1)? respectively rep-
resent the leftmost and rightmost charac-
ters of a string. ?(Left2)? and ?(Right2)?
respectively represent the leftmost and
rightmost character bigrams of a string.
Dic: Entries in the Corpus dictionary. As mi-
nor categories we used inflection types such
as a basic form as well as minor part-of-
speech categories. ?Major&Minor? indi-
cates possible combinations between major
and minor part-of-speech categories. When
the target string is in the dictionary, the
part-of-speech attached to the entry corre-
sponding to the string is used as a feature
value. If an entry has two or more parts-
of-speech, the part-of-speech which leads to
the highest probability in a sentence esti-
mated from our model is selected as a fea-
ture value.
Length: Length of a string
TOC: Types of characters used in a string.
?(Beginning)? and ?(End)?, respectively,
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the ?(Begin-
ning)? and ?(End)? are the same character.
?TOC(0)(Transition)? represents the tran-
sition from the leftmost character to the
rightmost character in a string. ?TOC(-
1)(Transition)? represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
character in the target string. For example,
when the adjacent morpheme on the left
is ??? (sensei, teacher)? and the target
string is ?? (ni, case marker),? the feature
value ?Kanji?Hiragana? is selected.
POS: Part-of-speech.
3.2 Results and Discussion
Results of the morphological analysis obtained
by our method are shown in Table 2. Recall
is the percentage of morphemes in the test cor-
pus whose segmentation and major POS tag are
identified correctly. Precision is the percentage
of all morphemes identified by the system that
are identified correctly. The F-measure is de-
fined by the following equation.
F ? measure =
2 ? Recall ? Precision
Recall + Precision
This result shows that there is no significant
difference between accuracies obtained by us-
ing two types of sentence boundaries. However,
we found that the errors that occurred around
utterance boundaries were reduced in the re-
sult obtained with sentence boundaries assigned
by the sentence boundary identification system.
This shows that there is a high possibility that
we can achieve better accuracy if we use bound-
aries assigned by the sentence boundary identi-
fication system as sentence boundaries and if we
use utterance boundaries as features.
In these experiments, we used only the en-
tries with a Corpus dictionary. Next we show
the experimental results with dictionaries de-
veloped for a corpus on a certain domain. We
added to the Corpus dictionary all the approx-
imately 200,000 entries of the JUMAN dictio-
nary (Kurohashi and Nagao, 1999). We also
added the entries of a dictionary developed by
ATR. We call it the ATR dictionary.
Results obtained with each dictionary or each
combination of dictionaries are shown in Ta-
ble 3. In this table, OOV indicates Out-of-
Vocabulary rates. The accuracy obtained with
the JUMAN dictionary or the ATR dictionary
was worse than the accuracy obtained without
those dictionaries. This is because the segmen-
Table 1: Features.
Feature number Feature type Feature value (Number of value)
1 String(0) (223,457)
2 String(-1) (20,769)
3 Substring(0)(Left1) (2,492)
4 Substring(0)(Right1) (2,489)
5 Substring(0)(Left2) (74,046)
6 Substring(0)(Right2) (73,616)
7 Substring(-1)(Left1) (2,237)
8 Substring(-1)(Right1) (2,489)
9 Substring(-1)(Left2) (12,726)
10 Substring(-1)(Right2) (12,241)
11 Dic(0)(Major) Noun, Verb, Adj, . . . Undefined (13)
12 Dic(0)(Minor) Common noun, Topic marker, Basic form. . . (223)
13 Dic(0)(Major&Minor) Noun&Common noun, Verb&Basic form, . . . (239)
14 Length(0) 1, 2, 3, 4, 5, 6 or more (6)
15 Length(-1) 1, 2, 3, 4, 5, 6 or more (6)
16 TOC(0)(Beginning) Kanji, Hiragana, Number, Katakana, Alphabet (5)
17 TOC(0)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
18 TOC(0)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (25)
19 TOC(-1)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
20 TOC(-1)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (18)
21 POS(-1) Verb, Adj, Noun, . . . (12)
22 Comb(1,21) Combinations Feature 1 and 21 (142,546)
23 Comb(1,2,21) Combinations Feature 1, 2 and 21 (216,431)
24 Comb(1,13,21) Combinations Feature 1, 13 and 21 (29,876)
25 Comb(1,2,13,21) Combinations Feature 1, 2, 13 and 21 (158,211)
26 Comb(11,21) Combinations Feature 11 and 21 (156)
27 Comb(12,21) Combinations Feature 12 and 21 (1,366)
28 Comb(13,21) Combinations Feature 13 and 21 (1,518)
Table 2: Results of Experiments (Segmentation and major POS tagging).
Boundary Recall Precision F-measure
utterance 93.97% (64,198/68,315) 93.25% (64,198/68,847) 93.61
sentence 93.97% (64,195/68,315) 93.18% (64,195/68,895) 93.57
tation of morphemes and the definition of part-
of-speech categories in the JUMAN and ATR
dictionaries are different from those in the CSJ.
Given a sentence, for every string consisting
of five or fewer characters as well as every string
appearing in a dictionary, whether or not the
string is a morpheme was determined by our
morpheme model. However, we speculate that
we can ignore strings consisting of two or more
characters when they are not found in the dic-
tionary when OOV is low. Therefore, we carried
out the additional experiments ignoring those
strings. In the experiments, given a sentence,
for every string consisting of one character and
every string appearing in a dictionary, whether
or not the string is a morpheme is determined
by our morpheme model. Results obtained un-
der this condition are shown in Table 4. We
compared the accuracies obtained with dictio-
naries including the Corpus dictionary, whose
OOVs are relatively low. The accuracies ob-
tained with the additional dictionaries increased
while those obtained only with the Corpus dic-
tionary decreased. These results show that a
dictionary whose OOV in the test corpus is low
contributes to increasing the accuracy when ig-
noring the possibility that strings that consist
of two or more characters and are not found in
the dictionary become a morpheme.
These results show that a dictionary devel-
oped for a corpus on a certain domain can be
used to improve accuracy in analyzing a corpus
on another domain.
The accuracy in segmentation and major
POS tagging obtained for spontaneous speech
was worse than the approximately 95% obtained
for newspaper articles. We think the main rea-
son for this is the errors and the inconsistency
of the corpus, and the difficulty in recognizing
characteristic expressions often used in spoken
language such as fillers, mispronounced words,
and disfluencies. The inconsistency of the cor-
pus is due to the way the corpus was made, i.e.,
completely by human beings, and it is also due
Table 3: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.64% (63,288/68,315) 91.83% (63,288/68,917) 92.24 1.84%
Corpus sentence 92.61% (63,265/68,315) 91.79% (63,265/68,923) 92.20 1.84%
JUMAN utterance 90.28% (61,676/68,315) 90.07% (61,676/68,478) 90.17 6.13%
JUMAN sentence 90.33% (61,710/68,315) 90.22% (61,710/68,403) 90.27 6.13%
ATR utterance 89.80% (61,348/68,315) 90.12% (61,348/68,073) 89.96 8.14%
ATR sentence 89.96% (61,453/68,315) 90.30% (61,453/68,057) 90.13 8.14%
Corpus+JUMAN utterance 92.03% (62,872/68,315) 91.77% (62,872/68,507) 91.90 0.52%
Corpus+JUMAN sentence 92.09% (62,913/68,315) 91.80% (62,913/68,534) 91.95 0.52%
Corpus+ATR utterance 92.35% (63,086/68,315) 92.03% (63,086/68,547) 92.19 0.64%
Corpus+ATR sentence 92.30% (63,057/68,315) 91.94% (63,057/68,585) 92.12 0.64%
JUMAN+ATR utterance 91.60% (62,579/68,315) 91.57% (62,579/68,339) 91.59 4.61%
JUMAN+ATR sentence 91.66% (62,618/68,315) 91.67% (62,618/68,311) 91.66 4.61%
Corpus+JUMAN+ATR utterance 91.72% (62,658/68,315) 91.66% (62,658/68,357) 91.69 0.47%
Corpus+JUMAN+ATR sentence 91.72% (62,657/68,315) 91.62% (62,657/68,391) 91.67 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
Table 4: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.80% (63,395/68,315) 90.47% (63,395/70,075) 91.62 1.84%
Corpus sentence 92.71% (63,333/68,315) 90.48% (63,333/70,000) 91.58 1.84%
Corpus+JUMAN utterance 92.45% (63,154/68,315) 91.60% (63,154/68,942) 92.02 0.52%
Corpus+JUMAN sentence 92.48% (63,179/68,315) 91.71% (63,179/68,893) 92.09 0.52%
Corpus+ATR utterance 92.91% (63,474/68,315) 91.81% (63,474/69,137) 92.36 0.64%
Corpus+ATR sentence 92.75% (63,361/68,315) 91.76% (63,361/69,053) 92.25 0.64%
Corpus+JUMAN+ATR utterance 92.30% (63,055/68,315) 91.57% (63,055/68,858) 91.94 0.47%
Corpus+JUMAN+ATR sentence 92.28% (63,039/68,315) 91.55% (63,039/68,860) 91.91 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
to the definition of morphemes. Several incon-
sistencies in the test corpus existed, such as: ?
?? (tokyo, Noun)(Tokyo), ? (to, Other)(the
Metropolis), ? (ritsu, Other)(founded), ?
? (daigaku, Noun)(university),? and ???
(toritsu, Noun)(metropolitan), ?? (daigaku,
Noun)(university).? Both of these are the
names representing the same university. The
???? is partitioned into two in the first one
while it is not partitioned into two in the second
one according to the definition of morphemes.
When such inconsistencies in the corpus exist, it
is difficult for our model to discriminate among
these inconsistencies because we used only bi-
gram information as features. To achieve bet-
ter accuracy, therefore, we need to use trigram
or longer information. To correctly recognize
characteristic expressions often used in spoken
language, we plan to extract typical patterns
used in the expressions, to generalize the pat-
terns manually, and to generate possible expres-
sions using the generalized patterns, and finally,
to add such patterns to the dictionary. We also
plan to expand our model to skip fillers, mispro-
nounced words, and disfluencies because those
expressions are randomly inserted into text and
it is impossible to learn the connectivity be-
tween those randomly inserted expressions and
others.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis without a Dictionary for
Japanese. In Proceedings of the NLPRS, pages 541?544.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proceedings of the
LREC, pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distribu-
tional Analysis. In Proceedings of the COLING, pages
1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method
for Japanese Unknown Words using a Statistical Model
of Morphology and Context. In Proceedings of the ACL,
pages 277?284.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 91?99.
Morphological Analysis of a Large Spontaneous Speech Corpus in Japanese
Kiyotaka Uchimoto? Chikashi Nobata? Atsushi Yamada?
Satoshi Sekine? Hitoshi Isahara?
?Communications Research Laboratory
3-5, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes two methods for de-
tecting word segments and their morpho-
logical information in a Japanese sponta-
neous speech corpus, and describes how
to tag a large spontaneous speech corpus
accurately by using the two methods. The
first method is used to detect any type of
word segments. The second method is
used when there are several definitions for
word segments and their POS categories,
and when one type of word segments in-
cludes another type of word segments. In
this paper, we show that by using semi-
automatic analysis we achieve a precision
of better than 99% for detecting and tag-
ging short words and 97% for long words;
the two types of words that comprise the
corpus. We also show that better accuracy
is achieved by using both methods than by
using only the first.
1 Introduction
The ?Spontaneous Speech: Corpus and Process-
ing Technology? project is sponsoring the construc-
tion of a large spontaneous Japanese speech corpus,
Corpus of Spontaneous Japanese (CSJ) (Maekawa
et al, 2000). The CSJ is a collection of mono-
logues and dialogues, the majority being mono-
logues such as academic presentations and simu-
lated public speeches. Simulated public speeches
are short speeches presented specifically for the cor-
pus by paid non-professional speakers. The CSJ in-
cludes transcriptions of the speeches as well as audio
recordings of them. One of the goals of the project
is to detect two types of word segments and cor-
responding morphological information in the tran-
scriptions. The two types of word segments were
defined by the members of The National Institute for
Japanese Language and are called short word and
long word. The term short word approximates a dic-
tionary item found in an ordinary Japanese dictio-
nary, and long word represents various compounds.
The length and part-of-speech (POS) of each are dif-
ferent, and every short word is included in a long
word, which is shorter than a Japanese phrasal unit,
a bunsetsu. If all of the short words in the CSJ
were detected, the number of the words would be
approximately seven million. That would be the
largest spontaneous speech corpus in the world. So
far, approximately one tenth of the words have been
manually detected, and morphological information
such as POS category and inflection type have been
assigned to them. Human annotators tagged every
morpheme in the one tenth of the CSJ that has been
tagged, and other annotators checked them. The hu-
man annotators discussed their disagreements and
resolved them. The accuracies of the manual tagging
of short and long words in the one tenth of the CSJ
were greater than 99.8% and 97%, respectively. The
accuracies were evaluated by random sampling. As
it took over two years to tag one tenth of the CSJ ac-
curately, tagging the remainder with morphological
information would take about twenty years. There-
fore, the remaining nine tenths of the CSJ must be
tagged automatically or semi-automatically.
In this paper, we describe methods for detecting
the two types of word segments and corresponding
morphological information. We also describe how
to tag a large spontaneous speech corpus accurately.
Henceforth, we call the two types of word segments
short word and long word respectively, or merely
morphemes. We use the term morphological anal-
ysis for the process of segmenting a given sentence
into a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a POS cate-
gory.
2 Problems and Their Solutions
As we mentioned in Section 1, tagging the whole of
the CSJ manually would be difficult. Therefore, we
are taking a semi-automatic approach. This section
describes major problems in tagging a large sponta-
neous speech corpus with high precision in a semi-
automatic way, and our solutions to those problems.
One of the most important problems in morpho-
logical analysis is that posed by unknown words,
which are words found in neither a dictionary nor
a training corpus. Two statistical approaches have
been applied to this problem. One is to find un-
known words from corpora and put them into a
dictionary (e.g., (Mori and Nagao, 1996)), and the
other is to estimate a model that can identify un-
known words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both ap-
proaches. They proposed a morphological analysis
method based on a maximum entropy (ME) model
(Uchimoto et al, 2001). Their method uses a model
that estimates how likely a string is to be a mor-
pheme as its probability, and thus it has a potential
to overcome the unknown word problem. Therefore,
we use their method for morphological analysis of
the CSJ. However, Uchimoto et al reported that the
accuracy of automatic word segmentation and POS
tagging was 94 points in F-measure (Uchimoto et
al., 2002). That is much lower than the accuracy ob-
tained by manual tagging. Several problems led to
this inaccuracy. In the following, we describe these
problems and our solutions to them.
? Fillers and disfluencies
Fillers and disfluencies are characteristic ex-
pressions often used in spoken language, but
they are randomly inserted into text, so detect-
ing their segmentation is difficult. In the CSJ,
they are tagged manually. Therefore, we first
delete fillers and disfluencies and then put them
back in their original place after analyzing a
text.
? Accuracy for unknown words
The morpheme model that will be described
in Section 3.1 can detect word segments and
their POS categories even for unknown words.
However, the accuracy for unknown words is
lower than that for known words. One of the
solutions is to use dictionaries developed for a
corpus on another domain to reduce the num-
ber of unknown words, but the improvement
achieved is slight (Uchimoto et al, 2002). We
believe that the reason for this is that defini-
tions of a word segment and its POS category
depend on a particular corpus, and the defi-
nitions from corpus to corpus differ word by
word. Therefore, we need to put only words
extracted from the same corpus into a dictio-
nary. We are manually examining words that
are detected by the morpheme model but that
are not found in a dictionary. We are also
manually examining those words that the mor-
pheme model estimated as having low proba-
bility. During the process of manual exami-
nation, if we find words that are not found in
a dictionary, those words are then put into a
dictionary. Section 4.2.1 will describe the ac-
curacy of detecting unknown words and show
how much those words contribute to improving
the morphological analysis accuracy when they
are detected and put into a dictionary.
? Insufficiency of features
The model currently used for morphological
analysis considers the information of a target
morpheme and that of an adjacent morpheme
on the left. To improve the model, we need to
consider the information of two or more mor-
phemes on the left of the target morpheme.
However, too much information often leads to
overtraining the model. Using all the informa-
tion makes training the model difficult when
there is too much of it. Therefore, the best
way to improve the accuracy of the morpholog-
ical information in the CSJ within the limited
time available to us is to examine and revise
the errors of automatic morphological analysis
and to improve the model. We assume that the
smaller the probability estimated by a model
for an output morpheme is, then the greater
the likelihood is that the output morpheme is
wrong. Therefore, we examine output mor-
phemes in ascending order of their probabili-
ties. The expected improvement of the accu-
racy of the morphological information in the
whole of the CSJ will be described in Sec-
tion 4.2.1
Another problem concerning unknown words
is that the cost of manual examination is high
when there are several definitions for word seg-
ments and their POS categories. Since there
are two types of word definitions in the CSJ,
the cost would double. Therefore, to reduce the
cost, we propose another method for detecting
word segments and their POS categories. The
method will be described in Section 3.2, and
the advantages of the method will be described
in Section 4.2.2
The next problem described here is one that we
have to solve to make a language model for auto-
matic speech recognition.
? Pronunciation
Pronunciation of each word is indispensable for
making a language model for automatic speech
recognition. In the CSJ, pronunciation is tran-
scribed separately from the basic form writ-
ten by using kanji and hiragana characters as
shown in Fig. 1. Text targeted for morpho-
Basic form Pronunciation
0017 00051.425-00052.869 L:
(F??) (F??)
????? ?????????
0018 00053.073-00054.503 L:
???? ????
0019 00054.707-00056.341 L:
???????? ?????????
?Well, I?m going to talk about morphological analysis.?
Figure 1: Example of transcription.
logical analysis is the basic form of the CSJ
and it does not have information on actual pro-
nunciation. The result of morphological anal-
ysis, therefore, is a row of morphemes that
do not have information on actual pronuncia-
tion. To estimate actual pronunciation by using
only the basic form and a dictionary is impossi-
ble. Therefore, actual pronunciation is assigned
to results of morphological analysis by align-
ing the basic form and pronunciation in the
CSJ. First, the results of morphological anal-
ysis, namely, the morphemes, are transliterated
into katakana characters by using a dictionary,
and then they are aligned with pronunciation
in the CSJ by using a dynamic programming
method.
In this paper, we will mainly discuss methods for
detecting word segments and their POS categories in
the whole of the CSJ.
3 Models and Algorithms
This section describes two methods for detecting
word segments and their POS categories. The first
method uses morpheme models and is used to detect
any type of word segment. The second method uses
a chunking model and is only used to detect long
word segments.
3.1 Morpheme Model
Given a tokenized test corpus, namely a set of
strings, the problem of Japanese morphological
analysis can be reduced to the problem of assign-
ing one of two tags to each string in a sentence. A
string is tagged with a 1 or a 0 to indicate whether
it is a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. A tag desig-
nated as a 1 is thus assigned one of a number, n, of
grammatical attributes assigned to morphemes, and
the problem becomes to assign an attribute (from 0
to n) to every string in a given sentence.
We define a model that estimates the likelihood
that a given string is a morpheme and has a gram-
matical attribute i(1 ? i ? n) as a morpheme
model. We implemented this model within an ME
modeling framework (Jaynes, 1957; Jaynes, 1979;
Berger et al, 1996). The model is represented by
Eq. (1):
p
?
(a|b) =
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
Z
?
(b)
(1)
Short word Long word
Word Pronunciation POS Others Word Pronunciation POS Others
?? (form) ????(keitai) Noun ????? (morphological
analysis)
????????
?
(keitaisokaiseki) Noun
? (element) ? (so) Suffix
?? (analysis)????(kaiseki) Noun
? ? (ni) PPP case marker ???? (about) ???? (nitsuite) PPP case marker,
compound
word
?? (relate) ?? (tsui) Verb KA-GYO, ADF, eu-
phonic change
? ? (te) PPP conjunctive
? ? (o) Prefix ??????(talk) ??????? (ohanashiitasi) Verb SA-GYO,
ADF
?? (talk) ??? (hanashi) Verb SA-GYO, ADF
???(do) ??? (itashi) Verb SA-GYO, ADF
?? ?? (masu) AUX ending form ?? ?? (masu) AUX ending form
PPP : post-positional particle , AUX : auxiliary verb , ADF : adverbial form
Figure 2: Example of morphological analysis results.
Z
?
(b) =
?
a
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
, (2)
where a is one of the categories for classification,
and it can be one of (n+1) tags from 0 to n (This is
called a ?future.?), b is the contextual or condition-
ing information that enables us to make a decision
among the space of futures (This is called a ?his-
tory.?), and Z
?
(b) is a normalizing constant deter-
mined by the requirement that
?
a
p
?
(a|b) = 1 for
all b. The computation of p
?
(a|b) in any ME model
is dependent on a set of ?features? which are binary
functions of the history and future. For instance, one
of our features is
g
i,j
(a, b) =
{
1 : if has(b, f
j
) = 1 & a = a
i
f
j
= ?POS(?1)(Major) : verb,??
0 : otherwise.
(3)
Here ?has(b, f
j
)? is a binary function that returns
1 if the history b has feature f
j
. The features used
in our experiments are described in detail in Sec-
tion 4.1.1.
Given a sentence, probabilities of n tags from 1
to n are estimated for each length of string in that
sentence by using the morpheme model. From all
possible division of morphemes in the sentence, an
optimal one is found by using the Viterbi algorithm.
Each division is represented as a particular division
of morphemes with grammatical attributes in a sen-
tence, and the optimal division is defined as a di-
vision that maximizes the product of the probabil-
ities estimated for each morpheme in the division.
For example, the sentence ???????????
??????? in basic form as shown in Fig. 1 is
analyzed as shown in Fig. 2. ??????? is ana-
lyzed as three morphemes, ??? (noun)?, ?? (suf-
fix)?, and ??? (noun)?, for short words, and as one
morpheme, ?????? (noun)? for long words.
In conventional models (e.g., (Mori and Nagao,
1996; Nagata, 1999)), probabilities were estimated
for candidate morphemes that were found in a dic-
tionary or a corpus and for the remaining strings
obtained by eliminating the candidate morphemes
from a given sentence. Therefore, unknown words
were apt to be either concatenated as one word or di-
vided into both a combination of known words and
a single word that consisted of more than one char-
acter. However, this model has the potential to cor-
rectly detect any length of unknown words.
3.2 Chunking Model
The model described in this section can be applied
when several types of words are defined in a cor-
pus and one type of words consists of compounds of
other types of words. In the CSJ, every long word
consists of one or more short words.
Our method uses two models, a morpheme model
for short words and a chunking model for long
words. After detecting short word segments and
their POS categories by using the former model,
long word segments and their POS categories are de-
tected by using the latter model. We define four la-
bels, as explained below, and extract long word seg-
ments by estimating the appropriate labels for each
short word according to an ME model. The four la-
bels are listed below:
Ba: Beginning of a long word, and the POS cat-
egory of the long word agrees with the short
word.
Ia: Middle or end of a long word, and the POS cat-
egory of the long word agrees with the short
word.
B: Beginning of a long word, and the POS category
of the long word does not agree with the short
word.
I: Middle or end of a long word, and the POS cat-
egory of the long word does not agree with the
short word.
A label assigned to the leftmost constituent of a long
word is ?Ba? or ?B?. Labels assigned to other con-
stituents of a long word are ?Ia?, or ?I?. For exam-
ple, the short words shown in Fig. 2 are labeled as
shown in Fig. 3. The labeling is done deterministi-
cally from the beginning of a given sentence to its
end. The label that has the highest probability as es-
timated by an ME model is assigned to each short
word. The model is represented by Eq. (1). In Eq.
(1), a can be one of four labels. The features used in
our experiments are described in Section 4.1.2.
Short word Long word
Word POS Label Word POS
?? Noun Ba ????? Noun
? Suffix I
?? Noun Ia
? PPP Ba ???? PPP
?? Verb I
? PPP Ia
? Prefix B ?????? Verb
?? Verb Ia
??? Verb Ia
?? AUX Ba ?? AUX
PPP : post-positional particle , AUX : auxiliary verb
Figure 3: Example of labeling.
When a long word that does not include a short
word that has been assigned the label ?Ba? or ?Ia?,
this indicates that the word?s POS category differs
from all of the short words that constitute the long
word. Such a word must be estimated individually.
In this case, we estimate the POS category by us-
ing transformation rules. The transformation rules
are automatically acquired from the training corpus
by extracting long words with constituents, namely
short words, that are labeled only ?B? or ?I?. A rule
is constructed by using the extracted long word and
the adjacent short words on its left and right. For
example, the rule shown in Fig. 4 was acquired in
our experiments. The middle division of the con-
sequent part represents a long word ???? (auxil-
iary verb), and it consists of two short words ???
(post-positional particle) and ??? (verb). If several
different rules have the same antecedent part, only
the rule with the highest frequency is chosen. If no
rules can be applied to a long word segment, rules
are generalized in the following steps.
1. Delete posterior context
2. Delete anterior and posterior contexts
3. Delete anterior and posterior contexts and lexi-
cal entries.
If no rules can be applied to a long word segment in
any step, the POS category noun is assigned to the
long word.
4 Experiments and Discussion
4.1 Experimental Conditions
In our experiments, we used 744,204 short words
and 618,538 long words for training, and 63,037
short words and 51,796 long words for testing.
Those words were extracted from one tenth of the
CSJ that already had been manually tagged. The
training corpus consisted of 319 speeches and the
test corpus consisted of 19 speeches.
Transcription consisted of basic form and pronun-
ciation, as shown in Fig. 1. Speech sounds were
faithfully transcribed as pronunciation, and also rep-
resented as basic forms by using kanji and hiragana
characters. Lines beginning with numerical digits
are time stamps and represent the time it took to
produce the lines between that time stamp and the
next time stamp. Each line other than time stamps
represents a bunsetsu. In our experiments, we used
only the basic forms. Basic forms were tagged with
several types of labels such as fillers, as shown in
Table 1. Strings tagged with those labels were han-
dled according to rules as shown in the rightmost
columns in Table 1.
Since there are no boundaries between sentences
in the corpus, we selected the places in the CSJ that
Anterior context Target words Posterior context
Entry ?? (it, go) ? (te)? (mi, try) ?? (tai, want)
POS Verb PPP Verb AUX
Label Ba B I Ba
Antecedent part
?
Anterior context Long word Posterior context
?? (it, go) ?? (temi, try) ?? (tai, want)
Verb AUX AUX
Consequent part
Figure 4: Example of transformation rules.
Table 1: Type of labels and their handling.
Type of Labels Example Rules
Fillers (F??) delete all
Disfluencies (D?)????? (D2?)? delete all
No confidence in
transcription
(? ?????) leave a candidate
Entirely (?) delete all
Several can- (? ???,????) leave the former
didates exist candidate
Citation on sound or
words
(M?)? (M?)??? leave a candidate
Foreign, archaic, or
dialect words
(O???????) leave a candidate
Personal name, dis-
criminating words,
and slander
???? (R??)??? leave a candidate
Letters and their
pronunciation in
katakana strings
(A????;EU) leave the former
candidate
Strings that cannot
be written in kanji
characters
(K? (F??)??;?) leave the latter can-
didate
are automatically detected as pauses of 500 ms or
longer and then designated them as sentence bound-
aries. In addition to these, we also used utterance
boundaries as sentence boundaries. These are au-
tomatically detected at places where short pauses
(shorter than 200 ms but longer than 50 ms) follow
the typical sentence-ending forms of predicates such
as verbs, adjectives, and copula.
4.1.1 Features Used by Morpheme Models
In the CSJ, bunsetsu boundaries, which are phrase
boundaries in Japanese, were manually detected.
Fillers and disfluencies were marked with the labels
(F) and (D). In the experiments, we eliminated fillers
and disfluencies but we did use their positional infor-
mation as features. We also used as features, bun-
setsu boundaries and the labels (M), (O), (R), and
(A), which were assigned to particular morphemes
such as personal names and foreign words. Thus, the
input sentences for training and testing were charac-
ter strings without fillers and disfluencies, and both
boundary information and various labels were at-
tached to them. Given a sentence, for every string
within a bunsetsu and every string appearing in a
dictionary, the probabilities of a in Eq. (1) were es-
timated by using the morpheme model. The output
was a sequence of morphemes with grammatical at-
tributes, as shown in Fig. 2. We used the POS cate-
gories in the CSJ as grammatical attributes. We ob-
tained 14 major POS categories for short words and
15 major POS categories for long words. Therefore,
a in Eq. (1) can be one of 15 tags from 0 to 14 for
short words, and it can be one of 16 tags from 0 to
15 for long words.
Table 2: Features.
Number Feature Type Feature value
(Number of value) (Short:Long)
1 String(0) (113,474:117,002)
2 String(-1) (17,064:32,037)
3 Substring(0)(Left1) (2,351:2,375)
4 Substring(0)(Right1) (2,148:2,171)
5 Substring(0)(Left2) (30,684:31,456)
6 Substring(0)(Right2) (25,442:25,541)
7 Substring(-1)(Left1) (2,160:2,088)
8 Substring(-1)(Right1) (1,820:1,675)
9 Substring(-1)(Left2) (11,025:12,875)
10 Substring(-1)(Right2) (10,439:13,364)
11 Dic(0)(Major) Noun, Verb, Adjective, . . . Unde-
fined (15:16)
12 Dic(0)(Minor) Common noun, Topic marker, Ba-
sic form. . . (75:71)
13 Dic(0)(Major&Minor) Noun&Common noun,
Verb&Basic form, . . . (246:227)
14 Dic(-1)(Minor) Common noun, Topic marker, Ba-
sic form. . . (16:16)
15 POS(-1) Noun, Verb, Adjective, . . . (14:15)
16 Length(0) 1, 2, 3, 4, 5, 6 or more (6:6)
17 Length(-1) 1, 2, 3, 4, 5, 6 or more (6:6)
18 TOC(0)(Beginning) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
19 TOC(0)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
20 TOC(0)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (25:25)
21 TOC(-1)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
22 TOC(-1)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (16:15)
23 Boundary Bunsetsu(Beginning), Bun-
setsu(End), Label(Beginning),
Label(End), (4:4)
24 Comb(1,15) (74,602:59,140)
25 Comb(1,2,15) (141,976:136,334)
26 Comb(1,13,15) (78,821:61,813)
27 Comb(1,2,13,15) (156,187:141,442)
28 Comb(11,15) (209:230)
29 Comb(12,15) (733:682)
30 Comb(13,15) (1,549:1,397)
31 Comb(12,14) (730:675)
The features we used with morpheme models in
our experiments are listed in Table 2. Each feature
consists of a type and a value, which are given in the
rows of the table, and it corresponds to j in the func-
tion g
i,j
(a, b) in Eq. (1). The notations ?(0)? and
?(-1)? used in the feature-type column in Table 2 re-
spectively indicate a target string and the morpheme
to the left of it. The terms used in the table are ba-
sically as same as those that Uchimoto et al used
(Uchimoto et al, 2002). The main difference is the
following one:
Boundary: Bunsetsu boundaries and positional in-
formation of labels such as fillers. ?(Begin-
ning)? and ?(End)? in Table 2 respectively indi-
cate whether the left and right side of the target
strings are boundaries.
We used only those features that were found three or
more times in the training corpus.
4.1.2 Features Used by a Chunking Model
We used the following information as features
on the target word: a word and its POS cate-
gory, and the same information for the four clos-
est words, the two on the left and the two on
the right of the target word. Bigram and tri-
gram words that included a target word plus bigram
and trigram POS categories that included the tar-
get word?s POS category were used as features. In
addition, bunsetsu boundaries as described in Sec-
tion 4.1.1 were used. For example, when a target
word was ??? in Fig. 3, ???, ????, ???, ??
??, ???, ?Suffix?, ?Noun?, ?PPP?, ?Verb?, ?PPP?,
???&??, ??&???, ?? &?? &??, ??
&??&??, ?Noun&PPP?, ?PPP&Verb?, ?Suf-
fix&Noun&PPP?, ?PPP&Verb&PPP?, and ?Bun-
setsu(Beginning)? were used as features.
4.2 Results and Discussion
4.2.1 Experiments Using Morpheme Models
Results of the morphological analysis obtained by
using morpheme models are shown in Table 3 and
4. In these tables, OOV indicates Out-of-Vocabulary
rates. Shown in Table 3, OOV was calculated as the
proportion of words not found in a dictionary to all
words in the test corpus. In Table 4, OOV was cal-
culated as the proportion of word and POS category
pairs that were not found in a dictionary to all pairs
in the test corpus. Recall is the percentage of mor-
phemes in the test corpus for which the segmentation
and major POS category were identified correctly.
Precision is the percentage of all morphemes identi-
fied by the system that were identified correctly. The
F-measure is defined by the following equation.
F ? measure =
2? Recall ? Precision
Recall + Precision
Table 3: Accuracies of word segmentation.
Word Recall Precision F OOV
Short 97.47% (61,444
63,037
) 97.62% (61,444
62,945
) 97.54 1.66%
99.23% (62,553
63,037
) 99.11% (62,553
63,114
) 99.17 0%
Long 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21 5.81%
99.05% (51,306
51,796
) 98.58% (51,306
52,047
) 98.81 0%
Table 4: Accuracies of word segmentation and POS
tagging.
Word Recall Precision F OOV
Short 95.72% (60,341
63,037
) 95.86% (60,341
62,945
) 95.79 2.64%
97.57% (61,505
63,037
) 97.45% (61,505
63,114
) 97.51 0%
Long 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21 6.93%
97.30% (50,396
51,796
) 96.83% (50,396
52,047
) 97.06 0%
Tables 3 and 4 show that accuracies would im-
prove significantly if no words were unknown. This
indicates that all morphemes of the CSJ could be an-
alyzed accurately if there were no unknown words.
The improvements that we can expect by detecting
unknown words and putting them into dictionaries
are about 1.5 in F-measure for detecting word seg-
ments of short words and 2.5 for long words. For de-
tecting the word segments and their POS categories,
for short words we expect an improvement of about
2 in F-measure and for long words 3.
Next, we discuss accuracies obtained when un-
known words existed. The OOV for long words
was 4% higher than that for short words. In gen-
eral, the higher the OOV is, the more difficult de-
tecting word segments and their POS categories
is. However, the difference between accuracies
for short and long words was about 1% in recall
and 2% in precision, which is not significant when
we consider that the difference between OOVs for
short and long words was 4%. This result indi-
cates that our morpheme models could detect both
known and unknown words accurately, especially
long words. Therefore, we investigated the recall
of unknown words in the test corpus, and found
that 55.7% (928/1,667) of short word segments and
74.1% (2,660/3,590) of long word segments were
detected correctly. In addition, regarding unknown
words, we also found that 47.5% (791/1,667) of
short word segments plus their POS categories and
67.3% (2,415/3,590) of long word segments plus
their POS categories were detected correctly. The
recall of unknown words was about 20% higher for
long words than for short words. We believe that
this result mainly depended on the difference be-
tween short words and long words in terms of the
definitions of compound words. A compound word
is defined as one word when it is based on the def-
inition of long words; however it is defined as two
or more words when it is based on the definition of
short words. Furthermore, based on the definition of
short words, a division of compound words depends
on its context. More information is needed to pre-
cisely detect short words than is required for long
words. Next, we extracted words that were detected
by the morpheme model but were not found in a dic-
tionary, and investigated the percentage of unknown
words that were completely or partially matched to
the extracted words by their context. This percent-
age was 77.6% (1,293/1,667) for short words, and
80.6% (2,892/3,590) for long words. Most of the re-
maining unknown words that could not be detected
by this method are compound words. We expect that
these compounds can be detected during the manual
examination of those words for which the morpheme
model estimated a low probability, as will be shown
later.
The recall of unknown words was lower than that
of known words, and the accuracy of automatic mor-
phological analysis was lower than that of manual
morphological analysis. As previously stated, to
improve the accuracy of the whole corpus we take
a semi-automatic approach. We assume that the
smaller the probability is for an output morpheme
estimated by a model, the more likely the output
morpheme is wrong, and we examine output mor-
phemes in ascending order of their probabilities. We
investigated how much the accuracy of the whole
corpus would increase. Fig. 5 shows the relation-
ship between the percentage of output morphemes
whose probabilities exceed a threshold and their
93
94
95
96
97
98
99
100
20 30 40 50 60 70 80 90 100
Pr
ec
is
io
n 
(%
)
Output Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 5: Partial analysis.
precision. In this figure, ?short without UKW?,
?long without UKW??, ?short with UKW?, and
?long with UKW? represent the precision for short
words detected assuming there were no unknown
words, precision for long words detected assuming
there were no unknown words, precision of short
words including unknown words, and precision of
long words including unknown words, respectively.
When the output rate in the horizontal axis in-
creases, the number of low-probability morphemes
increases. In all graphs, precisions monotonously
decrease as output rates increase. This means that
tagging errors can be revised effectively when mor-
phemes are examined in ascending order of their
probabilities.
Next, we investigated the relationship between the
percentage of morphemes examined manually and
the precision obtained after detected errors were re-
vised. The result is shown in Fig. 6. Precision
represents the precision of word segmentation and
POS tagging. If unknown words were detected and
put into a dictionary by the method described in the
fourth paragraph of this section, the graph line for
short words would be drawn between the graph lines
?short without UKW? and ?short with UKW?, and
the graph line for long words would be drawn be-
tween the graph lines ?long without UKW? and
?long with UKW?. Based on test results, we can
expect better than 99% precision for short words
and better than 97% precision for long words in the
whole corpus when we examine 10% of output mor-
93
94
95
96
97
98
99
100
0 20 40 60 80 100 120
Pr
ec
is
io
n 
(%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 6: Relationship between the percentage of
morphemes examined manually and precision ob-
tained after revising detected errors (when mor-
phemes with probabilities under threshold and their
adjacent morphemes are examined).
0
10
20
30
40
50
60
0 5 10 15 20 25 30 35 40 45 50
Er
ro
r R
at
es
 in
 E
xa
m
in
ed
 M
or
ph
em
es
 (%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"short_with_UKW"
"long_without_UKW"
"long_with_UKW"
Figure 7: Relationship between percentage of mor-
phemes examined manually and error rate of exam-
ined morphemes.
phemes in ascending order of their probabilities.
Finally, we investigated the relationship between
percentage of morphemes examined manually and
the error rate for all of the examined morphemes.
The result is shown in Fig. 7. We found that about
50% of examined morphemes would be found as er-
rors at the beginning of the examination and about
20% of examined morphemes would be found as
errors when examination of 10% of the whole cor-
pus was completed. When unknown words were de-
tected and put into a dictionary, the error rate de-
creased; even so, over 10% of examined morphemes
would be found as errors.
4.2.2 Experiments Using Chunking Models
Results of the morphological analysis of long
words obtained by using a chunking model are
shown in Table 5 and 6. The first and second lines
Table 5: Accuracies of long word segmentation.
Model Recall Precision F
Morph 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21
Chunk 97.65% (50,580
51,796
) 97.41% (50,580
51,911
) 97.54
Chunk 98.84% (51,193
51,796
) 98.66% (51,193
51,888
) 98.75
Table 6: Accuracies of long word segmentation and
POS tagging.
Model Recall Precision F
Morph 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21
Chunk 95.59% (49,513
51,796
) 95.38% (49,513
51,911
) 95.49
Chunk 98.56% (51,051
51,796
) 98.39% (51,051
51,888
) 98.47
Chunk w/o TR 92.61% (47,968
51,796
) 92.40% (47,968
51,911
) 92.51
TR : transformation rules
show the respective accuracies obtained when OOVs
were 5.81% and 6.93%. The third lines show the ac-
curacies obtained when we assumed that the OOV
for short words was 0% and there were no errors in
detecting short word segments and their POS cate-
gories. The fourth line in Table 6 shows the accuracy
obtained when a chunking model without transfor-
mation rules was used.
The accuracy obtained by using the chunking
model was one point higher in F-measure than that
obtained by using the morpheme model, and it was
very close to the accuracy achieved for short words.
This result indicates that errors newly produced by
applying a chunking model to the results obtained
for short words were slight, or errors in the results
obtained for short words were amended by apply-
ing the chunking model. This result also shows that
we can achieve good accuracy for long words by ap-
plying a chunking model even if we do not detect
unknown long words and do not put them into a dic-
tionary. If we could improve the accuracy for short
words, the accuracy for long words would be im-
proved also. The third lines in Tables 5 and 6 show
that the accuracy would improve to over 98 points
in F-measure. The fourth line in Tables 6 shows that
transformation rules significantly contributed to im-
proving the accuracy.
Considering the results obtained in this section
and in Section 4.2.1, we are now detecting short and
long word segments and their POS categories in the
whole corpus by using the following steps:
1. Automatically detect and manually examine
unknown words for short words.
2. Improve the accuracy for short words in the
whole corpus by manually examining short
words in ascending order of their probabilities
estimated by a morpheme model.
3. Apply a chunking model to the short words to
detect long word segments and their POS cate-
gories.
As future work, we are planning to use an active
learning method such as that proposed by Argamon-
Engelson and Dagan (Argamon-Engelson and Da-
gan, 1999) to more effectively improve the accuracy
of the whole corpus.
5 Conclusion
This paper described two methods for detecting
word segments and their POS categories in a
Japanese spontaneous speech corpus, and describes
how to tag a large spontaneous speech corpus accu-
rately by using the two methods. The first method is
used to detect any type of word segments. We found
that about 80% of unknown words could be semi-
automatically detected by using this method. The
second method is used when there are several defi-
nitions for word segments and their POS categories,
and when one type of word segments includes an-
other type of word segments. We found that better
accuracy could be achieved by using both methods
than by using only the first method alone.
Two types of word segments, short words and
long words, are found in a large spontaneous speech
corpus, CSJ. We found that the accuracy of auto-
matic morphological analysis for the short words
was 95.79 in F-measure and for long words, 95.49.
Although the OOV for long words was much higher
than that for short words, almost the same accuracy
was achieved for both types of words by using our
proposed methods. We also found that we can ex-
pect more than 99% of precision for short words,
and 97% for long words found in the whole corpus
when we examined 10% of output morphemes in as-
cending order of their probabilities as estimated by
the proposed models.
In our experiments, only the information con-
tained in the corpus was used; however, more appro-
priate linguistic knowledge than that could be used,
such as morphemic and syntactic rules. We would
like to investigate whether such linguistic knowl-
edge contributes to improved accuracy.
References
S. Argamon-Engelson and I. Dagan. 1999. Committee-Based
Sample Selection For Probabilistic Classifiers. Artificial In-
telligence Research, 11:335?360.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
Maximum Entropy Approach to Natural Language Process-
ing. Computational Linguistics, 22(1):39?71.
E. T. Jaynes. 1957. Information Theory and Statistical Me-
chanics. Physical Review, 106:620?630.
E. T. Jaynes. 1979. Where do we Stand on Maximum Entropy?
In R. D. Levine and M. Tribus, editors, The Maximum En-
tropy Formalism, page 15. M. I. T. Press.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis Without a Dictionary for
Japanese. In Proceedings of NLPRS, pages 541?544.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Sponta-
neous Speech Corpus of Japanese. In Proceedings of LREC,
pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distributional
Analysis. In Proceedings of COLING, pages 1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method for
Japanese Unknown Words Using a Statistical Model of Mor-
phology and Context. In Proceedings of ACL, pages 277?
284.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Using
Maximum Entropy Aided by a Dictionary. In Proceedings
of EMNLP, pages 91?99.
K. Uchimoto, C. Nobata, A. Yamada, S. Sekine, and H. Isahara.
2002. Morphological Analysis of The Spontaneous Speech
Corpus. In Proceedings of COLING, pages 1298?1302.
