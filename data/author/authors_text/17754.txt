Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 28?36,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Statistical Representation of Grammaticality Judgements: the Limits of
N-Gram Models
Alexander Clark, Gianluca Giorgolo, and Shalom Lappin
Department of Philosophy, King?s College London
firstname.lastname@kcl.ac.uk
Abstract
We use a set of enriched n-gram models to track
grammaticality judgements for different sorts of
passive sentences in English. We construct these
models by specifying scoring functions to map the
log probabilities (logprobs) of an n-gram model for
a test set of sentences onto scores which depend
on properties of the string related to the parame-
ters of the model. We test our models on classifica-
tion tasks for different kinds of passive sentences.
Our experiments indicate that our n-gram models
achieve high accuracy in identifying ill-formed pas-
sives in which ill-formedness depends on local rela-
tions within the n-gram frame, but they are far less
successful in detecting non-local relations that pro-
duce unacceptability in other types of passive con-
struction. We take these results to indicate some of
the strengths and the limitations of word and lexical
class n-gram models as candidate representations of
speakers? grammatical knowledge.
1 Introduction
Most advocates (Pereira, 2000; Bod et al, 2003)
and critics (Chomsky, 1957; Fong et al, 2013) of a
probabilistic view of grammatical knowledge have
assumed that this view identifies the grammatical
status of a sentence directly with the probability of
its occurrence. By contrast, we seek to character-
ize grammatical knowledge statistically, but with-
out reducing grammaticality directly to probabil-
ity. Instead we specify a set of scoring procedures
for mapping the logprob value of a sentence into
a relative grammaticality score, on the basis of the
properties of the sentence and of the logprobs that
an n-gram word model generates for the corpus
containing the sentence. A scoring procedure in
this set generates scores in terms of which we con-
struct a grammaticality classifier, using a param-
eterized standard deviation from the mean value.
The classifier provides a procedure for testing the
accuracy of different scoring criteria in separat-
ing grammatical from ungrammatical passive sen-
tences.
We evaluate this approach by applying it to
the task of distinguishing well and ill-formed sen-
tences with passive constructions headed by four
different sorts of verbs: intransitives (appear,
last), pseudo-transitives, which take a restricted
set of notional objects (laugh a hearty laugh,
weigh 10 kg), ambiguous transitives, which allow
both agentive and thematic subjects (the jeans /
the tailor fitted John), and robust transitives that
passivize freely (write, move). Intransitives and
pseudo-transitives generally yield ill-formed pas-
sives. Passives formed from ambiguous transitives
tend to be well-formed only on the agentive read-
ing. Robust transitives, for the most part, yield
acceptable passives, even if they are semantically
(or pragmatically) odd.
Experimenting with several scoring procedures
and alternative values for our standard deviation
parameter, we found that our classifier can distin-
guish pairwise between elements of the first two
classes of passives and those of the latter two with
a high degree of accuracy. However, its perfor-
mance is far less reliable in identifying the differ-
ence between ambiguous and robust transitive pas-
sives. The first classification task relies on local
lexical patterns that can be picked up by n-gram
models, while the second requires identification of
anomalous relations between passivized verbs and
by-phrases, which are not generally accessible to
measurement within the range of an n-gram.
We also observed that as we increased the size
of the training corpus, the performance of our en-
riched models on the classification task also in-
creased. This result suggests that better n-gram
language models are more sensitive to the sorts of
patterns that our scoring procedures rely on to gen-
erate accurate grammaticality classifications.
We note the important difference between
28
grammaticality and acceptability. Following stan-
dard assumptions, we take grammaticality to be
a theoretical notion, and acceptability to be an
empirically testable property. Acceptability is, in
part, determined by grammaticality, but also by
factors such as sentence length, processing limi-
tations, semantic acceptability and many other el-
ements. Teasing apart these two concepts, and ex-
plicating their precise relationship raises a host of
subtle methodological issues that we will not ad-
dress here. Oversimplifying somewhat, we are try-
ing to reconstruct a gradient notion of grammati-
cality which is derived from probabilistic models,
that can serve as a core component of a full model
of acceptability.
We distinguish our task from the standard task
of error detection in NLP (e.g. Post (2011)),
that can be used in various language processing
systems, such as machine translation (Pauls and
Klein, 2012), language modeling and so on. In
error detection, the problem is a supervised learn-
ing task. Given a corpus of examples labeled as
grammatical or ungrammatical, the problem is to
learn a classifier to distinguish them. We use su-
pervised learning as well, but only to measure the
upper bound of an unsupervised learning method.
We assume that native speakers do not, in general,
have access to systematic sets of ungrammatical
sentences that they can use to calibrate their judge-
ment of acceptability. Rather ungrammatical sen-
tences are unusual or unlikely. However, we use
some ungrammatical sentences to set an optimal
threshold for our scoring procedures.
2 Enriched N-Gram Language Models
We assume that we have some high quality lan-
guage model which defines a probability distri-
bution over whole sentences. As has often been
noted, it is not possible to reduce grammatical-
ity directly to a probability of this type, for sev-
eral reasons. First, if one merely specifies a fixed
probability value as a threshold for grammatical-
ity, where strings are deemed to be grammatical
if and only if their probability is higher than the
threshold, then one is committed to the existence
of only a finite number of grammatical sentences.
The probabilities of the possible strings of words
in a language sum to 1, and so at most 1/ sen-
tences can have a probability of at least . Second,
probability can be affected by factors that do not
influence grammaticality. For example, the word
?yak? is rarer (and therefore less probable) than the
word ?horse?, but this does not affect the relative
grammaticality of ?I saw a horse? versus ?I saw a
yak?. Third, a short ungrammatical sentence may
have a higher probability than a long grammatical
sentence with many rare words.
In spite of these arguments against a naive re-
duction of grammaticality, probabilistic inference
does play a role in linguistic judgements, as in-
dicated by the fact that they are often gradient.
Probabilistic inference is pervasive throughout all
domains of cognition (Chater et al, 2006), and
therefore it is plausible to assume that knowledge
of language is also probabilistic in nature. More-
over language models do seem to play a crucial
role in speech recognition and sentence process-
ing. Without them we would not be able to under-
stand speech in a noisy environment.
We propose to accommodate these different
considerations by using a scoring function to map
probabilities to grammaticality rankings. This
function does not apply directly to probabilities,
but rather to the parameters of the language model.
The probability of a particular sentence with re-
spect to a log-linear language model will be the
product of certain parameters: in log space, the
sum. We define scores that operate on this collec-
tion of parameters.
2.1 Scores
We have experimented with scores of two differ-
ent types that correlate with the grammaticality
of a sentence. Those of the first type are dif-
ferent implementations of the idea of normaliz-
ing the logprob assigned by an n-gram model to
a string by eliminating the significance of factors
that do not influence the grammatical status of a
sentence, such as sentence length and word fre-
quency. Scores of the second type are based on the
intuition that the (un)grammaticality of a sentence
is largely determined by its problematic compo-
nents. These scores are functions of the lowest
scoring n-grams in the sentence.
Mean logprob (ML) This score is the logprob
of the entire sentence divided by the length of the
sentence, or equivalently the mean of the logprobs
for the single trigrams:
ML = 1n logPTRIGRAM(?w1, . . . , wn?)
By normalizing the logprob for the entire sentence
by its length we eliminate the effect of sentence
length on the acceptability score.
29
Weighted mean logprob (WML) This score is
calculated by dividing the logprob of the entire
sentence by the sum of the unigram probabilities
of the lexical items that compose the sentence:
WML = logPTRIGRAM(?w1,...,wn?)logPUNIGRAM(?w1,...,wn?)
This score eliminates at the same time the effect of
the length of the sentence and the lower probabil-
ity assigned to sentences with rare lexical items.
Synctactic log odds ratio (SLOR) This score
was first used by Pauls and Klein (2012) and
performs a normalization very similar to WML
(we will see below that in fact the two scores are
basically equivalent):
SLOR =
logPTRIGRAM(?w1,...,wn?)?logPUNIGRAM(?w1,...,wn?)
n
Minimum (Min) This score is equal to the low-
est logprob assigned by the model to the n-grams
of the sentence divided by the unigram logprob of
the lexical item heading the n-gram:
Min = mini
[
logP (wi|wi?2wi?1)
logP (wi)
]
In this way, if a single n-gram is assigned a low
probability (normalized for the frequency of its
head lexical item), then this low score is in some
sense propagated to the whole sentence.
Mean of the first quartile (MFQ) This score
is a generalization of the Min score. We order
the single n-gram logprobs from the lowest to the
highest, and we consider the first (lowest) quar-
tile. We then normalize the logprobs for these n-
grams by the unigram probability of the head lex-
ical item, and we take the mean of these scores.
In this way we obtain a score that is more robust
than the simple Min, as, in general, a grammatical
anomaly influences the logprob of more than one
n-gram.
2.2 N-Gram Models
We are using n-gram models on the understand-
ing that they are fundamentally inadequate for de-
scribing natural languages in their full syntactic
complexity. In spite of their limitations, they are a
good starting point, as they perform well as lan-
guage models across a wide range of language
modeling tasks. They are easy to train, as they
do not require annotated training data.
We do not expect that our n-gram based gram-
maticality scores will be able to idenitfy all of the
cases of ungrammaticality that we encounter. Our
working hyposthesis is that they can capture cases
of ill-formedness that depend on local factors, that
can be identified within n-gram frames, as op-
posed to those which involve non-local relations.
If these models can detect local grammaticality vi-
olations, then we will have a basis for thinking
that richer, more structured language models can
recognize non-local as well as local sources of un-
grammaticality.
3 Experiments with Passives
Rather than trying to test the performance of these
models over all types of ungrammaticality, we
limit ourselves to a case study of the passive. By
tightly controlling the verb types and grammat-
ical construction to which we apply our models
we are better able to study the power and the lim-
its of these models as candidate representations of
grammatical knowledge.
3.1 Types of Passives
Our controlled experiments on passives are, in
part, inspired by speakers? judgments discussed in
Ambridge et al (2008). Their experimental work
measures the acceptability of various passive sen-
tences.
The active-passive alternation in English is ex-
emplified by the pair of sentences
? John broke the window.
? The window was broken by John.
The acceptability of the passive sentence de-
pends largely on lexical properties of the verb.
Some verbs do not allow the formation of the pas-
sive, as in the case of pure intransitive verbs like
appear, discussed below, which permit neither the
active transitive, nor the passive.
We conducted some prelimiary experiments,
not reported here, on modelling the data on pas-
sives from recent work in progress that Ben Am-
bridge and his colleagues are doing, and which
he was kind enough to make available to us. We
observed that the scores we obtained for our lan-
guage models did not fully track these judgements,
but we did notice that we obtained much better
correlation at the low end of the judgment distri-
bution. In Ambridge?s current data this judgement
range corresponds to passives constructed with in-
transitive verbs.
The Ambridge data indicates that the capacity
of verbs to yield well-formed passive verb phrases
30
forms a continuum. Studying the judgement pat-
terns in this data we identified four reasonably
salient points along this hierarchial continuum.
First, at the low end, we have intransitives
like appear: (*John appeared the book. *The
book was appeared). Next we have what may be
described as pseudo-transitives verbs like laugh,
which permit only notional NP objects and do not
easily passivize (Mary laughed a hearty laugh/*a
joke. ?A hearty laugh/*A joke was laughed by
Mary) above them. These are followed by cases
of ambiguous transitives like fit, which, in active
form, carry two distinct readings that correspond
to an agentive and a thematic subject, respectively.
? The tailor fitted John for a new suit.
? The jeans fitted John
Only the agentive reading can be passivized.
? John was fitted by the tailor.
? *John was fitted by the jeans.
Finally, the most easily passivized verbs are ro-
bust transitives, which take the widest selection of
NP subjects in passive form (John wrote the book.
The book was written by John).
This continuum causes well-formedness in pas-
sivization to be a gradient property, as the Am-
bridge data illustrates. Passives tend to be more
or less acceptable along this spectrum. The gradi-
ence of acceptability for passives implies the par-
tial overlap of the score distributions for the differ-
ent types of passives that our experiments show.
The experiments were designed to test our hy-
pothesis that n-gram based language models are
capable of detecting ungrammatical patterns only
in cases where they do not depend on relations
between words that cross the n-word boundary
applied in training. Therefore we expect such a
model to be capable of detecting the ungrammati-
cality of a sentence like A horrible death was died
by John, because the trigrams death was died, was
died by and died by John are unlikely to appear
in any corpus of English. On the other hand, we
do not expect a trigram model to store the infor-
mation necessary to identify the relative anomaly
of a sentence like Two hundred people were held
by the theater, because all the trigrams (as well as
the bigrams and the unigrams) that constitute the
sentence are likely to appear with reasonable fre-
quency in a large corpus of English.
The experiments generalize this observation
and test the performance of n-gram models on a
wider range of verb types. To quantify the per-
formance of the different models we derive simple
classifiers using the scores we have defined and
testing them in a binary classification task. This
task measures the ability of the classifier to dis-
tinguish between grammatical sentences, and sen-
tences containing different types of grammatical
errors.
The models are trained in an unsupervised man-
ner using only corpus data, which we assume to be
uniformly grammatical. In order to evaluate the
scoring methods, we use some supervised data to
set the optimal value of a simple threshold. This is
not however a supervised classification task: we
want to see how well the scores could be used
to separate grammatical and ungrammatical data,
and though unorthodox, this seems a more direct
way of measuring this conditional property than
stipulating some fixed threshold.
3.2 Training data
We used the British National Corpus (BNC) (BNC
Consortium, 2007) to obtain our training data. We
trained six different language models, using six
different subcorpora of the BNC. The first model
used the entire collection of written texts anno-
tated in the BNC, for a total of approximately 100
million words. The other models were trained on
increasingly smaller portions of the written texts
collection: 40 million words, 30 million words, 15
million words, 7.6 million words, and 3.8 million
words. We constructed these corpora by randomly
sampling an appropriate number of complete sen-
tences.
All models were trained on word sequences.
For smoothing the n-gram probability distribu-
tions we used Kneser-Ney interpolation, as de-
scribed in Goodman (2001).
3.3 Test data
We constructed the test data for our hypothesis in
a controlled fashion. We first compiled a list of
verbs for each of the four verb types that we con-
sider (intransitives, pseudo-transitives, ambiguous
transitives, and robust transitives). We selected
verbs from the BNC that appeared at least 100
times in their past participle form in the entire cor-
pus in order to ensure a sufficient number of pas-
31
sive uses in the training data.1 We selected 40 in-
transitive verbs, 13 pseudo transitives, 23 ambigu-
ous transitives and 40 transitive verbs. To clas-
sify the verbs we relied on our intuitions as native
speakers of English.
Using these lists we automatically generated
four corpora by selecting an agent and a patient
from a predefined pool of NPs, randomly select-
ing a determiner (if necessary) and a number (if
the NP allows plurals). The resulting corpora are
of the following sizes:
? intransitive verbs ? 24480 words, 3240 sen-
tences,
? pseudo transitive verbs ? 7956 words, 1053
sentences,
? ambiguous transitive verbs ? 14076 words,
1863 sentences,
? robust transitive verbs ? 24480 words, 3240
sentences.
Each corpus was evaluated by the six models.
We computed our derived scores for each sentence
on the basis of the logprobs that the language mod-
els assigns.
3.4 Binary classifiers
For each model and for each score we constructed
a set of simple binary classifiers on the basis of
the results obtained for the transitive verb corpus.
We took the mean of each score assigned by the
model to the transitive sentences, and we set dif-
ferent thresholds by subtracting from this value
a number of standard deviations ranging from 0
to 2.75. The rationale behind these classifiers is
that, assuming the passives of the robust transi-
tives to be grammatical, the scores for the other
cases should be comparatively lower. Therefore
by setting a threshold ?to the left? of the mean we
should be able to distinguish between grammati-
cal sentences, whose score is to the right of the
threshold, and ungrammatical ones, expected to a
have a score lower than the threshold. Formally
the classifier is defined as follows:
cs(w) =
{
+ if s(w) ? m? S ? ?
? otherwise
(1)
1Notice that in most cases the past participle form is the
same as the simple past form, and for this reason we set the
threshold to such a high value.
where s is one of our scores, w is the sentence to
be classified, s(w) represents the value assigned
by the score to sentence w, m is the mean for
the score in the transitive condition, ? is the stan-
dard deviation for the score again in the transitive
condition, and S is a factor by which we move
the threshold away from the mean. The classi-
fier assigns the grammatical (+) tag only to those
sentences that are assigned values higher than the
threshold m? S ? ?.
Alternatively in terms of the widely used z-
score, defined as zs(w) = (s(w) ?m)/? we can
say that w is classified as grammatical iff zs(w) ?
?S.
4 Results
For reasons of space we will limit the presenta-
tion of our detailed results to the 100 million word
model, as it offers the sharpest effects. We will,
however, also report comparisons on the most im-
portant metrics for the complete set of models.
In Figure 1 we show the distribution of the five
scores for the four different corpora (transitive,
ambiguous, pseudo, and intransitive) obtained us-
ing the 100 million word model. In all cases we
observe the same general pattern: the sentences in
the corpus generated with robust transitives are as-
signed comparatively high scores, and these grad-
ually decrease when we consider the ambiguous,
the pseudo and the intransitive conditions. Inter-
estingly, this order reflects the degree of ?transi-
tivity? that these verb types exhibit. Notice, how-
ever, that the four conditions seem to group into
two different macro-distributions. On the right
we have the transitive-ambiguous sentences and
on the left the pseudo-intransitive cases. This par-
tially confirms our hypothesis that n-gram mod-
els have problems recognizing lexical dependen-
cies that determine the felicitousness of passives
constructed using ambiguous transitive verbs, as
these are, for the most part, non-local. Neverthe-
less, it is important to note that the overlap of the
distributions for these two cases is also due to the
fact that many cases in the ambiguous transitive
corpus are indeed grammatical.
Figure 2 summarizes the (balanced) accuracies
obtained by our classifiers for each comparison,
by each model. These results confirm our hy-
pothesis that the classifiers tend to perform better
when distinguishing passive sentences constructed
with a robust transitive verbs from those headed by
32
Logprob ML WML
SLOR Min MFQ
0.00
0.05
0.10
0.15
0.0
0.5
1.0
1.5
2.0
0
2
4
6
0.0
0.5
1.0
1.5
2.0
0
2
4
6
0
1
2
3
4
?30 ?25 ?20 ?15 ?2.5 ?2.0 ?1.5 ?1.0 ?0.9 ?0.8 ?0.7 ?0.6 ?0.5
0.0 0.5 1.0 ?1.75 ?1.50 ?1.25 ?1.00 ?0.75 ?1.75 ?1.50 ?1.25 ?1.00 ?0.75
den
sity
condition
transitive
ambiguous
pseudo
intransitive
Figure 1: Distributions of the six scores Logprob, ML, WML, SLOR, Min and MFQ for the four differ-
ent conditions (robust transitive passives, ambiguous transitive passives, pseudo transitive passives and
intransitive passives) for the 100 million words language model.
pseudo-transitives and intransitives.
In the comparison between transitive and am-
biguous transitive sentences, the classifiers are
?stuck? at around 60% accuracy. Using larger
training corpora produces only a marginal im-
provement. This contrasts with what we observe
for the transitive/pseudo and transitive/intransitive
classification tasks. In the transitive/pseudo task,
we already obtain reasonable accuracy with the
model trained with the smallest BNC subset.
Oddly, the overall best result is achieved with 30
million words, although the result obtained with
the model trained on the full BNC corpus is not
much lower. For the transitive/intransitive classifi-
cation task we observe a much steadier and larger
growth in accuracy, reaching the overall best result
of 85.1%. Table 1 reports the best results for each
comparison by each language model. For each
condition we report the best accuracy obtained, the
corresponding F1 score, the score that achieves the
best result, and the best accuracy obtained by just
using the logprobs. These results are obtained us-
ing different values for the S parameter. However,
in general the best results are obtained when the S
parameter is set to a value in the interval [0.5, 1.5].
In comparing the performance of the individ-
ual scores, we first notice that, while for the tran-
sitive/ambiguous comparison all scores perform
pretty much at the same level, there is a clear hier-
archy between scores for the other comparisons.
We observe that the baseline raw logprob as-
signed by the n-grams models performs much
worse than the scores, resulting in roughly 10%
less accuracy than the best performing score in ev-
ery condition. ML performs slightly better, obtain-
ing around 5% greater accuracy than logprob as a
predictor. This shows that even though the length
of the sentences in our test data is relatively con-
stant (between 9 and 11 words), there is still an
improvement if we take this structural factor into
account. The two scores WML and SLOR display
the same pattern, showing that they are effectively
equivalent. This is not surprising given that they
are designed to modify the raw logprob by tak-
33
transitive
ambiguous transitivepseudo transitiveintransitive
l l l l l l l l l l l l
l l l l l l
l l l l l l l l ll l l l
l l l
l l l
l l
l l l
l l l
l l
l l l l l l l l l l l l
l l l l l l l l l
l l l l l l l
l l l
l
l l l l l l l l
l
l
l l l
l l l l l
l
l
l l l l l l l l l l l l
l l l l l l l l
l l l l l l l
l l
l l
l l l l l l
l l l l
l l l
l l l
l l l l l l l l l l l l
l l l l l l l l l
l l
l l l l l l l l l
l l
l l
l l l l
l l l l
l l l
l l l l l
l
l l l
l l l l l l l l l l l l
l l l l l l l l l l
l l l
l l l
l l l l l l
l l l
l l l
l l l l l
l l l l
l
l
l l l
l l
l
l
l l l l l l l l l l l l
l l l l l l l l l l
l l
l l l
l l l l l l l
l l l l l l l l l l
l
l l
l l l
l l l l l
l l l
l l l l
l l l l l
l
l
l
l l
l
l l l l l
l
l
l
l l
l
l
l
l l l
l l l l l
l l l l
l
l
l
l
l
l
l
l l l
l l
l l l l l
l l
l
l
l l l
l l
l l l
l
l
l
l
l
l l l l l
l
l
l
l
l
l
l l
l l l l l l
l l
l
l
l
l
l
l l
l
l
l
l
l l
l l l l l l
l l l
l l l
l l l l l l
l
l
l
l
l l l l l
l
l
l
l
l
l
l
l
l l l l l l
l l l
l
l
l l
l l l
l
l
l
l
l l
l l
l l l
l l
l
l l
l l
l
l l l l l
l
l
l
l
l l
l
l
l l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l l
l l
l
l
l
l
l
l
l
l l l l
l
l
l l l l l l
l
l l
l l l
l l
l l l l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l l
l l l
l
l
l
l
l
l l
l
l
l
l l
l l l
l
l
l
l l
l l
l
l l
l l l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l l
l l
l l l
l
l
l
l l
l
l l l
l
l
l
l l
l l l l
l
l
l
l l l l l l
l l
l l l l
l l l l l
l
l
l
l l l
l
l l l l
l
l
l
l
l l
l
l
l
l
l l
l l l l l
l
l
l
l
l
l
l l l l
l
l
l l l
l l l l l l l
l l l l l
l l
l l l
l
l
l
l
l
l
l l l l l
l
l
l
l l
l
l l
l l l l l l
l l
l
l
l
l
l
l l l
l
l
l l l
l l l l l l
l
l
l
l l l
l l
l l l
l
l
l
l
l
l
l
l l
l l l
l
l
l
l
l
l
l l
l
l
l
l l l l l
l
l
l
l
l
l
l
l l
l l
l
l
l
l l
l l
l l l l l
l
l
l
l l
l
l l l l l
l
l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l
l
l l
l
l
l
l
l
l
l l l l
l
l
l
l l
l l
l l l l
l
l
l
l l l
l
l l
l l l
l
l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l l
l l l
l
l
l
l
l
l l
l
l
l l l l
l
l
l
l
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l l l l l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
3.8M
7.6M
15M
30M
40M
100M
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75S
Bala
nced
 acc
urac
y Scorel
l
l
l
l
l
LogprobMLWMLSLORMinMFQ
Figure 2: Accuracies for the classifiers for each model. S represents the number of standard deviations
?to the left? of the mean of the transitive condition score, used to set the threshold.
ing into account exactly the same factors (length
of the sentence and frequency of the unigrams that
compose the sentence). These two scores perform
generally better in the transitive/ambiguous com-
parison, and they achieve good performance when
the size of the training model is small. However,
for the most part, the two scores derived from the
logprobs of the least probable n-grams in the sen-
tence, Min and MFQ, get the best results. Min
exhibits erratic behavior (mainly due to its non-
normal distribution for each condition, as shown
in figure 1), and it seems to be more stable only
in the presence of a large training set. MFQ has
a much more robust contour, as it is significantly
less dependent on the choice of S.
5 Conclusions and Future Work
In Clark and Lappin (2011) we propose a model
of negative evidence that uses probability of oc-
currence in primary linguistic data as the basis for
estimating non-grammaticality through relatively
34
Model Comparison Best accuracy F1 Best performing score Logprob accuracy
transitive/ambiguous 60.9% 0.7 SLOR 57.3%
3.8M transitive/pseudo 77% 0.81 MFQ 67.6%
transitive/intransitive 73.8% 0.72 SLOR 65.6%
transitive/ambiguous 62.9% 0.68 MFQ 57.8%
7.6M transitive/pseudo 78.5% 0.76 MFQ 69.1%
transitive/intransitive 75.8% 0.72 MFQ 67.3%
transitive/ambiguous 62.3% 0.66 WML 57.8%
15M transitive/pseudo 72.6% 0.78 SLOR 66.5%
transitive/intransitive 79.5% 78.3 MFQ 69.5%
transitive/ambiguous 63.3% 0.75 WML 58.9%
30M transitive/pseudo 83.1% 0.88 Min 71.2%
transitive/intransitive 81.8% 0.82 MFQ 72.2%
transitive/ambiguous 63.8% 0.75 SLOR 59.5%
40M transitive/pseudo 80.1% 0.86 Min 69.7%
transitive/intransitive 83.5% 0.83 SLOR 72.6%
transitive/ambiguous 63.3% 0.75 SLOR 58.4%
100M transitive/pseudo 80.3% 0.9 MFQ 71.3%
transitive/intransitive 85.1% 0.85 SLOR 73.8%
Table 1: Best accuracies
low frequency in a sample of this data. Here we
follow Clark et al (2013) in effectively inverting
this strategy.
We identify a set of scoring functions based on
parameters of probabilistic models that we use to
define a grammaticality threshold, which we use
to classify strings as grammatical or ill-formed.
This model offers a stochastic characterisation of
grammaticality without reducing grammaticality
to probability.
We expect enriched lexical n-gram models of
the kind that we use here to be capable of rec-
ognizing the distinction between grammatical and
ungrammatical sentences when it depends on local
factors within the frame of the n-grams on which
they are trained. We further expect them not to be
able to identify this distinction when it depends on
non-local relations that fall outside of the n-gram
frame.
It might be thought that this hypothesis con-
cerning the capacities and limitations of n-gram
models is too obvious to require experimental sup-
port. In fact, this is not the case. Reali and Chris-
tiansen (2005) show that n-gram models can be
used to distinguish grammatical from ungrammat-
ical auxiliary fronted polar questions with a high
degree of success. More recently Frank et al
(2012) argue for the view that a purely sequen-
tial, non-hierarchical view of linguistic structure is
adequate to account for most aspects of linguistic
knowledge and processing.
We have constructed an experiment with differ-
ent (pre-identified) passive structures that provides
significant support for our hypothesis that lexical
n-gram models are very good at capturing local
syntactic relations, but cannot handle more distant
dependencies.
In future work we will be experimenting with
more expressive language models that can repre-
sent non-local syntactic relations. We will pro-
ceed conservatively by first extending our enriched
lexical n-gram models to chunking models, and
then to dependency grammar models, using only
as much syntactic structure as is required to iden-
tify the judgement patterns that we are studying.
To the extent that this research is successful it
will provide motivation for the view that syntactic
knowledge is inherently probabilistic in nature.
Acknowledgments
The research described in this paper was done in the
framework of the Statistical Models of Grammaticality
(SMOG) project at King?s College London, funded by grant
ES/J022969/1 from the Economic and Social Research Coun-
cil of the UK. We are grateful to Ben Ambridge for providing
us with the data from his experiments and for helpful dis-
cussion of the issues that we address in this paper. We also
thank the three anonymous CMCL 2013 reviewers for useful
comments and suggestions, that we have taken account of in
preparing the final version of the paper.
35
References
Ben Ambridge, Julian M Pine, Caroline F Rowland, and
Chris R Young. 2008. The effect of verb semantic
class and verb frequency (entrenchment) on childrens and
adults graded judgements of argument-structure overgen-
eralization errors. Cognition, 106(1):87?129.
BNC Consortium. 2007. The British National Corpus, ver-
sion 3 (BNC XML Edition). Distributed by Oxford Uni-
versity Computing Services on behalf of the BNC Consor-
tium.
R. Bod, J. Hay, and S. Jannedy. 2003. Probabilistic linguis-
tics. MIT Press.
N. Chater, J.B. Tenenbaum, and A. Yuille. 2006. Probabilis-
tic models of cognition: Conceptual foundations. Trends
in Cognitive Sciences, 10(7):287?291.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
A. Clark and S. Lappin. 2011. Linguistic Nativism and the
Poverty of the Stimulus. Wiley-Blackwell, Malden, MA.
A. Clark, G. Giorgolo, and S. Lappin. 2013. Towards a sta-
tistical model of grammaticality. In Proceedings of the
35th Annual Conference of the Cognitive Science Society.
Sandiway Fong, Igor Malioutov, Beracah Yankama, and
Robert C. Berwick. 2013. Treebank parsing and
knowledge of language. In Aline Villavicencio, Thierry
Poibeau, Anna Korhonen, and Afra Alishahi, editors, Cog-
nitive Aspects of Computational Language Acquisition,
Theory and Applications of Natural Language Processing,
pages 133?172. Springer Berlin Heidelberg.
Stefan Frank, Rens Bod, and Morten Christiansen. 2012.
How hierarchical is language use? In Proceedings of the
Royal Society B, number doi: 10.1098/rspb.2012.1741.
J.T. Goodman. 2001. A bit of progress in language model-
ing. Computer Speech & Language, 15(4):403?434.
A. Pauls and D. Klein. 2012. Large-scale syntactic language
modeling with treelets. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics,
pages 959?968. Jeju, Korea.
F. Pereira. 2000. Formal grammar and information theory:
together again? Philosophical Transactions of the Royal
Society of London. Series A: Mathematical, Physical and
Engineering Sciences, 358(1769):1239?1253.
M. Post. 2011. Judging grammaticality with tree substitution
grammar derivations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 217?222.
F. Reali and M.H. Christiansen. 2005. Uncovering the rich-
ness of the stimulus: Structure dependence and indirect
statistical evidence. Cognitive Science, 29(6):1007?1028.
36
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 19?27,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Monads as a Solution for Generalized Opacity
Gianluca Giorgolo
University of Oxford
Ash Asudeh
University of Oxford and Carleton University
{gianluca.giorgolo,ash.asudeh}@ling-phil.ox.ac.uk
Abstract
In this paper we discuss a conservative ex-
tension of the simply-typed lambda calcu-
lus in order to model a class of expres-
sions that generalize the notion of opaque
contexts. Our extension is based on previ-
ous work in the semantics of programming
languages aimed at providing a mathemat-
ical characterization of computations that
produce some kind of side effect (Moggi,
1989), and is based on the notion of mon-
ads, a construction in category theory that,
intuitively, maps a collection of ?simple?
values and ?simple? functions into a more
complex value space, in a canonical way.
The main advantages of our approach with
respect to traditional analyses of opacity
are the fact that we are able to explain in
a uniform way a set of different but re-
lated phenomena, and that we do so in a
principled way that has been shown to also
explain other linguistic phenomena (Shan,
2001).
1 Introduction
Opaque contexts have been an active area of re-
search in natural language semantics since Frege?s
original discussion of the puzzle (Frege, 1892). A
sentence like (1) has a non-contradictory interpre-
tation despite the fact that the two referring expres-
sions Hesperus and Phosphorus refer to the same
entity, the planet we know as Venus.
(1) Reza doesn?t believe Hesperus is Phos-
phorus.
The fact that a sentence like (1) includes the
modal believe has influenced much of the analy-
ses proposed in the literature, and has linked the
phenomenon with the notion of modality. In this
paper we challenge this view and try to position
data like (1) inside a larger framework that also
includes other types of expressions.
We decompose examples like (1) along two di-
mensions: the presence or absence of a modal ex-
pression, and the way in which we multiply re-
fer to the same individual. In the case of (1), we
have a modal and we use two different co-referring
expressions. Examples (2)-(4) complete the land-
scape of possible combinations:
(2) Dr. Octopus punched Spider-Man but he
didn?t punch Spider-Man.
(3) Mary Jane loves Peter Parker but she
doesn?t love Spider-Man.
(4) Reza doesn?t believe Jesus is Jesus.
(2) is clearly a contradictory statement, as we
predicate of Dr. Octopus that he has the property
of having punched Spider-Man and its negation.
Notice that in this example there is no modal and
the exact same expression is used twice to refer
to the object individual. In the case of (3) we
still have no modal and we use two different but
co-referring expressions to refer to the same in-
dividual. However in this case the statement has
a non-contradictory reading. Similarly (4) has a
non-contradictory reading, which states that, ac-
cording to the speaker, Reza doesn?t believe that
the entity he (Reza) calls Jesus is the entity that the
speaker calls Jesus (e.g., is not the same individual
or does not have the same properties). This case is
symmetrical to (3), as here we have a modal ex-
pression but the same expression is used twice to
refer to the same individual.
If the relevant reading of (4) is difficult to get,
consider an alternative kind of scenario, as in (5),
in which the subject of the sentence, Kim, suffers
from Capgras Syndrome
1
and thinks that Sandy is
an impostor. The speaker says:
1
From Wikipedia: ?[Capgras syndrome] is a disorder in
which a person holds a delusion that a friend, spouse, par-
19
(5) Kim doesn?t believe Sandy is Sandy
Given the definition of Capgras Syndrome (in
fn. 1), there is a clear non-contradictory reading
available here, in which the speaker is stating that
Kim does not believe that the entity in question,
that the speaker and (other non-Capgras-sufferers)
would call Sandy, is the entity that she associate
with the name Sandy.
We propose an analysis of the non-contradictory
cases based on the intuition that the apparently co-
referential expressions are in fact interpreted us-
ing different interpretation functions, which cor-
respond to different perspectives that are pitted
against each other in the sentences. Furthermore,
we propose that modal expressions are not the only
ones capable of introducing a shift of perspective,
but also that verbs that involve some kind of men-
tal attitude of the subject towards the object have
the same effect.
Notice that this last observation distinguishes
our approach from one where a sentence like (3) is
interpreted as simply saying that Mary Jane loves
only one guise of the entity that corresponds to Pe-
ter Parker but not another one. The problem with
this interpretation is that if it is indeed the case
that different co-referring expressions simply pick
out different guises of the same individual, then a
sentence like (6) should have a non-contradictory
reading, while this seems not to be the case.
(6) Dr. Octopus killed Spider-Man but he
didn?t kill Peter Parker.
While a guise-based interpretation is compatible
with our analysis
2
, it is also necessary to correctly
model the different behaviour of verbs like love
with respect to others like punch or kill. In fact,
we need to model the difference between, for ex-
ample, kill and murder, since murder does involve
a mental attitude of intention and the correspond-
ing sentence to (6) is not necessarily contradictory:
(7) Dr. Octopus murdered Spider-Man but he
didn?t murder Peter Parker.
The implementation of our analysis is based on
monads. Monads are a construction in category
theory that defines a canonical way to map a set
of objects and functions that we may consider as
ent, or other close family member has been replaced by an
identical-looking impostor.?
2
Indeed, one way to understand guises is as different ways
in which we interpret a referring term (Heim, 1998).
simple into a more complex object and function
space. They have been successfully used in the
semantics of programming languages to charac-
terise computations that are not ?pure?. By pure
we mean code objects that are totally referentially
transparent (i.e. do not depend on external factors
and return the same results given the same input
independently of their execution context), and also
that do not have effects on the ?real world?. In
contrast, monads are used to model computations
that for example read from or write to a file, that
depend on some random process or whose return
value is non-deterministic.
In our case we will use the monad that describe
values that are made dependent on some exter-
nal factor, commonly known in the functional pro-
gramming literature as the Reader monad.
3
We
will represent linguistic expressions that can be as-
signed potentially different interpretations as func-
tions from interpretation indices to values. Effec-
tively we will construct a different type of lexicon
that does not represent only the linguistic knowl-
edge of a single speaker but also her (possibly par-
tial) knowledge of the language of other speak-
ers. So, for example, we will claim that (4) can
be assigned a non-contradictory reading because
the speaker?s lexicon also includes the information
regarding Reza?s interpretation of the name Jesus
and therefore makes it possible for the speaker to
use the same expression, in combination with a
verb such as believe, to actually refer to two dif-
ferent entities. In one case we will argue that the
name Jesus is interpreted using the speaker?s inter-
pretation while in the other case it is Reza?s inter-
pretation that is used.
Notice that we can apply our analysis to any
natural language expression that may have differ-
ent interpretations. This means that, for exam-
ple, we can extend our analysis, which is limited
to referring expressions here for space reasons, to
other cases, such as the standard examples involv-
ing ideally synonymous predicates like groundhog
and woodchuck (see, e.g., Fox and Lappin (2005)).
The paper is organised as follows: in section
2 we discuss the technical details of our analysis;
in section 3 we discuss our analyses of the moti-
vating examples; section 4 compares our approach
with a standard approach to opacity; we conclude
in section 5.
3
Shan (2001) was the first to sketch the idea of using the
Reader monad to model intensional phenomena.
20
2 Monads and interpretation functions
To avoid introducing the complexities of the cat-
egorical formalism, we introduce monads as they
are usually encountered in the computer science
literature. A monad is defined as a triple ??, ?, ??.
? is what we call a functor, in our case a mapping
between types and functions. We call the com-
ponent of ? that maps between types ?
1
, while
the one that maps between functions ?
2
. In our
case ?
1
will map each type to a new type that
corresponds to the original type with an added
interpretation index parameter. Formally, if i is
the type of interpretation indices, then ?
1
maps
any type ? to i ? ? . In terms of functions,
?
2
maps any function f : ? ? ? to a function
f
?
: (i ? ?) ? i ? ?. ?
2
corresponds to func-
tion composition:
?
2
(f) = ?g.?i.f(g(i)) (8)
In what follows the component ?
2
will not be used
so we will use ? as an abbreviation for ?
1
. This
means that we will write ?? for the type i? ? .
? (pronounced ?unit?) is a polymorphic func-
tion that maps inhabitants of a type ? to inhabitants
of its image under ?, formally ? : ??.? ? ?? .
Using the computational metaphor, ? should em-
bed a value in a computation that returns that value
without any side-effect. In our case ? should sim-
ply add a vacuous parameter to the value:
?(x) = ?i.x (9)
? (pronounced ?bind?) is a polymorphic func-
tion of type ??.??.?? ? (? ? ??) ? ??, and
acts as a sort of enhanced functional application.
4
Again using the computational metaphor, ? takes
care of combining the side effects of the argument
and the function and returns the resulting compu-
tation. In the case of the monad we are interested
in, ? is defined as in (10).
a ? f = ?i.f(a(i))(i) (10)
Another fundamental property of ? is that, by
imposing an order of evaluation, it will provide
us with an additional scoping mechanism distinct
from standard functional application. This will al-
low us to correctly capture the multiple readings
4
We use for ? the argument order as it is normally used in
functional programming. We could swap the arguments and
make it look more like standard functional application. Also,
we write ? in infix notation.
associated with the expressions under considera-
tion.
We thus add two operators, ? and ?, to the
lambda calculus and the reductions work as ex-
pected for (9) and (10). These reductions are im-
plicit in our analyses in section 3.
2.1 Compositional logic
For composing the meanings of linguistic re-
sources we use a logical calculus adapted for the
linear case
5
from the one introduced by Benton et
al. (1998). The calculus is based on a language
with two connectives corresponding to our type
constructors: (, a binary connective, that corre-
sponds to (linear) functional types, and ?, a unary
connective, that represents monadic types.
The logical calculus is described by the proof
rules in figure 1. The rules come annotated with
lambda terms that characterise the Curry-Howard
correspondence between proofs and meaning
terms. Here we assume a Lexical Functional
Grammar-like setup (Dalrymple, 2001), where
a syntactic and functional grammar component
feeds the semantic component with lexical re-
sources already marked with respect to their
predicate-argument relationships. Alternatively
we could modify the calculus to a categorial set-
ting, by introducing a structural connective, and
using directional versions of the implication con-
nective together with purely structural rules to
control the compositional process.
We can prove that the Cut rule is admissible,
therefore the calculus becomes an effective (al-
though inefficient) way of computing the meaning
of a linguistic expression.
A key advantage we gain from the monadic ap-
proach is that we are not forced to generalize all
lexical entries to the ?worst case?. With the log-
ical setup we have just described we can in fact
freely mix monadic and non monadic resources.
For example, in our logic we can combine a pure
version of a binary function with arguments that
are either pure or monadic, as the following are all
5
Linearity is a property that has been argued for in the
context of natural language semantics by various researchers
(Moortgat (2011), Asudeh (2012), among others).
21
id
x : A ` x : A
? ` B B,? ` C
Cut
?,? ` C
?, x : A ` t : B
( R
? ` ?x.t : A( B
? ` t : A ?, x : B ` u : C
( L
?,?, y : A( B ` u[y(t)/x] : C
? ` x : A
?R
? ` ?(x) : ?A
?, x : A ` t : ?B
?L
?, y : ?A ` y ? ?x.t : ?B
Figure 1: Sequent calculus for a fragment of multiplicative linear logic enriched with a monadic modality,
together with a Curry-Howard correspondence between formulae and meaning terms.
provable theorems in this logic.
A( B( C,A,B ` ?C (11)
A( B( C,?A,B ` ?C (12)
A( B( C,A,?B ` ?C (13)
A( B( C,?A,?B ` ?C (14)
In contrast, the following is not a theorem in the
logic:
A( B( C, I ( A, I ( B 6` I ( C (15)
In general, then, if we were to simply lift the type
of the lexical resources whose interpretation may
be dependent on a specific point of view, we would
be forced to lift all linguistic expressions that may
combine with them, thus generalizing to the worst
case after all.
The monadic machinery also achieves a higher
level of compositionality. In principle we could
directly encode our monad using the ? type con-
structor, with linear implication,(, on the logical
side. However this alternative encoding wouldn?t
have the same deductive properties. Compare the
pattern of inferences we have for the monadic
type, in (11)?(14), with the equivalent one for the
simple type:
A( B( C,A,B ` C (16)
A( B( C, I ( A,B ` I ( C (17)
A( B( C,A, I ( B ` I ( C (18)
A( B( C, I ( A, I ( B ` (19)
I ( I ( C
In the case of the simple type, the final formula we
derive depends in some non-trivial way on the en-
tire collection of resources on the left-hand side of
the sequent. In contrast in the case of the monadic
type, the same type can be derived for all config-
urations. What is important is that we can pre-
dict the final formula without having to consider
the entire set of resources available. This shows
that the compositionality of our monadic approach
cannot be equivalently recapitulated in a simple
type theory.
3 Examples
The starting point for the analysis of examples (1)-
(4) is the lexicon in table 1. The lexicon represents
the linguistic knowledge of the speaker, and at
the same time her knowledge about other agents?
grammars.
Most lexical entries are standard, since we do
not need to change the type and denotation of lex-
ical items that are not involved in the phenomena
under discussion. So, for instance, logical opera-
tors such as not and but are interpreted in the stan-
dard non-monadic way, as is a verb like punch or
kill. Referring expressions that are possibly con-
tentious, in the sense that they can be interpreted
differently by the speaker and other agents, instead
have the monadic type ?e. This is reflected in
their denotation by the fact that their value varies
according to an interpretation index. We use a
special index ? to refer to the speaker?s own per-
spective, and assume that this is the default index
used whenever no other index is specifically in-
troduced. For example, in the case of the name
Spider-Man, the speaker is aware of his secret
identity and therefore interprets it as another name
for the individual Peter Parker, while Mary Jane
and Dr. Octopus consider Spider-Man a different
entity from Peter Parker.
We assume that sentences are interpreted in a
model in which all entities are mental entities, i.e.
that there is no direct reference to entities in the
world, but only to mental representations. Enti-
ties are therefore relativized with respect to the
agent that mentally represents them, where non-
contentious entities are always relativized accord-
ing to the speaker. This allows us to represent the
22
WORD DENOTATION TYPE
Reza r
?
e
Kim k
?
e
Dr. Octopus o
?
e
Mary Jane mj
?
e
Peter Parker pp
?
e
not ?p.?p t? t
but ?p.?q.p ? q t? t? t
is ?x.?y.x = y e? e? t
punch ?o.?s.punch(s)(o) e? e? t
believe ?c.?s.?i.B(s)(c(?(s))) ?t? e? ?t
love ?o.?s.?i.love(s)(o(?(s))) ?e? e? ?t
Hesperus ?i.
{
es
r
if i = r,
v
?
if i = ?
?e
Phosphorus ?i.
{
ms
r
if i = r,
v
?
if i = ?
?e
Spider-Man ?i.
{
sm
i
if i = o or i = mj,
pp
?
if i = ?
?e
Jesus ?i.
{
j
r
if i = r,
j
?
if i = ?
?e
Sandy ?i.
{
imp
k
if i = k,
s
?
if i = ?
?e
Table 1: Speaker?s lexicon
fact that different agents may have distinct equiv-
alencies between entities. For example, Reza in
our model does not equate the evening star and
the morning star, but the speaker equates them
with each other and with Venus. Therefore, the
speaker?s lexicon in table 1 represents the fact
that the speaker?s epistemic model includes what
the speaker knows about other agents? models,
e.g. that Reza has a distinct denotation (from the
speaker) for Hesperus, that Mary Jane has a dis-
tinct representation for Spider-Man, that Kim has
a distinct representation for Sandy, etc.
The other special lexical entries in our lexicon
are those for verbs like believe and love. The two
entries are similar in the sense that they both take
an already monadic resource and actively supply
a specific interpretation index that corresponds to
the subject of the verbs. The function ?maps each
entity to the corresponding interpretation index,
i.e. ? : e ? i. For example, in the lexical en-
tries for believe and love, ?maps the subject to the
interpretation index of the subject. Thus, the entry
for believe uses the subject?s point of view as the
perspective used to evaluate its entire complement,
while love changes the interpretation of its object
relative to the perspective of its subject. However
we will see that the interaction of these lexical en-
tries and the evaluation order imposed by ?will al-
low us to let the complement of a verb like believe
and the object of a verb like love escape the spe-
cific effect of forcing the subject point of view, and
instead we will be able to derive readings in which
the arguments of the verb are interpreted using the
speaker?s point of view.
Figure 2 reports the four non-equivalent read-
ings that we derive in our system for example (1),
repeated here as (20).
6
(20) Reza doesn?t believe that Hesperus is
Phosphorus.
Reading (21) assigns to both Hesperus and
Phosphorus the subject interpretation and results,
after contextualising the sentence by applying it
to the standard ? interpretation index, in the truth
conditions in (25), i.e. that Reza does not be-
lieve that the evening star is the morning star. This
6
The logic generates six different readings but the monad
we are using here has a commutative behaviour, so four of
these readings are pairwise equivalent.
23
JbelieveK (JHesperusK ? ?x. JPhosphorusK ? ?y.?(JisK (x)(y)))(JRezaK) ? ?z.?(JnotK (z)) (21)
JHesperusK ? ?x. JbelieveK (JPhosphorusK ? ?y.?(JisK (x)(y)))(JRezaK) ? ?z.?(JnotK (z)) (22)
JPhosphorusK ? ?x. JbelieveK (JHesperusK ? ?y.?(JisK (y)(x)))(JRezaK) ? ?z.?(JnotK (z)) (23)
JHesperusK ? ?x. JPhosphorusK ? ?y. JbelieveK (?(JisK (x)(y)))(JRezaK) ? ?z.?(JnotK (z)) (24)
Figure 2: Non-equivalent readings for Reza doesn?t believe Hesperus is Phosphorus.
reading would not be contradictory in an epistemic
model (such as Reza?s model) where the evening
star and the morning star are not the same entity.
?B(r)(es
r
= ms
r
) (25)
In the case of (22) and (23) we get a similar ef-
fect although here we mix the epistemic models,
and one of the referring expressions is interpreted
under the speaker perspective while the other is
again interpreted under Reza?s perspective. For
these two readings we obtain respectively the truth
conditions in (26) and (27).
?B(r)(v
?
= ms
r
) (26)
?B(r)(v
?
= es
r
) (27)
Finally for (24) we get the contradictory reading
that Reza does not believe that Venus is Venus, as
both referring expressions are evaluated using the
speaker?s interpretation index.
?B(r)(v
?
= v
?
) (28)
The different contexts for the interpretation of
referring expressions are completely determined
by the order in which we evaluate monadic re-
sources. This means that, just by looking at the
linear order of the lambda order, we can check
whether a referring expression is evaluated inside
the scope of a perspective changing operator such
as believe, or if it is interpreted using the standard
interpretation.
If we consider a case like sentence (2), we ought
to get only a contradictory reading as the statement
is intuitively contradictory. Our analysis produces
a single reading that indeed corresponds to a con-
tradictory interpretation:
JSpider-ManK ? ?x. JSpider-ManK ?
?y.?(JbutK (JpunchK (JDr. OctopusK)(x))
(JnotK (JpunchK (JDr. OctopusK)(y)))) (29)
The verb punch is not a verb that can change
the interpretation perspective and therefore the po-
tentially controversial name Spider-Man is inter-
preted in both instances using the speaker?s inter-
pretation index. The result are unsatisfiable truth
conditions, as expected:
punch(o
?
)(pp
?
) ? ?punch(o
?
)(pp
?
) (30)
In contrast a verb like love is defined in our lex-
icon as possibly changing the interpretation per-
spective of its object to that of its subject. There-
fore in the case of a sentence like (3), we ex-
pect one reading where the potentially contentious
name Spider-Man is interpreted according to the
subject of love, Mary Jane. This is in fact the re-
sult we obtain. Figure 3 reports the two readings
that our framework generates for (3).
Reading (31), corresponds to the non contradic-
tory interpretation of sentence (3), where Spider-
Man is interpreted according to Mary Jane?s per-
spective and therefore is assigned an entity differ-
ent from Peter Parker:
love(mj
?
)(pp
?
) ? ?love(mj
?
)(sm
mj
) (33)
Reading (32) instead generates unsatisfiable truth
conditions, as Spider-Man is identified with Peter
Parker according to the speaker?s interpretation:
love(mj
?
)(pp
?
) ? ?love(mj
?
)(pp
?
) (34)
Our last example, (4), repeated here as (35), is
particularly interesting as we are not aware of pre-
vious work that discusses this type of sentence.
The non-contradictory reading that this sentence
has seems to be connected specifically to two dif-
ferent interpretations of the same name, Jesus,
both under the syntactic scope of the modal be-
lieve.
(35) Reza doesn?t believe Jesus is Jesus.
24
JloveK (?(JPeter ParkerK))(JMary JaneK) ? ?p. JloveK (JSpider-ManK)(JMary JaneK) ?
?q.?(JbutK (p)(JnotK (q))) (31)
JloveK (?(JPeter ParkerK))(JMary JaneK) ? ?p. JSpider-ManK ? ?x. JloveK (?(x))(JMary JaneK) ?
?q.?(JbutK (p)(JnotK (q))) (32)
Figure 3: Non-equivalent readings for Mary Jane loves Peter Parker but she doesn?t love Spider-Man.
JbelieveK (JJesusK ? ?x. JJesusK ? ?y.?(JisK (x)(y)))(JRezaK) ? ?z.?(JnotK (z)) (36)
JJesusK ? ?x. JJesusK ? ?y. JbelieveK (?(JisK (x)(y)))(JRezaK) ? ?z.?(JnotK (z)) (37)
JJesusK ? ?x. JbelieveK (JJesusK ? ?y.?(JisK (x)(y)))(JRezaK) ? ?z.?(JnotK (z)) (38)
Figure 4: Non-equivalent readings for Reza doesn?t believe Jesus is Jesus.
Our system generates three non-equivalent read-
ings, reported here in figure 4.
7
Reading (36) and (37) corresponds to two con-
tradictory readings of the sentence: in the first
case both instances of the name Jesus are inter-
preted from the subject perspective and therefore
attribute to Reza the non-belief in a tautology, sim-
ilarly in the second case, even though in this case
the two names are interpreted from the perspec-
tive of the speaker. In contrast the reading in (38)
corresponds to the interpretation that assigns two
different referents to the two instances of the name
Jesus, producing the truth conditions in (39) which
are satisfiable in a suitable model.
?B(r)(j
?
= j
r
) (39)
The analysis of the Capgras example (5), re-
peated in (40), is equivalent; the non-contradictory
reading is shown in (41).
(40) Kim doesn?t believe Sandy is Sandy.
?B(k)(s
?
= imp
k
) (41)
We use imp
k
as the speaker?s representation of
the ?impostor? that Kim thinks has taken the place
of Sandy.
More generally, there are again three non-
equivalent readings, including the one above,
which are just those in figure 4, with JJesusK re-
placed by JSandyK and JRezaK replaced by JKimK.
7
Again, there are six readings that correspond to different
proofs, but given the commutative behaviour of the Reader
monad, the fact that equality is commutative, and the fact that
we have in this case two identical lexical items, only three of
them are non-equivalent readings.
4 Comparison with traditional
approaches
In this section we try to sketch how a traditional
approach to opaque contexts, such as one based
on a de dicto/de re ambiguity with respect to a
modal operator, would fare in the analysis of (4),
our most challenging example.
To try to explain the two readings in the con-
text of a standard possible worlds semantics, we
could take (4) to be ambiguous with respect to a
de dicto/de re reading. In the case of the de dicto
reading (which corresponds to the non-satisfiable
reading) the two names are evaluated under the
scope of the doxastic operator believe, i.e. they
both refer to the same entity that is assigned to the
name Jesus in each accessible world. Clearly this
is always the case, and so (4) is not satisfiable. In
the case of the de re reading, we assume that the
two names are evaluated at different worlds that
assign different referents to the two names. One
of these two worlds will be the actual world and
the other one of the accesible worlds. The reading
is satisfiable if the doxastic modality links the ac-
tual world with one in which the name Jesus refers
to a different entity. Notice that for this analysis to
work we need to make two assumptions: 1. that
names behave as quantifiers with the property of
escaping modal contexts, 2. that names can be as-
signed different referents in different worlds, i.e.
we have to abandon the standard notion that names
are rigid designators (Kripke, 1972). In contrast,
in our approach we do not need to abandon the
idea of rigid designation for names (within each
25
agent?s model).
However, such an approach would present a
number of rather serious problems. The first is
connected with the assumption that names are
scopeless. This is a common hypothesis in natural
language semantics and indeed if we model names
as generalized quantifiers they can be proven to be
scopeless (Zimmermann, 1993). But this is prob-
lematic for our example. In fact we would predict
that both instances of the name Jesus escape the
scope of believe. The resulting reading would bind
the quantified individual to the interpretation of Je-
sus in the actual world. In this way we only cap-
ture the non-satisfiable reading. To save the sco-
pal approach we would need to assume that names
in fact are sometimes interpreted in the scope of
modal operators.
One way to do this would be to set up our se-
mantic derivations so that they allow different sco-
pal relations between quantifiers and other opera-
tors. The problem with this solution is that for sen-
tences like (4) we generate twelve different deriva-
tions, some of which do not correspond to valid
readings of the sentence.
Even assuming that we find a satisfactory solu-
tion for these problems, the scopal approach can-
not really capture the intuitions behind opacity in
all contexts. Consider again (4) and assume that
there are two views about Jesus: Jesus as a divine
being and Jesus as a human being. Assume that Je-
sus is a human being in the actual world and that
Reza is an atheist, then the only possible reading
is the non-satisfiable one, as the referent for Jesus
will be the same in the actual world and all acces-
sible Reza-belief-worlds. The problem is that the
scopal approach assumes a single modal model,
while in this case it seems that there are two doxas-
tic models, Reza?s model and the speaker?s model,
under discussion. In contrast, in our approach the
relevant part of Reza?s model is embedded inside
the speaker?s model and interpretation indices in-
dicate which interpretation belongs to Reza and
which to the speaker.
Finally an account of modality in terms of sco-
pal properties is necessarily limited to cases in
which modal operators are present. While this
may be a valid position in the case of typical in-
tensional verbs like seek or want, it would not be
clear how we could extend this approach to cases
like 3, as the verb love has no clear modal con-
notation. Thus, the scopal approach would not be
sufficiently general.
5 Conclusion
We started by discussing a diverse collection of
expressions that share the common property of
showing nontrivial referential behaviours. We
have proposed a common analysis of all these
expressions in terms of a combination of differ-
ent interpretation contexts. We have claimed that
the switch to a different interpretation context is
triggered by specific lexical items, such as modal
verbs but also verbs that express some kind of
mental attitude of the subject of the verb towards
its object. The context switch is not obligatory,
as witnessed by the multiple readings that the sen-
tences discussed seem to have. We implemented
our analysis using monads. The main idea of
our formal implementation is that referring ex-
pressions that have a potential dependency from
an interpretation context can be implemented as
functions from interpretation indices to fully in-
terpreted values. Similarly, the linguistic triggers
for context switch are implemented in the lexi-
con as functions that can modify the interpreta-
tion context of their arguments. Monads allow
us to freely combine these ?lifted? meanings with
standard ones, avoiding in this way to generalize
our lexicon to the worst case. We have also seen
how more traditional approaches, while capable of
dealing with some of the examples we discuss, are
not capable of providing a generalised explanation
of the observed phenomena.
Acknowledgements
The authors would like to thank our anonymous
reviewers for their comments. This research is
supported by a Marie Curie Intra-European Fel-
lowship from the Europoean Commision under
contract number 327811 (Giorgolo) and an Early
Researcher Award from the Ontario Ministry of
Research and Innovation and NSERC Discovery
Grant #371969 (Asudeh).
26
References
Ash Asudeh. 2012. The Logic of Pronominal Resump-
tion. Oxford Studies in Theoretical Linguistics. Ox-
ford University Press, New York.
Nick Benton, G. M. Bierman, and Valeria de Paiva.
1998. Computational types from a logical per-
spective. Journal of Functional Programming,
8(2):177?193.
Mary Dalrymple. 2001. Lexical Functional Grammar.
Academic Press, San Diego, CA.
Chris Fox and Shalom Lappin. 2005. Foundations of
Intensional Semantics. Blackwell, Oxford.
Gottlob Frege. 1892.
?
Uber Sinn und Bedeutung.
Zeitschrift f?ur Philosophie und philosophische Kri-
tik, 100:25?50.
Gottlob Frege. 1952. On sense and reference. In Pe-
ter T. Geach and Max Black, editors, Translations
from the Philosophical Writings of Gottlob Frege,
pages 56?78. Blackwell, Oxford. Translation of
Frege (1892).
Irene Heim. 1998. Anaphora and semantic interpreta-
tion: A reinterpretation of Reinhart?s approach. In
Uli Sauerland and Orin Percus, editors, The Inter-
pretive Tract, volume 25 of MIT Working Papers in
Linguistics, pages 205?246. MITWPL, Cambridge,
MA.
Saul Kripke. 1972. Naming and necessity. In Donald
Davidson and Gilbert Harman, editors, Semantics of
Natural Language, pages 253?355. Reidel.
Eugenio Moggi. 1989. Computational lambda-
calculus and monads. In LICS, pages 14?23. IEEE
Computer Society.
Michael Moortgat. 2011. Categorial type logics. In
Johan van Benthem and Alice ter Meulen, editors,
Handbook of Logic and Language, pages 95?179.
Elsevier, second edition.
Chung-chieh Shan. 2001. Monads for natural lan-
guage semantics. In Kristina Striegnitz, editor, Pro-
ceedings of the ESSLLI-2001 Student Session, pages
285?298. 13th European Summer School in Logic,
Language and Information.
Thomas Ede Zimmermann. 1993. Scopeless quanti-
fiers and operators. Journal of Philosophical Logic,
22(5):545?561, October.
27
