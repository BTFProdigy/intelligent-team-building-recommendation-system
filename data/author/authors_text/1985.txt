Proceedings of the Workshop on Linguistic Distances, pages 35?42,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Sentence Comparison
using Robust Minimal Recursion Semantics
and an Ontology
Rebecca Dridan
}
and Francis Bond

}
rdrid@csse.unimelb.edu.au

bond@cslab.kecl.ntt.co.jp
}
The University of Melbourne

NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
Abstract
We design and test a sentence com-
parison method using the framework
of Robust Minimal Recursion Seman-
tics which allows us to utilise the deep
parse information produced by Jacy, a
Japanese HPSG based parser and the
lexical information available in our on-
tology. Our method was used for both
paraphrase detection and also for an-
swer sentence selection for question an-
swering. In both tasks, results showed
an improvement over Bag-of-Words, as
well as providing extra information use-
ful to the applications.
1 Introduction
Comparison between sentences is required for
many NLP applications, including question
answering, paraphrasing, text summarization
and entailment tasks. In this paper we show
an RMRS (Robust Minimal Recursion Seman-
tics, see Section 1.1) comparison algorithm
that can be used to compare sentences in
any language that has RMRS generating tools
available. Lexical resources of any language
can be plugged in to give a more accurate and
informative comparison.
The simplest and most commonly used
methods of judging sentence similarity use
word overlap { either looking for matching
word sequences, or comparing a Bag-of-Words
representation of each sentence. Bag-of-Words
discards word order, and any structure desig-
nated by such, so that the cat snored and the
dog slept is equivalent to the dog snored and
the cat slept. Sequence matching on the other
hand requires exact word order matching and
hence the game began quietly and the game qui-
etly began are not considered a match. Neither
method allows for synonym matching.
Hirao et al (2004) showed that they could
get a much more robust comparison using
dependency information rather than Bag-of-
Words, since they could abstract away from
word order but still compare the important
elements of a sentence. Using deep parsing
information, such as dependencies, but also
deep lexical resources where available, enables
a much more informative and robust compar-
ison, which goes beyond lexical similarity. We
use the RMRS framework as our comparison
format because it has the descriptive power to
encode the full semantics, including argument
structure. It also enables easy combination of
deep and shallow information and, due to its
at structure, is easy to manage computation-
ally.
1.1 Robust Minimal Recursion
Semantics
Robust Minimal Recursion Semantics
(RMRS) is a form of at semantics which is
designed to allow deep and shallow processing
to use a compatible semantic representation,
while being rich enough to support gener-
alized quantiers (Frank, 2004). The main
component of an RMRS representation is
a bag of elementary predicates and their
arguments.
An elementary predicate always has a
unique label, a relation type, a relation name
and an ARG0 feature. The example in Fig-
ure 1 has a label of h5 which uniquely identi-
es this predicate. Relation types can either
be realpred for a predicate that relates di-
rectly to a content word from the input text, or
gpred for grammatical predicates which may
not have a direct referent in the text. For ex-
amples in this paper, a realpred is distin-
guished by an underscore ( ) before the rela-
tion name.
The gpred relation names come from a
35
"unten s
lbl h5
arg0 e6
#
Figure 1: Elementary predicate for-2 unten
\drive"
closed-set which specify common grammatical
relations, but the realpred names are formed
from the word in the text they relate to and
this is one way in which RMRS allows under-
specication. A full relation name is of the
form lemma pos sense, where the pos (part
of speech) is drawn from a small set of general
types including noun, verb and sahen (verbal
noun). The sense is a number that identies
the sense of the word within a particular gram-
mar being used. The POS and sense informa-
tion are only used when available and hence
the unten s 1 is more specic but compati-
ble with unten s or even unten.
The arg0 feature (e6 in Figure 1) is the
referential index of the predicate. Predicates
with the same arg0 are said to be referen-
tially co-indexed and therefore have the same
referent in the text.
A shallow parse might provide only the fea-
tures shown in Figure 1, but a deep parse can
also give information about other arguments
as well as scoping constraints. The features
arg1..arg4 specify the indices of the semantic
arguments of the relevant predicate, similar to
PropBank's argument annotation (Kingsbury
et al, 2002). While the RMRS specication
does not dene semantic roles for the argn
features, in practice arg1 is generally used for
the agent and arg2 for the patient. Fea-
tures arg3 and arg4 have less consistency in
their roles.
We will use (1) and (2) as examples of sim-
ilar sentences. They are denition sentences
for one sense of ),'* doraiba- \driver",
taken from two dierent lexicons.
(1) .30 & -2 !% 1
jidosha wo unten suru hito
car acc drive do person
\a person who drives a car"
(2) .30 #" $ -2 /
jidosha nado no unten sha
car etc. adn drive -er
\a driver of cars etc."
Examples of deep and shallow RMRS results
for (1) are given in Figure 2. Deep results for
(2) are given in Figure 3.
2 Algorithm
The matching algorithm is loosely based on
RMRS comparison code included in the LKB
(Copestake, 2002: hhttp://www.delph-in.
net/lkb/i), which was used in Ritchie (2004),
however that code used no outside lexical re-
sources and we have substantially changed the
matching algorithm.
The comparison algorithm is language inde-
pendent and can be used for any RMRS struc-
tures. It rst compares all elementary predi-
cates from the RMRSs to construct a list of
match records and then examines, and poten-
tially alters, the list of match records accord-
ing to constraints encoded in the argn vari-
ables. Using the list of scored matches, the
lowest scoring possible match set is found and,
after further processing on that set, a similar-
ity score is returned. The threshold for de-
ciding whether a pair of sentences should be
considered similar or not can be determined
separately for dierent applications.
2.1 Matching Predicates
The elementary predicates (EPs) of our RMRS
structures are divided into two groups - those
that have a referent in the text, hereafter
known as content EPs, and those that don't.
There are three kinds of content EP: real-
preds, which correspond to content bearing
words that the grammar knows; gpreds with
a carg (constant argument) feature, which
are used to represent proper names and num-
bers; and gpreds with a predicate name start-
ing with generic such as generic verb which
are used for unknown words that have only
been identied by their part of speech. All
other EPs have no referent and are used to
provide information about the content EPs or
about the structure of the sentence as a whole.
These non-content EPs can provide some use-
ful information, but generally only in relation
to other content EPs.
Each content EP of the rst RMRS is com-
pared to all content EPs in the second RMRS,
as shown in Figure 4.
Matches are categorised as exact, syn-
onym, hypernym, hyponym or no match
and a numerical score is assigned. The nu-
36
26
6
6
6
6
6
6
6
6
6
6
6
6
4
text ',)%&+ $*
top h1
rels
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
"
proposition m rel
lbl h1
arg0 e2
marg h3
#"
unknown rel
lbl h4
arg0 e2
arg x5
#

jidousha n
lbl h6
arg0 x7

2
6
4
udef rel
lbl h8
arg0 x7
rstr h9
body h10
3
7
5
2
6
4
unten s
lbl h11
arg0 e13
arg1 u12
arg2 x7
3
7
5

hito n
lbl h14
arg0 x5

2
6
4
udef rel
lbl h15
arg0 x5
rstr h16
body h17
3
7
5
"
proposition m rel
lbl h10001
arg0 e13
marg h18
#
2
6
4
topic rel
lbl h10002
arg0 e19
arg1 e13
arg2 x5
3
7
5
9
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
;
hcons fh3 qeq h4 ; h9 qeq h6 ; h16 qeq h14 ; h18 qeq h11g
ing fh11 ing h10002 ; h14 ing h10001g
3
7
7
7
7
7
7
7
7
7
7
7
7
7
5
2
6
6
6
4
text ',)%&+ $*
top h9
rels
Proceedings of ACL-08: HLT, pages 613?621,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Enhancing Performance of Lexicalised Grammars
Rebecca Dridan?, Valia Kordoni?, Jeremy Nicholson??
?Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany
?Dept of Computer Science and Software Engineering and NICTA, University of Melbourne, Australia
{rdrid,kordoni}@coli.uni-sb.de, jeremymn@csse.unimelb.edu.au
Abstract
This paper describes how external resources
can be used to improve parser performance for
heavily lexicalised grammars, looking at both
robustness and efficiency. In terms of robust-
ness, we try using different types of external
data to increase lexical coverage, and find that
simple POS tags have the most effect, increas-
ing coverage on unseen data by up to 45%. We
also show that filtering lexical items in a su-
pertagging manner is very effective in increas-
ing efficiency. Even using vanilla POS tags we
achieve some efficiency gains, but when us-
ing detailed lexical types as supertags weman-
age to halve parsing time with minimal loss of
coverage or precision.
1 Introduction
Heavily lexicalised grammars have been used in ap-
plications such as machine translation and informa-
tion extraction because they can produce semantic
structures which provide more information than less
informed parsers. In particular, because of the struc-
tural and semantic information attached to lexicon
items, these grammars do well at describing com-
plex relationships, like non-projectivity and center
embedding. However, the cost of this additional in-
formation sometimes makes deep parsers that use
these grammars impractical. Firstly because, if the
information is not available, the parsers may fail to
produce an analysis, a failure of robustness. Sec-
ondly, the effect of analysing the extra information
can slow the parser down, causing efficiency prob-
lems. This paper describes experiments aimed at
improving parser performance in these two areas, by
annotating the input given to one such deep parser,
the PET parser (Callmeier, 2000), which uses lex-
icalised grammars developed under the HPSG for-
malism (Pollard and Sag, 1994).
2 Background
In all heavily lexicalised formalisms, such as LTAG,
CCG, LFG and HPSG, the lexicon plays a key role
in parsing. But a lexicon can never hope to contain
all words in open domain text, and so lexical cover-
age is a central issue in boosting parser robustness.
Some systems use heuristics based on numbers, cap-
italisation and perhaps morphology to guess the cat-
egory of the unknown word (van Noord and Mal-
ouf, 2004), while others have focused on automati-
cally expanding the lexicon (Baldwin, 2005; Hock-
enmaier et al, 2002; O?Donovan et al, 2005). An-
other method, described in Section 4, uses external
resources such as part-of-speech (POS) tags to select
generic lexical entries for out-of-vocabulary words.
In all cases, we lose some of the depth of informa-
tion the hand-crafted lexicon would provide, but an
analysis is still produced, though possibly less than
fully specified.
The central position of these detailed lexicons
causes problems, not only of robustness, but also of
efficiency and ambiguity. Many words may have
five, six or more lexicon entries associated with
them, and this can lead to an enormous search space
for the parser. Various means of filtering this search
space have been attempted. Kiefer et al (1999) de-
scribes a method of filtering lexical items by specify-
ing and checking for required prefixes and particles
613
which is particularly effective for German, but also
applicable to English. Other research has looked at
using dependencies to restrict the parsing process
(Sagae et al, 2007), but the most well known fil-
tering method is supertagging. Originally described
by Bangalore and Joshi (1994) for use in LTAG pars-
ing, it has also been used very successfully for CCG
(Clark, 2002). Supertagging is the process of assign-
ing probable ?supertags? to words before parsing to
restrict parser ambiguity, where a supertag is a tag
that includes more specific information than the typ-
ical POS tags. The supertags used in each formal-
ism differ, being elementary trees in LTAG and CCG
categories for CCG. Section 3.2 describes an exper-
iment akin to supertagging for HPSG, where the su-
pertags are HPSG lexical types. Unlike elementary
trees and CCG categories, which are predominantly
syntactic categories, the HPSG lexical types contain
a lot of semantic information, as well as syntactic.
In the case study we describe here, the tools,
grammars and treebanks we use are taken from
work carried out in the DELPH-IN1 collaboration.
This research is based on using HPSG along with
Minimal Recursion Semantics (MRS: Copestake et
al. (2001)) as a platform to develop deep natural
language processing tools, with a focus on multi-
linguality. The grammars are designed to be bi-
directional (used for generation as well as parsing)
and so contain very specific linguistic information.
In this work, we focus on techniques to improve
parsing, not generation, but, as all the methods in-
volve pre-processing and do not change the gram-
mar itself, we do not affect the generation capabil-
ities of the grammars. We use two of the DELPH-
IN wide-coverage grammars: the English Resource
Grammar (ERG: Copestake and Flickinger (2000))
and a German grammar, GG (Mu?ller and Kasper,
2000; Crysmann, 2003). We also use the PET parser,
and the [incr tsdb()] system profiler and treebanking
tool (Oepen, 2001) for evaluation.
3 Parser Restriction
An exhaustive parser, such as PET, by default pro-
duces every parse licensed by the grammar. How-
ever, in many application scenarios, this is unnec-
essary and time consuming. The benefits of us-
1http://wiki.delph-in.net/
ing a deep parser with a lexicalised grammar are
the precision and depth of the analysis produced,
but this depth comes from making many fine dis-
tinctions which greatly increases the parser search
space, making parsing slow. By restricting the lexi-
cal items considered during parsing, we improve the
efficiency of a parser with a possible trade-off of los-
ing correct parses. For example, the noun phrase
reading of The dog barks is a correct parse, although
unlikely. By blocking the use of barks as a noun
in this case, we lose this reading. This may be an
acceptable trade-off in some applications that can
make use of the detailed information, but only if it
can be delivered in reasonable time. An example
of such an application is the real-time speech trans-
lation system developed in the Verbmobil project
(Wahlster, 2000), which integrated deep parsing re-
sults, where available, into its appointment schedul-
ing and travel planning dialogues. In these exper-
iments we look at two methods of restricting the
parser, first by using POS tags and then using lexical
types. To control the trade-off between efficiency
and precision, we vary which lexical items are re-
stricted according to a likelihood threshold from the
respective taggers. Only open class words are re-
stricted, since it is the gross distinctions between, for
instance, noun and verb that we would like to utilise.
Any differences between categories for closed class
words are more subtle and we feel the parser is best
left to make these distinctions without restriction.
The data set used for these experiments is the jh5
section of the treebank released with the ERG. This
text consists of edited written English in the domain
of Norwegian hiking instructions from the LOGON
project (Oepen et al, 2004).
3.1 Part of Speech Tags
We use TreeTagger (Schmid, 1994) to produce POS
tags and then open class words are restricted if the
POS tagger assigned a tag with a probability over
a certain threshold. A lower threshold will lead to
faster parsing, but at the expense of losing more cor-
rect parses. We experiment with various thresholds,
and results are shown in Table 1. Since a gold stan-
dard treebank for our data set was available, it was
possible to evaluate the accuracy of the parser. Eval-
uation of deep parsing results is often reported only
in terms of coverage (number of sentences which re-
614
Threshold Coverage Precision Time
gold 93.5% 92.2% N/A
unrestricted 93.3% 92.4% 0.67s
1.00 90.7% 91.9% 0.59s
0.98 88.8% 89.3% 0.49s
0.95 88.4% 89.5% 0.48s
0.90 86.4% 88.5% 0.44s
0.80 84.3% 87.0% 0.43s
0.60 81.5% 87.3% 0.39s
Table 1: Results obtained when restricting the parser lex-
icon according to the POS tag, where words are restricted
according to a threshold of POS probabilities.
ceive an analysis), because, since the hand-crafted
grammars are optimised for precision over cover-
age, the analyses are assumed to be correct. How-
ever, in this experiment, we are potentially ?dilut-
ing? the precision of the grammar by using external
resources to remove parses and so it is important that
we have some idea of how the accuracy is affected.
In the table, precision is the percentage of sentences
that, having produced at least one parse, produced a
correct parse. A parse was judged to be correct if it
exactly matched the gold standard tree in all aspects,
syntactic and semantic.
The results show quite clearly how the coverage
drops as the average parse time per sentence drops.
In hybrid applications that can back-off to less infor-
mative analyses, this may be a reasonable trade-off,
enabling detailed analyses in shorter times where
possible, and using the shallower analyses other-
wise.
3.2 Lexical Types
Another option for restricting the parser is to use the
lexical types used by the grammar itself, in a simi-
lar method to that described by Prins and van Noord
(2003). This could be considered a form of supertag-
ging as used in LTAG and CCG. Restricting by lex-
ical types should have the effect of reducing ambi-
guity further than POS tags can do, since one POS
tag could still allow the use of multiple lexical items
with compatible lexical types. On the other hand, it
could be considered more difficult to tag accurately,
since there are many more lexical types than POS
tags (almost 900 in the ERG) and less training data
is available.
Configuration Coverage Precision Time
gold 93.5% 92.2% N/A
unrestricted 93.3% 92.4% 0.67s
0.98 with POS 93.5% 91.9% 0.63s
0.95 with POS 93.1% 92.4% 0.48s
0.90 with POS 92.9% 92.3% 0.37s
0.80 with POS 91.8% 91.8% 0.31s
0.60 with POS 86.2% 93.5% 0.21s
0.98 no POS 92.9% 92.3% 0.62s
0.95 no POS 90.9% 91.0% 0.48s
0.90 no POS 87.7% 89.2% 0.42s
0.80 no POS 79.7% 84.6% 0.33s
0.60 no POS 67.0% 84.2% 0.23s
Table 2: Results obtained when restricting the parser lex-
icon according to the predicted lexical type, where words
are restricted according to a threshold of tag probabilities.
Two models, with and without POS tags as features, were
used.
While POS taggers such as TreeTagger are com-
mon, and there some supertaggers are available, no-
tably that of Clark and Curran (2007) for CCG,
no standard supertagger exists for HPSG. Conse-
quently, we developed a Maximum Entropy model
for supertagging using the OpenNLP implementa-
tion.2 Similarly to Zhang and Kordoni (2006), we
took training data from the gold?standard lexical
types in the treebank associated with ERG (in our
case, the July-07 version). For each token, we ex-
tracted features in two ways. One used features only
from the input string itself: four characters from the
beginning and end of the target word token, and two
words of context (where available) either side of the
target. The second used the features from the first,
along with POS tags given by TreeTagger for the
context tokens.
We held back the jh5 section of the treebank for
testing the Maximum Entropy model. Again, the
lexical items that were to be restricted were con-
trolled by a threshold, in this case the probabil-
ity given by the maximum entropy model. Table
2 shows the results achieved by these two models,
with the unrestricted results and the gold standard
provided for comparison.
Here we see the same trends of falling coverage
2http://maxent.sourceforge.net/
615
with falling time for both models, with the POS
tagged model consistently outperforming the word-
form model. To give a clearer picture of the com-
parative performance of all three experiments, Fig-
ure 1 shows how the results vary with time for both
models, and for the POS tag restricted experiment.
Here we can see that the coverage and precision of
the lexical type restriction experiment that uses the
word-form model is just above that of the POS re-
stricted one. However the POS tagged model clearly
outperforms both, showing minimal loss of coverage
or precision at a threshold which halved the average
parsing time. At the lowest parsing time, we see
that precision of the POS tagged model even goes
up. This can be explained by noting that coverage
here goes down, and obviously we are losing more
incorrect parses than correct parses.
This echoes the main result from Prins and van
Noord (2003), that filtering the lexical categories
used by the parser can significantly reduce parsing
time, while maintaining, or even improving, preci-
sion. The main differences between our method and
that of Prins and van Noord are the training data and
the tagging model. The key feature of their exper-
iment was the use of ?unsupervised? training data,
that is, the uncorrected output of their parser. In this
experiment, we used gold standard training data, but
much less of it (just under 200 000 words) and still
achieved a very good precision. It would be inter-
esting to see what amount of unsupervised parser
output we would require to achieve the same level
of precision. The other difference was the tagging
model, maximum entropy versus Hidden Markov
Model (HMM). We selected maximum entropy be-
cause Zhang and Kordoni (2006) had shown that
they got better results using a maximum entropy tag-
ger instead of a HMM one when predicting lexical
types, albeit for a slightly different purpose. It is not
possible to directly compare results between our ex-
periments and those in Prins and van Noord, because
of different languages, data sets and hardware, but it
is worth noting that parsing times are much lower in
our setup, perhaps more so than can be attributed to
4 years hardware improvement. While the range of
sentence lengths appears to be very similar between
the data sets, one possible reason for this could be
the very large number of lexical categories used in
their ALPINO system.
65
70
75
80
85
90
95
0.2 0.3 0.4 0.5 0.6 0.7
Average time per sentence (seconds)
Coverage
Gold standard
POS tags
3
3
3
33
3
3
Lexical types (no POS model)
+
+
+
+
+
+
Lexical types (with POS model)
2
2 2 2
2
2
Unrestricted
?
?
75
80
85
90
95
0.2 0.3 0.4 0.5 0.6 0.7
Average time per sentence (seconds)
Precision
Gold standard
POS tags
3 3
3
33
3
3
Lexical types (no POS model)
+ +
+
+
+
+
Lexical types (with POS model)
2
2 2 2 2
2
Unrestricted
?
?
Figure 1: Coverage and precision varying with time for
the three restriction experiments. Gold standard and un-
restricted results shown for comparison.
While this experiment is similar to that of Clark
and Curran (2007), it differs in that their supertag-
ger assign categories to every word, while we look
up every word in the lexicon and the tagger is used to
filter what the lexicon returns, only if the tagger con-
fidence is sufficiently high. As Table 2 shows, when
we use the tags for which the tagger had a low confi-
dence, we lose significant coverage. In order to run
as a supertagger rather than a filter, the tagger would
need to be much more accurate. While we can look
at multi-tagging as an option, we believe much more
training data would be needed to achieve a sufficient
level of tag accuracy.
Increasing efficiency is important for enabling
these heavily lexicalised grammars to bring the ben-
efits of their deep analyses to applications, but simi-
616
larly important is robustness. The following section
is aimed at addressing this issue of robustness, again
by using external information.
4 Unknown Word Handling
The lexical information available to the parser is
what makes the depth of the analysis possible, and
the default configuration of the parser uses an all-
or-nothing approach, where a parse is not produced
if all the lexical information is not available. How-
ever, in order to increase robustness, it is possible to
use underspecified lexical information where a fully
specified lexical item is not available. One method
of doing this, built in to the PET parser, is to use
POS tags to select generic lexical items, and hence
allow a (less than fully specified) parse to be built.
The six data sets used for these experiments were
chosen to give a range of languages and genres.
Four sets are English text: jh5 described in Sec-
tion 3; trec consisting of questions from TREC and
included in the treebanks released with the ERG;
a00 which is taken from the BNC and consists of
factsheets and newsletters; and depbank, the 700
sentences of the Briscoe and Carroll version of Dep-
Bank (Briscoe and Carroll, 2006) taken from the
Wall Street Journal. The last two data sets are Ger-
man text: clef700 consisting of German questions
taken from the CLEF competition and eiche564 a
sample of sentences taken from a treebank parsed
with the German HPSG grammar, GG and consist-
ing of transcribed German speech data concerning
appointment scheduling from the Verbmobil project.
Vital statistics of these data sets are described in Ta-
ble 3.
We used TreeTagger to POS tag the six data sets,
with the tagger configured to assign multiple tags,
where the probability of the less likely tags was at
least half that of the most likely tag. The data was
input using a PET input chart (PIC), which allows
POS tags to be assigned to each token, and then
parsed each with the PET parser.3 All English data
sets used the July-07 CVS version of the ERG and
the German sets used the September 2007 version
of GG. Unlike the experiments described in Sec-
tion 3, adding POS tags in this way will have no
effect on sentences which the parser is already able
3Subversion revision 384
Language
Number
of
Sentences
Ave.
Sentence
Length
jh5 English 464 14.2
trec English 693 6.9
a00 English 423 17.2
depbank English 700 21.5
clef German 700 7.5
eiche564 German 564 11.5
Table 3: Data sets used in input annotation experiments.
to parse. The POS tags will only be considered when
the parser has no lexicon entry for a given word, and
hence can only increase coverage. Results are shown
in Table 4, comparing the coverage over each set to
that obtained without using POS tags to handle un-
known words. Coverage here is defined as the per-
centage of sentences with at least one parse.
These results show very clearly one of the poten-
tial drawbacks of using a highly lexicalised gram-
mar formalism like HPSG: unknown words are one
of the main causes of parse failure, as quantified in
Baldwin et al (2004) and Nicholson et al (2008).
In the results here, we see that for jh5, trec and
eiche564, adding unknown word handling made al-
most no difference, since the grammars (specifically
the lexicons) have been tuned for these data sets. On
the other hand, over unseen texts, adding unknown
word handling made a dramatic difference to the
coverage. This motivates strategies like the POS tag
annotation used here, as well as the work on deep
lexical acquisition (DLA) described in Zhang and
Kordoni (2006) and Baldwin (2005), since no gram-
mar could ever hope to cover all words used within
a language.
As mentioned in Section 3, coverage is not the
only evaluation metric that should be considered,
particularly when adding potentially less precise in-
formation to the parsing process (in this case POS
tags). Since the primary effect of adding POS tags
is shown with those data sets for which we do not
have gold standard treebanks, evaluating accuracy
in this case is more difficult. However, in order to
give some idea of the effects on precision, a sample
of 100 sentences from the a00 data set was evaluated
for accuracy, for this and the following experiments.
617
In this instance, we found there was only a slight
drop in precision, where the original analyses had a
precision of 82% and the precision of the analyses
when POS tags were used was 80%.
Since the parser has the means to accept named
entity (NE) information in the input, we also ex-
perimented with using generic lexical items gener-
ated from NE data. We used SProUT (Becker et al,
2002) to tag the data sets and used PET?s inbuilt NE
handling mechanism to add NE items to the input,
associated with the appropriate word tokens. This
works slightly differently from the POS annotation
mechanism, in that NE items are considered by the
parser, even when the associated words are in the
lexicon. This has the effect of increasing the number
of analyses produced for sentences that already have
a full lexical span, but could also increase coverage
by enabling parses to be produced where there is no
lexical span, or where no parse was possible because
a token was not recognised as part of a name. In or-
der to isolate the effect of the NE data, we ran one
experiment where the input was annotated only with
the SProUT data, and another where the POS tags
were also added. These results are also in Table 4.
Again, we see coverage increases in the three un-
seen data sets, a00, depbank and clef, but not to the
same extent as the POS tags. Examining the re-
sults in more detail, we find that the increases come
almost exclusively from sentences without lexical
span, rather than in sentences where a token was
previously not recognised as part of a name. This
means that the NE tagger is operating almost like a
POS tagger that only tags proper nouns, and as the
POS tagger tags proper nouns quite accurately, we
find the NE tagger gives no benefit here. When ex-
amining the precision over our sample evaluation set
from a00, we find that using the NE data alone adds
no correct parses, while using NE data with POS
tags actually removes correct parses when compared
with POS alone, since the (in these cases, incorrect)
NE data is preferred over the POS tags. It is possible
that another named entity tagger would give better
results, and this may be looked at in future experi-
ments.
Other forms of external information might also be
used to increase lexical coverage. Zhang and Kor-
doni (2006) reported a 20% coverage increase over
baseline using a lexical type predictor for unknown
words, and so we explored this avenue. The same
maximum entropy tagger used in Section 3 was used
and each open class word was tagged with its most
likely lexical type, as predicted by the maximum en-
tropy model. Table 5 shows the results, with the
baseline and POS annotated results for comparison.
As with the previous experiments, we see a cover-
age increase in those data sets which are considered
unseen text for these grammars. Again it is clear
that the use of POS tags as features obviously im-
proves the maximum entropy model, since this sec-
ond model has almost 10% better coverage on our
unseen texts. However, lexical types do not appear
to be as effective for increasing lexical coverage as
the POS tags. One difference between the POS and
lexical type taggers is that the POS tagger could pro-
duce multiple tags per word. Therefore, for the next
experiment, we altered the lexical type tagger so it
could also produce multiple tags. As with the Tree-
Tagger configuration we used for POS annotation,
extra lexical type tags were produced if they were at
least half as probable as the most likely tag. A lower
probability threshold of 0.01 was set, so that hun-
dreds of tags of equal likelihood were not produced
in the case where the tagger was unable to make an
informed prediction. The results with multiple tag-
ging are also shown in Table 5.
The multiple tagging version gives a coverage in-
crease of between 2 and 10% over the single tag ver-
sion of the tagger, but, at least for the English data
sets, it is still less effective than straight-forward
POS tagging. For the German unseen data set, clef,
we do start getting above what the POS tagger can
achieve. This may be in part because of the features
used by the lexical type tagger ? German, being
a more morphologically rich language, may benefit
more from the prefix and suffix features used in the
tagger.
In terms of precision measured on our sample
evaluation set, the single tag version of the lexical
type tagger which used POS tag features achieved
a very good precision of 87% where, of all the extra
sentences that could now be parsed, only one did not
have a correct parse. In an application where preci-
sion is considered much more important than cover-
age, this would be a good method of increasing cov-
erage without loss of accuracy. The single tag ver-
sion that did not use POS tags in the model achieved
618
Baseline with POS NE only NE+POS
jh5 93.1% 93.3% 93.1% 93.3%
trec 97.1% 97.5% 97.4% 97.7%
a00 50.1% 83.9% 53.0% 85.8%
depbank 36.3% 76.9% 51.1% 80.4%
clef 22.0% 67.7% 42.3% 75.3%
eiche564 63.8% 63.8% 64.0% 64.0%
Table 4: Parser coverage with baseline using no unknown word handling and unknown word handling using POS tags,
SProUT named entity data as the only annotation, or SProUT tags in addition to POS annotation.
Single Lexical Types Multiple Lexical Types
Baseline POS -POS +POS -POS +POS
jh5 93.1% 93.3% 93.3% 93.3% 93.5% 93.5%
trec 97.1% 97.5% 97.3% 97.4% 97.3% 97.4%
a00 50.1% 83.9% 63.8% 72.6% 65.7% 78.5%
depbank 36.3% 76.9% 51.7% 64.4% 53.9% 69.7%
clef 22.0% 67.7% 59.9% 66.8% 69.7% 76.9%
eiche564 63.8% 63.8% 63.8% 63.8% 63.8% 63.8%
Table 5: Parser coverage using a lexical type predictor for unknown word handling. The predictor was run in single tag
mode, and then in multi-tag mode. Two different tagging models were used, with and without POS tags as features.
the same precision as with using only POS tags, but
without the same increase in coverage. On the other
hand, the multiple tagging versions, which at least
started approaching the coverage of the POS tag ex-
periment, dropped to a precision of around 76%.
From the results of Section 3, one might expect
that at least the lexical type method of handling un-
known words might at least lead to quicker parsing
than when using POS tags, however POS tags are
used differently in this situation. When POS tags
are used to restrict the parser, any lexicon entry that
unifies with the generic part-of-speech lexical cate-
gory can be used by the parser. That is, when the
word is restricted to, for example, a verb, any lexi-
cal item with one of the numerous more specific verb
categories can be used. In contrast, in these experi-
ments, the lexicon plays no part. The POS tag causes
one underspecified lexical item (per POS tag) to be
considered in parsing. While these underspecified
items may allow more analyses to be built than if
the exact category was used, the main contribution
to parsing time turned out to be the number of tags
assigned to each word, whether that was a POS tag
or a lexical type. The POS tagger assigned multiple
tags much less frequently than the multiple tagging
lexical type tagger and so had a faster average pars-
ing time. The single tagging lexical type tagger had
only slightly fewer tags assigned overall, and hence
was slightly faster, but at the expense of a signifi-
cantly lower coverage.
5 Conclusion
The work reported here shows the benefits that can
be gained by utilising external resources to anno-
tate parser input in highly lexicalised grammar for-
malisms. Even something as simple and readily
available (for languages likely to have lexicalised
grammars) as a POS tagger can massively increase
the parser coverage on unseen text. While annotat-
ing with named entity data or a lexical type supertag-
ger were also found to increase coverage, the POS
tagger had the greatest effect with up to 45% cover-
age increase on unseen text.
In terms of efficiency, POS tags were also shown
to speed up parsing by filtering unlikely lexicon
items, but better results were achieved in this case
by using a lexical type supertagger. Again encour-
aging the use of external resources, the supertagging
was found to be much more effective when POS tags
619
were used to train the tagging model, and in this con-
figuration, managed to halve the parsing time with
minimal effect on coverage or precision.
6 Further Work
A number of avenues of future research were sug-
gested by the observations made during this work.
In terms of robustness and increasing lexical cover-
age, more work into using lexical types for unknown
words could be explored. In light of the encourag-
ing results for German, one area to look at is the ef-
fect of different features for different languages. Use
of back-off models might also be worth considering
when the tagger probabilities are low.
Different methods of using the supertagger could
also be explored. The experiment reported here used
the single most probable type for restricting the lex-
icon entries used by the parser. Two extensions of
this are obvious. The first is to use multiple tags
over a certain threshold, by either inputting multi-
ple types as was done for the unknown word han-
dling, or by using a generic type that is compatible
with all the predicted types over a certain threshold.
The other possible direction to try is to not check
the predicted type against the lexicon, but to simply
construct a lexical item from the most likely type,
given a (high) threshold probability. This would be
similar to the CCG supertagging mechanism and is
likely to give generous speedups at the possible ex-
pense of precision, but it would be illuminating to
discover how this trade-off plays out in our setup.
References
Timothy Baldwin, Emily M. Bender, Dan Flickinger, Ara
Kim, and Stephan Oepen. 2004. Road-testing the
English Resource Grammar over the British National
Corpus. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC 2004), pages 2047?50, Lisbon, Portugal.
Timothy Baldwin. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of the
ACL-SIGLEX 2005 Workshop on Deep Lexical Acqui-
sition, pages 67?76, Ann Arbor, USA.
Srinivas Bangalore and Aravind K. Joshi. 1994. Dis-
ambiguation of super parts of speech (or supertags):
Almost parsing. In Proceedings of the 15th COLING
Conference, pages 154?160, Kyoto, Japan.
Markus Becker, Witold Drozdzynski, Hans-Ulrich
Krieger, Jakub Piskorski, Ulrich Scha?fer, and Feiyu
Xu. 2002. SProUT - Shallow Processing with Typed
Feature Structures and Unification. In Proceedings of
the International Conference on NLP (ICON 2002),
Mumbai, India.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalised statistical parser on the
PARC DepBank. In Proceedings of the 44th Annual
Meeting of the ACL, pages 41?48, Sydney, Australia.
Ulrich Callmeier. 2000. PET - a platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6(1):99?107.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark. 2002. Supertagging for combinatory cat-
egorical grammar. In Proceedings of the 6th Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Frameworks, pages 101?106, Venice, Italy.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of the Second conference on Language Resources
and Evaluation (LREC-2000), Athens, Greece.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of the
39th Annual Meeting of the ACL and 10th Conference
of the EACL (ACL-EACL 2001), Toulouse, France.
Berthold Crysmann. 2003. On the efficient implemen-
tation of German verb placement in HPSG. In Pro-
ceedings of RANLP 2003, pages 112?116, Borovets,
Bulgaria.
Julia Hockenmaier, Gann Bierner, and Jason Baldridge.
2002. Extending the coverage of a CCG system. Re-
search in Language and Computation.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and
Rob Malouf. 1999. A bag of useful techniques for ef-
ficient and robust parsing. In Proceedings of the 37th
Annual Meeting of the ACL, pages 473?480, Mary-
land, USA.
Stefan Mu?ller and Walter Kasper. 2000. HPSG analysis
of German. In Verbmobil: Foundations of Speech-to-
Speech Translation, pages 238?253. Springer, Berlin,
Germany.
Jeremy Nicholson, Valia Kordoni, Yi Zhang, Timothy
Baldwin, and Rebecca Dridan. 2008. Evaluating and
extending the coverage of HPSG grammars. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC 2008), Mar-
rakech, Morocco.
620
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef van
Genabith, and Andy Way. 2005. Large-scale induc-
tion and evaluation of lexical resources from the Penn-
II and Penn-III treebanks. Computational Linguistics,
31:pp 329?366.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik
Velldal, Dorothee Beermann, John Carroll, Dan
Flickinger, Lars Hellan, Janne Bondi Johannessen,
Paul Meurer, Torbj?rn Nordga?rd, and Victoria Rose?n.
2004. Soma? kapp-ete med trollet? Towards MRS-
based Norwegian?English machine translation. In
Proceedings of the 10th International Conference on
Theoretical and Methodological Issues in Machine
Translation, Baltimore, USA.
Stephan Oepen. 2001. [incr tsdb()] ? competence and
performance laboratory. User manual, Computational
Linguistics, Saarland University, Saarbru?cken, Ger-
many.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago, USA.
Robbert Prins and Gertjan van Noord. 2003. Reinforcing
parser preferences through tagging. Traitement Au-
tomatique des Langues, 44(3):121?139.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Proceedings of the 45th Annual Meeting of the ACL,
pages 624?631, Prague, Czech Republic.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
Gertjan van Noord and Robert Malouf. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In IJCNLP-04 Workshop Beyond Shallow Anal-
yses ? Formalisms and statistical modelling for deep
analyses.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer-
Verlag, Berlin.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
pages 275?280, Genoa, Italy.
621
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694?704,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Unsupervised Parse Selection for HPSG
Rebecca Dridan and Timothy Baldwin
Dept. of Computer Science and Software Engineering
University of Melbourne, Australia
rdridan@csse.unimelb.edu.au, tb@ldwin.net
Abstract
Parser disambiguation with precision gram-
mars generally takes place via statistical rank-
ing of the parse yield of the grammar using
a supervised parse selection model. In the
standard process, the parse selection model is
trained over a hand-disambiguated treebank,
meaning that without a significant investment
of effort to produce the treebank, parse selec-
tion is not possible. Furthermore, as treebank-
ing is generally streamlined with parse selec-
tion models, creating the initial treebank with-
out a model requires more resources than sub-
sequent treebanks. In this work, we show that,
by taking advantage of the constrained nature
of these HPSG grammars, we can learn a dis-
criminative parse selection model from raw
text in a purely unsupervised fashion. This al-
lows us to bootstrap the treebanking process
and provide better parsers faster, and with less
resources.
1 Introduction
Parsing with precision grammars is generally a two-
stage process: (1) the full parse yield of the preci-
sion grammar is calculated for a given item, often
in the form of a packed forest for efficiency (Oepen
and Carroll, 2000; Zhang et al, 2007); and (2) the
individual analyses in the parse forest are ranked us-
ing a statistical model (?parse selection?). In the do-
main of treebank parsing, the Charniak and Johnson
(2005) reranking parser adopts an analogous strat-
egy, except that ranking and pruning are incorpo-
rated into the first stage, and the second stage is
based on only the top-ranked parses from the first
stage. For both styles of parsing, however, parse se-
lection is based on a statistical model learned from a
pre-existing treebank associated with the grammar.
Our interest in this paper is in completely remov-
ing this requirement of parse selection on explicitly
treebanked data, ie the development of fully unsu-
pervised parse selection models.
The particular style of precision grammar we ex-
periment with in this paper is HPSG (Pollard and
Sag, 1994), in the form of the DELPH-IN suite
of grammars (http://www.delph-in.net/).
One of the main focuses of the DELPH-IN collab-
oration effort is multilinguality. To this end, the
Grammar Matrix project (Bender et al, 2002) has
been developed which, through a set of question-
naires, allows grammar engineers to quickly pro-
duce a core grammar for a language of their choice.
Bender (2008) showed that by using and expanding
on this core grammar, she was able to produce a
broad-coverage precision grammar of Wambaya in
a very short amount of time. However, the Gram-
mar Matrix can only help with the first stage of pars-
ing. The statistical model used in the second stage
of parsing (ie parse selection) requires a treebank to
learn the features, but as we explain in Section 2, the
treebanks are created by parsing, preferably with a
statistical model. In this work, we look at methods
for bootstrapping the production of these statistical
models without having an annotated treebank. Since
many of the languages that people are building new
grammars for are under-resourced, we can?t depend
on having any external information or NLP tools,
and so the methods we examine are purely unsuper-
vised, using nothing more than the grammars them-
694
selves and raw text. We find that, not only can we
produce models that are suitable for kick-starting the
treebanking process, but the accuracy of these mod-
els is comparable to parsers trained on gold standard
data (Clark and Curran, 2007b; Miyao and Tsujii,
2008), which have been successfully used in appli-
cations (Miyao et al, 2008).
2 The problem
The current method of training a parse selection
model uses the [incr tsdb()] treebanking mechanism
(Oepen, 2001) and works well for updating models
for mature grammars, although even for these gram-
mars, building a new model for a different domain
requires a time-consuming initial treebanking effort.
The treebanks used with DELPH-IN grammars are
dynamic treebanks (Oepen et al, 2004) created by
parsing text and having an annotator select the cor-
rect analysis (or discard all of them). The annotation
process involves making binary decisions based on
so-called parse discriminants (Carter, 1997). When-
ever the grammar is changed, the treebank can be
quickly updated by re-parsing and re-applying the
old annotation decisions. This treebanking process
not only produces gold standard trees, but also a set
of non-gold trees which provides the negative train-
ing data necessary for a discriminative maximum en-
tropy model.
The standard process for creating a parse selection
model is:
1. parse the training set, recording up to 500
highest-ranking parses for each sentence;
2. treebank the training set;
3. extract features from the gold and non-gold
parses;
4. learn feature weights using the TADM toolkit.1
(Malouf, 2002)
The useful training data from this process is the
parses from those sentences for which: more than
one parse was found; and at least one parse has been
annotated as correct. That is, there needs to be both
gold and non-gold trees for any sentence to be used
in training the discriminative model.
1http://tadm.sourceforge.net/
There are two issues with this process for new
grammars. Firstly, treebanking takes many person-
hours, and is hence both time-consuming and ex-
pensive. Complicating that is the second issue: N -
best parsing requires a statistical model. While it is
possible to parse exhaustively with no model, pars-
ing is much slower, since the unpacking of results
is time-consuming. Selective unpacking (Zhang et
al., 2007) speeds this up a great deal, but requires
a parse selection model. Treebanking is also much
slower when the parser must be run exhaustively,
since there are usually many more analyses to man-
ually discard.
This work hopes to alleviate both problems. By
producing a statistical model without requiring hu-
man treebanking, we can have a working and effi-
cient parser with less human effort. Even if the top-
1 parses this parser produces are not as accurate as
those trained on gold standard data, this model can
be used to produce the N -best analyses for the tree-
banker. Since our models are much better than ran-
dom selection, we can afford to reduce N and still
have a reasonable likelihood that the correct parse
is in that top N , making the job of the treebanker
much faster, and potentially leading to even better
parse selection accuracy based on semi-supervised
or fully-supervised parse selection.
3 Data and evaluation
Our ultimate goal is to use these methods for under-
resourced languages but, since there are no pre-
existing treebanks for these languages, we have no
means to measure which method produces the best
results. Hence, in this work, we experiment with
languages and grammars where we have gold stan-
dard data, in order to be able to evaluate the qual-
ity of the parse selection models. Since we have
gold-standard trained models to compare with, this
enables us to fully explore how these unsupervised
methods work, and show which methods are worth
trying in the more time-consuming and resource-
intensive future experiments on other languages. It
is worth reinforcing that the gold-standard data is
used for evaluation only, except in calculating the
supervised parse selection accuracy as an upper-
bound.
The English Resource Grammar (ERG:
695
Language Sentences Average Averagewords parses
Japanese 6769 10.5 49.6
English 4855 9.0 59.5
Table 1: Initial model training data, showing the average
word length per sentence, and also the ambiguity mea-
sured as the average number of parses found per sentence.
Flickinger (2002)) is an HPSG-based grammar
of English that has been under development for
many person years. In order to examine the
cross-lingual applicability of our methods, we also
use Jacy, an HPSG-based grammar of Japanese
(Siegel and Bender, 2002). In both cases, we use
grammar versions from the ?Barcelona? release,
from mid-2009.
3.1 Training Data
Both of our grammars come with statistical models,
and the parsed data and gold standard annotations
used to create these models are freely available. As
we are trying to simulate a fully unsupervised setup,
we didn?t want any influence from these earlier mod-
els. Hence, in our experiments we used the parsed
data from those sentences that received less than 500
parses and ignored any ranking, thus annulling the
effects of the statistical model. This led to a re-
duced data set, both in the number of sentences, and
in the fact that the more ambiguous sentences were
discarded, but it allows clean comparison between
different methods, without incorporating external in-
formation. The details of our training sets are shown
in Table 1,2 indicating that the sentence lengths are
relatively short, and hence the ambiguity (measured
as average parses per sentence) is low for both our
grammars. The ambiguity figures also suggest that
the Japanese grammar is more constrained (less am-
biguous) than the English grammar, since there are,
on average, more parses per sentence for English,
even with a lower average sentence length.
3.2 Test Data
The test data sets used throughout our experiments
are described in Table 2. The tc-006 data set is from
2Any sentences that do not have both gold and non-gold
analyses (ie, had no correct parse, only one parse, or none) are
not included in these figures.
Test Set Language Sentences Average Averagewords parses
tc-006 Japanese 904 10.7 383.9
jhpstgt English 748 12.8 4115.1
catb English 534 17.6 9427.3
Table 2: Test data, showing the average word length per
sentence, and also the ambiguity measured as the average
number of parses found per sentence. Note that the ambi-
guity figures for the English test sets are under-estimates,
since some of the longer sentences timed out before giv-
ing an analysis count.
the same Tanaka Corpus (Tanaka, 2001) which was
used for the Japanese training data. There is a wider
variety of treebanked data available for the English
grammar than for the Japanese. We use the jhp-
stgt data set, which consists of text from Norwegian
tourism brochures, from the same LOGON corpus
as the English training data (Oepen et al, 2004). In
order to have some idea of domain effects, we also
use the catb data set, the text of an essay on open-
source development.3 We see here that the sentences
are longer, particularly for the English data. Also,
since we are not artificially limiting the parse am-
biguity by ignoring those with 500 or more parses,
the ambiguity is much higher. This ambiguity figure
gives some indication of the difficulty of the parse
selection task. Again we see that the English sen-
tences are more ambiguous, much more in this case,
making the parse selection task difficult. In fact,
the English ambiguity figures are an under-estimate,
since some of the longer sentences timed out before
producing a parse count. This ambiguity can be a
function of the sentence length or the language it-
self, but also of the grammar. A more detailed and
informative grammar makes more distinctions, not
all of which are relevant for every analysis.
3.3 Evaluation
The exact match metric is the most common accu-
racy metric used in work with the DELPH-IN tool
set, and refers to the percentage of sentences for
which the top parse matched the gold parse in every
way. This is akin to the sentence accuracy that is oc-
casionally reported in the parsing literature, except
3The Cathedral and the Bazaar, by Eric Raymond.
Available from: http://catb.org/esr/writings/
cathedral-bazaar/
696
that it also includes fine-grained syntactico-semantic
features that are not often present in other parsing
frameworks. Exact match is a useful metric for parse
selection evaluation, but it is very blunt-edged, and
gives no way of evaluating how close the top parse
was to the gold standard. Since these are very de-
tailed analyses, it is possible to get one detail wrong
and still have a useful analysis. Hence, in addition
to exact match, we also use the EDMNA evalua-
tion defined by Dridan (2009). This is a predicate?
argument style evaluation, based on the semantic
output of the parser (MRS: Minimal Recursion Se-
mantics (Copestake et al, 2005)). This metric is
broadly comparable to the predicate?argument de-
pendencies of CCGBank (Hockenmaier and Steed-
man, 2007) or of the ENJU grammar (Miyao and
Tsujii, 2008), and also somewhat similar to the
grammatical relations (GR) of the Briscoe and Car-
roll (2006) version of DepBank. The EDMNA met-
ric matches triples consisting of predicate names and
the argument type that connects them.4
4 Initial Experiments
All of our experiments are based on the same basic
process: (1) for each sentence in the training data
described in Section 3.1, label a subset of analyses
as correct and the remainder as incorrect; (2) train
a model using the same features and learner as in
the standard process of Section 2; (3) parse the test
data using that model; and (4) evaluate the accuracy
of the top analyses. The differences lay in how the
?correct? analyses are selected each time. Each of
the following sections detail different methods for
nominating which of the (up to 500) analyses from
the training data should be considered pseudo-gold
for training the parse selection model.
4.1 Upperbound and baseline models
As a first step we evaluated each data set using an
upperbound and a baseline model. The upperbound
model in this case is the model trained with gold
standard annotations. These accuracy figures are
slightly lower than others found in the literature for
this data, since, to allow for comparison, we lim-
ited the training data to the sets described in Table 1.
4The full EDM metric also includes features such as tense
and aspect, but this is less comparable to the other metrics men-
tioned.
Test Set Exact EDMMatch Precision Recall F-score
tc-006 72.90 0.961 0.957 0.959
jhpstgt 48.07 0.912 0.908 0.910
catb 22.29 0.838 0.839 0.839
Table 3: Accuracy of the gold standard-based parse se-
lection model.
Test Set Exact EDMMatch Precision Recall F-score
tc-006 17.26 0.779 0.839 0.807
jhpstgt 12.48 0.720 0.748 0.734
catb 8.30 0.646 0.698 0.671
Table 4: Accuracy of the baseline model, trained on ran-
domly selected pseudo-gold analyses.
By throwing out those sentences with more than 500
parses, we exclude much of the data that is used in
the standard model and so our exact match figures
are slightly lower than might be expected.
For the baseline model, we used random selection
to select our gold analyses. For this experiment, we
randomly assigned one parse from each sentence in
the training data to be correct (and the remainder of
analyses as incorrect), and then used that ?gold stan-
dard? to train the model. Results for the upperbound
and baseline models are shown in Tables 3 and 4.
As expected, the results for Japanese are much
higher, since the lower ambiguity makes this an eas-
ier task. The catb test set results suffer, not only
from being longer, more ambiguous sentences, but
also because it is completely out of the domain of
the training data.
The exact match results from the random baseline
are approximately what one might expect, given the
respective ambiguity levels in Table 2. The EDM
figures are perhaps higher than might be expected
given random selection from the entire parse forest.
This results from using a precision grammar, with
an inbuilt notion of grammaticality, hence constrain-
ing the parser to only produce somewhat reasonable
parses, and creating a reasonably high baseline for
our parse selection experiments.
We also tried a separate baseline, eliminating the
parse selection model altogether, and using random
selection directly to select the top analysis. The ex-
act match and EDM precision results were slightly
lower than using random selection to train a model,
697
which may be due to the learner giving weight to
features that are common across the training data,
but the differences weren?t significant. Recall was
significantly lower when using random selection di-
rectly, due to the time outs caused by running with-
out a model. For this reason, we use the random
selection-based model results as our baseline for the
other unsupervised parse selection models, noting
that correctly identifying approximately three quar-
ters of the dependencies in the jhpstgt set, and over
80% when using the Japanese grammar, is a fairly
high baseline.
4.2 First attempts
As a first approach to unsupervised parse selection,
we looked at two heuristics to designate some num-
ber of the analyses as ?gold? for training. Both of
these heuristics looked independently at the parses
of each sentence, rather than calculating any num-
bers across the whole training set.
The first method builds on the observation from
the random selection-based model baseline exper-
iment that just giving weight to common features
could improve parser accuracy. In this case, we
looked at the edges of the parsing chart. For each
sentence, we counted the number of times an edge
was present in an analysis, and used that number
(normalised by the total number of times any edge
was used) as the edge weight. We then calculated
an analysis score by summing the edge weights of
all the edges in that analysis, and dividing by the
number of edges, to give an average edge weight for
an analysis. All analyses that had the best analysis
score for a sentence were designated ?gold?. Since it
was possible for multiple analyses to have the same
score, there could be multiple gold analyses for any
one sentence. If all the analyses had the same score,
this sentence could not be used as part of the train-
ing data. This method has the effect of selecting the
parse(s) most like all the others, by some definitions
the centroid of the parse forest. This has some rela-
tionship to the partial training method described by
Clark and Curran (2006), where the most frequent
dependencies where used to train a model for the
C&C CCG parser. In that case, however, the de-
pendencies were extracted only from analyses that
matched the gold standard supertag sequence, rather
than the whole parse forest.
Test Set Exact Match F-scoreEdges Branching Edges Branching
tc-006 17.48 21.35 0.815 0.822
jhpstgt 15.27 17.53 0.766 0.780
catb 9.36 10.86 0.713 0.712
Table 5: Accuracy for each test set, measured both as per-
centage of sentences that exactly matched the gold stan-
dard, and f-score over elementary dependencies.
The second heuristic we tried is one often used as
a baseline method: degree of right (or left) branch-
ing. In this instance, we calculated the degree of
branching as the number of right branches in a parse
divided by the number of left branches (and vice
versa for Japanese, a predominantly left-branching
language). In the same way as above, we designated
all parses with the best branching score as ?gold?.
Again, this is not fully discriminatory, and it was
common to get multiple gold trees for a given sen-
tence.
Table 5 shows the results for these two methods.
All the results show an improvement over the base-
line, with all but the F-score for the Edges method
of tc-006 being at a level of statistical significance.5
The only statistically significant difference between
the Edges and Branching methods is over the jhp-
stgt data set. While improvement over random is
encouraging, the results were still uninspiring and
so we moved on to slightly more complex methods,
described in the next section.
5 Supertagging Experiments
The term supertags was first used by Bangalore and
Joshi (1999) to describe fine-grained part of speech
tags which include some structural or dependency
information. In that original work, the supertags
were LTAG (Schabes and Joshi, 1991) elementary
trees, and they were used for the purpose of speed-
ing up parsing by restricting the allowable leaf types.
Subsequent work involving supertags has mostly fo-
cussed on this efficiency goal, but they can also be
used to inform parse selection. Dalrymple (2006)
and Blunsom (2007) both look at how discrimina-
tory a tag sequence is in filtering a parse forest. This
5All statistical significance tests in these experiments use the
computationally-intensive randomisation test described in Yeh
(2000), with p < 0.05.
698
work has shown that tag sequences can be success-
fully used to restrict the set of parses produced, but
generally are not discriminatory enough to distin-
guish a single best parse. Toutanova et al (2002)
present a similar exploration but also go on to in-
clude probabilities from a HMM model into the
parse selection model as features. There has also
been some work on using lexical probabilities for
domain adaptation of a model (Hara et al, 2007;
Rimell and Clark, 2008). In Dridan (2009), tag se-
quences from a supertagger are used together with
other factors to re-rank the top 500 parses from the
same parser and English grammar we use in this re-
search, and achieve some improvement in the rank-
ing where tagger accuracy is sufficiently high. We
use a similar method, one level removed, in that we
use the tag sequences to select the ?gold? parse(s)
that are then used to train a model, as in the previous
sections.
5.1 Gold Supertags
In order to test the viability of this method, we first
experimented using gold standard tags, extracted
from the gold standard parses. Supertags come in
many forms, depending on both the grammar for-
malism and the implementation. For this work, we
use HPSG lexical types (lextypes), the native word
classes in the grammars. These lextypes encode part
of speech and subcategorisation information, as well
as some more idiosyncratic features of words, such
as restrictions on preposition forms, mass/count dis-
tinctions and comparative versus superlative forms
of adjectives. As a few examples from the En-
glish grammar, v np le represents a basic transi-
tive verb, while n pp c-of le represents a count
noun that optionally takes a prepositional phrase
complement headed by of. The full definition of a
lextype consists of a many-featured AVM (attribute
value matrix), but the type names have been de-
liberately chosen to represent the main features of
each type. In the Dridan (2009) work, parse ranking
showed some improvement when morphological in-
formation was added to the tags. Hence, we also
look at more fine-grained tags constructed by con-
catenating appropriate morphological rules onto the
lextypes, as in v np le:past verb orule (ie a
simple transitive verb with past tense).
We used these tags by extracting the tag sequence
Test Set Exact Match F-scorelextype +morph lextype +morph
tc-006 40.49 41.37 0.903 0.903
jhpstgt 32.93 32.93 0.862 0.858
catb 20.41 19.85 0.798 0.794
Table 6: Accuracy using gold tag sequence compatibility
to select the ?gold? parse(s).
from the leaf types of all the parses in the forest,
marking as ?gold? any parse that had the same se-
quence as the gold standard parse and then training
the models as before. Table 6 shows the results from
parsing with models based on both the basic lextype
and the lextype with morphology. The results are
promising. They still fall well below training purely
on gold standard data (at least for the in-domain
sets), since the tag sequences are not fully discrimi-
natory and hence noise can creep in, but accuracy is
significantly better than the heuristic methods tried
earlier. This suggested that, at least with a reason-
ably accurate tagger, this was a viable strategy for
training a model. With no significant difference be-
tween the basic and +morph versions of the tag set,
we decided to use the basic lextypes as tags, since
a smaller tag set should be easier to tag with. How-
ever, we first had to train a tagger, without using any
gold standard data.
5.2 Unsupervised Supertagging
Research into unsupervised part-of-speech tagging
with a tag dictionary (sometimes called weakly su-
pervised POS tagging) has been going on for many
years (cf Merialdo (1994), Brill (1995)), but gener-
ally using a fairly small tag set. The only work we
know of on unsupervised tagging for the more com-
plex supertags is from Baldridge (2008), and more
recently, Ravi et al (2010a). In this work, the con-
straining nature of the (CCG) grammar is used to
mitigate the problem of having a much more am-
biguous tag set. Our method has a similar under-
lying idea, but the implementation differs both in
the way we extract the word-to-tag mappings, and
also how we extract and use the information from
the grammar to initialise the tagger model.
We chose to use a simple first-order Hidden
Markov Model (HMM) tagger, using the implemen-
699
tation of Dekang Lin,6 which re-estimates probabil-
ities, given an initial model, using the Baum-Welch
variant of the Expectation-Maximisation (EM) algo-
rithm. One possibility for an initial model was to ex-
tract the word-to-lextype mappings from the gram-
mar lexicon as Baldridge does, and make all starting
probabilities uniform. However, our lexicon maps
between lextypes and lemmas, rather than inflected
word forms, which is what we?d be tagging. That
is to say, from the lexicon we could learn that the
lemma walk can be tagged as v pp* dir le, but
we could not directly extract the fact that therefore
walked should also receive that tag.7 For this rea-
son, we decided it would be simplest to initialise
our probability estimates using the output of the
parser, feeding in only those tag sequences which
are compatible with analyses in the parse forest for
that item. This method takes advantage of the fact
that, because the grammars are heavily constrained,
the parse forest only contains viable tag sequences.
Since parsing without a model is slow, we restricted
the training set to those sentences shorter than a
specific word length (12 for English and 15 for
Japanese, since that was the less ambiguous gram-
mar and hence faster).
Table 7 shows how much parsed data this gave us.
From this parsed data we extracted tag-to-word and
tag-to-tag frequency counts from all parses for all
sentences, and used these frequencies to produce the
emission and transition probabilities, respectively.
The emission probabilities were taken directly from
the normalised frequency counts, but for the tran-
sition probabilities we allow for all possible transi-
tions, and add one to all counts before normalising.
This model we call our initial counts model. The
EM trained model is then produced by starting with
this initial model and running the Baum-Welch al-
gorithm using raw text sentences from the training
corpus.
5.3 Supertagging-based parse selection models
We use both the initial counts and EM trained
models to tag the training data from Table 1 and
then compared this with the extracted tag sequences
6Available from http://webdocs.cs.ualberta.
ca/?lindek/hmm.htm
7Morphological processing occurs before lexicon lookup in
the PET parser.
Japanese English
Parsed Sentences 9760 3770
Average Length 9.63 6.36
Average Parses 80.77 96.29
Raw Sentences 13500 9410
Raw Total Words 146053 151906
Table 7: Training data for the HMM tagger (both the
parsed data from which the initial probabilities were de-
rived, and the raw data which was used to estimated the
EM trained models).
Test Set
Exact Match F-score
Initial EM Initial EM
counts trained counts trained
tc-006 32.85 40.38 0.888 0.898
jhpstgt 26.29 24.04 0.831 0.827
catb 14.61 14.42 0.782 0.783
Table 8: Accuracy using tag sequences from a HMM tag-
ger to select the ?correct? parse(s). The initial counts
model was based on using counts from a parse forest
to approximate the emission and transition probabilities.
The EM trained model used the BaumWelch algorithm to
estimate the probabilities, starting from the initial counts
state.
used in the gold tag experiment. Since we could
no longer assume that our tag sequence would be
present within the extracted tag sequences, we used
the percentage of tokens from a parse whose lextype
matched our tagged sequence as the parse score.
Again, we marked as ?gold? any parse that had the
best parse score for each sentence, and trained a new
parse selection model.
Table 8 shows the results of parsing with these
models. The results are impressive, significantly
higher than all our previous unsupervised methods.
Interestingly, we note that there is no significant
difference between the initial count and EM trained
models for the English data. To explore why this
might be so, we looked at the tagger accuracy for
both models over the respective training data sets,
shown in Table 9. The results are not conclusive. For
both languages, the EM trained model is less accu-
rate, though not significantly so for Japanese. How-
ever, this insignificant tagger accuracy decrease for
Japanese produced a significant increase in parser
accuracy, while a more pronounced tagger accuracy
decrease had no significant effect on parser accuracy
in English.
700
Language Initial counts EM trained
Japanese 84.4 83.3
English 71.7 64.6
Table 9: Tagger accuracy over the training data, using
both the initial counts and the EM trained models.
There is much potential for further work in this
direction, experimenting with more training data or
more estimation iterations, or even looking at dif-
ferent estimators as suggested in Johnson (2007)
and Ravi et al (2010b). There is also the issue of
whether tag accuracy is the best measure for indicat-
ing potential parse accuracy. The Japanese parsing
results are already equivalent to those achieved us-
ing gold standard tags. It is possible that parsing ac-
curacy is reasonably insensitive to tagger accuracy,
but it is also possible that there is a better metric to
look at, such as tag accuracy over frequently con-
fused tags.
6 Discussion
The results of Table 8 show that, using no human
annotated data, we can get exact match results that
are almost half way between our random baseline
and our gold-standard-trained upperbound. EDM F-
scores of 90% and 83% over in-domain data com-
pare well with dependency-based scores from other
parsers, although a direct comparison is very diffi-
cult to do (Clark and Curran, 2007a; Miyao et al,
2007). It still remains to see whether this level of ac-
curacy is good enough to be useful. The main aim of
this work is to bootstrap the treebanking process for
new grammars, but to conclusively show the efficacy
of our methods in that situation requires a long-term
experiment that we are now starting, based on the
results we have here. Another possible use for these
methods was alluded to in Section 2: producing a
new model for a new domain.
Results at every stage have been much worse for
the catb data set, compared to the other jhpstgt En-
glish data set. While sentence length plays some
part, the major reason for this discrepancy was do-
main mismatch between the training and test data.
One method that has been successfully used for do-
main adaption in parsing is self-training (McClosky
et al, 2006). In this process, data from the new do-
main is parsed with the parser trained on the old do-
Source of ?Gold? Data Exact Match F-score
Random Selection 8.30 0.671
Supertags (initial counts) 14.61 0.782
Gold Standard 22.29 0.839
Self-training 15.92 0.791
Table 10: Accuracy results over the out-of-domain catb
data set, using the initial counts unsupervised model to
produce in-domain training data in a self-training set up.
The previous results are shown for easy comparison.
main, and then the top analyses of the parsed new
domain data are added to the training data, and the
parser is re-trained. This is generally considered a
semi-supervised method, since the original parser
is trained on gold standard data. In our case, we
wanted to test whether parsing data from the new do-
main using our unsupervised parse selection model
was accurate enough to still get an improvement us-
ing self-training for domain adaptation.
It is not immediately clear what one might con-
sider to be the ?domain? of the catb test set, since do-
main is generally very vaguely defined. In this case,
there was a limited amount of text available from
other essays by the same author.8 While the topics
of these essays vary, they all relate to the social side
of technical communities, and so we used this to rep-
resent in-domain data for the catb test set. It is, how-
ever, a fairly small amount of data for self-training,
being only around 1000 sentences. We added the re-
sults of parsing this data to the training set we used
to create the initial counts model and again retrained
and parsed. Table 10 shows the results. Previous re-
sults for the catb data set are given for comparison.
The results show that the completely unsuper-
vised parse selection method produces a top parse
that is at least accurate enough to be used in self-
training, providing a cheap means of domain adapta-
tion. In future work, we hope to explore this avenue
of research further.
7 Conclusions and Further Work
Comparing Tables 8 and 4, we can see that for both
English and Japanese, we are able to achieve parse
selection accuracy well above our baseline of a ran-
8http://www.catb.org/esr/writings/
homesteading/
701
dom selection-based model using only the informa-
tion available in the grammar and raw text. This
was in part because it is possible to extract a rea-
sonable tagging model from uncorrected parse data,
due to the constrained nature of these grammars.
These models will hopefully allow grammar engi-
neers to more easily build statistical models for new
languages, using nothing more than their new gram-
mar and raw text.
Since fully evaluating the potential for building
models for new languages is a long-term ongoing
experiment, we looked at a more short-term eval-
uation of our unsupervised parse selection meth-
ods: building models for new domains. A pre-
liminary self-training experiment, using our initial
counts tagger trained model as the starting point,
showed promising results for domain adaptation.
There are plenty of directions for further work
arising from these results. The issues surrounding
what makes a good tagger for this purpose, and how
can we best learn one without gold training data,
would be one possibly fruitful avenue for further
exploration. Another interesting slant would be to
investigate domain effects of the tagger. Previous
work has already found that training just a lexical
model on a new domain can improve parsing results.
Since the optimal tagger ?training? we saw here (for
English) was merely to read off frequency counts for
parsed data, it would be easy to retrain the tagger on
different domains. Alternatively, it would be inter-
esting so see how much difference it makes to train
the tagger on one set of data, and use that to tag a
model training set from a different domain. Other
methods of incorporating the tagger output could
also be investigated. Finally, a user study involv-
ing a grammar engineer working on a new language
would be useful to validate the results we found here
and confirm whether they are indeed helpful in boot-
strapping a new grammar.
Acknowledgements
This research was supported by Australian Research
Council grant no. DP0988242 and Microsoft Re-
search Asia.
References
Jason Baldridge. 2008. Weakly supervised supertagging
with grammar-informed initialization. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (Coling 2008), pages 57?64, Manch-
ester, UK.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proceedings of the Workshop on Grammar Engineer-
ing and Evaluation at the 19th International Con-
ference on Computational Linguistics, pages 8?14,
Taipei, Taiwan.
Emily M. Bender. 2008. Evaluating a crosslinguistic
grammar resource: A case study of Wambaya. In Pro-
ceedings of the 46th Annual Meeting of the ACL, pages
977?985, Columbus, USA.
Philip Blunsom. 2007. Structured Classification for
Multilingual Natural Language Processing. Ph.D.
thesis, Department of Computer Science and Software
Engineering, the University of Melbourne.
Eric Brill. 1995. Unsupervised learning of disambigua-
tion rules for part of speech tagging. In Proceedings
of the Third Workshop on Very Large Corpora, pages
1?13, Cambridge, USA.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalised statistical parser on the
PARC DepBank. In Proceedings of the 44th Annual
Meeting of the ACL and the 21st International Confer-
ence on Computational Linguistics, pages 41?48, Syd-
ney, Australia.
David Carter. 1997. The treebanker: a tool for super-
vised training of parsed corpora. In Proceedings of a
Workshop on Computational Environments for Gram-
mar Development and Linguistic Engineering, pages
9?15, Madrid, Spain.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Ann Arbor, USA.
Stephen Clark and James R. Curran. 2006. Partial train-
ing for a lexicalized-grammar parser. In Proceedings
of the Human Language Technology Conference of the
North American Chapter of the ACL (NAACL), pages
144?151, New York City, USA.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the ACL, pages 248?255, Prague, Czech Republic.
702
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An in-
troduction. Research on Language and Computation,
vol 3(no 4):pp 281?332.
Mary Dalrymple. 2006. How much can part-of-speech
tagging help parsing? Natural Language Engineering,
12(4):373?389.
Rebecca Dridan. 2009. Using lexical statistics to im-
prove HPSG parsing. Ph.D. thesis, Saarland Univer-
sity.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1?
17. Stanford: CSLI Publications.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an HPSG
parser. In Proceedings of the 10th International Con-
ference on Parsing Technology (IWPT 2007), pages
11?22, Prague, Czech Republic.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396, September.
Mark Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305,
Prague, Czech Republic.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conference on Natural Language
Learning, Taipei, Taiwan.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, pages 152?159, New York City, USA.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Computa-
tional Linguistics, 34(1):35?80.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of the GEAF 2007
Workshop, Palo Alto, California.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of the 46th Annual Meeting of the ACL,
pages 46?54, Columbus, USA.
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing - practical results. In
Proceedings of the 1st Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 162?169, Seattle, USA.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christopher D. Manning. 2004. LinGO redwoods. a
rich and dynamic treebank for HPSG. Journal of Re-
search in Language and Computation, 2(4):575?596.
Stephan Oepen. 2001. [incr tsdb()] ? competence and
performance laboratory. User manual, Computational
Linguistics, Saarland University, Saarbru?cken, Ger-
many.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago, USA.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010a.
Minimized models and grammar-informed initializa-
tion for supertagging with highly ambiguous lexicons.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 495?503,
Uppsala, Sweden.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010b. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 940?948, Beijing, China.
Laura Rimell and Stephen Clark. 2008. Adapting
a lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 475?484, Honolulu, USA.
Yves Schabes and Aravind K. Joshi. 1991. Parsing with
lexicalized tree adjoining grammar. In Masaru Tomita,
editor, Current Issues in Parsing Technology, chap-
ter 3, pages 25?48. Kluwer.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of japanese. In Proceedings of the 3rd
Workshop on Asian Language Resources and Interna-
tional Standardization. Coling 2002 Post-Conference
Workshop., Taipei, Taiwan.
Yasuhito Tanaka. 2001. Compilation of a multilingual
parallel corpus. In Proceedings of PACLING 2001,
pages 265?268, Kitakyushu, Japan.
Kristina Toutanova, Chistopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT2002), pages 253?263.
703
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbrcken, Germany.
Yi Zhang, Stephan Oepen, and John Carroll. 2007. Ef-
ficiency in unification-based n-best parsing. In Pro-
ceedings of the 10th international conference on pars-
ing technologies (IWPT 2007), pages 48?59, Prague,
Czech Republic.
704
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1201?1212,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Ubertagging: Joint segmentation and supertagging for English
Rebecca Dridan
Institutt for Informatikk
Universitetet i Oslo
rdridan@ifi.uio.no
Abstract
A precise syntacto-semantic analysis of En-
glish requires a large detailed lexicon with the
possibility of treating multiple tokens as a sin-
gle meaning-bearing unit, a word-with-spaces.
However parsing with such a lexicon, as in-
cluded in the English Resource Grammar, can
be very slow. We show that we can apply
supertagging techniques over an ambiguous
token lattice without resorting to previously
used heuristics, a process we call ubertagging.
Our model achieves an ubertagging accuracy
that can lead to a four to eight fold speed up
while improving parser accuracy.
1 Introduction and Motivation
Over the last decade or so, supertagging has become
a standard method for increasing parser efficiency
for heavily lexicalised grammar formalisms such as
LTAG (Bangalore and Joshi, 1999), CCG (Clark and
Curran, 2007) and HPSG (Matsuzaki et al, 2007).
In each of these systems, fine-grained lexical cate-
gories, known as supertags, are used to prune the
parser search space prior to full syntactic parsing,
leading to faster parsing at the risk of removing nec-
essary lexical items. Various methods are used to
configure the degree of pruning in order to balance
this trade-off.
The English Resource Grammar (ERG;
Flickinger (2000)) is a large hand-written HPSG-
based grammar of English that produces fine-
grained syntacto-semantic analyses. Given the high
level of lexical ambiguity in its lexicon, parsing
with the ERG should therefore also benefit from
supertagging, but while various attempts have
shown possibilities (Blunsom, 2007; Dridan et
al., 2008; Dridan, 2009), supertagging is still not
a standard element in the ERG parsing pipeline.
There are two main reasons for this. The first is
that the ERG lexicon does not assign simple atomic
categories to words, but instead builds complex
structured signs from information about lemmas and
lexical rules, and hence the shape and integration
of the supertags is not straightforward. Bangalore
and Joshi (2010) define a supertag as a primitive
structure that contains all the information about
a lexical item, including argument structure, and
where the arguments should be found. Within
the ERG, that information is not all contained in
the lexicon, but comes from different places. The
choice, therefore, of what information may be
predicted prior to parsing and how it should be
integrated into parsing is an open question.
The second reason that supertagging is not stan-
dard with ERG processing is one that is rarely con-
sidered when processing English, namely ambigu-
ous segmentation. In most mainstream English pars-
ing, the segmentation of parser input into tokens that
will become the leaves of the parse tree is consid-
ered a fixed, unambiguous process. While recent
work (Dridan and Oepen, 2012) has shown that pro-
ducing even these tokens is not a solved problem,
the issue we focus on here is the ambiguous map-
ping from these tokens to meaning-bearing units that
we might call words. Within the ERG lexicon are
many multi-token lexical entries that are sometimes
referred to as words-with-spaces. These multi-token
entries are added to the lexicon where the grammar-
ian finds that the semantics of a fixed expression is
non-compositional and has the distributional prop-
erties of other single word entries. Some examples
include an adverb-like all of a sudden, a preposition-
like for example and an adjective-like over and done
with. Each of these entries create an segmentation
ambiguity between treating the whole expression as
a single unit, or allowing analyses comprising en-
1201
tries triggered by the individual tokens. Previous su-
pertagging research using the ERG has either used
the gold standard tokenisation, hence making the
task artificially easier, or else tagged the individual
tokens, using various heuristics to apply multi-token
tags to single tokens. Neither approach has been
wholly satisfactory.
In this work we avoid the heuristic approaches
and learn a sequential classification model that can
simultaneously determine the most likely segmen-
tation and supertag sequences, a process we dub
ubertagging. We also experiment with more fine-
grained tag sets than have been previously used, and
find that it is possible to achieve a level of ubertag-
ging accuracy that can improve both parser speed
and accuracy for a precise semantic parser.
2 Previous Work
As stated above, supertagging has become a stan-
dard tool for particular parsing paradigms, but the
definitions of a supertag, the methods used to learn
them, and the way they are used in parsing varies
across formalisms. The original supertags were 300
LTAG elementary trees, predicted using a fairly sim-
ple trigram tagger that provided a configurable num-
ber of tags per token, since the tagger was not ac-
curate enough to make assigning a single tree vi-
able parser input (Bangalore and Joshi, 1999). The
C&C CCG parser uses a more complex Maximum
Entropy tagger to assign tags from a set of 425 CCG
lexical categories (Clark and Curran, 2007). They
also found it necessary to supply more than one tag
per token, and hence assign all tags that have a prob-
ability within a percentage ? of the most likely tag
for each token. Their standard parser configura-
tion uses a very restrictive ? value initially, relax-
ing it when no parse can be found. Matsuzaki et al
(2007) use a supertagger similar to the C&C tagger
alongside a CFG filter to improve the speed of their
HPSG parser, feeding sequences of single tags to the
parser until a parse is possible. As in the ERG, cate-
gory and inflectional information are separate in the
automatically-extracted ENJU grammar: their su-
pertag set consists of 1361 tags constructed by com-
bining lexical categories and lexical rules. Figure 1
shows examples of supertags from these three tag
sets, all describing the simple transitive use of lends.
S
NP0? VP
V
lends
NP1?
(a) LTAG
(S[dcl]\NP)/NP
(b) CCG
[NP.nom<V.bse>NP.acc]-singular3rd verb rule
(c) ENJU HPSG
Figure 1: Examples of supertags from LTAG, CCG
and ENJU HPSG, for the word lends.
The ALPINO system for parsing Dutch is the
closest in spirit to our ERG parsing setup, since
it also uses a hand-written HPSG-based grammar,
including multi-token entries in its lexicon. Prins
and van Noord (2003) use a trigram HMM tagger
to calculate the likelihood of up to 2392 supertags,
and discard those that are not within ? of the most
likely tag. For their multi-token entries, they as-
sign a constructed category to each token, so that
instead of assigning preposition to the expres-
sion met betrekking tot (?with respect to?), they
use (1,preposition), (2,preposition),
(3,preposition). Without these constructed
categories, they would only have 1365 supertags.
Most previous supertagging attempts with the
ERG have used the grammar?s lexical types, which
describe the coarse-grained part of speech, and the
subcategorisation of a word, but not the inflection.
Hence both lends and lent have a possible lexical
type v np* pp* to le, which indicates a verb,
with optional noun phrase and prepositional phrase
arguments, where the preposition has the form to.
The number of lexical types changes as the gram-
mar grows, and is currently just over 1000. Dridan
(2009) and Fares (2013) experimented with other tag
types, but both found lexical types to be the opti-
mal balance between predictability and efficiency.
Both used a multi-tagging approach dubbed selec-
tive tagging to integrate the supertags into the parser.
This involved only applying the supertag filter when
the tag probability is above a configurable threshold,
and not pruning otherwise.
For multi-token entries, both Blunsom (2007) and
1202
adverb adverb adverb
adverb ditto ditto
1,adverb 2,adverb 3,adverb
all in all
Figure 2: Options for tagging parts of the multi-
token adverb all in all separately.
Dridan (2009) assigned separate tags to each token,
with Blunsom (2007) assigning a special ditto tag
all but the initial token of a multi-token entry, while
Dridan (2009) just assigned the same tag to each to-
ken (leading to example in the expression for exam-
ple receiving p np i le, a preposition-type cate-
gory). Both of these solutions (demonstrated in Fig-
ure 2), as well as that of Prins and van Noord (2003),
in some ways defeat one of the purposes of treating
these expressions as fixed units. The grammarian,
by assigning the same category to, for example, all
of a sudden and suddenly, is declaring that these two
expressions have the same distributional properties,
the properties that a sequential classifier is trying to
exploit. Separating the tokens loses that informa-
tion, and introduces extra noise into the sequence
model.
Ytrest?l (2012) and Fares (2013) treat the multi-
entry tokens as single expressions for tagging, but
with no ambiguity. Ytrest?l (2012) manages this
by using gold standard tokenisation, which is, as he
states, the standard practice for statistical parsing,
but is an artificially simplified setup. Fares (2013) is
the only work we know about that has tried to predict
the final segmentation that the ERG produces. We
compare segmentation accuracy between our joint
model and his stand-alone tokeniser in Section 6.
Looking at other instances of joint segmentation
and tagging leads to work in non-whitespace sepa-
rated languages such as Chinese (Zhang and Clark,
2010) and Japanese (Kudo et al, 2004). While at a
high level, this work is solving the same problem,
the shape of the problems are quite different from
a data point of view. Regular joint morphological
analysis and segmentation has much greater ambi-
guity in terms of possible segmentations but, in most
cases, less ambiguity in terms of labelling than our
situation. This also holds for other lemmatisation
and morphological research, such as Toutanova and
Cherry (2009). While we drew inspiration from this
aj - i le
Foreign
v nger-tr dlr
v prp olr
v np*-pp* to le
lending
v pst olr
v - unacc le
increased
w period plr
av - s-vp-po le
as well.
p vp i le
as
w period plr
av - dg-v le
well.
Figure 3: A selection from the 70 lexitems instanti-
ated for Foreign lending increased as well.
related area, as well as from the speech recognition
field, differences in the relative frequency of obser-
vations and labels, as well as in segmentation ambi-
guity mean that conclusions found in these areas did
not always hold true in our problem space.
3 The Parser
The parsing environment we work with is the PET
parser (Callmeier, 2000), a unification-based chart
parser that has been engineered for efficiency with
precision grammars, and incorporates subsumption-
based ambiguity packing (Oepen and Carroll, 2000)
and statistical model driven selective unpacking
(Zhang et al, 2007). Parsing in PET is divided in
two stages. The first stage, lexical parsing, covers
everything from tokenising the raw input string to
populating the base of the parse chart with the ap-
propriate lexical items, ready for the second ? syn-
tactic parsing ? stage. In this work, we embed our
ubertagging model between the two stages. By this
point, the input has been segmented into what we
call internal tokens, which broadly means
splitting at whitespace and hyphens, and making
?s a separate token. These tokens are subject to a
morphological analysis component which proposes
possible inflectional and derivational rules based on
word form, and then are used in retrieving possible
lexical entries from the lexicon. The results of ap-
plying the appropriate lexical rules, plus affixation
rules triggered by punctuation, to the lexical entries
form a lexical item object, that for this work we dub
a lexitem.
Figure 3 shows some examples of lexitems
instantiated after the lexical parsing stage when
1203
analysing Foreign lending increased as well. The
pre-terminal labels on these subtrees are the lexical
types that have previously been used as supertags
for the ERG. For uninflected words, with no punctu-
ation affixed, the lexical type is the only element in
the lexitem, other than the word form (e.g. Foreign,
as). In this example, we also see lexitems with in-
flectional rules (v prp olr, v pst olr), deriva-
tional rules (v nger-tr dlr) and punctuation af-
fixation rules (w period plr).
These lexitems are put in to a chart, forming a
lexical lattice, and it is over this lattice that we apply
our ubertagging model, removing unlikely lexitems
before they are seen by the syntactic parsing stage.
4 The Data
The primary data sets we use in these experiments
are from the 1.0 version of DeepBank (Flickinger et
al., 2012), an HPSG annotation of the Wall Street
Journal text used for the Penn Treebank (PTB; Mar-
cus et al (1993)). The current version has gold stan-
dard annotations for approximately 85% of the first
22 sections. We follow the recommendations of the
DeepBank developers in using Sections 00?19 for
training, Section 20 (WSJ20) for development and
Section 21 (WSJ21) as test data.
In addition, we use two further sources of training
data: the training portions of the LinGO Redwoods
Treebank (Oepen et al, 2004), a steadily growing
collection of gold standard HPSG annotations in a
variety of domains; and the Wall Street Journal sec-
tion of the North American News Corpus (NANC),
which has been parsed, but not manually annotated.
This builds on observations by Prins and van Noord
(2003), Dridan (2009) and Ytrest?l (2012) that even
uncorrected parser output makes very good train-
ing data for a supertagger, since the constraints in
the parser lead to viable, if not entirely correct se-
quences. This allows us to use much larger training
sets than would be possible if we required manually
annotated data.
In final testing, we also include two further data
sets to observe how domain affects the contribution
of the ubertagging. These are both from the test
portion of the Redwoods Treebank: CatB, an es-
say about open-source software;1 and WeScience13,
1http://catb.org/esr/writings/
text from Wikipedia articles about Natural Language
Processing from the WeScience project (Ytrest?l et
al., 2009). Table 1 summarises the vital statistics of
the data we use.
With the focus on multi-token lexitems, it is in-
structive to see just how frequent they are. In terms
of type frequency, almost 10% of the approximately
38500 lexical entries in the current ERG lexicon
have more than one token in their canonical form.2
However, while this is a significant percentage of the
lexicon, they do not account for the same percentage
of tokens during parsing. An analysis of WSJ00:19
shows that approximately one third of the sentences
had at least one multi-token lexitem in the unpruned
lexical lattice, and in just under half of those, the
gold standard analysis included a multi-word entry.
That gives the multi-token lexitems the awkward
property of being rare enough to be difficult for a
statistical classifier to accurately detect (just under
1% of the leaves of gold parse trees contain multi-
ple tokens), but too frequent to ignore. In addition,
since these multi-token expressions have often been
distinguished because they are non-compositional,
failing to detect the multi-word usage can lead to
a disproportionately adverse effect on the semantic
analysis of the text.
5 Ubertagging Model
Our ubertagging model is very similar to a standard
trigram Hidden Markov Model (HMM), except that
the states are not all of the same length. Our states
are based on the lexitems in the lexical lattice pro-
duced by the lexical parsing stage of PET, and as
such, can be partially overlapping. We formalise this
be defining each state by its start position, end po-
sition, and tag. This turns out to make our model
equivalent to a type of Hidden semi-Markov Model
called a segmental HMM in Murphy (2002). In a
segmental HMM, the states are segments with a tag
(t) and a length in frames (l). In our setup, the
frames are the ERG internal tokens and the segments
are the lexitems, which are the potential candidates
cathedral-bazaar/ by Eric S. Raymond
2While the parser has mechanisms for handling words un-
known to the lexicon, with the current grammar these mecha-
nisms will never propose a multi-token lexitem, and so only the
multi-token entries explicitly in the lexicon will be recognised
as such.
1204
Lexitems
Data Set Source Use Gold? Trees All M-T
WSJ00:19 DeepBank 1.0 ?00?19 train yes 33783 661451 6309
Redwoods Redwoods Treebank train yes 39478 432873 6568
NANC LDC2008T15 train no 2185323 42376523 399936
WSJ20 DeepBank 1.0 ?20 dev yes 1721 34063 312
WSJ21 DeepBank 1.0 ?21 test yes 1414 27515 253
WeScience13 Redwoods Treebank test yes 802 11844 153
CatB Redwoods Treebank test yes 608 11653 115
Table 1: Test, development and training data used in these experiments. The final two columns show the
total number of lexitems used for training (All), as well as how many of those were multi-token lexitems
(M-T).
to become leaves of the parse tree. As indicated
above, the majority of segments (over 99%) will be
one frame long, but segments of up to four frames
are regularly seen in the training data.
A standard trigram HMM has a transition proba-
bility matrix A, where the elements Aijk represent
the probability P (k|ij), and an emission probability
matrix B whose elements Bjo record the probabili-
ties P (o|j). Given these matrices and a vector of ob-
served frames, O, the posterior probabilities of each
state at frame v are calculated as:3
P (qv = qy|O) =
?v(qy)?v(qy)
P (O)
(1)
where ?v(qy) is the forward probability at frame v,
given a current state qy (i.e. the probability of the
observation up to v, given the state):
?v(qy) ? P (O0:v|qv = qy) (2)
=
?
qx
?v(qxqy) (3)
?v(qxqy) = Bqyov
?
qw
?v?1(qwqx)Aqwqxqy (4)
?v(qy) is the backwards probability at frame v, given
a current state qy (the probability of the observation
3Since we will require per-state probabilities for integration
to the parser, we focus on the calculation of posterior probabil-
ities, rather than determing the single best path.
from v, given the state):
?v(qy) ? P (Ov+1:V |qv = qy) (5)
=
?
qx
?v(qxqy) (6)
?v(qxqy) =
?
qz
?v+1(qyqz)AqxqyqzBqzov+1 (7)
and the probability of the full observation sequence
is equal to the forward probability at the end of the
sequence, or the backwards probability at the start
of the sequence:
P (O) = ?V (?E?) = ?0(?S?) (8)
In implementation, our model varies only in what
we consider the previous or next states. While v still
indexes frames, qv now indicates a state that ends
with frame v, and we look forwards and backwards
to adjacent states, not frames, formally designated in
terms of l, the length of the state. Hence, we modify
equation (4):
?v(qxqy) = BqyOv?l+1:v
?
qw
?v?l(qwqx)Aqwqxqy
(9)
where v?l indexes the frame before the current state
starts, and hence we are summing over all states
that lead directly to our current state. An equivalent
modification to equation (7) gives:
?v(qxqy) =
?
qz
?Qn
?
l(qz)
?v+l(qyqz)AqxqyqzBqzOv+1:v+l
(10)
1205
Type Example #Tags
LTYPE v np-pp* to le 1028
INFL v np-pp* to le:v pas odlr 3626
FULL v np-pp* to le:v pas odlr:w period plr 21866
w period plr
v pas odlr
v np-pp* to le
recommended.
Figure 4: Possible tag types and their tag set size, with examples derived from the lexitem on the right.
where Qn is the set of states that start at v + 1 (i.e.,
the states immediately following the current state),
and l(qz) is the length of state qz .
We construct the transition and emission prob-
ability matrices using relative frequencies directly
observed from the training data, where we make
the simplifying assumption that P (qk|qiqj) ?
P (t(qk)|t(qi)t(qk)). Which is to say, while lex-
items with the same tag, but different length will
trigger distinct states with distinct emission proba-
bilities, they will have the same transition probabili-
ties, given the same proceeding tag.4 Even with our
large training set, some tag trigrams are rare or un-
seen. To smooth these probabilities, we use deleted
interpolation to calculate a weighted sum of the tri-
gram, bigram and unigram probabilities, since it has
been successfully used in effective PoS taggers like
the TnT tagger (Brants, 2000). Future work will
look more closely at the effects of different smooth-
ing methods.
6 Intrinsic Ubertag Evaluation
In order to develop and tune the ubertagging model,
we first looked at segmentation and tagging per-
formance in isolation over the development set.
We looked at three tag granularities: lexical types
(LTYPE) which have previously been shown to be the
optimal granularity for supertagging with the ERG,
inflected types (INFL) which encompass inflectional
and derivational rules applied to the lexical type, and
the full lexical item (FULL), which also includes af-
fixation rules used for punctuation handling. Exam-
ples of each tag type are shown in Figure 4, along
with the number of tags of each type seen in the
training data.
4Since the multi-token lexical entries are defined because
they have the same properties as the single token variants, there
is no reason to think the length of a state should influence the
tag sequence probability.
Segmentation Tagging
Tag Type F1 Sent. F1 Sent.
FULL 99.55 94.48 93.92 42.13
INFL 99.45 93.55 93.74 41.49
LTYPE 99.40 93.03 93.27 38.12
Table 2: Segmentation and tagging performance of
the best path found for each model, measured per
segment in terms of F1, and also as complete sen-
tence accuracy.
Single sequence results Table 2 shows the results
when considering the best path through the lattice.
In terms of segmentation, our sentence accuracy is
comparable to that of the stand-alone segmentation
performance reported by Fares et al (2013) over
similar data.5 In that work, the authors used a bi-
nary CRF classifier to label points between objects
they called micro-tokens as either SPLIT or NOS-
PLIT. The CRF classifier used a less informed in-
put (since it was external to the parser), but a much
more complex model, to produce a best single path
sentence accuracy of 94.06%. Encouragingly, this
level of segmentation performance was shown in
later work to produce a viable parser input (Fares,
2013).
Switching to the tagging results, we see that the
F1 numbers are quite good for tag sets of this size.6
The best tag accuracy seen for ERG LTYPE-style
tags was 95.55 in Ytrest?l (2012), using gold stan-
dard segmentation on a different data set. Dridan
(2009) experimented with a tag granularity similar
to our INFL (letype+morph) and saw a tag ac-
curacy of 91.51, but with much less training data.
From other formalisms, Kummerfeld et al (2010)
5Fares et al (2013) used a different section of an earlier ver-
sion of DeepBank, but with the same style of annotation.
6We need to measure F1 rather than tag accuracy here, since
the number of tokens tagged will vary according to the segmen-
tation.
1206
report a single tag accuracy of 95.91, with the
smaller CCG supertag set. Despite the promising
tag F1 numbers however, the sentence level accu-
racy still indicates a performance level unacceptable
for parser input. Comparing between tag types, we
see that, possibly surprisingly, the more fine-grained
tags are more accurately assigned, although the dif-
ferences are small. While instinctively a larger tag
set should present a more difficult problem, we find
that this is mitigated both by the sparse lexical lattice
provided by the parser, and by the extra constraints
provided by the more informative tags.
Multi-tagging results The multi-tagging methods
from previous supertagging work becomes more
complicated when dealing with ambiguous tokeni-
sation. Where, in other setups, one can compare
tag probabilities for all tags for a particular token,
that no longer holds directly when tokens can par-
tially overlap. Since ultimately, the parser uses lex-
items which encompass segmentation and tagging
information, we decided to use a simple integration
method, where we remove any lexitem which our
model assigns a probability below a certain thresh-
old (?). The effect of the different tag granular-
ities is now mediated by the relationship between
the states in the ubertagging lattice and the lexitems
in the parser?s lattice: for the FULL model, this is
a one-to-one relationship, but states from the mod-
els that use coarser-grained tags may affect multiple
lexitems. To illustrate this point, Figure 5 shows
some lexitems for the token forecast,, where there
are multiple possible analyses for the comma. A
FULL tag of v cp le:v pst olr:w comma plr
will select only lexitem (b), whereas an INFL tag
v cp le:v pst olr will select (b) and (c) and
the LTYPE tag v cp le picks out (a), (b) and (c).
On the other hand, where there is no ambiguity in
inflection or affixation, an LTYPE tag of n - mc le
may relate to only a single lexitem ((f) in this case).
Since we are using an absolute, rather than rel-
ative, threshold, the number needs to be tuned for
each model7 and comparisons between models can
only be made based on the effects (accuracy or prun-
ing power) of the threshold. Table 3 shows how
a selection of threshold values affect the accuracy
7A tag set size of 1028 will lead to higher probabilities in
general than a tag set size of 21866.
w comma-nf plr
v cp le
forecast,
(a)
w comma-nf plr
v pst olr
v cp le
forecast,
(b)
w comma plr
v pst olr
v cp le
forecast,
(c)
w comma plr
v pst olr
v np le
forecast,
(d)
w comma plr
v pas olr
v np le
forecast,
(e)
w comma plr
n - mc le
forecast,
(f)
Figure 5: Some of the lexitems triggered by fore-
cast, in Despite the gloomy forecast, profits were up.
Tag Lexitems
Type ? Acc. Kept Ave.
FULL 0.00001 99.71 41.6 3.34
FULL 0.0001 99.44 33.1 2.66
FULL 0.001 98.92 25.5 2.05
FULL 0.01 97.75 19.4 1.56
INFL 0.0001 99.67 37.9 3.04
INFL 0.001 99.25 29.0 2.33
INFL 0.01 98.21 21.6 1.73
INFL 0.02 97.68 19.7 1.58
LTYPE 0.0002 99.75 66.3 5.33
LTYPE 0.002 99.43 55.0 4.42
LTYPE 0.02 98.41 43.5 3.50
LTYPE 0.05 97.54 39.4 3.17
Table 3: Accuracy and ambiguity after pruning lex-
items in WSJ20, at a selection of thresholds ? for
each model. Accuracy is measured as the percent-
age of gold lexitems remaining after pruning, while
ambiguity is presented both as a percentage of lex-
items kept, and the average number of lexitems per
initial token still remaining.
1207
 0.975
 0.98
 0.985
 0.99
 0.995
 1
 1  2  3  4  5  6  7  8  9
Ac
cu
rac
y
Average lexitems per initial token
Tag accuracy versus ambiguity
FULLINFLLTYPE
Figure 6: Accuracy over gold lexitems versus aver-
age lexitems per initial token over the development
set, for each of the different ubertagging models.
and pruning impact of our different disambiguation
models, where the accuracy is measured in terms
of percentage of gold lexitems retained. The prun-
ing effect is given both as percentage of lexitems
retained after pruning, and average number of lex-
items per initial token.8 Comparison between the
different models can be more easily made by exam-
ining Figure 6. Here we see clearly that the LTYPE
model provides much less pruning for any given
level of lexitem accuracy, while the performance of
the other models is almost indistinguishable.
Analysis The current state-of-the-art POS tagging
accuracy (using the 45 tags in the PTB) is approx-
imately 97.5%. The most restrictive ? value we
report for each model was selected to demonstrate
that level of accuracy, which we can see would lead
to pruning over 80% of lexitems when using FULL
tags, an average of 1.56 tags per token. While
this level of accuracy has been sufficient for statisti-
cal treebank parsing, previous work (Dridan, 2009)
has shown that tag accuracy cannot directly predict
parser performance, since errors of different types
can have very different effects. This is hard to
quantify without parsing, but we made a qualitative
analysis at the lexitems that were incorrectly being
8The average number of lexitems per token for the unre-
stricted parser is 8.03, although the actual assignment is far from
uniform, with up to 70 lexitems per token seen for the very am-
biguous tokens.
pruned. For all models, the most difficult lexitems
to get correct were proper nouns, particular those
that are also used as common nouns (e.g. Bank, Air-
line, Report). While capitalisation provides a clue
here, it is not always deterministic, particularly since
the treebank incorporates detailed decisions regard-
ing the distinction between a name and a capitalised
common noun that require real world knowledge,
and are not necessarily always consistent. Almost
two thirds of the errors made by the FULL and INFL
models are related to these decisions, but only about
40% for the LTYPE model. The other errors are pre-
dominately over noun and verb type lexitems, as the
open classes, with the only difference between mod-
els being that the FULL model seems marginally bet-
ter at classifying verbs. The next section describes
the end-to-end setup and results when parsing the
development set.
7 Parsing
With encouraging ubertagging results, we now take
the next step and evaluate the effect on end-to-end
parsing. Apart from the issue of different error types
having unpredictable effects, there are two other
factors that make the isolated ubertagging results
only an approximate indication of parsing perfor-
mance. The first confounding factor is the statisti-
cal parsing disambiguation model. To show the ef-
fect of ubertagging in a realistic configuration, we
only evaluate the first analysis that the parser returns.
That means that when the unrestricted parser does
not rank the gold analysis first, errors made by our
model may not be visible, because we would never
see the gold analysis in any case. On the other hand,
it is possible to improve parser accuracy by pruning
incorrect lexitems that were in a top ranked, non-
gold analysis.
The second new factor that parser integration
brings to the picture is the effect of resource limi-
tations. For reasons of tractability, PET is run with
per sentence time and memory limits. For treebank
creation, these limits are quite high (up to four min-
utes), but for these experiments, we set the time-
out to a more practical 60 seconds and the memory
limit to 2048Mb. Without lexical pruning, this leads
to approximately 3% of sentences not receiving an
analysis. Since the main aim of ubertagging is to in-
1208
Tag F1
Type ? Lexitem Bracket Time
No Pruning 94.06 88.58 6.58
FULL 0.00001 95.62 89.84 3.99
FULL 0.0001 95.95 90.09 2.69
FULL 0.001 95.81 89.88 1.34
FULL 0.01 94.19 88.29 0.64
INFL 0.0001 96.10 90.37 3.45
INFL 0.001 96.14 90.33 1.78
INFL 0.01 95.07 89.27 0.84
INFL 0.02 94.32 88.49 0.64
LTYPE 0.0002 95.37 89.63 4.73
LTYPE 0.002 96.03 90.20 2.89
LTYPE 0.02 95.04 89.04 1.23
LTYPE 0.05 93.36 87.26 0.88
Table 4: Lexitem and bracket F1over WSJ20, with
average per sentence parsing time in seconds.
crease efficiency, we would expect to regain at least
some of these unanalysed sentences, even when a
lexitem needed for the gold analysis has been re-
moved.
Table 4 shows the parsing results at the same
threshold values used in Table 3. Accuracy is cal-
culated in terms of F1 both over lexitems, and PAR-
SEVAL-style labelled brackets (Black et al, 1991),
while efficiency is represented by average parsing
time per sentence. We can see here that an ubertag-
ging F1 of below 98 (cf. Table 3) leads to a drop
in parser accuracy, but that an ubertagging perfor-
mance of between 98 and 99 can improve parser F1
while also achieving speed increases up to 8-fold.
From the table we confirm that, contrary to ear-
lier pipeline supertagging configurations, tags of a
finer granularity than LTYPE can deliver better per-
formance, both in terms of accuracy and efficiency.
Again, comparing graphically in Figure 7 gives a
clearer picture. Here we have graphed labelled
bracket F1 against parsing time for the full range of
threshold values explored, with the unpruned pars-
ing results indicated by a cross.
From this figure, we see that the INFL model, de-
spite being marginally less accurate when measured
in isolation, leads to slightly more accurate parse re-
sults than the FULL model at all levels of efficiency.
Looking at the same graph for different samples
of the development set (not shown) shows some
86
87
88
89
90
 0  1  2  3  4  5  6  7
F1
Time per sentence
Parser accuracy versus efficiency
FULLINFLLTYPEUnrestrictedSelected configuration
Figure 7: Labelled bracket F1 versus parsing time
per sentence over the development set, for each of
the different ubertagging models. The cross indi-
cates unpruned performance, while the circle pin-
points the configuration we chose for the final test
runs.
variance in which threshold value gives the best F1,
but the relative differences and basic curve shape re-
mains the same. From these different views, using
the guideline of maximum efficiency without harm-
ing accuracy we selected our final configuration: the
INFL model with a threshold value of 0.001 (marked
with a circle in Figure 7). On the development set,
this configuration leads to a 1.75 point improvement
in F1 in 27% of the parsing time.
8 Final Results
Table 5 shows the results obtained when parsing us-
ing the configuration selected on the development
set, over our three test sets. The first, WSJ21 is from
the same domain as the development set. Here we
see that the effect over the WSJ21 set fairly closely
mirrored that of the development set, with an F1 in-
crease of 1.81 in 29% of the parsing time.
The Wikipedia domain of our WeScience13 test
set, while very different to the newswire domain of
the development set could still be considered in do-
main for the parsing and ubertagging models, since
there is Wikipedia data in the training sets. With
an average sentence length of 15.18 (compared to
18.86 in WSJ21), the baseline parsing time is faster
than for WSJ21, and the speedup is not quite as large
1209
Baseline Pruned
Data Set F1 Time F1 Time
WSJ21 88.12 6.06 89.93 1.77
WeScience13 86.25 4.09 87.14 1.48
CatB 86.31 5.00 87.11 1.78
Table 5: Parsing accuracy in terms of labelled
bracket F1 and average time per sentence when pars-
ing the test sets, without pruning, and then with lex-
ical pruning using the INFL model with a threshold
of 0.001.
but still welcome, at 36% of the baseline time. The
increase is accuracy is likewise smaller (due to less
issues with resource exhaustion in the baseline), but
as our primary goal is to not harm accuracy, the re-
sults are pleasing.
The CatB test set is the standard out-of-domain
test for the parser, and is also out of domain for
the ubertagging model. The average sentence length
is not much below that of WSJ21, at 18.61, but
the baseline parsing speed is still noticeably faster,
which appears to be a reflection of greater structural
ambiguity in the newswire text. We still achieve a re-
duction in parsing time to 35% of the baseline, again
with a small improvement in accuracy.
The across-the-board performance improvement
on all our test sets suggests that, while tuning the
pruning threshold could help, it is a robust parame-
ter that can provide good performance across a va-
riety of domains. This means that we finally have a
robust supertagging setup for use with the ERG that
doesn?t require heuristic shortcuts and can be reli-
ably applied in general parsing.
9 Conclusions and Outlook
In this work we have demonstrated a lexical disam-
biguation process dubbed ubertagging that can as-
sign fine-grained supertags over an ambiguous to-
ken lattice, a setup previously ignored for English. It
is the first completely integrated supertagging setup
for use with the English Resource Grammar, which
avoids the previously necessary heuristics for deal-
ing with ambiguous tokenisation, and can be ro-
bustly configured for improved performance without
loss of accuracy. Indeed, by learning a joint segmen-
tation and supertagging model, we have been able
to achieve usefully high tagging accuracies for very
fine-grained tags, which leads to potential parser
speedups of between 4 and 8 fold.
Analysis of the tagging errors still being made
have suggested some possibly avoidable inconsis-
tencies in the grammar and treebank, which have
been fed back to the developers, hopefully leading
to even better results in the future.
In future work, we will investigate more advanced
smoothing methods to try and boost the ubertagging
accuracy. We also intend to more fully explore the
domain adaptation potentials of the lexical model
that have been seen in other parsing setups (see
Rimell and Clark (2008) for example), as well as ex-
amine the limits on the effects of more training data.
Finally, we would like to explore just how much the
statistic properties of our data dictate the success of
the model by looking at related problems like mor-
phological analysis of unsegmented languages such
as Japanese.
Acknowledgements
I am grateful to my colleagues from the Oslo Lan-
guage Technology Group and the DELPH-IN con-
sortium for many discussions on the issues involved
in this work, and particularly to Stephan Oepen who
inspired the initial lattice tagging idea. Thanks also
to three anonymous reviewers for their very con-
structive feedback which improved the final ver-
sion. Large-scale experimentation and engineering
is made possible though access to the TITAN high-
performance computing facilities at the University
of Oslo, and I am grateful to the Scientific Com-
putating staff at UiO, as well as to the Norwegian
Metacenter for Computational Science and the Nor-
wegian tax payer.
1210
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237 ? 265.
Srinavas Bangalore and Aravind Joshi, editors. 2010.
Supertagging: Using Complex Lexical Descriptions in
Natural Language Processing. The MIT Press, Cam-
bridge, US.
Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Phil Harrison, Don Hin-
dle, Robert Ingria, Fred Jelinek, Judith Klavans, Mark
Liberman, Mitch Marcus, S. Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of the Workshop on
Speech and Natural Language, page 306 ? 311, Pacific
Grove, USA.
Philip Blunsom. 2007. Structured Classification for
Multilingual Natural Language Processing. Ph.D.
thesis, Department of Computer Science and Software
Engineering, University of Melbourne.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the Sixth Conference
on Applied Natural Language Processing ANLP-2000,
page 224 ? 231, Seattle, USA.
Ulrich Callmeier. 2000. PET. A platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6(1):99 ? 108, March.
Stephen Clark and James R. Curran. 2007. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Meeting of the Asso-
ciation for Computational Linguistics, page 248 ? 255,
Prague, Czech Republic.
Rebecca Dridan and Stephan Oepen. 2012. Tokeniza-
tion. Returning to a long solved problem. A survey,
contrastive experiment, recommendations, and toolkit.
In Proceedings of the 50th Meeting of the Association
for Computational Linguistics, page 378 ? 382, Jeju,
Republic of Korea, July.
Rebecca Dridan, Valia Kordoni, and Jeremy Nicholson.
2008. Enhancing performance of lexicalised gram-
mars. page 613 ? 621.
Rebecca Dridan. 2009. Using lexical statistics to im-
prove HPSG parsing. Ph.D. thesis, Department of
Computational Linguistics, Saarland University.
Murhaf Fares, Stephan Oepen, and Yi Zhang. 2013. Ma-
chine learning for high-quality tokenization. Replicat-
ing variable tokenization schemes. In Computational
Linguistics and Intelligent Text Processing, page 231 ?
244. Springer.
Murhaf Fares. 2013. ERG tokenization and lexical cat-
egorization: a sequence labeling approach. Master?s
thesis, Department of Informatics, University of Oslo.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank. A dynamically annotated treebank of the
Wall Street Journal. In Proceedings of the 11th Inter-
national Workshop on Treebanks and Linguistic Theo-
ries, page 85 ? 96, Lisbon, Portugal. Edic?o?es Colibri.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1):15 ? 28.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to japanese
morphological analysis. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, page 230 ? 237.
Jonathan K. Kummerfeld, Jessika Roesner, Tim Daw-
born, James Haggerty, James R. Curran, and Stephen
Clark. 2010. Faster parsing by supertagger adapta-
tion. In Proceedings of the 48th Meeting of the Asso-
ciation for Computational Linguistics, page 345 ? 355,
Uppsala, Sweden.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pora of English: The Penn Treebank. Computational
Linguistics, 19:313 ? 330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertag-
ging and CFG-filtering. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI 2007), page 1671 ? 1676, Hyderabad, India.
Kevin P. Murphy. 2002. Hidden semi-Markov models
(HSMMs).
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing. Practical results. In
Proceedings of the 1st Meeting of the North American
Chapter of the Association for Computational Linguis-
tics, page 162 ? 169, Seattle, WA, USA.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG. Re-
search on Language and Computation, 2(4):575 ? 596.
Robbert Prins and Gertjan van Noord. 2003. Reinforcing
parser preferences through tagging. Traitement Au-
tomatique des Langues, 44(3):121 ? 139.
Laura Rimell and Stephen Clark. 2008. Adapting
a lexicalized-grammar parser to contrasting domains.
page 475 ? 484.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the 47th Meeting of the
Association for Computational Linguistics, page 486 ?
494, Singapore.
Gisle Ytrest?l. 2012. Transition-based Parsing for
Large-scale Head-Driven Phrase Structure Gram-
mars. Ph.D. thesis, Department of Informatics, Uni-
versity of Oslo.
1211
Gisle Ytrest?l, Stephan Oepen, and Dan Flickinger.
2009. Extracting and annotating Wikipedia sub-
domains. In Proceedings of the 7th International
Workshop on Treebanks and Linguistic Theories, page
185 ? 197, Groningen, The Netherlands.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, page 843 ? 852, Cambridge, MA,
USA.
Yi Zhang, Stephan Oepen, and John Carroll. 2007. Ef-
ficiency in unification-based n-best parsing. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, page 48 ? 59, Prague, Czech Repub-
lic, July.
1212
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 378?382,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Tokenization: Returning to a Long Solved Problem
A Survey, Contrastive Experiment, Recommendations, and Toolkit
Rebecca Dridan & Stephan Oepen
Institutt for Informatikk, Universitetet i Oslo
{rdridan |oe}@ifi.uio.no
Abstract
We examine some of the frequently disre-
garded subtleties of tokenization in Penn Tree-
bank style, and present a new rule-based pre-
processing toolkit that not only reproduces the
Treebank tokenization with unmatched accu-
racy, but also maintains exact stand-off point-
ers to the original text and allows flexible con-
figuration to diverse use cases (e.g. to genre-
or domain-specific idiosyncrasies).
1 Introduction?Motivation
The task of tokenization is hardly counted among the
grand challenges of NLP and is conventionally in-
terpreted as breaking up ?natural language text [...]
into distinct meaningful units (or tokens)? (Kaplan,
2005). Practically speaking, however, tokeniza-
tion is often combined with other string-level pre-
processing?for example normalization of punctua-
tion (of different conventions for dashes, say), dis-
ambiguation of quotation marks (into opening vs.
closing quotes), or removal of unwanted mark-up?
where the specifics of such pre-processing depend
both on properties of the input text as well as on as-
sumptions made in downstream processing.
Applying some string-level normalization prior to
the identification of token boundaries can improve
(or simplify) tokenization, and a sub-task like the
disambiguation of quote marks would in fact be hard
to perform after tokenization, seeing that it depends
on adjacency to whitespace. In the following, we
thus assume a generalized notion of tokenization,
comprising all string-level processing up to and in-
cluding the conversion of a sequence of characters
(a string) to a sequence of token objects.1
1Obviously, some of the normalization we include in the to-
kenization task (in this generalized interpretation) could be left
to downstream analysis, where a tagger or parser, for example,
could be expected to accept non-disambiguated quote marks
(so-called straight or typewriter quotes) and disambiguate as
Arguably, even in an overtly ?separating? lan-
guage like English, there can be token-level ambi-
guities that ultimately can only be resolved through
parsing (see ? 3 for candidate examples), and indeed
Waldron et al (2006) entertain the idea of down-
stream processing on a token lattice. In this article,
however, we accept the tokenization conventions
and sequential nature of the Penn Treebank (PTB;
Marcus et al, 1993) as a useful point of reference?
primarily for interoperability of different NLP tools.
Still, we argue, there is remaining work to be done
on PTB-compliant tokenization (reviewed in? 2),
both methodologically, practically, and technologi-
cally. In ? 3 we observe that state-of-the-art tools
perform poorly on re-creating PTB tokenization, and
move on in ? 4 to develop a modular, parameteri-
zable, and transparent framework for tokenization.
Besides improvements in tokenization accuracy and
adaptability to diverse use cases, in ? 5 we further
argue that each token object should unambiguously
link back to an underlying element of the original
input, which in the case of tokenization of text we
realize through a notion of characterization.
2 Common Conventions
Due to the popularity of the PTB, its tokenization
has been a de-facto standard for two decades. Ap-
proximately, this means splitting off punctuation
into separate tokens, disambiguating straight quotes,
and separating contractions such as can?t into ca
and n?t. There are, however, many special cases?
part of syntactic analysis. However, on the (predominant) point
of view that punctuation marks form tokens in their own right,
the tokenizer would then have to adorn quote marks in some
way, as to whether they were split off the left or right periph-
ery of a larger token, to avoid unwanted syntactic ambiguity.
Further, increasing use of Unicode makes texts containing ?na-
tively? disambiguated quotes more common, where it would
seem unfortunate to discard linguistically pertinent information
by normalizing towards the poverty of pure ASCII punctuation.
378
documented and undocumented. In much tagging
and parsing work, PTB data has been used with
gold-standard tokens, to a point where many re-
searchers are unaware of the existence of the orig-
inal ?raw? (untokenized) text. Accordingly, the for-
mal definition of PTB tokenization2 has received lit-
tle attention, but reproducing PTB tokenization au-
tomatically actually is not a trivial task (see ? 3).
As the NLP community has moved to process data
other than the PTB, some of the limitations of the
PTB tokenization have been recognized, and many
recently released data sets are accompanied by a
note on tokenization along the lines of: Tokenization
is similar to that used in PTB, except . . . Most ex-
ceptions are to do with hyphenation, or special forms
of named entities such as chemical names or URLs.
None of the documentation with extant data sets is
sufficient to fully reproduce the tokenization.3
The CoNLL 2008 Shared Task data actually pro-
vided two forms of tokenization: that from the PTB
(which many pre-processing tools would have been
trained on), and another form that splits (most) hy-
phenated terms. This latter convention recently
seems to be gaining ground in data sets like the
Google 1T n-gram corpus (LDC #2006T13) and
OntoNotes (Hovy et al, 2006). Clearly, as one
moves towards a more application- and domain-
driven idea of ?correct? tokenization, a more trans-
parent, flexible, and adaptable approach to string-
level pre-processing is called for.
3 A Contrastive Experiment
To get an overview of current tokenization methods,
we recovered and tokenized the raw text which was
the source of the (Wall Street Journal portion of the)
PTB, and compared it to the gold tokenization in the
syntactic annotation in the treebank.4 We used three
common methods of tokenization: (a) the original
2See http://www.cis.upenn.edu/~treebank/
tokenization.html for available ?documentation? and a
sed script for PTB-style tokenization.
3?vrelid et al (2010) observe that tokenizing with the GE-
NIA tagger yields mismatches in one of five sentences of the
GENIA Treebank, although the GENIA guidelines refer to
scripts that may be available on request (Tateisi & Tsujii, 2006).
4The original WSJ text was last included with the 1995 re-
lease of the PTB (LDC #95T07) and required alignment with
the treebank, with some manual correction so that the same text
is represented in both raw and parsed formats.
Tokenization Differing Levenshtein
Method Sentences Distance
tokenizer.sed 3264 11168
CoreNLP 1781 3717
C&J parser 2597 4516
Table 1: Quantitative view on tokenization differences.
PTB tokenizer.sed script; (b) the tokenizer from the
Stanford CoreNLP tools5; and (c) tokenization from
the parser of Charniak & Johnson (2005). Table 1
shows quantitative differences between each of the
three methods and the PTB, both in terms of the
number of sentences where the tokenization differs,
and also in the total Levenshtein distance (Leven-
shtein, 1966) over tokens (for a total of 49,208 sen-
tences and 1,173,750 gold-standard tokens).
Looking at the differences qualitatively, the most
consistent issue across all tokenization methods was
ambiguity of sentence-final periods. In the treebank,
final periods are always (with about 10 exceptions)
a separate token. If the sentence ends in U.S. (but
not other abbreviations, oddly), an extra period is
hallucinated, so the abbreviation also has one. In
contrast, C&J add a period to all final abbreviations,
CoreNLP groups the final period with a final abbre-
viation and hence lacks a sentence-final period to-
ken, and the sed script strips the period off U.S. The
?correct? choice in this case is not obvious and will
depend on how the tokens are to be used.
The majority of the discrepancies in the sed script
tokenization come from an under-restricted punctu-
ation rule that incorrectly splits on commas within
numbers or ampersands within names. Other than
that, the problematic cases are mostly shared across
tokenization methods, and include issues with cur-
rencies, Irish names, hyphenization, and quote dis-
ambiguation. In addition, C&J make some addi-
tional modifications to the text, lemmatising expres-
sions such as won?t as will and n?t.
4 REPP: A Generalized Framework
For tokenization to be studied as a first-class prob-
lem, and to enable customization and flexibility to
diverse use cases, we suggest a non-procedural,
rule-based framework dubbed REPP (Regular
5See http://nlp.stanford.edu/software/
corenlp.shtml, run in ?strictTreebank3? mode.
379
>wiki
#1
!([? ])([])}?!,;:??]) ([? ]|$) \1 \2 \3
!(?|[? ]) ([[({??])([? ]) \1 \2 \3
#
>1
:[[:space:]]+
Figure 1: Simplified examples of tokenization rules.
Expression-Based Pre-Processing)?essentially a
cascade of ordered finite-state string rewriting rules,
though transcending the formal complexity of regu-
lar languages by inclusion of (a) full perl-compatible
regular expressions and (b) fixpoint iteration over
groups of rules. In this approach, a first phase of
string-level substitutions inserts whitespace around,
for example, punctuation marks; upon completion of
string rewriting, token boundaries are stipulated be-
tween all whitespace-separated substrings (and only
these).
For a good balance of human and machine read-
ability, REPP tokenization rules are specified in a
simple, line-oriented textual form. Figure 1 shows
a (simplified) excerpt from our PTB-style tokenizer,
where the first character on each line is one of four
REPP operators, as follows: (a) ?#? for group forma-
tion; (b) ?>? for group invocation, (c) ?!? for substi-
tution (allowing capture groups), and (d) ?:? for to-
ken boundary detection.6 In Figure 1, the two rules
stripping off prefix and suffix punctuation marks ad-
jacent to whitespace (i.e. matching the tab-separated
left-hand side of the rule, to replace the match with
its right-hand side) form a numbered group (?#1?),
which will be iterated when called (?>1?) until none
of the rules in the group fires (a fixpoint). In this ex-
ample, conditioning on whitespace adjacency avoids
the issues observed with the PTB sed script (e.g. to-
ken boundaries within comma-separated numbers)
and also protects against infinite loops in the group.7
REPP rule sets can be organized as modules, typ-
6Strictly speaking, there are another two operators, for line-
oriented comments and automated versioning of rule files.
7For this example, the same effects seemingly could be ob-
tained without iteration (using greatly more complex rules); our
actual, non-simplified rules, however, further deal with punctu-
ation marks that can function as prefixes or suffixes, as well as
with corner cases like factor(s) or Ca[2+]. Also in mark-up re-
moval and normalization, we have found it necessary to ?parse?
nested structures by means of iterative groups.
ically each in a file of its own, and invoked selec-
tively by name (e.g. ?>wiki? in Figure 1); to date,
there exist modules for quote disambiguation, (rele-
vant subsets of) various mark-up languages (HTML,
LATEX, wiki, and XML), and a handful of robust-
ness rules (e.g. seeking to identify and repair ?sand-
wiched? inter-token punctuation). Individual tok-
enizers are configured at run-time, by selectively ac-
tivating a set of modules (through command-line op-
tions). An open-source reference implementation of
the REPP framework (in C++) is available, together
with a library of modules for English.
5 Characterization for Traceability
Tokenization, and specifically our notion of gener-
alized tokenization which allows text normalization,
involves changes to the original text being analyzed,
rather than just additional annotation. As such, full
traceability from the token objects to the original
text is required, which we formalize as ?character-
ization?, in terms of character position links back to
the source.8 This has the practical benefit of allow-
ing downstream analysis as direct (stand-off) anno-
tation on the source text, as seen for example in the
ACL Anthology Searchbench (Sch?fer et al, 2011).
With our general regular expression replacement
rules in REPP, making precise what it means for a
token to link back to its ?underlying? substring re-
quires some care in the design and implementation.
Definite characterization links between the string
before (I) and after (O) the application of a sin-
gle rule can only be established in certain positions,
viz. (a) spans not matched by the rule: unchanged
text in O outside the span matched by the left-hand
side regex of the rule can always be linked back to
I; and (b) spans caught by a regex capture group:
capture groups represent the same text in the left-
and right-hand sides of a substitution, and so can be
linked back to O.9 Outside these text spans, we can
only make definite statements about characterization
links at boundary points, which include the start and
end of the full string, the start and end of the string
8If the tokenization process was only concerned with the
identification of token boundaries, characterization would be
near-trivial.
9If capture group references are used out-of-order, however,
the per-group linkage is no longer well-defined, and we resort
to the maximum-span ?union? of boundary points (see below).
380
matched by the rule, and the start and end of any
capture groups in the rule.
Each character in the string being processed has
a start and end position, marking the point before
and after the character in the original string. Before
processing, the end position would always be one
greater than the start position. However, if a rule
mapped a string-initial, PTB-style opening double
quote (``) to one-character Unicode ?, the new first
character of the string would have start position 0,
but end position 2. In contrast, if there were a rule
!wo(n?t) will \1 (1)
applied to the string I won?t go!, all characters in the
second token of the resulting string (I will n?t go!)
will have start position 2 and end position 4. This
demonstrates one of the formal consequences of our
design: we have no reason to assign the characters ill
any start position other than 2.10 Since explicit char-
acter links between each I and O will only be estab-
lished at match or capture group boundaries, any text
from the left-hand side of a rule that should appear in
O must be explicitly linked through a capture group
reference (rather than merely written out in the right-
hand side of the rule). In other words, rule (1) above
should be preferred to the following variant (which
would result in character start and end offsets of 0
and 5 for both output tokens):
!won?t will n?t (2)
During rule application, we keep track of charac-
ter start and end positions as offsets between a string
before and after each rule application (i.e. all pairs
?I,O?), and these offsets are eventually traced back
to the original string at the time of final tokenization.
6 Quantitative and Qualitative Evaluation
In our own work on preparing various (non-PTB)
genres for parsing, we devised a set of REPP rules
with the goal of following the PTB conventions.
When repeating the experiment of ? 3 above us-
ing REPP tokenization, we obtained an initial dif-
ference in 1505 sentences, with a Levenshtein dis-
10This subtlety will actually be invisible in the final token
objects if will remains a single token, but if subsequent rules
were to split this token further, all its output tokens would have a
start position of 2 and an end position of 4. While this example
may seem unlikely, we have come across similar scenarios in
fine-tuning actual REPP rules.
tance of 3543 (broadly comparable to CoreNLP, if
marginally more accurate).
Examining these discrepancies, we revealed some
deficiencies in our rules, as well as some peculiari-
ties of the ?raw? Wall Street Journal text from the
PTB distribution. A little more than 200 mismatches
were owed to improper treatment of currency sym-
bols (AU$) and decade abbreviations (?60s), which
led to the refinement of two existing rules. Notable
PTB idiosyncrasies (in the sense of deviations from
common typography) include ellipses with spaces
separating the periods and a fairly large number of
possessives (?s) being separated from their preced-
ing token. Other aspects of gold-standard PTB tok-
enization we consider unwarranted ?damage? to the
input text, such as hallucinating an extra period af-
ter U.S. and splitting cannot (which adds spuri-
ous ambiguity). For use cases where the goal were
strict compliance, for instance in pre-processing in-
puts for a PTB-derived parser, we added an optional
REPP module (of currently half a dozen rules) to
cater to these corner cases?in a spirit similar to the
CoreNLP mode we used in ? 3. With these extra
rules, remaining tokenization discrepancies are con-
tained in 603 sentences (just over 1%), which gives
a Levenshtein distance of 1389.
7 Discussion?Conclusion
Compared to the best-performing off-the-shelf sys-
tem in our earlier experiment (where it is reason-
able to assume that PTB data has played at least
some role in development), our results eliminate two
thirds of the remaining tokenization errors?a more
substantial reduction than recent improvements in
parsing accuracy against the PTB, for example.
Of the remaining differences, over 350 are con-
cerned with mid-sentence period ambiguity, where
at least half of those are instances where a pe-
riod was separated from an abbreviation in the
treebank?a pattern we do not wish to emulate.
Some differences in quote disambiguation also re-
main, often triggered by whitespace on both sides of
quote marks in the raw text. The final 200 or so dif-
ferences stem from manual corrections made during
treebanking, and we consider that these cases could
not be replicated automatically in any generalizable
fashion.
381
References
Charniak, E., & Johnson, M. (2005). Coarse-to-fine
n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(pp. 173?180). Ann Arbor, USA.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L.,
& Weischedel, R. (2006). Ontonotes. The 90%
solution. In Proceedings of the Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (pp. 57?60). New York City,
USA.
Kaplan, R. M. (2005). A method for tokenizing
text. Festschrift for Kimmo Koskenniemi on his
60th birthday. In A. Arppe, L. Carlson, K. Lind?n,
J. Piitulainen, M. Suominen, M. Vainio, H. West-
erlund, & A. Yli-Jyr? (Eds.), Inquiries into words,
constraints and contexts (pp. 55 ? 64). Stanford,
CA: CSLI Publications.
Levenshtein, V. (1966). Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet
Physice ? Doklady, 10, 707?710.
Marcus, M. P., Santorini, B., & Marcinkiewicz,
M. A. (1993). Building a large annotated corpus
of English. The Penn Treebank. Computational
Linguistics, 19, 313 ? 330.
?vrelid, L., Velldal, E., & Oepen, S. (2010). Syn-
tactic scope resolution in uncertainty analysis. In
Proceedings of the 23rd international conference
on computational linguistics (pp. 1379 ? 1387).
Beijing, China.
Sch?fer, U., Kiefer, B., Spurk, C., Steffen, J., &
Wang, R. (2011). The ACL Anthology Search-
bench. In Proceedings of the ACL-HLT 2011 sys-
tem demonstrations (pp. 7?13). Portland, Oregon,
USA.
Tateisi, Y., & Tsujii, J. (2006). GENIA anno-
tation guidelines for tokenization and POS tag-
ging (Technical Report # TR-NLP-UT-2006-4).
Tokyo, Japan: Tsujii Lab, University of Tokyo.
Waldron, B., Copestake, A., Sch?fer, U., & Kiefer,
B. (2006). Preprocessing and tokenisation stan-
dards in DELPH-IN tools. In Proceedings of the
5th International Conference on Language Re-
sources and Evaluation (pp. 2263 ? 2268). Genoa,
Italy.
382
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 69?78,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Simple Negation Scope Resolution through Deep Parsing:
A Semantic Solution to a Semantic Problem
Woodley Packard
?
, Emily M. Bender
?
, Jonathon Read
?
, Stephan Oepen
??
, and Rebecca Dridan
?
?
University of Washington, Department of Linguistics
?
Teesside University, School of Computing
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
ebender@uw.edu, sweaglesw@sweaglesw.org, j.read@tees.ac.uk, {oe |rdridan}@ifi.uio.no
Abstract
In this work, we revisit Shared Task 1
from the 2012
*
SEM Conference: the au-
tomated analysis of negation. Unlike the
vast majority of participating systems in
2012, our approach works over explicit
and formal representations of proposi-
tional semantics, i.e. derives the notion of
negation scope assumed in this task from
the structure of logical-form meaning rep-
resentations. We relate the task-specific
interpretation of (negation) scope to the
concept of (quantifier and operator) scope
in mainstream underspecified semantics.
With reference to an explicit encoding
of semantic predicate-argument structure,
we can operationalize the annotation deci-
sions made for the 2012
*
SEM task, and
demonstrate how a comparatively simple
system for negation scope resolution can
be built from an off-the-shelf deep parsing
system. In a system combination setting,
our approach improves over the best pub-
lished results on this task to date.
1 Introduction
Recently, there has been increased community in-
terest in the theoretical and practical analysis of
what Morante and Sporleder (2012) call modality
and negation, i.e. linguistic expressions that mod-
ulate the certainty or factuality of propositions.
Automated analysis of such aspects of meaning
is important for natural language processing tasks
which need to consider the truth value of state-
ments, such as for example text mining (Vincze
et al, 2008) or sentiment analysis (Lapponi et al,
2012). Owing to its immediate utility in the cura-
tion of scholarly results, the analysis of negation
and so-called hedges in bio-medical research liter-
ature has been the focus of several workshops, as
well as the Shared Task at the 2011 Conference on
Computational Language Learning (CoNLL).
Task 1 at the First Joint Conference on Lex-
ical and Computational Semantics (
*
SEM 2012;
Morante and Blanco, 2012) provided a fresh, prin-
cipled annotation of negation and called for sys-
tems to analyze negation?detecting cues (affixes,
words, or phrases that express negation), resolv-
ing their scopes (which parts of a sentence are ac-
tually negated), and identifying the negated event
or property. The task organizers designed and
documented an annotation scheme (Morante and
Daelemans, 2012) and applied it to a little more
than 100,000 tokens of running text by the nov-
elist Sir Arthur Conan Doyle. While the task and
annotations were framed from a semantic perspec-
tive, only one participating system actually em-
ployed explicit compositional semantics (Basile et
al., 2012), with results ranking in the middle of
the 12 participating systems. Conversely, the best-
performing systems approached the task through
machine learning or heuristic processing over syn-
tactic and linguistically relatively coarse-grained
representations; see ? 2 below.
Example (1), where ?? marks the cue and {}
the in-scope elements, illustrates the annotations,
including how negation inside a noun phrase can
scope over discontinuous parts of the sentence.
1
(1) {The German} was sent for but professed to
{know} ?nothing? {of the matter}.
In this work, we return to the 2012
*
SEM
task from a deliberately semantics-centered point
of view, focusing on the hardest of the three
sub-problems: scope resolution.
2
Where Morante
and Daelemans (2012) characterize negation as an
?extra-propositional aspect of meaning? (p. 1563),
1
Our running example is a truncated variant of an item
from the Shared Task training data. The remainder of the
original sentence does not form part of the scope of this cue.
2
Resolving negation scope is a more difficult sub-problem
at least in part because (unlike cue and event identification) it
is concerned with much larger, non-local and often discontin-
uous parts of each utterance. This intuition is confirmed by
Read et al (2012), who report results for each sub-problem
using gold-standard inputs; in this setup, scope resolution
showed by far the lowest performance levels.
69
we in fact see it as a core piece of composi-
tionally constructed logical-form representations.
Though the task-specific concept of scope of
negation is not the same as the notion of quan-
tifier and operator scope in mainstream under-
specified semantics, we nonetheless find that re-
viewing the 2012
*
SEM Shared Task annotations
with reference to an explicit encoding of seman-
tic predicate-argument structure suggests a sim-
ple and straightforward operationalization of their
concept of negation scope. Our system imple-
ments these findings through a notion of functor-
argument ?crawling?, using as our starting point
the underspecified logical-form meaning represen-
tations provided by a general-purpose deep parser.
Our contributions are three-fold: Theoretically,
we correlate the structures at play in the Morante
and Daelemans (2012) view on negation with
formal semantic analyses; methodologically, we
demonstrate how to approach the task in terms of
underspecified, logical-form semantics; and prac-
tically, our combined system retroactively ?wins?
the 2012
*
SEM Shared Task. In the following
sections, we review related work (? 2), detail our
own setup (? 3), and present and discuss our ex-
perimental results (? 4 and ? 5, respectively).
2 Related Work
Read et al (2012) describe the best-performing
submission to Task 1 of the 2012
*
SEM Confer-
ence. They investigated two approaches for scope
resolution, both of which were based on syntac-
tic constituents. Firstly, they created a set of 11
heuristics that describe the path from the preter-
minal of a cue to the constituent whose projec-
tion is predicted to match the scope. Secondly
they trained an SVM ranker over candidate con-
stituents, generated by following the path from a
cue to the root of the tree and describing each
candidate in terms of syntactic properties along
the path and various surface features. Both ap-
proaches attempted to handle discontinuous in-
stances by applying two heuristics to the predicted
scope: (a) removing preceding conjuncts from the
scope when the cue is in a conjoined phrase and
(b) removing sentential adverbs from the scope.
The ranking approach showed a modest advan-
tage over the heuristics (with F
1
equal to 77.9
and 76.7, respectively, when resolving the scope
of gold-standard cues in evaluation data). Read et
al. (2012) noted however that the annotated scopes
did not align with the Shared Task?provided con-
stituents for 14% of the instances in the training
data, giving an F
1
upper-bound of around 86.0 for
systems that depend on those constituents.
Basile et al (2012) present the only submission
to Task 1 of the 2012
*
SEM Conference which
employed compositional semantics. Their scope
resolution pipeline consisted primarily of the C&C
parser and Boxer (Curran et al, 2007), which pro-
duce Discourse Representation Structures (DRSs).
The DRSs represent negation explicitly, including
representing other predications as being within the
scope of negation. Basile et al (2012) describe
some amount of tailoring of the Boxer lexicon to
include more of the Shared Task scope cues among
those that produce the negation operator in the
DRSs, but otherwise the system appears to directly
take the notion of scope of negation from the DRS
and project it out to the string, with one caveat: As
with the logical-forms representations we use, the
DRS logical forms do not include function words
as predicates in the semantics. Since the Shared
Task gold standard annotations included such ar-
guably semantically vacuous (see Bender, 2013,
p. 107) words in the scope, further heuristics are
needed to repair the string-based annotations com-
ing from the DRS-based system. Basile et al re-
sort to counting any words between in-scope to-
kens which are not themselves cues as in-scope.
This simple heuristic raises their F
1
for full scopes
from 20.1 to 53.3 on system-predicted cues.
3 System Description
The new system described here is what we call
the MRS Crawler. This system operates over
the normalized semantic representations provided
by the LinGO English Resource Grammar (ERG;
Flickinger, 2000).
3
The ERG maps surface strings
to meaning representations in the format of Mini-
mal Recursion Semantics (MRS; Copestake et al,
2005). MRS makes explicit predicate-argument
relations, as well as partial information about
scope (see below). We used the grammar together
with one of its pre-packaged conditional Maxi-
mum Entropy models for parse ranking, trained
on a combination of encyclopedia articles and
tourism brochures. Thus, the deep parsing front-
end system to our MRS Crawler has not been
3
In our experiments, we use the 1212 release of the ERG,
in combination with the ACE parser (http://sweaglesw
.org/linguistics/ace/). The ERG and ACE are DELPH-
IN resources; see http://www.delph-in.net.
70
? h
1
,
h
4
:_the_q?0:3?(ARG0 x
6
, RSTR h
7
, BODY h
5
), h
8
:_german_n_1?4:10?(ARG0 x
6
),
h
9
:_send_v_for?15:19?(ARG0 e
10
, ARG1 , ARG2 x
6
), h
2
:_but_c?24:27?(ARG0 e
3
, L-HNDL h
9
, R-HNDL h
14
),
h
14
:_profess_v_to?28:37?(ARG0 e
13
, ARG1 x
6
, ARG2 h
15
), h
16
:_know_v_1?41:45?(ARG0 e
17
, ARG1 x
6
, ARG2 x
18
),
h
20
:_no_q?46:53?(ARG0 x
18
, RSTR h
21
, BODY h
22
), h
19
:thing?46:53?(ARG0 x
18
),
h
19
:_of_p?54:56?(ARG0 e
23
, ARG1 x
18
, ARG2 x
24
),
h
25
:_the_q?57:60?(ARG0 x
24
, RSTR h
27
, BODY h
26
), h
28
:_matter_n_of?61:68?(ARG0 x
24
, ARG1 )
{ h
27
=
q
h
28
, h
21
=
q
h
19
, h
15
=
q
h
16
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: MRS analysis of our running example (1).
adapted to the task or its text type; it is applied
in an ?off the shelf? setting. We combine our
system with the outputs from the best-performing
2012 submission, the system of Read et al (2012),
firstly by relying on the latter for system negation
cue detection,
4
and secondly as a fall-back in sys-
tem combination as described in ? 3.4 below.
Scopal information in MRS analyses delivered
by the ERG fixes the scope of operators?such as
negation, modals, scopal adverbs (including sub-
ordinating conjunctions like while), and clause-
embedding verbs (e.g. believe)?based on their
position in the constituent structure, while leaving
the scope of quantifiers (e.g. a or every, but also
other determiners) free. From these underspec-
ified representations of possible scopal configu-
rations, a scope resolution component can spell
out the full range of fully-connected logical forms
(Koller and Thater, 2005), but it turns out that such
enumeration is not relevant here: the notion of
scope encoded in the Shared Task annotations is
not concerned with the relative scope of quantifiers
and negation, such as the two possible readings of
(2) represented informally below:
5
(2) Everyone didn?t leave.
a. ?(x)?leave(x) ? Everyone stayed.
b. ??(x)leave(x) ? At least some stayed.
However, as shown below, the information about
fixed scopal elements in an underspecified MRS is
sufficient to model the Shared Task annotations.
3.1 MRS Crawling
Fig. 1 shows the ERG semantic analysis for our
running example. The heart of the MRS is a mul-
tiset of elementary predications (EPs). Each ele-
4
Read et al (2012) predicted cues using a closed vocabu-
lary assumption with a supervised classifier to disambiguate
instances of cues.
5
In other words, a possible semantic interpretation of the
(string-based) Shared Task annotation guidelines and data is
in terms of a quantifier-free approach to meaning representa-
tion, or in terms of one where quantifier scope need not be
made explicit (as once suggested by, among others, Alshawi,
1992). From this interpretation, it follows that the notion of
scope assumed in the Shared Task does not encompass inter-
actions of negation operators and quantifiers.
mentary prediction includes a predicate symbol,
a label (or ?handle?, prefixed to predicates with
a colon in Fig. 1), and one or more argument
positions, whose values are semantic variables.
Eventualities (e
i
) in MRS denote states or activ-
ities, while instance variables (x
j
) typically corre-
spond to (referential or abstract) entities. All EPs
have the argument position ARG0, called the dis-
tinguished variable (Oepen and L?nning, 2006),
and no variable is the ARG0 of more than one non-
quantifier EP.
The arguments of one EP are linked to the argu-
ments of others either directly (sharing the same
variable as their value), or indirectly (through so-
called ?handle constraints?, where =
q
in Fig. 1 de-
notes equality modulo quantifier insertion). Thus
a well-formed MRS forms a connected graph. In
addition, the grammar links the EPs to the ele-
ments of the surface string that give rise to them,
via character offsets recorded in each EP (shown
in angle brackets in Fig. 1). For the purposes of
the present task, we take a negation cue as our en-
try point into the MRS graph (as our initial active
EP), and then move through the graph according
to the following simple operations to add EPs to
the active set:
Argument Crawling Add to the scope all EPs
whose distinguished variable or label is an argu-
ment of the active EP; for arguments of type h
k
,
treat any =
q
constraints as label equality.
Label Crawling Add all EPs whose label is iden-
tical to that of the active EP.
Functor Crawling Add all EPs that take the dis-
tinguished variable or label of the active EP as an
argument (directly or via =
q
constraints).
Our MRS crawling algorithm is sketched in
Fig. 2. To illustrate how the rules work, we will
trace their operation in the analysis of example (1),
i.e. traverse the EP graph in Fig. 1.
The negation cue is nothing, from character po-
sition 46 to 53. This leads us to _no_q as our en-
try point into the graph. Our algorithm states that
for this type of cue (a quantifier) the first step is
71
1: Activate the cue EP
2: if the cue EP is a quantifier then
3: Activate EPs reached by functor crawling from the distinguished variable (ARG0) of the cue EP
4: end if
5: repeat
6: for each active EP X do
7: Activate EPs reached by argument crawling or label crawling unless they are co-modifiers of the negation cue.
a
8: Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctions
reached by ARG1: whether, when, because, to, with, although, unless, until, or as.
9: end for
10: until a fixpoint is reached (no additional EPs were activated)
11: Deactivate zero-pronoun EPs (from imperative constructions)
12: Apply semantically empty word handling rules (iterate until a fixpoint is reached)
13: Apply punctuation heuristics
Figure 2: Algorithm for scope detection by MRS crawling
a
Formally: If an EP shares its label with the negation cue, or is a quantifier whose restriction (RSTR) is =
q
equated with the
label of the negation cue, it cannot be in-scope unless its ARG0 is an argument of the negation cue, or the ARG0 of the negation
cue is one of its own arguments. See ? 3.3 for elaboration.
functor crawling (see ? 3.3 below), which brings
_know_v_1 into the scope. We proceed with ar-
gument crawling and label crawling, which pick
up _the_q?0:3? and _german_n_1 as the ARG1.
Further, as the ARG2 of _know_v_1, we reach
thing and through recursive invocation we acti-
vate _of_p and, in yet another level of recursion,
_the_q?57:60? and _matter_n_of. At this point,
crawling has no more links to follow. Thus, the
MRS crawling operations ?paint? a subset of the
MRS graph as in-scope for a given negation cue.
3.2 Semantically Empty Word Handling
Our crawling rules operate on semantic represen-
tations, but the annotations are with reference to
the surface string. Accordingly, we need projec-
tion rules to map from the ?painted? MRS to the
string. We can use the character offsets recorded
in each EP to project the scope to the string. How-
ever, the string-based annotations also include
words which the ERG treats as semantically vacu-
ous. Thus in order to match the gold annotations,
we define a set of heuristics for when to count vac-
uous words as in scope. In (1), there are no se-
mantically empty words in-scope, so we illustrate
these heuristics with another example:
(3) ?I trust that {there is} ?nothing? {of consequence
which I have overlooked}??
The MRS crawling operations discussed above
paint the EPs corresponding to is, thing, of, conse-
quence, I, and overlooked as in-scope (underlined
in (3)). Conversely, the ERG treats the words that,
there, which, and have as semantically empty. Of
these, we need to add all except that to the scope.
Our vacuous word handling rules use the syntac-
tic structure provided by the ERG as scaffolding to
help link the scope information gleaned from con-
tentful words to vacuous words. Each node in the
syntax tree is initially colored either in-scope or
out-of-scope in agreement with the decision made
by the crawler about the lexical head of the corre-
sponding subtree. A semantically empty word is
determined to be in-scope if there is an in-scope
syntax tree node in the right position relative to it,
as governed by a short list of templates organized
by the type of the semantically empty word (par-
ticles, complementizers, non-referential pronouns,
relative pronouns, and auxiliary verbs).
As an example, the rule for auxiliary verbs like
have in our example (3) is that they are in scope
when their verb phrase complement is in scope.
Since overlooked is marked as in-scope by the
crawler, the semantically empty have becomes in-
scope as well. Sometimes the rules need to be
iterated. For example, the main rule for relative
pronouns is that they are in-scope when they fill
a gap in an in-scope constituent; which fills a gap
in the constituent have overlooked, but since have
is the (syntactic) lexical head of that constituent,
the verb phrase is not considered in-scope the first
time the rules are tried.
Similar rules deal with that (complementizers
are in-scope when the complement phrase is an ar-
gument of an in-scope verb, which is not the case
here) and there (non-referential pronouns are in-
scope when they are the subject of an in-scope VP,
which is true here).
72
3.3 Re-Reading the Annotation Guidelines
Our MRS crawling algorithm was defined by look-
ing at the annotated data rather than the annota-
tion guidelines for the Shared Task (Morante et al,
2011). Nonetheless, our algorithm can be seen as
a first pass formalization of the guidelines. In this
section, we briefly sketch how our algorithm cor-
responds to different aspects of the guidelines.
For negated verbs, the guidelines state that ?If
the negated verb is the main verb in the sen-
tence, the entire sentence is in scope.? (Morante
et al, 2011, 17). In terms of our operations de-
fined over semantic representations, this is ren-
dered as follows: all arguments of the negated
verb are selected by argument crawling, all in-
tersective modifiers by label crawling, and func-
tor crawling (Fig. 2, line 8) captures modal auxil-
iaries and non-intersective modifiers. The guide-
lines treat predicative adjectives under a separate
heading from verbs, but describe the same desired
annotations (scope over the whole clause; ibid.,
p. 20). Since these structures are analogous in the
semantic representations, the same operations that
handle negated verbs also handle negated predica-
tive adjectives correctly.
For negated subjects and objects, the guidelines
state that the negation scopes over ?all the clause?
and ?the clause headed by the verb? (Morante et
al., 2011, 19), respectively. The examples given in
the annotation guidelines suggest that these are in
fact meant to refer to the same thing. The negation
cue for a negated nominal argument will appear
as a quantifier EP in the MRS, triggering line 3 of
our algorithm. This functor crawling step will get
to the verb?s EP, and from there, the process is the
same as the last two cases.
In contrast to subjects and objects, negation of
a clausal argument is not treated as negation of the
verb (ibid., p. 18). Since in this case, the negation
cue will not be a quantifier in the MRS, there will
be no functor crawling to the verb?s EP.
For negated modifiers, the situation is somewhat
more complex, and this is a case where our crawl-
ing algorithm, developed on the basis of the anno-
tated data, does not align directly with the guide-
lines as given. The guidelines state that negated at-
tributive adjectives have scope over the entire NP
(including the determiner) (ibid., p. 20) and anal-
ogously negated adverbs have scope over the en-
tire clause (ibid., p. 21). However, the annotations
are not consistent, especially with respect to the
treatment of negated adjectives: while the head
noun and determiner (if present) are typically an-
notated as in scope, other co-modifiers, especially
long, post-nominal modifiers (including relative
clauses) are not necessarily included:
(4) ?A dabbler in science, Mr. Holmes, a picker up
of shells on the shores of {the} great ?un?{known
ocean}.
(5) Our client looked down with a rueful face at {his}
own ?un?{conventional appearance}.
(6) Here was {this} ?ir?{reproachable Englishman}
ready to swear in any court of law that the accused
was in the house all the time.
(7) {There is}, on the face of it, {something}
?un?{natural about this strange and sudden friend-
ship between the young Spaniard and Scott Eccles}.
Furthermore, the guidelines treat relative clauses
as subordinate clauses and thus negation inside a
relative clause is treated as bound to that clause
only, and includes neither the head noun of the
relative clause nor any of its other dependents in
its scope. However, from the perspective of MRS,
a negated relative clause is indistinguishable from
any other negated modifier of a noun. This treat-
ment of relative clauses (as well as the inconsis-
tencies in other forms of co-modification) is the
reason for the exception noted at line 7 of Fig. 2.
By disallowing the addition of EPs to the scope if
they share the label of the negation cue but are not
one of its arguments, we block the head noun?s EP
(and any EPs only reachable from it) in cases of
relative clauses where the head verb inside the rel-
ative clause is negated. It also blocks co-modifiers
like great, own, and the phrases headed by ready
and about in (4)?(7). As illustrated in these exam-
ples, this is correct some but not all of the time.
Having been unable to find a generalization cap-
turing when comodifiers are annotated as in scope,
we stuck with this approximation.
For negation within clausal modifiers of verbs,
the annotation guidelines have further informa-
tion, but again, our existing algorithm has the cor-
rect behavior: The guidelines state that a negation
cue inside of the complement of a subordinating
conjunction (e.g. if ) has scope only over the sub-
ordinate clause (ibid., p. 18 and p. 26). The ERG
treats all subordinating conjunctions as two-place
predicates taking two scopal arguments. Thus,
as with clausal complements of clause-embedding
verbs, the embedding subordinating conjunction
and any other arguments it might have are inac-
cessible, since functor crawling is restricted to a
handful of specific configurations.
73
As is usually the case with exercises in for-
malization, our crawling algorithm generalizes be-
yond what is given explicitly in the annotation
guidelines. For example, all arguments that are
treated as semantically nominal (including PP ar-
guments where the preposition is semantically
null) are treated in the same way as subjects and
objects; similarly, all arguments which are seman-
tically clausal (including certain PP arguments)
are handled the same way as clausal complements.
This is possible because we take advantage of the
high degree of normalization that the ERG accom-
plishes in mapping to the MRS representation.
There are also cases where we are more spe-
cific. The guidelines do not handle coordination in
detail, except to state that in coordinated clauses
negation is restricted to the clause it appears in
(ibid., p. 17?18) and to include a few examples of
coordination under the heading ?ellipsis?. In the
case of VP coordination, our existing algorithm
does not need any further elaboration to pick up
the subject of the coordinated VP but not the non-
negated conjunct, as shown in discussion of (1) in
? 3.1 above. In the case of coordination of negated
NPs, recall that to reach the main portion of the
negated scope we must first apply functor crawl-
ing. The functor crawling procedure has a general
mechanism to transparently continue crawling up
through coordinated structures while blocking fu-
ture crawling from traversing them again.
6
On the other hand, there are some cases in the
annotation guidelines which our algorithm does
not yet handle. We have not yet provided any anal-
ysis of the special cases for save and expect dis-
cussed in Morante et al, 2011, pp. 22?23, and also
do not have a means of picking out the overt verb
in gapping constructions (p. 24).
Finally, we note that even carefully worked out
annotation guidelines such as these are never fol-
lowed perfectly consistently by the human annota-
tors who apply them. Because our crawling algo-
rithm so closely models the guidelines, this puts
our system in an interesting position to provide
feedback to the Shared Task organizers.
3.4 Fall-Back Configurations
The close match between our crawling algorithm
and the annotation guidelines supported by the
mapping to MRS provides for very high precision
6
This allows ate to be reached in We ate bread but no fish.,
while preventing but and bread from being reached, which
they otherwise would via argument crawling from ate.
and recall when the analysis engine produces the
desired MRS.
7
However, the analysis engine does
not always provide the desired analysis, largely
because of idiosyncrasies of the genre (e.g. voca-
tives appearing mid-sentence) that are either not
handled by the grammar or not well modeled in the
parse selection component. In addition, as noted
above, there are a handful of negation cues we do
not yet handle. Thus, we also tested fall-back con-
figurations which use scope predictions based on
MRS in some cases, and scope predictions from
the system of Read et al (2012) in others.
Our first fall-back configuration (Crawler
N
in
Table 1) uses MRS-based predictions whenever
there is a parse available and the cue is one that
our system handles. Sometimes, the analysis
picked by the ERG?s statistical model is not the
correct analysis for the given context. To com-
bat such suboptimal parse selection performance,
we investigated using the probability of the top
ranked analysis (as determined by the parse selec-
tion model and conditioned on the sentence) as a
confidence metric. Our second fall-back configu-
ration (Crawler
P
in Table 1) uses MRS-based pre-
dictions when there is a parse available whose con-
ditional probability is at least 0.5.
8
4 Experiments
We evaluated the performance of our system using
the Shared Task development and evaluation data
(respectively CDD and CDE in Table 1). Since we
do not attempt to perform cue detection, we report
performance using gold cues and also using the
system cues predicted by Read et al (2012). We
used the official Shared Task evaluation script to
compute all scores.
4.1 Data Sets
The Shared Task data consists of chapters from
the Adventures of Sherlock Holmes mystery nov-
els and short stories. As such, the text is carefully
edited turn-of-the-20th-century British English,
9
7
And in fact, the task is somewhat noise-tolerant: some
parse selection decisions are independent of each other, and
a mistake in a part of the analysis far enough away from the
negation cue does not harm performance.
8
This threshold was determined empirically on the devel-
opment data. We also experimented with other confidence
metrics?the probability ratio of the top-ranked and second
parse or the entropy over the probability distribution of the
top 10 parses?but found no substantive differences.
9
In contrast, the ERG was engineered for the analysis of
contemporary American English, and an anecdotal analysis
of parse failures and imperfect top-ranked parses suggests
74
Gold Cues System Cues
Scopes Tokens Scopes Tokens
Set Method Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
C
D
D
Ranker 100.0 68.5 81.3 84.8 86.8 85.8 91.7 66.1 76.8 79.5 84.9 82.1
Crawler 100.0 53.0 69.3 89.3 67.0 76.6 90.8 53.0 66.9 84.7 65.9 74.1
Crawler
N
100.0 64.9 78.7 89.0 83.5 86.1 90.8 64.3 75.3 82.6 82.1 82.3
Crawler
P
100.0 70.2 82.5 86.4 86.8 86.6 91.2 67.9 77.8 80.0 84.9 82.4
Oracle 100.0 76.8 86.9 91.5 89.1 90.3
C
D
E
Ranker 98.8 64.3 77.9 85.3 90.7 87.9 87.4 61.5 72.2 82.0 88.8 85.3
Crawler 100.0 44.2 61.3 85.8 68.4 76.1 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
N
98.6 56.6 71.9 83.8 88.4 86.1 86.0 54.2 66.5 78.4 85.7 81.9
Crawler
P
98.8 65.5 78.7 86.1 90.4 88.2 87.6 62.7 73.1 82.6 88.5 85.4
Oracle 100.0 70.3 82.6 89.5 93.1 91.3
Table 1: Scope resolution performance of various configurations over each subset of the Shared Task
data. Ranker refers to the system of Read et al (2012); Crawler refers to our current system in isolation,
or falling back to the Ranker prediction either when the sentence is not covered by the parser (Crawler
N
),
or when the parse probability is predicted to be less than 0.5 (Crawler
P
); finally, Oracle simulates best
possible selection among the Ranker and Crawler predictions (and would be ill-defined on system cues).
annotated with token-level information about the
cues and scopes in every negated sentence. The
training set contains 848 negated sentences, the
development set 144, and the evaluation set 235.
As there can be multiple usages of negation in one
sentence, this corresponds to 984, 173, and 264
instances, respectively.
Being rule-based, our system does not require
any training data per se. However, the majority of
our rule development and error analysis were per-
formed against the designated training data. We
used the designated development data for a single
final round of error analysis and corrections. The
system was declared frozen before running with
the formal evaluation data. All numbers reported
here reflect this frozen system.
10
4.2 Results
Table 1 presents the results of our various config-
urations in terms of both (a) whole scopes (i.e. a
true positive is only generated when the predicted
scope matches the gold scope exactly) and (b) in-
scope tokens (i.e. a true positive for every token
the system correctly predicts to be in scope). The
table also details the performance upper-bound for
system combination, in which an oracle selects the
system prediction which scores the greater token-
wise F
1
for each gold cue.
The low recall levels for Crawler can be mostly
that the archaic style in the 2012
*
SEM Shared Task texts
has a strong adverse effect on the parser.
10
The code and data are available from http://www
.delph-in.net/crawler/, for replicability (Fokkens et al,
2013).
attributed to imperfect parser coverage. Crawler
N
,
which falls back just for parse failure brings the
recall back up, and results in F
1
levels closer to
the system of Read et al (2012), albeit still not
quite advancing the state of the art (except over
the development set). Our best results are from
Crawler
P
, which outperforms all other configura-
tions on the development and evaluation sets.
The Oracle results are interesting because they
show that there is much more to be gained in com-
bining our semantics-based system with the Read
et al (2012) syntactically-focused system. Further
analysis of these results to draw out the patterns of
complementary errors and strengths is a promising
avenue for future work.
4.3 Error Analysis
To shed more light on specific strengths and weak-
nesses of our approach, we performed a manual er-
ror analysis of scope predictions by Crawler, start-
ing from gold cues so as to focus in-depth analy-
sis on properties specific to scope resolution over
MRSs. This analysis was performed on CDD, in
order to not bar future work on this task. Of the
173 negation cue instances in CDD, Crawler by it-
self makes 94 scope predictions that exactly match
the gold standard. In comparison, the system of
Read et al (2012) accomplishes 119 exact scope
matches, of which 80 are shared with Crawler; in
other words, there are 14 cue instances (or 8%
of all cues) in which our approach can improve
over the best-performing syntax-based submission
to the original Shared Task.
75
We reviewed the 79 negation instances where
Crawler made a wrong prediction in terms of ex-
act scope match, categorizing the source of failure
into five broad error types:
(1) Annotation Error In 11% of all instances, we
consider the annotations erroneous or inconsistent.
These judgments were made by two of the authors,
who both were familiar with the annotation guide-
lines and conventions observable in the data. For
example, Morante et al (2011) unambiguously
state that subordinating conjunctions shall not be
in-scope (8), whereas relative pronouns should be
(9), and a negated predicative argument to the cop-
ula must scope over the full clause (10):
(8) It was after nine this morning {when we} reached
his house and {found} ?neither? {you} ?nor?
{anyone else inside it}.
(9) ?We can imagine that in the confusion of flight
something precious, something which {he could}
?not? {bear to part with}, had been left behind.
(10) He said little about the case, but from that little we
gathered that he also was not ?dis?{satisfied} at the
course of events.
(2) Parser Failure Close to 30% of Crawler fail-
ures reflect lacking coverage in the ERG parser,
i.e. inputs for which the parser does not make
available an analysis (within certain bounds on
time and memory usage).
11
In this work, we have
treated the ERG as an off-the-shelf system, but
coverage could certainly be straightforwardly im-
proved by adding analyses for phenomena partic-
ular to turn-of-the-20th-century British English.
(3) MRS Inadequacy Another 33% of our false
scope predictions are Crawler-external, viz. owing
to erroneous input MRSs due to imperfect disam-
biguation by the parser or other inadequacies in
the parser output. Again, these judgments (assign-
ing blame outside our own work) were double-
checked by two authors, and we only counted
MRS imperfections that actually involve the cue
or in-scope elements. Here, we could anticipate
improvements by training the parse ranker on in-
domain data or otherwise adapting it to this task.
(4) Cue Selection In close to 9% of all cases,
there is a valid MRS, but Crawler fails to pick out
an initial EP that corresponds to the negation cue.
This first type of genuine crawling failure often re-
lates to cues expressed as affixation (11), as well
11
Overall parsing coverage on this data is about 86%, but
of course all parser failures on sentences containing negation
surface in our error analysis of Crawler in isolation.
Scopes Tokens
Method Prec Rec F
1
Prec Rec F
1
C
D
E
Boxer 76.1 41.0 53.3 69.2 82.3 75.2
Crawler 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
P
87.6 62.7 73.1 82.6 88.5 85.4
Table 2: Comparison to Basile et al (2012).
as to rare usages of cue expressions that predomi-
nantly occur with different categories, e.g. neither
as a generalized quantifier (12):
(11) Please arrange your thoughts and let me know, in
their due sequence, exactly what those events are
{which have sent you out} ?un?{brushed} and un-
kempt, with dress boots and waistcoat buttoned
awry, in search of advice and assistance.
(12) You saw yourself {how} ?neither? {of the inspec-
tors dreamed of questioning his statement}, extraor-
dinary as it was.
(5) Crawler Deficiency Finally, a little more
than 16% of incorrect predictions we attribute to
our crawling rules proper, where we see many
instances of under-coverage of MRS elements
(13, 14) and a few cases of extending the scope too
wide (15). In the examples below, erroneous scope
predictions by Crawler are indicated through un-
derlining. Hardly any of the errors in this category,
however, involve semantically vacuous tokens.
(13) He in turn had friends among the indoor
servants who unite in {their} fear and
?dis?{like of their master}.
(14) He said little about the case, but from that
little we gathered that {he also was} ?not?
{dissatisfied at the course of events}.
(15) I tell you, sir, {I could}n?t move a finger, ?nor?
{get my breath}, till it whisked away and was gone.
5 Discussion and Comparison
The example in (1) nicely illustrates the strengths
of the MRS Crawler and of the abstraction pro-
vided by the deep linguistic analysis made pos-
sible by the ERG. The negated verb in that sen-
tence is know, and its first semantic argument is
The German. This semantic dependency is di-
rectly and explicitly represented in the MRS, but
the phrase expressing the dependent is not adja-
cent to the head in the string. Furthermore, even
a system using syntactic structure to model scope
would be faced with a more complicated task than
our crawling rules: At the level of syntax the de-
pendency is mediated by both verb phrase coordi-
nation and the control verb profess, as well as by
the semantically empty infinitival marker to.
76
The system we propose is very similar in spirit
to that of Basile et al (2012). Both systems map
from logical forms with explicit representations of
scope of negation out to string-based annotations
in the format provided by the Shared Task gold
standard. The main points of difference are in the
robustness of the system and in the degree of tai-
loring of both the rules for determining scope on
the logical form level and the rules for handling se-
mantically vacuous elements. The system descrip-
tion in Basile et al (2012) suggests relatively little
tailoring at either level: aside from adjustments to
the Boxer lexicon to make more negation cues take
the form of the negation operator in the DRS, the
notion of scope is directly that given in the DRS.
Similarly, their heuristic for picking up semanti-
cally vacuous words is string-based and straight-
forward. Our system, on the other hand, models
the annotation guidelines more closely in the def-
inition of the MRS crawling rules, and has more
elaborated rules for handling semantically empty
words. The Crawler alone is less robust than the
Boxer-based system, returning no output for 29%
of the cues in CDE. These factors all point to
higher precision and lower recall for the Crawler
compared to the Boxer-based system. At the to-
ken level, that is what we see. Since full-scope re-
call depends on token-level precision, the Crawler
does better across the board at the full-scope level.
A comparison of the results is shown in Table 2.
A final key difference between our results and
those of Basile et al (2012) is the cascading with
a fall-back system. Presumably a similar system
combination strategy could be pursued with the
Boxer-based system in place of the Crawler.
6 Conclusion and Outlook
Our motivation in this work was to take the design
of the 2012
*
SEM Shared Task on negation analy-
sis at face value?as an overtly semantic problem
that takes a central role in our long-term pursuit of
language understanding. Through both theoreti-
cal and practical reflection on the nature of repre-
sentations at play in this task, we believe we have
demonstrated that explicit semantic structure will
be a key driver of further progress in the analy-
sis of negation. We were able to closely align
two independently developed semantic analyses?
the negation-specific annotations of Morante et al
(2011), on the one hand, and the broad-coverage,
MRS meaning representations of the ERG, on the
other hand. In our view, the conceptual correla-
tion between these two semantic views on nega-
tion analysis reinforces their credibility.
Unlike the rather complex top-performing sys-
tems from the original 2012 competition, our MRS
Crawler is defined by a small set of general rules
that operate over general-purpose, explicit mean-
ing representations. Thus, our approach scores
high on transparency, adaptability, and replicabil-
ity. In isolation, the Crawler provides premium
precision but comparatively low recall. Its limi-
tations, we conjecture, reflect primarily on ERG
parsing challenges and inconsistencies in the tar-
get data. In a sense, our approach pushes a
larger proportion of the task into the parser, mean-
ing (a) there should be good opportunities for
parser adaptation to this somewhat idiosyncratic
text type; (b) our results can serve to offer feed-
back on ERG semantic analyses and parse rank-
ing; and (c) there is a much smaller proportion
of very task-specific engineering. When embed-
ded in a confidence-thresholded cascading archi-
tecture, our system advances the state of the art
on this task, and oracle combination scores sug-
gest there is much remaining room to better ex-
ploit the complementarity of approaches in our
study. In future work, we will seek to better un-
derstand the division of labor between the systems
involved through contrastive error analysis and
possibly another oracle experiment, constructing
gold-standard MRSs for part of the data. It would
also be interesting to try a task-specific adaptation
of the ERG parse ranking model, for example re-
training on the pre-existing treebanks but giving
preference to analyses that lead to correct Crawler
results downstream.
Acknowledgments
We are grateful to Dan Flickinger, the main devel-
oper of the ERG, for many enlightening discus-
sions and continuous assistance in working with
the analyses available from the grammar. This
work grew out of a discussion with colleagues of
the Language Technology Group at the University
of Oslo, notably Elisabeth Lien and Jan Tore L?n-
ning, to whom we are indebted for stimulating co-
operation. Furthermore, we have benefited from
comments by participants of the 2013 DELPH-
IN Summit, in particular Joshua Crowgey, Guy
Emerson, Glenn Slayden, Sanghoun Song, and
Rui Wang.
77
References
Alshawi, H. (Ed.). 1992. The Core Language Engine.
Cambridge, MA, USA: MIT Press.
Basile, V., Bos, J., Evang, K., and Venhuizen, N.
2012. UGroningen. Negation detection with Dis-
course Representation Structures. In Proceedings of
the 1st Joint Conference on Lexical and Computa-
tional Semantics (p. 301 ? 309). Montr?al, Canada.
Bender, E. M. 2013. Linguistic fundamentals for nat-
ural language processing: 100 essentials from mor-
phology and syntax. San Rafael, CA, USA: Morgan
& Claypool Publishers.
Copestake, A., Flickinger, D., Pollard, C., and Sag,
I. A. 2005. Minimal Recursion Semantics. An intro-
duction. Research on Language and Computation,
3(4), 281 ? 332.
Curran, J., Clark, S., and Bos, J. 2007. Linguistically
motivated large-scale NLP with C&C and Boxer.
In Proceedings of the 45th Meeting of the Associa-
tion for Computational Linguistics Demo and Poster
Sessions (p. 33 ? 36). Prague, Czech Republic.
Flickinger, D. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engi-
neering, 6 (1), 15 ? 28.
Fokkens, A., van Erp, M., Postma, M., Pedersen,
T., Vossen, P., and Freire, N. 2013. Offspring
from reproduction problems. What replication fail-
ure teaches us. In Proceedings of the 51th Meet-
ing of the Association for Computational Linguistics
(p. 1691 ? 1701). Sofia, Bulgaria.
Koller, A., and Thater, S. 2005. Efficient solving and
exploration of scope ambiguities. In Proceedings of
the 43rd Meeting of the Association for Computa-
tional Linguistics: Interactive Poster and Demon-
stration Sessions (p. 9 ? 12). Ann Arbor, MI, USA.
Lapponi, E., Read, J., and ?vrelid, L. 2012. Repre-
senting and resolving negation for sentiment analy-
sis. In Proceedings of the 2012 ICDM workshop on
sentiment elicitation from natural text for informa-
tion retrieval and extraction. Brussels, Belgium.
Morante, R., and Blanco, E. 2012. *SEM 2012 Shared
Task. Resolving the scope and focus of negation. In
Proceedings of the 1st Joint Conference on Lexical
and Computational Semantics (p. 265 ? 274). Mon-
tr?al, Canada.
Morante, R., and Daelemans, W. 2012. ConanDoyle-
neg. Annotation of negation in Conan Doyle stories.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation. Istanbul,
Turkey.
Morante, R., Schrauwen, S., and Daelemans, W. 2011.
Annotation of negation cues and their scope guide-
lines v1.0 (Tech. Rep. # CTRS-003). Antwerp, Bel-
gium: Computational Linguistics & Psycholinguis-
tics Research Center, Universiteit Antwerpen.
Morante, R., and Sporleder, C. 2012. Modality and
negation. An introduction to the special issue. Com-
putational Linguistics, 38(2), 223 ? 260.
Oepen, S., and L?nning, J. T. 2006. Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Read, J., Velldal, E., ?vrelid, L., and Oepen, S. 2012.
UiO1. Constituent-based discriminative ranking for
negation resolution. In Proceedings of the 1st Joint
Conference on Lexical and Computational Seman-
tics (p. 310 ? 318). Montr?al, Canada.
Vincze, V., Szarvas, G., Farkas, R., M?ra, G., and
Csirik, J. 2008. The BioScope corpus. Biomedical
texts annotated for uncertainty, negation and their
scopes. BMC Bioinformatics, 9(Suppl 11).
78
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228?236,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Effects of Semantic Annotations on Precision Parse Ranking
Andrew MacKinlay??, Rebecca Dridan??, Diana McCarthy?? and Timothy Baldwin??
? Dept. of Computing and Information Systems, University of Melbourne, Australia
? NICTA Victoria Research Laboratories, University of Melbourne, Australia
? Department of Informatics, University of Oslo, Norway
? Computational Linguistics and Phonetics, Saarland University, Germany
amack@csse.unimelb.edu.au, rdridan@ifi.uio.no,
diana@dianamccarthy.co.uk, tb@ldwin.net
Abstract
We investigate the effects of adding semantic
annotations including word sense hypernyms
to the source text for use as an extra source
of information in HPSG parse ranking for the
English Resource Grammar. The semantic an-
notations are coarse semantic categories or en-
tries from a distributional thesaurus, assigned
either heuristically or by a pre-trained tagger.
We test this using two test corpora in different
domains with various sources of training data.
The best reduces error rate in dependency F-
score by 1% on average, while some methods
produce substantial decreases in performance.
1 Introduction
Most start-of-the-art natural language parsers (Char-
niak, 2000; Clark and Curran, 2004; Collins, 1997)
use lexicalised features for parse ranking. These are
important to achieve optimal parsing accuracy, and
yet these are also the features which by their nature
suffer from data-sparseness problems in the training
data. In the absence of reliable fine-grained statis-
tics for a given token, various strategies are possible.
There will often be statistics available for coarser
categories, such as the POS of the particular token.
However, it is possible that these coarser represen-
tations discard too much, missing out information
which could be valuable to the parse ranking. An
intermediate level of representation could provide
valuable additional information here. For example,
?This research was conducted while the second author was
a postdoctoral researcher within NICTA VRL.
?The third author is a visiting scholar on the Erasmus
Mundus Masters Program in ?Language and Communication
Technologies? (LCT, 2007?0060).
assume we wish to correctly attach the prepositional
phrases in the following examples:
(1) I saw a tree with my telescope
(2) I saw a tree with no leaves
The most obvious interpretation in each case has the
prepositional phrase headed by with attaching in dif-
ferent places: to the verb phrase in the first example,
and to the noun tree in the second. Such distinctions
are difficult for a parser to make when the training
data is sparse, but imagine we had seen examples
such as the following in the training corpus:
(3) Kim saw a eucalypt with his binoculars
(4) Sandy observed a willow with plentiful foliage
There are few lexical items in common, but in each
case the prepositional phrase attachment follows the
same pattern: in (3) it attaches to the verb, and in
(4) to the noun. A conventional lexicalised parser
would have no knowledge of the semantic similarity
between eucalypt and tree, willow and tree, binoc-
ulars and telescope, or foliage and leaves, so would
not be able to make any conclusions about the earlier
examples on the basis of this training data. However
if the parse ranker has also been supplied with in-
formation about synonyms or hypernyms of the lex-
emes in the training data, it could possibly have gen-
eralised, to learn that PPs containing nouns related
to seeing instruments often modify verbs relating to
observation (in preference to nouns denoting inani-
mate objects), while plant flora can often be modi-
fied by PPs relating to appendages of plants such as
leaves. This is not necessarily applicable only to PP
attachment, but may help in a range of other syntac-
tic phenomena, such as distinguishing between com-
plements and modifiers of verbs.
228
The synonyms or hypernyms could take the form
of any grouping which relates word forms with se-
mantic or syntactic commonality ? such as a label
from the WordNet (Miller, 1995) hierarchy, a sub-
categorisation frame (for verbs) or closely related
terms from a distributional thesaurus (Lin, 1998).
We present work here on using various levels
of semantic generalisation as an attempt to im-
prove parse selection accuracy with the English Re-
source Grammar (ERG: Flickinger (2000)), a preci-
sion HPSG-based grammar of English.
2 Related Work
2.1 Parse Selection for Precision Grammars
The focus of this work is on parsing using hand-
crafted precision HPSG-based grammars, and in
particular the ERG. While these grammars are care-
fully crafted to avoid overgeneration, the ambiguity
of natural languages means that there will unavoid-
ably be multiple candidate parses licensed by the
grammar for any non-trivial sentence. For the ERG,
the number of parses postulated for a given sentence
can be anywhere from zero to tens of thousands. It
is the job of the parse selection model to select the
best parse from all of these candidates as accurately
as possible, for some definition of ?best?, as we dis-
cuss in Section 3.2.
Parse selection is usually performed by training
discriminative parse selection models, which ?dis-
criminate? between the set of all candidate parses.
A widely-used method to achieve this is outlined
in Velldal (2007). We feed both correct and incor-
rect parses licensed by the grammar to the TADM
toolkit (Malouf, 2002), and learn a maximum en-
tropy model. This method is used by Zhang et al
(2007) and MacKinlay et al (2011) inter alia. One
important implementation detail is that rather than
exhaustively ranking all candidates out of possibly
many thousands of trees, Zhang et al (2007) showed
that it was possible to use ?selective unpacking?,
which means that the exhaustive parse forest can be
represented compactly as a ?packed forest?, and the
top-ranked trees can be successively reconstructed,
enabling faster parsing using less memory.
2.2 Semantic Generalisation for parse ranking
Above, we outlined a number of reasons why
semantic generalisation of lexemes could enable
parsers to make more efficient use of training data,
and indeed, there has been some prior work investi-
gating this possibility. Agirre et al (2008) applied
two state-of-the-art treebank parsers to the sense-
tagged subset of the Brown corpus version of the
Penn Treebank (Marcus et al, 1993), and added
sense annotation to the training data to evaluate their
impact on parse selection and specifically on PP-
attachment. The annotations they used were oracle
sense annotations, automatic sense recognition and
the first sense heuristic, and it was this last method
which was the best performer in general. The sense
annotations were either the WordNet synset ID or
the coarse semantic file, which we explain in more
detail below, and replaced the original tokens in
the training data. The largest improvement in pars-
ing F-score was a 6.9% reduction in error rate for
the Bikel parser (Bikel, 2002), boosting the F-score
from 0.841 to 0.852, using the noun supersense only.
More recently, Agirre et al (2011) largely repro-
duced these results with a dependency parser.
Fujita et al (2007) add sense information to im-
prove parse ranking with JaCy (Siegel and Bender,
2002), an HPSG-based grammar which uses simi-
lar machinery to the ERG. They use baseline syn-
tactic features, and also add semantic features based
on dependency triples extracted from the semantic
representations of the sentence trees output by the
parser. The dataset they use has human-assigned
sense tags from a Japanese lexical hierarchy, which
they use as a source of annotations. The dependency
triples are modified in each feature set by replacing
elements of the semantic triples with corresponding
senses or hypernyms. In the best-performing con-
figuration, they use both syntactic and semantic fea-
tures with multiple levels of the the semantic hier-
archy from combined feature sets. They achieve a
5.6% improvement in exact match parsing accuracy.
3 Methodology
We performed experiments in HPSG parse rank-
ing using the ERG, evaluating the impact on parse
selection of semantic annotations such as coarse
sense labels or synonyms from a distributional the-
229
WESCIENCE LOGON
Total Sentences 9632 9410
Parseable Sentences 9249 8799
Validated Sentences 7631 8550
Train/Test Sentences 6149/1482 6823/1727
Tokens/sentence 15.0 13.6
Training Tokens 92.5k 92.8k
Table 1: Corpora used in our experiments, with total sen-
tences, how many of those can be parsed, how many of
the parseable sentences have a single gold parse (and are
used in these experiments), and average sentence length
saurus. Our work here differs from the aforemen-
tioned work of Fujita et al (2007) in a number of
ways. Firstly, we use purely syntactic parse selec-
tion features based on the derivation tree of the sen-
tence (see Section 3.4.3), rather than ranking using
dependency triples, meaning that our method is in
principle able to be integrated into a parser more eas-
ily, where the final set of dependencies would not be
known in advance. Secondly, we do not use human-
created sense annotations, instead relying on heuris-
tics or trained sense-taggers, which is closer to the
reality of real-world parsing tasks.
3.1 Corpora
Following MacKinlay et al (2011), we use two pri-
mary training corpora. First, we use the LOGON
corpus (Oepen et al, 2004), a collection of En-
glish translations of Norwegian hiking texts. The
LOGON corpus contains 8550 sentences with ex-
actly one gold parse, which we partitioned ran-
domly by sentence into 10 approximately equal sec-
tions, reserving two sections as test data, and us-
ing the remainder as our training corpus. These
sentences were randomly divided into training and
development data. Secondly, we use the We-
Science (Ytrest?l et al, 2009) corpus, a collection
of Wikipedia articles related to computational lin-
guistics. The corpus contains 11558 sentences, from
which we randomly chose 9632, preserving the re-
mainder for future work. This left 7631 sentences
with a single gold tree, which we divided into a
training set and a development set in the same way.
The corpora are summarised in Table 1.
With these corpora, we are able to investigate in-
domain and cross-domain effects, by testing on a
different corpus to the training corpus, so we can
examine whether sense-tagging alleviates the cross-
domain performance penalty noted in MacKinlay et
al. (2011). We can also use a subset of each training
corpus to simulate the common situation of sparse
training data, so we can investigate whether sense-
tagging enables the learner to make better use of a
limited quantity of training data.
3.2 Evaluation
Our primary evaluation metric is Elementary De-
pendency Match (Dridan and Oepen, 2011). This
converts the semantic output of the ERG into a set
of dependency-like triples, and scores these triples
using precision, recall and F-score as is conven-
tional for other dependency evaluation. Following
MacKinlay et al (2011), we use the EDMNA mode
of evaluation, which provides a good level of com-
parability while still reflecting most the semantically
salient information from the grammar.
Other work on the ERG and related grammars has
tended to focus on exact tree match, but the granu-
lar EDM metric is a better fit for our needs here ?
among other reasons, it is more sensitive in terms
of error rate reduction to changes in parse selection
models (MacKinlay et al, 2011). Additionally, it is
desirable to be able to choose between two different
parses which do not match the gold standard exactly
but when one of the parses is a closer match than the
other; this is not possible with exact match accuracy.
3.3 Reranking for parse selection
The features we are adding to the parse selection
procedure could all in principle be applied by the
parser during the selective unpacking stage, since
they all depend on information which can be pre-
computed. However, we wish to avoid the need for
multiple expensive parsing runs, and more impor-
tantly the need to modify the relatively complex in-
ternals of the parse ranking machinery in the PET
parser (Callmeier, 2000). So instead of performing
the parse ranking in conjunction with parsing, as is
the usual practice, we use a pre-parsed forest of the
top-500 trees for each corpus, and rerank the forest
afterwards for each configuration shown.
The pre-parsed forests use the same models which
were used in treebanking. Using reranking means
that the set of candidate trees is held constant, which
230
means that parse selection models never get the
chance to introduce a new tree which was not in
the original parse forest from which the gold tree
was annotated, which may provide a very small per-
formance boost (although when the parse selection
models are similar as is the case for most of the mod-
els here, this effect is likely to be very small).
3.4 Word Sense Annotations
3.4.1 Using the WordNet Hierarchy
Most experiments we report on here make some
use of the WordNet sense inventory. Obviously we
need to determine the best sense and corresponding
WordNet synset for a given token. We return to this
in Section 3.4.2, but for now assume that the sense
disambiguation is done.
As we are concerned primarily with making
commonalities between lemmas with different base
forms apparent to the parse selection model, the fine-
grained synset ID will do relatively little to provide
a coarser identifier for the token ? indeed, if two
tokens with identical forms were assigned different
synset IDs, we would be obscuring the similarity.1
We can of course make use of the WordNet hier-
archy, and use hypernyms from the hierarchy to tag
each candidate token, but there are a large number
of ways this can be achieved, particularly when it
is possibly to assign multiple labels per token as is
the case here (which we discuss in Section 3.4.3).
We apply two relatively simple strategies. We noted
in Section 2.2 that Agirre et al (2008) found that
the semantic file was useful. This is the coarse lex-
icographic category label, elsewhere denoted super-
sense (Ciaramita and Altun, 2006), which is the
terminology we use. Nouns are divided into 26
coarse categories such as ?animal?, ?quantity? or
?phenomenon?, and verbs into 15 categories such as
?perception? or ?consumption?. In some configura-
tions, denoted SS, we tag each open-class token with
one of the supersense labels.
Another configuration attempts to avoid making
assumptions about which level of the hierarchy will
be most useful for parse disambiguation, instead
leaving it the MaxEnt parse ranker to pick those la-
bels from the hierarchy which are most useful. Each
1This could be useful for verbs since senses interact strongly
subcategorisation frames, but that is not our focus here.
open class token is labelled with multiple synsets,
starting with the assigned leaf synset and travelling
as high as possible up the hierarchy, with no distinc-
tion made between the different levels in the hier-
archy. Configurations using this are designated HP,
for ?hypernym path?.
3.4.2 Disambiguating senses
We return now to the question of determination
of the synset for a given token. One frequently-
used and robust strategy is to lemmatise and POS-
tag each token, and assign it the first-listed sense
from WordNet (which may or may not be based on
actual frequency counts). We POS-tag using TnT
(Brants, 2000) and lemmatise using WordNet?s na-
tive lemmatiser. This yields a leaf-level synset, mak-
ing it suitable as a source of annotations for both SS
and HP. We denote this ?WNF? for ?WordNet First?
(shown in parentheses after SS or HP).
Secondly, to evaluate whether a more informed
approach to sense-tagging helps beyond the naive
WNF method, in the ?SST? method, we use the out-
puts of SuperSense Tagger (Ciaramita and Altun,
2006), which is optimised for assigning the super-
senses described above, and can outperform a WNF-
style baseline on at least some datasets. Since this
only gives us coarse supersense labels, it can only
provide SS annotations, as we do not get the leaf
synsets needed for HP. The input we feed in is POS-
tagged with TnT as above, for comparability with
the WNF method, and to ensure that it is compati-
ble with the configuration in which the corpora were
parsed ? specifically, the unknown-word handling
uses a version of the sentences tagged with TnT. We
ignore multi-token named entity outputs from Su-
perSense Tagger, as these would introduce a con-
founding factor in our experiments and also reduce
comparability of the results with the WNF method.
3.4.3 A distributional thesaurus method
A final configuration attempts to avoid the need
for curated resources such as WordNet, instead us-
ing an automatically-constructed distributional the-
saurus (Lin, 1998). We use the thesaurus from
McCarthy et al (2004), constructed along these
lines using the grammatical relations from RASP
(Briscoe and Carroll, 2002) applied to 90 millions
words of text from the British National Corpus.
231
root_frag
np_frg_c
hdn_bnp_c
aj-hdn_norm_c
legal_a1
"legal"
n_pl_olr
issue_n1
"issues"
Figure 1: ERG derivation tree for the phrase Legal issues
[n_-_c_le "issues"]
[n_pl_olr n_-_c_le "issues"]
[aj-hdn_norm_c n_pl_olr n_-_c_le "issues"]
(a) Original features
[n_-_c_le noun.cognition]
[n_pl_olr n_-_c_le noun.cognition]
[aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition]
(b) Additional features in leaf mode, which augment the original
features
[noun.cognition "issues"]
[n_pl_olr noun.cognition "issues"]
[aj-hdn_norm_c n_pl_olr noun.cognition "issues"]
(c) Additional features in leaf-parent (?P?) mode, which augment
the original features
Figure 2: Examples of features extracted from for
"issues" node in Figure 1 with grandparenting level
of 2 or less
To apply the mapping, we POS-tag the text with
TnT as usual, and for each noun, verb and adjec-
tive we lemmatise the token (with WordNet again,
falling back to the surface form if this fails), and
look up the corresponding entry in the thesaurus. If
there is a match, we select the top five most simi-
lar entries (or fewer if there are less than five), and
use these new entries to create additional features,
as well as adding a feature for the lemma itself in all
cases. This method is denoted LDT for ?Lin Distri-
butional Thesaurus?. We note that many other meth-
ods could be used to select these, such as different
numbers of synonyms, or dynamically changing the
number of synonyms based on a threshold against
the top similarity score, but this is not something we
evaluate in this preliminary investigation.
Adding Word Sense to Parse Selection Models
We noted above that parse selection using the
methodology established by Velldal (2007) uses
human-annotated incorrect and correct derivation
trees to train a maximum entropy parse selection
model. More specifically, the model is trained using
features extracted from the candidate HPSG deriva-
tion trees, using the labels of each node (which are
the rule names from the grammar) and those of a
limited number of ancestor nodes.
As an example, we examine the noun phrase Le-
gal issues from the WESCIENCE corpus, for which
the correct ERG derivation tree is shown in Figure 1.
Features are created by examining each node in the
tree and at least its parent, with the feature name set
to the concatenation of the node labels. We also gen-
erally make used of grandparenting features, where
we examine earlier ancestors in the derivation tree.
A grandparenting level of one means we would also
use the label of the grandparent (i.e. the parent?s par-
ent) of the node, a level of two means we would add
in the great-grandparent label, and so on. Our exper-
iments here use a maximum grandparenting level of
three. There is also an additional transformation ap-
plied to the tree ? the immediate parent of each leaf
is, which is usually a lexeme, is replaced with the
corresponding lexical type, which is a broader par-
ent category from the type hierarchy of the grammar,
although the details of this are not relevant here.
For the node labelled "issues" in Figure 1 with
grandparenting levels from zero to two, we would
extract the features as shown in Figure 2(a) (where
the parent node issue_n1 has already been re-
placed with its lexical type n_-c_le).
In this work here, we create variants of these fea-
tures. A preprocessing script runs over the training
or test data, and for each sentence lists variants of
each token using standoff markup indexed by char-
acter span, which are created from the set of addi-
tional semantic tags assigned to each token by the
word sense configuration (from those described in
Section 3.4) which is currently in use. These sets of
semantic tags for a given word could be a single su-
persense tag, as in SS, a set of synset IDs as in HP
or a set of replacement lemmas in LDT. In all cases,
the set of semantic tags could also be empty ? if ei-
ther the word has a part of speech which we are not
232
Test Train SS (WNF) SSp(WNF)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 85.09/82.33/83.69 +0.09 84.81/82.20/83.48 ?0.11
WESC (92k) 86.56/83.58/85.05 86.83/84.04/85.41 +0.36 87.03/83.96/85.47 +0.42
LOG (23k) 88.60/87.23/87.91 88.72/87.20/87.95 +0.04 88.43/87.00/87.71 ?0.21
LOG (92k) 91.74/90.15/90.94 91.82/90.07/90.94 ?0.00 91.90/90.13/91.01 +0.07
WESC
WESC (23k) 86.80/84.43/85.60 87.12/84.44/85.76 +0.16 87.18/84.50/85.82 +0.22
WESC (92k) 89.34/86.81/88.06 89.54/86.76/88.13 +0.07 89.43/87.23/88.32 +0.26
LOG (23k) 83.74/81.41/82.56 84.02/81.43/82.71 +0.15 84.10/81.67/82.86 +0.31
LOG (92k) 85.98/82.93/84.43 86.02/82.69/84.32 ?0.11 85.89/82.76/84.30 ?0.13
Table 2: Results for SS (WNF) (supersense from first WordNet sense), evaluated on 23k tokens (approx 1500
sentences) of either WESCIENCE or LOGON, and trained on various sizes of in-domain and cross-domain training
data. Subscript ?p? indicates mappings were applied to leaf parents rather than leaves.
Test Train SS (SST) SSp(SST)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.97/82.38/83.65 +0.06 85.32/82.66/83.97 +0.37
WESC (92k) 86.56/83.58/85.05 87.05/84.47/85.74 +0.70 86.98/83.87/85.40 +0.35
LOG (23k) 88.60/87.23/87.91 88.93/87.50/88.21 +0.29 88.84/87.40/88.11 +0.20
LOG (92k) 91.74/90.15/90.94 91.67/90.02/90.83 ?0.10 91.47/89.96/90.71 ?0.23
WESC
WESC (23k) 86.80/84.43/85.60 86.88/84.29/85.56 ?0.04 87.32/84.48/85.88 +0.27
WESC (92k) 89.34/86.81/88.06 89.53/86.54/88.01 ?0.05 89.50/86.56/88.00 ?0.05
LOG (23k) 83.74/81.41/82.56 84.06/81.30/82.66 +0.10 83.96/81.64/82.78 +0.23
LOG (92k) 85.98/82.93/84.43 86.13/82.96/84.51 +0.08 85.76/82.84/84.28 ?0.16
Table 3: Results for SS (SST) (supersense from SuperSense Tagger)
Test Train HPWNF HPp(WNF)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.56/82.03/83.28 ?0.32 84.74/82.20/83.45 ?0.15
WESC (92k) 86.56/83.58/85.05 86.65/84.22/85.42 +0.37 86.41/83.65/85.01 ?0.04
LOG (23k) 88.60/87.23/87.91 88.58/87.26/87.92 +0.00 88.58/87.35/87.96 +0.05
LOG (92k) 91.74/90.15/90.94 91.68/90.19/90.93 ?0.01 91.66/89.85/90.75 ?0.19
WESC
WESC (23k) 86.80/84.43/85.60 86.89/84.19/85.52 ?0.08 87.18/84.43/85.78 +0.18
WESC (92k) 89.34/86.81/88.06 89.74/86.96/88.33 +0.27 89.23/86.88/88.04 ?0.01
LOG (23k) 83.74/81.41/82.56 83.87/81.20/82.51 ?0.04 83.47/81.00/82.22 ?0.33
LOG (92k) 85.98/82.93/84.43 85.89/82.38/84.10 ?0.33 85.75/83.03/84.37 ?0.06
Table 4: Results for HPWNF (hypernym path from first WordNet sense)
Test Train LDTp(5)
P/ R/ F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.48/82.18/83.31 ?0.28
WESC (92k) 86.56/83.58/85.05 86.36/84.14/85.23 +0.19
LOG (23k) 88.60/87.23/87.91 88.28/86.99/87.63 ?0.28
LOG (92k) 91.74/90.15/90.94 91.01/89.25/90.12 ?0.82
WESC
WESC (23k) 86.80/84.43/85.60 86.17/83.51/84.82 ?0.78
WESC (92k) 89.34/86.81/88.06 88.31/85.61/86.94 ?1.12
LOG (23k) 83.74/81.41/82.56 83.60/81.18/82.37 ?0.19
LOG (92k) 85.98/82.93/84.43 85.74/82.96/84.33 ?0.11
Table 5: Results for LDT (5) (Lin-style distributional thesaurus, expanding each term with the top-5 most similar)
233
attempting to tag semantically, or if our method has
no knowledge of the particular word.
The mapping is applied at the point of feature ex-
traction from the set of derivation trees ? at model
construction time for the training set and at rerank-
ing time for the development set. If a given leaf to-
ken has some set of corresponding semantic tags, we
add a set of variant features for each semantic tag,
duplicated and modified from the matching ?core?
features described above. There are two ways these
mappings can be applied, since it is not immedi-
ately apparent where the extra lexical generalisation
would be most useful. The ?leaf? variant applies to
the leaf node itself, so that in each feature involving
the leaf node, add a variant where the leaf node sur-
face string has been replaced with the new seman-
tic tag. The ?parent? variant, which has a subscript
?P? (e.g. SSp(WNF) ) applies the mapping to the
immediate parent of the leaf node, leaving the leaf
itself unchanged, but creating variant features with
the parent nodes replaced with the tag.
For our example here, we assume that we have
an SS mapping for Figure 2(a), and that this has
mapped the token for "issues" to the WordNet
supersense noun.cognition. For the leaf vari-
ant, the extra features that would be added (either for
considering inclusion in the model, or for scoring a
sentence when reranking) are shown in Figure 2(b),
while those for the parent variant are in Figure 2(c).
3.4.4 Evaluating the contribution of sense
annotations
Wewish to evaluate whether adding sense annota-
tions improve parser accuracy against the baseline of
training a model in the conventional way using only
syntactic features. As noted above, we suspect that
this semantic generalisation may help in cases where
appropriate training data is sparse ? that is, where
the training data is from a different domain or only
a small amount exists. So to evaluate the various
methods in these conditions, we train models from
small (23k token) training sets and large (96k token)
training sets created from subsets of each corpus
(WESCIENCE and LOGON). For the baseline, we
train these models without modification. For each
of the various methods of adding semantic tags, we
then re-use each of these training sets to create new
models after adding the appropriate additional fea-
tures as described above, to evaluate whether these
additional features improve parsing accuracy
4 Results
We present an extensive summary of the results ob-
tained using the various methods in Tables 2, 3, 4
and 5. In each case we show results for applying
to the leaf and to the parent. Aggregating the re-
sults for each method, the differences range between
substantially negative and modestly positive, with a
large number of fluctuations due to statistical noise.
LDT is the least promising performer, with only
one very modest improvement, and the largest de-
creases in performance, of around 1%. The HP-
WNF and HPp(WNF) methods make changes in
either direction ? on average, over all four train-
ing/test combinations, there are very small drops
in F-score of 0.02% for HPWNF, and 0.06% for
HPp(WNF), which indicates that neither of the
methods is likely to be useful in reliably improving
parser performance.
The SS methods are more promising. SS (WNF)
and SSp(WNF) methods yield an average im-
provement of 0.10% each, while SS (SST) and
SSp(SST) give average improvements of 0.12%
and 0.13% respectively (representing an error rate
reduction of around 1%). Interestingly, the increase
in tagging accuracy we might expect using Super-
Sense Tagger only translates to a modest (and prob-
ably not significant) increase in parser performance,
possibly because the tagger is not optimised for the
domains in question. Amongst the statistical noise
it is hard to discern overall trends; surprisingly, it
seems that the size of the training corpus has rela-
tively little to do with the success of adding these su-
persense annotations, and that the corpus being from
an unmatched domain doesn?t necessarily mean that
sense-tagging will improve accuracy either. There
may be a slight trend for sense annotations to be
more useful when WESCIENCE is the training cor-
pus (either in the small or the large size).
To gain a better insight into how the effects
change as the size of the training corpus changes for
the different domains, we created learning curves for
the best-performing method, SSp(SST) (although
as noted above, all SS methods give similar levels
of improvement), shown in Figure 3. Overall, these
234
0 20 40 60 80 100
Training Tokens (thousands)
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
E
D
M
N
A
 
F
-
s
c
o
r
e
Trained on LOGON Corpus
Test Corpus
LOGON*
LOGON* +SS
WeSc
WeSc +SS
(a) LOGON
0 20 40 60 80 100
Training Tokens (thousands)
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
E
D
M
N
A
 
F
-
s
c
o
r
e
Trained on WeScience Corpus
Test Corpus
LOGON
LOGON +SS
WeSc*
WeSc* +SS
(b) WESCIENCE
Figure 3: EDMNA learning curves for SS (SST) (supersense from SuperSense Tagger). ?*? denotes in-domain
training corpus.
graphs support the same conclusions as the tables
? the gains we see are very modest and there is a
slight tendency for WESCIENCE models to benefit
more from the semantic generalisation, but no strong
tendencies for this to work better for cross-domain
training data or small training sets.
5 Conclusion
We have presented an initial study evaluat-
ing whether a fairly simple approach to using
automatically-created coarse semantic annotations
can improve HPSG parse selection accuracy using
the English Resource Grammar. We have provided
some weak evidence that adding features based on
semantic annotations, and in particular word super-
sense, can provide modest improvements in parse
selection performance in terms of dependency F-
score, with the best-performing method SSp(SST)
providing an average reduction in error rate over 4
training/test corpus combinations of 1%. Other ap-
proaches were less promising. In all configurations,
there were instances of F-score decreases, some-
times substantial.
It is somewhat surprising that we did not achieve
reliable performance gains which were seen in the
related work described above. One possible expla-
nation is that the model training parameters were
suboptimal for this data set since the characteris-
tics of the data are somewhat different than with-
out sense annotations. The failure to improve some-
what mirrors the results of Clark (2001), who was at-
tempting to improve the parse ranking performance
of the unification-based based probabilistic parser of
Carroll and Briscoe (1996). Clark (2001) used de-
pendencies to rank parses, and WordNet-based tech-
niques to generalise this model and learn selectional
preferences, but failed to improve performance over
the structural (i.e. non-dependency) ranking in the
original parser. Additionally, perhaps the changes
we applied in this work to the parse ranking could
possibly have been more effective with features
based on semantic dependences as used by Fujita
et al (2007), although we outlined reasons why we
wished to avoid this approach.
This work is preliminary and there is room for
more exploration in this space. There is scope for
much more feature engineering on the semantic an-
notations, such as using different levels of the se-
mantic hierarchy, or replacing the purely lexical fea-
tures instead of augmenting them. Additionally,
more error analysis would reveal whether this ap-
proach was more useful for avoiding certain kinds
of parser errors (such as PP-attachment).
Acknowledgements
NICTA is funded by the Australian Government as
represented by the Department of Broadband, Com-
munications and the Digital Economy and the Aus-
tralian Research Council through the ICT Centre of
Excellence program.
235
References
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of ACL-08: HLT, pages
317?325, Columbus, Ohio, June.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency parsing
with semantic classes. In Proceedings of the 49th An-
nual Meeting of the Association of Computational Lin-
guistics, ACL-HLT 2011 Short Paper, Portland, Ore-
gon?.
D. M. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of the second international conference on Human
Language Technology Research, pages 178?182, San
Francisco, CA, USA.
T. Brants. 2000. Tnt ? a statistical part-of-speech tag-
ger. In Proceedings of the Sixth Conference on Ap-
plied Natural Language Processing, pages 224?231,
Seattle, Washington, USA, April.
T. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd International Conference on Language Resources
and Evaluation, pages 1499?1504.
U. Callmeier. 2000. Pet ? a platform for experimenta-
tion with efficient HPSG processing techniques. Nat.
Lang. Eng., 6(1):99?107.
J. Carroll and E. Briscoe. 1996. Apportioning devel-
opment effort in a probabilistic lr pars- ing system
through evaluation. In Proceedings of the SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 92?100, Philadelphia, PA.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st North American chapter of
the Association for Computational Linguistics confer-
ence, pages 132?139.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594?602, Sydney, Australia,
July.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104?111.
S. Clark. 2001. Class-based Statistical Models for Lex-
ical Knowledge Acquisition. Ph.D. thesis, University
of Sussex.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23, Madrid, Spain, July.
R. Dridan and S. Oepen. 2011. Parser evaluation us-
ing elementary dependency matching. In Proceedings
of the 12th International Conference on Parsing Tech-
nologies, pages 225?230, Dublin, Ireland, October.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Nat. Lang. Eng., 6
(1):15?28.
S. Fujita, F. Bond, S. Oepen, and T. Tanaka. 2007. Ex-
ploiting semantic information for HPSG parse selec-
tion. In ACL 2007 Workshop on Deep Linguistic Pro-
cessing, pages 25?32, Prague, Czech Republic, June.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2,
pages 768?774.
A. MacKinlay, R. Dridan, D. Flickinger, and T. Baldwin.
2011. Cross-domain effects on parse selection for pre-
cision grammars. Research on Language & Computa-
tion, 8(4):299?340.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
the Sixth Conference on Natural Language Learning
(CoNLL-2002), pages 49?55.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
the penn treebank. Comput. Linguist., 19(2):313?330.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant word senses in untagged text. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 279?es.
G.A. Miller. 1995. WordNet: a lexical database for En-
glish. Communications of the ACM, 38(11):39?41.
S. Oepen, D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods: A rich and dynamic
treebank for HPSG. Research on Language & Com-
putation, 2(4):575?596.
M. Siegel and E.M. Bender. 2002. Efficient deep pro-
cessing of japanese. In Proceedings of the 3rd work-
shop on Asian language resources and international
standardization-Volume 12, pages 1?8.
E. Velldal. 2007. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo Department of Informatics.
G. Ytrest?l, D. Flickinger, and S. Oepen. 2009. Ex-
tracting and annotating Wikipedia sub-domains ? to-
wards a new eScience community resourc. In Pro-
ceedings of the Seventh International Workshop on
Treebanks and Linguistic Theories, Groeningen, The
Netherlands, January.
Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency in
unification-based n-best parsing. In IWPT ?07: Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, pages 48?59, Morristown, NJ, USA.
236
