Workshop on Computational Linguistics for Literature, pages 45?53,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Function Words for Chinese Authorship Attribution 
 
Bei Yu 
School of Information Studies 
Syracuse University 
byu@syr.edu 
  
 
 
Abstract 
This study explores the use of function 
words for authorship attribution in modern 
Chinese (C-FWAA). This study consists of 
three tasks: (1) examine the C-FWAA 
effectiveness in three genres: novel, essay, 
and blog; (2) compare the strength of 
function words as both genre and 
authorship indicators, and explore  the 
genre interference on C-FWAA; (3) 
examine whether C-FWAA is sensitive to 
the time periods when the texts were 
written. 
1 Introduction 
Function words are an important feature set for 
Authorship Attribution (hereafter ?AA?) because 
they are considered topic-independent or context-
free, and that they are largely used in an 
unconscious manner (Holmes, 1994; Stamatatos, 
2009; Koppel et al, 2009). The Federalist Papers 
(Mostellar and Wallace, 1964) may be the most 
famous example of AA in English. Mostellar and 
Wallace (1964) conducted a detailed study of 
searching and testing function words to distinguish 
Hamilton and Madison as the authors of the 
disputed Federalist Papers.  
Although Function Word based Authorship 
Attribution (hereafter ?FWAA?) has been 
successful in many studies (Stamatatos, 2009), 
Juola (2008) argued that FWAA are mainly 
applied in English texts, and it may not be 
appropriate for other highly inflected languages, 
like Finnish and Turkish. This may not be the case 
in that it is the content words, not the function 
words, that are inflected in those languages. 
However, function words are indeed rarely used 
for AA in non-English texts. It was left out in the 
comprehensive authorship analysis of The Quiet 
Don (in Russian) by Kjetsaa et al (1984). The 
literature review for this study found several 
examples of FWAA in Modern Greek (Mikros and 
Argiri, 2003) and Arabic (Abbasi and Chen, 2005). 
Overall, the effectiveness of FWAA has not been 
tested on many languages. 
Some studies on FWAA also reported 
negative results. Holmes (1994), in his 
comprehensive survey on authorship attribution, 
cited doubts given by (Damerau, 1975) and 
(Oakman, 1980),  and called for further 
investigation on the stability of function word use 
within an author?s work and between works by the 
same author.  
Another problem for FWAA is to explain 
exactly what authorial characteristics are captured 
by function words, since function words may also 
characterize other textual properties like genre, 
author gender, and even topic, although function 
words are generally considered topic-independent 
or context-free (Stamatatos, 2009; Herring and 
Paolillo, 2006; Clement and Sharp 2003; Mikros 
and Argiri, 2007).  
Clement and Sharp (2003) found that function 
words worked as well as content words in 
identifying document topics. Their further 
investigation showed that author and topic are not 
arbitrarily orthogonal to each other. Using the 
significance level of two-way ANOVA test as 
measure, Mikros and Argiri (2007) found that 
some function words in Modern Greek can 
distinguish both topic and author, providing further 
evidence for possible topic-author correlation 
based on function word dimensions. 
Function words are also used as indicators for 
author gender (Argamon et al, 2002; Koppel et al, 
2003) and text genre (Biber, 1993). Koppel et al 
(2003) found gender preference on certain personal 
45
pronouns and prepositions. Herring and Paolillo 
(2006) repeated Argamon and Koppel?s 
experiment by mixing genre and gender in the data 
set, and discovered that the same gender indicators 
actually captured genre characteristics. 
In summary, related work has shown that 
function words may contribute to distinguishing 
topic, authorship, author gender, and genre. A 
question soon emerges: which dimension do 
function words characterize the most saliently? In 
other words, given a document set of mixed author, 
topic, and genre, would they interfere with each 
other in classification tasks? Answer to this 
question would help guide experiment design for 
AA tasks, and explain the real authorial 
characteristics captured by function words.  
This paper aims to study the use of function 
words for Chinese authorship attribution (C-
FWAA), since FWAA has not been well-studied in 
Chinese. Existing studies of C-FWAA are limited 
to the analysis of famous authorship dispute cases 
like whether Gao E or Cao Xueqin wrote the last 
40 chapters of the Dream of the Red Chamber, and 
no consensus was reached among these C-FWAA 
studies (Zeng and Zhu, 2006). Therefore no 
baseline was available yet for general-purpose C-
FWAA studies. 
This study consists of three tasks. First, 
examine the effectiveness of C-FWAA in three 
genres of creative writing: novel, essay, and blog. 
Second, compare the strength of function words as 
both genre and authorship indicators, and explore 
the genre interference on C-FWAA. Third, 
examine whether C-FWAA is sensitive to the time 
periods when the texts were written.  
The third task is proposed for a unique reason 
that the influence of ancient Chinese (???) on 
modern Chinese (??? ) may affect function 
word use. For example, ?also? corresponds to ??? 
in ancient Chinese, and ??? in modern Chinese. 
??? (??s? or ?of?), ??? (?-ly?), and ??? (?so?) 
are only used in modern Chinese. The government 
of Republic of China (RoC, 1912-1949) and the 
government of People?s Republic of China (PRC, 
1949- ) both made changes to the Chinese 
language. Hence the hypothesis is that Chinese 
function word use may also reflect the time period 
of literary works.  
2 Experiment set up 
2.1 Constructing Chinese function word list  
Various function word lists have been used in AA 
tasks in English, and the selection process usually 
follows arbitrary criteria (Stamatatos, 2009). To 
construct the Chinese function word list, this study 
chose 300 most frequent characters from Jun Da?s 
Modern Chinese Character Frequency List (Du, 
2005), removing the characters that contain solid 
meaning, e.g. ??? (?to come?), and removing all 
personal pronouns, e.g. ??? (?myself?) in that 
they have been known as genre/register indicators 
(Biber, 1993). This screening process resulted in 
35 function words (see Table 1). Detailed English 
translation can be found in (Du, 2005).  
Every text document was then converted to a 
vector of 35 dimensions, each corresponding to 
one function word. The value for each dimension 
is the corresponding function word?s number of 
occurrences per thousand words. 
 
? / of ? / be,yes ? / no ?/* 
? / at/in ? / exist ? / this ? / for 
? / -ly ? / also ? / so ? / then 
? / that ? /** ? / *** ? / of 
? / can ? / question ? / but ? / so 
? / no ? / at ? / also ? / only 
? / no ? / also ? / if ? / but 
? / it ? / this ? / and ? / hold 
? / all ? / passive ? / but  
Note: * completion mark; ** according to; *** on-going 
status mark 
 
Table 1: Chinese function word list 
2.2 EM clustering algorithm  
This study chose EM clustering algorithm as the 
main method to evaluate the effectiveness of C-
FWAA. Most AA studies use supervised learning 
methods in that AA is a natural text categorization 
problem. However, training data may not be 
available in many AA tasks, and unsupervised 
learning methods are particularly useful in such 
cases. In addition, this study aims to examine the 
clusters emerging from the data and explain 
whether they represent authors, genres, or time 
periods.  
46
This study uses Weka?s Simple EM 
algorithm for all experiments. This algorithm first 
runs k-Means 10 times with different random 
seeds, and then chooses the partition with minimal 
squared error to start the expectation maximization 
iteration. Weka calculates the clustering accuracy 
as follows: after clustering the data, Weka 
determines the majority class in each cluster and 
prints a confusion matrix showing how many 
errors there would be if the clusters were used 
instead of the true class (Witten et al, 2011). 
2.3 Selecting writers and their works 
To exclude gender?s affect, all writers chosen in 
this study are males. Parallel analysis for female 
writers will be conducted in future work. 
Representative writers from three different time 
periods were selected to examine the relationship 
between time period and function word use. The 
first time period (TP1) is the 1930-40s, when 
modern Chinese replaced ancient Chinese to be the 
main form of writing in China, and before the PRC 
was founded. The second time period (TP2) is the 
1980-90s, after the Cultural Revolution was over. 
The third time period (TP3) is the 2000s, when the 
publishing business has been strongly affected by 
the free-market economy. Three representative 
writers were chosen for each time period. The time 
period from the foundation of PRC (1949) to the 
end of the Cultural Revolution was excluded from 
this study because during that time most literary 
works were written under strong political 
guidelines. Tables 2 and 3 listed the representative 
writers and their selected works. Two long novels 
are separated into chapters in order to test whether 
C-FWAA is able to assign all chapters in a book to 
one cluster. Common English translations of the 
titles are found through Google Search. Chinese 
Pin Yin was provided for hard-to-translate titles. 
All writers have to meet the requirements that 
their works cross at least two genres: fiction (novel) 
and non-fiction (essay). The TP3 (2000s) writers 
should have well-maintained blogs as well. 
Therefore this study will examine C-FWAA 
effectiveness in three genres: novel, essay, and 
blog. 
All electronic copies of the selected works were 
downloaded from online literature repositories 
such as YiFan Public Library1 and TianYa Book2. 
                                                          
1 URL http://www.shuku.net:8082/novels/cnovel.html 
Time period Authors 
TP1  
(1930-40s) 
???(Shen CongWen, SCW) 
???(Qian ZhongShu, QZS) 
???(Wang ZengQi, WZQ) 
TP2  
(1980-90s) 
??(Wang Shuo, WS) 
???(Wang XiaoBo, WXB) 
???(Jia PingWa, JPW) 
TP3  
(2000s) 
???(Guo JingMing, GJM) 
??(Han Han, HH) 
??(Shi Kang, SK) 
 
Table 2: selected writers in three time periods 
 
TP Writer #Novels essays blogs 
1 
???3 WZQ 5 6  
??? QZS 14* 10  
??? SCW 11** 7  
2 
?? WS 5 16 30 
??? WSB 3 10  
??? JPW 3 10  
3 
??? GJM 8 6  
?? HH 5 11 92 
?? SK 4 14 30 
Note: *one long novel ??(Fortress Besieged) is 
separated into 10 chapters. **one long novel ??
(Border Town) is separated into 7 chapters. 
 
Table 3: statistics of selected works 
3 Experiment and result  
3.1 Test the effectiveness of EM algorithm for 
FWAA 
The first experiment was to test the effectiveness 
of the EM algorithm for FWAA. The famous 
Federalist Papers data set was used as the test case. 
The Federalist Papers experiment was repeated 
using the function words provided in (Mostellar 
and Wallace, 1964). The original Ferderalist 
Papers and their author identifications were 
downloaded from the Library of Congress website4. 
Function words were extracted using a Perl script 
and the word frequencies (per thousand words) 
were calculated. The 85 essays consist of 51 by 
Hamilton, 15 by Madison, 3 jointly by Hamilton 
                                                                                           
2 URL http://www.tianyabook.com/ 
3???(Wang Zengqi) is an exception in that his writing 
career started in the 1930s but peaked in the 1980s. 
4 URL: http://thomas.loc.gov/home/histdox/fedpapers.html 
47
and Madison, 5 by Jay, and 11 with disputed 
authorship. Mosteller and Wallace (1964) 
supported the opinion that Madison wrote all 11 
disputed essays, which is also the mainstream 
opinion among historians. 
In the first round of experiment, Jay?s five 
essays and the three jointly-written ones were 
excluded, making the task easier. The cluster 
number was set to two. EM returned results similar 
to that in (Mostellar and Wallace, 1964) by 
assigning all disputed papers to Madison (Table 4). 
However it did make several mistakes by assigning 
3 Hamilton?s essays to Madison and one 
Madison?s essay to Hamilton, resulting in an 
overall accuracy of (66-4)/66=94% in the not-
disputed subset. 
 
 C0 (Hamilton) C1 (Madison) 
Hamilton 48 3 
Madison 1 14 
Disputed 0 11 
 
Table 4: Hamilton vs. Madison (clustering 
errors in bold) 
 
In the second round Jay?s five essays were 
added to the test data. The cluster number was then 
changed to three. The EM algorithm successfully 
attributed the essays to their real authors with only 
one error (assigning one Madison?s essay to Jay, 
see the confusion matrix in Table 5). It also 
assigned all disputed essays to Madison. The 3-
author AA result in Table 4 seems even better than 
the 2-author AA result, but the difference is small. 
 
 C 0 C1 C 2 
Hamilton 51 0 0 
Madison 0 14 1 
Jay 0 0 5 
Disputed 0 11 0 
  
Table 5: Hamilton vs. Madison vs. Jay 
 
In the third round the three jointly-written 
essays were added to the test data. These jointly-
written essays may resemble either Hamilton or 
Madison, which would result in 3 clusters still, or 
they may exhibit a unique style and thus form a 
new cluster. The test result shows that these three 
jointly-authored essays did confuse the algorithm 
no matter if the cluster number is set to three or 
four. When setting the cluster number to three 
(Table 6), all three joint essays were assigned to 
C2, which also attracted 11 Hamilton?s, 2 
Madison?s, 2 Jay?s, and 1 disputed essays. 
Increasing the cluster number to 4 does not reduce 
the confusion: Hamilton still dominated Cluster 0 
with 40 out of 51 essays in it; C1 is still dominated 
by Madison (13 out of 15) and the disputed essays 
(9 out 11). Jay?s essays were split into C1 and C2. 
This result actually shows that function words are 
highly sensitive to noise like the jointly-written 
essays. 
 
 C0 C1 C2 
H-M 0 0 3 
Hamilton 40 0 11 
Madison 0 13 2 
Jay 0 3 2 
disputed 1 9 1 
  
Table 6: impact of the jointly-written essays 
3.2 Chinese FWAA with genre and time 
period controlled  
This section describes the experiments and results 
for task 1: evaluating the effectiveness of C-
FWAA using EM and the 35 Chinese function 
words as features. Controlling the time period and 
genre, the same experiment was repeated for each 
genre and each TP.  
In the first round, the authors within each TP 
were paired up in the novel genre to distinguish 
them, which is expected to be easier than 
distinguishing multiple authors. The results in 
Table 7 show that the authors of TP1 and TP2 
novels are perfectly distinguishable, but those in 
TP3 are not.  
Compared to the writers of TP1 and TP2, 
writers in TP3 face a new market-driven economy. 
Writing-for-profit becomes acceptable and even 
necessary for many writers. TP3 writers like Han 
Han (HH) and Guo JingMing (GJM) obtained huge 
financial success from the publication market. 
Both of them also received doubts regarding the 
authenticity of their works.  
Guo Jingming was found to plagiarize in his 
book Meng Li Hua Luo Zhi Duo Shao, which was 
also not assigned to his main cluster by C-FWAA. 
Guo JingMing founded a writing studio and hired 
48
employees to publish and market his books. He 
publicly admits the existence of ?group writing? 
practice in his studio because his name is used 
more as a brand than as an author. 
C-FWAA also encountered difficulty in 
distinguishing Han Han and Shi Kang?s novels. 
This finding is consistent with the fact that Han 
Han publicly acknowledged that his Xiang Shao 
Nian La Fei Chi mimicked Shi Kang?s style. Since 
the beginning of 2012, a huge debate surged in 
Chinese social media over whether Han Han?s 
books and blogs were ghost-penned by his father 
and others. In this striking ?crowd-sourcing 
Shelock Holmes? movement, numerous doubts 
were raised based on netizens? amateur content 
analysis on contradicting statements in Han Han?s 
public videos and different book versions. A 
separate study is undergoing to analyze the stylistic 
similarity between Han Han and the candidate 
pens.  
As described in Section 3.1, FWAA is highly 
sensitive to noise like joint authorship. This may 
explain the low performance of C-FWAA in TP3 
when plagiarism, group writing, and ghostwriting 
are involved.  
After C-FWAA on the novel genre, the same 
experiment was then repeated on the other two 
genres: essay and blog. The results in Table 7 show 
an average accuracy .87 for essays and .83 for 
blogs. Overall, this round of experiment 
demonstrates that C-FWAA is effective in 
distinguishing two authors in all genres and time 
periods. 
 
 Author pair Novel Essay Blog 
TP1 
WZQ-SCW 1 .77  
SCW-QZS 1 .94  
WZQ-QZS 1 .81  
TP2 
WS-JPW 1 1.00  
WS-WXB 1 .96  
WXB-JPW 1 .85  
TP3 
GJM-HH .77 1  
GJM-SK .75 .65  
HH-SK .56 .84 .84 
TP2-3 HH-WS   .77 
 SK-WS   .88 
avg  .90 .87 .83 
 
Table 7: pair-wise C-FWAA  
 
In the second round C-FWAA was tested on the 
task of distinguishing three authors, also starting 
from the novel genre and TP1. In the 3-cluster 
result (Table 8), C0 is devoted to SCW?s novel ?
? (Border Town), a masterpiece in Chinese 
literature, C1 captured all other SCW novels, and 
WZQ and QZS remain in C2 together. WZQ and 
QZS were further separated after increasing the 
cluster number to four (with only two errors, 
highlighted in Table 8, of assigning QZS?s two 
works God?s Dream and the Foreword of Fortress 
Besieged to SCW). Two long novels that are 
separated into chapters are also successfully 
assigned into same clusters except for the 
Foreword of Fortress Besieged.  
The 3-author experiment was then repeated on 
TP2 and obtained 100% accurate results.   
The 3-author AA result for TP3 is similar to its 
2-author result: HH and SK remain in one cluster. 
When increasing the cluster number to 4, GJM still 
dominated C0 and C1, but now HH and SK were 
separated into C2 and C3 respectively. 
The C-FWAA accuracy was then calculated by 
choosing the better result from 3-cluster and 4-
cluster experiments (Table 8). Overall, C-FWAA is 
able to distinguish three authors in the novel genre 
effectively. 
 
30s-
40s 
cluster num = 3 cluster num = 4 
C0 C1 C2 C0 C1 C2 C3 
SCW 7 4 0 7 4 0 0 
WZQ 0 0 5 0 0 5 0 
QZS 0 0 14 0 2 0 12 
 
2000s cluster num = 3 cluster num = 4 
C0 C1 C2 C0 C1 C2 C3 
GJM 4 3 1 4 3 1 0 
HH 1 0 4 1 0 3 1 
SK 0 1 3 0 1 0 3 
 
TP Accuracy  
TP1 28/30=.93 
TP2 11/11=1.00 
TP3 13/17=.76 
Avg .90 
 
Table 8: 3-author C-FWAA on Chinese novels 
  
The above experiment was then repeated on the 
essay and blog genres. In the essay genre, the 
49
average 3-author C-FWAA accuracy is .83, .89, 
.84 for TP1, TP2, and TP3 respectively (Table 9), 
average accuracy .85. For blogs the accuracy is .68 
(Table 10).  
 
30s-
40s 
TP1  2000s TP2 C0 C1 C2 C0 C1 C2 
SCW 5 2 0 GJM 6 0 0 
WZQ 0 6 0 HH 0 11 0 
QZS 0 2 8 SK 1 4 9 
 
80s-
90s 
cluster num = 3 cluster num = 4 
C0 C1 C2 C0 C1 C2 C3 
WS 16 0 0 15 0 1 0 
WXB 0 10 0 0 8 1 1 
JPW 2 4 4 0 0 1 9 
 
Time period Accuracy  
1930s-1940s 19/23=.83 
1980s-1990s 32/36=.89 
2000s 26/31=.84 
Average .85 
 
Table 9: 3-author C-FWAA on Chinese essays 
 
Acc=104/152=.68 C0 C1 C2 C3 C4 
HH 63 7 8 2 12 
WS 11 13 1 0 5 
SK 1 1 28 0 0 
 
Table 10: 3-author C-FWAA on Chinese blogs 
 
Comparing the C-FWAA accuracy on three 
genres, we can see that function words are quite 
effective in distinguish writers in all three genres. 
It is the most effective in novels, then essays, and 
blogs are the hardest. One possible explanation is 
that novels are the longest, essays are shorter, and 
blogs are the shortest. Hence novels provide the 
largest amount of data for precise measure of 
authorial characteristics. Further examination is 
needed to test this hypothesis. Another possible 
explanation is that blogs pose less constraint on the 
writers with regard to the writing format, and thus 
writers may write in much freer and more informal 
style. Overall, C-FWAA reached over 80% 
accuracy in distinguishing two or three authors in 
all three genres. This concludes the task #1. 
3.3 Function words as genre indicators with 
author and time period controlled 
This section reports a series of experiments that 
aim to evaluate the effectiveness of function words 
as genre indicators and the genre interference on 
C-FWAA. The first round of experiment examines 
whether the function words can distinguish novels 
from essays in each TP. The cluster number was 
set to two and the clustering result was compared 
against the genre labels. The error analysis also 
reveals which genre is less cohesive (failing to 
hold all of its instances in one cluster). 
 
TP Author Accuracy Which genre is less 
cohesive? 
TP1 
WZQ .73 Essay (3->novel) 
SCW .78 Essay (3->novel) 
QZS 1  
TP2 
JPW .54 Essay (7->novel) 
WS 1  
WXB .85 Essay (2->novel) 
TP3 
GJM .71 Novel (4->essay) 
HH .63 Both (5 essay->novel; 1 
novel->essay) 
SK .66 Essay (2->novel) 
 avg .77  
 
Table 11: function words as genre indicator (novel vs. 
essay) 
 
The results in Table 11 show that the average 
accuracy (over 9 authors) is .77 to distinguish an 
author?s novels and essays, demonstrating that 
function words are also strong genre indicators. 
For some authors QZS, WS, and WXB, their 
novels and essays are highly separable based on 
function word use. Interestingly, for all writers, 
their novels hold together perfectly except for GJM, 
but the essays often spread across two clusters. 
Again, the explanation may still be that novels are 
longer than essays, and thus provide more precise 
style estimation. If so, novels and essays may not 
be a fair comparison. However, the lengths of 
essays and blogs are similar. Therefore, the above 
experiment was repeated to distinguish essays and 
blogs from same authors. The results in Table 12 
show that this task is not easier. The average 
accuracy is .71, which is a little worse than .77 in 
distinguishing novels and essays. Once again, one 
genre, this time it is the essay, that hold together 
very well, and blogs spread across clusters. 
50
Combining the results in Section 3.2 and this 
section, we can see that function words are 
indicators of both authorship and genre, and the C-
FWAA performance is affected by genre: it is the 
easiest for novel, then essay, and hardest for blogs.  
 
Author Acc #E->B #B->E 
WS .80 0/16   9/30 
HH .56 0/11 58/92 
SK .78 5/14   5/31 
Avg .71 .12 .36 
 
Table 12: function words as genre indicator 
(essay vs. blog) 
3.4 Which one do function words 
characterize more saliently, genre or 
authorship? 
In the experiments reported in this section TP was 
still controlled, but in each TP the three authors 
and two genres are mixed together. The experiment 
was repeated for each TP. Each experiment 
consists of two steps. First, the cluster number was 
set to two, and the clustering result was compared 
against the genre labels. Second, the cluster 
number was set to three, and the result was 
compared against the author labels. If genre plays 
stronger impact on function word use, we should 
see high accuracy in the 2-cluster result, and if 
authorship is more salient, the 3-cluster result 
should be better. The results show that for all three 
TPs, the author-genre mix decreased the 
performance of authorship clustering (column #3 
?AA in mixed genres? vs. column #4 ?AA in 
novel? and column #5 ?AA in essay?), indicating 
clear genre interference to authorship attribution. 
In comparison, the genre clustering in mixed 
authors (column #1) was worse than genre 
clustering in single author (column #6) in TP1 only. 
In TP2 and TP3 genre clustering in mixed-authors 
yielded higher accuracy than that in single-author, 
showing that mixing authors may increase or 
decrease genre identification performance. 
To better understand the interference between 
authorship and genre, the 3-cluster result for each 
TP was visualized in Figures 1-3. The clusters in 
TP1 (Figure 1) include authorship cluster C0 
(bottom row: SCW), genre cluster C2 (top: essay), 
and mixed cluster C1 (middle: WZQ, QZS, novels, 
and essays), demonstrating competing influence of 
authorship and genre on function words. The 
clusters in TP2 (Figure 2) are more genre-oriented, 
with C0 dominated by novels and C1 and C2 by 
essays. The clusters in TP3 (Figure 3) are also as 
mixed as in TP1, but more authorship-oriented, 
with C0 dominated by Shi Kang, C1 by Guo 
JingMing, and C2 by Han Han. In summary, 
function words characterize authors more saliently 
in TP1 and TP3, and genres more saliently in TP2. 
Therefore, we conclude for task #2 that the level of 
genre interference on authorship attribution is not 
arbitrary but is actually dependent on individual 
data set. 
   
 2-genre 
clustering 
3-author 
clustering 
Novel 
AA 
Essay 
AA 
N-E 
genre 
TP1 .51 .64 .93 .83 .84 
TP2 .89 .70 1.00 .89 .80 
TP3 .70 .75 .76 .84 .67 
 
Table 13: genre vs. authorship 
 
  
Figure 1: mixing authorship and genre in TP1  
  
Figure 2: mixing authorship and genre in TP2 
 
51
 Figure 3: mixing authorship and genre in TP3 
3.5 Is C-FWAA dependent on time period? 
The task #3 is to examine whether C-FWAA is 
dependent on time period. The hypothesis is that 
writers of different times may use the function 
words differently because of the drastic change in 
Mandarin Chinese throughout the 20th century. 
When mixing the novels written in TP1, TP2, and 
TP3, the algorithm may be more sensitive to the 
time period than individual authorship. If the 
hypothesis is true, we should see the clustering 
result aligns with the time period, not authorship or 
genre. This time the cluster number is set to -1, 
which allows EM to use cross validation to 
automatically determine the optimal number of 
clusters (Smyth, 1996; McGregor et al, 2004). 
EM returns 4 clusters: C0 is dominated by QZS 
(1940s), C1 by WZQ, WS, and JPW (1980-90s), 
C2 by SCW (1930s) and WXB (1980-90s), C3 by 
GJM (2000s). Therefore no obvious relationship is 
observed between the clusters and the time periods. 
Further, all TP1 and TP2 writers share one thing in 
common ? their works stay in one cluster, but TP3 
writers? works spread across multiple clusters: 
GJM 2, SK 3, and HH 4. This result is consistent 
with two facts that Han Han publicly 
acknowledged that (1) his Xiang Shao Nian La Fei 
Chi mimicked Shi Kang?s style, and (2) his San 
Chong Men mimicked Qian ZhongShu?s Wei 
Cheng. 
 Figure 4: clustering all novels from 9 authors 
 
Repeating the experiment on essays resulted in 
only two clusters. Most writers? essays remain in 
one cluster with few exceptions (e.g. SCW, QZS, 
WXB and JPW in C0, and WZQ, WS and GJM in 
C1), while HH and SK?s essays spread across the 
two clusters. The clusters do not seem to relate to 
the time periods either. What do these two clusters 
mean then? An examination of the cluster 
assignment of HH?s essays reveals that his essay 
books Du, Jiu Zhe Yang Piao Lai Piao Qu, and Ke 
Ai De Hong Shui Meng Shou belong to C1, all 
written in casual and conversational style, and the 
more formal essays like Qiu Yi, Shu Dian, Bei 
Zhong Kui Ren, and Yi Qi Chen Mo belong to C1. 
Interestingly, most essays in C1 are doubted to be 
penned by his father. This result suggests that the 
clustering result actually captured two sub-genres 
in essays. However, further analysis is needed to 
test this hypothesis. In summary, no solid 
relationship was found between time period and 
Chinese function word use.  
  
Figure 5: clustering all essays from 9 authors 
4 Conclusion and limitations 
This study made three contributions. First, it 
examined the effectiveness of using function words 
for Chinese authorship attribution (C-FWAA) in 
three different genres: novel, essay, and blog. 
Overall C-FWAA is able to distinguish three 
authors in each genre with various level of success. 
C-FWAA is the most effective in distinguishing 
authors of novels (averaged accuracy 90%), 
followed by essay (85%), and blog is the hardest 
(68%). Second, this study confirmed that Chinese 
function words are strong indicators of both genre 
and authorship. When the data set mixed authors 
and genres, these two factors may interfere with 
each other, and in such cases it depends on the data 
set which factor do function words characterize 
more saliently. Third, this study examined the 
hypothesized relationship between time period and 
52
Chinese function word use in novels and essays 
between 1930s and 2000s, but did not find 
evidence to support this hypothesis.  
This study has several limitations that need to 
be improved in future work. First, the data set is 
small and not quite balanced. More authors and 
works will be added in the future. Second, the 
random seed for EM is set to the default value 100 
in Weka. However, EM clustering result may vary 
to some extent with different random seeds. More 
rigorous design is needed for robust performance 
comparison. One design is to run each clustering 
experiment multiple times, each time with a 
different random seed. The clustering accuracy 
will be averaged over all runs. This new design 
will allow for performance comparison based on 
paired-sample t-test significance. Third, the 
Cultural Revolution time period is excluded from 
this study due to strong political influence on 
writers. One reviewer pointed out that this time 
period should be valuable for examining the 
relationship between authorship, genre, and time 
period. Relevant data will be collected in future 
study.  
5 Acknowledgment 
Sincere thanks to Peiyuan Sun for his assistance in 
data collection and the anonymous reviewers for 
the insightful comments. 
References  
Ahmed Abbasi & Hsinchun Chen. 2005. Applying 
Authorship Analysis to Extremist-Group Web Forum 
Messages. IEEE Intelligent Systems, 
September/October  2005, 67-76. 
Shlomo Argamon, Moshe Koppel, Jonathan Fine and 
Anat Rachel Shimoni. 2003.Gender, genre, and 
writing style in formal written texts. Text 23: 321?
346. 
Ross Clement and David Sharp. 2003. Ngram and 
Bayesian Classification of Documents for Topic and 
Authorship. Literary and Linguistic Computing, 
18(4):423-447 
Jun Da. Modern Chinese Character Frequency List. 
2005.  http://lingua.mtsu.edu/chinese-
computing/statistics/char/list.php?Which=MO 
Fred J. Damerau. 1975. The use of function word 
frequencies as indicators of style. Computers and 
Humanities, 9:271-280 
Susan C. Herring and John C. Paolillo. 2006. Gender 
and Genre Variation in Weblogs. Journal of 
Sociolinguistics, 10(4):439-459. 
David I. Holmes. 1994. Authorship Attribution. 
Computers and Humanities, 28:87-106. 
Patrick Juola. 2008. Authorship Attribution. 
Foundations and Trends in Information Retrieval, 
1(3):233-334. 
Geir Kjetsaa, Sven Gustavsson, Bengt Beckman, and 
Steinar Gil. 1984. The Authorship of The Quiet Don. 
Solum Forlag A.S.: Oslo; Humanities Press: New 
Jersey. 
Moshe Koppel, Shlomo Argomon, and Anat Rachel 
Shimoni. 2002. Automatically Categorizing Written 
Texts by Author Gender. Literary and Linguistic 
Computing 17:401?412. 
Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 
2009. Computational Methods for Authorship 
Attribution. JASIST, 60(1):9-26.  
Anthony McGregor, Mark Hall, Perry Lorier, and James 
Brunskill. 2004. Flow Clustering Using Machine 
Learning Techniques. PAM 2004, LNCS 3015, 205-
214. Springer-Verlag: Berlin. 
George K. Mikros & Eleni K. Argiri. 2007. 
Investigating topic influence in authorship 
attribution. Proceedings of the SIGIR?07 Workshop 
on Plagiarism Analysis, Authorship Identification, 
and Near-Duplicate Detection, 29?35. 
Frederick Mosteller and David L. Wallace. 1964. 
Inference and Disputed Authorship: The Federalist. 
CSLI Publications. 
Robert L. Oakman. 1980. Computer Methods for 
Literary Research. University of South Carolina 
Press, Columbia, SC. 
Efstathios Stamatatos. 2009. A Survey of Modern 
Authorship Attribution Methods. JASIST, 60(3):538-
556. 
Padhraic Smyth. 1996. Clustering Using Monte Carlo 
Cross-Validation. Proceedings of KDD?96, 126-133. 
Ian Witten, Eibe Frank, and Mark A. Hall. 2011. Data 
Mining: Practical Machine Learning Tools and 
Techniques. 3rd edition. Morgan-Kaufmann. 
Yi-ping Zeng and Xiao-wen Zhu. 2006. Application of 
computational methods to the Study of Stylistics in 
China. Journal of Fujian Normal University 
(Philosophy and Social Science Edition), 136 (1): 14-
17. 
53
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 19?23,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Classifying Negative Findings in Biomedical Publications 
 
 
Bei Yu 
School of Information Studies 
Syracuse University 
byu@syr.edu 
Daniele Fanelli 
School of Library and Information Science 
University of Montreal 
email@danielefanelli.com 
 
 
 
 
Abstract 
Publication bias refers to the phenome-
non that statistically significant, ?posi-
tive? results are more likely to be pub-
lished than non-significant, ?negative? 
results. Currently, researchers have to 
manually identify negative results in a 
large number of publications in order to 
examine publication biases. This paper 
proposes an NLP approach for automati-
cally classifying negated sentences in bi-
omedical abstracts as either reporting 
negative findings or not. Using multino-
mial na?ve Bayes algorithm and bag-of-
words features enriched by parts-of-
speeches and constituents, we built a 
classifier that reached 84% accuracy 
based on 5-fold cross validation on a bal-
anced data set. 
1 Introduction 
Publication bias refers to the phenomenon that 
statistically significant, ?positive? results are 
more likely to be published than non-significant, 
?negative? results (Estabrook et al., 1991). Due 
to the ?file-drawer? effect (Rosenthal, 1979), 
negative results are more likely to be ?filed 
away? privately than to be published publicly.    
Publication bias poses challenge for an accu-
rate review of current research progress. It 
threatens the quality of meta-analyses and sys-
tematic reviews that rely on published research 
results (e.g., the Cochrane Review). Publication 
bias may be further spread through citation net-
work, and amplified by citation bias, a phenome-
non that positive results are more likely to be 
cited than negative results (Greenberg, 2009). 
To address the publication bias problem, some 
new journals were launched and dedicated to 
publishing negative results, such as the Journal 
of Negative Results in Biomedicine, Journal of 
Pharmaceutical Negative Results, Journal of 
Negative Results in Ecology and Evolutionary 
Biology, and All Results Journals: Chem. Some 
quantitative methods like the funnel plot (Egger 
et al., 1997) were used to measure publication 
bias in publications retrieved for a certain topic.  
A key step in such manual analysis is to exam-
ine the article abstracts or full-texts to see wheth-
er the findings are negative or not. For example, 
Hebert et al. (2002) examined the full text of 
1,038 biomedical articles whose primary out-
comes were hypothesis testing results, and found 
234 (23%) negative articles. Apparently, such 
manual analysis approach is time consuming. An 
accurate, automated classifier would be ideal to 
actively track positive and negative publications. 
This paper proposes an NLP approach for au-
tomatically identifying negative results in bio-
medical abstracts. Because one publication may 
have multiple findings, we currently focus on 
classifying negative findings at sentence level: 
for a sentence that contains the negation cues 
?no? and/or ?not?, we predict whether the sen-
tence reported negative finding or not. We con-
structed a training data set using manual annota-
tion and convenience samples. Two widely-used 
text classification algorithms, Multinomial naive 
Bayes (MNB) and Support Vector Machines 
(SVM), were compared in this study. A few text 
representation approaches were also compared 
by their effectiveness in building the classifier. 
The approaches include (1) bag-of-words 
(BOW), (2) BOW with PoS tagging and shallow 
parsing, and (3) local contexts of the negation 
cues ?no? and ?not?, including the words, PoS 
tags, and constituents. The best classifier was 
built using MNB and bag-of-words features en-
riched with PoS tags and constituent markers. 
The best performance is 84% accuracy based on 
5-fold cross validation on a balanced data set. 
19
2 Related work 
The problem of identifying negative results is 
related to several other BioNLP problems, espe-
cially on negation and scientific claim identifica-
tion.  
    The first relevant task is to identify negation 
signals and their scopes (e.g., Morante and 
Daelemans, 2008;2009; Farkas et al., 2010; 
Agarwal et al., 2011). Manually-annotated cor-
pora like BioScope (Szarvas et al., 2008) were 
created to annotate negations and their scopes in 
biomedical abstracts in support of automated 
identification. This task targets a wide range of 
negation types, such as the presence or absence 
of clinical observations in narrative clinical re-
ports (Chapman et al., 2001). In comparison, our 
task focuses on identifying negative findings on-
ly. Although not all negations report negative 
results, negation signals are important rhetorical 
device for authors to make negative claims. 
Therefore, in this study we also examine preci-
sion and recall of using negation signals as pre-
dictors of negative findings.  
    The second relevant task is to identify the 
strength and types of scientific claims. Light et 
al. (2004) developed a classifier to predict the 
level of speculations in sentences in biomedical 
abstracts. Blake (2010) proposed a ?claim 
framework? that differentiates explicit claims, 
observations, correlations, comparisons, and im-
plicit claims, based on the certainty of the causal 
relationship that was presented. Blake also found 
that abstracts contained only 7.84% of all scien-
tific claims, indicating the need for full-text 
analysis. Currently, our preliminary study exam-
ines abstracts only, assuming the most important 
findings are reported there. We also focus on 
coarse-grained classification of positive vs. nega-
tive findings at this stage, and leave for future 
work the task of differentiating negative claims 
in finer-granularity. 
3 The NLP approach 
3.1 The definition of negative results 
When deciding what kinds of results count as 
?negative?, some prior studies used ?non-
significant? results as an equivalent for ?negative 
results? (e.g. Hebert et al., 2002; Fanelli, 2012). 
However, in practice, the definition of ?negative 
results? is actually broader. For example, the 
Journal of Negative Results in Biomedicine 
(JNRBM), launched in 2002, was devoted to 
publishing ?unexpected, controversial, provoca-
tive and/or negative results,? according to the 
journal?s website. This broader definition has its 
pros and cons. The added ambiguity poses chal-
lenge for manual and automated identification. 
At the same time, the broader definition allows 
the inclusion of descriptive studies, such as the 
first JNRBM article (Hebert et al., 2002).  
Interestingly, Hebert et al. (2002) defined 
?negative results? as ?non-significant outcomes? 
and drew a negative conclusion that ?prominent 
medical journals often provide insufficient in-
formation to assess the validity of studies with 
negative results?, based on descriptive statistics, 
not hypothesis testing. This finding would not be 
counted as ?negative? unless the broader defini-
tion is adopted. 
In our study, we utilized the JNRBM articles 
as a convenience sample of negative results, and 
thus inherit its broader definition.          
3.2 The effectiveness of negation cues as 
predictors 
The Bioscope corpus marked a number of nega-
tion cues in the abstracts of research articles, 
such as ?not?, ?no?, ?without?, etc. It is so far the 
most comprehensive negation cue collection we 
can find for biomedical publications. However, 
some challenges arise when applying these nega-
tion cues to the task of identifying negative re-
sults.   
First, instead of focusing on negative results, 
the Bioscope corpus was annotated with cues 
expressing general negation and speculations. 
Consequently, some negation cues such as ?un-
likely? was annotated as a speculation cue, not a 
negation cue, although ?unlikely? was used to 
report negative results like  
 
?These data indicate that changes in Wnt ex-
pression per se are unlikely to be the cause of 
the observed dysregulation of ?-catenin ex-
pression in DD? (PMC1564412).  
 
Therefore, the Bioscope negation cues may 
not have captured all negation cues for reporting 
negative findings. To test this hypothesis, we 
used the JNRBM abstracts (N=90) as a conven-
ience sample of negative results, and found that 
81 abstracts (88.9%) contain at least one Bio-
scope negation cue. Note that because the 
JNRBM abstracts consist of multiple subsections 
?background?, ?objective?, ?method?, ?result?, 
and ?conclusion?, we used the ?result? and ?con-
clusions? subsections only to narrow down the 
search range.   
20
Among the 9 missed abstracts, 5 used cues not 
captured in Bioscope negation cues: ?insuffi-
cient?, ?unlikely?, ?setbacks?, ?worsening?, and 
?underestimates?. However, the authors? writing 
style might be affected by the fact that JNRBM 
is dedicated to negative results. One hypothesis 
is that the authors would feel less pressure to use 
negative tones, and thus used more variety of 
negation words. Hence we leave it as an open 
question whether the new-found negation cues 
and their synonyms are generalizable to other 
biomedical journal articles. 
The rest 4 abstracts (PMC 1863432, 1865554, 
1839113, and 2746800) did not report explicit 
negation results, indicating that sometimes ab-
stracts alone are not enough to decide whether 
negative results were reported, although the per-
centage is relatively low (4.4%). Hence, we de-
cided that missing target is not a major concern 
for our task, and thus would classify a research 
finding as positive if no negation cues were 
found.   
Second, some positive research results may be 
mistaken as negative just because they used ne-
gation cues. For example, ?without? is marked as 
a negation cue in Bioscope, but it can be used in 
many contexts that do not indicate negative re-
sults, such as  
 
?The effects are consistent with or without the 
presence of hypertension and other comorbidi-
ties and across a range of drug classes.? 
(PMC2659734) 
 
To measure the percentage of false alarm, we 
applied the aforementioned trivial classifier to a 
corpus of 600 abstracts in 4 biomedical disci-
plines, which were manually annotated by Fan-
elli (2012). This corpus will be referred to as 
?Corpus-600? hereafter. Each abstract is marked 
as ?positive?, ?negative?, ?partially positive?, or 
?n/a?, based on hypothesis testing results. The 
latter two types were excluded in our study. The 
trivial classifier predicted an abstract as ?posi-
tive? if no negation cues were found. Table 1 
reported the prediction results, including the pre-
cision and recall in identifying negative results. 
This result corroborates with our previous find-
ing that the inclusiveness of negation cues is not 
the major problem since high recalls have been 
observed in both experiments. However, the low 
precision is the major problem in that the false 
negative predictions are far more than the true 
negative predictions. Hence, weeding out the 
negations that did not report negative results be-
came the main purpose of this preliminary study.  
 
 
Discipline #abstracts Precision Recall 
Psychiatry 140 .11 .92 
Clinical  
Medicine 
127 .16 .94 
Neuroscience 144 .20 .95 
Immunology 140 .18 .95 
Total 551 .16 .94 
 
Table 1: results of cue-based trivial classifier 
 
3.3 Classification task definition 
This preliminary study focuses on separating 
negations that reported negative results and those 
not. We limit our study to abstracts at this time. 
Because a paper may report multiple findings, 
we performed the prediction at sentence level, 
and leave for future work the task of aggregating 
sentence-level predictions to abstract-level or 
article-level. By this definition, we will classify 
each sentence as reporting negative finding or 
not. A sentence that includes mixed findings will 
be categorized as reporting negative finding. 
?Not? and ?no? are the most frequent negation 
cues in the Bioscope corpus, accounting for more 
than 85% of all occurrences of negation cues. In 
this study we also examined whether local con-
text, such as the words, parts-of-speeches, and 
constituents surrounding the negation cues, 
would be useful for predicting negative findings. 
Considering that different negation cues may be 
used in different contexts to report negative find-
ings, we built a classifier based on the local con-
texts of ?no? and ?not?. Contexts for other nega-
tion cues will be studied in the future.  
Therefore, our goal is to extract sentences con-
taining ?no? or ?not? from abstracts, and predict 
whether they report negative findings or not. 
3.4 Training data 
We obtained a set of ?positive examples?, 
which are negative-finding sentences, and a set 
of ?negative examples? that did not report nega-
tive findings. The examples were obtained in the 
following way.  
Positive examples. These are sentences that 
used ?no? or ?not? to report negative findings. 
We extracted all sentences that contain ?no? or 
?not? in JNRBM abstracts, and manually marked 
each sentence as reporting negative findings or 
21
not. Finally we obtained 158 sentences reporting 
negative findings.  
To increase the number of negative-finding 
examples and add variety to writing styles, we 
repeat the above annotations to all Lancet ab-
stracts (?result? and ?finding? subsections only) 
in the PubMed Central open access subset, and 
obtained 55 more such sentences. Now we have 
obtained 213 negative-finding examples in total. 
Negative examples. To reduce the workload 
for manual labeling, we utilized the heuristic rule 
that a ?no? or ?not? does not report negative re-
sult if it occurs in a positive abstract, therefore 
we extracted such sentences from positive ab-
stracts in ?Corpus-600?. These are the negative 
examples we will use. To balance the number of 
positive and negative examples, we used a total 
of 231 negative examples in two domains (132 in 
clinical medicine and 99 in neuroscience) instead 
of all four domains, because there are not enough 
positive examples.  
Now the training data is ready for use. 
3.5 Feature extraction 
We compared three text representation methods 
by their effectiveness in building the classifier. 
The approaches are (1) BOW: simple bag-of-
words, (2) E-BOW: bag-of-words enriched with 
PoS tagging and shallow parsing, and (3) LCE-
BOW: local contexts of the negation cues ?no? 
and ?not?, including the words, PoS tags, and 
constituents. For (2) and (3), we ran the 
OpenNLP chunker through all sentences in the 
training data. For (3), we extracted the following 
features for each sentence: 
 
? The type of chunk (constituent) where 
?no/not? is in (e.g. verb phrase ?VP?); 
 
? The types of two chunks before and after the 
chunk where ?not? is in; 
 
? All words or punctuations in these chunks;  
 
? The parts-of-speech of all these words. 
 
See Table 2 below for an example of negative 
finding: row 1 is the original sentence; row 2 is 
the chunked sentence, and row 3 is the extracted 
local context of the negation cue ?not?. These 
three representations were then converted to fea-
ture vectors using the ?bag-of-words? representa-
tion. To reduce vocabulary size, we removed 
words that occurred only once.  
 
 
     
(1) 
Vascular mortality did not differ signifi-
cantly (0.19% vs 0.19% per year, p=0.7). 
(2) 
 "[NP Vascular/JJ mortality/NN ] [VP 
did/VBD not/RB differ/VB ] [ADVP sig-
nificantly/RB ] [PP (/-LRB- ] [NP 019/CD 
%/NN ] [PP vs/IN ] [NP 019/CD %/NN ] 
[PP per/IN ] [NP year/NN ] ,/, [NP 
p=07/NNS ] [VP )/-RRB- ] ./." 
(3) 
?na na VP ADVP PP   did not differ signif-
icantly    VBD RB VB RB? 
 
Table 2: text representations 
 
3.6 Classification result 
We applied two supervised learning algorithms, 
multinomial na?ve Bayes (MNB), and Support 
Vector Machines (Liblinear) to the unigram fea-
ture vectors. We used the Sci-kit Learn toolkit to 
carry out the experiment, and compared the algo-
rithms? performance using 5-fold cross valida-
tion. All algorithms were set to the default pa-
rameter setting.  
 
Representation MNB SVM 
Presence 
vs. 
absence 
BOW .82 .79 
E-BOW .82 .79 
LCE-BOW .72 .72 
tf 
BOW .82 .79 
E-BOW .84 .79 
LCE-BOW .72 .72 
Tfidf 
BOW .82 .75 
E-BOW .84 .73 
LCE-BOW .72 .75 
 
Table 3: classification accuracy  
 
Table 3 reports the classification accuracy. Be-
cause the data set contains 213 positive and 231 
negative examples, the majority vote baseline is 
.52. Both algorithms combined with any text rep-
resentation methods outperformed the majority 
baseline significantly. Among them the best clas-
sifier is a MNB classifier based on enriched bag-
of-words representation and tfidf weighting. Alt-
hough LCE-BOW reached as high as .75 accura-
cy using SVM and tfidf weighting, it did not per-
form as well as the other text representation 
methods, indicating that the local context with 
+/- 2 window did not capture all relevant indica-
tors for negative findings.  
Tuning the regularization parameter C in 
SVM did not improve the accuracy. Adding bi-
22
grams to the feature set resulted in slightly lower 
accuracy. 
 
4 Conclusion 
In this study we aimed for building a classifier to 
predict whether a sentence containing the words 
?no? or ?not? reported negative findings. Built 
with MNB algorithms and enriched bag-of-
words features with tfidf weighting, the best 
classifier reached .84 accuracy on a balanced 
data set.    
    This preliminary study shows promising re-
sults for automatically identifying negative find-
ings for the purpose of tracking publication bias. 
To reach this goal, we will have to aggregate the 
sentence-level predictions on individual findings 
to abstract- or article-level negative results. The 
aggregation strategy is dependent on the decision 
of which finding is the primary outcome when 
multiple findings are present. We leave this as 
our future work. 
Reference 
S. Agarwal, H. Yu, and I. Kohane, I. 2011. BioNOT: 
A searchable database of biomedical negated sen-
tences. BMC bioinformatics, 12: 420. 
C. Blake. 2010. Beyond genes, proteins, and ab-
stracts: Identifying scientific claims from full-text 
biomedical articles. Journal of biomedical infor-
matics, 43(2): 173-189. 
W. W. Chapman, W. Bridewell, P. Hanbury, G. F. 
Cooper, and B. G. Buchanan. 2001. Evaluation of 
negation phrases in narrative clinical reports. Pro-
ceedings of the AMIA Symposium, 105. 
P. J. Easterbrook, R. Gopalan, J. A. Berlin, and D. R. 
Matthews. 1991. Publication bias in clinical re-
search. Lancet, 337(8746): 867-872. 
M. Egger, G. D. Smith, M. Schneider, and C. Minder. 
1997. Bias in meta-analysis detected by a simple, 
graphical test. BMJ 315(7109): 629-634. 
D. Fanelli. 2012. Negative results are disappearing 
from most disciplines and coun-
tries. Scientometrics 90(3): 891-904. 
R. Farkas, V. Vincze, G. M?ra, J. Csirik, and G. Szar-
vas. 2010. The CoNLL-2010 shared task: learning 
to detect hedges and their scope in natural language 
text. Proceedings of the Fourteenth Conference on 
Computational Natural Language Learning---
Shared Task, 1-12. 
S. A. Greenberg. 2009. How citation distortions create 
unfounded authority: analysis of a citation net-
work. BMJ 339, b2680. 
R. S. Hebert, S. M. Wright, R. S. Dittus, and T. A. 
Elasy. 2002. Prominent medical journals often pro-
vide insufficient information to assess the validity 
of studies with negative results. Journal of Nega-
tive Results in Biomedicine 1(1):1. 
M. Light, X-Y Qiu, and P. Srinivasan. 2004. The lan-
guage of bioscience: Facts, speculations, and 
statements in between. In Proceedings of BioLink 
2004 workshop on linking biological literature, on-
tologies and databases: tools for users, pp. 17-24. 
R. Morante, A. Liekens, and W. Daelemans. 2008. 
Learning the scope of negation in biomedical texts. 
In Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, pp. 715-
724. 
R. Morante, and W. Daelemans. 2009. A metalearn-
ing approach to processing the scope of negation. 
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, 21-29. 
R. Rosenthal. 1979. The file drawer problem and tol-
erance for null results. Psychological Bulle-
tin, 86(3): 638. 
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008. 
The BioScope corpus: annotation for negation, un-
certainty and their scope in biomedical texts. 
In Proceedings of the Workshop on Current Trends 
in Biomedical Natural Language Processing, pp. 
38-45. 
23
