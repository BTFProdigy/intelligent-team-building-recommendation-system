Proceedings of the 4th International Workshop on Computational Terminology, pages 104?113,
Dublin, Ireland, August 23 2014.
Identifying Portuguese Multiword Expressions using Different 
Classification Algorithms - A Comparative Analysis 
 
 
Alexsandro Fonseca 
University of Quebec in 
Montreal 
201 President Kennedy, 
Montreal, QC, Canada  
affonseca@gmail.com 
 
Fatiha Sadat 
University of Quebec in 
Montreal 
201 President Kennedy, 
Montreal, QC, Canada  
sadat.fatiha@uqam.ca 
Alexandre Blondin Mass? 
University of Quebec in 
Chicoutimi 
555, boul. de l'Univ. 
Chicoutimi, QC, G7H 2B1  
alexandre.blondin. 
masse@gmail.com 
 
Abstract 
This paper presents a comparative analysis based on different classification algorithms and tools for the 
identification of Portuguese multiword expressions. Our focus is on two-word expressions formed by 
nouns, adjectives and verbs. The candidates are selected on the basis of the frequency of the bigrams; 
then on the basis of the grammatical class of each bigram?s constituent words. This analysis compares 
the performance of three different multi-layer perceptron training functions in the task of extracting 
different patterns of multiword expressions, using and comparing nine different classification 
algorithms, including decision trees, multilayer perceptron and SVM. Moreover, this analysis compares 
two different tools, Text-NSP and Termostat for the identification of multiword expressions using 
different association measures. 
1 Introduction 
The exact definition of a multiword expression (MWE) is a challenging task and it varies from author 
to author. For example, Moon (1998) says: ?? there is no unified phenomenon to describe but rather a 
complex of features that interact in various, often untidy, ways and represent a broad continuum 
between non-compositional (or idiomatic) and compositional groups of words.? Moreover, this 
phenomenon receives different names in the literature (Proost, 2005): phraseological units, fixed 
expressions, word combinations, phrasemes, etc. 
In this study, we consider MWE in a similar way Mel'?uk (1998) defines a phraseme: a phrase 
which is not free, i.e. the expression?s signifier and/or signified are not unrestrictedly and regularly 
constructed. 
A phrase P is unrestrictedly constructed when the rules applied to construct P are not mandatory. 
For example, instead of the phrase: ?doing a research? it is possible to say ?performing a research?, 
?executing a research? i.e., this expression is not fixed. However, in a sign like ?No smoking?, it is not 
common to see variants like ?Smoking prohibited? or ?Do not smoke?, although those are 
grammatically correct variants which express the same meaning. Then, ?No smoking? is a phraseme 
(MWE), because it is not unrestrictedly constructed. 
A phrase P is regularly constructed when the words forming it are combined following the general 
rules of the grammar and its sense can be derived exclusively from the sense of its constituent words. 
The phrase: ?he died yesterday?, is regularly constructed because it follows the rules of the grammar 
and its sense follows from the sense the words forming it. However, the expression ?kicked the 
bucket? is not regularly constructed, in relation to its meaning (the combination of words follows the 
rules of the grammar), because its sense, ?died?, cannot be derived from the sense of its constituent 
______________ 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
104
words. On the other hand, the expression ?passing by? is not regularly constructed because it does not 
follow the general rules of the grammar. 
According to Mel??uk (1998), it is possible to divide the phrasemes (MWEs) in two groups: 
pragmatemes and semantic phrasemes. As pragmatemes, we can have: 
- Expressions in which both the signified and the signifier are not unrestrictedly constructed 
(although they are regularly constructed), e.g. ?all you can eat?, or  
- Expressions in which only the signified is not unrestrictedly constructed. For example, in a 
library it is possible to have signs like ?Please be quiet?, ?No talking please?, etc. In this case, 
the signifier (the form, the words forming the expression) is more or less free; however, the 
sense is always the same. 
In semantic phrasemes, the signified is free (it is constructed unrestrictedly; however, it is not 
constructed regularly) and the signifier is not free. We can have three types of semantic phrasemes: 
- Idioms: the sense of the expression goes beyond the sense of its constituent words, and does 
not include their senses. Examples: ?of course?, ?(to) pull (someone?s) leg?, ?(to) spill the 
beans?; 
- Collocations: the sense of the expression includes the sense of one of its constituent words, say, 
w1. The other word is freely chosen and w1 is chosen contingent to it. Collocations can be 
(Manning and Sch?tze, 1999): light verbs constructions (e.g. make a call, take a decision), verb 
particle constructions (e.g. to switch on, to pass by), proper names (e.g. San Francisco, Bill 
Gates) and terminological expressions, i.e., multiword terms (e.g. gross domestic product, light 
year).  
- Quasi-phrasemes or quasi-idioms: the signified of the expression contains the signified of its 
constituent words; however, it also contains a signified that goes beyond the signified of the 
isolated words (e.g. (to) start a family, bed and breakfast).  
For a more complete explanation about pragmatemes and semantic phrasemes, refer to (Mel??uk, 
1998) or to (Morgan, 1978). For a more detailed linguistic description on the properties of MWEs, see 
(Baldwin and Kim, 2010). 
In this paper, we assume as MWE any kind of phraseme. However, we are interested in the study 
of Portuguese two-word expressions formed mostly by nouns, adjectives and verbs. For this reason, 
since most of pragmatemes and idioms are formed by more than two words, basically our focus is on 
quasi-phrasemes and collocations (mostly light verbs constructions, proper names and multiword 
terms (MWT)). 
The literature on MWE extraction describes different methods for the identification or extraction of 
MWEs. Many of them rely on association measures, such as Dice?s coefficient (Smadja, 1996) or 
mutual information (Church and Hanks, 1990). A complete explanation on the use of this association 
measures on the task of extraction MWEs from text can be found in (Manning and Sch?tze, 1999). 
The main idea behind such measures is that the higher the association among the words that appear 
together in a text, the higher the probability that they constitute a single semantic unit. 
There are other methods, which use linguistic information or hybrid approaches that combine 
statistical measures with the linguistic information, such as the grammatical class of each word, the 
sense of the expression or the syntactic regularities. Yet others are based on classification algorithms, 
popular in machine learning systems. 
In this study we performed two types of comparison. In the first one, we compared the performance 
of nine different classification algorithms in the task of identifying MWEs. In the second, we 
compared two different tools, Text-NSP and Termostat, using different association measures, in the 
task of extracting MWEs from text. Although our focus is in general MWE, the current study could 
also be applied to corpus in a specific area for the extraction of multiword terms (MWT). 
2 Related Work 
Baptista (1994) presents a linguistic study about the nominal expressions formed by more than two 
words in Portuguese. From a set of 10,000 expressions, he created a typology of nominal MWEs. He 
found that 70% of the nominal MWEs follow only five different patterns (A = adjective, N = noun, V 
= verb and P = preposition): A-N, N-A, N-P-N, N-N and V-N. He analyses the syntactic proprieties of 
each of these groups, focusing his attention on the patterns N-A and N-P-N, which he considers less 
105
rigid and more difficult to treat automatically. Finally, he integrates the MWEs? morphological 
information to an electronic dictionary.  
Antunes and Mendes (2013) propose the creation of a MWE typology that includes its semantic, 
syntactic and pragmatic properties, aiming the annotation of a MWE lexicon using this typology 
information. They divide the MWEs in three groups, from a semantic standpoint: expressions with 
compositional meaning, e.g. ?banana bread?, expression with partial idiomatic meaning, e.g. ?vontade 
de ferro? (iron will) and expressions with total idiomatical meaning (or with no compositionality), e.g. 
?spill the beans?. Within each of these three groups, the expressions are subdivided according to their 
grammatical categories and lexical and syntactical fixedness.  
After a survey and a comparison on different association measures, algorithms and tools used on 
the identification of MWEs, Portela (2011) presents a study on the identification of Portuguese MWEs 
following two patterns, N-A and N-P-N, using different association measures. After the extraction of 
candidates, syntactic criteria are applied to them, to verify their fixedness and determine if a candidate 
is a MWE. Examples of syntactic criteria applied to bigrams following the pattern N-A and N-P-N:  
- Loss of adjective?s predicative characteristic: when the adjective comes after the noun and it 
can be paraphrased by a copulative verb (e.g. verb ?to be?) + the same adjective, keeping the 
same sense, the adjective has a predicative function. For example, in the expression: ?homem 
cansado? (tired man, lit. man tired), it is possible to substitute ?cansado? for ?que estava 
cansado? (that was tired), and the adjective?s predicative characteristic is maintained.  
However, in the expression ?sorriso amarelo? (false, not natural smile, lit. smile yellow), if we 
substitute the expression for ?sorriso que ? amarelo?, (smile that is yellow), the predicative 
characteristic is not maintained, because the original sense is lost. This loss of predicative 
characteristic shows that the expression is fixed, and it is evidence that the expression is a 
MWE. 
- Insertion of elements in the expression (N-P-N): consider the expression ?livro de bolso? 
(pocket book, lit. book of pocket). It is not possible to freely insert a modifier, for example 
?*livro do Paulo de bolso? (lit. book of Paulo of Pocket). In this example, the modifier can be 
inserted only at the end of the expression: ?livro de bolso do Paulo?. This kind of fixedness is 
evidence that the expression is a MWE. 
3 Methodology 
We restricted the present study on the extraction of two-word MWEs. For their data, for example, Piao 
et al. (2003) found that 81.88% of the recognized MWEs were bigrams. 
The current study uses CETENFolha (Corpus de Extractos de Textos Electr?nicos/NILC Folha de 
S?o Paulo) as a Brazilian Portuguese corpus, available on the Linguateca Portuguesa website, which is 
part of a project on the automatic processing of the Portuguese language (Kinoshita et al., 2006). 
CETENFolha is composed by excerpts from the Brazilian newspaper "Folha de S?o Paulo", and 
contains over 24 million words. At the current stage, we use a small fraction of the corpus, comprising 
3,409 excerpts of text (about 250,000 words). Each excerpt corresponds to individual news covering 
different areas. The number 3,409 represents 1% of the number of excerpts composing the corpus. 
We performed different types of evaluation. First, we generated a reference file containing the most 
frequent MWEs in the corpus and we compared nine different classification algorithms against this 
reference in the task of identifying Portuguese MWEs. Second, we tested a multilayer perceptron 
using three different training functions in the task of classifying MWEs in different patterns. We also 
extracted automatically the 2,000 most frequent bigrams from the entire corpus and we identified, by 
hand, which ones are MWEs, and we classified them in patterns. Finally, we used two different tools 
for the identification of MWEs: Text-NSP (Banerjee and Pedersen, 2003) and Termostat (Drouin, 
2003). For these tools, we are interested in two types of evaluation. In the first evaluation, we used our 
reference list to automatically compare the best candidates obtained by each tool against this reference. 
In the second evaluation, we manually counted the number of MWEs, among a list of the 500-best 
candidates ranked by one of the association measures, log-likelihood, and we calculated the precision 
for each tool. 
106
3.1 Reference File Creation 
Before the indexation, some pre-processing methods on the corpus were performed, such as 
lemmatization and elimination of stop words (articles, prepositions, conjunctions). In this study, we 
are mostly interested in analyzing MWEs formed by nouns, adjectives and verbs. And since those stop 
words are very common in Portuguese, their elimination reduces considerably the number of MWE 
candidates that would not be relevant to this study. In this case, some common Portuguese MWEs are 
not considered, especially the ones following the pattern noun-preposition-noun, e.g. ?teia de aranha? 
(cobweb), or the pattern preposition-noun, e.g. ??s vezes? (sometimes).  
We obtained 49,589 bigrams and we established a frequency of 3 as a threshold. We selected 1,170 
bigrams that appeared more than 3 times in our corpus? excerpts as our MWE candidates, and by hand 
we recognized 447 of them as Portuguese MWEs, and we considered those 447 MWEs as our 
reference file. 
It is important to note that our reference file does not contain all the two-word MWEs in the corpus? 
excerpt, since we generated more than 49,000 bigrams, and we could not evaluate all of them by hand. 
Furthermore, the corpus is formed by newspaper texts, treating different subjects, thus it is more 
difficult to create a closed set of all possible two-word MWEs. Therefore, our evaluation in the present 
study is based on a comparison of how many of the most frequent two-word MWEs in our corpus are 
ranked as n-best candidates by some of the association measures implemented by each tool. 
3.2 Comparison of Different Classification Algorithms 
First, we computed the frequency of each of those 1,170 bigrams and the frequency of its constituent 
words. Then, we classified by hand each of the words according to their grammatical class: 1 for 
nouns, 2 for adjectives, 3 for verbs, 4 for other classes (adverbs, pronouns and numbers) and 5 for 
proper names. We decided not to use a POS-tagger to guarantee the correct grammatical class 
assignment to each word. This gave us 25 patterns of bigrams: N-N (noun-noun), N-A (noun-
adjective), N-V (noun-verb), V-N, PN-PN (proper name-proper name), etc.  
Second, we created a matrix of 1,170 lines and five columns. For each line, the first column 
represents the frequency of a bigram in the excerpt of text, the second column represents the frequency 
of the first bigram?s word, the third column represents the frequency of the second bigram?s word, the 
fourth column represents the grammatical class of the first bigram?s word and the fifth column 
represents the grammatical class of the second bigram?s word. This matrix was used to evaluate the 
precision and recall of nine different classification algorithms: decision tree, random forest, ada boost 
(using decision stamp as classifier), bagging (using fast decision tree learner as classifier), KNN (K 
nearest neighbors), SVM, multilayer perceptron, na?ve Bayesian net and Bayesian net. 
3.3 Bigrams Pattern Classification 
We chose one of the algorithms with the best performance (multi-layer perceptron) and we evaluated 
it using three different training functions, Bayesian regulation back propagation (br), Levenberg-
Marquardt (lm) and scaled conjugate gradient (scg), and we compared their performance in the 
classification of different patterns of bigrams as MWE. The data used for the classification is 
formatted in the same way as in the Subsection 3.2. However, for this comparison, we used only the 
patterns that gave 10 or more samples of MWE, for example, the patterns: N-A, N-N and N-PN. 
3.4 The Text-NSP Tool 
Text-NSP is a tool used in the task of MWE extraction from texts (Banerjee and Pedersen, 2003).  In 
order to use Text-NSP tool, we do not provide a file containing the POS patterns of the bigrams that 
we would like to extract as MWE candidates. Therefore, before applying this tool, the only pre-
processing task we performed with the source corpus, was removing the XML tags they contained. 
The next step was to define a stop words list file, since we were interested in finding MWEs following 
the bigram?s patterns formed only by nouns, adjectives, verbs and others classes (adverbs, pronouns 
and numbers), e.g. N-N, N-A, N-V, O-N. 
We ran the program using the ?count.pl? script, giving the stop words file and the corpus files as 
parameters, and 2 as n-gram value, which refers to our aim to generate only bigrams. 
107
The output file is a list of all bigrams in the corpus, and each line contains a bigram, the frequency 
of the bigram, and the frequency of each of the two words forming the bigram. 
Using the output file and the ?statistics.pl? script, we generated the candidates? files ranked by four 
different association measures: Dice's coefficient (dice), log-likelihood (ll), pointwise mutual 
information (pmi) and Student's t-test (t). Then we transformed each of the candidate files to the XML 
format used by MWEtoolkit (Ramisch, 2012) and used MWEtoolkit?s scripts to create files with the n-
best candidates (n = 50, 100, 500, 1000 and 3000) and compare each candidate file against the 
reference file. 
3.5 The Termostat Tool 
Termostat (Drouin, 2003) is a tool developed for an automatic extraction of terms. It can be currently 
used with five different languages: English, French, Italian, Portuguese and Spanish. It generates 
statistics for simple and complex expressions. Since in this study we are interested in MWE, we 
extracted only the complex expressions.  
As for Text-NSP, Termostat requires the elimination of the XML tags the corpus contained; which 
was the only pre-processing step of the corpus. 
After the analysis of the corpus, the system generated the lists of expressions ranked by four 
association measures: log-likelihood (ll), chi-squared (?2), log-odds ratio (lor) and the ?sp?cificit?? 
measure (Lafon, 1980) (sp). 
Then we proceeded as for Text-NSP: we created files with the n-best candidates, ranked by the four 
association measures and compared each candidate file against the reference file. 
3.6 Comparison between the 500-best Candidates of each Tool 
Using the association measure that is implemented by both tools, the log-likelihood, we analyzed the 
500-best candidates ranked by this association measure using each tool. We selected by hand the 
MWEs among those candidates and we calculated the precision of each tool, for the n-best first 
candidates (n = 50, 100, 150?500). 
4 Evaluations  
4.1 Comparison of Different Classification Algorithms 
First, we had to proceed to an indirect estimative of the recall. We found 49,589 bigrams in the 
selected excerpts of texts, and the manual evaluation of each one, in order to decide which one is a 
MWE, would take too much time. So, we estimated the amount of MWEs for the total 49,589 bigrams 
as in (Piao et al., 2003). Using 100 excerpts of text we generated all the bigrams, with all frequencies. 
We obtained 1,715 bigrams.  
Then, we found by hand 136 MWEs, which tells us that about 7.93% of the bigrams are MWEs. 
Considering that the corpus is homogeneous, we can extrapolate and say that about 7.93% of the 
49,589 bigrams in our total excerpts are MWEs, which gives 3,932 MWEs. Since we found 447 
MWEs after applying the filter of frequency (> 3), our base recall is 11.37% (447/3,932). We used this 
base recall as a multiplying factor for the recall given by each classification algorithm. 
We used our generated data to test nine different classification algorithms: decision tree, random 
forest, ada boost, bagging, KNN (K nearest neighbors), SVM, multilayer perceptron, na?ve Bayesian 
net and Bayesian net. The main parameters used with each algorithm are listed below. 
Decision tree: C4.5 algorithm (Quinlan, 1993) with confidence factor = 0.25. 
Random Forest (Breiman, 2001): number of trees = 10; max depth = 0; seed = 1. 
Ada Boost (Freund and Schapire, 1996): classifier = decision stamp; weight threshold = 100; 
iterations = 10; seed = 1. 
Bagging (Breiman, 1996): classifier = fast decision tree learner (min. number = 2; min. variance = 
0.001; number of folds = 3; seed = 1; max. depth = -1); bag size percent = 100; seed = 1; number of 
execution slots = 1; iterations = 10. 
KNN (Aha and Kibler, 1991): K = 3; window size = 0; search algorithm = linear NN search 
(distance function = Euclidian distance). 
SVM (Chang and Lin, 2001): cache size = 40; cost = 1; degree = 3; eps = 0.001; loss = 0.1; kernel 
type = radial basis function; nu = 0.5; seed = 1. 
108
Multilayer perceptron: learning rate = 0.3; momentum = 0.2; training time = 500; validation 
threshold = 500; seed = 0;  
Bayesian net: search algorithm = k2 (Cooper and Herskovits, 1992); estimator = simple estimator 
(alpha = 0.5). 
The results are summarized in Table 1, where Recall-1 is the recall given by each algorithm based 
on the 447 MWEs found among the MWE candidates and Recall-2 is Recall-1 multiplied by 0.1137 
(base recall, as previously calculated), which gives an estimative of the recall for the entire corpus. 
As we see in Table 1, the values of precision are very similar for all the algorithms, varying 
between 0.830 (random forest) and 0.857 (bagging), with the exception of SVM, which gave a 
precision of 0.738. The recall-1values were between 0.831 and 0.857 (0.655 for SVM) and the recall-2 
between 9.4% and 9.7% (7.4% for SVM). 
We observe that we obtained good precision and weak recall. This is due, as observed by Piao et al. 
(2003), to the fact that the extraction of the MWE candidates is based only on the frequency of the 
bigrams, and only after the extraction of these candidates we applied the linguistic information 
(classification in grammatical classes). 
However, we must consider that, although we extracted only about 11% of the MWEs, these 11% 
are the most frequent and they represent about 46% of all the MWEs in the corpus, if we sum up the 
frequency of each MWE. Together, the 447 MWEs found appear 4,824 times in our corpus? excerpt, 
while the remaining 3,485 (from a predicted 3,932 MWEs in the corpus? excerpt) appear 5,576 times. 
In absolute terms we have: 4,824 / (4,824+5,576) = 0.46. 
 
Algorithm TP Rate FP Rate Precision Recall Recall-2 
Decision tree 0.853 0.158 0.854 0.853 0.097 
Random forest 0.831 0.194 0.830 0.831 0.094 
Ada boost 0.837 0.196 0.836 0.837 0.095 
Bagging 0.857 0.163 0.857 0.857 0.097 
KNN ? k = 3 0.846 0.171 0.846 0.846 0.096 
SVM 0.655 0.553 0.738 0.655 0.074 
M. perceptron 0.852 0.174 0.851 0.852 0.097 
Na?ve B. net 0.836 0.170 0.839 0.836 0.095 
Bayesian net 0.842 0.170 0.843 0.842 0.096 
      
Base recall 0.1137     
 
Table 1: True-positive rate, false-positive rate, precision and recall for nine classification algorithms. 
4.2 Bigrams Patterns Classification 
We obtained eight patterns that together represent 59% of the candidate bigrams (689/1,170) and 94% 
of the MWEs that appear three or more times in the corpus (420/447). The rest of the bigrams? 
patterns (41%) rarely formed MWE (only 6% of the total MWEs).  Table 2 shows the results. ?N? 
stands for ?Noun?, ?A? for adjective, ?O? for other classes (adverbs, pronouns and numbers) and ?PN? 
for ?proper names?.  
Analyzing the table, we had best results with the patterns N-A (e.g. ?comiss?o t?cnica?, ?banco 
central?, ?imposto ?nico?) and PN-PN (?Fidel Castro?, ?Jos? Sarney?, ?Max Mosley?). The function 
lm gave the best value for the F1 measure (0.912) for the pattern N-A, and the function scg gave the 
best value for the pattern PN-PN (0.931). 
In general, we had the weakest results with the patterns O-N, e.g. ?terceiro mundo?, (third world) 
and A-PN, e.g. ?Nova York?, ?Santa Catarina?. Using the training functions ?lm? and ?scg?, none of 
the 10 MWEs belonging to the pattern O-O, e.g. ?at? agora? (until now), ?al?m disso? (moreover, lit. 
beyond this) was recognized, and none of the 46 MWEs belonging to the pattern O-N was recognized, 
when using the training function ?scg?. 
The last line of each table presents the total values for the eight patterns, for the three learning 
functions. We had the best precision and recall using the ?lm? function. 
 
 
109
    br   lm   scg  
Pattern Bigrams MWE Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 
N-A 229 193 0.867 0.912 0.889 0.845 0.990 0.912 0.850 0.969 0.906 
O-N 164 46 0.378 0.304 0.337 0.647 0.239 0.349 0.720 0.000 0.000 
PN-PN 117 101 0.862 0.931 0.895 0.863 1.000 0.927 0.871 1.000 0.931 
A-N 53 21 0.813 0.619 0.703 0.810 0.810 0.810 0.630 0.810 0.708 
O-O 46 10 0.357 0.500 0.417 0.000 0.000 0.000 0.783 0.000 0.000 
N-PN 34 16 0.438 0.438 0.438 0.688 0.688 0.688 0.222 0.125 0.160 
N-N 31 20 0.647 0.550 0.595 0.696 0.800 0.744 0.692 0.900 0.783 
A-PN 15 13 0.750 0.231 0.353 0.500 0.154 0.235 0.667 0.154 0.250 
All Pat. 689 420 0.776 0.769 0.773 0.819 0.831 0.825 0.815 0.779 0.797 
 
Table 2: Multi-layer perceptron precision, recall and F-measure in the classification of the most 
common bigram?s patterns using different training functions: Bayesian regulation back-propagation 
(br), Levenberg-Marquardt (lm) and scaled conjugate gradient (scg). 
 
Using Text-NSP tool, we extracted from the entire corpus all the bigrams (including the ones 
formed by stop words) and we analyzed by hand the 2,000 most frequent bigrams. We found 165 two-
word MWEs formed by nouns, adjectives, verbs and other classes (adverbs, pronouns and numerals) 
and we classified them according to their pattern. Table 3 shows the number of MWEs and their total 
frequency in the corpus, classified by patterns. The words belonging to the classes of adverb, pronoun 
and numeral were classified as ?O? (other classes).  
The much smaller proportion of bigrams recognized as MWEs (165/2000) in comparison to the 
previous analysis (447/1,170) is explained by the fact that in the previous analysis we had eliminated 
the stop words before generating the bigrams, and now all the bigrams were generated. This created 
many bigrams composed by prepositions or conjunctions that do not form MWE, for example: ?de 
um?, ?de uma?, ?de S?o?, ?que os?, ?diz que?, ?do que?, ?em que?.  
We note that the five most common patterns are the same as found before, in the small excerpt of 
text, with the pattern N-A giving the greatest number of expressions, e.g. ?ano passado? (last year, lit. 
year last), ?Banco Central? (Central Bank, lit. bank central), ?norte americano? (north American), 
?sele??o brasileira? (Brazilian team, lit. selection Brazilian), ?equipe econ?mica? (economic team, lit. 
team economic). In terms of frequency, the MWEs following the pattern N-A represent about 38% of 
the most frequent two-word MWEs found in the corpus. 
It is important to observe that, although we are not differentiating Brazilian and Portuguese MWEs 
in this study, the recognized MWEs follow the Brazilian orthography (e.g. ?equipe econ?mica?  vs 
?equipa econ?mica?, ?sele??o brasileira? vs ?selec??o brasileira?), since we used a Brazilian 
Portuguese corpus. 
 
 Pattern MWE  Frequency 
N-A 58          101,442  
O-N 27            29,697  
PN-PN 24            39,270  
O-O 23            13,923  
A-N 13            51,460  
N-N 12            21,559  
A-O 2              1,975  
A-PN 2              2,115  
V-N 2              2,263  
N-PN 1              2,589  
N-V 1              1,423  
Total 165          267,716  
 
Table 3: Frequency of the most common MWEs patterns extracted from the entire corpus 
110
4.3 Text-NSP 
Before applying this tool, the only pre-processing performed in the corpus was to remove the XML 
tags. The next step was to define a stop words list file like in Subsections 4.1 and 4.2. 
We ran the program using the script ?count.pl?, giving as parameter the stop word file and the 
corpus file, and 2 as n-gram value, meaning that we wanted to generate only bigrams. 
The exit file is a list of all bigrams in the corpus? excerpt, and each line contains a bigram, the 
frequency of the bigram, and the frequency of each of the two words forming the bigram. 
Using the output file and the script ?statistics.pl? we generated the candidates? files ranked by the 
four association measures listed in Subsection 3.4. Then we transformed each of the candidates? files 
to the XML format used by the MWEtoolkit and we used the MWEtoolkit?s scripts to create files with 
the n-best candidates and to evaluate each of the files against our reference file. Table 4a shows the 
results of this evaluation. 
The results show that for values of n = 50, 100 and 500 we had the best results using the log-
likelihood measure and for n = 1000 and 3000, Student?s t-test gave the best results.  
Table 4b shows the precision, recall and F-measure that we obtained using the log-likelihood 
measure. We had very good values of precision using the Text-NSP using this measure. For example, 
from the 50 best ranked candidates by this measure, 31 were MWEs present in our reference list.  
4.4 Termostat 
Termostat generated n-grams following eleven POS patterns, all of them are nominal ones: N-N, N-A, 
N-P-N, N-N-N, N-P-N-A, N-N-N-N, N-V-N, N-N-N-N-N, N-A-A, N-N-A and N-A-N. In total, 4,284 
n-grams were generated, and we selected only the bigrams (N-N and N-A), which gave 3,458 bigrams 
(81% of all n-grams). The last five patterns listed above produced less than ten candidates each one 
and the patterns N-P-N-A, N-N-N-N produced less than 30 candidates each one. 
Those 3,458 candidates were ranked according to the four association measures listed in Subsection 
3.5. Then we compared the n-best candidates against our reference file. The results are in Table 5a. 
Table 5b shows the precision, recall and F-measure that we obtained using the log-likelihood measure. 
Looking at Table 5a, we notice that we had best performance with ?2 for the 50 and 100 best 
candidates and for the 500, 1000 and 3000 best candidates we had better results using the ll measure. 
Comparing with Text-NSP, Termostat had best performance for the first 50 and 100 candidates. 
However, Text-NSP outperformed for n = 500, 1000 and 3000, when using the ll measure and 
Student?s t-test.  
 
 dice ll pmi t  ll TP Prec. Recall F1 
50 7 31 0 23  50 31 0.62 0.07 0.12 
100 7 64 0 39  100 64 0.64 0.14 0.23 
500 8 241 1 180  500 241 0.48 0.54 0.51 
1000 11 314 4 331  1000 314 0.31 0.70 0.43 
3000 69 375 11 392  3000 375 0.13 0.84 0.22 
     (a)                                               (b) 
 
Table 4: Text-NSP: Number of MWEs among the first n-best candidates, ranked by four association 
measures (a) and precision, recall and F-measure for the log-likelihood measure (b). 
 
 ?2 ll lor sp  ll TP Prec. Recall F1 
 50 42 38 32 38  50 38 0.76 0.09 0.15 
100 72 68 66 68  100 68 0.68 0.15 0.25 
500 153 162 117 159  500 162 0.32 0.36 0.34 
1000 181 197 127 192  1000 197 0.20 0.44 0.27 
3000 198 211 143 208  3000 211 0.07 0.47 0.12 
   (a)       (b) 
 
Table 5: Termostat: Number of MWEs among the first n-best candidates, ranked by four 
association measures (a) and precision, recall and F-measure for the log-likelihood measure (b). 
111
4.5 Comparing the 500-best candidates of each tool 
We analyzed by hand the 500-best candidates obtained using Text-NSP and Termostat, ranked by the 
log-likelihood association measure, to decide which ones are MWEs. Table 6 shows the precision 
given by each tool, for the first n candidates, n = 50, 100, 150?500. 
With Termostat, we had the best precision for all values of n candidates, going from 86% for the 
first 50 candidates to 82% for the first 500 candidates. Using Text-NSP, the precision starts with 82% 
for the first best 50 candidates and decreases to 72% for the first 500-best candidates. 
As in the tests performed in Subsection 4.2, the most common patterns of MWE found by both 
tools were noun-adjective, e.g. ?Congresso Nacional?, ?emenda constitucional?, ?deputado federal? 
and proper name-proper name, e.g. ?Fernando Collor?, ?Get?lio Vargas?, ?Itamar Franco?. 
 
n first cand. Text-NSP Termostat 
50 0.82 0.86 
100 0.82 0.85 
150 0.83 0.86 
200 0.79 0.84 
250 0.76 0.84 
300 0.75 0.84 
350 0.74 0.83 
400 0.74 0.82 
450 0.73 0.81 
500 0.72 0.82 
 
Table 6: Text-NSP and Termostat precision for the first n best candidates, using log likelihood 
association measure. 
5 Conclusions and Future Work 
In this paper, we presented a comparative study on different classification algorithms and tools for the 
identification of Portuguese multiword expressions, using information about the frequency, the 
grammatical classes of the words and bigrams and different association measures. 
In what concerns the classification algorithms, bagging, decision trees and multi-layer perceptron 
had a slightly better precision. Using multi-layer perceptron with three different training functions, we 
identified the part-of-speech patterns that are best classified as two-word MWEs. Using the function 
Levenberg-Marquardt we had better results in classifying the pattern noun-adjective (the most 
common in our corpus) and we were more successful in classifying MWEs following the pattern 
?proper name-proper name? using the function scaled conjugate gradient. 
With the objective of making an estimative on the part-of-speech patterns followed by the most 
frequent two-word MWEs in the corpus, we applied Text-NSP to the extraction of the 2,000 most 
frequent bigrams and we identified and classified the MWEs, according to their part-of-speech 
patterns. As a result, we found that the patterns ?noun-adjective? and ?proper name-proper name? are 
the most common two-word MWE patterns in the corpus. We also found that verbs do not form a 
great variety of two-word MWE in Portuguese. 
The comparison between tools for the automatic identification of MWEs showed that Termostat 
had better precision than Text-NSP when applied to a small number of candidates (50 and 100). When 
the number of candidates increases, Text-NSP had better precision using log-likelihood measure and 
Student?s t-test association measures. 
As future work, we intend to apply the same tools, especially Termostat, to a specific domain 
corpus, in order to compare their performance in the identification of Portuguese multiword terms, not 
limiting the study to bigrams, but also analyzing n-grams in general. 
References 
Aha, D. and Kibler, D. (1991). Instance-based learning algorithms. In: Machine Learning. 6:37-66. 
112
Antunes, S. and Mendes, A. (2013). MWE in Portuguese - Proposal for a Typology for Annotation in Running 
Text. Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pp. 87?92, Atlanta, Georgia. 
Baldwin, T. and Kim, S. N. (2010). Multiword expressions. Nitin Indurkhya and Fred J. Damerau (eds.), In: 
Handbook of Natural Language Processing, Second Ed. Chapman & Hall/CRC, London, UK., pp. 267-292. 
Banerjee, S and Pedersen, T. (2003). The Design, Implementation, and Use of the Ngram Statistic Package. In: 
Proceedings of Fourth International Conference on Intelligent Text Processing and Computational 
Linguistics, pp. 370-381, Mexico City. http://search.cpan.org/~tpederse/Text-NSP/ 
Baptista, J. (1994). Estabelecimento e Formaliza??o de Classes de Nomes Compostos. Master Thesis. Faculdade 
de Letras, Universidade de Lisboa, 145 pp. 
Breiman, L. (2001). Random Forests. In: Machine Learning. 45(1):5-32. 
Breiman , L. (1996). Bagging predictors. In: Machine Learning. 24(2):123-140. 
Chang, Chih-Chung and Lin, Chih-Jen (2001). LIBSVM - A Library for Support Vector Machines. 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/. 
Church, K. W. and Hanks, P (1990). Word Association Norms, Mutual Information and Lexicography. In: 
Computational Linguistics, 16(1):22?29. 
Cooper, G.  and Herskovits, E. (1992). A Bayesian Method for the Induction of Probabilistic Networks from 
Data. In: Machine Learning. 9(4):309-347. 
Drouin, P. (2003). Term Extraction Using Non-technical Corpora as a Point of Leverage, In: Terminology, 9(1): 
99-117. -  http://termostat.ling.umontreal.ca/ 
Freund, Y. and Schapire, R. E (1996). Experiments with a new boosting algorithm. In: Thirteenth International 
Conference on Machine Learning, San Francisco, pp. 148-156. 
Kinoshita, J., Nascimento Salvador, L.D., Dantas de Menezes, C., E. (2006). CoGrOO: a Brazilian-Portuguese 
Grammar Checker based on the CETENFOLHA Corpus. In: Proceedings of Fifth International Conference 
on Language Resources and Evaluation, pp. 2190-2193. 
Lafon, P. (1980). Sur la Variabilit? de la Fr?quence des Formes dans un Corpus. In: MOTS, no 1, pp. 128-165. 
Manning, C. D. and Sch?tze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge, 
MA: The MIT Press, 1999, 680 pp. 
Mel'?uk, I. (1998). Collocations and Lexical Functions. In: A.P. Cowie (ed.), Phraseology. Theory, Analysis, and 
Applications, 1998, Oxford: Clarendon Press, pp. 23-53. 
Moon, R. E. (1998). Fixed Expressions and Idioms in English: A Corpus Based Approach. Oxford: Clarendon 
Press, 356 pp. 
Morgan, J. L. (1978). Two Types of Convention in Indirect Speech acts. In: P. Cole (ed.), Syntax and Semantics, 
v.9. Pragmatics (New York etc.: Academic Press), pp. 261-80. 
Piao, S., Rayson, P., Archer, D., Wilson, A., and McEnery, T.  (2003). Extracting Multiword Expressions with a 
Semantic Tagger. In: Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, at ACL 
2003, 41st Annual Meeting of the Association for Computational Linguistics, pp. 49-56, Sapporo, Japan. 
Portela, R. J. R. (2011). Identifica??o Autom?tica de Nomes Compostos. Instituto Superior T?cnico, 
Universidade T?cnica de Lisboa. Master Thesis. November 2011, Lisbon, Portugal, 104 pp. 
Proost, K. (2007). Conceptual Structure in Lexical Items: The Lexicalisation of Communication Concepts in 
English, German and Dutch. John Benjamins Pub. Co, 304 pp. 
 
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 303 pp. 
Ramisch, C. (2012). A Generic and Open Framework for MWE Treatment ? From Acquisition to Applications - 
Ph.D. Thesis, Universidade Federal do Rio Grande do Sul - UFRGS, Brazil, 248 pp. 
http://mwetoolkit.sourceforge.net/PHITE.php?sitesig=MWE 
Smadja, F. A. (1996). Translating Collocations for Bilingual Lexicons: A Statistical Approach. Association for 
Computational Linguistics, 22 (1):1-38. 
113
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 53?62,
Dublin, Ireland, August 24 2014.
  
A Comparative Study of Different Classification Methods for the Identification of Brazilian Portuguese Multiword Expressions 
  Alexsandro Fonseca Fatiha Sadat  Universit? du Qu?bec ? Montr?al, 201 av. President Kennedy,  Montreal, QC, H2X 3Y7, Canada affonseca@gmail.com sadat.fatiha@uqam.ca Abstract 
This paper presents a comparative study of different methods for the identification of multiword expressions, applied to a Brazilian Portuguese corpus. First, we selected the candidates based on the frequency of bigrams. Second, we used the linguistic information based on the grammatical classes of the words forming the bigrams, together with the frequency information in order to compare the performance of different classification algorithms. The focus of this study is related to different classification techniques such as support-vector machines (SVM), multi-layer perceptron, na?ve Bayesian nets, decision trees and random forest. Third, we evaluated three different multi-layer perceptron training functions in the task of classifying different patterns of multiword expressions. Finally, our study compared two different tools, MWEtoolkit and Text-NSP, for the extraction of multiword expression candidates using different association measures.  1 Introduction The identification of multiword expressions (MWEs) and their appropriate handling is necessary in constructing professional tools for language manipulation (Hurskainen, 2008). MWEs are considered as a very challenging problem for various natural language processing (NLP) applications, such as machine translation.  There are several definitions of MWE in the scientific literature. Smadja (1993) defines MWE as an arbitrary and recurrent word combination; while Choueka (1988) defines them as a syntactic and semantic unit whose exact meaning or connotation cannot be derived directly and unambiguously from the meaning or connotation of its components.  Moreover, Sag et al. (2002) defines MWE as an idiosyncratic interpretation that exceeds the limit of the word (or spaces). We adopt in this paper a definition similar to the one given by Sag et al. (2002): a MWE is an expression formed by two or more words, whose meaning can vary from totally dependent to completely independent of the meaning of its constituent words. Examples of MWEs: ?take care?, ?Bill Gates?, ?coffee break? and ?by the way?. This study treats only two-word MWEs. We are not considering some common Portuguese MWEs, such as ?tempo de espera? (waiting time, lit.: time of waiting), ?dar um tempo? (to have a break, lit.: to give a time) or ?come?ar tudo de novo? (restart, lit. start everything of new). However, our experience and some related work show that we are already covering the majority of MWEs. For their data, for example, Piao et al. (2003, Section 5) found that 81.88% of the recognized MWEs were bigrams. Moreover, our focus is in MWE formed by nouns, adjectives, verbs and adverbs. As a consequence, two-word MWEs formed by prepositions were not considered, such as ?de novo? (again, lit. of new), ?? toa? (for nothing), ?apesar de? (despite of) or ?desde ontem? (since yesterday). In resume, we evaluated the performance of different classification algorithms and tools for the recognition of two-word MWEs formed by nouns, adjectives, verbs and adverbs. We intend, in the future, to extend this study to MWEs formed by words belonging to any grammatical class and having any number of words.   _______________ This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
53
The correct identification of MWEs is important for different NLP applications, such as machine translation, information retrieval and the semantic web, to which the principle of syntactic or semantic unit is important (Watrin and Fran?ois, 2011). Methods for identifying MWEs rely on statistical measures, especially association measures, such as mutual information (Church and Hanks, 1990), log-likelihood or Dice?s coefficient (Smadja, 1996). The basic idea behind such measures can be summarized as follows: the higher the association among the words that appear together in a text, the higher the probability that they constitute a single semantic unit.   There are other methods, which use linguistic information or hybrid approaches that combine statistical measures with the linguistic information, such as the grammatical class of each word, the sense of the composite expression or the syntactic regularities. 2 Related Work Dias and Lopes (2005) present a method for the extraction of MWEs based only on statistics with an application on the Portuguese language. This method consists of a new association measure called ?mutual expectation?. Their method can be applied to extract MWEs formed by two or more words, contiguous or not. The mutual expectation method is based on the LocalMaxs (Silva and Lopes, 1999) algorithm. This algorithm deduces that a n-gram is a MWE if the degree of attraction between its words is greater or equal to the degree of attraction of all its subsets of n-1 words (i.e. all groups of n-1 words contained by the n-gram) and if it is strictly greater than the degree of attraction of all of its super groups of n+1 words (i.e. all groups of n+1 words containing the n-gram). When the n-gram is a bigram (n = 2), only the degree of attraction of its super groups of n+1 words is calculated. Ramisch et al. (2008) analyze the extraction of MWEs based only on statistical information, comparing three association measures: the mutual information, chi-squared and permutation entropy. Then they introduce a method called entropy of permutation and insertion (a hybrid approach), that takes into account linguistic information of the MWE type. Following some patterns, they modify each original MWE candidate by inserting some types of words in some positions and they test if the new MWE are still MWE and they try to identify which kind of modification an MWE type accepts or refuses in a particular language. The new measure is calculated using a formula that combines the probability of occurrence of the original and of the generated MWE. Agarwal et al. (2004) present an approach for extracting MWEs in languages with few resources based on a morphological analyser and a moderate size untagged text corpus. First, they divide the MWEs in categories. For example, Category-2 is formed by noun-noun, adjective-noun and verb-verb bigrams. Then they apply a set of rules to identify or eliminate candidates as MWE. Those rules take into consideration the precedent and/or the next word in the pair and the possible inflections of the words. After this step, association measures are computed. Piao et al. (2003) use, what they call, a semantic field annotator. They use a semantic tagger for the English language called USAS, developed in Lancaster University. This tagger labels words and expressions in a text using 21 categories. For example, Category-A is used for ?general and abstract terms?, Category-B is used for ?the body and the individual?, Category-E is used for ?emotion?, etc. A text labeled with those categories is used to extract the MWE candidates. The differential of this approach is that the candidates are selected not based only on statistical measures. The problem with this is that most of the MWEs, about 68% in the work of Piao et al., appear in the text with a low frequency. As a consequence, most of the methods for extracting MWEs give good precision, but low recall. 3 The Data The current study used the corpus CETENFolha (Corpus de Extractos de Textos Eletr?nicos NILC/Folha de S?o Paulo), available on the website Linguateca Portuguesa (CETENFolha, 2008).  This corpus is composed by excerpts from Brazilian newspaper "Folha de S?o Paulo", and contains over 24 million words. It is part of a project on the automatic processing of the Portuguese (Kinoshita et al., 2006). As the current stage, we used a small fraction of the corpus, composed by 3,409 excerpts of text (about 250,000 words). Each excerpt corresponds to individual news, which covers different areas.  
54
4 Comparison of different classification algorithms 4.1 Pre-processing the data Before the indexation, some pre-processing methods on the corpus were completed, such as lemmatization and elimination of stop words (articles, prepositions, conjunctions). In this study, we are mostly interested in analyzing MWEs formed by nouns, adjectives, adverbs and verbs. And since those stop words are very common in Portuguese, their elimination reduces considerably the number of MWE candidates that would not be relevant to this study.  We created two indexes: one formed only of bigrams and the other only by unigrams. Our results show 49,589 bigrams, with 1,170 having a frequency higher than 3. We selected those 1,170 bigrams as our MWE candidates. By hand, from the 1,170 candidates, we recognized 447 as being Portuguese MWEs.  The main criterion used to consider a bigram as a MWE was that the bigram had a sense on its own. For example: proper names, like ?Adelson Barbosa?, ?George Bush? and ?Belo Horizonte?; support verb constructions: ?tomar cuidado? (to take care), ?fazer sentido? (to make sense); expressions having some idiomatic sense: ?abrir m?o? (to give up, lit. to open hand), ?fazer quest?o? (to insist, to require [that something be done in a specific way], lit. to make question); fixed expressions: ?bens dur?veis? (durable goods), ?senso comum? (common sense), ?curto prazo? (short term). Example of bigrams not considered as MWE: ?Brasil foi? (Brazil was), ?apenas dois? (only two), ?bomba matou? (bomb killed), etc. For each bigram, we found the frequency of its constituent words in the unigram index. Then, we classified by hand each of the words by their grammatical class: 1 for nouns, 2 for adjectives, 3 for verbs, 4 for other classes (mostly adverbs and pronouns) and 5 for proper names. This gave us 25 patterns of bigrams: N-N, N-ADJ, N-V, V-N, PN-PN, etc. We decided not to use a POS-tagger, to ensure that each word would have its grammatical class assigned correctly, creating the most correct possible training and testing data sets for the classification algorithms. We then created a matrix of 1,170 lines and five columns. For each line, the first column represents the frequency of a bigram in the excerpt of text, the second column represents the frequency of the first bigram?s word, the third column represents the frequency of the second bigram?s word, the fourth column represents the grammatical class of the first bigram?s word and the fifth column represents the grammatical class of the second bigram?s word. This matrix was used to evaluate the precision and recall of different classification algorithms.  4.2 Evaluation  We applied nine different classification algorithms to our data set. The parameters used with each algorithm are listed below. Decision tree: C4.5 algorithm (Quinlan, 1993) with confidence factor = 0.25. Random Forest (Breiman, 2001): number of trees = 10; max depth = 0; seed = 1. Ada Boost (Freund and Schapire, 1996): classifier = decision stamp; weight threshold = 100; iterations = 10; seed = 1. Bagging (Breiman, 1996): classifier = fast decision tree learner (min. number = 2; min. variance = 0.001; number of folds = 3; seed = 1; max. depth = -1); bag size percent = 100; seed = 1; number of execution slots = 1; iterations = 10. KNN (Aha and Kibler, 1991): K = 3; window size = 0; search algorithm = linear NN search (distance function = Euclidian distance). SVM (Chang and Lin, 2001): cache size = 40; cost = 1; degree = 3; eps = 0.001; loss = 0.1; kernel type = radial basis function; nu = 0.5; seed = 1. Multilayer perceptron: learning rate = 0.3; momentum = 0.2; training time = 500; validation threshold = 500; seed = 0;  Bayesian net: search algorithm = k2 (Cooper and Herskovits, 1992); estimator = simple estimator (alpha = 0.5). As we can see in Table 1, the values of precision are very similar for all the algorithms, varying between 0.830 (random forest) and 0.857 (bagging), with the exception of SVM, which gave a precision of 0.738. The recall values were between 0.831 and 0.857 (0.655 for SVM). 
55
We obtained good precision and recall.  However, we must consider that the values of recall are based only on the MWEs present in our reference list, and not in the entire corpus, since we could not count all the MWEs present in the corpus.  Algorithm TP Rate FP Rate Precision Recall Decision tree 0.853 0.158 0.854 0.853 Random forest 0.831 0.194 0.830 0.831 Ada boost 0.837 0.196 0.836 0.837 Bagging 0.857 0.163 0.857 0.857 KNN ? k = 3 0.846 0.171 0.846 0.846 SVM 0.655 0.553 0.738 0.655 M. perceptron 0.852 0.174 0.851 0.852 Na?ve B. net 0.836 0.170 0.839 0.836 Bayesian net 0.842 0.170 0.843 0.842  Table 1: True-positive rate, false-positive rate, precision and recall for nine classification algorithms. 5 Bigrams patterns classification We chose one of the algorithms with the best performance (multi-layer perceptron) and we evaluated it using three different training functions, bayesian regulation back propagation (br), Levenberg-Marquardt (lm) and scaled conjugate gradient (scg), and compared their performance in the classification of different patterns of bigrams as MWE. For this comparison we used the patterns that gave 10 or more samples of MWEs. We had eight patterns that together represent 59% of the candidate bigrams (689/1,170) and 94% of the MWEs that appear three or more times in the corpus (420/447). The tables 2.a, 2.b and 2.c show the results. ?N? stands for ?Noun?, ?A? for adjective, ?O? for other classes (mostly adverbs and pronouns) and ?PN? for ?proper names?.  Analyzing the three tables, we see that we had best results with the patterns N-A (e.g. ?agencias internacionais?, ?ajuste fiscal?, ?America Latina?) and PN-PN (?Adelson Barbosa?, ?Ayrton Senna?, ?Bill Clinton?). The function lm gave the best value for the F1 measure (0.912) for the pattern N-A, and the function scg gave the best value for the pattern PN-PN (0.931). In general, we obtained the weakest results with the patterns O-N (e.g. ex-presidente, primeiro mundo) and A-PN (S?o Paulo, Nova York). Using the training functions ?lm? and ?scg?, none of the 10 MWEs belonging to the pattern O-O (apesar disso, al?m disso) was recognized, and none of the 46 MWEs belonging to the pattern O-N was recognized, when using the training function ?scg?. The last line of each table show the total values for the eight patterns, for the three learning functions. We had the best precision and recall using the ?lm? function.  Pattern Bigrams MWE TP FP TN FN Prec. Recall F1 N-A 229 193 176 27 9 17 0.867 0.912 0.889 O-N 164 46 14 23 95 32 0.378 0.304 0.337 PN-PN 117 101 94 15 1 7 0.862 0.931 0.895 A-N 53 21 13 3 29 8 0.813 0.619 0.703 O-O 46 10 5 9 27 5 0.357 0.500 0.417 N-PN 34 16 7 9 9 9 0.438 0.438 0.438 N-N 31 20 11 6 5 9 0.647 0.550 0.595 A-PN 15 13 3 1 1 10 0.750 0.231 0.353 All Pat. 689 420 323 93 176 97 0.776 0.769 0.773  Table 2a: Multi-layer perceptron using Bayesian regulation back-propagation as training function: precision, recall and F-measure in the classification of the most common bigram?s patterns.     
56
Pattern Bigrams MWE TP FP TN FN Prec. Recall F1 N-A 229 193 191 35 1 2 0.845 0.990 0.912 O-N 164 46 11 6 112 35 0.647 0.239 0.349 PN-PN 117 101 101 16 0 0 0.863 1.000 0.927 A-N 53 21 17 4 28 4 0.810 0.810 0.810 O-O 46 10 0 2 34 10 0.000 0.000 0.000 N-PN 34 16 11 5 13 5 0.688 0.688 0.688 N-N 31 20 16 7 4 4 0.696 0.800 0.744 A-PN 15 13 2 2 0 11 0.500 0.154 0.235 All Pat. 689 420 349 77 192 71 0.819 0.831 0.825  Table 2b: Multi-layer perceptron using Levenberg-Marquardt as training function: precision, recall and F-measure in the classification of the most common bigram?s patterns.  Pattern Bigrams MWE TP FP TN FN Prec. Recall F1 N-A 229 193 187 33 3 6 0.850 0.969 0.906 O-N 164 46 0 0 118 46 0.720 0.000 0.000 PN-PN 117 101 101 15 1 0 0.871 1.000 0.931 A-N 53 21 17 10 22 4 0.630 0.810 0.708 O-O 46 10 0 0 36 10 0.783 0.000 0.000 N-PN 34 16 2 7 11 14 0.222 0.125 0.160 N-N 31 20 18 8 3 2 0.692 0.900 0.783 A-PN 15 13 2 1 1 11 0.667 0.154 0.250 All Pat. 689 420 327 74 195 93 0.815 0.779 0.797  Table 2c: Multi-layer perceptron using scaled conjugate gradient as training function: precision, recall and F-measure in the classification of the most common bigram?s patterns. 6 Evaluation of two different tools Using the same excerpts of our corpus, we proceeded to the evaluation of two different tools for extracting MWEs from text: MWEtoolkit1 (Ramisch, 2012) and Text-NSP2 (Banerjee and Pedersen, 2003). 6.1 MWEtoolkit Before using this tool, we POS-tagged the corpus using TreeTagger3 (Schmid, 1994), with a Portuguese parameter file. Then we transformed the tagged corpus to the xml format used by MWEtoolkit using MWEtoolkit script treetagger2xml. After generating the index, we defined the patterns file using the following bigrams patterns: N-N, N-ADJ, N-V, ADJ-ADJ, ADJ-N, ADJ-V, V-V, V-N and V-ADJ. There is not a PN tag for proper name in TreeTagger, so the proper names were treated as nouns (N). And we decided not to use the other grammatical classes (adverbs, pronouns, etc.), labeled as ?O? in the previous section, because the only patterns that gave more than 10 MWEs with frequency higher than 3 using those classes were O-N and O-O, and we did not obtain good values for their classification using the multi-layer perceptron classification algorithms. We used those patterns to generate all the bigrams and we obtained 28,738 candidates, with their frequencies and the frequencies of each word composing the bigram. Then we calculated five different association measures for each candidate: maximum likelihood estimator (mle), pointwise mutual information (pmi), Student's t-test (t), Dice's coefficient (dice), and log-likelihood (ll).     _________________________________ 1 http://mwetoolkit.sourceforge.net/PHITE.php?sitesig=MWE 2 http://search.cpan.org/~tpederse/Text-NSP/ 3 http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/ 
57
 Then we created candidates files ordered by each of these five association measures and we ranked the n best candidates, n = 50, 100, 500, 1000 and 5000. Finally, we used MWEtoolkit?s automatic evaluation script to evaluate each of these ranked candidates against our reference file. The reference file was created with the 447 MWEs selected according to the method described in Section 4, i. e., all the bigrams that appear three or more times in our corpus and that we manually considered as a MWE. It is important to note that our reference file does not contain all the MWEs with two words in the corpus, since we generated more than 49,000 bigrams and we could not evaluate all of them by hand. Furthermore, the corpus is formed by newspaper texts, treating different subjects, thus it is more difficult to create a closed set of all possible two-word MWEs. Therefore, our evaluation is a comparison of how many of the most frequent two-word MWEs in our corpus are ranked as the n best candidates by each of the association measures. Table 3 and Figure 1 show the result of our evaluation using the MWEtoolkit. Each number in the table represents how many of the MWEs in our reference list were found among the n best ranked candidates. For example, for the ?ll? measure, among the 50 best ranked candidates 27 are MWEs that appear in our reference list.    dice ll mle pmi t 50 0 27 5 0 11 100 12 55 10 0 31 500 34 152 49 1 105 1000 59 170 87 9 169 5000 161 187 179 94 186  Table 3: MWEtoolkit:  number of MWEs among the first n-best candidates, ranked by five association measures.  
  Figure 1: MWEtoolkit: five association measures performance comparison.  ll TP Prec. Recall F1 50 27 0.54 0.06 0.11 100 55 0.55 0.12 0.20 500 152 0.30 0.34 0.32 1000 170 0.17 0.38 0.23 5000 187 0.04 0.41 0.07  Table 4: MWEtoolkit: precision, recall and F-measure for the log-likelihood measure.  Analyzing the results, we notice that with log-likelihood measure we could find the highest number of MWEs present in our reference list, for all values of n. Since our reference list is formed by the most frequent MWEs in the corpus (frequency higher than three), this is an evidence of how suitable this measure is when the task is to find the most frequent two-word MWEs. Table 4 presents the results of precision, recall and F-measure for the ll-measure for different values of n candidates. However, it should be kept in mind that precision and recall here are based on our reference list, which does not contain all the two-word MWEs in the corpus. 
0	 ?
50	 ?
100	 ?
150	 ?
200	 ?
50	 ? 100	 ? 500	 ? 1000	 ? 5000	 ?
dice	 ?
ll	 ?
mle	 ?
pmi	 ?
t	 ?
58
6.2 Text-NSP Before applying this tool, the only pre-processing performed was to remove the XML tags. The next step was to define a stop words list file, since we are interested in finding MWEs following the patterns N-N, N-ADJ, N-V, like in Sections 4 and 5. We ran the program using the script ?count.pl?, giving as parameter the stop words file and the corpus file, and 2 as n-gram value, meaning that we wanted to generate only bigrams. The exit file is a list of all bigrams in the corpus, and each line contains a bigram, the frequency of the bigram, and the frequency of each of the two words forming the bigram. Using the exit file and the script ?statistics.pl? we generated the candidates? files ranked by four different association measures: Dice's coefficient (dice), log-likelihood (ll), pointwise mutual information (pmi) and Student's t-test (t). Maximum likelihood estimator is not implemented by Text-NSP. Then we transformed each of the candidates files to the XML format used by the MWEtoolkit and we used the MWEtoolkit scripts to create files with the n best candidates (n = 50, 100, 500, 1000 and 5000) and to evaluate each of the files against our reference file. Table 5 and Figure 2 show the results of those evaluations.    dice ll pmi t 50 7 31 0 23 100 7 64 0 39 500 8 241 1 180 1000 11 314 4 331 5000 73 382 15 406  Table 5: Text-NSP:  number of MWEs among the first n-best candidates, ranked by four association measures.  
  Figure 2: Text-NSP: four association measures performance comparison.  ll TP Prec. Recall F1 50 31 0.62 0.07 0.12 100 64 0.64 0.14 0.23 500 241 0.48 0.54 0.51 1000 314 0.31 0.70 0.43 5000 382 0.08 0.85 0.14  Table 6: Text-NSP: precision, recall and F-measure for the log-likelihood measure.  The results show that for values of n = 50,100, and 500 we obtained the best results using the log-likelihood measure and for n = 1000 and 5000, Student?s t-test gave the best results.  Comparing with MWEtoolkit, we had better results with Text-NSP for the log-likelihood and the Student?s t-test measures, and weaker results for the dice and pmi measures.  Table 6 shows the precision, recall and F-measure that we obtained for the log-likelihood measure. We had very good precision values using the Text-NSP with the log-likelihood measure. For example, from the 50 best ranked candidates by this measure, 31 were MWEs present in our reference list.  
0	 ?
100	 ?
200	 ?
300	 ?
400	 ?
500	 ?
50	 ? 100	 ? 500	 ? 1000	 ? 5000	 ?
dice	 ?
ll	 ?
pmi	 ?
t	 ?
59
6.3 Comparison between MWEtoolkit and Text-NSP  Using the 500 best candidates generated by MWEtoolkit and Text-NSP, ranked by Student?s t-test, we analyzed by hand those 500 candidates to decide which ones are Brazilian Portuguese MWEs. Table 7 shows the precision given by each of the tools for the first n candidates, n = 50, 100, 150?500. Text-NSP showed higher precision than MWEtoolkit for all values of n candidates, especially for the smaller values of n. With MWEtoolkit, the precision was around 40%, while with Text-NSP it starts with 62% for the first best 50 candidates and decreases to 48% for the first best 500 candidates. We can suppose that for an application interested in a small number of Brazilian Portuguese MWE candidates, Text-NSP would be a better choice, and as the number of candidates increases, the programs tend to have similar performance.  Checking the best ranked candidates generated by MWEtoolkit, we noted that it ranked well some bigrams formed by a noun + the preposition ?a? (the/fem.), a pattern that is common in a Brazilian Portuguese corpus, but that usually does not form MWEs. This happened, despite not having any pattern that includes preposition in our patterns? list, because the POS-tagger used (TreeTagger) wrongly labelled those ?a? prepositions as nouns. The same is true for the pronoun ?seu/sua? (his/her), which was labelled as adjective. This can explain the difference in performance between the tools, when comparing the implementation of the same association measures. As in the tests performed in Section 5, the most common patterns of MWE found by both programs were noun-adjective (e.g. Casa Branca, plano real, Estados Unidos) and proper name-proper name (e.g. Fernando Henrique, Ayrton Senna, Paulo Maluf).  n first cand. MWEtoolkit Text-NSP 50 0.34 0.62 100 0.47 0.57 150 0.43 0.55 200 0.41 0.53 250 0.40 0.54 300 0.41 0.53 350 0.37 0.53 400 0.41 0.50 450 0.42 0.52 500 0.40 0.48  Table 7: MWEtoolkit and Text-NSP precision for the first n best candidates, using Student?s t-test association measure. 7 Conclusions and future work We obtained very similar results using different algorithms for the classification of MWEs, with bagging, decision trees and multi-layer perceptron having a slightly better performance. Using multi-layer perceptron with three different training functions, we identified the bigram?s patterns that are better classified as MWE. With the function Levenberg-Marquardt we had better results in classifying the pattern noun-adjective (the most common in our corpus) and the function Scaled Conjugate Gradient was the most successful in classifying MWEs following the pattern proper name-proper name. The comparison between two programs for automatic extraction of MWEs showed that Text-NSP had a better precision than MWEtoolkit, especially for smaller number of candidates. As the number of candidates increases, the difference in performance between the two programs decreases.  It is important to note that MWEtoolkit is more complete, in the sense it implements more statistical measures, makes the comparison between the output candidates file and a reference list file and generates a list of candidates having more complete information, including all the statistical measures of each candidate in the same file, and in a XML format more easily consumable by other programs. 
60
As a future work, we intend to perform a similar comparison of tools and classification algorithms for the extraction of Brazilian Portuguese MWEs, not limiting our candidates to bigrams, but studying n-grams in general, also allowing noncontiguous n-grams.  References  Agarwal, A., Ray, B., Choudhury, M., Sarkar, S., Basu, A.: Automatic Extraction of Multiword Expressions in Bengali: An Approach for Miserly Resource Scenarios. In: Proceedings of ICON 2004, pp. 165-174. Macmillan, Basingstoke (2004). Aha, D. and Kibler, D. (1991). Instance-based learning algorithms. Machine Learning. 6:37-66. Antunes, S. and Mendes, A. MWE in Portuguese - Proposal for a Typology for Annotation in Running Text. Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pp. 87?92, Atlanta, Georgia, 13-14 June 2013.  Banerjee, S and Pedersen, T. (2003). The Design, Implementation, and Use of the Ngram Statistic Package. In Proceedings of th Fourth International Conference on Intelligent Text Processing and Computational Linguistics, pp. 370-381, Mexico City. Breiman, L. (2001). Random Forests. Machine Learning. 45(1):5-32. Breiman , L. (1996). Bagging predictors. Machine Learning. 24(2):123-140. CETENFolha (Corpus de Extractos de Textos Electr?nicos NILC/Folha de S. Paulo) (2008). Linguateca ? Portugal ? www.linguateca.pt/ACDC/ Chang, Chih-Chung and Lin, Chih-Jen (2001). LIBSVM - A Library for Support Vector Machines. http://www.csie.ntu.edu.tw/~cjlin/libsvm/. Choueka, Y. (1988). Looking for needles in a haystack or locating interesting collocational expressions in large textual databases. In RIAO?88, pp. 609?624. Church, K. W. and Hanks, P (1990). Word Association Norms, Mutual Information and Lexicography. Computational Linguistics, 16(1):22?29. Cooper, G.  and Herskovits, E. (1992). A Bayesian method for the induction of probabilistic networks from data. Machine Learning. 9(4):309-347. Dias, G. H. and Lopes, J.G.P. (2005). Extrac??o autom?tica de unidades polilexicais para o portugu?s. In: A L?ngua Portuguesa no Computador. S?o Paulo: Ed. Mercado de Letras.  Freund, Y. and Schapire, R. E (1996). Experiments with a new boosting algorithm. In: Thirteenth International Conference on Machine Learning, San Francisco, pp. 148-156. Hendrickx, I., Mendes, A. and Antunes, S. (2010). Proposal for Multi-Word Expression Annotation in Running Text Portuguese. Hurskainen, A. (2008). Multiword Expressions and Machine Translation. Technical Reports in Language Technology Report No 1, 2008 (http://www.njas.helsinki.fi/salama). Kinoshita, J., Nascimento Salvador, L.D. and Dantas de Menezes, C., E. (2006). CoGrOO: a Brazilian-Portuguese Grammar Checker based on the CETENFOLHA Corpus. In proceedings of LREC 2006. http://www.pcs.usp.br/~cogroo/papers/Artigo_LREC_2006.pdf Piao, S., Rayson, P., Archer, D., Wilson, A., and McEnery, T.  (2003). Extracting Multiword Expressions with a Semantic Tagger. In: Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, at ACL 2003, 41st Annual Meeting of the Association for Computational Linguistics, 2003-07-12, Sapporo, Japan. Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers. Ramisch, C. (2012). A generic and open framework for MWE treatment ? from acquisition to applications - Ph.D. Thesis, Universidade Federal do Rio Grande do Sul (UFRGS), Brazil. Ramisch, C., Schreiner, P., Idiart, M. and Villavicencio, A. (2008). An Evaluation of Methods for the Extraction of Multiword Expressions, Proceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions (MWE 2008), Marrakech, Morocco, June, 2008. 
61
Sag, I., Baldwin, T., Bond, F., Copestake, A. and Flickinger, D. (2002). Multiword expressions: A pain in the neck for NLP. In Proc. of the 3rd CICLing (CICLing-2002), volume 2276/2010 de LNCS, pp. 1?15, Mexico City, Mexico. Schmid, Helmut (1994).  Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings of International Conference on New Methods in Language Processing, Manchester, UK. Silva, J., and Lopes, G. (1999). A local Maxima Method and a Fair Dispersion Normalization for Extracting Multiword Units. In 6th Meeting on the Mathematics of Language, pp. 369-381.  Smadja, F. A. (1996). Translating Collocations for Bilingual Lexicons: A Statistical Approach. Association for Computational Linguistics, 22 (1):1-38. Smadja, F. A. (1993). Retrieving collocations from text: Xtract. Computational Linguistics., 19(1):143?177. Watrin, Patrick and Fran?ois, Tomas (2011). An N-gram frequency database reference to handle MWE extraction in NLP applications. Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pp. 83?91. 
62
