Empirical Estimates of Adaptation: 
The chance of Two Noriegas is closer to p/2 than p 2 
Kenneth W. Church 
AT&T Labs-Research, 180 Park Ave., Florham Park, NJ., USA 
kwc@research.att.com 
Abstract 
Repetition is very common. Adaptive language models, which allow probabilities to change or adapt 
after seeing just a few words of a text, were introduced in speech recognition to account for text cohesion. 
Suppose a document mentions Noriega once. What is the chance that he will be mentioned again? if the 
first instance has probability p, then under standard (bag-of words) independence assumptions, two in- 
stances ought to have probability p2, but we find the probability is actually closer to p/2. The first men- 
tion of a word obviously depends on frequency, but surprisingly, the second does not. Adaptation de- 
pends more on lexical content han fl'equency; there is more adaptation for content words (proper nouns, 
technical terminology and good keywords for information retrieval), and less adaptation for function 
words, cliches and ordinary first names. 
1. Introduction 
Adaptive language models were introduced in the 
Speech Recognition literature to model repetition. 
Jelinek (1997, p. 254) describes cache-based 
models which combine two estimates of word 
(ngram) probabilities, Prl., a local estimate based 
on a relatively small cache of recently seen 
words, and PrG, a global estimate based on a 
large training corpus. 
1. Additive: 
Pr A (w)= XPrL (w) +(1 - X) PrG (w) 
2. Case-based: 
JX I Pr/.(w) if w~ cache 
" Pr G (w) otherwise PF c (W) = 1~ 2 
Intuitively, if a word has been mentioned re- 
cently, then (a) the probability of that word (and 
related words) should go way up, and (b) many 
other words should go down a little. We will re- 
fer to (a) as positive adaptation and (b) as 
negative adaptation. Our empirical experiments 
confirm the intuition that positive adaptation, 
Pr(+adapt), is typically much larger than neg- 
ative adaptation, Pr( - adapt). That is, 
Pr( +adapt) >> Pr(prior) > Pr(-adapt). Two 
methods, Pr( + adapt ) and Pr( + adapt2), will 
be introduced for estimating positive adaptation. 
1. Pr( +adapt 1)=PrOve test\[w~ history) 
2. Pr(+adapt2)=Pr(k'>_2lk>_l )=d.f2/dfl 
The two methods produce similar results, usually 
well within a factor of two of one another. The 
first lnethod splits each document into two equal 
pieces, a hislory portion and a test portion. The 
adapted probabilities are modeled as the chance 
that a word will appeal" in the test portion, given 
that it appeared in the history. The second 
method, suggested by Church and Gale (1995), 
models adaptation as the chance of a second lnen- 
tion (probability that a word will appear two or 
inore times, given that it appeared one or more 
times). Pr(+adapt2) is approximated by 
dJ2/dfl, where c./\['k is the number of documents 
that contain the word/ngram k or more times. 
(dfa. is a generalization of document .frequeno,, 
d.f~ a standard term in information Retrieval.) 
Both inethods are non-parametric (unlike cache 
lnodels). Parametric assumptions, when appro- 
priate, can be very powerful (better estimates 
from less training data), but errors resulting from 
inappropriate assumptions can outweigh tile 
benefits. In this elnpirical investigation of the 
magnitude and shape o1' adaptation we decided to 
use conservative non-parametric methods to 
hedge against he risk of inappropriate parametric 
assumptions. 
The two plots (below) illustrate some of the 
reasons for being concerned about standard 
parametric assumptions. The first plot shows the 
number of times that tile word "said" appears ill 
each of the 500 documents ill the Brown Corpus 
(Francis & Kucera, 1982). Note that there are 
quite a few documents with more than 15 in- 
stances of "said," especially in Press and Fic- 
tion. There are also quite a few documents with 
hardly any instances of "said," especially in the 
Learned genre. We have found a similar pattern 
in other collections; "said" is more common in 
newswire (Associated Press and Wall Street 
Journal) than technical writing (Department of 
Energy abstracts). 
180 
The second plot (below) conlpares these f:hown 
Corpus o\[-)sorvations to a Poisson. Tile circles 
indicate the nulnber of docun-ierits that have .r in- 
stances of "said." As mentioned above, Press 
and Fiction docunlents can lilentioil "said" 15 
times or lllore, while doculllOlltS in the Learned 
genre might not mention the word at all. The line 
shows what woukt be expected under a Poisson. 
Clearly the line does not fit the circles very well. 
The probability of "said" depends on many 
factors (e.g, genre, topic, style, author) that make 
the distributions broader than chance (Poisson). 
We lind especially broad distributions for words 
that adapt a lot. 
"said" in Brown Corpus 
Re 
Press 
,bbi ,,s 
ion 
Bc 
_ore 
I i 
;i q 
i \[,i!0! 
! 
ili111'I,;,!~,1 ? ~ ,:,iil 
Learned i o (4 O \  '| 
Ile-Lettt.'s FictiohHU n r 
, i 
'1' 
i I,~ i 
0 100 200 300 400 500 
document number 
Poisson Doesn't Fit 
g 
ix) 
O 
O 
;O 
o 
' O0oO O 
Ooo o 
O ? 0 
0 5 10 
oo  0 
0 o 
o o 
o 0 0 o o oo  
15 20 25 30 
freq 
We will show that adaptation is huge. 
Pr(+ adapt) is ot'ten several orders of magnitude 
larger than Pr(prior). In addition, we lind that 
Pr(+adapt) has a very different shape fiom 
Pr(prior). By construction, Pr(prior) wu'ies 
over many orders o1' magnitude depending on the 
frequency of the word. Interestingly, though, we 
find that Pr(+adapt) has ahnost no dependence 
on word frequency, although there is a strong 
lexical dependence. Some words adapt more than 
others. The result is quite robust. Words that ad- 
apt more in one cortms also tend to adapt more in 
another corpus of similar material. Both the 
magnitude and especially the shape (lack of de- 
pendence on fiequency as well as dependence on 
content) are hard to capture ill an additive-based 
cache model. 
Later in the paper, we will study neighbmw, 
words that do not appear in the history but do 
appear in documents near the history using an in- 
formation retrieval notion of near. We find that 
neighbors adapt more than non-neighbors, but not 
as much as the history. The shape is in between 
as well. Neighbors have a modest dependency on 
fiequency, more than the history, but not as much 
as the prior. 
Neighbors are an extension of Florian & Yarow- 
sky (1999), who used topic clustering to build a 
language model for contexts uch as: "It is at 
least on the Serb side a real setback to lhe x." 
Their work was motivated by speech recognition 
apl)lications where it would be desirable for the 
hmguage model to l'avor x = "peace" over x = 
"piece." Obviously, acoustic evidence is not 
very hell~l'tfl in this case. Trigrams are also not 
very helpful because the strongest clues (e.g., 
"Serb," "side" and "setback") are beyond the 
window of three words. Florian & Yarowsky 
cluster documents into about 102 topics, and 
compute a separate trigram language model for 
each topic. Neighbors are similar in spirit, but 
support more topics. 
2. Es t imates  o f  Adaptation: Method 1 
Method 1 splits each document into two equal 
pieces. The first hall' of each document is re- 
ferred to as the histoo, portion of the document 
and the second half of each document is referred 
to as the test portion of the documenl. The task is 
to predict he test portion of the document given 
the histm3,. We star! by computing a contingency 
table for each word, as ilhlstrated below: 
l)ocuments containing "hostages" in 1990 AP 
test test 
history a =638 b =505 
histoo, c =557 d =76787 
This table indicates that there are (a) 638 doc- 
uments with hostages in both the first half 
(history) and the second half (test), (b) 505 doc- 
uments with "hostages" in just the first half, (c) 
557 documents with "hostages" in just the 
second halt', and (d) 76,787 documents with 
"hostages" in neither half. Positive and negative 
adaptation are detined in terms a, b, c and d. 
181 
Pr( + adapt I ) = Pr(w E test Iw e histoo,) 
a 
a+b 
Pr ( -adapt  l )=Pr (we test\]-~we histoo,)= c 
c+d 
Adapted probabilities will be compared to: 
Pr (prior) = Pr( w e test) = ( a + c ) /D 
whereD =a+b +c+d.  
Positive adaptation tends to be much large, than 
the prior, which is just a little larger than negative 
adaptation, as illustrated in the table below for the 
word "hostages" in four years of the Associated 
Press (AP) newswire. We find remarkably con- 
sistent results when we compare one yea," of the 
AP news to another (though topics do come and 
go over time). Generally, the differences of 
interest are huge (orders of magnitude) compared 
to the differences among various control condi- 
tions (at most factors of two or three). Note that 
values are more similar within colmnns than 
across columns. 
Pr(+adapt) >> Pr(prior) > Pr(-adapt) 
prior +adapt -adapt source w 
0.014 0.56 0.0069 AP87 
0.015 0.56 0.0072 AP90 
0.013 0.59 0.0057 AP91 
0.0044 0.39 0.0030 AP93 
hostages 
3. Adaptation is Lexical 
We find that some words adapt more than others, 
and that words that adapt more in one year of the 
AP also tend to adapt more in another year of the 
AP. In general, words that adapt a lot tend to 
have more content (e.g., good keywords for infor- 
mation retrieval (IR)) and words that adapt less 
have less content (e.g., function words). 
It is often assumed that word fi'equency is a good 
(inverse) con'elate of content. In the psycholin- 
guistic literature, the term "high frequency" is 
often used syrlouymously with "function 
words," and "low frequency" with "content 
words." In IR, inverse document fiequency 
(IDF) is commonly used for weighting keywords. 
The table below is interesting because it ques- 
tions this very basic assumption. We compare 
two words, "Kennedy" and "except," that are 
about equally frequent (similar priors). 
Intuitively, "Kennedy" is a content word and 
"except" is not. This intuition is supported by 
the adaptation statistics: the adaptation ratio, 
Pr(+adapt) /Pr(pr ior) ,  is nmch larger for 
"Kennedy" than for "except." A similar pattern 
holds for negative adaptation, but in the reverse 
direction. That is, Pr ( -adapt ) /P r (pr io r )  is 
lnuch slnaller for "Kennedy" than for "except." 
Kenneclv adapts more than except 
prior +adapt -adapt source w 
0.012 0.27 0.0091 AP90 
0.015 0.40 0.0084 AP91 
0.014 0.32 0.0094 AP93 
Kennedy 
0.016 0.049 0.016 AP90 
0.014 0.047 0.014 AP91 
0.012 0.048 0.012 AP93 
except 
In general, we expect more adaptation for better 
keywords (e.g., "Kennedy") and less adaptatiou 
for less good keywords (e.g., fnnction words such 
as "except"). This observation runs counter to 
the standard practice of weighting keywords 
solely on the basis of frequency, without con- 
sidering adaptation. In a related paper, Umemura 
and Church (submitted), we describe a term 
weighting method that makes use of adaptation 
(sometimes referred to as burstiness). 
Distinctive surnames adapt more 
than ordinary first names 
prior +adapt -adapt source w 
0.0079 0.71 0.0026 AP90 Noriega 
0.0038 0.80 0.0009 AP91 
0.0006 0.90 0.0002 AP90 Aristide 
0.0035 0.77 0.0009 AP91 
0.0011 0.47 0.0006 AP90 Escobar 
0.0014 0.74 0.0006 AP91 
0.068 0.18 0.059 AP90 John 
0.066 0.16 0.057 AP91 
0.025 0.11 0.022 AP90 George 
0.025 0.13 0.022 AP91 
0.029 0.15 0.025 AP90 Paul 
0.028 0.13 0.025 AP91 
The table above compares surnames with first 
names. These surnames are excellent keywords 
unlike the first names, which are nearly as useless 
for IR as function words. The adaptation ratio, 
Pr(+adapt) /Pr (pr ior ) ,  is much larger for the 
surnames than for the first names. 
What is the probability of seeing two Noriegas in 
a document? The chance of the first one is 
p=0.006. According to the table above, the 
chance of two is about 0.75p, closer to p/2 than 
1 )2. Finding a rare word like Noriega in a doc- 
ument is like lighming. We might not expect 
182 
lightning to strike twice, but it hapt)ens all the 
time, especially for good keywords. 
4. Smoothing (for low frequency words) 
Thus fitr, we have seen that adaptation can be 
large, but to delnonstrate ile shape property (lack 
of dependence on frequency), tile counts in the 
contingency table need to be smoothed. The 
problem is that the estimates of a, b, c, d, and es- 
pecially estimates of the ratios of these quantities, 
become unstable when the counts are small. The 
standard methods of smoothing in tile speech re- 
cognition literature are Good-Turing (GT) and 
tteld-Out (He), described in sections 15.3 & 15.4 
of Jelinek (1997). In both cases, we let r be an 
observed count of an object (e.g., the fi'equency 
of a word and/or ngram), and r* be our best 
estimate of r in another COl'pUS of the same size 
(all other things being equal). 
4.1 Standard Held-Out (He)  
He splits the training corpus into two halves. 
The first half is used to count r for all objects of 
intercst (e.g., the frequency of all words in vocal> 
ulary). These counts are then used to group 
objects into bins. The r m bin contains all (and 
only) tile words with count r. For each bin, we 
colnpute N r, tile number of words in the r m bin. 
The second half of the training corpus is then 
used to compute Cr, tile a,,,,re,,'~m~ ~,.~ frequency of 
all the words in the r ~h bin. The final result is 
simply: r*=Cr./N,,  ll' the two halves o1' tile 
trail)ing corpora or the lest corpora have dilTercnt 
sizes, then r* should be scaled appropriately. 
We chose He  in this work because it makes few 
assumptions. There is no parametric model. All 
that is assumed is that tile two halves of tile 
training corpus are similar, and that both are 
similar to the testing corpus. Even this assulnp- 
tion is a matter of some concern, since major 
stories come and go over time. 
4.2 Application of He  to Contingency Tables 
As above, the training corpus is split into two 
halves. We used two different years of AP news. 
The first hall' is used to count document 
frequency rl/: (Document frequency will be used 
instead of standard (term) frequency.) Words are 
binned by df and by their cell in the coutingency 
table. The first half of tile corpus is used to 
compute the number of words in each bin: Nd, ., 
N4fj,, N41.(: and Ndl.,,t; the second half of the 
corpus is used to compute the aggregate doc- 
ument flequency for the words in each bin: C,!f, a, 
C41.,l), Cdl:,,c and C4f,d. The final result is stro- 
p!y: 
c~}.=C,/.~/N~l.r and d~i=C4f,,//N4f,~/. We ? ./ .I,: ,/,' , I  
compute tile probabilities as before, but replace a, 
b, c, d with a *, b *, c *, d*, respectively. 
h 
o 
5 
n 
2 
? 
1 
History (h) >> Neighborhood (n) >> Prior (p) 
n " n 
P 
P 
5 IO Jro 1oo 5oo lOOO 
Oocunmnt Frequent}, (d\[) 
With these smoothed estimates, we arc able to 
show that Pr(+adcq~t), labeled h in tile plot 
above, is larger and less dependent on fi'equency 
than l)r(prior), labeled p. The plot shows a third 
group, labeled n for neighbors, which will be 
described later. Note that Ihe ns fall between tile 
ps and tile hs. 
Thus far, we have seen that adaptation can be 
huge: Pr(+a&q)l)>> Pr(prior), often by two or 
three orders of magnitude. Perhaps even more 
surprisingly, although Ihe first mention depends 
strongly on frequency (d./), the second does not. 
Some words adapt more (e.g., Noriega, Aristide, 
Escobar) and some words adapt less (e.g., John, 
George, Paul). Tile results are robust. Words 
that adapt more in one year of AP news tend to 
adapt more in another year, and vice versa. 
5. Method 2: l'r( + adapt2 )
So far, we have limited our attention to the rel- 
atively simple case where the history and the test 
arc tile same size. In practice, this won't be the 
case. We were concerned that tile observations 
above might be artil'acts omehow caused by this 
limitation. 
We exl~erimented with two approaches for under- 
standing the effect of this limitation and found 
that the size of the history doesn't change 
Pr(+adal)t ) very much. The first approach split 
the history and the test at wlrious points ranging 
from 5% to 95%. Generally, Pr(+adaptl ) in- 
creases as the size of the test portion grows rel- 
ative to the size o f  the history, but the effect is 
183 
relatively small (more like a factor of two than an 
order of magnitude). 
We were even more convinced by the second 
approach, which uses Pr(+adapt2 ), a completely 
different argument for estimating adaptation and 
doesn't depend on the relative size of the history 
and the test. The two methods produce 
remarkably silnilar results, usually well within a 
factor of two of one another (even when adapted 
probabilities are orders of magnitude larger than 
the prior). 
Pr(+adapt2)  makes use of d./)(w), a generaliz- 
ation of document frequency, d,/)(w) is the 
number of documents with .j or more instances of 
w; (dfl is the standard notion of dJ). 
Pr( + adapt 2 ) = Pr(k>_2 \[k>_ 1 ) = df2/(!/" 1 
Method 2 has some advantages and some disad- 
vantages in comparison with method 1. On the 
positive side, method 2 can be generalized to 
compute the chance of a third instance: 
Pr(k>_31k>_2 ). But unfortunately, we do not 
know how to use method 2 to estimate negative 
adaptation; we leave that as an open question. 
2 
Adaptation is huge (and hardly dependent on frequency) 
pt Ik  >= 3 \] k >= 2) = d\[3* / dr2" 
. . . . . .  
2 2 2222 
2 
Pr(k >= 2 I k >= 1) = dr2' / dr1" 
, 
111111 
1 1 
1 
I 
10 
Pr(k >= 1)=df l * /D  
I I : 
1 100 1000 10030 100000 
Document  Froquoncy (all} 
The plot (above) is similar to the plot in section 
4.2 which showed that adapted probabilities 
(labeled h) are larger and less dependent on 
frequency than the prior (labeled p). So too, the 
plot (above) shows that the second and third men- 
tions of a word (labeled 2 and 3, respectively) are 
larger and less dependent on frequency than the 
first mention (labeled 1). The plot in section 4.2 
used method 1 whereas the plot (above) uses 
method 2. Both plots use the He  smoothing, so 
there is only one point per bin (df value), rather 
than one per word. 
6. Neighborhoods (Near) 
Florian and Yarowsky's example, "It is at least 
on the Serb side a real setback to the x," provides 
a nice motivation for neighborhoods. Suppose 
the context (history) mentions a number of words 
related to a peace process, but doesn't mention 
the word "peace." Intuitively, there should still 
be some adaptation. That is, the probability of 
"peace" should go up quite a bit (positive adap- 
tation), and the probability of many other words 
such as "piece" should go down a little (negative 
adaptation). 
We start by partitioniug the vocabulary into three 
exhaustive and mutually exclusive sets: hist, near 
and other (abbreviations for history, neighbor- 
hood and otherwise, respectively). The first set, 
hist, contains the words that appear in the first 
half of the document, as before. Other is a 
catchall for the words that are in neither of the 
first two sets. 
The interesting set is near. It is generated by 
query expansion. The history is treated as a 
query in an information retrieval document- 
ranking engine. (We implemented our own 
ranking engine using simple IDF weighting.) 
The neighborhood is the set of words that appear 
in the k= 10 or k = 100 top documents returned by 
the retrieval engine. To ensure that the three sets 
partition the vocabulary, we exclude the history 
fiom the neighborhood: 
near = words in query expansion of hist - hist 
The adaptation probabilities are estimated using a 
contingency table like before, but we now have a 
three-way partition (hist, near and other) of the 
vocabulary instead of the two-way partition, as il- 
lustrated below. 
Documents containing 
I test 
histo O, a =2125 
c = 1963 
"peace" in 1991 AP 
b =2t60 
d =74573 
hist 
i lea l "  
other 
test 
a =2125 b =2160 
e =1479 f=22516 
g =484 h =52057 
In estilnating adaptation probabilities, we con- 
tinue to use a, b, c and d as before, but four new 
variables are introduced: e, f, g and h, where 
c=e+g andd=f+h.  
184 
Pr(wc test) =(a +c) /D 
p; (  w c tes; l wc  hist ) =al( a + b ) 
l ' r (we test lwc near) =e/(e +f )  
l 'r( w E test \[ w c other) =g/ (g  + h) 
prior 
hist 
neat 
other 
The table below shows that "Kennedy" adapts 
more than "except" and that "peace" adapts 
more than "piece." That is, "Kennedy" has a 
larger spread than "except" between tile history 
and tile otherwise case. 
.l)Jior hist near other src w 
0.026 0.40 0.022 0.0050 AP91 Kennedy 
0.020 0.32 0.025 0.0038 AP93 
0.026 0.05 0.018 0.0122 AP91 except 
0.019 0.05 0.014 0.0081 AP93 
0.077 0.50 0.062 0.0092 AP91 peace 
0.074 0.49 0.066 0.0069 AP93 
0.015 0.10 0.014 0.0066 AP91 piccc 
0.013 0.08 0.015 0.0046 AP93 
When (.\]/' is small (d/'< 100), I\]O smoothing is 
used to group words into bins by {!/\] Adaptation 
prol)abilities are computed for each bill, rather 
than for each word. Since these probabilities are 
implicitly conditional on ,qJ; they have ah'eady 
been weighted by (!fin some sense, and therefore, 
it is unnecessary to introduce an additional 
explicit weighting scheme based on (!/'or a simple 
transl'orm thereof such as II)1:. 
The experiments below split tile neighborhood 
into four chisses, ranging fronl belier nei.g, hbors 
to wmwe neighbotw, del)ending oil expansion 
frequency, e/\] el'(1) is a number between 1 and k, 
indicating how many of the k top scoring doc- 
uments contain I. (Better neighbors appear in 
more of the top scoring documents, and worse 
neighbors appear in fewer.) All the neighborhood 
classes fall between hist and other, with better 
neighbors adapting tllore than ~,OlSe neighbors. 
7. Experimental Results 
Recall that the task is to predict the test portion 
(the second half) of a document given the histoo, 
(the first half). The following table shows a 
selection of words (sorted by the third cohunn) 
from the test portion of one of the test doculnents. 
The table is separated into thirds by horizontal 
lines. The words in the top third receive nmch 
higher scores by the proposed method (S) than by 
a baseline (B). These words are such good 
keywords that one can faMy contidently guess 
what the story is about. Most of these words re- 
ceive a high score because they were mentioned 
in the history portion of the document, but 
"laid-off" receives a high score by the neighl)or- 
hood mechanism. Although "hiid-off" is not 
mentioned explicitly in the history, it is obviously 
closely related to a number of words that were, 
especially "layoffs," but also "notices" and 
"cuts." It is reassuring to see tile neighborhood 
mechanism doing what it was designed to do. 
The middle third shows words whose scores are 
about the same as the baseline. These words tend 
to be function words and other low content words 
that give tts little sense of what the document is 
about. The bottoln third contains words whose 
scores are much lower than the baseline. These 
words tend to be high in content, but misleading. 
The word ' al us , "  for example, might suggest 
that story is about a military conflict. 
S l?, Iog2(S/B) Set Ternl 
0.19 0.00 I 1.06 hist Binder 
0.22 0.00 7.45 hist layoff 
0.06 0.00 5.71 hist notices 
0.36 0.01 5.66 hist 13oeing 
0.02 0.00 5.11 near3 laid-off 
0.25 0.02 3.79 hist cuts 
0.01 0.01 0.18 near3 projects 
0.89 0.81 0.15 hist said 
0.06 0.05 0. l 1 near4 announced 
0.06 0.06 0.09 near4 As 
0.00 0.00 0.09 ncat+l employed 
0.00 0.00 -0.61 other 714 
0.00 0.01 -0.77 other managed 
0.01 0.02 -1.05 near2 additional 
0.00 0.01 - 1.56 other wave 
0.00 0.03 -3.41 other arms 
The proposed score, S, shown in colunln 1, is: 
Pr(wlhist) if wc hist 
Pr (wlnear i )  if wc nearj 
Pr(w\[near 2) if wc near a 
Prs (w)= Pr(winear 3) if w6near  3 
l 'r(wJnear4) if we near4 
Pr(wiother) otherwise 
where near I through near 4 are four neighbor- 
l\]oods (k=100). Words in near4 are the best 
neighbors (e\[>10) and words in near I are the 
worst neighbors (e/'= 1). Tile baseline, B, shown 
in column 2, is: Prl~(w)=df/D. Colun\]i\] 3 con\]- 
pares the first two cohnnns. 
We applied this procedure to a year of the AP 
news and found a sizable gain in information on 
185 
average: 0.75 bits per word type per doculnent. 
In addition, there were many more big winners 
(20% of the documents gained 1 bit/type) than 
big losers (0% lost 1 bit/type). The largest 
winners include lists of major cities and their 
temperatures, lists of major currencies and their 
prices, and lists of commodities and their prices. 
Neighborhoods are quite successful in guessing 
the second half of such lists. 
On the other hand, there were a few big losers, 
e.g., articles that summarize the major stories of 
the clay, week and year. The second half of a 
summary article is almost never about the same 
subject its the first half. There were also a few 
end-of-document delimiters that were garbled in 
translnission causing two different documents to 
be treated as if they were one. These garbled 
documents tended to cause trouble for the 
proposed method; in such cases, the history 
comes fi'om one document and the test comes 
from another. 
In general, the proposed adaptation method per- 
formed well when the history is helpful for pre- 
dicting the test portion of the document, and it 
performed poorly when the history is misleading. 
This suggests that we ought to measure topic 
shifts using methods uggested by Hearst (1994) 
and Florian & Yarowsky (1999). We should not 
use the history when we believe that there has 
been a major topic shift. 
8. Conclusions 
Adaptive language models were introduced to 
account for repetition. It is well known that the 
second instance of a word (or ngram) is nmch 
more likely than the first. But what we find 
surprising is just how large the effect is. The 
chance of two Noriegas is closer to p/2 than p 2. 
in addition to the magnitude of adaptation, we 
were also surprised by the shape: while the first 
instance of a word depends very strongly on 
frequency, the second does not. Adaptation de- 
pends more on content than flequency; adaptation 
is stronger for content words such as proper 
nouns, technical terminology and good keywords 
for information retrieval, and weaker for functioll 
words, cliches and first nalnes. 
The shape and magnitude of adaptation has im- 
plications for psycholinguistics, information 
retrieval and language modeling. Psycholinguis- 
tics has tended to equate word frequency with 
content, but our results suggest hat two words 
with similar frequency (e.g., "Kennedy" and 
"except") can be distinguished on the basis of 
their adaptation. Information retrieval has tended 
to use frequency in a similar way, weighting 
terms by IDF (inverse document frequency), with 
little attention paid to adaptation. We propose a
term weighting method that makes use of adapta- 
tion (burstiness) and expansion frequency in a 
related paper (Umelnura nd Church, submitted). 
Two estimation methods were introduced to 
demonstrate ile magnitude and shape of adapta- 
tion. Both methods produce similar results. 
? Pr(+ adapt I ) = Pr(test\] hist) 
? Pr(+adapt2)=Pr(k>2\]k>_l ) 
Neighborhoods were then introduced for words 
such as "laid-off" that were not in the history but 
were close ("laid-off" is related to "layoff," 
which was in the history). Neighborhoods were 
defined in terms of query expansion. The history 
is treated as a query in an information retriewd 
document-ranking system. Words in the k top- 
ranking documents (but not in the history) are 
called neighbors. Neighbors adapt more dmn 
other terms, but not as much as words that actual- 
ly appeared in the history. Better neighbors 
(larger et) adapt more than worse neighbors 
(slnaller el). 
References 
Church, K. and Gale, W. (1995) "Poisson 
Mixtures," Journal of Natural Language Engi- 
neering, 1:2, pp. 163-190. 
Floriau, P,. and Yarowsky, D. (1999) "l)ynamic 
Nonlocal Language Modeling via Hierarchical 
Topic-Based Adaptation," ACL, pp. 167-174. 
Francis, W., and Kucera, H. (1982) Frequency 
Analysis of English Usage, Houghton Mifflin 
Colnpany, Boston, MA. 
Hearst, M. (1994) Context and Structure in 
Automated Full-Text Information Access, PhD 
Thesis, Berkeley, available via www.sims.ber- 
keley.edu/~hearst. 
Jelinek, F. (1997) Statistical Methods.for Speech 
Recognition, MIT Press, Cambridge, MA, USA. 
Umemura, K. and Church, K. (submitted) 
"Empirical Term Weighting: A Framework for 
Studying Limits, Stop Lists, Burstiness and 
Query Expansion. 
186 
Using Suffix Arrays to Compute Term 
Frequency and Document Frequency for 
All Substrings in a Corpus 
Mikio Yamamoto* 
University of Tsukuba 
Kenneth W. Churcht 
AT&T Labs--Research 
Bigrams and trigrams are commonly used in statistical natural anguage processing; this paper 
will describe techniques for working with much longer n-grams. Suffix arrays (Manber and My- 
ers 1990) were /irst introduced to compute the frequency and location of a substring (n-gram) 
in a sequence (corpus) of length N. To compute frequencies over all N(N + 1)/2 substrings in a 
corpus, the substrings are grouped into a manageable number of equivalence lasses. In this way, 
a prohibitive computation over substrings is reduced to a manageable computation over classes. 
This paper presents both the algorithms and the code that were used to compute term frequency 
(tf) and document frequency (dr)for all n-grams in two large corpora, an English corpus of 
50 million words of Wall Street Journal and a Japanese corpus of 216 million characters of 
Mainichi Shimbun. 
The second half of the paper uses these frequencies to find "interesting" substrings. Lexi- 
cographers have been interested in n-grams with high mutual information (MI) where the joint 
term frequency ishigher than what would be expected by chance, assuming that the parts of the 
n-gram combine independently. Residual inverse document frequency (RIDF) compares docu- 
ment frequency to another model of chance where terms with a particular term frequency are 
distributed randomly throughout the collection. MI tends to pick out phrases with noncompo- 
sitional semantics (which often violate the independence assumption) whereas RIDF tends to 
highlight echnical terminology, names, and good keywords for information retrieval (which tend 
to exhibit nonrandom distributions over documents). The combination of both MI and RIDF is 
better than either by itself in a Japanese word extraction task. 
1. Introduction 
We will use suffix arrays (Manber and Myers 1990) to compute a number of type/token 
statistics of interest, including term frequency and document frequency, for all n-grams 
in large corpora. Type/token statistics model the corpus as a sequence of N tokens 
(characters, words, terms, n-grams, etc.) drawn from a vocabulary of V types. Different 
tokenizing rules will be used for different corpora and for different applications. In 
this work, the English text is tokenized into a sequence of English words delimited by 
white space and the Japanese text is tokenized into a sequence of Japanese characters 
(typically one or two bytes each). 
Term frequency (tf) is the standard notion of frequency in corpus-based natural 
language processing (NLP); it counts the number of times that a type (term/word/n- 
gram) appears in a corpus. Document frequency (df) is borrowed for the information 
* Institute of Information Sciences and Electronics, 1-1-1 Tennodai, Tsukuba 305-8573, Japan 
t 180 Park Avenue, Florham Park, NJ 07932 
Computational Linguistics Volume 27, Number 1 
retrieval iterature (Sparck Jones 1972); it counts the number of documents that con- 
tain a type at least once. Term frequency is an integer between 0 and N; document 
frequency is an integer between 0 and D, the number of documents in the corpus. 
The statistics, tf and df, and functions of these statistics uch as mutual information 
(MI) and inverse document frequency (IDF), are usually computed over short n-grams 
such as unigrams, bigrams, and trigrams (substrings of 1-3 tokens) (Charniak 1993; 
Jelinek 1997). This paper will show how to work with much longer n-grams, including 
million-grams and even billion-grams. 
In corpus-based NLP, term frequencies are often converted into probabilities, using 
the maximum likelihood estimator (MLE), the Good-Turing method (Katz 1987), or 
Deleted Interpolation (Jelinek 1997, Chapter 15). These probabilities are used in noisy 
channel applications uch as speech recognition to distinguish more likely sequences 
from less likely sequences, reducing the search space (perplexity) for the acoustic 
recognizer. In information retrieval, document frequencies are converted into inverse 
document frequency (IDF), which plays an important role in term weighting (Sparck 
Jones 1972). 
dr(t) 
IDF(t)  = - log  2 D 
IDF(t)  can be interpreted as the number of bits of information the system is given if it 
is told that the document in question contains the term t. Rare terms contribute more 
bits than common terms. 
Mutual information (MI) and residual IDF (RIDF) (Church and Gale 1995) both 
compare tf and df to what would be expected by chance, using two different notions of 
chance. MI compares the term frequency of an n-gram to what would be expected if the 
parts combined independently, whereas RIDF combines the document frequency of a 
term to what would be expected if a term with a given term frequency were randomly 
distributed throughout the collection. MI tends to pick out phrases with noncompo- 
sitional semantics (which often violate the independence assumption) whereas RIDF 
tends to highlight technical terminology, names, and good keywords for information 
retrieval (which tend to exhibit nonrandom distributions over documents). 
Assuming a random distribution of a term (Poisson model), the probability p~(k) 
that a document will have exactly k instances of the term is: 
e-O Ok 
pa(k) = rc(O,k) - k! ' 
where 0 = np, n is the average length of a document, and p is the occurrence probability 
of the term. That is, 
Nt f  tf 8--  
DN D" 
Residual IDF is defined as the following formula. 
Residual IDF = observed IDF - predicted IDF 
df log(1 = - log  + - 
df ---- - l og~ +log(1 - e-~) 
The rest of the paper is divided into two sections. Section 2 describes the algorithms 
and the code that were used to compute term frequencies and document frequencies 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
for all substrings in two large corpora, an English corpus of 50 million words of the 
Wall Street Journal, and a Japanese corpus of 216 million characters of the Mainichi 
Shimbun. 
Section 3 uses these frequencies tofind "interesting" substrings, where what counts 
as "interesting" depends on the application. MI finds phrases of interest o lexicog- 
raphy, general vocabulary whose distribution is far from random combination of the 
parts, whereas RIDF picks out technical terminology, names, and keywords that are 
useful for information retrieval, whose distribution over documents i far from uni- 
form or Poisson. These observations may be particularly useful for a Japanese word 
extraction task. Sequences of characters that are high in both MI and RIDF are more 
likely to be words than sequences that are high in just one, which are more likely than 
sequences that are high in neither. 
2. Computing tf and df for All Substrings 
2.1 Suffix Arrays 
This section will introduce an algorithm based on suffix arrays for computing tf and 
df and many functions of these quantities for all substrings in a corpus in O(NlogN) 
time, even though there are N(N + 1)/2 such substrings in a corpus of size N. The 
algorithm groups the N(N + 1)/2 substrings into at most 2N - 1 equivalence classes. 
By grouping substrings in this way, many of the statistics of interest can be computed 
over the relatively small number of classes, which is manageable, rather than over the 
quadratic number of substrings, which would be prohibitive. 
The suffix array data structure (Manber and Myers 1990) was introduced as a 
database indexing technique. Suffix arrays can be viewed as a compact representa- 
tion of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been 
extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive 
introduction to suffix trees. Hui (1992) shows how to compute df for all substrings 
using generalized suffix trees. The major advantage of suffix arrays over suffix trees 
is space. The space requirements for suffix trees (but not for suffix arrays) grow with 
alphabet size: O(N\]~\]) space, where \]~\] is the alphabet size. The dependency on al- 
phabet size is a serious issue for Japanese. Manber and Myers (1990) reported that 
suffix arrays are an order of magnitude more efficient in space than suffix trees, even 
in the case of relatively small alphabet size (IGI = 96). The advantages of suffix arrays 
over suffix trees become much more significant for larger alphabets such as Japanese 
characters (and English words). 
The suffix array data structure makes it convenient to compute the frequency 
and location of a substring (n-gram) in a long sequence (corpus). The early work was 
motivated by biological applications such as matching of DNA sequences. Suffix arrays 
are closely related to PAT arrays (Gonnet, Baeza-Yates, and Snider 1992), which were 
motivated in part by a project at the University of Waterloo to distribute the Oxford 
English Dictionary with indexes on CD-ROM. PAT arrays have also been motivated 
by applications in information retrieval. A similar data structure to suffix arrays was 
proposed by Nagao and Mori (1994) for processing Japanese text. 
The alphabet sizes vary considerably in each of these cases. DNA has a relatively 
small alphabet of just 4 characters, whereas Japanese has a relatively large alphabet of 
more than 5,000 characters. The methods uch as suffix arrays and PAT arrays scale 
naturally over alphabet size. In the experimental section (Section 3) using the Wall Street 
Journal corpus, the suffix array is applied to a large corpus of English text, where the al- 
phabet is assumed to be the set of all English words, an unbounded set. It is sometimes 
assumed that larger alphabets are more challenging than smaller ones, but ironically, 
Computational Linguistics Volume 27, Number 1 
it can be just the reverse because there is often an inverse relationship between the size 
of the alphabet and the length of meaningful or interesting substrings. For expository 
convenience, this section will use the letters of the alphabet, a-z, to denote tokens. 
This section starts by reviewing the construction of suffix arrays and how they 
have been used to compute the frequency and locations of a substring in a sequence. 
We will then show how these methods can be applied to find not only the frequency 
of a particular substring but also the frequency of all substrings. Finally, the methods 
are generalized to compute document frequencies as well as term frequencies. 
A suffix array, s, is an array of all N suffixes, sorted alphabetically. A suffix, s\[i\], 
also known as a semi-infinite string, is a string that starts at position i in the corpus 
and continues to the end of the corpus. In practical implementations, it is typically 
denoted by a four-byte integer, i. In this way, a small (constant) amount of space is 
used to represent a very long substring, which one might have thought would require 
N space. 
A substring, sub(i,j), is a prefix of a suffix. That is, sub(i,j), is the first j characters 
of the suffix s\[i\]. The corpus contains N(N + 1)/2 substrings. 
The algorithm, su f f ix_ar ray ,  presented below takes a corpus and its length N as 
input, and outputs the suffix array, s. 
su f f i x_ar ray  *-- function(corpus, N){ 
Initialize s to be a vector of integers from 0 to N - 1. 
Let each integer denote a suffix starting at s\[i\] in the corpus. 
Sort s so that the suffixes are in alphabetical order. 
Return s. } 
The C program below implements this algorithm. 
char *corpus ; 
int suffix_compare(int *a, int *b) 
{ return strcmp(corpus+*a, corpus+*b);} 
int *suffix_array(int n){ 
int i, *s = (int *)malloc(n*sizeof(int)); 
for(i=O; i < n; i++) s\[i\] = i; /* initialize */ 
qsort(s, n, sizeof(int), suffix_compare); /* sort */ 
return s;} 
Figures i and 2 illustrate a simple example where the corpus ("to_be_ormot_to_be") 
consists of N = 18 (19 bytes): 13 alphabetic haracters plus 5 spaces (and 1 null 
termination). The C program (above) starts by allocating memory for the suffix array 
(18 integers of 4 bytes each). The suffix array is initialized to the integers from 0 
to 17. Finally, the suffix array is sorted by alphabetical order. The suffix array after 
initialization is shown in Figure 1. The suffix array after sorting is shown in Figure 2. 
As mentioned above, suffix arrays were designed to make it easy to compute the 
frequency (tf) and locations of a substring (n-gram or term) in a sequence (corpus). 
Given a substring or term, t, a binary search is used to find the first and last suffix 
that start with t. Let s\[i\] be the first such suffix and s~\] be the last such suffix. Then 
tf(t) = j - i + 1 and the term is located at positions: {s\[i\], s\[i + 1\] . . . .  ,s~\]}, and only 
these positions. 
Figure 2 shows how this procedure can be used to compute the frequency and 
locations of the term "to_be" in the corpus "to_be_or_not_to_be". As illustrated in the 
figure, s\[i = 16\] is the first suffix to start with the term "to_be" and s~ = 17\] is the last 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Input  corpus :  " to_be_or_not_ to_be"  
Position: 
Characters: 
Initialized 
Suffix Array 
s\[O\] 0 
s\[l\] 1 
s \ [2\ ]  2 
s \ [3\ ]  3 
s\ [13\]  13 
s\[18\]  14 
s \[15\] 15 
s \[16\] 15 
s\ [17\]  17 
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 I t lo l_ lb le l_ lo l r l_ ln lo l t l_ l t lo l_ lb le l  .~" 
: : : : ""{ i i i ! 
' ' ' ! Suffixes denoted by s \[ i \ ]  i i i i i 
-" / ." / to  b~ o~ not  to  b~ I /  / i i i 
.... . j...: o or to j :" ' i i ' '  I ........ " ..,'" I be  or  not  to  be I ." / / / / I " ~  be orNot_ to_be  ...... ,'"'" ...'/" .y ./" / 
Figure 1 
Illustration of a suffix array, s, that has just been initialized and not yet sorted. Each element in 
the suffix array, s\[i\], is an integer denoting a suffix or a semi-infinite string, starting at 
position i in the corpus and extending to the end of the corpus. 
Suf f ix  Ar ray  
s\[0\] 1151 
s \[1\] 2., 
s\[2\] 8 
s\[3\] 5 
s \[4\] 12 
s \[5\] 16 
s \ [6 \ ]  3 
s \[7\] 17 
s \[8\] 4 
s \ [9 \ ]  9 
s \ [10\]  14 
s i l l \ ]  1 
s \ [12 \ ]  6 
s\[13\] 10 
s \[14\]  7 
s \ [15\]  11 
s \[16\] 13 
s\[17\] 0 
Figure 2 
Suf f ixes  denoted  by  s \[ i \] 
_De  
be_or_not_ to_be  
_not to be 
_or_not_ to_be  
_to_be 
be 
be_or_no t_t o_be 
e 
e_or_no t_t o_be 
not_ to_be 
obe  
o be or_not_ to_be 
or_not to be 
ot_to_be 
r_not_ to_be 
t to be 
to_be 
to be or_not_ to_be 
Illustration of the suffix array in Figure 1 after sorting. The integers in s are sorted so that the 
semi-infinite strings are now in alphabetical order. 
Computational Linguistics Volume 27, Number 1 
suffix to start with this term. Consequently, tf("to_be") = 17-16+ 1 = 2. Moreover, the 
term appears at positions("to_be") = {s\[16\], s\[17\] } = {13, 0}, and only these positions. 
Similarly, the substring "to" has the same tf and positions, as do the substrings, "to_" 
and "to_b". Although there may be N(N + 1)/2 ways to pick i and j, it will turn out 
that we need only consider 2N - 1 of them when computing tf for all substrings. 
Nagao and Mori (1994) ran this procedure quite successfully on a large corpus 
of Japanese text. They report that it takes O(NlogN) time, assuming that the sort 
step performs O(N log N) comparisons, and that each comparison takes constant time. 
While these are often reasonable assumptions, we have found that if the corpus con- 
tains long repeated substrings (e.g., duplicated articles), as our English corpus does 
(Paul and Baker 1992), then the sort can consume quadratic time, since each compar- 
ison can take order N time. Like Nagao and Mori (1994), we were also able to apply 
this procedure quite successfully to our Japanese corpus, but for the English corpus, 
after 50 hours of CPU time, we gave up and turned to Manber and Myers's (1990) al- 
gorithm, which took only two hours. 1 Manber and Myers' algorithm uses some clever, 
but difficult to describe, techniques to achieve O(N log N) time, even for a corpus with 
long repeated substrings. For a corpus that would otherwise consume quadratic time, 
the Manber and Myers algorithm is well worth the effort, but otherwise, the procedure 
described above is simpler, and can even be a bit faster. 
The "to_be_or_not_to_be" example used the standard English alphabet (one byte per 
character). As mentioned above, suffix arrays can be generalized in a straightforward 
way to work with larger alphabets such as Japanese (typically two bytes per character). 
In the experimental section (Section 3), we use an open-ended set of English words as 
the alphabet. Each token (English word) is represented as a four-byte pointer into a 
symbol table (dictionary). The corpus "to_be_or_not_to_be", for example, is tokenized 
into six tokens: "to", "be", "or", "not", "to", and "be", where each token is represented 
as a four-byte pointer into a dictionary. 
2.2 Longest Common Prefixes (LCPs) 
Algorithms from the suffix array literature make use of an auxiliary array for storing 
LCPs (longest common prefixes). The lcp array contains N + 1 integers. Each element, 
lcp\[i\], indicates the length of the common prefix between s\[i - 1\] and s\[i\]. We pad the 
lcp vector with zeros (lcp\[O\] = lcp\[N\] = 0) to simplify the discussion. The padding 
avoids the need to test for certain end conditions. 
Figure 3 shows the lcp vector for the suffix array of "to_be_or_not_to_be". For 
example, since s\[10\] and s\[11\] both start with the substring "o_be", lcp\[11\] is set to 4, the 
length of the longest common prefix. Manber and Myers (1990) use the Icp vector in 
their O(P+log N) algorithm for computing the frequency and location of a substring of 
length P in a sequence of length N. They showed that the Icp vector can be computed 
in O(N log N) time. These algorithms are much faster than the obvious traightforward 
implementation when the corpus contains long repeated substrings, though for many 
corpora, the complications required to avoid quadratic behavior are unnecessary. 
2.3 Classes of Substrings 
Thus far we have seen how to compute tf for a single n-gram, but how do we compute 
tf and df for all n-grams? As mentioned above, the N(N + 1)/2 substrings will be 
clustered into a relatively small number of classes, and then the statistics will be 
1 We used Doug McIlroy's implementation, available on the Web at: http://cm.belMabs.com/cm/cs/ 
who/doug/ssort.c. 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
computed over the classes rather than over the substrings, which would be prohibitive. 
The reduction of the computat ion over substrings to a computat ion over classes is 
made possible by four properties. 
Properties 1-2: all substrings in a class have the same statistics (at least 
for the statistics of interest, namely tf and dJ), 
Property 3: the set of all substrings is partit ioned by the classes, and 
Property 4: there are many fewer classes (order N) than substrings 
(order N2). 
Classes are defined in terms of intervals. Let (i,j) be an interval on the suffix array, 
s\[i\], s\[i + 1\] . . . . .  s~\]. Class((i,j)) is the set of substrings that start every suffix within the 
interval and no suffix outside the interval. It follows from this construction that all 
substrings in a class have tf = j - i + 1. 
The set of substrings in a class can be constructed from the lcp vector: 
class((i,j)) = {s\[i\]ml ax(Icp\[i\], lcp~ + 1\]) < m ~ min(lcp\[i + 1\], lcp\[i + 2\] . . . . .  lcp~'\])}, 
where s\[i\]m denotes the first m characters of the suffix s\[i\]. We will refer to lcp\[i\] and 
Icp~+l\] as bound ing  lcps and Icp\[i+l\], lcp\[i+2\] . . . . .  lcp~\] as interior lcps. The equation 
Suffix Array i Suffix denoted by s \[ i \] 
s\[O\] 15 _be; 
s\[l\] 2 _b~_or_not  to be 
s\[2\] 8 _}not to be 
s\[3\] 5 br not to be --\] - -  
s\[4\] 12 :--to_be 
s\[5\] IB ibei 
s\[6\] 3 :he'or_not to be 
s\[7\] 
s \[8\] i e~_or_not to be 
s \[ 9 \] not_ to_be 
s \[I0\] o_b4 l en~=4 
sill\] ~ or not to be 
s\[12\] qr_not to be 
s\[13\] q't to be 
s\[14\] r_not to be 
s\[15\] tJ to be 
s \[ 16 \] rio_be, 
s \[ 17 \] to_be\[~or_not to be 
The doted lines denote lcp's. 
Figure 3 
Lcp vector 
icp\[O O" 
icp \[i 3 
icp \[2 I 
icp \[3 I 
Icp \[4 I 
i cp \ [5  0 
icp\[6\] 2 
icp\[7\] 0 
icp\[8\] I 
Icp\[9\] 0 
icp\[lO\] 0 
l cp  \ [12 \ ]  1 
Icp\[13\] 1 
icp \[14\] 0 
icp\[15\] 0 
icp\[16\] I 
icp\[17\] 5 
icp\[18\] O- 
-- always 0 
always 0 
The longest common prefix is a vector of N + 1 integers, lcp\[i\] denotes the length of the 
common prefix between the suffix s\[i - 1\] and the suffix s\[i\]. Thus, for example, s\[10\] and s\[11\] 
share a common prefix of four characters, and therefore lcp\[11\] = 4. The common prefix is 
highlighted by a dotted line in the suffix array. The suffix array is the same as in the previous 
figure. 
7 
Computational Linguistics Volume 27, Number 1 
Vertical lines denote lcps. Gray area denotes Boundaries 
endpoints of substrings in class(<10,11>). 
S \[9\]lli' n o t to be ~'~f7 ? f<10 ' l l>  
I,Ill 0 __ b e - ~ X<";flO,11> . . . . . . . . . .  / s \[ 10 \] 
s\[ll\]\]~o\]_ b el_or_not_to_be ~1 4_ I 
s\[12\]I lo:;Ir n~t  to be ........ <10,13> 
s\[14\]s\[13\]\]i!I~to ~-t~_be e ............ ~ 
Bounding lcps, LBL, SIL, Interior lcp of <10,11> 
LCP-del imited Class 
interval 
<10,11> 
<10,13> 
<9,9> 
<10,10> 
<11,11> 
<12,12> 
<13,13> 
<14,14> 
LBL SIL 
Figure 4 
{"O_", "o_b", "o_be"} 1 4 
{"o"} 0 1 
{"n", "no", "not", ...} 0 infinity 
{} 4 infinity 
{"o_be_","o_beo",. . .} 4 infinity 
{"or", "or_", "or_n", ...} 1 infinity 
{"ot", "ot_", "oCt", ...} 1 infinity 
"r", "r " " " _ , r_n,. . .} 0 infinity 
tf 
Six suffixes are copied from Figure 3, s\[9\]-s\[14\], along with eight of their lcp-delimited 
intervals. Two of the lcp-delimited intervals are nontrivial (tf > 1), and six are trivial (tf = 1). 
Intervals are associated with classes, sets of substrings. These substrings start every suffix 
within the interval and no suffix outside the interval. All of the substrings within a class have 
the same term frequency (and document frequency). 
above can be rewritten as 
class((i, jl ) = {s\[i\]mlLBL((i, j l) < m <_ SIL((i,  j l )},  
where LBL (longest bounding lcp) is 
LBL(( i , j ) )  = max(lcp\[i\], lcp~ + 1\]), 
and SIL (shortest interior lcp) is 
sIn(( i , j ) )  = min(lcp\[ i  + 1\], Icp\[i + 2\] . . . . .  lcp~\]). 
By construction, the class will be empty unless there is some room for m between 
the LBL and SIL. We say that an interval is lcp-delimited when this room exists 
(that is, LBL  < SIL). Except for trivial intervals where tf = 1 (see below), classes are 
nonempty iff the interval is lcp-delimited. Moreover, the number of substrings in a 
nontrivial class depends on the amount of room between the LBL and the SIL. That 
is, \[class((i,j))l = SIL(( i , j ))  - LBL(( i , j ) ) .  
Figure 4 shows eight examples of lcp-delirnited intervals. The top part of the figure 
highlights the interval (10,11) with dashed horizontal lines. Solid vertical ines denote 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
bounding lcps, and thin vertical ines denote interior lcps (there is only one interior lcp 
in this case). The interval {10, 11 / is lcp-delimited because the bounding lcps, Icp\[lO\] --= 0 
and lcp\[12\] = 1, are smaller than the interior lcp, lcp\[11\] = 4. That is, the LBL (= 1) is 
less than the SIL (= 4). Thus there is room for m between the LBL of s\[10\]m and the SIL 
of s\[10\]m. The endpoints m between LBL and SIL are highlighted in gray. The class is 
nonempty. Its size depends on the width of the gray area: class((lO, 11)) = {s\[10\]mll < 
m _< 4} = {"o_", "o_b", "o_be"}. These substrings have the same tf: tf = j - i + 1 = 
11 - 10 + 1 = 2. Each of these substrings occurs exactly twice in the corpus. 
Every substring in the class starts every suffix in the interval (10,11), and no suffix 
outside (10,11). In particular, the substring "o" is excluded from the class, because it 
is shared by suffixes outside the interval, namely s\[12\] and s\[13\]. The longer substring, 
"o_be_", is excluded from the class because it is not shared by s\[10\], a suffix within 
the interval. 
We call an interval trivial if the interval starts and ends at the same place: (i, i). 
The remaining six intervals mentioned in Figure 4 are trivial intervals. We call the 
class of a trivial interval a trivial class. As in the nontrivial case, the class contains 
all (and only) the substrings that start every suffix within the interval and no suf- 
fix outside the interval. We can express the class of a trivial interval, class((i, i)), as 
{s\[i\]m\]LBL < m <_ SIL}. The trivial case is the same as the nontrivial case, except hat 
the SIL of a trivial interval is defined to be infinite. As a result, trivial classes are 
usually quite large, because they contain all prefixes of s\[i\] that are longer than the 
LBL. They cover all (and only) the substrings with tf = 1, typically the bulk of the 
N(N+ 1)/2 substrings in a corpus. The trivial class of the interval (11,11), for example, 
contains 13 substrings: "o_be_", "o_be_o", "o_be_or", and so on. Of course, there are 
some exceptions: the trivial class, class((lO, 10)), in Figure 4, for example, is very small 
(= empty set). 
Not every interval is lcp-delimited. The interval, I l l ,  12), for example, is not lcp- 
delimited because there is no room for m of s\[ll\]m between the LBL (= 4) and the 
SIL (= 1). When the interval is not lcp-delimited, the class is empty. There are no 
substrings tarting all the suffixes within the interval (11,12), and not starting any 
suffix outside the interval. 
It is possible for lcp-delimited intervals to be nested, as in the case of (10,11) and 
(10, 13). We say that one interval (i,j) is nested within another (u,v) if i < u < v < j 
(and (i,j) ~ (u, v)). Nested intervals have distinct SILs and disjoint classes. (Two classes 
are disjoint if the corresponding sets of substrings are disjoint.) 2The substrings in the 
class of the nested interval, (u, v), are longer than the substrings in the class of the 
outer interval, (i,j). 
Although it is possible for lcp-delimited intervals to be nested, it is not possible for 
lcp-delimited intervals to overlap. We say that one nontrivial interval Ca, b) overlaps 
another nontrivial interval (c, d) if a < c < b < d. If two intervals overlap, then at 
least one of the intervals is not lcp-delimited and has an empty class. If an interval 
(a, b) is lcp-delimited, an overlapped interval (c, d) is not lcp-delimited. Because a 
bounding lcp of /a, b) must be within (c, d) and an interior lcp of (a, b) must be a 
bounding lcp of (c,d), SIL((c,d)) <_ LBL((a,b)) <_ SIL((a,b)) <_ LBL((c,d)). That is, the 
overlapped interval (c, d) is not lcp-delimited. The fact that lcp-delimited intervals are 
nested and do not overlap will turn out to be convenient for enumerating lcp-delimited 
intervals. 
2 Because (u, v I is lcp-delimited, there must be a bounding lcp of lu, v I that is smaller than any lcp 
within (u, v). This bounding lcp must be within (i,j), and as a result, class((i,j)) and class((u, v)) 
must be disjoint. 
Computational Linguistics Volume 27, Number 1 
2.4 Four Properties 
As mentioned above, classes are constructed so that it is practical to reduce the com- 
putation of various statistics over substrings to a computation over classes. This sub- 
section will discuss four properties of classes that help make this reduction feasible. 
The first two properties are convenient because they allow us to associate tf and 
df with classes rather than with substrings. The substrings in a class all have the same 
tf value (property 1) and the same df value (property 2). That is, if Sl and s2 are two 
substrings in class((i,j)) then 
Property 1: tf(sl) = t f ( s2)  = j - i + 1 
Property 2: dr(s1) = rid(s2 ). 
Both of these properties follow straightforwardly from the construction of intervals. 
The value of tf is a simple function of the endpoints; the calculation of df is more 
complicated and will be discussed in Section 2.6. While tf and df treat each member 
of a class as equivalent, not all statistics do. Mutual information (MI) is an important 
counter example; in most cases, MI(sl) ~ MI(s2). 
The third property is convenient because it allows us to iterate over classes rather 
than substrings, without worrying about missing any of the substrings. 
Property 3: The classes partition the set of all substrings. 
There are two parts to this argument: every substring belongs to at most one class 
(property 3a), and every substring belongs to at least one class (property 3b). 
Demonstration of property 3a (proof by contradiction): Suppose there is a sub- 
string, s, that is a member of two distinct classes: class((i,j}) and class((u,v)). There 
are three possibilities: one interval precedes the other, they are properly nested, or 
they overlap. In all three cases, s cannot be a member of both classes. If one interval 
precedes the other, then there must be a bounding lcp between the two intervals which 
is shorter than s. And therefore, s cannot be in both classes. The nesting case was men- 
tioned previously where it was noted that nested intervals have disjoint classes. The 
overlapping case was also discussed previously where it was noted that two overlap- 
ping intervals cannot both be lcp-delimited, and therefore at least one of the classes 
would have to be empty. 
Demonstration of property 3b (constructive argument): Let s be an arbitrary sub- 
string in the corpus. There will be at least one suffix in the suffix array that starts with 
s. Let i be the first such suffix and let j be the last such suffix. By construction, the 
interval (i,j} is lcp-delimited (LBL((i,j}) < Is I and SIL((i,j}) >__ Is\[), and therefore, s is 
an element of class((i,j}). 
Finally, as mentioned above, computing over classes is much more efficient han 
computing over the substrings themselves because there are many fewer classes (at 
most 2N - 1) than substrings (N(N + 1)/2). 
Property 4: There are at most N nonempty classes with tf = 1 and at 
most N - 1 nonempty classes with tf > 1. 
The first clause is relatively straightforward. There are N trivial intervals (i, i/. 
These are all and only the intervals with tf = 1. By construction, these intervals are 
lcp-delimited, though it is possible that a few of the classes could be empty. 
To argue the second clause, we make use of a uniqueness property: an lcp- 
delimited interval (i, jl can be uniquely determined by an SIL and a representative ele- 
10 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
ment k, where i < k < j. For convenience, we will choose k such that SIL(<i,j>) = Icp\[k\], 
but we could have uniquely determined the lcp-delimited interval by choosing any k 
such that i < k G j. 
The uniqueness property can be demonstrated using a proof by contradiction. 
Suppose there were two distinct lcp-delimited intervals, <i,j> and <u, v>, with the same 
representative k, where i < k < j and u < k G v. Since they share a common repre- 
sentative, k, one interval must be nested inside the other. But nested intervals have 
disjoint classes and different SILs. 
Given this uniqueness property, we can determine the N-  1 upper bound on 
the number of lcp-delimited intervals by considering the N - 1 elements in the Icp 
vector. Each of these elements, lcp\[k\], has the opportunity to become the SIL of an 
lcp-delimited interval <i,j> with a representative k. Thus there could be as many as 
N-  1 lcp-delimited intervals (though there could be fewer if some of the opportunities 
don't work out). Moreover, there cannot be any more intervals with tf > 1, because if 
there were one, its SIL should have been in the Icp vector. (Note that this lcp counting 
argument does not count trivial intervals because their S1Ls \[= infinity\] are not in the 
lcp vector; the Icp vector contains integers less than N.) 
From property 4, it follows that there are at most N distinct values of RIDE The 
N trivial intervals <i, i> have just one RIDF value since tf = df = 1 for these intervals. 
The other N - 1 intervals could have as many as another N - 1 RIDF values. Similar 
arguments hold for many other statistics that make use of tf and dr, and treat all 
members of a class as equivalent. 
In summary, the four properties taken collectively make it practical to compute 
tf, dr, and RIDF over a relatively small number of classes; it would be prohibitively 
expensive to compute these quantities directly over the N(N + 1)/2 substrings. 
2.5 Computing All Classes Using Suffix Arrays 
This subsection describes a single-pass procedure, print_LDIs, for computing t / fo r  
all LDIs (lcp-delimited intervals). Since lcp-delimited intervals are properly nested, the 
procedure is based on a push-down stack. The procedure outputs four quantities for 
each lcp-delimited interval, <i,j>. The four quantities are the two endpoints (i and j), the 
term frequency (tf) and a representative (k), such that i < k _< j and lcp\[k\] : SIL(<i,j>). 
This procedure will be described twice. The first implementation is expressed in a 
recursive form; the second implementation avoids recursion by implementing the stack 
explicitly. 
The recursive implementation is presented first, because it is simpler. The function 
print_LDIs is initially called with pr in t  LDIs (0,0), which will cause the function to 
be called once for each value of k between 0 and N - 1. k is a representative in the 
range: i < k < j, where i and j are the endpoints of an interval. For each of the 
N values of k, a trivial LDI is reported at <k, k>. In addition, there could be up to 
N-  1 nontrivial intervals, where k is the representative and lcp\[k\] is the SIL. Recall 
that lcp-delimited intervals are uniquely determined by a representative k such that 
i < k < j where SIL(<i,j)) = Icp\[k\]. Not all of these candidates will produce LDIs. 
The recursion searches for j's such that LBL((i,j>) < SIL(<i,j>), but reports intervals at 
(i,j> only when the inequality is a strict inequality, that is, LBL(<i,j>) < SIL(<i,j>). The 
program stack keeps track of the left and right edges of these intervals. While Icp\[k\] 
is monotonically increasing, the left edge is remembered on the stack, as pr?nt LDIs 
is called recursively. The recursion unwinds as lcp\[j\] < lcp\[k\]. Figure 5 illustrates the 
function calls for computing the nontrivial lcp-delimited intervals in Figure 4. C code 
is provided in Appendix A. 
11 
Computational Linguistics Volume 27, Number 1 
s \ [9  
s\[lO 
s \ [ l l  
s \ [12  
s \ [13 \ ]  o" t  ..... t . . .  
s \ [14 \ ]  r _ n o . . . 
Vert ical  l ines denote  lcps. 
~.print LDIs( i=9,k=10) 
.. . . . . . . .  ~ \[print CDIs( i=10,k=l l )  
\] n o . -~-" j _  . . . .  ,-'" | pr int  <10,11>, rep=11, tf=2" 
\] "5~"" b e . , ' " ' "  \[return(j=11) 
.... , "print_LDIs( i=10,k=12) 
\] o _ b e. . . . . . . . .  -, , \ [pr int_LDIs ( i=12,k=13)  
\] o "?'"'---_.n ... . . .  -:- | pr int  nothing, 
| (<12,13> isn t lcp-de l imi ted)  
Lreturn(j=13) 
pr int  "<10,13>, rep=13, tf=4" 
return( j -13)  
Figure 5 
Trace of function calls for computing the nontrivial cp-delimited intervals in Figure 4. In this 
trace, trivial intervals are omitted. Print_LDIs(i = x,k = y) represents a function call with 
arguments, i and k. Indentation represents he nest of recursive calls. Print_LDIs(i,k) searches 
the right edge, j, of the non-trivial cp-delimited interval, <i,j>, whose SIL is lcp\[k\]. Each 
representative, k, value is given to the function print_LDIs just once (dotted arcs). 
pr int_LDIs +- function(i, k) { 
j~k .  
Output a trivial lcp-delimited interval <k, k> with tf  = 1. 
While lcp\[k\] <_ lcp~ + 1\] and j + 1 < N, do j *-- print_LDIs(k, j + 1). 
Output an interval <i,j> with tf  = j - i + 1 and rep = k, if it is lcp-delimited. 
Return j. } 
The second implementation (below) introduces its own explicit stack, a complica- 
tion that turns out to be important in practice, especially for large corpora. C code is 
provided in Appendix B. 
pr int_LDIs_stack *-- function(N){ 
stack_i ~-- an integer array for the stack of the left edges, i. 
stack_k ~ an integer array for the stack of the representatives, k. 
stack_i\[O\] *-- O. 
stack_k\[O\] *-- O. 
sp *--- 1 (a stack pointer). 
For j  *-- 0,1,2 . . . . .  N -  1 do 
Output an lcp-delimited interval (j,j> with tf  = 1. 
While Icp~ + 1\] < lcp\[stackd~\[sp - 1\]\] do 
Output an interval <i,j> with tf  = j - i + 1, if it is lcp-delimited. 
sp* - - - sp -1 .  
stack_i\[sp\] ~-- stack_k\[sp - 1\]. 
stack_k \[sp\] *--- j + 1. 
sp* - - - sp+l .}  
12 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Suffix Array 
s\[O\] 2 I 
s\[l\] 15l 
s\[2\] !2_\] 
s\[3\] b I 
s \[4\] 8 1 
s \ [5 \ ]  181 
- - - - - -4  
s \ [6 \ ]  3 I 
s \ [7 \ ]  161 
s \ [8 \ ]  4 i 
s \ [9 \ ]  17\[ 
s \[i0\] 9 I 
s\[ll\] 
s\[12\] 141 
s\[13\] 6 1 
s\[14\] 1--51 
s\[15\] 
s\[16\] 111 
s\[iv\] o--I 
s\[18\] 1~1 
Suffixes denoted 
by s \[i\] 
_be$ 
_be$ 
_to_be$ 
$ 
$ 
$ 
be$ 
be$ 
e$ 
e$ 
not_to_be$ 
o_be$ 
o_be$ 
orS 
ot to be$ 
r$ 
t_to_be$ 
to_be$ 
to_be$ 
Document id's 
icp\[i\] ofs\[i\] 
0 0 
4 2 
I 2 
0 0 
I I 
I 2 
0 0 
3 2 
0 0 
2 2 
0 2 
0 0 
5 2 
1 I 
I 2 
0 I 
0 2 
I 0 
6 2 
0 
I nput  documents :  dO = "to_be$" - -  
d l  = "orS" 
d2  = "not_to_be$"  
Corpus  = dO + d l  + d2  = " to_be$or$not_ to_be$"  
Resulting non-trivial lcp-delimited intervals: 
('rep' means a representative, k.) 
(0,1>, rep= 1, tf=2, df=2 
(0, 2>, rep= 2, tf=3, df=2 
(3, 5), rep= 4, tf=3, df=3 
(6, 7>, rep= 7, tf=2, df=2 
<8, 9/, rep= 9, tf=2, df=2 
(11,12>, rep=12, tf=2, df=2 
<11,14}, rep=13, tf=4, df=3 
(17,18>, rep=18, tf=2, df=2 
<16,18), rep=17, tf=3, dr=2 
Figure 6 
A suffix array for a corpus consisting of three documents. The special character $ denotes the 
end of a document. The procedure outputs a sequence of intervals with their term frequencies 
and document frequencies. These results are also presented for the nontrivial intervals. 
2.6 Computing df for All Classes 
Thus far we have seen how to compute term frequency, tf, for all substrings (n-grams) 
in a sequence (corpus). This section will extend the solution to compute document 
frequency, dr, as well as term frequency. The solution runs in O(NlogN) time and 
O(N) space. C code is provided in Appendix C. 
This section will use the running example shown in Figure 6, where the corpus is: 
"to_be$or$not_to_be$'. The corpus consists of three documents, "to_be$', "orS", and 
"not_to_be$'. The special character $ is used to denote the end of a document. The 
procedure outputs a sequence of intervals with their term frequencies and document 
frequencies. These results are also presented for the nontrivial intervals. 
The suffix array is computed using the same procedures discussed above. In ad- 
dition to the suffix array and the lcp vector, Figure 6 introduces a new third table 
that is used to map from suffixes to document ids. This table of document ids will be 
13 
Computational Linguistics Volume 27, Number 1 
used by the function get_docnum to map from suffixes to document ids. Suffixes are 
terminated in Figure 6 after the first end of document symbol, unlike before, where 
suffixes were terminated with the end of corpus symbol. 
A straightforward method for computing df for an interval is to enumerate the 
suffixes within the interval and then compute their document ids, remove duplicates, 
and return the number of distinct documents. Thus, for example, df("o') in Figure 6, 
can be computed by finding corresponding interval, (11,14}, where every suffix within 
the interval starts with "o" and no suffix outside the interval starts with "o'. Then 
we enumerate the suffixes within the interval {s\[11\],s\[12\],s\[13\],s\[14\]}, compute their 
document ids, {0, 2,1, 2}, and remove duplicates. In the end we discover that df("o") -- 
3. That is, "o" appears in all three documents. 
Unfortunately, this straightforward approach is almost certainly too slow. Some 
document ids will be computed multiple times, especially when suffixes appear in 
nested intervals. We take advantage of the nesting property of lcp-delimited intervals 
to compute all df's efficiently. The df of an lcp-delimited interval can be computed 
recursively in terms of its constituents (nested subintervals), thus avoiding unnecessary 
recomputation. 
The procedure print_LDIs_w?th_df presented below is similar to print_LDIs_- 
stack but modified to compute df  as well as tf. The stack keeps track of i and k, as 
before, but now the stack also keeps track of df. 
i, the left edge of an interval, 
k, the representative (SIL = lcp\[k\]), 
df, partial results for dr, counting documents seen thus far, minus duplicates. 
print_LDIs with_dr +--- function(N){ 
stack_i ~- an integer array for the stack of the left edges, i. 
stack_k ~ an integer array for the stack of the representatives, k. 
s tackdf  ~-- an integer array for the stack of the df counter. 
doclink\[O..D\] : an integer array for the document link initialized with -1. 
D = the number of documents. 
stack_i\[O\] ~ O. 
stack_k\[O\] *--- O. 
stack_dr\[O\] *--- 1. 
sp ~ 1 (a stack pointer). 
(1) For j  ~ 0 ,1 ,2 , . . . ,N -  1 do 
(2) (Output a trivial lcp-delimited interval q,j> with tf = 1 and d/= 1.) 
(3) doc *-- get_docnum(s~\]) 
(4) if doclink\[doc\] ~ -1, do 
(5) let x be the largest x such that doclink\[doc\] >_ stack_i N .  
(6) stack_dr\[x\] *--- stack_df\[x\] - 1. 
(7) doclink\[ doc\] ~-- j. 
(8) df ,-- 1. 
(9) While lcp~" + 1\] < lcp\[stack_k\[sp - 1\]\] do 
(10) df *--- stack~tf\[sp - 1\] + df . 
(11) Output a nontrivial interval (i,j) with tf = j - i + 1 and dr, 
if it is lcp-delimited. 
(12) sp ~-- sp - 1. 
(13) stack_i\[sp\] ~-- stack_k\[sp - 1\]. 
(14) stack_k \[sp\] ~-- j + 1. 
14 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
(15) staek_e/\[sp\] ~ ad. 
(16) sp *-- sp + 1. } 
Lines 5 and 6 take care of duplicate documents. The duplication processing makes 
use of doclink (an array of length D, the number of documents in the collection), which 
keeps track of which suffixes have been seen in which document, doclink is initialized 
with -1 indicating that no suffixes have been seen yet. As suffixes are processed, oclink 
is updated (on line 7) so that doclink \[d\] contains the most recently processed suffix in 
document d. As illustrated in Figure 7, when j = 16 (snapshot A), the most recently 
processed suffix in document 0 is s\[11\] ("o_be$"), the most recently processed suffix 
in document 1 is s\[15\] ("r$"), and the most recently processed suffix in document 2 
is s\[16\] ("t_to_be$"). Thus, doclink\[0\]= 11,doclink\[1\]= 15, and doclink\[2\]= 16. After 
processing s\[17\] ("to_be$"), which is in document 0, doclink\[0\] is updated from 11 to 
17, as shown in snapshot B of Figure 7. 
Stackdf keeps track of document frequencies as suffixes are processed. The invari- 
ant is: stack_dr\[x\] contains the document frequency for suffixes een thus far starting 
at i = stack_i\[x\]. (x is a stack offset.) When a new suffix is processed, line 5 checks for 
double counting by searching for intervals on the stack (still being processed) that have 
suffixes in the same document as the current suffix. If there is any double counting, 
stackdf is decremented appropriately on line 6. 
There is an example of this decrementing in snapshot C of Figure 7, highlighted 
by the circle around the binding of df to 0 on the stack element: \[i = 0, k = 17, 
df = 0\]. Note that df was previously bound to 1 in snapshot B. The binding of df was 
decremented when processing s\[18\] because s\[18\] is in the same document as s\[16\]. 
This duplication was identified by line 5. The decrementing was performed by line 6. 
Intervals are processed in depth-first order, so that more deeply nested intervals 
are processed before less deeply nested intervals. In this way, double counting is only 
an issue for intervals higher on the stack. The most deeply nested intervals are trivial 
intervals. They are processed first. They have a df of 1 (line 8). For the remaining 
nontrivial intervals, staekdf contains the partial results for intervals in process. As the 
stack is popped, the df values are aggregated up to compute the df value for the outer 
intervals. The aggregation occurs on line 10 and the popping of the stack occurs on line 
12. The aggregation step is illustrated in snapshots C and D of Figure 7 by the two ar- 
rows with the "+" combination symbol pointing at a value of df in an output statement. 
2.7 Class Arrays 
The classes identified by the previous calculation are stored in a data structure we call 
a class array, to make it relatively easy to look up the term frequency and document 
frequency for an arbitrary substring. The class array is a stored list of five-tuples: (SIL, 
LBL, tf, df, longest suffix I. The fifth element of the five-tuple is a canonical member of 
the class (the longest suffix). The five-tuples are sorted by the alphabetical order of 
the canonical members. In our C code implementation, classes are represented by five 
integers, one for each element in the five-tuple. Since there are N trivial classes and 
at most N - 1 nontrivial classes, the class array will require at most 10N - 5 integers. 
However, for many practical applications, the trivial classes can be omitted. 
Figure 8 shows an example of the nontrivial class array for the corpus: "to_be$or$ 
not_to_be$". The class array makes it relatively easy to determine that the substring 
"o" appears in all three documents. That is, df("o") = 3. We use a binary search to 
find that tuple c\[5\] is the relevant five-tuple for "o". Having found the relevant tuple, 
it requires a simple record access to return the document frequency field. 
15 
Computational Linguistics Volume 27, Number 1 
Doclink 
doclink\[0\]=11 
doclink\[1\]=15 
doclink\[2\]=16 
Local variables Stack 
I j=16 df:Ll'  I I 
lines 13-16: push(i=16, k=l 7, df=l ) 
line l : j~ j+ l  
doclink\[0\]=114 J =17 df=l 
doclink\[1\]=15 I~ i=16, k=17, df=l 
doclink\[_2\]--16 I ~ i=15, k=16, df=l 
\ 
lop\[k=17\] ~. Icp\[j+1=18\] lines 3-6: x 
1 6 ~ l ine  7: doclink\[doc(j=17)=0\] .~ j=17~ 
~l ines  13-16:push(i=17, k=l 8, df=l ) 
J:'7 df:\Li=17, k:18, df=1, l )  
doclink\[1\]=15/ "'--.~=16, k=17, df=l~ / 
doclink\[2\]=16.J.............._~ I df=l l Y 
linesl-6:x__l ~ I J \  + - 
I cp \ [k=17\ ]  > Icp\[j+1=18\] _/line 7: ~ " ' " ~  
5 0 ~ lines 8-11: output LDI--<17,18~=2~=2 
/ "  line12:pap0 ~ ~ ~ ~  
~7- - -~/ / \ ]v j :  18 ~~- -  l .... : ........ ------ ) 
doclink\[1\]= 15 dt / -'----~-.=16, k= 17,(.df=O.), " 
~doclink\[2\]=18)l l i - -~ ,  df=l \ 
Icp\[k=16\] > Icp\[j+1=18\] ~ lines 9-11" output LD1=<16,18>, tf=3, df=2 - 
1 0 ' line 12: pop() 
I j=18 df=2 
. . . . . . . . . . . . . . . . . . . . .  
doclink\[0\]=17 
doclink\[1 \]=15 
doclink\[2\]=l 8 i=15, k=16, df=l 
(A) 
(B) 
(c) 
(D) 
Figure 7 
Snapshots of the doclink array and the stack during the processing of print_LDIs_with_df on 
the corpus: "to_be$or$not_to_be$'. The four snapshots A-D illustrate the state as j progresses 
from 16 to 18. Two nontrivial intervals are emitted while j is in this range: (17,18) and (16,18). 
The more deeply nested interval is emitted before the less deeply nested interval. 
16 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Class array The longest suffix 
(Pointer to cor ~us) denoted by c\[i\] SIL 
c \[0\] 0 _ 1 
c \[i\] 0 _be$ 4 
c \ [2 \ ]  5 $ 1 
c \[3\] 3 be$ 3 
c \[4\] 4 e$ 2 
c \ [5 \ ]  1 o 1 
c \[6\] 1 o_be$ 5 
c\[7\] 11 t 1 
c \ [8 \ ]  0 to_be$ 6 
Figure 8 
An example of the class array for the cor 
LBL tf df 
0 3 2 
1 2 2 
0 3 3 
0 2 2 
0 2 2 
0 4 3 
1 2 2 
0 3 2 
1 2 2 
pus: "to _be$or$not_to_be$". 
3. Experimental Results 
3.1 RIDF and MI for English and Japanese 
We used the methods described above to compute dr, tf, and RIDF for all substrings in 
two corpora of newspapers summarized in Table 1. MI was computed for the longest 
substring in each class. The entire computation took a few hours on a MIPS10000 with 
16 Gbytes of main memory. The processing time was dominated by the calculation of 
the suffix array. 
The English collection consists of 50 million words (113 thousand articles) of the 
Wall Street Journal (distributed by the ACL/DCI) and the Japanese collection consists of 
216 million characters (436 thousand articles) of the CD-Mainichi Shimbun from 1991- 
1995 (which are distributed in CD-ROM format). The English corpus was tokenized 
into words delimited by white space, whereas the Japanese corpus was tokenized into 
characters (typically two bytes each). 
Table I indicates that there are a large number of nontrivial classes in both corpora. 
The English corpus has more substrings per nontrivial class than the Japanese corpus. 
It has been noted elsewhere that the English corpus contains quite a few duplicated 
articles (Paul and Baker 1992). The duplicated articles could explain why there are so 
many substrings per nontrivial class in the English corpus when compared with the 
Japanese corpus. 
Table 1 
Statistics of the English and Japanese corpora. 
Statistic Wall Street Journal Mainichi Shimbun 
N (corpus ize in tokens) 
V (vocabulary in types) 
# articles 
# nontrivial classes 
# substrings in nontrivial classes 
substrings per class (in nontrivial 
classes) 
49,810,697 words 215,789,699 characters 
410,957 5,509 
112,915 435,859 
16,519,064 82,442,441 
2,548,140,677 1,388,049,631 
154.3 16.8 
17 
Computational Linguistics Volume 27, Number 1 
25 
20 
15 
\[~10 
5 
0 
-5 
-10 
5f, I
ll.. . . . . . . . . . . . .  __, , . - .  , _ -  . . . . . . . .  i1:  . . . .  ,~- -15  --20 25 3'0 35 40 f - - -5  . . . .  ,(f 15 20 25 3() 35 40 
? . Length  -, Length 
Figure 9 
The left panel plots MI as a function of the length of the n-gram; the right panel plots RIDF as 
a function of the length of the n-gram. Both panels were computed from the Japanese corpus. 
Note that while there is more dynamic range for shorter n-grams than for longer n-grams, 
there is plenty of dynamic range for n-grams well beyond bigrams and trigrams. 
For subsequent processing, we excluded substrings with tf < 10 to avoid noise, 
resulting in about 1.4 million classes (1.6 million substrings) for English and 10 million 
classes (15 million substrings) for Japanese. We computed RIDF and MI values for the 
longest substring in each of these 1.4 million English classes and 10 million Japanese 
classes. These values can be applied to the other substrings in these classes for RIDF, 
but not for MI. (As mentioned above, two substrings in the same class need not have 
the same MI value.) 
MI of each longest substring, t is computed by the following formula. 
MI(t  = xYz) 
p(xvz)  
= log p(zy)p(z ly  ) 
q(~Yz) 
N 
l?r:, q(xY) q(Yz) 
N q(Y) 
ff(~g~)ff(Y) 
= log t f (xY) t f (Yz ) "  
where x and z are tokens, and Y and zYz are n-grams (sequences of tokens). When Y 
is the empty string, tf(Y) -- N. 
Figure 9 plots RIDF and MI values of 5,000 substrings randomly selected as a func- 
tion of string length. In both cases, shorter substrings have more dynamic range. That 
is, RIDF and MI vary more for bigrams than million-grams. But there is considerable 
dynamic range for n-grams well beyond bigrams and trigrams. 
3.2 Little Correlation between RIDF and MI 
We are interested in comparing and contrasting RIDF and MI. Figure 10 shows that 
RIDF and MI are largely independent. There is little if any correlation between the 
RIDF of a string and the MI of the same string. Panel (a) compares RIDF and MI 
for a sample of English word sequences from the WSJ corpus (excluding unigrams); 
panel (b) makes the same comparison but for Japanese phrases identified as keywords 
on the CD-ROM. In both cases, there are many substrings with a large RIDF value 
and a small MI, and vice versa. 
18 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
2 
1 
2 'i? ~ 
: , ? ~ .~ : 
a ..," 
~, ? ~ ?" * . 
i ? ? 
' ?  # ? ?  ? ? 1 ? ? ? ? ? ? 
; ' . ' .  - ? ? l L - -M.  ? 
l ,%,  ; ? 
. . . .  ' : "  . . . . .  0 -~:: 
i i i 
0 MI fo 2o -10 0 MI 10 20 
(a) English strings 
(word sequences) 
(b) Japanese strings 
(character sequences) 
F igure  10 
Both panels plot RIDF versus MI. Panel (a) plots RIDF and MI for a sample of English 
n-grams; panel (b) plots RIDF and MI for Japanese phrases identified as keywords on the 
CD-ROM. The right panel highlights the 10% highest RIDF and 10% lowest MI with a box, as 
well as the 10% lowest RIDF and the 10% highest MI. Arrows point to the boxes for clarity. 
We believe the two statistics are both useful but in different ways. Both pick out 
interesting n-grams, but n-grams with large MI are interesting in different ways from 
n-grams with large RIDF. Consider the English word sequences in Table 2, which all 
contain the word having. These sequences have large MI values and small RIDF values. 
In our collaboration with lexicographers, especially those working on dictionaries for 
learners, we have found considerable interest in statistics such as MI that pick out 
these kinds of phrases. Collocations can be quite challenging for nonnative speakers 
of the language. On the other hand, these kinds of phrases are not very good keywords 
for information retrieval. 
Table  2 
English word sequences containing the word having. Note that these phrases have large MI 
and low RIDF. They tend to be more interesting for lexicography than information retrieval. 
The table is sorted by MI. 
tf df RIDF MI Phrase 
18 18 -0.0 10.5 admits to having 
14 14 -0.0 9.7 admit to having 
25 23 0.1 8.9 diagnosed as having 
20 20 -0.0 7.4 suspected of having 
301 293 0.0 7.3 without having 
15 13 0.2 7.0 denies having 
59 59 -0.0 6.8 avoid having 
18 18 -0.0 6.0 without ever having 
12 12 -0.0 5.9 Besides having 
26 26 -0.0 5.8 denied having 
19 
Computational Linguistics Volume 27, Number 1 
Table 3 
English word sequences containing the word Mr. (sorted by RIDF). The word sequences near 
the top of the list are better keywords than the sequences near the bottom of the list. None of 
them are of much interest o lexicography. 
tf df RIDF MI Phrase 
11 3 1.9 0.6 . Mr. Hinz 
18 5 1.8 6.6 Mr. Bradbury 
51 16 1.7 6.7 Mr. Roemer 
67 25 1.4 6.8 Mr. Melamed 
54 27 1.0 5.8 Mr. Burnett 
11 8 0.5 1.1 Mr. Eiszner said 
53 40 0.4 0.3 Mr. Johnson. 
21 16 0.4 0.2 Mr. Nichols said. 
13 10 0.4 0.4 . Mr. Shulman 
176 138 0.3 0.5 Mr. Bush has 
13 11 0.2 1.5 to Mr. Trump's 
13 11 0.2 -0.9 Mr. Bowman, 
35 32 0.1 1.2 wrote Mr. 
12 11 0.1 1.7 Mr. Lee to 
22 21 0.1 1.4 facing Mr. 
11 11 -0.0 0.7 Mr. Poehl also 
13 13 -0.0 1.4 inadequate. " Mr. 
16 16 -0.0 1.6 The 41-year-old Mr. 
19 19 -0.0 0.5 14. Mr. 
26 26 -0.0 0.0 in November. Mr. 
27 27 -0.0 -0.0 " For his part,  Mr. 
38 38 -0.0 1.4 . AMR, 
39 39 -0.0 -0.3 for instance, Mr. 
Table 3 shows MI and RIDF values for a sample of word sequences containing 
the word Mr. The table is sorted by RIDE The sequences near the top of the list are 
better keywords than the sequences further down. None of these sequences would be 
of much interest o a lexicographer (unless he or she were studying names). Many of 
the sequences have rather small MI values. 
Table 4 shows a few word sequences tarting with the word the with large MI 
values. All of these sequences have high MI (by construction), but some are high 
in RIDF as well (labeled B), and some are not (labeled A). Most of the sequences 
are interesting in one way or another, but the A sequences are different from the 
B sequences. The A sequences would be of more interest to someone studying the 
grammar  in the WSJ subdomain,  whereas the B sequences would be of more interest 
to someone studying the terminology in this subdomain. The B sequences in Table 4 
tend to pick out specific events in the news, if not specific stories. The phrase, the Basic 
Law, for example, picks out a pair of stories that discuss the event of the handover  of 
Hong Kong to China, as il lustrated in the concordance shown in Table 5. 
Table 6 shows a number  of word sequences with high MI containing common 
prepositions. The high MI indicates an interesting association, but again most have 
low RIDF and are not particularly good keywords,  though there are a few exceptions 
(Just for Men, a wel l -known brand name, has a high RIDF and is a good keyword). 
The Japanese substrings are similar to the English substrings. Substrings with high 
RIDF pick out specific documents (and/or  events) and therefore tend to be relatively 
good keywords. Substrings with high MI have nonindependent distributions (if not 
noncomposit ional semantics), and are therefore likely to be interesting to a lexicogra- 
pher or linguist. Substrings that are high in both are more likely to be meaningful  units 
20 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Table 4 
English word sequence containing the. All of these phrases have high MI. Some have high 
RIDF, and some do not. 
(A): Low RIDF (poor keywords) 
tf df RIDF MI Phase 
11 11 -0.0 11.1 the up side 
73 66 0.1 9.3 the will of 
16 16 -0.0 8.6 the sell side 
17 16 0.1 8.5 the Stock Exchange of 
16 15 0.1 8.5 the buy side 
20 20 -0.0 8.4 the down side 
55 54 0.0 8.3 the will to 
14 14 -0.0 8.1 the saying goes 
15 15 -0.0 7.6 the going gets 
(B): High RIDF (better keywords) 
tf df RIDF MI Phase 
37 3 3.6 2.3 the joint commission 
66 8 3.0 3.6 the SSC 
55 7 3.0 2.0 The Delaware & 
37 5 2.9 3.6 the NHS 
22 3 2.9 3.4 the kibbutz 
22 3 2.9 4.1 the NSA's 
29 4 2.9 4.2 the DeBartolos 
36 5 2.8 2.3 the Basic Law 
21 3 2.8 2.3 the national output 
(words or phrases) than substrings that are high in just one or the other. Meaningless 
fragments tend to be low in both MI and RIDE 
We grouped the Japanese classes into nine cells depending on whether the RIDF 
was in the top 10%, the bottom 10%, or in between, and whether the MI was in the 
top 10%, the bottom 10%, or in between. Substrings in the top 10% in both RIDF and 
MI tend to be meaningful words such as (in English translation) merger, stock certi~'cate, 
dictionary, wireless, and so on. Substrings in the bottom 10% in both RIDF and MI tend 
to be meaningless fragments, or straightforward compositional combinations of words 
such as current regular-season game. Table 7 shows examples where MI and RIDF point 
in opposite directions (see highlighted rectangles in panel (b) of Figure 10). 
We have observed previously that MI is high for general vocabulary (words found 
in dictionary) and RIDF is high for names, technical terminology, and good keywords 
for information retrieval. Table 7 suggests an intriguing pattern. Japanese uses different 
character sets for general vocabulary and loan words. Words that are high in MI tend 
to use the general vocabulary character sets (hiragana nd kanji) whereas words that 
are high in RIDF tend to use the loan word character sets (katakana nd English). 
(There is an important exception, though, for names, which will be discussed in the 
next subsection.) 
The character sets largely reflect the history of the language. Japanese uses four 
character sets (Shibatani 1990). Typically, functional words of Japanese origin are writ- 
ten in hiragana. Words that were borrowed from Chinese many hundreds of years ago 
are written in kanji. Loan words borrowed more recently from Western languages are 
written in katakana. Truly foreign words are written in the English character set (also 
known as romaji). We were pleasantly surprised to discover that MI and RIDF were 
distinguishing substrings on the basis of these character set distinctions. 
21 
Computational Linguistics Volume 27, Number 1 
Table 5 
Concordance of the phrase the Basic Law. Note that most of the instances of the Basic Law appear 
in just two stories, as indicated by the doc-id (the token-id of the first word in the document). 
token-id left context right context doc-id 
2229521: line in the drafting of the Basic Law that will determine how Hon 2228648 
2229902: s policy as expressed in the Basic Law - as Gov. Wilson's debut s 2228648 
9746758: he U.S. Constitution and the Basic Law of the Federal Republic of 9746014 
11824764: any changes must follow the Basic Law, Hong Kong's miniconstitut 11824269 
33007637: sts a tentative draft of the Basic Law, and although this may be 33007425 
33007720: the relationship between the Basic Law and the Chinese Constitutio 33007425 
33007729: onstitution. Originally the Basic Law was to deal with this topic 33007425 
33007945: wer of interpretation f the Basic Law shall be vested in the NPC 33007425 
33007975: tation of a provision of the Basic Law, the courts of the HKSAR { 33007425 
33008031: interpret provisions of the Basic Law. If a case involves the in 33007425 
33008045: tation of a provision of the Basic Law concerning defense, foreig 33007425 
33008115: etation of an article of the Basic Law regarding " defense, forei 33007425 
33008205: nland representatives of the Basic Law Drafting Committee fear tha 33007425 
33008398: e : Mainland drafters of the Basic Law simply do not appreciate th 33007425 
33008488: pret al the articles of the Basic Law. While recognizing that th 33007425 
33008506: y and power to interpret the Basic Law, it should irrevocably del 33007425 
33008521: pret those provisions of the Basic Law within the scope of Hong Ko 33007425 
33008545: r the tentative draft of the Basic Law, I cannot help but conclud 33007425 
33008690: d of being guaranteed by the Basic Law, are being redefined out o 33007425 
33008712: uncilor, is a member of the Basic Law Drafting Committee. 33007425 
39020313: sts a tentative draft of the Basic Law, and although this may be 39020101 
39020396: the relationship between the Basic Law and the Chinese Constitutio 39020101 
39020405: onstitution. Originally the Basic Law was to deal with this topic 39020101 
39020621: wer of interpretation f the Basic Law shall be vested in the NPC 39020101 
39020651: tation of a provision of the Basic Law, the courts of the HKSAR { 39020101 
39020707: interpret provisions of the Basic Law . If a case involves the in 39020101 
39020721: tation of a provision of the Basic Law concerning defense, foreig 39020101 
39020791: etation of an article of the Basic Law regarding " defense, forei 39020101 
39020881: nland representatives of the Basic Law Drafting Committee fear tha 39020101 
39021074: e : Mainland drafters of the Basic Law simply do not appreciate th 39020101 
39021164: pret al the articles of the Basic Law. While recognizing that th 39020101 
39021182: y and power to interpret the Basic Law, it should irrevocably del 39020101 
39021197: pret those provisions of the Basic Law within the scope of Hong Ko 39020101 
39021221: r the tentative draft of the Basic Law, I cannot help but conclud 39020101 
39021366: d of being guaranteed by the Basic Law, are being redefined out o 39020101 
39021388: uncilor, is a member of the Basic Law Drafting Committee. 39020101 
3.3 Names  
As mentioned above, names are an important exception to the rule that kanji (Chinese 
characters) are used for general vocabulary (words found in the dictionary) that were 
borrowed hundreds of years ago and katakana characters are used for more recent 
loan words (such as technical terminology). As illustrated in Table 7, kanji are also 
used for the names of Japanese people and katakana re used for the names of people 
from other countries. 
Names are quite different in English and Japanese. Figure 11 shows a striking con- 
trast in the distributions of MI and RIDF values. MI has a more compact distribution 
in English than Japanese. Japanese names cluster into two groups, but English names 
do not. 
The names shown in Figure 11 were collected using a simple set of heuristics. For 
English, we selected substrings tarting with the titles Mr., Ms., or Dr. For Japanese, we 
selected keywords (as identified by the CD-ROM) ending with the special character 
22 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Table 6 
English word sequences containing common prepositions. All have high MI (by construction); 
most do not have high RIDF (though there are a few exceptions such as Just for Men, a 
well-known brand name). 
tf df RIDF MI Preposition = "for" 
14 14 -0.0 14.6 feedlots for fattening 
15 15 -0.0 14.4 error for subgroups 
12 12 -0.0 14.1 Voice for Food 
10 5 1.0 13.8 Quest for Value 
12 4 1.6 13.8 Friends for Education 
13 13 -0.0 13.7 Commissioner for Refugees 
23 21 0.1 13.7 meteorologist for Weather 
10 2 2.3 13.4 Just for Men 
10 9 0.2 13.4 Witness for Peace 
19 16 0.2 12.9 priced for reoffering 
tf df RIDF MI Preposition = "on" 
1l 5 1.1 14.3 Terrorist on Trial 
11 10 0.1 13.1 War on Poverty 
13 12 0.1 12.7 Institute on Drug 
16 16 -0.0 12.6 dead on arrival 
12 12 -0.0 11.6 from on high 
12 12 -0.0 11.6 knocking on doors 
22 18 0.3 11.3 warnings on cigarette 
11 11 -0.0 11.2 Subcommittee on Oversight 
17 12 0.5 11.2 Group on Health 
22 20 0.1 11.1 free on bail 
tf df RIDF MI Preposition = "by" 
11 11 -0.0 12.9 piece by piece 
13 13 -0.0 12.6 guilt by association 
13 13 -0.0 12.5 step by step 
15 15 -0.0 12.4 bit by bit 
16 16 -0.0 l l .8 engineer by training 
61 59 0.0 11.5 side by side 
17 17 -0.0 11.5 each by Korea's 
12 12 -0.0 11.3 hemmed in by 
11 11 -0.0 10.8 dictated by formula 
20 20 -0.0 10.7 70%-owned by Exxon 
tf df RIDF MI Preposition = "of" 
11 10 0.1 16.8 Joan of Arc 
12 5 1.3 16.2 Ports of Call 
16 16 -0.0 16.1 Articles of Confederation 
14 13 0.1 16.1 writ of mandamus 
10 9 0.2 15.9 Oil of Olay 
11 11 -0.0 15.8 shortness of breath 
10 9 0.2 15.6 Archbishop of Canterbury 
10 8 0.3 15.3 Secret of My 
12 12 -0.0 15.2 Lukman of Nigeria 
16 4 2.0 15.2 Days of Rage 
(-shi), which  is rough ly  the equ iva lent  of the Engl ish titles Mr. and Ms. In both  cases, 
phrases  were  requ i red  to have  tf > 10. 3 
3 This procedure produced the interesting substring, Mr. From, where both words would normally 
appear on a stop list. This name has a large RIDF. (The MI, though, is small because the parts are so 
high in frequency.) 
23 
Computational Linguistics Volume 27, Number 1 
Tab le  7 
Examples of keywords with extreme values of RIDF and MI that point in opposite directions. 
The top half (high RIDF and low MI) tends to have more loan words, largely written in 
katakana nd English. The bottom half (low RIDF and high MI) tends to have more general 
vocabulary, largely written in Chinese kanji. 
RIDF  MI  Subst r ings  Features 
High Low :~k~E(native lastname) 
10% 10% SUN (company ame) 
U - -  if(foreign name) 
~'c~ U(brush) 
',1 7 7' -- (sofa) 
Low High ~;~< f'~.~,~ (huge) 
10% 10% ~ ~J 1~ (passive) 
~t,~ O (determination) 
J~j(native full name) 
~j~(native full name) 
kanji character 
English character 
katakana character 
hiragana character 
loan word, katakana 
general vocabulary 
general vocabulary, kanji 
general vocabulary 
kanji character 
kanji character 
5 
4 
1 
qZ 
0 
? 
+-+ 
.~ ? 
" ++I +~C ? 
+.+ . . 
'o * 0++ , <~ 
'."? ,+ . , ,<~%,~ " 
: .+ ' ;+  . 
i i 
0 4 MI 8 12 
(a) Eng l i sh  names  
F igure  11 
MI and RIDF of people's names. 
2 
I 
+ o + ??++ ??  
o+~.~.o~,...~$?,,,'~ ? o? ,.. +,#.,.~_.,,.~ le++,++.+.?~ ?? 
? - ' . I L ' ,+ I~+, , .~ , .~  + , I ~ < , Q "  ++CP++ .,, 
? ? <+~lh; , I l lml l I l " \ ] IR~.& ?*<,?  ? ,,~'<,Ir...,~b-~. <, o,. o 
.+ ~+~mlL',ll,,l-l-l-l-l~ "; l ~+? ? ,,? t 
+ <, <+ 
+ ? . 
+ "~."r'I,l~lll3P~+V ." 
. ? . ? ? . ? . i 
-8 -4 0 MI 4 8 12 
(b) Japanese  names  
The Engl ish names have a sharp cutoff a round MI = 7 due in large part  to 
the title Mr. MI( 'Mr. ' ,  x) = log 2 ~ N  - -  l?g2 tf('Mr.',x)tf(x) ---- 7.4 - log 2 ~'t f (x) '  Since 
log 2 ~t f (x )  is a small posit ive number,  typical ly 0-3, MI('Mr',x) < 7.4. 
Names  general ly have RIDF values ranging f rom practical ly noth ing (for com- 
mon names like Jones) to extremely large values for excellent keywords.  The Japanese 
names,  however,  cluster into two groups,  those with RIDF above 0.5, and those with 
RIDF below 0.5. The separat ion above and be low RIDF = 0.5, we believe, is a reflec- 
24 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
tion of the well-known distinction between ew information and given information i  
discourse structure. It is common in both English and Japanese, for the first mention 
of a name in a news article to describe the name in more detail than subsequent uses. 
In English, for example, terms like spokesman or spokeswoman and appositives are quite 
common for the first use of a name, and less so, for subsequent uses. In Japanese, the 
pattern appears to be even more rigid than in English. The first use will very often 
list the full name (first name plus last name), unlike subsequent uses, which almost 
always omit the first name. As a consequence, the last name exhibits a large range 
of RIDF values, as in English, but the full name will usually (90%) fall below the 
RIDF = 0.5 threshold. The MI values have a broader ange as well, depending on the 
compositionality of the name. 
To summarize, RIDF and MI can be used to identify a number of interesting 
similarities and differences in the use of names. Names are interestingly different 
from general vocabulary. Many names are very good keywords and have large RIDE 
General vocabulary tends to have large MI. Although we observe this basic pattern 
over both English and Japanese, names bring up some interesting differences between 
the two languages such as the tendency for Japanese names to fall into two groups 
separated by the RIDF = 0.5 threshold. 
3.4 Word Extraction 
RIDF and MI may be useful for word extraction. In many languages such as Chinese, 
Japanese, and Thai, word extraction isnot an easy task, because, unlike English, many 
of these languages do not use delimiters between words. Automatic word extraction 
can be applied to the task of dictionary maintenance. Since most NLP applications 
(including word segmentation for these languages) are dictionary based, word extrac- 
tion is very important for these languages. Nagao and Mori (1994) and Nagata (1996) 
proposed n-gram methods for Japanese. Sproat and Shih (1990) found MI to be useful 
for word extraction i  Chinese. 
We performed the following simple experiment to see if both MI and RIDF could 
be useful for word extraction in Japanese. We extracted four random samples of 100 
substrings each. The four samples cover all four combinations of high and low RIDF 
and high and low MI, where high is defined to be in the top 10% and low is defined 
to be in the bottom 10%. Then we manually scored each sample substring using our 
own subjective judgment. Substrings were labeled "good" (the substring is a word), 
"bad" (the substring is not a word), or "gray" (the judge is not sure). The results are 
presented in Table 8, which shows that substrings with high scores in both dimensions 
are more likely to be words than substrings that score high in just one dimension. 
Substrings with low scores in both dimensions are very unlikely to be words. These 
results demonstrate plausibility for the use of multiple statistics. The approach could 
be combined with other methods in the literature such as Kita et al (1994) to produce 
a more practical system. In any case, automatic word extraction is not an easy task for 
Japanese (Nagata 1996). 
4. Conc lus ion 
Bigrams and trigrams are commonly used in statistical natural anguage processing; 
this paper described techniques for working with much longer n-grams, including 
million-grams and even billion-grams. We presented algorithms (and C code) for com- 
puting term frequency (tf) and document frequency (dr) for all n-grams (substrings) 
in a corpus (sequence). The method took only a few hours to compute tf and df for 
all the n-grams in two large corpora, an English corpus of 50 million words of Wall 
25 
Computational Linguistics Volume 27, Number 1 
Table 8 
The combination of RIDF and MI is better in a word extraction task than either by itself, 
which is better than neither. Each cell reports performance over a sample of 100 substrings. 
Substrings were subjectively judged to be "good" (the substring is a word), "bad" (the 
substring is not a word), or "gray" (the judge is not sure). Two performance values are 
reported, indicating what percentage of the 100 substrings are words. The larger performance 
values count the "gray" substrings as words; the smaller performance values count the "gray" 
substrings as nonwords. 
All MI MI(high 10%) MI(low 10%) 
All RIDF - -  20-44% 2-11% 
RIDF(high 10%) 29-51% 38-55% 11-35% 
RIDF(low 10%) 3-18% 4-13% 0-8% 
Street Journal news articles and a Japanese corpus of 216 million characters of Mainichi 
Shimbun ews articles. 
The method works by grouping substrings into classes so that the computation 
of tf and df over order N 2 substrings can be reduced to a computation over order N 
classes. The reduction makes use of four properties: 
Properties 1-2: all substrings in a class have the same statistics (at least 
for the statistics of interest, namely tf and dJ), 
Property 3: the set of all substrings are partit ioned by the classes, and 
Property 4: there are many fewer classes (order N) than substrings 
(order N2). 
The second half of the paper used the results of computing tf and df for all n-grams 
in the two large corpora mentioned above. We compared and contrasted RIDF and 
MI, statistics that are motivated by work in lexicography and information retrieval. 
Both statistics compare the frequency of an n-gram to chance, but they use different 
notions of chance. RIDF looks for n-grams whose distributions pick out a relatively 
small number documents, unlike a random (Poisson) distribution. These n-grams tend 
to be good keywords for information retrieval, such as technical terms and names. MI 
looks for n-grams whose internal structure cannot be attributed to compositionality. MI
tends to pick out general vocabulary--words and phrases that appear in dictionaries. 
We believe that both statistics are useful, but in different and complementary ways. 
In a Japanese word extraction task, the combination of MI and RIDF performed better 
than either by itself. 
Appendix: C Code 
The following code is also available at http: / /www.mi lab. is . tsukuba.ac. jp /~myama/ 
tfdf. 
A: C Code to Print All LCP-Delimited Intervals using C Language's Stack 
The function output  (below) is called 2N - 1 times. It will output an interval if the 
interval is lcp-delimited (LBL < SIL). Trivial intervals are always lcp-delimited. Non- 
trivial intervals are lcp-delimited if the bounding lcps are smaller than the SIL=lcp\[k\], 
26 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
where k is the representative. 
void output(int i, int j, int k){ 
int LBL = (Icp\[i\] > icp\[j+l\]) ? icp\[i\] : icp\[j+l\]; 
int SIL = icp\[k\]; 
if(i==j) printf("trivial <~d,~d>, tf=l\n ", i, j); 
else if(LBL < SIL) printf("non-trivial <~d, ~d>, rep=~d, tf=~d\n", 
i, j, k, j-i+i); 
int print_LDIs(int i, int k){ 
int j = k; 
output(k,k,O); /* trivial intervals */ 
while(icp\[k\] <= icp\[j+l\] && j+l < N) j = print_LDIs(k, j+l); 
output(i,j,k); /~ non-trivial intervals */ 
return j;} 
B: C Code to Print Al l  LCP-Del imited Intervals Us ing an Own Stack 
print_LDIs_stack is similar to print_LDIs, but uses its own stack. It takes the corpus 
size, N, as an argument. 
#define STACK_SIZE i00000 
#define Top_i (stack\[sp-l\].i) 
#define Top_k (stack\[sp-l\].k) 
struct STACK {int i; int k;} stack\[STACK_SIZE\]; 
int sp = O; /~ stack pointer ~/ 
void push(int i, int k) { 
if(sp >= STACK_SIZE) { 
fprintf(stderr, "stack overflow\n"); 
exit(2);} 
stack\[sp\].i = i; 
stack\[sp++\].k = k;} 
void pop() {sp--;} 
void print_LDIs_stack(int N) { 
int j; 
push(O,O); 
for(j = O; j < N; j++) { 
output(j, j, 0); 
while(icp\[j+l\] < Icp\[Top_k\]) { 
output(Top_i, j, Top_k); 
pop();} 
push(Top_k, j+l);}} 
27 
Computational Linguistics Volume 27, Number 1 
C: C Code to Print All LCP-Delimited Intervals with tf and df 
The steps 5 and 6 of the algorithm in Section 2.6 are implemented as the function 
dec df using the binary search. 
#define STACK_SIZE I00000 
#define Top_i (stack\[sp-l\] .i) 
#define Top_k (stack\[sp-l\] .k) 
#define Top_dr (stack\[sp-l\] .dr) 
struct STACK {int i; int k; int dr;} stack\[STACK_SIZE\]; 
int sp = O; /* stack pointer */ 
void push(int i, int k, int dr) { 
if(sp >= STACK_SIZE){ 
fprintf(stderr, "stack overflowkn"); 
exit (2) ;} 
stack\[sp\] .i = i; 
stack\[sp\] .k = k; 
stack \[sp++\] . df = df ; } 
void pop() {sp--;} 
void output(int i, int j, int k, int dr) { 
int LBL; 
if(icp\[i\] > icp\[j+l\]) LBL = icp\[i\]; 
else LBL = icp\[3+l\]; 
if(i==j) printf("trivial <~d,~d>, tf=l\n '', i, j); 
else if(LBL < Icp\[k\]) 
printf("non-trivial <~d, ~d>, rep=~d, tf=~d, df=~d\n", 
i, j, k, j-i+l, df); 
* Print_LDIs with_dr does not only print tf, but also df. 
* It takes the corpus size, N, and the number of documents, D. 
*doc( )  returns the document number of the suffix array's index. 
* dec_dr() decrease a dr-counter in the stack when duplicate 
* counting occurs. 
*/ 
void dec_df(int docid) { 
int beg=0, end=sp, mid=sp/2; 
while(beg != mid) { 
if(doclink\[docid\] >= stack\[mid\].i) beg = mid; 
else end = mid; 
mid = (beg + end) / 2; 
} 
stack\[mid\].df--; 
28 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
print_LDIs_with_df( int  N, int D) { 
int i, j, dr; 
docl ink = (int ~)malloc(sizeof( int)  ? D); 
for(i = O; i < D; i++) doclink\[i\] = -I; 
push(O,O, I) ; 
for(j = O; j < N; j++) { 
output (j ,j ,0, I) ; 
i f(docl ink\[doc(j)\]  != -I) dec_df(doc(j));  
doclink\[doc(j)\] = j ; 
df = i; 
while (icp\[j+l\] < icp\[Top_k\]) { 
df = Top_dr + dr; 
output (Top_i, j, Top_k, dr) ; 
pop() ; 
} 
push(Top_k, j+l, dr); 
} 
Acknowledgments 
We would like to thank the anonymous 
reviewers for Computational Linguistics who 
made insightful comments on an earlier 
draft. 
References 
Charniak, Eugene. 1993. Statistical Language 
Learning. MIT Press. 
Church, Kenneth W. and William A. Gale. 
1995. Poisson mixtures. Natural Language 
Engineering, 1(2):163-190. 
Gonnet, Gaston H., Ricardo A. Baeza-Yates, 
and Tim Snider. 1992. New indices for 
text: PAT trees and PAT arrays. In 
Information Retrieval: Data Structure & 
Algorithms, pages 66-82. Prentice Hall 
PTR. 
Gusfield, Dan. 1997. Algorithms on Strings, 
Trees, and Sequences: Computer Science and 
Computational Biology. Cambridge 
University Press. 
Hui, Lucas Chi Kwong. 1992. Color set size 
problem with applications to string 
matching. In Lecture Notes in Computer 
Science, Volume 644. Springer (CPM92: 
Combinatorial Pattern Matching, 3rd 
Annual Symposium), pages 230-243. 
Jelinek, Frederick. 1997. Statistical Methods 
/or Speech Recognition. MIT Press. 
Katz, Slava M. 1987. Estimation of 
probabilities from sparse data for the 
language model component of a speech 
recognizer. IEEE Transactions on Acoustics, 
Speech, and Signal Processing, 
ASSP-35(3):400-401. 
Kita, Kenji, Yasuhiko Kato, Takashi Omoto, 
and Yoneo Yano. 1994. A comparative 
study of automatic extraction of 
collocations from corpora: Mutual 
information vs. cost criteria. Journal of 
Natural Language Processing, 1(1):21-33. 
Manber, Udi and Gene Myers. 1990. Suffix 
arrays: A new method for on-line string 
searches. In Proceedings ofThe First Annual 
ACM-SIAM Symposium on Discrete 
Algorithms, pages 319-327. URL=http:// 
glimpse.cs.arizona.edu / udi.html. 
McCreight, Edward M. 1976. A 
space-economical suffix tree construction 
algorithm. Journal of the ACM, 23:262- 
272. 
Nagao, Makoto and Shinsuke Mori. 1994. A 
new method of n-gram statistics for large 
number of n and automatic extraction of 
words and phrases from large text data of 
Japanese. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics, pages 611-615. 
Nagata, Masaaki. 1996. Automatic 
extraction of new words from Japanese 
texts using generalized forward-backward 
search. In Proceedings ofthe Conference on 
Empirical Methods in Natural Language 
Processing, pages 48-59. 
Paul, Douglas B. and Janet M. Baker. 1992. 
The design for the Wall Street 
Journal-based CSR corpus. In Proceedings 
of DARPA Speech and Natural Language 
Workshop, pages 357-361. 
Shibatani, Masayoshi. 1990. The Languages of 
Japan. Cambridge Language Surveys. 
Cambridge University Press. 
29 
Computational Linguistics Volume 27, Number 1 
Sparck Jones, Karen. 1972. A statistical 
interpretation of term specificity and its 
application in retrieval. Journal of 
Documentation, 28(1):11-21. 
Sproat, Richard and Chilin Shih. 1990. A 
statistical method for finding word 
boundaries in Chinese text. Computer 
Processing of Chinese and Oriental Languages, 
4(4):336-351. 
Ukkonen, Esko. 1995. On-line construction 
of suffix trees. Algorithmica, 14(3):249-260. 
30 
Empirical Term Weighting and Expansion Frequency 
Kyoji Umemura 
Toyohashi University of Technology 
Toyohashi Aichi 441-8580 Japan 
umemura@tut  i cs .  ru t .  ac .  jp  
Kenneth W. Church 
AT&T Labs-Research 
180 Park Ave., Florham Park, NJ. 
kwc~research ,  art. com 
Abstract 
We propose an empirical method for estimating 
term weights directly from relevance judgements, 
avoiding various standard but potentially trouble- 
some assumptions. It is common to assume, for ex- 
ample, that weights vary with term frequency (t f )  
and inverse document frequency (idf) in a particu- 
lar way, e.g., t f .  idf, but the fact that there are so 
many variants of this formula in the literature sug- 
gests that there remains considerable uncertainty 
about these assumptions. Our method is similar to 
the Berkeley regression method where labeled rel- 
evance judgements are fit as a linear combination 
of (transforms of) t f, idf, etc. Training meth- 
ods not only improve performance, but also ex- 
tend naturally to include additional factors such 
as burstiness and query expansion. The proposed 
histogram-based training method provides a sim- 
ple way to model complicated interactions among 
factors such as t f ,  idf, burstiness and expansion 
frequency (a generalization of query expansion). 
The correct handling of expanded term is realized 
based on statistical information. Expansion fre- 
quency dramatically improves performance from 
a level comparable to BKJJBIDS, Berkeley's en- 
try in the Japanese NACSIS NTCIR-1 evaluation 
for short queries, to the level of JCB1, the top 
system in the evaluation. JCB1 uses sophisti- 
cated (and proprietary) natural anguage process- 
ing techniques developed by Just System, a leader 
in the Japanese word-processing industry. We are 
encouraged that the proposed method, which is 
simple to understand and replicate, can reach this 
level of performance. 
1 In t roduct ion  
An empirical method for estimating term weights 
directly from relevance judgements is proposed. 
The method is designed to make as few assump- 
tions as possible. It is similar to Berkeley's use 
of regression (Cooper et al, 1994) (Chen et al, 
1999) where labeled relevance judgements are fit 
as a linear combination of (transforms of) t f ,  idf, 
etc., but avoids potentially troublesome assump- 
tions by introducing histogram methods. Terms 
are grouped into bins. Weights are computed 
based on the number of relevant and irrelevant 
documents associated with each bin. The result- 
? t: a term 
? d: a document 
? t f ( t ,  d): term freq = # of instances of t in d 
? df(t): doc freq = # of docs d with t f(t ,  d) > 1 
? N: # of documents in collection 
? idf(t): inverse document freq: -log2 d~t) 
? df(t ,  tel, t f0): # of relevant documents d with 
t f(t ,  d) = tfo 
? df(t, rel, tfo): # of irrelevant documents d
with tf(t ,  d) = tfo 
? el(t): expansion frequency = # docs d in 
query expansion with t f ( t ,  d) > 1 
? TF(t): standard notion of frequency in 
corpus-based NLP: TF(t)  = ~d tf(t ,  d) 
? B(t): burstiness: B(t) = 1 iff ~ is large. df(t) 
Table 1: Notation 
ing weights usually lie between 0 and idf, which 
is a surprise; standard formulas like t f .  idf would 
assign values well outside this range. 
The method extends naturally to include ad- 
ditional factors such as query expansion. Terms 
mentioned explicitly in the query receive much 
larger weights than terms brought in via query 
expansion. In addition, whether or not a term 
t is mentioned explicitly in the query, if t ap- 
pears in documents brought in by query expan- 
sion (el(t) > 1) then t will receive a much larger 
weight than it would have otherwise (ef(t) = 0). 
The interactions among these factors, however, are 
complicated and collection dependent. It is safer 
to use histogram methods than to impose unnec- 
essary and potentially troublesome assumptions 
such as normality and independence. 
Under the vector space model, the score for a 
document d and a query q is computed by sum- 
ming a contribution for each term t over an ap- 
propriate set of terms, T. T is often limited to 
terms shared by both the document and the query 
(minus stop words), though not always (e.g, query 
expansion). 
117 
i# 
12.89 
10.87 
9.79 
8.96 
7.75 
6.82 
5.78 
4.74 
3.85 
2.85 
1.78 
0.88 
t /=O t f= l  t / :=2 t f=3 t f>4 
-0.37 9.73 11.69 12.45 13.59 
-0.49 8.00 9.95 11.47 12.06 
-0.86 7.36 9.38 10.63 10.88 
-0.60 6.26 7.99 8.99 9.41 
-0.34 4.62 5.82 6.62 7.98 
-1.26 3.94 6.05 7.59 8.98 
-0.83 3.16 5.17 5.77 7.00 
-0.84 2.46 3.91 4.54 5.58 
-0.60 1.58 2:.76 3.57 4.55 
-1.02 1.00 1.72 2.55 3.96 
-1.33 -0.06 1.05 2.46 4.50 
-0.16 0.17 0.19 -0.10 -0.37 
Table 2: Empirical estimates of A as a function of 
t f  and idf. Terms are assi._~ed to bins based on 
idf. The column labeled idf is the mean idf for 
the terms in each bin. A is estimated separately for 
each bin and each t f  value, based on the labeled 
relevance judgements. 
score~(d, q) = E t/(t,  d) . idf(t) 
tET 
Under the probabilistic retrieval model, docu- 
ments are scored by summing a similar contribu- 
tion for each term t. 
= ~ l P(tJrel) 
In this work, we use A to refer to term weights. 
q) = d, q) 
tET  
This paper will start by showing how to estimate A 
from relevance judgements. Three parameteriza- 
tions will be considered: (1) fit-G, (2) fit-B, which 
introduces burstiness, and (3) fit-E, which intro- 
duces expansion frequency. The evaluation section 
shows that each model improves on the previous 
one. But in addition to performance, we are also 
interested in the interpretations of the parameters. 
2 Superv ised  Tra in ing  
The statistical task is to compute A, our best esti- 
mate of A, based on a training set. This paper will 
use supervised methods where the training mate- 
rials not only include a large number of documents 
but also a few queries labeled with relevance judge- 
ments. 
To make the training task more manageable, it 
is common practice to map the space of all terms 
into a lower dimensional feature space. In other 
words, instead of estimating a different A for each 
term in the vocabulary, we can model A as a func- 
tion of tf and idf and various other features of 
Train 
/ ~4 .~ / 1 ~ 
0 2 4 6 8 10 12 
IDF 
Test 
4 ~ s~- 
. 
~ 2  11  
0 2 4 6 8 10 12 
IDF 
Figure 1: Empirical weights, A. Top panel shows 
values in previous table. Most points fall between 
the dashed lines (lower limit of A = 0 and upper 
limit of A = idf). The plotting character denotes 
t f .  Note that the line with t f  = 4 is above the 
line with t f  = 3, which is above the line with 
t f  = 2, and so on. The higher lines have larger 
intercepts and larger slopes than the lower lines. 
That is, when we fit A ,~, a(tf) + b(tf) ,  idf, with 
separate regression coefficients, a(tf) and b(tf), 
for each value of t f ,  we find that both a(tf) and 
b(tf) increase with t\]. 
terms. In this way, all of the terms in a bin are 
assigned the weight, A. The common practice, 
for example, of assigning t f  ? idf weights can be 
interpreted as grouping all terms with the same 
idf into a bin and assigning them all the same 
weight, namely t f .  idf. Cooper and his colleagues 
at Berkeley (Cooper et al, 1994) (Chen et al, 
1999) have been using regression methods to fit 
as a linear combination of idf , log(t f )  and var- 
ious other features. This method is also grouping 
terms into bins based on their features and assign- 
ing similar weights to terms with similar features. 
In general, term weighting methods that are fit 
to data are more flexible than weighting methods 
that are not fit to data. We believe this additional 
flexibility improves precision and recall (table 8). 
Instead of multiple regression, though, we 
choose a more empirical approach. Parametric as- 
118 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
20 
21 
Description (function of term t) 
df(t, rel,O) _-- # tel does d with t f ( t ,d)  = 0 
dr(t, tel, 1) _= # rel does d with tf(t ,  d) = 1 
dr(t, rel, 2) _= # rel does d with t f(t ,  d) = 2 
df(t, rel,3) ~ # rel does d with t f ( t ,d)  = 3 
df(t, rel,4+) ~ # tel does d with t f ( t ,d)  _> 
dr(t, tel, O) ~ # tel does d with t f(t ,  d) = 0 
dr(t, rel, 1) ~_ # tel does d with t f(t ,  d) = 1 
dr(t, tel, 2) ~ # rel does d with t f(t ,  d) = 2 
where dr(bin, rel, t f )  is 
1 
dr(bin, tel, t f )  ~ Ib/=l ~ df(t, re l , t f )  
tEbin 
Similarly, the denominator can be approximated 
as :  
dr(bin, tel, t \]) P(bin, tfl~) ~ log2 
df ( t ,~ ,3)  -= #reml does d with t / ( t ,d)  = 3 
df(t, rel,4+) ~ # tel does d with t f ( t ,d)  _> 
# tel does d 
# tel does d 
freq of term in corpus: TF(t)  = ~a tf (t ,  d) 
# does d in collection = N 
dff = # does d with t f(t ,  d) _> 1 
where dr(bin, tel, t f )  is 
1 
dff(bin, tel, t / )  ~ Ib/nl ~ dff(t, ~ ,  t f)  
tEbin 
ef  = # does d in query exp. with t f(t ,  d) > 1 ~re t is an estimate of the total number of relevant 
where: D (description), E (query expansion) documents. Since some queries have more rele- 
25 burstiness: B
Table 3: Training file schema: a record of 25 fields 
is computed for each term (ngram) in each query 
in training set. 
sumptions, when appropriate, can be very pow- 
erful (better estimates from less training data), 
but errors resulting from inappropriate assump- 
tions can outweigh the benefits. In this empirical 
investigation of term weighting we decided to use 
conservative non-parametric histogram methods 
to hedge against the risk of inappropriate para- 
metric assumptions. 
Terms are assigned to bins based on features 
such as idf, as illustrated in table 2. (Later we 
will also use B and/or ef  in the binning process.) 
is computed separately for each bin, based on the 
use of terms in relevant and irrelevant documents, 
according to the labeled training material. 
The estimation method starts with a training 
file which indicates, among other things, the num- 
ber of relevant and irrelevant documents for each 
term t in each training query, q. That is, for 
each t and q, we are are given dr(t, rel, tfo) and 
dr(t, tel, tfo), where dr(t, tel, tfo) is the number 
of relevant documents d with tf(t ,  d) = tfo, and 
df(t, rel, tfo) is the number of irrelevant docu- 
ments d with tf(t ,  d) = tfo. The schema for the 
training file is described in table 3. From these 
training observations we wish to obtain a mapping 
from bins to As that can be applied to unseen test 
material. We interpret )~ as a log likelihood ratio: 
, P(bin, tflrel) ~(bin, t / )  = ~og2-z-::-- 
~'\[bin, t / IN )  
where the numerator can be approximated as: 
,.~ _ dr(bin, rel, t f )  P(bin, triter) ~ togs 
Nrel 
vant documents than others, N~t is computed by 
averaging: 
1 
tEbin 
To ensure that Nr~l + ~"~/= N, where N is the 
number of documents in the collection, we define 
This estimation procedure is implemented with 
the simple awk program in figure 2. The awk pro- 
gram reads each line of the training file, which con- 
tains a line for each term in each training query. 
As described in table 3, each training line contains 
25 fields. The first five fields contain dr(t, tel, t f)  
for five values of t f ,  and the next five fields con- 
tain df(t, rel, t f )  for the same five values of t f .  
The next two fields contain N ,a  and N;-~. As the 
awk program reads each of these lines from the 
training file, it assigns each term in each train- 
ing query to a bin (based on \[log2(df)\], except 
when df < 100), and maintains running sums of 
the first dozen fields which are used for comput- 
ing dr(bin, rel, t f),  df(bin, re'---l, tf) ,  l~rret and I~--~ 
for five values of t f .  Finally, after reading all the 
training material, the program outputs the table 
of ks shown in table 2. The table contains a col- 
umn for each of the five t f  values and a row for 
each of the dozen idf bins. Later, we will consider 
more interesting binning rules that make use of 
additional statistics uch as burstiness and query 
expansion. 
2.1 Interpolating Between Bins 
Recall that the task is to apply the ks to new un- 
seen test data. One could simply use the ks in 
table 2 as is. That is, when we see a new term 
in the test material, we find the closest bin in ta- 
ble 2 and report the corresponding ~ value. But 
since the idf of a term in the test set could easily 
fall between two bins, it seems preferable to find 
the two closest bins and interpolate between them. 
119 
awk ' funct ion  log2(x)  { 
re turn  log(x ) / log(2)  } 
$21 - / 'D /  { N = $14; df=$15;  
# binning ru le  
if(df < I00) {bin = O} 
else {bin=int (log2 (dr)) } ; 
docfreq\[bin\] += df; 
Nbin \[bin\] ++; 
# average df(t,rel,tf), df(t,irrel,tf) 
for(i=l;i<=12;i++) n\[i,bin\]+=$i } 
END {for(bin in Nbin) { 
nbin = Nbin\[bin\] 
Nrel = n\[l l ,bin\]/nbin 
Nirrel = N-Nrel 
idf = -log2 ( (docfreq \[bin\]/nbin)/N) 
printf("Y.6.2f ", idf) 
for (i=l ; i<=5 ; i++) { 
if(Nrel==O) prel = 0 
else prel = (n\[i,bin\]/nbin)/Nrel 
if(Nirrel == O) pirrel = 0 
else pirrel = (n\[i+5,bin\]/nbin)/Nirrel 
if(prel <= 0 \]} pirrel <= O) { 
printf "Y.6s ", "NA" } 
else { 
printf "Y.6.2f ", log2(prel/pirrel)} } 
print ""}}' 
Figure 2: awk program for computing ks. 
We use linear regression to interpolate along the 
idf dimension, as illustrated in table 4. Table 4 is 
a smoothed version of table 2 where A ~ a + b.idf. 
There are five pairs of coefficients, a and b, one for 
each value of t f .  
Note that interpolation is generally not neces- 
sary on the t f  dimension because t f  is highly 
quantized. As long as t f  < 4, which it usually 
is, the closest bin is an exact match. Even when 
tff > 4, there is very little room for adjustments if 
we accept he upper limit of A < idf. 
Although we interpolate along the idf dimen- 
sion, interpolation is not all that important along 
that dimension either. Figure 1 shows that the 
differences between the test data and the train- 
ing data dominate the issues that interpolation is
attempting to deal with. The main advantage of 
regression is computational convenience; it is eas- 
ier to compute a + b. idf than to perform a binary 
search to find the closest bin. 
Previous work (Cooper et al, 1994) used mul- 
tiple regression techniques. Although our perfor- 
mance is similar (until we include query expan- 
sion) we believe that it is safer and easier to treat 
each value of t f  as a separate regression for rea- 
sons discussed in table 5. In so doing, we are ba- 
sically restricting the regression analysis to such 
an extent hat it is unlikely to do much harm (or 
much good). Imposing the limits of 0 < A _< idf 
also serves the purpose of preventing the regres- 
sion from wandering too far astray. 
tf a b 
0 -0.95 0.05 
1 -0.98 0.69 
2 -0.15 0.78 
3 0.53 0.81 
4+ 1.32 0.77 
Table 4: Regression coefficients for method fit-G. 
This table approximates the data in table 1 with 
~ a(t f )  + b(t f ) ,  idf. Note that both the inter- 
cepts, a(tf) ,  and the slopes, b(tf), increase with 
t f  (with a minor exception for b(4+)). 
tf 
0 
1 
2 
3 
4 
5 
a(tf) bit/) 
-0.95 0.05 
-0.98 0.69 
-0.15 0.78 
0.53 0.81 
1.32 0.77 
1.32 0.77 
a2 + c2. log(1 + t f )  b2 
-4.1 0.66 
-1.4 0.66 
0.18 0.66 
1.3 0.66 
2.2 0.66 
2.9 0.66 
Table 5: A comparison of the regression coeffi- 
cients for method fit-G with comparable coeffi- 
cients from the multiple regression: A = a2 + b2 ? 
idf + c2 ? log(1 + t f )  where a2 ---- -4.1,  b2 = 0.66 
and c2 = 3.9. The differences in the two fits are 
particularly large when t f  = 0; note that b(0) is 
negligible (0.05) and b2 is quite large (0.66). Re- 
ducing the number of parameters from 10 to 3 in 
this way increases the sum of square errors, which 
may or may not result in a large degradation in 
precision and recall. Why take the chance? 
3 Burs t iness  
Table 6 is like tables 4 but the binning rule not 
only uses idf, but also burstiness (B). Burstiness 
(Church and Gale, 1995)(Katz, 1996)(Church, 
2000) is intended to account for the fact that some 
very good keywords uch as "Kennedy" tend to 
be mentioned quite a few times in a document 
or not at all, whereas less good keywords uch as 
"except" tend to be mentioned about the same 
number of times no matter what the document 
tf  
0 
1 
2 
3 
4+ 
B=0 
a b 
-0.05 -0.00 -0.61 
-1.23 0.63 -0.80 
-0.76 0.71 -0.05 
0.00 0.69 0.23 
0.68 0.71 0.75 
B=i  
a b 
0.02 
0.79 
0.79 
0.82 
0.83 
Table 6: Regression coefficients for method fit-B. 
Note that the slopes and intercepts are larger when 
B = 1 than when B = 0 (except when t f  = 0). 
Even though A usually lies between-0 and idf, we 
restrict A to 0 < A < idf, just to make sure. 
120 
tf ef 
1 0 
2 0 
3 0 
4+ 0 
1 
2 
3 
4+ 
1 
2 
3 
4+ 
2 
2 
2 
2 
1 3 
2 3 
3 3 
4+ 3 
where=D 
a b 
-1.57 0.37 
-3.41 0.82 
-1.30 0.11 
0.40 0.06 
-1.84 0.87 
-2.12 1.10 
-0.66 0.95 
0.84 0.98 
-1.87 0.92 
-1.77 1.12 
-1.72 1.10 
-3.06 1.71" 
-2.52 0.95 
-1.81 1.02 
0.45 0.85 
0.38 1.22 
where=E 
a b 
-2.64 
-2.70 
-2.98 
-3.35 
-3.00 
-2.78 
-3.07 
-3.25 
0.68 
0.71 
0.74 
0.78 
0.86 
0.85 
0.93 
0.79 
-2.71 0.91 
-2.28 0.88 
-2.63 0.97 
-3.66 1.14 
Table 7: Many of the regression coefficients for 
method fit-E. (The coefficients marked with an 
asterisk are worrisome because the bins are too 
small and/or the slopes fall well outside the nor- 
mal range of 0 to 1.) The slopes rarely exceeded .8 
is previous models (fit-G and fit-B), whereas fit-E 
has more slopes closer to 1. The larger slopes are 
associated with robust conditions, e.g., terms ap- 
pearing in the query (where = D), the document 
(t f  > 1) and the expansion (el > 1). If a term 
appears in several documents brought in by query 
? expansion (el > 2), then the slope can be large 
even if the term is not explicitly mentioned in the 
query (where = E). The interactions among t f  , 
idf, ef and where are complicated and not easily 
captured with a straightforward multiple regres- 
sion. 
is about. Since "Kennedy" and "except" have 
similar idf values, they would normally receive 
similar term weights, which doesn't seem right. 
Kwok (1996) suggested average term frequency, 
avtf = TF(t)/df(t),  be used as a tie-breaker for 
cases like this, where TF(t) = ~a if(t ,  d) is the 
standard notion of frequency in the corpus-based 
NLP. Table 6 shows how Kwok's suggestion can 
be reformulated in our empirical framework. The 
table shows the slopes and intercepts for ten re- 
gressions, one for each combination of t f  and B 
(B = 1 iff avtf is large. That is, B = 1 iff 
TF(t)/df(t) > 1.83 - 0.048-idf). 
4 Query  Expans ion  
We applied query expansion (Buckley et al, 1995) 
to generate an expanded part of the query. The 
original query is referred to as the description (D) 
and the new part is referred to as the expansion 
(E). (Queries also contain a narrative (N) part that 
is not used in the experiments below so that our 
results could be compared to previously published 
results.) 
The expansion is formed by applying a base- 
line query engine (fit-B model) to the description 
part of the query. Terms that appear in the top 
k = 10 retrieved ocuments are assigned to the E 
portion of the query (where(t) = E), unless they 
were previously assigned to some other portion of 
the query (e.g., where(t) = D). All terms, t, no 
matter where they appear in the query, also re- 
ceive an expansion frequency el, an integer from 
0 to k = 10 indicating how many of the top k 
documents contain t. 
The fit-E model is: A = a(tf, where, ef) + 
b( t f , where, el) ? i df , where the regression coeffi- 
cients, a and b, not only depend on t f  as in fit-G, 
but also depend on where the term appears in the 
query and expansion frequency el.  We consider 5 
values of t f ,  2 values of where (D and E) and 6 
values of ef  (0, 1, 2, 3, 4 or more). 32 of these 
60 pairs of coefficients are shown in table 7. As 
before, most of the slopes are between 0 and 1. 
is usually between 0 and idf, but we restrict A to 
0 < A < idf, just to make sure. 
In tables 4-7, the slopes usually lie between 0 
and 1. In the previous models, fit-B and fit-G, 
the largest slopes were about 0.8, whereas in fit- 
E, the slope can be much closer to 1. The larger 
slopes are associated with very robust conditions, 
e.g., terms mentioned explicitly in all three areas of 
interest: (1) the query (where = D), (2) the doc- 
ument (t f  > 1) and (3) the expansion (el > 1). 
Under such robust conditions, we would expect o 
find very little shrinking (downweighting to com- 
pensate for uncertainty). 
On the other hand, when the term is not men- 
tioned in one of these areas, there can be quite 
a bit of shrinking. Table 7 shows that the slopes 
are generally much smaller when the term is not 
in the query (where = E) or when the term is 
not in the expansion (el = 0). However, there are 
some exceptions. The bottom right corner of ta- 
ble 7 contains ome large slopes even though these 
terms are not mentioned explicitly in the query 
(where = E). The mitigating factor in this case 
is the large el. If a term is mentioned in several 
documents in the expansion (el _> 2), then it is 
not as essential that it be mentioned explicitly in 
the query. 
With this model, as with fit-G and fit-B, ~ tends 
to increase monotonically with t f  and idf, though 
there are some interesting exceptions. When the 
term appears in the query (where = D) but not 
in the expansion (el = 0), the slopes are quite 
small (e.g., b(3,D,0) = 0.11), and the slopes actu- 
ally decrease as t f  increases (b(2, D, 0) = 0.83 > 
b(3,D,0) = 0.11). We normally expect to see 
slopes of .7 or more when t.f > 3, but in this case 
(b(3, D, 0) = 0.11), there is a considerable shrink- 
ing because we very much expected to see the term 
in the expansion and we d idn' t .  ... 
As we have seen, the interactions among t f, idf, 
e f  and where are complicated and probably de- 
121 
filter trained on sys. 
NA ? JCB1 
2+, El tf, where,ef fit-E 
2 B,tf fit-B 
2, K tf + ... BKJJBIDS 
2, K B,tf fit-B 
2, K tf  fit-G 
2, K none log(1 + t f ) .  idf 
2, K none t f .  idf 
11 
.360 
.354 
.283 
.272 
.264 
.257 
.249 
.112 
Table 8: Training helps: methods above the line 
use training (with the possible xception of JCB1); 
methods below the line do not. 
pend on many factors uch as language, collection, 
typical query patterns and so on. To cope with 
such complications, we believe that it is safer to 
use histogram methods than to try to account for 
all of these interactions at once in a single multiple 
regression. The next section will show that fit-E 
has very encouraging performance. 
5 Experiments 
Two measures of performance are reported: (1) 11 
point average precision and (2) R, precision after 
retrieving Nrd documents, where Nrd is the num- 
ber of relevant documents. We used the "short 
query" condition of the NACSIS NTCIR-1 Test 
Collection (Kando et al, 1999) which consists of 
about 300,000 documents in Japanese, plus about 
30 queries with labeled relevance judgement for 
training and 53 queries with relevance judgements 
for testing. The result of "short query" is shown in 
page 25 of(Kando et al, 1999), which shows that 
"short query" is hard for statistical methods. 
Two previously published systems are included 
in the tables below: JCB1 and BKJJBIDS. JCB1, 
submitted by Just System, a company with a com- 
mercially successful product for Japanese word- 
processing, produced the best results using sophis- 
ticated (and proprietary) natural language pro- 
cessing techniques.(Fujita, 1999) BKJJBIDS used 
Berkeley's logistic regression methods (with about 
half a dozen variables) to fit term weights to the 
labeled training material. 
Table 8 shows that training often helps. The 
methods above the line (with the possible excep- 
tion of JCB1) use training; the methods below the 
line do not. Fit-E has very respectable perfor- 
mance, nearly up to the level of JCB1, not bad for 
a purely statistical method. 
The performance of fit-B is close to that of 
BKJJBIDS. For comparison sake, fit-B is shown 
both with and without the K filter. The K filter 
restricts terms to sequences of Katakana nd Kanji 
characters. BKJJBIDS uses a similar heuristic to 
eliminate Japanese function words. Although the 
K filter does not change performance very much, 
the use of this filter changes the relative order of 
fit-B and BKJJBIDS. These results suggest hat 
R ? 2: restrict terms to bigrams explicitly men- 
.351 tioned in query (where ~- D) 
.363 ? 2+: restrict terms to bigrams, but include 
.293 where = E as well as where = D 
.282 
.282 * W: restrict terms to words, as identified by 
.267 Chasen (Matsumoto et al, 1997) 
.262 ? K: restrict terms to sequences of Katakana 
.138 and/or Kanji characters 
? B: restrict erms to bursty (B -- 1) terms 
? Ek: require terms to appear in more than k 
docs brought in by query expansion (el(t) > 
k). 
Table 9: Filters: results vary somewhat depending 
on these choices, though not too much, which is 
fortunate, since since we don't understand stop 
lists very well. 
filter trained on sys. 
2+, E1 tf, where,ef fit-E 
2+, E2 tf, where,ef fit-E 
2+, E4 tf, where,ef fit-E 
2+ tf, where,ef fit-E 
NA NA JCB1 
11 R 
.354 .363 
.350 .359 
.333 .341 
.332 .366 
.360 .351 
Table 10: The best filters (Ek) improve the per- 
formance of the best method (fit-E) to nearly the 
level of JCB1. 
the K filter is slightly unhelpful. 
A number of filters have been considered (ta- 
ble 9). Results vary somewhat depending on these 
choices, though not too much, which is fortunate, 
since since we don't understand stop lists very 
well. To the extent hat there is a pattern, we sus- 
pect that words axe slightly better than bigrams, 
and that the E filter is slightly better than the B 
filter which is slightly better than the K filter. Ta- 
ble 10 shows that the best filters (Ek) improve the 
performance of the best method (fit-E) to nearly 
the level of JCB1. 
filter sys. UL 
2 fit-B + 
2 fit-B + 
2 fit-B - 
2 fit-B - 
2 fit-G + 
2 fit-G - 
2 fit-G + 
2 fit-G - 
LL I I  
+ .283 
- .280 
+ .280 
- .275 
+ .266 
? .251 
- .248 
- .232 
R 
.293 
.296 
.296 
.288 
.279 
.268 
.259 
.249 
Table 11: Limits do no harm: two limits are 
slightly better than one, and one is  slightly bet- 
ter than none. (UL  = upper limit of ~ < idf; LL 
= lower limit of 0 _< ~) 
122 
The final experiment (table 11) shows that re- 
stricting ~ to 0 < ~ < id\] improves performance 
slightly. The combination of both the upper limit 
and the lower limit is slightly better than just one 
limit which is better than none. We view limits as 
a robustness device. Hopefully, they won't have 
to do much but every once in a while they prevent 
the system from wandering far astray. 
6 Conclusions 
This paper introduced an empirical histogram- 
based supervised learning method for estimating 
term weights, ~. Terms are assigned to bins based 
on features uch as inverse document frequency, 
burstiness and expansion frequency. A different 
is estimated for each bin and each t f  by counting 
the number of relevant and irrelevant documents 
associated with the bin and tff value. Regression 
techniques are used to interpolate between bins, 
but care is taken so that the regression cannot do 
too much harm (or too much good). Three varia- 
tions were considered: fit-G, fit-B and fit-E. The 
performance of query expansion (fit-E) is particu- 
larly encouraging. Using simple purely statistical 
methods, fit-E is nearly comparable to JCB1, a 
sophisticated natural language processing system 
developed by Just System, a leader in the Japanese 
word processing industry. 
.-: In addition to performance, we are also inter- 
ested in the interpretation of the weights. Empiri- 
cal weights tend to lie between 0 and idf. We find 
these limits to be a surprise given that standard 
term weighting formulas uch as t f .  idf generally 
do not conform to these limits. In addition, we 
find that ~ generally grows linearly with idf, and 
that the slope is between 0 and 1. We interpret the 
slope as a statistical shrink. The larger slopes are 
associated with very robust conditions, e.g., terms 
mentioned explicitly in all three areas of interest: 
(1) the query (where = D), (2) the document 
( t f  _> 1) and (3) the expansion (ef > 1). There 
is generally more shrinking for terms brought in 
by query expansion (where = E), but if a term 
is mentioned in several documents in the expan- 
sion (el > 2), then it is not as essential that the 
term be mentioned explicitly in the query. The 
interactions among t f, id\], where, B, el, etc., are 
complicated, and therefore, we have found it safer 
and easier to use histogram methods than to try 
to account for  all of the interactions at once in a 
single multiple regression. 
Acknowdedgement 
Authors thank Prof. Mitchell P. Marcus of Uni- 
versity of Pennsylvania for the valuable discussion 
about noise reduction in context of information 
retrieval. This reseach is supported by Sumitomo 
Electric. 
Re ferences  
Chris Buckley, Gerard Salton, James Allan, and Amit 
Singhal. 1995. Automatic query expansion us- 
ing smart: Trec 3. In The Third Text REtrieval 
Conference(TREC-3), pages 69-80. 
Aitao Chen, Fredric C. Gey, Kazuaki Kishida, Hailing 
Jiang, and Qun Liang. 1999. Comparing multiple 
methods for japanese and japanese-english text re- 
trieval. In NTCIR Workshop 1, pages 49-58, Tokyo 
Japan, Sep. 
Kenneth W. Church and William A. Gale. 1995. 
Poisson mixture. Natural Language Engineering, 
1(2):163-190. 
Kenneth W. Church. 2000. Empirical estimates of 
adaptation: The chance of two noriegas is closer 
to p/2 than p2. In Coling-2000, pages 180-186. 
William S. Cooper, Aitao Chen, and Fredric C. Gey. 
1994. Full text retrieval based on probabilistic equa- 
tion with coefficients fitted by logistic regressions. 
In The Second Text REtrieval Conference(TREU- 
2), pages 57-66. 
Sumio Fujita. 1999. Notes on phrasal index- 
ing: Jscb evaluation experiments at ntcir ad 
hoc'. In NTCIR Workshop 1, pages 101-108, 
http://www.rd.nacsis.ac.jp/ -ntcadm/, Sep. 
Noriko Kando, Kazuko Kuriyama, Toshihiko Nozue, 
Koji Eguchi, and Hiroyuki Katoand Souichiro Hi- 
daka. 1999. Overview of ir tasks at the first nt- 
cir workshop. In NTCIR Workshop 1, pages 11-44, 
http://www.rd.nacsis.ac.jp/ "ntcadm/, Sep. 
Slava M. Katz. 1996. Distribution of content words 
and phrases in text and language modelling. Natural 
Language Engineering, 2(1):15-59. 
K. L. Kwok. 1996. A new method of weighting query 
terms for ad-hoc retrieval. In SIGIR96, pages 187- 
195, Zurich, Switzerland. 
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, 
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki 
Imamura. 1997. Japanese morphological nalysis 
system chasen manual. Technical Report NAIST- 
IS-TR97007, NAIST, Nara, Japan, Feb. 
123 
 
	

  	
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 199?207, Prague, June 2007. c?2007 Association for Computational Linguistics
Compressing Trigram Language Models With Golomb Coding 
Ken Church 
Microsoft 
One Microsoft Way 
Redmond, WA, USA 
Ted Hart 
Microsoft 
One Microsoft Way 
Redmond, WA, USA 
Jianfeng Gao 
Microsoft 
One Microsoft Way 
Redmond, WA, USA 
 
{church,tedhar,jfgao}@microsoft.com 
 
 
Abstract 
Trigram language models are compressed 
using a Golomb coding method inspired by 
the original Unix spell program.  
Compression methods trade off space, time 
and accuracy (loss).  The proposed 
HashTBO method optimizes space at the 
expense of time and accuracy.  Trigram 
language models are normally considered 
memory hogs, but with HashTBO, it is 
possible to squeeze a trigram language 
model into a few megabytes or less.  
HashTBO made it possible to ship a 
trigram contextual speller in Microsoft 
Office 2007. 
1 Introduction 
This paper will describe two methods of com-
pressing trigram language models: HashTBO and 
ZipTBO. ZipTBO is a baseline compression me-
thod that is commonly used in many applications 
such as the Microsoft IME (Input Method Editor) 
systems that convert Pinyin to Chinese and Kana to 
Japanese. 
Trigram language models have been so success-
ful that they are beginning to be rolled out to appli-
cations with millions and millions of users: speech 
recognition, handwriting recognition, spelling cor-
rection, IME, machine translation and more.  The 
EMNLP community should be excited to see their 
technology having so much influence and visibility 
with so many people.  Walter Mossberg of the 
Wall Street Journal called out the contextual spel-
ler (the blue squiggles) as one of the most notable 
features in Office 2007: 
There are other nice additions. In Word, Out-
look and PowerPoint, there is now contextual spell 
checking, which points to a wrong word, even if 
the spelling is in the dictionary. For example, if 
you type ?their? instead of ?they're,? Office 
catches the mistake. It really works. 1 
The use of contextual language models in spel-
ling correction has been discussed elsewhere: 
(Church and Gale, 1991), (Mays et al 1991), (Ku-
kich, 1992) and (Golding and Schabes, 1996).  
This paper will focus on how to deploy such me-
thods to millions and millions of users.  Depending 
on the particular application and requirements, we 
need to make different tradeoffs among: 
1. Space (for compressed language model), 
2. Runtime (for n-gram lookup), and 
3. Accuracy (losses for n-gram estimates). 
HashTBO optimizes space at the expense of the 
other two.  We recommend HashTBO when space 
concerns dominate the other concerns; otherwise, 
use ZipTBO. 
 There are many applications where space is ex-
tremely tight, especially on cell phones.  HashTBO 
was developed for contextual spelling in Microsoft 
Office 2007, where space was the key challenge.  
The contextual speller probably would not have 
shipped without HashTBO compression.   
We normally think of trigram language models 
as memory hogs, but with HashTBO, a few mega-
bytes are more than enough to do interesting things 
with trigrams.  Of course, more memory is always 
better, but it is surprising how much can be done 
with so little.   
For English, the Office contextual speller started 
with a predefined vocabulary of 311k word types 
and a corpus of 6 billion word tokens.   (About a 
                                                 
1 
http://online.wsj.com/public/article/SB11678611102296
6326-
T8UUTIl2b10DaW11usf4NasZTYI_20080103.html?m
od=tff_main_tff_top  
199
third of the words in the vocabulary do not appear 
in the corpus.)  The vocabularies for other lan-
guages tend to be larger, and the corpora tend to be 
smaller.  Initially, the trigram language model is 
very large.  We prune out small counts (8 or less) 
to produce a starting point of 51 million trigrams, 
14 million bigrams and 311k unigrams (for Eng-
lish).  With extreme Stolcke, we cut the 51+14+0.3 
million n-grams down to a couple million.  Using a 
Golomb code, each n-gram consumes about 3 
bytes on average. 
With so much Stolcke pruning and lossy com-
pression, there will be losses in precision and re-
call.  Our evaluation finds, not surprisingly, that 
compression matters most when space is tight.  
Although HashTBO outperforms ZipTBO on the 
spelling task over a wide range of memory sizes, 
the difference in recall (at 80% precision) is most 
noticeable at the low end (under 10MBs), and least 
noticeable at the high end (over 100 MBs).  When 
there is plenty of memory (100+ MBs), the differ-
ence vanishes, as both methods asymptote to the 
upper bound (the performance of an uncompressed 
trigram language model with unlimited memory). 
2 Preliminaries 
Both methods start with a TBO (trigrams with 
backoff) LM (language model) in the standard 
ARPA format.  The ARPA format is used by many 
toolkits such as the CMU-Cambridge Statistical 
Language Modeling Toolkit.2 
2.1 Katz Backoff 
No matter how much data we have, we never 
have enough.  Nothing has zero probability.  We 
will see n-grams in the test set that did not appear 
in the training set.  To deal with this reality, Katz 
(1987) proposed backing off from trigrams to bi-
grams (and from bigrams to unigrams) when we 
don?t have enough training data.   
Backoff doesn?t have to do much for trigrams 
that were observed during training.  In that case, 
the backoff estimate of  ?(?? |???2???1)  is simply 
a discounted probability ??(??|???2???1). 
The discounted probabilities steal from the rich 
and give to the poor.  They take some probability 
mass from the rich n-grams that have been seen in 
training and give it to poor unseen n-grams that 
                                                 
2 http://www.speech.cs.cmu.edu/SLM 
might appear in test.  There are many ways to dis-
count probabilities.  Katz used Good-Turing 
smoothing, but other smoothing methods such as 
Kneser-Ney are more popular today. 
Backoff is more interesting for unseen trigrams.  
In that case, the backoff estimate is: 
? ???2???1 ??(??|???1) 
The backoff alphas (?) are a normalization fac-
tor that accounts for the discounted mass.  That is, 
 
? ???2???1 
=
1?  ?(??|???2???1)? ? :?(? ??2? ??1? ?)
1?  ?(??|???1)? ? :?(? ??2? ??1? ?)
 
 
where ? ???2???1?? > 0  simply says that the 
trigram was seen in training data. 
3 Stolcke Pruning 
Both ZipTBO and HashTBO start with Stolcke 
pruning (1998).3   We will refer to the trigram lan-
guage model after backoff and pruning as a pruned 
TBO LM. 
Stolcke pruning looks for n-grams that would 
receive nearly the same estimates via Katz backoff 
if they were removed.  In a practical system, there 
will never be enough memory to explicitly mate-
rialize all n-grams that we encounter during train-
ing.  In this work, we need to compress a large set 
of n-grams (that appear in a large corpus of 6 bil-
lion words) down to a relatively small language 
model of just a couple of megabytes. We prune as 
much as necessary to make the model fit into the 
memory allocation (after subsequent Hash-
TBO/ZipTBO compression).   
Pruning saves space by removing n-grams sub-
ject to a loss consideration: 
1. Select a threshold ?. 
2. Compute the performance loss due to prun-
ing each trigram and bigram individually us-
ing the pruning criterion. 
3. Remove all trigrams with performance loss 
less than ? 
4. Remove all bigrams with no child nodes (tri-
gram nodes) and with performance loss less 
than ?   
5. Re-compute backoff weights. 
                                                 
3 
http://www.nist.gov/speech/publications/darpa98/html/l
m20/lm20.htm  
200
Stolcke pruning uses a loss function based on 
relative entropy.  Formally, let P denote the tri-
gram probabilities assigned by the original un-
pruned model, and let P? denote the probabilities in 
the pruned model.  Then the relative entropy 
D(P||P?) between the two models is 
 
? ? ?,? [log?? ? ? ? log?(?,?)]
? ,?
 
 
where h is the history.  For trigrams, the history is 
the previous two words.  Stolcke showed that this 
reduces to 
?? ? {?(?|?) 
[log? ? ?? + log?? ? ? log?(?|?)] 
+[log??(?) ? log?(?)]  ? ? ? 
? :? ? ,? >0
} 
 
where ??(?)  is the revised backoff weight after 
pruning and h? is the revised history after dropping 
the first word.  The summation is over all the tri-
grams that were seen in training: ? ?,? > 0.  
Stolcke pruning will remove n-grams as neces-
sary, minimizing this loss. 
3.1 Compression on Top of Pruning 
After Stolcke pruning, we apply additional com-
pression (either ZipTBO or HashTBO).  ZipTBO 
uses a fairly straightforward data structure, which 
introduces relatively few additional losses on top 
of the pruned TBO model.  A few small losses are 
introduced by quantizing the log likelihoods and 
the backoff alphas, but those losses probably don?t 
matter much.  More serious losses are introduced 
by restricting the vocabulary size, V, to the 64k 
most-frequent words.  It is convenient to use byte 
aligned pointers.   The actual vocabulary of more 
than 300,000 words for English (and more for oth-
er languages) would require 19-bit pointers (or 
more) without pruning.   Byte operations are faster 
than bit operations.  There are other implementa-
tions of ZipTBO that make different tradeoffs, and 
allow for larger V without pruning losses. 
HashTBO is more heroic.  It uses a method in-
spired by McIlroy (1982) in the original Unix Spell 
Program, which squeezed a word list of N=32,000 
words into a PDP-11 address space (64k bytes).  
That was just 2 bytes per word!   
HashTBO uses similar methods to compress a 
couple million n-grams into half a dozen mega-
bytes, or about 3 bytes per n-gram on average (in-
cluding log likelihoods and alphas for backing off).  
ZipTBO is faster, but takes more space (about 4 
bytes per n-gram on average, as opposed to 3 bytes 
per n-gram).  Given a fixed memory budget, 
ZipTBO has to make up the difference with more 
aggressive Stolcke pruning.  More pruning leads to 
larger losses, as we will see, for the spelling appli-
cation.   
Losses will be reported in terms of performance 
on the spelling task.  It would be nice if losses 
could be reported in terms of cross entropy, but the 
values output by the compressed language models 
cannot be interpreted as probabilities due to quan-
tization losses and other compression losses. 
4 McIlroy?s Spell Program 
McIlroy?s spell program started with a hash ta-
ble.  Normally, we store the clear text in the hash 
table, but he didn?t have space for that, so he 
didn?t.   Hash collisions introduce losses. 
McIlroy then sorted the hash codes and stored 
just the interarrivals of the hash codes instead of 
the hash codes themselves.  If the hash codes, h, 
are distributed by a Poisson process, then the inte-
rarrivals, t, are exponentially distributed: 
 
Pr ? = ????? ,  
 
where ? =
?
?
.  Recall that the dictionary contains 
N=32,000 words.  P is the one free parameter, the 
range of the hash function.   McIlroy hashed words 
into a large integer mod P, where P is a large 
prime that trades off space and accuracy.  Increas-
ing P consumes more space, but also reduces 
losses (hash collisions). 
McIlroy used a Golomb (1966) code to store the 
interarrivals.  A Golomb code is an optimal Huff-
man code for an infinite alphabet of symbols with 
exponential probabilities. 
The space requirement (in bits per lexical entry) 
is close to the entropy of the exponential. 
 
? = ?  Pr ? log2 Pr ? ??
?
?=0
 
? =  
1
log? 2
+  log2
1
?
   
 
201
 The ceiling operator ? ?  is introduced because 
Huffman codes use an integer number of bits to 
encode each symbol. 
We could get rid of the ceiling operation if we 
replaced the Huffman code with an Arithmetic 
code, but it is probably not worth the effort. 
Lookup time is relatively slow.  Technically, 
lookup time is O(N), because one has to start at the 
beginning and add up the interarrivals to recon-
struct the hash codes.  McIlroy actually introduced 
a small table on the side with hash codes and off-
sets so one could seek to these offsets and avoid 
starting at the beginning every time.  Even so, our 
experiments will show that HashTBO is an order 
of magnitude slower than ZipTBO. 
Accuracy is also an issue.  Fortunately, we don?t 
have a problem with dropouts.  If a word is in the 
dictionary, we aren?t going to misplace it.  But two 
words in the dictionary could hash to the same val-
ue.  In addition, a word that is not in the dictionary 
could hash to the same value as a word that is in 
the dictionary.  For McIlroy?s application (detect-
ing spelling errors), the only concern is the last 
possibility.  McIlroy did what he could do to miti-
gate false positive errors by increasing P as much 
as he could, subject to the memory constraint (the 
PDP-11 address space of 64k bytes). 
We recommend these heroics when space domi-
nates other concerns (time and accuracy). 
5 Golomb Coding 
Golomb coding takes advantage of the sparseness 
in the interarrivals between hash codes.  Let?s start 
with a simple recipe.  Let t be an interarrival.   We 
will decompose t into a pair of a quotient (tq) and a 
remainder (tr).  That is, let ? = ??? + ??  where 
?? = ??/ ??  and ?? = ? mod ?.  We choose m to 
be a power of two near ? ?  
? ? 
2
 =  
?
2?
 , where 
E[t] is the expected value of the interarrivals, de-
fined below.  Store tq in unary and tr in binary. 
Binary codes are standard, but unary is not.  To 
encode a number z in unary, simply write out a 
sequence of z-1 zeros followed by a 1.  Thus, it 
takes z bits to encode the number z in unary, as 
opposed to  log2 ? bits in binary. 
This recipe consumes ?? + log2?  bits.  The 
first term is for the unary piece and the second 
term is for the binary piece. 
Why does this recipe make sense?  As men-
tioned above, a Golomb code is a Huffman code 
for an infinite alphabet with exponential probabili-
ties.  We illustrate Huffman codes for infinite al-
phabets by starting with a simple example of a 
small (very finite) alphabet with just three sym-
bols: {a, b, c}. Assume that half of the time, we 
see a, and the rest of the time we see b or c, with 
equal probabilities: 
 
Symbol Code Length Pr 
A 0 1 50% 
B 10 2 25% 
C 11 2 25% 
 
The Huffman code in the table above can be read 
off the binary tree below.   We write out a 0 when-
ever we take a left branch and a 1 whenever we 
take a right branch.  The Huffman tree is con-
structed so that the two branches are equally likely 
(or at least as close as possible to equally likely). 
 
 
 
 
Now, let?s consider an infinite alphabet where 
Pr ? =
1
2
 , Pr ? =
1
4
  and the probability of the 
t+1st symbol is Pr ? = (1? ?)??  where ? =
1
2
.  
In this case, we have the following code, which is 
simply t in unary.  That is, we write out 1?t  zeros 
followed by a 1. 
 
Symbol Code Length Pr 
A 1 1 2?1 
B 01 2 2?2 
C 001 3 2?3 
 
202
The Huffman code reduces to unary when the 
Huffman tree is left branching: 
 
 
 
In general, ? need not be ?.  Without loss of ge-
nerality, assume Pr ? =  1 ? ? ??  where 
1
2
? ? < 1 and ? ? 0.  ? depends on E[t], the ex-
pected value of the interarrivals: 
 
? ? =
?
?
=
?
1? ?
? ? =
? ? 
1 + ? ? 
 
 
Recall that the recipe above calls for expressing 
t as ? ? ?? + ??  where ?? = ?
?
?
?  and ?? =
? mod ?.  We encode tq in unary and tr
 in binary.  
(The binary piece consumes log2?  bits, since tr 
ranges from 0 to m.) 
How do we pick m?   For convenience, let m be 
a power of 2.   The unary encoding makes sense as 
a Huffman code if ?? ?
1
2
.   
Thus, a reasonable choice 4  is ? ?  
? ? 
2
 .   If 
? =
? ? 
1+? ? 
, then ?? =
? ? ?
 1+? ?  ?
? 1?
?
? ? 
.  Set-
ting ?? ?
1
2
, means ? ?
? ? 
2
. 
                                                 
4 This discussion follows slide 29 of 
http://www.stanford.edu/class/ee398a/handouts/lectures/
01-EntropyLosslessCoding.pdf.   See (Witten et al 
6  HashTBO Format 
The HashTBO format is basically the same as McI-
lroy?s format, except that McIlroy was storing 
words and we are storing n-grams.    One could 
store all of the n-grams in a single table, though we 
actually store unigrams in a separate table.  An n-
gram is represented as a key of n integers (offsets 
into the vocabulary) and two values, a log likelih-
ood and, if appropriate, an alpha for backing off.    
We?ll address the keys first. 
6.1 HashTBO Keys 
Trigrams consist of three integers (offsets into 
the Vocabulary): ?1?2?3. These three integers are 
mapped into a single hash between 0 and ? ? 1 in 
the obvious way: 
 
???? =  ?3?
0 +?2?
1 +?1?
2  mod ?  
 
where V is vocabulary size.  Bigrams are hashed 
the same way, except that the vocabulary is padded 
with an extra symbol for NA (not applicable).  In 
the bigram case, ?3 is NA. 
We then follow a simple recipe for bigrams and 
trigrams: 
1. Stolcke prune appropriately 
2. Let N be the number of n-grams 
3. Choose an appropriate P (hash range) 
4. Hash the N n-grams 
5. Sort the hash codes 
6. Take the first differences (which are mod-
eled as interarrivals of a Poisson process) 
7. Golomb code the first differences  
 
We did not use this method for unigrams, since 
we assumed (perhaps incorrectly) that we will have 
explicit likelihoods for most of them and therefore 
there is little opportunity to take advantage of 
sparseness. 
Most of the recipe can be fully automated with a 
turnkey process, but two steps require appropriate 
hand intervention to meet the memory allocation 
for a particular application: 
1. Stolcke prune appropriately, and 
2. Choose an appropriate P  
 
                                                                             
1999) and http://en.wikipedia.org/wiki/Golomb_coding, 
for similar discussion, though with slightly different 
notation.  The primary reference is (Golomb, 1966). 
203
Ideally, we?d like to do as little pruning as poss-
ible and we?d like to use as large a P as possible, 
subject to the memory allocation.  We don?t have a 
principled argument for how to balance Stolcke 
pruning losses with hashing losses; this can be ar-
rived at empirically on an application-specific ba-
sis.  For example, to fix the storage per n-gram at 
around 13 bits: 
13 =  
1
log? 2
+ log2
1
?
  
 
If we solve for ?, we obtain 0000,20/1?? .  In 
other words, set P to a prime near N000,20 and 
then do as much Stolcke pruning as necessary to 
meet the memory constraint.   Then measure your 
application?s accuracy, and adjust accordingly. 
6.2 HashTBO Values and Alphas 
There are N log likelihood values, one for each 
key.  These N values are quantized into a small 
number of distinct bins.  They are written out as a 
sequence of N Huffman codes.  If there are Katz 
backoff alphas, then they are also written out as a 
sequence of N Huffman codes.  (Unigrams and 
bigrams have alphas, but trigrams don?t.) 
6.3 HashTBO Lookup 
The lookup process is given an n-gram, 
???2???1?? , and is asked to estimate a log likelih-
ood, log Pr ??  ???2???1) .  Using the standard 
backoff model, this depends on the likelihoods for 
the unigrams, bigrams and trigrams, as well as the 
alphas. 
The lookup routine not only determines if the n-
gram is in the table, but also determines the offset 
within that table.  Using that offset, we can find the 
appropriate log likelihood and alpha.  Side tables 
are maintained to speed up random access. 
7 ZipTBO Format 
ZipTBO is a well-established representation of 
trigrams.  Detailed descriptions can be found in 
(Clarkson and Rosenfeld 1997; Whittaker and Raj 
2001). 
ZipTBO consumes 8 bytes per unigram, 5 bytes 
per bigram and 2.5 bytes per trigram.  In practice, 
this comes to about 4 bytes per n-gram on average. 
Note that there are some important interactions 
between ZipTBO and Stolcke pruning.  ZipTBO is 
relatively efficient for trigrams, compared to bi-
grams.   Unfortunately, aggressive Stolcke pruning 
generates bigram-heavy models, which don?t com-
press well with ZipTBO. 
 
 
probs 
&
weights
bounds
BIGRAM
ids
probs 
& 
weights
W[i-2]w[i-1]
W[i-2]w[i-1]w[i]
ids probs
bounds
2 1/2
TRIGRAM
UNIGRAM
ids
2 1 2
2 2 4
 
Figure 1.  Tree structure of n-grams in ZipTBO 
format, following Whittaker and Ray (2001) 
 
7.1 ZipTBO Keys 
The tree structure of the trigram model is im-
plemented using three arrays. As shown in Figure 
1, from left to right, the first array (called unigram 
array) stores unigram nodes, each of which 
branches out into bigram nodes in the second array 
(bigram array).  Each bigram node then branches 
out into trigram nodes in the third array (trigram 
array).  
The length of the unigram array is determined 
by the vocabulary size (V).  The lengths of the oth-
er two arrays depend on the number of bigrams 
and the number of trigrams, which depends on how 
aggressively they were pruned.  (We do not prune 
unigrams.) 
We store a 2-byte word id for each unigram, bi-
gram and trigram. 
The unigram nodes point to blocks of bigram 
nodes, and the bigram nodes point to blocks of tri-
gram nodes.  There are boundary symbols between 
blocks (denoted by the pointers in Figure 1).   The 
boundary symbols consume 4 bytes for each uni-
gram and 2 bytes for each bigram. 
In each block, nodes are sorted by their word 
ids. Blocks are consecutive, so the boundary value 
204
of an n?1-gram node together with the boundary 
value of its previous n?1-gram node specifies, in 
the n-gram array, the location of the block contain-
ing all its child nodes. To locate a particular child 
node, a binary search of word ids is performed 
within the block. 
 
Figure 3.  The differences between the methods in 
Figure 2 vanish if we adjust for prune size. 
7.2 ZipTBO Values and Alphas 
Like HashTBO, the log likelihood values and 
backoff alphas are quantized to a small number of 
quantization levels (256 levels for unigrams and 16 
levels for bigrams and trigrams).   Unigrams use a 
full byte for the log likelihoods, plus another full 
byte for the alphas.  Bigrams use a half byte for the 
log likelihood, plus another half byte for the al-
phas.  Trigrams use a half byte for the log likelih-
ood.  (There are no alphas for trigrams.) 
7.3 ZipTBO Bottom Line 
1. 8 bytes for each unigram:  
a. 2 bytes for a word id + 
b. 4 bytes for two boundary symbols +  
c. 1 byte for a log likelihood +  
d. 1 byte for an alpha 
2. 5 bytes for each bigram:  
a. 2 bytes for a word id +  
b. 2 bytes for a boundary symbol +  
c. ? bytes for a log likelihood + 
d. ? bytes for an alpha 
3. 2.5 bytes for each trigram: 
a. 2 bytes for a word id + 
b. ? bytes for a log likelihood 
8 Evaluation 
We normally think of trigram language models 
as memory hogs, but Figure 2 shows that trigrams 
can be squeezed down to a megabyte in a pinch.  
Of course, more memory is always better, but it is 
surprising how much can be done (27% recall at 
80% precision) with so little memory. 
Given a fixed memory budget, HashTBO out-
performs ZipTBO which outperforms StdTBO, a 
baseline system with no compression.  Compres-
sion matters more when memory is tight.  The gap 
between methods is more noticeable at the low end 
(under 10 megabytes) and less noticeable at the 
high end (over 100 megabytes), where both me-
thods asymptote to the performance of the StdTBO 
baseline. 
All methods start with Stolcke pruning.   Figure 
3 shows that the losses are largely due to pruning.  
0.25
0.35
0.45
0.55
1 10 100 1000
R
e
ca
ll 
at
 8
0
%
 P
re
ci
si
o
n
Prune Size (MBs)
HashTBO ZipTBO StdTBO
 
Figure 2. When there is plenty of memory, per-
formance (recall @ 80% precision) asymptotes to 
the performance of baseline system with no com-
pression (StdTBO).   When memory is tight, 
HashTBO >> ZipTBO >> StdTBO. 
 
 
 
Figure 4. On average, HashTBO consumes about 
3 bytes per n-gram, whereas ZipTBO consumes 4. 
0.25
0.35
0.45
0.55
1 10 100
R
e
ca
ll 
at
 8
0
%
 P
re
ci
si
o
n
Memory (MBs)
HashTBO ZipTBO StdTBO
y = 3E-06x - 0.0519
y = 4E-06x + 1.5112
0
2
4
6
8
10
12
14
0 5
0
0
,
0
0
0
1
,
0
0
0
,
0
0
0
1
,
5
0
0
,
0
0
0
2
,
0
0
0
,
0
0
0
2
,
5
0
0
,
0
0
0
3
,
0
0
0
,
0
0
0
3
,
5
0
0
,
0
0
0
4
,
0
0
0
,
0
0
0
4
,
5
0
0
,
0
0
0
Ngrams
M
e
g
a
b
y
t
e
s
HashTBO ZipTBO
205
All three methods perform about equally well, as-
suming the same amount of pruning.   
The difference is that HashTBO can store more 
n-grams in the same memory and therefore it 
doesn?t have to do as much pruning.  Figure 4 
shows that HashTBO consumes 3 bytes per n-gram 
whereas ZipTBO consumes 4. 
Figure 4 combines unigrams, bigrams and tri-
grams into a single n-gram variable.  Figure 5 drills 
down into this variable, distinguishing bigrams 
from trigrams.  The axes here have been reversed 
so we can see that HashTBO can store more of 
both kinds in less space.  Note that both HashTBO 
lines are above both ZipTBO lines.   
 
Figure 5. HashTBO stores more bigrams and tri-
grams than ZipTBO in less space. 
 
In addition, note that both bigram lines are 
above both trigram lines (triangles).  Aggressively 
pruned models have more bigrams than trigrams!   
Linear regression on this data shows that Hash-
TBO is no better than ZipTBO on trigrams (with 
the particular settings that we used), but there is a 
big difference on bigrams.  The regressions below 
model M (memory in bytes) as a function of bi and 
tri, the number of bigrams and trigrams, respec-
tively.  (Unigrams are modeled as part of the inter-
cept since all models have the same number of un-
igrams.) 
 
???????? = 0.8 + 3.4?? + 2.6??? 
??????? = 2.6 + 4.9?? + 2.6??? 
 
As a sanity check, it is reassuring that ZipTBO?s 
coefficients of 4.9 and 2.6 are close to the true val-
ues of 5 bytes per bigram and 2.5 bytes per tri-
gram, as reported in Section 7.3. 
According to the regression, HashTBO is no 
better than ZipTBO for trigrams.  Both models use 
roughly 2.6 bytes per trigram.  When trigram mod-
els have relatively few trigrams, the other coeffi-
cients matter.  HashTBO uses less space for bi-
grams (3.4 bytes/bigram << 4.9 bytes/bigram) and 
it has a better intercept (0.8 << 2.6). 
We recommend HashTBO if space is so tight 
that it dominates other concerns.  However, if there 
is plenty of space, or time is an issue, then the tra-
deoffs work out differently.   Figure 6 shows that 
ZipTBO is an order of magnitude faster than 
HashTBO.  The times are reported in microseconds 
per n-gram lookup on a dual Xeon PC with a 3.6 
ghz clock and plenty of RAM (4GB).  These times 
were averaged over a test set of 4 million lookups.  
The test process uses a cache.  Turning off the 
cache increases the difference in lookup times. 
 
Figure 6. HashTBO is slower than ZipTBO. 
9 Conclusion 
Trigram language models were compressed 
using HashTBO, a Golomb coding method 
inspired by McIlroy?s original spell program for 
Unix.  McIlroy used the method to compress a 
dictionary of 32,000 words into a PDP-11 address 
space of 64k bytes.  That is just 2 bytes per word! 
We started with a large corpus of 6 billion words 
of English.  With HashTBO, we could compress 
the trigram language model into just a couple of 
megabytes using about 3 bytes per n-gram 
(compared to 4 bytes per n-gram for the ZipTBO 
baseline).  The proposed HashTBO method is not 
fast, and it is not accurate (not lossless), but it is 
hard to beat if space is tight, which was the case 
for the contextual speller in Microsoft Office 2007. 
  
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
0 5 10 15
Memory (MB)
N
g
r
a
m
s
 
(
M
i
l
l
i
o
n
s
)
HashTBO Bigrams HashTBO Trigrams
ZipTBO Bigrams ZipTBO Trigrams
0
1
2
3
4
5
6
7
0 5 10 15
Memory (MB)
T
i
m
e
HashTBO ZipTBO
206
Acknowledgments 
We would like to thank Dong-Hui Zhang for his 
contributions to ZipTBO. 
References 
Ashok K. Chandra, Dexter C. Kozen, and Larry 
J.Stockmeyer. 1981 Alternation. Journal of the Asso-
ciation for Computing Machinery, 28(1):114-133.  
Church, K., and Gale, W. 1991 Probability Scoring for 
Spelling Correction, Statistics and Computing. 
Clarkson, P. and Robinson, T. 2001 Improved language 
modeling through better language model evaluation 
measures, Computer Speech and  Language, 15:39-
53, 2001. 
Dan Gusfield. 1997 Algorithms on Strings, Trees and 
Sequences. Cambridge University Press, Cambridge, 
UK 
Gao, J. and Zhang, M., 2002 Improving language model 
size reduction using better pruning criteria. ACL 
2002: 176-182. 
Gao, J., Goodman, J., and Miao, J. 2001 The use of 
clustering techniques for language modeling ? appli-
cation to Asian languages. Computational Linguis-
tics and Chinese Language Processing, 6:1, pp 27-
60. 
Golding, A. R. and Schabes, Y. 1996 Combining Tri-
gram-based and feature-based methods for context-
sensitive spelling correction,  ACL, pp. 71-78. 
Golomb, S.W. 1966 Run-length encodings IEEE Trans-
actions on Information Theory, 12:3, pp. 399-40. 
Goodman, J. and Gao, J. 2000 Language model size 
reduction by pruning and clustering, ICSLP-2000, 
International Conference on Spoken Language 
Processing, Beijing, October 16-20, 2000. 
Mays, E., Damerau, F. J., and Mercer, R. L. 1991 Con-
text based spelling correction. Inf. Process. Manage. 
27, 5 (Sep. 1991), pp. 517-522.  
Katz, Slava, 1987 Estimation of probabilities from 
sparse data for other language component of a 
speech recognizer. IEEE transactions on Acoustics, 
Speech and Signal Processing,  35:3, pp. 400-401. 
Kukich, Karen, 1992 Techniques for automatically cor-
recting words in text, Computing Surveys, 24:4, pp. 
377-439. 
M. D. McIlroy, 1982 Development of a spelling list, 
IEEE Trans. on Communications 30 pp. 91-99.  
Seymore, K., and Rosenfeld, R. 1996 Scalable backoff 
language models. Proc. ICSLP, Vol. 1, pp.232-235. 
Stolcke, A. 1998 Entropy-based Pruning of Backoff Lan-
guage Models. Proc. DARPA News Transcription and 
Understanding Workshop, 1998, pp. 270--274, Lans-
downe, VA. 
Whittaker, E. and Ray, B. 2001 Quantization-based lan-
guage model compression. Proc. Eurospeech, pp. 
33-36. 
Witten, I. H., Moffat, A., and Bell, T. C. 1999 Manag-
ing Gigabytes (2nd Ed.): Compressing and Indexing 
Documents and Images. Morgan Kaufmann Publish-
ers Inc. 
207
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 708?715, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Sketches to Estimate Associations
Ping Li
Department of Statistics
Stanford University
Stanford, California 94305
pingli@stat.stanford.edu
Kenneth W. Church
Microsoft Research
One Microsoft Way
Redmond, Washington 98052
church@microsoft.com
Abstract
We should not have to look at the en-
tire corpus (e.g., the Web) to know if two
words are associated or not.1 A powerful
sampling technique called Sketches was
originally introduced to remove duplicate
Web pages. We generalize sketches to
estimate contingency tables and associa-
tions, using a maximum likelihood esti-
mator to find the most likely contingency
table given the sample, the margins (doc-
ument frequencies) and the size of the
collection. Not unsurprisingly, computa-
tional work and statistical accuracy (vari-
ance or errors) depend on sampling rate,
as will be shown both theoretically and
empirically. Sampling methods become
more and more important with larger and
larger collections. At Web scale, sampling
rates as low as 10?4 may suffice.
1 Introduction
Word associations (co-occurrences) have a wide
range of applications including: Speech Recogni-
tion, Optical Character Recognition and Information
Retrieval (IR) (Church and Hanks, 1991; Dunning,
1993; Manning and Schutze, 1999). It is easy to
compute association scores for a small corpus, but
more challenging to compute lots of scores for lots
of data (e.g. the Web), with billions of web pages
(D) and millions of word types (V ). For a small
corpus, one could compute pair-wise associations by
multiplying the (0/1) term-by-document matrix with
its transpose (Deerwester et al, 1999). But this is
probably infeasible at Web scale.
1This work was conducted at Microsoft while the first author
was an intern. The authors thank Chris Meek, David Hecker-
man, Robert Moore, Jonathan Goldstein, Trevor Hastie, David
Siegmund, Art Own, Robert Tibshirani and Andrew Ng.
Approximations are often good enough. We
should not have to look at every document to de-
termine that two words are strongly associated. A
number of sampling-based randomized algorithms
have been implemented at Web scale (Broder, 1997;
Charikar, 2002; Ravichandran et al, 2005).2
A conventional random sample is constructed by
selecting Ds documents from a corpus of D doc-
uments. The (corpus) sampling rate is DsD . Of
course, word distributions have long tails. There
are a few high frequency words and many low fre-
quency words. It would be convenient if the sam-
pling rate could vary from word to word, unlike con-
ventional sampling where the sampling rate is fixed
across the vocabulary. In particular, in our experi-
ments, we will impose a floor to make sure that the
sample contains at least 20 documents for each term.
(When working at Web scale, one might raise the
floor somewhat to perhaps 104.)
Sampling is obviously helpful at the top of the
frequency range, but not necessarily at the bottom
(especially if frequencies fall below the floor). The
question is: how about ?ordinary? words? To answer
this question, we randomly picked 15 pages from
a Learners? dictionary (Hornby, 1989), and selected
the first entry on each page. According to Google,
there are 10 million pages/word (median value, ag-
gregated over the 15 words), no where near the floor.
Sampling can make it possible to work in mem-
ory, avoiding disk. At Web scale (D ? 10 billion
pages), inverted indexes are large (1500 GBs/billion
pages)3, probably too large for memory. But a sam-
ple is more manageable; the inverted index for a
10?4 sample of the entire web could fit in memory
on a single PC (1.5 GB).
2http://labs.google.com/sets produces fascinating sets, al-
though we don?t know how it works. Given the seeds, ?Amer-
ica? and ?China,? http://labs.google.com/sets returns: ?Amer-
ica, China, Japan, India, Italy, Spain, Brazil, Persia, Europe,
Australia, France, Asia, Canada.?
3This estimate is extrapolated from Brin and Page (1998),
who report an inverted index of 37.2 GBs for 24 million pages.
708
Table 1: The number of intermediate results after the
first join can be reduced from 504,000 to 120,000,
by starting with ?Schwarzenegger & Austria? rather
than the baseline (?Schwarzenegger & Terminator?).
The standard practice of starting with the two least
frequent terms is a good rule of thumb, but one can
do better, given (estimates of) joint frequencies.
Query Hits (Google)
Austria 88,200,000
Governor 37,300,000
Schwarzenegger 4,030,000
Terminator 3,480,000
Governor & Schwarzenegger 1,220,000
Governor & Austria 708,000
Schwarzenegger & Terminator 504,000
Terminator & Austria 171,000
Governor & Terminator 132,000
Schwarzenegger & Austria 120,000
1.1 An Application: The Governator
Google returns the top k hits, plus an estimate of
how many hits there are. Table 1 shows the number
of hits for four words and their pair-wise combina-
tions. Accurate estimates of associations would have
applications in Database query planning (Garcia-
Molina et al, 2002). Query optimizers construct a
plan to minimize a cost function (e.g., intermediate
writes). The optimizer could do better if it could
estimate a table like Table 1. But efficiency is im-
portant. We certainly don?t want to spend more time
optimizing the plan than executing it.
Suppose the optimizer wanted to construct a plan
for the query: ?Governor Schwarzenegger Termi-
nator Austria.? The standard solution starts with
the two least frequent terms: ?Schwarzenegger? and
?Terminator.? That plan generates 504,000 interme-
diate writes after the first join. An improvement
starts with ?Schwarzenegger? with ?Austria,? reduc-
ing the 504,000 down to 120,000.
In addition to counting hits, Table 1 could also
help find the top k pages. When joining the first pair
of terms, we?d like to know how far down the rank-
ing we should go. Accurate estimates of associations
would help the optimizer make such decisions.
It is desirable that estimates be consistent, as well
as accurate. Google, for example, reports 6 million
hits for ?America, China, Britain,? and 23 million for
?America, China, Britain, Japan.? Joint frequencies
decrease monotonically: s ? S =? hits(s) ? hits(S).
f = a + c
f = a + b
D = a+b+c+d
a
c
y ~y
~x
x
d y
xb
(a)
x
~x
y ~y
a b
c d y
x
s s s s
s s
ss
n  = a + b
sD = a +b + c +d
n  = a + c
s s
ss
(b)
Figure 1: (a): A contingency table for word x and
word y. Cell a is the number of documents that con-
tain both x and y, b is the number that contain x but
not y, c is the number that contain y but not x, and
d is the number that contain neither x nor y. The
margins, fx = a + b and fy = a + c are known as
document frequencies in IR. D is the total number
of documents in the collection. (b): A sample con-
tingency table, with ?s? indicating the sample space.
1.2 Sampling and Estimation
Two-way associations are often represented as two-
way contingency tables (Figure 1(a)). Our task is to
construct a sample contingency table (Figure 1(b)),
and estimate 1(a) from 1(b). We will use a max-
imum likelihood estimator (MLE) to find the most
likely contingency table, given the sample and vari-
ous other constraints. We will propose a sampling
procedure that bridges two popular choices: (A)
sampling over documents and (B) sampling over
postings. The estimation task is straightforward and
well-understood for (A). As we consider more flexi-
ble sampling procedures such as (B), the estimation
task becomes more challenging.
Flexible sampling procedures are desirable. Many
studies focus on rare words (Dunning, 1993; Moore,
2004); butterflies are more interesting than moths.
The sampling rate can be adjusted on a word-by-
word basis with (B), but not with (A). The sampling
rate determines the trade-off between computational
work and statistical accuracy.
We assume a standard inverted index. For each
word x, there are a set of postings, X. X contains a
set of document IDs, one for each document contain-
ing x. The size of postings, fx = |X|, corresponds
to the margins of the contingency tables in Figure
1(a), also known as document frequencies in IR.
The postings lists are approximated by sketches,
skX, first introduced by Broder (1997) for remov-
ing duplicate web pages. Assuming that document
IDs are random (e.g., achieved by a random permu-
tation), we can compute skX, a random sample of
709
X, by simply selecting the first few elements of X.
In Section 3, we will propose using sketches
to construct sample contingency tables. With this
novel construction, the contingency table (and sum-
mary statistics based on the table) can be estimated
using conventional statistical methods such as MLE.
2 Broder?s Sketch Algorithm
One could randomly sample two postings and inter-
sect the samples to estimate associations. The sketch
technique introduced by Broder (1997) is a signifi-
cant improvement, as demonstrated in Figure 2.
Assume that each document in the corpus of size
D is assigned a unique random ID between 1 and D.
The postings for word x is a sorted list of fx doc IDs.
The sketch, skX, is the first (smallest) sx doc IDs in
X. Broder used MINs(Z) to denote the s smallest
elements in the set, Z . Thus, skX = MINsx(X).
Similarly, Y denotes the postings for word y, and
skY denotes its sketch, MINsy(Y ). Broder assumed
sx = sy = s.
Broder defined resemblance (R) and sample re-
semblance (Rs) to be:
R = aa + b + c , Rs =
|MINs(skX ? skY ) ? skX ? skY |
|MINs(skX ? skY )|
.
Broder (1997) proved that Rs is an unbiased esti-
mator of R. One could use Rs to estimate a but he
didn?t do that, and it is not recommended.4
Sketches were designed to improve the coverage
of a, as illustrated by Monte Carlo simulation in Fig-
ure 2. The figure plots, E
(as
a
)
, percentage of inter-
sections, as a function of (postings) sampling rate,
s
f , where fx = fy = f , sx = sy = s. The solid lines
(sketches), E (asa
)
? sf , are above the dashed curve
(random sampling), E (asa
)
= s2f2 . The difference is
particularly important at low sampling rates.
3 Generalizing Sketches: R? Tables
Sketches were first proposed for estimating resem-
blance (R). This section generalizes the method to
construct sample contingency tables, from which we
can estimate associations: R, LLR, cosine, etc.
4There are at least three problems with estimating a from
Rs. First, the estimate is biased. Secondly, this estimate uses
just s of the 2 ? s samples; larger samples ? smaller errors.
Thirdly, we would rather not impose the restriction: sx = sy.
0  0.2 0.4 0.6 0.8 10
0.5
1
Sampling rates
Pe
rc
en
ta
ge
 o
f i
ne
rs
ec
tio
ns
Random sampling
Sketch
Figure 2: Sketches (solid curves) dominate random
sampling (dashed curve). a=0.22, 0.38, 0.65, 0.80,
0.85f , f=0.2D, D=105. There is only one dashed
curve across all values of a. There are different but
indistinguishable solid curves depending on a.
Recall that the doc IDs span the integers from 1
to D with no gaps. When we compare two sketches,
skX and skY , we have effectively looked at Ds =
min{skX(sx), skY(sy)} documents, where skX(j) is
the jth smallest element in skX. The following
construction generates the sample contingency ta-
ble, as, bs, cs, ds (as in Figure 1(b)). The example
shown in Figure 3 may help explain the procedure.
Ds = min{skX(sx), skY(sy)}, as = |skX ? skY |,
nx = sx ? |{j : skX(j) > Ds}|,
ny = sy ? |{j : skY(j) > Ds}|,
bs = nx ? as, cs = ny ? as, ds = Ds ? as ? bs ? cs.
Given the sample contingency table, we are now
ready to estimate the contingency table. It is suffi-
cient to estimate a, since the rest of the table can be
determined from fx, fy and D. For practical appli-
cations, we recommend the convenient closed-form
approximation (8) in Section 5.1.
4 Margin-Free (MF) Baseline
Before considering the proposed MLE method, we
introduce a baseline estimator that will not work as
well because it does not take advantage of the mar-
gins. The baseline is the multivariate hypergeomet-
ric model, usually simplified as a multinomial by as-
suming ?sample-with-replacement.?
The sample expectations are (Siegrist, 1997),
E(as) = DsD a, E(bs) =
Ds
D b,
E(cs) = DsD c, E(ds) =
Ds
D d. (1)
710
Y:  2   4   5   8   15    19   21     24   27   28   31 
f
X:  3   4   7   9   10   15   18      19   24   25   28
= 11 = 5 = 18f a Dx y = 11 s
= 5= 7= 7sy= 7sx
b c= 5= 2as s s= 3
n nx y
ds = 8
(a)
    
 
9     10     11    12    13    14    15   16  
1      2      3      4      5      6      7      8   
17   18     19    20    . . . . . .             D = 36 
(b)
Figure 3: (a): The two sketches, skX and skY
(larger shaded box), are used to construct a sam-
ple contingency table: as, bs, cs, ds. skX consists
of the first sx = 7 doc IDs in X, the postings for
word x. Similarly, skY consists of the first sy = 7
doc IDs in Y , the postings for word y. There are 11
doc IDs in both X and Y , and a = 5 doc IDs in
the intersection: {4, 15, 19, 24, 28}. (a) shows that
Ds = min(18, 21) = 18. Doc IDs 19 and 21 are
excluded because we cannot determine if they are in
the intersection or not, without looking outside the
box. As it turns out, 19 is in the intersection and
21 is not. (b) enumerates the Ds = 18 documents,
showing which documents contain x (small circles)
and which contain y (small squares). Both proce-
dures, (a) and (b), produce the same sample contin-
gency table: as = 2, bs = 5, cs = 3 and ds = 8.
The margin-free estimator and its variance are
a?MF =
D
Ds
as, Var(a?MF ) =
D
Ds
1
1
a + 1D?a
D ?Ds
D ? 1 . (2)
For the multinomial simplification, we have
a?MF,r = DDs
as, Var(a?MF,r) = DDs
1
1
a + 1D?a
. (3)
where ?r? indicates ?sample-with-replacement.?
The term D?DsD?1 ? D?DsD is often called the
?finite-sample correction factor? (Siegrist, 1997).
5 The Proposed MLE Method
The task is to estimate the contingency table from
the samples, the margins and D. We would like to
use a maximum likelihood estimator for the most
probable a, which maximizes the (full) likelihood
(probability mass function, PMF) P (as, bs, cs, ds; a).
Unfortunately, we do not know the exact expres-
sion for P (as, bs, cs, ds; a), but we do know the con-
ditional probability P (as, bs, cs, ds|Ds; a). Since the
doc IDs are uniformly random, sampling the first
Ds contiguous documents is statistically equivalent
to randomly sampling Ds documents from the cor-
pus. Based on this key observation and Figure 3,
conditional on Ds, P (as, bs, cs, ds|Ds; a) is the PMF
of a two-way sample contingency table.
We factor the full likelihood into:
P (as, bs, cs, ds; a) = P (as, bs, cs, ds|Ds; a)? P (Ds; a).
P (Ds; a) is difficult. However, since we do not ex-
pect a strong dependency of Ds on a, we maxi-
mize the partial likelihood instead, and assume that
is good enough. An example of partial likelihood is
the Cox proportional hazards model in survival anal-
ysis (Venables and Ripley, 2002, Section 13.3) .
Our partial likelihood is
P (as, bs, cs, ds|Ds; a) =
` a
as
?`fx?a
bs
?`fy?a
cs
?`D?fx?fy+a
ds
?
`D
Ds
?
?
as?1
Y
i=0
(a? i) ?
bs?1
Y
i=0
(fx ? a? i) ?
cs?1
Y
i=0
(fy ? a? i)
?
ds?1
Y
i=0
(D ? fx ? fy + a? i), (4)
where
(n
m
)
= n!m!(n?m)! . ??? is ?proportional to.?
We now derive an MLE for (4), a result that was
not previously known, to the best of our knowledge.
Let a?MLE maximizes logP (as, bs, cs, ds|Ds; a):
as?1
X
i=0
log(a? i) +
bs?1
X
i=0
log (fx ? a? i)
+
cs?1
X
i=0
log (fy ? a? i) +
ds?1
X
i=0
log (D ? fx ? fy + a? i) ,
whose first derivative, ? logP (as,bs,cs,ds|Ds;a)?a , is
as?1
X
i=0
1
a? i ?
bs?1
X
i=0
1
fx ? a? i
?
cs?1
X
i=0
1
fy ? a? i
+
ds?1
X
i=0
1
D ? fx ? fy + a? i
. (5)
Since the second derivative, ?
2 logP (as,bs,cs,ds|Ds;a)
?a2 ,
is negative, the log likelihood function is concave,
hence has a unique maximum. One could numeri-
cally solve (5) for ? logP (as,bs,cs,ds|Ds;a)?a = 0. How-
ever, we derive the exact solution using the follow-
ing updating formula from (4):
711
P (as, bs, cs, ds|Ds; a) = P (as, bs, cs, ds|Ds; a? 1)?
fx ? a + 1? bs
fx ? a + 1
fy ? a + 1? cs
fy ? a + 1
D ? fx ? fy + a
D ? fx ? fy + a? ds
a
a? as
= P (as, bs, cs, ds|Ds; a? 1)? g(a). (6)
Since our MLE is unique, it suffices to find a from
g(a) = 1, which is a cubic function in a.
5.1 A Convenient Practical Approximation
Rather than solving the cubic equation for the ex-
act MLE, the following approximation may be more
convenient. Assume we sample nx = as + bs from
X and obtain as co-occurrences without knowledge
of the samples from Y . Further assuming ?sample-
with-replacement,? as is then binomially distributed,
as ? Binom(nx, afx ). Similarly, assume as ?
Binom(ny, afy ). Under these assumptions, the PMF
of as is a product of two binomial PMFs:
 
fx
nx
!
? a
fx
?as ?fx ? a
fx
?bs
 
fy
ny
!
? a
fy
?as ?fy ? a
fy
?cs
? a2as (fx ? a)bs (fy ? a)cs . (7)
Setting the first derivative of the logarithm of (7) to
be zero, we obtain 2asa ? bsfx?a ?
cs
fy?a = 0, which is
quadratic in a and has a solution:
a?MLE,a = fx (2as + cs) + fy (2as + bs)2 (2as + bs + cs)
?
q
(fx (2as + cs)? fy (2as + bs))2 + 4fxfybscs
2 (2as + bs + cs)
. (8)
Section 6 shows that a?MLE,a is very close to a?MLE .
5.2 Theoretical Evaluation: Bias and Variance
How good are the estimates? A popular metric
is mean square error (MSE): MSE(a?) = E (a?? a)2 =
Var (a?) +Bias2 (a?). If a? is unbiased, MSE(a?) =Var (a?) =
SE2 (a?), where SE is the standard error. Here all ex-
pectations are conditional on Ds.
Large sample theory (Lehmann and Casella,
1998, Chapter 6) says that, under ?sample-with-
replacement,? a?MLE is asymptotically unbiased and
converges to Normal with mean a and variance 1I(a) ,
where I(a), the Fisher Information, is
I(a) = ?E
?
?2
?a2 logP (as, bs, cs, ds|Ds; a, r)
?
. (9)
Under ?sample-with-replacement,? we have
P (as, bs, cs, ds|Ds; a, r) ?
? a
D
?as
?
?fx ? a
D
?bs
?
?
fy ? a
D
?cs
?
?
D ? fx ? fy + a
D
?ds
, (10)
Therefore, the Fisher Information, I(a), is
E(as)
a2 +
E(bs)
(fx ? a)2
+ E(cs)
(fy ? a)2
+ E(ds)
(D ? fx ? fy + a)2
.
(11)
We plug (1) from the margin-free model into (11)
as an approximation, to obtain
Var (a?MLE) ?
D
Ds ? 1
1
a + 1fx?a +
1
fy?a +
1
D?fx?fy+a
, (12)
which is 1I(a) multiplied by
D?Ds
D , the ?finite-
sample correction factor,? to consider ?sample-
without-replacement.?
We can see that Var (a?MLE) is less than
Var (a?MF ) in (2). In addition, a?MLE is asymptoti-
cally unbiased while a?MF is no longer unbiased un-
der margin constraints. Therefore, we expect a?MLE
has smaller MSE than a?MF . In other words, the pro-
posed MLE method is more accurate than the MF
baseline, in terms of variance, bias and mean square
error. If we know the margins, we ought to use them.
5.3 Unconditional Bias and Variance
a?MLE is also unconditionally unbiased:
E (a?MLE ? a) = E (E (a?MLE ? a|Ds)) ? E(0) = 0. (13)
The unconditional variance is useful because often
we would like to estimate the errors before knowing
Ds (e.g., for choosing sample sizes).
To compute the unconditional variance of a?MLE ,
we should replace DDs with E
(
D
Ds
)
in (12). We
resort to an approximation for E
?
D
Ds
?
. Note that
skX(sx) is the order statistics of a discrete random
variable (Siegrist, 1997) with expectation
E
`
skX(sx)
?
= sx(D + 1)fx + 1
? sxfx
D. (14)
By Jensen?s inequality, we know that
E
?
Ds
D
?
? min
 
E
`
skX(sx)
?
D ,
E
`
skY(sy)
?
D
!
= min
?
sx
fx
, syfy
?
(15)
E
? D
Ds
?
? 1
E
`Ds
D
? ? max
?fx
sx
, fysy
?
. (16)
712
Table 2: Gold standard joint frequencies, a. Docu-
ment frequencies are shown in parentheses. These
words are frequent, suitable for evaluating our algo-
rithms at very low sampling rates.
THIS HAVE HELP PROGRAM
THIS (27633) ? 13517 7221 3682
HAVE (17396) 13517 ? 5781 3029
HELP (10791) 7221 5781 ? 1949
PROGRAM (5327) 3682 3029 1949 ?
Replacing the inequalities with equalities underes-
timates the variance, but only slightly.
5.4 Smoothing
Although not a major emphasis here, our evalua-
tions will show that a?MLE+S , a smoothed version
of the proposed MLE method, is effective, espe-
cially at low sampling rates. a?MLE+S uses ?add-
one? smoothing. Given that such a simple method
is as effective as it is, it would be worth considering
more sophisticated methods such as Good-Turing.
5.5 How Many Samples Are Sufficient?
The answer depends on the trade-off between com-
putation and estimation errors. One simple rule is
to sample ?2%.? (12) implies that the standard er-
ror is proportional to
p
D/Ds ? 1. Figure 4(a) plots
p
D/Ds ? 1 as a function of sampling rate, Ds/D, in-
dicating a ?elbow? about 2%. However, 2% is too
large for high frequency words.
A more reasonable metric is the ?coefficient of
variation,? cv = SE(a?)a . At Web scale (10 billion
pages), we expect that a very small sampling rate
such as 10?4 or 10?5 will suffice to achieve a rea-
sonable cv (e.g., 0.5). See Figure 4(b).
6 Evaluation
Two sets of experiments were run on a collection of
D = 216 web pages, provided by MSN. The first ex-
periment considered 4 English words shown in Ta-
ble 2, and the second experiment considers 968 En-
glish words with mean df = 2135 and median df =
1135. They form 468,028 word pairs, with mean co-
occurrences = 188 and median = 74.
6.1 Small Dataset Monte Carlo Experiment
Figure 5 evaluates the various estimate methods by
MSE over a wide range of sampling rates. Doc IDs
0   0.02 0.05 0.1 0.150
10
20
30
Samplig rates
R
el
at
iv
e 
SE
(a)
105 106 107 108 109 1010
10?5
10?3
10?1
100
 D
Sa
m
pl
in
g 
ra
te
s
 f
x
 = 0.0001?D
 f
x
 =0.01?D
0.001
 fy = 0.1? fx
 a = 0.05 ? fy
(b)
Figure 4: How large should the sampling rate be?
(a): We can sample up to the ?elbow point? (2%),
but after that there are diminishing returns. (b): An
analysis based on cv = SEa = 0.5 suggests that we can
get away with much lower sampling rates. The three
curves plot the critical value for the sampling rate,
Ds
D , as a function of corpus size, D. At Web scale,
D ? 1010, sampling rates above 10?3 to 10?5 sat-
isfy cv < 0.5, at least for these settings of fx, fy
and a. The settings were chosen to simulate ?ordi-
nary? words. The three curves correspond to three
choices of fx: D/100, D/1000, and D/10, 000.
fy = fx/10, a = fy/20. SE is based on (12).
were randomly permuted 105 times. For each per-
mutation we constructed sketches from the inverted
index at a series of sampling rates. The figure shows
that the proposed method, a?MLE , is considerably
better (by 20% ? 40%) than the margin-free base-
line, a?MF . Smoothing is effective at low sampling
rates. The recommended approximation, a?MLE,a, is
remarkably close to the exact solution.
Figure 6 shows agreement between the theoreti-
cal and empirical unconditional variances. Smooth-
ing reduces variances, at low sampling rates. We
used the empirical E
?
D
DS
?
to compute the theoreti-
cal variances. The approximation, max
(
fx
sx ,
fy
sy
)
, is
> 0.95E
?
D
DS
?
at sampling rates > 0.01.
Figure 7 verifies that the proposed MLE is unbi-
ased, unlike the margin-free baselines.
6.2 Large Dataset Experiment
The large experiment considers 968 English words
(468,028 pairs) over a range of sampling rates. A
floor of 20 was imposed on sample sizes.
As reported in Figure 8, the large experiment con-
firms once again that proposed method, a?MLE , is
considerably better than the margin-free baseline (by
713
0.001 0.01 0.1 10
0.2
0.4
N
or
m
al
iz
ed
 M
SE
0.
5
MF
MLE,a
MLE
MLE+S
IND
THIS ? HAVE
0.001 0.01 0.1 10
0.2
0.4
THIS ? HELP
0.001 0.01 0.1 10
0.2
0.4
N
or
m
al
iz
ed
 M
SE
0.
5
THIS ? PROGRAM
0.001 0.01 0.1 10
0.2
0.4 HAVE ? HELP
0.001 0.01 0.1 10
0.2
0.4
0.5
N
or
m
al
iz
ed
 M
SE
0.
5
Sampling rates
HAVE ? PROGRAM
IND
MF
MLE+S
MLE,a
MLE
0.001 0.01 0.1 10
0.2
0.4
0.6
Sampling rates
HELP ? PROGRAM
Figure 5: The proposed method, a?MLE outperforms
the margin-free baseline, a?MF , in terms of MSE
0.5
a .
The recommended approximation, a?MLE,a, is close
to a?MLE . Smoothing, a?MLE+S , is effective at low
sampling rates. All methods are better than assum-
ing independence (IND).
15% ? 30%). The recommended approximation,
a?MLE,a, is close to a?MLE . Smoothing, a?MLE+S
helps at low sampling rates.
6.3 Rank Retrieval: Top k Associated Pairs
We computed a gold standard similarity cosine rank-
ing of the 468,028 pairs using a 100% sample: cos =
a?
fxfy
. We then compared the gold standard to rank-
ings based on smaller samples. Figure 9(a) com-
pares the two lists in terms of agreement in the top k.
For 3 ? k ? 200, with a sampling rate of 0.005, the
agreement is consistently 70% or higher. Increasing
sampling rate, increases agreement.
The same comparisons are evaluated in terms of
precision and recall in Figure 9(b), by fixing the top
1% of the gold standard list but varying the top per-
centages of the sample list. Again, increasing sam-
pling rate, increases agreement.
0.001 0.01 0.1 10
0.1
0.2
Sampling rates
N
or
m
al
iz
ed
 s
ta
nd
ar
d 
er
ro
r
MLE
MLE+S
Theore.
HAVE ? PROGRAM
0.001 0.01 0.1 10
0.1
0.2
0.3
0.4
Sampling rates
MLE
MLE+S
Theore.
HELP ? PROGRAM
Figure 6: The theoretical and empirical variances
show remarkable agreement, in terms of SE(a?)a .
Smoothing reduces variances at low sampling rates.
0.001 0.01 0.1 10  
0.02
0.05
Sampling rates
N
or
m
al
iz
ed
 a
bs
ol
ut
e 
bi
as
HAVE ? PROGRAM
MF
MLE
MLE+S
0.001 0.01 0.1 10  
0.2
0.04
0.06
Sampling rates
HELP ? PROGRAM
MLE+S
MF
MLE
Figure 7: Biases in terms of |E(a?)?a|a . a?MLE is prac-
tically unbiased, unlike a?MF . Smoothing increases
bias slightly.
7 Conclusion
We proposed a novel sketch-based procedure for
constructing sample contingency tables. The
method bridges two popular choices: (A) sam-
pling over documents and (B) sampling over post-
ings. Well-understood maximum likelihood estima-
tion (MLE) techniques can be applied to sketches
(or to traditional samples) to estimate word associa-
tions. We derived an exact cubic solution, a?MLE , as
well as a quadratic approximation, a?MLE,a. The ap-
proximation is recommended because it is close to
the exact solution, and easy to compute.
The proposed MLE methods were compared em-
pirically and theoretically to a margin-free (MF)
baseline, finding large improvements. When we
know the margins, we ought to use them.
Sample-based methods (MLE & MF) are often
better than sample-free methods. Associations are
often estimated without samples. It is popular to
assume independence: (Garcia-Molina et al, 2002,
Chapter 16.4), i.e., a? ? fxfyD . Independence led to
large errors in our experiments.
Not unsurprisingly, there is a trade-off between
computational work (space and time) and statistical
714
0.001 0.01 0.1 10
0.2
0.4
0.6
Sampling rates
R
el
at
iv
e 
av
g.
 a
bs
. e
rro
r IND
MLE+S
MLE
MF
MLE,a
Figure 8: We report the (normalized) mean absolute
errors (divided by the mean co-occurrences, 188).
All curves are averaged over three permutations.
The proposed MLE and the recommended approxi-
mation are very close and both are significantly bet-
ter than the margin-free (MF) baseline. Smoothing,
a?MLE+S , helps at low sampling rates. All estima-
tors do better than assuming independence.
accuracy (variance or errors); reducing the sampling
rate saves work, but costs accuracy. We derived
formulas for variance, showing precisely how accu-
racy depends on sampling rate. Sampling methods
become more and more important with larger and
larger collections. At Web scale, sampling rates as
low as 10?4 may suffice for ?ordinary? words.
We have recently generalized the sampling algo-
rithm and estimation method to multi-way associa-
tions; see (Li and Church, 2005).
References
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. In Proceedings
of the Seventh International World Wide Web Confer-
ence, pages 107?117, Brisbane, Australia.
A. Broder. 1997. On the resemblance and containment
of documents. In Proceedings of the Compression and
Complexity of Sequences, pages 21?29, Positano, Italy.
M. S. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of the thiry-
fourth annual ACM symposium on Theory of comput-
ing, pages 380?388, Montreal, Quebec, Canada.
K. Church and P. Hanks. 1991. Word association norms,
mutual information and lexicography. Computational
Linguistics, 16(1):22?29.
S. Deerwester, S. T. Dumais, G. W. Furnas, and T. K.
Landauer. 1999. Indexing by latent semantic analy-
3 10 100 2000  
20
40
60
80
100
Top
Pe
rc
en
ta
ge
 o
f a
gr
ee
m
en
t ( 
% 
)
0.5
0.005
(a)
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Recall
Pr
ec
is
io
n
Top 1 %
0.005 0.01
0.03
0.02
0.5
(b)
Figure 9: (a): Percentage of agreements in the gold
standard and reconstructed (from samples) top 3 to
200 list. (b):Precision-recall curves in retrieving the
top 1% gold standard pairs, at different sampling
rates. For example, 60% recall and 70% precision
is achieved at sampling rate = 0.02.
sis. Journal of the American Society for Information
Science, 41(6):391?407.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
H. Garcia-Molina, J. D. Ullman, and J. D. Widom. 2002.
Database Systems: the Complete Book. Prentice Hall,
New York, NY.
A. S. Hornby, editor. 1989. Oxford Advanced Learner?s
Dictionary. Oxford University Press, Oxford, UK.
E. L. Lehmann and G. Casella. 1998. Theory of Point
Estimation. Springer, New York, NY, second edition.
P. Li and K. W. Church. 2005. Using sketches to esti-
mate two-way and multi-way associations. Technical
report, Microsoft Research, Redmond, WA.
C. D. Manning and H. Schutze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press, Cambridge, MA.
R. C. Moore. 2004. On log-likelihood-ratios and the
significance of rare events. In Proceedings of EMNLP
2004, pages 333?340, Barcelona, Spain.
D. Ravichandran, P. Pantel, and E. Hovy. 2005. Ran-
domized algorithms and NLP: Using locality sensitive
hash function for high speed noun clustering. In Pro-
ceedings of ACL, pages 622?629, Ann Arbor.
K. Siegrist. 1997. Finite Sampling Models,
http://www.ds.unifi.it/VL/VL EN/urn/index.html. Vir-
tual Laboratories in Probability and Statistics.
W. N. Venables and B. D. Ripley. 2002. Modern Ap-
plied Statistics with S. Springer-Verlag, New York,
NY, fourth edition.
715
Proceedings of NAACL HLT 2007, Companion Volume, pages 17?20,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
K-Best Suffix Arrays 
 
 
Kenneth Church 
 
Bo Thiesson 
 
Robert Ragno 
Microsoft Microsoft Microsoft 
One Microsoft Way One Microsoft Way One Microsoft Way 
Redmond, WA 98052 Redmond, WA 98052 Redmond, WA 98052 
church@microsoft.com thiesson@microsoft.com rragno@microsoft.com 
 
Abstract 
Suppose we have a large dictionary of 
strings.  Each entry starts with a figure of 
merit (popularity).  We wish to find the k-
best matches for a substring, s, in a dicti-
noary, dict.  That is,  grep s dict | sort ?n | 
head ?k, but we would like to do this in 
sublinear time.  Example applications: (1) 
web queries with popularities, (2) prod-
ucts with prices and (3) ads with click 
through rates.  This paper proposes a 
novel index, k-best suffix arrays, based on 
ideas borrowed from suffix arrays and kd-
trees.  A standard suffix array sorts the 
suffixes by a single order (lexicographic) 
whereas k-best suffix arrays are sorted by 
two orders (lexicographic and popularity). 
Lookup time is between log N and sqrt N. 
1 Standard Suffix Arrays 
This paper will introduce k-best suffix arrays, 
which are similar to standard suffix arrays (Manber 
and Myers, 1990),  an index that makes it conven-
ient to compute the frequency and location of a 
substring, s, in a long sequence, corpus.  A suffix 
array, suf, is an array of all N suffixes, sorted al-
phabetically.  A suffix, suf[i], also known as a 
semi-infinite string, is a string that starts at position 
j in the corpus and continues to the end of the cor-
pus.  In practical implementations, a suffix is a 4-
byte integer, j.  In this way, an int (constant space) 
denotes a long string (N bytes). 
The make_standard_suf program below creates 
a standard suffix array.  The program starts with a 
corpus, a global variable containing a long string 
of N characters.  The program allocates the suffix 
array suf and initializes it to a vector of N ints (suf-
fixes) ranging from 0 to N?1.  The suffix array is 
sorted by lexicographic order and returned. 
 
int* make_standard_suf () { 
  int N = strlen(corpus); 
  int* suf = (int*)malloc(N * sizeof(int)); 
  for (int i=0; i<N; i++) suf[i] = i; 
  qsort(suf, N, sizeof(int), lexcomp); 
  return suf;} 
 
int lexcomp(int* a, int* b) 
{ return strcmp(corpus + *a, corpus + *b);} 
 
This program is simple to describe (but inefficient, 
at least in theory) because strcmp can take O(N) 
time in the worst case (where the corpus contains 
two copies of an arbitrarily long string).  See 
http://cm.bell-labs.com/cm/cs/who/doug/ssort.c for 
an implementation of the O(N log N) Manber and 
Myers algorithm.  However, in practice, when the 
corpus is a dictionary of relatively short entries 
(such as web queries), the worst case is unlikely to 
come up.  In which case, the simple make_suf pro-
gram above is good enough, and maybe even better 
than the O(N log N) solution. 
1.1 Standard Suffix Array Lookup 
To compute the frequency and locations of a sub-
string s, use a pair of binary searches to find i and 
j, the locations of the first and last suffix in the suf-
fix array that start with s.  Each suffix between i 
and j point to a location of s in the corpus.  The 
frequency is simply: j ? i + 1.   
Here is some simple code.  We show how to 
find the first suffix.  The last suffix is left as an 
exercise.  As above, we ignore the unlikely worst 
17
case (two copies of a long string).  See references 
mentioned above for worst case solutions. 
 
void standard_lookup(char* s, int* suf, int N){ 
  int* i = find_first_suf(s, suf, N); 
  int* j = find_last_suf(s, suf, N); 
  for (int* k=i; k<=j; k++) output(*k);} 
 
int* find_first_suf(char* s, int* suf, int N) { 
  int len = strlen(s); 
  int* high = suf + N; 
  while (suf + 2 < high) { 
    int* mid = suf + (high?suf)/2; 
    int c = strncmp(s, corpus + *mid, len); 
    if (c == 0) high = mid+1; 
    else if (c < 0) high = mid; 
    else suf = mid;}   
  for ( ; suf < high; suf++) 
    if (strncmp(s, corpus + *suf, len) == 0) 
      return suf; 
  return NULL;} // not found 
2 K-Best Suffix Arrays 
K-best suffix arrays are like standard suffix arrays, 
except there are two orders instead of one.  In addi-
tion to lexicographic order, we assume a figure of 
merit, which we will refer to as popularity.  For 
example, the popularity of a string could be its fre-
quency in a search log.  The code below assumes 
that the corpus is a sequence of strings that comes 
pre-sorted by popularity, and then the popularities 
have been stripped off.   These assumptions make 
it very easy to compare two strings by popularity.  
All popcomp has to do is to compare the two posi-
tions in the corpus.1 
The make_kbest_suf program below is similar to 
the make_standard_suf program above except we 
now sort by the two orders at alternating depths in 
the tree.  First we sort lexicographically and then 
we sort by popularity and so on, using a construc-
tion similar to KD-Trees (Bentley, 1975).  The 
code below is simple to describe (though there are 
more efficient implementations that avoid unnec-
essary qsorts). 
 
int* make_kbest_suf () { 
  int N = strlen(corpus); 
  int* suf = (int*)malloc(N * sizeof(int)); 
                                                          
1
 With a little extra book keeping, one can keep a table on the 
side that makes it possible to map back and forth between 
popularity rank and the actual popularity.  This turns out to be 
useful for some applications. 
  for (int i=0; i<N; i++) suf[i]=i; 
  process(suf, suf+N, 0); 
  return suf;} 
 
void process(int* start, int* end, int depth) { 
  int* mid = start + (end ? start)/2; 
  if (end <= start+1) return; 
  qsort(start, end-start, sizeof(int),  
            (depth & 1) ? popcomp : lexcomp); 
  process(start, mid, depth+1); 
  process(mid+1, end, depth+1);} 
 
int popcomp(int* a, int* b) {   
  if (*a > *b) return 1; 
  if (*a < *b) return ?1; 
  return 0;} 
 
2.1 K-Best Suffix Array Lookup 
To find the k-best matches for a particular sub-
string s, we do what we would normally do for 
standard suffix arrays on lexicographic splits.  
However, on popularity splits, we search the more 
popular half first and then we search the less popu-
lar half, if necessary. 
An implementation of kbest-lookup is given be-
low.  D denotes the depth of the search thus far.  
Kbest-lookup is initially called with D of 0.  Pro-
pose maintains a heap of the k-best matches found 
thus far.  Done returns true if its argument is less 
popular than the kth best match found thus far. 
 
void kbest_lookup(char* s, int* suf, int N, int D){ 
  int* mid = suf + N/2; 
  int len = strlen(s); 
 
  if (N==1 && strncmp(s, corpus+*suf, len)==0) 
 propose(*suf);  
  if (N <= 1) return; 
 
  if (D&1) {   // popularity split 
    kbest_lookup(s, suf, mid?suf, D+1); 
    if (done(*mid)) return; 
    if (strncmp(s, corpus + *mid, len) == 0)  
 propose(*mid); 
    kbest_lookup(s, mid+1, (suf+N)?mid?1,  
   D+1);} 
  else {   // lexicographic split 
    int c = strncmp(s, corpus + *mid, len); 
    int n = (suf+N)?mid?1; 
    if (c < 0) kbest_lookup(s, suf, mid-suf, D+1); 
    else if (c > 0) kbest_lookup(s, mid+1, n, D+1); 
    else { kbest_lookup(s, suf, mid-suf, depth+1); 
              propose(*mid); 
              kbest_lookup(s, mid+1, n, D+1); }}} 
18
2.2 A Short Example: To be or not to be 
Suppose we were given the text, ?to be or not to 
be.?  We could then generate the following dic-
tionary with frequencies (popularities). 
 
Popularity Word 
2 to 
2 be 
1 or 
1 not 
 
The dictionary is sorted by popularity.  We treat 
the second column as an N=13 byte corpus (with 
underscores at record boundaries): to_be_or_not_ 
 
Standard  K-Best  
suf corpus + suf[i] suf corpus + suf[i] 
12 _ 2 _be_or_not_ 
2 _be_or_not_ 3 be_or_not_ 
8 _not_ 4 e_or_not_ 
5 _or_not_ 5 _or_not_ 
3 be_or_not_ 8 _not_ 
4 e_or_not_ 12 _ 
9 not_ 9 not_ 
1 o_be_or_not_ 1 o_be_or_not_ 
6 or_not_ 6 or_not_ 
10 ot_ 0 to_be_or_not_ 
7 r_not_ 7 r_not_ 
11 t_ 10 ot_ 
0 to_be_or_not_ 11 t_ 
 
The standard suffix array is the 1st column of the 
table above.  For illustrative convenience, we show 
the corresponding strings in the 2nd column.  Note 
that the 2nd column is sorted lexicographically. 
The k-best suffix array is the 3rd column with the 
corresponding strings in the 4th column.  The first 
split is a lexicographic split at 9 (?not_?).  On both 
sides of that split we have a popularity split at 5 
(?_or_not_?) and 7 (?r_not_?). (Recall that relative 
popularity depends on corpus position.)  Following 
there are 4 lexicographic splits, and so on. 
If k-best lookup were given the query string s = 
?o,? then it would find 1 (o_be_or_not_), 6 
(or_not_) and 10 (ot_) as the best choices (in that 
order).   The first split is a lexicographic split.  All 
the matches are below 9 (not_).  The next split is 
on popularity.  The matches above this split (1&6) 
are as popular as the matches below this split (10).   
It is often desirable to output matching records 
(rather than suffixes).  Records are output in popu-
larity order.  The actual popularity can be output, 
using the side table mentioned in footnote 1: 
 
Popularity Record 
2 to 
1 or 
1 not 
2.3 Time and Space Complexity 
The space requirements are the same for both stan-
dard and k-best suffix arrays.  Both indexes are 
permutations of the same suffixes. 
The time requirements are quite different.  Stan-
dard suffix arrays were designed to find all 
matches, not the k-best.  Standard suffix arrays can 
find all matches in O(log N) time.  However, if we 
attempt to use standard suffix arrays to find the k-
best, something they were not designed to do, then 
it could take a long time to sort through the worst 
case (an embarrassment of riches with lots of 
matches).  When the query matches every string in 
the dictionary, standard suffix arrays do not help us 
find the best matches. K-best suffix arrays were 
designed to handle an embarrassment of riches, 
which is quite common, especially when the sub-
string s is short.  Each popularity split cuts the 
search space in half when there are lots of lexico-
graphic matches. 
The best case for k-best suffix arrays is when the 
popularity splits always work in our favor and we 
never have to search the less popular half.  The 
worst case is when the popularity splits always fail, 
such as when the query string s is not in the corpus. 
In this case, we must always check both the popu-
lar half and the unpopular half at each split, since 
the failure to find a lexicographic match in the first 
tells us nothing about the existence of matches in 
the second. 
Asymptotically, k-best lookup takes between log 
N and sqrt N time.  To see this complexity result, 
let P(N) be the work to process N items starting 
with a popularity splits and let L(N) be the work to 
process N items starting with a lexicographic 
splits.  
Thus, 
19
P(N) = ?L(N/2) + C1 
L(N) = P(N/2) + C2 
 
where ? = 2?p, when p is the probability that the 
popular half contains sufficient matches.  ? lies 
between 1 (best case) and 2 (worst case).  C1 and 
C2 are constants.  Thus, 
 
P(N) = ? P(N/4) + C                                       (1) 
 
where C = C1 + ?C2.   Using the master method 
(Cormen et al 2001), P(N) = O(log2N) in the best 
case (?=1). In the worst case (?=2), P(N) = O(sqrt 
N).  In general, for ? > 1, P(N) = O(N(log2 ?)/2). 
In practical applications, we expect popularity 
splits to work more often than not, and therefore 
we expect the typical case to be closer to the best 
case than the worst case.   
3 Empirical Study 
The plot below shows the k-best lookup time as 
a function of square root of corpus size.  We ex-
tracted sub-corpora from a 150 MB collection of 
8M queries, sorted by popularity, according to the 
logs from Microsoft www.live.com. All experi-
ments were performed on a Pentium 4, 3.2GHz 
dual processor machine with enough memory to 
avoid paging. 
The line of diamonds shows the worst case, 
where we the query string is not in the index.  Note 
that the diamonds fit the regression line quite well, 
confirming the theory in the previous section:  The 
worst case lookup is O(sqrt N). 
0
10
20
30
40
50
0 1000 2000 3000
Sqrt(Corpus size)
Ti
m
e 
(se
c) 
fo
r 
10
k 
lo
o
ku
ps
 
 
To simulate a more typical scenario, we con-
structed random samples of queries by popularity, 
represented by squares in the figure.  Note that the 
squares are well below the line, demonstrating that 
these queries are considerably easier than the worst 
case. 
K-best suffix arrays have been used in auto-
complete applications (Church and Thiesson, 
2005).  The triangles with the fastest lookup times 
demonstrate the effectiveness of the index for this 
application.  We started with the random sample 
above, but replaced each query q in the sample 
with a substring of q (of random size). 
4 Conclusion 
A new data structure, k-best suffix arrays, was pro-
posed.  K-best suffix arrays are sorted by two or-
ders, lexicographic and popularity, which make it 
convenient to find the most popular matches, espe-
cially when there are lots of matches.  In many ap-
plications, such as the web, there are often 
embarrassments of riches (lots of matches).  
Lookup time varies from log N to sqrt N, de-
pending on the effectiveness of the popularity 
splits.  In the best case (e.g., very short query 
strings that match nearly everything), the popular-
ity splits work nearly every time and we rarely 
have to search the less popular side of a popularity 
split.  In this case, the time is close to log N.  On 
the other hand, in the worst case (e.g., query 
strings that match nothing), the popularity splits 
never work, and we always have to search both 
sides of a popularity split.  In this case, lookup 
time is sqrt N.  In many cases, popularity splits 
work more often than not, and therefore, perform-
ance is closer to log N than sqrt N. 
References  
Jon Louis Bentley. 1975.  Multidimensional Binary 
Search Trees Used for Associative Searching, Com-
munications of the ACM, 18:9, pp. 509-517. 
Kenneth Church and Bo Thiesson. 2005.  The Wild 
Thing,  ACL, pp. 93-96. 
 
Udi Manber and Gene Myers. 1990. Suffix Arrays: A 
New Method for On-line String Searches,  SODA, pp. 
319-327. 
Thomas H. Cormen, Charles E. Leiserson, Ronald L. 
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms, Second Edition. MIT Press and McGraw-
Hill, pp.73?90. 
20
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 93?96, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Wild Thing! 
 
Kenneth Church Bo Thiesson 
Microsoft Research 
Redmond, WA, 98052, USA 
{church, thiesson}@microsoft.com 
 
 
 
 
Abstract 
Suppose you are on a mobile device with 
no keyboard (e.g., a cell or PDA).  How 
can you enter text quickly?  T9?  Graffiti? 
This demo will show how language model-
ing can be used to speed up data entry, both 
in the mobile context, as well as the desk-
top.  The Wild Thing encourages users to 
use wildcards (*).  A language model finds 
the k-best expansions.  Users quickly figure 
out when they can get away with wild-
cards.  General purpose trigram language 
models are effective for the general case 
(unrestricted text), but there are important 
special cases like searching over popular 
web queries, where more restricted lan-
guage models are even more effective. 
1 Motivation: Phone App 
Cell phones and PDAs are everywhere.  Users love 
mobility.  What are people doing with their phone?  
You?d think they would be talking on their phones, 
but a lot of people are typing.  It is considered rude 
to talk on a cell in certain public places, especially 
in Europe and Asia.  SMS text messaging enables 
people to communicate, even when they can?t talk. 
It is bizarre that people are typing on their 
phones given how painful it is.   ?Talking on the 
phone? is a collocation, but ?typing on the phone? 
is not.  Slate (slate.msn.com/id/2111773) recently 
ran a story titled: ?A Phone You Can Actually 
Type On? with the lead: 
?If you've tried to zap someone a text mes-
sage recently, you've probably discovered 
the huge drawback of typing on your cell 
phone. Unless you're one of those cyborg 
Scandinavian teenagers who was born with 
a Nokia in his hand, pecking out even a 
simple message is a thumb-twisting chore.?  
 
There are great hopes that speech recognition 
will someday make it unnecessary to type on your 
phone (for SMS or any other app), but speech rec-
ognition won?t help with the rudeness issue.  If 
people are typing because they can?t talk, then 
speech recognition is not an option.  Fortunately, 
the speech community has developed powerful 
language modeling techniques that can help even 
when speech is not an option. 
2 K-Best String Matching 
Suppose we want to search for MSN using a cell 
phone.  A standard approach would be to type 6 
<pause> 777 <pause> 66, where 6  M, 777  S 
and 66  N.  (The pauses are necessary for disam-
biguation.)   Kids these days are pretty good at typ-
ing this way, but there has to be a better solution. 
T9 (www.t9.com) is an interesting alternative.  
The user types 676 (for MSN).  The system uses a 
(unigram) language model to find the k-best 
matches.  The user selects MSN from this list.  
Some users love T9, and some don?t. 
The input, 676, can be thought of as short hand 
for the regular expression:  
/^[6MNOmno][7PRSprs][6MNOmno]$/ 
using standard Unix notation.  Regular expressions 
become much more interesting when we consider 
wildcards.  So-called ?word wheeling? can be 
thought of as the special case where we add a 
wildcard to the end of whatever the user types.  
Thus, if the user types 676 (for MSN), we would 
find the k-best matches for:  
/^[6MNOmno][7PRSprs][6MNOmno].*/ 
93
See Google Suggests1 for a nice example of 
word wheeling.  Google Suggests makes it easy to 
find popular web queries (in the standard non-
mobile desktop context).  The user types a prefix.  
After each character, the system produces a list of 
the k most popular web queries that start with the 
specified prefix. 
Word wheeling not only helps when you know 
what you want to say, but it also helps when you 
don?t.  Users can?t spell.  And things get stuck on 
the tip of their tongue.  Some users are just brows-
ing.  They aren?t looking for anything in particular, 
but they?d like to know what others are looking at. 
The popular query application is relatively easy 
in terms of entropy.  About 19 bits are needed to 
specify one of the 7 million most popular web que-
ries.  That is, if we assign each web query a prob-
ability based on query logs collected at msn.com, 
then we can estimate entropy, H, and discover that 
H?19.  (About 23 bits would be needed if these 
pages were equally likely, but they aren?t.)  It is 
often said that the average query is between two 
and three words long, but H is more meaningful 
than query length. 
General purpose trigram language models are 
effective for the general case (unrestricted text), 
but there are important special cases like popular 
web queries, where more restricted language mod-
els are even more effective than trigram models.  
Our language model for web queries is simply a 
list of queries and their probabilities.  We consider 
queries to be a finite language, unlike unrestricted 
text where the trigram language model allows sen-
tences to be arbitrarily long. 
Let?s consider another example.  The MSN 
query was too easy.  Suppose we want to find 
Condoleezza Rice, but we can?t spell her name.  
And even if we could, we wouldn?t want to.  Typ-
ing on a phone isn?t fun. 
We suggest spelling Condoleezza as 2*, where 
2  [ABCabc2] and * is the wildcard.  We then 
type ?#? for space.  Rice is easy to spell: 7423.   
Thus, the user types, 2*#7423, and the system 
searches over the MSN query log to produce a list 
of k-best (most popular) matches (k defaults to 10): 
1. Anne Rice 
2. Book of Shadows 
3. Chris Rice 
4. Condoleezza Rice 
                                                           
1 http://www.google.com/webhp?complete=1  
5. Ann Rice 
? 
8. Condoleeza Rice 
The letters matching constants in the regular ex-
pression are underlined.  The other letters match 
wildcards.  (An implicit wildcard is appended to 
the end of the input string.) 
Wildcards are very powerful.   Strings with 
wildcards are more expressive than prefix match-
ing (word wheeling).  As mentioned above, it 
should take just 19 bits on average to specify one 
of the 7 million most popular queries.   The query 
2*#7423 contains 7 characters in an 12-character 
alphabet (2-9  [A-Za-z2-9] in the obvious way, 
except that 0  [QZqz0]; #  space; * is wild).  7 
characters in a 12 character alphabet is 7 log212 = 
25 bits.  If the input notation were optimal (which 
it isn?t), it shouldn?t be necessary to type much 
more than this on average to specify one of the 7 
million most popular queries. 
Alphabetic ordering causes bizarre behavior.  
Yellow Pages are full of company names starting 
with A, AA, AAA, etc..  If prefix matching tools like 
Google Suggests take off, then it is just a matter of 
time before companies start to go after valuable 
prefixes: mail, maps, etc.  Wildcards can help soci-
ety avoid that non-sense.  If you want to find a top 
mail site, you can type, ?*mail? and you?ll find: 
Gmail, Hotmail, Yahoo mail, etc.. 
3 Collaboration & Personalization 
Users quickly learn when they can get away with 
wildcards.  Typing therefore becomes a collabora-
tive exercise, much like Palm?s approach to hand-
writing recognition. Recognition is hard.  Rather 
than trying to solve the general case, Palm encour-
ages users to work with the system to write in a 
way that is easier to recognize (Graffiti).  The sys-
tem isn?t trying to solve the AI problem by itself, 
but rather there is a man-machine collaboration 
where both parties work together as a team. 
Collaboration is even more powerful in the 
web context.  Users issue lots of queries, making it 
clear what?s hot (and what?s not).  The system con-
structs a language model based on these queries to 
direct users toward good stuff.   More and more 
users will then go there, causing the hot query to 
move up in the language model.  In this way, col-
laboration can be viewed as a positive feedback 
94
loop.  There is a strong herd instinct; all parties 
benefit from the follow-the-pack collaboration. 
In addition, users want personalization.  When 
typing names of our friends and family, technical 
terms, etc., we should be able to get away with 
more wildcards than other users would.  There are 
obvious opportunities for personalizing the lan-
guage model by integrating the language model 
with a desktop search index (Dumais et al 2003). 
4 Modes, Language Models and Apps 
The Wild Thing demo has a switch for turning on 
and off phone mode to determine whether input 
comes from a phone keypad or a standard key-
board.  Both with and without phone mode, the 
system uses a language model to find the k-best 
expansions of the wildcards. 
The demo contains a number of different lan-
guage models, including a number of standard tri-
gram language models.  Some of the language 
models were trained on large quantities (6 Billion 
words) of English.  Others were trained on large 
samples of Spanish and German.  Still others were 
trained on small sub-domains (such as ATIS, 
available from www.ldc.upenn.edu).  The demo 
also contains two special purpose language models 
for searching popular web queries, and popular 
web domains. 
Different language models are different.  With 
a trigram language model trained on general Eng-
lish (containing large amounts of newswire col-
lected over the last decade), 
pres* rea* *d y* t* it is v* 
imp*  President Reagan said 
yesterday that it is very impor-
tant 
With a Spanish Language Model, 
pres* rea*  presidente Reagan 
In the ATIS domain,  
pres* rea*  <UNK> <UNK> 
The tool can also be used to debug language 
models.  It turns out that some French slipped into 
the English training corpus.  Consequently, the 
English language model expanded the * in en * de 
to some common French words that happen to be 
English words as well: raison, circulation, oeuvre, 
place, as well as <OOV>.  After discovering this, 
we discovered quite a few more anomalies in the 
training corpus such as headers from the AP news. 
There may also be ESL (English as a Second 
Language) applications for the tool.  Many users 
have a stronger active vocabulary than passive vo-
cabulary.  If the user has a word stuck on the tip of 
their tongue,  they can type a suggestive context 
with appropriate wildcards and there is a good 
chance the system will propose the word the user is 
looking for. 
Similar tricks are useful in monolingual con-
texts.  Suppose you aren?t sure how to spell a ce-
lebrity?s name.  If you provide a suggestive 
context, the language model is likely to get it right:  
ron* r*g*n  Ronald Reagan 
don* r*g*n  Donald Regan 
c* rice  Condoleezza Rice 
To summarize, wildcards are helpful in quite a 
few apps: 
? No keyboard: cell phone, PDA, Tablet PC. 
? Speed matters: instant messaging, email. 
? Spelling/ESL/tip of the tongue. 
? Browsing: direct users toward hot stuff. 
5 Indexing and Compression 
The k-best string matching problem raises a num-
ber of interesting technical challenges.   We have 
two types of language models: trigram language 
models and long lists (for finite languages such as 
the 7 million most popular web queries).  
The long lists are indexed with a suffix array.  
Suffix arrays2 generalize very nicely to phone 
mode, as described below.  We treat the list of web 
queries as a text of N bytes.  (Newlines are re-
placed with end-of-string delimiters.)  The suffix 
array, S, is a sequence of N ints.  The array is ini-
tialized with the ints from 0 to N?1.  Thus, S[i]=i, 
for 0?i<N.  Each of these ints represents a string, 
starting at position i in the text and extending to the 
end of the string.  S is then sorted alphabetically. 
Suffix arrays make it easy to find the frequency 
and location of any substring.  For example, given 
the substring ?mail,? we find the first and last suf-
fix in S that starts with ?mail.?  The gap between 
these two is the frequency.  Each suffix in the gap 
points to a super-string of ?mail.? 
To generalize suffix arrays for phone mode we 
replace alphabetical order (strcmp) with phone or-
der (phone-strcmp).  Both strcmp and phone-
strcmp consider each character one at a time.  In 
standard alphabetic ordering, ?a?<?b?<?c?, but in 
                                                           
2 An excellent discussion of suffix arrays including source 
code can be found at www.cs.dartmouth.edu/~doug.   
95
phone-strcmp, the characters that map to the same 
key on the phone keypad are treated as equivalent. 
We generalize suffix arrays to take advantage 
of popularity weights.  We don?t want to find all 
queries that contain the substring ?mail,? but 
rather, just the k-best (most popular).  The standard 
suffix array method will work, if we add a filter on 
the output that searches over the results for the k-
best.  However, that filter could take O(N) time if 
there are lots of matches, as there typically are for 
short queries. 
An improvement is to sort the suffix array by 
both popularity and alphabetic ordering, alternating 
on even and odd depths in the tree.  At the first 
level, we sort by the first order and then we sort by 
the second order and so on, using a construction, 
vaguely analogous to KD-Trees (Bentley, 1975).  
When searching a node ordered by alphabetical 
order, we do what we would do for standard suffix 
arrays.  But when searching a node ordered by 
popularity, we search the more popular half before 
the second half.  If there are lots of matches, as 
there are for short strings, the index makes it very 
easy to find the top-k quickly, and we won?t have 
to search the second half very often.  If the prefix 
is rare, then we might have to search both halves, 
and therefore, half the splits (those split by popu-
larity) are useless for the worst case, where the 
input substring doesn?t match anything in the table.  
Lookup is O(sqrt N).3 
Wildcard matching is, of course, a different 
task from substring matching.  Finite State Ma-
chines (Mohri et al 2002) are the right way to 
think about the k-best string matching problem 
with wildcards.  In practice, the input strings often 
contain long anchors of constants (wildcard free 
substrings).  Suffix arrays can use these anchors to 
generate a list of candidates that are then filtered 
by a regex package. 
                                                           
3 Let F(N) be the work to process N items on the 
frequency splits and let A(N) be the work to proc-
ess N items on the alphabetical splits.  In the worst 
case, F(N) = 2A(N/2) + C1 and A(N) = F(N/2) + C2, 
where C1  and C2 are two constants.  In other 
words, F(N) = 2F(N/4) + C, where C = C1 + 2C2.  
We guess that F(N) = ? sqrt(N) + ?, where ? and ? 
are constant.  Substituting this guess into the recur-
rence, the dependencies on N cancel.  Thus, we 
conclude, F(N) = O(sqrt N).  
Memory is limited in many practical applica-
tions, especially in the mobile context.  Much has 
been written about lossless compression of lan-
guage models.  For trigram models, we use a lossy 
method inspired by the Unix Spell program (McIl-
roy, 1982).   We map each trigram <x, y, z> into a 
hash code h = (V2 x + V y + z) % P, where V is the 
size of the vocabulary and P is an appropriate 
prime.  P trades off memory for loss.  The cost to 
store N trigrams is: N [1/loge2 + log2(P/N)] bits.   
The loss, the probability of a false hit, is 1/P. 
The N trigrams are hashed into h hash codes.  
The codes are sorted.  The differences, x, are en-
coded with a Golomb code4 (Witten et al 1999), 
which is an optimal Huffman code, assuming that 
the differences are exponentially distributed, which 
they will be, if the hash is Poisson. 
6 Conclusions 
The Wild Thing encourages users to make use of 
wildcards, speeding up typing, especially on cell 
phones.  Wildcards are useful when you want to 
find something you can?t spell, or something stuck 
on the tip of your tongue.   Wildcards are more 
expressive than standard prefix matching, great for 
users, and technically challenging (and fun) for us. 
References  
J. L. Bentley (1975), Multidimensional binary search 
trees used for associative searching, Commun. ACM, 
18:9, pp 509-517. 
S. T. Dumais, E. Cutrell, et al(2003). Stuff I've Seen: A 
system for personal information retrieval and re-use, 
SIGIR. 
M. D. McIlroy (1982), Development of a spelling list, 
IEEE Trans. on Communications 30, 91-99. 
M. Mohri, F. C. N. Pereira, and M. Riley. Weighted 
Finite-State Transducers in Speech Recognition. 
Computer Speech and Language, 16(1):69-88, 2002. 
I. H. Witten, A. Moffat and T. C. Bell, (1999),  Manag-
ing Gigabytes: Compressing and Indexing Docu-
ments and Images, by Morgan Kaufmann Publishing, 
San Francisco, ISBN 1-55860-570-3. 
                                                           
4 In Golomb, x = xq m + xr, where xq = floor(x/m) 
and xr = x mod m.  Choose m to be a power of two 
near ceil(? E[x])=ceil(? P/N).  Store quotients xq 
in unary and remainders xr in binary.  z in unary is 
a sequence of z?1 zeros followed by a 1.  Unary is 
an optimal Huffman code when Pr(z)=(?)z+1.  Stor-
age costs are: xq bits for xq + log2m bits for xr. 
96
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1428?1436,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Using Word-Sense Disambiguation Methods to Classify Web Queries by
Intent
Emily Pitler
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler@seas.upenn.edu
Ken Church
Johns Hopkins University
Human Language Technology Center of Excellence
Baltimore, MD 21211
Kenneth.Church@jhu.edu
Abstract
Three methods are proposed to classify
queries by intent (CQI), e.g., navigational,
informational, commercial, etc. Follow-
ing mixed-initiative dialog systems, search
engines should distinguish navigational
queries where the user is taking the ini-
tiative from other queries where there are
more opportunities for system initiatives
(e.g., suggestions, ads). The query in-
tent problem has a number of useful appli-
cations for search engines, affecting how
many (if any) advertisements to display,
which results to return, and how to ar-
range the results page. Click logs are
used as a substitute for annotation. Clicks
on ads are evidence for commercial in-
tent; other types of clicks are evidence for
other intents. We start with a simple Na??ve
Bayes baseline that works well when there
is plenty of training data. When train-
ing data is less plentiful, we back off
to nearby URLs in a click graph, using
a method similar to Word-Sense Disam-
biguation. Thus, we can infer that de-
signer trench is commercial because it is
close to www.saksfifthavenue.com, which
is known to be commercial. The baseline
method was designed for precision and
the backoff method was designed for re-
call. Both methods are fast and do not re-
quire crawling webpages. We recommend
a third method, a hybrid of the two, that
does no harm when there is plenty of train-
ing data, and generalizes better when there
isn?t, as a strong baseline for the CQI task.
1 Classify Queries By Intent (CQI)
Determining query intent is an important prob-
lem for today?s search engines. Queries are short
(consisting of 2.2 terms on average (Beitzel et al,
2004)) and contain ambiguous terms. Search en-
gines need to derive what users want from this lim-
ited source of information. Users may be search-
ing for a specific page, browsing for information,
or trying to buy something. Guessing the correct
intent is important for returning relevant items.
Someone searching for designer trench is likely
to be interested in results or ads for trench coats,
while someone searching for world war I trench
might be irritated by irrelevant clothing advertise-
ments.
Broder (2002) and Rose and Levinson (2004)
categorized queries into those with navigational,
informational, and transactional or resource-
seeking intent. Navigational queries are queries
for which a user has a particular web page in mind
that they are trying to navigate to, such as grey-
hound bus. Informational queries are those like
San Francisco, in which the user is trying to gather
information about a topic. Transactional queries
are those like digital camera or download adobe
reader, where the user is seeking to make a trans-
action or access an online resource.
Knowing the intent of a query greatly affects the
type of results that are relevant. For many queries,
Wikipedia articles are returned on the first page
of results. For informational queries, this is usu-
ally appropriate, as a Wikipedia article contains
summaries of topics and links to explore further.
However, for navigational or transactional queries,
Wikipedia is not as appropriate. A user looking
for the greyhound bus homepage is probably not
interested in facts about the company. Similarly,
someone looking to download adobe reader will
not be interested in Wikipedia?s description of the
product?s history. Conversely, for informational
queries, Wikipedia articles tend to be appropriate
while advertisements are not. The user searching
for world war I trench might find the Wikipedia
article on trench warfare useful, while he is prob-
1428
(a) The advertisements and related searches are probably more likely to be clicked on than
the top result for designer trench.
(b) The top result will receive more clicks than the spelling suggestion. Wikipedia often
receives lots of clicks, but not for commercial queries like bestbuy.
Figure 1: Results pages from two major search engines. A search results page has limited real estate that
must be divided between search results, spelling suggestions, query suggestions, and ads.
ably not interested in purchasing clothing, or even
World War I related products. We noticed empiri-
cally that queries in the logs tend to have a high
proportion of clicks on the Wikipedia article or
the ads, but almost never both. The Wikipedia
page for Best Buy in Figure 1(b) is probably a
waste of space. Knowing whether a particular
query is navigational, informational, or transac-
tional would improve search and advertising rel-
evance.
After a query is issued, search engines return
a list of results, and possibly also advertisements,
suggestions of related searches, and spelling sug-
gestions. For different queries, these alternatives
have varying utilities to the users. Consider the
queries in Figures 1(a) and 1(b). For designer
trench, the advertisements may well be more use-
ful to the user than the standard set of results. The
query suggestions for designer trench all would
help refine the query, whereas the suggestions for
bestbuy are less useful, as they would either re-
turn the same set of results or take the user to Best
Buy?s competitors? sites. The spelling suggestion
for best buy instead of bestbuy is also unnecessary.
Devoting more page space to the content that is
likely to be clicked on could help improve the user
experience.
In this paper we consider the task of: given a
class of queries, which types of answer (standard
search, ads, query suggestions, or spelling sug-
1429
gestions) are likely to be clicked on? Typos will
tend to have more clicks on the spelling sugges-
tions, informational queries will have more clicks
on Wikipedia pages, and commercial queries will
have more clicks on the ads. The observed behav-
ior of where users click tells us something about
the hidden intentions of the users when they issue
that query.
We focus on commercial intent (Dai et al,
2006), the intent to purchase a product or service,
to illustrate our method of predicting query intent.
The business model of web search today is heav-
ily dependent on advertising. Advertisers bid on
queries, and then the search results page also con-
tains ?sponsored? sites by the advertisers who won
the auction for that query. It is thus advantageous
for the advertisers to bid on queries which are most
likely to result in a commercial transaction. If
a query is classified as likely implying commer-
cial intent, but the advertisers have overlooked this
query, then the search engine may want to sug-
gest that advertisers bid on that query. The search
engine may also want to treat queries classified
as having commercial intent differently, by rear-
ranging the appearance of the page, or by showing
more or fewer advertisements.
This paper starts with a simple Na??ve Bayes
baseline to classify queries by intent (CQI). Super-
vised methods work well, especially when there is
plenty of annotated data for testing and training.
Unfortunately, since we don?t have as much anno-
tated data as we might like, we propose two work-
arounds:
1. Use click logs as a substitute for annotated
data. Clicks on ads are evidence for commer-
cial intent; other types of clicks are evidence
for other intents.
2. We propose a method similar to Yarowsky
(1995) to generalize beyond the training set.
2 Related Work
Click logs have been used for a variety of tasks
involved in information retrieval, including pre-
dicting which pages are the best results for queries
(Piwowarski and Zaragoza, 2007; Joachims, 2002;
Xue et al, 2004), choosing relevant advertise-
ments (Chakrabarti et al, 2008), suggesting re-
lated queries (Beeferman and Berger, 2000), and
personalizing results (Tan et al, 2006). Queries
that have a navigational intent tended to have
a highly skewed click distribution, while users
clicked on a wider range of results after issuing
informational queries. Lee et al (2005) used the
click distributions to classify navigational versus
informational intents.
While navigational, informational, and
resource-seeking are very broad intentions, other
researchers have looked at personalization and
intent on a per user basis. Downey et al (2008)
use the last URL visited in a session or the last
search engine result visited as a proxy for the
user?s information goal, and then looked at the
correspondence between information needs and
queries (how the goals are expressed).
We are interested in a granularity of intent
in between navigational/informational/resource-
seeking and personalized intents. For these sorts
of intents, the web pages associated with queries
provide useful information. To classify queries
into an ontology of commercial queries, Broder
et al (2007) found that a classifier that used the
text of the top result pages performed much bet-
ter than a classifier that used only the query string.
While the results are quite good on their hierarchy
of 6000 types of commercial intents, they manu-
ally constructed about 150 hand-picked examples
each for each of the 6000 intents. Beitzel et al
(2005) do semi-supervised learning over the query
logs to classify queries into topics, but also train
with hundreds of thousands of manually annotated
queries. Thus, while we also use the query logs
and the identities of web pages of associated with
each query, we are interested in finding methods
that can be applied when that much annotation is
prohibitive.
Semi-supervised methods over the click graph
make it possible to train classifiers after starting
from a much smaller set of seed queries. Li et al
(2008) used the semi-supervised learning method
described in Zhou et al (2004) to gain a much
larger training set of examples, and then trained
classifiers for product search or job search on the
expanded set. Random walk methods over the
click graph have also been used to propagate re-
lations between URLs, for tasks such as finding
?adult? content (Craswell and Szummer, 2007)
and suggesting related queries (Antonellis et al,
2008) and content (Baluja et al, 2008). In our
work we also seek to classify query intent us-
ing the click graph, but we demonstrate the ef-
fectiveness of a simple method by building deci-
1430
sion lists of URLs. In addition, we evaluate our
method automatically by using user click rates,
rather than assembling hand-labeled examples for
training and testing.
Dai et al (2006) also classified queries by com-
mercial intent, but their method involved crawling
the top landing pages for each query, which can
be quite time-consuming. In this paper we investi-
gate the commercial intent problem when crawling
pages is not feasible, and use only the identities of
the top URLs.
3 Using Click Logs as a Substitute for
Annotation
Prior work has used click logs in lieu of manual
annotations of relevance ratings, either of web-
pages (Joachims, 2002) or of sponsored search ad-
vertisements (Ciaramita et al, 2008). Here we use
the click logs as a large-scale source of intents.
Logs from Microsoft?s Live Search are used for
training and test purposes. Logs from May 2008
were used for training, and logs from June 2008
were used for testing.
The logs distinguish four types of clicks: (a)
search results, (b) ads, (c) spelling suggestions and
(d) query suggestions. Some prototypical queries
of each type are shown in Table 1. As mentioned
above, clicks on ads are evidence for commercial
intent; other types of clicks are evidence for other
intents. The query, ebay official, is assumed to be
commercial intent, because a large fraction of the
clicks are on ads. In contrast, typos tend to have
relatively more clicks on ?did-you-mean? spelling
suggestions.
The query logs contain over a terabyte of
data for each day, and our experiments were
done using months of logs at a time. We
used SCOPE (Chaiken et al, 2008), a script-
ing programming language designed for doing
Map-Reduce (Dean and Ghemawat, 2004) style
computations, to distribute the task of aggre-
gating the counts of each query over thousands
of servers. As the same query is often issued
several times by multiple users across an en-
tire month of search logs, we summarize each
query with four ratios?search results clicks:overall
clicks, ad clicks:overall clicks, spelling sugges-
tion clicks:overall clicks, and query suggestion
clicks:overall clicks.
A couple of steps were taken to ensure reliable
ratios. We are classifying types, not tokens, and
so limited ourselves to those queries with 100 or
more clicks. This still leaves us with over half a
million distinct queries for training and for test-
ing, yet alows us to use click ratios as a substitute
for annotating these huge data sets. If a query was
only issued once and the user clicked on an ad,
that may be more a reflection of the user, rather
than reflecting that the query is 100% commer-
cial. In addition, the ratios compare clicks of one
type with clicks of another, rather than compar-
ing clicks with impressions. There is less risk of a
failure to find fallacy if we count events (clicks) in-
stead of non-events (non-clicks). There are many
reasons for non-clicks, only some of which tell us
about the meaning of the query. There are bots that
crawl pages and never click. Many links can?t be
seen (e.g., if they are below the fold).
Queries are labeled as positive examples of
commercial intent if their ratio is in the top half of
the training set, and negative otherwise. A similar
procedure is used to label queries with the three
other intents.
Our task is to predict future click patterns based
on past click patterns. Note that a query may ap-
pear in both the test set and the training set, al-
though not necessarily with the same label. In fact,
because of the robustness requirement of 100+
clicks, many queries appear in both sets; 506,369
out of 591,122 of the test queries were also present
in the training month. The overlap reflects natural
processes on the web, with a long tail (of queries
that will never be seen again) and a big fat head (of
queries that come up again and again). Throwing
away the overlap would both drastically reduce the
size of the data and make the problem less realistic
for a commercial application.
We therefore report results on various training
set sizes so that we can show both: (a) the abil-
ity of the proposed method to generalize to unseen
queries, and (b) the high performance of the base-
lines in a realistic setting. We vary the number of
new queries by training the methods on subsets of
20%, 40%, 60%, 80%, and 100% of the positive
examples (along with all the negative examples)
in the training set. This led to the test set having
17%, 34%, 52%, 67%, and 86% actual overlap of
these queries, respectively, with the training sets.
1431
Click Type Query Type Example
(Area on Results Page) (Intent)
Spelling Suggestion Typo www.lastmintue.com.au
Ad Commercial Intent ebay official
Query Suggestion Suggestible sears employees (where there are some popular query suggestions
indicating how current employees can navigate to the benefits site,
as well as how others can apply for employment)
Search Result Standard Search craigslist, denver, co
Table 1: Queries with a high percentage of clicks in each category
4 Three CQI Methods
4.1 Method 1: Look-up Baseline
The baseline method checks if a query was present
in the training set, and if so, outputs the label from
the training set. If the query was not present, it
backs off to the appropriate default label: ?non-
commercial? for the commercial intent task (and
?non-suggestible?, ?not a typo?, etc. for the other
CQI tasks). This very simple baseline method
is effective because the ratios tend to be fairly
stable from one month to the next. The query,
ebay official, for example, has relatively high ad
clicks in both the training month as well as the
test month. The next section will propose an al-
ternative method to address the main weakness of
the baseline method, the inability to generalize be-
yond the queries in the training set.
Figure 2: saks and bluefly trench coats are known
to be commercial, while world war I trench is
known to be non-commercial. What about de-
signer trench? We can classify it as commercial
because it shares URLs with the known commer-
cial queries.
4.2 Method 2: Using Click Graph Context to
Generalize Beyond the Queries in the
Training Set
To address the generalization concern, we propose
a method inspired by Yarowsky (1994). Word
sense disambiguation is a classic problem in nat-
ural language processing. Some words have mul-
tiple senses; for instance, bank can either mean
a riverbank or a financial institution, and for var-
ious tasks such as information retrieval, parsing,
or information extraction, it is useful to be able to
differentiate between the possible meanings.
When a word is being used in each sense, it
tends to appear in a different context. For exam-
ple, if the word muddy is nearby bank, the author
is probably using the riverbank sense of the term,
while if the word deposit is nearby, the word is
probably being used with the financial sense.
Yarowksy (1995) thus creates a list of each pos-
sible context, sorted by how strong the evidence is
for a particular sense. To classify a new example,
Yarowsky (1994) finds the most informative collo-
cation pattern that applies to the test example.
In this work, rather than using the surrounding
words as context as in text classification, we con-
sider the surrounding URLs in the click graph as
context. A sample portion of the click graph is
shown in figure 2. The figure shows queries on
the left and URLs on the right. The click graph
was computed on a very large sample of logs com-
puted well before the training period. There is an
edge from a query q to a URL u if at least 10 users
issued q and then clicked on u.
For each URL, we look at its neighboring
queries and calculate the log likelihood ratio of
their labels in the training set. We classify a new
query q according to URL
?
, the neighboring URL
with the strongest opinion (highest absolute value
of the log likelihood ratio). That is, we compute
URL
?
with:
1432
argmax
U
i
?Nbr(q)
?
?
?
?
log
Pr(Intent|U
i
)
Pr(?Intent|U
i
)
?
?
?
?
If the neighboring opinion is positive (that is,
Pr(Intent|URL
?
) > Pr(?Intent|URL
?
)), then
the query q is assigned a positive label. Otherwise,
q is assigned a negative label.
In Figure 2, we classify designer trench as a
commercial query based on the neighbor with
the strongest opinion. In this case, there
was a tie between two neighbors with equally
strong opinions: www.saksfifthavenue.com and
www.bluefly.com/Designer-Trench-Coats. Both
neighbors are associated with queries that were
labeled commercial in the training set: saks and
bluefly trench coats, respectively.
This method allows the labels of training set
queries to propagate through the URLs to new test
set queries.
4.3 Method 3: Hybrid (?Better Together?)
We recommend a hybrid of the two methods:
? Method 1: the look-up baseline
? Method 2: use click graph context to gener-
alize beyond the queries in the training set
Method 1 is designed for precision and method 2
is designed for recall. The hybrid uses method
1 when applicable, and otherwise, backs off to
method 2.
5 Results
5.1 Commercial Intent
Table 2 and Figures 3(a) and 3(b) compare the per-
formance on the proposed hybrid method with the
baseline. When there is plenty of training mate-
rial, both methods perform about equally well (the
look-up baseline has an F-score of 84.1%, com-
pared with the hybrid method?s F-score of 85.3%),
but generalization becomes important when train-
ing data is severely limited. Figure 3(a) shows
that the proposed method does no harm and might
even help a little when there is plenty of training
data. The hybrid?s main benefit is generalization
to queries beyond the training set. If we severely
limit the size of the training set to just 20% of the
month, as in Figure 3(b), then the proposed hybrid
method is substantially better than the baseline. In
this case, the proposed hybrid method?s F-score
is 65.8%, compared with the look-up method?s F-
score of 28.4%.
5.2 Other types of clicks
Table 3 and Figures 4(a) and 4(b) show a similar
pattern for the query suggestion task. In fact, the
pattern is perhaps even stronger for the query sug-
gestion task than commercial intent. When the full
training set is used, the hybrid method achieves
an F-score of 91.9% (precision = 91.5%, recall =
92.3%). When only 20% of the training data is
used, the hybrid method has an F-score of 73.9%,
compared with the baseline?s F-score of 29.6%. A
similar pattern was observed for clicks on search
results.
The one exception is the spelling suggestion
task, where the context heuristic proved ineffec-
tive, for reasons that should not be surprising in
retrospect. Click graph distance is an effective
heuristic for many intents, but not for typos. Users
who issue misspelled the query have the same
goals as users who correctly spell the query, so
we shouldn?t expect URLs to be able to differ-
entiate them. For misspelled queries, for exam-
ple, yuotube, there are correctly spelled queries,
like youtube, with the same intent that will tend to
be associated with the same set of URLs (such as
www.youtube.com).
6 Conclusion and Future Work
We would like to be able to distinguish web
queries by intent. Unfortunately, we don?t have
annotated data for query intent, but we do have
access to large quantities of click logs. The logs
distinguish four types of clicks: (a) search results,
(b) ads, (c) spelling suggestions and (d) query sug-
gestions. Clicks on ads are evidence for commer-
cial intent; other types of clicks are evidence for
other intents. Click logs are huge sources of data,
and while there are privacy concerns, anonymized
logs are beginning to be released for research pur-
poses (Craswell et al, 2009).
Besides commercial intent, queries can also be
divided into two broader classes: queries in which
the user is browsing and queries for which the user
is navigating. Clicks on the ads and query sug-
gestions indicate that users are browsing and will-
ing to look at these alternative suggestions, while
clicks on the search results indicate that the users
were navigating to what they were searching for.
Clicks on typos indicate neither, as presumably the
users are not entering typos on purpose.
Just as dialogue management systems learn
policies for when to allow user initiative (the user
1433
(a) (b)
Figure 3: Better together: proposed hybrid is no worse than baseline (left) and generalizes better to
unseen tail queries (right). The two panels are the same, except that the training set was reduced on the
right to test generalization error.
(a) (b)
Figure 4: Similar to Figures 3(a) and 3(b), adding the decision list method generalizes over the look-up
method for the ?suggestible? task.
can respond in an open way) versus system ini-
tiative (the system asks the user questions with a
restricted set of possible answers) (Rela?no et al,
1999; Scheffler and Young, 2002; Singh et al,
2002), search engines may want to learn policies
for when the user just wants the search results or
when the user is open to suggestions. When users
want help (they want the search engine to suggest
results), more space on the page should be devoted
to the ads and the query suggestions. When the
users know what it is they want, more of the page
should be given to the search results they asked
for.
We started with a simple baseline for predicting
click location that had great precision, but didn?t
generalize well beyond the queries in the train-
ing set. To improve recall, we proposed a con-
text heuristic that backs off in the click graph.
The backoff method is similar to Yarowsky?s Word
Sense Disambiguation method, except that context
is defined in terms of URLs nearby in click graph
distance, as opposed to words nearby in the text.
Our third method, a hybrid of the baseline
method and the backoff method, is the strongest
baseline we have come up with. The evaluation
showed that the hybrid does no harm when there
is plenty of training data, and generalizes better
when there isn?t.
A direction for further research would be to see
if propagating query intent through URLs that are
not direct neighbors but are further away, perhaps
through random walk methods (Baluja et al, 2008;
1434
Training Size F-score Precision / Recall
Baseline Method 2 Hybrid Baseline Method 2 Hybrid
100% 84.1 75.6 85.3 88.2 / 80.4 76.6 / 74.6 85.7 / 85.0
80% 74.4 74.8 83.5 88.2 / 64.3 79.3 / 70.7 86.7 / 80.6
60% 62.4 72.9 80.7 88.3 / 48.2 82.5 / 65.3 87.9 / 74.6
40% 47.9 70.1 76.0 77.5 / 34.7 78.5 / 63.3 80.7 / 66.0
20% 28.4 62.5 65.8 77.6 / 17.4 75.9 / 53.1 74.3 / 59.1
Table 2: The baseline and hybrid methods have comparable F-scores when there is plenty of training
data, but generalization becomes important when training data is severely limited. The proposed hybrid
method generalizes better as indicated by the widening gap in F-scores with smaller and smaller training
sets.
Training Size F-score Precision / Recall
Baseline Method 2 Hybrid Baseline Method 2 Hybrid
100% 91.0 86.2 91.9 94.9 / 87.4 90.7 / 82.3 91.5 / 92.3
80% 80.5 85.2 90.6 94.9 / 69.9 91.6 / 79.7 91.9 / 89.4
60% 67.6 83.3 88.6 94.9 / 52.4 92.6 / 75.8 92.3 / 85.1
40% 51.0 79.5 84.7 94.9 / 34.9 87.6 / 72.7 93.0 / 77.8
20% 29.6 69.8 73.9 81.5 / 18.1 90.6 / 56.8 94.0 / 60.8
Table 3: F-scores on the query suggestion task. As in the commercial intent task, the proposed hybrid
method does no harm when there is plenty of training data, but generalizes better when training data is
severely limited.
Antonellis et al, 2008) improves classification.
Similar methods could be applied in future work
to many other applications such labeling queries
and URLs by: language, market, location, time,
intended for a search vertical (such as medicine,
recipes), intended for a type of answer (maps, pic-
tures), as well as inappropriate intent (porn, spam).
In addition to click type, there are many other
features in the logs that could prove useful for
classifying queries by intent, e.g., who issued the
query, when and where. Similar methods could
also be used to personalize search (Teevan et al,
2008); for queries that mean different things to dif-
ferent people, the Yarowsky method could be ap-
plied to variables such as user, time and place, so
the results reflect what a particular user intended
in a particular context.
7 Acknowledgments
We thank Sue Dumais for her helpful comments
on an early draft of this work. We would also like
to thank the members of the Text Mining, Search,
and Navigation (TMSN) group at Microsoft Re-
search for useful discussions and the anonymous
reviewers for their helpful comments.
References
I. Antonellis, H. Garcia-Molina, and C.C. Chang.
2008. Simrank++: query rewriting through link
analysis of the clickgraph (poster). WWW.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008.
Video suggestion and discovery for youtube: taking
random walks through the view graph. WWW.
D. Beeferman and A. Berger. 2000. Agglomerative
clustering of a search engine query log. In SIGKDD,
pages 407?416.
S.M. Beitzel, E.C. Jensen, A. Chowdhury, D. Gross-
man, and O. Frieder. 2004. Hourly analysis of a
very large topically categorized web query log. SI-
GIR, pages 321?328.
S.M. Beitzel, E.C. Jensen, O. Frieder, D.D. Lewis,
A. Chowdhury, and A. Kolcz. 2005. Improving
automatic query classification via semi-supervised
learning. ICDM, pages 42?49.
A.Z. Broder, M. Fontoura, E. Gabrilovich, A. Joshi,
V. Josifovski, and T. Zhang. 2007. Robust classifi-
cation of rare queries using web knowledge. SIGIR,
pages 231?238.
A. Broder. 2002. A taxonomy of web search. SIGIR,
36(2).
R. Chaiken, B. Jenkins, P.
?
A. Larson, B. Ramsey,
D. Shakib, S. Weaver, and J. Zhou. 2008. SCOPE:
1435
Easy and efficient parallel processing of massive
data sets. Proceedings of the VLDB Endowment
archive, 1(2):1265?1276.
D. Chakrabarti, D. Agarwal, and V. Josifovski. 2008.
Contextual advertising by combining relevance with
click feedback. WWW.
M. Ciaramita, V. Murdock, and V. Plachouras. 2008.
Online learning from click data for sponsored
search.
N. Craswell and M. Szummer. 2007. Random walks
on the click graph. In Proceedings of the 30th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 239?246.
N. Craswell, R. Jones, G. Dupret, and E. Viegas (Con-
ference Chairs). 2009. Wscd ?09: Proceedings of
the 2009 workshop on web search click data.
H.K. Dai, L. Zhao, Z. Nie, J.R. Wen, L. Wang, and
Y. Li. 2006. Detecting online commercial intention
(OCI). WWW, pages 829?837.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified Data Processing on Large Clusters. OSDI,
pages 137?149.
D. Downey, S. Dumais, D. Liebling, and E. Horvitz.
2008. Understanding the relationship between
searchers? queries and information goals. In CIKM.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD), ACM.
U. Lee, Z. Liu, and J. Cho. 2005. Automatic identifi-
cation of user goals in Web search. In WWW, pages
391?400.
X. Li, Y.Y. Wang, and A. Acero. 2008. Learning
query intent from regularized click graphs. In SI-
GIR, pages 339?346.
B. Piwowarski and H. Zaragoza. 2007. Predictive user
click models based on click-through history. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
pages 175?182.
J. Rela?no, D. Tapias, M. Rodr??guez, M. Charfuel?an,
and L. Hern?andez. 1999. Robust and flexible
mixed-initiative dialogue for telephone services. In
Proceedings of EACL.
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. WWW, pages 13?19.
K. Scheffler and S. Young. 2002. Automatic learn-
ing of dialogue strategy using dialogue simulation
and reinforcement learning. In Proceedings of HLT,
pages 12?19.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with reinforce-
ment learning: Experiments with the NJFun sys-
tem. Journal of Artificial Intelligence Research,
16(1):105?133.
Bin Tan, Xuehua Shen, and ChengXiang Zhai. 2006.
Mining long-term search history to improve search
accuracy. pages 718?723. KDD.
J. Teevan, S.T. Dumais, and D.J. Liebling. 2008. To
personalize or not to personalize: modeling queries
with variation in user intent. SIGIR, pages 163?170.
Gui-Rong Xue, Hua-Jun Zeng, Zheng Chen, Yong
Yu, Wei-Ying Ma, WenSi Xi, and WeiGuo Fan.
2004. Optimizing web search using web click-
through data. In CIKM ?04: Proceedings of the thir-
teenth ACM international conference on Informa-
tion and knowledge management, pages 118?126.
D. Yarowsky. 1994. Decision lists for lexical ambigu-
ity resolution: Application to accent restoration in
Spanish and French. ACL, pages 88?95.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. ACL, pages
189?196.
D. Zhou, O. Bousquet, T.N. Lal, J. Weston, and
B. Scholkopf. 2004. Learning with Local and
Global Consistency. In NIPS.
1436
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, page 1,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Invited Presentation
Repetition and Language Models and Comparable Corpora
Ken Church
Human Language Technology Center of Excellence
Johns Hopkins University
Kenneth.Church@jhu.edu
I will discuss a couple of non-standard fea-
tures that I believe could be useful for working
with comparable corpora. Dotplots have been
used in biology to find interesting DNA sequences.
Biology is interested in ordered matches, which
show up as (possibly broken) diagonals in dot-
plots. Information Retrieval is more interested in
unordered matches (e.g., cosine similarity), which
show up as squares in dotplots. Parallel corpora
have both squares and diagonals multiplexed to-
gether. The diagonals tell us what is a translation
of what, and the squares tell us what is in the same
language. I would expect dotplots of compara-
ble corpora would contain lots of diagonals and
squares, though the diagonals would be shorter
and more subtle in comparable corpora than in par-
allel corpora.
There is also an opportunity to take advantage
of repetition in comparable corpora. Repetition is
very common. Standard bag-of-word models in
Information Retrieval do not attempt to model dis-
course structure such as given/new. The first men-
tion in a news article (e.g., ?Manuel Noriega, for-
mer President of Panama?) is different from sub-
sequent mentions (e.g., ?Noriega?). Adaptive lan-
guage models were introduced in Speech Recogni-
tion to capture the fact that probabilities change or
adapt. After we see the first mention, we should
expect a subsequent mention. If the first men-
tion has probability p, then under standard (bag-
of-words) independence assumptions, two men-
tions ought to have probability p2, but we find
the probability is actually closer to p/2. Adapta-
tion matters more for meaningful units of text. In
Japanese, words (meaningful sequences of char-
acters) are more likely to be repeated than frag-
ments (meaningless sequences of characters from
words that happen to be adjacent). In newswire,
we find more adaptation for content words (proper
nouns, technical terminology and good keywords
for information retrieval), and less adaptation for
function words, clich?s and ordinary first names.
There is more to meaning than frequency. Content
words are not only low frequency, but likely to be
repeated.
1
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 886?894,
Beijing, August 2010
Using Web-scale N-grams to Improve Base NP Parsing Performance
Emily Pitler
Computer and Information Science
University of Pennsylvania
epitler@seas.upenn.edu
Shane Bergsma
Department of Computing Science
University of Alberta
sbergsma@ualberta.ca
Dekang Lin
Google, Inc.
lindek@google.com
Kenneth Church
Human Language Technology Center of Excellence
Johns Hopkins University
kenneth.church@jhu.edu
Abstract
We use web-scale N-grams in a base NP
parser that correctly analyzes 95.4% of the
base NPs in natural text. Web-scale data
improves performance. That is, there is no
data like more data. Performance scales
log-linearly with the number of parame-
ters in the model (the number of unique
N-grams). The web-scale N-grams are
particularly helpful in harder cases, such
as NPs that contain conjunctions.
1 Introduction
Noun phrases (NPs) provide an index to the
world?s information. About 70% of web queries
are NPs (Barr et al, 2008). A robust NP parser
could help search engines improve retrieval per-
formance on multi-word NP queries (Zhai, 1997).
For example, by knowing the correct parse of
?washed (baby carrots),? a search engine could
ensure that returned pages (and advertisements)
concern clean carrots rather than clean babies. NP
structure is also helpful for query expansion and
substitution (Jones et al, 2006).
This paper is concerned with base NP pars-
ing. We are given a base NP string as input,
and the task is to produce a parse tree as output.
Base NPs are NPs that do not contain embedded
noun phrases. These are sometimes called NP
chunks, or core/non-recursive NPs (Church, 1988;
Ramshaw and Marcus, 1995). Correctly parsing
(or, equivalently, bracketing) base NPs is chal-
lenging because the same part-of-speech (POS)
sequence can be parsed differently depending on
the specific words involved. For example, ?retired
(science teacher)? and ?(social science) teacher?
have different structures even though they have
identical POS sequences.
Lexical statistics are therefore needed in order
to parse the above examples, and they must be
computed over a lot of text to avoid sparsity. All
of our lexical statistics are derived from a new
and improved web-scale N-gram corpus (Lin et
al., 2010), which we call Google V2.
Despite the importance of base NPs, most
sentence parsers do not parse base NPs, since
the main training corpus for parsers, the Penn
Treebank (PTB) (Marcus et al, 1994), leaves a
flat structure for base NPs. Recent annotations
by Vadas and Curran (2007a) added NP structure
to the PTB. We use these annotations (described
in Section 3) for our experiments.
NP parsers usually focus on bracketing three-
word noun compounds. Parsing three-word noun
compounds is a fairly artificial task; we show that
sequences of three nouns make up less than 1%
of the three-word-or-longer base NPs in natural
text. As the NP length increases, the number of
possible binary trees (parses) increases with the
Catalan numbers (Church and Patil, 1982). NPs of
length three have just two possible parses (chance
is 50%), while NPs of length six already have
forty-two possible parses (chance is 2%). Long
NPs therefore provide much more opportunity to
improve performance over the baseline. In Table
1 (Section 7), we show the distribution of base NP
length in the PTB. While most NPs are of length
three, NP length has a long tail.
886
The three-word noun compound assumption
also restricts research to the case in which all
words are nouns, while base NPs also contain de-
terminers, possessives, adjectives, and conjunc-
tions. Conjunctions and their scopes are particu-
larly challenging. For example, in the NP, ?French
television and movie producers,? a parser should
conjoin ?(television) and (movie),? as opposed to
?(French television) and (movie),? ?(French tele-
vision) and (movie producers)? or ?(television)
and (movie producers).?
To resolve these issues, we train a classifier
which uses contextual information from the entire
NP and lexical statistics derived from the web-
scale N-gram corpus to predict if a given span
is a constituent. Our parser then uses this clas-
sifier to produce a score for every possible NP-
internal bracketing and creates a chart of bracket-
ing scores. This chart can be used as features in a
full sentence parser or parsed directly with a chart
parser. Our parses are highly accurate, creating a
strong new standard for this task.
Finally, we present experiments that investigate
the effects of N-gram frequency cutoffs and vari-
ous sources of N-gram data. We show an interest-
ing relationship between accuracy and the number
of unique N-gram types in the data.
2 Related Work
2.1 Three-Word Noun Compounds
The most commonly used data for NP parsing is
from Lauer (1995), who extracted 244 three-word
noun compounds from the Grolier encyclopedia.
When there are only three words, this task reduces
to a binary decision:
? Left Branching: * [retired science] teacher
? Right Branching: retired [science teacher]
In Lauer (1995)?s set of noun compounds, two-
thirds are left branching.
The main approach to these three-word noun
compounds has been to compute association
statistics between pairs of words and then choose
the bracketing that corresponds to the more highly
associated pair. The two main models are the
adjacency model (Marcus, 1980; Liberman and
Sproat, 1992; Pustejovsky et al, 1993; Resnik,
1993) and the dependency model (Lauer, 1995).
Under the adjacency model, the bracketing deci-
sion is made by comparing the associations be-
tween words one and two versus words two and
three (i.e. comparing retired science versus sci-
ence teacher). In contrast, the dependency model
compares the associations between one and two
versus one and three (retired science versus retired
teacher). Lauer (1995) compares the two models
and finds the dependency model to be more accu-
rate.
Nakov and Hearst (2005) compute the associ-
ation scores using frequencies, conditional proba-
bilities, ?2, and mutual information, for both pairs
of words and for linguistically-motivated para-
phrases. Lapata and Keller (2005) found that us-
ing web-scale data for associations is better than
using the (smaller) 100M-word British National
Corpus.
2.2 Longer NPs
Focusing on only the three word case misses a
large opportunity for base NP parsing. NPs longer
than three words commonly occur, making up
29% of our test set. In addition, a chance baseline
does exponentially worse as the length of the NP
increases. These longer NPs are therefore a major
opportunity to improve overall base NP parsing.
Since in the general case, NP parsing can no
longer be thought of as a single binary classifica-
tion problem, different strategies are required.
Barker (1998) reduces the task of parsing
longer NPs to making sequential three-word de-
cisions, moving a sliding window along the NP.
The window is first moved from right-to-left, in-
serting right bracketings, and then again from left-
to-right, finalizing left bracketings. While Barker
(1998) assumes that these three-word decisions
can be made in isolation, this is not always valid.1
Vadas and Curran (2007b) employ Barker?s algo-
rithm, but use a supervised classifier to make the
sequential bracketing decisions. Because these
approaches rely on a sequence of binary decisions,
1E.g., although the right-most three words are identical
in 1) ?soap opera stars and television producers,? and 2)
?movie and television producers,? the initial right-bracketing
decision for ?and television producers? should be different
in each.
887
early mistakes can cascade and lead to a chain of
incorrect bracketings.
Our approach differs from previous work in NP
parsing; rather than greedily inserting brackets as
in Barker?s algorithm, we use dynamic program-
ming to find the global maximum-scoring parse.
In addition, unlike previous approaches that have
used local features to make local decisions, we use
the full NP to score each potential bracketing.
A related line of research aims to segment
longer phrases that are queried on Internet search
engines (Bergsma and Wang, 2007; Guo et al,
2008; Tan and Peng, 2008). Bergsma and Wang
(2007) focus on NP queries of length four or
greater. They use supervised learning to make
segmentation decisions, with features derived
from the noun compound bracketing literature.
Evaluating the benefits of parsing NP queries,
rather than simply segmenting them, is a natural
application of our system.
3 Annotated Data
Our training and testing data are derived from re-
cent annotations by Vadas and Curran (2007a).
The original PTB left a flat structure for base noun
phrases. For example, ?retired science teacher,?
would be represented as:
(NP (JJ retired) (NN science) (NN teacher))
Vadas and Curran (2007a) annotated NP-internal
structure by adding annotations whenever there is
a left-bracketing. If no annotations were added,
right-branching is assumed. The inter-annotator
agreement for exactly matching the brackets on an
NP was 98.5%.
This data provides a valuable new resource for
parsing research, but little work has so far made
use of it. Vadas and Curran (2007b) perform
some preliminary experiments on NP bracketing,
but use gold standard part-of-speech and named-
entity annotations as features in their classifier.
Our work establishes a strong and realistic stan-
dard on this data; our results will serve as a basis
for further research on this topic.
4 Unlabeled N-gram Data
All of our N-gram features described in Sec-
tion 6.1 rely on probabilities derived from unla-
beled data. To use the largest amount of data
possible, we exploit web-scale N-gram corpora.
N-gram counts are an efficient way to compress
large amounts of data (such as all the text on the
web) into a manageable size. An N-gram corpus
records how often each unique sequence of words
occurs. Co-occurrence probabilities can be calcu-
lated directly from the N-gram counts. To keep
the size manageable, N-grams that occur with a
frequency below a particular threshold can be fil-
tered.
The corpus we use is Google V2 (Lin et al,
2010): a new N-gram corpus with N-grams of
length 1-5 that we created from the same 1 tril-
lion word snapshot of the web as Google N-grams
Version 1 (Brants and Franz, 2006), but with sev-
eral enhancements. Duplicate sentences are re-
moved, as well as ?sentences? which are probably
noise (indicated by having a large proportion of
non-alphanumeric characters, being very long, or
being very short). Removing duplicate sentences
is especially important because automatically-
generated websites, boilerplate text, and legal dis-
claimers skew the source web data, with sentences
that may have only been authored once occurring
millions of times. We use the suffix array tools
described in Lin et al (2010) to quickly extract
N-gram counts.
5 Base NP Parsing Approach
Our goal is to take a base NP string as input and
produce a parse tree as output. In practice, it
would be most useful if the NP parse could be
integrated into a sentence parser. Previous NP
parsers are difficult to apply in practice.2 Work
in prepositional phrase attachment that assumes
gold-standard knowledge of the competing attach-
ment sites has been criticized as unrealistic (At-
terer and Schu?tze, 2007).
Our system can easily be integrated into full
parsers. Its input can be identified quickly and
reliably and its output is compatible with down-
stream parsers.
2For example, Vadas and Curran (2007b) report results on
NP parsing, but these results include NPs containing preposi-
tional or adverbial phrases (confirmed by personal communi-
cation). Practical application of their system would therefore
require resolving prepositional phrase attachment as a pre-
processing step.
888
Our parser?s input is base NPs, which can be
identified with very high accuracy. Kudo and Mat-
sumoto (2001) report 95.8% NP chunking accu-
racy on PTB data.
Once provided with an NP, our system uses a
supervised classifier to predict the probability of
a particular contiguous subsequence (span) of the
NP being a constituent, given the entire NP as con-
text. This probability can be inserted into the chart
that a standard chart parser would use.
For example, the base NP ?French television
and movie producers? would be decomposed into
nine different classification problems, scoring the
following potential bracketings:
(French television) and movie producers
French (television and) movie producers
(French television and) movie producers ...
French television and (movie producers)
In Section 6, we detail the set of statistical and
structural features used by the classifier.
The output of our classifier can be easily used
as a feature in a full-sentence structured prediction
parser, as in Taskar et al (2004). Alternatively,
our work could be integrated into a full-sentence
parser by using our feature representations di-
rectly in a discriminative CFG parser (Finkel et
al., 2008), or in a parse re-ranker (Ratnaparkhi et
al., 1994; Collins and Koo, 2005; Charniak and
Johnson, 2005).
While our main objective is to use web-scale
lexical statistics to create an accurate classifier for
base NP-internal constituents, we do produce a
parse tree for evaluation purposes. The probabil-
ity of a parse tree is defined as the product of the
probabilities of all the spans (constituents) in the
tree. The most probable tree is computed with the
CYK algorithm.
6 Features
Over the course of development experiments, we
discovered that the more position-specific our fea-
tures were, the more effectively we could parse
NPs. We define a word?s position as its distance
from the right of the full NP, as the semantic head
of NPs is most often the right-most word. Ulti-
mately, we decided to conjoin each feature with
the position of the proposed bracketing. Since
the features for differing proposed bracketings are
now disjoint, this is equivalent to scoring bracket-
ings with different classifiers, with each classifier
chosen according to the bracketing position. We
now outline the feature types that are common,
but weighted differently, in each proposed brack-
eting?s feature set.
6.1 N-gram Features
All of the features described in this section require
estimates of the probability of specific words or
sequences of words. All probabilities are com-
puted using Google V2 (Section 4).
6.1.1 PMI
Recall that the adjacency model for the three-
word task uses the associations of the two pairs of
adjacent words, while the dependency model uses
the associations of the two pairs of attachment
sites for the initial noun. We generalize the ad-
jacency and dependency models by including the
pointwise mutual information (Church and Hanks,
1990) between all pairs of words in the NP:
PMI(x, y) = log p(?x y?)p(?x?)p(?y?) (1)
For NPs of length n, for each proposed bracket-
ing, we include separate features for the PMI be-
tween all
(n
2
)
pairs of words in the NP. For NPs in-
cluding conjunctions, we include additional PMI
features (Section 6.1.2).
Since these features are also tied to the pro-
posed bracketing positions (as explained above),
this allows us to learn relationships between var-
ious associations within the NP and each poten-
tial bracketing. For example, consider a proposed
bracketing from word 4 to word 5. We learn that
a high association of words inside a bracketing
(here, a high association between word 4 and word
5) indicates a bracketing is likely, while a high
association between words that cross a proposed
bracketing (e.g., a high association between word
3 and word 4) indicates the bracketing is unlikely.
The value of these features is the PMI, if it is
defined. If the PMI is undefined, we include one
of two binary features:
p(?x y?) = 0 or p(?x?) ? p(?y?) = 0.
889
We illustrate the PMI features with an example.
In deciding whether (movie producers) is a rea-
sonable bracketing within ?French television and
movie producers,? the classifier weighs features
for all of:
PMI(French, television)
PMI(French, and)
. . .
PMI(television, producers)
PMI(and, producers)
PMI(movie, producers)
6.1.2 Conjunctions
Properly handling NPs containing conjunc-
tions (NP+conj) requires special statistical fea-
tures. For example, television and movie are
commonly conjoined, but the relevant statistics
that suggest placing brackets around the phrase
?television and movie? are not provided by the
above PMI features (i.e., this is not clear from
PMI(television, and), PMI(television, movie), nor
PMI(and, movie)). Rather, we want to know if the
full phrase ?television and movie? is common.
We thus have additional NP+conj features that
consider the PMI association across the word and:
PMIand(x, y) = log
p(?x and y?)
p(?x and?)p(?and y?) (2)
When PMIand between a pair of words is high,
they are likely to be the constituents of a conjunc-
tion.
Let NP=(w1 . . . wi?1, ?and?, wi+1 . . . wn) be
an NP+conj. We include the PMIand features be-
tween wi?1 and all w ? wi+1 . . . wn. In the exam-
ple ?French television and movie producers,? we
would include features PMIand(television, movie)
and PMIand(television, producers).
In essence, we are assuming wi?1 is the head
of one of the items being conjoined, and we score
the likelihood of each of the words to the right
of the and being the head for the other item. In
our running example, the conjunction has narrow
scope, and PMIand(television, movie) is greater
than PMIand(television, producers), indicating to
our classifier that (television and movie) is a good
bracketing. In other examples the conjunction will
join heads that are further apart, as in ((French TV)
and (British radio)) stars, where both of the fol-
lowing hold:
PMIand(TV, radio) > PMIand(TV, British)
PMIand(TV, radio) > PMIand(TV, stars)
6.2 Lexical
We include a binary feature to indicate the pres-
ence of a particular word at each position in the
NP. We learn that, for instance, the word Inc. in
names tends to occur outside of brackets.
6.3 Shape
Previous work on NP bracketing has used gold-
standard named entity tags (Vadas and Curran,
2007b) as features. We did not want to use any
gold-standard features in our experiments, how-
ever NER information is helpful in separating pre-
modifiers from names, i.e. (news reporter) (Wal-
ter Cronkite).
As an expedient way to get both NER informa-
tion and useful information from hyphenated ad-
jectives, abbreviations, and other punctuation, we
normalize each string using the following regular
expressions:
[A-Z]+ ? A [a-z]+ ? a
We use this normalized string as an indicator
feature. E.g. the word ?Saudi-born? will fire the
binary feature ?Aa-a.?
6.4 Position
We also include the position of the proposed
bracketing as a feature. This represents the prior
of a particular bracketing, regardless of the actual
words.
7 Experiments
7.1 Experimental Details
We use Vadas and Curran (2007a)?s annotations
(Section 3) to create training, development and
testing data for base NPs, using standard splits of
the Penn Treebank (Table 1). We consider all non-
trivial base NPs, i.e., those longer than two words.
For training, we expand each NP in our train-
ing set into independent examples corresponding
to all the possible internal NP-bracketings, and
represent these examples as feature vectors (Sec-
tion 5). Each example is positively labeled if it is
890
Data Set Train Dev Test Chance
PTB Section 2-22 24 23
Length=3 41353 1428 2498 50%
Length=4 12067 445 673 20%
Length=5 3930 148 236 7%
Length=6 1272 34 81 2%
Length>6 616 29 34 < 1%
Total NPs 59238 2084 3522
Table 1: Breakdown of the PTB base NPs used in
our experiments. Chance = 1/Catalan(length).
Features All NPs NP+conj NP-conj
All features 95.4 89.7 95.7
-N-grams 94.0 84.0 94.5
-lexical 92.2 87.4 92.5
-shape 94.9 89.7 95.2
-position 95.3 89.7 95.6
Right 72.6 58.3 73.5bracketing
Table 2: Accuracy (%) of base NPs parsing; abla-
tion of different feature classes.
consistent with the gold-standard bracketing, oth-
erwise it is a negative example.
We train using LIBLINEAR, an efficient linear
Support Vector Machine (SVM).3 We use an L2-
loss function, and optimize the regularization pa-
rameter on the development set (reaching an opti-
mum at C=1). We converted the SVM output to
probabilities.4 Perhaps surprisingly, since SVMs
are not probabilistic, performance on the devel-
opment set with these SVM-derived probabilities
was higher than using probabilities from the LIB-
LINEAR logistic regression solver.
At test time, we again expand the NPs and cal-
culate the probability of each constituent, insert-
ing the score into a chart. We run the CYK algo-
rithm to find the most probable parse of the entire
NP according to the chart. Our evaluation metric
is Accuracy: the proportion of times our proposed
parse of the NP exactly matches the gold standard.
8 Results
8.1 Base NPs
Our method improves substantially over the base-
line of assuming a completely right-branching
structure, 95.4% versus 72.6% (Table 2). The ac-
curacy of the constituency classifier itself (before
the CYK parser is used) is 96.1%.
The lexical features are most important, but all
feature classes are somewhat helpful. In particu-
lar, including N-gram PMI features significantly
improves the accuracy, from 94.0% to 95.4%.5
Correctly parsing more than 19 base NPs out of 20
is an exceptional level of accuracy, and provides a
strong new standard on this task. The most com-
parable result is by Vadas and Curran (2007b),
who achieved 93.0% accuracy on a different set of
PTB noun phrases (see footnote 2), but their clas-
sifier used features based on gold-standard part-
of-speech and named-entity information.
Exact match is a tough metric for parsing, and
the difficulty increases as the length of the NP
increases (because there are more decisions to
make correctly). At three word NPs, our accu-
racy is 98.5%; by six word NPs, our accuracy
drops to 79.0% (Figure 1). Our method?s accu-
racy decreases as the length of the NP increases,
but much less rapidly than a right-bracketing or
chance baseline.
8.2 Base NPs with Conjunctions
N-gram PMI features help more on NP+conj than
on those that do not contain conjunctions (NP-
conj) (Table 2). N-gram PMI features are the most
important features for NP+conj, increasing accu-
racy from 84.0% to 89.7%, a 36% relative reduc-
tion in error.
8.3 Effect of Thresholding N-gram data
We now address two important related questions:
1) how does our parser perform as the amount
of unlabeled auxiliary data varies, and 2) what
is the effect of thresholding an N-gram corpus?
The second question is of widespread relevance as
3www.csie.ntu.edu.tw/
?
cjlin/liblinear/
4Following instructions in http://www.csie.ntu.
edu.tw/
?
cjlin/liblinear/FAQ.html
5McNemar?s test, p < 0.05
891
 1
 10
 100
6543
Ac
cu
ra
cy
 (%
)
Length of Noun Compound (words)
Proposed
Right-bracketing
Chance
Figure 1: Accuracy (log scale) over different NP
lengths, of our method, the right-bracketing base-
line, and chance (1/Catalan(length)).
thresholded N-gram corpora are now widely used
in NLP. Without thresholds, web-scale N-gram
data can be unmanageable.
While we cannot lower the threshold after cre-
ating the N-gram corpus, we can raise it, filtering
more N-grams, and then measure the relationship
between threshold and performance.
Threshold Unique N-grams Accuracy
10 4,145,972,000 95.4%
100 391,344,991 95.3%
1,000 39,368,488 95.2%
10,000 3,924,478 94.8%
100,000 386,639 94.8%
1,000,000 37,567 94.4%
10,000,000 3,317 94.0%
Table 3: There is no data like more data. Accuracy
improves with the number of parameters (unique
N-grams).
We repeat the parsing experiments while in-
cluding in our PMI features only N-grams with
a count ?10 (the whole data set), ?100, ?1000,
. . ., ?107. All other features (lexical, shape, posi-
tion) remain unchanged. The N-gram data almost
perfectly exhibits Zipf?s power law: raising the
threshold by a factor of ten decreases the number
of unique N-grams by a factor of ten (Table 3).
The improvement in accuracy scales log-linearly
with the number of unique N-grams. From a prac-
tical standpoint, we see a trade-off between stor-
Corpus # of tokens ? # of types
NEWS 3.2 B 1 3.7 B
Google V1 1,024.9 B 40 3.4 B
Google V2 207.4 B 10 4.1 B
Table 4: N-gram data, with total number of words
(tokens) in the original corpus (in billions, B), fre-
quency threshold used to filter the data, ? , and to-
tal number of unique N-grams (types) remaining
in the data after thresholding.
age and accuracy. There are consistent improve-
ments in accuracy from lowering the threshold
and increasing the amount of auxiliary data. If for
some application it is necessary to reduce storage
by several orders of magnitude, then one can eas-
ily estimate the resulting impact on performance.
We repeat the thresholding experiments using
two other N-gram sources:
NEWS: N-gram data created from a large set
of news articles including the Reuters and Giga-
word (Graff, 2003) corpora, not thresholded.
Google V1: The original web-scale N-gram
corpus (Section 4).
Details of these sources are given in Table 4.
For a given number of unique N-grams, using
any of the three sources does about the same (Fig-
ure 2). It does not matter that the source corpus
for Google V1 is about five times larger than the
source corpus for Google V2, which in turn is
sixty-five times larger than NEWS (Table 4). Ac-
curacies increase linearly with the log of the num-
ber of types in the auxiliary data set.
Google V1 is the one data source for which
the relationship between accuracy and number of
N-grams is not monotonic. After about 100 mil-
lion unique N-grams, performance starts decreas-
ing. This drop shows the need for Google V2.
Since Google V1 contains duplicated web pages
and sentences, mistakes that should be rare can
appear to be quite frequent. Google V2, which
comes from the same snapshot of the web as
Google V1, but has only unique sentences, does
not show this drop.
We regard the results in Figure 2 as a compan-
ion to Banko and Brill (2001)?s work on expo-
nentially increasing the amount of labeled train-
ing data. Here we see that varying the amount of
892
 94
 94.5
 95
 95.5
 96
1e91e81e71e61e51e4
Ac
cu
ra
cy
 (%
)
Number of Unique N-grams
Google V1
Google V2
NEWS
Figure 2: There is no data like more data. Ac-
curacy improves with the number of parameters
(unique N-grams). This trend holds across three
different sources of N-grams.
unlabeled data can cause an equally predictable
improvement in classification performance, with-
out the cost of labeling data.
Suzuki and Isozaki (2008) also found a log-
linear relationship between unlabeled data (up to
a billion words) and performance on three NLP
tasks. We have shown that this trend continues
well beyond Gigaword-sized corpora. Brants et
al. (2007) also found that more unlabeled data (in
the form of input to a language model) leads to
improvements in BLEU scores for machine trans-
lation.
Adding noun phrase parsing to the list of prob-
lems for which there is a ?bigger is better? rela-
tionship between performance and unlabeled data
shows the wide applicability of this principle. As
both the amount of text on the web and the power
of computer architecture continue to grow expo-
nentially, collecting and exploiting web-scale aux-
iliary data in the form of N-gram corpora should
allow us to achieve gains in performance linear in
time, without any human annotation, research, or
engineering effort.
9 Conclusion
We used web-scale N-grams to produce a new
standard in performance of base NP parsing:
95.4%. The web-scale N-grams substantially im-
prove performance, particularly in long NPs that
include conjunctions. There is no data like more
data. Performance improves log-linearly with the
number of parameters (unique N-grams). One can
increase performance with larger models, e.g., in-
creasing the size of the unlabeled corpora, or by
decreasing the frequency threshold. Alternatively,
one can decrease storage costs with smaller mod-
els, e.g., decreasing the size of the unlabeled cor-
pora, or by increasing the frequency threshold. Ei-
ther way, the log-linear relationship between accu-
racy and model size makes it easy to estimate the
trade-off between performance and storage costs.
Acknowledgments
We gratefully acknowledge the Center for Lan-
guage and Speech Processing at Johns Hopkins
University for hosting the workshop at which this
research was conducted.
References
Atterer, M. and H. Schu?tze. 2007. Prepositional
phrase attachment without oracles. Computational
Linguistics, 33(4):469?476.
Banko, M. and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In ACL.
Barker, K. 1998. A trainable bracketer for noun mod-
ifiers. In Twelfth Canadian Conference on Artificial
Intelligence (LNAI 1418).
Barr, C., R. Jones, and M. Regelson. 2008. The lin-
guistic structure of English web-search queries. In
EMNLP.
Bergsma, S. and Q.I. Wang. 2007. Learning noun
phrase query segmentation. In EMNLP-CoNLL.
Brants, T. and A. Franz. 2006. The Google Web 1T
5-gram Corpus Version 1.1. LDC2006T13.
Brants, T., A.C. Popat, P. Xu, F.J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In EMNLP.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In ACL.
Church, K.W. and P. Hanks. 1990. Word associa-
tion norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Church, K. and R. Patil. 1982. Coping with syntactic
ambiguity or how to put the block in the box on the
table. Computational Linguistics, 8(3-4):139?149.
893
Church, K.W. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In ANLP.
Collins, M. and T. Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational
Linguistics, 31(1):25?70.
Finkel, J.R., A. Kleeman, and C.D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In ACL.
Graff, D. 2003. English Gigaword. LDC2003T05.
Guo, J., G. Xu, H. Li, and X. Cheng. 2008. A unified
and discriminative model for query refinement. In
SIGIR.
Jones, R., B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In WWW.
Kudo, T. and Y. Matsumoto. 2001. Chunking with
support vector machines. In NAACL.
Lapata, M. and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1?31.
Lauer, M. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In ACL.
Liberman, M. and R. Sproat. 1992. The stress and
structure of modified noun phrases in English. Lex-
ical matters, pages 131?181.
Lin, D., K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2010. New tools for
web-scale n-grams. In LREC.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Marcus, M.P. 1980. Theory of Syntactic Recogni-
tion for Natural Languages. MIT Press, Cambridge,
MA, USA.
Nakov, P. and M. Hearst. 2005. Search engine statis-
tics beyond the n-gram: Application to noun com-
pound bracketing. In CoNLL.
Pustejovsky, J., P. Anick, and S. Bergler. 1993. Lex-
ical semantic techniques for corpus analysis. Com-
putational Linguistics, 19(2):331?358.
Ramshaw, L.A. and M.P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In 3rd
ACL Workshop on Very Large Corpora.
Ratnaparkhi, A., S. Roukos, and R.T. Ward. 1994.
A maximum entropy model for parsing. In Third
International Conference on Spoken Language Pro-
cessing.
Resnik, P. 1993. Selection and information: a class-
based approach to lexical relationships. Ph.D. the-
sis, University of Pennsylvania.
Suzuki, J. and H. Isozaki. 2008. Semi-supervised se-
quential labeling and segmentation using giga-word
scale unlabeled data. In ACL.
Tan, B. and F. Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In WWW.
Taskar, B., D. Klein, M. Collins, D. Koller, and
C. Manning. 2004. Max-margin parsing. In
EMNLP.
Vadas, D. and J.R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In ACL.
Vadas, D. and J.R. Curran. 2007b. Large-scale su-
pervised models for noun phrase bracketing. In PA-
CLING.
Zhai, C. 1997. Fast statistical parsing of noun phrases
for document indexing. In ANLP.
894
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 460?470,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
NLP on Spoken Documents without ASR
Mark Dredze, Aren Jansen, Glen Coppersmith, Ken Church
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
mdredze,aren,coppersmith,Kenneth.Church@jhu.edu
Abstract
There is considerable interest in interdis-
ciplinary combinations of automatic speech
recognition (ASR), machine learning, natu-
ral language processing, text classification and
information retrieval. Many of these boxes,
especially ASR, are often based on consid-
erable linguistic resources. We would like
to be able to process spoken documents with
few (if any) resources. Moreover, connect-
ing black boxes in series tends to multiply er-
rors, especially when the key terms are out-of-
vocabulary (OOV). The proposed alternative
applies text processing directly to the speech
without a dependency on ASR. The method
finds long (? 1 sec) repetitions in speech,
and clusters them into pseudo-terms (roughly
phrases). Document clustering and classi-
fication work surprisingly well on pseudo-
terms; performance on a Switchboard task ap-
proaches a baseline using gold standard man-
ual transcriptions.
1 Introduction
Can we do IR-like tasks without ASR? Information
retrieval (IR) typically makes use of simple features
that count terms within/across documents such as
term frequency (tf) and inverse document frequency
(IDF). Crucially, to compute these features, it is suf-
ficient to count repetitions of a term. In particular,
for many IR-like tasks, there is no need for an au-
tomatic speech recognition (ASR) system to label
terms with phonemes and/or words.
This paper builds on Jansen et al (2010), a
method for discovering terms with zero resources.
This approach identifies long, faithfully repeated
patterns in the acoustic signal. These acoustic repe-
titions often correspond to terms useful for informa-
tion retrieval tasks. Critically, this method does not
require a phonetically interpretable acoustic model
or knowledge of the target language.
By analyzing a large untranscribed corpus of
speech, this discovery procedure identifies a vast
number of repeated regions that are subsequently
grouped using a simple graph-based clustering
method. We call the resulting groups pseudo-terms
since they typically represent a single word or phrase
spoken at multiple points throughout the corpus.
Each pseudo-term takes the place of a word or
phrase in bag of terms vector space model of a text
document, allowing us to apply standard NLP algo-
rithms. We show that despite the fully automated
and noisy method by which the pseudo-terms are
created, we can still successfully apply NLP algo-
rithms with performance approaching that achieved
with the gold standard manual transcription.
Natural language processing tools can play a key
role in understanding text document collections.
Given a large collection of text, NLP tools can clas-
sify documents by category (classification) and or-
ganize documents into similar groups for a high
level view of the collection (clustering). For exam-
ple, given a collection of news articles, these tools
can be applied so that the user can quickly see the
topics covered in the news articles, and organize the
collection to find all articles on a given topic. These
tools require little or no human input (annotation)
and work across languages.
Given a large collection of speech, we would like
460
tools that perform many of the same tasks, allow-
ing the user to understand the contents of the col-
lection while listening to only small portions of the
audio. Previous work has applied these NLP tools
to speech corpora with similar results (see Hazen
et al (2007) and the references therein.) However,
unlike text, which requires little or no preprocess-
ing, audio files are typically first transcribed into
text before applying standard NLP tools. Automatic
speech recognition (ASR) solutions, such as large
vocabulary continuous speech recognition (LVCSR)
systems, can produce an automatic transcript from
speech, but they require significant development ef-
forts and training resources, typically hundreds of
hours of manually transcribed speech. Moreover,
the terms that may be most distinctive in particular
spoken documents often lie outside the predefined
vocabulary of an off-the-shelf LVCSR system. This
means that unlike with text, where many tools can be
applied to new languages and domains with minimal
effort, the equivalent tools for speech corpora often
require a significant investment. This greatly raises
the entry threshold for constructing even a minimal
tool set for speech corpora analysis.
The paper proceeds as follows. After a review
of related work, we describe Jansen et al (2010),
a method for finding repetitions in speech. We
then explain how these repetitions are grouped into
pseudo-terms. Document clustering and classifica-
tion work surprisingly well on pseudo-terms; perfor-
mance on a Switchboard task approaches a baseline
based on gold standard manual transcriptions.
2 Related Work
In the low resource speech recognition regime,
most approaches have focused on coupling small
amounts of orthographically transcribed speech (10s
of hours) with much larger collections of untran-
scribed speech (100s or 1000s of hours) to train ac-
curate acoustic models with semi-supervised meth-
ods (Novotney and Schwartz, 2009). In these ef-
forts, the goal is to reduce the annotation require-
ments for the construction of competent LVCSR sys-
tems. This semi-supervised paradigm was relaxed
even further with the pursuit of self organizing units
(SOUs), phone-like units for which acoustic mod-
els are trained with completely unsupervised meth-
ods (Garcia and Gish, 2006). Even though the move
away from phonetic acoustic models improves the
universality of the architecture, small amounts of or-
thographic transcription are still required to connect
the SOUs with the lexicon.
The segmental dynamic time warping (S-DTW)
algorithm (Park and Glass, 2008) was the first truly
zero resource effort, designed to discover portions of
the lexicon directly by searching for repeated acous-
tic patterns in the speech signal. This work im-
plicitly defined a new direction for speech process-
ing research: unsupervised spoken term discovery,
the entry point of our speech corpora analysis sys-
tem. Subsequent extensions of S-DTW (Jansen et
al., 2010) permit applications to much larger speech
collections, a flexibility that is vital to our efforts.
As mentioned above, the application of NLP
methods to speech corpora have traditionally relied
on high resource ASR systems to provide automatic
word or phonetic transcripts. Spoken document
topic classification has been an application of partic-
ular interest (Hazen et al, 2007), for which the rec-
ognized words or phone n-grams are used to charac-
terize the documents. These efforts have produced
admirable results, with ASR transcript-based per-
formance approached that obtained using the gold
standard manual transcripts. Early efforts to per-
form automatic topic segmentation of speech input
without the aid of ASR systems have been promis-
ing (Malioutov et al, 2007), but have yet to exploit
the full the range of NLP tools.
3 Identifying Matched Regions
Our goal is to identify pairs of intervals within and
across utterances of several speakers that contain
the same linguistic content, preferably meaningful
words or terms.
The spoken term discovery algorithm of Jansen et
al. (2010) efficiently searches the space of
(n
2
)
in-
tervals, where n is the number of speech frames.1
Jansen et al (2010) is based on dotplots (Church and
Helfman, 1993), a method borrowed from bioinfor-
matics for finding repetitions in DNA sequences.
1Typically, each frame represents a 25 or 30 ms window of
speech sampled every 10 ms
461
  
t e x t  p r o c e s s i n g  v s .  s p e e c h  p r o c e s s i n gt
ex
t 
pr
oc
es
si
ng
 v
s.
 s
pe
ec
h 
pr
oc
es
si
ng
Figure 1: An example of a dotplot for the string ?text
processing vs. speech processing? plotted against itself.
The box calls out the repeated substring: ?processing.?
3.1 Acoustic Dotplots
When applied to text, the dotplot construct is re-
markably simple: given character strings s1 and s2,
the dotplot is a Boolean similarity matrix K(s1, s2)
defined as
Kij(s1, s2) = ?(s1[i], s2[j]).
Substrings common to s1 and s2 manifest them-
selves as diagonal line segments in the visualization
of K. Figure 1 shows an example text dotplot where
both s1 and s2 are taken to be the string ?text pro-
cessing vs. speech processing.? The boxed diago-
nal line segment arises from the repeat of the word
?processing,? while the main diagonal line trivially
arises from self-similarity. Thus, the search for line
segments inK off the main diagonal provides a sim-
ple algorithmic means to identify repeated terms of
possible interest, albeit sometimes partial, in a col-
lection of text documents. The challenge is to gen-
eralize these dotplot techniques for application to
speech, an inherently noisy, real-valued data stream.
The strategy is to replace character strings with
frame-based speech representations of the form
x = x1, x2, . . . xN , where each xi ? Rd is a d-
dimensional vector space representation of the ith
overlapping window of the signal. Given vector time
series x = x1, x2, . . . xN and y = y1, y2, . . . yM for
two spoken documents, the acoustic dotplot is the
real-valuedN?M cosine similarity matrixK(x,y)
defined as
Time (s)
Tim
e (s
)
K(qi,qj)
 
 
1 2 3 4 5 6 7 8
1
2
3
4
5
6
7
8
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Figure 2: An example of an acoustic dotplot for 8 seconds
of speech (posteriorgrams) plotted against itself. The box
calls out a repetition of interest.
Kij(x,y) =
1
2
[
1 +
?xi, yj?
?xi??yj?
]
. (1)
Even though the application to speech is a distinctly
noisier endeavor, sequences of frames repeated be-
tween the two audio clips will still produce approx-
imate diagonal lines in the visualization of the ma-
trix. The search for matched regions thus reduces
to the robust search for diagonal line segments in
K, which can be efficiently performed with standard
image processing techniques.
Included in this procedure is the application of a
diagonal median filter of duration ? seconds. The
choice of ? determines an approximate threshold
on the duration of the matched regions discovered.
Large ? values (? 1 sec) will produce a relatively
sparse list of matches corresponding to long words
or short phrases; smaller ? values (< 0.5 sec)
will admit shorter words and syllables that may be
less informative from a document analysis perspec-
tive. Given the approximate nature of the procedure,
shorter ? values also admit less reliable matches.
3.2 Posteriorgram Representation
The acoustic dotplot technique can operate on any
vector time series representation of the speech sig-
nal, including a standard spectrogram. However, at
the individual frame level, the cosine similarities be-
tween frequency spectra of distinct speakers produc-
ing the same phoneme are not guaranteed to be high.
462
Time (s)
Pho
ne
 
 
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
silaa
aeah
aoaw
axaxr
ayb
chd
dheh
elem
ener
eyf
ghh
ihiy
jhk
lm
nng
owoy
pr
ssh
tth
uhuw
vw
yz
zh
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 3: An example of a posteriorgram.
Thus, to perform term discovery across a multi-
speaker corpus, we require a speaker-independent
representation. Phonetic posteriorgrams are a suit-
able choice, as each frame is represented as the pos-
terior probability distribution over a set of speech
sounds given the speech observed at the particular
point in time, which is largely speaker-independent
by construction. Figure 3 shows an example poste-
riorgram for the utterance ?I had to do that,? com-
puted with a multi-layer perceptron (MLP)-based
English phonetic acoustic model (see Section 5 for
details). Each row of the figure represents the pos-
terior probability of the given phone as a function of
time through the utterance and each column repre-
sents the posterior distribution over the phone set at
that particular point in time.
The construction of speaker independent acous-
tic models typically requires a significant amount of
transcribed speech. Our proposed strategy is to em-
ploy a speaker independent acoustic model trained
in a high resource language or domain to interpret
multi-speaker data in the zero resource target set-
ting.2 Indeed, we do not need to know a language
to detect when a word of sufficient length has been
repeated in it.3 By computing cosine similarities
2A similarly-minded approach was taken in Hazen et al
(2007) and extended in Hazen and Margolis (2008), where the
authors use Hungarian phonetic trigrams features to character-
ize English spoken documents for a topic classification task.
3While in this paper our acoustic model is based on our eval-
uation corpus, this is not a requirement of our approach. Future
work will investigate performance of other acoustic models.
of phonetic posterior distribution vectors (as op-
posed to reducing the speech to a one-best phonetic
token sequence), the phone set used need not be
matched to the target language. With this approach,
a speaker-independent model trained on the phone
set of a reference language may be used to perform
speaker independent term discovery in any other.
In addition to speaker independence, the use of
phonetic posteriorgrams introduces representational
sparsity that permits efficient dotplot computation
and storage. Notice that the posteriorgram dis-
played in Figure 3 consists of mostly near-zero val-
ues. Since cosine similarity (Equation 1) between
two frames can only be high if they have significant
mass on the same phone, most comparisons need not
be made. Instead, we can apply a threshold and store
each posteriorgram as an inverted file, performing
inner product multiplies and adds only when they
contribute. Using a grid of approximately 100 cores,
we were able to perform theO(n2) dotplot computa-
tion and line segment search for 60+ hours of speech
(corresponding to a 500 terapixel dotplot) in approx-
imately 5 hours.
Figure 2 displays the posteriorgram dotplot for
8 seconds of speech against itself (i.e., x = y).
The prominent main diagonal line results from self-
similarity, and thus is ignored in the search. The
boxed diagonal line segment results from two dis-
tinct occurrences of the term one million dollars.
The large black boxes in the image result from
long stretches of silence of filled pauses; fortunately,
these are easily filtered with speech activity detec-
tion or simple measures of posteriorgram stability.
4 Creating Pseudo-Terms
Spoken documents will be represented as bags of
pseudo-terms, where pseudo-terms are computed
from acoustic repetitions described in the previous
section. Let M be a set of matched regions (m),
each consisting of a pair of speech intervals con-
tained in the corpus (m = [t(i)1 , t
(i)
2 ], [t
(j)
1 , t
(j)
2 ] indi-
cates the speech from t(i)1 to t
(i)
2 is an acoustic match
to the speech from t(j)1 to t
(j)
2 ). If a particular term
occurs k times, the setM can include as many as
(k
2
)
distinct elements corresponding to that term, so we
require a procedure to group them into clusters. We
call the resulting clusters pseudo-terms since each
463
cluster is a placeholder for a term (word or phrase)
spoken in the collection. Given the match list M
and the pseudo-term clusters, it is relatively straight-
forward to represent spoken documents as bags of
pseudo-terms.
To perform this pseudo-term clustering we repre-
sented matched regions as vertices in a graph with
edges representing similarities between these re-
gions. We employ a graph-clustering algorithm that
extracts connected components. Let G = (V,E) be
an unweighted, undirected graph with vertex set V
and edge set E. Each vi ? V corresponds to a sin-
gle speech interval ([t(i)1 , t
(i)
2 ]) present in M (each
m ?M has a pair of such intervals, so |V | = 2|M|)
and each eij ? E is an edge between vertex vi and
vj .
The set E consists of two types of edges. The
first represents repeated speech at distinct points
in the corpus as determined by the match list M.
The second represents near-identical intervals in the
same utterance (i.e. the same speech) since a sin-
gle interval can show up in several matches in M
and the algorithm in Section 3 explicitly ignores
self-similarity. Given the intervals [t(i)1 , t
(i)
2 ] and
[t(j)1 , t
(j)
2 ] contained in the same utterance and with
corresponding vertices vi, vj ? V , we introduce
an edge eij if fractional overlap fij exceeds some
threshold ? , where fij = max(0, rij) and
rij =
(t(i)2 ? t
(i)
1 ) + (t
(j)
2 ? t
(j)
1 )
max(t(i)2 , t
(j)
2 )?min(t
(i)
1 , t
(j)
1 )
? 1. (2)
From the graph G, we produce one pseudo-term
for each connected component. More sophisticated
edge weighting schemes would likely provide ben-
efit. In particular, we expect improved clustering
by introducing weights that reflect acoustic sim-
ilarity between match intervals, rather than rely-
ing solely upon the term discovery algorithm to
make a hard decision. Such confidence weights
would allow even shorter pseudo-terms to be con-
sidered (by reducing ?) without greatly increasing
false alarms. With such a shift, more sophisticated
graph-clustering mechanisms would be warranted
(e.g. Clauset et al (2004)). We plan to pursue this
in future work.
Counts Terms
5 keep track of
5 once a month
2 life insurance
2 capital punishment
9 paper; newspaper
3 talking to you
Table 1: Pseudo-terms resulting from a graph clustering
of matched regions (? = 0.75, ? = 0.95). Counts indi-
cate the number of times the times the pseudo-terms ap-
pear across 360 conversation sides in development data.
Table 1 contains several examples of pseudo-
terms and the matched regions included in each
group. The orthographic forms are taken from the
transcripts in the data (see Section 5). Note that for
some pseudo-terms, the words match exactly, while
for others, the phrases are distinct but phonetically
similar. However, even in this case, there is often
substantial overlap in the spoken terms.
5 Data
For our experiments we used the Switchboard
Telephone Speech Corpus (Godfrey et al, 1992).
Switchboard is a collection of roughly 2,400 two-
sided telephone conversations with a single partici-
pant per side. Over 500 participants were randomly
paired and prompted with a topic for discussion.
Each conversation belongs to one of 70 pre-selected
topics with the two sides restricted to separate chan-
nels of the audio.
To develop and evaluate our methods, we cre-
ated three data sets from the Switchboard corpus: a
development data set, a held out tuning data
set and an evaluation data set. The develop-
ment data set was created by selecting the six most
commonly prompted topics (recycling, capital pun-
ishment, drug testing, family finance, job benefits,
car buying) and randomly selecting 60 sides of con-
versations evenly across the topics (total 360 con-
versation sides.) This corresponds to 35.7 hours of
audio. Note that each participant contributed at most
one conversation side per topic, so these 360 conver-
sation sides represent 360 distinct speakers. All al-
gorithm development and experimentation was con-
ducted exclusively on the development data.
For the tuning data set, we selected an additional
60 sides of conversations evenly across the same six
topics used for development, for a total of 360 con-
464
versations and 37.5 hours of audio. This data was
used to validate our experiments on the develop-
ment data by confirming the heuristic used to select
algorithmic parameters, as described below. This
data was not used for algorithm development. The
evaluation data set was created once parameters had
been selected for a final evaluation of our methods.
We selected this data by sampling 100 conversation
sides from the next six most popular conversation
topics (family life, news media, public education,
exercise/fitness, pets, taxes), yielding 600 conversa-
tion sides containing 61.6 hours of audio.
In our experiments below, we varied the match
duration ? between 0.6 s and 1.0 s and the overlap
threshold ? between 0.75 and 1.0. We measured the
resulting effects on the number of unique pseudo-
terms generated by the process. In general, de-
creasing ? results in more matched regions increas-
ing the number of pseudo-terms. Similarly, increas-
ing ? forces fewer regions to be merged, increasing
the total number of pseudo-terms. Table 2 shows
how these parameters change the number of pseudo-
terms (features) per document and the average num-
ber of occurrences of each pseudo-term. The user
could tune these parameters to select pseudo-terms
that were long and occurred in many documents. In
the next sections, we consider how these parameters
effect performance of various learning settings.
To provide the requisite speaker independent
acoustic model, we compute English phone pos-
teriorgrams using the multi-stream multi-layer
perceptron-based architecture of Thomas et al
(2009), trained on 300 hours of conversational tele-
phone speech. While this is admittedly a large
amount of supervision, it is important to emphasize
our zero resource term discovery algorithm does not
rely on the phonetic interpretability of this refer-
ence acoustic model. The only requirement is that
the same target language phoneme spoken by dis-
tinct speakers map to similar posterior distributions
over the reference language phoneme set. Thus,
even though we evaluate the system on matched-
language Switchboard data, it can be just as easily
applied to any target language with no language-
specific knowledge or training resources required.4
4The generalization of the speaker independence of acous-
tic models across languages is not well understood. Indeed, the
performance of our proposed system would depend to some ex-
? ? Features Feat. Frequency Feat./Doc.
0.6 0.75 5,809 2.15 34.7
0.6 0.85 23,267 2.22 143.4
0.6 0.95 117,788 2.38 779.8
0.6 1.0 333,816 2.32 2153.4
0.75 0.75 8,236 2.31 52.8
0.75 0.85 18,593 2.36 121.7
0.75 0.95 48,547 2.36 318.2
0.75 1.0 90,224 2.18 546.9
0.85 0.75 5,645 2.52 39.5
0.85 0.85 8,832 2.44 59.8
0.85 0.95 15,805 2.24 98.3
0.85 1.0 24,480 2.10 142.4
1.0 0.75 1,844 2.39 12.3
1.0 0.85 2,303 2.24 14.4
1.0 0.95 3,239 2.06 18.6
1.0 1.0 4,205 1.93 22.7
Table 2: Statistics on the number of features (pseudo-
terms) generated for different settings of the match dura-
tion ? and the overlap threshold ? .
6 Document Clustering
We begin by considering document clustering, a
popular approach to discovering latent structure in
document collections. Unsupervised clustering al-
gorithms sort examples into groups, where each
group contains documents that are similar. A user
exploring a corpus can look at a few documents in
each cluster to gain an overview of the content dis-
cussed in the corpus. For example, clustering meth-
ods can be used on search results to provide quick
insight into the coverage of the returned documents
(Zeng et al, 2004).
Typically, documents are clustered based on a bag
of words representation. In the case of clustering
conversations in our collection, we would normally
obtain a transcript of the conversation and then ex-
tract a bag of words representation for clustering.
The resulting clusters may represent topics, such as
the six topics used in our switchboard data. Such
groupings, available with no topic labeled training
data, can be a valuable tool for understanding the
contents of a speech data collection. We would like
to know if similar clustering results can be obtained
without the use of a manual or automatic transcript.
In our case, we substitute the pseudo-terms discov-
ered in a conversation for the transcript, representing
tent on the phonetic similarity of the target and reference lan-
guage. Unsupervised learning of speaker independent acoustic
models remains an important area of future research.
465
the document as a bag of pseudo-terms instead of ac-
tual words. Can a clustering algorithm achieve sim-
ilar results along topical groups with our transcript-
free representation as it can with a full transcript?
In our experiments, we use the six topic labels
provided by Switchboard as the clustering labels.
The goal is to cluster the data into six balanced
groups according to these topics. While Switch-
board topics are relatively straightforward to iden-
tify since the conversations were prompted with spe-
cific topics, we believe this task can still demon-
strate the effectiveness of our representation relative
to the baseline methods. After all, topic classifica-
tion without ASR is still a difficult task.
6.1 Evaluation Metrics
There are numerous approaches to evaluating clus-
tering algorithms. We consider several methods: Pu-
rity, Entropy and B-Cubed. For a full treatment of
these metrics, see Amigo? et al (2009).
Purity measures the precision of each cluster, i.e.,
how many examples in each cluster belong to the
same true topic. Purity ranges between zero and one,
with one being optimal. While optimal purity can be
obtained by putting each document in its own clus-
ter, we fix the number of clusters in all experiments
so purity numbers are comparable. The purity of a
cluster is defined as the largest percentage of exam-
ples in a cluster that have the same topic label. Purity
of the entire clustering is the average purity of each
cluster:
purity(C,L) =
1
N
?
ci?C
max
lj?L
|ci ? lj | (3)
where C is the clustering, L is the reference label-
ing, and N are the number of examples. Following
this notation, ci is a specific cluster and lj is a spe-
cific true label.
Entropy measures how the members of a cluster
are distributed amongst the true labels. The global
metric is computed by taking the weighted aver-
age of the entropy of the members of each cluster.
Specifically, entropy(C,L) is given by:
?
?
ci?C
Ni
N
?
lj?L
P (ci, lj) log2 P (ci, lj) (4)
where Ni is the number of instances in cluster i,
P (ci, lj) is the probability of seeing label lj in clus-
ter ci and the other variables are defined as above.
B-Cubed measures clustering effectiveness from
the perspective of a user?s inspecting the clustering
results (Bagga and Baldwin, 1998). B-Cubed preci-
sion can be defined as an algorithm as follows: sup-
pose a user randomly selects a single example. She
then proceeds to inspect every other example that
occurs in the same cluster. How many of these items
will have the same true label as the selected exam-
ple (precision)? B-Cubed recall operates in a sim-
ilar fashion, but it measures what percentage of all
examples that share the same label as the selected
example will appear in the selected cluster. Since B-
Cubed averages its evaluation over each document
and not each cluster, it is less sensitive to small er-
rors in large clusters as opposed to many small errors
in small clusters. We include results for B-Cubed
F1, the harmonic mean of precision and recall.
6.2 Clustering Algorithms
We considered several clustering algorithms: re-
peated bisection, globally optimal repeated bisec-
tion, and agglomerative clustering (see Karypis
(2003) for implementation details). Each bisection
algorithm is run 10 times and the optimal clustering
is selected according to a provided criteria function
(no true labels needed). For each clustering method,
we evaluated several criteria functions. Addition-
ally, we considered different scalings of the feature
values (the number of times the pseudo-terms ap-
pear in each document). We found that scaling each
feature by the inverse document frequency, effec-
tively TFIDF, produced the best results, so we use
that scaling in all of our experiments. We also ex-
plored various similarity metrics and found cosine
similarity to be the most effective.
We used the Cluto clustering library for all clus-
tering experiments (Karypis, 2003). In the following
section, we report results for the optimal clustering
configuration based on experiments on the develop-
ment data.
6.3 Baselines
We compared our pseudo-term feature set perfor-
mance to two baselines: (1) Phone Trigrams and
466
(2) Word Transcripts. The Phone Trigram base-
line is derived automatically using an approach sim-
ilar to Hazen et al (2007). This baseline is based
on a vanilla phone recognizer on top of the same
MLP-based acoustic model (see Section 5 and the
references therein for details) used to discover the
pseudo-terms. In particular, the phone posterior-
grams were transformed to frame-level monophone
state likelihoods (through division by the frame-
level priors). These state likelihoods were then used
along with frame-level phone transition probabilities
to Viterbi decode each conversation side. It is impor-
tant to emphasize that the reliability of phone recog-
nizers depends on the phone set matching the appli-
cation language. Using the English acoustic model
in this manner on another language will significantly
degrade the performance numbers reported below.
The Word Transcript baseline starts with Switch-
board transcripts. This baseline serves as an upper
bound of what large vocabulary recognition can pro-
vide for this task. n-gram features are computed
from the transcript. Performance is reported sepa-
rately for unigrams, bigrams and trigrams.
6.4 Results
To optimize parameter settings, match duration (?)
and overlap threshold (? ) were swept over a wide
range (0.6 < ? < 1.0 and 0.75 < ? < 1.0) using a
variety of clustering algorithms and training criteria.
Initial results on development data showed promis-
ing performance for the default I2 criteria in Cluto
(repeated bisection set to maximize the square root
of within cluster similarity). Representative results
on development data with various parameter settings
for this clustering configuration appear in Table 3.
A few observations about results on development
data. First, the three evaluation metrics are strongly
correlated. Second, for each ? the same narrow
range of ? values achieve good results. In general,
settings of ? > 0.9 were all comparable. Essen-
tially, setting a high threshold for merging matched
regions was sufficient without further tuning. Third,
we observed that decreasing ? meant more features,
but that these additional features did not necessarily
lead to more useful features for clustering. For ex-
ample, ? = 0.70 gave a small number of reasonably
good features, while ? = 0.60 can give an order of
magnitude more features without much of a change
Pseudo-term Results
? ? Features Purity Entropy B3 F1
0.60 0.95 117,788 0.9639 0.2348 0.9306
0.60 0.96 143,299 0.9750 0.1664 0.9518
0.60 0.97 178,559 0.9667 0.2116 0.9366
0.60 0.98 223,511 0.9528 0.2717 0.9133
0.60 0.99 333,630 0.9583 0.2641 0.9210
0.60 1.0 333,816 0.9583 0.2641 0.9210
0.70 0.93 58,303 0.9528 0.3114 0.9105
0.70 0.94 66,054 0.9667 0.2255 0.9358
0.70 0.95 74,863 0.9583 0.2669 0.9210
0.70 0.96 86,070 0.9611 0.2529 0.9260
0.70 0.97 100,623 0.9639 0.2326 0.9312
0.70 0.98 117,535 0.9556 0.2821 0.9158
0.70 0.99 161,219 0.9056 0.4628 0.8372
0.70 1.0 161,412 0.9333 0.4011 0.8760
Phone Recognizer Baseline
Type Features Purity Entropy B3 F1
Phone Trigram 28,110 0.6194 1.3657 0.5256
Manual Word Transcript Baselines
Type Features Purity Entropy B3 F1
Word Unigram 7,330 0.9917 0.0559 0.9839
Word Bigram 74,216 0.9833 0.1111 0.9678
Word Trigram 224,934 0.9889 0.0708 0.9787
Table 3: Clustering results on development data using
globally optimal repeated bisection and I2 criteria. The
best results over the manual word transcript baselines
and for each match duration (?) are highlighted in bold.
Pseudo-term results are better than the phonetic baseline
and almost as good as the transcript baseline.
in clustering performance. Finally, while pseudo-
term results are not as good as with the manual
transcripts (unigrams), they achieve similar results.
Compared with the phone trigram features deter-
mined by the phone recognizer output, the pseudo-
terms perform significantly better. Note that these
two automatic approaches were built using the iden-
tical MLP-based phonetic acoustic model.
We sought to select the optimal parameter settings
for running on the evaluation data using the devel-
opment data and the held out tuning data. We de-
fined the following heuristic to select the optimal pa-
rameters. We choose settings for ?, ? and the clus-
tering parameters that independently maximize the
performance averaged over all runs on development
data. We then selected the single run correspond-
ing to these parameter settings and checked the re-
sult on the held out tuning data. This setting was
also the best performer on the held out set, so we
used these parameters for evaluation. The best per-
forming parameters were globally optimal repeated
467
? ? Features Purity Entropy B3 F1
0.70 0.98 123,901 0.9778 0.1574 0.9568
Phone Trigram 28,374 0.6389 1.2345 0.5513
Word Unigram 7,640 0.9972 0.0204 0.9945
Word Bigram 77,201 0.9972 0.0204 0.9945
Word Trigram 233,744 0.9972 0.0204 0.9945
Table 4: Results on held out tuning data. The parameters
(globally optimal repeated bisection clustering with I2
criteria, ? = 0.70 seconds and ? = 0.98) were selected
using the development data and validated on tuning data.
Note that the clusters produced by each manual transcript
test were identical in this case.
? ? Features Purity Entropy B3 F1
0.70 0.98 279,239 0.9517 0.3366 0.9073
Phone Trigram 31,502 0.7000 1.0496 0.6355
Word Unigram 9939 0.9883 0.0831 0.9772
Word Bigram 110,859 0.9883 0.0910 0.9771
Word Trigram 357,440 0.9900 0.0775 0.9803
Table 5: Results on evaluation data. The parameters
(globally optimal repeated bisection clustering with I2
criteria, ? = 0.7 seconds and ? = 0.98) were selected
using the development data and validated on tuning data.
bisection clustering with I2 criteria, ? = 0.7 s and
? = 0.98. Note that examining Table 3 alone may
suggest other parameters, but we found our selection
method to yield optimal results on the tuning data.
Results on held out tuning and evaluation data for
this setting compared to the manual word transcripts
and phone recognizer output are shown in Tables
4 and 5. On both the tuning data and evaluation
data, we obtain similar results as on the development
data. While the manual transcript baseline is bet-
ter than our pseudo-term representations, the results
are quite competitive. This demonstrates that use-
ful clustering results can be obtained without a full-
blown word recognizer. Notice also that the pseudo-
term performance remains significantly higher than
the phone recognizer baseline on both sets.
7 Supervised Document Classification
Unsupervised clustering methods are attractive since
they require no human annotations. However, ob-
taining a few labeled examples for a simple label-
ing task can be done quickly, especially with crowd
sourcing systems such as CrowdFlower and Ama-
zon?s Mechanical Turk (Snow et al, 2008; Callison-
Burch and Dredze, 2010). In this setting, a user
may listen to a few conversations and label them by
topic. A supervised classification algorithm can then
be trained on these labeled examples and used to au-
tomatically categorize the rest of the data. In this
section, we evaluate if supervised algorithms can be
trained using the pseudo-term representation of the
speech.
We set up a multi-class supervised classification
task, where each document is labeled using one of
the six Switchboard topics. A supervised learning
algorithm is trained on a sample of labeled docu-
ments and is then asked to label some test data. Re-
sults are measured in terms of accuracy. Since the
documents are a balanced sample of the six topics,
random guessing would yield an accuracy of 0.1667.
We proceed as with the clustering experiments.
We evaluate different representations for various set-
tings of ? and ? and different classifier parameters
on the development data. We then select the opti-
mal parameter settings and validate this selection on
the held out tuning data, before generating the final
representations for the evaluation once the optimal
parameters have been selected.
For learning we require a multi-class classifier
training algorithm. We evaluated four popular
learning algorithms: a) MIRA?a large margin on-
line learning algorithm (Crammer et al, 2006); b)
Confidence Weighted (CW) learning?a probabilis-
tic large margin online learning algorithm (Dredze
et al, 2008; Crammer et al, 2009); c) Maxi-
mum Entropy?a log-linear discriminative classi-
fier (Berger et al, 1996); and d) Support Vec-
tor Machines (SVM)?a large margin discriminator
(Joachims, 1998).5 For each experiment, we used
default settings of the parameters (tuning did not sig-
nificantly change the results) and 10 online iterations
for the online methods (MIRA, CW). Each reported
result is based on 10-fold cross validation.
Table 6 shows results for various parameter set-
tings and the four learning algorithms on develop-
ment data. As before, we observe that values for
? > 0.9 tend to do well. The CW learning algo-
rithm performs the best on this data, followed by
Maximum Entropy, MIRA and SVM. The optimal
? for classification is 0.75, close to the 0.7 value
selected in clustering. As before, pseudo-terms do
5We used the ?variance? formulation with k = 1 for CW
learning, Gaussian regularization for the Maximum Entropy
classifier, and a linear kernel for the SVM.
468
? ? MaxEnt SVM CW MIRA
0.60 0.99 0.8972 0.6944 0.8667 0.8972
0.60 1.0 0.8972 0.6944 0.8639 0.8944
0.70 0.97 0.9000 0.7722 0.8500 0.8056
0.70 0.98 0.8806 0.7417 0.8917 0.8167
0.70 0.99 0.9000 0.6556 0.9194 0.9056
0.70 1.0 0.8917 0.6556 0.9194 0.9083
0.75 0.94 0.8778 0.7806 0.8639 0.8056
0.75 0.95 0.8778 0.7694 0.8889 0.8111
0.75 0.96 0.9028 0.7778 0.9000 0.8778
0.75 0.97 0.9111 0.7722 0.9250 0.9278
0.75 0.98 0.9056 0.7417 0.9194 0.9167
0.85 0.85 0.8639 0.7833 0.8500 0.8167
0.85 0.90 0.8611 0.7528 0.8611 0.8583
0.85 0.91 0.8389 0.7500 0.8722 0.8556
0.85 0.92 0.8528 0.7222 0.8944 0.8556
Phone Trigram 0.6111 0.7139 0.9138 0.5000
Word Unigram 0.9472 0.8861 0.9861 0.9306
Word Bigram 0.9250 0.8833 0.9917 0.9278
Word Trigram 0.9278 0.8611 0.9889 0.9222
Table 6: The top 15 results (measured as average accu-
racy across the 4 algorithms) for pseudo-terms on de-
velopment data. The best pseudo-term and manual tran-
script results for each algorithm are bolded. All results
are based on 10-fold cross validation. Pseudo-term re-
sults are better than the phonetic baseline and almost as
good as the transcript baseline.
well, though not as well as the upper bound based
on manual transcripts. The performance for pseudo-
terms and phone trigrams are roughly comparable,
though we expect pseudo-terms to be more robust
across languages.
Using the same selection heuristic as in cluster-
ing, we select the optimal parameter settings, vali-
date them on the held out tuning data, and compute
results on evaluation data. The best performing con-
figuration was for ? = 0.75 seconds and ? = 0.97.
Notice these parameters are very similar to the best
parameters selected for clustering. Results on held
out tuning and evaluation data for this setting com-
pared to the manual transcripts are shown in Tables
7 and 8. As with clustering, we see good overall
performance as compared with manual transcripts.
While the performance drops, results suggest that
useful output can be obtained without a transcript.
8 Conclusions
We have presented a new strategy for applying stan-
dard NLP tools to speech corpora without the aid
of a large vocabulary word recognizer. Built in-
stead on top of the unsupervised discovery of term-
? ? MaxEnt SVM CW MIRA
0.75 0.97 0.8722 0.7389 0.8972 0.8750
Phone Trigram 0.7167 0.6972 0.9056 0.5083
Word Unigram 0.9500 0.9056 0.9806 0.9250
Word Bigram 0.9444 0.9111 0.9833 0.9250
Word Trigram 0.9417 0.8972 0.9778 0.9250
Table 7: Results on held out tuning data. The parameters
(? = 0.75 seconds and ? = 0.97) were selected using the
development data and validated on tuning data. All re-
sults are based on 10-fold cross validation. Pseudo-term
results are very close to the transcript baseline and often
better than the phonetic baseline.
? ? MaxEnt SVM CW MIRA
0.75 0.97 0.8683 0.7167 0.7850 0.7150
Phone Trigram 0.8600 0.7750 0.9183 0.6233
Word Unigram 0.9533 0.9317 0.9850 0.9267
Word Bigram 0.9467 0.9200 0.9900 0.9367
Word Trigram 0.9383 0.9233 0.9817 0.9367
Table 8: Results on evaluation data. The parameters
(? = 0.75 seconds and ? = 0.97) were selected using the
development data and validated on tuning data. All re-
sults are based on 10-fold cross validation. Pseudo-term
results are very close to the transcript baseline and often
better than the phonetic baseline.
like units in the speech, we perform unsupervised
topic clustering as well as supervised classification
of spoken documents with performance approaching
that achieved with the manual word transcripts, and
generally matching or exceeding that achieved with
a phonetic recognizer. Our study identified several
opportunities and challenges in the development of
NLP tools for spoken documents that rely on little
or no linguistic resources such as dictionaries and
training corpora.
References
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4).
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of the 17th international conference on
Computational linguistics-Volume 1, pages 79?85. As-
sociation for Computational Linguistics.
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
469
Turk. In Workshop on Creating Speech and Language
Data With Mechanical Turk at NAACL-HLT.
K. W. Church and J. I. Helfman. 1993. Dotplot: A
program for exploring self-similarity in millions of
lines of text and code. Journal of Computational and
Graphical Statistics.
Aaron Clauset, Mark E J Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Physical Review E, 70.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
Koby Crammer, Mark Dredze, and Alex Kulesza. 2009.
Multi-class confidence weighted algorithms. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
ICASSP.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In ICASSP.
Timothy J. Hazen and Anna Margolis. 2008. Discrimi-
native feature weighting using MCE training for topic
identification of spoken audio recordings. In ICASSP.
Timothy J. Hazen, Fred Richardson, and Anna Margo-
lis. 2007. Topic identification from audio recordings
using word and phone recognition lattices. In IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding.
Aren Jansen, Kenneth Church, and Hynek Hermansky.
2010. Towards spoken term discovery at scale with
zero resources. In Interspeech.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learning
(ECML).
George Karypis. 2003. CLUTO: A software package for
clustering high-dimensional data sets. Technical Re-
port 02-017, University of Minnesota, Dept. of Com-
puter Science.
Igor Malioutov, Alex Park, Regina Barzilay, and James
Glass. 2007. Making Sense of Sound: Unsupervised
Topic Segmentation Over Acoustic Input. In ACL.
Scott Novotney and Richard Schwartz. 2009. Analysis
of low-resource acoustic model self-training. In Inter-
speech.
Alex Park and James R. Glass. 2008. Unsupervised pat-
tern discovery in speech. IEEE Transactions of Audio,
Speech, and Language Processing.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: Evaluating non-
expert annotations for natural language tasks. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 254?263. Asso-
ciation for Computational Linguistics.
S. Thomas, S. Ganapathy, and H. Hermansky. 2009.
Phoneme recognition using spectral envelope and
modulation frequency features. In Proc. of ICASSP.
H.J. Zeng, Q.C. He, Z. Chen, W.Y. Ma, and J. Ma. 2004.
Learning to cluster web search results. In Conference
on Research and development in information retrieval
(SIGIR).
470
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1116?1127,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Fast Re-scoring Strategy to Capture Long-Distance Dependencies
Anoop Deoras
HLT-COE and CLSP
Johns Hopkins University
Baltimore MD 21218, USA
adeoras@jhu.edu,
Toma?s? Mikolov
Brno University of Technology
Speech@FIT
Czech Republic
imikolov@fit.vutbr.cz,
Kenneth Church
HLT-COE and CLSP
Johns Hopkins University
Baltimore MD 21218, USA
kenneth.church@jhu.edu
Abstract
A re-scoring strategy is proposed that makes
it feasible to capture more long-distance de-
pendencies in the natural language. Two pass
strategies have become popular in a num-
ber of recognition tasks such as ASR (au-
tomatic speech recognition), MT (machine
translation) and OCR (optical character recog-
nition). The first pass typically applies a
weak language model (n-grams) to a lattice
and the second pass applies a stronger lan-
guage model to N best lists. The stronger lan-
guage model is intended to capture more long-
distance dependencies. The proposed method
uses RNN-LM (recurrent neural network lan-
guage model), which is a long span LM, to re-
score word lattices in the second pass. A hill
climbing method (iterative decoding) is pro-
posed to search over islands of confusability
in the word lattice. An evaluation based on
Broadcast News shows speedups of 20 over
basic N best re-scoring, and word error rate
reduction of 8% (relative) on a highly compet-
itive setup.
1 Introduction
Statistical Language Models (LMs) have received
considerable attention in the past few decades. They
have proved to be an essential component in many
statistical recognition systems such as ASR (au-
tomatic speech recognition), MT (machine trans-
lation) and OCR (optical character recognition).
The task of a language model is to assign prob-
ability to any word sequence possible in the lan-
guage. The probability of the word sequence W ?
w1, . . . , wm ? wm1 is typically factored using the
chain rule:
P (wm1 ) =
m?
i=1
P (wi|wi?11 ) (1)
In modern statistical recognition systems, an LM
tends to be restricted to simple n-gram models,
where the distribution of the predicted word depends
on the previous (n ? 1) words i.e. P (wi|wi?11 ) ?
P (wi|wi?1i?n+1).
Noam Chomsky argued that n-grams cannot learn
long-distance dependencies that span over more than
n words (Chomsky, 1957, pp.13). While that might
seem obvious in retrospect, there was a lot of ex-
citement at the time over the Shannon-McMillan-
Breiman Theorem (Shannon, 1948) which was inter-
preted to say that, in the limit, under just a couple of
minor caveats and a little bit of not-very-important
fine print, n-gram statistics are sufficient to capture
all the information in a string (such as an English
sentence). Chomsky realized that while that may be
true in the limit, n-grams are far from the most parsi-
monious representation of many linguistic facts. In
a practical system, we will have to truncate n-grams
at some (small) fixed n (such as trigrams or perhaps
5-grams). Truncated n-gram systems can capture
many agreement facts, but not all.1
By long-distance dependencies, we mean facts
like agreement and collocations that can span over
many words. With increasing order of n-gram mod-
els we can, in theory, capture more regularities in the
1The discussion in this paragraph is taken as-is from an arti-
cle (to appear) by Church (2012).
1116
language. In addition, if we can move to more gen-
eral models then we could hope to capture more, as
well. However, due to data sparsity, it is hard to es-
timate a robust n-gram distribution for large values
of n ( say, n > 10) using the conventional Max-
imum Likelihood techniques, unless a more robust
technique is employed for modeling which gener-
alizes well on unseen events. Some of these well
known long span / complex language models which
have shown to perform very well on many speech
tasks include: structured language model (Chelba
and Jelinek, 2000; Roark, 2001; Wang and Harper,
2002; Filimonov and Harper, 2009), latent seman-
tic analysis language model (Bellegarda, 2000),
topic mixture language models (Iyer and Ostendorf,
1999), whole sentence exponential language mod-
els (Rosenfeld, 1997; Rosenfeld et al, 2001), feed-
forward neural networks (Bengio et al, 2001), re-
current neural network language models (Mikolov
et al, 2010), among many others.
Although better modeling techniques can now
capture longer dependencies in a language, their
incorporation in decoders of speech recognition or
machine translation systems becomes computation-
ally challenging. Due to the prohibitive increase in
the search space of sentence hypotheses (or longer
length word sub sequences), it becomes challenging
to use a long span language model in the first pass
decoding. A word graph (word lattices for speech
recognition systems and hypergraphs for machine
translation systems), encoding exponential number
of hypotheses is hence outputted at the first pass out-
put on which a sophisticated and complex language
model is deployed for re-scoring. However, some-
times even re-scoring of this refined search space
can be computationally expensive due to explosion
of state space.
Previously, we showed in (Deoras et al, 2011)
how to tackle the problem of incorporating long span
information during decoding in speech recogni-
tion systems by variationaly approximating (Bishop,
2006, pp. 462) the long span language model by a
tractable substitute such that this substitute model
comes closest to the long span model (closest in
terms of Kullback Leibler Divergence (Cover and
J.A.Thomas, 1991, pp. 20)). The tractable substi-
tute was then used directly in the first pass speech
recognition systems. In this paper we propose an
approach that keeps the model intact but approxi-
mates the search space instead (which can become
intractable to handle especially under a long span
model), thus enabling the use of full blown model
for re-scoring.With this approach, we can achieve
full lattice re-scoring with a complex model, at a
cost more than 20 times less than of a naive brute
force approach that is commonly used today.
The rest of the paper is organized as follows:
We discuss a particular form of long span language
model in Sec. 2. In Sec. 3 we discuss two standard
re-scoring techniques and then describe and demon-
strate our proposed technique in Sec. 4. We present
experimental results in Sec. 5 followed by conclu-
sions and some remarks in Sec. 6.
2 Recurrent Neural Networks (RNN)
There is a long history of using neural networks to
model sequences. Elman (1990) used recurrent neu-
ral network for modeling sentences of words gen-
erated by an artificial grammar. Work on statistical
language modeling of real natural language data, to-
gether with an empirical comparison of performance
to standard techniques was done by Bengio et al
(2001). His work has been followed by Schwenk
(2007), who has shown that neural network language
models actually work very well in the state-of-the-
art speech recognition systems. Recurrent Neu-
ral Network based Language Models (RNN-LMs)
(Mikolov et al, 2010) improved the ability of the
original model to capture patterns in the language
without using any additional features (such as part
of speech, morphology etc) i.e. other than lexical
ones. The RNN-LM was shown to have superior
performance than the original feedforward neural
network (Mikolov et al, 2011b). Recently, we also
showed that this model outperforms many other ad-
vanced language modeling techniques (Mikolov et
al., 2011a). We hence decided to work with this
model. This model uses whole history to make pre-
dictions, thus it lies outside the family of n-gram
models. Power of the model comes at a considerable
computational cost. Due to the requirement of un-
limited history, many optimization tricks for rescor-
ing with feedforward-based NNLMs as presented by
Schwenk (2007) cannot be applied during rescoring
with RNN LM. Thus, this model is a good candidate
1117
w( t )
s( t )
y( t )
(delayed)
U V
Figure 1: Schematic Representation of Recurrent Neu-
ral Network Language Model. The network has an input
layer w, a hidden layer s and an output layer y. Matrices
U and V represent synapses.
to show effectiveness and importance of our work.
The basic RNNLM is shown in Fig. 1. The model
has an input layer w(t) that encodes previous word
using 1 of N coding (thus, the size of the input layer
is equal to the size of the vocabulary, and only the
neuron that corresponds to the previous word in a
sequence is set to 1). The hidden layer s(t) has addi-
tional recurrent connections that are delayed by one
time step. After the network is trained, the output
layer y(t) represents probability distribution for the
current word, given the previous word and the state
of the hidden layer from the previous time step.
The training is performed by ?backpropagation-
through-time? algorithm that is commonly used for
training recurrent neural networks (Rumelhart et al,
1986). More details about training, setting initial pa-
rameters, choosing size of the hidden layer etc. are
presented in (Mikolov et al, 2010). Additional ex-
tensions that allow this model to be trained on large
corpora are presented in (Mikolov et al, 2011b).
3 Standard Approaches for Rescoring
3.1 Word Lattice Rescoring
A word lattice, L, obtained at the output of the first
pass decoding, encodes exponential number (expo-
nential in the number of states (nodes) present in
the lattice) of hypotheses in a very compact data
structure. It is a directed acyclic graph G =
(V, E , ns, Ne), where V and E denote set of vertices
(nodes / states) and edges (arcs / links), respectively.
ns and Ne denote the unique start state and set of
end states.
A path, pi, in a lattice is an element of E? with
consecutive transitions. We will denote the origin /
previous state of this path by p[pi] and destination /
next state of this path by n[pi]. A path, pi is called
a complete path if p[pi] = ns and n[pi] ? Ne. A
path, pi, is called a partial path if p[pi] = ns but n[pi]
may or may not belong to Ne. A path, pi, is called
a trailing path if p[pi] may or may not be equal to
ns and n[pi] ? Ne. We will also denote the time
stamp at the start of the path by Ts[pi] and the time
stamp at the end of the path by Te[pi]. Since there
are nodes attached to the start and end of any path,
we will denote the time stamp at any node u ? V by
T [u]. Associated with every path, pi, is also a word
sequence W [pi] ? W?, where W is the vocabulary
used during speech recognition. For the sake of sim-
plicity, we will distinguish word sequence of length
1 from the word sequences of length greater than 1
by using lower and upper casing i.e. w[?] and W [?]
respectively.
The acoustic likelihood of the path pi ? E? is then
given as:
A[pi] =
|pi|?
j=1
P (aj |w[pij ])
where ?j ? {1, 2, . . . , |pi|} pij ? E , pi = |pi|j=1pij
and P (aj |w[pij ]) is the acoustic likelihood of the
acoustic substring aj , spanning between Ts[pij ] and
Te[pij ], conditioned on the word w[pij ] associated
with the edge pij .2 Similarly, the language model
score of the path pi is given as:
L[pi] =
|pi|?
j=1
P (w[pij ]|w[pij?1], . . . , w[pij?m+1])
where P (w[pij ]|w[pij?1], . . . , w[pij?m+1]) is the
m-th order Markov approximation for estimating the
probability of a word given the context upto that
point. The speech recognizer, which uses m-th or-
der Markov LM for first pass recognition, imposes a
constraint on the word lattice such that at each state
there exists an unambiguous context of consecutive
m? 1 words.
A first pass output is then a path pi? having Max-
imum a Posterior (MAP) probability.3 Thus pi? is
2We will use  symbol to denote concatenation of paths or
word strings.
3Note that asterisk symbol here connotes that the path is op-
1118
obtained as:
pi? = argmax
pi:p[pi]=ns
n[pi]?Ne
A[pi]?L[pi],
where ? is the scaling parameter needed to balance
the dynamic variability between the distributions of
acoustic and language model (Ogawa et al, 1998).
Efficient algorithms such as single source shortest
path (Mohri et al, 2000) can be used for finding out
the MAP path.
Under a new n-gram Language Model, rescor-
ing involves replacing the existing language model
scores of all paths pi. If we denote the new language
model by Lnew and correspondingly the score of the
path pi by Lnew[pi], then it is simply obtained as:
Lnew[pi] =
|pi|?
j=1
P (w[pij ]|w[pij?1], . . . , w[pij?n+1])
where P (w[pij ]|w[pij?1], . . . , w[pij?n+1]) is the n-
th order Markov approximation for estimating the
probability of a word given the unambiguous con-
text of n ? 1 words under the new rescoring LM.
If the Markov rescoring n-gram LM needs a bigger
context for the task of prediction (i.e. n > m, where
m? 1 is the size of the unambiguous context main-
tained at every state of the word lattice), then each
state of the lattice has to be split until an unambigu-
ous context of length as large as that required by the
new re-scoring language model is not maintained.
The best path, pi? is then obtained as:
pi? = argmax
pi:p[pi]=ns
n[pi]?Ne
A[pi]?Lnew[pi],
where ? acts as the new scaling parameter which
may or may not be equal to the old scaling parameter
?.
It should be noted that if the rescoring LM needs a
context of the entire past in order to predict the next
word, then the lattice has to be expanded by splitting
the states many more times. This usually blows up
the search space even for a reasonably small number
timal under some model. This should not be confused with the
Kleene stars appearing as superscripts for E andW , which serve
the purpose of regular expressions implying 0 or many occu-
rances of the element of E and V respectively.
of state splitting iterations. When the task is to do
rescoring under a long span LM, such as RNN-LM,
then exact lattice re-scoring option is not feasible. In
order to tackle this problem, a suboptimal approach
via N best list rescoring is utilized. The details of
this method are presented next.
3.2 N best List Rescoring
N best list re-scoring is a popular way to cap-
ture some long-distance dependencies, though the
method can be slow and it can be biased toward the
weaker language model that was used in the first
pass.
Given a word lattice, L, top N paths
{pi1, . . . , piN} are extracted such that their joint
likelihood under the baseline acoustic and language
models are in descending order i.e. that:
A[pi1]?L[pi1] ? A[pi2]?L[pi2] ? . . . ? A[piN ]?L[piN ]
Efficient algorithms exist for extractingN best paths
from word lattices (Chow and Schwartz, 1989;
Mohri and Riley, 2002). If a new language model,
Lnew, is provided, which now need not be restricted
to finite state machine family, then that can be de-
ployed to get the score of the entire path pi. If we
denote the new LM scores by Lnew[?], then under N
best list paradigm, optimal path p?i is found out such
that:
p?i = argmax
pi?{pi1,...,piN}
A[pi]?Lnew[pi], (2)
where ? acts as the new scaling parameter which
may or may not be equal to ?. If N  |L| (where
|L| is the total number of complete paths in word lat-
tice, which are exponentially many), then the path
obtained using (2) is not guaranteed to be optimal
(under the rescoring model). The short list of hy-
potheses so used for re-scoring would yield subop-
timal output if the best path pi? (according to the
new model) is not present among the top N candi-
dates extracted from the lattice. This search space
is thus said to be biased towards a weaker model
mainly because the N best lists are representative of
the model generating them. To illustrate the idea,
we demonstrate below a simple analysis on a rel-
atively easy task of speech transcription on WSJ
data.4 In this setup, the recognizer made use of a bi-
4Full details about the setup can be found in (Deoras et al,
2010)
1119
gram LM to produce lattices and hence N best lists.
Each hypothesis in this set got a rank with the top
most and highest scoring hypothesis getting a rank
of 1, while the bottom most hypothesis getting a
rank of N . We then re-scored these hypotheses with
a better language model (either with a higher order
Markov LM i.e. a trigram LM (tg) or the log linear
combination of n-gram models and syntactic mod-
els (n-gram+syntactic) and re-ranked the hypothe-
ses to obtain their new ranks. We then used Spear-
man?s rank correlation factor, ?, which takes values
in [?1,+1], with ?1 meaning that the two ranked
lists are negatively correlated (one list is in a reverse
order with respect to the other list) and +1 mean-
ing that the two ranked lists are positively correlated
(the two lists are exactly the same). Spearman?s rank
correlation factor is given as:
? = 1? 6
?N
n=1 d2n
N(N2 ? 1) , (3)
where dn is the difference between the old and new
rank of the nth entry (in our case, difference between
n(? {1, 2, . . . , N}) and the new rank which the nth
hypothesis got under the rescoring model).
Table 1 shows how the correlation factor drops
dramatically when a better and a complementary
LM is used for re-scoring, suggesting that theN best
lists are heavily biased towards the starting models.
Huge re-rankings suggests there is an opportunity to
improve and also a need to explore more hypotheses,
i.e. beyond N best lists.
Model (?) WER (%)
bg 1.00 18.2%
tg 0.41 17.4%
n-gram+syntactic 0.33 15.8%
Table 1: Spearman Rank Correlation on the N best list
extracted from a bi-gram language model (bg) and re-
scored with relatively better language models including,
trigram LM (tg), and the log linear combination of n-
gram models, and syntactic models (n-gram+syntactic).
With a bigger and a better LM, the WER decreases at
the expense of huge re-rankings of N best lists, only
suggesting the fact that N best lists generated under a
weaker model, are not reflective enough of a relatively
better model.
In the next section, we propose an algorithm
which keeps the representation of search space as
simple as that of N best list, but does not restrict it-
self to topN best paths alone and hence does not get
biased towards the starting weaker model.
4 Proposed Approach for Rescoring
A high level idea of our proposed approach is to
identify islands of confusability in the word lattice
and replace the problem of global search over word
lattice by series of local search problems over these
islands in an iterative manner. The motivation be-
hind this strategy is the observation that the recog-
nizer produces bursts of errors such that they have
a temporal scope. The recognizer output (sentence
hypotheses) when aligned together typically shows
a pattern of confusions both at the word level and
at the phrase level. Regions where there are sin-
gleton words competing with one another (reminis-
cent of a confusion bin of a Confusion Network
(CN) (Mangu, 2000)), choice of 1 word edit dis-
tance works well for the formation of local neigh-
borhood. Regions where there are phrases com-
peting with other phrases, choice of variable length
neighborhood works well. Previously, Richardson
et al (1995) demonstrated a hill climbing frame-
work by exploring 1 word edit distance neighbor-
hood, while in our own previous work (Deoras and
Jelinek, 2009), we demonstrated working of iterative
decoding algorithm, a hill climbing framework, for
CNs, in which the neighborhood was formed by all
words competing with each other in any given time
slot, as defined by a confusion bin.
In this work, we propose a technique which gen-
eralizes very well on word lattices and overcomes
the limitations posed by a CN or by the limited na-
ture of local neighborhood. The size of the neigh-
borhood in our approach is a variable factor which
depends upon the confusability in any particular re-
gion of the word lattice. Thus the local neighbor-
hood are in some sense a function of the confusabil-
ity present in the lattice rather than some predeter-
mined factor. Below we describe the process, virtue
of which, we can cut the lattice to form many self
contained smaller sized sub lattices. Once these sub
lattices are formed, we follow a similar hill climbing
procedure as proposed in our previous work (Deoras
and Jelinek, 2009).
1120
4.1 Islands of Confusability
We will continue to follow the notation introduced
in section 3.1. Before we define the procedure for
cutting the lattice into many small self contained
lattices, we will define some more terms necessary
for the ease of understandability of the algorithm.5
For any node v ? V , we define forward probability,
?(v), as the probability of any partial path pi ? E?,
s.t. p[pi] = ns, n[pi] = v and it is given as:
?(v) =
?
pi?E?
s.t.p[pi]=ns,n[pi]=v
A[pi]?L[pi] (4)
Similarly, for any node v ? V , we define the
backward probability, ?(v), as the probability of any
trailing path pi ? E?, s.t. p[pi] = v, n[pi] ? Ne and it
is given as:
?(v) =
?
pi?E?
s.t.p[pi]=v,n[pi]?Ne
A[pi]?L[pi] (5)
If we define the sum of joint likelihood under the
baseline acoustic and language models of all paths
in the lattice by Z, then it can simply be obtained as:
Z =
?
u?Ne ?(u) = ?(ns)In order to cut the lattice, we want to identify sets
of nodes, S1, S2, . . . , S|S| such that for any set Si ?
S following conditions are satisfied:
1. For any two nodes u, v ? Si we have that:
T [u] = T [v]. We will define this common time
stamp of the nodes in the set by T [Si].
2. 6 ? pi ? E such that Ts[pi] < T [Si] < Te[pi].
The first property can be easily checked by first
pushing states into a linked list associated with each
time marker (this can be done by iterating over all
the states of the graph) then iterating over the unique
time markers and retrieving back the nodes asso-
ciated with it.The second property can be checked
by first iterating over the unique time markers and
for each of the marker, iterating over the arcs and
terminating the loop as soon as some arc is found
5Goel and Byrne (2000) previously demonstrated the lat-
tice segmentation procedure to solve the intractable problem of
MBR decoding. The cutting procedure in our work is different
from theirs in the sense that we rely on time information for
collating competing phrases, while they do not.
out violating property 2 for the specific time marker.
Thus the time complexity for checking property 1 is
O(|V|) and that for property 2 isO(|T |?|E|), where
|T | is the total number of unique time markers. Usu-
ally |T |  |E| and hence the time complexity for
checking property 2 is almost linear in the number
of edges. Thus effectively, the time complexity for
cutting the lattice is O(|V|+ |E|).
Having formed such sets, we can now cut the
lattice at time stamps associated with these sets
i.e. that: T [S1], . . . , T [S|S|]. It can be easily seen
that the number of sub lattices, C, will be equal
to |S| ? 1.We will identify these sub lattices as
L1,L1, . . . ,LC . At this point, we have not formed
self contained lattices yet by simply cutting the par-
ent lattice at the cut points.
Once we cut the lattice at these cut points, we im-
plicitly introduce many new starting nodes and end-
ing nodes for any sub lattice. We will refer to these
nodes as exposed starting nodes and exposed end-
ing nodes. Thus for some jth sub lattice, Lj , there
will be as many new exposed starting nodes as there
are nodes in the set Sj and as many exposed ending
nodes as there are nodes in the set Sj+1. In order
to make these sub lattices consistent with the defini-
tion of a word lattice (see Sec. 3.1), we unify all the
exposed starting nodes and exposed ending nodes.
To unify the exposed starting nodes, we introduce
as many new edges as there are nodes in the set Sj
such that they have a common starting node, ns[Lj ],
(newly created) and distinct ending nodes present
in Sj . To unify the exposed ending nodes of Lj ,
we introduce as many new edges as there are nodes
in the set Sj+1 such that they have distinct starting
nodes present in Sj+1 and a common ending node
ne[Lj ] (newly created). From the totality of these
new edges and nodes along with the ones already
present in Lj forms an induced directed acyclic sub-
graph G[Lj ] = (V[Lj ], E [Lj ], ns[Lj ], ne[Lj ]).
For any path pi ? E [Lj ] such that p[pi] = ns[Lj ]
and n[pi] ? Sj , we assign the value of ?(n[pi])
to denote the joint likelihood A[pi]?L[pi] and as-
sign epsilon for word associated with these edges
i.e. w[pi]. We assign T [Sj ] ? ?T to denote Ts[pi]
and T [Sj ] to denote Te[pi]. Similarly, for any path
pi ? E [Lj ] such that p[pi] ? Sj+1 and n[pi] = ne[Lj ],
1121
we assign the value of ?(p[pi])6 to denote the joint
likelihood A[pi]?L[pi] and assign epsilon for word
associated with these edges i.e. w[pi]. We assign
T [Sj+1] to denote Ts[pi] and T [Sj+1] + ?T to de-
note Te[pi]. This completes the process and we ob-
tain self contained lattices, which if need be, can be
independently decoded and/or analyzed.
4.2 Iterative Decoding on Word Lattices
Once we have formed the self contained lattices,
L1,L1, . . . ,LC , where C is the total number of sub
lattices formed, then the idea is to divide the re-
scoring problem into many small re-scoring prob-
lems carried over the sub lattices one at a time by
fixing single best paths from all the remaining sub
lattices.
The inputs to the algorithm are the sub lattices
(produced by cutting the parent lattice generated un-
der some Markov n-gram LM) and a new rescor-
ing LM, which now need not be restricted to fi-
nite state machine family. The output of the al-
gorithm is a word string, W?, such that it is the
concatenation of final decoded word strings from
each sub lattice. Thus if we denote the final de-
coded path (under some decoding scheme, which
will become apparent next) in the jth sub lattice
by pi?j and the concatenation symbol by ???, then
W? = W [pi?1] ?W [pi?2] ? . . . ?W [pi?C ] = Cj=1W [pi?j ].
Algorithm 1 Iterative Decoding on word lattices.
Require: {L1,L1, . . . ,LC}, Lnew
PrevHyp? null
CurrentHyp?Cj=1W [p?ij ]
while PrevHyp 6= CurrentHyp do
for i? 1 . . . C do
p?ii ? argmax
pii?E?i :
p[pii]=ns[Li]
n[pii]=ne[Li]
(
Lnew[ 1? ? . . . ?pii ? . . . ? k?]
?A[pii]?
?k
j=1
j 6=i
A[ j? ]?
)
end for
PrevHyp? CurrentHyp
CurrentHyp?Cj=1W [p?ij ]
end while
?j ? {1, 2, . . . , C} pi?j ? p?ij
6The values of ?(?) and ?(?) are computed under parent lat-
tice structure.
The algorithm is initialized by setting PrevHypo
to null and CurrHypo to the concatenation of 1-best
output from each sub lattice. During the initializa-
tion step, each sub lattice is analyzed independent of
any other sub lattice and under the baseline acoustic
scores and baseline n-gram LM scores, 1-best path
is found out. Thus if we define the best path under
baseline model in some jth sub-lattice by p?ij , Cur-
rHypo is then initialized to: W [p?i1] ? W [p?i2] ? . . . ?
W [p?iC ]. The algorithm then runs as long as Cur-
rHypo is not equal to PrevHypo. In each iteration,
the algorithm sequentially re-scores each sub-lattice
by keeping the surrounding context fixed. Once all
the sub lattices are re-scored, that constitutes one it-
eration. At the end of each iteration, CurrHypo is
set to the concatenation of 1 best paths from each
sub lattice while PrevHypo is set to the old value
of CurrHypo. Thus if we are analyzing some ith
sub-lattice in some iteration, then 1-best paths from
all but this sub-lattice is kept fixed and a new 1-best
path under the re-scoring LM is found out. It is not
hard to see that the likelihood of the output under
the new re-scoring model is guaranteed to increase
monotonically after every decoding step.
Since the cutting of parent lattices produce many
small lattices with considerably lesser number of
nodes, in practice, an exhaustive search for the 1-
best hypothesis can be carried out via N best list.
Algorithm 1 outlines the steps for iterative decoding
on word lattices.
4.3 Entropy Pruning
In this section, we will discuss a speed up technique
based on entropy of the lattice. Entropy of a lattice
reflects the confidence of the recognizer in recogniz-
ing the acoustics. Based on the observation that if
the N best list / lattice generated under some model
has a very low entropy, then the Spearman?s rank
correlation factor, ? (Eqn. 3), tends to be higher
even when the N best lists / lattice is re-ranked with
a bigger and a better model. A low entropy under
the baseline model only reflects the confidence of
the recognizer in recognizing the acoustic. Table 2
shows the rank correlation values between two sets
of N best lists. Both sets are produced by a bi-
gram LM (bg). The entropy of N best lists in the
first set is 0.05 nats or less. The N best lists in the
second set have an entropy greater than 0.05 nats.
1122
Both these sets are re-ranked with bigger and bet-
ter models (see Table 1 for model definitions). We
can see from Table 2 that the rank correlation values
tend to be higher (indicating little re-rankings) when
the entropy of the N best list, under the baseline
model, is lower. Similarly, the rank-correlation val-
ues tend to be lower (indicating more re-rankings)
whenever the entropy of the N best list is higher.
Note that these entropy values are computed with re-
spect to the starting model (in this case, bigram LM).
Of course, if the starting LM is much weaker than
the rescoring model, then the entropy values need
not be reflective of the difficulty of the overall task.
This observation then suggests that it is safe to re-
score only those N best lists whose entropy under
the starting model is higher than some threshold.
Rescoring Model ?(H?0.05) ?(H>0.05)
bg 1.00 1.00
tg 0.58 0.38
n-gram+syntactic 0.54 0.31
Table 2: Spearman Rank Correlation on the N best list
extracted from a bi-gram language model (bg) and re-
scored with relatively better language models (see Table 1
for model definitions). Entropy under the baseline model
correlates well with the rank correlation factor, suggest-
ing that exhaustive search need not be necessary for ut-
terances yielding lower entropy.
While computation of entropy for N best list is
tractable, for a word lattice, the computation of en-
tropy is intractable if one were to enumerate all the
hypotheses. Even if we were able to enumerate all
hypotheses, this method tends to be slower. Using
efficient semiring techniques introduced by Li and
Eisner (2009) or using posterior probabilities on the
edges leading to end states, we can compute the en-
tropy of a lattice in one single forward pass using
dynamic programming. It should, however, be noted
that, for dynamic programming technique to work,
only n-gram LMs can be used. One has to resort to
approximate entropy computation via N best list, if
entropy under long span LM is desired.
4.3.1 Speed Up for Iterative Decoding
Our speed up technique is simple. Once we have
formed self contained sub lattices, we want to prune
all but the top few best complete paths (obtained un-
der baseline / starting model) of those sub lattices
whose entropy is below some threshold. Thus, be-
lieving in the original model?s confidence, we want
to focus only on those sub lattices which the recog-
nizer found difficult to decode in the first pass. All
other part of the parent lattice will be not be ana-
lyzed. The thresholds for pruning is very application
and corpus specific and needs to be tuned on some
held out data.
5 Experiments and Results
We performed recognition on the Broadcast News
(BN) dev04f, rt03 and rt04 task using the state-
of-the-art acoustic models trained on the English
Broadcast News (BN) corpus (430 hours of audio)
provided to us by IBM (Chen et al, 2009). IBM also
provided us its state-of-the-art speech recognizer,
Attila (Soltau et al, 2010) and two Kneser-Ney
smoothed backoff n-gram LMs containing 4.7M n-
grams (n ? 4) and 54M n-grams (n ? 4), both
trained on 400M word tokens. We will refer to them
as KN:BN-Small and KN:BN-Big respectively. We
refer readers to (Chen et al, 2009) for more details
about the recognizer and corpora used for training
the models.
We trained two RNN based language models -
the first one, denoted further as RNN-limited, was
trained on a subset of the training data (58M tokens).
It used 400 neurons in the hidden layer. The second
model, denoted as RNN-all, was trained on all of
the training data (400M tokens), but due to the com-
putational complexity issues, we had to restrict its
hidden layer size to 320 neurons.
We followed IBM?s multi-pass decoding recipe
using KN:BN-Small in the first pass followed by ei-
ther N best list re-scoring or word lattice re-scoring
using bigger and better models.7 For the purpose
of re-scoring, we combined all the relevant statisti-
cal models in one unified log linear framework rem-
iniscent of work by Beyerlein (1998). We, however,
trained the model weights by optimizing expected
WER rather than 1-best loss as described in (De-
oras et al, 2010). Training was done on N best
lists of size 2K. We will refer to the log linear com-
7The choice of the order and size of LM to be used in the
first pass decoding was determined by taking into consideration
the capabilities of the decoder.
1123
100 101 102 103 104 105 10610.8
11
11.2
11.4
11.6
11.8
12
Size of Search Space (Number of Hypotheses for evaluation)
1 be
st W
ER(%
)
Plot of 1 best WER v/s Search Space Size
 
 
N BestIter. Dec. (ID)ID with Ent. PruningViterbi BaselineViterbi Rescoring
Figure 2: Plot of WER (y axis) on rt03+dev04f set versus
the size of the search space (x axis). The baseline WER
obtained using KN:BN-Small is 12% which then drops
to 11% when KN:BN-Big is used for re-scoring. N best
list search method obtains the same reduction in WER
by evaluating as many as 228K sentence hypotheses on
an average. The proposed method obtains the same re-
duction by evaluating 14 times smaller search space. The
search effort reduces further to 40 times if entropy based
pruning is employed during re-scoring.
bination of KN:BN-Big and RNN-limited by KN-
RNN-lim; KN:BN-Big and RNN-all by KN-RNN-
all and KN:BN-Big, RNN-limited and RNN-all by
KN-RNN-lim-all.
We used two sets for decoding: rt03+dev04f set
was used as a development set while rt04 was used
as a blind set for the purpose of evaluating the per-
formance of long span RNN models using the pro-
posed approach. We made use of OpenFst C++ li-
braries (Allauzen et al, 2007) for manipulating lat-
tice graphs and generating N best lists. Due to the
presence of hesitation tokens in reference transcripts
and the need to access the silence/pause tokens for
penalizing short sentences, we treated these tokens
as regular words before extracting sentence hypothe-
ses. This, and poorly segmented nature of the test
corpora, led to huge enumeration of sentence hy-
potheses.
5.1 n-gram LM for re-scoring
In this setup, we used KN:BN-Small as the base-
line starting LM which yielded the WER of 12%
on rt03+dev04f set. Using KN:BN-Big as the re-
scoring LM, the WER dropped to 11%. Since the
re-scoring LM belonged to the n-gram family, it was
possible to compute the optimal word string by re-
scoring the whole lattice (see Sec. 3.1). We now
compare the performance of N best list approach
(Sec. 3.2) with our proposed approach (Sec. 4).
N best list achieved the best possible reduction by
evaluating as many as 228K sentence hypotheses
on an average. As against that, our proposed ap-
proach achieved the same performance by evaluat-
ing 16.6K sentence hypotheses, thus reducing the
search efforts by 13.75 times. By carrying out en-
tropy pruning (see Sec. 4.3 ) on sub lattices, our pro-
posed approach required as little as 5.6K sentence
hypotheses evaluations to obtain the same optimal
performance, reducing the search effort by as much
as 40.46 times. For the purpose of this experiment,
entropy based pruning was carried out when the en-
tropy of the sub lattice was below 5 nats. Table 3
compares the two search methods for this setup and
Fig. 2 shows a plot of WER versus the size of the
search space (in terms of number of sentence hy-
potheses evaluated by an n-gram language model).
On rt04, the KN:BN-Small LM gave a WER of
14.1% which then dropped to 13.1% after re-scoring
with KN:BN-Big. Since the re-scoring model was
an n-gram LM, it was possible to obtain the opti-
mal performance via lattice update technique (see
Sec. 3.1). We then carried out the re-scoring of the
word lattices under KN:BN-Big using our proposed
technique and found it to give the same performance
yielding the WER of 13.1%.
5.2 Long Span LM for re-scoring
In this setup, we used the strongest n-gram LM
as our baseline. We thus used KN:BN-Big as the
baseline LM which yielded the WER of 11% on
rt03+dev04f. We then used KN-RNN-lim-all for re-
scoring. Due to long span nature of the re-scoring
LM, it was not possible to obtain the optimal WER
performance. Hence we have compared the perfor-
mance of our proposed method with N best list ap-
proach. N best list achieved the lowest possible
WER after evaluating as many as 33.8K sentence
hypotheses on an average. As against that, our pro-
posed approach in conjunction with entropy pruning
obtained the same performance by evaluating just
1.6K sentence hypotheses, thus reducing the search
by a factor of 21. Fig 3 shows a plot of WER versus
1124
100 101 102 103 10410.3
10.4
10.5
10.6
10.7
10.8
10.9
11
11.1
Size of Search Space (Number of Hypotheses for evaluation)
1 be
st W
ER(%
)
Plot of 1 best WER v/s Search Space Size
 
 
N BestID with Ent. PruningViterbi Baseline
Figure 3: Plot of WER (y axis) on rt03+dev04f set versus
the size of the search space (x axis). The baseline WER
obtained using KN:BN-Big is 11% which then drops to
10.4% when KN-RNN-lim-all is used for re-scoring. N
best list search method obtains this reduction in WER by
evaluating as many as 33.8K sentence hypotheses on an
average, while the proposed method (with entropy prun-
ing) obtains the same reduction by evaluating 21 times
smaller search space.
the size of the search space (in terms of number of
sentence hypotheses evaluated by a long span lan-
guage model).
In-spite of starting off with a very strong n-gram
LM, theN best lists so extracted were still not repre-
sentative enough of the long span rescoring models.
Had we started off with KN:BN-Small, the N best
list re-scoring method would have had no chance of
finding the optimal hypothesis in reasonable size of
hypotheses search space. Table 4 compares the two
search methods for this setup when many other long
span LMs were also used for re-scoring.
On rt04, the KN:BN-Big LM gave a WER of
13.1% which then dropped to 12.15% after re-
scoring with KN-RNN-lim-all using our proposed
technique.8 Since the re-scoring model was not an
n-gram LM, it was not possible to obtain the optimal
performance but we could enumerate huge N best
list to approximate this value. Our proposed method
is much faster than huge N best lists and no worse
in terms of WER. As far as we know, the result ob-
tained on these sets is the best performance ever
reported on the Broadcast News corpus for speech
8The WER obtained using KN-RNN-lim and KN-RNN-all
were 12.5% and 12.3% respectively.
recognition.
Models WER NBest ID Saving
KN:BN-Small 12.0 - - -
KN:BN-Big 11.0 228K 5.6K 40
Table 3: The starting LM is a weak n-gram LM (KN:BN-
Small) and the re-scoring LM is a much stronger but n-
gram LM (KN:BN-Big). The baseline WER in this case
is 12% and the optimal performance by the re-scoring LM
is 11.0%. The proposed method outperforms N best list
approach, in terms of search efforts, obtaining optimal
WER.
Models WER NBest ID Saving
KN:BN-Big 11.0 - - -
KN-RNN-lim 10.5 42K 1.1K 38
KN-RNN-all 10.5 26K 1.3K 20
KN-RNN-lim-all 10.4 34K 1.6K 21
Table 4: The starting LM is a strong n-gram LM
(KN:BN-Big) and the re-scoring model is a long span
LM (KN-RNN-*). The baseline WER is 11.0%. Due
to long span nature of the LM, optimal WER could not
be estimated. The proposed method outperfoms N best
list approach on every re-scoring task.
6 Conclusion
We proposed and demonstrated a new re-scoring
technique for general word graph structures such as
word lattices. We showed its efficacy by demonstrat-
ing huge reductions in the search effort to obtain a
new state-of-the-art performance on a very compet-
itive speech task of Broadcast news. As part of the
future work, we plan to extend this technique for hy-
pergraphs and lattices in re-scoring MT outputs with
complex and long span language models.
Acknowledgement
This work was partly funded by Human Language
Technology, Center of Excellence and by Tech-
nology Agency of the Czech Republic grant No.
TA01011328, and Grant Agency of Czech Repub-
lic project No. 102/08/0707. We would also like to
acknowledge the contribution of Frederick Jelinek
towards this work. He would be a co-author if he
were available and willing to give his consent.
1125
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A General and Efficient Weighted Finite-State Trans-
ducer Library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
J. R. Bellegarda. 2000. Exploiting latent semantic infor-
mation in statistical language modeling. Proceedings
of IEEE, 88(8):1279?1296.
Yoshua Bengio, Re?jean Ducharme, and Pascal Vincent.
2001. A Neural Probabilistic Language Model. In
Proceedings of Advances in Neural Information Pro-
cessing Systems.
Peter Beyerlein. 1998. Discriminative Model Combina-
tion. In Proc. of IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured Language Modeling. Computer Speech and Lan-
guage, 14(4):283?332.
S. F. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya,
and A. Sethy. 2009. Scaling shrinkage-based lan-
guage models. In Proc. of IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 299?304.
Noam Chomsky. 1957. Syntactic Structures. The
Hague: Mouton.
Yen-Lu Chow and Richard Schwartz. 1989. The N-Best
algorithm: an efficient procedure for finding top N sen-
tence hypotheses. In Proceedings of the workshop on
Speech and Natural Language, HLT ?89, pages 199?
202, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kenneth Church. 2012. A Pendulum Swung Too Far.
Linguistic Issues in Language Technology - LiLT. to
appear.
T.M. Cover and J.A.Thomas. 1991. Elements of Infor-
mation Theory. John Wiley and Sons, Inc. N.Y.
Anoop Deoras and Frederick Jelinek. 2009. Iterative De-
coding: A Novel Re-Scoring Framework for Confu-
sion Networks. In Proc. of IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 282 ?286.
Anoop Deoras, Denis Filimonov, Mary Harper, and Fred
Jelinek. 2010. Model Combination for Speech Recog-
nition using Empirical Bayes Risk Minimization. In
Proc. of IEEE Workshop on Spoken Language Tech-
nology (SLT).
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink, Mar-
tin Karafia?t, and Sanjeev Khudanpur. 2011. Varia-
tional Approximation of Long-Span Language Mod-
els for LVCSR. In Proc. of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Jeffery Elman. 1990. Finding Structure in Time. In Cog-
nitive Science, volume 14, pages 179?211.
Denis Filimonov and Mary Harper. 2009. A Joint Lan-
guage Model with Fine-grain Syntactic Tags. In Proc.
of 2009 Conference on Empirical Methods in Natural
Language Processing.
V. Goel and W. Byrne. 2000. Minimum Bayes Risk Au-
tomatic Speech Recognition. Computer, Speech and
Language.
Rukmini Iyer and Mari Ostendorf. 1999. Modeling Long
Distance Dependence in Language: Topic Mixtures
Versus Dynamic Cache Models. IEEE Transactions
on Speech and Audio Processing, 7(1):30?39.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 40?51, Singapore,
August.
Lidia Luminita Mangu. 2000. Finding consensus in
speech recognition. Ph.D. thesis, The Johns Hopkins
University. Adviser-Brill, Eric.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget,
Jan ?Honza? C?ernocky?, and Sanjeev Khudanpur.
2010. Recurrent Neural Network Based Language
Model. In Proc. of the ICSLP-Interspeech.
Toma?s? Mikolov, Anoop Deoras, Stefan Kombrink, Luka?s?
Burget, and Jan ?Honza? C?ernocky?. 2011a. Empirical
Evaluation and Combination of Advanced Language
Modeling Techniques. In Proc. of Interspeech.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget,
Jan ?Honza? C?ernocky?, and Sanjeev Khudanpur.
2011b. Extensions of Recurrent Neural Network Lan-
guage Model. In Proc. of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Mehryar Mohri and Michael Riley. 2002. An Efficient
Algorithm for the N-Best-Strings Problem. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP).
M. Mohri, F.C.N. Pereira, and M. Riley. 2000. The de-
sign principles of a weighted finite-state transducer li-
brary. Theoretical Computer Science, 231:17-32.
A. Ogawa, K. Takeda, and F. Itakura. 1998. Balanc-
ing Acoustic and Linguistic Probabilities. In Proc. of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
1126
F. Richardson, M. Ostendorf, and J.R. Rohlicek. 1995.
Lattice-based search strategies for large vocabulary
speech recognition. In Proc. of IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP).
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001.
Whole-Sentence Exponential Language Models: a Ve-
hicle for Linguistic-Statistical Integration. Computer
Speech and Language, 15(1).
Roni Rosenfeld. 1997. A Whole Sentence Maximum
Entropy Language Model. In Proc. of IEEE workshop
on Automatic Speech Recognition and Understanding
(ASRU), Santa Barbara, California, December.
D.E. Rumelhart, G. E. Hinton, and R.J. Williams. 1986.
Learning representations by back-propagating errors.
Nature, 323:533?536.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
C. E. Shannon. 1948. A Mathematical Theory of
Communication. The Bell System Technical Journal,
27:379?423, 623?656.
H. Soltau, G. Saon, and B. Kingsbury. 2010. The IBM
Attila speech recognition toolkit. In Proc. of IEEE
Workshop on Spoken Language Technology (SLT).
Wen Wang and Mary Harper. 2002. The SuperARV lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
1127
A Sketch Algorithm for Estimating Two-Way
and Multi-Way Associations
Ping Li?
Stanford University
Kenneth W. Church??
Microsoft Corporation
We should not have to look at the entire corpus (e.g., the Web) to know if two (or more) words are
strongly associated or not. One can often obtain estimates of associations from a small sample.
We develop a sketch-based algorithm that constructs a contingency table for a sample. One can
estimate the contingency table for the entire population using straightforward scaling. However,
one can do better by taking advantage of the margins (also known as document frequencies). The
proposed method cuts the errors roughly in half over Broder?s sketches.
1. Introduction
We develop an algorithm for efficiently computing associations, for example, word
associations.1 Word associations (co-occurrences, or joint frequencies) have a wide range
of applications including: speech recognition, optical character recognition, and infor-
mation retrieval (IR) (Salton 1989; Church and Hanks 1991; Dunning 1993; Baeza-Yates
and Ribeiro-Neto 1999; Manning and Schutze 1999). The Know-It-All project computes
such associations at Web scale (Etzioni et al 2004). It is easy to compute a few association
scores for a small corpus, but more challenging to compute lots of scores for lots of data
(e.g., the Web), with billions of Web pages (D) and millions of word types.
Web search engines produce estimates of page hits, as illustrated in Tables 1?
3.2 Table 1 shows hits for two high frequency words, a and the, suggesting that the
total number of English documents is roughly D ? 1010. In addition to the two high-
frequency words, there are three low-frequency words selected from The New Oxford
Dictionary of English (Pearsall 1998). The low-frequency words demonstrate that there
are many hits, even for relatively rare words.
How many page hits do ?ordinary? words have? To address this question, we ran-
domly picked 15 pages from a learners? dictionary (Hornby 1989), and selected the first en-
try on each page. According to Google, there are 10 million pages/word (median value,
aggregated over the 15 words). To compute all two-way associations for the 57,100 en-
tries in this dictionary would probably be infeasible, let alne all multi-way associations.
? Department of Statistical Science, Cornell University, Ithaca, NY 14853. E-mail: pl332@cornell.edu.
?? Microsoft Research, Microsoft Corp., Redmond, WA 98052. E-mail: church@microsoft.com.
1 This paper considers boolean (0/1) data. See Li, Church, and Hastie (2006, 2007) for generalizations to
real-valued data (and lp distances).
2 All experiments with MSN.com and Google were conducted in August 2005.
Submission received: 6 December 2005; revised submission received: 5 September 2006; accepted for
publication: 7 December 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 3
Table 1
Page hits for a few high-frequency words and a few low-frequency words (as of August 2005).
Query Hits (MSN.com) Hits (Google)
A 2,452,759,266 3,160,000,000
The 2,304,929,841 3,360,000,000
Kalevala 159,937 214,000
Griseofulvin 105,326 149,000
Saccade 38,202 147,000
Table 2
Estimates of page hits are not always consistent. Joint frequencies ought to decrease
monotonically as we add terms to the query, but estimates produced by current state-of-the-art
search engines sometimes violate this invariant.
Query Hits (MSN.com) Hits (Google)
America 150,731,182 393,000,000
America, China 15,240,116 66,000,000
America, China, Britain 235,111 6,090,000
America, China, Britain, Japan 154,444 23,300,000
Table 3
This table illustrates the usefulness of joint counts in query planning for databases. To minimize
intermediate writes, the optimal order of joins is: ((?Schwarzenegger? ? ?Austria?) ?
?Terminator?) ? ?Governor,? with 136,000 intermediate results. The standard practice starts
with the least frequent terms, namely, ((?Schwarzenegger? ? ?Terminator?) ? ?Governor?) ?
?Austria,? with 579,100 intermediate results.
Query Hits (Google)
Austria 88,200,000
Governor 37,300,000
One-way Schwarzenegger 4,030,000
Terminator 3,480,000
Governor, Schwarzenegger 1,220,000
Governor, Austria 708,000
Schwarzenegger, Terminator 504,000
Two-way Terminator, Austria 171,000
Governor, Terminator 132,000
Schwarzenegger, Austria 120,000
Governor, Schwarzenegger, Terminator 75,100
Three-way Governor, Schwarzenegger, Austria 46,100
Schwarzenegger, Terminator, Austria 16,000
Governor, Terminator, Austria 11,500
Four-way Governor, Schwarzenegger, Terminator, Austria 6,930
Estimates are often good enough. We should not have to look at every document
to determine whether two words are strongly associated or not. One could use the
estimated co-occurrences from a small sample to compute the test statistics, most com-
monly Pearson?s chi-squared test, the likelihood ratio test, Fisher?s exact test, cosine
similarity, or resemblance (Jaccard coefficient) (Dunning 1993; Manning and Schutze
1999; Agresti 2002; Moore 2004).
306
Li and Church Sketch for Estimating Associations
Sampling can make it possible to work in physical memory, avoiding disk accesses.
Brin and Page (1998) reported an inverted index of 37.2 GBs for 24 million pages. By
extrapolation, we should expect the size of the inverted indexes for current Web scale
to be 1.5 TBs/billion pages, probably too large for physical memory. A sample is more
manageable.
When estimating associations, it is desirable that the estimates be consistent. Joint
frequencies ought to decrease monotonically as we add terms to the query. Table 2
shows that estimates produced by current search engines are not always consistent.
1.1 The Data Matrix, Postings, and Contingency Tables
We assume a term-by-document matrix, A, with n rows (words) and D columns (doc-
uments). Because we consider boolean (0/1) data, the (i, j)th entry of A is 1 if word i
occurs in document j and 0 otherwise. Computing all pair-wise associations of A is a
matrix multiplication, AAT.
Because word distributions have long tails, the term-by-document matrix is highly
sparse. It is common practice to avoid materializing the zeros in A, by storing the matrix
in adjacency format, also known as postings, and an inverted index (Witten, Moffat, and
Bell 1999, Section 3.2). For each word W, the postings list, P, contains a sorted list of
document IDs, one for each document containing W.
Figure 1(a) shows a contingency table. The contingency table for words W1 and W2
can be expressed as intersections (and complements) of their postings P1 and P2 in the
obvious way:
a = |P1 ? P2|, b = |P1 ? ?P2|, c = |?P1 ? P2|, d = |?P1 ? ?P2| (1)
where ?P1 is short-hand for ?? P1, and ? = {1, 2, 3, . . . , D} is the set of all document
IDs. As shown in Figure 1(a), we denote the margins by f1 = a + b = |P1| and f2 = a +
c = |P2|.
For larger corpora, it is natural to introduce sampling. For example, we can ran-
domly sample Ds (out of D) documents, as illustrated in Figure 1(b). This sampling
scheme, which we call sampling over documents, is simple and easy to describe?but we
can do better, as we will see in the next subsection.
Figure 1
(a) A contingency table for word W1 and word W2. Cell a is the number of documents that
contain both W1 and W2, b is the number that contain W1 but not W2, c is the number that
contain W2 but not W1, and d is the number that contain neither. The margins, f1 = a + b and
f2 = a + c are known as document frequencies in IR. D = a + b + c + d is the total number
of documents in the collection. For consistency with the notation we use for multi-way
associations, a, b, c, and d are also denoted, in parentheses, by x1, x2, x3, and x4, respectively.
(b) A sample contingency table (as, bs, cs, ds), where the subscript s indicates the sample space.
The cells are also numbered as (s1, s2, s3, s4).
307
Computational Linguistics Volume 33, Number 3
1.2 Sampling Over Documents and Sampling Over Postings
Sampling over documents selects Ds documents randomly from a collection of D docu-
ments, as illustrated in Figure 1.
The task of computing associations is broken down into three subtasks:
1. Compute sample contingency table.
2. Estimate contingency table for population from sample.
3. Summarize contingency table to produce desired measure of association:
cosine, resemblance, mutual information, correlation, and so on.
Sampling over documents is simple and well understood. The estimation task is
straightforward if we ignore the margins. That is, we simply scale up the sample in
the obvious way: a?MF = as DDs . We refer to these estimates as the ?margin-free? baseline.
However, we can do better when we know the margins, f1 = a + b and f2 = a + c (called
document frequencies in IR), using a maximum likelihood estimator (MLE) with fixed
margin constraints.
Rare words can be a challenge for sampling over documents. In terms of the term-
by-document matrix A, sampling over documents randomly picks a fraction ( DsD ) of
columns from A. This is a serious drawback because A is highly sparse (as word
distributions have long tails) with a few high-frequency words and many low-frequency
words. The jointly non-zero entries in A are unlikely to be sampled unless the sampling
rate DsD is high. Moreover, the word sparsity differs drastically from one word to another;
it is thus desirable to have a sampling mechanism that can adapt to the data sparsity
with flexible sample sizes. One size does not fit all.
?Sampling over postings? is an interesting alternative to sampling over docu-
ments. Unfortunately, it doesn?t work out all that well either (at least using a sim-
ple straightforward implementation), but we present it here nevertheless, because it
provides a convenient segue between sampling over documents and our sketch-based
recommendation.
?Naive sampling over postings? obtains a random sample of size k1 from P1, de-
noted as Z1, and a random sample Z2 of size k2 from P2. Also, we denote aNs = |Z1 ? Z2|.
We then use aNs to infer a. For simplicity, assume k1 = k2 = k and f1 = f2 = f . It follows
that3 E
(
aNs
a
)
= k
2
f 2 . In other words, under naive sampling over postings, one could
estimate the associations by f
2
k2 a
N
s .
3 Suppose there are m defectives among N objects. We randomly pick k objects (without replacement) and
obtain x defectives. Then x follows a hypergeometric distribution, x ? HG(N, m, k). It is known that E(x) =
m
N k. In our setting, suppose we know that among Z1 (of size k1), there are a
Z1
s samples that belong to the
original intersection P1 ? P2. Similarly, suppose we know that there are a
Z2
s samples among Z2 (of size k2)
that belong to P1 ? P2. Then aNs = |Z1 ? Z2| ? HG(a, a
Z1
s , a
Z2
s ). Therefore E
(
aNs
)
= 1a a
Z1
s a
Z2
s . Because a
Z1
s
and aZ2s are both random, we should use conditional expectations: E
(
aNs
)
= E
(
E
(
aNs |a
Z1
s , a
Z2
s
))
=
E
(
1
a a
Z1
s a
Z2
s
)
= 1a E
(
aZ1s
)
E
(
aZ2s
)
. (Recall that Z1 and Z2 are independent.) Note that a
Z1
s ? HG( f1, a, k1)
and aZ2s ? HG( f2, a, k2), that is, E
(
aZ1s
)
= af1
k1 and E
(
aZ2s
)
= af2
k2. Therefore, E
(
aNs
)
= 1a af1
k1 af2 k2,
namely, E
(
aNs
a
)
= k1k2f1f2
.
308
Li and Church Sketch for Estimating Associations
Figure 2
The proposed sketch method (solid curve) produces larger counts (as) with less work (k).
With ?naive sampling over postings,? there is an undesirable quadratic: E
(
aNs
a
)
= k2
f 2
(dashed
curve), whereas with sketches, E
( as
a
)
? kf . These results were generated by simulation,
with f1 = f2 = f = 0.2D, D = 105 and a = 0.22, 0.38, 0.65, 0.80, 0.85f . There is only one
dashed curve across all values of a. There are different (but indistinguishable) solid curves
depending on a.
Of course, the quadratic relation, E
(
aNs
a
)
= k
2
f 2 , is undesirable; 1% effort returns only
0.01% useful information. Ideally, to maximize the signal, we?d like to see large counts
in a small sample, not small counts in a large sample. The crux is as, which tends to have
the smallest counts. We?d like as to be as large as possible, but we?d also like to do as
little work (k) as possible. The next subsection on sketches proposes an improvement,
where 1% effort returns roughly 1% useful information, as illustrated in Figure 2.
1.3 An Improvement Based on Sketches
A sketch is simply the front of the postings (after a random permutation). We find it
helpful, as an informal practical metaphor, to imagine a virtual machine architecture
where sketches (Broder 1997), the front of the postings, reside in physical memory, and
the rest of the postings are stored on disk. More formally, the sketch, K = MINk(?(P)),
contains the k smallest postings, after applying a random permutation ? to document
IDs, ? = {1, 2, 3, . . . , D}, to eliminate whatever structure there might be.
Given two words, W1 and W2, we have two sets of postings, P1 and P2, and two
sketches, K1 = MINk1 (?(P1)) and K2 = MINk2 (?(P2)). We construct a sample contin-
gency table from the two sketches. Let ?s = {1, 2, 3, . . . , Ds} be the sample space, where
Ds is set to min(max(K1), max(K2)). With this choice of Ds, all the document IDs in the
sample space,?s, can be assigned to the appropriate cell in the sample contingency table
without looking outside the sketch. One could use a smaller Ds, but doing so would
throw out data points unnecessarily.
The sample contingency table is constructed from K1 and K2 in O(k1 + k2) time,
using a straightforward linear pass over the two sketches:
as = |K1 ? K2 ? ?s| = |K1 ? K2| bs = |K1 ? ?K2 ? ?s|
(2)
cs = |?K1 ? K2 ? ?s| ds = |?K1 ? ?K2 ? ?s|
309
Computational Linguistics Volume 33, Number 3
The final step is an estimation task. The margin-free (MF) estimator recovers the
original contingency table by a simple scaling. For better accuracy, one could take
advantage of the margins by using a maximum likelihood estimator (MLE).
With ?sampling over documents,? it is convenient to express the sampling rate in
terms of Ds and D, whereas with sketches, it is convenient to express the sampling rate
in terms of k and f . The following two approximations allow us to flip back and forth
between the two views:
E
(
Ds
D
)
? min
(
k1
f1
, k2
f2
)
(3)
E
(
D
Ds
)
? max
(
f1
k1
,
f2
k2
)
(4)
In other words, using sketches with size k, the corresponding sample size Ds in
?sampling over documents? would be Ds ? Df k, where Df represents the data sparsity.
Because the estimation errors (variances) are inversely proportional to sample size,
we know the proposed algorithm improves ?sampling over documents? by a factor
proportional to the data sparsity.
1.4 Improving Estimates Using Margins
When we know the margins, we ought to use them. The basic idea is to maximize the
likelihood of the sample contingency table under margin constraints. In the pair-wise
case, we will show that the resultant maximum likelihood estimator is the solution to a
cubic equation, which has a remarkably accurate quadratic approximation.
The use of margins for estimating contingency tables was suggested in the 1940s
(Deming and Stephan 1940; Stephan 1942) for a census application. They developed
a straightforward iterative estimation method called iterative proportional scaling,
which was an approximation to the maximum likelihood estimator.
Computing margins is usually much easier than computing interactions. For a data
matrix A of n rows and D columns, computing all marginal l2 norms costs only O(nD),
whereas computing all pair-wise associations (or l2 distances) costs O(n2D). One could
compute the margins in a separate prepass over the data, without increasing the time
and space complexity, though we suggest computing the margins while applying the
random permutation ? to all the document IDs on all the postings.
1.5 An Example
Let?s start with conventional random sampling over documents, using a running exam-
ple in Figure 3. We choose a sample of Ds = 18 documents randomly out of a collection
of D = 36. After applying the random permutation, document IDs will be uniformly
random. Thus, we can construct the random sample by picking any Ds documents. For
convenience, we pick the first Ds. The sample contingency table is then constructed, as
illustrated in Figure 3.
The recommended procedure is illustrated in Figure 4. The two sketches, K1 and
K2, are highlighted in the large box. We find it convenient, as an informal practi-
cal metaphor, to think of the large box as physical memory. Thus, the sketches re-
side in physical memory, and the rest are paged out to disk. We choose Ds to be
min(max(K1), max(K2)) = min(18, 21) = 18, so that we can compute the sample contin-
310
Li and Church Sketch for Estimating Associations
Figure 3
In this example, the corpus contains D = 36 documents. The population is: ? = {1, 2, . . . , D}.
The sample space is ?s = {1, 2, . . . , Ds}, where Ds = 18. Circles denote documents containing
W1, and squares denote documents containing W2. The sample contingency table is: as =
|{4, 15}| = 2, bs = |{3, 7, 9, 10, 18}| = 5, cs = |{2, 5, 8}| = 3, ds = |{1, 6, 11, 12, 13, 14, 16, 17}| = 8.
Figure 4
This procedure, which we recommend, produces the same sample contingency table as in
Figure 3: as = 2, bs = 5, cs = 3, and ds = 8. The two sketches, K1 and K2 (larger shaded box),
reside in physical memory, and the rest of the postings are paged out to disk. K1 contains
of the first k1 = 7 document IDs in P1 and K2 contains of the first k2 = 7 IDs in P2. We
assume P1 and P2 are already permuted, otherwise we should write ?(P1) and ?(P2) instead.
Ds = min(max(K1), max(K2))= min(18, 21) = 18. The sample contingency table is computed
from the sketches (large box) in time k1 + k2, but documents exceeding Ds are excluded from ?s
(small box), because we can?t tell if they are in the intersection or not, without looking outside
the sketch. As it turns out, 19 is in the intersection and 21 is not.
gency table for?s = {1, 2, 3, . . . , Ds} in physical memory in time O (k1 + k2) from K1 and
K2. In this example, documents 19 and 21 (highlighted in the smaller box) are excluded
from ?s. It turns out that 19 is part of the intersection, and 21 is not, but we would have
to look outside the sketches (and suffer a page fault) to determine that. The resulting
sample contingency table is the same as in Figure 3:
as = |{4, 15}| = 2 bs = |K1 ? ?s| ? as = 7 ? 2 = 5
cs = |K2 ? ?s| ? as = 5 ? 2 = 3 ds = Ds ? (as + bs + cs) = 8
1.6 A Five-Word Example
Figure 5 shows an example with more than two words. There are D = 15 documents in
the collection. We generate a random permutation ? as shown in Figure 5(b). For every
ID in postings Pi in Figure 5(a), we apply the random permutation ?, but we only store
the ki smallest IDs as a sketch Ki, that is, Ki = MINki (?(Pi)). In this example, we choose
k1 = 4, k2 = 4, k3 = 4, k4 = 3, k5 = 6. The sketches are stored in Figure 5(c). In addition,
because ?(Pi) operates on every ID in Pi, we know the total number of non-zeros in Pi,
denoted by fi = |Pi|.
The estimation procedure is straightforward if we ignore the margins. For example,
suppose we need to estimate the number of documents containing the first two words.
In other words, we need to estimate the inner product between P1 and P2, denoted
by a(1,2). (We have to use the additional subscript (1,2) because we have more than
311
Computational Linguistics Volume 33, Number 3
Figure 5
The original postings sets are given in (a). There are D = 15 documents in the collection. We
generate a random permutation ? as shown in (b). We apply ? to the postings Pi and store the
sketch Ki = MINki (?(Pi)). For example, ?(P1) = {11, 13, 1, 12, 15, 6, 8}. We choose k1 = 4; and
hence the four smallest IDs in ?(P1) are K1 = {1, 6, 8, 11}. We choose k2 = 4, k3 = 4, k4 = 3,
and k5 = 6.
just two words in the vocabulary.) We calculate, from sketches K1 and K2, the sample
inner product as,(1,2) = |{6}| = 1, and the corresponding corpus sample size, denoted
by Ds,(1,2) = min(max(K1), max(K2)) = min(11, 12) = 11. Therefore, the ?margin-free?
estimate of a(1,2) is simply as,(1,2) DDs,(1,2) = 1
15
11 = 1.4.
This estimate can be compared to the ?truth,? which is obtained from the complete
postings list, as opposed to the sketch. In this case, P1 and P2 have 4 documents in
common. And therefore, the estimation error is 4 ? 1.4 or 2.6 documents.
Similarly, for P1 and P5, Ds,(1,5) = min(11, 6) = 6, as,(1,5) = 2. Hence, the ?margin-
free? estimate of a(1,5) is simply 2 156 = 5.0. In this case, the estimate matches the ?truth?
perfectly.
The procedure can be easily extended to more than two rows. Suppose we
would like to estimate the three-way inner product (three-way joins) among P1,
P4, and P5, denoted by a(1,4,5). We calculate the three-way sample inner product
from K1, K4, and K5, as,(1,4,5) = |{6}| = 1, and the corpus sample size Ds,(1,4,5) =
min(max(K1), max(K4), max(K5)) = min(11, 12, 6) = 6. Then the ?margin-free? estimate
of a(1,4,5) is 1 156 = 2.5.
Of course, we can improve these estimates by taking advantage of the margins.
2. Applications
There is a large literature on sketching techniques (e.g., Alon, Matias, and Szegedy 1996;
Broder 1997; Vempala 2004). Such techniques have applications in information retrieval,
databases, and data mining (Broder et al 1997; Haveliwala, Gionis, and Indyk 2000;
Haveliwala et al 2002).
Broder?s sketches (Broder 1997) were originally introduced to detect duplicate
documents in Web crawls. Many URLs point to the same (or nearly the same) HTML
blobs. Approximate answers are often good enough. We don?t need to find all such
pairs, but it is handy to find many of them, without spending more than it is worth on
computational resources.
In IR applications, physical memory is often a bottleneck, because the Web collec-
tion is too large for memory, but we want to minimize seeking data in the disk as the
query response time is critical (Brin and Page 1998). As a space saving device, dimension
reduction techniques use a compact representation to produce approximate answers in
physical memory.
312
Li and Church Sketch for Estimating Associations
Section 1 mentioned page hit estimation. If we have a two-word query, we?d like
to know how many pages mention both words. We assume that pre-computing and
storing page hits is infeasible, at least not for infrequent pairs of words (and multi-word
sequences).
It is customary in information retrieval to start with a large boolean term-by-
document matrix. The boolean values indicate the presence or absence of a term in a
document. We assume that these matrices are too large to store in physical memory.
Depending on the specific applications, we can construct an inverted index and store
sketches either for terms (to estimate word association) or for documents (to estimate
document similarity).
2.1 Association Rule Mining
?Market-basket? analysis and association rules (Agrawal, Imielinski, and Swami 1993;
Agrawal and Srikant 1994; Agrawal et al 1996; Hastie, Tibshirani, and Friedman
2001, Chapter 14.2) are useful tools for mining commercial databases. Commercial
databases tend to be large and sparse (Aggarwal and Wolf 1999; Strehl and Ghosh
2000). Various sampling algorithms have been proposed (Toivonen 1996; Chen, Haas,
and Scheuermann 2002). The proposed algorithm scales better than traditional ran-
dom sampling (i.e., a fixed sample of columns of the data matrix) for reasons men-
tioned earlier. In addition, the proposed algorithm makes it possible to estimate
association rules on-line, which may have some advantage in certain applications
(Hidber 1999).
2.2 All Pair-Wise Associations (Distances)
In many applications, including distance-based classification or clustering and bi-gram
language modeling (Church and Hanks 1991), we need to compute all pair-wise asso-
ciations (or distances). Given a data matrix A of n rows and D columns, brute force
computation of AAT would cost O(n2D), or more efficiently, O(n2 f? ), where f? is the
average number of non-zeros among all rows of A. Brute force could be very time-
consuming. In addition, when the data matrix is too large to fit in the physical memory,
the computation may become especially inefficient.
Using our proposed algorithm, the cost of computing AAT can be reduced to
O(nf? ) + O(n2k?), where k? is the average sketch size. It costs O(nf? ) for constructing
sketches and O(n2k?) for computing all pair-wise associations. The savings would be
significant when k? 
 f? . Note that AAT is called ?Gram Matrix? in machine learning; and
various algorithms have been proposed for speeding up the computation (e.g., Drineas
and Mahoney 2005).
Ravichandran, Pantel, and Hovy (2005) computed pair-wise word associations
(boolean data) among n ? 0.6 million nouns in D ? 70 million Web pages, using random
projections. We have discovered that in boolean data, our method exhibits (much)
smaller errors (variances); but we will present the detail in other papers (Li, Church,
and Hastie 2006, 2007).
For applications which are mostly interested in finding the strongly associated
pairs, the n2 might appear to be a show stopper. But actually, in a practical application,
we implemented an inverted index on top of the sketches, which made it possible to
find many of the most interesting associations quickly.
313
Computational Linguistics Volume 33, Number 3
2.3 Database Query Optimization
In databases, an important task is to determine the order of joins, which has a
large impact on the system performance (Garcia-Molina, Ullman, and Widom 2002,
Chapter 16). Based on the estimates of two-way, three-way, and even higher-order join
sizes, query optimizers construct a plan to minimize a cost function (e.g., intermediate
writes). Efficiency is critical as we certainly do not want to spend more time optimizing
the plan than executing it.
We use an example (called Governator) to illustrate that estimates of two-way and
multi-way association can help the query optimizer.
Table 3 shows estimates of hits for four words and their two-way, three-way, and
four-way combinations. Suppose the optimizer wants to construct a plan for the query:
?Governor, Schwarzenegger, Terminator, Austria.? The standard solution starts with the
least frequent terms: ((?Schwarzenegger? ? ?Terminator?) ? ?Governor?) ? ?Austria.?
That plan generates 579,100 intermediate writes after the first and second joins. An im-
provement would be ((?Schwarzenegger? ? ?Austria?) ? ?Terminator?) ? ?Governor,?
reducing the 579,100 down to 136,000.
3. Outline of Two-Way Association Results
To approximate the associations between words W1 and W2, we work with sketches K1
and K2. We first determine Ds = min(max(K1), max(K2)) and then construct the sample
contingency table on ?s = {1, 2, . . . , Ds}. The contingency table for the entire document
collection,? = {1, 2, . . . , D}, is estimated using a maximum likelihood estimator (MLE):
a?MLE = argmax
a
Pr (as, bs, cs, ds|Ds; a) (5)
Section 5 will show that a?MLE is the solution to a cubic equation:
f1 ? a + 1 ? bs
f1 ? a + 1
f2 ? a + 1 ? cs
f2 ? a + 1
D ? f1 ? f2 + a
D ? f1 ? f2 + a ? ds
a
a ? as = 1 (6)
Instead of solving a cubic equation, we recommend a convenient and accurate quadratic
approximation:
a?MLE,a =
f1 (2as + cs) + f2 (2as + bs) ?
?
(
f1 (2as + cs) ? f2 (2as + bs)
)2
+ 4f1 f2bscs
2 (2as + bs + cs)
(7)
We will compare the proposed MLE to two baselines: the independence baseline,
a?IND, and the margin-free baseline, a?MF:
a?IND =
f1f2
D a?MF = as
D
Ds
(8)
The margin-free baseline has smaller errors than the independence baseline, but we
can do even better if we know the margins, as is common in practice.
As expected, computational work and statistical accuracy (variance or errors) de-
pend on sampling rate. The larger the sample, the better the estimate, but the more
work we have to do.
314
Li and Church Sketch for Estimating Associations
These results are demonstrated both empirically and theoretically. In our field, it is
customary to end with a large empirical evaluation. But there are always lingering ques-
tions. Do the results generalize to other collections with more documents or different
documents? This paper attempts to put such questions to rest by deriving closed-form
expressions for the variances.
Var (a?MLE) ?
E
(
D
Ds
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
, (9)
?
max
(
f1
k1
, f2k2
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
. (10)
Var (a?MF) =
E
(
D
Ds
)
? 1
1
a + 1D?a
?
max
(
f1
k1
, f2k2
)
? 1
1
a + 1D?a
. (11)
These formulas establish the superiority of the proposed method over the alterna-
tives, not just for a particular data set, but more generally. These formulas will also be
used to determine stopping rules. How many samples do we need? We will use such
an argument to suggest that a sampling rate of 10?3 may be sufficient for certain Web
applications.
The proposed method generalizes naturally to multi-way associations, as presented
in Section 6. Section 7 describes Broder?s sketches, which were designed for estimating
resemblance, a particular association statistic. It will be shown, both theoretically and
empirically, that our proposed method reduces the mean square error (MSE) by about
50%. In other words, the proposed method achieves the same accuracy with about half
the sample size (work).
4. Evaluation of Two-Way Associations
We evaluated our two-way association sampling/estimation algorithm with a chunk
of Web crawls (D = 216) produced by the crawler for MSN.com. We collected two sets
of English words which we will refer to as the small data set and the large data set.
The small data set contains just four high frequency words: THIS, HAVE, HELP and
PROGRAM (see Table 4), whereas the large data set contains 968 words (i.e., 468,028
pairs). The large data set was constructed by taking a random sample of English words
that appeared in at least 20 documents in the collection. The histograms of the margins
and co-occurrences have long tails, as expected (see Figure 6).
For the small data set, we applied 105 independent random permutations to the
D = 216 document IDs, ? = {1, 2, . . . , D}. High-frequency words were selected so we
could study a large range of sampling rates ( kf ), from 0.002 to 0.95. A pair of sketches
was constructed for each of the 6 pairs of words in Table 4, each of the 105 permutations
and each sampling rate. The sketches were then used to compute a sample contingency
table, leading to an estimate of co-occurrence, a?. An error was computed by comparing
this estimate, a?, to the appropriate gold standard value for a in Table 4. Mean square
errors (MSE = E(a? ? a)2) and other statistics were computed by aggregating over the 105
315
Computational Linguistics Volume 33, Number 3
Table 4
Small dataset: co-occurrences and margins for the population. The task is to estimate these
values, which will be referred to as the gold standard, from a sample.
Case # Words Co-occurrence (a) Margin ( f1) Margin ( f2)
Case 2-1 THIS, HAVE 13,517 27,633 17,369
Case 2-2 THIS, HELP 7,221 27,633 10,791
Case 2-3 THIS, PROGRAM 3,682 27,633 5,327
Case 2-4 HAVE, HELP 5,781 17,369 10,791
Case 2-5 HAVE, PROGRAM 3,029 17,369 5,327
Case 2-6 HELP, PROGRAM 1,949 17,369 5,327
Monte Carlo trials. In this way, the small data set experiment made it possible to verify
our theoretical results, including the approximations in the variance formulas.
The larger experiment contains many words with a large range of frequencies;
and hence the experiment was repeated just six times (i.e., six different permutations).
With such a large range of frequencies and sampling rates, there is a danger that some
samples would be too small, especially for very rare words and very low sampling rates.
A floor was imposed to make sure that every sample contains at least 20 documents.
4.1 Results from Large Monte Carlo Experiment
Figure 7 shows that the proposed methods (solid lines) are better than the baselines
(dashed lines), in terms of MSE, estimated by the large Monte Carlo experiment over the
small data set, as described herein. Note that errors generally decrease with sampling
rate, as one would expect, at least for the methods that take advantage of the sample.
The independence baseline (a?IND), which does not take advantage of the sample, has
very large errors. The sample is a very useful source of information; even a small sample
is much better than no sample.
The recommended quadratic approximation, a?MLE,a, is remarkably close to the ex-
act MLE solution. Both of the proposed methods, a?MLE,a and a?MLE (solid lines), have
Figure 6
Large data set: histograms of document frequencies, df (left), and co-occurrences, a (right). Left:
max document frequency df = 42,564, median = 1135, mean = 2135, standard deviation = 3628.
Right: max co-occurrence a = 33,045, mean = 188, median = 74, standard deviation = 459.
316
Li and Church Sketch for Estimating Associations
much smaller MSE than the margin-free baseline a?MF (dashed lines), especially at low
sampling rates. When we know the margins, we ought to use them.
Note that MSE can be decomposed into variance and bias: MSE(a?) = E (a? ? a)2 = Var (a?)
+Bias2 (a?). If a? is unbiased, MSE(a?) = Var (a?) = SE2 (a?), where SE is called ?standard error.?
4.1.1 Margin Constraints Improve Smoothing. Though not a major emphasis of this paper,
Figure 8 shows that smoothing is effective at low sampling rates, but only for those
methods that take advantage of the margin constraints (solid lines as opposed to dashed
lines). Figure 8 compares smoothed estimates (a?MLE, a?MLE,a, and a?MF) with their un-
smoothed counterparts. The y-axis reports percentage improvement of the MSE due
to smoothing. Smoothing helps the proposed methods (solid lines) for all six word
pairs, and hurts the baseline methods (dashed lines), for most of the six word pairs. We
believe margin constraints keep the smoother from wandering too far astray; without
margin constraints, smoothing can easily do more harm than good, especially when the
smoother isn?t very good. In this experiment, we used the simple ?add-one? smoother
that replaces as, bs, cs, and ds with as + 1, bs + 1, cs + 1, and ds + 1, respectively. We could
have used a more sophisticated smoother (e.g., Good?Turing), but if we had done so,
it would have been harder to see how the margin constraints keep the smoother from
wandering too far astray.
4.1.2 Monte Carlo Verification of Variance Formula. How accurate is the ap-
proximation of the variance in Equations (9) and (11)? Figure 9 shows that the
Monte Carlo simulation is remarkably close to the theoretical formula (9). Formula
(11) is the same as (9), except that E
(
D
Ds
)
is replaced with the approximation
Figure 7
The proposed estimator, a?MLE, outperforms the margin-free baseline, a?MF, in terms of
?
MSE
a .
The quadratic approximation, a?MLE,a, is close to a?MLE. All methods are better than assuming
independence (IND).
317
Computational Linguistics Volume 33, Number 3
Figure 8
Smoothing improves the proposed MLE estimators but hurts the margin-free estimator in most
cases. The vertical axis is the percentage of relative improvement in
?
MSE of each smoothed
estimator with respect to its un-smoothed version.
Figure 9
Normalized standard error, SE(a?)a , for the MLE. The theoretical variance formula (9) fits the
simulation results so well that the curves are indistinguishable. Also, smoothing is effective in
reducing variance, especially at low sampling rates.
max
(
f1
k1
, f2k2
)
. Theoretically, we expect max
(
f1
k1
, f2k2
)
? E
(
D
Ds
)
. Figure 10 verifies the
inequality, and shows that the inequality is not too far from an equality. We will
use (11) instead of (9), because the differences are not too large, and (11) is more
convenient.
4.1.3 Monte Carlo Estimate of Bias. Finally, we also compare the biases in Figure 11 for
Case 2-5 and Case 2-6. The figure shows that the MLE estimator is essentially unbiased.
318
Li and Church Sketch for Estimating Associations
Figure 10
For all 6 cases, the ratios max
(
f1
k1
, f2k2
)
/
E
(
D
Ds
)
are close to 1, and the differences roughly
monotonically decrease with increasing sampling rates. When the sampling rates ? 0.005
(roughly the sketch sizes ? 20), max
(
f1
k1
, f2k2
)
is an accurate approximation of E
(
D
Ds
)
.
Figure 11
Biases in terms of |E(a?)?a|a . a?MLE is practically unbiased. Smoothing increases bias slightly.
4.2 Results from Large Data Set Experiment
In Figure 12, the large data set experiment confirms the findings of the large Monte
Carlo experiment: The proposed MLE method is better than the margin-free and inde-
pendence baselines. The recommended quadratic approximation, a?MLE,a, is close to the
exact solution, a?MLE.
4.3 Rank Retrieval by Cosine
We are often interested in finding top ranking pairs according to some measure of sim-
ilarity such as cosine. Performance improves with sampling rate for this task (as well
as almost any other task; there is no data like more data), but nevertheless, Figure 13
shows that we can find many of the top ranking pairs, even at low sampling rates.
Note that the estimate of cosine, a?
f1f2
, depends solely on the estimate of a, because
we know the margins, f1 and f2. If we sort word pairs by their cosines, using estimates
of a based on a small sample, the rankings will hopefully be close to what we would
319
Computational Linguistics Volume 33, Number 3
Figure 12
(a) The proposed MLE methods (solid lines) have smaller errors than the baselines (dashed
lines). We report the mean absolute errors (normalized by the mean co-occurrences, 188). All
curves are averaged over six permutations. The two solid lines, the proposed MLE and the
recommended quadratic approximation, are close to one another. Both are well below the
margin-free (MF) baseline and the independence (IND) baseline. (b) Percentage of improvement
due to smoothing. Smoothing helps MLE, but hurts MF.
Figure 13
We can find many of the most obvious associations with very little work. Two sets of cosine
scores were computed for the 468,028 pairs in the large dataset experiment. The gold standard
scores were computed over the entire dataset, whereas sample scores were computed over a
sample of the data set. The plots show the percentage of agreement between these two lists, as a
function of S. As expected, agreement rates are high (? 100%) at high sampling rates (0.5). But it
is reassuring that agreement rates remain pretty high (? 70%) even when we crank the sampling
rate way down (0.003).
obtain if we used the entire data set. This section will compare the rankings based on a
small sample to a gold standard, the rankings based on the entire data set.
How should we evaluate rankings? We follow the suggestion in Ravichandran,
Pantel, and Hovy (2005) of reporting the percentage of agreements in the top-S.
That is, we compare the top-S pairs based on a sample with the top-S pairs based
on the entire data set. We report the intersection of the two lists, normalized by S.
Figure 13(a) emphasizes high precision region (3 ? S ? 200), whereas Figure 13(b)
emphasizes higher recall, extending S to cover all 468,028 pairs in the large dataset
experiment. Of course, agreement rates are high at high sampling rates. For example, we
have nearly ? 100% agreement at a sampling rate of 0.5. It is reassuring that agreement
rates remain fairly high (? 70%), even when we push the sampling rate way down
320
Li and Church Sketch for Estimating Associations
(0.003). In other words, we can find many of the most obvious associations with very
little work.
The same comparisons can be evaluated in terms of precision and recall, by fix-
ing the top-LG gold standard list but varying the length of the sample list LS. More
precisely, recall = relevant/LG, and precision = relevant/LS, where ?relevant? means
the retrieved pairs in the gold standard list. Figure 14 gives a graphical representation
of this evaluation scheme, using notation in Manning and Schutze (1999), Chapter 8.1.
Figure 15 presents the precision?recall curves for LG = 1%L and 10%L, where L =
468, 028. For each LG, there is one precision?recall curve corresponding to each sampling
rate. All curves indicate the precision?recall trade-off and that the only way to improve
both precision and recall simultaneously is to increase the sampling rate.
4.4 Summary
To summarize the main results of the large and small data set experiments, we found
that the proposed MLE (and the recommended quadratic approximation) have smaller
Figure 14
Definitions of recall and precision. L = total number of pairs. LG = number of pairs from the top
of the gold standard similarity list. LS = number of pairs from the top of the reconstructed
similarity list.
Figure 15
Precision?recall curves in retrieving the top 1% and top 10% gold standard pairs, at different
sampling rates from 0.003 to 0.5. Note that the precision is always larger than LGL .
321
Computational Linguistics Volume 33, Number 3
errors than the two baselines (the MF baseline and the independence (IND) base-
line). Margin constraints improve smoothing, because the margin constraints keep the
smoother from wandering too far astray. Monte Carlo simulations verified the variance
formulas (9) and (11), and showed that the proposed MLE method is essentially un-
biased. The ranking experiment showed that we can find many of the most obvious
associations with very little work.
5. The Maximum Likelihood Estimator (MLE)
Section 4 evaluated the proposed method empirically; this section will explore the sta-
tistical theory behind the method. The task is to estimate the contingency table (a, b, c, d)
from the sample contingency table (as, bs, cs, ds), the margins, and D.
We can factor the (full) likelihood (probability mass function, PMF) Pr(as, bs, cs, ds; a)
into
Pr(as, bs, cs, ds; a) = Pr(as, bs, cs, ds|Ds; a) ? Pr(Ds; a) (12)
We seek the a that maximizes the partial likelihood Pr(as, bs, cs, ds|Ds; a), that is,
a?MLE = argmax
a
Pr (as, bs, cs, ds|Ds; a) = argmax
a
log Pr (as, bs, cs, ds|Ds; a) (13)
Pr(as, bs, cs, ds|Ds; a) is just the PMF of a two-way sample contingency table. That is
relatively straightforward, but Pr(Ds; a) is difficult. As illustrated in Figure 16, there is no
strong dependency of Ds on a, and therefore, we can focus on the easy part.
Before we delve into maximizing Pr(as, bs, cs, ds|Ds; a) under margin constraints, we
will first consider two simplifications, which lead to two baseline estimators. The inde-
pendence baseline does not use any samples, whereas the margin-free baseline does not
take advantage of the margins.
Figure 16
This experiment shows that E(Ds) is not sensitive to a. D = 2 ? 107, f1 = D/20, f2 = f1/2.
The different curves correspond to a = 0, 0.05, 0.2, 0.5, and 0.9 f2. These curves are almost
indistinguishable except at very low sampling rates. Note that, at sampling rate = 10?5,
the sample size k2 = 5 only.
322
Li and Church Sketch for Estimating Associations
5.1 The Independence Baseline
Independence assumptions are often made in databases (Garcia-Molina, Ullman, and
Widom 2002, Chapter 16.4) and NLP (Manning and Schutze 1999, Chapter 13.3). When
two words W1 and W2 are independent, the size of intersections, a, follows a hypergeo-
metric distribution,
Pr(a) =
(
f1
a
)(
D ? f1
f2 ? a
)/(
D
f2
)
, (14)
where
(
n
m
)
= n!m!(n?m)! . This distribution suggests an estimator
a?IND = E(a) =
f1 f2
D . (15)
Note that (14) is also a common null-hypothesis distribution in testing the indepen-
dence of a two-way contingency table, that is, the so-called Fisher?s exact test (Agresti
2002, Section 3.5.1).
5.2 The Margin-Free Baseline
Conditional on Ds, the sample contingency table (as, bs, cs, ds) follows the multivariate
hypergeometric distribution with moments4
E(as|Ds) =
Ds
D a, E(bs|Ds) =
Ds
D b, E(cs|Ds) =
Ds
D c, E(ds|Ds) =
Ds
D d,
Var(as|Ds) = Ds aD
(
1 ? aD
) D ? Ds
D ? 1 (16)
where the term D?DsD?1 ? 1 ?
Ds
D , is known as the ?finite population correction factor.?
An unbiased estimator and its variance would be
a?MF = DDs
as, Var(a?MF|Ds) = D
2
D2s
Var(as|Ds) = DDs
1
1
a + 1D?a
D ? Ds
D ? 1 . (17)
We refer to this estimator as ?margin-free? because it does not take advantage of the
margins.
The multivariate hypergeometric distribution can be simplified to a multinomial
assuming ?sample-with-replacement,? which is often a good approximation when DsD
is small. According to the multinomial model, an estimator and its variance would be:
a?MF,r = DDs
as, Var(a?MF,r|Ds) = DDs
1
1
a + 1D?a
(18)
That is, for the margin-free model, the ?sample-with-replacement? simplification still
results in the same estimator but slightly overestimates the variance.
4 http://www.ds.unifi.it/VL/VL EN/urn/urn4.html.
323
Computational Linguistics Volume 33, Number 3
Note that these expectations in (16) hold both when the margins are known, as well
as when they are not known, because the samples (as, bs, cs, ds) are obtained randomly
without consulting the margins. Of course, when we know the margins, we can do
better than when we don?t.
5.3 The Exact MLE with Margin Constraints
Considering the margin constraints, the partial likelihood Pr (as, bs, cs, ds|Ds; a) can be
expressed as a function of a single unknown parameter, a:
Pr (as, bs, cs, ds|Ds; a) =
(
a
as
)( b
bs
)(
c
cs
)( d
ds
)
( a+b+c+d
as+bs+cs+ds
)
=
(
a
as
)(f1?a
bs
)(f2?a
cs
)(D?f1?f2+a
ds
)
(
D
Ds
)
? a!
(a ? as)!
? ( f1 ? a)!
( f1 ? a ? bs)!
? ( f2 ? a)!
( f2 ? a ? cs)!
? (D ? f1 ? f2 + a)!
(D ? f1 ? f2 + a ? ds)!
(19)
=
as?1
?
i=0
(a ? i) ?
bs?1
?
i=0
( f1 ? a ? i) ?
cs?1
?
i=0
( f2 ? a ? i) ?
ds?1
?
i=0
(D ? f1 ? f2 + a ? i)
where the multiplicative terms not mentioning a are discarded, because they do not
contribute to the MLE.
Let a?MLE be the value of a that maximizes the partial likelihood (19), or equivalently,
maximizes the log likelihood, log Pr (as, bs, cs, ds|Ds; a):
as?1
?
i=0
log(a ? i) +
bs?1
?
i=0
log
(
f1 ? a ? i
)
+
cs?1
?
i=0
log
(
f2 ? a ? i
)
+
ds?1
?
i=0
log
(
D ? f1 ? f2 + a ? i
)
whose first derivative, ? log Pr(as,bs,cs,ds|Ds;a)?a , is
as?1
?
i=0
1
a ? i ?
bs?1
?
i=0
1
f1 ? a ? i
?
cs?1
?
i=0
1
f2 ? a ? i
+
ds?1
?
i=0
1
D ? f1 ? f2 + a ? i
(20)
Because the second derivative, ?
2 log Pr(as,bs,cs,ds|Ds;a)
?a2 ,
?
as?1
?
i=0
1
(a ? i)2 ?
bs?1
?
i=0
1
( f1 ? a ? i)2
?
cs?1
?
i=0
1
( f2 ? a ? i)2
?
ds?1
?
i=0
1
(D ? f1 ? f2 + a ? i)2
is negative, the log likelihood function is concave, and therefore, there is a unique
maximum. One could solve (20) for ? log Pr(as,bs,cs,ds|Ds;a)?a = 0 numerically, but it turns out
there is a more direct solution using the updating formula from (19):
Pr (as, bs, cs, ds|Ds; a) = Pr (as, bs, cs, ds|Ds; a ? 1) ? g(a)
324
Li and Church Sketch for Estimating Associations
Because we know that the MLE exists and is unique, it suffices to find the a such that
g(a) = 1,
g(a) = aa ? as
f1 ? a + 1 ? bs
f1 ? a + 1
f2 ? a + 1 ? cs
f2 ? a + 1
D ? f1 ? f2 + a
D ? f1 ? f2 + a ? ds
= 1 (21)
which is cubic in a (because the fourth term vanishes).
We recommend a straightforward numerical procedure for solving g(a) = 1. Note
that g(a) = 1 is equivalent to q(a) = log g(a) = 0. The first derivative of q(a) is
q?(a) =
(
1
f1 ? a + 1
? 1
f1 ? a + 1 ? bs
)
+
(
1
f2 ? a + 1
? 1
f2 ? a + 1 ? cs
)
(22)
+
(
1
D ? f1 ? f2 + a
? 1
D ? f1 ? f2 + a ? ds
)
+
(
1
a ?
1
a ? as
)
We can solve for q(a) = 0 iteratively using Newton?s method: a(new) = a(old) ? q(a
(old) )
q?(a(old) ) . See
Appendix 1 for a C code implementation.
5.4 The ?Sample-with-Replacement? Simplification
Under the ?sample-with-replacement? assumption, the likelihood function is slightly
simpler:
Pr(as, bs, cs, ds|Ds; a, r) =
(
Ds
as, bs, cs, ds
)
(
a
D
)as ( b
D
)bs ( c
D
)cs ( d
D
)ds
? aas ( f1 ? a)bs ( f2 ? a)cs (D ? f1 ? f2 + a)ds (23)
Setting the first derivative of the log likelihood to be zero yields a cubic equation:
as
a ?
bs
f1 ? a
? cs
f2 ? a
+
ds
D ? f1 ? f2 + a
= 0 (24)
As shown in Section 5.2, using the margin-free model, the ?sample-with-
replacement? assumption amplifies the variance but does not change the estimation.
With our proposed MLE, the ?sample-with-replacement? assumption will change the
estimation, although in general we do not expect the differences to be large. Figure 17
gives an (exaggerated) example, to show the concavity of the log likelihood and the
difference caused by assuming ?sample-with-replacement.?
5.5 A Convenient Practical Quadratic Approximation
Solving a cubic equation for the exact MLE may be so inconvenient that one may prefer
the less accurate margin-free baseline because of its simplicity. This section derives a
convenient closed-form quadratic approximation to the exact MLE.
The idea is to assume ?sample-with-replacement? and that one can identify as from
K1 without knowledge of K2. In other words, we assume a
(1)
s ? Binomial
(
as + bs, af1
)
,
325
Computational Linguistics Volume 33, Number 3
Figure 17
An example: as = 20, bs = 40, cs = 40, ds = 800, f1 = f2 = 100, D = 1000. The estimated a? = 43 for
?sample-with-replacement,? and a? = 51 for ?sample-without-replacement.? (a) The likelihood
profile, normalized to have a maximum = 1. (b) The log likelihood profile, normalized to have a
maximum = 0.
a(2)s ? Binomial
(
as + cs, af2
)
, and a(1)s and a
(2)
s are independent with a
(1)
s = a
(2)
s = as.
The PMF of
(
a(1)s , a
(2)
s
)
is a product of two binomials:
[
(
f1
as + bs
)(
a
f1
)as ( f1 ? a
f1
)bs
]
?
[(
f2
as + cs
)(
a
f2
)as ( f2 ? a
f2
)cs]
? a2as
(
f1 ? a
)bs ( f2 ? a
)cs (25)
Setting the first derivative of the logarithm of (25) to be zero, we obtain
2as
a ?
bs
f1 ? a
? cs
f2 ? a
= 0 (26)
which is quadratic in a and has a convenient closed-form solution:
a?MLE,a =
f1 (2as + cs) + f2 (2as + bs) ?
?
( f1 (2as + cs) ? f2 (2as + bs))2 + 4f1 f2bscs
2 (2as + bs + cs)
(27)
The second root can be ignored because it is always out of range:
f1 (2as + cs) + f2 (2as + bs) +
?
( f1 (2as + cs) ? f2 (2as + bs))2 + 4f1 f2bscs
2 (2as + bs + cs)
? f1 (2as + cs) + f2 (2as + bs) + | f1 (2as + cs) ? f2 (2as + bs) |
2 (2as + bs + cs)
?
{
f1 if f1 (2as + cs) ? f2 (2as + bs)
f2 if f1 (2as + cs) < f2 (2as + bs)
? min( f1, f2)
The evaluation in Section 4 showed that a?MLE,a is close to a?MLE.
326
Li and Church Sketch for Estimating Associations
5.6 The Conditional Variance and Bias
Usually, a maximum likelihood estimator is nearly unbiased. Furthermore, assuming
?sample-with-replacement,? we can apply the large sample theory5 (Lehmann and
Casella 1998, Theorem 6.3.10), which says that a?MLE is asymptotically unbiased and
converges in distribution to a Normal with mean a and variance 1I(a) , where I(a), the
expected Fisher Information, is
I(a) = ?E
(
?2
?a2
log Pr (as, bs, cs, ds|Ds; a, r)
)
= E
(
as
a2
+
bs
( f1 ? a)2
+
cs
( f2 ? a)2
+
ds
(D ? f1 ? f2 + a)2
?
?
?
?
Ds
)
=
E(as|Ds)
a2
+
E(bs|Ds)
(
f1 ? a
)2 +
E(cs|Ds)
(
f2 ? a
)2 +
E(ds|Ds)
(
D ? f1 ? f2 + a
)2
=
Ds
D
(
1
a +
1
f1 ? a
+ 1
f2 ? a
+ 1
D ? f1 ? f2 + a
)
(28)
where we evaluate E(as|Ds), E(bs|Ds), E(cs|Ds), E(ds|Ds) by (16).
For ?sampling-without-replacement,? we correct the asymptotic variance 1I(a) by
multiplying by the finite population correction factor 1 ? DsD :
Var (a?MLE|Ds) ? 1I(a)
(
1 ? DsD
)
=
D
Ds
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
(29)
Comparing (17) with (29), we know that Var (a?MLE|Ds) < Var (a?MF|Ds), and the dif-
ference could be substantial. In other words, when we know the margins, we ought to
use them.
5.7 The Unconditional Variance and Bias
Errors are a combination of variance and bias. Fortunately, we don?t need to be con-
cerned about bias, at least asymptotically:
E (a?MLE ? a) = E (E (a?MLE ? a|Ds)) ? E(0) = 0 (30)
The unconditional variance can be computed using the conditional variance
formula:
Var (a?MLE) = E (Var (a?MLE |Ds )) + Var (E (a?MLE |Ds ))
?
E
(
D
Ds
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
(31)
5 See Rosen (1972a, 1972b) for the rigorous regularity conditions that ensure convergence in the case of
?sample-without-replacement.?
327
Computational Linguistics Volume 33, Number 3
because E (a?MLE|Ds) ? a, which is a constant. Hence Var (E (a?MLE|Ds)) ? 0.
To evaluate E
(
D
Ds
)
exactly, we need PMF Pr(Ds; a), which is unavailable. Even if it
were available, E
(
D
Ds
)
probably wouldn?t have a convenient closed-form.
Here we recommend the approximations, (3) and (4), mentioned previously. To de-
rive these approximations, recall that Ds = min (max(K1), max(K2)). Using the discrete
order statistics distribution (David 1981, Exercise 2.1.4),6 we obtain:
E (max(K1)) =
k1(D + 1)
f1 + 1
? k1
f1
D, E (max(K2)) ?
k2
f2
D (32)
The min function can be considered to be concave. By Jensen?s inequality (see Cover
and Thomas 1991, Theorem 2.6.2), we know that
E
(
Ds
D
)
= E
(
min
(
max(K1k1)
D ,
max(K2)
D
))
? min
(
E(max(K1)
D ,
E(max(K2)
D
)
= min
(
k1
f1
, k2
f2
)
(33)
The reciprocal function is convex. Again by Jensen?s inequality, we have
E
(
D
Ds
)
= E
(
1
Ds/D
)
? 1
E
(
Ds
D
) ? max
(
f1
k1
,
f2
k2
)
(34)
By replacing the inequalities with equalities, we obtain (35) and (36):
E
(
Ds
D
)
? min
(
k1
f1
, k2
f2
)
(35)
E
(
D
Ds
)
? max
(
f1
k1
,
f2
k2
)
(36)
In our experiments, when the sample size is reasonably large (Ds ? 20), the errors
in (35) and (36) are usually within 5%.
Approximations (35) and (36) provide an intuitive relationship between two views
of the sampling rate: (a) DsD , which depends on corpus size and (b)
k
f , which depends on
the size of the postings. The difference between these two views is important when the
term-by-document matrix is sparse, which is often the case in practice.
Using (36), we obtain the following approximation for the unconditional variance:
Var (a?MLE) ?
max
(
f1
k1
, f2k2
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
(37)
6 Also, see http://www.ds.unifi.it/VL/VL EN/urn/urn5.html.
328
Li and Church Sketch for Estimating Associations
5.8 The Variance of h(a?MLE )
We can estimate any function h(a) by h(a?MLE). In practical applications, h could be any
measure of association including cosine, resemblance, mutual information, etc. When
h(a) is a nonlinear function of a, h(a?MLE) will be biased. One can remove the bias to
some extent using Taylor expansions. See some examples in Li and Church (2005).
Bias correction is important for small samples and highly nonlinear h?s (e.g., the log
likelihood ratio, LLR).
The bias of h(a?MLE) decreases with sample size. Precisely, the delta method (Agresti
2002, Chapter 3.1.5) says that h(a?MLE) is asymptotically unbiased and the variance of
h(a?MLE) is
Var(h(a?MLE)) ? Var(a?MLE)(h?(a))2 (38)
provided h?(a) exists and is non-zero. Non-asymptotically, it is easy to show that
Var(h(a?MLE)) ? Var(a?MLE)(h?(a))2 if h(a) is convex (39)
Var(h(a?MLE)) ? Var(a?MLE)(h?(a))2 if h(a) is concave (40)
5.9 How Many Samples Are Sufficient?
The answer depends on the trade-off between computational costs (time and space)
and estimation errors. For very infrequent words, we might afford to sample 100%. In
general, a reasonable criterion is the coefficient of variation, cv = SE(a?)a , SE =
?
Var(a?).
We consider the estimate is accurate if the cv is below some threshold ?0 (e.g., ?0 = 0.1).
The cv can be expressed as
cv =
SE(a?)
a ?
1
a
?
?
?
?
max
(
f1
k1
, f2k2
)
? 1
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
(41)
Figure 18(a) plots the required sampling rate min
(
k1
f1
, k2f2
)
computed from (41). The
figure shows that at Web scale (i.e., D ? 10 billion), a sampling rate as low as 10?3 may
suffice for ?ordinary? words (i.e., f1 ? 107 = 0.001D). Figure 18(b) plots the required
sample size k1, for the same experiment in Figure 18(a), where for simplicity, we assume
k1
f1
= k2f2 . The figure shows that, after D is large enough, the required sample size does
not increase as much.
To apply (41) to the real data, Table 5 presents the critical sampling rates and sample
sizes for all pair-wise combinations of the four-word query Governor, Schwarzenegger,
Terminator, Austria. Here we assume the estimates in Table 3 are exact. The table verifies
that only a very small sample may suffice to achieve a reasonable cv.
5.10 Tail Bound and Multiple Comparisons Effect
To choose the sample size, it is often necessary to consider the effect of multiple compar-
isons. For example, when we estimate all pair-wise associations among n data points,
329
Computational Linguistics Volume 33, Number 3
Figure 18
(a) An analysis based on cv = SEa = 0.1 suggests that we can get away with very low sampling
rates. The three curves plot the critical value for the sampling rate, min
(
k1
f1
, k2f2
)
, as a function of
corpus size, D. At Web scale, D ? 1010, sampling rates above 10?2 to 10?4 satisfy cv ? 0.1, at
least for these settings of f1, f2, and a. The settings were chosen to simulate ?ordinary? words.
The three curves correspond to three choices of f1: D/100, D/1000, and D/10, 000. f2 = f1/10,
a = f2/20. (b) The critical sample size k1 (assuming
k1
f1
= k2f2 ), corresponding to the sampling rates
in (a).
Table 5
The critical sampling rates and sample sizes (for cv = 0.1) are computed for all two-way
combinations among the four words Governor, Schwarzenegger, Terminator, Austria, assuming the
estimated document frequencies and two-way associations in Table 3 are exact. The required
sampling rates are all very small, verifying our claim that for ?ordinary? words, a sampling rate
as low as 10?3 may suffice. In these computations, we used D = 5 ? 109 for the number of
English documents in the collection.
Query Critical Sampling Rate
Governor, Schwarzenegger 5.6 ? 10?5
Governor, Terminator 7.2 ? 10?4
Governor, Austria 1.4 ? 10?4
Schwarzenegger, Terminator 1.5 ? 10?4
Schwarzenegger, Austria 8.1 ? 10?4
Terminator, Austria 5.5 ? 10?4
we are estimating n(n?1)2 pairs simultaneously. A convenient approach is to bound the
tail probability
Pr (|a?MLE ? a| > a) ? ?/p (42)
where ? (e.g., 0.05) is the level of significance,  is the specified accuracy (e.g.,  < 0.5),
and p is the correction factor for multiple comparisons. The most conservative choice is
p = n22 , known as the Bonferroni Correction. But often it is reasonable to let p be much
smaller (e.g., p = 100).
We can gain some insight from (42). In particular, our previous argument based on
coefficient of variations (cv) is closely related to (42).
330
Li and Church Sketch for Estimating Associations
Assuming a?MLE ? N (a, Var (a?MLE)), then, based on the known normal tail bound,
Pr (|a?MLE ? a| > a) ? 2 exp
(
? 
2a2
2Var (a?MLE)
)
= 2 exp
(
? 
2
2cv2
)
(43)
combined with (42), leads to the following criterion on cv
cv ? 
?
? 1
2 log
(
?/2p
) (44)
For example, if we let ? = 0.05, p = 100, and  = 0.4, then (44) will output cv ? 0.1.
5.11 Sample Size Selection Based on Storage Constraints
Suppose we can compute the maximum allowed total samples, T, for example, based
on the available memory. That is,
?n
i=1 ki = T, where n is the total number of words. We
could allocate T according to document frequencies fj, that is,
kj =
fj
?n
i=1 fi
T (45)
Usually, we will need to define a lower bound kl and an upper bound ku, which have
to be selected from engineering experience, depending on the specific applications. We
will truncate the computed kj if it is outside [kl, ku]. Equation (45) implies a uniform
corpus sampling rate, which may not be always desirable, but the confinement by
[kl, ku] can effectively vary the sampling rates.
More carefully, we can minimize the total number of ?unused? samples. For a pair,
Wi and Wj, if
ki
fi
? kjfj , then on average, there are
(
ki
fi
? kjfj
)
fi samples unused in Ki. This
is the basic idea behind the following linear program for choosing the ?optimal? sample
sizes:
Minimize
n
?
i=1
n
?
j=i+1
[
fi
(
ki
fi
?
kj
fj
)
+
+ fj
(
kj
fj
? ki
fi
)
+
]
subject to
n
?
i=1
ki = T, ki ? fi, kl ? ki ? ku (46)
where (z)+ = max(0, z), is the positive part of z. This program can be modified (possibly
no longer a linear program) to consider other factors in different applications. For
example, some applications may care more about the very rare words, so we would
weight the rare words more.
5.12 When Will Sketches Not Perform Well?
We consider three scenarios. (A) f1 and f2 are both large; (B) f1 and f2 are both small; (C)
f1 is very large but f2 is very small. Conventional sampling over documents can handle
situation (A), but will perform poorly on (B) because there is a good chance that the
sample will miss the rare words. The sketch algorithm can handle both (A) and (B) well.
331
Computational Linguistics Volume 33, Number 3
In fact, it will do very well when both words are rare because the equivalent sampling
rate DsD ? min
(
k1
f1
, k2f2
)
can be high, even 100%.
When f2 
 f1, no sampling method can work well unless we are willing to sample
P1 with a sufficiently large sample. Otherwise even if we let
k2
f2
= 100%, the corpus
sampling rate, DsD ?
k1
f1
, will be low. For example, Google estimates 14,000,000 hits
for Holmes, 37,500 hits for Diaconis, and 892 joint hits. Assuming D = 5 ? 109 and
cv = 0.1, the critical sample size for Holmes would have to be 1.4 ? 106, probably too
large as a sample.7
6. Extension to Multi-Way Associations
Many applications involve multi-way associations, for example, association rules, data-
bases, and Web search. The ?Governator? example in Table 3, for example, made use
of both two-way and three-way associations. Fortunately, our sketch construction and
estimation algorithm can be naturally extended to multi-way associations. We have
already presented an example of estimating multi-way associations in Section 1.6. When
we do not consider the margins, the estimation task is as simple as in the pair-wise case.
When we do take advantage of margins, estimating multi-way associations amounts to
a convex program. We will also analyze the theoretical variances.
6.1 Multi-Way Sketches
Suppose we are interested in the associations among m words, denoted by W1,
W2, . . . , Wm. The document frequencies are f1, f2, . . . , and fm, which are also the lengths
of the postings P1, P2, . . . , Pm. There are N = 2m combinations of associations, denoted
by x1, x2, . . . , xN. For example,
a = x1 = |P1 ? P2 ? . . . ? Pm?1 ? Pm|
x2 = |P1 ? P2 ? . . . ? Pm?1 ? ?Pm|
x3 = |P1 ? P2 ? . . . ? ?Pm?1 ? Pm|
. . .
xN?1 = |?P1 ? ?P2 ? . . . ? ?Pm?1 ? Pm|
xN = |?P1 ? ?P2 ? . . . ? ?Pm?1 ? ?Pm| (47)
which can be directly corresponded to the binary representation of integers.
Using the vector and matrix notation, X = [x1, x2, . . . , xN]T, F = [ f1, f2, . . . , fm, D]T,
where the superscript ?T? stands for ?transpose?, that is, we always work with col-
umn vectors. We can write down the margin constraints in terms of a linear matrix
equation as
AX = F (48)
7 Readers familiar with random projections can verify that in this case we need k = 6.6 ? 107 projections in
order to achieve cv = 0.1. See Li, Hastie, and Church (2006a, 2006b) for the variance formula of random
projections.
332
Li and Church Sketch for Estimating Associations
where A is the constraint matrix. If necessary, we can use A(m) to identify A for different
m values. For example, when m = 2 or m = 3,
A(2) =
?
?
1 1 0 0
1 0 1 0
1 1 1 1
?
? A(3) =
?
?
?
?
1 1 1 1 0 0 0 0
1 1 0 0 1 1 0 0
1 0 1 0 1 0 1 0
1 1 1 1 1 1 1 1
?
?
?
?
(49)
For each word Wi, we sample the ki smallest elements from its permuted postings,
?(Pi), to form a sketch, Ki. Recall ? is a random permutation on ? = {1, 2, . . . , D}. We
compute
Ds = min{max(K1), max(K2), . . . , max(Km)}. (50)
After removing the elements in all m Ki?s that are larger than Ds, we intersect these
m trimmed sketches to generate the sample table counts. The samples are denoted as
S = [s1, s2, . . . , sN]T.
Conditional on Ds, the samples S are statistically equivalent to Ds random samples
over documents from the corpus. The corresponding conditional PMF and log PMF
would be
Pr(S|Ds; X) =
(
x1
s1
)(
x2
s2
)
. . .
(
xN
sN
)
(
D
Ds
) ?
N
?
i=1
si?1
?
j=0
(xi ? j) (51)
log Pr(S|Ds; X) ? Q =
N
?
i=1
si?1
?
j=0
log(xi ? j) (52)
The log PMF is concave, as in two-way associations. A partial likelihood MLE solu-
tion, namely, the X? that maximizes log Pr(S|Ds; X?), will again be adopted, which leads
to a convex optimization problem. But first, we shall discuss two baseline estimators.
6.2 Baseline Independence Estimator
Assuming independence, an estimator of x1 would be
x?1,IND = D
m
?
i=1
fi
D (53)
which can be easily proved using a conditional expectation argument.
By the property of the hypergeometric distribution, E(|Pi ? Pj|) =
fi fj
D . Therefore,
E(x1) = E(|P1 ? P2 ? . . . ? Pm|) = E(| ?mi=1 Pi|)
= E(E(|P1 ? (?mi=2Pi)||(?mi=2Pi))) =
f1
DE(| ?
m
i=2 Pi|)
=
f1 f2 . . . fm?2
Dm?2
E(|Pm?1 ? Pm|) = D
m
?
i=1
fi
D (54)
333
Computational Linguistics Volume 33, Number 3
6.3 Baseline Margin-Free Estimator
The conditional PMF Pr(S|Ds; X) is a multivariate hypergeometric distribution, based
on which we can derive the margin-free estimator:
E(si|Ds) =
Ds
D xi, x?i,MF =
D
Ds
si, Var(x?i,MF|Ds) = DDs
1
1
xi +
1
D?xi
D ? Ds
D ? 1 (55)
We can see that the margin-free estimator remains its simplicity in the multi-way case.
6.4 The MLE
The exact MLE can be formulated as a standard convex optimization problem,
minimize ? Q = ?
N
?
i=1
si?1
?
j=0
log(xi ? j)
subject to AX = F, and X  S (56)
where X  S is a compact representation for xi ? si, 1 ? i ? N.
This optimization problem can be solved by a variety of standard methods such
as Newton?s method (Boyd and Vandenberghe 2004, Chapter 10.2). Note that we can
ignore the implicit inequality constraints, X  S, if we start with a feasible initial guess.
It turns out that the formulation in (56) will encounter numerical difficulty due
to the inner summation in the objective function Q. Smoothing will bring in more
numerical issues. Recall that in estimating two-way associations we do not have this
problem, because we have eliminated the summation in the objective function, using an
(integer) updating formula. In multi-way associations, it seems not easy to reformulate
the objective function Q in a similar form.
To avoid the numerical problems, a simple solution is to assume ?sample-with-
replacement,? under which the conditional likelihood and log likelihood become
Pr(S|Ds; X, r) ?
N
?
i=1
(xi
D
)si ?
N
?
i=1
xsii (57)
log Pr(S|Ds; X, r) ? Qr =
N
?
i=1
si log xi (58)
Our MLE problem can then be reformulated as
minimize ? Q = ?
N
?
i=1
si log xi
subject to AX = F, and X  S (59)
which is again a convex program. To simplify the notation, we neglect the subscript ?r.?
334
Li and Church Sketch for Estimating Associations
We can compute the gradient (Q) and Hessian (2Q). The gradient is a vector of
the first derivatives of Q with respect to xi, for 1 ? i ? N,
Q =
[
?Q
?xi
, 1 ? i ? N
]
=
[ s1
x1 ,
s2
x2 , . . . ,
sN
xN
]T
(60)
The Hessian is a matrix whose (i, j)th entry is the partial derivative ?
2Q
?xixj
, that is,
2Q = ?diag
[
s1
x21
, s2
x22
, . . . , sN
x2N
]
(61)
The Hessian has a very simple diagonal form, implying that Newton?s method will
be a good algorithm for solving this optimization problem. We implement, in Appen-
dix 2, the equality constrained Newton?s method with feasible start and backtracking
line search (Boyd and Vandenberghe 2004, Algorithm 10.1). A key step is to solve for
Newton?s step, Xnt:
[
?2 Q AT
A 0
] [
Xnt
dummy
]
=
[
Q
0
]
. (62)
Because the Hessian 2Q is a diagonal matrix, solving for Newton?s step in (62) can
be sped up substantially (e.g., using the block matrix inverse formula).
6.5 The Covariance Matrix
We apply the large sample theory to estimate the covariance matrix of the MLE. Recall
that we have N = 2m variables and m + 1 constraints. The effective number of variables
would be 2m ? (m + 1), which is also the dimension of the covariance matrix.
We seek a partition of A = [A1, A2], such that A2 is invertible. We may have to
switch some columns of A in order to find an invertible A2. In our construction, the
jth column of A2 is the column of A such that last entry of the jth row of A is 1. An
example for m = 3 would be
A(3)1 =
?
?
?
?
1 1 1 0
1 1 0 1
1 0 1 1
1 1 1 1
?
?
?
?
A(3)2 =
?
?
?
?
1 0 0 0
0 1 0 0
0 0 1 0
1 1 1 1
?
?
?
?
(63)
where A(3)1 is the [1 2 3 5] columns of A
(3) and A(3)2 is the [4 6 7 8] columns of A
(3). We can
see that A2 constructed this way is always invertible because its determinant is always
one.
Corresponding to the partition of A, we partition X = [X1, X2]T. For example, when
m = 3, X1 = [x1, x2, x3, x5]T, X2 = [x4, x6, x7, x8]T. We can then express X2 to be
X2 = A
?1
2 (F ? A1X1) = A
?1
2 F ? A
?1
2 A1X1 (64)
335
Computational Linguistics Volume 33, Number 3
The log likelihood function Q, which is separable, can then be expressed as
Q(X) = Q1(X1) + Q2(X2) (65)
By the matrix derivative chain rule, the Hessian of Q with respect to X1 would be
21Q = 21Q1 +21Q2 = 21Q1 +
(
A?12 A1
)T
22 Q2
(
A?12 A1
)
(66)
where we use 21 and 22 to indicate the Hessians are with respect to X1 and X2,
respectively.
Conditional on Ds, the Expected Fisher Information of X1 is
I(X1) = E
(
?21 Q|Ds
)
= ?E(21Q1|Ds) ?
(
A?12 A1
)T
E(22Q2|Ds)
(
A?12 A1
)
(67)
where
E(?21 Q1|Ds) = diag
[
E
(
si
x2i
)
, xi ? X1
]
=
Ds
D diag
[
1
xi , xi ? X1
]
(68)
E(?22 Q2|Ds) =
Ds
D diag
[
1
xi , xi ? X2
]
(69)
By the large sample theory, and also considering the finite population correction
factor, we can approximate the (conditional) covariance matrix of X1 to be
Cov(X1|Ds) ? I(X1)?1
(
1 ? DsD
)
=
(
D
Ds
? 1
)(
diag
[
1
xi , xi ? X1
]
+
(
A?12 A1
)T
diag
[
1
xi , xi ? X2
] (
A?12 A1
)
)?1
(70)
For a sanity check, we verify that this approach recovers the same variance formula
in the two-way association case. Recall that, when m = 2, we have
2Q = ?
?
?
?
?
?
?
?
s1
x21
0 0 0
0
s2
x22
0 0
0 0
s3
x23
0
0 0 0
s4
x24
?
?
?
?
?
?
?
, 21Q1 = ?
s1
x21
, 22Q2 = ?
?
?
?
?
s2
x22
0 0
0
s3
x23
0
0 0
s4
x24
?
?
?
?
(71)
A(2) =
?
?
1 1 0 0
1 0 1 0
1 1 1 1
?
? , A(2)1 =
?
?
1
1
1
?
? , A(2)2 =
?
?
1 0 0
0 1 0
1 1 1
?
? (72)
336
Li and Church Sketch for Estimating Associations
(
A?12 A1
)T
22 Q2A?12 A1 = ?
[
1 1 ?1
]
?
?
?
?
s2
x22
0 0
0
s3
x23
0
0 0
s4
x24
?
?
?
?
?
?
1
1
?1
?
? = ? s2
x22
? s3
x23
? s4
x24
(73)
Hence,
?21 Q =
s1
x21
+
s2
x22
+
s3
x23
+
s4
x24
=
as
a2
+
bs
( f1 ? a)2
+
cs
( f2 ? a)2
+
ds
(D ? f1 ? f2 + a)2
(74)
which leads to the same Fisher Information for the two-way association as we have
derived.
6.6 The Unconditional Covariance Matrix
Similar to two-way associations, the unconditional variance of the proposed MLE can
be estimated by replacing DDs in (70) with E
(
D
Ds
)
, namely,
Cov(X1) ?
(
E
(
D
Ds
)
? 1
)
?
(
diag
[
1
xi , xi ? X1
]
+
(
A?12 A1
)T
diag
[
1
xi , xi ? X2
] (
A?12 A1
)
)?1
(75)
Similar to two-way associations, we recommend the following approximations:
E
(
Ds
D
)
? min
(
k1
f1
, k2
f2
, . . . , km
fm
)
(76)
E
(
D
Ds
)
? max
(
f1
k1
,
f2
k2
, . . . ,
fm
km
)
(77)
Again, the approximation (76) will overestimate E
(
Ds
D
)
and (77) will underestimate
E
(
D
Ds
)
hence also underestimating the unconditional variance.
6.7 Empirical Evaluation
We use the same four words as in Table 4 to evaluate the multi-way association al-
gorithm, as merely a sanity check. There are four different combinations of three-way
associations and one four-way association, as listed in Table 6.
We present results for x1 (i.e., a in two-way associations) for all cases. The evalua-
tions for four three-way cases are presented in Figures 19, 20 and 21. From these figures,
we see that the proposed MLE has lower MSE than the MF. As in the two-way case,
smoothing helps MLE but still hurts MF in most cases. Also, the experiments verify that
our approximate variance formulas are fairly accurate.
Figure 22 presents the evaluation results for the four-way association case, includ-
ing MSE, smoothing, and variance. The results are similar to the three-way case.
337
Computational Linguistics Volume 33, Number 3
Table 6
The same four words as in Table 4 are used for evaluating multi-way associations. There are in
total four three-way combinations and one four-way combination.
Case No. Words Co-occurrences
Case 3-1 THIS, HAVE, HELP 4940
Three-way Case 3-2 THIS, HAVE, PROGRAM 2575
Case 3-3 THIS, HELP, PROGRAM 1626
Case 3-4 HAVE, HELP, PROGRAM 1460
Four-way Case 4 THIS, HAVE, HELP, PROGRAM 1316
We have used the empirical E
(
D
Ds
)
to compute the unconditional variance. Fig-
ure 23 plots max
(
f1
k1
, f2k2 , . . . ,
fm
km
)
/ DDs for all cases. The figure indicates that using
max
(
f1
k1
, f2k2 , . . . ,
fm
km
)
to estimate E
(
D
Ds
)
is still fairly accurate when the sample size is
reasonable.
Combining the results of two-way associations for the same four words, we can
study the trend how the proposed MLE improve the MF baseline. Figure 24(a) sug-
Figure 19
In terms of
?
MSE(x1 )
x1 , the proposed MLE is consistently better than the MF, which is better than
the IND, for four three-way association cases.
338
Li and Church Sketch for Estimating Associations
Figure 20
The simple ?add-one? smoothing improves the estimation accuracies for the proposed MLE.
Smoothing, however, in all cases except Case 3-1 hurts the margin-free estimator.
gests that the proposed MLE is a big improvement over the MF baseline for two-
way associations, but the improvement becomes less and less noticeable with higher
order associations. This observation is not surprising, because the number of degrees
of freedom, 2m ? (m + 1), increases exponentially with m. In order words, the margin
constraints are most effective for small m, but the effectiveness decreases rapidly with m.
On the other hand, smoothing becomes more and more important as m increases,
as shown in Figure 24(b), partly because of the data sparsity in high order associations.
7. Related Work: Comparison with Broder?s Sketches
Broder?s sketches (Broder 1997), originally introduced for removing duplicates in the
AltaVista index, have been applied to a variety of applications (Broder et al 1997;
Haveliwala, Gionis, and Indyk 2000; Haveliwala et al 2002). Broder et al (1998, 2000)
presented some theoretical aspects of the sketch algorithm. There has been considerable
exciting work following up on this line of research including Indyk (2001), Charikar
(2002), and Itoh, Takei, and Tarui (2003).
Broder and his colleagues introduced two algorithms, which we will refer to as
the ?original sketch? and the ?minwise sketch? for estimating resemblance, R = |P1?P2||P1?P2| .
The original sketch uses a single random permutation on ? = {1, 2, 3, . . . , D}, and the
minwise sketch uses k random permutations. Both algorithms have similar estimation
accuracies, as will see.
339
Computational Linguistics Volume 33, Number 3
Figure 21
In terms of SE(x1 )x1 , the theoretical variance of MLE fits the empirical values very well. At low
sampling rates, smoothing effectively reduces the variance. Note that we plug in the empirical
E
(
D
Ds
)
into (75) to estimate the unconditional variance. The errors due to this approximation are
presented in Figure 23.
Figure 22
Four-way associations (Case 4). (a) The proposed MLE has smaller MSE than the margin-free
(MF) baseline, which has smaller MSE than the independence baseline. (b) Smoothing
considerably improves the accuracy for MLE and also slightly improves MF. (c) For the
proposed MLE, the theoretical prediction fits the empirical variance very well. Smoothing
considerably reduces variance.
Our proposed sketch algorithm is closer to Broder?s original sketch, with a few
important differences. A key difference is that Broder?s original sketch throws out half
of the sample, whereas we throw out less. In addition, the sketch sizes are fixed over all
words for Broder, whereas we allow different sizes for different words. Broder?s method
was designed for a single statistic (resemblance), whereas we generalize the method to
340
Li and Church Sketch for Estimating Associations
Figure 23
The ratios max
(
f1
k1
, f2k2 , . . . ,
fm
km
)
/ DDs are plotted for all cases. At sampling rates > 0.01, the ratios
are > 0.9 ? 0.95, indicating good accuracy.
Figure 24
(a) Combining the three-way, four-way, and two-way association results for the four words in
the evaluations, the average relative improvements of
?
MSE suggests that the proposed MLE is
consistently better than the MF baseline but the improvement decreases monotonically as the
order of associations increases. (b) Average
?
MSE improvements due to smoothing imply that
smoothing becomes more and more important as the order of association increases.
compute contingency tables (and summaries thereof). Broder?s method was designed
for pairwise associations, whereas our method generalizes to multi-way associations.
Finally, Broder?s method was designed for boolean data, whereas our method general-
izes to reals.
7.1 Broder?s Minwise Sketch
Suppose a random permutation ?1 is performed on the document IDs. We denote the
smallest IDs in the postings P1 and P2, by min(?1(P1)) and min(?1(P2)), respectively.
Obviously,
Pr (min(?1(P1)) = min(?1(P2))) =
|P1 ? P2|
|P1 ? P2|
= R (78)
341
Computational Linguistics Volume 33, Number 3
After k minwise independent permutations, denoted as ?1, ?2, . . . , ?k, we can
estimate R without bias, as a binomial probability, namely,
R?B,r = 1k
k
?
i=1
{min(?i(P1)) = min(?i(P2))} and Var
(
R?B,r
)
= 1
k
R(1 ? R) (79)
7.2 Broder?s Original Sketch
A single random permutation ? is applied to the document IDs. Two sketches are con-
structed: K1 = MINk1 (?(P1)), K2 = MINk2 (?(P2)).
8 Broder (1997) proposed an unbiased
estimator for the resemblance:
R?B =
|MINk(K1 ? K2) ? K1 ? K2|
|MINk(K1 ? K2)|
(80)
Note that intersecting by MINk(K1 ? K2) throws out half the samples, which can be
undesirable (and unnecessary).
The following explanation for (80) is slightly different from Broder (1997). We
can divide the set P1 ? P2 (of size a + b + c = f1 + f2 ? a) into two disjoint sets: P1 ? P2
and P1 ? P2 ? P1 ? P2. Within the set MINk(K1 ? K2) (of size k), the document IDs that
belong to P1 ? P2 would be MINk(K1 ? K2) ? K1 ? K2, whose size is denoted by aBs . This
way, we have a hypergeometric sample, that is, we sample k document IDs from P1 ? P2
randomly without replacement and obtain aBs IDs that belong to P1 ? P2. By the property
of the hypergeometric distribution, the expectation of aBs would be
E
(
aBs
)
= ak
f1 + f2 ? a
=? E
(
aBs
k
)
= a
f1 + f2 ? a
=
|P1 ? P2|
|P1 ? P2|
=? E(R?B) = R (81)
The variance of R?B, according to the hypergeometric distribution, is:
Var
(
R?B
)
= 1
k
R(1 ? R) f1 + f2 ? a ? k
f1 + f2 ? a ? 1
(82)
where the term f1+ f2?a?kf1+ f2?a?1 is the ?finite population correction factor.?
The minwise sketch can be considered as a ?sample-with-replacement? variate of
the original sketch. The analysis of minwise sketch is slightly simpler mathematically
whereas the original sketch is more efficient. The original sketch requires only one
random permutation and has slightly smaller variance than the minwise sketch, that
is, Var
(
R?B,r
)
? Var
(
R?B
)
. When k is reasonably small, as is common in practice, two
sketch algorithms have similar errors.
7.3 Why Our Algorithm Improves Broders?s Sketch
Our proposed sketch algorithm starts with Broder?s original (one permutation) sketch;
but our estimation method differs in two important aspects.
8 Actually, the method required fixing sketch sizes: k1 = k2 = k, a restriction that we find convenient to relax.
342
Li and Church Sketch for Estimating Associations
Firstly, Broder?s estimator (80) uses k out of 2 ? k samples. In particular, it uses only
aBs = |MINk(K1 ? K2) ? K1 ? K2| intersections, which is always smaller than as = |K1 ?
K2| available in the samples. In contrast, our algorithm takes advantage of all useful
samples up to Ds = min(max(K1), max(K2)), particularly all as intersections. If
k1
f1
= k2f2 ,
that is, if we sample proportionally to the margins:
k1 = 2k
f1
f1 + f2
k2 = 2k
f2
f1 + f2
(83)
it is expected that almost all samples will be utilized.
Secondly, Broder?s estimator (80) considers a two-cell hypergeometric model (a, b +
c) whereas the two-way association is a four-cell model (a, b, c, d), which is used in our
proposed estimator. Simpler data models often result in simpler estimation methods but
with larger errors.
Therefore, it is obvious that our proposed method has smaller estimator errors.
Next, we compare our estimator with Broder?s sketches in terms of the theoretical
variances.
7.4 Comparison of Variances
Broder?s method was designed to estimate resemblance. Thus, this section will compare
the proposed method with Broder?s sketches in terms of resemblance, R.
We can compute R from our estimated association a?MLE:
R?MLE =
a?MLE
f1 + f2 ? a?MLE
(84)
R?MLE is slightly biased. However, because the second derivative R??(a)
R??(a) =
2( f1 + f2)
( f1 + f2 ? a)3
? 2( f1 + f2)
max( f1, f2)3
? 4
max( f1, f2)2
(85)
is small (i.e., the nonlinearity is weak), it is unlikely that the bias will be noticeable in
practice.
By the delta method as described in Section 5.8, the variance of R?MLE is
approximately
Var
(
R?MLE
)
? Var(a?MLE)(R?(a))2 =
max
(
f1
k1
, f2k2
)
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
( f1 + f2)2
( f1 + f2 ? a)4
(86)
conservatively ignoring the ?finite population correction factor,? for convenience.
Define the ratio of the variances to be VB =
Var(R?MLE)
Var(R?B)
, then
VB =
Var
(
R?MLE
)
Var
(
R?B
) =
max
(
f1
k1
, f2k2
)
1
a + 1f1?a +
1
f2?a +
1
D?f1?f2+a
( f1 + f2)2
( f1 + f2 ? a)2
k
a( f1 + f2 ? 2a)
(87)
343
Computational Linguistics Volume 33, Number 3
To help our intuitions, let us consider some reasonable simplifications to VB. As-
suming a << min( f1, f2) < max( f1, f2) << D, then approximately
VB ?
k max( f1k1 ,
f2
k2
)
f1 + f2
=
?
?
?
?
?
max( f1, f2 )
f1+ f2
if k1 = k2 = k
1
2 if k1 = 2k
f1
f1+ f2
, k2 = 2k
f2
f1+ f2
(88)
which indicates that the proposed method is a considerable improvement over Broder?s
sketches. In order to achieve the same accuracy, our method requires only half as many
samples.
Figure 25 plots the VB in (87) for the whole range of f1, f2, and a, assuming equal
samples: k1 = k2 = k. We can see that VB ? 1 always holds and VB = 1 only when f1 =
f2 = a. There is also the possibility that VB is close to zero.
Proportional samples further reduce VB, as shown in Figure 26.
Figure 25
We plot VB in (87) for the whole range of f1, f2, and a, assuming equal samples: k1 = k2 = k. (a),
(b), (c), and (d) correspond to f2 = 0.2f1, f2 = 0.5f1, f2 = 0.8f1, and f2 = f1, respectively. Different
curves are for different f1?s, ranging from 0.05D to 0.95D spaced at 0.05D. The horizontal lines
are max( f1,f2 )f1+f2 . We can see that for all cases, VB ? 1 holds. VB = 1 when f1 = f2 = a, a trivial case.
When a/f2 is small, VB ? max( f1,f2 )f1+f2 holds well. It is also possible that VB is very close to zero.
344
Li and Church Sketch for Estimating Associations
Figure 26
Compared with equal samples in Figure 25, proportional samples further reduce VB.
We can show algebraically that VB in (87) is always less than unity unless f1 = f2 = a.
For convenience, we use the notion a, b, c, d in (87). Assuming k1 = k2 = k and f1 > f2,
we obtain
VB =
a + b
1
a + 1b +
1
c + 1d
(2a + b + c)2
(a + b + c)2
1
a(b + c)
(89)
To show VB ? 1, it suffices to show
(a + b)(2a + b + c)2bcd ? (bcd + acd + abd + abc)(a + b + c)2(b + c) (90)
which is equivalent to following true statement:
(a3(b ? c)2 + bc2(b + c)2 + a2(2b + c)(b2 ? bc + 2c2) + a(b + c)(b3 + 4bc2 + c2))d
+ abc(b + c)(a + b + c)2 ? 0 (91)
7.5 Empirical Evaluations
We have theoretically shown that our proposed method is a considerable improvement
over Broder?s sketch. Next, we would like to evaluate these theoretical results using the
same experiment data as in evaluating two-way associations (i.e., Table 4).
Figure 27 compares the MSE. Here we assume equal samples and later we will
show that proportional samples could further improve the results. The figure shows
that our MLE estimator is consistently better than Broder?s sketch. In addition, the
approximate MLE a?MLE,a still gives very close answers to the exact MLE, and the
simple ?add-one? smoothing improves the estimations at low sampling rates, quite
substantially.
Figure 28 illustrates the bias. As expected, estimating resemblance from a?MLE intro-
duces a small bias. This bias will be ignored since it is small compared to the MSE.
Figure 29 verifies that the variance of our estimator is always smaller than Broder?s
sketch. Our theoretical variance in (86) underestimates the true variances because the
approximation E
(
D
Ds
)
= max
(
f1
k1
, f2k2
)
underestimates the variance. In addition, because
345
Computational Linguistics Volume 33, Number 3
Figure 27
When estimating the resemblance, our algorithm gives consistently more accurate answers
than Broder?s sketch. In our experiments, Broder?s ?minwise? construction gives almost the
same answers as the ?original? sketch, thus only the ?minwise? results are presented here.
The approximate MLE again gives very close answers to the exact MLE. Also, smoothing
improves at low sampling rates.
the resemblance R(a) is a convex function of a, the delta method also underestimates
the variance. However, Figure 29 shows that the errors are not very large, and become
negligible with reasonably large sample sizes (e.g., 50). This evidence suggests that the
variance formula (86) is reliable.
Figure 28
Our proposed MLE has higher bias than the ?minwise? estimator because of the non-linearity of
resemblance. However, the bias is very small compared with the MSE.
346
Li and Church Sketch for Estimating Associations
Figure 29
Our proposed estimator has consistently smaller variances than Broder?s sketch. The theoretical
variance, computed by (86), slightly underestimates the true variance with small samples. Here
we did not plot the theoretical variance for Broder?s sketch because it is very close to the
empirical curve.
Finally, in Figure 30, we show that with proportional samples, our algorithm further
improves the estimates in terms of MSE. With equal samples, our estimators improve
Broder?s sketch by 30?50%. With proportional samples, improvements become 40?80%.
Note that the maximum possible improvement is 100%.
8. Conclusion
In databases, data mining, and information retrieval, there has been considerable in-
terest in sampling and sketching techniques (Chaudhuri, Motwani, and Narasayya
1998; Indyk and Motwani 1998; Manku, Rajagopalan, and Lindsay 1999; Charikar
2002; Achlioptas 2003; Gilbert et al 2003; Li, Hastie, and Church 2007; Li 2006), which
are useful for numerous applications such as association rules (Brin et al 1997; Brin,
Motwani, and Silverstein 1997), clustering (Guha, Rastogi, and Shim 1998; Broder 1998;
Aggarwal et al 1999; Haveliwala, Gionis, and Indyk 2000; Haveliwala et al 2002), query
optimization (Matias, Vitter, and Wang 1998; Chaudhuri, Motwani, and Narasayya
1999), duplicate detection (Broder 1997; Brin, Davis, and Garcia-Molina 1995), and
more. Sampling methods become more and more important with larger and larger
collections.
The proposed method generates random sample contingency tables directly from
the sketch, the front of the inverted index. Because the term-by-document matrix is
extremely sparse, it is possible for a relatively small sketch, k, to characterize a large
sample of Ds documents. The front of the inverted index not only tells us about the
presence of the word in the first k documents, but it also tells us about the absence
of the word in the remaining Ds ? k documents. This observation becomes increas-
ingly important with larger Web collections (with ever increasing sparsity). Typically,
Ds  k.
347
Computational Linguistics Volume 33, Number 3
Figure 30
Compared with Broder?s sketch, the relative MSE improvement should be, approximately,
min( f1, f2 )
f1+ f2
with equal samples, and 12 with proportional samples. The two horizontal lines in each
figure correspond to these two approximates. The actual improvements could be lower or
higher. The figure verifies that proportional samples can considerably improve the accuracies.
To estimate the contingency table for the entire population, one can use the ?margin-
free? baseline, which simply multiplies the sample contingency table by the appropriate
scaling factor. However, we recommend taking advantage of the margins (also known
as document frequencies). The maximum likelihood solution under margin constraints
is a cubic equation, which has a remarkably accurate quadratic approximation. The pro-
posed MLE methods were compared empirically and theoretically to the MF baseline,
finding large improvements. When we know the margins, we ought to use them.
Our proposed method differs from Broder?s sketches in important aspects. (1) Our
sketch construction allows more flexibility in that the sketch size can be different from
one word to the next. (2) Our estimation is more accurate. The estimator in Broder?s
sketches uses one half of the samples whereas our method always uses more. More
samples lead to smaller errors. (3) Broder?s method considers a two-cell model whereas
our method works with a more refined (hence more accurate) four-cell contingency
table model. (4) Our method extends naturally to estimating multi-way associations. (5)
Although this paper only considers boolean (0/1) data, our method extends naturally
to general real-valued data; see Li, Church, and Hastie (2006, 2007).
Although we have used ?word associations? for explaining the algorithm, the
method is a general sampling technique, with potential applications in Web search,
databases, association rules, recommendation systems, nearest neighbors, and machine
learning such as clustering.
Acknowledgments
The authors thank Trevor Hastie, Chris
Meek, David Heckerman, Mark Manasse,
David Siegmund, Art Owen, Robert
Tibshirani, Bradley Efron, Andrew Ng, and Tze
Leung Lai. Much of the work was conducted
at Microsoft while the first author was an
intern during the summers of 2004 and 2005.
348
Li and Church Sketch for Estimating Associations
Appendix 1: Sample C Code for Estimating Two-Way Associations
#include <stdio.h>
#include <math.h>
#define MAX(x,y) ( (x) > (y) ? (x) : (y) )
#define MIN(x,y) ( (x) < (y) ? (x) : (y) )
#define EPS 1e-10
#define MAX_ITER 50
int est_a_appr(int as,int bs,int cs, int f1, int f2);
int est_a_mle(int as,int bs, int cs, int ds, int f1, int f2,int D);
int main(void)
{
int f1 = 10000, f2 = 5000, D = 65536; // test data
int as = 25, bs = 45, cs = 150, ds = 540;
int a_appr = est_a_appr(as,bs,cs,f1,f2);
int a_mle = est_a_mle(as,bs,cs,ds,f1,f2,D);
printf("Estimate a_appr = %d\n",a_appr); // output 1138
printf("Estimate a_mle = %d\n",a_mle); // output 821
return 0;
}
// The approximate MLE is the solution to a quadratic equation
int est_a_appr(int as,int bs,int cs, int f1, int f2)
{
int sx = 2*as + bs, sy = 2*as + cs, sz = 2*as+bs+cs;
double tmp = (double)f1*sy + (double)f2*sx;
return (int)((tmp-sqrt(tmp*tmp-8.0*f1*f2*as*sz))/sz/2.0);
}
// Newton?s method to solve for the exact MLE
int est_a_mle(int as,int bs, int cs, int ds, int f1, int f2,int D)
{
int a_min = MAX(as,ds+f1+f2-D), a_max = MIN(f1-bs,f2-cs);
int a1 = est_a_appr(as,bs,cs,f1,f2); // A good start
a1 = MAX( a_min, MIN(a1, a_max) ); // Sanity check
int k = 0, a = a1;
do {
a = a1;
double q = log(a+EPS) - log(a-as+EPS)
+log(f1-a-bs+1+EPS) - log(f1-a+1+EPS)
+log(f2-a-cs+1+EPS) - log(f2-a+1+EPS)
+log(D-f1-f2+a+EPS) - log(D-f1-f2-ds+a+EPS);
double dq = 1.0/(a+EPS)-1.0/(a-as+EPS)
-1.0/(f1-a-bs+1+EPS) + 1.0/(f1-a+1+EPS)
-1.0/(f2-a-cs+1+EPS) + 1.0/(f2-a+1+EPS)
-1.0/(D-f1-f2-ds+a+EPS) + 1.0/(D-f1-f2+a+EPS);
a1 = (int)(a - q/dq); a1 = MAX(a_min, MIN(a1,a_max));
if( ++k > MAX_ITER ) break;
}while( a1 != a );
return a;
}
Appendix 2: Sample Matlab Code for Estimating Multi-Way Associations
function test_program
% A short program for testing the multi-way association algorithm.
% First generate a random gold standard dataset. Then construct
% sketches by sampling a certain portion of the postings. Associations
% are estimated by the exact MLE as well as the margin-free (MF) method.
%
clear all;
m = max(2,ceil(rand*6)); % Number of words (random)
D = 1000*m; % Total number of documents
f = ceil(rand(m,1)*D/2); % document frequencies (random)
349
Computational Linguistics Volume 33, Number 3
P{1} = sort(randsample(D,f(1))); % Posting of the first word (random)
Pc = setdiff(1:D, P{1})?; % Compliment of the posting
% The postings of words 2 to m are randomly generated. 30% are
% sampled from the postings of word 1.
for i = 2:m
k = ceil(0.3*min(f(i),f(1)));
P{i} = sort([randsample(P{1},k);randsample(Pc,f(i)-k)]); % Postings
end
X = compute_intersection(P,D); % Gold standard associations
pc = 1; % Pseudo-count(pc), pc=0 for no smoothing, pc=1 for "add-one".
sampling_rate = 0.1;
for i = 1:m
k = ceil(sampling_rate*f(i));
K{i} = P{i}(1:k); % Sketches
end
% Estimate the associations and covariance matrices
[X_MLE, X_MF, Var_c, Var_o] = multi_way_est(K,f,D,pc);
% Display the estimations of associations
[X X_MLE X_MF] % [Gold standard, MLE, MF]
__________________________________________________
function [X_MLE, X_MF, Var_c, Var_o] = multi_way_est(K,f,D,pc);
% Matlab code for estimating multi-way associations
% K: Sketches (Cell array data type)
% f: Document frequencies, a column vector
% D: Total number of documents
% pc: Pseudo-count for smoothing.
% X_MLE: Maximum likelihood estimator (MLE), a column vector
% X_MF : Margin-free (MF) estimator, a column vector
% Var_c: Conditional (on Ds) covariance matrix, using the estimated X,
% Var_o: Covariance computed using the observed Fisher information
%
pc = max(pc,1e-4); % Always use a small pc for numerical stability.
m = length(K); % The order of associations, i.e., number of words.
[A,A1,A2,A3,ind1,ind2] = gen_A(m); % Margin constraint matrix
for i = 1:m;
last_elem(i) = K{i}(end);
end
Ds = min(last_elem);
for i = 1:m
K{i} = K{i}(find(K{i}<=Ds)); % Trim sketches according to D_s
end
S = compute_intersection(K,Ds); % Intersect the sketches to get samples
[X_MLE, X_MF] = newton_est(pc,S,Ds,D,A,f); % Estimate X
% Conditional variance
Z_c = 1./(X_MLE+eps); Z1_c = diag(Z_c(ind1)); Z2_c = diag(Z_c(ind2));
Var_c = inv(Z1_c + A3?*Z2_c*A3)*(D/Ds-1);
% Observed variance
Z_o = S./(X_MLE+eps).^2; Z1_o = diag(Z_o(ind1)); Z2_o = diag(Z_o(ind2));
Var_o = inv(Z1_o + A3?*Z2_o*A3)*(D-Ds)/D;
_________________________________________________________
function [X_MLE,X_MF] = newton_est(pc,S,Ds,D,A,f)
% Estimate multi-way associations by solving a convex
% optimization problem using the Newton?s method.
%
NEWTON_ERR = 0.001; % Threshold for termination.
MAX_ITER = 50; % Maximum allowed iteration.
N = length(S); m = length(f); F = [f;D];
pc = min(pc,(D-Ds)/N); % Adjust pc, if Ds is close to D.
% Solve a quadratic programming problem to find an initial
350
Li and Church Sketch for Estimating Associations
% guess of the MLE that minimizes the 2-norm with respect to
% the MF estimation and satisfies the constraints.
while(1)
X_MF = (S+pc)./(Ds+N*pc)*D; % Margin-free estimations.
[X0,dummy,flag] = quadprog(2*eye(2^m),-2*X_MF,[],[],A,F,S+pc);
if(flag == 1) break; end
pc = pc/2; % Occasionally need reduce pc for a feasible solution.
end
S = S + pc; X_MLE = X0; iter = 0;
while(1);
D1 = -S./(X_MLE+eps); % Gradient (first derivatives)
D2 = diag(S./(X_MLE.^2+eps)); % Hessian (second derivatives)
% Solve a linear system of equations for the Newton?s step.
M = [D2 A?; A zeros(size(A,1),size(A,1))];
dx = M\[-D1; zeros(size(A,1),1)]; dx = dx(1:size(D2,1));
lambda = (dx?*D2*dx)^0.5; % Measure of errors
iter = iter + 1;
if(iter>MAX_ITER | lambda^2/2<NEWTON_ERR) break; end
% Backtracking line search for a good Newton step size.
z = 1; Alpha = 0.1; Beta = 0.5; iter2 = 0;
while(min(X_MLE+z*dx-S)<0|S?*log(X_MLE./(X_MLE+z*dx))>=Alpha*z*D1?*dx);
if(iter2 >= MAX_ITER) break; end
z = Beta*z; iter2 = iter2 + 1;
end
X_MLE = X_MLE + z*dx;
end
_________________________________________________________
function S = compute_intersection(K,Ds);
% Compute the intersections to generate a table with N = 2^m
% cells. The cells are ordered in terms of the binary representation
% of integers from 0 to 2^m-1, where m is the number of words.
%
m = length(K); bin_rep = char(dec2bin(0:2^m-1)); S = zeros(2^m,1);
for i = 0:2^m-1;
if(bin_rep(i+1,1) == ?0?)
c{i+1} = K{1};
else
c{i+1} = setdiff([1:Ds]?,K{1});
end
for j = 2:m
if(bin_rep(i+1,j) == ?0?)
c{i+1} = intersect(c{i+1},K{j});
else
c{i+1} = setdiff(c{i+1},K{j});
end
end
S(i+1) = length(c{i+1});
end
_________________________________________________________
function [A,A1,A2,A3,ind1,ind2] = gen_A(m)
% Generate the margin constraint matrix and compute its decompositions
% for analyzing the covariance matrix
%
t1 = num2str(dec2bin(0:2^m-1)); t2 = zeros(2^m,m*2-1);
t2(:,1:2:end) = t1; t2(:,2:2:end) = ?,?;
A = xor(str2num(char(t2))?,1); A = [A;ones(1,2^m)];
for i = 1:size(A,1);
[last_one(i)] = max(find(A(i,:)==1));
end
ind1 = setdiff((1:size(A,2)),last_one); ind2 = last_one;
A1 = A(:,ind1); A2 = A(:,ind2); A3 = inv(A2)*A1;
351
Computational Linguistics Volume 33, Number 3
References
Achlioptas, Dimitris. 2003. Database-friendly
random projections: Johnson-Lindenstrauss
with binary coins. Journal of Computer and
System Sciences, 66(4):671?687.
Aggarwal, Charu C., Cecilia Magdalena
Procopiuc, Joel L. Wolf, Philip S. Yu, and
Jong Soo Park. 1999. Fast algorithms for
projected clustering. In SIGMOD,
pages 61?72, Philadelphia, PA.
Aggarwal, Charu C. and Joel L. Wolf. 1999.
A new method for similarity indexing
of market basket data. In SIGMOD,
pages 407?418, Philadelphia, PA.
Agrawal, Rakesh, Tomasz Imielinski, and
Arun Swami. 1993. Mining association
rules between sets of items in large
databases. In SIGMOD, pages 207?216,
Washington, DC.
Agrawal, Rakesh, Heikki Mannila,
Ramakrishnan Srikant, Hannu Toivonen,
and A. Inkeri Verkamo. 1996. Fast
discovery of association rules. In U. M.
Fayyad, G. Pratetsky-Shapiro, P. Smyth,
and R. Uthurusamy, editors. Advances in
Knowledge Discovery and Data Mining.
AAAI/MIT Press, pages 307?328,
Cambridge, MA.
Agrawal, Rakesh and Ramakrishnan Srikant.
1994. Fast algorithms for mining
association rules in large databases.
In VLDB, pages 487?499, Santiago
de Chile, Chile.
Agresti, Alan. 2002. Categorical Data Analysis.
John Wiley & Sons, Inc., Hoboken, NJ,
second edition.
Alon, Noga, Yossi Matias, and Mario
Szegedy. 1996. The space complexity of
approximating the frequency moments.
In STOC, pages 20?29, Philadelphia, PA.
Baeza-Yates, Ricardo and Berthier
Ribeiro-Neto. 1999. Modern Information
Retrieval. ACM Press, New York, NY.
Boyd, Stephen and Lieven Vandenberghe.
2004. Convex Optimization. Cambridge
University Press, Cambridge, UK.
Brin, Sergey, James Davis, and Hector
Garcia-Molina. 1995. Copy detection
mechanisms for digital documents. In
SIGMOD, pages 398?409, San Jose, CA.
Brin, Sergey and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web
search engine. In WWW, pages 107?117,
Brisbane, Australia.
Brin, Sergy, Rajeev Motwani, and Craig
Silverstein. 1997. Beyond market baskets:
Generalizing association rules to
correlations. In SIGMOD, pages 265?276,
Tucson, AZ.
Brin, Sergy, Rajeev Motwani, Jeffrey D.
Ullman, and Shalom Tsur. 1997. Dynamic
itemset counting and implication rules
for market basket data. In SIGMOD,
pages 265?276, Tucson, AZ.
Broder, Andrei Z. 1997. On the resemblance
and containment of documents. In The
Compression and Complexity of Sequences,
pages 21?29, Positano, Italy.
Broder, Andrei Z. 1998. Filtering
near-duplicate documents. In FUN, Isola
d?Elba, Italy.
Broder, Andrei Z., Moses Charikar, Alan M.
Frieze, and Michael Mitzenmacher. 1998.
Min-wise independent permutations
(extended abstract). In STOC,
pages 327?336, Dallas, TX.
Broder, Andrei Z., Moses Charikar, Alan M.
Frieze, and Michael Mitzenmacher. 2000.
Min-wise independent permutations.
Journal of Computer Systems and Sciences,
60(3):630?659.
Broder, Andrei Z., Steven C. Glassman,
Mark S. Manasse, and Geoffrey Zweig.
1997. Syntactic clustering of the
web. In WWW, pages 1157?1166,
Santa Clara, CA.
Charikar, Moses S. 2002. Similarity
estimation techniques from rounding
algorithms. In STOC, pages 380?388,
Montreal, Canada.
Chaudhuri Surajit, Rajeev Motwani, and
Vivek R. Narasayya. 1998. Random
sampling for histogram construction:
How much is enough? In SIGMOD,
pages 436?447, Seattle, WA.
Chaudhuri, Surajit, Rajeev Motwani, and
Vivek R. Narasayya. 1999. On random
sampling over joins. In SIGMOD,
pages 263?274, Philadelphia, PA.
Chen, Bin, Peter Haas, and Peter
Scheuermann. 2002. New two-phase
sampling based algorithm for discovering
association rules. In KDD, pages 462?468,
Edmonton, Canada.
Church, Kenneth and Patrick Hanks. 1991.
Word association norms, mutual
information and lexicography.
Computational Linguistics, 16(1):22?29.
Cover, Thomas M. and Joy A. Thomas. 1991.
Elements of Information Theory. John Wiley
& Sons, Inc., New York, NY.
David, Herbert A. 1981. Order Statistics.
John Wiley & Sons, Inc., New York, NY,
second edition.
Deming, W. Edwards and Frederick F.
Stephan. 1940. On a least squares
adjustment of a sampled frequency table
when the expected marginal totals are
352
Li and Church Sketch for Estimating Associations
known. The Annals of Mathematical
Statistics, 11(4):427?444.
Drineas, Petros and Michael W. Mahoney.
2005. Approximating a gram matrix for
improved kernel-based learning. In COLT,
pages 323?337, Bertinoro, Italy.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Etzioni, Oren, Michael Cafarella, Doug
Downey, Stanley Kok, Ana-Maria Popescu,
Tal Shaked, Stephen Soderland, Daniel S.
Weld, and Alexander Yates. 2004.
Web-scale information extraction in
knowitall (preliminary results).
In WWW, pages 100?110, New York, NY.
Garcia-Molina, Hector, Jeffrey D. Ullman,
and Jennifer Widom. 2002. Database
Systems: The Complete Book. Prentice Hall,
New York, NY.
Gilbert, Anna C., Yannis Kotidis,
S. Muthukrishnan, and Martin J. Strauss.
2003. One-pass wavelet decompositions
of data streams. IEEE Transactions on
Knowledge and Data Engineering,
15(3):541?554.
Guha Sudipto, Rajeev Rastogi, and Kyuseok
Shim. 1998. Cure: An efficient clustering
algorithm for large databases. In SIGMOD,
pages 73?84, Seattle, WA.
Hastie, T., R. Tibshirani, and J. Friedman.
2001. The Elements of Statistical Learning:
Data Mining, Inference, and Prediction.
Springer, New York, NY.
Haveliwala, Taher H., Aristides Gionis, and
Piotr Indyk. 2000. Scalable techniques
for clustering the Web. In WebDB,
pages 129?134, Dallas, TX.
Haveliwala, Taher H., Aristides Gionis,
Dan Klein, and Piotr Indyk. 2002.
Evaluating strategies for similarity search
on the web. In WWW, pages 432?442,
Honolulu, HI.
Hidber, Christian. 1999. Online association
rule mining. In SIGMOD, pages 145?156,
Philadelphia, PA.
Hornby, Albert Sydney, editor. 1989. Oxford
Advanced Learner?s Dictionary of Current
English. Oxford University Press, Oxford,
UK, fourth edition.
Indyk, Piotr. 2001. A small approximately
min-wise independent family of hash
functions. Journal of Algorithm, 38(1):84?90.
Indyk, Piotr and Rajeev Motwani. 1998.
Approximate nearest neighbors: Towards
removing the curse of dimensionality.
In STOC, pages 604?613, Dallas, TX.
Itoh, Toshiya, Yoshinori Takei, and Jun Tarui.
2003. On the sample size of k-restricted
min-wise independent permutations
and other k-wise distributions. In STOC,
pages 710?718, San Diego, CA.
Lehmann, Erich L. and George Casella. 1998.
Theory of Point Estimation. Springer,
New York, NY, second edition.
Li, Ping. 2006. Very sparse stable random
projections, estimators and tail bounds
for stable random projections.
Technical report, available from
http://arxiv.org/PS cache/cs/pdf/
0611/0611114v2.pdf.
Li, Ping and Kenneth W. Church. 2005.
Using sketches to estimate two-way and
multi-way associations. Technical Report
TR-2005-115, Microsoft Research,
Redmond, WA, September.
Li, Ping, Kenneth W. Church, and Trevor J.
Hastie. 2006. Conditional random
sampling: A sketched-based sampling
technique for sparse data. Technical Report
2006-08, Department of Statistics, Stanford
University.
Li, Ping, Kenneth W. Church, and Trevor J.
Hastie. 2007. Conditional random
sampling: A sketch-based sampling
technique for sparse data. In NIPS,
pages 873?880. Vancouver, BC, Canada.
Li, Ping, Trevor J. Hastie, and Kenneth W.
Church. 2006a. Improving random
projections using marginal information.
In COLT, pages 635?649, Pittsburgh, PA.
Li, Ping, Trevor J. Hastie, and Kenneth W.
Church. 2006b. Very sparse random
projections. In KDD, pages 287?296,
Philadelphia, PA.
Li, Ping, Trevor J. Hastie, and Kenneth W.
Church. 2007. Nonlinear estimators
and tail bounds for dimensional
reduction in l1 using Cauchy random
projections. In COLT, pages 514?529,
San Diego, CA.
Manku, Gurmeet Singh, Sridhar
Rajagopalan, and Bruce G. Lindsay.
1999. Random sampling techniques
for space efficient online computation
of order statistics of large datasets.
In SIGCOMM, pages 251?262,
Philadelphia, PA.
Manning, Chris D. and Hinrich Schutze.
1999. Foundations of Statistical Natural
Language Processing. The MIT Press,
Cambridge, MA.
Matias, Yossi, Jeffrey Scott Vitter, and Min
Wang. 1998. Wavelet-based histograms
for selectivity estimation. In SIGMOD,
pages 448?459, Seattle, WA.
Moore, Robert C. 2004. On log-likelihood-
ratios and the significance of rare events.
353
Computational Linguistics Volume 33, Number 3
In EMNLP, pages 333?340, Barcelona,
Spain.
Pearsall, Judy, editor. 1998. The New Oxford
Dictionary of English. Oxford University
Press, Oxford, UK.
Ravichandran, Deepak, Patrick Pantel,
and Eduard Hovy. 2005. Randomized
algorithms and NLP: Using locality
sensitive hash function for high speed
noun clustering. In ACL, pages 622?629,
Ann Arbor, MI.
Rosen, Bengt. 1972a. Asymptotic theory
for successive sampling with varying
probabilities without replacement, I.
The Annals of Mathematical Statistics,
43(2):373?397.
Rosen, Bengt. 1972b. Asymptotic theory
for successive sampling with varying
probabilities without replacement, II.
The Annals of Mathematical Statistics,
43(3):748?776.
Salton, Gerard. 1989. Automatic Text
Processing: The Transformation, Analysis, and
Retrieval of Information by Computer.
Addison-Wesley, New York, NY.
Stephan, Frederick F. 1942. An iterative
method of adjusting sample frequency
tables when expected marginal totals are
known. The Annals of Mathematical
Statistics, 13(2):166?178.
Strehl, Alexander and Joydeep Ghosh.
2000. A scalable approach to balanced,
high-dimensional clustering of
market-baskets. In HiPC, pages 525?536,
Bangalore, India.
Toivonen, Hannu. 1996. Sampling large
databases for association rules. In VLDB,
pages 134?145, Bombay, India.
Vempala, Santosh. 2004. The Random
Projection Method. American Mathematical
Society, Providence, RI.
Witten, Ian H., Alstair Moffat, and
Timothy C. Bell. 1999. Managing Gigabytes:
Compressing and Indexing Documents and
Images. Morgan Kaufmann Publishing,
San Francisco, CA, second edition.
354
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346?1355,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Large Monolingual and Bilingual Corpora to
Improve Coordination Disambiguation
Shane Bergsma, David Yarowsky, Kenneth Church
Deptartment of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
sbergsma@jhu.edu, yarowsky@cs.jhu.edu, kenneth.church@jhu.edu
Abstract
Resolving coordination ambiguity is a clas-
sic hard problem. This paper looks at co-
ordination disambiguation in complex noun
phrases (NPs). Parsers trained on the Penn
Treebank are reporting impressive numbers
these days, but they don?t do very well on this
problem (79%). We explore systems trained
using three types of corpora: (1) annotated
(e.g. the Penn Treebank), (2) bitexts (e.g. Eu-
roparl), and (3) unannotated monolingual (e.g.
Google N-grams). Size matters: (1) is a mil-
lion words, (2) is potentially billions of words
and (3) is potentially trillions of words. The
unannotated monolingual data is helpful when
the ambiguity can be resolved through associ-
ations among the lexical items. The bilingual
data is helpful when the ambiguity can be re-
solved by the order of words in the translation.
We train separate classifiers with monolingual
and bilingual features and iteratively improve
them via co-training. The co-trained classifier
achieves close to 96% accuracy on Treebank
data and makes 20% fewer errors than a su-
pervised system trained with Treebank anno-
tations.
1 Introduction
Determining which words are being linked by a co-
ordinating conjunction is a classic hard problem.
Consider the pair:
+ellipsis rocket\w1 and mortar\w2 attacks\h
?ellipsis asbestos\w1 and polyvinyl\w2 chloride\h
+ellipsis is about both rocket attacks and mortar at-
tacks, unlike ?ellipsis which is not about asbestos
chloride. We use h to refer to the head of the phrase,
and w1 and w2 to refer to the other two lexical items.
Natural Language Processing applications need to
recognize NP ellipsis in order to make sense of new
sentences. For example, if an Internet search en-
gine is given the phrase rocket attacks as a query, it
should rank documents containing rocket and mor-
tar attacks highly, even though rocket and attacks
are not contiguous in the document. Furthermore,
NPs with ellipsis often require a distinct type of re-
ordering when translated into a foreign language.
Since coordination is both complex and produc-
tive, parsers and machine translation (MT) systems
cannot simply memorize the analysis of coordinate
phrases from training text. We propose an approach
to recognizing ellipsis that could benefit both MT
and other NLP technology that relies on shallow or
deep syntactic analysis.
While the general case of coordination is quite
complicated, we focus on the special case of com-
plex NPs. Errors in NP coordination typically ac-
count for the majority of parser coordination errors
(Hogan, 2007). The information needed to resolve
coordinate NP ambiguity cannot be derived from
hand-annotated data, and we follow previous work
in looking for new information sources to apply
to this problem (Resnik, 1999; Nakov and Hearst,
2005; Rus et al, 2007; Pitler et al, 2010).
We first resolve coordinate NP ambiguity in a
word-aligned parallel corpus. In bitexts, both mono-
lingual and bilingual information can indicate NP
structure. We create separate classifiers using mono-
lingual and bilingual feature views. We train the
two classifiers using co-training, iteratively improv-
ing the accuracy of one classifier by learning from
the predictions of the other. Starting from only two
1346
initial labeled examples, we are able to train a highly
accurate classifier using only monolingual features.
The monolingual classifier can then be used both
within and beyond the aligned bitext. In particular,
it achieves close to 96% accuracy on both bitext data
and on out-of-domain examples in the Treebank.
2 Problem Definition and Related Tasks
Our system operates over a part-of-speech tagged in-
put corpus. We attempt to resolve the ambiguity in
all tag sequences matching the expression:
[DT|PRP$] (N.*|J.*) and [DT|PRP$] (N.*|J.*) N.*
e.g. [the] rocket\w1 and [the] mortar\w2 attacks\h
Each example ends with a noun, h. Preceding h
are a pair of possibly-conjoined words, w1 and w2,
either nouns (rocket and mortar), adjectives, or a
mix of the two. We allow determiners or possessive
pronouns before w1 and/or w2. This pattern is very
common. Depending on the domain, we find it in
roughly one of every 10 to 20 sentences. We merge
identical matches in our corpus into a single exam-
ple for labeling. Roughly 38% of w1,w2 pairs are
both adjectives, 26% are nouns, and 36% are mixed.
The task is to determine whether w1 and w2 are
conjoined or not. When they are not conjoined, there
are two cases: 1) w1 is actually conjoined with w2 h
as a whole (e.g. asbestos and polyvinyl chloride),
or 2) The conjunction links something higher up in
the parse tree, as in, ?farmers are getting older\w1
and younger\w2 people\h are reluctant to take up
farming.? Here, and links two separate clauses.
Our task is both narrower and broader than pre-
vious work. It is broader than previous approaches
that have focused only on conjoined nouns (Resnik,
1999; Nakov and Hearst, 2005). Although pairs
of adjectives are usually conjoined (and mixed tags
are usually not), this is not always true, as in
older/younger above. For comparison, we also state
accuracy on the noun-only examples (? 8).
Our task is more narrow than the task tackled
by full-sentence parsers, but most parsers do not
bracket NP-internal structure at all, since such struc-
ture is absent from the primary training corpus for
statistical parsers, the Penn Treebank (Marcus et al,
1993). We confirm that standard broad-coverage
parsers perform poorly on our task (? 7).
Vadas and Curran (2007a) manually annotated NP
structure in the Penn Treebank, and a few custom NP
parsers have recently been developed using this data
(Vadas and Curran, 2007b; Pitler et al, 2010). Our
task is more narrow than the task handled by these
parsers since we do not handle other, less-frequent
and sometimes more complex constructions (e.g.
robot arms and legs). However, such constructions
are clearly amenable to our algorithm. In addition,
these parsers have only evaluated coordination res-
olution within base NPs, simplifying the task and
rendering the aforementioned older/younger prob-
lem moot. Finally, these custom parsers have only
used simple count features; for example, they have
not used the paraphrases we describe below.
3 Supervised Coordination Resolution
We adopt a discriminative approach to resolving co-
ordinate NP ambiguity. For each unique coordinate
NP in our corpus, we encode relevant information
in a feature vector, x?. A classifier scores these vec-
tors with a set of learned weights, w?. We assume N
labeled examples {(y1, x?1), ..., (yN , x?N )} are avail-
able to train the classifier. We use ?y = 1? as the
class label for NPs with ellipsis and ?y = 0? for
NPs without. Since our particular task requires a bi-
nary decision, any standard learning algorithm can
be used to learn the feature weights on the train-
ing data. We use (regularized) logistic regression
(a.k.a. maximum entropy) since it has been shown
to perform well on a range of NLP tasks, and also
because its probabilistic interpretation is useful for
co-training (? 4). In binary logistic regression, the
probability of a positive class takes the form of the
logistic function:
Pr(y = 1) = exp(w? ? x?)1 + exp(w? ? x?)
Ellipsis is predicted if Pr(y = 1) > 0.5 (equiva-
lently, w? ? x? > 0), otherwise we predict no ellipsis.
Supervised classifiers easily incorporate a range
of interdependent information into a learned deci-
sion function. The cost for this flexibility is typically
the need for labeled training data. The more features
we use, the more labeled data we need, since for
linear classifiers, the number of examples needed to
reach optimum performance is at most linear in the
1347
Phrase Evidence Pattern
dairy and meat English: ... production of dairy and meat... h of w1 and w2
production English: ... dairy production and meat production... w1 h and w2 h
(ellipsis) English: ... meat and dairy production... w2 and w1 h
Spanish: ... produccio?n la?ctea y ca?rnica... h w1 ... w2
? production dairy and meat
Finnish: ... maidon- ja lihantuotantoon... w1- ... w2h
? dairy- and meatproduction
French: ... production de produits laitiers et de viande... h ... w1 ... w2
? production of products dairy and of meat
asbestos and English: ... polyvinyl chloride and asbestos... w2 h and w1
polyvinyl English: ... asbestos , and polyvinyl chloride... w1 , and w2 h
chloride English: ... asbestos and chloride... w1 and h
(no ellipsis) Portuguese: ... o amianto e o cloreto de polivinilo... w1 ... h ... w2
? the asbestos and the chloride of polyvinyl
Italian: ... l? asbesto e il polivinilcloruro... w1 ... w2h
? the asbestos and the polyvinylchloride
Table 1: Monolingual and bilingual evidence for ellipsis or lack-of-ellipsis in coordination of [w1 and w2 h] phrases.
number of features (Vapnik, 1998). In ? 4, we pro-
pose a way to circumvent the need for labeled data.
We now describe the particular monolingual and
bilingual information we use for this problem. We
refer to Table 1 for canonical examples of the two
classes and also to provide intuition for the features.
3.1 Monolingual Features
Count features These real-valued features encode
the frequency, in a large auxiliary corpus, of rel-
evant word sequences. Co-occurrence frequencies
have long been used to resolve linguistic ambigui-
ties (Dagan and Itai, 1990; Hindle and Rooth, 1993;
Lauer, 1995). With the massive volumes of raw
text now available, we can look for very specific
and indicative word sequences. Consider the phrase
dairy and meat production (Table 1). A high count
in raw text for the paraphrase ?production of dairy
and meat? implies ellipsis in the original example.
In the third column of Table 1, we suggest a pat-
tern that generalizes the particular piece of evidence.
It is these patterns and other English paraphrases
that we encode in our count features (Table 2). We
also use (but do not list) count features for the four
paraphrases proposed in Nakov and Hearst (2005,
? 3.2.3). Such specific paraphrases are more com-
mon than one might think. In our experiments, at
least 20% of examples have non-zero counts for a
5-gram pattern, while over 70% of examples have
counts for a 4-gram pattern.
Our features also include counts for subsequences
of the full phrase. High counts for ?dairy produc-
tion? alone or just ?dairy and meat? also indicate el-
lipsis. On the other hand, like Pitler et al (2010), we
have a feature for the count of ?dairy and produc-
tion.? Frequent conjoining of w1 and h is evidence
that there is no ellipsis, that w1 and h are compatible
and heads of two separate and conjoined NPs.
Many of our patterns are novel in that they include
commas or determiners. The presence of these of-
ten indicate that there are two separate NPs. E.g.
seeing asbestos , and polyvinyl chloride or the as-
bestos and the polyvinyl chloride suggests no ellip-
sis. We also propose patterns that include left-and-
right context around the NP. These aim to capture
salient information about the NP?s distribution as an
entire unit. Finally, patterns involving prepositions
look for explicit paraphrasing of the nominal rela-
tions; the presence of ?h PREP w1 and w2? in a cor-
pus would suggest ellipsis in the original NP.
In total, we have 48 separate count features, re-
quiring counts for 315 distinct N-grams for each ex-
ample. We use log-counts as the feature value, and
use a separate binary feature to indicate if a partic-
ular count is zero. We efficiently acquire the counts
using custom tools for managing web-scale N-gram
1348
Real-valued count features. C(p) ? count of p
C(w1) C(w2) C(h)
C(w1 CC w2) C(w1 h) C(w2 h)
C(w2 CC w1) C(w1 CC h) C(h CC w1)
C(DT w1 CC w2) C(w1 , CC w2)
C(DT w2 CC w1) C(w2 , CC w1)
C(DT w1 CC h) C(w1 CC w2 ,)
C(DT h CC w1) C(w2 CC w1 ,)
C(DT w1 and DT w2) C(w1 CC DT w2)
C(DT w2 and DT w1) C(w2 CC DT w1)
C(DT h and DT w1) C(w1 CC DT h)
C(DT h and DT w2) C(h CC DT w1)
C(?L-CTXTi? w1 and w2 h) C(w1 CC w2 h)
C(w1 and w2 h ?R-CTXTi?) C(h PREP w1)
C(h PREP w1 CC w2) C(h PREP w2)
Count feature filler sets
DT = {the, a, an, its, his} CC = {and, or, ?,?}
PREP = {of, for, in, at, on, from, with, about}
Binary features and feature templates ? {0, 1}
wrd1=?wrd(w1)? tag1=?tag(w1)?
wrd2=?wrd(w2)? tag2=?tag(w2)?
wrdh=?wrd(h)? tagh=?tag(h)?
wrd12=?wrd(w1),wrd(w2)? wrd(w1)=wrd(w2)
tag12=?tag(w1),tag(w2)? tag(w1)=tag(w2)
tag12h=?tag(w1),tag(w1),tag(h)?
Table 2: Monolingual features. For counts using the
filler sets CC, DT and PREP, counts are summed across
all filler combinations. In contrast, feature templates are
denoted with ???, where the feature label depends on the
?bracketed argument?. E.g., we have separate count fea-
ture for each item in the L/R context sets, where
{L-CTXT} = {with, and, as, including, on, is, are, &},
{R-CTXT} = {and, have, of, on, said, to, were, &}
data (? 5). Previous approaches have used search
engine page counts as substitutes for co-occurrence
information (Nakov and Hearst, 2005; Rus et al,
2007). These approaches clearly cannot scale to use
the wide range of information used in our system.
Binary features Table 2 gives the binary features
and feature templates. These are templates in the
sense that every unique word or tag fills the tem-
plate and corresponds to a unique feature. We can
thus learn if particular words or tags are associated
with ellipsis. We also include binary features to flag
the presence of any optional determiners before w1
or w2. We also have binary features for the context
words that precede and follow the tag sequence in
the source corpus. These context features are analo-
gous to the L/R-CTXT features that were counted in
the auxiliary corpus. Our classifier learns, for exam-
Monolingual: x?m Bilingual: x?b
C(w1):14.4 C(detl=h * w1 * w2),Dutch:1
C(w2):15.4 C(detl=h * * w1 * * w2),Fr.:1
C(h):17.2 C(detl=h w1 h * w2),Greek:1
C(w1 CC w2):9.0 C(detl=h w1 * w2),Spanish:1
C(w1 h):9.8 C(detl=w1- * w2h),Swedish:1
C(w2 h):10.2 C(simp=h w1 w2),Dutch:1
C(w2 CC w1):10.5 C(simp=h w1 w2),French:1
C(w1 CC h):3.5 C(simp=h w1 h w2),Greek:1
C(h CC w1):6.8 C(simp=h w1 w2),Spanish:1
C(DT w2 CC w1:7.8 C(simp=w1 w2h),Swedish:1
C(w1 and w2 h and):2.4 C(span=5),Dutch:1
C(h PREP w1 CC w2):2.6 C(span=7),French:1
wrd1=dairy:1 C(span=5),Greek:1
wrd2=meat:1 C(span=4),Spanish:1
wrdh=production:1 C(span=3),Swedish:1
tag1=NN:1 C(ord=h w1 w2),Dutch:1
tag2=NN:1 C(ord=h w1 w2),French:1
tagh=NN:1 C(ord=h w1 h w2),Greek:1
wrd12=dairy,meat:1 C(ord=h w1 w2),Spanish:1
tag12=NN,NN:1 C(ord=w1 w2 h),Swedish:1
tag(w1)=tag(w2):1 C(ord=h w1 w2):4
tag12h=NN,NN,NN:1 C(ord=w1 w2 h):1
Table 3: Example of actual instantiated feature vectors
for dairy and meat production (in label:value format).
Monolingual feature vector, x?m, on the left (both count
and binary features, see Table 2), Bilingual feature vec-
tor, x?b, on the right (see Table 4).
ple, that instances preceded by the words its and in
are likely to have ellipsis: these words tend to pre-
cede single NPs as opposed to conjoined NP pairs.
Example Table 3 provides part of the actual in-
stantiated monolingual feature vector for dairy and
meat production. Note the count features have log-
arithmic values, while only the non-zero binary fea-
tures are included.
A later stage of processing extracts a list of feature
labels from the training data. This list is then used
to map feature labels to integers, yielding the stan-
dard (sparse) format used by most machine learning
software (e.g., 1:14.4 2:15.4 3:17.2 ... 7149:1 24208:1).
3.2 Bilingual Features
The above features represent the best of the infor-
mation available to a coordinate NP classifier when
operating on an arbitrary text. In some domains,
however, we have additional information to inform
our decisions. We consider the case where we seek
to predict coordinate structure in parallel text: i.e.,
English text with a corresponding translation in one
1349
or more target languages. A variety of mature NLP
tools exists in this domain, allowing us to robustly
align the parallel text first at the sentence and then
at the word level. Given a word-aligned parallel cor-
pus, we can see how the different types of coordinate
NPs are translated in the target languages.
In Romance languages, examples with ellipsis,
such as dairy and meat production (Table 1), tend to
correspond to translations with the head in the first
position, e.g. ?produccio?n la?ctea y ca?rnica? in Span-
ish (examples taken from Europarl (Koehn, 2005)).
When there is no ellipsis, the head-first syntax leads
to the ?w1 and h w2? ordering, e.g. amianto e o
cloreto de polivinilo in Portuguese. Another clue
for ellipsis is the presence of a dangling hyphen, as
in the Finnish maidon- ja lihantuotantoon. We find
such hyphens especially common in Germanic lan-
guages like Dutch. In addition to language-specific
clues, a translation may resolve an ambiguity by
paraphrasing the example in the same way it may
be paraphrased in English. E.g., we see hard and
soft drugs translated into Spanish as drogas blandas
y drogas duras with the head, drogas, repeated (akin
to soft drugs and hard drugs in English).
One could imagine manually defining the rela-
tionship between English NP coordination and the
patterns in each language, but this would need to be
repeated for each language pair, and would likely
miss many useful patterns. In contrast, by represent-
ing the translation patterns as features in a classifier,
we can instead automatically learn the coordination-
translation correspondences, in any language pair.
For each occurrence of a coordinate NP in a word-
aligned bitext, we inspect the alignments and de-
termine the mapping of w1, w2 and h. Recall that
each of our examples represents all the occurrences
of a unique coordinate NP in a corpus. We there-
fore aggregate translation information over all the
occurrences. Since the alignments in automatically-
aligned parallel text are noisy, the more occurrences
we have, the more translations we have, and the
more likely we are to make a correct decision. For
some common instances in Europarl, like Agricul-
ture and Rural Development, we have thousands of
translations in several languages.
Table 4 provides the bilingual feature templates.
The notation indicates that, for a given coordi-
nate NP, we count the frequency of each transla-
C?detl(w1,w2,h)?,?LANG?
C?simp(w1,w2,h)?,?LANG?
C?span(w1,w2,h)?,?LANG?
C?ord(w1,w2,h)?,?LANG?
C?ord(w1,w2,h)?
Table 4: Real-valued bilingual feature templates. The
shorthand is detl=?detailed pattern,? simp=?simple pat-
tern,? span=?span of pattern,? ord=?order of words.? The
notation C?p?,?LANG?means the number of times we see
the pattern (or span) ?p? as the aligned translation of the
coordinate NP in the target language ?LANG?.
tion pattern in each target language, and generate
real-valued features for these counts. The feature
counts are indexed to the particular pattern and lan-
guage. We also have one language-independent fea-
ture, C?ord(w1,w2,h)?, which gives the frequency of
each ordering across all languages. The span is the
number of tokens collectively spanned by the trans-
lations of w1, w2 and h. The ?detailed pattern? rep-
resents the translation using wildcards for all other
foreign words, but maintains punctuation. Letting
?*? stand for the wildcard, the detailed patterns for
the translations of dairy and meat production in Ta-
ble 1 would be [h w1 * w2] (Spanish), [w1- * w2h]
(Finnish) and [h * * w1 * * w2] (French). Four
or more consecutive wildcards are converted to ?...?.
For the ?simple pattern,? we remove the wildcards
and punctuation. Note that our aligner allows the
English word to map to multiple target words. The
simple pattern differs from the ordering in that it de-
notes how many tokens each of w1, w2 and h span.
Example Table 3 also provides part of the actual
instantiated bilingual feature vector for dairy and
meat production.
4 Bilingual Co-training
We exploit the orthogonality of the monolingual
and bilingual features using semi-supervised learn-
ing. These features are orthogonal in the sense that
they look at different sources of information for each
example. If we had enough training data, a good
classifier could be trained using either monolingual
or bilingual features on their own. With classifiers
trained on even a little labeled data, it?s feasible that
for a particular example, the monolingual classifier
might be confident when the bilingual classifier is
1350
Algorithm 1 The bilingual co-training algorithm: subscript m corresponds to monolingual, b to bilingual
Given: ? a set L of labeled training examples in the bitext, {(x?i, yi)}
? a set U of unlabeled examples in the bitext, {x?j}
? hyperparams: k (num. iterations), um and ub (size smaller unlabeled pools), nm and nb
(num. new labeled examples each iteration), C: regularization param. for classifier training
Create Lm ? L
Create Lb ? L
Create a pool Um by choosing um examples randomly from U .
Create a pool Ub by choosing ub examples randomly from U .
for i = 0 to k do
Use Lm to train a classifier hm using only x?m, the monolingual features of x?
Use Lb to train a classifier hb using only x?b, the bilingual features of x?
Use hm to label Um, move the nm most-confident examples to Lb
Use hb to label Ub, move the nb most-confident examples to Lm
Replenish Um and Ub randomly from U with nm and nb new examples
end for
uncertain, and vice versa. This suggests using a
co-training approach (Yarowsky, 1995; Blum and
Mitchell, 1998). We train separate classifiers on the
labeled data. We use the predictions of one classi-
fier to label new examples for training the orthogo-
nal classifier. We iterate this training and labeling.
We outline how this procedure can be applied to
bitext data in Algorithm 1 (above). We follow prior
work in drawing predictions from smaller pools, Um
and Ub, rather than from U itself, to ensure the la-
beled examples ?are more representative of the un-
derlying distribution? (Blum and Mitchell, 1998).
We use a logistic regression classifier for hm and
hb. Like Blum and Mitchell (1998), we also create
a combined classifier by making predictions accord-
ing to argmaxy=1,0 Pr(y|xm)Pr(y|xb).
The hyperparameters of the algorithm are 1) k,
the number of iterations, 2) um and ub, the size of
the smaller unlabeled pools, 3) nm and nb, the num-
ber of new labeled examples to include at each itera-
tion, and 4) the regularization parameter of the logis-
tic regression classifier. All such parameters can be
tuned on a development set. Like Blum and Mitchell
(1998), we ensure that we maintain roughly the true
class balance in the labeled examples added at each
iteration; we also estimate this balance using devel-
opment data.
There are some differences between our approach
and the co-training algorithm presented in Blum and
Mitchell (1998, Table 1). One of our key goals is to
produce an accurate classifier that uses only mono-
lingual features, since only this classifier can be ap-
plied to arbitrary monolingual text. We thus break
the symmetry in the original algorithm and allow hb
to label more examples for hm than vice versa, so
that hm will improve faster. This is desirable be-
cause we don?t have unlimited unlabeled examples
to draw from, only those found in our parallel text.
5 Data
Web-scale text data is used for monolingual feature
counts, parallel text is used for classifier co-training,
and labeled data is used for training and evaluation.
Web-scale N-gram Data We extract our counts
from Google V2: a new N-gram corpus (with
N-grams of length one-to-five) created from the
same one-trillion-word snapshot of the web as the
Google 5-gram Corpus (Brants and Franz, 2006),
but with enhanced filtering and processing of the
source text (Lin et al, 2010, Section 5). We get
counts using the suffix array tools described in (Lin
et al, 2010). We add one to all counts for smooth-
ing.
Parallel Data We use the Danish, German, Greek,
Spanish, Finnish, French, Italian, Dutch, Por-
tuguese, and Swedish portions of Europarl (Koehn,
2005). We also use the Czech, German, Span-
ish and French news commentary data from WMT
1351
2010.1 Word-aligned English-Foreign bitexts are
created using the Berkeley aligner.2 We run 5 itera-
tions of joint IBM Model 1 training, followed by 3-
to-5 iterations of joint HMM training, and align with
the competitive-thresholding heuristic. The English
portions of all bitexts are part-of-speech tagged with
CRFTagger (Phan, 2006). 94K unique coordinate
NPs and their translations are then extracted.
Labeled Data For experiments within the paral-
lel text, we manually labeled 1320 of the 94K co-
ordinate NP examples. We use 605 examples to set
development parameters, 607 examples as held-out
test data, and 2, 10 or 100 examples for training.
For experiments on the WSJ portion of the Penn
Treebank, we merge the original Treebank annota-
tions with the NP annotations provided by Vadas and
Curran (2007a). We collect all coordinate NP se-
quences matching our pattern and collapse them into
a single example. We label these instances by deter-
mining whether the annotations have w1 and w2 con-
joined. In only one case did the same coordinate NP
have different labels in different occurrences; this
was clearly an error and resolved accordingly. We
collected 1777 coordinate NPs in total, and divided
them into 777 examples for training, 500 for devel-
opment and 500 as a final held-out test set.
6 Evaluation and Settings
We evaluate using accuracy: the percentage of ex-
amples classified correctly in held-out test data.
We compare our systems to a baseline referred to
as the Tag-Triple classifier. This classifier has a
single feature: the tag(w1), tag(w2), tag(h) triple.
Tag-Triple is therefore essentially a discriminative,
unlexicalized parser for our coordinate NPs.
All classifiers use L2-regularized logistic regres-
sion training via LIBLINEAR (Fan et al, 2008). For
co-training, we fix regularization at C = 0.1. For all
other classifiers, we optimize the C parameter on the
development data. At each iteration, i, classifier hm
annotates 50 new examples for training hb, from a
pool of 750 examples, while hb annotates 50 ? i new
examples for hm, from a pool of 750 ? i examples.
This ensures hm gets the majority of automatically-
labeled examples.
1
www.statmt.org/wmt10/translation-task.html
2
nlp.cs.berkeley.edu/pages/wordaligner.html
 86
 88
 90
 92
 94
 96
 98
 100
 0  10  20  30  40  50  60
Ac
cu
ra
cy
 (%
)
Co-training iteration
Bilingual View
Monolingual View
Combined
Figure 1: Accuracy on Bitext development data over the
course of co-training (from 10 initial seed examples).
We also set k, the number of co-training itera-
tions. The monolingual, bilingual, and combined
classifiers reach their optimum levels of perfor-
mance after different numbers of iterations (Fig-
ure 1). We therefore set k separately for each, stop-
ping around 16 iterations for the combined, 51 for
the monolingual, and 57 for the bilingual classifier.
7 Bitext Experiments
We evaluate our systems on our held-out bitext data.
The majority class is ellipsis, in 55.8% of exam-
ples. For comparison, we ran two publicly-available
broad-coverage parsers and analyzed whether they
correctly predicted ellipsis. The parsers were the
C&C parser (Curran et al, 2007) and Minipar (Lin,
1998). They achieved 78.6% and 77.6%.3
Table 5 shows that co-training results in much
more accurate classifiers than supervised training
alone, regardless of the features or amount of ini-
tial training data. The Tag-Triple system is the
weakest system in all cases. This shows that better
monolingual features are very important, but semi-
supervised training can also make a big difference.
3We provided the parsers full sentences containing the NPs. We
directly extracted the labels from the C&C bracketing, while
for Minipar we checked whether w1 was the head of w2. Of
course, the parsers performed very poorly on ellipsis involving
two nouns (partly because NP structure is absent from their
training corpora (see ? 2 and also Vadas and Curran (2008)),
but neither exceeded 88% on adjective or mixed pairs either.
1352
# of Examples
System 2 10 100
Tag-Triple classifier 67.4 79.1 82.9
Monolingual classifier 69.9 90.8 91.6
Co-trained Mono. classifier 96.4 95.9 96.0
Relative error reduction via co-training 88% 62% 52%
Bilingual classifier 76.8 85.5 92.1
Co-trained Bili. classifier 93.2 93.2 93.9
Relative error reduction via co-training 71% 53% 23%
Mono.+Bili. classifier 69.9 91.4 94.9
Co-trained Combo classifier 96.7 96.7 96.7
Relative error reduction via co-training 89% 62% 35%
Table 5: Co-training improves accuracy (%) over stan-
dard supervised learning on Bitext test data for different
feature types and number of training examples.
System Accuracy ?
Monolingual alone 91.6 -
+ Bilingual 94.9 39%
+ Co-training 96.0 54%
+ Bilingual & Co-training 96.7 61%
Table 6: Net benefits of bilingual features and co-training
on Bitext data, 100-training-example setting. ? = rela-
tive error reduction over Monolingual alone.
Table 6 shows the net benefit of our main contri-
butions. Bilingual features clearly help on this task,
but not as much as co-training. With bilingual fea-
tures and co-training together, we achieve 96.7% ac-
curacy. This combined system could be used to very
accurately resolve coordinate ambiguity in parallel
data prior to training an MT system.
8 WSJ Experiments
While we can now accurately resolve coordinate NP
ambiguity in parallel text, it would be even better
if this accuracy carried over to new domains, where
bilingual features are not available. We test the ro-
bustness of our co-trained monolingual classifier by
evaluating it on our labeled WSJ data.
The Penn Treebank and the annotations added by
Vadas and Curran (2007a) comprise a very special
corpus; such data is clearly not available in every
domain. We can take advantage of the plentiful la-
beled examples to also test how our co-trained sys-
tem compares to supervised systems trained with in-
System Training WSJ Acc.Set # Nouns All
Nakov & Hearst - - 79.2 84.8
Tag-Triple WSJ 777 76.1 82.4
Pitler et al WSJ 777 92.3 92.8
MonoWSJ WSJ 777 92.3 94.4
Co-trained Bitext 2 93.8 95.6
Table 7: Coordinate resolution accuracy (%) on WSJ.
domain labeled examples, and also other systems,
like Nakov and Hearst (2005), which although un-
supervised, are tuned on WSJ data.
We reimplemented Nakov and Hearst (2005)4 and
Pitler et al (2010)5 and trained the latter on WSJ an-
notations. We compare these systems to Tag-Triple
and also to a supervised system trained on the WSJ
using only our monolingual features (MonoWSJ).
The (out-of-domain) bitext co-trained system is the
best system on the WSJ data, both on just the ex-
amples where w1 and w2 are nouns (Nouns), and on
all examples (All) (Table 7).6 It is statistically sig-
nificantly better than the prior state-of-the-art Pitler
et al system (McNemar?s test, p<0.05) and also
exceeds the WSJ-trained system using monolingual
features (p<0.2). This domain robustness is less sur-
prising given its key features are derived from web-
scale N-gram data; such features are known to gen-
eralize well across domains (Bergsma et al, 2010).
We tried co-training without the N-gram features,
and performance was worse on the WSJ (85%) than
supervised training on WSJ data alone (87%).
9 Related Work
Bilingual data has been used to resolve a range of
ambiguities, from PP-attachment (Schwartz et al,
2003; Fossum and Knight, 2008), to distinguishing
grammatical roles (Schwarck et al, 2010), to full
dependency parsing (Huang et al, 2009). Related
4Nakov and Hearst (2005) use an unsupervised algorithm that
predicts ellipsis on the basis of a majority vote over a number
of pattern counts and established heuristics.
5Pitler et al (2010) uses a supervised classifier to predict brack-
etings; their count and binary features are a strict subset of the
features used in our Monolingual classifier.
6For co-training, we tuned k on the WSJ dev set but left other
parameters the same. We start from 2 training instances; results
were the same or slightly better with 10 or 100 instances.
1353
work has also focused on projecting syntactic an-
notations from one language to another (Yarowsky
and Ngai, 2001; Hwa et al, 2005), and jointly pars-
ing the two sides of a bitext by leveraging the align-
ments during training and testing (Smith and Smith,
2004; Burkett and Klein, 2008) or just during train-
ing (Snyder et al, 2009). None of this work has fo-
cused on coordination, nor has it combined bitexts
with web-scale monolingual information.
Most prior work has focused on leveraging the
alignments between a single pair of languages. Da-
gan et al (1991) first articulated the need for ?a mul-
tilingual corpora based system, which exploits the
differences between languages to automatically ac-
quire knowledge about word senses.? Kuhn (2004)
used alignments across several Europarl bitexts to
devise rules for identifying parse distituents. Ban-
nard and Callison-Burch (2005) used multiple bi-
texts as part of a system for extracting paraphrases.
Our co-training algorithm is well suited to using
multiple bitexts because it automatically learns the
value of alignment information in each language. In
addition, our approach copes with noisy alignments
both by aggregating information across languages
(and repeated occurrences within a language), and
by only selecting the most confident examples at
each iteration. Burkett et al (2010) also pro-
posed exploiting monolingual-view and bilingual-
view predictors. In their work, the bilingual view
encodes the per-instance agreement between mono-
lingual predictors in two languages, while our bilin-
gual view encodes the alignment and target text to-
gether, across multiple instances and languages.
The other side of the coin is the use of syntax to
perform better translation (Wu, 1997). This is a rich
field of research with its own annual workshop (Syn-
tax and Structure in Translation).
Our monolingual model is most similar to pre-
vious work using counts from web-scale text, both
for resolving coordination ambiguity (Nakov and
Hearst, 2005; Rus et al, 2007; Pitler et al, 2010),
and for syntax and semantics in general (Lapata
and Keller, 2005; Bergsma et al, 2010). We do
not currently use semantic similarity (either tax-
onomic (Resnik, 1999) or distributional (Hogan,
2007)) which has previously been found useful for
coordination. Our model can easily include such in-
formation as additional features. Adding new fea-
tures without adding new training data is often prob-
lematic, but is promising in our framework, since the
bitexts provide so much indirect supervision.
10 Conclusion
Resolving coordination ambiguity is hard. Parsers
are reporting impressive numbers these days, but
coordination remains an area with room for im-
provement. We focused on a specific subcase, com-
plex NPs, and introduced a new evaluation set. We
achieved a huge performance improvement from
79% for state-of-the-art parsers to 96%.7
Size matters. Most parsers are trained on a mere
million words of the Penn Treebank. In this work,
we show how to take advantage of billions of words
of bitexts and trillions of words of unlabeled mono-
lingual text. Larger corpora make it possible to
use associations among lexical items (compare dairy
production vs. asbestos chloride) and precise para-
phrases (production of dairy and meat). Bitexts are
helpful when the ambiguity can be resolved by some
feature in another language (such as word order).
The Treebank is convenient for supervised train-
ing because it has annotations. We show that even
without such annotations, high-quality supervised
models can be trained using co-training and features
derived from huge volumes of unlabeled data.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. ACL,
pages 597?604.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proc. ACL, pages 865?874.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proc.
COLT, pages 92?100.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
EMNLP, pages 877?886.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proc. CoNLL, pages 46?53.
7Evaluation scripts and data are available online:
www.clsp.jhu.edu/?sbergsma/coordNP.ACL11.zip
1354
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-scale NLP with C&C and
Boxer. In Proc. ACL Demo and Poster Sessions, pages
33?36.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In Proc. COLING, pages 330?332.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proc.
ACL, pages 130?137.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve PP-
attachment ambiguity in English. In Proc. AMTA Stu-
dent Workshop, pages 48?53.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proc. ACL,
pages 680?687.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. EMNLP, pages 1222?1231.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. MT Summit X.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proc. ACL, pages 470?477.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech and Language Processing, 2(1):1?31.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proc. ACL, pages
47?54.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
N-grams. In Proc. LREC.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proc. LREC Workshop on the Evalu-
ation of Parsing Systems.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Preslav Nakov and Marti Hearst. 2005. Using the web as
an implicit training set: application to structural ambi-
guity resolution. In Proc. HLT-EMNLP, pages 17?24.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS
Tagger. crftagger.sourceforge.net.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth
Church. 2010. Using web-scale N-grams to improve
base NP parsing performance. In In Proc. COLING,
pages 886?894.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Vasile Rus, Sireesha Ravi, Mihai C. Lintean, and
Philip M. McCarthy. 2007. Unsupervised method for
parsing coordinated base noun phrases. In Proc. CI-
CLing, pages 229?240.
Florian Schwarck, Alexander Fraser, and Hinrich
Schu?tze. 2010. Bitext-based resolution of German
subject-object ambiguities. In Proc. HLT-NAACL,
pages 737?740.
Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003.
Disambiguation of English PP attachment using mul-
tilingual aligned data. In Proc. MT Summit IX, pages
330?337.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proc. EMNLP, pages 49?56.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proc. ACL-IJCNLP, pages 1041?1050.
David Vadas and James R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proc. ACL,
pages 240?247.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In PA-
CLING, pages 104?112.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proc. ACL, pages 104?
112.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In Proc. NAACL, pages
1?8.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. ACL,
pages 189?196.
1355
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 137?144,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
How Many Multiword Expressions do People Know?  Kenneth Church HLT COE Johns Hopkins University Kenneth.Church@jhu.edu  Abstract What is a multiword expression (MWE) and how many are there?  What is a MWE?  What is many?   Mark Liberman gave a great invited talk at ACL-89 titled ?how many words do people know?? where he spent the entire hour questioning the question.  Many of these same questions apply to multiword expressions.  What is a word?  What is many?  What is a person?  What does it mean to know?  Rather than answer these questions, this paper will use these questions as Liberman did, as an excuse for surveying how such issues are addressed in a variety of fields: computer science, web search, linguistics, lexicogra-phy, educational testing, psychology, statis-tics, etc. 1 How many words do people know? One can find all sorts of answers on the web: ? Very low: Apparently I only knew 7,000 words when I was seven and 14,000 when I was fourteen. I learned from exposure. Now things are not that easy in a second language, but it just shows that the brain can absorb information from sheer input.1 ? Low: 12,000 ? 20,000 words2 ? Higher: 988,9683 ? Even higher: 13,588,3914 
                                                            1 http://thelinguist.blogs.com/how_to_learn_english_and/2009/02/how-many-words-do-you-know-how-many-have-you-looked-up-in-a-dictionary.html 2 http://answers.yahoo.com/question/index?qid=20061105205054AA5YL0B 3 http://www.independent.co.uk/news/world/americas/english-language-nears-the-one-millionword-milestone-473935.html 
2 Motivation As mentioned in the abstract, Liberman used his ACL-89 invited talk to survey how various fields approach these issues.  He started his ACL-89 invited talk by questioning every word in the title of his talk: How many words do people know?  1. What is a word?   Is a word defined in terms of meaning?  Sound?  Syntax?  Spelling?  White space?  Distribution?  Etymology?   Learnability? 2. What is a person?  Child?  Adult?  Native speaker?  Language Learner? 3. What does it mean to know something?    Active knowledge is different from passive knowledge.    What is (Artificial) Intelli-gence?  Is vocabulary size a measure of in-telligence?  (Terman, 1918) 4. What do we mean by many?  Is there a limit like 20,000 or 1M  or 13.6M or does vocabulary size (V) keep growing with experience (larger corpora ?? larger V)? The original motivation for Liberman?s talk came from a very practical business concern.  At the time, Liberman was running a speech synthesis effort at AT&T Bell Labs.  As the manager of this effort, Liberman would receive questions from the business asking how large the synthesizer?s dict-ionary would have to be for such and such com-mercial application.  Vocabulary size was also a hot topic in many other engineering applications.  How big does the dictionary have to be for X?  X can be  anything from parsing, part of speech tagging, spelling correction, machine translation, word breaking for Chinese and Japanese (and English), speech recog-                                                                                             4 Franz and Brants (2006) 
137
nition, speech synthesis, web search or some other application.   3 Dictionary Estimates These questions reminded Liberman of similar questions that his colleagues in lexicography were receiving from their marketing departments.  Many dictionaries and other reference books lead with a marketing pitch such as: ?Most comprehensive: more than 330,000 words and phrases [MWEs]?? (Kipfer, 2001).  The very smallest dictionaries are called ?gems.?  They typically contain 20,000 words.  Unabridged collegiate dictionaries have about 500,000 words.5  The Oxford English Dictionary (OED) has 600,000 entries.6  All of these dictionaries limit themselves to what is known as general vocabulary, words that would be expected to be understood by a general audience.  General vocabulary is typically contrasted with technical terminology, words that would only be understood by domain experts in a particular topic. There are reference books that specialize on place names (Gazetteers), surnames, technical termi-nology, quotations, etc., but standard dictionaries of general vocabulary tend to avoid proper nouns, complex nominals (e.g., ?staff meeting?), abbrevi-ations, acronyms, technical terminology, digit sequences, street addresses, trademarks, product numbers, etc.7   Even the largest dictionaries may not have all that much coverage because in prac-tice, one often runs into texts that go well beyond general vocabulary. 4 Broder?s Taxonomy of Web Queries Obviously, the web goes well beyond general vocabulary.  Web queries tend to be short phrases (MWEs), often a word or two such as a product number.  Broder (2002) introduced a taxonomy of 
                                                            5 http://www.collinslanguage.com/shop/english-dictionary-landing.aspx 6 http://www.oed.com/public/about 7 See Sproat (1994) and references therein for more on complex nominals.  See Coker et al(1990) for coverage statistics on surnames.  See Liberman and Church (1991) for more on abbreviations, acronyms, digit sequences and more.  See Dagan and Church (1994) for more on technical terminology. 
queries that has become widely accepted.   His percentage estimates were estimated from AltaVista query logs and could use updating.   ? Naviational (20%) ? Informational (48%) ? Transactional (30%) Navigational queries are extremely common these days, perhaps even more common than 20%.  The user intent is to navigate to a particular url:  ? google ?? www.google.com  ? Greyhound Bus ?? www.greyhound.com ? American Airlines ?? www.aa.com  Broder?s examples of informational queries are: cars, San Francisco, normocytic anemia, Scoville heat units.  The user intent is to research a particular information need.  The user expects to read one or more static web pages in order to address the information need.  Broder italicized ?static? to distinguish informational queries from transactional queries.  Transactional queries are intended to reach a site where further (non-static) action will take place: shopping, directions, web-mediated services, medical advice, gaming, down-loading music, pictures, videos, etc. 5 User Intent & One Sense Per Query I prefer a two-way distinction between  1. Navigational queries: user knows where she wants to go, and  2. Non-navigational queries: user is open to suggestions. Google, for example, offers the following ?related search? suggestions for ?camera:? digital camera, video camera,  history of the camera, sony camera,  ritz camera, Nikon camera, camera brands, cam-era reviews, camera store, beach camera, canon, photography, bestbuy, camara, cannon, cir-cuit city. camero. Olympus, camcorder, b&h. These kinds of suggestions can be very successful when the user is open to suggestions, but not for navigational queries.   There are a number of other mechanisms for making suggestions such as ads and did-you-mean spelling suggestions. 
138
Pitler and Church (2009) used click logs to classify queries by intent (CQI).   Consider five types of clicks.  Some types of clicks are evidence that the user knows where she wants to go, and some are evidence that the user is open to suggestions.  1. Algo: clicks on the so-called 10 blue links 2. Paid: clicks on commercial ads 3. Wikipedia: clicks on Wikipedia entries 4. Spelling Corrections: did you mean ?? 5. Other suggestions from search engine Many queries are strongly associated with one type of click (more than others).  ? Commercial queries ?? clicks on ads  ? Non-commercial queries  ?? Wikipedia. There is a one-sense-per-X constraint (Gale et al 1992; Yarowsky, 1993).  It is unlikely that the same query will be ambiguous with both commercial and non-commercial senses.  Indeed, the click logs show that both ads and Wikipedia are effective, but they have complementary distributions.  There are few queries with both clicks on ads and clicks on Wikipedia entries.   For a commercial query like, ?JC Penney,? it is ok for Google to return an ad and a store locator map, but Google shouldn?t return a Wikipedia discussion of the history of the company.  Although the click logs are very large, they are never large enough.  How do we resolve the user intention when the click logs are too sparse to resolve the ambiguity directly?  Pitler suggested using word sense disambiguation methods.   For example, her method labels the ambiguous query ?designer trench? as commercial because it is closer (in random walk distance) to a couple of stores than to a Wikipedia discussion of trench warfare during World War I.  More generally, random walk methods (like word sense disambiguation) can be used to resolve all sorts of hidden variables such as gender, age, location, political orientation, user intent, etc.  Did the user mean X?   Does the user know what she wants, or is she open to suggestions? 
5.1 User Intent & Spelling Correction Spelling correction is an extreme case where it is often relatively easy for the system to determine user intent.  On the web, spelling correction has become synonymous with did-you-mean.  The synonymy makes it clear that the point of spelling correction is to get at what users mean as opposed to what they say.  Then you should say what you mean,' the March Hare went on.  `I do,' Alice hastily replied; `at least--at least I mean what I say--that's the same thing, you know.'  `Not the same thing a bit!' said the Hatter.  (Lewis Carroll, 1865)   See Kukich (1992) for a comprehensive survey on spelling correction.  Boswell (2004) is a nice research exam; it is short and crisp and recent.  I?ve worked on Microsoft?s spelling correction products in two different divisions: Office and Web Search.  One might think that correcting documents in Microsoft Word would be similar to correcting web queries, but in fact, the two applications have remarkably little in common.  A dictionary of general vocabulary is essential for correcting documents and nearly useless for correcting web queries.  General vocabulary is more important in documents than web queries.  The surveys mentioned above are more appropriate for correcting documents than web queries.  Cucerzan and Brill (2004) propose an iterative process that is more appropriate for web queries.  In Table 1, they show a number of (mis)spellings of Albert Einstein?s name from a query log, sorted by frequency: albert einstein (4834), albert einstien (525), albert einstine (149), albert einsten (27), albert einsteins (25), etc.  Their method takes a web query that may or may not be misspelled and considers nearby corrections with higher frequencies.   The method continues to iterate in this way until it converges at a fixed point.  The iteration makes it possible to correct multiple errors.  For example, anol scwartegger ?? arnold schwartznegger ?? arnold schwarznegger ?? arn-old schwarzenegger.  They find that context is often very helpful.  In general, it is easier to correct 
139
the combination of the first name and the last name together than separately.   So too, it is probably easier to correct MWEs as a combination than to correct each of the parts separately. 5.2 User Intent & Spoken Queries Queries often depend on context in complex and unexpected ways.  It has been said that there is no there there on the web, but queries from cell phones are often looking for stuff that has a ?there? (a location), and moreover the location is often near the user (e.g., restaurants, directions).  Users now have the option to enter queries by voice in addition to the keyboard.  Kamvar and Beeferman (2010) found voice was relatively popular on mobile devices with ?compressed? (hard-to-use) keyboards.  They also found some topics were relatively more likely to be spoken:  ? Food & Drink: Starbucks, tuna fish, Mex-ican food ? Business Listings: Starbucks Holmdel NJ, Lake George ? Properties relating to places: weather Holmdel NJ, best gas prices ? Shopping & Travel: Rapids Water Park coupons, black Converse shoes, Costco, Walmart Other topics such as adult queries are relatively less likely to be spoken, presumably because users don?t want to be overheard.  Privacy is more of a concern for some topics and less for others. 6 What is ?large?? The term ?large vocabulary? has been a moving target.  Vocabulary sizes have been increasing with advances in technology.  Around the time of Liberman?s ACL-89 talk, the speech recognition community was working really hard on a 20,000-word task.   Since it was so hard at the time to scale up recognizers to such large vocabularies, some researchers were desperately hoping that 20,000 words would be sufficient to achieve broad coverage of unrestricted language.  At that time, I gave a workshop talk that used a much larger vocabulary of 400,000 words (Church 
and Gale, 1989).  A leading researcher pulled me aside and begged me to tell him that I had made a mistake and there was no need to go beyond 20,000 words.  Similar questions came up when Google released their ngram counts over a trillion word corpus (Franz and Brants, 2006).  There was considerable pushback from the community over the size of the vocabulary (13,588,391).  Norvig (personal com-munication) called me up to ask if their estimate of 13.6 million seemed unreasonable.  While I had no reason to question Google?s estimate, I was reluctant to make a strong statement, given Efron and Thisted (1976).   Efron and Thisted studied a similar question: How many words did Shakespeare know (but didn?t use)?  They conclude that one can extrapolate corpus size a little bit (e.g., a factor of two) but not too much (e.g., an order of magnitude).  Since Google is working with corpora that are many orders of magnitude larger than what I had the opportunity to work with, it would require way too much extrapolation to answer Norvig?s question based on my relatively limited experience. 7 Vocabulary Grows with Experience Many people share the (mistaken) intuition that there is an upper bound on the size of the vocabulary.  Marketing pitches such ?330,000 words? (above) suggest that there is a reasonable upper bound that a person could hope to master (something considerably more manageable than Google?s estimate of 13.6 million).  In fact, the story is probably worse than that.  At ACL-1989, Liberman showed plots like those below.8  These plots make it clear that vocabulary (V) is going up and up and up with corpus size (N).  There appears to be no end in sight.  It is unlikely that there is an upper bound.  20k isn?t enough.  Nor is 400k, or even 13.6 million?  The different curves call out differences in what counts as a word.  Do we consider morphologically related forms to be one word or two?  How about                                                             8 Plots borrowed with permission from Language Log: http://itre.cis.upenn.edu/~myl/languagelog/archives/005514.html  
140
upper and lower case?  MWEs?  The different curves correspond to different choices.  No matter how we define a word, we find that vocabulary grows (rapidly) with corpus size for as far as we can see.  This observation appears to hold across a broad set of conditions (languages, definitions of word/ngram, etc.)   Vocabulary be-comes larger and larger with experience.  Similar comments apply to ngrams and MWEs.  
 
   
There is wide agreement that there?s no data like more data (Mercer, 1985).9   Google quoted Mer-cer in their announcement of ngram counts (Franz and Brants, 2006).  Banko and Brill (2001) observed that performance goes up and up and up with experience (data).  In the plot below, they note that the differences be-tween lines (learners) are small compared to the gains to be had by simply collecting more data.  Based on this observation, Brill has suggested (probably in jest) that we should fire everyone and spend the money on collecting data.  
 Another interpretation is that experience improves performance on a host of tasks.  This pattern might help account for the large correlation (0.91) in Terman (1918).  Terman suggests that vocabulary size should not be viewed as a measure of intelligence but rather a measure of experience.  He uses the term ?mental age? for experience, and measures ?mental age? by performance on a standardized test.  After adjusting for the large cor-relation between vocabulary size and experience, there is little evidence of a connection between vocabulary size and intelligence (or much of any-thing else).  Terman also considers a number of other factors such as gender and the language spoken at home (and testing errors), but ultimately concludes that experience dominates all of these alternative factors. 
                                                            9 Jelinek (2004) attributes this position to Mercer (1985) http://www.lrec-conf.org/lrec2004/doc/jelinek.pdf.  
141
8 What is a Word?  MWE? We tend to think that white space makes it pretty easy to tokenize English text into words. Ob-viously, white space makes the task much easier than it would be otherwise.  There is a considerable literature on word breaking in Chinese and Japan-ese which is considerably more challenging than English largely because there is no white space in Chinese and Japanese.   There are a number of popular dictionary-based solutions such as Cha-Sen10 and Juman.11  Sproat et al(1996) proposed an alternative solution based on distributional statistics such as mutual information.  The situation may not be all that different in English.  English is full of multiword expressions.  An obvious example involves words that look like prepositions: up, in, on, with.   A great example is often attributed to Winston Churchill: This is the sort of arrant nonsense up with which I will not put.12  One could argue that ?put up with? is a phrasal verb and therefore it should be treated more like a fixed expression (or a word) than a stranded preposition. 8.1 Preventing Bloopers Almost any high frequency verb (go, make, do, have, give, call) can form a phrase with almost any high frequency function word (it, up, in, on, with, out, down, around, over), often with non-compositional (taboo) semantics.  This fact led to a rather entertaining failure mode with a word sense program that was trained on a combination of Roget?s Thesaurus and Grolier?s Encyclopedia (Yarowsky, 1992).  Yarowsky?s program had a tendency to label high frequency words incorrectly with taboo senses due to a mismatch between Groliers and Roget?s.  Groliers was written for the parents of middle-American school children and therefore avoided taboo language, whereas Roget?s was edited by Chapman, an authority on American Slang (taboo language).  The mismatch was particularly nasty for high frequency words, which are very common                                                             10 http://chasen.naist.jp/hiki/ChaSen/ 11 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html 12 For a discussion of the source of this quotation, see http://itre.cis.upenn.edu/~myl/languagelog/archives/002670.html . 
in Groliers, but unlikely to be mentioned in Roget?s, except when the semantics are non-compositional (taboo).   Consequently, there was an embarrassingly high probability that Yarow-sky?s program would find embarrassing inter-pretations of benign texts.  While some of these mistakes are somewhat understandable and even somewhat amusing in a research prototype, such mistakes are no laughing matter in a commercial product.  The testers at Microsoft worked really hard to make sure that their products don?t make inappropriate suggest-ions.  Despite their best efforts, there have been a few highly publicized mistakes13  and there will probably be more unless we find better ways to prevent bloopers. 8.2 Complex Nominals and What is a Word? Complex nominals are probably more common than phrasal verbs.  Is ?White House? one word or two?  Is a word defined in terms of spelling?  White space?  These days, among computational linguists, there would be considerable sympathy for using distri-butional statistics such as word frequency and mutual information to find MWEs.   Following Firth (1957), we know a word by the company that it keeps.  In Church and Hanks (1990), we suggest-ed using pointwise mutual information as a heur-istic to look for pairs of words that have non-com-positional distributional statistics.  That is, if the joint probability, P(x,y), of seeing two words together in a context (e.g., window of 5 words) is much higher than chance, P(x)P(y), then there is probably a hidden variable such as meaning that is causing the deviation from chance.  In this way, we are able to discover lots of word associations (e.g., doctor?nurse), collocates, fixed expressions, etc.  If the list of MWEs becomes too large and too unmanageable, one could turn to a method like Stolcke pruning to cut back the list as necessary.  Stolcke pruning is designed to prune ngram models so they fit in a manageable amount of memory.  Suppose we have an ngram model with too many                                                             13 http://www.zdnet.co.uk/news/it-strategy/1999/07/01/microsoft-sued-for-racist-application-2072468/ 
142
ngrams and we have to drop some of them.  Which ones should we drop?  Stolcke pruning computes a loss in terms of relative entropy for dropping each ngram in the model.  The method drops the ngram that minimizes loss.  When an ngram is dropped from the model, that sequence is modeled with a backed off estimate from other ngrams.  Stolcke pruning can be thought of as introducing compositionality assumptions.   Suppose, for example, that ?nice house? has more compositional statistics than ?white house.?  That is, Pr(nice house) ? Pr(nice) Pr(house) whereas Pr(white house) >> Pr(white) Pr(house).  In this case, Stolcke pruning would drop ?nice house? before it drops ?white house.? 8.3 Linguistic Diagnostics  Linguists would feel more comfortable with defining word in terms of sound (phonology) and meaning (semantics).   It is pretty clear that ?White House? has non-compositional sound and meaning.  The ?White House? does not refer to a house that happens to be white, which is what would be expected under compositional semantics.  It is accented on the left (the WHITE house) in contrast with the general pattern where adjective-noun complex nominals are typically accented on the right (a nice HOUSE), though there are many exceptions to this rule (Sproat 1994).14  Linguists would also feel comfortable with diag-nostic tests based on paraphrases and trans-formations.  Fixed expressions are fixed.  One can?t paraphrase a ?red herring? as ?*herring that is red.?  They resist regular inflection: ?*two red herrings.?  In Bergsma et al(2011), we use a paraphrase diagnostic to distinguish [N & N] N from N & [ N N]:  ? [dairy and meat] production o meat and dairy production o production of meat and dairy o production de produits [laitiers et de viand] (French) 
                                                            14 Sproat has posted a list of 7831 English binary noun compounds with hand assigned accent labels at: http://www.cslu.ogi.edu/~sproatr/newindex/ap90nominals.txt 
? asbestos and [polyvinyl chloride] o polyvinyl chloride and asbestos o asbestos and chloride o l?asbesto e il [polivinilcloruro] (Italian) The first three paraphrases make it clear that ?dairy and meat? is a constituent whereas the last three paraphrases make it clear that ?polyvinyl chloride? is a constituent.  Comparable corpora can be viewed as a rich source of paraphrase data, as in-dicated by the French and Italian examples above.   9 Conclusions How many multiword expressions (MWEs) do people know?  The question is related to how many words do people know. 20k?  400k?  1M?  13M?  Is there a bound or does vocabulary size increase with experience (corpus size)?  Is vocab-ulary size a measure of intelligence or just exper-ience?  Dictionary sizes are just a lower bound because they focus on general vocabulary and avoid much of what matters on the web.  Spelling correction is not the same for documents of general vocabulary and web queries.  One can use Stolcke pruning and other composi-tionality tricks to cut down on the number of the number of multiword units that people must know.  But obviously, the number they must know is just a lower bound on the number they may know.  There are lots of engineering motivations for want-ing to know how many words and MWEs people know.  How big does the dictionary have to be for X (where X is parsing, tagging, spelling correction, machine translation, word breaking for Chinese and Japanese (and English), speech recognition, speech synthesis or some other application)?  Rather than answer these questions, this paper used these questions as Liberman did, as an excuse for surveying how such issues are addressed in a variety of fields: computer science, web search, linguistics, lexicography, educational testing, psy-chology, statistics, etc.  
143
References  Harald  Baayen  (2001) Word Frequency Distributions. Kluwer, Dordrecht.  Michele Banko and Eric Brill. (2001) ?Scaling to very very large corpora for natural language disambiguation,? ACL.  Shane Bergsma, David Yarowsky and Kenneth Church (2011), ?Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation,? ACL.  Dustin Boswell (2004) ?Speling Korecksion: A Survey of Techniques from Past to Present,? http://dustwell.com/PastWork/SpellingCorrectionResearchExam.pdf   Andrei Broder. 2002. A taxonomy of web search. SIGIR Forum 36, 2 (September 2002), 3-10.   Lewis Carroll, 1865, Alice?s Adventures in Wonder-land. Kenneth Church and William Gale (1989) ?Enhanced Good-Turing and Cat-Cal: two new methods for estimating probabilities of English bigrams.? HLT.  Kenneth Church and Patrick Hanks. (1990) ?Word association norms, mutual information, and lexico-graphy.? CL.  Cecil Coker, Kenneth Church and Mark Liberman (1990) ?Morphology and rhyming: two powerful alternatives to letter-to-sound rules for speech synthesis,? In ESCA Workshop on Speech Synthesis SSW1-1990, 83-86.  Silviu Cucerzan and Eric Brill (2004) ?Spelling Cor-rection as an Iterative Process that Exploits the Collective Knowledge of Web Users, EMNLP.  Ido Dagan and Kenneth Church. (1994) ?Termight: identifying and translating technical terminology,? ANLC.   William Gale, Kenneth Church and David Yarowsky. (1992) ?One sense per discourse,? HLT.  Bradley Efron and Ronald Thisted, (1976) ?Estimating the number of unseen species: How many words did Shakespeare know?? Biometrika, 63, 3, pp. 435-447.  John Firth, (1957) ?A Synopsis of Linguistic Theory 1930-1955,? in Studies in Linguistic Analysis, 
Philological Society, Oxford; reprinted in Palmer, F. (ed.) 1968 Selected Papers of J. R. Firth, Longman, Harlow.  Alex Franz and Thorsten Brants (2006) http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html.  Fred Jelinek (2004) ?Some of my Best Friends are Lin-guists,? LREC.   Maryan Kamvar and Doug Beeferman, (2010) ?Say What?  Why users choose t speak their web queries,? Interspeech. Barbara Kipfer (ed.) (2001) Roget?s Thesaurus, Sixth Edition, HarperCollins, NY, NY, USA.  Karen Kukich. 1992. Techniques for automatically correcting words in text. ACM Comput. Surv. 24, 4. Mark Liberman (1989) ?How many words do people know?? ACL.  Mark Liberman and Kenneth Church (1991). "Text analysis and word pronunciation in text-to-speech synthesis." In Advances in Speech Signal Processing, edited by S. Furui and M. Sondhi.  Frederick Mosteller and David Wallace. Inference and Disputed Authorship: the Federalist, Addison-Wes-ley, 1964.  Emily Pitler and Kenneth Church. (2009) ?Using word-sense disambiguation methods to classify web queries by intent,? EMNLP.  Richard Sproat, William Gale, Chilin Shih, and Nancy Chang. 1996. A stochastic finite-state word-segment-ation algorithm for Chinese, CL.  Richard Sproat (1994) ?English noun-phrase accent prediction for text-to-speech,? Computer Speech and Language, 8, pp. 79-94.  Andreas Stolcke (1998) ?Entropy-based Pruning of Backoff Language Models? Proc. DARPA News Transcription and Understanding Workshop.  Lewis Terman, (1918) ?The vocabulary test as a measure of intelligence,? Journal of Educational Psychology, Vol 9(8), pp. 452-466.  David Yarowsky. 1993. One sense per collocation, HLT.  
144
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 6?9,
Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational Linguistics
The Case for Empiricism (With and Without Statistics) 
 
Kenneth Church 
1101 Kitchawan Road 
Yorktown Heights, NY 10589 
USA 
Kenneth.Ward.Church@gmail.com 
 
  
 
Abstract 
These days we tend to use terms like empirical 
and statistical as if they are interchangeable, but 
it wasn?t always this way, and probably for good 
reason.  In A Pendulum Swung Too Far (Church, 
2011), I argued that graduate programs should 
make room for both Empiricism and Rational-
ism.  We don?t know which trends will dominate 
the field tomorrow, but it is a good bet that it 
won?t be what?s hot today.  We should prepare 
the next generation of students for all possible 
futures, or at least all probable futures.  This pa-
per argues for a diverse interpretation of Empiri-
cism, one that makes room for everything from 
Humanities to Engineering (and then some). 
 
 
   
Figure 1: Lily Wong Fillmore (standing) and 
Charles (Chuck) Fillmore 
 
1 Lifetime Achievement Award (LTA) 
Since the purpose of this workshop is to cele-
brate Charles (Chuck) Fillmore, I would like to 
take this opportunity to summarize some of the 
points that I made in my introduction to Chuck?s 
LTA talk at ACL-2012. 
I had the rather unusual opportunity to see his 
talk (a few times) before writing my introduction 
because Chuck video-taped his talk in advance.1  
I knew that he was unable to make the trip, but I 
had not appreciated just how serious the situation 
was.  I found out well after the fact that the LTA 
meant a lot to him, so much so that he postponed 
an operation that he probably shouldn?t have 
postponed (over his doctor?s objection), so that 
he would be able to answer live questions via 
Skype after the showing of his video tape. 
I started my introduction by crediting Lily 
Wong Fillmore, who understood just how much 
Chuck wanted to be with us in Korea, but also, 
just how impossible that was.  Let me take this 
opportunity to thank her once again for her con-
tributions to the video (technical lighting, edit-
ing, encouragement and so much more). 
For many of us in my generation, C4C, 
Chuck?s ?The Case for Case? (Fillmore, 1968) 
was the introduction to a world beyond Rational-
ism and Chomsky.  This was especially the case 
for me, since I was studying at MIT, where we 
learned many things (but not Empiricism). 
After watching Chuck?s video remarks, I was 
struck by just how nice he was.  He had nice 
things to say about everyone from Noam Chom-
sky to Roger Schank.  But I was also struck by 
just how difficult it was for Chuck to explain 
how important C4C was (or even what it said 
and why it mattered).  To make sure that the in-
ternational audience wasn?t misled by his up-
bringing and his self-deprecating humor, I 
showed a page of ?Minnesota Nice? stereotypes, 
while reminding the audience that stereotypes 
aren?t nice, but as stereotypes go, these stereo-
types are about as nice as they get. 
                                                 
1 The video is available online at 
https://framenet.icsi.berkeley.edu/fndrupal/node/5489.  
6
Chuck, of course, is too nice to mention that 
Fillmore (1967) had 6000 citations in Google 
Scholar as of ACL-2012.2  He also didn?t men-
tion that he has another half dozen papers with 
1000 or more citations including an ACL paper 
on FrameNet (Baker et al, 1998).3 
I encouraged the audience to read C4C.  Not 
only is it an example of a great linguistic argu-
ment, but it also demonstrates a strong command 
of the classic literature as well as linguistic facts.  
Our field is too ?silo?-ed.  We tend to cite recent 
papers by our friends, with too little discussion 
of seminal papers, fields beyond our own, and 
other types of evidence that go beyond the usual 
suspects.  We could use more ?Minnesota Nice.? 
I then spent a few slides trying to connect the 
dots between Chuck?s work and practical engi-
neering apps, suggesting a connection between 
morphology and Message Understanding Con-
ference (MUC)-like tasks.  We tend to think too 
much about parsing (question 1), though ques-
tion 2 is more important for tasks such as infor-
mation extraction and semantic role labeling. 
1. What is the NP (and the VP) under S?  
2. Who did what to whom? 
 
 
 
Figure 2: An example of information extraction 
in commercial practice. 
 
Context-Free Grammars are attractive for lan-
guages with more word order and less morphol-
ogy (such as English), but Case Grammar may 
be more appropriate for languages with more 
morphology and less word order (such as Latin, 
Greek & Japanese).  I then gave a short (over-
simplified) tutorial on Latin and Japanese gram-
mar, suggesting a connection between Latin cas-
es (e.g., nominative, accusative, ablative, etc.) 
and Japanese function words (e.g., the subject 
                                                 
2 Citations tend to increase over time, especially for 
important papers like Fillmore (1967), which has 
more than 7000 citations as of April 2014. 
3 See framenet.icsi.berkeley.edu for more recent pub-
lications such as Ruppenhofer et al (2006). 
marker ga and the direct object marker wo, etc.).  
From there, I mentioned a few historical connec-
tions  
? Case Grammar ? Frames ? FrameNet 
? Valency4 ? Scripts (Roger Schank) 
? Chuck ? Sue Atkins (Lexicography) 
The verb ?give,? for example, requires three 
arguments: Jones (agent) gave money (object) to 
the school (beneficiary).  In Latin, these argu-
ments are associated with different cases (nomi-
native, accusative, etc.).  Under the frame view, 
similar facts are captured with a commercial 
transaction frame, which connects arguments 
across verbs such as: buy, sell, cost and spend.5 
 
V
E
R
B
 
B
U
Y
E
R
 
G
O
O
D
S
 
S
E
L
L
E
R
 
M
O
N
E
Y
 
P
L
A
C
E
 
buy subject object from for at 
sell to     
cost 
indirect 
object 
subject  object at 
spend subject on  object at 
 
Lexicographers such as Sue Atkins use patterns 
such as: 
? Risk <valued object> for <situation> | 
<purpose> | <beneficiary> | <motivation> 
to address similar alternations.  My colleague 
Patrick Hanks uses a similar pattern to motivate 
our work on using statistics to find collocations: 
? Save <good thing> from <bad situation> 
 Lexicographers use patterns like this to account 
for examples such as: 
? Save whales from extinction 
? Ready to risk everything for what he be-
lieves. 
where we can?t swap the arguments: 
? *Save extinction from whales 
The challenge for the next generation is to move 
this discussion from lexicography and general 
linguistics to computational linguistics.   Which 
of these representations are most appropriate for 
practical NLP apps?  Should we focus on part of 
speech tagging statistics, word order or frames 
                                                 
4 http://en.wikipedia.org/wiki/Valency_(linguistics)  
5 For more discussion of this table, see www.uni-
stuttgart.de/ linguistik/ sfb732/ files/ 
hamm_framesemantics.pdf 
7
(typical predicate-argument relations and collo-
cations)? 
Do corpus-based lexicography methods scale 
up?  Are they too manually intensive?  If so, 
could we use machine learning methods to speed 
up manual methods?  Just as statistical parsers 
learn phrase structure rules such as S ? NP VP, 
we may soon expect machine learning systems to 
learn valency, collocations and typical predicate-
argument relations. 
How large do the corpora have to be to learn 
what?  When can we expect to learn frames?   In 
the 1980s, corpora were about 1 million words 
(Brown Corpus).  That was large enough to make 
a list of common content words, and to train part 
of speech taggers.  A decade later, we had 100 
million word corpora such as the British National 
Corpus.  This was large enough to see associa-
tions between common predicates and function 
words such as ?save? + ?from.?  Since then, with 
the web, data has become more and more availa-
ble.  Corpus growth may well be indexed to the 
price of disks (improving about 1000x per dec-
ade).  Coming soon, we can expect 1M2 word 
corpora.  (Google may already be there.)  That 
should be large enough to see associations of 
pairs of content words (collocations).  At that 
point, machine learning methods should be able 
to learn many of the patterns that lexicographers 
have been talking about such as: risk valued ob-
ject for purpose. 
We should train the next generation with the 
technical engineering skills so they will be able 
to take advantage of the opportunities, but more 
importantly, we should encourage the next gen-
eration to read the seminal papers in a broad 
range of disciplines so the next generation will 
know about lots of interesting linguistic patterns 
that will, hopefully, show up in the output of 
their machine learning systems. 
2 Empirical / Corpus-Based Traditions 
As mentioned above, there is a direct connection 
between Fillmore and Corpus-Based Lexicogra-
phers such as Sue Atkins (Fillmore and Atkins, 
1992).  Corpus-based work has a long tradition 
in lexicography, linguistics, psychology and 
computer science, much of which is documented 
in the Newsletter of the International Computer 
Archive of Modern English (ICAME).6  Accord-
ing to Wikipedia,7 ICAME was co-founded by 
                                                 
6 
http://icame.uib.no/archives/No_1_ICAME_News.pdf  
7 http://en.wikipedia.org/wiki/W._Nelson_Francis  
Nelson Francis, who is perhaps best known for 
his collaboration with Henry Ku?era on the 
Brown Corpus.8   The Brown Corpus dates back 
to the 1960s, though the standard reference was 
published two decades later (Francis and Ku?era, 
1982).   
The Brown Corpus has been extremely influ-
ential across a wide range of fields.  According 
to Google Scholar, the Brown Corpus has more 
than 3000 citations.  Many of these references 
have been extremely influential themselves in a 
number of different fields.  At least9 ten of these 
references have at least 2000 citations in at least 
five fields: 
? Information Retrieval (Baeza-Yates and 
Ribeiro-Neto, 1999),  
? Lexicography (Miller, 1995),  
? Sociolinguistics (Biber, 1991),  
? Psychology (MacWhinney, 2000)  
? Computational Linguistics (Marcus et al, 
1993; Jurafsky and Martin, 2000; Church 
and Hanks, 1990; Resnik, 1995) 
All of this work is empirical, though much of 
it is not all that statistical.   The Brown Corpus 
and corpus-based methods have been particularly 
influential in the Humanities, but less so in other 
fields such as Machine Learning and Statistics.  I 
remember giving talks at top engineering univer-
sities and being surprised, when reporting exper-
iments based on the Brown Corpus, that it was 
still necessary in the late 1990s to explain what 
the Brown Corpus was, as well as the research 
direction that it represented.  While many of the-
se top universities were beginning to warm up to 
statistical methods and machine learning, there 
has always been less awareness of empiricism 
and less sympathy for the research direction.  
These days, I fear that the situation has not im-
proved all that much.  In fact, there may be even 
less room than ever for empirical work (unless it 
is statistical). 
It is ironic how much the field has changed 
(and how little it has changed).  Back in the early 
1990s, it was difficult to publish papers that di-
gressed from the strict rationalist tradition that 
dominated the field at the time.  We created the 
Workshop on Very Large Corpora (WVLC 
                                                 
8 http://en.wikipedia.org/wiki/Brown_Corpus  
9 Google Scholar is an amazing resource, but not per-
fect.  There is at least one error of omission: Manning 
and Sch?tze (1999). 
8
evolved into EMNLP) to make room for a little 
work of a different kind.  But over the years, the 
differences between the main ACL conference 
and EMNLP have largely disappeared, and the 
similarities between EMNLP and ICAME have 
also largely disappeared.   While it is nice to see 
the field come together as it has, it is a shame 
that these days, it is still difficult to publish a 
paper that digresses from the strict norms that 
dominate the field today, just as it used to be dif-
ficult years ago to publish papers that digressed 
from the strict norms that dominated the field at 
the time.  Ironically, the names of our meetings 
no longer make much sense.  There is less dis-
cussion than there used to be of the E-word in 
EMNLP and the C-word in WVLC. 
One of the more bitter sweet moments at a 
WVLC/EMNLP meeting was the invited talk by 
Ku?era and Francis at WVLC-1995, 10  which 
happened to be held at MIT.  Just a few years 
earlier, it would have been unimaginable that 
such a talk could have been so appreciated at 
MIT of all places, given so many years of such 
hostility to all things empirical.   
Their talk was the first and last time that I re-
member a standing ovation at WVLC/EMNLP, 
mostly because of their contributions to the field, 
but also because they both stood up for the hour 
during their talk, even though they were well 
past retirement (and standing wasn?t easy, espe-
cially for Francis).   
Unfortunately, while there was widespread 
appreciation for their accomplishments, it was 
difficult for them to appreciate what we were 
doing.  I couldn?t help but notice that Henry was 
trying his best to read other papers in the 
WVLC-1995 program (including one of mine), 
but they didn?t make much sense to him.  It was 
already clear then that the field had taken a hard 
turn away from the Humanities (and C4C and 
FrameNet) toward where we are today (more 
Statistical than Empirical). 
3 Conclusion 
Fads come and fads go, but seminal papers such 
as ?Case for Case? are here to stay.  As men-
tioned above, we should train the next generation 
with the technical engineering skills to take ad-
vantage of the opportunities, but more important-
ly, we should encourage the next generation to 
read seminal papers in a broad range of disci-
                                                 
10 http://aclweb.org/anthology//W/W95/W95-
0100.pdf  
plines so they know about lots of interesting lin-
guistic patterns that will, hopefully, show up in 
the output of their machine learning systems. 
References 
Ricardo Baeza-Yates and Berthier Ribeiro-Neto.1999. 
Modern information retrieval. Vol. 463. ACM 
Press, New York, NY, USA. 
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. ?The berkeley framenet project,? 
ACL.  
Douglas Biber. 1991. Variation across speech and 
writing. Cambridge University Press. 
Kenneth Church. 2011. A pendulum swung too far, 
Linguistic Issues in Language Technology, 6(5). 
Kenneth Church and Patrick Hanks. 1990 "Word as-
sociation norms, mutual information, and lexicog-
raphy." Computational linguistics 16(1): 22-29 
Charles J. Fillmore. 1968. ?The Case for Case.? In 
Bach and Harms (Ed.): Universals in Linguistic 
Theory. Holt, Rinehart, and Winston, New York, 
NY, USA, pp. 1-88. 
Charles J. Fillmore and Beryl TS Atkins. 1992. ?To-
ward a frame-based lexicon: The semantics of 
RISK and its neighbors.? Frames, fields, and con-
trasts, pp. 75-102, Lawrence Erlbaum Associates, 
Hillsdale, NJ, USA. 
W. Nelson Francis and Henry Ku?era. 1982 Frequen-
cy analysis of English usage. Houghton Mifflin, 
Boston, MA, USA. 
Dan Jurafsky and James H. Martin. 2000 Speech & 
Language Processing. Pearson Education India. 
Brian MacWhinney. 2000. The CHILDES Project: 
The database. Vol. 2. Psychology Press. 
Christopher D. Manning and  Hinrich Sch?tze. 
1999. Foundations of statistical natural language 
processing. MIT Press.  Cambridge, MA, USA. 
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and 
Beatrice Santorini. 1993.  "Building a large anno-
tated corpus of English: The Penn Tree-
bank." Computational linguistics 19(2): 313-330. 
George A. Miller. 1995. "WordNet: a lexical database 
for English." Communications of the ACM 38(11): 
39-41. 
Philip Resnik. 1995. "Using information content to 
evaluate semantic similarity in a taxonomy." arXiv 
preprint cmp-lg/9511007 
Josef Ruppenhofer, Michael Ellsworth, Miriam RL 
Petruck, Christopher R. Johnson, and Jan 
Scheffczyk. 2006. FrameNet II: Extended theory 
and practice. framenet.icsi.berkeley.edu 
9
