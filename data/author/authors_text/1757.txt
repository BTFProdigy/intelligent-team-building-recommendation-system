Aspects of Pattern-matching in Data-Oriented Parsing 
Guy De Pauw 
CNTS 
University of Antwert) 
Abst rac t  
Data-Oriented Parsing (DOP) ranks mnong the best pars- 
ing schemes, pairing state-of-the art parsing accuracy to 
the psycholinguistic insight that larger clmnks of syn- 
tactic structures are relevant grammatical and proba- 
bilistic units. Parsing with the DOp-model~ however, 
seems to involve a lot of CPU cycles and a consider- 
able amomtt of double work, brought on by the concept 
of multiple derivations, which is necessary for probabilis- 
tic processing, lint which is not convincingly related to a 
proper linguistic backbone. It is however possible to re- 
interpret he poP-model as a pattern-matching model, 
which tries to maximize the size of the substructures 
that construct the parse, rather than the probability of 
the parse. By emphasizing this memory-based aspect of 
the DoP-model, it is possible to do away with multiple 
derivations, opening up possibilities for efiqcient Viterbi- 
style optimizations, while still retaining acceptable pars- 
ing accuracy through enhanced context-sensitivity. 
1 I n t roduct ion  
The machine learning paradigm of Memory- 
Based Learning, based on the assumpt ion  that 
new problems are solved by direct refbrence to 
stored experiences of previously solved prob- 
lems, has beest successfully applied to a number 
of linguistic phenomena, such as part-of-speech 
tagging, NP-clmnking and stress acquisition 
(consult Daelemans (1999) for an overview). 
To solve these particular problems, linguistic 
information eeded to trigger the correct dis- 
ambiguation, is encoded in a linear feature 
value representation a d presented to a mem- 
ory based learner, such as TiMBL (Daelemans 
et al, 1999). 
Yet, many of the intricacies of the domain of 
syntax do not translate well to a linear repre- 
sentation, so that established MBL-methods are 
necessarily limited to low-level syntactic analy- 
sis, like the atbrementioned NP-chunking task. 
Data Oriented Parsing (Bod, 1999), a state- 
of-the art natural language parsing system, 
translates very well to a Memory Based Learn- 
ing context. This paper describes a re- 
interpretation of the soP-model, in which the 
pattern-match, infl aspects of tim model are ex- 
ploited, so that parses are analyzed by trying 
to match a stew analysis to the largest possible 
substructures recorded in memory. 
A short introduction to Data Oriented Pars- 
ing will be presented in Section 2, followed by an 
explanation of the term pattern-matehin9 in the 
context of this paper. Section 4 describes the 
experimental setup and the corlms. The parsing 
phase that precedes the disambiguation phase 
will be outlined in Section 5 and a description 
of the 3 disambiguating models, POFG, PMPG 
and the combined system PCFG@PMPG (:an be 
found in Sections 6, 7 and 8. 
2 Data  Or iented  Pars ing  
Data Oriented Parsing, originally conceived by 
Remko Scha (Scha, 1990), has been successfully 
applied to syntactic natural language parsing 
by ll,ens Bod (1995), (1999). The aim of Data 
Oriented Parsing (henceforth DOP) is to develop 
a per\[ormanee model of natural anguage, that 
models language use rather than some type of 
competence. It adapts the psycholinguistic in- 
sight that language users analyze sentences us- 
ing previously registered constructions and that 
not only rewrite rules, but cornt)lete substruc- 
tures of any given depth cast be linguistically 
relevant milts tbr parsing. 
2.1 Arehiteeture 
The core of a DOP-system is its TREEBANK: an  
annotated corlms is used to induce all substruc- 
t, ures of arbitrary depth, together with their re- 
spective probabilities, which is a expressed by 
236 
S 
J ~  
NP \111 
I 
Peter 
VP 
killed NP 
i~ raccoo l l  
S NP \;P 
NP VP Peter killed NP 
NI' 
.~l raCCOOll 
Figure 1: Mult iple l)eriw~tions 
its fl:equency in the TREEBANK relative to l;he 
numl)er of substructures with the Sanle root- 
node.  
Figure 1 shows the coral)|nation ol)eral;ion 
that  is needed to tbrm the correct l)arse tree 
for the sentence Peter" killed a raccoon. Given a 
treet)ank of substructures, the systcln tries to 
match the leftmost open nod(; of a substruc- 
ture |;hat is consistent with the parse tree, with 
the top-node of another sul)structur(;, consistent 
with the parse tree. 
Usually, ditferent conlt)inations of sul)struc- 
tllrO.s are possible, as is i~l(ti(:ated in Figure 
1: in the examl)le at the left-hand side the 
tree-structure (:an t)e built l)y (:o11111ining all S- 
structure wil;h a st)coiffed NP a.lld a flllly spe(:i- 
fled vp-structure. The right example shows an- 
other possible Colnl)ination, where a parse tree is 
1)uilt t)y conll)ining the \]ninimal sut)s|;rltcl;ures. 
Nol;e that  t\]\]cse are (:(msisl;(mt wit\]l ol'dinary 
rewrite-rules, such as s -+ NP VP. 
One t)artit:ul;~r 1)~trse tree may t;hus (:()\]lsist ()f 
several (lill.(u'(ml; deriva, t io'n.s..To lind l;hc 1)rot) -
al)ility (If ;I, (terivation, we lnultit)ly tim t)rot)a- 
1)ilities of the substructures thai; were used to 
l.()rm the derivation. To lind the t)robal)ility of 
a parse, we must; in tlrilmit)le sum the t)rol)at)il- 
ities of all its deriw~tions. 
It is COlnl/utationally hardly tra(:tat)h; to COil- 
sider all deriw~tiolls t.()r each pars('. Since 
VITF, RBI ol)timization only su('(:ceds in finding 
the most 1)robal)h'~ (teriw~tion as opposed to the 
most 1)robal)le l)arse, the MONTE CARLO al- 
gorithm is introduced as a proper al)proxima- 
tion I;hat randomly generates a large nlmfl)er of 
deriw~tions. The most prol)al/le l)arse is (:onsi(t- 
ered to be the parse that is most often observed 
in this derivation forest. 
2.2 Exper imenta l  Resu l t s  of  HOP 
The basic 1)op-model, POP1, was testc,(t (111 
a manual ly edited version (if the ATIS-corlnlS 
(Marcus, Sant(lrini, and Marcinkiewicz, 199a). 
The syst;eln was trained on 603 Selltelmes (t)arl; - 
ofstmech tag sequelmes) and (;wfluated on a test 
set (if 75 SCld;ences. Parse accuracy was used as 
an evahlation metric, expressing t;11(; percentage 
of sentences in the test set for which the tlarse 
l)rOl)osed by the system is COlnpletely identi- 
cal to the one in l;lle original eort)us, l)ifl'er- 
eat exl)erilnents were conducted in which max|- 
11111111 sul)structure size was varied. Wi th  DoPl-  
lillfited to a sul)sl;ructure-size (If 1 (equiw~lenl; 
1;O a PCFG), t)arse accuracy is 47%. hi the (/p- 
l;ima\] D()l'-mo(lel, in whi(:h sut)stru(:ture-siz(; is 
1lot limited, a 1)arse accuracy of 85% is (ll)- 
tni\]lc(t. 
2.3 Shor t  Assessment  o f  DOP 
DOI'I in its ot)tinlal fornl achieves a very high 
parse accuarcy. The comt)utational costs of the 
syste111, however, are equally high. Bed (19951 
reported an average t/arse tilne of 3.5 hours 11(;1 . 
Sellte.n(:e. Even though (:urrent 1)arse tilne is 
rcl)ortc.d to l)e 11,or(; reasollal)le, tile oi)timal 
D()P algoril:lml in whi(:h n(/('onstr;dlts are made 
on tll('~ size (1t' sut)structures, nlay not yet 1)e 
tract;able for life-siz( ~. COl'l)()ra. 
In a context-free grammar framework (con- 
sistent with \])()P l imited to a sutlstru(:tm:e-size 
(If 1), there is only (me way a t/arse tree can 
t)e t'ornmd (t'(/1: exalnl/le, the right hand side of 
Figure \]), nleaning that  there is Olfly one del:iva- 
tioll for a given 1)arse tree. This allows efficient 
VITEll.BI style Ol)tillfization. 
To elmo(le (:ontext-sellsitivity in the systeln, 
DOP is tbr(:ed to introduce multiple deriw~tiolls, 
so that repeatedly the same l)arse tree needs to 
1)e g(;lmrated, l)rillging at/(/ut a lot of COl l l \ ] ) l l ta , -  
tional overhead. 
Even though the use of larger syntactic coil- 
texts is highly relewmt fl'om a psycholinguisI,ic 
t)oint-ofview, there is 11o explicit l)reference l)e- 
ing lnade t'(/1' larger substructures in the DOP 
nlodel. While the MONTE CARLO optimizatiolx 
scheme nlaxinlizes the prot)ability of the (teriw> 
tions and seelns to 1)refer derivations nlade up 
of larger substructures, it; may 1)e ild;eresting to 
237 
Disambiguator Parse Accuracy (/562) 
PCFG 
PMPG 
I'CFG@PMP(I 
(a) Correct Analysis 
S 
NP-SBJ VP 
, 
prp 
vbp  NP  
NP PP PP 
dt m~ in NP to NP 
I I 
l t I l  I) \ ]H I  t) 
% F \] Parse Accuracy oi* parsable sentences (/456) % 
373 66.4 83.0 I 373 
327 58.2 75.1 327 
402 71.5 85.2 402 
Table 1: Exl)erimental Results 
(b) PCFG-Analysis 
S 
NP-SB J  VP  
I 
prp  
vbp NP PP PP 
81.8 
71.7 
88.2 
ell; mt in NP to NP 
I I 
ni lp nn  t) 
Figure 2: PCFG Error AnMysis 
see if we can make this assumption explicit. 
3 Pat tern -match ing  
When we look at natural anguage parsing fl:om 
a memory-based point of view, one might say 
that a sentence is analyzed by looking u t) the 
most similar structure for the different analy- 
ses of that sentence in meinory. The parsing 
system described in this paper tries to mimic 
this 1)ehavior by interpreting the pop-model as 
a memory-t)ased model, in which analyses are 
being matched with syntactic patterns recorded 
in memory. Similarity t)etween the proposed 
analysis and tile patterns in memory is com- 
Imted according to: 
? the number of patterns needed to construct 
a tree (to be minimized) 
? the size of the patterns that are used to 
construct a tree (to be maximized) 
Tile nearest neighbor tbr a given analysis can 
be defined as the derivation that shares the 
largest amount of common nodes. 
4 The  exper imenta l  Setup  
10-tbld cross-validation was used to appropri- 
ately evaluate the algorithms, as tile dataset 
(see Section 4.1) is rather small. Like DoPl the 
system is trained and tested on part-of-speech 
tag sequences. In a first phase, a simple bottom- 
up chart parser, trained on the training parti- 
tions, was used to generate parse forests tbr the 
1)art-of speech tag sequences of the test parti- 
tion. Next, the parse tbrests were sent to the 3 
algorithms (hencetbrth the disambiguators) to 
order these parse forests, the first parse of the 
ordered parse forest being the one proposed by 
the disanfl)iguator. 
In this paper, 3 disambiguators are described: 
? PCFG: siml)le Prol)abilistic Context-Free 
(~ra ln lnar  
? PMPG:  the DOP approximation, Patten> 
Matching Probabilistic Grammar 
? PCFGq-PMPG:  a combined system, inte- 
grating PCFG and PMPG 
The evaluation metric used is pars(; accuracy, 
but also tile typical parser evaluation metric F- 
measure (precision/recall) is given ms a means 
of reference to other systems. 
4.1 The Corpus 
The ext)eriments were conducted oil all edited 
version of tile ATIS-II-corpus (Marcus, San- 
torini, and Marcinkiewicz, 1993), which con- 
sists of 578 sentences. Quite a lot of errors and 
inconsistencies were found, but not corrected, 
since we want our (probabilistic) system to be 
238 
l i l le to deal with this kind of noise. Seman- 
ti(:ally oriented tlags like -TMP all(1 -Dill,, lllOSI; 
often used in conjmml;ion with l'p, have been 
renlove(t~ since l;here is no way of rel;rieving this 
kind of semanti(: intbrmation from t;11(; t)art;-o5 
sl)ee(:h tags of the ATIS-(:ortms. Synta(:ti(: flags 
like -sILL on the other hand, \]lave 1)een main- 
taine(t. Internal relations (denoted by llllllleric 
tlags) were removed and tbr 1)ractical reasons, 
scntenee-lellgth was l imited 1;o 15 words max. 
The edited (:orl)us retained 562 sentences. 
5 Pars ing  
As a first phase, a 1)ottom-ut) (:hart parser 
i)al"sed t;he test sol;. This t)roved to t)e quite 
l)rol)lemati(:, since overall, 1()6 out of 562 sen- 
ten(:es (190/(0) could not 1)e t)arsed, (111(', to the 
sl,arsencss of the gramnmr, meanil,g I;ha(; l;he 
at)l)ropriate rewrite rule needed to (:onstru('l; the 
(:orre(:t t)~lrse tree tbr a senten(:c, in the test set, 
wasn't featured in the, in(tu(:ed grammar.  NP- 
annol ;at ; ion seem(~(t 1;o 1)(; t;lle lml, in (:aus(~ \]'or 11n- 
l)arsal)ility. An NP like restriction code AP/57 
is repres(ml;ed 1)y the, rewrite rule: 
NP -~ NN NN sym sym sym C\]) CD 
Highly st)ccitt(: and tint stru(:tur(;s like these 
are s(:ar(:e an(t are usually ll()t induced from the 
training set whell nee(h;d to parse the test set. 
On-going re, sear(:h tries 1;o iml)h;ln(ml; gl"am- 
mal;i(:a.1 SlnOothing ;ts :t soluti(m to |;his 1)rol)hml, 
but one might also (:onsid('a: genera.ling parse 
fol"eSi;S with an in(tep(mdent ~,;l"allllll;Ll', ilMu(:e(l 
fronl the entire (:orlms (training setq-t('~si;s(',l;) or 
a difl'erent corlms. 111 t)()th cases, however, we 
would need to apply 1)robal)ilisti(" smoothing to 
be al)le to assign t)rot)at)ilities to llllkllown s(;,l;llc- 
lures/rules. Neither grammatical ,  nor t)rot)a- 
bilistic smoothing was imt)lemented in the (;ell- 
text of the exl)eriments, (les(:ril)ed in this 1)at)er. 
The sl/ars(mess of the grammar 1)roves t;o l)e 
a serious 1)otl;hme(:k fi)r pars(', a(:(:ura(:y, l imiting 
our (lisamlliguators t;o a maximuln tlarsc act:u- 
racy of 81%. 
6 PCFd-exper iments  
a PCFG constru(:ts parse trees by using simple 
rewrite-rules. The prot)al)ility of ~ parse tree 
(;~7tll })e (:omlml;ed l)y mull;it)lying the t)robat)ili- 
ties (1t" the. rewrite-rules that w(~.re used to (:on- 
st;fuel; the t)ars(:. Note that a l'CFd is i(h;nti(:al 
tO DOP\]  whe l l  we l imit I;he maximum sul)Stl'UC- 
tures size to \], only Mlowing deriwd;ions of the 
type found at the r ight-hand side of Figure 1. 
6.1 Exper imenta l  Resu l t s  
The first line of Tat)le I shows the, rc, sull;s for the 
l'CF(~-(',xl)eriments: 66.4% parse accuracy is an 
adequate result for this baseline model. We also 
look at l)arsc accuracy for parsable sentences 
(an estimal;e of the parse accuracy we 1night 
get if we had a more suited parse forest gener- 
ator) and w(; notice that  we are able to a(:hieve 
a 81.8% parse ae(:ur~my. This is already quite 
high, trot on exmnining the parsed data, serious 
and fluManmntal l imitations to the POPO-mo(lcl 
can be el)served 
6.2 Error Ana lys i s  
Figm'c 2, disl)lays the mosl; common tyl)c of mis- 
take mad(; l)y 1)CFG~S. :\]'lit; (;orr0,cl; t )arse l;ree 
('ouht r(;i)res(mt an mlalysis for 1;11(; senten(:e: 
I ".;ant o, fli.qht f rom \]h'us.scl.s to 2bronto. 
This examt)le shows thai; ~t PCFG h~ls a I;(~,n- 
dency to prctbr tlatter strueture, s over emt)edde, d 
stru(:t;ures. This is a trivial effect of 1;11(; mathc- 
mat;it'll tbrmula used to conqml;e the t)rol)at)il - 
il;y of a I)arse-tr(;(;: emt/cdded structure require 
more r(;writ(' rules, adding more fat:tots to the 
multii)li(:ation , whi(:h will alm(/st ilw, vit~d)ly r(;- 
suit in :t lower l)rol)al)ilit;y. 
11; is all  1111J'()ri;llllal;e 1)r()I)(;rl;y of I'CFG~s t;hal; 
the mmfl)er of no(l(;s in the 1)atse tree is invers(~ly 
1)rot)ortiomd;e to il;s t)rol)al)ility. ()n(; might t)e 
inclin(xl to n(n'malizc a parse tree's pr()bat)ility 
relative t(/the mnnt)er of nodes in the tree, but a 
more linguistically solmd alternative is at hand: 
the enhancenmnt of context sensii;ivity through 
the use of larger synl;tt(:ti(: (:ont(;xt; within t)arse 
tre(:s (:;/,11 make our disaml)iguat;or lnore rolmst. 
7 pMpo-exper iments  
The 1)att(;rn-Matching Prol)al)ilistie Gramnmr 
is a memory-based interpretation of a \])OI'-  
model, in which a s(mtence is analyzed t)y 
matching the largest, possible chunks of syn- 
t;acti(" strut:lure Oll the sentence. To COml)ile 
t/~rse trees into pat, terns, all substructm'es ill
the l;raining set are eneo(ted 1)y assigning l;hem 
specific indexes, NP(o)345 e.g. denotil~g a fully 
specified NP-sl;ruel;urc. This apt)roa(:h was in- 
sl)ired 1)y Goodman (199(i), in which Goodman 
239 
unsuccessflflly uses a system of indexed parse 
trees to transform DOP into aSl equivalent PCFG. 
The system of indexing (which is detailed in De 
Pauw (2000)) used in tim experiments described 
in this paper, is however specifically geared to- 
wards encoding contextual intbnnation i  parse 
trees. 
Gives, an indexed training set, indexes can 
then be matched on a test set parse tree in a 
bottom-up fashion. In the tbllowing example, 
boxed nodes indicate nodes that have been re- 
trieved from memory. 
S 
vp  
prp vbp \ [~ 
dt nn i 
I 
mlp 
In this example we can see that an NP, con- 
sisting of a flflly specified embedded NP and 
l 'P, has l)een completely retrieved from men> 
ory, meaning that the NP in its entirety can 
be observed in the training set. However, no 
vp was tbund that consists of a VBP and that 
particular NP. Disambiguating with PMPG coil- 
sequently involves pruning all nodes retrieved 
frolu i l lei l lory: 
S 
NP-SBJ VP 
vbp NP 
Finally, the probability for this pruned parse 
tree is computed in a pCFO-type manner, not 
adding the retrieved nodes to the product: 
P(parse) = P(s --+ NP-SBJ VP) . P(vp --+ vb I) NP) 
7.1 Exper imenta l  Resu l ts  
The results tbr the PMPG-exI)erinmnts can be 
ibund on the second line of Table 1. On some 
partitions, PMPG pcrtbrmed insignificantly bet- 
ter than PCFG, but Table 1 shows that tile re- 
sults for the context sensitive scheme are much 
worse. 58.2% overall parse accuracy and 71.7% 
parse accuracy on parsable sentences indicates 
that PMPG is *sot a valid approximation of DOP'S 
context-sensitivity. 
7.2 Error Analys is  
The dramatic drop in parsing accuracy calls tbr 
an error analysis of the parsed data. Figure 3 
is a prototypical mistake PMPG has made. The 
correct analysis could represent a parse tree for 
a sentence like: 
What flights can I get f i rm Brussels to 2brvnto. 
The PMPG analysis would never have been 
considered a likely candidate by a common 
PCFG. This particular sentence in fact was ef  
tbrtlessly disambignated by the PCFG . Yet 
the fact that large chunks of tree-structure are 
retrieved Dora memory, make it the preferred 
parse for the PMPG. We notice tbr instance that 
a large part of the sentence can be matched 
on an SBAR structure, which has no relevance 
whatsoever .  
Clearly, PMPG overestimates ubstructure 
size as a feature for disambiguation. It's inter- 
esting however to see that it is a working imple- 
mentation of context sensitivity, eagerly match- 
ing patterns from memory. At the same time, it 
has lost track of common-sense PCFG tactics, it 
is in the combination of the two that one may 
find a decent disambiguator and accurate im- 
plementation of context-sensitivity. 
8 A Combined  System (PMPG@PCFG) 
Table 1 showed that 81.8(/o of the time, a PCFG 
finds the correct parse (Ibr t)arsable sentences), 
meaning that the correct parse is at the first 
place in the ordered parse tbrest. 99% of the 
time, the correct parse can be tbund among the 
10 most probable parses in the ordered pars(; 
forest. This opens up a myriad of possibili- 
ties tbr optin, ization. One might for instance 
use a best-first strategy to generate only the 10 
best parses, significantly reducing parse and dis- 
ambiguation time. An optimized isanNiguator 
might theretbre include a preparatory phase in 
wtfich a common-sense PCFG retains the most 
probable parses, so that a nlore sophisticated 
tbllow-up scheme ,teed not bother with sense- 
less analyses. 
In our experiments, we combined the 
common-sense logic of a PCFG and used its 
output as the PMPG'8 input. This is a well- 
established technique usually refi~rred to as sys- 
tent combination (see van Halteren, Zavrel, and 
Daelemans (1998) for an application of this 
240 
technique to I)art-ofst)ee('h tagging): 
I 
I)CFG 
I 
I, ,nos,: In'obable parso, s \[ 
I 
I'MI'(; 
I 
\]most In'obablc Imrse \[ 
We art'. also presented with th(', possibility to 
assign a weight to each algorithm's decision. 
The probability of a parse can the })e described 
with the following formula: 
I~/,(rewrito,-rule)i 
i 
l)(\]m'""s'(O = (# non-inde, xed nodes),, 
The weight of ea(:h algorithm's (lc(:ision, as 
well as the mnnt)er of 1HOSt )robM)h; parses that 
m:e extrat)olated for the 1)attern-m~tt:hing al o- 
rithnq are parameters to 1)e optimized. Futm:e 
work will include evaluation on a validation set 
to retrieve the ol)timal va, hles for these 1)aram- 
e, tcrs. 
8.1 Resu l ts  
The third line in Tattle 1 shows that the com- 
1)ined system 1)ert'orlns better them either one, 
wit;h a parse accuracy of 71.5% and close I;o 90% 
1)~trs(; at:curacy on t)arsal)l(~ scnt(m(:es, whi(:h w(', 
(-nn consider an at)l)roximat;ion of results rc- 
porteA for DOP1. Error annlysis shows that 
the combined system is ilMe, ed M)Ie to overt:ore(; 
difficulties of both Mgorithms. The examtflo, 
in Figure 2 as well as the, ex~mlple in Figure 
3 were disanllfiguated correctly using the com- 
bined syst(;m 
9 Future  Research  
Even thoug\]l t\]le PMPG shows a lot of promise 
in its parse at:curacy, the following extensions 
ne, ed to be researched: 
Optimizing PMPG@PCFG for comtmta- 
tional etfieieney: the graph in Section 8 
shows a possible optimized parsing system, 
in which a pre-processing POF(I generates 
the n most likely candidates to 1)e extrap- 
olated tbr the actual disantbiguator. Full 
parse forests were generated for the exper- 
iments descrit)e,d in this paper, so that the 
efiiciency gain of such a system Calmot t)e 
prot)erly estimated. 
PMPG@PCFG as all approximation eeds to 
be compm'ed to actual D()P~ by having DOP 
parse the data used in this experiment, and 
by having PMPG-I-I 'CFG parse the data used 
in the exl)erilnents described in Bod (1999). 
The l)ottlelmck of the sparse grammar 
1)roblem prevents us from flflly exploiting 
the disambiguating power of the pattern- 
matching algorithln. The ORAEL-system 
(GRammar Adaptation, Evolution and 
Learning) that is currently being devel- 
olmd , tries to address the t)roblem of gram- 
matical spars(mess by using evolutionary 
te(:lmiques to g('ncrate,, Ol)l;imizo, and com- 
l)lemeld, g~rallllllars. 
10 Conc lus ions  
Even though l)()l'\] exhil)its outstanding pars- 
ing 1)eh~vior, the et|iciency of the model is 
rathe, r problematic. The introduction of mul- 
tit fie deriwd;ions causes a considerable amount 
of computational overhead. Neither is it clear 
how the concept of multiple deriwd;ions trans- 
lal;es to a t)sycholinguistic context: there is no 
proof thai; lmlguage users consider (titf'(;rcnt in- 
st~mtiations of th(; same parse, whmt deciding 
on the correct anMysis for a given sentence. 
A 1)M;tcrn-m~t:chil~g schcnm w~s 1)rcsenLcd 
that tried to dis~mfl)iguate parse forests by 
trying to maximize the size of the sul)strnc- 
tures that can 1)e retrie, ved from inoanory. 
This straightforward memory-based intert)rcta- 
tion yields sut)-standm'd parsing accuracy. But 
the (:oml)ination of common-sense l)robal)ili- 
ties nnd enhanced context-sensitivity provides 
a workM)le t)arse forest disambiguator, indicat- 
ing that language users might exert a COml)lex 
corot)libation of memory-based recollection tech- 
niques and stored statistical data to analyze ut- 
terances. 
References  
Bod, R. 1995. Enriching linguistics with stat:istics: Per- 
fornmnce models of natural anguage. Dissertation, 
II,LC, Univcrsiteit wm Alnsterdanl. 
Bod, l{.ens. 1999. Be, pond Grammar An E:rpericncc- 
Based ~lTu:ory of Language. Cambridge, Fngland: 
Cambridge University Press. 
Daelcmans, W., J. Zavrcl, K. Van der Sloot, and 
A. Van den Bosch. 19!)9. TiMt3L: Tillmrg Memory 
241 
(a) Correct Analysis 
S 
WHNP SQ 
WHNP PP PP 
wdt mm xxx xxx / 
(b) PMPG Analysis 
vbp NP-SBJ VP 
' 
prp 
vb NP PP PP 
xxx in NP to NP 
I I 
nnp mtp 
wdt fills 
S 
NP-SBJ \ [~\ ]  
/ ~ nnp mlp 
xxx  
I NP- 
, 
XXX V 
I 
prp 
F igure  3: PMPG Er ror  Ana lys i s  
Based Learner, version 2.0, reference manual. Tech- 
nical Report ILK-9901, ILK, Tilburg University. 
Daelemans, Walter. 1999. Memory-based language pro- 
ccssing. Journal for Ez'perimcntal nd Theoretical Ar- 
tificial Intelligence, 11:3:287 467. 
De Pauw, Guy. 2000. Probabilistischc Parsers - Con- 
te~:tgcvocligheid cn Pattcrn-Matehin 9. Antwerpen, 
Belgium: Antwerp Papers in Linguistics. 
Goodman, Joshua. 1996. Efficient algorithms for parsing 
the dop model. In Proceedings of the Co@fence on 
Empirical Methods in Natural Language Processing. 
pages 143 152. 
Marcus, M., B. Santorini, and M.A. Marcinkiewicz. 
1993. Building a large amlotatcd corpus of en- 
glish: The Petal Tl-eebank. Computational Lingnis- 
ties, 19(2):313-330. 
Scha, R. 1990. Taaltheorie en taaltectmologie: com- 
petence cn performance. In Q. A. M. dt Kort 
and G. L. J. Lcerdam, editors, Computcrtocpassin- 
.qcn in dc Nccrlandistick, LVVN-jaarboek. Landelijke 
Vereniging van Ncerlandici. 
van Halteren, It., J. Zavrel, and W. Daclemans. 1998. 
Improving data-driven wordclass tagging by system 
combination. In Proceedings of the 36th Annual Meet- 
ing of the Association for Computational Linguistics, 
Montr'eal, Quebec, Canada, pages 491-497, Montreal, 
Canada, August 10-14. 
242 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 19-24, Lisbon, Portugal, 2000. 
The Role of Algorithm Bias vs Information Source in 
Learning Algorithms for Morphosyntactic Disambiguation 
Guy De Pauw and Wal ter  Dae lemans  
CNTS - Language Technology Group 
UIA - University of Antwerp 
Universiteitsplein 1, 2610 Antwerpen, Belgium 
{depauw, daelem}~uia.ua, ac.be 
Abst rac t  
Morphosyntactic Disambiguation (Part of 
Speech tagging) is a useful benchmark problem 
for system comparison because it is typical 
for a large class of Natural Language Process- 
ing (NLP) problems that can be defined as 
disambiguation in local context. This paper 
adds to the literature on the systematic and 
objective evaluation of different methods to 
automatically earn this type of disambiguation 
problem. We systematically compare two 
inductive learning approaches to tagging: MX- 
POST (based on maximum entropy modeling) 
and MBT (based on memory-based learning). 
We investigate the effect of different sources 
of information on accuracy when comparing 
the two approaches under the same conditions. 
Results indicate that earlier observed differ- 
ences in accuracy can be attributed largely to 
differences in information sources used, rather 
than to algorithm bias. 
1 Compar ing  Taggers  
Morphosyntactic Disambiguation (Part of 
Speech tagging) is concerned with assigning 
morpho-syntactic categories (tags) to words in 
a sentence, typically by employing a complex 
interaction of contextual and lexical clues to 
trigger the correct disambiguation. As a con- 
textual clue, we might for instance assume that 
it is unlikely that a verb will follow an article. 
As a lexical (morphological) clue, we might 
assign a word like better the tag comparative if
we notice that its suffix is er. 
POS tagging is a useful first step in text anal- 
ysis, but also a prototypical benchmark task for 
the type of disambiguation problems which is 
paramount in natural anguage processing: as- 
signing one of a set of possible labels to a linguis- 
tic object given different information sources de- 
rived from the linguistic context. Techniques 
working well in the area of POS tagging may 
also work well in a large range of other NLP 
problems such as word sense disambiguation 
and discourse segmentation, when reliable an- 
notated corpora providing good predictive in- 
formation sources for these problems become 
available. 
F inding the information sources relevant for 
solving a particular task, and  optimally inte- 
grating them with an inductive mode l  in a dis- 
ambiguator  has been the basic idea of most  of 
the recent empirical research on this type of 
NLP  prob lems and  part of speech tagging \] in 
particular. 
It is unfortunate, however, that this line of re- 
search most  often refrains f rom investigating the 
role of each component  proper, so that it is not 
always clear whether  differences in accuracy are 
due to inherent bias in the learning algorithms 
used, or to different sources of information used 
by the algorithms. 
This paper  expands  on an empirical compar-  
ison (van Halteren et al, 1998) in which TRI- 
GRAM tagging, BR ILL  tagging, MAXIMUM EN-  
TROPY and MEMORY BASED tagging were com- 
pared on the LOB corpus. We will provide a 
more  detailed and  systematic compar ison be- 
tween MAXIMUM ENTROPY MODEL ING (aatna-  
parkhi, 1996) and  MEMORY BASED LEARNING 
(Dae lemans  et al, 1996) for morpho-syntact ic 
d isambiguat ion and  we investigate whether  ear- 
lier observed differences in tagging accuracy can 
be attributed to algorithm bias, information 
source issues or both. 
1See van Halteren (ed.) (1999) for a comprehensive 
overview of work on morphosyntactic disambiguation, 
including empirical approaches. 
19 
After a brief introduction of the 2 algorithms 
used in the comparison (Section 2), we will 
outline the experimental setup in Section 3. 
Next we compare both algorithms on respec- 
tively typical MSW-features (Section 4) and typ- 
ical MXPOSW-features (Section 5), followed by 
a brief error analysis and some concluding re- 
marks. 
2 A lgor i thms and  Implementat ion  
In this Section, we provide a short description 
of the two learning methods we used and their 
associated implementations. 
2.1 Memory-Based Learning 
Memory-Based Learning is based on the as- 
sumption that new problems are solved by 
direct reference to stored experiences of pre- 
viously solved problems, instead of by refer- 
ence to rules or other knowledge structures 
extracted from those experiences (Stanfill and 
Waltz, 1986). A memory-based (case-based) 
approach to tagging has been investigated in 
Cardie (1994) and Daelemans et al (1996). 
Implementat ion 
For our experiments we have used TIMBL 2 
(Daelemans et al, 1999a). TIMBL includes a 
number of algorithmic variants and parameters. 
The base model (ISl) defines the distance be- 
tween a test item and each memory item as 
the number of features for which they have a 
different value. Information gain can be intro- 
duced ( IB i - IG)  to weigh the cost of a feature 
value mismatch. The heuristic approximation 
of computationally expensive pure MBL vari- 
ants, (IGTREE), creates an oblivious decision 
tree with features as tests, ordered according 
to information gain of features. The number of 
nearest neighbors that are taken into account 
for extrapolation, can be determined with the 
parameter K.
For typical symbolic (nominal) features, val- 
ues are not ordered. In the previous variants, 
mismatches between values are all interpreted 
as equally important, regardless of how similar 
(in terms of classification behavior) the values 
are. We adopted the modified value difference 
metric (MVDM) to assign a different distance be- 
tween each pair of values of the same feature. 
2TIMBL is available from: http://ilk.kub.nl/ 
For more references and information about 
these algorithms we refer to Daelemans et al 
(1999a). 
2.2 Max imum Entropy 
In this classification-based approach, diverse 
sources of information are combined in an expo- 
nential statistical model that computes weights 
(parameters) for all features by iteratively max- 
imizing the likelihood of the training data. The 
binary features act as constraints for the model. 
The general idea of maximum entropy model- 
ing is to construct a model that meets these 
constraints but is otherwise as uniform as pos- 
sible. A good introduction to the paradigm of 
maximum entropy can be found in Berger et al 
(1996). 
MXPOST (Ratnaparkhi, 1996) applied maxi- 
mum Entropy learning to the tagging problem. 
The binary features of the statistical model are 
defined on the linguistic context of the word 
to be disambiguated (two positions to the left, 
two positions to the right) given the tag of 
the word. Information sources used include the 
words themselves, the tag of the previous words, 
and for unknown words: prefix letters, suffix 
letters, and information about whether a word 
contains a number, an upcase character, or a 
hyphen. These are the primitive information 
sources which are combined uring feature gen- 
eration. 
In tagging an unseen sentence, a beam search 
is used to find the sequence of tags with the 
highest probability, using binary features ex- 
tracted from the context to predict the most 
probable tags for each word. 
Implementat ion  
For our experiments, we used MACCENT, an  
implementation f maximum entropy modeling 
that allows symbolic features as input. 3 The 
package takes care of the translation of sym- 
bolic values to binary feature vectors, and im- 
plements the iterative scaling approach to find 
the probabilistic model. The only parameters 
that are available in the current version are the 
maximum number of iterations and a value fre- 
quency threshold which is set to 2 by default 
(values occurring only once are not taken into 
account). 
aDetails on how to obtain MACCENT can be found on: 
http://www.cs.kuleuven.ac.be/-ldh/ 
20 
3 Exper imenta l  Setup  
We have set up the experiments in such a way 
that neither tagger is given an unfair advantage 
over the other. The output of the actual taggers 
(MBT and MXPOST) is not suitable to study the 
proper effect of the relevant issues of informa- 
tion source and algorithmic parameterisation, 
since different information sources are used for 
each tagger. Therefore the taggers need to be 
emulated using symbolic learners and a prepro- 
cessing front-end to translate the corpus data 
into feature value vectors. 
The tagging experiments were performed on 
the LOB-corpus (Johansson et al 1986). The 
corpus was divided into 3 partitions: an 80% 
training partition, consisting of 931.062 words, 
and two 10% partitions: the VALIDATION SET 
(114.479 words) and the TEST SET (115.101 
words) on which the learning algorithms were 
evaluated. 
The comparison was done in both direc- 
tions: we compared both systems using infor- 
mation sources as described in Daelemans et al 
(1996) as well as those described in Ratnaparkhi 
(1996). 
Corpus  P reprocess ing  
Since the implementations of both learning al- 
gorithms take propositional data as their input 
(feature-value pairs), it is necessary to translate 
the corpora into this format first. This can be 
done in a fairly straightforward manner, as is il- 
lustrated in Tables 1 and 2 for the sentence She 
looked him up and down. 
word d 
She * 
looked PP3A 
him VBD 
up PP30 
and RP 
down CC 
RP 
f a 
PP3A VBD-VBN 
VBD-VBN PP30 
PP30 RP-IN 
RP-IN CC 
CC RP 
RP SPER 
SPER * 
value 
PP3A 
VBD 
PP30 
RP 
CC 
RP 
SPER 
Table 1: Contextual features 
The disambiguation of known words is usu- 
ally based on contextual features. A word is 
considered to be known when it has an ambigu- 
ous tag (henceforth ambitag) attr ibuted to it in 
the LEXICON, which is compiled in the same way 
as for the MBT-tagger (Daelemans et al, 1996). 
A lexicon entry like telephone for example car- 
ries the ambitag NN-VB, meaning that it was 
observed in the training data as a noun or a 
verb and that it has more often been observed 
as a noun (frequency being expressed by order). 
Surrounding context for the focus word (fi are 
disambiguated tags (a 0 on the left-hand side and 
ambiguous tags (a) on the right-hand side. 
In order to avoid the unrealistic situation that 
all disambiguated tags assigned to the left con- 
text of the target word are correct, we simulated 
a realistic situation by tagging the validation 
and test set with a trained memory-based or 
maximum entropy tagger (trained on the train- 
ing set), and using the tags predicted by this 
tagger as left context tags. 
word p s s s c h 
She S S h e T F 
looked 1 k e d F F 
him h h i m F F 
up u * u p F F 
and a a n d F F 
down d o w n F F 
? * F F 
Table 2: Morphological features 
Unknown words need more specific word-form 
information to trigger the correct disambigua- 
tion. Prefix-letters (p), suffix-letters (s), the oc- 
currence of a hyphen (h) or a capital (c) are all 
considered to be relevant features for the dis- 
ambiguation of unknown words. 
4 Us ing  MBT- type  features  
This section describes tagging experiments for 
both algorithms using features as described in 
Daelemans et al (1996). A large number of 
experiments were done to find the most suitable 
feature selection for each algorithm, the most 
relevant results of which axe presented here. 
Va l idat ion  Phase  
In the validation phase, both learning algo- 
rithms iteratively exhaust different feature com- 
binations on the VALIDATION SET, as well as 
leaxner-specific parameterisations. For each al- 
gorithm, we try all feature combinations that 
hardware restrictions allow: we confined our- 
selves to a context of maximum 6 surrounding 
21 
Known Words % f df fa dfa ddfaa dddfaaa 
TIMBL IGTREE 92.5 95.1 95.9 97.2 97.2 97.2 
TIMBL IB1 92.5 95.1 95.9 97.2 97.4 97.3 
TIMBL IBi K----5 92.5 95.1 95.6 93.8 96.4 97.0 
TIMBL IB1 K----10 92.5 95.1 95.6 93.4 93.7 96.1 
TIMBL MVDM 92.5 95.1 95.9 97.4 97.4 97.2 
TIMBL MVDM K=5 92.5 95.1 95.2 97.5 97.5 97.4 
TIMBL MVDM K----10 92.5 95.1 94.9 97.5 97.5 97.3 
MACCENT 92.5 94.5 95.8 97.5 97.6 97.4 
Unknown Words % ddaap ddaas ddaaps ddaapss ddaapsscn ddaapsshcn 
TIMBL IGTREE 42.1 65.9 65.2 65.8 68.6 70.0 
TIMBL ml 53.8 63.7 66.3 68.3 68.8 70.7 
TIMBL IB1 K----5 54.2 61.6 66.7 71.4 72.5 774.3 
TIMBL IB1 K----10 49.5 55.3 64.2 68.4 70.3 72.7 
TIMBL MVDM 58.1 72.0 70.9 75.1 71.0 73.3 
TIMBL MVDM K----5 61.2 72.0 75.6 79.7 75.5 77.6 
TIMBL MVDM K----10 61.7 72.7 76.0 '79.7 77.1 77.9 
MACCENT 61.8 67.0 74.8 '78.6 75.3 77.0 
Table 3: Validation Phase Results 
tags or less, since we already noticed perfor- 
mance degradation for both systems when us- 
ing a context of more than 5 surrounding tags. 
For unknown words, we have to discern between 
2 different tuning phases. First, we find the 
optimal contextual feature set, next the opti- 
mal morphological features, presupposing both 
types of features operate independently. 
We investigate seven of the variations of 
Memory-Based Learning available in TIMBL (see 
Daelemans et al (1999b) for details) and one 
instantiation of maccent, since the current ver- 
sion does not implement many variations. 
A summary of the most relevant results of 
the validation phase can be found in Table 3. 
The result of the absolute optimal feature set 
for each algorithm is indicated in bold. For 
some contexts, we observe a big difference be- 
tween IGTREE and IBi-IG and IB1-MVDM. For 
unknown words, the abstraction made by the 
mWREE-algorithm seems to be quite harmful 
compared to the true lazy learning of the other 
variants (see Daelemans et al (1999b) for a pos- 
sible explanation for this type of behaviour). 
Of all algorithms, Maximum Entropy has the 
highest agging accuracy for known words, out- 
performing TIMBL-algorithms however by only 
a very small margin. The overall optimal con- 
text for the algorithms turned out to be dfa and 
ddfaa respectively, while enlarging the context 
on either side of the focus word resulted in a 
lower tagging accuracy. 
Overall, we noticed a tendency for TIMBL to 
perform better when the information source is 
rather limited (i.e. when few features are used), 
while MACCENT seems more robust when deal- 
ing with a more elaborate feature space. 
Test Phase  
The Test Phase of the experiment consists of 
running the optimised subalgorithm paired with 
the optimal feature set on the test set. TIMBL, 
augmented with the Modified Value Difference 
Metric and k set to 5, was used to disambiguate 
known words with a dfa feature value, unknown 
words with the features ddaapss. MACCENT 
used the same features for unknown words, but 
used more elaborate features (ddfaa) to disam- 
biguate known words. The results of the opti- 
mised algorithms on the test set can be found 
in Table 4. 
TIMBL MACCENT 
Known Words 97.6 97.7 
Unknown Words 77.3 78.2 
Total 97.2 97.2 
Sentence 62.7 63.5 
Table 4: Test results with MBT features 
Overall tagging accuracy is similar for both 
algorithms, indicating that for the overall tag- 
ging problem, the careful selection of optimal 
information sources in a validation phase, has 
a bigger influence on accuracy than inherent 
properties or bias of the two learning algorithms 
22 
Algorithm Accuracy (%) on test set 
IGTREE K----1 94.3 
T IMBLMVDMK:5  92.8 
Maccent 94.3 
Maccent Beam(n=5) 94.3 
Table 5: Test results with MXPOST features 
tested. 
Beam Search 
Note that MACCENT does not include the beam 
search over N highest probability tag sequence 
candidates at sentence level, which is part of 
the MXPOST tagger (but not part of maximum 
entropy-based learning proper; it could be com- 
bined with MBL as well). To make sure that 
this omission does not affect maximum entropy 
learning adversely for this task, we implemented 
the beam search, and compared the results with 
the condition in which the most probable tag 
is used, for different beam sizes and different 
amounts of training data. The differences in 
accuracy were statistically not significant (beam 
search even turned out to be significantly worse 
for small training sets). The beam search very 
rarely changes the probability order suggested 
by MACCENT, and when it does, the number of 
times the suggested change is correct is about 
equal to the number of times the change is 
wrong. This is in contrast with the results of 
Ratnaparkhi (1996), and will be investigated 
further in future research. 
5 Us ing  MXPOST-type features  
In order to complete the systematic ompari- 
son, we compared maximum entropy (again us- 
ing the MACCENT implementation) with MBL 
when using the features uggested in (Ratna- 
parkhi, 1996). Due to the computational ex- 
pense of the iterative scaling method that is in- 
herent o maximum entropy learning, it was not 
tractable to incorporate an extensive validation 
phase for feature selection or algorithmic vari- 
ant selection. We simply took the features ug- 
gested in that paper, and 2 different settings for 
our MBL implementation, IGTREE and MVDM 
K----5, the latter being the optimal algorithm for 
the previous experiments. The results on the 
test set are shown in Table 5. 
Beam search 
Notice that again, the sentence level beam 
search does not add significantly to accuracy. 
Also note that the results report in Table 5 dif- 
fer significantly from those reported for MXPOST 
in (van Halteren et al, 1998). The difference in 
tagging accuracy is most likely due to the prob- 
lematic translation of MXPOST'S binary features 
to nominal features. This involves creating in- 
stances with a fixed number of features (not just 
the active features for the instance as is the 
case in MXPOST),  resulting in a bigger, less 
manageable instance space. When IGTREE com- 
presses the elaborate instance space, we conse- 
quently notice a significant improvement over a 
MVDM approach. 
6 Error  Ana lys i s  
The following table contains ome more detailed 
information about the distribution of the er- 
rors 4: 
Known Unknown 
Both wrong - same tag 1384 335 
Both Wrong - different ag 117 130 
Only MACCENT Wrong 1008 181 
Only TIMBL Wrong 1103 193 
In 87% of the cases where both algorithms are 
wrong, they assign the same tag to a word. This 
indicates that about 55% of the errors can either 
be attributed to a general shortcoming present 
in both algorithms or to an inadequate informa- 
tion source. We can also state that 97.8% of the 
time, the two algorithms agree on which tag to 
assign to a word (even though they both agree 
on the wrong tag 1.7% of the time). 
We also observed the same (erroneous) tag- 
ging behavior in both algorithms for lower- 
frequency tags, the interchanging of noun tags 
and adjective tags, past tense tags and past par- 
ticiple tags and the like. 
Another issue is the information value of the 
ambitag. We have observed several cases where 
the correct ag was not in the distribution spec- 
ified by the ambitag, which has substantial in- 
formation value. In our test set, this is the 
case for 1235 words (not considering unknown 
words). 553 times, neither algorithm finds the 
correct ag. Differences can be observed in the 
4The error analysis described in this Section, is based 
on the first set of experiments in which MBT-features 
were used to disambiguate he test set. 
23 
way the algorithms deal with the information 
value of the ambitag, with Maximum Entropy 
exhibiting a more conservative approach with 
respect to the distribution suggested by the am- 
bitag, more reluctant to break free from the am- 
bitag. It only finds the correct part-of-speech 
tag 507 times, whereas TiMBL performs better 
at 594 correct ags. There is a downside to this: 
sometimes the correct ag is featured in the am- 
bitag, but the algorithm breaks free from the 
ambitag nevertheless. This happens to TiMBL 
267 times, and 288 times to MACCENT. 
In any case, the construction of the ambitag 
seems to be a problematic ssue that needs to be 
resolved, since its problematic nature accounts 
for almost 40% of all tagging errors. This is 
especially a problem for MBT as it relies on am- 
bitags in its representation. 
7 Concluding Remarks 
A systematic omparison between two state- 
of-the-art agging systems (maximum entropy 
and memory-based learning) was presented. By 
carefully controlling the information sources 
available to the learning algorithms when used 
as a tagger generator, we were able to show that, 
although there certainly are differences between 
the inherent bias of the algorithms, these differ- 
ences account for less variability in tagging ac- 
curacy than suggested in previous comparisons 
(e.g. van Halteren et al (1998)). 
Even though overall tagging accuracy of both 
learning algorithms turns out to be very similar, 
differences can be observed in terms of accuracy 
on known and unknown words separately, but 
also in the differences in the (erroneous) tagging 
behaviour the two learning algorithms exhibit. 
Furthermore, evidence can be found that 
given the same information source, different 
learning algorithms, and also different instan- 
tiations of the same learning algorithm, yield 
small, but significant differences in tagging ac- 
curacy. This may be in line with theoretical 
work by Roth (1998);Roth (1999) in which both 
maximum entropy modeling and memory-based 
learning (among other learning algorithms) are 
shown to search for a decision surface which is a 
linear function in the feature space. The results 
put forward in this paper support the claim 
that, although the linear separator found can 
be different for different learning algorithms, the 
feature space used is more important. 
We also showed that which information 
sources, algorithmic parameters, and even algo- 
rithm variants are optimal depends on a com- 
plex interaction of learning algorithm, task, and 
data set, and should accordingly be decided 
upon by cross-validation. 
References 
Adam Berger, Stephen Della Pietra, and Vincent 
Della Pietra. 1996. Maximum Entropy Approach 
to Natural Language Processing. Computational 
Linguistics, 22(1). 
C. Caxdie. 1994. Domain Specific Knowledge Acqui- 
sition for Conceptual Sentence Analysis. Ph.D. 
thesis, University of Massachusets, Amherst, MA. 
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 
1996. MBT: A memory-based part of speech tag- 
ger generator. In E. Ejerhed and I. Dagan, ed- 
itors, Proc. of Fourth Workshop on Very Large 
Corpora, pages 14-27. ACL SIGDAT. 
W. Daelemans, A. Van den Bosch, and J. Zavrel. 
1999a. Forgetting exceptions i harmful in lan- 
guage learning. Machine Learning, Special issue 
on Natural Language Learning, 34:11-41. 
W. Daelemans, J. Zavrel, K. Van der Sloot, and 
A. Van den Bosch. 1999b. TiMBL: Tilburg Mem- 
ory Based Learner, version 2.0, reference manual. 
Technical Report ILK-9901, ILK, Tilburg Univer- 
sity. 
A. Ratnaparkhi. 1996. A maximum entropy part-of- 
speech tagger. In Proc. of the Conference on Em- 
pirical Methods in Natural Language Processing, 
May 1Y-18, 1996, University of Pennsylvania. 
Dan Roth. 1998. Learning to resolve natural an- 
guage ambiguities: A unified approach. In Pro- 
ceedings of the 15th National Conference on Ar- 
tificial Intelligence (AAAI-98) and of the lOth 
Conference on Innovative Applications of Artifi- 
cial Intelligence (IAAI-98), pages 806-813, Menlo 
Park, July 26-30. AAAI Press. 
Dan Roth. 1999. Learning in natural language. In 
Proceedings of the 16th Joint Conference on Arti- 
ficial Intelligence. 
C. Stanfill and D. Waltz. 1986. Toward memory- 
based reasoning. Communications of the ACM, 
29(12):1213-1228, December. 
H. van Halteren, J. Zavrel, and W. Daelemans. 
1998. Improving data-driven wordclass tagging by 
system combination. In Proceedings of the 36th 
Annual Meeting of the Association for Compu- 
tational Linguistics, Montr'eal, Quebec, Canada, 
pages 491-497, Montreal, Canada, August 10-14. 
H. van Halteren (ed.). 1999. Syntactic Word- 
class Tagging. Kluwer Academic Publishers, Dor- 
drecht, The Netherlands. 
24 
A Comparison of Two Different Approaches
to Morphological Analysis of Dutch
Guy De Pauw1, Tom Laureys2, Walter Daelemans1, Hugo Van hamme2
1 University of Antwerp 2 K.U.Leuven
CNTS - Language Technology Group ESAT
Universiteitsplein 1 Kasteelpark Arenberg 10
2610 Antwerpen (Belgium) 3001 Leuven (Belgium)
firstname.lastname@ua.ac.be firstname.lastname@esat.kuleuven.ac.be
Abstract
This paper compares two systems for computa-
tional morphological analysis of Dutch. Both
systems have been independently designed as
separate modules in the context of the FLa-
VoR project, which aims to develop a modular
architecture for automatic speech recognition.
The systems are trained and tested on the same
Dutch morphological database (CELEX), and
can thus be objectively compared as morpho-
logical analyzers in their own right.
1 Introduction
For many NLP and speech processing tasks, an
extensive and rich lexical database is essential.
Even a simple word list can often constitute
an invaluable information source. One of the
most challenging problems with lexicons is the
issue of out-of-vocabulary words. Especially for
languages that have a richer morphology such
as German and Dutch, it is often unfeasible to
build a lexicon that covers a sufficient number
of items. We can however go a long way into
resolving this issue by accounting for novel pro-
ductions through the use of a limited lexicon
and a morphological system.
This paper describes two systems for morpho-
logical analysis of Dutch. They are conceived as
part of a morpho-syntactic language model for
inclusion in a modular speech recognition engine
being developed in the context of the FLaVoR
project (Demuynck et al, 2003). The FLaVoR
project investigates the feasibility of using pow-
erful linguistic information in the recognition
process. It is generally acknowledged that more
accurate linguistic knowledge sources improve
on speech recognition accuracy, but are only
rarely incorporated into the recognition process
(Rosenfeld, 2000). This is due to the fact that
the architecture of most current speech recog-
nition systems requires all knowledge sources to
be compiled into the recognition process at run
time, making it virtually impossible to include
extensive language models into the process.
The FLaVoR project tries to overcome this
restriction by using a more flexible architec-
ture in which the search engine is split into
two layers: an acoustic-phonemic decoding layer
and a word decoding layer. The reduction in
data flow performed by the first layer allows for
more complex linguistic information in the word
decoding layer. Both morpho-phonological
and morpho-syntactic modules function in the
word decoding process. Here we focus on the
morpho-syntactic model which, apart from as-
signing a probability to word strings, provides
(scored) morphological analyses of word can-
didates. This morphological analysis can help
overcome the previously mentioned problem of
out-of-vocabulary words, as well as enhance the
granularity of the speech recognizer?s language
model.
Successful experiments on introducing mor-
phology into a speech recognition system have
recently been reported for the morphologically
rich languages of Finnish (Siivola et al, 2003)
and Hungarian (Szarvas and Furui, 2003), so
that significant advances can be expected for
FLaVoR?s target language Dutch as well. But
as the modular nature of the FLaVoR architec-
ture requires the modules to function as stand-
alone systems, we are also able to evaluate and
compare the modules more generally as mor-
phological analyzers in their own right, which
can be used in a wide range of natural lan-
guage applications such as information retrieval
or spell checking.
In this paper, we describe and evaluate these
two independently developed systems for mor-
phological analysis: one system uses a machine
learning approach for morphological analysis,
while the other system employs finite state tech-
niques. After looking at some of the issues when
dealing with Dutch morphology in section 2, we
discuss the architecture of the machine learn-
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
ing approach in section 3, followed by the finite
state method in section 4. We discuss and com-
pare the results in section 5, after which we draw
conclusions.
2 Dutch Morphology: Issues and
Resources
Dutch can be situated between English and Ger-
man if we define a scale of morphological rich-
ness in Germanic languages. It lacks certain
aspects of the rich inflectional system of Ger-
man, but features a more extensive inflection,
conjugation and derivation system than En-
glish. Contrary to English, Dutch for instance
includes a set of diminutive suffixes: e.g. ap-
pel+tje (little apple) and has a larger set of suf-
fixes to handle conjugation.
Compounding in Dutch can occur in dif-
ferent ways: words can simply be concate-
nated (e.g. plaats+bewijs (seat ticket)), they
can be conjoined using the ?s? infix (e.g. toe-
gang+s+bewijs (entrance ticket)) or the ?e(n)?
infix (e.g. fles+en+mand (bottle basket)). In
Dutch affixes are used to produce derivations:
e.g. aanvaard+ing (accept-ance).
Morphological processes in Dutch account for
a wide range of spelling alternations. For in-
stance: a syllable containing a long vowel is
written with two vowels in a closed syllable (e.g.
poot (paw)) or with one vowel in an open syl-
lable (e.g. poten (paws)). Consonants in the
coda of a syllable can become voiced (e.g. huis -
huizen (house(s)) or doubled (e.g. kip - kippen
(chicken(s))). These and other types of spelling
alternations make morphological segmentation
of Dutch word forms a challenging task. It
is therefore not surprising to find that only a
handful of research efforts have been attempted.
(Heemskerk, 1993; Dehaspe et al, 1995; Van
den Bosch et al, 1996; Van den Bosch and
Daelemans, 1999; Laureys et al, 2002). This
limited number may to some extent also be due
to the limited amount of Dutch morphological
resources available.
The Morphological Database of CELEX
Currently, CELEX is the only extensive and
publicly available morphological database for
Dutch (Baayen et al, 1995). Unfortunately,
this database is not readily applicable as an in-
formation source in a practical system due to
both a considerable amount of annotation er-
rors and a number of practical considerations.
Since both of the systems described in this pa-
per are data-driven in nature, we decided to
semi-automatically make some adjustments to
allow for more streamlined processing. A full
overview of these adjustments can be found in
(Laureys et al, 2004) but we point out some of
the major problems that were rectified:
? Annotation of diminutive suffix and unan-
alyzed plurals and participles was added
(e.g. appel+tje).
? Inconsistent treatment of several suffixes
was resolved (e.g. acrobaat+isch (acro-
bat+ic) vs. agnostisch (agnostic)).
? Truncation operations were removed
(e.g. filosoof+(isch+)ie (philosophy)).
The Task: Morphological Segmentation
The morphological database of CELEX con-
tains hierarchically structured and fully tagged
morphological analyses, such as the following
analysis for the word ?arbeidsfilosofie? (labor
philosophy):
N





H
HH
H
HH
H
N
arbeid
N?N.N
s
N


H
H
N
filosoof
N?N.
ie
The systems described in this paper deal with
the most important subtask of morphological
analysis: segmentation, i.e. breaking up a word
into its respective morphemes. This type of
analysis typically also requires the modeling of
the previously mentioned spelling changes, ex-
emplified in the above example (arbeidsfilosofie
? arbeid+s+filosoof+ie). In the next 2 sec-
tions, we will describe two different approaches
to the segmentation/alternation task: one us-
ing a machine-learning method, the other using
finite state techniques. Both systems however
were trained and tested on the same data, i.e.
the Dutch morphological database of CELEX.
3 A Machine Learning Approach
One of the most notable research efforts model-
ing Dutch morphology can be found in Van den
Bosch and Daelemans (1999). Van den Bosch
and Daelemans (1999) define morphological
analysis as a classification task that can be
learned from labeled data. This is accomplished
at the level of the grapheme by recording a local
context of five letters before and after the focus
letter and associating this context with a mor-
phological classification which not only predicts
a segmentation decision, but also a graphemic
(alternation) and hierarchical mapping.
The system described in Van den Bosch
and Daelemans (1999) employs the ib1-ig
memory-based learning algorithm, which uses
information-gain to attribute weighting to the
features. Using this method, the system is able
to achieve an accuracy of 64.6% of correctly an-
alyzed word forms. On the segmentation task
alone, the system achieves a 90.7% accuracy of
correctly segmented words. On the morpheme
level, a 94.5% F-score is observed.
Towards a Cascaded Alternative
The machine learning approach to morpholog-
ical analysis described in this paper is inspired
by the method outlined in Van den Bosch and
Daelemans (1999), but with some notable differ-
ences. The first difference is the data set used:
rather than using the extended morphological
database, we concentrated on the database ex-
cluding inflections and conjugated forms. These
morphological processes are to a great extent
regular in Dutch. As derivation and compound-
ing pose the most challenging task when mod-
eling Dutch morphology, we have opted to con-
centrate on those processes first. This allows us
to evaluate the systems with a clearer insight
into the quality of the morphological analyzers
with respect to the hardest tasks.
Further, the systems described in this paper
use the adjusted version of CELEX described in
section 2, instead of the original dataset. The
main reason for this can be situated in the con-
text of the FLaVoR project: since our mor-
phological analyzer needs to operate within a
speech recognition engine, it is paramount that
our analyzers do not have to deal with truncated
forms, as it would require us to hypothesize
unrealized forms in the acoustic input stream.
Even though using the modified dataset does
not affect the general applicability of the mor-
phological analyzer itself, it does entail that a
direct comparison with the results in Van den
Bosch and Daelemans (1999) is not possible.
The overall design of our memory-based sys-
tem for morphological analysis differs from the
one described in Van den Bosch and Daelemans
(1999) as our approach takes a more traditional
stance with respect to classification. Rather
than encoding different types of classification
in conglomerate classes, we have set up a cas-
caded approach in which each classification task
(spelling alternation, segmentation) is handled
separately. This allows us to identify problems
at each point in the task and enables us to op-
timize each classification step accordingly. To
avoid percolation of bad classification decisions
at one point throughout the entire classifica-
tion cascade, we ensure that all solutions are re-
tained throughout the entire process, effectively
turning later classification steps into re-ranking
mechanisms.
Alternation
The input of the morphological analyzer is a
word form such as ?arbeidsfilosofie?. As a first
step to arrive at the desired segmented output
?arbeid+s+filosoof+ie?, we need to account for
the spelling alternation. This is done as a pre-
cursor to the segmentation task, since prelimi-
nary experiments showed that segmentation on
a word form like ?arbeidsfilosoofie? is easier to
model accurately than segmentation on the ini-
tial word form.
First, we record all possible alternations on
the training set. These range from general al-
ternations like doubling the vowel of the last
syllable (e.g. arbeidsfilosoof) to very detailed,
almost word-specific alternations (e.g. Europa
? euro). Next, these alternations in the train-
ing set are annotated and an instance base is ex-
tracted. Table 1 shows an example of instances
for the word ?aanbidder? (admirer). In this ex-
ample we see that alternation number 3 is asso-
ciated with the double ?d?, denoting a deletion
of that particular letter.
Precision Recall F-score
MBL 80.37% 88.12% 84.07%
Table 2: Results for alternation experiments
These instances were used to train the
memory-based learner TiMBL (Daelemans et
al., 2003). Table 2 displays the results for the
alternation task on the test set. Even though
these appear quite modest, the only restriction
we face with respect to consecutive processing
steps lies in the recall value. The results show
that 255 out of 2,146 alternations in the test set
were not retrieved. This means that we will not
be able to correctly analyze 2.27% of the test
set (which contains 11,256 items).
Left Context Focus Right Context Combined Class
- - - - - a a n b i d ?a -aa aan 0
- - - - a a n b i d d -aa aan anb 0
- - - a a n b i d d e aan anb nbi 0
- - a a n b i d d e r anb nbi bid 0
- a a n b i d d e r - nbi bid idd 0
a a n b i d d e r - - bid idd dde 0
a n b i d d e r - - - idd dde der 3
n b i d d e r - - - - dde der er- 0
b i d d e r - - - - - der er- r? 0
Table 1: Alternation instances for ?aanbidder?
Segmentation
A memory-based learner trained on an instance
base extracted from the training set constitutes
the segmentation system. An initial feature set
was extracted using a windowing approach sim-
ilar to the one described in Van den Bosch and
Daelemans (1999). Preliminary experiments
were however unable to replicate the high seg-
mentation accuracy of said method, so that ex-
tra features needed to be added. Table 3 shows
an example of instances extracted for the word
?rijksontvanger? (state collector). Experiments
on a held-out validation set confirmed both left
and right context sizes determined in Van den
Bosch and Daelemans (1999)1 . The last two
features are combined features from the left and
right context and were shown to be beneficial
on the validation set. They denote a group con-
taining the focus letter and the two consecutive
letters and a group containing the focus letter
and the three previous letters respectively.
A numerical feature (?Dist? in Table 3) was
added that expresses the distance to the previ-
ous morpheme boundary. This numerical fea-
ture avoids overeager segmentation, i.e. a small
value for the distance feature has to be compen-
sated by other strong indicators for a morpheme
boundary. We also consider the morpheme that
was compiled since the last morpheme boundary
(features in the column ?Current Morpheme?).
A binary feature indicates whether or not this
morpheme can be found in the lexicon extracted
from the training set. The next two features
consider the morpheme formed by adding the
next letter in line.
Note however that the introduction of these
features makes it impossible to precompile the
instance base for the test set, since for instance
1Context size was restricted to four graphemes for
reasons of space in Table 3.
the distance to the previous morpheme bound-
ary can obviously not be known before actual
segmentation takes place. We therefore set up
a server application and generated instances on
the fly.
1,141,588 instances were extracted from the
training set and were used to power a TiMBL
server. The optimal algorithmic parameters
were determined with cross-validation on the
training set2. A client application extracted
instances from the test set and sent them to
the server on the fly, using the previous out-
put of the server to determine the value of the
above-mentioned features. We also adjusted the
verbosity level of the output so that confidence
scores were added to the classifier?s decision.
A post-processing step generated all possible
segmentations for all possible alternations. The
possible segmentations for the word ?apotheker?
(pharmacist) for example constituted the fol-
lowing set: {(apotheek)(er), (apotheker),
(apotheeker), (apothek)(er)}. Next, the confi-
dence scores of the classifier?s output were mul-
tiplied for each possible segmentation to ex-
press the overall confidence score for the mor-
pheme sequence. Also, a lexicon extracted from
the training set with associated probabilities
was used to compute the overall probability of
the morpheme sequence (using a Laplace-type
smoothing process to account for unseen mor-
phemes). Finally, a bigram model computed the
probability of the possible morpheme sequences
as well.
Table 4 describes the results at different
stages of processing and expresses the number of
words that were correctly segmented. Only us-
ing the confidence scores output by the memory-
based learner (equivalent to using a non-ranking
2ib1-ig was used with Jeffrey divergence as distance
metric, no weighting, considering 11 nearest neighbors
using inverse linear weighting.
Left Right Current Next
Context Focus Context Dist Morpheme Morpheme Combined Class
- - - - r i j k s 0 r 1 ri 0 rij ?r 0
- - - r i j k s o 1 ri 0 rij 1 ijk ?ri 0
- - r i j k s o n 2 rij 1 rijk 1 jks -rij 0
- r i j k s o n t 3 rijk 1 rijks 0 kso rijk 1
r i j k s o n t v 0 s 1 so 0 son ijks 1
i j k s o n t v a 0 o 0 on 1 ont jkso 0
j k s o n t v a n 1 on 1 ont 1 ntv kson 0
k s o n t v a n g 2 ont 1 ontv 0 tva sont 0
s o n t v a n g e 3 ontv 0 ontva 0 van ontv 0
o n t v a n g e r 4 ontva 0 ontvan 0 ang ntva 0
n t v a n g e r - 5 ontvan 0 ontvang 1 nge tvan 0
t v a n g e r - - 6 ontvang 1 ontvange 0 ger vang 1
v a n g e r - - - 0 e 1 er 1 er- ange 0
a n g e r - - - - 1 er 1 er- 0 r? nger 1
Table 3: Instances for Segmentation Task for the word ?rijksontvanger?.
Ranking Method Full Word Score
MBL 81.36%
Lexical 84.56%
Bigram 82.44%
MBL+Lexical 86.37%
MBL+Bigram 85.79%
MBL+Lexical+Bigram 87.57%
Table 4: Results at different stages of post-
processing for segmentation task
approach) achieves a low score of 81.36%. Us-
ing only the lexical probabilities yields a better
score, but the combination of the two achieves
a satisfying 86.37% accuracy. Adding bigram
probabilities to the product further improves ac-
curacy to 87.57%. In Section 5 we will look at
the results of the memory-based morphological
analyzer in more detail.
4 A Finite State Approach
Since the invention of the two-level formalism by
Kimmo Koskenniemi (Koskenniemi, 1983) finite
state technology has been the dominant frame-
work for computational morphological analysis.
In the FLaVoR project a finite state morpholog-
ical analyzer for Dutch is being developed. We
have several motivations for this. First, until
now no finite state implementation for Dutch
morphology was freely available. In addition,
finite state morphological analysis can be con-
sidered a kind of reference for the evaluation
of other analysis techniques. In the current
project, however, most important is the inher-
ent bidirectionality of finite state morphologi-
cal processing. This bidirectionality should al-
low for a flexible integration of the morphologi-
cal model in the speech recognition engine as it
leaves open a double option: either the morpho-
logical system acts in analysis mode on word hy-
potheses offered by the recognizer?s search algo-
rithm, or the system works in generation mode
on morpheme hypotheses. Only future practi-
cal implementation of the complete recognition
system will reveal which option is preferable.
After evaluation of several finite state imple-
mentations it was decided to implement the cur-
rent system in the framework of the Xerox finite
state tools, which are well described and allow
for time and space efficient processing (Beesley
and Karttunen, 2003). The output of the fi-
nite state morphological analyzer is further pro-
cessed by a filter and a probabilistic score func-
tion, as will be detailed later.
Morphotactics and Orthographic
Alternation
The morphological system design is a composi-
tion of finite state machines modeling morpho-
tactics and orthographic alternations. For mor-
photactics a lexicon of 29,890 items was gen-
erated from the training set (118 prefixes, 189
suffixes, 3 infixes and 29,581 roots). The items
were divided in 23 lexicon classes, each of which
could function as an item?s continuation class.
The resulting finite state network has 24,858
states and 61,275 arcs.
The Xerox finite state tools allow for a speci-
fication of orthographical alternation by means
of (conditional) replace rules. Each replace
rule compiles into a single finite state trans-
ducer. These transducers can be put in cas-
cade or in parallel. In the case at hand, all
transducers were put in cascade. The result-
ing finite state transducer contains 3,360 states
and 81,523 arcs. The final transducer (a com-
position of the lexical network and the ortho-
graphical transducer) contains 29,234 states and
106,105 arcs.
Dealing with Overgeneration
As the finite state machine has no memory
stack3, the use of continuation classes only
allows for rather crude morphotactic model-
ing. For example, in ?on-ont-vlam-baar? (un-in-
flame-able) the noun ?vlam? first combines with
the prefix ?ont? to form a verb. Next, the suffix
?baar? is attached and an adjective is built. Fi-
nally, the prefix ?on? negates the adjective. This
example shows that continuation classes cannot
be strictly defined: the suffix ?baar? combines
with a verb but immediately follows a noun
root, while the prefix ?on? requires an adjective
but is immediately followed by another prefix.
Obviously, such a model leads to overgenera-
tion. In practice, the average number of anal-
yses per test set item is 7.65. The maximum
number of analyses is 1,890 for the word ?be-
lastingadministratie? (tax administration).
In section 3 the numerical feature ?Dist? was
used to avoid overeager segmentation. We apply
a similar kind of filter to the segmentations gen-
erated by the morphological finite state trans-
ducer. A penalty function for short morphemes
is defined: 1- and 2-character morphemes re-
ceive penalty 3, 3-character morphemes penalty
1. Both an absolute and relative4 penalty
threshold are set. Optimal threshold values (11
and 2.5 respectively) were determined on the
basis of the training set. Analyses exceeding
one of both thresholds are removed. This filter
proves quite effective as it reduces the average
number of analyses per item with 36.6% to 4.85.
Finally, all remaining segmentation hypothe-
ses are scored and ranked using an N-gram mor-
pheme model. We applied a bigram and trigram
model, both using Katz back-off and modified
Kneser-Ney smoothing. The bigram slightly
3Actually, the Xerox finite state tools do allow for a
limited amount of ?memory? by a restricted set of uni-
fication operations termed flag diacritics. Yet, they are
insufficient for modeling long distance dependencies with
hierarchical structure.
4Relative to the number of morphemes.
outperformed the trigram model, showing that
the training data is rather sparse. Tables 5, 6
and 7 all show results obtained with the bigram
model.
Monomorphemic Items
The biggest remaining problem at this stage of
development is the scoring of monomorphemic
test items which are not included as word
roots in the lexical finite state network. Some-
times these items do not receive any analysis
at all, in which case we correctly consider them
monomorphemic. Mostly however, monomor-
phemes are wrongly analyzed as morphologi-
cally complex. Scoring all test items as poten-
tially monomorphemic does not offer any solu-
tion, as the items at hand were not in the train-
ing data and thus receive just the score for un-
known items. This problem of spurious analyses
accounts for 57.23% of all segmentation errors
made by the finite state system.
5 Comparing the Two Systems
System 1-best 2-best 3-best
Baseline 18.64
MBM 87.57 91.20 91.68
FSM 89.08 90.87 91.01
Table 5: Full Word Scores (%) on the segmen-
tation task
To evaluate both morphological systems, we
defined a training and test set. Of the 124,136
word forms in CELEX, 110,627 constitute the
training set. The test set is further split up into
words with only one possible analysis (11,256
word forms) and words with multiple analyses
(2,253). Since the latter set requires morpho-
logical processes beyond segmentation, we focus
our evaluation on the former set in this paper.
For the machine learning experiments, we also
defined a held-out validation set of 5,000 word
forms, which is used to perform parameter op-
timization and feature selection.
Tables 5, 6 and 7 show a comparison of the
results5. Table 5 describes the percentage of
words in the test set that have been segmented
correctly. We defined a baseline system which
considers all words in the test set as monomor-
phemic. Obviously this results in a very low
5MBM: the memory-based morphological analyzer,
FSM: the finite state morphological analyzer
full word score (which shows us that 18.64% of
the words in the test set are actually monomor-
phemic). The finite state system seems to have
a slight edge over the memory-based analyzer
when we looking at the single best solution. Yet,
when we consider 2-best and 3-best scores, the
memory-based analyzer in turn beats the finite
state analyzer.
System Precision Recall F?=1
Baseline 18.64 07.94 11.14
MBM 91.63 90.52 91.07
FSM 89.60 94.00 91.75
Table 7: Precision and Recall Scores (%) (mor-
phemes) on the segmentation task
We also calculated Precision and Recall on
morpheme boundaries. The results are dis-
played in Table 6. This score expresses how
many of the morpheme boundaries have been
retrieved. We make a distinction between word-
internal morpheme boundaries and all mor-
pheme boundaries. The former does not in-
clude the morpheme boundaries at the end of
a word, while the latter does. We provide the
latter in reference to Van den Bosch and Daele-
mans (1999), but the focus lies on the results
for word-internal boundaries as these are non-
trivial. We notice that the memory-based sys-
tem outperforms the finite state system, but the
difference is once again small. However, when
we look at Table 7 in which we calculate the
amount of full morphemes that have been cor-
rectly retrieved (meaning that both the start
and end-boundary have been correctly placed),
we see that the finite state method has the ad-
vantage.
Slight differences in accuracy put aside, we
find that both systems achieve similar scores on
this dataset. When we look at the output, we do
notice that these systems are indeed performing
quite well. There are not many instances where
the morphological analyzer cannot be said to
have found a decent alternative analysis to the
gold standard one. In many cases, both systems
even come up with a more advanced morpholog-
ical analysis: e.g. ?gekwetst? (hurt) is featured
in the database as a monomorphemic artefact.
Both systems described in this paper correctly
segment the word form as ?ge+kwets+t?, even
though they have not yet specifically been de-
signed to handle this type of inflection.
When performing an error analysis of the out-
put, one does notice a difference in the way
the systems have tried to solve the erroneous
analyses. The finite state method often seems
to generate more morpheme boundaries than
necessary, while the reverse is the case for the
memory-based system, which seems too eager
to revert to monomorphemic analyses when in
doubt. This behavior might also explain the
reversed situation when comparing Table 6 to
Table 7. Also noteworthy is the fact that al-
most 60% of the errors is made on wordforms
that both systems were not able to analyze cor-
rectly. Work is currently also underway to im-
prove the performance by combining the rank-
ings of both systems, as there is a large degree
of complementarity between the two systems.
Each system is able to uniquely find the correct
segmentation for about 5% of the words in the
test set, yielding an upperbound performance of
98.75% on the full word score for an optimally
combined system.
6 Conclusion
Current work in the project focuses on further
developing the morphological analyzer by try-
ing to provide part-of-speech tags and hierar-
chical bracketing properties to the segmented
morpheme sequences in order to comply with
the type of analysis found in the morphologi-
cal database of CELEX. We will further try to
incorporate other machine learning algorithms
like maximum entropy and support vector ma-
chines to see if it is at all possible to overcome
the current accuracy threshold. Algorithmic
parameter ?degradation? will be attempted to
entice more greedy morpheme boundary place-
ment in the raw output, in the hope that the
post-processing mechanism will be able to prop-
erly rank the extra alternative segmentations.
Finally, we will experiment on the full CELEX
data set (including inflection) as featured in
Van den Bosch and Daelemans (1999).
In this paper we described two data-driven
systems for morphological analysis. Trained
and tested on the same data set, these systems
achieve a similar accuracy, but do exhibit quite
different processing properties. Even though
these systems were originally designed to func-
tion as language models in the context of a mod-
ular architecture for speech recognition, they
constitute accurate and elegant morphological
analyzers in their own right, which can be incor-
porated in other natural language applications
as well.
System Precision Recall F?=1
All Intern All Intern All Intern
Baseline 100 0 42.59 0 59.74 0
MBM 94.15 89.71 93.00 87.81 93.57 88.75
FSM 90.25 83.58 94.68 90.73 92.41 87.01
Table 6: Precision and Recall Scores (%) (morpheme boundaries) on the segmentation task
Acknowledgements
The research described in this paper was funded by
IWT in the GBOU programme, project FLaVoR:
Flexible Large Vocabulary Recognition: Incorporat-
ing Linguistic Knowledge Sources Through a Modu-
lar Recogniser Architecture. (Project number 020192).
http://www.esat.kuleuven.ac.be/spch/projects/FLaVoR.
References
R.H. Baayen, R. Piepenbrock, and L. Gulik-
ers. 1995. The Celex Lexical Database (Re-
lease2) [CD-ROM]. Linguistic Data Consor-
tium, University of Pennsylvania, Philadel-
phia, U.S.A.
K. R. Beesley and L. Karttunen, editors. 2003.
Finite State Morphology. CSLI Publications,
Stanford.
W. Daelemans, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 2003.
TiMBL: Tilburg memory based learner, ver-
sion 5.0, reference guide. ILK Technical Re-
port 01-04, Tilburg University. Available
from http://ilk.kub.nl.
L. Dehaspe, H. Blockeel, and L. De Raedt.
1995. Induction, logic and natural language
processing. In Proceedings of the joint
ELSNET/COMPULOG-NET/EAGLES
Workshop on Computational Logic for
Natural Language Processing.
K. Demuynck, T. Laureys, D. Van Compernolle,
and H. Van hamme. 2003. Flavor: a flexi-
ble architecture for LVCSR. In Proceedings of
the 8th European Conference on Speech Com-
munication and Technology, pages 1973?1976,
Geneva, Switzerland, September.
J. Heemskerk. 1993. A probabilistic context-
free grammar for disambiguation in morpho-
logical parsing. Technical Report 44, itk,
Tilburg University.
K. Koskenniemi. 1983. Two-level morphology:
A general computational model for word-form
recognition and production. Ph.D. thesis, De-
partment of General Linguistics, University
of Helsinki.
T. Laureys, V. Vandeghinste, and J. Duchateau.
2002. A hybrid approach to compounds in
LVCSR. In Proceedings of the 7th Interna-
tional Conference on Spoken Language Pro-
cessing, volume I, pages 697?700, Denver,
U.S.A., September.
T. Laureys, G. De Pauw, H. Van hamme,
W. Daelemans, and D. Van Compernolle.
2004. Evaluation and adaptation of the
CELEX Dutch morphological database. In
Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation,
Lisbon, Portugal, May.
R. Rosenfeld. 2000. Two decades of statisti-
cal language modeling: Where do we go from
here? Proceedings of the IEEE, 88(8):1270?
1278.
V. Siivola, T. Hirismaki, M. Creutz, and M. Ku-
rimo. 2003. Unlimited vocabulary speech
recognition based on morphs discovered in
an unsupervised manner. In Proceedings of
the 8th European Conference on Speech Com-
munication and Technology, pages 2293?2296,
Geneva, Switzerland, September.
M. Szarvas and S. Furui. 2003. Finite-state
transducer based modeling of morphosyntax
with applications to Hungarian LVCSR. In
Proceedings of the International Conference
on Acoustics, Speech and Signal Processing,
pages 368?371, Hong Kong, China, May.
A. Van den Bosch and W. Daelemans. 1999.
Memory-based morphological analysis. In
Proceedings of the 37th Annual Meeting of
the Association for Computational Linguis-
tics, pages 285?292, New Brunswick, U.S.A.,
September.
A. Van den Bosch, W. Daelemans, and A. Wei-
jters. 1996. Morphological analysis as classi-
fication: an inductive-learning approach. In
K. Oflazer and H. Somers, editors, Proceed-
ings of the Second International Conference
on New Methods in Natural Language Pro-
cessing, NeMLaP-2, Ankara, Turkey, pages
79?89.
Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ? AfLaT 2009, pages 9?16,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
The SAWA Corpus: a Parallel Corpus English - Swahili
Guy De Pauw
CNTS - Language Technology Group, University of Antwerp, Belgium
School of Computing and Informatics, University of Nairobi, Kenya
guy.depauw@ua.ac.be
Peter Waiganjo Wagacha
School of Computing and Informatics, University of Nairobi, Kenya
waiganjo@uonbi.ac.ke
Gilles-Maurice de Schryver
African Languages and Cultures, Ghent University, Belgium
Xhosa Department, University of the Western Cape, South Africa
gillesmaurice.deschryver@ugent.be
Abstract
Research in data-driven methods for Ma-
chine Translation has greatly benefited
from the increasing availability of paral-
lel corpora. Processing the same text in
two different languages yields useful in-
formation on how words and phrases are
translated from a source language into a
target language. To investigate this, a par-
allel corpus is typically aligned by linking
linguistic tokens in the source language to
the corresponding units in the target lan-
guage. An aligned parallel corpus there-
fore facilitates the automatic development
of a machine translation system and can
also bootstrap annotation through projec-
tion. In this paper, we describe data col-
lection and annotation efforts and prelim-
inary experimental results with a parallel
corpus English - Swahili.
1 Introduction
Language technology applications such as ma-
chine translation can provide an invaluable, but
all too often ignored, impetus in bridging the dig-
ital divide between the Western world and Africa.
Quite a few localization efforts are currently un-
derway that improve ICT access in local African
languages. Vernacular content is now increasingly
being published on the Internet, and the need for
robust language technology applications that can
process this data is high.
For a language like Swahili, digital resources
have become increasingly important in everyday
life, both in urban and rural areas, particularly
thanks to the increasing number of web-enabled
mobile phone users in the language area. But most
research efforts in the field of natural language
processing (NLP) for African languages are still
firmly rooted in the rule-based paradigm. Lan-
guage technology components in this sense are
usually straight implementations of insights de-
rived from grammarians. While the rule-based
approach definitely has its merits, particularly in
terms of design transparency, it has the distinct
disadvantage of being highly language-dependent
and costly to develop, as it typically involves a lot
of expert manual effort.
Furthermore, many of these systems are decid-
edly competence-based. The systems are often
tweaked and tuned towards a small set of ideal
sample words or sentences, ignoring the fact that
real-world language technology applications have
to be principally able to handle the performance
aspect of language. Many researchers in the field
are quite rightly growing weary of publications
that ignore quantitative evaluation on real-world
data or that report incredulously high accuracy
scores, excused by the erroneously perceived reg-
ularity of African languages.
In a linguistically diverse and increasingly com-
puterized continent such as Africa, the need for a
more empirical approach to language technology
is high. The data-driven, corpus-based approach
described in this paper establishes such an alter-
native, so far not yet extensively investigated for
African languages. The main advantage of this
9
approach is its language independence: all that is
needed is (linguistically annotated) language data,
which is fairly cheap to compile. Given this data,
existing state-of-the-art algorithms and resources
can consequently be re-used to quickly develop ro-
bust language applications and tools.
Most African languages are however resource-
scarce, meaning that digital text resources are few.
An increasing number of publications however are
showing that carefully selected procedures can in-
deed bootstrap language technology for Swahili
(De Pauw et al, 2006; De Pauw and de Schryver,
2008), Northern Sotho (de Schryver and De Pauw,
2007) and smaller African languages (Wagacha et
al., 2006a; Wagacha et al, 2006b; De Pauw and
Wagacha, 2007; De Pauw et al, 2007a; De Pauw
et al, 2007b).
In this paper we outline on-going research on
the development of a parallel corpus English -
Swahili. The parallel corpus is designed to boot-
strap a data-driven machine translation system for
the language pair in question, as well as open up
possibilities for projection of annotation.
We start off with a short survey of the different
approaches to machine translation (Section 2) and
showcase the possibility of projection of annota-
tion (Section 3). We then concentrate on the re-
quired data collection and annotation efforts (Sec-
tion 4) and describe preliminary experiments on
sentence, word and morpheme alignment (Sec-
tions 5 and 6). We conclude with a discussion of
the current limitations to the approach and provide
pointers for future research (Section 7).
2 Machine Translation
The main task of Machine Translation (MT) can
be defined as having a computer take a text in-
put in one language, the Source language (SL),
decode its meaning and re-encode it producing as
output a similar-meaning text in another language,
the Target language (TL). The idea of building
an application to automatically convert text from
one language to an equivalent text-meaning in
a second language traces its roots back to Cold
War intelligence efforts in the 1950?s and 60?s for
Russian-English text translations. Since then a
large number of MT systems have been developed
with varying degrees of success. For an excellent
overview of the history of MT, we refer the reader
to Hutchins (1986).
The original dream of creating a fully automatic
MT system has long since been abandoned and
most research in the field currently concentrates
on minimizing human pre- and post-processing ef-
fort. A human translator is thus considered to
work alongside the MT system to produce faster
and more consistent translations.
The Internet brought in an interesting new di-
mension to the purpose of MT. In the mid 1990s,
free on-line translation services began to surface
with an increasing number of MT vendors. The
most famous example is Yahoo!?s Babelfish , of-
fering on-line versions of Systran to translate En-
glish, French, German, Spanish and other Indo-
European languages. Currently Google.inc is also
offering translation services. While these systems
provide far from perfect output, they can often
give readers a sense of what is being talked about
on a web page in a language (and often even char-
acter set) foreign to them.
There are roughly three types of approaches to
machine translation:
1. Rule-based methods perform translation us-
ing extensive lexicons with morphological,
syntactic and semantic information, and large
sets of manually compiled rules, making
them very labor intensive to develop.
2. Statistical methods entail the collection and
statistical analysis of bilingual text corpora,
i.e. parallel corpora. The technique tries
to find the highest probability translation of
a sentence or phrase among the exponential
number of choices.
3. Example-based methods are similar to sta-
tistical methods in that they are parallel cor-
pus driven. An Example-Based Machine
Translator (EBMT) scans for patterns in both
languages and relates them in a translation
memory.
Most MT systems currently under development
are based on methods (2) and/or (3). Research
in these fields has greatly benefited from the in-
creasing availability of parallel corpora, which are
needed to bootstrap these approaches. Such a par-
allel corpus is typically aligned by linking, either
automatically or manually, linguistic tokens in the
source language to the corresponding units in the
target language. Processing this data enables the
development of fast and effective MT systems in
both directions with a minimum of human involve-
ment.
10
English Swahili English Swahili
Sentences Sentences Words Words
New Testament 7.9k 189.2k 151.1k
Quran 6.2k 165.5k 124.3k
Declaration of HR 0.2k 1.8k 1.8k
Kamusi.org 5.6k 35.5k 26.7k
Movie Subtitles 9.0k 72.2k 58.4k
Investment Reports 3.2k 3.1k 52.9k 54.9k
Local Translator 1.5k 1.6k 25.0k 25.7k
Full Corpus Total 33.6k 33.6k 542.1k 442.9k
Table 1: Overview of the data in the SAWA Corpus
3 Projection of Annotation
While machine translation constitutes the most
straightforward application of a parallel corpus,
projection of annotation has recently become an
interesting alternative use of this type of resource.
As previously mentioned, most African languages
are resource-scarce: annotated data is not only un-
available, but commercial interest to develop these
resources is limited. Unsupervised approaches
can be used to bootstrap annotation of a resource-
scarce language (De Pauw and Wagacha, 2007; De
Pauw et al, 2007a) by automatically finding lin-
guistic patterns in large amounts of raw text.
Projection of annotation attempts to achieve the
same goal, but through the use of a parallel cor-
pus. These techniques try to transport the annota-
tion of a well resourced source language, such as
English, to texts in a target language. As a natu-
ral extension of the domain of machine translation,
these methods employ parallel corpora which are
aligned at the sentence and word level. The di-
rect correspondence assumption coined in Hwa et
al. (2002) hypothesizes that words that are aligned
between source and target language, must share
linguistic features as well. It therefore allows for
the annotation of the words in the source language
to be projected unto the text in the target language.
The following general principle holds: the closer
source and target language are related, the more
accurate this projection can be performed. Even
though lexical and structural differences between
languages prevent a simple one-to-one mapping,
knowledge transfer is often able to generate a well
directed annotation of the target language.
This holds particular promise for the annotation
of dependency analyses for Swahili, as exempli-
fied in Figure 1, since dependency grammar fo-
root The cat fell into the water
root Paka alianguka ndani ya maji
main
dt subj pp dt
comp
main
subj pp
comp
??
Figure 1: Projection of Dependency Analysis An-
notation
cuses on semantic relationships, rather than core
syntactic properties, that are much more trouble-
some to project across languages. The idea is that
a relationship that holds between two words in the
source language (for instance the subj relationship
between cat and fell), also holds for the corre-
sponding linguistic tokens in the target language,
i.e. paka and alianguka.
In the next section we describe data collection
and preprocessing efforts on the SAWA Corpus,
a parallel corpus English - Swahili (cf. Table 1),
which will enable this type of projection of anno-
tation, as well as the development of a data-driven
machine translation system.
4 Data Collection and Annotation
While digital data is increasingly becoming avail-
able for Swahili on the Internet, sourcing useful
11
Figure 2: Manual word alignment using the UMIACS interface
bilingual data is far from trivial. At this stage in
the development of the MT system, it is paramount
to use faithfully translated material, as this benefits
further automated processing. The corpus-based
MT approaches we wish to employ, require word
alignment to be performed on the texts, during
which the words in the source language are linked
to the corresponding words in the target language
(also see Figures 1 and 2).
But before we can do this, we need to perform
sentence-alignment, during which we establish an
unambiguous mapping between the sentences in
the source text and the sentences in the target text.
While some data is inherently sentence-aligned,
other texts require significant preprocessing before
word alignment can be performed.
The SAWA Corpus currently consists of a rea-
sonable amount of data (roughly half a million
words in each language), although this is not
comparable to the resources available to Indo-
European language pairs, such as the Hansard cor-
pus (Roukos et al, 1997) (2.87 million sentence
pairs). Table 1 gives an overview of the data avail-
able in the SAWA Corpus. For each segment it lists
the number of sentences and words in the respec-
tive languages.
4.1 Sentence-aligned Resources
We found digitally available Swahili versions of
the New Testament and the Quran for which we
sourced the English counterparts. This is not a
trivial task when, as in the case of the Swahili
documents, the exact source of the translation is
not provided. By carefully examining subtle dif-
ferences in the English versions, we were how-
ever able to track down the most likely candidate.
While religious material has a specific register and
may not constitute ideal training material for an
open-ended MT system, it does have the advan-
tage of being inherently aligned on the verse level,
facilitating further sentence alignment. Another
typical bilingual text is the UN Declaration of Hu-
man Rights, which is available in many of the
world?s languages, including Swahili. The manual
sentence alignment of this text is greatly facilitated
by the fixed structure of the document.
The downloadable version of the on-line dictio-
nary English-Swahili (Benjamin, 2009) contains
individual example sentences associated with the
dictionary entries. These can be extracted and
used as parallel data in the SAWA corpus. Since
at a later point, we also wish to study the specific
linguistic aspects of spoken language, we opted
to have some movie subtitles manually translated.
These can be extracted from DVDs and while the
12
language is compressed to fit on screen and con-
stitutes scripted language, they nevertheless pro-
vide a reasonable approximation of spoken lan-
guage. Another advantage of this data is that it
is inherently sentence-aligned, thanks to the tech-
nical time-coding information. It also opens up
possibilities for MT systems with other language
pairs, since a commercial DVD typically contains
subtitles for a large number of other languages as
well.
4.2 Paragraph-aligned Resources
The rest of the material consists of paragraph-
aligned data, which was manually sentence-
aligned. We obtained a substantial amount of data
from a local Kenyan translator. Finally, we also
included Kenyan investment reports. These are
yearly reports from local companies and are pre-
sented in both English and Swahili. A major dif-
ficulty was extracting the data from these docu-
ments. The company reports are presented in col-
orful brochures in PDF format, meaning automatic
text exports require manual post-processing and
paragraph alignment (Figure 3). They neverthe-
less provide a valuable resource, since they come
from a fairly specific domain and are a good sam-
ple of the type of text the projected MT system
may need to process in a practical setting.
The reader may note that there is a very diverse
variety of texts within the SAWA corpus, ranging
from movie subtitles to religious texts. While it
certainly benefits the evaluation to use data from
texts in one specific language register, we have
chosen to maintain variety in the language data at
this point. Upon evaluating the decoder at a later
stage, we will however investigate the bias intro-
duced by the specific language registers in the cor-
pus.
4.3 Word Alignment
All of the data in the corpus was subsequently
tokenized, which involves automatically cleaning
up the texts, conversion to UTF-8, and splitting
punctuation from word forms. The next step in-
volved scanning for sentence boundaries in the
paragraph-aligned text, to facilitate the automatic
sentence alignment method described in Section 5.
While not necessary for further processing, we
also performed manual word-alignment annota-
tion. This task can be done automatically, but it
is useful to have a gold-standard reference against
which we can evaluate the automated method.
Figure 3: Text Extraction from Bilingual Invest-
ment Report
Monitoring the accuracy of the automatic word-
alignment method against the human reference,
will allow us to tweak parameters to arrive at the
optimal settings for this language pair.
We used the UMIACS word alignment interface
(Hwa and Madnani, 2004) for this purpose and
asked the annotators to link the words between the
two sentences (Figure 2). Given the linguistic dif-
ferences between English and Swahili, this is by
no means a trivial task. Particularly the morpho-
logical richness of Swahili means that there is a lot
of convergence from words in English to words
in Swahili (also see Section 6). This alignment
was done on some of the manual translations of
movie subtitles, giving us a gold-standard word-
alignment reference of about 5,000 words. Each
annotator?s work was cross-checked by another
annotator to improve correctness and consistency.
5 Alignment Experiments
There are a number of packages available to
process parallel corpora. To preprocess the
paragraph-aligned texts, we used Microsoft?s
bilingual sentence aligner (Moore, 2002). The
13
Precision Recall F(? = 1)
39.4% 44.5% 41.79%
Table 2: Precision, Recall and F-score for the
word-alignment task using GIZA++
output of the sentence alignment was conse-
quently manually corrected. We found that 95% of
the sentences were correctly aligned with most er-
rors being made on sentences that were not present
in English, i.e. instances where the translator de-
cided to add an extra clarifying sentence to the di-
rect translation from English. This also explains
why there are more Swahili words in the paragraph
aligned texts than in English, while the situation is
reversed for the sentence aligned data.
For word-alignment, the state-of-the-art method
is GIZA++ (Och and Ney, 2003), which imple-
ments the word alignment methods IBM1 to IBM5
and HMM. While this method has a strong Indo-
European bias, it is nevertheless interesting to see
how far we can get with the default approach used
in statistical MT.
We evaluate by looking at the word alignments
proposed by GIZA++ and compare them to the
manually word-aligned section of the SAWA Cor-
pus. We can quantify the evaluation by calculat-
ing precision and recall and their harmonic mean,
the F-score (Table 2). The former expresses how
many links are correct, divided by the total num-
ber of links suggested by GIZA++. The latter is
calculated by dividing the number of correct links,
by the total number of links in the manual annota-
tion. The underwhelming results presented in Ta-
ble 2 can be attributed to the strong Indo-European
bias of the current approaches. It is clear that extra
linguistic data sources and a more elaborate explo-
ration of the experimental parameters of GIZA++
will be needed, as well as a different approach to
word-alignment. In the next section, we describe
a potential solution to the problem by defining the
problem on the level of the morpheme.
6 Alignment into an Agglutinating
Language
The main problem in training a GIZA++ model for
the language pair English - Swahili is the strong
agglutinating nature of the latter. Alignment pat-
terns such as the one in Figures 1 and 2 are not
impossible to retrieve. But no corpus is exhaus-
tive enough to provide enough linguistic evidence
Precision Recall F(? = 1)
50.2% 64.5% 55.8%
Table 3: Precision, Recall and F-score for the
morpheme/word-alignment task using GIZA++
to unearth strongly converging alignment patterns,
such as the one in Example 1.
(1) I have turned him down
Nimemkatalia
Morphologically deconstructing the word how-
ever can greatly relieve the sparse data problem for
this task:
(2) I have turned him down
Ni- me- m- katalia
The isolated Swahili morphemes can more eas-
ily be linked to their English counterparts, since
there will be more linguistic evidence in the par-
allel corpus, linking for example ni to I and m
to him. To perform this kind of morphological
analysis, we developed a machine learning system
trained and evaluated on the Helsinki corpus of
Swahili (Hurskainen, 2004). Experimental results
show that the data-driven approach achieves state-
of-the-art performance in a direct comparison with
a rule-based method, with the added advantage of
being robust to word forms for previously unseen
lemmas (De Pauw and de Schryver, 2008). We
can consequently use morphological deconstruc-
tion as a preprocessing step for the alignment task,
similar to the method described by Goldwater and
McClosky (2005), Oflazer (2008) and Stymne et
al. (2008).
We have no morphologically aligned parallel
data available, so evaluation of the morphology-
based approach needs to be done in a roundabout
way. We first morphologically decompose the
Swahili data and run GIZA++ again. Then we re-
compile the Swahili words from the morphemes
and group the word alignment links accordingly.
Incompatible linkages are removed. The updated
scores are presented in Table 3. While this cer-
tainly improves on the scores in Table 2, we need
to be aware of the difficulty that the morphological
preprocessing step will introduce in the decoding
phase, necessitating the introduction of a language
model that not only works on the word level, but
14
also on the level of the morpheme.
For the purpose of projection of annotation, this
is however not an issue. We performed a prelim-
inary experiment with a dependency-parsed En-
glish corpus, projected unto the morphologically
decompounded tokens in Swahili. We are cur-
rently lacking the annotated gold-standard data to
perform quantitative evaluation, but have observed
interesting annotation results, that open up pos-
sibilities for the morphological analysis of more
resource-scarce languages.
7 Discussion
In this paper we presented parallel corpus collec-
tion work that will enable the construction of a
machine translation system for the language pair
English - Swahili, as well as open up the possibil-
ity of corpus annotation through projection. We
are confident that we are approaching a critical
amount of data that will enable good word align-
ment that can subsequently be used as a model for
an MT decoding system, such as the Moses pack-
age (Koehn et al, 2007). While the currently re-
ported scores are not yet state-of-the-art, we are
confident that further experimentation and the ad-
dition of more bilingual data as well as the intro-
duction of extra linguistic features will raise the
accuracy level of the proposed MT system.
Apart from the morphological deconstruction
described in Section 6, the most straightforward
addition is the introduction of part-of-speech tags
as an extra layer of linguistic description, which
can be used in word alignment model IBM5. The
current word alignment method tries to link word
forms, but knowing that for instance a word in the
source language is a noun, will facilitate linking
it to a corresponding noun in the target language,
rather than considering a verb as a possible match.
Both for English (Ratnaparkhi, 1996) and Swahili
(De Pauw et al, 2006), we have highly accurate
part-of-speech taggers available.
Another extra information source that we have
so far ignored is a digital dictionary as a seed for
the word alignment. The kamusiproject.org elec-
tronic dictionary will be included in further word-
alignment experiments and will undoubtedly im-
prove the quality of the output.
Once we have a stable word alignment mod-
ule, we will further conduct learning curve exper-
iments, in which we train the system with grad-
ually increasing amounts of data. This will pro-
vide us with information on how much more data
we need to achieve state-of-the-art performance.
This additional data can be automatically found
by parallel web mining, for which a few sys-
tems have recently become available (Resnik and
Smith, 2003).
Furthermore, we will also look into the use
of comparable corpora, i.e. bilingual texts that
are not straight translations, but deal with the
same subject matter. These have been found to
work as additional material within a parallel cor-
pus (McEnery and Xiao, 2007) and may further
help improve the development of a robust, open-
ended and bidirectional machine translation sys-
tem for the language pair English - Swahili. The
most innovative prospect of the parallel corpus is
the annotation of dependency analysis in Swahili,
not only on the syntactic level, but also on the
level of the morphology. The preliminary exper-
iments indicate that this approach might provide a
valuable technique to bootstrap annotation in truly
resource-scarce languages.
Acknowledgments
The research presented in this paper was made
possible through the support of the VLIR-IUC-
UON program and was partly funded by the SAWA
BOF UA-2007 project. The first author is funded
as a Postdoctoral Fellow of the Research Founda-
tion - Flanders (FWO). We are greatly indebted to
Dr. James Omboga Zaja for contributing some of
his translated data, to Mahmoud Shokrollahi-Far
for his advice on the Quran and to Anne Kimani,
Chris Wangai Njoka and Naomi Maajabu for their
annotation efforts.
References
M. Benjamin. 2009. The Kamusi Project. Available at:
http://www.kamusiproject.org (Accessed: 14 Jan-
uary 2009).
G. De Pauw and G.-M. de Schryver. 2008. Improv-
ing the computational morphological analysis of a
Swahili corpus for lexicographic purposes. Lexikos,
18:303?318.
G. De Pauw and P.W. Wagacha. 2007. Bootstrapping
morphological analysis of G??ku?yu? using unsuper-
vised maximum entropy learning. In Proceedings
of the eighth INTERSPEECH conference, Antwerp,
Belgium.
G. De Pauw, G.-M. de Schryver, and P.W. Wa-
gacha. 2006. Data-driven part-of-speech tagging of
15
Kiswahili. In P. Sojka, I. Kopec?ek, and K. Pala, edi-
tors, Proceedings of Text, Speech and Dialogue, 9th
International Conference, volume 4188 of Lecture
Notes in Computer Science, pages 197?204, Berlin,
Germany. Springer Verlag.
G. De Pauw, P.W. Wagacha, and D.A. Abade. 2007a.
Unsupervised induction of Dholuo word classes
using maximum entropy learning. In K. Getao
and E. Omwenga, editors, Proceedings of the First
International Computer Science and ICT Confer-
ence, pages 139?143, Nairobi, Kenya. University of
Nairobi.
G. De Pauw, P.W. Wagacha, and G.-M. de Schryver.
2007b. Automatic diacritic restoration for resource-
scarce languages. In Va?clav Matous?ek and Pavel
Mautner, editors, Proceedings of Text, Speech and
Dialogue, Tenth International Conference, volume
4629 of Lecture Notes in Computer Science, pages
170?179, Heidelberg, Germany. Springer Verlag.
G.-M. de Schryver and G. De Pauw. 2007. Dictio-
nary writing system (DWS) + corpus query package
(CQP): The case of Tshwanelex. Lexikos, 17:226?
246.
S. Goldwater and D. McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In Pro-
ceedings of the Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 676?683, Van-
couver, Canada.
Google. 2009. Google Translate. Available
at http://www.google.com/translate (Accessed: 14
January 2009.
A. Hurskainen. 2004. HCS 2004 ? Helsinki Corpus of
Swahili. Technical report, Compilers: Institute for
Asian and African Studies (University of Helsinki)
and CSC.
W.J. Hutchins. 1986. Machine translation: past,
present, future. Ellis, Chichester.
R. Hwa and N. Madnani. 2004. The UMI-
ACS Word Alignment Interface. Available at:
http://www.umiacs.umd.edu/?nmadnani/alignment/
forclip.htm (Accessed: 14 January 2009).
R. Hwa, Ph. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 392?399, Philadelphia, PA, USA.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), demonstration session, Prague, Czech Re-
public.
A.M. McEnery and R.Z Xiao. 2007. Parallel and
comparable corpora: What are they up to? In In-
corporating Corpora: Translation and the Linguist.
Translating Europe. Multilingual Matters, Cleve-
don, UK.
R.C. Moore. 2002. Fast and accurate sentence align-
ment of bilingual corpora. In Proceedings of the 5th
Conference of the Association for Machine Transla-
tion in the Americas on Machine Translation: From
Research to Real Users, volume 2499 of Lecture
Notes in Computer Science, pages 135?144, Berlin,
Germany. Springer Verlag.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
K. Oflazer. 2008. Statistical machine translation into
a morphologically complex language. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 376?388, Berlin, Germany. Springer Verlag.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In E. Brill and K. Church,
editors, Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
133?142. Association for Computational Linguis-
tics.
Ph. Resnik and N.A. Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(1):349?
380.
S. Roukos, D. Graff, and D. Melamed. 1997.
Hansard French/English. Available at:
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC95T20 (Accessed: 14 January
2009).
S. Stymne, M. Holmqvist, and L. Ahrenberg. 2008.
Effects of morphological analysis in translation be-
tween German and English. In Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 135?138, Columbus, USA.
P.W. Wagacha, G. De Pauw, and K. Getao. 2006a.
Development of a corpus for G??ku?yu? using machine
learning techniques. In J.C. Roux, editor, Proceed-
ings of LREC workshop - Networking the develop-
ment of language resources for African languages,
pages 27?30, Genoa, Italy, May, 2006. European
Language Resources Association, ELRA.
P.W. Wagacha, G. De Pauw, and P.W. Githinji. 2006b.
A grapheme-based approach for accent restoration
in G??ku?yu?. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
pages 1937?1940, Genoa, Italy, May, 2006. Euro-
pean Language Resources Association, ELRA.
Yahoo! 2009. Babelfish. Available at
http://babelfish.yahoo.com (Accessed: 14 January
2009).
16
Proceedings of the 1st Workshop on Speech and Multimodal Interaction in Assistive Environments, pages 34?42,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Towards a Self-Learning Assistive Vocal Interface:
Vocabulary and Grammar Learning
Janneke van de Loo1, Jort F. Gemmeke2, Guy De Pauw1
Joris Driesen2, Hugo Van hamme2, Walter Daelemans1
1CLiPS - Computational Linguistics, University of Antwerp, Antwerp, Belgium
2ESAT - PSI Speech Group, KU Leuven, Leuven, Belgium
janneke.vandeloo@ua.ac.be, jort.gemmeke@esat.kuleuven.be, guy.depauw@ua.ac.be,
joris.driesen@esat.kuleuven.be, hugo.vanhamme@esat.kuleuven.be, walter.daelemans@ua.ac.be
Abstract
This paper introduces research within the
ALADIN project, which aims to develop an as-
sistive vocal interface for people with a phys-
ical impairment. In contrast to existing ap-
proaches, the vocal interface is self-learning,
which means it can be used with any language,
dialect, vocabulary and grammar. This pa-
per describes the overall learning framework,
and the two components that will provide vo-
cabulary learning and grammar induction. In
addition, the paper describes encouraging re-
sults of early implementations of these voca-
bulary and grammar learning components, ap-
plied to recorded sessions of a vocally guided
card game, Patience.
1 Introduction
Voice control of devices we use in our daily lives is
still perceived as a luxury, since often cheaper and
more straightforward alternatives are available, such
as pushing a button or using remote controls. But
what if pushing buttons is not trivial? Physically
impaired people with restricted (upper) limb mo-
tor control are permanently in the situation where
voice control could significantly simplify some of
the tasks they want to perform (Noyes and Frankish,
1992). By regaining the ability to control more de-
vices in the living environment, voice control could
contribute to their independence of living and their
quality of life.
Unfortunately, the speech recognition technology
employed for voice control still lacks robustness to
speaking style, regional accents and noise, so that
users are typically forced to adhere to a restrictive
grammar and vocabulary in order to successfully
command and control a device.
In this paper we describe research in the ALADIN
project1, which aims to develop an assistive vocal
interface for people with a physical impairment. In
contrast to existing vocal interfaces, the vocal inter-
face is self-learning: The interface should automa-
tically learn what the user means with commands,
which words are used and what the user?s vocal char-
acteristics are. Users should formulate commands
as they like, using the words and grammatical con-
structs they like and only addressing the functional-
ity they are interested in.
We distinguish two separate modules that estab-
lish self-learning: The word finding module works
on the acoustic level and attempts to automatically
induce the vocabulary of the user during training, by
associating recurring acoustic patterns (commands)
with observed changes in the user?s environment
(control). The grammar induction module works
alongside the word finding module to automatically
detect the compositionality of the user?s utterances,
further enabling the user to freely express com-
mands in their own words.
This paper presents a functional description of the
ALADIN learning framework and describes feasibi-
lity experiments with the word finding and grammar
induction modules. In Section 2 we outline the over-
all learning framework, the knowledge representa-
tion that is used and the rationale behind the word
finding and grammar induction modules. In Sec-
tion 3 we briefly describe the Patience corpus used
1Adaptation and Learning for Assistive Domestic Vocal
INterfaces. Project page: http://www.esat.kuleuven
.be/psi/spraak/projects/ALADIN
34
Word finding Input: Audio 
Grammar induction 
Semantics Frame  Description  
Grammar Frame  Description  Input: Controls Device State  Output: Controls Device State  
Figure 1: Schematic overview of the ALADIN framework.
in the feasibility experiments, as well as the experi-
mental setup. In Section 4 we show and discuss our
experimental results and we present our conclusions
and thoughts on future work in Section 5.
2 The ALADIN framework
The ALADIN learning framework consists of sev-
eral modules, which are shown schematically in
Fig. 1. On the left-hand side, the provided input is
shown, which consists of a spoken utterance (com-
mand) coupled with a control input, such as the but-
ton press on a remote control or a mouse click, pos-
sibly augmented with the internal state of a device
(for example the current volume of a television).
In order to provide a common framework for all
possible actions we wish to distinguish, we adopt
the use of frames, a data structure that encapsulates
the control inputs and/or device states relevant to the
execution of each action. Frames consist of one or
multiple slots, which each can take a single value
from a set of predefined values. In Section 2.1 we
discuss the frame representation in detail.
During training, the word finding module builds
acoustic representations of recurring acoustic pat-
terns, given a (small) set of training commands, each
described by a frame description and features ex-
tracted from the audio signal. Using the frame de-
scription, the module maps such acoustic representa-
tions to each slot-value pair in each frame. When us-
ing the framework for decoding spoken commands,
the output of the module is a score for each slot-
value pair in each frame, representing the probabil-
ity that this slot-value pair was present in the spoken
command.
During training, the grammar induction module
builds a model of the grammatical constructs em-
ployed by the user, using the frame description and
the output of the word finding module. The output of
the word finding module consists of estimates of the
slot-value pair scores described above, based on the
presence of automatically derived recurring acoustic
patterns.
The semantics module, operational during decod-
ing, processes the output of the word finding mod-
ule to create a single frame description most likely
to match the spoken command. This can then be
converted to a control representation the target de-
vice can work with. The module can make use of
a grammar module that describes which slot-value
pair combinations (and sequences) are likely to oc-
cur for each frame. Such a grammar description
should ideally be provided by the grammar induc-
tion module, but could optionally be hand-crafted.
2.1 Frame description
Each action that can be performed with a device is
represented in the form of a frame. A frame is a
data structure that represents the semantic concepts
that are relevant to the execution of the action and
which users of the command and control (hence-
forth C&C) application are likely to refer to in their
commands. It usually contains one or multiple slots,
each associated with a single value. The slots in
an action frame represent relevant properties of the
action. Such frame-based semantic representations
have previously been successfully deployed in C&C
applications and spoken dialog systems (Wang et al,
2005).
For our research, we distinguish three types of
frames. The first, the action frame, is automatically
generated during training by the device that is con-
trolled with a conventional control method, such as
button presses. Depending on the frame, more slots
may be defined than are likely to be referred to in any
single command. The second frame type, the oracle
35
Frame Slot Value
<from suit> c
<from value> 11
<from column> 3
<from hand> -
<to suit> h
<to value> 12
<to foundation> -
<to column> 4
Figure 2: An example of a Patience move and the
automatically generated movecard action frame. A
card is defined as the combination of a suit - (h)earts,
(d)iamonds, (c)lubs or (s)pades - and a value, from ace
(1) to king (13). We also distinguish slots for the ?hand? at
the bottom, the seven columns in the center of the playing
field and the four foundation stacks at the top right.
action frame, is a manually constructed subset of the
action frame based on a transcription of the spoken
command. In this subset, only those slots that are
referred to in the spoken command, are filled in. Fi-
nally, we define the oracle command frame, which is
a version of the oracle action frame that can assign
multiple values to each slot in order to deal with pos-
sible ambiguities in the spoken command.
We will illustrate these frame types with an exam-
ple from one the target applications in the ALADIN
project: a voice-controlled version of the card game
Patience. In this game, one of the possible actions
is moving a card in the playing field. This action
is described by an action frame dubbed movecard,
which contains slots specifying which card is moved
and to which position it is moved. Fig. 2 shows an
example of such a move, and the automatically gen-
erated action frame description of that move.
For instance, if the move in Fig. 2 was asso-
ciated with the spoken command ?put the jack of
clubs on the red queen?, the oracle action frame
of that particular move would only have the fol-
lowing slot values filled in: <from suit>=c,
<from value>=11, <to suit>=h and
<to value>=12, since the columns are not
referred to in the spoken command. Also, since no
slot was defined that is associated with the color of
the card, the spoken command is ambiguous and
during decoding, such a command might also be as-
sociated with a frame containing the slot-value pair
<to suit>=d. As a result, the oracle command
frame will be constructed with <to suit>=h,d
rather than <to suit>=h.
2.2 Word finding
The word finding module is tasked with creating
acoustic representations of recurring acoustic pat-
terns, guided by action frames. As such, the learning
task is only weakly supervised: rather than having
knowledge of the sequence of words that were spo-
ken, as common in Automatic Speech Recognition
(ASR), we only have knowledge of the slot-value
pairs in the action frame, each of which may have
been referred to in the utterance with one or multi-
ple words, and in any order. To meet these require-
ments, we turn to a technique called non-negative
matrix factorization (NMF).
2.2.1 Supervised NMF
NMF is an algorithm that factorizes a non-
negative M?N matrix V into a non-negative M?R
matrix W and a non-negative R ? N matrix H:
V ? W ? H. In our approach, we construct the
NMF problem as follows:
V =
[
V0
V1
]
?
[
W0
W1
]
H = WH (1)
with the matrix V1 composed of N spoken com-
mands, each represented by a vectorial representa-
tion of dimension M1. The columns of V0 asso-
ciate each spoken command with a label vector of
dimension M0 that represents the frequency with
36
which a particular label occurred in that spoken
command. After factorization, the matrix W1 con-
tains R acoustic patterns of dimension M1, and the
matrix H indicates the weights with which these R
acoustic patterns are linearly combined for each spo-
ken command n, 1 ? n ? N , to form the observed
spoken commands in V1. The columns of the ma-
trix W0 describe the mapping between the R acous-
tic patterns in W1 and the M0 labels that can be
associated with each spoken command. In addition
to columns of W1 associated with labels, we use
a number of so-called ?garbage columns? to capture
acoustic representations not associated with labels,
for example to capture less meaningful words such
as ?please?.
To decode a spoken command (the ?testing?
phase), we find a vector h for which holds: v1tst =
W1htst, with W1 the matrix found during train-
ing. vtst1 is the M1 dimensional acoustic represen-
tation of the spoken command we wish to decode,
and htst is the R-dimensional vector that indicates
which acoustic patterns in W1 need to be linearly
combined to explain vtst1 . Finally, we calculate the
label association with the spoken command vtst1 us-
ing: a = W0htst, where a is a M0 dimensional
vector giving a score for each label.
For more details on how to carry out these fac-
torizations, we refer the reader to Lee and Seung
(1999). For a discussion on representing spoken
commands of varying length as a M1-dimensional
vector, and the constraints under which it holds that
the spoken command is the linear combination of
R such vectors from W1, we refer the reader to
(Van hamme, 2008; Driesen and Van hamme, 2012;
Driesen et al, 2012) and the references therein.
2.2.2 Frame decoding
In our framework, we consider each unique
slot-value pair of each frame (for example
<to suit>=h of the frame movecard) as
a single label, making the total number of labels
M0 equal to the cumulative number of different
values in all slots in all frames. This way, each
frame description is uniquely mapped to a binary
vector v1, and likewise, the decoded label vector a
is uniquely mapped back to a frame description.
Put the jack of clubs on the queen of hearts
O O I FV O I FS O O I TV O I TS
Figure 3: Example of a command transcription, annotated
with concept tags.
2.3 Grammar induction
The task of the grammar module is to automatically
induce a grammar during the training phase, that de-
tects the compositionality of the utterances and re-
lates it to the associated meaning. In this case, the
grammatical properties of the utterances are associ-
ated with action frames, containing slots and values.
This grammar induction is performed on the basis
of the output of the word finding module (hypothe-
sized ?word? units, represented as acoustic patterns
and possibly associated frame slot values) and the
generated frame descriptions of the actions. Further-
more, the grammar may also serve as an additional
aid during the decoding process, by providing infor-
mation regarding the probability of specific frame
slot sequences in the data.
There are different options with respect to the type
of grammar that can be induced. It could for instance
be a traditional context-free grammar, meaning that
the contents of the frame description of the action
are derived on the basis of a parse tree of the ut-
terance. Unfortunately, context-free grammars have
been proven to be very hard to automatically induce
(de Marcken, 1999; Klein, 2005), particularly on the
basis of limited training data.
Encouraging results have been reported in the un-
supervised induction of sequence tags (Collobert et
al., 2011). In the context of the ALADIN project,
we therefore decided to adopt a concept tagging ap-
proach as a shallow grammar interface between ut-
terance and meaning. In this vein, each command is
segmented into chunks of words, which are tagged
with the semantic concepts (i.e. frame slots) to
which they refer.
We use a tagging framework which is based on
so-called IOB tagging, commonly used in the con-
text of phrase chunking tasks (Ramshaw and Mar-
cus, 1995). Words inside a chunk are labeled with a
tag starting with I and words outside the chunks are
labeled with an O tag, which means that they do not
refer to any concept in the action frame. Fig. 3 illus-
trates the concept tagging approach for an example
command.
37
3 Experimental setup
The experiments described in this paper pertain to
a vocal interface for the card game Patience. This
presents an appropriate case study, since a C&C in-
terface for this game needs to learn a non-trivial,
but fairly restrictive vocabulary and grammar. Com-
mands such as ?put the four of clubs on the five of
hearts? or ?put the three of hearts in column four?
are not replaceable by holistic commands, and iden-
tifying the individual components of the utterance
and their interrelation is essential for the derivation
of its meaning. This makes the Patience game a
more interesting test case than domotica applica-
tions such as controlling lights, doors or a television,
where the collection of unordered sets of keywords
is usually sufficient to understand the commands.
In this section, we will describe the corpus col-
lected to enable this case study, as well as the setup
for exploratory experiments with the techniques out-
lined in Section 2.
3.1 Patience corpus
The Patience corpus consists of more than two thou-
sand spoken commands in (Belgian) Dutch2, tran-
scribed and manually annotated with concept tags.
Eight participants were asked to play Patience on a
computer using spoken commands, which were sub-
sequently executed by the experimenter. The partic-
ipants were told to advance the game by using their
own commands freely, in terms of vocabulary and
grammatical constructs. The audio signals of the
commands were recorded and the associated actions
were stored in the form of action frames. There are
two types of frames: a movecard frame, describ-
ing the movement of a card on the playing field (e.g.
Fig. 2), and a dealcard frame that contains no
frame slots, but simply triggers a new hand. Oracle
action and command frames were derived on the ba-
sis of the automatically generated action frames and
the manually annotated concept tags.
Each participant played in two separate sessions,
with at least three weeks in between, so as to capture
potential variation in command use over time. The
participants? ages range between 22 and 73 and we
balanced for gender and education level. We col-
lected between 223 and 278 commands (in four to
2Note however that the ALADIN system is inherently lan-
guage independent, which is why we present the examples in
English.
six games) per participant. The total number of col-
lected commands is 2020, which means an average
of 253 commands per participant and the average
number of moves per game is 55. The total num-
ber of frame slot-value pairs is 63.
The experimental setup tries to mimic the
ALADIN learning situation as much as possible.
For each participant, a separate learning curve was
made, since the learning process in the targeted
ALADIN application will be personalized as well.
For each learning curve, the last fifty utterances of
a participant were used as a constant test set. The
remaining utterances of the same participant were
used as training material. The chronological order
of the commands, as they were uttered by the partic-
ipant, was preserved, in order to account for the de-
velopment of the users? command structure and vo-
cabulary use during the games. In each experiment,
the first k utterances were used as training data, k be-
ing an increasing number of slices of ten utterances
for the grammar induction experiments and 25 utter-
ances for the word finding experiments.
3.2 Word finding
Spoken commands are represented by a His-
togram of Acoustic Co-occurrence (HAC) features
(Van hamme, 2008), constructed as follows: First,
we extract mel-cepstral coefficients (MFCC) from
audio signals sampled at 16kHz, framed using time
windows of 25ms and shifted in increments of 10ms.
From each of these frames, 13 cepstral coefficients,
along with their first and second order differences
are determined, yielding a 39 dimensional feature
vector. Mean and variance normalization are applied
on a per-utterance basis. Second, k-means clus-
tering of 50000 randomly selected frames is used
to create a Vector Quantization codebook with 200
codewords for each speaker, using k-means cluster-
ing. Finally, three sets of HAC features are con-
structed by counting the co-occurrences of the au-
dio expressed as VQ codewords, with time lags of 2,
5 and 9 frames. The final feature dimension M1 is
thus M1=3? 2002 = 120000.
In these initial experiments, we use the oracle ac-
tion frames to provide supervision. In the NMF
learning framework, two acoustic representations
were assigned to each label, with an additional 15
representations used as garbage columns. The to-
tal number of acoustic representations R is thus
38
R = 2 ? 63 + 15 = 141. For training, W1 is ini-
tialized randomly and W0 is initialized so that two
columns are mainly associated with each label (i.e.,
a one in the corresponding label position and a small
([0, 1e? 5]) random value for the other labels). The
remaining 15 garbage columns are randomly initial-
ized. Finally, the entries of V1 and V0 are scaled
so their cumulative weight is equal. During training,
the rows of H pertaining to non-garbage columns
in W0 are initialized to be the same as V0, with
a small ([0, 1e ? 5]) random value replacing values
that are zero. The rows of H pertaining to garbage
columns are initialized randomly. For the NMF fac-
torization, we minimized the Kullback-Leibler di-
vergence using 100 iterations of the procedure de-
scribed in Lee and Seung (1999).
In these experiments, frame decoding is guided
by a hand-crafted grammar, rather than an auto-
matically induced grammar. We defined 38 gram-
mar rules corresponding to various possible slot se-
quences, under the assumption that from slots pre-
cede to slots, and that suit slots precede value
slots. These 38 rules also include various slot se-
quences in which the command was underspecified.
A pilot experiment showed that this grammar cov-
ers 98% to 100% of the spoken commands, depend-
ing on the speaker. The hand-crafted grammar was
implemented as a labelvector-to-labelvector bigram
transition matrix, and Viterbi decoding was used to
generate a possible frame description for each gram-
mar rule. For scoring, the most likely frame descrip-
tion was selected based on the most likely Viterbi
path across grammar rules. Finally, we express re-
sults in terms of slot-value accuracy, which is the
ratio of the number of slot-value pairs correctly se-
lected, according to the oracle command frame, and
the total number of slot-value pairs in the oracle
command frame (expressed as a percentage).
3.3 Grammar induction
The exploratory experiments for the grammar in-
duction module serve as a proof-of-the-principle ex-
periment that showcases the learnability of the task
in optimal conditions and focuses on the minimally
required amount of training data needed to boot-
strap successful concept tagging. In these super-
vised learning experiments, the annotated corpus is
used as training material for a data-driven tagger,
which is subsequently used to tag previously unseen
data. As our tagger of choice, we opted for MBT,
the memory-based tagger (Daelemans et al, 2010),
although any type of data-driven tagger can be used.
In the targeted ALADIN application, the number
of utterances used to train the system should be as
small as possible, i.e. the training phase should be
as brief as possible in order to limit the amount of
extraneous physical work or assistance needed for
training by the physically impaired person. In or-
der to get an idea of the minimal number of train-
ing utterances needed to enable successful concept
tagging, we evaluated the supervised tagging perfor-
mance with increasing amounts of training data, re-
sulting in learning curves.
The metric used for the evaluation of the con-
cept tagger is the micro-averaged F-score of the pre-
dicted I chunks: the harmonic mean of the pre-
cision and recall of the chunks with I labels (i.e.
referring to slots in de frame description). This
means that the concept tags as well as the boundaries
of the predicted chunks are included in the evalu-
ation. Feature selection was performed on the ba-
sis of a development set (last 25% of the training
data) and establishes the best combination of disam-
biguation features, such as the number of (disam-
biguated) concept tags to the left, the tokens them-
selves (left/right/focus) and ambiguous tags (focus
token and right context). We compare our results
against a baseline condition, in which only the focus
word is used as a feature, in order to see the rela-
tive effect of the use of context information by the
tagger.
4 Results and discussion
4.1 Word finding
In Fig. 4a we can observe the results obtained with
a learning framework that combines word finding
with hand-crafted grammars. From these results, we
can observe that the slot-value accuracy obtained af-
ter using all available training material, varies be-
tween 40.4% for speaker 4 and 76.0% for speaker 1.
We can also observe that overall, the results for all
speakers show a fairly linear increase in accuracy as
more training material becomes available. The fact
that we do not yet observe that the accuracy levels
off with increasing training data, indicates that the
results are likely to further improve with more train-
ing data.
39
0 50 100 150 200 250
20
40
60
80
100
Size of training set (#utterances)
Slot
?val
ue a
ccur
acy 
(%)
1
1 1
1 1
1
1
1
1
2 2
2 2
2
2 2 2 2
3
3 3
3 3 3
3 3
3
3
4
4
4
4 4 4 4
4
5 5 5
5 5 5
5
6 6
6
6 6
6 6
6
7 7
7
7 7
7 7 7
8
8 8
8 8 8 8
8 8
p participant p
(a) Word finding results
0 50 100 150 200 250
20
40
60
80
100
Size of training set (#utterances)
I?Ch
unk 
F?s
core
 (%)
1 1
1
1 1 1 1
1
1 1
1 1 1 1 1 1 1 1 1 1 1 11
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
3 3 3 3 3
3 3
3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
4
4
4
4
4 4 4 4 4 4 4
4 4 4 4 4 4 4 4 45 5
5 5 5 5 5 5 5 5 5 5 5 5 5 5 55
6
6 6 6
6 6 6 6 6 6
6 6
6 6 6 6 6 6 6
7 7
7 7 7 7 7 7 7 7
7
7 7 7 7 7 7 7 7
8
8
8 8 8
8 8
8 8 8 8 8 8 8 8 8 8 8 8 8 88
p participant paverage baseline
(b) Grammar induction results
Figure 4: Learning curves viz. word finding accuracy (left) and grammar induction I chunk F-score (right).
This is also likely given the complexity of the
learning task: In Patience, about half of the spoken
commands pertains to dealcard frames, which
means it is very likely that some slot-value pairs
have never even occurred in the training data, even
after 200 spoken commands. We expect, however,
that we need at least a few repetitions of each slot-
value pair to build a robust acoustic representation:
the accuracy of correctly detecting the dealcard
frame, which has many repetitions in the training
data, is close to 100% for all speakers. Given such
data scarcity, the fact that we obtain accuracies up to
76% is encouraging.
Another observation that can be made is that for
some speakers, such as speaker 1, there is a larger
variation between consecutive training sizes - for ex-
ample for speaker 1 the best accuracy is obtained for
a training size of 175 spoken commands. There are
several possible reasons. For one, even though the
NMF learning problem is initialized using the con-
straints imposed by the frame labeling, the factor-
ization process may not achieve the global optimal
solution during training. This could be addressed by
performing multiple experiments with different ran-
dom initializations (Driesen et al, 2012).
Another issue is that the number of dealcard
frames varies between speakers, due to the rela-
tively small test set size of fifty spoken utterances.
With the dealcard typically recognized correctly,
this may account both for some of the differences
between speakers, as well as for the variation be-
tween training sizes observed for some speakers: If
the number of movecard frames in the test set is
small, this makes the average accuracy more sen-
sitive to errors on these frames. This issue could
be addressed by an alternative evaluation scheme in
which multiple occurrences of the same utterance
are only counted once.
4.2 Grammar induction
Fig. 4b displays the learning curves for the super-
vised concept tagging experiments. There is a large
amount of variation between the participants in ac-
curacy using the first 100 training utterances. Six out
of eight curves reach 95% or more with 130 train-
ing utterances, and level off after that. For two par-
ticipants, the accuracies reach 100%, with training
set sizes of 40 and 100 utterances respectively. The
baseline accuracies, averaged across all participants,
are also shown in Fig. 4b. These are significantly su-
perseded by the individual learning curves with op-
timized features, showing that the use of context in-
40
formation is important to enable successful concept
tagging on this dataset.
The fact that the tag accuracy for participant 6
remains relatively low (around 88%) is mainly due
to a rather high level of inconsistency and ambigu-
ity in the command structures that were used. One
remarkable source of errors in this case is a struc-
ture repeatedly occurring in the test set and occur-
ring only twice in the largest training set. It is a par-
ticularly difficult one: a structure in which multiple
cards are specified to be moved (in one pile), such
as in ?the black three, the red four and the black five
to the red six?. In such cases, only the highest card
of the moved pile (black five in the example) should
be labeled with I FS and I FV tags (since only that
card is represented in the action frame) and the lower
cards should be tagged with O tags.
The commands given by participants 2 and 5 were
structurally very consistent throughout the games,
resulting in very fast learning. Participant 5?s learn-
ing curve reaches a tag accuracy of 100% using
as little as forty training utterances, underlining the
learnability of this task in optimal conditions. Par-
ticipant 3?s curve reaches 100% accuracy, but has
a dip at the beginning of the curve. This is due to
the fact that in the utterance numbers 20-50, the suit
specification was often dropped (e.g. ?the three on
the four?), whereas in the utterances before and after
that, the suit specification was often included.
5 Conclusions and future work
In this paper, we introduced a self-learning frame-
work for a vocal interface that can be used with any
language, dialect, vocabulary and grammar. In ad-
dition to a description of the overall learning frame-
work and its internal knowledge representation, we
described the two building blocks that will provide
vocabulary learning and grammar induction. Our
experiments show encouraging results, both for vo-
cabulary learning and grammar induction, when ap-
plied to the very challenging task of a vocally guided
card game, Patience, with only limited training data.
Although the word finding experiments use the or-
acle action frames rather than the automatically gen-
erated frames as supervision information, the pre-
liminary experiments shown in this work are promis-
ing enough to have confidence that even with this
additional source of uncertainty, the goal of a self-
learning vocal interface is feasible. The concept tag-
ging experiments show that this type of representa-
tion is learnable in a supervised way with a high de-
gree of accuracy on the basis of a relatively limited
amount of data.
Future experiments will investigate how unsuper-
vised learning techniques can be used to bootstrap
concept tagging without using annotated and manu-
ally transcribed data. This will enable the output of
the grammar module to replace the manually crafted
grammar currently used by the word finding mod-
ule. Since the learning curves for the word finding
module still show significant room for improvement,
more data will need to be collected to adequately in-
vestigate the interaction between the two modules.
We expect the word finding results to improve
once speaker-specific grammars, provided by the
grammar induction module, can be incorporated.
The hand-crafted grammar employed in the word
finding experiments include almost all variations,
while a speaker-specific grammar will typically be
more restrictive. Another practical approach to im-
prove the user experience is to have the ALADIN sys-
tem produce an ordered set of several possible frame
descriptions, based on the knowledge of the playing
field and the rules of the game. Preliminary experi-
ments revealed that even with a small ordered set of
only five frame candidates, the slot-value accuracy
of the Patience word finding experiments increased
by 10% to 20% absolute. Furthermore, we expect
the number of repetitions needed for each slot-value
pair to reduce substantially if we allow sharing of
the acoustic representations between slots. For ex-
ample, it is very likely that the user will refer to the
suit of ?hearts? the same way, regardless of whether
it occurs in a from slot or in a to slot.
While the self-learning modules have not yet been
integrated and while there is still ample room for im-
provement within each module individually, the re-
sults of the feasibility experiments described in this
paper are encouraging. The insights gained from
these experiments form a solid basis for further ex-
perimentation and will serve to further streamline
the development of a language independent, self-
learning command & control vocal interface for peo-
ple with a physical impairment.
Acknowledgments
This research was funded by IWT-SBO grant
100049 (ALADIN).
41
References
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2461?2505.
W. Daelemans, J. Zavrel, A. van den Bosch, and K. Van
der Sloot. 2010. MBT: Memory-based tagger, version
3.2, reference guide. Technical Report 10-04, Univer-
sity of Tilburg.
C. de Marcken. 1999. On the unsupervised induc-
tion of phrase-structure grammars. In S. Armstrong,
K. Church, P. Isabelle, S. Manzi, E. Tzoukermann, and
D. Yarowsky, editors, Natural Language Processing
Using Very Large Corpora, volume 11 of Text, Speech
and Language Technology, pages 191?208. Kluwer
Academic Publishers.
J. Driesen and H. Van hamme. 2012. Fast word
acquisition in an NMF-based learning framework.
In Proceedings of the 36th International Conference
on Acoustics, Speech and Signal Processing, Kyoto,
Japan.
J. Driesen, J.F. Gemmeke, and H. Van hamme. 2012.
Weakly supervised keyword learning using sparse rep-
resentations of speech. In Proceedings of the 36th In-
ternational Conference on Acoustics, Speech and Sig-
nal Processing, Kyoto, Japan.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
D.D. Lee and H.S. Seung. 1999. Learning the parts of
objects by nonnegative matrix factorization. Nature,
401:788?791.
J. Noyes and C. Frankish. 1992. Speech recognition
technology for individuals with disabilities. Augmen-
tative and Alternative Communication, 8(4):297?303.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the Third ACL Workshop on Very Large Corpora,
pages 82?94, Cambridge, USA.
H. Van hamme. 2008. Hac-models: a novel approach to
continuous speech recognition. In Proceedings Inter-
national Conference on Spoken Language Processing,
pages 2554?2557, Brisbane, Australia.
Y. Wang, L. Deng, and A. Acero. 2005. An introduction
to statistical spoken language understanding. IEEE
Signal Processing Magazine, 22(5):16?31.
42
