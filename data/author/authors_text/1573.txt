 Evaluating Question-Answering Techniques in Chinese 
 
Xiaoyan Li  and  W. Bruce Croft 
Computer Science Department 
University of Massachusetts, Amherst, MA 
{xiaoyan, croft}@cs.umass.edu 
 
ABSTRACT 
An important first step in developing a cross-lingual 
question answering system is to understand whether 
techniques developed with English text will also work with 
other languages, such as Chinese. The Marsha Chinese 
question answering system described in this paper uses 
techniques similar to those used in the English systems 
developed for TREC. Marsha consists of three main 
components: the query processing module, the Hanquery 
search engine, and the answer extraction module. It also 
contains some specific techniques dealing with Chinese 
language characteristics, such as word segmentation and 
ordinals processing.  Evaluation of the system is done using 
a method based on the TREC question-answering track. 
The results of the evaluation show that the performance of 
Marsha is comparable to some English question answering 
systems in TREC 8 track. An English language version of 
Marsha further indicates that the heuristics used are 
applicable to the English question answering task.   
 
Keywords 
Question-Answering (QA); Search engine; multilingual 
retrieval, Chinese QA. 
 
1. Introduction 
A number of techniques for ?question answering? have 
recently been evaluated both in the TREC environment 
(Voorhees and Harman, 1999) and in the DARPA TIDES 
program. In the standard approach to information retrieval, 
relevant text documents are retrieved in response to a 
query. The parts of those documents that may contain the 
most useful information or even the actual answer to the 
query are typically indicated by highlighting occurrences of 
query words in the text. In contrast, the task of a question-
answering system is to identify text passages containing the 
relevant information and, if possible, extract the actual 
answer to the query. Question answering has a long history 
in natural language processing, and Salton?s first book 
(Salton, 1968) contains a detailed discussion of the 
relationship between information retrieval and question-
answering systems. The focus in recent research has been 
on extracting answers from very large text databases and 
many of the techniques use search technology as a major 
component. A significant number of the queries used in 
information retrieval experiments are questions, for 
example, TREC topic 338 ?What adverse effects have 
people experienced while taking aspirin repeatedly?? and 
topic 308 ?What are the advantages and/or disadvantages of 
tooth implants?? In question-answering experiments, the 
queries tend to be more restricted questions, where answers 
are likely to be found in a single text passage, for example, 
TREC question-answering question 11 ?Who was President 
Cleveland?s wife?? and question 14 ?What country is the 
biggest producer of Tungsten?? 
The TREC question-answering experiments have, to date, 
used only English text. As the first step towards our goal of 
cross-lingual question answering, we investigated whether 
the general approaches to question answering that have 
been used in English will also be effective for Chinese. 
Although it is now well known that statistical information 
 
 
 
retrieval techniques are effective in many languages, earlier 
research, such as Fujii and Croft (1993, 1999), was helpful 
in pointing out which techniques were particularly useful 
for languages like Japanese. This research was designed to 
provide similar information for question answering. In the 
next section, we describe the components of the Chinese 
question answering system (Marsha) and the algorithm used 
to determine answers. In section 3, we describe an 
evaluation of the system using queries obtained from 
Chinese students and the TREC-9 Chinese cross-lingual 
database (164,779 documents from the Peoples Daily and 
the Xing-Hua news agencies in the period 1991-1995). 
2. Overview of the Marsha Question 
Answering System 
The Chinese question-answering system consists of three 
main components. These are the query processing module, 
the Hanquery search engine, and the answer extraction 
module. The query processing module recognizes known 
question types and formulates queries for the search engine. 
The search engine retrieves candidate texts from a large 
database. The answer extraction module identifies text 
passages that are likely to contain answers and extracts 
answers, if possible, from these passages. This system 
architecture is very similar to other question-answering 
systems described in the literature. 
More specifically, the query processing module carries out 
the following steps: 
(1) The query is matched with templates to decide the 
question type and the ?question words? in the query. We 
define 9 question types. Most of these correspond to typical 
named entity classes used in information extraction systems. 
For each question type, there are one or more templates. 
Currently there are 170 templates.  If more than one 
template matches the question, we pick the longest match. 
For example, a question may include? ?(how many 
dollars). Then both (how many dollars) and 
(how many) will match the question. In this case, we will 
pick and assign ?MONEY? to the question type. 
The following table gives examples for each question type: 
TEMPLATE QUESTION 
TYPE 
 TRANSLATION 
 
PERSON which person 
 
LOCATION which city 
 
ORGANIZATIO
N 
what organization 
 
DATE what date 
 
TIME what time 
 
MONEY how many dollars 
 
PERCENTAGE what is the 
percentage
 
 NUMBER how many 
 
OTHER what is the meaning 
of  
 
(2) Question words are removed from the query. This is a 
form of ?stop word? removal. Words like ? ?
(which person) are removed from the query since they are 
unlikely to occur in relevant text. 
(3) Named entities in the query are marked up using BBN?s 
IdentiFinder system. A named entity is kept as a word after 
segmentation. 
(5) The query is segmented to identify Chinese words. 
(6) Stop words are removed. 
(7) The query is formulated for the Hanquery search engine. 
Hanquery is the Chinese version of Inquery (Broglio, 
Callan and Croft, 1996) and uses the Inquery query 
language that supports the specification of a variety of 
evidence combination methods. To support question 
answering, documents containing most of the query words 
were strongly preferred. If the number of query words left 
after the previous steps is greater than 4, then the operator 
#and (a probabilistic AND) is used. Otherwise, the 
probabilistic passage operator #UWn (unordered window) 
is used. The parameter n is set to twice the number of words 
in the query. 
Hanquery is used to retrieve the top 10 ranked documents. 
The answer extraction module then goes through the 
following steps: 
(8) IdentiFinder is used to mark up named entities in the 
documents. 
(9) Passages are constructed from document sentences. We 
used passages based on sentence pairs, with a 1-sentence 
overlap. 
(10) Scores are calculated for each passage. The score is 
based on five heuristics: 
?  First Rule: 
Assign 0 to a passage if no expected name entity is present. 
?  Second Rule:  
Calculate the number of match words in a passage. 
Assign 0 to the passage if the number of matching words is 
less than the threshold. Otherwise, the score of this passage  
is equal to the number of matching words (count_m). 
The threshold is defined as follows: 
threshold = count_q   if count_q<4 
threshold = count_q/2.0+1.0  if 4<=count_q<=8 
threshold = count_q/3.0+2.0  if count_q>8
count_q is the number of words in the query. 
?  Third Rule: 
Add 0.5 to score if all matching words are within one 
sentence. 
?  Fourth Rule: 
Add 0.5 to score if all matching words are in the same order 
as they are in the original question. 
?  Fifth Rule:  
score = score + count_m/(size of matching window) 
(11) Pick the best passage for each document and rank them. 
(12) Extract the answer from the top passage: 
Find all candidates according to the question type. For 
example, if the question type is LOCATION, then each 
location marked by IdentiFinder is an answer candidate. An 
answer candidate is removed if it appears in the original 
question. If no candidate answer is found, no answer is 
returned. 
Calculate the average distance between an answer candidate 
and the location of each matching word in the passage. 
Pick the answer candidate that has the smallest average 
distance as the final answer. 
3. Evaluating the System 
We used 51 queries to do the initial evaluation of the 
question-answering system. We selected 26 queries from 
240 questions collected from Chinese students in our 
department, because only these had answers in the test 
collection. The other 25 queries were constructed by either 
reformulating a question or asking a slightly different 
question. For example, given the question ?which city is the 
biggest city in China??  we also generated the questions 
?where is the biggest city in China?? and ?which city is the 
biggest city in the world??. 
The results for these queries were evaluated in a similar, but 
not identical way to the TREC question-answering track. 
An ?answer? in this system corresponds to the 50 byte 
responses in TREC and passages are approximately 
equivalent to the 250 byte TREC responses. 
For 33 of 51 queries, the system suggested answers. 24 of 
the 33 were correct. For these 24, the ?reciprocal rank? is 1, 
since only the top ranked passage is used to extract answers. 
Restricting the answer extraction to the top ranked passage 
also means that the other 27 queries have reciprocal rank 
values of 0. In TREC, the reciprocal ranks are calculated 
using the highest rank of the correct answer (up to 5). In our 
case, using only the top passage means that the mean 
reciprocal rank of 0.47 is a lower bound for the result of the 
50 byte task. 
 As an example, the question ?
? (Which city is the biggest city in China?), the answer 
returned is  (Shanghai). In the top ranked passage, 
?China? and ?Shanghai? are the two answer candidates that 
have the smallest distances. ?Shanghai? is chosen as the 
final answer since ?China? appears in the original question.  
As an example of an incorrect response, the question ?
? (In which year did Jun Xie defeat a Russian player and 
win the world chess championship for the first time?) 
produced an answer of  (today). There were two 
candidate answers in the top passage, ?October 18? and 
?today?. Both were marked as DATE by Identifinder, but 
?today? was closer to the matching words. This indicates 
the need for more date normalization and better entity 
classification in the system. 
For 44 queries, the correct answer was found in the top-
ranked passage. Even if the other queries are given a 
reciprocal rank of 0, this gives a mean reciprocal rank of 
0.86 for a task similar to the 250 byte TREC task. In fact, 
the correct answer for 4 other queries was found in the top 
5 passages, so the mean reciprocal rank would be somewhat 
higher. For 2 of the remaining 3 queries, Hanquery did not 
retrieve a document in the top 10 that contained an answer, 
so answer extraction could not work. 
4. Further Improvements 
These results, although preliminary, are promising. We 
have made a number of improvements in the new version 
(v2) of the system. Some of these are described in this 
section. 
One of the changes is designed to improve the system?s 
ability to extract answers for the questions that ask for a 
number. A number recognizer was developed to recognize 
numbers in Chinese documents. The numbers here are 
numbers other than DATE, MONEY and PERCENTAGE 
that are recognized by IdentiFinder. The version of 
IdentiFinder used in our system can only mark up seven 
types of name entities and this limits the system?s ability to 
answer other types of questions. The number recognizer is 
the first example of the type of refinement to named entity 
recognition that must be done for better performance. 
An example of a question requiring a numeric answer is:  
? ? (What is the number of 
Clinton?s presidency?)?. This question could be answered 
in Marsha v2 by extracting the marked up number from the 
best passage in the answer extraction part, while Marsha v1 
could only return the top 5 passages that were likely to have 
the answer to this question.  
Another improvement relates to the best matching window 
of a passage. The size of the matching window in each 
passage is an important part of calculating the belief score 
for the passage. Locating the best matching window is also  
important in the answer-extraction processing because the 
final answer picked is the candidate that has the smallest 
average distance from the matching window. The best 
matching window of a passage here is the window that has 
the most query words in it and has the smallest window 
size. In the previous version of our system, we only 
consider the first occurrence of each query word in a 
passage and index the position accordingly. The matching 
window is thus from the word of the smallest index to the 
word of the largest index in the passage. It is only a rough 
approximation of the best matching window though it 
works well for many of the passages. In the second version 
of Marsha, we developed a more accurate algorithm to 
locate the best matching window of each passage. This 
change helped Marsha v2 find correct answers for some 
questions that previously failed. The following is an 
example of such a question. 
For the question ? ? 
(How many people in the United States are below the 
poverty line?)?  
The best passage is as follows: 
  ?
? 
 This passage has two occurrences of query word ? ?. 
In v1, the first occurrence of ? ? is treated as the start of 
the matching window, whereas the second occurrence is 
actually the start of the best matching window.  There are 
two numbers ? ? (more than 2 million) and ?
? (33.585 million) in the passage. The right 
answer ? ? (33.585 million) is nearer to the 
best matching window and  ? ? (more than 2 
million) is nearer to the estimated matching window. 
Therefore, the right answer can be extracted after correctly 
locating the best matching window.  
The third improvement is with the scoring strategies of 
passages. Based on the observation that the size of the best 
matching window of a passage plays a more important role 
than the order of the query words in a passage, we adjusted 
the score bonus for same order satisfaction from 0.5 to 
0.05.  This adjustment makes a passage with a smaller 
matching window get a higher belief score than a passage 
that satisfies the same order of query words but has a bigger 
matching window. As an example, consider the question: 
 ? ? (Who was the first president in 
the United States?)?. 
 Passage 1 is the passage that has the right answer ?
?. 
Passage 1. 
?  #pn: #pm:
#xh:5#lm: #ti: #au:
#rw: #rw: #rw:
? 
Passage 2. 
?, ? 
Passage 1 and Passage 2 both have all query words. The 
size of the best matching window in Passage 1 is smaller 
than that in Passage 2 while query words in Passage 2 have 
the same order as that in the question. The scoring strategy 
in Marsha v2 selects Passage 1 and extracts the correct 
answer while Marsha v1 selected Passage 2.  
Special processing of ordinals has also been considered in 
Marsha v2. Ordinals in Chinese usually start with the 
Chinese character " " and are followed by a cardinal. It is 
better to retain ordinals as single words during the query 
generation in order to retrieve better relevant documents. 
However, the cardinals (part of the ordinals in Chinese) in a 
passage are marked up by the number recognizer for they 
might be answer candidates for questions asking for a 
number. Thus ordinals in Chinese need special care in a QA 
system. In Marsha v2, ordinals appearing in a question are 
first retained as single words for the purpose of generating a 
good query and then separated in the post processing after 
relevant documents are retrieved to avoid answer 
candidates being ignored. 
5. Comparison with English Question 
Answering Systems 
Some techniques used in Marsha are similar to the 
techniques in English question answering systems 
developed by other researchers. The template matching in 
Marsha for deciding the type of expected answer for a 
question is basically the same as the one used in the 
GuruQA (Prager et al, 2000) except that the templates 
consist of Chinese word patterns instead of English word 
patterns. Marsha has the ability of providing answers to 
eight types of questions: PERSON, LOCATION, 
ORGANIZATION, DATE, TIME, MONEY, 
PERCENTAGE, and NUMBER. The first seven types  
correspond to the named entities from IdentiFinder 
developed by BBN. We developed a Chinese number-
recognizer ourselves which marks up numbers in the 
passages as answer candidates for questions asking for a 
number. The number could be represented as a digit 
number or Chinese characters.  David A. Hull used a proper 
name tagger ThingFinder developed at Xerox in his 
question answering system. Five of the answer types 
correspond to the types of proper names from ThingFinder 
(Hull, 1999). The scoring strategy in Marsha is similar to 
the computation of score for an answer window in the 
LASSO QA system (Moldovan et al, 1999) in terms of the 
factors considered in the computation. Factors such as the  
number of matching words in the passage, whether all 
matching words in the same sentence, and whether the 
matching words in the passage have the same order as they 
are in the question are common to LASSO and Marsha.    
 We have also implemented an English language version of 
Marsha. The system implements the answer classes 
PERSON, ORGANIZATION, LOCATION, and DATE. 
Queries are generated in the same fashion as Marsha. If 
there are any phrases in the input query (named entities 
from IdentiFinder, quoted strings) these are added to an 
Inquery query in a #N operator all inside a #sum operator. 
For example: 
 Question: "Who is the author of "Bad Bad Leroy 
Brown" 
Inquery query: #sum( #uw8(author Bad Bad Leroy 
Brown) #6(Bad Bad Leroy Brown)) 
Where N is number of terms + 1 for named entities, and 
number of terms + 2 for quoted phrases. If a query retrieves 
no documents, a ?back off? query uses #sum over the query 
terms, with phrases dropped. The above would become 
#sum(author Bad Bad Leroy Brown). 
The system was tested against the TREC9 question 
answering evaluation questions. The mean reciprocal rank 
over 682/693 questions was 0.300 with 396 questions going 
unanswered. The U.Mass. TREC9 (250 byte) run had a 
score of 0.367. Considering only the document retrieval, we 
find a document containing an answer for 471 of the 
questions, compared to 477 for the official TREC9 run 
which used expanded queries. This indicates that the 
Marsha heuristics have applicability to the English question 
answering task and are not limited to the Chinese question 
answering task. 
6. Summary and Future Work 
The evaluations on Marsha, although preliminary, indicate 
that techniques developed for question answering in English 
are also effective in Chinese. In future research, we plan to 
continue to improve these techniques and carry out more 
careful evaluations to establish whether there are any 
significant differences in the question-answering task 
between these two languages. 
The evaluation of the English version of Marsha indicates 
that the Marsha heuristics work well in English as well as in 
Chinese. We now plan to incorporate these techniques in a 
cross-lingual question-answering system for English and 
Chinese. By using two systems with similar question 
processing strategies, we hope to exploit the query 
templates to produce accurate question translations. 
We have also started to develop a probabilistic model of 
question answering using the language model approach 
(Ponte and Croft, 1998).  This type of model will be 
essential for extending the capability of QA systems beyond 
a few common query forms. 
 
Acknowledgements 
This material is based on work supported in part by the 
Library of Congress and Department of Commerce under 
cooperative agreement number EEC-9209623 and in part 
by SPAWARSYSCEN-SD grant number N66001-99-1-
8912. 
Any opinions, findings and conclusions or 
recommendations expressed in this material are the 
author(s) and do not necessarily reflect those of the 
sponsor. 
We also want to express out thanks to people at CIIR for 
their help. Special thanks to David Fisher who implemented 
the English language version of Marsha, and Fangfang Feng 
for his valuable discussions on Chinese related research 
issues. 
7. References 
Broglio, J., Callan, J.P. and Croft, W.B. ?Technical Issues 
in Building an Information Retrieval System for Chinese,? 
CIIR Technical Report IR-86, Computer Science 
Department, University of Massachusetts, Amherst, (1996). 
H. Fujii and W.B. Croft, ?A Comparison of Indexing 
Techniques for Japanese Text Retrieval,? Proceedings of 
SIGIR 93, 237-246, (1993). 
H. Fujii and W.B. Croft, ?Comparing the performance of 
English and Japanese text databases?, in S. Armstrong et al
(eds.), Natural Language Processing using Very Large 
Corpora, 269-282, Kluwer, (1999). (This paper first 
appeared in a 1994 workshop) 
G. Salton, Automatic Information Organization and 
Retrieval, McGraw-Hill, (1968). 
E. Voorhees and D. Harman (eds.), The 7th Text Retrieval 
Conference (TREC-7), NIST Special Publication 500-242, 
(1999). 
Ponte, J. and Croft, W.B. "A Language Modeling Approach 
to Information Retrieval," in the Proceedings of SIGIR 98, 
pp. 275-281(1998). 
Moldovan, Dan et al ?LASSO: A Tool for Surfing the 
Answer Net,? in the proceedings of TREC-8, pp 175-183. 
(1999). 
Hull, David A., ?Xerox TREC-8 Questio Answering Track 
Report,? in the proceedings of TREC-8, pp743. 
Prager, John, Brown, Eric, and Coden, Anni, 
?Question_Answering by Predictive Annotation,? in the 
proceedings of SIGIR 2000. 
 
 
 
 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 684?691, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Translation Model for Sentence Retrieval
Vanessa Murdock and W. Bruce Croft
Center for Intelligent Information Retrieval
Computer Science Department
University of Massachusetts
Amherst, MA 01003
{vanessa,croft}@cs.umass.edu
Abstract
In this work we propose a transla-
tion model for monolingual sentence
retrieval. We propose four methods
for constructing a parallel corpus. Of
the four methods proposed, a lexi-
con learned from a bilingual Arabic-
English corpus aligned at the sentence
level performs best, significantly im-
proving results over the query likeli-
hood baseline. Further, we demon-
strate that smoothing from the local
context of the sentence improves re-
trieval over the query likelihood base-
line.
1 Introduction
Sentence retrieval is the task of retrieving a rel-
evant sentence in response to a user?s query.
Tasks such as question answering, novelty de-
tection and summarization often incorporate a
sentence retrieval module. In previous work we
examined sentence retrieval for question answer-
ing (Murdock and Croft, 2004). This involves
the comparison of two well-formed sentences,
one a question, one a statement. In this work we
compare well-formed sentences to queries, which
can be typical keyword queries of 1 to 3 terms,
or a set of sentences or sentence fragments. The
TREC Novelty Track provides this type of data
in the form of topic titles and descriptions, and
sentence-level relevance judgments for a small
subset of the collection.
We present a translation model specifically
for monolingual data, and show that it signif-
icantly improves sentence retrieval over query-
likelihood. Translation models train on a paral-
lel corpus and in previous work we used a cor-
pus of question/answer pairs. No such corpus
is available for the novelty data, so in this pa-
per we present four ways to construct a parallel
corpus, to estimate a translation model.
Many systems treat sentence retrieval as a
type of document or passage retrieval. In our
data a sentence is an average of 18 words, most
of which occur once. A document is an average
of 700 words, many of which are multiples of
the same term. It is much less likely for a word
and its synonym terms to appear in the same
sentence than in the same document.
Passages may be any length, either fixed or
variable, but are somewhat arbitrarily desig-
nated. Many systems that have a passage re-
trieval module, on closer inspection have de-
fined the passage to be a sentence. What is
needed is a sentence retrieval mechanism that
retains the benefits of passage retrieval, where
a passage is longer than a sentence. We pro-
pose that smoothing from the local context of
the sentence improves retrieval over the query
likelihood baseline, and the larger the context,
the greater the improvement.
We describe our translation model in sec-
tion 2, along with our smoothing approach. In
section 3 we discuss previous work in sentence
retrieval for the Novelty task, and translation
models for information retrieval tasks. Sec-
tion 4 presents four ways to estimate a trans-
lation model, in the absence of a parallel cor-
pus, and presents our experimental results. We
684
discuss the results in section 5, and present our
conclusions and future work in section 6.
2 Methodology
Our data was provided by NIST, as part of
the TREC Novelty Track1. The documents for
the TREC Novelty Track in 2002 were taken
from the TREC volumes 4 and 5, and consist
of news articles from the Financial Times, the
Foreign Broadcast Information Service, and the
Los Angeles Times from non-overlapping years.
In 2003 and 2004, the documents were taken
from the Aquaint Corpus, which is distributed
by the Linguistic Data Consortium2 and con-
sists of newswire text in English from the Xin-
hua News Service, the New York Times, and the
Associated Press from overlapping years.
We retrieved the top 1000 documents for
each topic from the TREC and Aquaint col-
lections, and sentence segmented the docu-
ments using MXTerminator (Reynar and Rat-
naparkhi, 1997), which is a freely available sen-
tence boundary detector. Each topic was in-
dexed separately and had an average of 30,000
sentences. It was impractical to do sentence-
level relevance assessments for the complete set
of 150,000 documents, so we used the relevance
assessments provided as part of the Novelty
task, recognizing that the results are a lower
bound on performance, because the relevance
assessments do not cover the collection. The
relevance assessments cover 25 known relevant
documents for each topic.
We evaluated precision at N documents be-
cause many systems using sentence retrieval em-
phasize the results at the top of the ranked list,
and are less concerned with the overall quality
of the list.
2.1 Translation Models
We incorporated a machine translation model
in two steps: estimation and ranking. In the
estimation step, the probability that a term in
the sentence ?translates? to a term in the query
is estimated using the implementation of IBM
1http://trec.nist.gov
2http://www.ldc.upenn.edu
Model 1 (Brown et al, 1990) in GIZA++ (Al-
Onaizan et al, 1999) out-of-the-box without
alteration. In the ranking step we incorpo-
rate the translation probabilities into the query-
likelihood framework.
In Berger and Lafferty (1999), the IBM Model
1 is incorporated thus:
P (qi|S) =
m
?
j=1
P (qi|sj)P (sj |S) (1)
where P (qi|sj) is the probability that term sj in
the sentence translates to term qi in the query.
If the translation probabilities are modified such
that P (qi|sj) = 1 if qi = sj and 0 otherwise,
this is Berger and Lafferty?s ?Model 0?, and it
is exactly the query-likelihood model (described
in section 2.2).
A major difference between machine transla-
tion and sentence retrieval is that machine trans-
lation assumes there is little, if any, overlap in
the vocabularies of the two languages. In sen-
tence retrieval we depend heavily on the overlap
between the two vocabularies. With the Berger
and Lafferty formulation in equation 1, the prob-
ability of a word translating to itself is estimated
as a fraction of the probability of the word trans-
lating to all other words. Because the probabil-
ities must sum to one, if there are any other
translations for a given word, its self-translation
probability will be less than 1.0. To accommo-
date this monolingual condition, we make the
following improvement.
Let ti = 1 if there exists a term in the sentence
sj such that qi = sj , and 0 otherwise:
?
1?j?n
p(qi|sj)p(sj |S) =?
tip(qi|S) + (1 ? ti)
?
1?j?n,sj 6=qi
p(qi|sj)p(sj |S)
(2)
The translation probabilities still sum to one.
We determined empirically that this adjustment
improved the results over IBM model 1, and over
Berger and Lafferty model 0.
685
2.2 Document Smoothing
Query likelihood is a generative model that as-
sumes that the sentence is a sample of a multino-
mial distribution of terms. Sentences are ranked
according to the probability they generate the
query. We estimate this probability by interpo-
lating the term distribution in the sentence with
the term distribution in the collection:
P (Q|S) = P (S)
|Q|
?
i=1
(
?P (qi|S) + (1 ? ?)P (qi|C)
)
(3)
where Q is the query, S is the sentence, P (S) is
the (uniform) prior probability of the sentence,
P (qi|S) is the probability that term qi in the
query appears in the sentence, and P (qi|C) is
the probability that qi appears in the collection.
In the experiments with document smoothing,
we estimate the probability of a sentence gener-
ating the query:
P (Q|S) =
P (S)
|Q|
?
i=1
(
?P (qi|S) + ?P (qi|DS) + ?P (qi|C)
)
(4)
where ?+? +? = 1.0 and P (qi|DS) is the prob-
ability that the term qi in the query appears
in the document the sentence came from. In
our case, since the sentences for each topic are
indexed separately, the collection statistics are
in reference to the documents in the individual
topic index.
3 Previous Work
The TREC Novelty Track ran for three years,
from 2002 to 2004. Overviews of the track
can be found in (Harman, 2002), (Soboroff and
Harman, 2003) and (Soboroff, 2004). A num-
ber of systems use traditional information re-
trieval techniques for sentence retrieval, using
various techniques to compensate for the sparse
term distributions in sentences. The Univer-
sity of Massachusetts (Larkey et al, 2002) and
Carnegie Mellon University (Collins-Thompson
et al, 2002) both ranked sentences by the co-
sine similarity of the sentence vector to the
query vector of tf.idf-weighted terms. Amster-
dam University (Monz et al, 2002) used tfc.nfx
term weighting which is a variant of tf.idf term
weighting that normalizes the lengths of the doc-
ument vectors. Meiji University (Ohgaya et al,
2003) expanded the query with concept groups,
and then ranked the sentences by the cosine sim-
ilarity between the expanded topic vector and
the sentence vector.
Berger and Lafferty (1999) proposed the use of
translation models for (mono-lingual) document
retrieval. They used IBM Model 1 (Brown et
al., 1990), to rank documents according to their
translation probability, given the query. They
make no adjustment for the fact that the query
and the document are in the same language, and
instead rely on the translation model to learn
the appropriate weights for word pairs. The
models are trained on parallel data artificially
constructed from the mutual information distri-
bution of terms in the document. The results
presented either were not tested for statistical
significance, or they were not statistically signif-
icant, because no significance results were given.
Berger et al (2000) used IBM Model 1 to rank
answers to questions in call-center data. In their
data, there were no answers that were not in
response to at least one of the questions, and all
questions had at least one answer. Furthermore,
there are multiples of the same question. The
task is to match questions and answers, given
that every question has at least one match in the
data. The translation models performed better
for this task than the tf.idf baseline.
4 Experiments and Results
In this section we describe four methods for es-
timating a translation model in the absence of
a parallel corpus. We describe experimental re-
sults for each of the translation models, as well
as for document smoothing.
4.1 Mutual Information and TREC
As in Berger and Lafferty (1999), a set of
documents was selected at random from the
TREC collection, and for each document we
686
Query MT MT
Likelihood (MI) (TREC)
Prec@5 0.1176 0.1149 0.1392*
Prec@10 0.1115 0.1047 0.1095
Prec@15 0.1023 0.0928* 0.0977
Prec@20 0.0973 0.0882* 0.0936
Prec@30 0.0890 0.0865 0.0874
Prec@100 0.0733 0.0680* 0.0705
R-Prec 0.0672 0.0642* 0.0671
Ave Prec 0.0257 0.0258 0.0264
Table 1: Comparing translation model-based re-
trieval with description queries. ?TREC? and
?MI? are two ways to estimate a translation
model. Results with an asterisk are significant
at the .05 level with a two-tailed t-test.
constructed a distribution according to each
term?s mutual information with the document,
and randomly generated five queries of 8 words
according to this distribution. We were retriev-
ing sentences rather than documents, so each
sentence in the document was ranked according
to its probability of having generated the query,
and then the query was aligned with the top 5
sentences. We call this approach ?MI?.
The second approach uses the TREC topic
titles and descriptions aligned with the top 5
retrieved sentences from documents known to
be relevant to those topics, excluding topics that
were included in the Novelty data. We call this
approach ?TREC?.
Table 1 shows the results of incorporating
translations for topic descriptions. Results in
the tables with an asterisk are significant at the
.05 level using a two-tailed t-test. The results
for sentence retrieval are lower than those typ-
ically obtained for document retrieval. Manual
inspection of the results indicates that the ac-
tual precision is much higher, and resembles the
results for document retrieval. The lower results
are an artifact of the way the relevance assess-
ments were obtained. The sentence-level judge-
ments from the TREC Novelty Track are only
for 25 documents per topic.
The Novelty data from 2003-2004 consists of
event and opinion queries. We observed that
Event Opinion
Query MT Query MT
Lklhd (TREC) Lklhd (TREC)
Prec@5 0.1149 0.1307 0.1234 0.1574
Prec@10 0.1089 0.1079 0.1170 0.1128
Prec@15 0.1036 0.1030 0.0993 0.0865
Prec@20 0.0985 0.0980 0.0947 0.0840
Prec@30 0.0901 0.0894 0.0865 0.0830
Prec@100 0.0729 0.0719 0.0743 0.0674
R-Prec 0.0658 0.0694 0.0701 0.0622
Ave Prec 0.0275 0.0289 0.0219 0.0211
Table 2: Comparing translation-based retrieval
for description queries, using the relevance judg-
ments provided by NIST. The translation model
was trained from TREC topics.
a number of the topic descriptions for event
topics had a high degree of vocabulary overlap
with the sentences in our data. This was not
true of the opinion queries. The results of us-
ing a translation-based retrieval on description
queries are given in table 2, broken down by
the sentiment of the query. The Novelty queries
from 2002 were included in the ?event? set.
Not all of the sentences judged relevant to
opinion topics express opinions. To assess
opinion-relevance we evaluated the top 10 sen-
tences, and marked sentences that expressed
opinions. In our data approximately 10% of
sentences in the top 10 express opinions. Ta-
ble 3 shows the result of using a translation
model trained on TREC data for description
queries, broken down by sentiment, with the
baselines evaluated for this particular set of rel-
evance judgments. For opinion questions, the
column labeled ?topical? indicates topical rele-
vance. The column labeled ?opinion? indicates
topical relevance that also expresses an opinion.
If we consider a sentence relevant to an opin-
ion question only if it expresses an opinion, we
see improvement in the results at the top of the
ranked list for those queries, using a transla-
tion model trained on TREC data. Of the 150
topics, only 50 were opinion topics, so although
the magnitude of the improvement in opinion
queries is large the results are not statistically
687
Topical Rel Express Opin
Query MT Query MT
Lklhd (TREC) Lklhd (TREC)
Prec@5 .7289 .7111 .3300 .3900
Prec@10 .7089 .6867 .3125 .3775
Prec@15 .5363 .4919 .2350 .2717
Prec@20 .4300 .4033 .1875 .2188
Prec@30 .3170 .2970 .1408 .1617
Prec@100 .1236 .1131* .0587 .0580
R-Prec .4834 .4653 .2947 .3597*
Ave Prec .4996 .4696 .2563 .3177
Table 3: Comparison of translation retrieval on
opinion queries, using truth data we created to
evaluate opinion questions. Translation models
were trained with TREC data. Results with an
asterisk are significant at the .05 level using a
two-tailed t-test.
significant with respect to the baseline.
4.2 Lexicons
External lexicons are often useful for transla-
tion and query expansion. The most obvious
approach was to incorporate a thesaurus into
the training process in GIZA as a dictionary,
which affects the statistics in the first iteration
of EM. This is intended to improve the qual-
ity of the alignments over subsequent iterations.
We incorporated the thesauri into the training
process of the data generated from the artificial
mutual information distribution. The dictionar-
ies had almost no effect on the results.
4.2.1 WordNet
We created a parallel corpus of synonym-term
pairs from WordNet, and added this data to the
artificial mutual information data to train the
translation model. The results of using this ap-
proach to retrieve sentences using title queries
are in figure 1, labeled ?MI WN?. Using Word-
Net alne, without the mutual information data,
is labeled ?WN Only?. The results are statisti-
cally significant using a Wilcoxon sign test at the
.05 level for precision at .10, .20 and .60. Query
likelihood retrieval is the baseline. The results
for description queries are not shown, and were
not significantly different from the baseline.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0  2  4  6  8  10
"baseline"
"MI_WN"
"WN_Only"
Figure 1: Comparing interpolated recall-
precision for title queries using WordNet. The
results are statistically significant using a
Wilcoxon sign test at the .05 level, for precision
at .10, .20 and .60.
4.2.2 Arabic-English corpus
Xu et al (2002) derive an Arabic thesaurus
from a parallel corpus. We derived an En-
glish thesaurus using the same approach, from a
pair of English-Arabic/Arabic-English lexicons,
learned from a parallel corpus. We assumed that
if two English terms translate to the same Ara-
bic term, the English terms are synonyms whose
probability is given by
P (e2|e1) =
?
a?A
P (e2|a)P (a|e1) (5)
Figure 2 shows the interpolated recall-
precision of these results, for description queries.
The English terms were not stemmed, and so
the baseline query-likelihood results are also not
stemmed. The results are statistically signifi-
cant using a Wilcoxon sign test at the .05 level,
for all retrieval levels. Not shown is the average
precision, which is also significantly better for
the Arabic-English lexicon than for the query-
likelihood. The results for title queries are not
shown, but are similar to those for descriptions.
688
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  2  4  6  8  10
"baseline"
"Arabic_English"
Figure 2: Comparing interpolated recall-
precision for description queries using a pair of
Arabic-English, English-Arabic lexicons. The
results are statistically significant using a
Wilcoxon sign test at the .05 level, for precision
all recall levels.
4.3 Document Smoothing
Smucker and Allan (2005) demonstrated
that under certain conditions, Jelinek-Mercer
smoothing is equivalent to Dirichlet smoothing,
and that the advantage of Dirichlet smoothing
is derived from the fact that it smoothes long
documents less than shorter documents. In our
data there is much less variance in the length
of a sentence than in the length of a document,
thus we do not expect to see as great a benefit
in performance from Dirichlet smoothing as has
been reported in Zhai and Lafferty (2001). In
fact we tried Absolute Discounting, Dirichlet,
Jelinek-Mercer and Laplace smoothing and
found them to produce equivalent results.
The vast majority of sentences in our data
are not stand-alone units, and the topic of the
sentence is also the topic of surrounding sen-
tences. We took a context of the surround-
ing 5 sentences, and the surrounding 11 sen-
tences (about 1/3 of the whole document). The
sentences were smoothed from the surrounding
context, backing-off to the whole document, us-
Query 5 Sents 11 Sents
Lklhd
Prec@5 0.1203 0.1527* 0.1541*
Prec@10 0.1122 0.1446* 0.1419*
Prec@15 0.1018 0.1329* 0.1405*
Prec@20 0.0973 0.1311* 0.1345*
Prec@30 0.0890 0.1191* 0.1286*
Prec@100 0.0732 0.0935* 0.1006*
R-Prec 0.0672 0.0881* 0.0933*
Ave Prec 0.0257 0.0410* 0.0485*
Table 4: Comparison of smoothing context on
description queries, retrieving sentences from
the top 1000 documents. Results with an aster-
isk are significant at the .05 level using a two-
tailed t-test.
ing Jelinek-Mercer smoothing. Table 4 shows a
comparison of the amount of context. Smooth-
ing from the local context is clearly better than
the baseline result.
We investigated the effect of smoothing from
the entire document. Table 5 shows the results.
Both topic titles and descriptions get signifi-
cantly better results with document smoothing.
4.4 Novelty Relevance Task
In the TREC Novelty Track, participants are
given a set of 25 documents most of which are
relevant for each topic. If we believe that a doc-
ument is relevant because it has relevant sen-
tences in it, then a ?good? sentence would come
from a ?good? document. This would suggest
that smoothing from the document the sentence
came from would improve retrieval. We found
that for title queries document smoothing im-
proved precision in the top 5 documents by
12.5%, which is statistically significant using a
two-tailed t-test at the .05 level. Precision in
the top 10 - 100 documents also improved re-
sults by an average of 5%, but the result is not
statistically significant. For description queries,
smoothing from the document had no effect.
For title queries, translation models improve
the average precision, and R-Precision. For both
title and description queries, the number of rel-
evant documents that are retrieved is also im-
proved with translation models.
689
Title Description
Query Doc Query Doc
Lklhd Smth Lklhd Smth
Prec@5 .0765 .2268* .1203 .2362*
Prec@10 .0805 .2262* .1122 .2128*
Prec@15 .0814 .2192* .1018 .2000*
Prec@20 .0765 .2124* .0973 .1893*
Prec@30 .0765 .2007* .0890 .1743*
Prec@100 .0675 .1638* .0732 .1335*
R-Prec .0646 .1379* .0672 .1226*
Ave Prec .0243 .0796* .0257 .0749*
Table 5: Comparison of document smoothing to
query likelihood retrieving sentences from the
top 1000 documents. Results with an asterisk
are significant at the .05 level using a two-tailed
t-test.
5 Discussion
The results for sentence retrieval are low, in
comparison to results we would expect for doc-
ument retrieval. We might think that although
we show improvements, nothing is working well.
In reality, the relevance assessments provided by
NIST as part of the Novelty Track only cover
25 documents per topic. Evaluating the top 10
sentences by hand shows that the systems give a
performance comparable to document retrieval
systems, and the low numbers are the result of
a lack of coverage in the assessments. Unfor-
tunately, there is no collection of documents of
significant size, where the relevance assessments
at the sentence level cover the collection. Con-
structing such a corpus would be a major un-
dertaking, outside of the scope of this paper.
The best performing method of constructing
a parallel corpus used a bilingual lexicon derived
from a sentence-aligned Arabic-English parallel
corpus. This suggests that data in which sen-
tences are actually translations of one another,
as opposed to sentences aligned with key terms
from the document, yield a higher quality lexi-
con. The model trained on the parallel corpus of
TREC topics and relevant sentences performed
better than the MI corpus, but not as well as the
Arabic-English corpus. The TREC corpus con-
sisted of approximately 15,000 sentence pairs,
whereas the Arabic-English corpus was trained
on more than a million sentence pairs. This may
account in part for the higher quality results. In
addition, the TREC corpus was created by re-
trieving the top 5 sentences from each relevant
document. Even when the document is known
to be relevant, the retrieval process is noisy. Fur-
thermore, although there were 15,000 sentence
pairs, there were only 450 unique queries, limit-
ing the size of the source vocabulary.
Opinion topics have much less vocabulary
overlap with relevant sentences than do event
topics. Translation models would be expected
to perform better when retrieving sentences that
contain synonym or related terms. For sentences
that have exact matches in the query, query like-
lihood will perform better.
We find that smoothing from the local con-
text of the sentence performs significantly bet-
ter than the baseline retrieval. The sentences are
all about the same length, so there is no perfor-
mance advantage to using Dirichlet smoothing,
whose smoothing parameter is a function of the
document length. The smoothing parameters
gave very little weight to the collection. As sen-
tences have few terms, relative to documents,
matching a term in the query is a good indica-
tion of relevance.
6 Conclusions and Future Work
We have shown that translation models improve
retrieval for title and opinion queries, and that
a translation model derived from a high-quality
bilingual lexicon significantly improves retrieval
for title and description queries. Smoothing
from the local context of a sentence dramati-
cally improves retrieval, with smoothing from
the document that contains the sentence per-
forming the best.
We evaluated sentences based on lexical sim-
ilarity, but structural similarity is also an im-
portant measure, which we plan to investigate
in the future. The translation model we used
was the most basic model. We used this model
because it had been shown effective in docu-
ment retrieval, and was easily incorporated in
the query-likelihood framework, but we intend
690
to explore more sophisticated translation mod-
els, and better alignment mechanisms. Prelimi-
nary results suggest that sentence retrieval can
be used to improve document retrieval, but we
plan a more extensive investigation of evaluat-
ing document similarity and relevance based on
sentence-level similarity.
7 Acknowledgements
The authors would like to thank Leah Larkey
for her Arabic-English lexicons. This work
was supported in part by the Center for In-
telligent Information Retrieval, in part by Ad-
vanced Research and Development Activity and
NSF grant #CCF-0205575 , and in part by
SPAWARSYSCEN-SD grant number N66001-
02-1-8903. Any opinions, findings and conclu-
sions or recommendations expressed in this ma-
terial are the author(s) and do not necessarily
reflect those of the sponsor.
References
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.
Smith, and D. Yarowsky. 1999. Statistical ma-
chine translation, final report, JHU workshop.
Adam Berger and John Lafferty. 1999. Information
retrieval as statistical translation. In Proceedings
of the 22nd Annual Conference on Research and
Development in Information Retrieval (ACM SI-
GIR).
Adam Berger, Rich Caruana, David Cohn, Dayne
Freitag, and Vibhu Mittal. 2000. Bridging the
lexical chasm: Statistical approaches to answer-
finding. In Proceedings of the 23rd Annual Con-
ference on Research and Development in Informa-
tion Retrieval (ACM SIGIR), pages 192?199.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Jelineck,
John D. Lafferty, Robert L. Mercer, and Paul S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?
85.
Kevyn Collins-Thompson, Paul Ogilvie, Yi Zhang,
and Jamie Callan. 2002. Information filtering,
novelty detection and named-page finding. In Pro-
ceedings of the Eleventh Text Retrieval Conference
(TREC).
Donna Harman. 2002. Overview of the TREC 2002
novelty track. In Proceedings of the Eleventh Text
Retrieval Conference (TREC).
Leah Larkey, James Allan, Margie Connell, Alvaro
Bolivar, and Courtney Wade. 2002. UMass at
TREC 2002: Cross language and novelty tracks.
In Proceedings of the Eleventh Text Retrieval Con-
ference (TREC), page 721.
Christof Monz, Jaap Kamps, and Maarten de Rijke.
2002. The University of Amsterdam at TREC
2002. In Proceedings of the Eleventh Text Re-
trieval Conference (TREC).
Vanessa Murdock and W. Bruce Croft. 2004. Simple
translation models for sentence retrieval in factoid
question answering. In Proceedings of the Infor-
mation Retrieval for Question Answering Work-
shop at SIGIR 2004.
Ryosuke Ohgaya, Akiyoshi Shimmura, and Tomohiro
Takagi. 2003. Meiji University web and novelty
track experiments at TREC 2003. In Proceedings
of the Twelth Text Retrieval Conference (TREC).
Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997. A maximum entropy approach to
identifying sentence boundaries. In Pro-
ceedings of the 5th Conference on Ap-
plied Natural Language Processing (ANLP).
http://www.cis.upenn.edu/a?dwait/statnlp.html.
Mark Smucker and James Allan. 2005. An investi-
gation of dirichlet prior smoothing?s performance
advantage. Technical Report IR-391, The Univer-
sity of Massachusetts, The Center for Intelligent
Information Retrieval.
Ian Soboroff and Donna Harman. 2003. Overview of
the TREC 2003 novelty track. In Proceedings of
the Twelfth Text Retrieval Conference (TREC).
Ian Soboroff. 2004. Overview of the TREC 2004
novelty track. In Proceedings of the Thirteenth
Text Retrieval Conference (TREC). forthcoming.
Jinxi Xu, Alexander Fraser, and Ralph Weischedel.
2002. Empirical studies in strategies for arabic re-
trieval. In Proceedings of the 25th Annual Confer-
ence on Research and Development in Information
Retrieval (ACM SIGIR).
ChengXiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to ad hoc information retrieval. In Proceedings of
the 24th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 334?342.
691
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102?111,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Annotation of Search Queries
Michael Bendersky
Dept. of Computer Science
University of Massachusetts
Amherst, MA
bemike@cs.umass.edu
W. Bruce Croft
Dept. of Computer Science
University of Massachusetts
Amherst, MA
croft@cs.umass.edu
David A. Smith
Dept. of Computer Science
University of Massachusetts
Amherst, MA
dasmith@cs.umass.edu
Abstract
Marking up search queries with linguistic an-
notations such as part-of-speech tags, cap-
italization, and segmentation, is an impor-
tant part of query processing and understand-
ing in information retrieval systems. Due
to their brevity and idiosyncratic structure,
search queries pose a challenge to existing
NLP tools. To address this challenge, we
propose a probabilistic approach for perform-
ing joint query annotation. First, we derive
a robust set of unsupervised independent an-
notations, using queries and pseudo-relevance
feedback. Then, we stack additional classi-
fiers on the independent annotations, and ex-
ploit the dependencies between them to fur-
ther improve the accuracy, even with a very
limited amount of available training data. We
evaluate our method using a range of queries
extracted from a web search log. Experimen-
tal results verify the effectiveness of our ap-
proach for both short keyword queries, and
verbose natural language queries.
1 Introduction
Automatic mark-up of textual documents with lin-
guistic annotations such as part-of-speech tags, sen-
tence constituents, named entities, or semantic roles
is a common practice in natural language process-
ing (NLP). It is, however, much less common in in-
formation retrieval (IR) applications. Accordingly,
in this paper, we focus on annotating search queries
submitted by the users to a search engine.
There are several key differences between user
queries and the documents used in NLP (e.g., news
articles or web pages). As previous research shows,
these differences severely limit the applicability of
standard NLP techniques for annotating queries and
require development of novel annotation approaches
for query corpora (Bergsma and Wang, 2007; Barr et
al., 2008; Lu et al, 2009; Bendersky et al, 2010; Li,
2010).
The most salient difference between queries and
documents is their length. Most search queries
are very short, and even longer queries are usually
shorter than the average written sentence. Due to
their brevity, queries often cannot be divided into
sub-parts, and do not provide enough context for
accurate annotations to be made using the stan-
dard NLP tools such as taggers, parsers or chun-
kers, which are trained on more syntactically coher-
ent textual units.
A recent analysis of web query logs by Bendersky
and Croft (2009) shows, however, that despite their
brevity, queries are grammatically diverse. Some
queries are keyword concatenations, some are semi-
complete verbal phrases and some are wh-questions.
It is essential for the search engine to correctly an-
notate the query structure, and the quality of these
query annotations has been shown to be a crucial
first step towards the development of reliable and
robust query processing, representation and under-
standing algorithms (Barr et al, 2008; Guo et al,
2008; Guo et al, 2009; Manshadi and Li, 2009; Li,
2010).
However, in current query annotation systems,
even sentence-like queries are often hard to parse
and annotate, as they are prone to contain mis-
spellings and idiosyncratic grammatical structures.
102
(a) (b) (c)
Term CAP TAG SEG
who L X B
won L V I
the L X B
2004 L X B
kentucky C N B
derby C N I
Term CAP TAG SEG
kindred C N B
where C X B
would C X I
i C X I
be C V I
Term CAP TAG SEG
shih C N B
tzu C N I
health L N B
problems L N I
Figure 1: Examples of a mark-up scheme for annotating capitalization (L ? lowercase, C ? otherwise), POS tags (N ?
noun, V ? verb, X ? otherwise) and segmentation (B/I ? beginning of/inside the chunk).
They also tend to lack prepositions, proper punctu-
ation, or capitalization, since users (often correctly)
assume that these features are disregarded by the re-
trieval system.
In this paper, we propose a novel joint query an-
notation method to improve the effectiveness of ex-
isting query annotations, especially for longer, more
complex search queries. Most existing research fo-
cuses on using a single type of annotation for infor-
mation retrieval such as subject-verb-object depen-
dencies (Balasubramanian and Allan, 2009), named-
entity recognition (Guo et al, 2009), phrase chunk-
ing (Guo et al, 2008), or semantic labeling (Li,
2010).
In contrast, the main focus of this work is on de-
veloping a unified approach for performing reliable
annotations of different types. To this end, we pro-
pose a probabilistic method for performing a joint
query annotation. This method allows us to exploit
the dependency between different unsupervised an-
notations to further improve the accuracy of the en-
tire set of annotations. For instance, our method
can leverage the information about estimated parts-
of-speech tags and capitalization of query terms to
improve the accuracy of query segmentation.
We empirically evaluate the joint query annota-
tion method on a range of query types. Instead of
just focusing our attention on keyword queries, as
is often done in previous work (Barr et al, 2008;
Bergsma and Wang, 2007; Tan and Peng, 2008;
Guo et al, 2008), we also explore the performance
of our annotations with more complex natural lan-
guage search queries such as verbal phrases and wh-
questions, which often pose a challenge for IR appli-
cations (Bendersky et al, 2010; Kumaran and Allan,
2007; Kumaran and Carvalho, 2009; Lease, 2007).
We show that even with a very limited amount of
training data, our joint annotation method signifi-
cantly outperforms annotations that were done in-
dependently for these queries.
The rest of the paper is organized as follows. In
Section 2 we demonstrate several examples of an-
notated search queries. Then, in Section 3, we in-
troduce our joint query annotation method. In Sec-
tion 4 we describe two types of independent query
annotations that are used as input for the joint query
annotation. Section 5 details the related work and
Section 6 presents the experimental results. We draw
the conclusions from our work in Section 7.
2 Query Annotation Example
To demonstrate a possible implementation of lin-
guistic annotation for search queries, Figure 1
presents a simple mark-up scheme, exemplified us-
ing three web search queries (as they appear in a
search log): (a) who won the 2004 kentucky derby,
(b) kindred where would i be, and (c) shih tzu health
problems. In this scheme, each query is marked-
up using three annotations: capitalization, POS tags,
and segmentation indicators.
Note that all the query terms are non-capitalized,
and no punctuation is provided by the user, which
complicates the query annotation process. While
the simple annotation described in Figure 1 can be
done with a very high accuracy for standard docu-
ment corpora, both previous work (Barr et al, 2008;
Bergsma and Wang, 2007; Jones and Fain, 2003)
and the experimental results in this paper indicate
that it is challenging to perform well on queries.
The queries in Figure 1 illustrate this point. Query
(a) in Figure 1 is a wh-question, and it contains
103
a capitalized concept (?Kentucky Derby?), a single
verb, and four segments. Query (b) is a combination
of an artist name and a song title and should be inter-
preted as Kindred ? ?Where Would I Be?. Query (c)
is a concatenation of two short noun phrases: ?Shih
Tzu? and ?health problems?.
3 Joint Query Annotation
Given a search query Q, which consists of a se-
quence of terms (q1, . . . , qn), our goal is to anno-
tate it with an appropriate set of linguistic structures
ZQ. In this work, we assume that the setZQ consists
of shallow sequence annotations zQ, each of which
takes the form
zQ = (?1, . . . , ?n).
In other words, each symbol ?i ? zQ annotates a
single query term.
Many query annotations that are useful for IR
can be represented using this simple form, includ-
ing capitalization, POS tagging, phrase chunking,
named entity recognition, and stopword indicators,
to name just a few. For instance, Figure 1 demon-
strates an example of a set of annotations ZQ. In
this example,
ZQ = {CAP,TAG,SEG}.
Most previous work on query annotation makes
the independence assumption ? every annotation
zQ ? ZQ is done separately from the others. That is,
it is assumed that the optimal linguistic annotation
z?(I)Q is the annotation that has the highest probabil-
ity given the query Q, regardless of the other anno-
tations in the set ZQ. Formally,
z?(I)Q = argmax
zQ
p(zQ|Q) (1)
The main shortcoming of this approach is in the
assumption that the linguistic annotations in the set
ZQ are independent. In practice, there are depen-
dencies between the different annotations, and they
can be leveraged to derive a better estimate of the
entire set of annotations.
For instance, imagine that we need to perform two
annotations: capitalization and POS tagging. Know-
ing that a query term is capitalized, we are more
likely to decide that it is a proper noun. Vice versa,
knowing that it is a preposition will reduce its proba-
bility of being capitalized. We would like to capture
this intuition in the annotation process.
To address the problem of joint query annotation,
we first assume that we have an initial set of annota-
tions Z?(I)Q , which were performed for query Q in-
dependently of one another (we will show an exam-
ple of how to derive such a set in Section 4). Given
the initial set Z?(I)Q , we are interested in obtaining
an annotation set Z?(J)Q , which jointly optimizes the
probability of all the annotations, i.e.
Z?(J)Q = argmax
ZQ
p(ZQ|Z?(I)Q ).
If the initial set of estimations is reasonably ac-
curate, we can make the assumption that the anno-
tations in the set Z?(J)Q are independent given the
initial estimates Z?(I)Q , allowing us to separately op-
timize the probability of each annotation z?(J)Q ?
Z?(J)Q :
z?(J)Q = argmax
zQ
p(zQ|Z?(I)Q ). (2)
From Eq. 2, it is evident that the joint an-
notation task becomes that of finding some opti-
mal unobserved sequence (annotation z?(J)Q ), given
the observed sequences (independent annotation set
Z?(I)Q ).
Accordingly, we can directly use a supervised se-
quential probabilistic model such as CRF (Lafferty
et al, 2001) to find the optimal z?(J)Q . In this CRF
model, the optimal annotation z?(J)Q is the label we
are trying to predict, and the set of independent an-
notations Z?(I)Q is used as the basis for the features
used for prediction. Figure 2 outlines the algorithm
for performing the joint query annotation.
As input, the algorithm receives a training set of
queries and their ground truth annotations. It then
produces a set of independent annotation estimates,
which are jointly used, together with the ground
truth annotations, to learn a CRF model for each an-
notation type. Finally, these CRF models are used
to predict annotations on a held-out set of queries,
which are the output of the algorithm.
104
Input: Qt ? training set of queries.
ZQt ? ground truth annotations for the training set of queries.
Qh ? held-out set of queries.
(1) Obtain a set of independent annotation estimates Z?(I)Qt
(2) Initialize Z?(J)Qt ? ?
(3) for each z?(I)Qt ? Z
?(I)
Qt :
(4) Z ?Qt ? Z
?(I)
Qt \ z
?(I)
Qt
(5) Train a CRF model CRF(zQt) using zQt as a label and Z ?Qt as features.
(6) Predict annotation z?(J)Qh , using CRF(zQt).
(7) Z?(J)Qh ? Z
?(J)
Qh ? z
?(J)
Qh .
Output: Z?(J)Qh ? predicted annotations for the held-out set of queries.
Figure 2: Algorithm for performing joint query annotation.
Note that this formulation of joint query anno-
tation can be viewed as a stacked classification, in
which a second, more effective, classifier is trained
using the labels inferred by the first classifier as fea-
tures. Stacked classifiers were recently shown to be
an efficient and effective strategy for structured clas-
sification in NLP (Nivre and McDonald, 2008; Mar-
tins et al, 2008).
4 Independent Query Annotations
While the joint annotation method proposed in Sec-
tion 3 is general enough to be applied to any set of
independent query annotations, in this work we fo-
cus on two previously proposed independent anno-
tation methods based on either the query itself, or
the top sentences retrieved in response to the query
(Bendersky et al, 2010). The main benefits of these
two annotation methods are that they can be easily
implemented using standard software tools, do not
require any labeled data, and provide reasonable an-
notation accuracy. Next, we briefly describe these
two independent annotation methods.
4.1 Query-based estimation
The most straightforward way to estimate the con-
ditional probabilities in Eq. 1 is using the query it-
self. To make the estimation feasible, Bendersky et
al. (2010) take a bag-of-words approach, and assume
independence between both the query terms and the
corresponding annotation symbols. Thus, the inde-
pentent annotations in Eq. 1 are given by
z?(QRY )Q = argmax
(?1,...,?n)
?
i?(1,...,n)
p(?i|qi). (3)
Following Bendersky et al (2010) we use a large
n-gram corpus (Brants and Franz, 2006) to estimate
p(?i|qi) for annotating the query with capitalization
and segmentation mark-up, and a standard POS tag-
ger1 for part-of-speech tagging of the query.
4.2 PRF-based estimation
Given a short, often ungrammatical query, it is hard
to accurately estimate the conditional probability in
Eq. 1 using the query terms alone. For instance, a
keyword query hawaiian falls, which refers to a lo-
cation, is inaccurately interpreted by a standard POS
tagger as a noun-verb pair. On the other hand, given
a sentence from a corpus that is relevant to the query
such as ?Hawaiian Falls is a family-friendly water-
park?, the word ?falls? is correctly identified by a
standard POS tagger as a proper noun.
Accordingly, the document corpus can be boot-
strapped in order to better estimate the query anno-
tation. To this end, Bendersky et al (2010) employ
the pseudo-relevance feedback (PRF) ? a method
that has a long record of success in IR for tasks such
as query expansion (Buckley, 1995; Lavrenko and
Croft, 2001).
In the most general form, given the set of all re-
trievable sentences r in the corpus C one can derive
p(zQ|Q) =
?
r?C
p(zQ|r)p(r|Q).
Since for most sentences the conditional proba-
bility of relevance to the query p(r|Q) is vanish-
ingly small, the above can be closely approximated
1http://crftagger.sourceforge.net/
105
by considering only a set of sentences R, retrieved
at top-k positions in response to the query Q. This
yields
p(zQ|Q) ?
?
r?R
p(zQ|r)p(r|Q).
Intuitively, the equation above models the query as
a mixture of top-k retrieved sentences, where each
sentence is weighted by its relevance to the query.
Furthermore, to make the estimation of the condi-
tional probability p(zQ|r) feasible, it is assumed that
the symbols ?i in the annotation sequence are in-
dependent, given a sentence r. Note that this as-
sumption differs from the independence assumption
in Eq. 3, since here the annotation symbols are not
independent given the query Q.
Accordingly, the PRF-based estimate for indepen-
dent annotations in Eq. 1 is
z?(PRF )Q = argmax
(?1,...,?n)
?
r?R
?
i?(1,...,n)
p(?i|r)p(r|Q).
(4)
Following Bendersky et al (2010), an estimate of
p(?i|r) is a smoothed estimator that combines the
information from the retrieved sentence r with the
information about unigrams (for capitalization and
POS tagging) and bigrams (for segmentation) from
a large n-gram corpus (Brants and Franz, 2006).
5 Related Work
In recent years, linguistic annotation of search
queries has been receiving increasing attention as an
important step toward better query processing and
understanding. The literature on query annotation
includes query segmentation (Bergsma and Wang,
2007; Jones et al, 2006; Guo et al, 2008; Ha-
gen et al, 2010; Hagen et al, 2011; Tan and Peng,
2008), part-of-speech and semantic tagging (Barr et
al., 2008; Manshadi and Li, 2009; Li, 2010), named-
entity recognition (Guo et al, 2009; Lu et al, 2009;
Shen et al, 2008; Pas?ca, 2007), abbreviation disam-
biguation (Wei et al, 2008) and stopword detection
(Lo et al, 2005; Jones and Fain, 2003).
Most of the previous work on query annotation
focuses on performing a particular annotation task
(e.g., segmentation or POS tagging) in isolation.
However, these annotations are often related, and
thus we take a joint annotation approach, which
combines several independent annotations to im-
prove the overall annotation accuracy. A similar ap-
proach was recently proposed by Guo et al (2008).
There are several key differences, however, between
the work presented here and their work.
First, Guo et al (2008) focus on query refine-
ment (spelling corrections, word splitting, etc.) of
short keyword queries. Instead, we are interested
in annotation of queries of different types, includ-
ing verbose natural language queries. While there
is an overlap between query refinement and annota-
tion, the focus of the latter is on providing linguistic
information about existing queries (after initial re-
finement has been performed). Such information is
especially important for more verbose and gramat-
ically complex queries. In addition, while all the
methods proposed by Guo et al (2008) require large
amounts of training data (thousands of training ex-
amples), our joint annotation method can be effec-
tively trained with a minimal human labeling effort
(several hundred training examples).
An additional research area which is relevant to
this paper is the work on joint structure model-
ing (Finkel and Manning, 2009; Toutanova et al,
2008) and stacked classification (Nivre and Mc-
Donald, 2008; Martins et al, 2008) in natural lan-
guage processing. These approaches have been
shown to be successful for tasks such as parsing and
named entity recognition in newswire data (Finkel
and Manning, 2009) or semantic role labeling in the
Penn Treebank and Brown corpus (Toutanova et al,
2008). Similarly to this work in NLP, we demon-
strate that a joint approach for modeling the linguis-
tic query structure can also be beneficial for IR ap-
plications.
6 Experiments
6.1 Experimental Setup
For evaluating the performance of our query anno-
tation methods, we use a random sample of 250
queries2 from a search log. This sample is manually
labeled with three annotations: capitalization, POS
tags, and segmentation, according to the description
of these annotations in Figure 1. In this set of 250
queries, there are 93 questions, 96 phrases contain-
2The annotations are available at
http://ciir.cs.umass.edu/?bemike/data.html
106
CAP
F1 (% impr) MQA (% impr)
i-QRY 0.641 (-/-) 0.779 (-/-)
i-PRF 0.711?(+10.9/-) 0.811?(+4.1/-)
j-QRY 0.620?(-3.3/-12.8) 0.805?(+3.3/-0.7)
j-PRF 0.718?(+12.0/+0.9) 0.840??(+7.8/+3.6)
TAG
Acc. (% impr) MQA (% impr)
i-QRY 0.893 (-/-) 0.878 (-/-)
i-PRF 0.916?(+2.6/-) 0.914?(+4.1/-)
j-QRY 0.913?(+2.2/-0.3) 0.912?(+3.9/-0.2)
j-PRF 0.924?(+3.5/+0.9) 0.922?(+5.0/+0.9)
SEG
F1 (% impr) MQA (% impr)
i-QRY 0.694 (-/-) 0.672 (-/-)
i-PRF 0.753?(+8.5/-) 0.710?(+5.7/-)
j-QRY 0.817??(+17.7/+8.5) 0.803??(+19.5/+13.1)
j-PRF 0.819??(+18.0/+8.8) 0.803??(+19.5/+13.1)
Table 1: Summary of query annotation performance for
capitalization (CAP), POS tagging (TAG) and segmenta-
tion. Numbers in parentheses indicate % of improvement
over the i-QRY and i-PRF baselines, respectively. Best
result per measure and annotation is boldfaced. ? and ?
denote statistically significant differences with i-QRY and
i-PRF, respectively.
ing a verb, and 61 short keyword queries (Figure 1
contains a single example of each of these types).
In order to test the effectiveness of the joint query
annotation, we compare four methods. In the first
two methods, i-QRY and i-PRF the three annotations
are done independently. Method i-QRY is based on
z?(QRY )Q estimator (Eq. 3). Method i-PRF is based
on the z?(PRF )Q estimator (Eq. 4).
The next two methods, j-QRY and j-PRF, are joint
annotation methods, which perform a joint optimiza-
tion over the entire set of annotations, as described
in the algorithm in Figure 2. j-QRY and j-PRF differ
in their choice of the initial independent annotation
set Z?(I)Q in line (1) of the algorithm (see Figure 2).
j-QRY uses only the annotations performed by i-
QRY (3 initial independent annotation estimates),
while j-PRF combines the annotations performed by
i-QRY with the annotations performed by i-PRF (6
initial annotation estimates). The CRF model train-
ing in line (6) of the algorithm is implemented using
CRF++ toolkit3.
3http://crfpp.sourceforge.net/
The performance of the joint annotation methods
is estimated using a 10-fold cross-validation. In or-
der to test the statistical significance of improve-
ments attained by the proposed methods we use a
two-sided Fisher?s randomization test with 20,000
permutations. Results with p-value < 0.05 are con-
sidered statistically significant.
For reporting the performance of our meth-
ods we use two measures. The first measure is
classification-oriented ? treating the annotation de-
cision for each query term as a classification. In case
of capitalization and segmentation annotations these
decisions are binary and we compute the precision
and recall metrics, and report F1 ? their harmonic
mean. In case of POS tagging, the decisions are
ternary, and hence we report the classification ac-
curacy.
We also report an additional, IR-oriented perfor-
mance measure. As is typical in IR, we propose
measuring the performance of the annotation meth-
ods on a per-query basis, to verify that the methods
have uniform impact across queries. Accordingly,
we report the mean of classification accuracies per
query (MQA). Formally, MQA is computed as
?N
i=1 accQi
N ,
where accQi is the classification accuracy for query
Qi, and N is the number of queries.
The empirical evaluation is conducted as follows.
In Section 6.2, we discuss the general performance
of the four annotation techniques, and compare the
effectiveness of independent and joint annotations.
In Section 6.3, we analyze the performance of the
independent and joint annotation methods by query
type. In Section 6.4, we compare the difficulty
of performing query annotations for different query
types. Finally, in Section 6.5, we compare the effec-
tiveness of the proposed joint annotation for query
segmentation with the existing query segmentation
methods.
6.2 General Evaluation
Table 1 shows the summary of the performance of
the two independent and two joint annotation meth-
ods for the entire set of 250 queries. For independent
methods, we see that i-PRF outperforms i-QRY for
107
CAP Verbal Phrases Questions Keywords
F1 MQA F1 MQA F1 MQA
i-PRF 0.750 0.862 0.590 0.839 0.784 0.687
j-PRF 0.687?(-8.4%) 0.839?(-2.7%) 0.671?(+13.7%) 0.913?(+8.8%) 0.814 (+3.8%) 0.732? (+6.6%)
TAG Verbal Phrases Questions Keywords
Acc. MQA Acc. MQA Acc. MQA
i-PRF 0.908 0.908 0.932 0.935 0.880 0.890
j-PRF 0.904 (-0.4%) 0.906 (-0.2%) 0.951? (+2.1%) 0.953? (+1.9%) 0.893 (+1.5%) 0.900 (+1.1%)
SEG Verbal Phrases Questions Keywords
F1 MQA F1 MQA F1 MQA
i-PRF 0.751 0.700 0.740 0.700 0.816 0.747
j-PRF 0.772 (+2.8%) 0.742?(+6.0%) 0.858?(+15.9%) 0.838?(+19.7%) 0.844 (+3.4%) 0.853?(+14.2%)
Table 2: Detailed analysis of the query annotation performance for capitalization (CAP), POS tagging (TAG) and
segmentation by query type. Numbers in parentheses indicate % of improvement over the i-PRF baseline. Best result
per measure and annotation is boldfaced. ? denotes statistically significant differences with i-PRF.
all annotation types, using both performance mea-
sures.
In Table 1, we can also observe that the joint anno-
tation methods are, in all cases, better than the cor-
responding independent ones. The highest improve-
ments are attained by j-PRF, which always demon-
strates the best performance both in terms of F1 and
MQA. These results attest to both the importance of
doing a joint optimization over the entire set of an-
notations and to the robustness of the initial annota-
tions done by the i-PRF method. In all but one case,
the j-PRF method, which uses these annotations as
features, outperforms the j-QRY method that only
uses the annotation done by i-QRY .
The most significant improvements as a result of
joint annotation are observed for the segmentation
task. In this task, joint annotation achieves close to
20% improvement in MQA over the i-QRY method,
and more than 10% improvement in MQA over the i-
PRF method. These improvements indicate that the
segmentation decisions are strongly guided by cap-
italization and POS tagging. We also note that, in
case of segmentation, the differences in performance
between the two joint annotation methods, j-QRY
and j-PRF, are not significant, indicating that the
context of additional annotations in j-QRY makes up
for the lack of more robust pseudo-relevance feed-
back based features.
We also note that the lowest performance im-
provement as a result of joint annotation is evi-
denced for POS tagging. The improvements of joint
annotation method j-PRF over the i-PRF method are
less than 1%, and are not statistically significant.
This is not surprising, since the standard POS tag-
gers often already use bigrams and capitalization at
training time, and do not acquire much additional
information from other annotations.
6.3 Evaluation by Query Type
Table 2 presents a detailed analysis of the perfor-
mance of the best independent (i-PRF) and joint (j-
PRF) annotation methods by the three query types
used for evaluation: verbal phrases, questions and
keyword queries. From the analysis in Table 2, we
note that the contribution of joint annotation varies
significantly across query types. For instance, us-
ing j-PRF always leads to statistically significant im-
provements over the i-PRF baseline for questions.
On the other hand, it is either statistically indistin-
guishable, or even significantly worse (in the case of
capitalization) than the i-PRF baseline for the verbal
phrases.
Table 2 also demonstrates that joint annotation
has a different impact on various annotations for the
same query type. For instance, j-PRF has a signif-
icant positive effect on capitalization and segmen-
tation for keyword queries, but only marginally im-
proves the POS tagging. Similarly, for the verbal
phrases, j-PRF has a significant positive effect only
for the segmentation annotation.
These variances in the performance of the j-PRF
method point to the differences in the structure be-
108
Annotation Performance by Query Type
F1
Verbal Phrases Questions Keyword Queries
60
65
70
75
80
85
90
95
10
0
CAP
SEG
TAG
Figure 3: Comparative performance (in terms of F1 for
capitalization and segmentation and accuracy for POS
tagging) of the j-PRF method on the three query types.
tween the query types. While dependence between
the annotations plays an important role for question
and keyword queries, which often share a common
grammatical structure, this dependence is less use-
ful for verbal phrases, which have a more diverse
linguistic structure. Accordingly, a more in-depth
investigation of the linguistic structure of the verbal
phrase queries is an interesting direction for future
work.
6.4 Annotation Difficulty
Recall that in our experiments, out of the overall 250
annotated queries, there are 96 verbal phrases, 93
questions and 61 keyword queries. Figure 3 shows a
plot that contrasts the relative performance for these
three query types of our best-performing joint an-
notation method, j-PRF, on capitalization, POS tag-
ging and segmentation annotation tasks. Next, we
analyze the performance profiles for the annotation
tasks shown in Figure 3.
For the capitalization task, the performance of j-
PRF on verbal phrases and questions is similar, with
the difference below 3%. The performance for key-
word queries is much higher ? with improvement
over 20% compared to either of the other two types.
We attribute this increase to both a larger number
of positive examples in the short keyword queries
(a higher percentage of terms in keyword queries is
capitalized) and their simpler syntactic structure (ad-
SEG F1 MQA
SEG-1 0.768 0.754
SEG-2 0.824? 0.787?
j-PRF 0.819? (+6.7%/-0.6%) 0.803? (+6.5%/+2.1%)
Table 3: Comparison of the segmentation performance
of the j-PRF method to two state-of-the-art segmentation
methods. Numbers in parentheses indicate % of improve-
ment over the SEG-1 and SEG-2 baselines respectively.
Best result per measure and annotation is boldfaced. ?
denotes statistically significant differences with SEG-1.
jacent terms in these queries are likely to have the
same case).
For the segmentation task, the performance is at
its best for the question and keyword queries, and at
its worst (with a drop of 11%) for the verbal phrases.
We hypothesize that this is due to the fact that ques-
tion queries and keyword queries tend to have repet-
itive structures, while the grammatical structure for
verbose queries is much more diverse.
For the tagging task, the performance profile is re-
versed, compared to the other two tasks ? the per-
formance is at its worst for keyword queries, since
their grammatical structure significantly differs from
the grammatical structure of sentences in news arti-
cles, on which the POS tagger is trained. For ques-
tion queries the performance is the best (6% increase
over the keyword queries), since they resemble sen-
tences encountered in traditional corpora.
It is important to note that the results reported in
Figure 3 are based on training the joint annotation
model on all available queries with 10-fold cross-
validation. We might get different profiles if a sep-
arate annotation model was trained for each query
type. In our case, however, the number of queries
from each type is not sufficient to train a reliable
model. We leave the investigation of separate train-
ing of joint annotation models by query type to fu-
ture work.
6.5 Additional Comparisons
In order to further evaluate the proposed joint an-
notation method, j-PRF, in this section we compare
its performance to other query annotation methods
previously reported in the literature. Unfortunately,
there is not much published work on query capi-
talization and query POS tagging that goes beyond
the simple query-based methods described in Sec-
109
tion 4.1. The published work on the more advanced
methods usually requires access to large amounts of
proprietary user data such as query logs and clicks
(Barr et al, 2008; Guo et al, 2008; Guo et al, 2009).
Therefore, in this section we focus on recent work
on query segmentation (Bergsma and Wang, 2007;
Hagen et al, 2010). We compare the segmentation
effectiveness of our best performing method, j-PRF,
to that of these query segmentation methods.
The first method, SEG-1, was first proposed by
Hagen et al (2010). It is currently the most effective
publicly disclosed unsupervised query segmentation
method. SEG-1 method requires an access to a large
web n-gram corpus (Brants and Franz, 2006). The
optimal segmentation for query Q, S?Q, is then ob-
tained using
S?Q = argmax
S?SQ
?
s?S,|s|>1
|s||s|count(s),
where SQ is the set of all possible query segmenta-
tions, S is a possible segmentation, s is a segment
in S, and count(s) is the frequency of s in the web
n-gram corpus.
The second method, SEG-2, is based on a success-
ful supervised segmentation method, which was first
proposed by Bergsma and Wang (2007). SEG-2 em-
ploys a large set of features, and is pre-trained on the
query collection described by Bergsma and Wang
(2007). The features used by the SEG-2 method are
described by Bendersky et al (2009), and include,
among others, n-gram frequencies in a sample of a
query log, web corpus and Wikipedia titles.
Table 3 demonstrates the comparison between the
j-PRF, SEG-1 and SEG-2 methods. When com-
pared to the SEG-1 baseline, j-PRF is significantly
more effective, even though it only employs bigram
counts (see Eq. 4), instead of the high-order n-grams
used by SEG-1, for computing the score of a seg-
mentation. This results underscores the benefit of
joint annotation, which leverages capitalization and
POS tagging to improve the quality of the segmen-
tation.
When compared to the SEG-2 baseline, j-PRF
and SEG-2 are statistically indistinguishable. SEG-2
posits a slightly better F1, while j-PRF has a better
MQA. This result demonstrates that the segmenta-
tion produced by the j-PRF method is as effective as
the segmentation produced by the current supervised
state-of-the-art segmentation methods, which em-
ploy external data sources and high-order n-grams.
The benefit of the j-PRF method compared to the
SEG-2 method, is that, simultaneously with the seg-
mentation, it produces several additional query an-
notations (in this case, capitalization and POS tag-
ging), eliminating the need to construct separate se-
quence classifiers for each annotation.
7 Conclusions
In this paper, we have investigated a joint approach
for annotating search queries with linguistic struc-
tures, including capitalization, POS tags and seg-
mentation. To this end, we proposed a probabilis-
tic approach for performing joint query annotation
that takes into account the dependencies that exist
between the different annotation types.
Our experimental findings over a range of queries
from a web search log unequivocally point to the su-
periority of the joint annotation methods over both
query-based and pseudo-relevance feedback based
independent annotation methods. These findings in-
dicate that the different annotations are mutually-
dependent.
We are encouraged by the success of our joint
query annotation technique, and intend to pursue the
investigation of its utility for IR applications. In the
future, we intend to research the use of joint query
annotations for additional IR tasks, e.g., for con-
structing better query formulations for ranking al-
gorithms.
8 Acknowledgment
This work was supported in part by the Center for In-
telligent Information Retrieval and in part by ARRA
NSF IIS-9014442. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect those of the sponsor.
110
References
Niranjan Balasubramanian and James Allan. 2009. Syn-
tactic query models for restatement retrieval. In Proc.
of SPIRE, pages 143?155.
Cory Barr, Rosie Jones, and Moira Regelson. 2008. The
linguistic structure of english web-search queries. In
Proc. of EMNLP, pages 1021?1030.
Michael Bendersky and W. Bruce Croft. 2009. Analysis
of long queries in a large scale search log. In Proc. of
Workshop on Web Search Click Data, pages 8?14.
Michael Bendersky, David Smith, and W. Bruce Croft.
2009. Two-stage query segmentation for information
retrieval. In Proc. of SIGIR, pages 810?811.
Michael Bendersky, W. Bruce Croft, and David A. Smith.
2010. Structural annotation of search queries using
pseudo-relevance feedback. In Proc. of CIKM, pages
1537?1540.
Shane Bergsma and Qin I. Wang. 2007. Learning noun
phrase query segmentation. In Proc. of EMNLP, pages
819?826.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Chris Buckley. 1995. Automatic query expansion using
SMART. In Proc. of TREC-3, pages 69?80.
Jenny R. Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc.
of NAACL, pages 326?334.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Proc. of SIGIR, pages 379?386.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In Proc. of SIGIR,
pages 267?274.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Braeutigam. 2010. The power of naive query
segmentation. In Proc. of SIGIR, pages 797?798.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query segmentation re-
visited. In Proc. of WWW, pages 97?106.
Rosie Jones and Daniel C. Fain. 2003. Query word dele-
tion prediction. In Proc. of SIGIR, pages 435?436.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proc. of WWW, pages 387?396.
Giridhar Kumaran and James Allan. 2007. A case for
shorter queries, and helping user create them. In Proc.
of NAACL, pages 220?227.
Giridhar Kumaran and Vitor R. Carvalho. 2009. Re-
ducing long queries using query quality predictors. In
Proc. of SIGIR, pages 564?571.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proc. of ICML, pages 282?289.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance
based language models. In Proc. of SIGIR, pages 120?
127.
Matthew Lease. 2007. Natural language processing for
information retrieval: the time is ripe (again). In Pro-
ceedings of PIKM.
Xiao Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proc. of ACL, pages 1337?
1345, Morristown, NJ, USA.
Rachel T. Lo, Ben He, and Iadh Ounis. 2005. Auto-
matically building a stopword list for an information
retrieval system. In Proc. of DIR.
Yumao Lu, Fuchun Peng, Gilad Mishne, Xing Wei, and
Benoit Dumoulin. 2009. Improving Web search rel-
evance with semantic features. In Proc. of EMNLP,
pages 648?657.
Mehdi Manshadi and Xiao Li. 2009. Semantic Tagging
of Web Search Queries. In Proc. of ACL, pages 861?
869.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP, pages 157?166.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL, pages 950?958.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
CIKM, pages 683?690.
Dou Shen, Toby Walkery, Zijian Zhengy, Qiang Yangz,
and Ying Li. 2008. Personal name classification in
web queries. In Proc. of WSDM, pages 149?158.
Bin Tan and Fuchun Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In Proc. of WWW, pages 347?356.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34:161?191,
June.
Xing Wei, Fuchun Peng, and Benoit Dumoulin. 2008.
Analyzing web text association to disambiguate abbre-
viation in queries. In Proc. of SIGIR, pages 751?752.
111
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 507?516,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Feature-Based Selection of Dependency Paths
in Ad Hoc Information Retrieval
K. Tamsin Maxwell
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB, UK
t.maxwell@ed.ac.uk
Jon Oberlander
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB, UK
j.oberlander@ed.ac.uk
W. Bruce Croft
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003, USA
croft@cs.umass.edu
Abstract
Techniques that compare short text seg-
ments using dependency paths (or simply,
paths) appear in a wide range of automated
language processing applications including
question answering (QA). However, few
models in ad hoc information retrieval (IR)
use paths for document ranking due to
the prohibitive cost of parsing a retrieval
collection. In this paper, we introduce a
flexible notion of paths that describe chains
of words on a dependency path. These
chains, or catenae, are readily applied in
standard IR models. Informative catenae
are selected using supervised machine
learning with linguistically informed fea-
tures and compared to both non-linguistic
terms and catenae selected heuristically
with filters derived from work on paths.
Automatically selected catenae of 1-2
words deliver significant performance
gains on three TREC collections.
1 Introduction
In the past decade, an increasing number of
techniques have used complex and effective
syntactic and semantic features to determine the
similarity, entailment or alignment between short
texts. These approaches are motivated by the idea
that sentence meaning can be flexibly captured by
the syntactic and semantic relations between words,
and encoded in dependency parse tree fragments.
Dependency paths (or simply, paths) are compared
using techniques such as tree edit distance (Pun-
yakanok et al, 2004; Heilman and Smith, 2010),
relation probability (Gao et al, 2004) and parse tree
alignment (Wang et al, 2007; Park et al, 2011).
Much work on sentence similarity using
dependency paths focuses on question answering
(QA) where textual inference requires attention
to linguistic detail. Dependency-based techniques
can also be highly effective for ad hoc information
retrieval (IR) (Park et al, 2011). However, few
path-based methods have been explored for ad
hoc IR, largely because parsing large document
collections is computationally prohibitive.
In this paper, we explore a flexible application
of dependency paths that overcomes this difficulty.
We reduce paths to chains of words called catenae
(Osborne and Gro?, 2012) that capture salient
semantic content in an underspecified manner.
Catenae can be used as lexical units in a reformu-
lated query to explicitly indicate important word
relationships while retaining efficient and flexible
proximity matching. Crucially, this does not
require parsing documents. Moreover, catenae are
compatible with a variety of existing IR models.
We hypothesize that catenae identify most units
of salient knowledge in text. This is because
they are a condition for ellipsis, in which salient
knowledge can be successfully omitted from text
(Osborne and Gro?, 2012). To our knowledge, this
paper is the first time that catenae are proposed
as a means for term selection in IR, and where
ellipsis is considered as a means for identification
of semantic units.
We also extend previous work with development
of a linguistically informed, supervised machine
learning technique for selection of informative
catenae. Previous heuristic filters for dependency
paths (Lin and Pantel, 2001; Shen et al, 2005;
Cui et al, 2005) can exclude informative relations.
Alternatively, treating all paths as equally infor-
mative (Punyakanok et al, 2004; Park et al, 2011;
Moschitti, 2008) can generate noisy word relations
and is computationally intensive.
The challenge of path selection is that no
explicit information in text indicates which paths
are relevant. Consider the catenae captured by
heuristic filters for the TREC1 query, ?What
role does blood-alcohol level play in automobile
accident fatalities? (#358, Table 1). It may appear
obvious that the component words of ?role play?
1Text REtrieval Conference, see http://trec.nist.gov/
507
blood alcohol
level play
auto accident
accident fatal
role play
play fatal
blood alcohol play
play accident fatal
auto accident fatal
level play fatal
role play fatal
role level play
blood alcohol
level play
auto accident
accident fatal
role blood
alcohol level
play auto
blood alcohol
level play
auto accident
accident fatal
role play
play fatal
Catenae Sequential dependenceGovernor?ependent
Query: What role does blood-alcohol level play in automobile* accident fatalities*?    (*abbreviated to `auto', `fatal')
auto accident
accident fatal
play fatal
play accident fatal
auto accident fatal
Predicate?rgument
auto accident
accident fatal
auto accident fatal
level play fatal
role play fatal
Nominal end slots
Table 1: Catenae derived from dependency paths, as selected by heuristic methods. Selections are
compared to sequential bigrams that use no linguistic knowledge.
and ?level play? do not have an important semantic
relationship relative to the query, yet these catenae
are described by parent-child relations that are
commonly used to filter paths in text processing
applications. Alternative filters that avoid such
trivial word combinations also omit descriptions of
key entities such as ?blood alcohol?, and identify
longer catenae that may be overly restrictive.
These shortcomings suggest that an optimized
selection process may improve performance of
techniques that use dependency paths in ad hoc IR.
We identify three previously proposed selection
methods, and compare them on the task of catenae
selection for ad hoc IR. Selections are tested
using three TREC collections: Robust04, WT10G,
and GOV2. This provides a diverse platform for
experiments. We also develop a linguistically
informed machine learning technique for catenae
selection that captures both key aspects of heuristic
filters, and novel characteristics of catenae and
paths. The basic idea is that selection, or weighting,
of catenae can be improved by features that are
specific to paths, rather than generic for all terms.
Results show that our selection method is more
effective in identifying key catenae compared
to previously proposed filters. Integration of the
identified catenae in queries also improves IR ef-
fectiveness compared to a highly effective baseline
that uses sequential bigrams with no linguistic
knowledge. This model represents the obvious
alternative to catenae for term selection in IR.
The rest of this paper is organised as follows.
?2 reviews related work, ?3 describes catenae
and their linguistic motivation and ?4 describes
our selection method. ?5 evaluates classification
experiments using the supervised filter. ?6 presents
the results of experiments in ad hoc IR. Finally, ?7
concludes the paper.
2 Related work
Techniques that compare short text segments
using dependency paths are applied to a wide range
of automated language processing tasks, including
paraphrasing, summarization, entailment detection,
QA, machine translation and the evaluation of
word, phrase and sentence similarity. A generic
approach uses a matching function to compare a
dependency path between any two stemmed terms
x and y in a sentence A with any dependency path
between x and y in sentence B. The match score
for A and B is computed over all dependency
paths in A.
In QA this approach improves question repre-
sentation, answer selection and answer ranking
compared to methods that use bag-of-words
and ngram features (Surdeanu et al, 2011). For
example, Lin and Pantel (2001) present a method
to derive paraphrasing rules for QA using analysis
of paths that connect two nouns; Echihabi and
Marcu (2003) align all paths in questions with
trees for heuristically pruned answers; Cui et
al. (2005) score answers using a variation of the
IBM translation model 1; Wang et al (2007)
use quasi-synchronous translation to map all
parent-child paths in a question to any path in an
answer; and Moschitti (2008) explores syntactic
and semantic kernels for QA classification.
In ad hoc IR, most models of term dependence
use word co-occurrence and proximity (Song and
Croft, 1999; Metzler and Croft, 2005; Srikanth and
Srihari, 2002; van Rijsbergen, 1993). Syntactic
language models for IR are a significant departure
from this trend (Gao et al, 2004; Lee et al, 2006;
Cai et al, 2007; Maisonnasse et al, 2007) that
use dependency paths to address long-distance
dependencies and normalize spurious differences
in surface text. Paths are constrained in both
508
prd loc pmod loc pmod
Is    polio under control  in   China ?
X1 X2 X3 X4 X5 X6
poliopolio controlcontrolcontrol ChinaChinapolio control China
polio       under        control
control       in       China
polio       under        control       in        China 
loc pmod
loc pmod
loc pmod loc pmod
Catenae ?toplisted? Dependency paths
Figure 1: Catenae are an economical and intuitive
representation of dependency paths.
queries and documents to parent-child relations.
In contrast, (Park et al, 2011) present a quasi-
synchronous translation model for IR that does not
limit paths. This is based on the observation that
semantically related words have a variety of direct
and indirect relations. All of these models require
parsing of an entire document collection.
Techniques using dependency paths in both QA
and ad hoc IR show promising results, but there
is no clear understanding of which path constraints
result in the greatest IR effectiveness. We directly
compare selections of catenae as a simplified
representation of paths.
In addition, a vast number of methods have
been presented for term weighting and selection
in ad hoc IR. Our supervised selection extends the
successful method presented by Bendersky and
Croft (2008) for selection and weighting of query
noun phrases (NPs). It also extends work for deter-
mining the variability of governor-dependent pairs
(Song et al, 2008). In contrast to this work, we
apply linguistic features that are specific to catenae
and dependency paths, and select among units
containing more than two content-bearing words.
3 Catenae as semantic units
Catenae (Latin for ?chain?, singular catena) are
dependency-based syntactic units. This section
outlines their unique semantic properties.
A catena is defined on a dependency graph that
has lexical nodes (or words) linked by binary asym-
metrical relations called dependencies. Depen-
dencies hold between a governor and a dependent
and may be syntactic or semantic in nature (Nivre,
2005). A dependency graph is usually acyclic such
that each node has only one governor, and one root
node of the tree does not depend on any other node.
A catena is a word, or sequence of words that are
continuous with respect to a walk on a dependency
Is polio under control in China, and is polio under control in India?
Antecedent
First conjunct:Antecedent clause Second conjunct:Elliptical/target clause
Elided text Remnant
Figure 2: Ellipsis in a coordinated construct.
graph. For example, Fig. 1 shows a dependency
parse that generates 21 catenae in total: (using
i for Xi) 1, 2, 3, 4, 5, 6, 12, 23, 34, 45, 56, 123,
234, 345, 456, 1234, 2345, 3456, 12345, 23456,
123456. We process catenae to remove stop words
on the INQUERY stoplist (Allan et al, 2000) and
lexical units containing 18 TREC description stop
words such as ?describe?. This results in a reduced
set of catenae as shown in Fig. 1.
A dependency path is ordered and includes both
word tokens and the relations between them. In
contrast, a catena is a set of word types that may
be ordered or partially ordered. A catena is an
economical, intuitive lexical unit that corresponds
to a dependency path and is argued to play an
important role in syntax (Osborne et al, 2012).
In this paper, we explore catenae instead of paths
for ad hoc IR due to their suitability for efficient IR
models and flexible representation of language se-
mantics. Specifically, we note that catenae identify
words that can be omitted in elliptical constructions
(Osborne et al, 2012). They thus represent salient
semantic information in text. To clarify this insight,
we briefly review catenae in ellipsis.
3.1 Semantic units in ellipsis
Fig. 2 shows terminology for the phenomenon
of ellipsis. The omitted words are called elided
text, and words that could be omitted, but are not,
we call elliptical candidates.
Ellipsis relies on the logical structure of a
coordinated construction in which two or more
elements, such as sentences, are joined by a
conjunctive word or phrase such as ?and? or
?more than?. A coordinated structure is required
because the omitted words are ?filled in? by
assuming a parallel relation p between the first
and second conjunct. In ellipsis, p is omitted and
its arguments are retained in text. In order for
ellipsis to be successful and grammatically correct,
p must be salient shared knowledge at the time of
communication (Prince, 1986; Steedman, 1990). If
p is salient then the omitted text can be inferred. If
p is not salient then the omission of words merely
results in ungrammatical, or incoherent, sentences.
This framework is practically illustrated in Fig.
509
   Is polio under control in China, and ?is polio under control? in India ?   Is polio under control in China, and is cancer under observation ?in China? ?* Is polio under control in China, and ?is? cancer ?nder? observation ?in China? ?* Is polio under control in China, and ?is polio? under ?ontrol in? India ?
Caatentn ?optpslin ds?ip to tlsat?c lyih s?i ?liosi
a?                                         in India    ?b?   is cancer under observation        ?c? *    cancer            observation        ?d? *                under                 India   ?
Is polio under control in China, and...
Caatpip niolio?in
Figure 3: For ellipsis to be successful, elided words must be catenae. Ellipsis candidates are catenae2.
Is    polio under control  in   China ?
X1 X2
X3 X4
X5 X6
Figure 4: A parse in which ?polio China? is a
catena.
3 for the query, ?Is polio under control in China??.
Sentences marked by * are incoherent, and it is
evident that the omitted words do not form a salient
semantic unit. They also do not form catenae. In
contrast, the omitted words in successful ellipsis
do form catenae, and they represent informative
word combinations with respect to the query. This
observation leads us to an ellipsis hypothesis:
Ellipsis hypothesis: For queries formulated
into coordinated structures, the subset of
catenae that are elliptical candidates identify
the salient semantic units in the query.
3.2 Limitations of paths and catenae
The prediction of salient semantic units by cate-
nae is quite robust. However, there are two prob-
lems that can limit the effectiveness of any tech-
nique that uses catenae or dependency paths in IR.
1) Syntactic ambiguity: We make the simpli-
fying assumption that the most probable parse of
a query is accurate and sufficient for the extraction
of relevant catenae. However, this is not always
true. For example, the sentence ?Is polio under
control in China, and under observation ??
constitutes successful ellipsis. The elided words
?polio in china? are relevant to a base query, ?Is
polio under control in China??. Unfortunately,
in Fig. 1 the elided text does not qualify as a
catena. A parse with alternative prepositional
phrase attachment is shown in Fig. 4. Here, the
successfully elided text does qualify as a catena.
This highlights the fact that a single dependency
parse may only partially represent the ambiguous
semantics of a query. More accurate parsing does
not address this problem.
2) Rising: Automatic extraction of catenae is
limited by the phenomenon of rising. Let the
Is poolooiundeoer cdeltoolsooolooC lhua
X4X3X2
X1
X5
X6 X7
Standard structure
?ooiundeoer cdeltoIs poolsooolooC lhua
X3X2X1 X4g X5
X6 X7
Rising structure
Figure 5: A parse with and without rising. The
dashed dependency edge marks where a head is
not also the governor and the g-script marks the
governor of the risen catena.
governor of a catena be the word that licenses
it (in Fig. 5 ?used? licenses ?a toxic chemical?
e.g. ?used what??). Let the head of a catena be
its parent in a dependency tree. Rising occurs
when the head is not the same as the governor.
This is frequently seen with wh-fronting questions
that start who, what etc., as well as with many
other syntactic discontinuities (Osborne and Gro?,
2012). More specifically, rising occurs when a
catena is separated from its governor by words
that its governor does not dominate, or the catena
dominates the governor, as in Fig. 5. Note that
in the risen structure, the words for the catena
?chemical as a weapon? are discontinuous on the
surface, interrupted by the word ?used?.
4 Selection method for catenae
Catenae describe relatively few of the possible
word combinations in a sentence, but still include
many combinations that do not result in successful
ellipsis and are not informative for IR.
This section describes our supervised method
for selection of informative catenae. Candidate
catenae are identified using two constraints that
enable more efficient extraction: stopwords are
removed, and stopped catenae must contain fewer
than four words (single words are permitted). We
use a pseudo-projective joint dependency parse
and semantic role labelling system (Johansson and
510
Nugues, 2008) to generate the dependency parse.
This enables us to explore semantic classification
features and is highly accurate. However, any
dependency parser may be applied instead. For
comparison, catenae extracted from 500 queries
using the Stanford dependency parser (de Marneffe
et al, 2006) overlap with 77% of catenae extracted
from the same queries using the applied parser.
4.1 Feature Classes
Four feature classes are presented in Table 2:
Ellipsis candidates: The ellipsis hypothesis
suggests that informative catenae are elliptical
candidates. However, queries are not in the
coordinated structures required for ellipsis. To
enable extraction of characteristic features we (a)
construct a coordinated query by adding the query
to itself; and (b) elide catenae from the second
conjunct. For example, for the query, Is polio
under control in China? we have:
(a) Is polio under control in China, and is
polio under control in China?
(b) Is polio under control in China, and is
polio in China?
We refer to the words in (b) as the query remainder
and use this to identify features detailed in Table 2.
Dependency path features: Part-of-speech
tags and semantic roles have been used to filter
dependency paths. We identify several features that
use these characteristics from prior work (Table 2).
In addition, variability in the separation distance
in documents observed for words that have
governor-dependent relations in queries has been
proposed for identification of promising paths
(Song et al, 2008). We also observe that due to the
phenomenon of rising, words that form catenae can
be discontinuous in text, and the ability of catenae
to match similar word combinations is limited by
variability of how they appear in documents. Thus,
we propose features for separation distance, but use
efficient collection statistics rather than summing
statistics for every document in a collection.
Co-occurrence features: A governor w1 tends
to subcategorize for its dependents wn. This
means that w1 often determines the choice of wn.
We conclude that co-occurrence is an important
feature of dependency relations (Mel?c?uk, 2003).
In addition, term frequencies and inverse document
frequencies calculated using word co-occurrence
measures are commonly used in IR. We use
features previously proposed for filtering terms in
IR (Bendersky and Croft, 2008) with two methods
to normalize co-occurrence counts for catenae of
different lengths: a factor |c||c|, where |c| is the
number of words in catena c (Hagen et al, 2011),
and the average score for a feature type over all
pairwise word combinations in c.
IR performance predictors: Catenae take the
same form as typical IR search terms. For this
reason, we also use predictors of IR effectiveness
previously applied to IR terms.
In general, path and co-occurrence features are
similar to those applied by Surdeanu et al (2011)
but we do not parse documents. Path features
are also similar to Song et al (2008), but more
efficient and suited to units of variable length.
Ellipsis features have not been used before.
5 Experimental setup
5.1 Classification
Catenae selection is framed as a supervised
classification problem trained on binary human
judgments of informativeness: how well catenae
represent a query and discriminate between
relevant and non-relevant documents in a col-
lection. Kappa for two annotators on catenae
in 100 sample queries was 0.63, and test-retest
reliability for individual judges was similar (0.62)3.
Although this is low, human annotations produced
consistently better classification accuracy than
other labelling methods explored.
We use the Weka (Hall et al, 2009) Ad-
aBoost.M1 meta-classifier (Freund and Schapire,
1996) with unpruned C4.5 decision trees as base
learners to classify catenae as informative or
not. Adaboost.M1 boosts decisions over T weak
learners for T features using weighted majority
voting. At each round, predictions of a new learner
are focused on incorrectly classified examples
from the previous round. Adaboost.M1 was
selected in preference to other algorithms because
it performed better in preliminary experiments,
leverages many weak features to advantage, and
usually does not overfit (Schapire et al, 1997).
Predictions are made using 10-fold cross-
validation. There are roughly three times the
number of uninformative catenae compared to
informative catenae. In addition, the number of
training examples is small (1295 to 5163 per collec-
tion). To improve classifier accuracy, the training
data for each collection is supplemented and
balanced by generating examples from queries for
3Catenae, judgments and annotation details available at
ciir.cs.umass.edu/?tmaxwell
511
isSeq
Minimum perplexity of ngrams with length 2, 3, and 4 in a window of up to a 3 words around the site of catenae omission. This is the area where ungrammaticality may be introduced. For the remainder R=`ABCDE&ABE' we compute ppl1 for ?ABE, &AB, ABE, &A, AB, BE?
R_ppl1
R_strict
Compliance with strict hand?oded rules for grammaticality of a remainder. Rules include unlikely orderings of punctuation and part?f?speech ?OS? tags ?.g. ,, ?, poor placement of determiners and punctuation, and orphaned words, such as adjectives without the nouns they modify.
R_relax
A relaxed version of hand?oded rules for R_strict. Some rules were observed to be overly aggressive in detection of ungrammatical remainders.
Ellipsis candidate features (E)
Co-occurrence features (C)
IR performance prediction features (I)
c_ppl1
Dependency path features (D) (continued)
Dependency paths traverse nodes including stopwords and may be filtered based on POS tags. We use perplexity for the sequence of POS tags in catenae before removing stopwords. This is computed using a POS language model built on ukWaC parsed wikipedia data ?aroni et al, 2009?.
phClass
Phrasal class for a catena, with options NP, VP and Other. A catena has a NP or VP class if it is, or is entirely contained by, an NP or VP ?ong et al, 2008?.
NP_split
Unsuccessful ellipsis often results if elided words only partly describe a base NP. Boolean feature for presence of a partial NP in the remainder. NPs ?nd PPs? are identified using the MontyLingua toolkit.
PP_split As for NP_split, defined for prepositional phrases (PP). 
F_split As for NP_split, defined for finite clauses.
semRole
Boolean feature indicating whether a catena describes all, or part of, a predicate?rgument structure ?AS?. Previous work approximated PAS by using paths between head nouns and verbs, and all paths excluding those within base chunks.
c_len Length of a stopped catenae. Longer terms tend to reduce IR recall.
Boolean indicating if catena words are sequential in stoplisted surface text. 
cf_ow
Frequency of a catena in the retrieval collection, words appearing ordered in a window the length of the catena. 
cf_uw As for cf_ow, but words may appear unordered.
cf_uw8 As for cf_uw, but the window has a length of 8 words.
idf_ow
Inverse document frequency ?idf? where document 
frequency ?df? of a catena is calculated using cf_ow 
windows. Let N  be the number of documents in the retrieval collection, then:
                      idf(Ci) = log2
N
df(Ci)
and idf(Ci) = N  if df(Ci) = 0.
idf_uw As for idf_ow, but words may appear unordered.
idf_uw8 As for idf_uw, but the window has a length of 8 words.
gf
Google ngrams frequency ?rants and Franz, 2006? from a web crawl of approximately one trillion English word tokens. Counts from a large collection are expected to be more reliable than those from 
smaller test collections.
WIG
Normalized Weighted Information Gain ?WIG? is the change in information over top ranked documents between a random ranked list and an actual ranked list retrieved with a catena c ?hou and Croft, 2007?. 
    wig(c) =
1k
?
d?Dk(c) log p(c|d) ? log p(c|C)?log p(c|C)
where Dk are the top k=50 documents retrieved 
with catena c from collection C, and p(c|?) are maximum likelihood estimates. A second feature uses the average WIG score for all pairwise word combinations in c.
qf_in
Frequency of appearance in queries from the Live Search 2006 search query log ?pproximately 15 million queries?. Query log frequencies are a measure of the likelihood that a catena will appear in any query. 
wf_in As for qf_in, but using frequency counts in Wikipedia titles instead of queries.
sepMode
Most frequent separation distance of words in catena c in the retrieval collection, with possible 
values S = ?1, 2, 3, long?. 1 means that all words are 
adjacent, 2 means separation by 0-1 words, and long 
means containment in a window of size 4 ? |c|.
H_c
Entropy for separation distance s of words in catena 
c in the retrieval collection.fs is the frequency of c 
in window size s, and fS is the frequency of c in a 
window of size 4 ? |c| . All f are normalized for 
catena length using |c||c| ?agen et al, 2011?.
              Hc =
?
s?S
fs + 0.5fS + 0.5 log2
fs + 0.5fS + 0.5
sepRatio
Where fs and fS are defined as for H_c:
                        sepRatioc =
fs>2 + 0.5fS + 0.5
wRatio
For words w in catena c; fS is defined as for H_c.
                   wRatioc =
0.5 + 1|c|?w?c fw
fS + 0.5
nomEnd
Boolean indicating whether the words at each end of the catena are nouns ?r the catena is a single noun?.
Dependency path features (D)
Table 2: Classifier features.
512
Feature Classes
Pr
 
ROB04
WT10G
GOV2
 
D-CIE-CIE-DE-D-CI
R
86.2 72.8
79.3 67.1
77.0 68.0
Pr R
83.5 67.5
76.9 59.7
70.9 61.8
Pr R
86.2 71.7
77.2 65.6
72.8 63.9
Pr R
86.2 72.0
79.6 66.1
75.5 67.2
 
Table 3: Average classifier precision (Pr) and recall
(R) over 10 folds. Pr is % positive predictions
that are correct. R is % positive labeled instances
predicted as positive. A combination of all classes
marginally performs best.
other collections used in this paper, plus TREC8-
QA. For example, training data for Robust04
includes data from WT10G, GOV2 and TREC8-
QA. Any examples that replicate catenae in the test
collection are excluded. For Robust04, WT10G
and GOV2 respectively, 30%, 82% and 69% of the
training data is derived from other collections.
5.2 Classification results
Average classification precision and recall is
shown in Table 3. Co-occurrence and IR effective-
ness prediction features (CI) was the most influen-
tial class, and accounted for 70% of all features in
the model. Performance is marginally better using
all features (E-D-CI) with a moderate improvement
over human agreement on the annotation task. The
E-D-CI filter is used in subsequent experiments.
Catenae were predicted for all queries. Predic-
tions were more accurate for Robust04 than the
other two collections. One potential explanation
is that Robust04 queries are longer on average
(up to 32 content words per query, compared to
up to 16 words) so they generate a more diverse
set of catenae that are more easily distinguished
with respect to informativeness. The proportion
of training data specific to the retrieval collection
may also be a factor. Longer queries produce a
greater number of catenae, so less training data
from other collections is required.
6 Evaluation framework
6.1 Baseline IR models
Baselines are a unigram query likelihood (QL)
model (bag of words) and a highly effective
sequential dependence (SD) variant of the Markov
random field (MRF) model (Metzler and Croft,
2005). SD uses a linear combination of three
cliques of terms, where each clique is prioritized
by a weight ?c. The first clique contains individual
words (query likelihood QL), ?1 = 0.85. The
second clique contains query bigrams that match
document bigrams in 2-word ordered windows
(?#1?), ?2 = 0.1. The third clique uses the same
bigrams as clique 2 with an 8-word unordered
window (?#uw8?), ?3 = 0.05. For example, the
query new york city in Indri4 query language is:
#weight(
?1 #combine(new york city)
?2 #combine(#1(new york) #1(york city))
?3 #combine(#uw8(new york) #uw8(york city)))
SD is a competitive baseline in IR (Bendersky
and Croft, 2008; Park et al, 2011; Xue et al,
2010). Our reformulated model uses the same
query format as SD, but the second and third
cliques contain filtered catenae instead of query
bigrams. In addition, because catenae may be
multi-word units, we adjust the unordered window
size to 4 ? |c|. So, if two catenae ?york? and ?new
york city? are selected, the last clique has the form:
?3 #combine( york #uw12(new york city))
This query representation enables word relations
to be explicitly indicated while maintaining
efficient and flexible matching of catenae in
documents. Moreover, it does not use dependency
relations between words during retrieval, so there
is no need to parse a collection.
6.2 Baseline catenae selection
We explore four filters for catenae. Three are
based on previous work and describe heuristic
features of promising catenae. The fourth is our
novel supervised classifier.
NomEnd: Catenae starting and ending with
nouns, or containing only one word that is a noun.
Paths between nouns are used by Lin and Pantel
(2001).
SemRol: Catenae in which all component
words are either predicates or argument heads.
This is based on work that uses paths between head
nouns and verbs (Shen et al, 2005), semantic roles
(Moschitti, 2008), and all dependency paths except
those that occur between words in the same base
chunk (e.g. noun / verb phrase) (Cui et al, 2005).
GovDep: Cantenae containing words with a
governor-dependent relation. Many IR models
use this form of path filtering e.g. (Gao et al,
2004; Wang et al, 2007). Relations are ?collapsed?
by removing stopwords to reduce the distance
between content nodes in a dependency graph.
4http://www.lemurproject.org/
513
ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrQL 25.25 28.69 19.55 22.77 25.77 31.26SD 26.57? 30.02? 20.63 24.31? 28.00? 33.30?NomEnd 25.91? 29.35? 20.81? 24.27? 27.41? 32.94?GovDep 26.26? 29.63? 21.06 24.23? 27.87? 33.51?SemRol 25.70? 29.06 19.78 22.93 26.76 32.49?SFeat 27.04? 30.11? 20.84? 24.31? 28.43? 33.84?SF-12 27.03? 30.20? 21.62? 24.81? 28.57? 34.01?
Table 4: IR results using filtered catenae consistently improve over non-linguistic methods.
Significance(p < .05) shown compared to QL (?) and SD (?).
ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrSF-12 27.03 30.20 21.62 24.81 28.57 34.01SF-123 26.83 30.34 21.34 24.64 28.77 34.24SF-NE 26.51 29.86 21.42 24.55 27.96 33.26SF-GD 26.22 29.48 20.33 23.72 28.30 33.83Gold 27.92 31.15 22.56 25.69 29.65 35.08
Table 5: Results with supervised selection of catenae with specified length (SF-12, SF-123) are more
effective than combinations of SFeat with heuristic NomEnd (SF-NE) or GovDep (SF-GD).
6.3 Experiments
Experiments compare queries reformulated
using catenae selected by baseline filters and our
supervised selection method (SFeat) to SD and
a bag-of-words model (QL). We also compare IR
effectiveness of all catenae filtered using SFeat
with approaches that combine SFeat with baseline
filters. All models are implemented using the Indri
retrieval engine version 4.12.
6.4 Results
Results in Table 4 show significant improvement
in mean average precision (MAP) of queries using
catenae compared to QL. Consistent improvements
over SD are also demonstrated for supervised
selection applied to all catenae (SFeat) and catenae
with only 1-2 words (SF-12) across all collections
(Table 5). Overall, changes are small and fairly
robust, with one half to two thirds of all queries
showing less than 10% change in MAP.
Unlike sFeat, other filters tend to decrease per-
formance compared to SD. Governor-dependent
relations for WT10G are an exception and we spec-
ulate that this is due to a negative influence of 3-
word catenae for this collection. Manual inspection
suggests that WT10G queries are short and have
relatively simple syntactic structure (e.g. few PP
attachment ambiguities). This means that 3-word
catenae (in all models except GovDep) tend to in-
clude uninformative words, such as ?reasons? in
?fasting religious reasons?. In contrast, 3-word cate-
nae in other collections tend to identify query sub-
concepts or phrases, such as ?science plants water?.
Classification results for catenae separated by
length, such that the classifier for catenae with a
specific length are trained on examples of catenae
with the same length, confirm this intuition. The
rejection rate for 3-word catenae is twice as high
for WT10G as for other collections. It is also
more difficult to distinguish informative 3-word
catenae compared to catenae with 1-2 words. To
assess the impact of classification accuracy on IR
effectiveness, Table 5 shows results with oracle
knowledge of annotator judgments.
The SF-12 model combines catenae predicted for
lengths 1 and 2. Its strong performance across all
collections suggests that most of the benefit derived
from catenae in IR is found in governor-dependent
and single word units, where single words are
important (GovDep uses only 2-word catenae).
Another major observation (Table 5) is that mixing
baseline heuristic filters with a supervised ap-
proach is not as successful as supervised selection
alone. In particular, performance decreases for
filtered governor-dependent pairs. This suggests
that some important word relations in GovDep and
NomEnd are captured by triangulation.
Finally, we review selected catenae for queries
that perform significantly better or worse than SD
(> 75% change in MAP). The best IR effectiveness
occurs when selected catenae clearly focus on the
most important aspect of a query. Poor perfor-
514
mance is caused by a lack of focus in a catenae set,
even though selected catenae are reasonable, or an
emphasis on words that are not central to the query.
The latter can occur when words that are not es-
sential to query semantics appear in many catenae
due to their position in the dependency graph.
7 Conclusion
We presented a flexible implementation of
dependency paths for long queries in ad hoc IR that
does not require dependency parsing a collection.
Our supervised selection technique for catenae
addresses the need to balance a representation of
language expressiveness with effective, efficient
statistical methods. This is a core challenge in
computational linguistics.
It is not possible to directly compare perfor-
mance of our approach with ad hoc techniques in
IR that parse a retrieval collection. However, we
note that a recent result using query translation
based on dependency paths (Park et al, 2011)
reports 14% improvement over query likelihood
(QL). Our approach achieves 7% improvement
over QL on the same collection. We conclude that
catenae do not replace path-based techniques, but
may offer some insight into their application, and
have particular value when it is not practical to
parse target documents to determine text similarity.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval. Any opinions,
findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
References
James Allan, Margaret E. Connell, W. Bruce Croft,
Fang-Fang Feng, David Fisher, and Xiaoyan Li.
2000. INQUERY and TREC-9. In Proceedings of
TREC-9, pages 551?562.
Michael Bendersky and W. Bruce Croft. 2008.
Discovering key concepts in verbose queries. In
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?08, pages 491?498,
New York, NY, USA. ACM.
Keke Cai, Jiajun Bu, Chun Chen, and Guang Qiu.
2007. A novel dependency language model for in-
formation retrieval. Journal of Zhejiang University
SCIENCE A, 8(6):871?882.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan,
and Tat-Seng Chua. 2005. Question answering
passage retrieval using dependency relations. In
Proceedings of the 28th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?05, pages 400?407,
New York, NY, USA. ACM.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-2006.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering.
In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics - Volume
1, ACL ?03, pages 16?23, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In ICML?96,
pages 148?156.
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and
Guihong Cao. 2004. Dependence language model
for information retrieval. In Proceedings of the 27th
annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?04, pages 170?177, New York, NY, USA.
ACM.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query segmentation
revisited. In Proceedings of the 20th international
conference on World wide web, WWW ?11, pages
97?106, New York, NY, USA. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an up-
date. SIGKDD Explorations Newsletter, 11:10?18,
November.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments,
paraphrases, and answers to questions. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT
?10, pages 1011?1019, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of
CoNNL 2008, pages 183?187.
Changki Lee, Gary Geunbae Lee, and Myung-Gil
Jang. 2006. Dependency structure language model
for information retrieval. In In ETRI journal,
volume 28, pages 337?346.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings
of ACM Conference on Knowledge Discovery
and Data Mining (KDD-01), pages 323?328, San
Francisco, CA.
515
Lo??c Maisonnasse, Eric Gaussier, and Jean-Pierre
Chevallet. 2007. Revisiting the dependence
language model for information retrieval. In
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?07, pages 695?696,
New York, NY, USA. ACM.
Igor A. Mel?c?uk. 2003. Levels of dependency in
linguistic description: Concepts and problems. In
V. Agel, L. Eichinger, H.-W. Eroms, P. Hellwig,
H. J. Herringer, and H. Lobin, editors, Dependency
and Valency. An International Handbook of Contem-
porary Research, volume 1, pages 188?229. Walter
De Gruyter, Berlin?New York.
Donald Metzler and W. Bruce Croft. 2005. A Markov
random field model for term dependencies. In
Proceedings of the 28th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?05, pages 472?479,
New York, NY, USA. ACM.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization.
In Proceeding of the 17th ACM conference on
Information and knowledge management, CIKM
?08, pages 253?262, New York, NY, USA. ACM.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical report, Va?xjo? University:
School of Mathematics and Systems Engineering.
Timothy Osborne and Thomas Gro?. 2012. Con-
structions are catenae: Construction grammar
meets dependency grammar. Cognitive Linguistics,
23(1):165?216.
Timothy Osborne, Michael Putnam, and Gro?. 2012.
Catenae: Introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396, December.
Jae Hyun Park, W. Bruce Croft, and David A. Smith.
2011. A quasi-synchronous dependence model for
information retrieval. In Proceedings of the 20th
ACM international conference on Information and
knowledge management, CIKM ?11, pages 17?26,
New York, NY, USA. ACM.
Ellen F. Prince. 1986. On the syntactic marking of
presupposed open propositions. In Proceedings of
the 22nd Annual Meeting of the Chicago Linguistic
Society, pages 208?222.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping
dependencies trees: An application to question
answering. In Proceedings of AI and MATH
Symposium 2004 (Special session: Intelligent Text
Processing).
Robert E. Schapire, Yoav Freund, Peter Bartlett, and
Wee Sun Lee. 1997. Boosting the margin: A new
explanation for the effectiveness of voting methods.
In Proceedings of ICML, pages 322?330.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for
question answering. In Proceedings of the Second
international joint conference on Natural Language
Processing, IJCNLP?05, pages 507?518, Berlin,
Heidelberg. Springer-Verlag.
Fei Song and W. Bruce Croft. 1999. A general
language model for information retrieval. In Pro-
ceedings of the 8th ACM international conference
on Information and knowledge management, CIKM
?99, pages 316?321, New York, NY, USA. ACM.
Young-In Song, Kyoung-Soo Han, Sang-Bum Kim,
So-Young Park, and Hae-Chang Rim. 2008. A
novel retrieval approach reflecting variability of syn-
tactic phrase representation. Journal of Intelligent
Information Systems, 31(3):265?286, December.
Munirathnam Srikanth and Rohini Srihari. 2002.
Biterm language models for document retrieval. In
Proceedings of the 25th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?02, pages 425?426,
New York, NY, USA. ACM.
Mark J. Steedman. 1990. Gapping as Constituent Co-
ordination. Linguistics and Philosophy, 13(2):207?
263, April.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers
to non-factoid questions from web collections.
Computational Linguistics, 37(2):351?383, June.
C. J. van Rijsbergen. 1993. A theoretical basis for the
use of co-occurrence data in information retrieval.
Journal of Documentation, 33(2):106?119.
Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.
2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of
the 2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 22?32, Prague, Czech Republic, June.
Association for Computational Linguistics.
Xiaobing Xue, Samuel Huston, and W. Bruce Croft.
2010. Improving verbose queries using subset
distribution. In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, CIKM ?10, pages 1059?1068, New
York, NY, USA. ACM.
516
