Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1?9,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Heterogeneous Transfer Learning for Image Clustering via the Social Web
Qiang Yang
Hong Kong University of Science and Technology, Clearway Bay, Kowloon, Hong Kong
qyang@cs.ust.hk
Yuqiang Chen Gui-Rong Xue Wenyuan Dai Yong Yu
Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China
{yuqiangchen,grxue,dwyak,yyu}@apex.sjtu.edu.cn
Abstract
In this paper, we present a new learning
scenario, heterogeneous transfer learn-
ing, which improves learning performance
when the data can be in different feature
spaces and where no correspondence be-
tween data instances in these spaces is pro-
vided. In the past, we have classified Chi-
nese text documents using English train-
ing data under the heterogeneous trans-
fer learning framework. In this paper,
we present image clustering as an exam-
ple to illustrate how unsupervised learning
can be improved by transferring knowl-
edge from auxiliary heterogeneous data
obtained from the social Web. Image
clustering is useful for image sense dis-
ambiguation in query-based image search,
but its quality is often low due to image-
data sparsity problem. We extend PLSA
to help transfer the knowledge from social
Web data, which have mixed feature repre-
sentations. Experiments on image-object
clustering and scene clustering tasks show
that our approach in heterogeneous trans-
fer learning based on the auxiliary data is
indeed effective and promising.
1 Introduction
Traditional machine learning relies on the avail-
ability of a large amount of data to train a model,
which is then applied to test data in the same
feature space. However, labeled data are often
scarce and expensive to obtain. Various machine
learning strategies have been proposed to address
this problem, including semi-supervised learning
(Zhu, 2007), domain adaptation (Wu and Diet-
terich, 2004; Blitzer et al, 2006; Blitzer et al,
2007; Arnold et al, 2007; Chan and Ng, 2007;
Daume, 2007; Jiang and Zhai, 2007; Reichart
and Rappoport, 2007; Andreevskaia and Bergler,
2008), multi-task learning (Caruana, 1997; Re-
ichart et al, 2008; Arnold et al, 2008), self-taught
learning (Raina et al, 2007), etc. A commonality
among these methods is that they all require the
training data and test data to be in the same fea-
ture space. In addition, most of them are designed
for supervised learning. However, in practice, we
often face the problem where the labeled data are
scarce in their own feature space, whereas there
may be a large amount of labeled heterogeneous
data in another feature space. In such situations, it
would be desirable to transfer the knowledge from
heterogeneous data to domains where we have rel-
atively little training data available.
To learn from heterogeneous data, researchers
have previously proposed multi-view learning
(Blum and Mitchell, 1998; Nigam and Ghani,
2000) in which each instance has multiple views in
different feature spaces. Different from previous
works, we focus on the problem of heterogeneous
transfer learning, which is designed for situation
when the training data are in one feature space
(such as text), and the test data are in another (such
as images), and there may be no correspondence
between instances in these spaces. The type of
heterogeneous data can be very different, as in the
case of text and image. To consider how hetero-
geneous transfer learning relates to other types of
learning, Figure 1 presents an intuitive illustration
of four learning strategies, including traditional
machine learning, transfer learning across differ-
ent distributions, multi-view learning and hetero-
geneous transfer learning. As we can see, an
important distinguishing feature of heterogeneous
transfer learning, as compared to other types of
learning, is that more constraints on the problem
are relaxed, such that data instances do not need to
correspond anymore. This allows, for example, a
collection of Chinese text documents to be classi-
fied using another collection of English text as the
1
training data (c.f. (Ling et al, 2008) and Section
2.1).
In this paper, we will give an illustrative exam-
ple of heterogeneous transfer learning to demon-
strate how the task of image clustering can ben-
efit from learning from the heterogeneous social
Web data. A major motivation of our work is
Web-based image search, where users submit tex-
tual queries and browse through the returned result
pages. One problem is that the user queries are of-
ten ambiguous. An ambiguous keyword such as
?Apple? might retrieve images of Apple comput-
ers and mobile phones, or images of fruits. Im-
age clustering is an effective method for improv-
ing the accessibility of image search result. Loeff
et al (2006) addressed the image clustering prob-
lem with a focus on image sense discrimination.
In their approach, images associated with textual
features are used for clustering, so that the text
and images are clustered at the same time. Specif-
ically, spectral clustering is applied to the distance
matrix built from a multimodal feature set associ-
ated with the images to get a better feature repre-
sentation. This new representation contains both
image and text information, with which the per-
formance of image clustering is shown to be im-
proved. A problem with this approach is that when
images contained in the Web search results are
very scarce and when the textual data associated
with the images are very few, clustering on the im-
ages and their associated text may not be very ef-
fective.
Different from these previous works, in this pa-
per, we address the image clustering problem as
a heterogeneous transfer learning problem. We
aim to leverage heterogeneous auxiliary data, so-
cial annotations, etc. to enhance image cluster-
ing performance. We observe that the World Wide
Web has many annotated images in Web sites such
as Flickr (http://www.flickr.com), which
can be used as auxiliary information source for
our clustering task. In this work, our objective
is to cluster a small collection of images that we
are interested in, where these images are not suf-
ficient for traditional clustering algorithms to per-
form well due to data sparsity and the low level of
image features. We investigate how to utilize the
readily available socially annotated image data on
the Web to improve image clustering. Although
these auxiliary data may be irrelevant to the im-
ages to be clustered and cannot be directly used
to solve the data sparsity problem, we show that
they can still be used to estimate a good latent fea-
ture representation, which can be used to improve
image clustering.
2 Related Works
2.1 Heterogeneous Transfer Learning
Between Languages
In this section, we summarize our previous work
on cross-language classification as an example of
heterogeneous transfer learning. This example
is related to our image clustering problem be-
cause they both rely on data from different feature
spaces.
As the World Wide Web in China grows rapidly,
it has become an increasingly important prob-
lem to be able to accurately classify Chinese Web
pages. However, because the labeled Chinese Web
pages are still not sufficient, we often find it diffi-
cult to achieve high accuracy by applying tradi-
tional machine learning algorithms to the Chinese
Web pages directly. Would it be possible to make
the best use of the relatively abundant labeled En-
glish Web pages for classifying the Chinese Web
pages?
To answer this question, in (Ling et al, 2008),
we developed a novel approach for classifying the
Web pages in Chinese using the training docu-
ments in English. In this subsection, we give a
brief summary of this work. The problem to be
solved is: we are given a collection of labeled
English documents and a large number of unla-
beled Chinese documents. The English and Chi-
nese texts are not aligned. Our objective is to clas-
sify the Chinese documents into the same label
space as the English data.
Our key observation is that even though the data
use different text features, they may still share
many of the same semantic information. What we
need to do is to uncover this latent semantic in-
formation by finding out what is common among
them. We did this in (Ling et al, 2008) by us-
ing the information bottleneck theory (Tishby et
al., 1999). In our work, we first translated the
Chinese document into English automatically us-
ing some available translation software, such as
Google translate. Then, we encoded the training
text as well as the translated target text together,
in terms of the information theory. We allowed all
the information to be put through a ?bottleneck?
and be represented by a limited number of code-
2
Figure 1: An intuitive illustration of different kinds learning strategies using classification/clustering of
image apple and banana as the example.
words (i.e. labels in the classification problem).
Finally, information bottleneck was used to main-
tain most of the common information between the
two data sources, and discard the remaining irrel-
evant information. In this way, we can approxi-
mate the ideal situation where similar training and
translated test pages shared in the common part are
encoded into the same codewords, and are thus as-
signed the correct labels. In (Ling et al, 2008), we
experimentally showed that heterogeneous trans-
fer learning can indeed improve the performance
of cross-language text classification as compared
to directly training learning models (e.g., Naive
Bayes or SVM) and testing on the translated texts.
2.2 Other Works in Transfer Learning
In the past, several other works made use of trans-
fer learning for cross-feature-space learning. Wu
and Oard (2008) proposed to handle the cross-
language learning problem by translating the data
into a same language and applying kNN on the
latent topic space for classification. Most learning
algorithms for dealing with cross-language hetero-
geneous data require a translator to convert the
data to the same feature space. For those data that
are in different feature spaces where no transla-
tor is available, Davis and Domingos (2008) pro-
posed a Markov-logic-based transfer learning al-
gorithm, which is called deep transfer, for trans-
ferring knowledge between biological domains
and Web domains. Dai et al (2008a) proposed
a novel learning paradigm, known as translated
learning, to deal with the problem of learning het-
erogeneous data that belong to quite different fea-
ture spaces by using a risk minimization frame-
work.
2.3 Relation to PLSA
Our work makes use of PLSA. Probabilistic la-
tent semantic analysis (PLSA) is a widely used
probabilistic model (Hofmann, 1999), and could
be considered as a probabilistic implementation of
latent semantic analysis (LSA) (Deerwester et al,
1990). An extension to PLSA was proposed in
(Cohn and Hofmann, 2000), which incorporated
the hyperlink connectivity in the PLSA model by
using a joint probabilistic model for connectivity
and content. Moreover, PLSA has shown a lot
of applications ranging from text clustering (Hof-
mann, 2001) to image analysis (Sivic et al, 2005).
2.4 Relation to Clustering
Compared to many previous works on image clus-
tering, we note that traditional image cluster-
ing is generally based on techniques such as K-
means (MacQueen, 1967) and hierarchical clus-
tering (Kaufman and Rousseeuw, 1990). How-
ever, when the data are sparse, traditional clus-
tering algorithms may have difficulties in obtain-
ing high-quality image clusters. Recently, several
researchers have investigated how to leverage the
auxiliary information to improve target clustering
3
performance, such as supervised clustering (Fin-
ley and Joachims, 2005), semi-supervised cluster-
ing (Basu et al, 2004), self-taught clustering (Dai
et al, 2008b), etc.
3 Image Clustering with Annotated
Auxiliary Data
In this section, we present our annotation-based
probabilistic latent semantic analysis algorithm
(aPLSA), which extends the traditional PLSA
model by incorporating annotated auxiliary im-
age data. Intuitively, our algorithm aPLSA per-
forms PLSA analysis on the target images, which
are converted to an image instance-to-feature co-
occurrence matrix. At the same time, PLSA is
also applied to the annotated image data from so-
cial Web, which is converted into a text-to-image-
feature co-occurrence matrix. In order to unify
those two separate PLSA models, these two steps
are done simultaneously with common latent vari-
ables used as a bridge linking them. Through
these common latent variables, which are now
constrained by both target image data and auxil-
iary annotation data, a better clustering result is
expected for the target data.
3.1 Probabilistic Latent Semantic Analysis
Let F = {fi}|F|i=1 be an image feature space, and
V = {vi}|V|i=1 be the image data set. Each image
vi ? V is represented by a bag-of-features {f |f ?
vi ? f ? F}.
Based on the image data set V , we can esti-
mate an image instance-to-feature co-occurrence
matrix A|V|?|F| ? R|V|?|F|, where each element
Aij (1 ? i ? |V| and 1 ? j ? |F|) in the matrix
A is the frequency of the feature fj appearing in
the instance vi.
LetW = {wi}|W|i=1 be a text feature space. The
annotated image data allow us to obtain the co-
occurrence information between images v and text
features w ? W . An example of annotated im-
age data is the Flickr (http://www.flickr.
com), which is a social Web site containing a large
number of annotated images.
By extracting image features from the annotated
images v, we can estimate a text-to-image fea-
ture co-occurrence matrix B|W|?|F| ? R|W|?|F|,
where each element Bij (1 ? i ? |W| and
1 ? j ? |F|) in the matrix B is the frequency
of the text feature wi and the image feature fj oc-
curring together in the annotated image data set.
V Z F
P (z|v) P (f |z)
Figure 2: Graphical model representation of PLSA
model.
LetZ = {zi}|Z|i=1 be the latent variable set in our
aPLSA model. In clustering, each latent variable
zi ? Z corresponds to a certain cluster.
Our objective is to estimate a clustering func-
tion g : V 7? Z with the help of the two co-
occurrence matrices A and B as defined above.
To formally introduce the aPLSA model, we
start from the probabilistic latent semantic anal-
ysis (PLSA) (Hofmann, 1999) model. PLSA is
a probabilistic implementation of latent seman-
tic analysis (LSA) (Deerwester et al, 1990). In
our image clustering task, PLSA decomposes the
instance-feature co-occurrence matrix A under the
assumption of conditional independence of image
instances V and image features F , given the latent
variables Z .
P (f |v) =
?
z?Z
P (f |z)P (z|v). (1)
The graphical model representation of PLSA is
shown in Figure 2.
Based on the PLSA model, the log-likelihood can
be defined as:
L =
?
i
?
j
Aij
?
j? Aij?
logP (fj |vi) (2)
where A|V|?|F| ? R|V|?|F| is the image instance-
feature co-occurrence matrix. The term AijP
j? Aij?
in Equation (2) is a normalization term ensuring
each image is giving the same weight in the log-
likelihood.
Using EM algorithm (Dempster et al, 1977),
which locally maximizes the log-likelihood of
the PLSA model (Equation (2)), the probabilities
P (f |z) and P (z|v) can be estimated. Then, the
clustering function is derived as
g(v) = argmax
z?Z
P (z|v). (3)
Due to space limitation, we omit the details for the
PLSA model, which can be found in (Hofmann,
1999).
3.2 aPLSA: Annotation-based PLSA
In this section, we consider how to incorporate
a large number of socially annotated images in a
4
VW
Z F
P (z
|v)
P (z|w)
P (f |z)
Figure 3: Graphical model representation of
aPLSA model.
unified PLSA model for the purpose of utilizing
the correlation between text features and image
features. In the auxiliary data, each image has cer-
tain textual tags that are attached by users. The
correlation between text features and image fea-
tures can be formulated as follows.
P (f |w) =
?
z?Z
P (f |z)P (z|w). (4)
It is clear that Equations (1) and (4) share a same
term P (f |z). So we design a new PLSA model by
joining the probabilistic model in Equation (1) and
the probabilistic model in Equation (4) into a uni-
fied model, as shown in Figure 3. In Figure 3, the
latent variables Z depend not only on the corre-
lation between image instances V and image fea-
tures F , but also the correlation between text fea-
turesW and image featuresF . Therefore, the aux-
iliary socially-annotated image data can be used
to help the target image clustering performance by
estimating good set of latent variables Z .
Based on the graphical model representation in
Figure 3, we derive the log-likelihood objective
function, in a similar way as in (Cohn and Hof-
mann, 2000), as follows
L =
?
j
[
?
?
i
Aij
?
j? Aij?
logP (fj |vi)
+(1? ?)
?
l
Blj
?
j? Blj?
logP (fj |wl)
]
,
(5)
where A|V|?|F| ? R|V|?|F| is the image instance-
feature co-occurrence matrix, and B|W|?|F| ?
R|W|?|F| is the text-to-image feature-level co-
occurrence matrix. Similar to Equation (2),
Aij
P
j? Aij?
and BljP
j? Blj?
in Equation (5) are the nor-
malization terms to prevent imbalanced cases.
Furthermore, ? acts as a trade-off parameter be-
tween the co-occurrence matrices A and B. In
the extreme case when ? = 1, the log-likelihood
objective function ignores all the biases from the
text-to-image occurrence matrix B. In this case,
the aPLSA model degenerates to the traditional
PLSA model. Therefore, aPLSA is an extension
to the PLSA model.
Now, the objective is to maximize the log-
likelihood L of the aPLSA model in Equation (5).
Then we apply the EM algorithm (Dempster et
al., 1977) to estimate the conditional probabilities
P (f |z), P (z|w) and P (z|v) with respect to each
dependence in Figure 3 as follows.
? E-Step: calculate the posterior probability of
each latent variable z given the observation
of image features f , image instances v and
text features w based on the old estimate of
P (f |z), P (z|w) and P (z|v):
P (zk|vi, fj) =
P (fj |zk)P (zk|vi)
?
k? P (fj |zk?)P (zk? |vi)
(6)
P (zk|wl, fj) =
P (fj |zk)P (zk|wl)
?
k? P (fj |zk?)P (zk? |wl)
(7)
? M-Step: re-estimates conditional probabili-
ties P (zk|vi) and P (zk|wl):
P (zk|vi) =
?
j
Aij
?
j? Aij?
P (zk|vi, fj) (8)
P (zk|wl) =
?
j
Blj
?
j? Blj?
P (zk|wl, fj) (9)
and conditional probability P (fj |zk), which
is a mixture portion of posterior probability
of latent variables
P (fj |zk) ? ?
?
i
Aij
?
j? Aij?
P (zk|vi, fj)
+ (1? ?)
?
l
Blj
?
j? Blj?
P (zk|wl, fj)
(10)
Finally, the clustering function for a certain im-
age v is
g(v) = argmax
z?Z
P (z|v). (11)
From the above equations, we can derive
our annotation-based probabilistic latent semantic
analysis (aPLSA) algorithm. As shown in Algo-
rithm 1, aPLSA iteratively performs the E-Step
and the M-Step in order to seek local optimal
points based on the objective function L in Equa-
tion (5).
5
Algorithm 1 Annotation-based PLSA Algorithm
(aPLSA)
Input: The V-F co-occurrence matrix A andW-
F co-occurrence matrix B.
Output: A clustering (partition) function g : V 7?
Z , which maps an image instance v ? V to a latent
variable z ? Z .
1: Initial Z so that |Z| equals the number clus-
ters desired.
2: Initialize P (z|v), P (z|w), P (f |z) randomly.
3: while the change of L in Eq. (5) between two
sequential iterations is greater than a prede-
fined threshold do
4: E-Step: Update P (z|v, f) and P (z|w, f)
based on Eq. (6) and (7) respectively.
5: M-Step: Update P (z|v), P (z|w) and
P (f |z) based on Eq. (8), (9) and (10) re-
spectively.
6: end while
7: for all v in V do
8: g(v)? argmax
z
P (z|v).
9: end for
10: Return g.
4 Experiments
In this section, we empirically evaluate the aPLSA
algorithm together with some state-of-art base-
line methods on two widely used image corpora,
to demonstrate the effectiveness of our algorithm
aPLSA.
4.1 Data Sets
In order to evaluate the effectiveness of our algo-
rithm aPLSA, we conducted experiments on sev-
eral data sets generated from two image corpora,
Caltech-256 (Griffin et al, 2007) and the fifteen-
scene (Lazebnik et al, 2006). The Caltech-256
data set has 256 image objective categories, rang-
ing from animals to buildings, from plants to au-
tomobiles, etc. The fifteen-scene data set con-
tains 15 scenes such as store and forest.
From these two corpora, we randomly generated
eleven image clustering tasks, including seven 2-
way clustering tasks, two 4-way clustering task,
one 5-way clustering task and one 8-way cluster-
ing task. The detailed descriptions for these clus-
tering tasks are given in Table 1. In these tasks,
bi7 and oct1 were generated from fifteen-scene
data set, and the rest were from Caltech-256 data
set.
DATA SET INVOLVED CLASSES DATA SIZE
bi1 skateboard, airplanes 102, 800
bi2 billiards, mars 278, 155
bi3 cd, greyhound 102, 94
bi4 electric-guitar, snake 122, 112
bi5 calculator, dolphin 100, 106
bi6 mushroom, teddy-bear 202, 99
bi7 MIThighway, livingroom 260, 289
quad1 calculator, diamond-ring, dolphin,
microscope 100, 118, 106, 116
quad2 bonsai, comet, frog, saddle 122, 120, 115, 110
quint1 frog, kayak, bear, jesus-christ, watch 115, 102, 101, 87,201
oct1
MIThighway, MITmountain,
kitchen, MITcoast, PARoffice, MIT-
tallbuilding, livingroom, bedroom
260, 374, 210, 360,
215, 356, 289, 216
tune1 coin, horse 123, 270
tune2 socks, spider 111, 106
tune3 galaxy, snowmobile 80, 112
tune4 dice, fern 98, 110
tune5 backpack, lightning, mandolin, swan 151, 136, 93, 114
Table 1: The descriptions of all the image clus-
tering tasks used in our experiment. Among
these data sets, bi7 and oct1 were generated
from fifteen-scene data set, and the rest were from
Caltech-256 data set.
To empirically investigate the parameter ? and
the convergence of our algorithm aPLSA, we gen-
erated five more date sets as the development sets.
The detailed description of these five development
sets, namely tune1 to tune5 is listed in Table 1
as well.
The auxiliary data were crawled from the Flickr
(http://www.flickr.com/) web site dur-
ing August 2007. Flickr is an internet community
where people share photos online and express their
opinions as social tags (annotations) attached to
each image. From Flicker, we collected 19, 959
images and 91, 719 related annotations, among
which 2, 600 words are distinct. Based on the
method described in Section 3, we estimated the
co-occurrence matrix B between text features and
image features. This co-occurrence matrix B was
used by all the clustering tasks in our experiments.
For data preprocessing, we adopted the bag-of-
features representation of images (Li and Perona,
2005) in our experiments. Interesting points were
found in the images and described via the SIFT
descriptors (Lowe, 2004). Then, the interesting
points were clustered to generate a codebook to
form an image feature space. The size of code-
book was set to 2, 000 in our experiments. Based
on the codebook, which serves as the image fea-
ture space, each image can be represented as a cor-
responding feature vector to be used in the next
step.
To set our evaluation criterion, we used the
6
Data Set KMeans PLSA STC aPLSA
separate combined separate combined
bi1 0.645?0.064 0.548?0.031 0.544?0.074 0.537?0.033 0.586?0.139 0.482?0.062
bi2 0.687?0.003 0.662?0.014 0.464?0.074 0.692?0.001 0.577?0.016 0.455?0.096
bi3 1.294?0.060 1.300?0.015 1.085?0.073 1.126?0.036 1.103?0.108 1.029?0.074
bi4 1.227?0.080 1.164?0.053 0.976?0.051 1.038?0.068 1.024?0.089 0.919?0.065
bi5 1.450?0.058 1.417?0.045 1.426?0.025 1.405?0.040 1.411?0.043 1.377?0.040
bi6 1.969?0.078 1.852?0.051 1.514?0.039 1.709?0.028 1.589?0.121 1.503?0.030
bi7 0.686?0.006 0.683?0.004 0.643?0.058 0.632?0.037 0.651?0.012 0.624?0.066
quad1 0.591?0.094 0.675?0.017 0.488?0.071 0.662?0.013 0.580?0.115 0.432?0.085
quad2 0.648?0.036 0.646?0.045 0.614?0.062 0.626?0.026 0.591?0.087 0.515?0.098
quint1 0.557?0.021 0.508?0.104 0.547?0.060 0.539?0.051 0.538?0.100 0.502?0.067
oct1 0.659?0.031 0.680?0.012 0.340?0.147 0.691?0.002 0.411?0.089 0.306?0.101
average 0.947?0.029 0.922?0.017 0.786?0.009 0.878?0.006 0.824?0.036 0.741?0.018
Table 2: Experimental result in term of entropy for all data sets and evaluation methods.
entropy to measure the quality of our clustering
results. In information theory, entropy (Shan-
non, 1948) is a measure of the uncertainty as-
sociated with a random variable. In our prob-
lem, entropy serves as a measure of randomness
of clustering result. The entropy of g on a sin-
gle latent variable z is defined to be H(g, z) ,
??c?C P (c|z) log2 P (c|z), where C is the class
label set of V and P (c|z) = |{v|g(v)=z?t(v)=c}||{v|g(v)=z}| ,
in which t(v) is the true class label of image v.
Lower entropy H(g,Z) indicates less randomness
and thus better clustering result.
4.2 Empirical Analysis
We now empirically analyze the effectiveness of
our aPLSA algorithm. Because, to our best of
knowledge, few existing methods addressed the
problem of image clustering with the help of so-
cial annotation image data, we can only compare
our aPLSA with several state-of-the-art cluster-
ing algorithms that are not directly designed for
our problem. The first baseline is the well-known
KMeans algorithm (MacQueen, 1967). Since our
algorithm is designed based on PLSA (Hofmann,
1999), we also included PLSA for clustering as a
baseline method in our experiments.
For each of the above two baselines, we have
two strategies: (1) separated: the baseline
method was applied on the target image data only;
(2) combined: the baseline method was applied
to cluster the combined data consisting of both
target image data and the annotated image data.
Clustering results on target image data were used
for evaluation. Note that, in the combined data, all
the annotations were thrown away since baseline
methods evaluated in this paper do not leverage
annotation information.
In addition, we compared our algorithm aPLSA
to a state-of-the-art transfer clustering strategy,
known as self-taught clustering (STC) (Dai et al,
2008b). STC makes use of auxiliary data to esti-
mate a better feature representation to benefit the
target clustering. In these experiments, the anno-
tated image data were used as auxiliary data in
STC, which does not use the annotation text.
In our experiments, the performance is in the
form of the average entropy and variance of five
repeats by randomly selecting 50 images from
each of the categories. We selected only 50 im-
ages per category, since this paper is focused on
clustering sparse data. Table 2 shows the perfor-
mance with respect to all comparison methods on
each of the image clustering tasks measured by
the entropy criterion. From the tables, we can see
that our algorithm aPLSA outperforms the base-
line methods in all the data sets. We believe that is
because aPLSA can effectively utilize the knowl-
edge from the socially annotated image data. On
average, aPLSA gives rise to 21.8% of entropy re-
duction and as compared to KMeans, 5.7% of en-
tropy reduction as compared to PLSA, and 10.1%
of entropy reduction as compared to STC.
4.2.1 Varying Data Size
We now show how the data size affects aPLSA,
with two baseline methods KMeans and PLSA as
reference. The experiments were conducted on
different amounts of target image data, varying
from 10 to 80. The corresponding experimental
results in average entropy over all the 11 clustering
tasks are shown in Figure 4(a). From this figure,
we observe that aPLSA always yields a significant
reduction in entropy as compared with two base-
line methods KMeans and PLSA, regardless of the
size of target image data that we used.
7
10 20 30 40 50 60 70 80
0.7
0.75
0.8
0.85
0.9
0.95
1
Data size per category
En
tro
py
 
 
KMeans
PLSA
aPLSA
(a)
0 0.2 0.4 0.6 0.8 1
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
?
En
tro
py
 
 
average over 5 development sets
(b)
0 50 100 150 200 250 300
0.5
0.55
0.6
0.65
0.7
0.75
Number of Iteration
En
tro
py
 
 
average over 5 development sets
(c)
Figure 4: (a) The entropy curve as a function of different amounts of data per category. (b) The entropy
curve as a function of different number of iterations. (c) The entropy curve as a function of different
trade-off parameter ?.
4.2.2 Parameter Sensitivity
In aPLSA, there is a trade-off parameter ? that af-
fects how the algorithm relies on auxiliary data.
When ? = 0, the aPLSA relies only on annotated
image data B. When ? = 1, aPLSA relies only
on target image data A, in which case aPLSA de-
generates to PLSA. Smaller ? indicates heavier re-
liance on the annotated image data. We have done
some experiments on the development sets to in-
vestigate how different ? affect the performance
of aPLSA. We set the number of images per cate-
gory to 50, and tested the performance of aPLSA.
The result in average entropy over all development
sets is shown in Figure 4(b). In the experiments
described in this paper, we set ? to 0.2, which is
the best point in Figure 4(b).
4.2.3 Convergence
In our experiments, we tested the convergence
property of our algorithm aPLSA as well. Fig-
ure 4(c) shows the average entropy curve given
by aPLSA over all development sets. From this
figure, we see that the entropy decreases very fast
during the first 100 iterations and becomes stable
after 150 iterations. We believe that 200 iterations
is sufficient for aPLSA to converge.
5 Conclusions
In this paper, we proposed a new learning scenario
called heterogeneous transfer learning and illus-
trated its application to image clustering. Image
clustering, a vital component in organizing search
results for query-based image search, was shown
to be improved by transferring knowledge from
unrelated images with annotations in a social Web.
This is done by first learning the high-quality la-
tent variables in the auxiliary data, and then trans-
ferring this knowledge to help improve the cluster-
ing of the target image data. We conducted experi-
ments on two image data sets, using the Flickr data
as the annotated auxiliary image data, and showed
that our aPLSA algorithm can greatly outperform
several state-of-the-art clustering algorithms.
In natural language processing, there are many
future opportunities to apply heterogeneous trans-
fer learning. In (Ling et al, 2008) we have shown
how to classify the Chinese text using English text
as the training data. We may also consider cluster-
ing, topic modeling, question answering, etc., to
be done using data in different feature spaces. We
can consider data in different modalities, such as
video, image and audio, as the training data. Fi-
nally, we will explore the theoretical foundations
and limitations of heterogeneous transfer learning
as well.
Acknowledgement Qiang Yang thanks Hong
Kong CERG grant 621307 for supporting the re-
search.
References
Alina Andreevskaia and Sabine Bergler. 2008. When spe-
cialists and generalists work together: Overcoming do-
main dependence in sentiment tagging. In ACL-08: HLT,
pages 290?298, Columbus, Ohio, June.
Andrew Arnold, Ramesh Nallapati, and William W. Cohen.
2007. A comparative study of methods for transductive
transfer learning. In ICDM 2007 Workshop on Mining
and Management of Biological Data, pages 77-82.
Andrew Arnold, Ramesh Nallapati, and William W. Cohen.
2008. Exploiting feature hierarchy for transfer learning in
named entity recognition. In ACL-08: HLT.
Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.
2004. A probabilistic framework for semi-supervised
clustering. In ACM SIGKDD 2004, pages 59?68.
John Blitzer, Ryan Mcdonald, and Fernando Pereira. 2006.
Domain adaptation with structural correspondence learn-
ing. In EMNLP 2006, pages 120?128, Sydney, Australia.
8
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In ACL 2007,
pages 440?447, Prague, Czech Republic.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT 1998, pages
92?100, New York, NY, USA. ACM.
Rich Caruana. 1997. Multitask learning. Machine Learning,
28(1):41?75.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation
with active learning for word sense disambiguation. In
ACL 2007, Prague, Czech Republic.
David A. Cohn and Thomas Hofmann. 2000. The missing
link - a probabilistic model of document content and hy-
pertext connectivity. In NIPS 2000, pages 430?436.
Wenyuan Dai, Yuqiang Chen, Gui-Rong Xue, Qiang Yang,
and Yong Yu. 2008a. Translated learning: Transfer learn-
ing across different feature spaces. In NIPS 2008, pages
353?360.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu.
2008b. Self-taught clustering. In ICML 2008, pages 200?
207. Omnipress.
Hal Daume, III. 2007. Frustratingly easy domain adaptation.
In ACL 2007, pages 256?263, Prague, Czech Republic.
Jesse Davis and Pedro Domingos. 2008. Deep transfer via
second-order markov logic. In AAAI 2008 Workshop on
Transfer Learning, Chicago, USA.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. L, and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American Society
for Information Science, pages 391?407.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em algo-
rithm. J. of the Royal Statistical Society, 39:1?38.
Thomas Finley and Thorsten Joachims. 2005. Supervised
clustering with support vector machines. In ICML 2005,
pages 217?224, New York, NY, USA. ACM.
G. Griffin, A. Holub, and P. Perona. 2007. Caltech-256 ob-
ject category dataset. Technical Report 7694, California
Institute of Technology.
Thomas Hofmann. 1999 Probabilistic latent semantic anal-
ysis. In Proc. of Uncertainty in Artificial Intelligence,
UAI99. Pages 289?296
Thomas Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning. vol-
ume 42, number 1-2, pages 177?196. Kluwer Academic
Publishers.
Jing Jiang and Chengxiang Zhai. 2007. Instance weighting
for domain adaptation in NLP. In ACL 2007, pages 264?
271, Prague, Czech Republic, June.
Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding
groups in data: an introduction to cluster analysis. John
Wiley and Sons, New York.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006.
Beyond bags of features: Spatial pyramid matching for
recognizing natural scene categories. In CVPR 2006,
pages 2169?2178, Washington, DC, USA.
Fei-Fei Li and Pietro Perona. 2005. A bayesian hierarchi-
cal model for learning natural scene categories. In CVPR
2005, pages 524?531, Washington, DC, USA.
Xiao Ling, Gui-Rong Xue, Wenyuan Dai, Yun Jiang, Qiang
Yang, and Yong Yu. 2008. Can chinese web pages be
classified with english data source? In WWW 2008, pages
969?978, New York, NY, USA. ACM.
Nicolas Loeff, Cecilia Ovesdotter Alm, and David A.
Forsyth. 2006. Discriminating image senses by clustering
with multimodal features. In COLING/ACL 2006 Main
conference poster sessions, pages 547?554.
David G. Lowe. 2004. Distinctive image features from scale-
invariant keypoints. International Journal of Computer
Vision (IJCV) 2004, volume 60, number 2, pages 91?110.
J. B. MacQueen. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings of
Fifth Berkeley Symposium on Mathematical Statistics and
Probability, pages 1:281?297, Berkeley, CA, USA.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceedings
of the Ninth International Conference on Information and
Knowledge Management, pages 86?93, New York, USA.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer,
and Andrew Y. Ng. 2007. Self-taught learning: transfer
learning from unlabeled data. In ICML 2007, pages 759?
766, New York, NY, USA. ACM.
Roi Reichart and Ari Rappoport. 2007. Self-training for
enhancement and domain adaptation of statistical parsers
trained on small datasets. In ACL 2007.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguistic
annotations. In ACL-08: HLT, pages 861?869.
C. E. Shannon. 1948. A mathematical theory of communi-
cation. Bell system technical journal, 27.
J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T.
Freeman. 2005. Discovering object categories in image
collections. In ICCV 2005.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The
information bottleneck method. 1999. In Proc. of the 37-
th Annual Allerton Conference on Communication, Con-
trol and Computing, pages 368?377.
Pengcheng Wu and Thomas G. Dietterich. 2004. Improving
svm accuracy by training on auxiliary data sources. In
ICML 2004, pages 110?117, New York, NY, USA.
Yejun Wu and Douglas W. Oard. 2008. Bilingual topic as-
pect classification with a few training examples. In ACM
SIGIR 2008, pages 203?210, New York, NY, USA.
Xiaojin Zhu. 2007. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Univer-
sity of Wisconsin-Madison.
9
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 410?419,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-Domain Co-Extraction of Sentiment and Topic Lexicons
Fangtao Li?, Sinno Jialin Pan?, Ou Jin?, Qiang Yang? and Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China
?{fangtao06@gmail.com, zxy-dcs@tsinghua.edu.cn}
?Institute for Infocomm Research, Singapore
?jspan@i2r.a-star.edu.sg
?Hong Kong University of Science and Technology, Hong Kong, China
?{kingomiga@gmail.com, qyang@cse.ust.hk}
Abstract
Extracting sentiment and topic lexicons is im-
portant for opinion mining. Previous works
have showed that supervised learning methods
are superior for this task. However, the perfor-
mance of supervised methods highly relies on
manually labeled training data. In this paper,
we propose a domain adaptation framework
for sentiment- and topic- lexicon co-extraction
in a domain of interest where we do not re-
quire any labeled data, but have lots of labeled
data in another related domain. The frame-
work is twofold. In the first step, we gener-
ate a few high-confidence sentiment and topic
seeds in the target domain. In the second
step, we propose a novel Relational Adaptive
bootstraPping (RAP) algorithm to expand the
seeds in the target domain by exploiting the
labeled source domain data and the relation-
ships between topic and sentiment words. Ex-
perimental results show that our domain adap-
tation framework can extract precise lexicons
in the target domain without any annotation.
1 Introduction
In the past few years, opinion mining and senti-
ment analysis have attracted much attention in Natu-
ral Language Processing (NLP) and Information Re-
trieval (IR) (Pang and Lee, 2008; Liu, 2010). Senti-
ment lexicon construction and topic lexicon extrac-
tion are two fundamental subtasks for opinion min-
ing (Qiu et al, 2009). A sentiment lexicon is a list
of sentiment expressions, which are used to indicate
sentiment polarity (e.g., positive or negative). The
sentiment lexicon is domain dependent as users may
use different sentiment words to express their opin-
ion in different domains (e.g., different products). A
topic lexicon is a list of topic expressions, on which
the sentiment words are expressed. Extracting the
topic lexicon from a specific domain is important
because users not only care about the overall senti-
ment polarity of a review but also care about which
aspects are mentioned in review. Note that, similar
to sentiment lexicons, different domains may have
very different topic lexicons.
Recently, Jin and Ho (2009) and Li et al (2010a)
showed that supervised learning methods can
achieve state-of-the-art results for lexicon extrac-
tion. However, the performance of these meth-
ods highly relies on manually annotated training
data. In most cases, the labeling work may be time-
consuming and expensive. It is impossible to anno-
tate each domain of interest to build precise domain-
dependent lexicons. It is more desirable to automat-
ically construct precise lexicons in domains of inter-
est by transferring knowledge from other domains.
In this paper, we focus on the co-extraction task
of sentiment and topic lexicons in a target domain
where we do not have any labeled data, but have
plenty of labeled data in a source domain. Our
goal is to leverage the knowledge extracted from the
source domain to help lexicon co-extraction in the
target domain. To address this problem, we propose
a two-stage domain adaptation method. In the first
step, we build a bridge between the source and tar-
get domains by identifying some common sentiment
words as sentiment seeds in the target domain, such
as ?good?, ?bad?, ?nice?, etc. After that, we gener-
ate topic seeds in the target domain by mining some
general syntactic relation patterns between the sen-
timent and topic words from the source domain. In
the second step, we propose a Relational Adaptive
bootstraPping (RAP) algorithm to expand the seeds
in the target domain. Our proposed method can uti-
410
lize useful labeled data from the source domain as
well as exploit the relationships between the topic
and sentiment words to propagate information for
lexicon construction in the target domain. Experi-
mental results show that our proposed method is ef-
fective for cross-domain lexicon co-extraction.
In summary, we have three main contributions: 1)
We give a systematic study on cross-domain senti-
ment analysis in word level. While, most of previous
work focused on document level; 2) A new two-step
domain adaptation framework, with a novel RAP al-
gorithm for seed expansion, is proposed. 3) We con-
duct extensive evaluation, and the experimental re-
sults demonstrate the effectiveness of our methods.
2 Related Work
2.1 Sentiment or Topic Lexicon Extraction
Sentiment or topic lexicon extraction is to iden-
tify the sentiment or topic words from text. In the
past, many machine learning techniques have been
proposed for this task. Hu and Liu et al (2004)
proposed an association-rule-based method to ex-
tract topic words and a dictionary-based method to
identify sentiment words, independently. Wiebe et
al. (2004) and Rioff et al (2003) proposed to
identify subjective adjectives and nouns using word
clustering based on their distributional similarity.
Popescu and Etzioni (2005) proposed a relaxed la-
beling approach to utilize linguistic rules for opinion
polarity detection. Some researchers also proposed
to use topic modeling to identify implicit topics and
sentiment words (Mei et al, 2007; Titov and Mc-
Donald, 2008; Zhao et al, 2010; Li et al, 2010b),
where a topic is a cluster of words, which is differ-
ent from our fine-grained topic-word extraction.
Jin and Ho (2009) and Li et al (2010a) both pro-
posed to use supervised sequential labeling methods
for topic and opinion extraction. Experimental re-
sults showed that the supervised learning methods
can achieve state-of-the-art performance on lexicon
extraction. However, these methods need to manu-
ally annotate a lot of training data in each domain.
Recently, Qiu et al (2009) proposed a rule-based
semi-supervised learning methods for lexicon ex-
traction. However, their method requires to manu-
ally define some general syntactic rules among sen-
timent and topic words. In addition, it still requires
some annotated words in the target domain. In this
paper, we do not assume any predefined rules and
labeled data be available in the target domain.
2.2 Domain Adaptation
Domain adaptation aims at transferring knowledge
across domains where data distributions may be dif-
ferent (Pan and Yang, 2010). In the past few years,
domain adaptation techniques have been widely ap-
plied to various NLP tasks, such as part-of-speech
tagging (Ando and Zhang, 2005; Jiang and Zhai,
2007; Daume? III, 2007), named-entity recognition
and shallow parsing (Daume? III, 2007; Jiang and
Zhai, 2007; Wu et al, 2009). There are also
lots of studies for cross-domain sentiment analy-
sis (Blitzer et al, 2007; Tan et al, 2007; Li et al,
2009; Pan et al, 2010; Bollegala et al, 2011; He
et al, 2011; Glorot et al, 2011). However, most
of them focused on coarse-grained document-level
sentiment classification, which is different from our
fine-grained word-level extraction. Our work is sim-
ilar to Jakob and Gurevych (2010) which proposed a
Conditional Random Field (CRF) for cross-domain
topic word extraction. However, the performance
of their method highly depends on the manually de-
signed features. In our experiments, we compare our
method with theirs, and find that ours can achieve
much better results on cross-domain lexicon extrac-
tion. Note that our work is also different from a re-
cent work (Du et al, 2010), which focused on identi-
fying the polarity of adjective words by using cross-
domain knowledge. While we extract both topic and
sentiment words and allow non-adjective sentiment
words, which is more practical.
3 Cross-Domain Lexicon Co-Extraction
3.1 Problem Definition
Recall that, we focus on the setting where we have
no labeled data in the target domain, while we have
plenty of labeled data in the source domain. De-
note DS = {(wSi , ySi)}n1i=1 the source domain data,
where wSi represents a word in the source domain.
ySi ? Y is the corresponding label of wSi . Simi-
larly, we denote DT = {wTj}n2j=1 the target domain
data, where the input wTj is a word in the target do-
main. In lexicon extraction, Y ? {1, 2, 3}, where
yi = 1 denotes the corresponding word wi a sen-
timent word, yi = 2 denotes wi a topic word, and
yi = 3 denotes wi neither a sentiment nor topic
word. Our goal is to predict labels on DT to extract
topic and sentiment words for constructing topic and
411
sentiment lexicons, respectively.
3.2 Motivating Examples
In this section, we use some examples to introduce
the motivation behind our proposed method. Table 1
shows several reviews from two domains: movie and
camera. From the table, we can observe that there
are some common sentiment words across different
domains, such as ?great?, ?excellent? and ?amaz-
ing?. However, the topic words may be different.
For example, in the movie domain, topic words in-
clude ?movie? and ?script?. While in the camera do-
main, topic words include ?camera? and ?photos?.
Domain Review
camera
The camera is great.
it is a very amazing product.
i highly recommend this camera.
takes excellent photos.
photos had some artifacts and noise.
movie
This movie has good script, great
casting, excellent acting.
I love this movie.
Godfather was the most amazing movie.
The movie is excellent.
Table 1: Reviews in camera and movie domains. Bold-
faces are topic words and Italics are sentiment words.
Based on the observations, we can build a connec-
tion between the source and target domains by iden-
tifying the common sentiment words. Furthermore,
intuitively, there are some general syntactic relation-
ships or patterns between topic and sentiment words
across different domains. Therefore, if we can mine
the patterns from the source and target domain data,
then we are able to construct an indirect connection
between topic words across domains by using the
common sentiment words as a bridge, which makes
knowledge transfer across domains possible.
Figure 1 shows two dependency trees for the sen-
tence ?the camera is great? in the camera domain
and the sentence ?the movie is excellent? in the
movie domain, respectively. As can be observed, the
relationships between the topic and sentiment words
in the two sentences are the same. They both share
a ?TOPIC-nsubj-SENTIMENT? relation. Let the
camera domain be the source domain and the movie
domain be the target domain. If the word ?excel-
lent? is identified as a common sentiment word, and
the ?TOPIC-nsubj-SENTIMENT? relation extracted
from the camera domain is recognized as a common
syntactic pattern, then the word ?movie? can be pre-
dicted as a topic word in the movie domain with high
probability. After new topic words are extracted in
the movie domain, we can apply the same syntac-
tic pattern or other syntactic patterns to extract new
sentiment and topic words iteratively.
great
camera is
The
nsubj cop
det
(a) Camera domain.
excellent
movie is
The
nsubj cop
det
(b) Movie domain.
Figure 1: Examples of dependency tree structure.
More specifically, we use the shortest path be-
tween a topic word and a sentiment word in the cor-
responding dependency tree to denote the relation
between them. To get more general paths, we do
not take original words in the path into considera-
tion, but use their POS tags instead, such as ?NN?,
?VB?, ?JJ?, etc. As an example shown in Figure 2,
we can extract two paths or relationships between
topic and sentiment words from the dependency tree
of the sentence ?The movie has good script?: ?NN-
amod-JJ? from ?script? and ?good?, and ?NN-nsubj-
VB-dobj-NN-amod-JJ? from ?movie? and ?good?.
has(VB)
script(NN)
the(DT)
movie(NN)
good(JJ)
dobj nsubj
amod det
Figure 2: Example of pattern extraction.
In the following sections, we present the proposed
two-stage domain adaptation framework: 1) gener-
ating some sentiment and topic seeds in the target
domain; and 2) expanding the seeds in the target do-
main to construct sentiment and topic lexicons.
4 Seed Generation
Our basic idea is to first identify several common
sentiment words across domains as sentiment seeds.
Meanwhile, we mine some general patterns between
sentiment and topic words from the source domain.
Finally, we use the sentiment seeds and general pat-
terns to generate topic seeds in the target domain.
412
4.1 Sentiment Seed Generation
To identify common sentiment words across do-
mains, we extract all sentiment words from the
source domain as candidates. For each candidate,
we calculate its score based on the following metric:
S1(wi) = (pS(wi) + pT (wi)) e(?|pS(wi)?pT (wi)|), (1)
where pS(wi) and pT (wi) are the probabilities of the
word wi occurring in the source and target domains,
respectively. If a word wi has high S1 score, which
implies that the word wi occurs frequently and simi-
larly in both domains, then it can be considered as a
common sentiment word (Pan et al, 2010; Blitzer et
al., 2007). We select top r candidates with highest
S1 scores as sentiment seeds.
4.2 Topic Seed Generation
We extract all patterns between sentiment and topic
words in the source domain as candidates. For each
pattern candidate, we calculate its score based on a
metric defined in AutoSlog-TS (Riloff, 1996):
S2(Rj) = Acc(Rj)? log2(Freq(Rj)), (2)
where Acc(Rj) is the accuracy of the pattern Rj in
the source domain, and Freq(Rj) is the frequency
of the pattern Rj observed in target domain. This
metric aims to identify the patterns that are precise
in the source domain and observed frequently in the
target domain. We also select the top r patterns
with highest S2 scores. With the patterns and sen-
timent seeds, we extract topic-word candidates and
measure their scores based on a variant metric of
quadratic combination (Zhang and Ye, 2008):
S3(wk) =
?
Rj?A, wi?B
(S2(Rj)? S1(wi)) , (3)
where B is a set of sentiment seeds and A is a set of
patterns which the words wi and wk satisfy. We then
select the top r candidates as topic seeds.
5 Seed Expansion
After generating the topic and sentiment seeds, we
aim to expand them in the target domain to construct
topic and sentiment lexicons. In this section, we pro-
pose a new bootstrapping-based method to address
this problem.
Bootstrapping is the process of improving the per-
formance of a weak classifier by iteratively adding
training data and retraining the classifier. More
specifically, bootstrapping starts with a small set
of labeled ?seeds?, and iteratively adds unlabeled
data that are labeled by the classifier to the train-
ing set based on some selection criterion, and retrain
the classifier. Many bootstrapping-based algorithms
have been proposed to information extraction and
other NLP tasks (Blum and Mitchell, 1998; Riloff
and Jones, 1999; Jones et al, 1999; Wu et al, 2009).
One important issue in bootstrapping is how to
design a criterion to select unlabeled data to be
added to the training set iteratively. Our proposed
bootstrapping for cross-domain lexicon extraction
is based on the following two observations: 1) Al-
though the source and target domains are different,
part of source domain labeled data is still useful for
lexicon extraction in the target domain after some
adaptation; 2) The syntactic relationships among
sentiment and topic words can be used to expand the
seeds in the target domain for lexicon construction.
Based on the two observations, we propose a
new bootstrapping-based method named Relational
Adaptive bootstraPping (RAP), as summarized in
Algorithm 1, for expanding lexicons across do-
mains. In each iteration, we employ a cross-domain
classifier trained on the source domain lexicons and
the extracted target domain lexicons to predict the
labels of the target unlabeled data, and select top k2
predicted topic and sentiment words as candidates
based on confidence. With the extracted syntactic
patterns in the previous iterations, we construct a
bipartite graph between sentiment and topic words
on the extracted target domain lexicons and candi-
dates. After that, a graph-based score refinement al-
gorithm is performed on the graph, and the top k1
candidates are added to the extracted lexicons based
on the final scores. Accordingly, with the new ex-
tracted lexicons, we update the syntactic patterns in
each iteration. The details of RAP are presented in
the following sections.
5.1 Cross-Domain Classifier
In this paper, we employ Transfer AdaBoost (TrAd-
aBoost) (Dai et al, 2007) as the cross-domain learn-
ing algorithm in RAP. In TrAdaBoost, each word
wSi (or wTj ) is represented by a feature vector xSi
(or xTj ). A classifier trained on the source domain
data DS = {(xSi , ySi)} may perform poor on xTj
because of domain difference. The main idea of
TrAdaBoost is to re-weight the source domain data
based on a few of target domain labeled data, which
is referred to as seeds in our task. The re-weighting
413
aims to reduce the effect of the ?bad? source do-
main data while encourage the ?good? ones to get
a more precise classifier in target domain. In each
iteration of RAP, we train cross-domain classifiers
fTO and fTP for sentiment- and topic- word extrac-
tion using TrAdaBoost separately (taking sentiment
or topic words as positive instances). We use linear
Support Vector Machines (SVMs) as the base clas-
sifier in TrAdaBoost. For features to represent each
word, we use lexicon features, such as the previous,
current and next words, and POS tag features, such
as the previous, current and next words? POS tags.
Algorithm 1 Relational Adaptive bootstraPping
Require: Target domain data DT = DlT
?DuT , where DlT
consists of sentiment seeds B and topic seeds C and their
initial scores S1(wi), ?wi ? B and S3(wj), ?wj ? C, DuT
is the set of unlabeled target domain data; labeled source
domain data DS ; a cross-domain classifier; iteration num-
ber M and candidate selection number k1, k2.
Ensure: Expand C and B in the target domain.
1: Initialize a pattern set A = ?, S?1(wi) = S1(wi), wi ? B
and S?3(wj) = S3(wj), wj ? C. Consider all patterns
observed in the source domain as pattern candidates P .
2: for m = 1 . . .M do
3: Extract new pattern candidates to P with DlT in target
domain, update pattern score S?2(Rj), where Rj ? P ,
based on Eq. (4), and select the top k1 patterns to the
pattern set A.
4: Learn the cross-domain classifiers fTO and fTP for
sentiment- and topic- word extraction with DS
?DlT
separately. Predict the sentiment score hTfO (wTj ) and
topic score hTfP (wTj ) on D
u
T , and select k2 sentiment
words and topic words with highest scores as candidates.
5: Construct a bipartite graph between sentiment and topic
words on DlT and the k2 sentiment- and topic- word can-
didates, and calculate the normalized weights ?ij?s for
each edge of the graph.
6: Refine the scores S?1 and S?3 of the k2 sentiment and
topic word candidates using Eqs. (5) and (6) iteratively.
7: Select k1 new sentiment words and k1 new topic words
with the final scores, and add them to lexicons B and C.
Update S?1(wi) and S?3(wj) accordingly.
8: end for
9: return Expanded lexicons B and C.
5.2 Graph Construction
Based on the cross-domain classifiers fTO and fTP ,
we can predict the sentiment label score hTfO(wTi)
and topic label score hTfP (wTi) for the target domain
data wTi . According to all predicted values, we re-
spectively select top k2 new sentiment- and topic-
words as candidates. Together with the extracted
sentiment and topic lexicons in the target domain,
we build a bipartite graph among them as shown in
Figure 3. In the bipartite graph, one set of nodes
represents topic words, including new topic candi-
dates and words in the lexicon C, and the other set
of nodes represents sentiment words, including new
sentiment candidates and words in the lexicon B.
For a pair of sentiment and topic words wOTi and w
P
Tj ,
if there is a pattern Rj in the pattern set A that they
can satisfy, then there exists an edge eij between
them. Furthermore, each edge eij is associated with
a nonnegative weight ?ij , which is measured as fol-
lows, ?ij =
?
Rk?E S?2(Rk), where S?2 is the pattern
score. Similar to the metric defined in Eq. (3), the
pattern score is defined as:
S?2(Rj) =
?
{wi,wk}?E
(
S?1(wi)? S?3(wk)
)
, (4)
where E = {{wi, wj}|, wi ? B,wj ? C and
wi, wj satisfy Rj , Rj ? A}. Note that in the be-
ginning of each iteration, S?2 is updated based on the
new sentiment score S?1 and topic score S?3. We fur-
ther normalize ?ij by ??ij = ?ij/(
?
ij ?ij).
Topic words Sentiment words
music
movie
recommend
good
boring
script
NN-nsubj-VB-dobj-NN-amod-JJ
NN-amod-JJ
NN-nsubj-JJ
NN-amod-JJ
NN-d
obj-V
B
Figure 3: Topic and sentiment word graph.
5.3 Score Computation
We construct the bipartite graph to exploit the re-
lationships between sentiment and topic words to
propagate information for lexicon extraction. We
use the following reinforcement formulas to itera-
tively update the final sentiment score S?1(wTj ) and
topic score S?3(wTi), respectively:
S?1(wTj ) = ?
?
i
S?3(wTi)??ij + (1? ?)hTfO (wTj ), (5)
S?3(wTi) = ?
?
j
S?1(wTj )??ij + (1? ?)hTfP (wTi), (6)
where ? is a trade-off parameter between the pre-
dicted value by cross-domain classifier and the re-
inforcement scores from other nodes connected by
414
edge eij . Here ? is empirically set to be 0.5. With
Eqs. (5) and (6), the sentiment scores and topic
scores are iteratively refined until the state of the
graph trends to be stable. This can be considered
as an extension to the HITS algorithm(Kleinberg,
1999). Finally, we select k1 ? k2 sentiment and
topic words from the k2 candidates based on their
refined scores, and add them to the target domain
lexicons, respectively. We also update the sentiment
score S?1 and topic score S?3 for next iteration.
5.4 Special Cases
We now introduce two special cases of the RAP al-
gorithm. In Eqs. (5) and (6), if the parameter ? = 1,
then RAP only uses the relationships between sen-
timent and topic words with their patterns to propa-
gate label information in the target domain without
using the cross-domain classifier. We call this reduc-
tion relational bootstrapping. If ? = 0, then RAP
only utilizes useful source domain labeled data to as-
sist learning of the target domain classifier without
considering the relationships between sentiment and
topic words. We call this reduction adaptive boot-
strapping, which can be considered as a bootstrap-
ping version of TrAdaBoost. We also empirically
study these two special cases in experiments.
6 Experiments on Lexicon Evaluation
6.1 Data Set and Evaluation Criteria
We use the review dataset from (Li et al, 2010a),
which contains 500 movie and 601 product reviews,
for evaluation. The sentiment and topic words are
manually annotated. In this dataset, all types of
sentiment words are annotated instead of adjective
words only. For example, the verbs, such as ?like?,
?recommend?, and nouns, such as ?masterpiece?,
are also labeled as sentiment words. We construct
two cross-domain lexicon extraction tasks: ?prod-
uct vs. movie? and ?movie vs. product?, where the
word before ?vs.? corresponds with the source do-
main and the word after ?vs.? corresponds with the
target domain. We evaluate our methods in terms of
precision, recall and F-score (F1).
6.2 Baselines
The results of in-domain classifiers, which are
trained on plenty of target domain labeled data, can
be treated as upper-bounds. We denote iSVM and
iCRF the in-domain SVM and CRF classifiers in
experiments, and compare our proposed methods,
RAP, relational bootstrapping, and adaptive boot-
strapping, with the following baselines,
Unsupervised Method (Un) we implement a rule-
based method for lexicon extraction based on (Hu
and Liu, 2004), where adjective words that match
a rule is recognized as sentiment words, and nouns
that match a rule are recognized as topic words.
Semi-Supervised Method (Semi) we implement
the double propagation model proposed in (Qiu et
al., 2009). Since this method requires some target
domain labeled data, we manually label 30 senti-
ment words in the target domain.
Cross-Domain CRF (Cross-CRF) we implement
a cross-domain CRF algorithm proposed by (Jakob
and Gurevych, 2010).
TrAdaBoost We apply TrAdaBoost (Dai et al,
2007) on the source domain labeled data and the
generated seeds in the target domain to train a lexi-
con extractor.
6.3 Comparison Results
Comparison results on lexicon extraction are shown
in Table 2 and Table 3. From Table 2, we can ob-
serve that our proposed methods are effective for
sentiment lexicon extraction. The relational boot-
strapping method performs better than the unsuper-
vised method, TrAdaBoost and the cross-domain
CRF algorithm, and achieves comparable results
with the semi-supervised method. However, com-
pared to the semi-supervised method, our proposed
relational bootstrapping method does not require any
labeled data in the target domain. We can also ob-
serve that the adaptive bootstrapping method and the
RAP method perform much better than other meth-
ods in terms of F-score. The reason is that part of
the source domain labeled data may be useful for
learning the target classifier after reweighting. In
addition, we also observe that embedding the TrAd-
aBoost algorithm into a bootstrapping process can
further boost the performance of the classifier for
sentiment lexicon extraction.
Table 3 shows the comparison results on topic lex-
icon extraction. From the table, we can observe that
different from the sentiment lexicon extraction task,
the relational bootstrapping method performs better
than the adaptive bootstrapping method slightly. The
reason may be that for the sentiment lexicon extrac-
tion task, there exist some common sentiment words
415
product vs. movie movie vs. product
Prec. Rec. F1 Prec. Rec. F1
Un 0.82 0.31 0.45 0.74 0.23 0.35
Semi 0.71 0.44 0.54 0.62 0.45 0.52
Cross-CRF 0.69 0.40 0.51 0.65 0.34 0.45
Tradaboost 0.73 0.41 0.52 0.72 0.42 0.52
Adaptive 0.68 0.53 0.59 0.63 0.52 0.57
Relational 0.55 0.51 0.53 0.57 0.51 0.54
RAP 0.69 0.59 0.64 0.66 0.59 0.62
iSVM 0.82 0.60 0.70 0.80 0.61 0.68
iCRF 0.80 0.66 0.72 0.80 0.62 0.69
Table 2: Results on sentiment lexicon extraction. Num-
bers in boldface denote significant improvement.
product vs. movie movie vs. product
Prec. Rec. F1 Prec. Rec. F1
Un 0.41 0.32 0.36 0.53 0.35 0.41
Semi 0.54 0.59 0.56 0.75 0.50 0.60
Cross-CRF 0.70 0.23 0.34 0.80 0.24 0.37
Tradaboost 0.64 0.45 0.53 0.57 0.47 0.51
Adaptive 0.76 0.44 0.56 0.70 0.52 0.59
Relational 0.57 0.58 0.58 0.61 0.57 0.59
RAP 0.80 0.56 0.66 0.73 0.58 0.65
iSVM 0.83 0.73 0.78 0.85 0.70 0.77
iCRF 0.84 0.78 0.81 0.87 0.73 0.80
Table 3: Results on topic lexicon extraction. Numbers in
boldface denote significant improvement.
across domains, thus part of the labeled source do-
main data may be useful for the target learning task.
However, for the topic lexicon extraction task, the
topic words may be totally different, and as a result,
we may not be able to find useful source domain
labeled data to boost the performance for lexicon
extraction in the target domain. In this case, mu-
tual label propagation between sentiment and topic
words may be more reasonable for knowledge trans-
fer. RAP absorbs the advantages of the adaptive and
relational bootstrapping methods, thus can get the
best results in both lexicon extraction tasks.
We also observe that relational bootstrapping can
get better recall, but lower precision, compared to
adaptive bootstrapping. This is because relational
bootstrapping only utilizes the patterns to propagate
label information, which may cover more topic and
sentiment seeds, but include some noisy words. For
example, given two phases ?like the camera? and
?recommend the camera?, we can extract a pattern
?VB-dobj-NN?. However, by using this pattern and
the topic word ?camera?, we may extract ?take? as
a sentiment word from another phase ?take the cam-
era?, which is incorrect. The adaptive bootstrapping
method can utilize various features to make predic-
tions more precisely, which may have higher preci-
sion, but encounter the lower recall problem. For ex-
ample, ?flash? is not identified as a topic word in the
target product domain (camera domain). Our RAP
method can exploit both relationships between sen-
timent and topic words and part of labeled source
domain data for cross-domain lexicon extraction. It
can correctly identify the above two cases.
6.3.1 Parameter Sensitivity Study
In this section, we conduct experiments to study
the effect of different parameter settings. There are
several parameters in the framework: the number
of generated seeds r, the number of new candidates
k2 and the number of selections k in each iteration,
and the number of iterations M (? is empirically set
to 0.5 ). For the parameter k2, we just set it to a
large number (k2 = 100) such that have rich candi-
dates to build the bipartite graph. In the experiments
reported in the previous section, we set r = 20,
k1 = 10 and M = 50. Figures 4(a) and 4(b) show
the results under varying values of r in the ?product
vs. movie? task. Observe that for sentiment word
extraction, the results of the proposed methods are
not sensitive to the values of r. While for the topic
word extraction, the proposed methods perform well
when r falls in the range from 15 to 20.
5 10 15 20 25 30
0.45
0.5
0.55
0.6
0.65
0.7
Values of r
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(a) Sentiment word extraction
5 10 15 20 25 30
0.45
0.5
0.55
0.6
0.65
0.7
Values of r
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(b) Topic word extraction
Figure 4: Results on varying values of r.
0 10 20 30 40 50
0.4
0.45
0.5
0.55
0.6
0.65
Number of iterations
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(a) Sentiment word extraction
0 10 20 30 40 50
0.4
0.45
0.5
0.55
0.6
0.65
0.7
Number of iterations
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(b) Topic word extraction
Figure 5: Results on varying values of M .
We also test the sensitivity of the parameter k1
and find that the proposed methods work well and
robust when k1 falls in the range from 10 to 20.
416
Figures 5(a) and 5(b) show the results under vary-
ing numbers of iterations in the ?product vs. movie?
task. As we can see, our proposed methods converge
well when M ? 40.
7 Application: Sentiment Classification
To further verify the usefulness of the lexicons ex-
tracted by the RAP method, we apply the extracted
sentiment lexicon for sentiment classification.
7.1 Experiment Setting
Our work is motivated by the work of (Pang and
Lee, 2004), which only used subjective sentences
for document-level sentiment classification, instead
of using all sentences. In this experiment, we only
use sentiment related words as features to represent
opinion documents for classification, instead of us-
ing all words. Our goal is compare the sentiment
lexicon constructed by the RAP method with other
general lexicons on the impact of for sentiment clas-
sification. The general lexicons used for comparison
are described in Table 4.
We use the dataset from (Blitzer et al, 2007) for
sentiment classification. It contains a collection of
product reviews from Amazon.com. The reviews are
about four product domains: books, dvds, electron-
ics and kitchen appliance. In each domain, there are
1000 positive and 1000 negative reviews. To con-
struct domain specific sentiment lexicons, we apply
RAP on each product domain with the movie domain
described in Section 6.1 as the source domain. Fi-
nally, we use linear SVM as the classifier and the
classification accuracy as the evaluate criterion.
Lexicon Name Size Description
Senti-WordNet 6957 Words with a subjective score > 0.6
(Esuli and Sebastiani, 2006)
HowNet 4619 Eng. translation of subj. Chinese
words (Dong and Dong, 2006)
Subj. Clues 6878 Lexicons from (Wilson et al, 2005)
Table 4: Description of different lexicons.
7.2 Experimental Results
Experimental results on sentiment classification are
shown in Table 5, where we denote ?All? using all
unigram and bigram features instead of using sub-
jective words. As we can see that a classifier trained
with features constructed by our RAP method per-
formance best in all domains. Note that the num-
ber of features (sentiment words) constructed by our
method is much smaller than that of all unigram
and bigram features, which can reduce the classi-
fier training time dramatically. These promising re-
sults imply that our RAP can be applied for senti-
ment classification effectively and efficiently.
All Senti HowNet Subj. Clue Ours
dvd 82.55 79.80 80.57 80.93 84.05
book 80.71 76.22 78.22 79.48 81.65
electronic 84.43 82.42 83.05 83.22 86.71
kitchen 87.70 81.78 84.17 84.23 88.83
Table 5: Sentiment classification results (accuracy in %).
Numbers in boldface denotes significant improvement.
8 Conclusions
In this paper, we propose a two-stage framework for
co-extraction of sentiment and topic lexicons across
domains where we have no labeled data in the tar-
get domain but have plenty of labeled data in an-
other domain. In the first stage, we propose a sim-
ple strategy to generate a few high-quality sentiment
and topic seeds for the target domain. In the second
stage, we propose a novel Relational Adaptive boot-
straPping (RAP) method to expand the seeds, which
can exploit the relationships between topic and opin-
ion words, and make use of part of useful source do-
main labeled data for help. Extensive experimental
results show our proposed method can extract pre-
cise sentiment and topic lexicons from the target do-
main. Furthermore, the extracted sentiment lexicon
can be applied to sentiment classification effectively.
In the future work, besides the heterogeneous
relationships between topic and sentiment words,
we intend to investigate the homogeneous relation-
ships among topic words and those among sentiment
words (Qiu et al, 2009) to further boost the perfor-
mance of RAP method. Furthermore, in our frame-
work, we do not identify the polarity of the extracted
sentiment lexicon. We also plan to embed this com-
ponent into our unified framework. Finally, it is also
interesting to exploit multi-domain knowledge (Li
and Zong, 2008; Bollegala et al, 2011) for cross-
domain lexicon extraction.
9 Acknowledgement
This work was supported by the Chinese Natu-
ral Science Foundation No.60973104, National Key
Basic Research Program 2012CB316301, and Hong
Kong RGC GRF Projects 621010 and 621211.
417
References
Rie K. Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. J. Mach. Learn. Res., 6:1817?1853.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 432?439,
Prague, Czech Republic. ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92?100.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a sentiment
sensitive thesaurus for cross-domain sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 132?141, Port-
land, Oregon. ACL.
Wenyuan Dai, Qiang Yang, Guirong Xue, and Yong Yu.
2007. Boosting for transfer learning. In Proceed-
ings of the 24th International Conference on Machine
Learning, pages 193?200, Corvalis, Oregon, USA,
June. ACM.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?
263, Prague, Czech Republic. ACL.
Zhendong Dong and Qiang Dong, editors. 2006.
HOWNET and the computation of meaning. World
Scientific Publishers, Norwell, MA, USA.
Weifu Du, Songbo Tan, Xueqi Cheng, and Xiaochun
Yun. 2010. Adapting information bottleneck method
for automatic construction of domain-oriented senti-
ment lexicon. In Proceedings of the 3rd ACM inter-
national conference on Web search and data mining,
pages 111?120, New York, NY, USA. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource for
opinion mining. In In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation, pages
417?422.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning, pages 513?520, Bellevue, Washing-
ton, USA.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Auto-
matically extracting polarity-bearing topics for cross-
domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 123?131, Portland, Oregon. ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177, Seat-
tle, WA, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1035?1045, Cambridge,
Massachusetts, USA. ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264?271, Prague, Czech
Republic. ACL.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized HMM-based learning framework for web opinion
mining. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, pages 465?
472, Montreal, Quebec, Canada. ACM.
Rosie Jones, Andrew Mccallum, Kamal Nigam, and
Ellen Riloff. 1999. Bootstrapping for text learning
tasks. In In IJCAI-99 Workshop on Text Mining: Foun-
dations, Techniques and Applications, pages 52?63.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. J. ACM, 46:604?632, Sept.
Shoushan Li and Chengqing Zong. 2008. Multi-domain
sentiment classification. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short
Papers, pages 257?260, Columbus, Ohio, USA. ACL.
Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.
2009. Knowledge transformation for cross-domain
sentiment classification. In Proceedings of the 32nd
international ACM SIGIR conference on Research and
development in information retrieval, pages 716?717,
Boston, MA, USA. ACM.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a.
Structure-aware review mining and summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 653?661, Beijing,
China.
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010b.
Sentiment analysis with global topics and local de-
pendency. In Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence, Atlanta, Geor-
gia, USA. AAAI Press.
418
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, Second
Edition.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, pages 171?180, Banff, Alberta, Canada.
ACM.
Sinno Jialin Pan and Qiang Yang. 2010. A survey
on transfer learning. IEEE Trans. Knowl. Data Eng.,
22(10):1345?1359, Oct.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Chen Zheng. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
Proceedings of the 19th International Conference on
World Wide Web, pages 751?760, Raleigh, NC, USA,
Apr. ACM.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, Barcelona, Spain. ACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 339?346, Vancouver, British
Columbia, Canada. ACL.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, pages 1199?
1204, Pasadena, California, USA. Morgan Kaufmann
Publishers Inc.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the 6th national con-
ference on Artificial intelligence, pages 474?479, Or-
lando, Florida, United States. AAAI.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the 7th conference
on natural language learning, pages 25?32, Edmon-
ton, Canada. ACL.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of
the Thirteenth National Conference on Artificial In-
telligence, pages 1044?1049, Portland, Oregon, USA.
AAAI Press/MIT Press.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi
Cheng. 2007. A novel scheme for domain-transfer
problem in the context of sentiment analysis. In Pro-
ceedings of the 16th ACM conference on Conference
on information and knowledge management, pages
979?982, Lisbon, Portugal. ACM.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the 46th Annual Meeting of the As-
sociation of Computational Linguistics: Human Lan-
guage Technologies, pages 308?316, Columbus, Ohio,
USA. ACL.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Comput. Linguist., 30:277?308, Sept.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354,
Vancouver, British Columbia, Canada. ACL.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1523?1532, Singapore. ACL.
Min Zhang and Xingyao Ye. 2008. A generation
model to unify topic relevance and lexicon-based sen-
timent for opinion retrieval. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 411?418, Singapore. ACM.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 56?65, Cambridge, Mas-
sachusetts, USA. ACL.
419
