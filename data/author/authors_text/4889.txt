Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 17?20, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Dynamically Generating a Protein Entity Dictionary Using Online Re-
sources 
Hongfang Liu Zhangzhi Hu Cathy Wu
Department of Information Systems Department of Biochemistry and Molecular Biology 
University of Maryland, Baltimore County Georgetown University Medical Center 
Baltimore, MD 21250 3900 Reservoir Road, NW, Washington, DC 20057 
hfliu@umbc.edu {zh9,wuc}@georgetown.edu 
 
 
Abstract: With the overwhelming amount of biological 
knowledge stored in free text, natural language proc-
essing (NLP) has received much attention recently to 
make the task of managing information recorded in 
free text more feasible. One requirement for most 
NLP systems is the ability to accurately recognize 
biological entity terms in free text and the ability to 
map these terms to corresponding records in data-
bases. Such task is called biological named entity 
tagging. In this paper, we present a system that 
automatically constructs a protein entity dictionary, 
which contains gene or protein names associated with 
UniProt identifiers using online resources. The system 
can run periodically to always keep up-to-date with 
these online resources. Using online resources that 
were available on Dec. 25, 2004, we obtained 
4,046,733 terms for 1,640,082 entities. The dictionary 
can be accessed from the following website: 
http://biocreative.ifsm.umbc.edu/biothesauru
s/.  
Contact: hfliu@umbc.edu 
 
1 Introduction  
With the use of computers in storing the explosive 
amount of biological information, natural language 
processing (NLP) approaches have been explored to 
make the task of managing information recorded in 
free text more feasible [1, 2]. One requirement for 
NLP is the ability to accurately recognize terms that 
represent biological entities in free text. Another re-
quirement is the ability to associate these terms with 
corresponding biological entities (i.e., records in bio-
logical databases) in order to be used by other auto-
mated systems for literature mining. Such task is 
called biological entity tagging. Biological entity 
tagging is not a trivial task because of several charac-
teristics associated with biological entity names, 
namely: synonymy (i.e., different terms refer to the 
same entity), ambiguity (i.e., one term is associated 
with different entities), and coverage (i.e., entity 
terms or entities are not present in databases or 
knowledge bases).  
Methods for biological entity tagging can be catego-
rized into two types: one is to use a dictionary and a 
mapping method [3-5], and the other is to markup 
terms in the text according to contextual cues, spe-
cific verbs, or machine learning  [6-10]. The per-
formance of biological entity tagging systems using 
dictionaries depends on the coverage of the diction-
ary as well as mapping methods that can handle syn-
onymous or ambiguous terms. Strictly speaking, 
tagging systems that do not use dictionaries are not 
biological entity tagging but biological term tagging, 
since tagged terms in text are not associated with 
specific biological entities stored in databases. It re-
quires an additional step to map terms mentioned in 
the text to records in biological databases in order to 
be automatically integrated with other system or da-
tabases. Due to the dynamic nature associated with 
the molecular biology domain, it is critical to have a 
comprehensive biological entity dictionary that is 
always up-to-date.  
In this paper, we present a system that constructs a 
large protein entity dictionary, BioThesaurus, using 
online resources. Terms in the dictionary are then 
curated based on high ambiguous terms to flag non-
sensical terms (e.g., Novel protein) and are also cu-
rated based on the semantic categories acquired from 
the UMLS to flag descriptive terms that associate 
with other semantic types other than gene or proteins 
(e.g., terms that refer to species, cells or other small 
molecules). In the following, we first provide back-
ground and related work on dictionary construction 
using online resources. We then present our method 
on constructing the dictionary.  
2 Resources 
The system utilizes several large size biological data-
bases including three NCBI databases (GenPept [11], 
RefSeq [12], and Entrez GENE [13]), PSD database 
from Protein Information Resources (PIR) [14], and 
17
UniProt [15]. Additionally, several model organism 
databases or nomenclature databases were used. Cor-
respondences among records from these databases 
are identified using the rich cross-reference informa-
tion provided by the iProClass database of PIR [14]. 
The following provides a brief description of each of 
the database.  
PIR Resources ? There are three databases in PIR: 
the Protein Sequence Database (PSD), iProClass, and 
PIR-NREF. PSD database includes functionally an-
notated protein sequences. The iProClass database is 
a central point for exploration of protein information, 
which provides summary descriptions of protein fam-
ily, function and structure for all protein sequences 
from PIR, Swiss-Prot, and TrEMBL (now UniProt). 
Additionally, it links to over 70 biological databases 
in the world. The PIR-NREF database is a compre-
hensive database for sequence searching and protein 
identification. It contains non-redundant protein se-
quences from PSD, Swiss-Prot, TrEMBL, RefSeq, 
GenPept, and PDB.  
Figure 1: The overall architecture of the system 
UniProt ? UniProt provides a central repository of 
protein sequence and annotation created by joining 
Swiss-Prot, TrEMBL, and PSD. There are three 
knowledge components in UniProt: Swissprot, 
TrEMBL, and UniRef. Swissprot contains manually-
annotated records with information extracted from 
literature and curator-evaluated computational analy-
sis. TrEMBL consists of computationally analyzed 
records that await full manual annotation. The Uni-
Prot Non-redundant Reference (UniRef) databases 
combine closely related sequences into a single re-
cord where similar sequences are grouped together. 
Three UniRef tables UniRef100, UniRef90 and Uni-
Ref50) are available for download: UniRef100 com-
bines identical sequences and sub-fragments into a 
single UniRef entry; and UniRef90 and UniRef50 are 
built by clustering UniRef100 sequences into clusters 
based on the CD-HIT algorithm [16] such that each 
cluster is composed of sequences that have at least 
90% or 50% sequence similarity, respectively, to the 
representative sequence. 
NCBI resources ? three data sources from NCBI 
were used in this study: GenPept, RefSeq, and Entrez 
GENE. GenPept entries are those translated from the 
GenBanknucleotide sequence database. RefSeq is a 
comprehensive, integrated, non-redundant set of se-
quences, including genomic DNA, transcript (RNA), 
and protein products, for major research organisms. 
Entrez GENE provides a unified query environment 
for genes defined by sequence and/or in NCBI's Map 
Viewer. It records gene names, symbols, and many 
other attributes associated with genes and the prod-
ucts they encode. 
The UMLS ? the Unified Medical Language System 
(UMLS) has been developed and maintained by Na-
tional Library of Medicine (NLM) [17]. It contains 
three knowledge sources: the Metathesaurus 
(META), the SPECIALIST lexicon, and the Seman-
tic Network. The META provides a uniform, inte-
grated platform for over 60 biomedical vocabularies 
and classifications, and group different names for the 
same concept. The SPECIALIST lexicon contains 
syntactic information for many terms, component 
words, and English words, including verbs, which do 
not appear in the META. The Semantic Network con-
tains information about the types or categories (e.g., 
?Disease or Syndrome?, ?Virus?) to which all META 
concepts have been assigned. 
Other molecular biology databases - We also in-
cluded several model organism databases or nomen-
clature databases in the construction of the 
dictionary, i.e., mouse - Mouse Genome Database 
(MGD) [18],  fly - FlyBase [19], yeast - Saccharomy-
ces Genome Database (SGD) [20], rat ? Rat Genome 
Database (RGD) [21], worm ? WormBase [22], Hu-
man Nomenclature Database (HUGO) [23], Online 
Mendelian Inheritance in Man  (OMIM) [24], and 
Enzyme Nomenclature Database (ECNUM) [25, 26]. 
3 System Description and Results 
The system was developed using PERL and the 
PERL module Net::FTP. Figure 1 depicts the overall 
architecture. It automatically gathers fields that con-
tain annotation information from PSD, RefSeq, 
Swiss-Prot, TrEMBL, GenBank, Entrez GENE, MGI, 
RGD, HUGO, ENCUM, FlyBase, and WormBase for 
each iProClass record from the distribution website 
18
Figure 2: Screenshot of retrieving il2 from BioThesaurus  
 
of each resource. Annotations extracted from each 
resource were then processed to extract terms where 
each term is associated with one or more UniProt 
unique identifiers and comprised the raw dictionary 
for BioThesaurus. The raw dictionary was computa-
tionally curated using the UMLS to flag the UMLS 
semantic types and remove several high frequent 
nonsensical terms. There were a total of 1,677,162 
iProclass records in the PIR release 59 (released on 
Dec 25 2004). From it, we obtained 4,046,733 terms 
for 1,640,082 entities. Note that about 27,000 records 
have no terms in the dictionary mostly because they 
are new sequences and have not been annotated and 
linked to other resources or terms associated with 
them are nonsensical. The dictionary can be searched 
through the following URL: 
http://biocreative.ifsm.umbc.edu/biothesaurus/Biothe
saurus.html. 
 
Figure 2 shows a screenshot when retrieving entities 
associated with term il2. It indicates that there are 
totally 71 entities in UniProt that il2 represents when 
ignoring textual variants. The first column of the ta-
ble is UniProt ID. The primary name is shown in the 
second column, the family classifications available 
from iProClass are shown in the following several 
columns, the taxonomy information is shown in the 
next. The popularity of the term (i.e., the number of 
databases that contain the term or its variants) is 
shown next. And the last column shows the links to 
the records from which the system extracted the 
terms. 
4 Discussion and Conclusion 
We demonstrated here a system which generates a 
protein entity dictionary dynamically using online 
resources. The dictionary can be used by biological 
entity tagging systems to map entity terms mentioned 
in the text to specific records in UniProt. 
 
Acknowledgements 
 
The project was supported by IIS-0430743 from the 
National Science Foundation.  
Reference 
1. Hirschman L, Park JC, Tsujii J, Wong L, Wu CH: 
Accomplishments and challenges in literature 
data mining for biology. Bioinformatics 2002, 
18(12):1553-1561. 
19
2. Shatkay H, Feldman R: Mining the biomedical 
literature in the genomic era: an overview. J 
Comput Biol 2003, 10(6):821-855. 
3. Krauthammer M, Rzhetsky A, Morozov P, Fried-
man C: Using BLAST for identifying gene and 
protein names in journal articles. Gene 2000, 
259(1-2):245-252. 
4. Jenssen TK, Laegreid A, Komorowski J, Hovig E: 
A literature network of human genes for high-
throughput analysis of gene expression. Nat 
Genet 2001, 28(1):21-28. 
5. Hanisch D, Fluck J, Mevissen HT, Zimmer R: 
Playing biology's name game: identifying pro-
tein names in scientific text. Pac Symp Biocom-
put 2003:403-414. 
6. Fukuda K, Tamura A, Tsunoda T, Takagi T: To-
ward information extraction: identifying pro-
tein names from biological papers. Pac Symp 
Biocomput 1998:707-718. 
7. Sekimizu T, Park HS, Tsujii J: Identifying the 
Interaction between Genes and Gene Products 
Based on Frequently Seen Verbs in Medline 
Abstracts. Genome Inform Ser Workshop Genome 
Inform 1998, 9:62-71. 
8. Narayanaswamy M, Ravikumar KE, Vijay-
Shanker K: A biological named entity recog-
nizer. Pac Symp Biocomput 2003:427-438. 
9. Tanabe L, Wilbur WJ: Tagging gene and protein 
names in biomedical text. Bioinformatics 2002, 
18(8):1124-1132. 
10. Lee KJ, Hwang YS, Kim S, Rim HC: Bio-
medical named entity recognition using two-
phase model based on SVMs. J Biomed Inform 
2004, 37(6):436-447. 
11. Benson DA, Karsch-Mizrachi I, Lipman DJ, 
Ostell J, Wheeler DL: GenBank: update. Nucleic 
Acids Res 2004, 32 Database issue:D23-26. 
12. Pruitt KD, Katz KS, Sicotte H, Maglott DR: 
Introducing RefSeq and LocusLink: curated 
human genome resources at the NCBI. Trends 
Genet 2000, 16(1):44-47. 
13. NCBI: Entrez Gene. In., vol. 
http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db
=gene; 2004. 
14. Wu CH, Yeh LS, Huang H, Arminski L, 
Castro-Alvear J, Chen Y, Hu Z, Kourtesis P, Led-
ley RS, Suzek BE et al The Protein Information 
Resource. Nucleic Acids Res 2003, 31(1):345-347. 
15. Apweiler R, Bairoch A, Wu CH, Barker 
WC, Boeckmann B, Ferro S, Gasteiger E, Huang 
H, Lopez R, Magrane M et al UniProt: the Uni-
versal Protein knowledgebase. Nucleic Acids Res 
2004, 32 Database issue:D115-119. 
16. Li W, Jaroszewski L, Godzik A: Clustering 
of highly homologous sequences to reduce the 
size of large protein databases. Bioinformatics 
2001, 17(3):282-283. 
17. Bodenreider O: The Unified Medical Lan-
guage System (UMLS): integrating biomedical 
terminology. Nucleic Acids Res 2004, 32 Data-
base issue:D267-270. 
18. Bult CJ, Blake JA, Richardson JE, Kadin 
JA, Eppig JT, Baldarelli RM, Barsanti K, Baya M, 
Beal JS, Boddy WJ et al The Mouse Genome 
Database (MGD): integrating biology with the 
genome. Nucleic Acids Res 2004, 32 Database is-
sue:D476-481. 
19. Consortium F: The FlyBase database of the 
Drosophila genome projects and community lit-
erature. Nucleic Acids Res 2003, 31(1):172-175. 
20. Cherry JM, Adler C, Ball C, Chervitz SA, 
Dwight SS, Hester ET, Jia Y, Juvik G, Roe T, 
Schroeder M et al SGD: Saccharomyces Ge-
nome Database. Nucleic Acids Res 1998, 
26(1):73-79. 
21. Twigger S, Lu J, Shimoyama M, Chen D, 
Pasko D, Long H, Ginster J, Chen CF, Nigam R, 
Kwitek A et al Rat Genome Database (RGD): 
mapping disease onto the genome. Nucleic Acids 
Res 2002, 30(1):125-128. 
22. Harris TW, Chen N, Cunningham F, Tello-
Ruiz M, Antoshechkin I, Bastiani C, Bieri T, 
Blasiar D, Bradnam K, Chan J et al WormBase: 
a multi-species resource for nematode biology 
and genomics. Nucleic Acids Res 2004, 32 Data-
base issue:D411-417. 
23. Povey S, Lovering R, Bruford E, Wright M, 
Lush M, Wain H: The HUGO Gene Nomencla-
ture Committee (HGNC). Hum Genet 2001, 
109(6):678-680. 
24. Hamosh A, Scott AF, Amberger JS, Boc-
chini CA, McKusick VA: Online Mendelian In-
heritance in Man (OMIM), a knowledgebase of 
human genes and genetic disorders. Nucleic Ac-
ids Res 2005, 33 Database Issue:D514-517. 
25. Gegenheimer P: Enzyme nomenclature: 
functional or structural? Rna 2000, 6(12):1695-
1697. 
26. Tipton K, Boyce S: History of the enzyme 
nomenclature system. Bioinformatics 2000, 
16(1):34-40. 
 
20
A Study of Text Categorization for Model Organism Databases  
 
Hongfang Liu 
University of Maryland at Bal-
timore County 
hfliu@umbc.edu 
Cathy Wu 
Georgetown University 
Medical Center 
wuc@georgetown.edu 
 
 
Abstract 
One of the routine tasks for model organ-
ism database curators is to identify and 
associate research articles to database en-
tries. Such task can be considered as text 
categorization which has been studied in 
the general English domain. The task can 
be decomposed into two text categoriza-
tion subtasks: i) finding relevant articles 
associating with specific model organ-
isms, and ii) routing the articles to spe-
cific entries or specific areas. In this 
paper, we investigated the first subtask 
and designed a study using existing refer-
ence information available at four well-
known model organism databases and 
investigated the problem of identifying 
relevant articles for these organisms. We 
used features obtained from abstract text 
and titles. Additionally, we studied the de-
termination power of other MEDLINE ci-
tation fields (e.g., Authors, 
MeshHeadings, Journals). Furthermore, 
we compared three supervised machine 
learning techniques on predicting to 
which organism the article belongs.  
1 Introduction 
With the accelerated accumulation of genetic in-
formation associated with popularly used genetic 
model organisms for the human genome project 
such as laboratory mouse, C. elegans, fruit fly, and 
Saccharomyces, model organism databases that 
contain curated genetic information specifically to 
the associated organism have been initialized and 
evolved to provide a central place for researchers 
to seek condense genetic information (Flybase, 
2003; Blake, 2003; Misra, 2003). At the same time, 
a rich amount of genetic and biomedical informa-
tion associated with these model organisms are 
published through scientific literature. One of the 
routine curation tasks for the database curators is to 
associate research articles to specific genetic en-
tries in the databases and identify key information 
mentioned in the articles. For example, a regular 
practice of mouse genetic database curators is to 
scan the current scientific literature, extract and 
enter the relevant information into databases 
(Blake, 2003). In Saccharomyces Genome Data-
base (SGD, Saccharomyces cerevisiae: 
http://www.yeastgenome.org), database curators 
are currently in the process of revising the informa-
tion associated with the Description field of an en-
try to ensure that the Description (which usually is 
a concise summary of the function and biological 
context of the associated entry) contains the most 
up-to-date information and is written in a consis-
tent style. One of the current objectives of Worm-
Base (http://www.wormbase.org) is to 
systematically curate the C. elegans literature. 
However, manually scanning scientific articles is a 
labor intensive task. Meanwhile, the outcome may 
be incomplete, i.e., curators may miss some critical 
papers. Additionally, more than 2000 completed 
references are added daily to MEDLINE alone. It 
seems impossible to be always up-to-date.  
The task of associating research articles with 
specific entries can be decomposed into two sub-
tasks: i) categorizing articles into several catego-
ries where articles with the same category are 
about the same model organism, and ii) associating 
the articles to specific entries or specific areas. 
Finding relevant articles specific to a particular 
model organism is a case of information retrieval. 
A simple way to retrieve relevant articles about a 
model organism is to retrieve articles containing 
terms that represent that organism. For example, if 
the term ?C. elegans? appears in a paper, most 
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 25-32.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
likely, the paper is relevant to C. elegans. Another 
way is to apply supervised machine learning tech-
niques on a list of category-labeled documents. In 
this paper, we designed a study on retrieving rele-
vant articles using MEDLINE reference informa-
tion obtained from four model organism databases 
aiming to answer the following questions: 
? Can we just use keywords to retrieve relevant 
MEDLINE references instead of using com-
plicated machine learning techniques? 
? How accurate is the retrieval when we use 
various MEDLINE fields such as Authors, 
MeshHeadings etc to retrieve articles? 
Which kind of feature representations has 
the best performance?  
? Which kind of machine learning algorithm is 
suitable for categorizing the articles to the 
appropriate categories? 
? How good is the MEDLINE citation informa-
tion when we used category-labeled docu-
ments obtained in the past to predict the 
category of new documents? 
In the following, we first provide background 
information about applying supervised machine 
learning techniques on text categorization. We then 
describe materials and methods. Finally, we pre-
sent our results and discussions. 
2 Background and Related Work 
Using keywords to retrieve relevant articles is to 
use a list of keywords to retrieve articles contain-
ing these keywords. The main component here is 
to derive a list of keywords for each category. Us-
ing supervised machine learning techniques to re-
trieve relevant articles requires a collection of 
category-labeled documents. The objective is to 
learn classifiers from these category-labeled 
documents. The construction process of a classifier 
given a list of category-labeled documents contains 
two components. The first component transfers 
each document into a feature representation. The 
second component uses a supervised learning algo-
rithm to learn classification knowledge that forms 
a classifier.  
Machine learning for text categorization re-
quires transforming each document into a feature 
representation (usually a feature vector) where fea-
tures are usually words or word stems in the 
document. In our study, in addition to word or 
word stems in free text, we also explored other 
features that could be extracted from the material 
we used for the study. 
Several supervised learning algorithms have 
been adapted for text categorization: Na?ve Bayes 
learning (Yang and Liu, 1999), neural networks 
(Wiener, 1995), instance-based learning (Iwayama 
and Takunaga, 1995), and Support vector machine 
(Joachims, 1998). Yang and Liu (1999) provided 
an overview and a comparative study about differ-
ent learning algorithms. In previous studies of ap-
plying supervised machine learning on the problem 
of word sense disambiguation, we investigated and 
implemented several supervised learning algo-
rithms including Na?ve Bayes learning, Decision 
List learning and Support Vector Machine for 
word sense disambiguation. There is not much dif-
ference between word sense disambiguation task 
and text categorization task. We can formulate a 
word sense disambiguation task as a text 
categorization task by considering senses of a word 
as categories (Sebastiani, 2002). We can also 
formulate a text categorization task by considering 
there is a hidden word (e.g., TC) in the text with 
multiple senses (i.e., categories). Note that in word 
sense disambiguation task, one occurrence of a 
word usually holds a unique sense. While for text 
categorization task, sometimes one document can 
be in multiple categories. After verifying that there 
were less than 1% of documents holding multiple 
categories (shown in detail in the following sec-
tion), for simplicity, we applied the implementa-
tions of supervised machine learning algorithm 
(used for word sense disambiguation) directly for 
text categorization by considering the disambigua-
tion of a hidden word (TC) in the context. The fol-
lowing summarizes the algorithms used in the 
study. For detail implementations of these algo-
rithms, readers can refer to (Liu, 2004).  
Na?ve Bayes learning (NBL) (Duda, 1973) is 
widely used in machine learning due to its effi-
ciency and its ability to combine evidence from a 
large number of features. An NBL classifier 
chooses the category with the highest conditional 
probability for a given feature vector; while the 
computation of conditional probabilities is based 
on the Na?ve Bayes assumption: the presence of 
one feature is independent of another when condi-
tioned on the category variable. The training of the 
Na?ve Bayes classifier consists of estimating the 
prior probabilities for different categories as well 
as the probabilities of each category for each fea-
ture.  
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
19
65
19
68
19
71
19
74
19
77
19
80
19
83
19
86
19
89
19
92
19
95
19
98
20
01
Year
N
um
be
r 
of
 C
ita
tio
ns
Yeast Fly Worm Mouse
0
0.02
0.04
0.06
0.08
0.1
0.12
19
65
19
68
19
71
19
74
19
77
19
80
19
83
19
86
19
89
19
92
19
95
19
98
20
01
Year
P
ro
p
or
tio
n
Yeast Fly Worm Mouse
The Decision List method (DLL) (Yarowsky, 
1994) is equivalent to simple case statements in 
most programming languages. In a DLL classifier, 
a sequence of tests is applied to each feature vec-
tor. If a test succeeds, then the sense associated 
with that test is returned. If the test fails, then the 
next test in the sequence is applied. This continues 
until the end of the list, where a default test simply 
returns the majority sense. Learning a decision list 
classifier consists of generating and ordering indi-
vidual tests based on the characteristics of the 
training data.  
(a) 
Support vector machine (SVM) (Vapnik, 
1998) is a supervised learning algorithm proposed 
by Vladimir Vapnik and his co-workers. For a bi-
nary classification task with classes {+1, ?1}, 
given a training set with n class-labeled instances, 
(x1, y1), (x2, y2), ..., (xi, yi), ?, (xn, yn), where xi 
is a feature vector for the ith instance and yi indi-
cates the class, an SVM classifier learns a linear 
decision rule, which is represented using a hyper-
plane. The tag of an unlabelled instance x is deter-
mined by which side of the hyperplane x lies. The 
purpose of training the SVM is to find a hyper-
plane that has the maximum margin to separate the 
two classes.  
Using a list of keywords to retrieve relevant ar-
ticles has been used frequently for NLP systems in 
the biological domain. For example, Iliopoulos et 
al. (2001) used keywords pertinent to a biological 
process or a single species to select a set of ab-
stracts for their system. Supervised machine learn-
ing has been used by Donaldson et al (2003) to 
recognize abstracts describing bio-molecular inter-
actions. The training articles in their study were 
collected and judged by domain experts. In our 
study, we compared keywords retrieving with su-
pervised machine learning algorithms. The cate-
gory-labeled training documents used in our study 
were automatically obtained from model organism 
databases and MEDLINE.  
3 Material and Methods 
3.1 Model Organism Databases 
The research done here is based on MEDLINE 
references associated with four model organisms 
 (i.e., mouse, fly, worm and yeast) obtained from      
Mouse Genome Informatics (MGD, Mus musculus: 
http:// www.informatics.jax.org, FlyBase (Droso-
phila melanogaster. http://www.flybase.org), 
WormBase (Caenorhabditis elegans: 
http://www.wormbase.org), and Saccharomyces 
Genome Database (SGD, Saccharomyces cere-
visiae: http://www.yeastgenome.org). We 
downloaded literature reference information from 
each database on March 2003. All databases pro-
vide PMID (unique identifier for MEDLINE cita-
tions) information except WormBase where some 
references use MEDLINEID (another unique iden-
tifier for MEDLINE citations) as reference identi-
fiers, some references use PMID as reference 
identifiers. Meanwhile, about two thirds of the ref-
erences in WormBase do not have reference identi-
fiers to MEDLINE, which we eliminated in our 
study since we were not able to get the MEDLINE 
Figure 1. References for four organism databases 
from 1966 to 2002. X-axis represents years from 
1965 to 2003 in ascending order. The Y axis in figure 
(a) represents the number of citations. The Y-axis in 
figure (b) represents the proportion of each year 
comparing to the total number of citations for a spe-
cific organism. 
 (b) 
citation information. We then used e-Fetch tools 
provided by Entrez (http://entrez.nlm.nih.gov) and 
fetched complete MEDLINE citations in XML 
format from MEDLINE.  
Year Tra Te AbT  ArT Aut Jou MH 
1990 21563 2963 251  0 0  11 3 
1991 24526 3394 250  0 2  21  0 
1992 27920 4355 300 0 1 8 2 
1993 32275 5267 391 0 1 46  0 
1994 37542 6474 450 0 3 34 1 
1995 44016 7178 490 0 4 12  1 
1996 51194 7782 613 0 2 3 4 
1997 58976 8115 593 0 4 8 10 
1998 67091 7726 519 0 2 8 12 
1999 74817 9057 599 0 10 15 23 
2000 83874 9234 587 0 4 16 28 
2001 93108 8479 362 0 4 16 309 
2002 101587 7688 237 0 6 7 1366 
2003 109275 569 6 0 0 0 146 
Total NA 88281 5647 0 43 206 1905 
Finally, we obtained 31,414 MEDLINE cita-
tions from Flybase, 26,046 from SGD, 3,926 from 
WormBase, and 48,458 from MGD. Figure 1 lists 
the statistical information according to the publica-
tion date for each organism, where X-axis repre-
sents year, Y-axis in Fig. 1(a) represents the 
number of citations and Y-axis in Fig. 1(b) repre-
sents the percentage of the number of citations to 
the total number of citations for each organism. 
Note that there were 1,005 citations holding multi-
ple categories (15 of them were referred by mouse, 
fly and yeast, 1 referred by fly, worm, and mouse, 
338 referred by mouse and yeast, 282 referred by 
fly and yeast, 310 referred by fly and mouse, 9 re-
ferred by worm and yeast, 36 referred by fly and 
worm, 5 referred by mouse and worm). However, 
comparing to the total of 109,844 citations, there 
were less than 1% of citations with multiple cate-
gories. For simplicity, we defined our categoriza-
tion task as a single category text categorization 
task.  
Table 1. The number of citations for training 
(Tra), testing (Te) for each year. Note that some 
fields in certain MEDLINE citations may be 
empty (e.g., not all references have abstracts), the 
number of these non-applicable citations for fea-
 
3.2 Methods 
We studied Taxonomy from NCBI 
(http://www.ncbi.nlm.nih.gov) and UMLS knowl-
edge sources (http://umlsks.nlm.nih.gov) and de-
rived a list of keywords for each organism and 
used them to retrieve relevant articles. If the title, 
the abstract or Mesh Headings of a MEDLINE ci-
tation contains these keywords, we considered it as 
a relevant article. Table 1 shows the list of key-
words we obtained for each model organism.  
MEDLINE citations also contain other informa-
tion such as Authors, Mesh Headings, and Journals 
etc besides abstracts and titles. Based on the intui-
tions that biologists tend to use the same organism 
for their research and a specific journal tend to 
publish papers in a limited number of areas, we 
also evaluated Authors and Journals as features. 
Additionally, Mesh Headings which were assigned 
manually by librarians to index papers represent 
key information of the papers, we also evaluated 
the categorization power of Mesh Headings in de-
termining which organism the paper belongs to. 
We then combined some or all features together 
and evaluated the prediction power. 
3
F
cl
ye
T
th
fo
frture representations abstracts (AbT), titles (ArT), 
authors (Aut), Journals (Jou), and Mesh Headings
(MH) for each year.
MOUSE Mouse, mice, mus muscaris, 
mus musculus, mus sp 
YEAST Saccharomyces, yeast, yeasts, 
candida robusta, oviformis, 
italicus, capensis, uvarum, ere-
visiae 
FLY drosophila, fly, flies 
WORM Elegans, worm, worms 
Table 2. Keywords used to retrieve relevant 
articles for four model organisms mouse, 
yeast, fly and worm. 
.3 Experiments 
or each year from 1990 to 2003, we trained a 
assifier using citations published in all previous 
ars and tested using citations in the current year. 
able 2 lists the detail about the training set and 
e test set for each year. We experimented the 
llowing feature representations: stemmed words 
om AbstractText, stemmed words from Title, 
Author, MeshHeading, and Journals. Since some 
of the MEDLINE fields may be empty (such as 
some citations do not contain abstracts), Table 2 
also provides the number of non-applicable refer-
ences each year for a given feature representation 
method. From Table 2, we found that every cita-
tion has a title. However, there are about 6.4% of 
citations (5,647 out of 88,281) that do not have 
abstracts. For each feature representation, we ap-
plied three supervised learning algorithms (i.e., 
Na?ve Bayes learning, Decision List learning, Sup-
port Vector Machine). 
For each combination of machine learning algo-
rithm and feature representation, we computed the 
performance using the F-measure, which is defined 
as 2*P*R/(P+R), where P is the precision (the 
number of citations predicted correctly to the total 
number of citations being predicted) and R is the 
recall (the number of citations predicted correctly 
to the total number of citations). 
We then sorted the feature representations ac-
cording to their F-measures and gradually com-
bined them into several complex feature 
representations. The feature vector of a complex 
feature representation is formed by simply combin-
ing the feature vector of its members. For example, 
suppose the feature vector of feature representation 
using stemmed words from the title contains an 
element A and the feature vector of feature repre-
sentation using stemmed words from the abstract 
contains an element B, then the feature vector of 
the complex representation obtained by combining 
stemmed words from title and stemmed words 
from abstracts will contain the two elements: Title: 
A and Abstract: B. These feature representations 
were then combined with the machine learning 
algorithm that has the best overall performance to 
build text categorization classifiers. Similarly, we 
evaluated these complex feature representations 
using citations published in all previous years as 
training citations and tested using citations pub-
lished in the current year. 
4 Results and Discussion 
Table 3 shows the detail F-measure obtained for 
each combination of machine learning algorithm, 
year, and feature representation. Among them, 
Support Vector machine along with stemmed 
words in abstracts achieved the best F-measure 
(i.e., 90.5%). Decision list learning along with 
stemmed words in titles achieved the second best 
F-measure (i.e., 90.1%). Feature representation 
using Mesh Headings along with Decision list 
learning or Support Vector machine has the third 
best F-measure (i.e., 88.7%). Feature representa-
tion using Author combined with Support Vector 
Machine has an F-measure of 71.8%. Feature rep-
resentation using Journals has the lowest F-
measure (i.e., 62.1%). From Table 3, we can see 
that Support Vector Machine has the best perform-
ance for almost each feature representation.  
Note that the results for feature representation 
Authors were significantly worse for year 2002. 
After reviewing some citations, we found that the 
format of the author field has changed since year 
2002 in MEDLINE citations. The current format 
results in less ambiguity among authors. However, 
we could not use the author fields of citations from 
previous years to predicate the category of docu-
ments for year 2002. Also, since a lot of citations 
in years 2002 and 2003 are in-process citations 
(i.e., people are still working on indexing these 
citations using Mesh Headings), feature representa-
tion using Mesh Headings had worse performance 
in these two years comparing to other years.  
According to the reported performance, we ex-
plored the following feature representations: i). 
stemmed words from titles and stemmed words 
from abstracts, ii) Mesh Headings, stemmed words 
from titles, and stemmed words from abstracts, iii) 
Authors, Mesh Headings, stemmed words from 
titles, and stemmed words from abstracts, and iv) 
Journals, Authors, Mesh Headings, stemmed words 
from titles and stemmed words from abstracts. 
Figure 2 shows the performance of these feature 
representations when using support vector machine 
as machine learning algorithm. Note that the F-
measures for complex feature representations that 
contain Abstract, Title, and Mesh Headings are 
indistinguishable. The inclusion of addition fea-
tures such as Authors or Journals does not improve 
F-measure visibly. Figure 2 also includes the meas-
ure for keyword retrieving, which is different from 
the measure for each complex feature representa-
tion. The performance of keyword retrieving is 
measured using the ratio of the number of citations 
in each organism that contain keywords from the 
list of keywords obtained for that organism to the 
total number of citations for the organism. The 
measure for each complex feature representation is 
the F-measure obtained using support vector ma. 
MeshHeading Journal Author AbstractText ArticleTitle Year  
  DLL NBL SVM DLL NBL SVM DLL NBL SVM DLL NBL SVM DLL NBL SVM 
1990 94.3 88.8 93.7 56.4 56.1 58.5 82.6 80.9 84.7 89.1 88.7 91.8 94.5 90.4 94.4 
1991 92.7 88.3 93.5 55.4 56.6 57.3 81.3 77.9 83.4 88.7 88.4 91.7 92.2 89.5 92.3 
1992 92.7 88.4 92.7 55.0 59.8 57.2 76.8 71.2 78.6 88.1 88.6 91.8 91.9 89.9 91.7 
1993 93.0 88.6 92.9 60.0 60.3 58.6 76.8 70.6 76.2 87.5 87.1 91.0 91.5 89.1 91.3 
1994 93.0 89.7 92.9 61.1 61.9 60.6 74.0 66.8 74.2 88.9 88.9 91.0 92.3 89.7 91.5 
1995 93.3 90.7 92.5 63.2 63.4 63.1 76.5 67.2 73.3 88.0 88.6 91.2 92.0 89.2 89.4 
1996 92.0 88.9 90.8 64.1 64.2 63.8 75.7 65.4 74.1 85.8 87.2 90.0 90.8 88.5 87.8 
1997 90.6 87.5 90.5 63.9 64.0 64.4 75.8 65.2 72.8 85.1 86.0 89.7 89.8 86.7 87.8 
1998 90.6 87.6 91.2 61.1 62.1 61.8 76.0 65.4 73.7 84.1 86.4 89.8 89.7 85.6 87.9 
1999 89.7 87.1 88.9 61.8 63.3 62.4 73.3 64.1 69.5 84.3 86.2 89.6 88.5 85.3 85.8 
2000 88.4 84.2 88.0 60.8 61.0 62.4 74.0 63.0 71.0 83.5 85.7 88.9 87.7 84.2 86.5 
2001 87.0 84.9 87.7 62.4 62.7 62.4 74.4 62.5 68.3 85.0 86.8 91.0 89.1 85.3 87.2 
2002 63.8 19.9 67.3 62.7 64.5 63.6 3.8 8.8 53.9 86.2 88.2 91.7 88.7 84.8 86.4 
2003 77.6 76.0 76.0 48.0 48.0 54.5 62.2 53.1 63.1 85.3 89.0 92.8 83.3 87.0 83.7 
Overall 88.7 82.1 88.8 61.3 62.1 61.8 69.3 61.6 71.8 86.0 87.2 90.5 90.1 87.0 88.5 
Keyword
Abstract
Table 3. The F-measure of supervised text categorization study on different combination of super-
vised machine learning algorithms and feature representations. The classifiers trained using citations 
published previous years and tested using citations published in the current year.  
F
V
t
N
s
lAbstact+Title
Abstract+Title+MeshHeading
Abstract+Title+MeshHeading+Author
Abstract+Title+MeshHeading+Author+Journal80%
82%
84%
86%
88%
90%
92%
94%
96%
98%
100%
19
90
19
91
19
92
19
93
19
94
19
95
19
96
19
97
19
98
19
99
20
00
20
01
20
02
20
03
igure 2. The F-measure of classifiers using complex feature representations learned using Support 
ector Machine and the percentage of the number of citations containing keywords associated with 
he corresponding organism comparing to the total number of citations associated with that organism. 
ote that F-measures for complex feature representations Abstract+Title+MeshHeading, Ab-
tract+Title+MeshHeading+Author, and Abstract+Title+MeshHeading+Author+Journal are over-
apped with each other.  
chine which was trained using citations from all 
previous years and tested using citations in current 
year. 
From the study, we answered at least partially 
the questions. We cannot just simply use keywords 
to retrieve MEDLINE citations for model organism 
databases. From Figure 2, we can see that using 
keywords to retrieve citations may miss 20% of the 
citations. However, when combining all feature 
representation together, using citations from previ-
ous years could correctly predict to which organ-
ism the current year citations belong with an 
overall F-Measure of 94.1%. 
For the supervised learning on text categoriza-
tion task, different MEDLINE citation fields have 
different power on predicting to which model or-
ganism the paper belongs. Feature representation 
using stemmed words from abstracts has the most 
stable and highest predicting power with an overall 
F-measure of 90.5%. Authors alone can predict the 
category with an overall F-measure of 71.8%. 
Among three supervised machine learning algo-
rithms, support vector machine achieves the best 
performance. For feature representations where 
there are only a few features in a feature vector 
with non-zero values, decision list learning 
achieved comparable performance with (some-
times superior than) support vector machine. For 
example, decision list learning achieved an F-
measure of 90.1% when using stemmed words 
from titles as feature representation method, which 
is superior than support vector machine (with an F- 
measure of 88.5%). Consistent with our findings in 
(Liu, 2004), the performance of Na?ve Bayes learn-
ing is very unstable. For example, when using 
stemmed words from abstracts, the performance of 
Na?ve Bayes learning is comparable to the other 
two machine learning algorithms. However, when 
using Mesh Headings as feature representation 
methods, the performance of Na?ve Bayes learning 
(with an F-measure of 82.1%) is much worse than 
decision list learning and support vector machine 
(with F-measures of over 88.0%).  
One limitation of the study is that we used only 
abstracts that are about one of the four model or-
ganisms. The evaluation would be more meaning-
ful if we could include abstracts that are outside of 
these four model organisms. However, such 
evaluation would involve human experts since we 
can not grantee that abstracts that are not included 
in these four model organism databases are not 
about one of the four model organisms. That is also 
the reason we cannot provide F-measures when we 
evaluated the performance of keyword retrieving 
since we cannot grantee that abstracts associated 
with one organism are not related to another organ-
ism since the list of references in each organism 
database is not complete. 
We could use previous published articles to-
gether with their categories to predict categories of 
the current articles where the list of categories is 
not limited to model organisms. It could be other 
categories such as the main themes for each para-
graph in each paper. We will conduct a serial of 
studies on text categorization in the biomedical 
literature under the condition of the availability of 
category-labeled examples. One future project 
would be to apply text categorization on citation 
information for the protein family classification 
and annotation in Protein Information Resources 
(Wu, 2003). 
As we know, homologous genes are usually 
represented in text using the same terms. Knowing 
to which organism the paper belongs can reduce 
the ambiguity of biological entity terms. For ex-
ample, if we know the paper is related to mouse, 
we can use entities that are specific to mouse for 
biological entity tagging. Future work will be 
combining text categorization with the task of bio-
logical entity tagging to reduce the ambiguity of 
biological entity names.  
5 Conclusion 
In this paper, we designed a study using existing 
reference information available at four well-known 
model organism databases and investigated the 
problem of identifying relevant articles for these 
organisms using MEDLINE. We compared the 
results obtained using keyword searching with su-
pervised machine learning techniques. We found 
out that keyword searching retrieved about 80% of 
the citations. When using supervised machine 
learning techniques, the overall F-measure of the 
best classifier is around 94.1%. Future work would 
be applying the supervised machine learning tech-
nique to the whole MEDLINE citation to retrieve 
relevant articles. Also we plan to apply text clus-
tering techniques or text categorization techniques 
for the routing problem inside a specific model 
organism database (such as routing to curators in a 
specific area). 
Acknowledgement 
We thank anonymous referees for their valuable com-
ments and insights. This work was supported in part by 
grant EIA-031 from the National Science Foundation.  
 
References 
The FlyBase Consortium. The Flybase database of the 
Drosophila genome projects and community litera-
ture, Nucleic Acid Research 2003, Vol 31 (1) 172-
175 
Blake J, Richardson J.E., Bult C.J., Kadin J.A., Eppig 
J.T. MGD: the Mouse Genome Database, Nucleic 
Acid Research, 2003, Vol 31 (1) 193-195 
Donaldson I, Martin J, de Bruijn B, et al PreBIND and 
Textomy--mining the biomedical literature for pro-
tein-protein interactions using a support vector ma-
chine  BMC Bioinformatics. 2003 Mar 27; 4(1): 11.  
Duda R, Hart P. ?Pattern Classification and Scene 
Analysis?. John Wiley and Sons, NY, 1973. 
Harris,T.W., Lee,R., Schwarz,E., et al WormBase: a 
cross-species database for comparative genomics. 
Nucleic Acids Research 2003, Vol 31, 133?137. 
Iliopoulos I, Enright AJ, Ouzounis CA. Textquest: 
document clustering of Medline abstracts for concept 
discovery in molecular biology, Pac Symp Biocom-
put. 2001; 384-95.  
Iwayama M, Tokunaga T. Cluster-based text categori-
zation: a comparison of category search strategies, 
SIGIR 1995, 273-281 
Joachims T. Text categorization with support vector 
machines: learning with many relevant features, 
ECML 1998, 137-142 
Liu H, V. Teller, C. Friedman. A Multi-Variable Com-
parison Study of Supervised Word Sense Disam-
biguation. Submitted to JAMIA 
Misra S, Madeline A.C., Mungall C.J., et al Annotation 
of the Drosophila melanogaster euchromatic ge-
nome: a systematic review, Genome Biology 2002, 3 
(12) 
Sebastiani F. Machine learning in automated text cate-
gorization. ACM Computing Surveys, 2002, Vol 34 
(1) 1-47 
Vapnik. Statistical Learning Theory John Wiley and 
Sons, NY, 1998 
Wiener ED, Pedersen JO, Weigend AS. A neural net-
work approach to topic spotting, SDAIR 1995, 317-
332 
Wu CH, L Yeh, H Huang, L Arminski, et al The Pro-
tein Information Recource Nucleic Acids Research, 
31: 345-347 
Yang Y, Liu X. A re-examination of text categorization 
methods, SIGIR 1999, 42-49  
Yarowsky D. Decision lists for lexical ambiguity resolu-
tion: Application to accent restoration in Spanish 
and French. 1994; ACL 32: 88-95 
 
 
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 148?154, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
MayoClinicNLP?CORE: Semantic representations for textual similarity
Stephen Wu
Mayo Clinic
Rochester, MN 55905
wu.stephen@mayo.edu
Dongqing Zhu & Ben Carterette
University of Delaware
Newark, DE 19716
{zhu,carteret}@cis.udel.edu
Hongfang Liu
Mayo Clinic
Rochester, MN 55905
liu.hongfang@mayo.edu
Abstract
The Semantic Textual Similarity (STS) task
examines semantic similarity at a sentence-
level. We explored three representations of
semantics (implicit or explicit): named enti-
ties, semantic vectors, and structured vectorial
semantics. From a DKPro baseline, we also
performed feature selection and used source-
specific linear regression models to combine
our features. Our systems placed 5th, 6th, and
8th among 90 submitted systems.
1 Introduction
The Semantic Textual Similarity (STS) task (Agirre
et al, 2012; Agirre et al, 2013) examines semantic
similarity at a sentence-level. While much work has
compared the semantics of terms, concepts, or doc-
uments, this space has been relatively unexplored.
The 2013 STS task provided sentence pairs and a
0?5 human rating of their similarity, with training
data from 5 sources and test data from 4 sources.
We sought to explore and evaluate the usefulness
of several semantic representations that have had
recent significance in research or practice. First,
information extraction (IE) methods often implic-
itly consider named entities as ad hoc semantic rep-
resentations, for example, in the clinical domain.
Therefore, we sought to evaluate similarity based on
named entity-based features. Second, in many appli-
cations, an effective means of incorporating distri-
butional semantics is Random Indexing (RI). Thus
we consider three different representations possi-
ble within Random Indexing (Kanerva et al, 2000;
Sahlgren, 2005). Finally, because compositional
distributional semantics is an important research
topic (Mitchell and Lapata, 2008; Erk and Pado?,
2008), we sought to evaluate a principled compo-
sition strategy: structured vectorial semantics (Wu
and Schuler, 2011).
The remainder of this paper proceeds as follows.
Section 2 overviews our similarity metrics, and Sec-
tion 3 overviews the systems that were defined on
these metrics. Competition results and additional
analyses are in Section 4. We end with discussion
on the results in Section 5.
2 Similarity measures
Because we expect semantic similarity to be multi-
layered, we expect that we will need many similar-
ity measures to approximate human similarity judg-
ments. Rather than reinvent the wheel, we have cho-
sen to introduce features that complement existing
successful feature sets. We utilized 17 features from
DKPro Similarity and 21 features from TakeLab,
i.e., the two top-performing systems in the 2012 STS
task, as a solid baseline.
These are summarized in Table 1. We introduce 3
categories of new similarity metrics, 9 metrics in all.
2.1 Named entity measures
Named entity recognition provides a common ap-
proximation of semantic content for the informa-
tion extraction perspective. We define three simple
similarity metrics based on named entities. First,
we computed the named entity overlap (exact string
matches) between the two sentences, where NEk
was the set of named entities found in sentence
Sk. This is the harmonic mean of how closely S1
148
Table 1: Full feature pool in MayoClinicNLP systems. The proposed MayoClinicNLP metrics are meant to comple-
ment DKPro (Ba?r et al, 2012) and TakeLab (?Saric? et al, 2012) metrics.
DKPro metrics (17) TakeLab metrics (21) Custom MayoClinicNLP metrics (9)
n-grams/WordNGramContainmentMeasure 1 stopword-filtered t ngram/UnigramOverlap
n-grams/WordNGramContainmentMeasure 2 stopword-filtered t ngram/BigramOverlap
n-grams/WordNGramJaccardMeasure 1 t ngram/TrigramOverlap
n-grams/WordNGramJaccardMeasure 2 stopword-filtered t ngram/ContentUnigramOverlap
n-grams/WordNGramJaccardMeasure 3 t ngram/ContentBigramOverlap
n-grams/WordNGramJaccardMeasure 4 t ngram/ContentTrigramOverlap
n-grams/WordNGramJaccardMeasure 4 stopword-filtered
t words/WeightedWordOverlap custom/StanfordNerMeasure overlap.txt
t words/GreedyLemmaAligningOverlap custom/StanfordNerMeasure aligngst.txt
t words/WordNetAugmentedWordOverlap custom/StanfordNerMeasure alignlcs.txt
esa/ESA Wiktionary t vec/LSAWordSimilarity NYT custom/SVSePhrSimilarityMeasure.txt
esa/ESA WordNet t vec/LSAWordSimilarity weighted NYT custom/SVSeTopSimilarityMeasure.txt
t vec/LSAWordSimilarity weighted Wiki custom/SemanticVectorsSimilarityMeasure d200 wr0.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6b.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6d.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6p.txt
n-grams/CharacterNGramMeasure 2 t other/RelativeLengthDifference
n-grams/CharacterNGramMeasure 3 t other/RelativeInfoContentDifference
n-grams/CharacterNGramMeasure 4 t other/NumbersSize
string/GreedyStringTiling 3 t other/NumbersOverlap
string/LongestCommonSubsequenceComparator t other/NumbersSubset
string/LongestCommonSubsequenceNormComparator t other/SentenceSize
string/LongestCommonSubstringComparator t other/CaseMatches
t other/StocksSize
t other/StocksOverlap
matches S2, and how closely S2 matches S1:
simneo(S1, S2) = 2 ? ?NE1 ?NE2??NE1? + ?NE2? (1)
Additionally, we relax the constraint of requiring
exact string matches between the two sentences by
using the longest common subsequence (Allison and
Dix, 1986) and greedy string tiling (Wise, 1996) al-
gorithms. These metrics give similarities between
two strings, rather than two sets of strings as we
have with NE1 and NE2. Thus, we follow previ-
ous work in greedily aligning these named entities
(Lavie and Denkowski, 2009; ?Saric? et al, 2012) into
pairs. Namely, we compare each pair (nei,1, nej,2)
of named entity strings in NE1 and NE2. The
highest-scoring pair is entered into a set of pairs, P .
Then, the next highest pair is added to P if neither
named entity is already in P , and discarded other-
wise; this continues until there are no more named
entities in either NE1 or NE2.
We then define two named entity aligning mea-
sures that use the longest common subsequence
(LCS) and greedy string tiling (GST) fuzzy string
matching algorithms:
simnea(S1, S2) =
?
(ne1,ne2)?P
f(ne1, ne2)
max (?NE1?, ?NE2?)
(2)
where f(?) is either the LCS or GST algorithm.
In our experiments, we performed named entity
recognition with the Stanford NER tool using the
standard English model (Finkel et al, 2005). Also,
we used UKP?s existing implementation of LCS and
GST (?Saric? et al, 2012) for the latter two measures.
2.2 Random indexing measures
Random indexing (Kanerva et al, 2000; Sahlgren,
2005) is another distributional semantics framework
for representing terms as vectors. Similar to LSA
(Deerwester et al, 1990), an index is created that
represents each term as a semantic vector. But
in random indexing, each term is represented by
an elemental vector et with a small number of
randomly-generated non-zero components. The in-
tuition for this means of dimensionality reduction is
that these randomly-generated elemental vectors are
like quasi-orthogonal bases in a traditional geomet-
ric semantic space, rather than, e.g., 300 fully or-
thogonal dimensions from singular value decompo-
sition (Landauer and Dumais, 1997). For a standard
model with random indexing, a contextual term vec-
tor ct,std is the the sum of the elemental vectors cor-
responding to tokens in the document. All contexts
for a particular term are summed and normalized to
produce a final term vector vt,std.
Other notions of context can be incorporated into
149
this model. Local co-occurrence context can be ac-
counted for in a basic sliding-window model by con-
sidering words within some window radius r (in-
stead of a whole document). Each instance of the
term t will have a contextual vector ct,win = et?r +
? + et?1 + et+1 +? + et+r; context vectors for each
instance (in a large corpus) would again be added
and normalized to create the overall vector vt,win.
A directional model doubles the dimensionality of
the vector and considers left- and right-context sepa-
rately (half the indices for left-context, half for right-
context), using a permutation to achieve one of the
two contexts. A permutated positional model uses a
position-specific permutation function to encode the
relative word positions (rather than just left- or right-
context) separately. Again, vt would be summed
and normalized over all instances of ct.
Sentence vectors from any of these 4 Random
Indexing-based models (standard, windowed, direc-
tional, positional) are just the sum of the vectors for
each term vS = ?t?S vt. We define 4 separate simi-
larity metrics for STS as:
simRI(S1, S2) = cos(vS1,vS2) (3)
We used the semantic vectors package (Widdows
and Ferraro, 2008; Widdows and Cohen, 2010) in
the default configuration for the standard model. For
the windowed, directional, and positional models,
we used a 6-word window radius with 200 dimen-
sions and a seed length of 5. All models were
trained on the raw text of the Penn Treebank Wall
Street Journal corpus and a 100,075-article subset of
Wikipedia.
2.3 Semantic vectorial semantics measures
Structured vectorial semantics (SVS) composes dis-
tributional semantic representations in syntactic
context (Wu and Schuler, 2011). Similarity met-
rics defined with SVS inherently explore the quali-
ties of a fully interactive syntax?semantics interface.
While previous work evaluated the syntactic contri-
butions of this model, the STS task allows us to eval-
uate the phrase-level semantic validity of the model.
We summarize SVS here as bottom-up vector com-
position and parsing, then continue on to define the
associated similarity metrics.
Each token in a sentence is modeled generatively
as a vector e? of latent referents i? in syntactic con-
text c? ; each element in the vector is defined as:
e?[i?] = P(x? ? lci?), for preterm ? (4)
where l? is a constant for preterminals.
We write SVS vector composition between two
word (or phrase) vectors in linear algebra form,1 as-
suming that we are composing the semantics of two
children e? and e? in a binary syntactic tree into
their parent e? :
e? = M? (L??? ? e?)? (L??? ? e?) ? 1 (5)
M is a diagonal matrix that encapsulates probabilis-
tic syntactic information; the L matrices are linear
transformations that capture how semantically rele-
vant child vectors are to the resulting vector (e.g.,
L??? defines the the relevance of e? to e?). These
matrices are defined such that the resulting e? is a
semantic vector of consistent P(x? ? lci?) probabil-
ities. Further detail is in our previous work (Wu,
2010; Wu and Schuler, 2011).
Similarity metrics can be defined in the SVS
space by comparing the distributions of the com-
posed e? vectors ? i.e., our similarity metric is
a comparison of the vector semantics at different
phrasal nodes. We define two measures, one cor-
responding to the top node c? (e.g., with a syntactic
constituent c? = ?S?), and one corresponding to the
left and right largest child nodes (e.g.,, c? = ?NP?
and c ?= ?VP? for a canonical subject?verb?object
sentence in English).
simsvs-top(S1, S2) = cos(e?(S1),e?(S2)) (6)
simsvs-phr(S1, S2) =max(
avgsim(e?(S1),e?(S2);e ?(S1),e ?(S2)),
avgsim(e?(S1),e ?(S2);e ?(S1),e?(S2))) (7)
where avgsim() is the harmonic mean of the co-
sine similarities between the two pairs of arguments.
Top-level similarity comparisons in (6) amounts to
comparing the semantics of a whole sentence. The
phrasal similarity function simsvs-phr(S1, S2) in (7)
thus seeks to semantically align the two largest sub-
trees, and weight them. Compared to simsvs-top,
1We define the operator ? as point-by-point multiplication
of two diagonal matrices and 1 as a column vector of ones, col-
lapsing a diagonal matrix onto a column vector.
150
the phrasal similarity function simsvs-phr(S1, S2) as-
sumes there might be some information captured in
the child nodes that could be lost in the final compo-
sition to the top node.
In our experiments, we used the parser described
in Wu and Schuler (2011) with 1,000 headwords
and 10 relational clusters, trained on the Wall Street
Journal treebank.
3 Feature combination framework
The similarity metrics of Section 2 were calculated
for each of the sentence pairs in the training set, and
later the test set. In combining these metrics, we ex-
tended a DKPro Similarity baseline (3.1) with fea-
ture selection (3.2) and source-specific models and
classification (3.3).
3.1 Linear regression via DKPro Similarity
For our baseline (MayoClinicNLPr1wtCDT), we
used the UIMA-based DKPro Similarity system
from STS 2012 (Ba?r et al, 2012). Aside from the
large number of sound similarity measures, this pro-
vided linear regression through the WEKA package
(Hall et al, 2009) to combine all of the disparate
similarity metrics into a single one, and some pre-
processing. Regression weights were determined on
the whole training set for each source.
3.2 Feature selection
Not every feature was included in the final linear re-
gression models. To determine the best of the 47
(DKPro?17, TakeLab?21, MayoClinicNLP?9) fea-
tures, we performed a full forward-search on the
space of similarity measures. In forward-search, we
perform 10-fold cross-validation on the training set
for each measure, and pick the best one; in the next
round, that best metric is retained, and the remaining
metrics are considered for addition. Rounds con-
tinue until all the features are exhausted, though a
stopping-point is noted when performance no longer
increases.
3.3 Subdomain source models and
classification
There were 5 sources of data in the training set:
paraphrase sentence pairs (MSRpar), sentence pairs
from video descriptions (MSRvid), MT evaluation
sentence pairs (MTnews and MTeuroparl) and gloss
pairs (OnWN). In our submitted runs, we trained
a separate, feature-selected model based on cross-
validation for each of these data sources. In train-
ing data on cross-validation tests, training domain-
specific models outperformed training a single con-
glomerate model.
In the test data, there were 4 sources, with 2
appearing in training data (OnWN, SMT) and 2
that were novel (FrameNet/Wordnet sense defini-
tions (FNWN), European news headlines (head-
lines)). We examined two different strategies for ap-
plying the 5-source trained models on these 4 test
sets. Both of these strategies rely on a multiclass
random forest classifier, which we trained on the 47
similarity metrics.
First, for each sentence pair, we considered the
final similarity score to be a weighted combination
of the similarity score from each of the 5 source-
specific similarity models. The combination weights
were determined by utilizing the classifier?s confi-
dence scores. Second, the final similarity was cho-
sen as the single source-specific similarity score cor-
responding to the classifier?s output class.
4 Evaluation
The MayoClinicNLP team submitted three systems
to the STS-Core task. We also include here a post-
hoc run that was considered as a possible submis-
sion.
r1wtCDT This run used the 47 metrics from
DKPro, TakeLab, and MayoClinicNLP as a
feature pool for feature selection. Source-
specific similarity metrics were combined with
classifier-confidence-score weights.
r2CDT Same feature pool as run 1. Best-match (as
determined by classifier) source-specific simi-
larity metric was used rather than a weighted
combination.
r3wtCD TakeLab features were removed from the
feature pool (before feature selection). Same
source combination as run 1.
r4ALL Post-hoc run using all 47 metrics, but train-
ing a single linear regression model rather than
source-specific models.
151
Table 2: Performance comparison.
TEAM NAME headlines rank OnWNrank FNWNrank SMT rank mean rank
UMBC EBIQUITY-ParingWords 0.7642 0.7529 0.5818 0.3804 0.6181 1
UMBC EBIQUITY-galactus 0.7428 0.7053 0.5444 0.3705 0.5927 2
deft-baseline 0.6532 0.8431 0.5083 0.3265 0.5795 3
MayoClinicNLP-r4ALL 0.7275 0.7618 0.4359 0.3048 0.5707
UMBC EBIQUITY-saiyan 0.7838 0.5593 0.5815 0.3563 0.5683 4
MayoClinicNLP-r3wtCD 0.6440 43 0.8295 2 0.3202 47 0.3561 17 0.5671 5
MayoClinicNLP-r1wtCDT 0.6584 33 0.7775 4 0.3735 26 0.3605 13 0.5649 6
CLaC-RUN2 0.6921 0.7366 0.3793 0.3375 0.5587 7
MayoClinicNLP-r2CDT 0.6827 23 0.6612 20 0.396 17 0.3946 5 0.5572 8
NTNU-RUN1 0.7279 0.5952 0.3215 0.4015 0.5519 9
CLaC-RUN1 0.6774 0.7667 0.3793 0.3068 0.5511 10
4.1 Competition performance
Table 2 shows the top 10 runs of 90 submitted in
the STS-Core task are shown, with our three sys-
tems placing 5th, 6th, and 8th. Additionally, we can
see that run 4 would have placed 4th. Notice that
there are significant source-specific differences be-
tween the runs. For example, while run 4 is better
overall, runs 1?3 outperform it on all but the head-
lines and FNWN datasets, i.e., the test datasets that
were not present in the training data. Thus, it is
clear that the source-specific models are beneficial
when the training data is in-domain, but a combined
model is more beneficial when no such training data
is available.
4.2 Feature selection analysis
0 10 20 30 40
0.
60
0.
65
0.
70
0.
75
0.
80
0.
85
0.
90
Step
Pe
a
rs
o
n
?s
 C
or
re
la
tio
n 
Co
ef
fic
ie
nt
MSRpar
MSRvid
SMTeuroparl
OnWN
SMTnews
ALL
Figure 1: Performance curve of feature selection for
r1wtCDT, r2CDT, and r4ALL
Due to the source-specific variability among the
runs, it is important to know whether the forward-
search feature selection performed as expected. For
source specific models (runs 1 and 3) and a com-
bined model (run 4), Figure 1 shows the 10-fold
cross-validation scores on the training set as the next
feature is added to the model. As we would ex-
pect, there is an initial growth region where the first
features truly complement one another and improve
performance significantly. A plateau is reached for
each of the models, and some (e.g., SMTnews) even
decay if too many noisy features are added.
The feature selection curves are as expected. Be-
cause the plateau regions are large, feature selection
could be cut off at about 10 features, with gains in
efficiency and perhaps little effect on accuracy.
The resulting selected features for some of the
trained models are shown in Table 3.
4.3 Contribution of MayoClinicNLP metrics
We determined whether including MayoClinicNLP
features was any benefit over a feature-selected
DKPro baseline. Table 4 analyzes this question
by adding each of our measures in turn to a base-
line feature-selected DKPro (dkselected). Note that
this baseline was extremely effective; it would have
ranked 4th in the STS competition, outperforming
our run 4. Thus, metrics that improve this baseline
must truly be complementary metrics. Here, we see
that only the phrasal SVSmeasure is able to improve
performance overall, largely by its contributions to
the most difficult categories, FNWN and SMT. In
fact, that system (dkselected + SVSePhrSimilari-
tyMeasure) represents the best-performing run of
any that was produced in our framework.
152
Table 3: Top retained features for several linear regression models.
OnWN - r1wtCDT and r2CDT (15 shown/19 selected) SMTnews - r1wtCDT and r2CDT (15 shown/17 selected) All - r4ALL (29 shown/29 selected)
t ngram/ContentUnigramOverlap t other/RelativeInfoContentDifference t vec/LSAWordSimilarity weighted NYT
t other/RelativeInfoContentDifference n-grams/CharacterNGramMeasure 2 n-grams/CharacterNGramMeasure 2
t vec/LSAWordSimilarity weighted NYT t other/CaseMatches string/LongestCommonSubstringComparator
esa/ESA Wiktionary string/GreedyStringTiling 3 t other/NumbersOverlap
t ngram/ContentBigramOverlap custom/RandomIndexingMeasure d200 wr6p t words/WordNetAugmentedWordOverlap
n-grams/CharacterNGramMeasure 2 custom/StanfordNerMeasure overlap n-grams/WordNGramJaccardMeasure 1
t words/WordNetAugmentedWordOverlap t vec/LSAWordSimilarity weighted NYT n-grams/CharacterNGramMeasure 3
t ngram/BigramOverlap t other/SentenceSize t other/SentenceSize
string/GreedyStringTiling 3 custom/RandomIndexingMeasure d200 wr0 t other/RelativeInfoContentDifference
string/LongestCommonSubsequenceNormComparator custom/SVSePhrSimilarityMeasure t ngram/ContentBigramOverlap
custom/RandomIndexingMeasure d200 wr0 esa/ESA Wiktionary n-grams/WordNGramJaccardMeasure 4
custom/StanfordNerMeasure aligngst string/LongestCommonSubstringComparator t other/NumbersSize
custom/StanfordNerMeasure alignlcs t other/NumbersSize t other/NumbersSubset
custom/StanfordNerMeasure overlap n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/SVSePhrSimilarityMeasure
custom/SVSePhrSimilarityMeasure custom/SVSeTopSimilarityMeasure custom/SemanticVectorsSimilarityMeasure d200 wr6p
esa/ESA WordNet
OnWN - r3wtCD (7 shown/7 selected) SMTnews - r3wtCD (15 shown/23 selected) esa/ESA Wiktionary
esa/ESA Wiktionary string/GreedyStringTiling 3 string/LongestCommonSubsequenceComparator
string/LongestCommonSubsequenceComparator custom/StanfordNerMeasure overlap string/LongestCommonSubsequenceNormComparator
string/GreedyStringTiling 3 n-grams/CharacterNGramMeasure 2 n-grams/WordNGramContainmentMeasure 1 stopword-filtered
string/LongestCommonSubsequenceNormComparator custom/RandomIndexingMeasure d200 wr6p word-sim/MCS06 Resnik WordNet
string/LongestCommonSubstringComparator n-grams/CharacterNGramMeasure 3 t ngram/ContentUnigramOverlap
word-sim/MCS06 Resnik WordNet string/LongestCommonSubsequenceComparator n-grams/WordNGramContainmentMeasure 2 stopword-filtered
n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/StanfordNerMeasure aligngst n-grams/WordNGramJaccardMeasure 2 stopword-filtered
custom/SVSePhrSimilarityMeasure t ngram/UnigramOverlap
esa/ESA Wiktionary t ngram/BigramOverlap
esa/ESA WordNet t other/StocksSize
n-grams/WordNGramContainmentMeasure 2 stopword-filtered t words/GreedyLemmaAligningOverlap
n-grams/WordNGramJaccardMeasure 1 t other/StocksOverlap
string/LongestCommonSubstringComparator
custom/RandomIndexingMeasure d200 wr6d
custom/RandomIndexingMeasure d200 wr0
Table 4: Adding customized features one at a time into optimized DKPro feature set. Models are trained across all
sources.
headlines OnWN FNWN SMT mean
dkselected 0.70331 0.79752 0.38358 0.31744 0.571319
dkselected + SVSePhrSimilarityMeasure 0.70178 0.79644 0.38685 0.32332 0.572774
dkselected + RandomIndexingMeasure d200 wr0 0.70054 0.79752 0.38432 0.31615 0.570028
dkselected + SVSeTopSimilarityMeasure 0.69873 0.79522 0.38815 0.31723 0.569533
dkselected + RandomIndexingMeasure d200 wr6d 0.69944 0.79836 0.38416 0.31397 0.569131
dkselected + RandomIndexingMeasure d200 wr6b 0.69992 0.79788 0.38435 0.31328 0.568957
dkselected + RandomIndexingMeasure d200 wr6p 0.69878 0.79848 0.37876 0.31436 0.568617
dkselected + StanfordNerMeasure aligngst 0.69446 0.79502 0.38703 0.31497 0.567212
dkselected + StanfordNerMeasure overlap 0.69468 0.79509 0.38703 0.31466 0.567200
dkselected + StanfordNerMeasure alignlcs 0.69451 0.79486 0.38657 0.31394 0.566807
(dk + all custom) selected 0.70311 0.79887 0.37477 0.31665 0.570586
Also, we see some source-specific behavior. None
of our introduced measures are able to improve the
headlines similarities. However, random indexing
improves OnWN scores, several strategies improve
the FNWN metric, and simsvs-phr is the only viable
performance improvement on the SMT corpus.
5 Discussion
Mayo Clinic?s submissions to Semantic Textual
Similarity 2013 performed well, placing 5th, 6th,
and 8th among 90 submitted systems. We intro-
duced similarity metrics that used different means
to do compositional distributional semantics along
with some named entity-based measures, finding
some improvement especially for phrasal similar-
ity from structured vectorial semantics. Through-
out, we utilized forward-search feature selection,
which enhanced the performance of the models. We
also used source-based linear regression models and
considered unseen sources as mixtures of existing
sources; we found that in-domain data is neces-
sary for smaller, source-based models to outperform
larger, conglomerate models.
Acknowledgments
Thanks to the developers of the UKP DKPro sys-
tem and the TakeLab system for making their code
available. Also, thanks to James Masanz for initial
implementations of some similarity measures.
153
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385?393. Association for Computational
Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Lloyd Allison and Trevor I Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics-Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, pages 435?
440. Association for Computational Linguistics.
Scott Deerwester, Susan Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random indexing of text samples for latent se-
mantic analysis. In Proceedings of the 22nd annual
conference of the cognitive science society, volume
1036. Citeseer.
T.K. Landauer and S.T. Dumais. 1997. A Solution to
Plato?s Problem: The Latent Semantic Analysis The-
ory of Acquisition, Induction, and Representation of
Knowledge. Psychological Review, 104:211?240.
Alon Lavie and Michael J Denkowski. 2009. The meteor
metric for automatic evaluation of machine translation.
Machine translation, 23(2-3):105?115.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, OH.
M. Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Index-
ing Workshop at the 7th International Conference on
Terminology and Knowledge Engineering, TKE, vol-
ume 5.
Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Dominic Widdows and Trevor Cohen. 2010. The seman-
tic vectors package: New algorithms and public tools
for distributional semantics. In Semantic Computing
(ICSC), 2010 IEEE Fourth International Conference
on, pages 9?15. IEEE.
D. Widdows and K. Ferraro. 2008. Semantic vec-
tors: a scalable open source package and online tech-
nology management application. Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), pages 1183?1190.
Michael J Wise. 1996. Yap3: Improved detection of sim-
ilarities in computer program and other texts. In ACM
SIGCSE Bulletin, volume 28, pages 130?134. ACM.
StephenWu andWilliam Schuler. 2011. Structured com-
position of semantic vectors. In Proceedings of the In-
ternational Conference on Computational Semantics.
Stephen Tze-Inn Wu. 2010. Vectorial Representations
of Meaning for a Computational Model of Language
Comprehension. Ph.D. thesis, Department of Com-
puter Science and Engineering, University of Min-
nesota.
154
