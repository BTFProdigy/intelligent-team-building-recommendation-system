English-to-Korean Transliteration using Multiple Unbounded 
Overlapping Phoneme Chunks 
In-Ho Kang and Gi lChang Kim 
Department  of Computer  Science 
Korea Advanced Inst i tute of Science and Technology 
Abst ract  
We present in this paper the method of 
English-to-Korean(E-K) transliteration and 
back-transliteration. In Korean technical 
documents, many English words are translit- 
erated into Korean words in w~rious tbrms in 
diverse ways. As English words and Korean 
transliterations are usually technical terms and 
proper nouns, it; is hard to find a transliteration 
and its variations in a dictionary. Theretbre 
an automatic transliteration system is needed 
to find the transliterations of English words 
without manual intervention. 
3?.o explain E-K transliteration t)henomena, 
we use phoneme chunks that do not have a 
length limit. By aI)plying phoneme chunks, 
we combine different length intbrmation with 
easy. The E-K transliteration method h~m 
three stet)s. In the first, we make a t)honeme 
network that shows all possible transliterations 
of the given word. In the second step, we apply 
t)honeme chunks, extracted fl'om training data, 
to calculate the reliability of each possible 
transliteration. Then we obtain probable 
transliterations of the given English word. 
1 Introduct ion 
In Korean technical documents, many English 
words are used in their original forms. But 
sometimes they are transliterated into Korean 
in different forms. Ex. 1, 2 show the examples 
of w~rious transliterations in KTSET 2.0(Park 
et al, 1996). 
(1) data 
(a) l:~\] o\] >\]-(teyitha) \[1,033\] 1 
(b) r~l o\] >\] (teyithe) \[527\] 
1the frequency in KTSET 
(2) digital 
(a) ~\] x\] ~-(ticithul) \[254\] 
(b) qN~-(t ichithM)\[7\]  
(c) ~1 z\] ~ (ticithel) \[6\] 
These various transliterations are not negligi- 
ble tbr natural angnage processing, especially ill 
information retrieval. Because same words are 
treated as different ones, the calculation based 
(m tile frequency of word would produce mis- 
leading results. An experiment shows that the 
effectiveness of infbrmation retrieval increases 
when various tbrms including English words are 
treated eqnivMently(Jeong et al, 1997). 
We may use a dictionary, to tind a correct 
transliteratkm and its variations. But it is not 
fhasible because transliterated words are usually 
technical terms and proper nouns that have rich 
productivity. Therefore an automatic translit- 
eration system is needed to find transliterations 
without manual intervention. 
There have been some studies on E-K 
transliteration. They tried to explain translit- 
eration as tflloneme-lmr-phoneme or alphabet- 
per-phonenm classification problem. They re- 
stricted the information length to two or three 
units beibre and behind an input unit. In tact, 
ninny linguistic phenomena involved in the E-K 
transliteration are expressed in terms of units 
that exceed a phoneme and an alphabet. For 
example, 'a' in 'ace' is transliterated into %11 
?l (ey0" lint in 'acetic', "ot (eft and ill 'acetone', 
"O}(a)". If we restrict the information length 
to two alphabets, then we cmmot explain these 
phenomena. Three words ge~ the same result 
~()r ~ a'.  
(3) ace otl o\] ~(eyisu) 
(4) acetic cq.q v I (esithik) 
418 
(5) acetone ol-J~ll ~-(aseython) 
In this t)at)er, we t)rot)ose /;he E-K transliter- 
al;ion model t)ased on l)honeme chunks that 
do not have a length limit and can explain 
transliter;~tion l)henolnem, in SOllle degree of 
reliability. Not a alphal)et-per-all)habet but a 
chunk-i)er-chunk classification 1)roblem. 
This paper is organized as tbllows. In section 
2, we survey an E-K transliteration. \]111 section 
3, we propose, phonenm chunks 1)asexl translit- 
eration and back-transliteration. In Seel;ion 4, 
the lesults of ext)erilnents are presented. Fi- 
nally, the con(:hlsion follows in section 5. 
2 Eng l i sh - to -Korean  t rans l i te ra t ion  
E-K transliteration models are (:lassitied in two 
methods: the l)ivot method and the direct 
method. In the pivot method, transliteration 
is done in two steps: (:onverting English words 
ill|;() pronunciation symbols and then (:onverting 
these symbols into Kore~m wor(ts by using the 
Korean stm~(tard conversion rule. In the direct 
method, English words are directly converted to 
Korean words without interlnediate stct)s. An 
exl)eriment shows that the direct method is bet- 
ter than the pivot method in tin(ling wtriations 
of a transliteration(Lee and (~hoi, 1998). Statis- 
ti(:al information, neural network and de(:ision 
tree were used to imt)lelneld; the direct method. 
2.1 S ta t i s t iea l  T rans l i te ra t ion  method 
An English word is divided into phoneme se- 
quence or alphal)et sequence as (21~(22~... ~e n. 
Then a corresponding Korean word is rel)- 
resented as kl, k2,. . .  , t~:n. If n correspond- 
ing Korean character (hi) does not exist, we 
fill the blank with '-'. For example, an En- 
glish word "dressing" and a Korean word "> 
N\] zg (tuleysing)" are represented as Fig. 1. The 
ut)per one in Fig. 1 is divided into an English 
phoneme refit and the lower one is divided into 
an alphabet mlit. 
dressh, g :---~ ~1~ 
d/~+r /a  +e/  4l + ss /  x + i /  I +n-g/ o 
l d/=---+r/e +e/41 +s/~.+s/ -+ i /  I +n/o  +g/ -  
Figure 1: An E-K transliteration exault)le 
The t)roblem in statistical transliteration 
reel;hod is to lind out the. lllOSt probable translit- 
eration fbr a given word. Let p(K) be the 1)tel)- 
ability of a Korean word K, then, for a given 
English word E, the transliteration probal)ility 
of a word K Call be written as P(KIE). By using 
the Bayes' theorem, we can rewrite the translit- 
eration i)rol)lem as follows: 
,a'.q maz p(K IE )  = a,..q ma:,: p(K)p(~IK)  (1) 
K K 
With the Markov Independent Assulni)tion , 
we apl)roximate p(K) and p(EIK) as tbllows: 
7~ 
i=2 
i=1 
As we do not know the t)rommciation of a 
given word, we consider all possible tfllonelne 
sequences, l?or exanlple, 'data' has tbllowing 
possible t)holmme sequences, 'd-a-t-a, d-at-a, 
da-ta, . . . ' .  
As the history length is lengthened, we. can 
get more discrimination. But long history in- 
fornlation c~mses a data sl)arseness prol)lenl. In 
order to solve, a Sl)arseness t)rol)len~, Ma.ximmn 
Entropy Model, Back-off, and Linear intert)ola- 
tion methods are used. They combine different 
st~tistical estimators. (Tae-il Kim, 2000) use u t) 
to five phonemes in feature finlction(Berger et 
a,l., 1996). Nine %ature flmctions are combined 
with Maximum Entrot)y Method. 
2.2 Neura l  Network  and  Dec is ion  Tree 
Methods based 011 neural network and decision 
tree detenninistically decide a Korean charac- 
ter for a given English input. These methods 
take two or three alphabets or t)honemes as 
an input and generate a Korean alphabet 
or phoneme as an output. (Jung-.\]ae Kim, 
1.999) proposed a neural network method that 
uses two surrom~ding t)holmmes as an intmt. 
(Kang, 1999) t)roposed a decision tree method 
that uses six surrounding alphabets. If all 
inl)ut does not cover the phenomena of prol)er 
transliterations, we cammt gel; a correct answer. 
419 
Even though we use combining methods to 
solve the data sparseness problem, the increase 
of an intbrmation length would double the 
complexity and the time cost of a problem. It 
is not easy to increase the intbrmation length. 
To avoid these difficulties, previous studies 
does not use previous outputs(ki_z). But it 
loses good information of target language. 
Our proposed method is based on the direct 
method to extract the transliteration and its 
variations. Unlike other methods that deter- 
mine a certain input unit's output with history 
information, we increase the reliability of a cer- 
tain transliteration, with known E-K transliter- 
ation t)henonmna (phoneme chunks). 
3 Trans l i te ra t ion  us ing  Mul t ip le  
unbounded over lapp ing  phoneme 
chunks  
For unknown data, we can estimate a Korean 
transliteration ti'onl hand-written rules. We 
can also predict a Korean transliteration with 
experimental intbrmation. With known English 
and Korean transliteration pairs, we can as- 
sume possible transliterations without linguistic 
knowledge. For example, 'scalar" has common 
part with 'scalc:~sqlN (suhhcyil)', ' casinoJ\[ 
xl  (t:hacino)', 't:oala:   e-l-&:hoalla)', and 
'car:~l-(kh.a)' (Fig. 2). We can assume possible 
transliteration with these words and their 
transliterations. From 'scale' and its transliter- 
ation l'-~\] ~ (sukheyil), the 'sc' in 'scalar' can be 
transliterated as '~:-J(sukh)'. From a 'casino' 
example, the 'c' has nlore evidence that can be 
transliterated as 'v  (kh)'. We assume that we 
can get a correct Korean transliteration, if we 
get useful experinlental information and their 
proper weight that represents reliability. 
3.1 The a l ignment  of  an Engl ish word  
with  a Korean  word 
We can align an English word with its translit- 
eration in alphabet unit or in phoneme unit. 
Korean vowels are usually aligned with English 
vowels and Korean consonants are aligned with 
English consonants. For example, a Korean 
consonant, '1~ (p)' can be aligned with English 
consonants 'b', 'p', and 'v'. With this heuristic 
we can align an English word with its translit- 
eration in an alphabet unit and a t)honeIne unit 
with the accuracy of 99.4%(Kang, 1999). 
s c a 1 a r 
s c a 1 e 
k o a 
1 n o 
I L ? 
C a r 
F 
Figure 2: the transliteration of 'scalar : ~ 
~\]-(sukhalla)' 
3.2 Ext ract ion  o f  Phoneme Chunks  
From aligned training data, we extract phoneme 
clumks. We emmw.rate all possible subsets of 
the given English-Korean aligned pair. During 
enumerating subsets, we add start and end posi- 
tion infbrmation. From an aligned data "dress- 
ing" and "~etl N (tuleysing)", we can get subsets 
as Table 12. 
Table 1: The extraction of phoneme chunks 
Context Output 
d 
r. # 
?,_)dr'c d/=--(d)+r/~- (r')+e/ql (ey) 
The context stands tbr a given English al- 
phabets, and the output stands for its translit- 
eration. We assign a proper weight to each 
phoneme chunk with Equation 4. 
C ( output ) 
wei.qh, t(contcxt : output) - C(contcxt) (4) 
C(x) means tile frequency of z in training data. 
Equation 4 shows that the ambiguous phe- 
nomenon gets the less evidence. The clnmk 
weight is transmitted to each phoneme symbol. 
To compensate for the length of phoneme, we 
multiply the length of phoneme to the weight of 
the phoneme chunk(Fig. 3). 
2@ means the start and end position of a word 
420 
weight(surfing: s/Z- + ur/4 + i f=  + i/l + r ig/o) = (Z 
4, 4, 4, 4, 4, 
o~ 2a a a 2o~ 
\]?igure 3: The weight of a clmnk and a t)honeme 
This chunk weight does not mean the. relia- 
t)ility of a given transliteration i)henomenon. 
We know real reliM)itity, after all overlapping 
phonenm chunks are applied. The chunk that 
has some common part with other chunks 
gives a context information to them. Therefore 
a chunk is not only an int)ut unit but also 
a means to (-Mculate the reliability of other 
dmnks. 
We also e, xl;ra(:t the  connection information. 
From Migned training (b:~ta, we obtain M1 pos- 
sible combinations of Korem~ characters and 
English chara(:ters. With this commction in- 
tbrmation, we exclude iml)ossit)h; connections 
of Korean characters ~md English phon(;nte se- 
quences. We can gel; t;he following (:ommction 
information from "dressing" examph'.(~12fl)le 2).
2?fl)le 2: Conne(:tion Information 
Enffli.sh, Kore.a',. 1 
\]lql't\[righ, t II Z(?lt l ,.,:.,1,,t / 
a ,. ( ,9 
, ('. 09  
3.3 A Trans l i te ra t ion  Network  
For a given word, we get al t)ossil)h~ t)honemes 
and make a Korean transliteration etwork. 
Each node in a net;work has an English t)honent(; 
and a ('orrcspondillg Korean character. Nodes 
are comm(:ted with sequence order. For exam- 
ple, 'scalar' has the Kore, an transliteration et- 
work as Fig. 4. In this network, we dis('ommct 
some no(les with extracted (:onne('tion infornla- 
tion. 
After drawing the Korean tr~msliteration net- 
work, we apply all possible phone, me, chunks 
to the. network. Each node increases its own 
weight with the weight of t)honeme symbol in a 
phoneme chunks (Fig. 5). By overlapping the 
weight, nodes in the longer clmnks get; more ev- 
idence. Then we get the best t)ath that has the 
Figure 4: Korean Transliteration Network for 
'scalar' 
highest sum of weights, with the Viterbi algo- 
ril, hm. The Tree.-Trcllis Mgorithm is used to gel; 
the variations(Soong and Huang, 1991). 
Figure 5: Weight aptflication examt)le 
4 E -K  back - t rans l i te ra t ion  
E-K back transliteration is a more difficult prot)- 
lem thtnt F,-K trmlsliteration. During the E-K 
trm~slit;cra|;ion~ (lifli'xent alphabets are treated 
cquiw~h'.ntly. \],~)r exmnph'., ~f, t / mM ~v~ b' 
spectively and the long sound and the short 
strand are also treated equivalently. Therefim', 
the number of possible English phone, rues per 
a Korean character is bigger than the number 
of Korean characters per an English phoneme. 
The ambiguity is increased. In  E-K back- 
transliteration, Korean 1)honemes and English 
phoneme, s switch their roles. Just switching the 
position. A Korean word ix Migned with an 
English word in a phoneme unit or a character 
refit (Fig. 6). 
\[ ~---~l~ : dressing\] 
F /d+- - / -+  ~/ r+ 41/e+~-/ss+ I / i+o /n  9 ,~ 
Figure 6: E-K back-transliteration examt)le 
421 
5 Exper iments  
Experiments were done in two points of view: 
the accuracy test and the variation coverage 
test. 
5.1 Test Sets 
We use two data sets for an accuracy test. Test 
Set I is consists of 1.,650 English and Korean 
word pairs that aligned in a phoneme unit. It 
was made by (Lee and Choi, 1998) and tested by 
many methods. To compare our method with 
other methods, we use this data set. We use 
same training data (1,500 words) and test data 
(150 words). Test Set I I  is consists of 7,185 
English and Korean word paii's. We use Test 
Set H to show the relation between the size of 
training data and the accuracy. We use 90% 
of total size as training data and 10% as test 
data. For a variation coverage test, we use Test 
Set I I I  that is extracted from KTSET 2.0. Test 
Set HI  is consists of 2,391 English words and 
their transliterations. An English word has 1.14 
various transliterations in average. 
5.2 Eva luat ion funct ions 
Accuracy was measured by the percentage of 
the number of correct transliterations divided 
by the number of generated transliterations. We 
(:all it as word accuracy(W.A.). We use one 
more measure, called character accuracy(C.A.) 
that measures the character edit distance be- 
tween a correct word and a generated word. 
no.  of  correct words 
W.A. = no. o.f .qenerated words (5) 
C.A. = L (6) 
where L is the length of the original string, and 
i, d, mid s are the number of insertion, deletion 
and substitution respectively. If the dividend is 
negative (when L < (i + d + s)), we consider it 
as zero(Hall and Dowling, 1980). 
For the real usage test, we used variation cov- 
erage (V.C.) that considers various usages. We 
evaluated both tbr the term frequency (tf) and 
document frequency (d J), where tfis the number 
of term appearance in the documents and df is 
the number of documents that contain the term. 
If we set the usage tf (or d./) of the translitera- 
tions to 1 tbr each transliteration, we can calcu- 
late the transliteration coverage tbr the unique 
word types, single .frequency(.sf). 
V.C. = {if ,  df, s f}  of  found  words (7) {t.f, 4f, <f} of  ,sed   o,'ds 
5.3 Accuracy  tests 
We compare our result \[PCa, PUp\] a with the 
simple statistical intbrmation based model(Lee 
and Choi, 1998) \[ST\], the Maxinmm Entropy 
based model(Tae-il Kim, 2000) \[MEM\], the 
Neural Network model(Jung-Jae Kim, 1999) 
INN\] and the Decision %'ee based model(Kang, 
1999)\[DT\]. Table 3 shows the result of E- 
K transliteration and back-transliteration test 
with Test ,get L 
Table 3: C.A. and W.A. with Test Set I 
E-K trans. 
method C.A. I W.A. 
ST 69.3% 40.7% 4 
MEM 72.3% 43.3% 
NN 79.0% 35.1% 
DT 78.1% 37.6% 
Pep 86.5% 55.3% 
PCa 85.3% 46.7% 
E-K back trans. 
C.A. \[ W.A. 
60.5% 
77.1% 31.0% 
81.4% 34.7% 
79.3% 32.6% 
95 
85 
75 
65 
55 
45 
35 
Fig. 7, 8 show the results of our proposed 
method with the size of training data, Test Set 
II. We compare our result with the decision tree 
based method. 
~ - - - - ~  I~C 'A 'PC?  I 
i - -~- w.A DT I 
+ W.A. POp 
~ W,A. BCa 
J J J ~ J i 
1000 2000 3000 4000 5000 6000 
Figure 7: E-K transliteration results with Test 
Set H 
aPC stands for phoneme chunks based method and 
a and b stands for aligned by an alphabet unit and a 
1)honeme unit respectively 
4with 20 higher rank results 
422 
90 
80-  
70 
6O 
5O 
40 
30 i 
20 t 
1000 2000 3000 4000 5000 6000 
"--?-- C.A, DT 
---U-- C.A. PCp 
C.A. PCa I 
!--x- ~A. Dr i 
I . _ _  144A. POp\] 
I .~ l~ W,A. POa 
Figure 8: E-K back-transliteration results with 
Test Set H 
With 7'c,s't Sc, t H~ we (:m~ get t;15(; fi)llowing 
result (Table, 4). 
Table d: C.A. and W.A. with the Test Set H 
E-K tr~ms. E-K back tr~ms. 
method C.A. 14LA. C.A. I W.A. 
PUp \[~9.5% 57.2% 84.9% 40.9% 
PCa \[19o.6% 58.3% s4.8% 4(/.8% 
5.4 Var iat ion coverage tests  
To (:oml)~re our result(PCp) with (Lee and 
()hoi, 1998), we tr~fincd our lnethods with the 
training data of Test Set L In ST, (Lee mid 
Choi, 1998) use 20 high rank results, but we 
j t l s t  l lSe  5 results. TM)le 5 shows the (:overage 
of ore: i)rol)osed me.thod. 
Table 5: w~riation eover~ge with Tc.~t Set II I  
method tf d.f ,~f 
ST 76.0% 73.9% 47.1% 
PCp 84.0% 84.0% 64.0% 
Fig. 9 shows the increase of (:overage with the 
number of outputs. 
5.5 Discussion 
We summarize the, information length ~md the 
kind of infonnation(Tnble 6). The results of 
experimenLs and information usage show theft 
MEM combines w~rious informal;ion better 
than DT and NN. ST does not list & previous 
inlmt (el- l)  but use ~ previous output(t,:i_~) to 
calculate the current outlml?s probability like 
95 
90 
85 
80 
70 
65 
60 
55 
5O 
45 
---,I 
! -m-e l f  
, , , ~ sf 
1 2 3 4 5 6 7 8 9 10 
Figure 9: The 17. C. result 
q~,l)le 6: Intbrmation Usage 
previous output 
ST  2 0 Y 
MEM 2 2 N 
NN \] 1 N 
DT 3 3 N 
PC Y 
Part-ofSt)eeeh rl'~gging probleln. But ST gets 
the lowest aecm'acy. It means that surrmmding 
alphal)ei;s give more informed;ion than t)revious 
outlmL. In other words, E-K trmlslii;e.ration is 
not the all)h~bet-per-alphabet or phonenle-per- 
t)honeme (:lassific~tion problem. A previous 
outI)ut does not give, enough information for 
cllrrent ltnit's dismnbiguat;ion. An input mill 
mid an OUtlmt unit shouht be exl:ende(t. E-K 
transliteration is a (:hunk-l)er-chunk classifica- 
tion prot)lenL 
We restri(:t the length of infiwm~tion, to see 
the influence of' phoneme-chunk size. Pig. 10 
shows the results. 
i 9oi ~ ~ ~ _ ~ " , ~  
F 70 6oi 
5040 / ~ #  .~ 
-- -- C.A. 7bst Sot f 30 / / I-~- c.A. z~.~ so,, I 
20 t~ / I~WA To,.t Set / i 
I o -?-  ~i L;, L,, ! 
x ~ 
0 
1 2 3 4 5 6 7 
Figure 10: the result of ~ length limit test 
423 
With the same length of information, we 
get the higher C.A. and W.A. than other 
methods. It means previous outputs give good 
information and our chunk-based nmthod is 
a good combining method. It also suggests 
that we can restrict he max size of chunk in a 
permissible size. 
PCa gets a higher accuracy than PCp. It 
is clue to the number of possible phoneme se- 
quences. A transliteration network that con- 
sists of phoneme nnit has more nodes than a 
transliteration network that consists of alpha- 
bet unit. With small training data, despite of 
the loss due to the phoneme sequences ambi- 
guity a phoneme gives more intbrmation than 
an alphabet. When the infbrmation is enough, 
PCa outpertbrms Pep. 
6 Conc lus ions  
We propose the method of English-to-Korean 
transliteration and back-transliteration with 
multiple mfl)ounded overlapping phoneme 
chunks. We showed that E-K transliteration 
and back-transliteration are not a t)honeme- 
per-phoneme and alphabet-per-alphabet 
classification problem. So we use phoneme 
chunks that do not have a length limit and 
can explain E-K transliteration phenomena. 
We get the reliability of a given transliter- 
ation phenomenon by applying overlapt)ing 
phoneme chunks. Our method is simple and 
does not need a complex combining method 
tbr w, rious length of information. The change 
of an intbrmation length does not affect the 
internal representation f the problem. Our 
chunk-based method can be used to other 
classification problems and can give a simple 
combining method. 
References  
Tae-il Kim. 2000. English to Korean translit- 
eration model using maxinmm entropy model 
for cross language information retrieval. Mas- 
ter's thesis, Seogang University (in Korean). 
Kil Soon aeong, Sllng Hyun Myaeng, Jae Sung 
Lee, and Key-Sun Choi. 1999. Automatic 
identification and back-transliteration of for- 
eign words tbr information retrieval, b~:for- 
mation Processing and Management. 
Key-Sun Choi Jung-Jae Kim, Jae Sung Lee. 
1999. Pronunciation unit based automatic 
English-Korean transliteration model using 
neural network. In Pwceedings of Korea Cog- 
nitive Science Association(in Korean). 
Byung-Ju Kang. 1999. Automatic Korean- 
English back-transliteration. I  Pwecedings 
of th, c 11th, Conference on Iiangul and Ko- 
rean Language Information Prvcessing( in Ko- 
Fean). 
Jae Sung Lee and Key-Sun Choi. 1998. English 
to Korean statistical transliteration for in- 
formation retrieval. Computer PTvcessin9 of 
Oriental Languages. 
K. Jeong, Y. Kwon, and S. H. Myaeng. 1997. 
The effect of a proper handling of foreign and 
English words in retrieving Korean text. In 
Proceedings of the 2nd Irdernational Work- 
shop on lrtforrnation Retrieval with Asian 
Languages. 
K. Knight and J. Graehl. 1997. Machine 
transliteration. In Proceedings o.f the 35th 
Annual Meeting of the Association J'or Com- 
putational Linguistics. 
Adam L. Berger, Stephen A. Della Pietra, and 
Vincent J. Della Pietra. 1996. A maximum 
entroI)y approach to natural language pro- 
cessing. Computational Linguistics. 
Y. C. Park, K. Choi, J. Kim, and Y. Kim. 
1996. Development of the data collection ver. 
2.0ktset2.0 tbr Korean intbrmation retrieval 
studies. In Artificial Intelligence Spring Con- 
ference. Korea Intbrmation Science Society 
(in Korean). 
Frank K. Soong and Eng-Fong Huang. 1991. 
A tree-trellis based Nst search for tinding 
the n best sentence hypotheses in eontimmus 
speech recognition. In IEEE International 
Conference on Acoustic Speech and Signal 
Pwcessing, pages 546-549. 
P. Hall and G. Dowling. 1980. Approximate 
string matching. Computing Surveys. 
424 
Unsupervised Named Entity Classification Models  
and their Ensembles 
 
Jae-Ho Kim*, In-Ho Kang, Key-Sun Choi* 
 
Korea Advanced Institute of Science and Technology (KAIST) / 
Korea Terminology Research Center for Language and Knowledge Engineering* (KORTERM) 
373-1, Guseong-dong, Yuseong-gu 
Daejeon, KOREA, 305-701 
{jjaeh@world, ihkang@csone, kschoi@world}.kaist.ac.kr 
 
Abstract  
This paper proposes an unsupervised 
learning model for classifying named 
entities. This model uses a training set, built 
automatically by means of a small-scale 
named entity dictionary and an unlabeled 
corpus. This enables us to classify named 
entities without the cost for building a large 
hand-tagged training corpus or a lot of rules. 
Our model uses the ensemble of three 
different learning methods and repeats the 
learning with new training examples 
generated through the ensemble learning. 
The ensemble of various learning methods 
brings a better result than each individual 
learning method. The experimental result 
shows 73.16% in precision and 72.98% in 
recall for Korean news articles.  
1 Introduction 
Named entity extraction is an important step for 
various applications in natural language 
processing. Named entity extraction involves 
identifying named entities in the text and 
classifying their types such as person, 
organization, location, time expressions, 
numeric expressions, and so on (Sekine and 
Eriguchi, 2000).  
One might think the named entities can be 
classified easily using dictionaries because most 
of named entities are proper nouns, but this is 
wrong opinion. As time passes, new proper 
nouns are created continuously. Therefore it is 
impossible to add all those proper nouns to a 
dictionary. Even though named entities are 
registered in the dictionary it is not easy to 
decide their senses. They have a semantic 
(sense) ambiguity that a proper noun has 
different senses according to the context (Nina 
Wacholder, et al, 1997). For example, ?United 
States? refers either to a geographical area or to 
the political body which governs this area. The 
semantic ambiguity is occured frequently in 
Korean (Seon, et al 2001). Let us illustrate this.  
Example 1 : Location 
Let?s meet at KAIST. 
 
KAIST       e-seo   man-na-ja .  
(PN:KAIST)  (PP:at)  (V:meet) 
 
Example 2 : Organization 
KAIST announced the list of successful candidates.
 
KAIST      e-seo     hab-gyeok-ja  
(PN:KAIST)  (PP)  (N:successful candidates) 
 
myeong-dan  eul   bal-pyo-haet-da .  
(N:list)      (PP)  (V:announced) 
 
PN : proper noun, N : noun, PP : postposition, V : verb 
In the above examples, ?KAIST? has different 
categories although same postposition, ?e-seo?, 
followed. The classification of named entities in 
Korean is a little more difficult than in English. 
There are two main approaches to classify 
named entities. The first approach employs 
hand-crafted rules. It costs too much to maintain 
rules because rules and dictionaries have to be 
changed according to the application. The 
second belongs to a supervised learning 
approach, which  employs a statistical method. 
As it is more robust and requires less human 
intervention, several statistical methods based on 
a hidden Markov model (Bikel et al, 1997), a 
Maximum Entropy model (Borthwich et al, 
1998) and a Decision Tree model (B?chet et al 
2000) have been studied. The supervised 
learning approach requires a hand-tagged 
training corpus, but it can not achieve a good 
performance without a large amount of data 
because of data sparseness problem. For 
example, Borthwich (1999) showed the 
performance of 83.45% in Precision and 77.42% 
in F-measure for identifying and classifying the 
8 IREX (IREX committee, 1999) categories, 
with 294,000 tokens IREX training corpus. It 
takes a lot of time and labor to build a large 
corpus like this.  
This paper proposes an unsupervised learning 
model that uses a small-scale named entity 
dictionary and an unlabeled corpus for 
classifiying named entities. Collins and Singer 
(1999) opened the possibility of using an 
unlabeled corpus to classify named entities. 
They showed that the use of unlabeled data can 
reduce the requirements for supervision to just 7 
simple seed rules. They used natural redundancy 
in the data : for many named-entity instances, 
both the spelling of the name and the context in 
which it appears are sufficient to determine its 
type.  
Our model considers syntactic relations in a 
sentence to resolve the semantic ambiguity and 
uses the ensemble of three different learning 
methods to improve the performance. They are  
Maximum Entropy Model, Memory-based 
Learning and Sparse Network of Winnows 
(Roth, 1998). 
This model classifies proper nouns appeared 
in the documents into person, organization and 
location on the assumption that the boundaries 
of proper nouns were already recognized. 
2 The System for NE Classification 
This section describes a system that classifies 
named entities by using a machine learning 
algorithm. The system consists of four modules 
as shown in Figure 1. 
First, we builds a training set, named entity 
tagged corpus, automatically. This set will be 
used to predict the categories of named entities 
within target documents received as the input of 
the system. 
The second module extracts syntactic 
relations from the training set and target 
documents. They are encoded to the format of 
training and test examples for machine learning.  
In the third module, each learning for 
classification is progressed independently by 
three learning methods. Three results generated 
by each learner are combined into one result. 
Finally, the system decides the category by 
using a rule for the test examples that did not be 
labeled yet. And then the system outputs a 
named entity tagged corpus. 
Extracting Syntactic Relations
Building 
a Training Set
Training Set
Target Documents
Ensemble Learning
Post-Processing
NE Tagged Corpus
Input
Output
Figure 1. System Architecture 
2.1 Building a Training Set 
The system requires a training set which has 
categories in order to get knowledge for the 
classification. We build a training set 
automatically using a named entity dictionary 
and a POS tagged corpus, and then use it instead 
of a hand-tagged set in machine learning.  
We randomly extract 1500 entries per each 
category (person, location, and organization) 
from a Proper Noun dictionary made by 
KORTERM and then reconstruct the named 
entity dictionary. The Proper Noun dictionary 
has about 51,000 proper nouns classified into 41 
categories (person, animal, plant and etc.). We 
do not extract homonyms to reduce the 
ambiguity. In order to show that it is possible to 
classify named entities with a small-scale 
dictionary, we limit the number of entries to be 
1500.  
We label the target word, proper noun or 
capital alphabet, appeared in the POS tagged 
corpus 1  by means of the NE dictionary 
mentioned above. The corpus is composed of 
                                                     
1 We used a KAIST POS tagged corpus 
one million eojeols2. It is not easy to classify 
named entity correctly only with a dictionary, 
since named entity has the semantic ambiguity. 
So we have to consider the context around the 
target word.  
In order to consider the context, we use 
co-occurrence information between the category 
(c) of a target word (tw) and a head word (hw) 
appeared on the left of the target word or the 
right of the target word. We modify categories 
labeled by the NE dictionary by following 
process. 
1. We extract pairs [c, hw] from the corpus 
labeled by means of the dictionary. 
2. If hw is occurred with several different 
categories, we suppose tw occurred with 
hw may have an ambiguity and then we 
remove the category label of tw. 
3. We make rules for predicting the category 
of tw from pairs [c, hw] and apply them to 
the corpus. The rule is that tw occurred 
with hw has a c.  
4. We extract sentences including the 
labeled target word in the corpus. 
In the step 3, 9 rules are made. We label the c 
for unlabeled target word occurred with hw if 
the pair [c, hw] is found more than a threshold. 
We set the threshold to be 10. Sentences 
including the 4,504 labeled target word are made 
as a tringing set in this process (Table 1). 
Table 1. The number of the target words in a 
training set 
State # of target words
Candidates in the corpus 37,831 
Labeled by the dictionary 3,899 
Removed by the ambiguity 778 
Added by 9 rules 1,383 
Total 4,504 
2.2 Extracting Syntactic Relations 
In order to predict the category, most of machine 
learning systems usually consider two words on 
the left and two ones on the right of a target 
word as a context (Uchimoto and et al 2000, 
                                                     
2 Korean linguistic units that is separated by blank or 
punctuation 
Petasis and et al 2000). However this method 
have some problems.  
If some words that are not helpful to predict 
the category are near the target word, they can 
cause an incorrect prediction. In the following 
example, ?Kim? can be predicted as an 
organization instead of a person because of a left 
word ?Jeong-bu? (the government).  
 
Example  
The goverment supports KIA on the premise that 
the chairman Kim submits a resignation. 
 
Jeong-bu        neun Kim  hoi-jang     i 
(N:the goverment)  (PP) (PN) (N:the chairman) (PP) 
 
sa-pyo        reul  je-chul-han-da neun 
(N:a resignation) (PP)   (V :submit)     (PP)  
 
jeon-je       ro  KIA  reul  ji-won-han-da. 
(N :the premise)(PP) (PN)   (PP)   (V :support) 
 
PN : proper noun, N : noun, PP : postposition, V : verb 
  
The system cannot consider important words 
that are out of the limit of the context. In the 
former example, the word ?je-chul-han-da? 
(submit) is an important feature for predicting 
the category of ?Kim?. If a Korean functional 
word is counted as one window, we cannot get 
this information within right 4 windows. Even if 
we do not count the functional words, 
sometimes it is neccessary to consider larger 
windows than 2 windows like above example. 
We notice that words that modify the target 
word or are modified by the target word are 
more helpful to the prediction than any other 
words in the sentence. So we extract the 
syntactic relations like Figure 2 as the context.  
BLANK Kim hoi-jang(chairman)
je-chul-han-da
(present)i
modifier targetword modifiee predicatejosa
BLANK KIA BLANK ji-won-han-da(support)reul
 
Figure 2. Syntactic relations for the target word 
The modifier is a word modifying the target 
word and the modifiee is one modified by the 
target word. Josa3 is a postposition that follows 
the target word and te predicate is a verb that 
predicates the target word. The ?BLANK? label 
represents that there is no word which 
corresponds to the slot of the templet. These 
syntactic relations are extracted by a simple 
heuristic parser. We will show that these 
syntactic relations bring to a better result 
through an experiment in the section 3. 
These syntactic relations seem to be language 
specific. Josa represents a case for the target 
word. If case information is extracted in a 
sentence, these syntactic relations like Figure 2 
are also made in other languages. 
As machine learner requires training and test 
examples represented in a feature-vector format, 
syntactic relations are encoded as Figure 3. 
 
Feature-vector format  
lexical morpheme (w) Modifier POS tag (t) 
lexical morpheme (w) Target word POS tag (t) 
lexical morpheme (w) Modifiee POS tag (t) 
Josa lexical morpheme (w) 
Predicate lexical morpheme (w) 
Category  Label tag 
 
Training example : [w, t, w, t, w, t, w, w, person] 
Test example    : [w, t, w, t, w, t, w, w, Blank] 
 
Figure 3: The format of an example for learning 
2.3 Ensemble Learning 
The ensemble of several classifiers can be 
improve the performance. Errors made by the 
minority can be removed through the ensemble 
of classifiers (Thomas G. Dietterich, 1997). In 
the base noun phrase identification, Tjong Kim 
Sang, et al (2000) showed that the result 
combined by seven different machine learning 
algorithms outperformed the best individual 
result. 
In our module, machine learners train with the 
training examples and then classify the named 
entities in the test examples. This process is 
shown in Figure 4. 
                                                     
3 Josa, attached to a nominal, is a postpositional 
particle in Korean. 
Ending Condition is satisfied?
Yes
No
Labeled Test Examples
Machine Learners learn with training examples
The classification for the test examples is progressed 
by three different learners independently
Three results are combined by combining techniques
Training examples are modified 
(labeled examples in the combined result + initial training examples)
Loop
Figure 4. The process of the Ensemble Learning 
This ensemble learning has two characteristics. 
One is that the classification is progressed by 
three different learners independently and those 
results are combined into one result. The other is 
that the learning is repeated with new training 
examples generated through the learning. It 
enables the system to receive an incremental 
feedback. 
Through the this learning method, we can get 
larger and more precise training examples for 
predicting the categories. It is important in an 
unsupervisd learning model because there is no 
labeled data for learning. 
2.3.1 Machine Learning algorithms 
We use three learning methods : Memory-based 
Learning, Sparse Network of Winnows, 
Maximum Entropy Model. We describe these 
methods briefly in this section.  
Memory-based Learning stores the training 
examples and classifies new examples by 
choosing the most frequent classification among 
training examples which are closest to a new 
example. Examples are represented as sets of 
feature-value pairs. Each feature receives a 
weight which is based on the amount of 
information which it provides for computing the 
classification of the examples in the training data. 
We use the TiMBL (Daelemans, et al, 1999), a 
Memory-Based Learning software package. 
Sparse Network of Winnows learning 
architecture is a sparse network of linear units. 
Nodes in the input layer of the network represent 
simple relations over the input example and 
things being used as the input features. Each 
linear unit is called a target node and represents 
classifications which are interested in the input 
examples. Given training examples, each input 
example is mapped into a set of features which 
are active (present) in it; this representation is 
presented to the input layer of SNoW and 
propagated to the target nodes. We use SnoW 
(Carlson, et al, 1999), Sparse Network of 
Winnows software package. 
Maximum Entropy Model (MEM) is 
especially suited for integrating evidences from 
various information sources. MEM allows the 
computation of p(f|h) for any f in the space of 
possible futures, F, and for every h in the space 
of possible histories, H. Futures are defined as 
the possible classification and a history is all of 
the conditioning data which enable us to make a 
decision in the space of futures. The 
computation of p(f|h) is dependent on a set of 
features which are binary functions of the 
histroy and future. A feature is represented as 
following.  
?
?
?
?
?
?
?
?
?
?
=
otherwise 0
  future  the  of  one  is  f  and     
condition  some  meets  h if1
),( fhg  
Given a set of features and some training 
examples, a weighing parameter i?  for every 
feature ig  is computed. This allows us to 
compute the conditional probability as follows : 
)()|(
),(
hZhfP
i
fhg
i
i
?
??
=
    
 
??=
f i
fhg
i
ihZ ),()( ??     
We use MEMT, Maximum Entropy Modeling 
Toolkit (Ristad, 1998), to compute the parameter 
for the features. 
2.3.2 Combining Techniques 
We use three different voting mechanisms to 
combine results generated by three learners. 
The first method is a majority voting. Each 
classification receives the same weight and the 
most frequent classification is chosen. The 
ending condition is satisfied when there is no 
difference between a result combined in this 
loop and one combined in the former loop.  
The second method is a probability voting. 
MEMT and SNoW propose the probabilities for 
all category, but Timbl proposes only one 
appropriate category for one test example. We 
set the probability for the category Timbl 
proposes to be 0.6 and for the others to be 0.2. 
For each category, we multiply probabilities 
proposed by 3 learners and then choose N 
examples that have the largest probability. In the 
next learning we set N = N + 100. When N is 
larger than a threshold, the ending condition is 
satisfied and the learning is over. We set it to be 
3/4 of the number of test examples.  
The last method is a mixed voting. We use 
two voting methods mentioned above one after 
another. First, we use probability voting. After 
the learning is over we use majority voting. The 
threshold of the probability voting is 1/2 of the 
number of test examples here.  
2.4 Post-Processing 
After the learning, the system modifies test 
examples by using a rule, one sense per 
discourse. One sense per discourse means that 
the sense of a target word is highly consistent 
within any given document. David Yarowsky 
(1995) showed it was accurate in the word sense 
disambiguation. We label the examples that are 
not labeled yet as the category of the labeled 
word in the discourse as following example and 
we output named entity tagged corpus. 
 
Example  
 
after the ensemble learning 
 
... ... KIA<type=organization> reul ji-won-han-da.  
KIA neon ... ...  
 
after post-processing 
 
... ... KIA<type=organization> reul ji-won-han-da. 
KIA<type=organization> neon ... ...  
   
3 Experimental Results 
We used Korean news articles that consist of 
24,647 eojeols and contain 2,580 named entities 
as a test set. The number of named entities 
which belong to each category is shown in Table 
2. When even a human could not classify named 
entities, ?Unknown? is labeled and it is ignored 
for the evaluation. ?Other? is used for the word 
outside the three categories.  
Table 3 shows the result of the classification. 
The first row shows the result of the 
classification using only a NE dictionary. The 
recall (14.84%) is very low because the system 
uses a small-scale dictionary. The precision 
(91.56%) is not 100% because of the semantic 
ambiguity. It means that it is necessary to refine 
classifications created by a dictionary. 
We build a training set with a NE dictionary 
and a POS tagged corpus and refine it with 
co-occurrence information. The second row 
shows the result of the classification using this 
training set without learning. We can observe 
that the quality of the training set is improved 
thanks to our refining method. 
A Mixed Voting shows the best results. It 
improves the performance by taking good 
characteristics of a majority voting and 
probability voting. 
Table 2. The number of named entities which 
belong to each category in the test set 
Category # of NEs Category # of NEs
Person 459 Other 307 
Organization 814 Unknown 242 
Location 758 Total 2,580 
Table 3. The result of the classification 
Method Precision  Recall F-measure
Dictionary 
based 91.56% 14.84% 25.54% 
Training set 
b1ased 94.32% 20.64% 33.87% 
Majority 
Voting 69.70% 65.74% 67.68% 
Probability 
Voting 75.90% 63.45% 69.12% 
Mixed 
Voting 73.16% 72.98% 73.07% 
We extract the syntatic relations and make 5 
windows (modifier, target word, modifiee, josa, 
predicate) as a context. We conduct a 
comparative experiment using the Uchimoto?s 
method, 5 windows (two words before/after the 
target word) and then we show that our method 
brings to a better result (Table 4). 
Table 4. Comparison with two kinds of window 
size 
Windows Precision  Recall F-measure
Uchimoto?s 66.86% 69.94% 68.37% 
Ours 73.16% 72.98% 73.07% 
We try to perform the co-training similar to 
one of Colins and Singer in the same 
experimental environment. We extract 
contextual rules from our 5 windows because we 
does not have a full parser. The learning is 
started from 417 spelling seed rules made by the 
NE dictionary. We use two independent context 
and spelling rules in turn. Table 5 shows that our 
method improve the recall much more on the 
same conditions. 
Table 5. Comparison with two kinds of 
unsupervised learning method 
Method Precision  Recall F-measure
Co-trianing 84.62% 37.63% 52.09% 
Ours 73.16% 72.98% 73.07% 
Through the ensemble of various learning 
methods, we get larger and more precise training 
examples for the classification. Table 6 shows 
that the ensemble learning brings a better result 
than each individual learning method. 
Table 6. The comparison of an ensemble learning 
and each individual learning 
Learner Precision  Recall F-measure
MEMT 65.19% 61.54% 63.31% 
SNoW 66.93% 70.53% 68.68% 
Timbl 64.14% 67.59% 65.82% 
Ensemble 73.16% 72.98% 73.07% 
Three learners can use different kinds of 
features instead of same features. We conduct a 
comparative experiment as following. As 
features, SNoW uses a modifier and a target 
word, Timbl uses a modifiee and a target word, 
and MEMT uses a josa, a predicate and a target 
word. Table 7 shows that the learning using 
different kinds of features has the low 
performance because of the lack of information. 
Table 7. The comparison with the learnings using 
different features 
Features Precision  Recall F-measure
Seperated  61.69% 49.85% 55.14% 
Same  73.16% 72.98% 73.07% 
The system repeats the learning with new 
training examples generated through the 
ensemble learning. We can see that this loop 
brings to the better result as shown in Table 8.  
After the learning, we apply the rule, a sense 
per discourse. ?Post? in Table 8 indicates the 
performance after this post-processing. It The 
post-processing improves the performance a 
little. 
Table 8. The improvement of the performance 
through the repeated learning 
Method Loop Precision Recall F-measure
1st 94.35% 20.76% 34.03% 
19th 76.72% 59.97% 67.32% Probability Voting Post 75.90% 63.45% 69.12% 
We extracted the syntactic relations by using a 
simple heuristic parser. Because this parser does 
not deal with complex sentences, the failure of 
parsing causes the lack of information or wrong 
learning. Most of errors are actually occurred by 
it, therefore we need to improve the performance 
of the parser.  
4 Conclusion 
We proposed an unsupervised learning model 
for classifying the named entities. This model 
used a training set, built automatically by a 
small-scale NE dictionary and an unlabeled 
corpus, instead of a hand-tagged training set for 
learning. The experimental result showed 
73.16% in precision and 72.98% in recall for 
Korean news articles. This means that it is 
possible to classify named entities without the 
cost for building a large hand-tagged training 
corpus or a lot of rules.  
The learning for classification was progressed 
by the ensemble of three different learning 
methods. Then the ensemble of various learning 
methods brings a better result than each 
individual learning method. 
References  
B?chet, Fr?d?ric, Alexis Nasr and Franck Genet, 2000. 
"Tagging Unknown Proper Names Using Decision 
Trees", In proceedings of the 38th Annual Meeting of the 
Association for Computational Linguistics. 
Bikel, Daniel M., Scott Miller, Richard Schwartz and Ralph 
Weischedel, 1997. "Nymble: a High-Performance 
Learning Name-finder", In Proceedings of the Fifth 
Conference on Applied Natural Language Processing. 
Borthwick, Andrew, John Sterling, Eugene Agichtein and 
Ralph Grishman, 1998. "NYU: Description of the MENE 
Named Entity System as Used in MUC-7", In 
Proceedings of the Seventh Message Understanding 
Conference (MUC-7). 
Borthwick, 1999. ?A Japanese Named Entity Recognizer 
Constructed by a Non-Speaker of Japanese?, IREX. 
Proceedings of the IREX workshop. 
Carlson, Andrew J., Chad M. Cumby, Jeff L. Rosen and 
Dan Roth, 1999. "SNoW User Guide", University of 
Illinois. http://l2r.cs.uiuc.edu/~cogcomp/ 
Collins, Michael and Yoram Singer. 1999. "Unsupervised 
models for named entity classification", In proceedings 
of the Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora. 
Daelemans, Walter, Jakub Zavrel, Ko van der Sloot and 
Antal van den Bosch, 1999. "TiMBL: Tilburg Memory 
Based Learner, version 4.0, Reference Guide", ILK 
Technical Report 01-04. http://ilk.kub.nl/  
Dietterich, T. G., 1997. ?Machine-Learning Research: Four 
Current Dirctions?, AI Magazine 18(4): 97 
IREX Committee (ed.), 1999. Proc. the IREX Workshop. 
http://cs.nyu.edu/cs/projects/proteus/irex 
Petasis, Georgios, Alessandro Cucchiarelli, Paola Velardi, 
Georgios Paliouras, Vangelis Karkaletsis and 
Constantine D. Spyropoulos, 2000. "Automatic 
adaptation of Proper Noun Dictionaries through 
cooperation of machine learning and probabilistic 
methods", Proceedings of the 23rd ACM SIGIR 
Conference on R&D in IR (SIGIR). 
Ristad, Eric Sven, 1998. "Maximum Entropy Modeling 
Toolkit". 
Roth, Dan, 1998. ?Learning to resolve natural language 
ambiguities: A unified approach?, In Proc. National 
Conference on Artificial Intelligence. 
Sekine, Satoshi and Yoshio Eriguchi. 2000. "Japanese 
Named Entity Extraction Evaluation", In the proceedings 
of the 18th COLING. 
Seon, Choong-Nyoung, Youngjoong Ko, Jeong-Seok Kim 
and Jungyun Seo, 2001. ?Named Entity Recognition 
using Machine Learning Methods and Pattern-Selection 
Rules?, Proceedings of the Sixth Natural Language 
Processing Pacific Rim Symposium. 
Tjong Kim Sang, Erik F., Walter Daelemans, Herv? D?jean, 
Rob Koeling, Yuval Krymolowski, Vasin Punyakanok, 
Dan Roth, 2000. ?Applying System Combination to Base 
Noun Phrase Identification?, In the proceedings of the 
18th COLING. 
Uchimoto, Kiyotaka, Qing Ma, Masaki Murata, Hiromi 
Ozaku and Hitoshi Isahara, 2000. ?Named Entity 
Extraction Based on A Maximum Entropy Model and 
Transformation Rules", In proceedings of the 38th 
Annual Meeting of the Association for Computational 
Linguistics. 
Wacholder, Nina, Yael Ravin and Misook Choi (1997) 
"Disambiguation of Proper Names in Text", Proceedings 
of the 5th Applied Natural Language Processing 
Conference. 
Yarowsky, David, 1995. "Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods", In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 
