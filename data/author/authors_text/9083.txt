The Research of Word Sense Disambiguation Method Based on 
Co-occurrence Frequency of Hownet* 
Erhong Yang, Guoqing Zhang, and Yongkui Zhang 
Dept of Computer Science, Shanxi University, 
TaiYuan 030006 ! P. R. China 
Email: zyk@sxu.edu.cn 
Abstract 
Word sense disambiguafion (WSD) is a difficult 
problem in natural language processing. In this 
paper, a sememe co-occurrence frequency 
based WSD method was introduced. In this 
method, Hownet was used as our information 
source , and a co-occurrence frequency 
database of sememes was constructed and then 
used for WSD. The experimental result showed 
that this method is successful. 
Keywords 
word sense disambiguation, Hownet, sememe, 
co-occurrence 
1. Introduction 
Word sense disarnbiguafion (WSD) is one of 
? the most difficult problems in NLP. It is helpful 
and in some instances required for such 
applications as machine translation, information 
retrieval, content and thematic analysis, 
hypertext navigation and so on. The problem of 
WSD was first put forward in 1949. And then 
in the following decades researchers adopted 
many methods to solve the problem of 
automatic word sense disambiguation, 
including:l) AI-based method, 2) knowledge- 
based method and 3) corpus-based method. 01 
Although some useful results have been got, the 
problem of word sense disambiguation is far 
from being solved. 
The difficult of WSD is as follow: 1) 
Evaluation of word sense disambiguation 
systems is not yet standardized. 2) The potential 
for WSD varies by task. 3) Adequately large 
sense-tagged data sets are difficult to obtain. 4) 
The field has narrowed own approaches, but 
only a little. \[21 
In this paper, we use a statistical based method 
to solve the problem of automatic word sense 
disambiguafion. \[31 In this method, a new 
knowledge base- . . . . .  Hownet t4'5\] was use as 
knowledge resources. And instead of words, the 
sememes which are defined in Hownet were 
used to get the statistical figure. By doing this, 
the problem of data sparseness was solved to a 
large degree. 
2. A Brief Introduction Of Hownet 
Hownet is a knowledge base which was 
released recently on Intemet. In Hownet, the 
concept which were represented by Chinese or 
English words were described and the relations 
between concepts and the attributes of concepts 
were revealed. In this paper, we use Chinese 
knowledge base, which is an important p.art of 
Hownet, as the resource of our disambiguafion. 
The format of this file is as follow: 
W_X =word 
E_X = some examples of this word 
G X= the pos of this word 
DEF= the definition of this word 
"This research project is supported by a grant from Shanxi Natural Science Foundation fChina 
60 
A important concept used in Hownet that 
we must introduce is sememe. In Hownet, 
sememes refer to some basic unit of senses. 
They are used to descnbe all the entries in 
Hownet and there are more than 1,500 sememe 
all together. 
3. Sense Co-occurrence Frequency 
Database 
It is well known that some words tend to 
co-occur frequently with some words than with 
others\[6\]. Similarly, some meaning of words 
tend to co-occur more often with some meaning 
of words than with others. If we can got the 
relations of word meanings quantitatively, it
would have some help on word sense 
disambiguafion. In Hownet, all words are 
defined with limited sememes and the 
combination of sememes i fixed. If we make 
statistic on the co-occurrence frequency of 
sememe so as to reflect the co-occurrence of 
words, the problem of data sparseness would be 
solved to a large degree. Based on the above 
thought, we built a sense co-occurrence 
frequency database to disambiguate word 
senses. 
3.1 The Preproeessing Of Hownet 
The Hownet we downloaded from Intemet is in 
the form of plain text. It is not convenient for 
computer to use and it must been converted into 
a database. In the database, ach lexical entry is 
converted into a record. The formalization 
description of the records is as follow: 
<lexical entry> ::= <NO.><morphology> 
<part-of-speech><definifion> 
Where NO. is the corresponding umber of 
this lexical entry in Hownet. And the definition 
is composed of several sememes (short for SU) 
which were divided by comma. In addition, we 
have deleted the Engfish sememees in order to 
saving space and speeding up the processing. 
Here are some examples after preprocessing: 
NO. Morphology 
21424 t~tSb 
18888 
18889 
18887 
18890 
Part-of-speech 
ADJ  
ADJ  , 
V 
V 
N 
definition 
I~ ,~,~,~ 
3.2 The Creation Of Sememe Co-occurrence 
Frequency Database 
The sememe co-occurrence frequency database 
is the basic of sense disambiguafion. Now we 
will introduce it briefly. 
The sememe co-occurrence frequency 
database is a table of two dimension. Each item 
corresponding to the co-occurrence frequency 
of a pair of sememes. 
Before introducing the sememe 
co-occurrence frequency database, we gave the 
following definition: 
Definiton: suppose word W has m sense 
items in hownet, and the corresponding 
definition of  each sense item is: Yn, Y\]2, .... Y1(,1); 
Y21, Y22, .... Y2(,a); ...; Ym\],Ym2, .... Y~,~> 
respectively. We call \[Yu,Y~ .... Yioada 
sememe set of  W(short for  SS), and call \[{ ym 
YI2, .... Yl(,a)},{ Y21, Y22, .... Y2(,a)}, .... 
\[ Yml.Ym2, .... y.c~m)}}the s meme xpansion of  
W (short for  SE). 
For example, in the above mentioned 
example, the word "~f l ' "  has only one sense 
item. The corresponding sememe set of this 
61 
sense item is {\]~'\]~i,~.l . l : ,~,~} andthe 
sememe xpansion of "~1"" is {()~'l~i, 
~.1.1:,@, ~} } . The word "~" has four sense 
items, and the corresponding sememe set of 
each item is { )~ i~,~,~,~},  {~.~-}, {~ 
} and { ~3~,,'~ } respectively. The sememe 
expansion of word "~"  is {{)~'l~,~ff~; 
~,~}, {~:} ,  { :~},  {?,,~,:~}}. 
When building the sememe co-occurrence 
frequency database, the corpus is segmented 
first and each word is tagged with its sememe 
expansion in Hownet. Then for each unique 
pair of words co-occurred in a sentence (here a 
sentence is a string of characters delimited by 
punctuations.), the co-occurrence data of 
sememes which belong to the definition of each 
words respectively were collect, when 
coUecting co0occurrence data, we adopt a 
principle that every pair of word which 
co-occurred in a sentence should have equal 
contribution to the sememe co-occurrence data 
regardless of the number of sense items of this 
word and the length of the definition. Moreover, 
the contribution of a word should be evenly 
distributed between all the senses of a word and 
the contribution of a sense should been evenly 
distributed between all the sememe in a sense. 
The algorithm is as follow: 
1.Initial each cell in the  sememe 
co-occurrence frequency database(short for 
SCFD) with 0. 
2.For each sentence S in training corpus, do 
3-7. 
3.For each word in sentense S, tag the 
sememe xpansion to it. 
4.For each unique pair of sememe 
expansion (SEi, SEj), do 5-7. 
5.For each sememe SUimp in each sememe 
set SSim in S~, do 6-7. 
6.For each sememe SUjm in each sememe 
set SSj, in SEj, do 7. 
7.Increase the value of cell SCFD(SUimp, 
SUjnq) and SCFD(SUjnq,SUimp) by the product 
of w(SUimp) and w(SUj~), where w(SUxyO is 
weight of SUxyz given by 
1 w(su >--ls ,l?lss l 
It can be concluded from the above 
algorithm that the SCFD are symmetrical. In 
order to saving space and speeding up the 
processing, we only save those cells (SUi,SUj) 
that satisfying SUi~<SUj. 
3.3 The Sememe Co-occurrence Frequency 
Database Based Disambiguafion Method 
3.3.1 The Sememe Co-occurrence Frequency 
Based Scoring Method 
When disambiguate a polysemous word, we 
given the following equation as the score of a 
sense item of the polysemous word and the 
context containing this polysemous word. The 
context of the word is the sentence containing 
this word. 
score(S, C) 
(1) = score(SS, C') - score(SS, GlobalSS) 
Where S is a sense item of polysemouse 
word W, C is the context containing W, SS is 
the corresponding sememe set of S, C' is the set 
of sememe expansion of words in C and 
GlobalSS is the sememe set that containing all 
of the sememe defined in Hownet. 
score(SS, C') = vsE~c~ SC?re(SS'SE')/lC' I (2) 
for any sememe set SS and sememe 
expansion set C'. 
score(SS, SE') = max score(SS, SS') (3) 
SS" eSE' 
for any sememe set SS and sememe 
expansion SE'. 
score(SS, SS') = vsuX ss. 
(4) 
for any sememe set SS and SS'. 
62 
score(SS,SU')=vsu~JsCOre(SU,SU') / S I
(5) 
for any sememe set SS and sememe SU'. 
score( SU , SU') = I ( SU, SU') (6) 
for any sememe SU and SU'. 
I (SU, SU') = log 2 f (SU,SU'). N ~ 
g(SU), g(SU') (7) 
Where f(SU,SU') is the co-occurrence 
frequency corresponding to sememe pair (SU, 
SU' ) in SCFD. And for g(SU) and N, we have 
the following equation: 
g(SU) = ~f(SU,SU')  (8) 
vsu' 
N= vsu~ f ( SU' SU') (9) 
In equation (7), the mutual-information- 
like measure deviated from the stardard 
mutual-information measure by multiple a extra 
multiplicative factor N, this is because that the 
scale of the corpus is not large enough that the 
mutual-information f some sememes pairs 
would be negtive if it was not normalized by a 
extra multiplicative factor N. In equation (9), 
the sum of f(SU, SU') was divided by 2, this is 
because for each pair of sememes, 
~ f (SU, SU') is increaseby2. 
VSU,VSU" 
When disambiguation, we tag the sememe 
T that satisfying the following equation to 
polysemous word W. 
T = arg max score(S, C) (10) 
s 
3.3.2 The Creation Of Mutual Information 
Database 
We have created a mutual information database 
according to (7),(8) and(9) Here is some 
examples: 
The examples in table 1 have a high mutual 
information. The sememe pairs in this table 
have certain semantic relations. While the 
examples in table 2 have a low mutual 
information. And the sememe pairs in this table 
have no patency semantic relations. 
Table 1 example of sememe pairs which have a high mutual information 
Sememe 1 Sememe 2 Mutual-Inf?rml Sememe 1 Sememe 2 Mutual-Informa 
ation \[ t ion 
~~.  ,~-~ 33.811057 :~'I~ :~'~ 27.418417 
~ ~gk: 29.441937 ~ ~ 27.234630 
5~ ~ 28.024560 ~ ~: 27.093292 
,~Ir~ 28.023521 'I~,~:~j ~ ~ 26.984521 
~ ~ 27.571478 {~i ~ 26.710478 
Table 2: example of sememe pairs which have a low mutual information 
Mutual-Inform Mutual-Inform Sememe 1 Sememe 2 Sememe I Sememe 2 ation %ion 
"~r~n i~ 8.693242 ~J(~ ~g 9.171023 
.~=t ~ 8.754611 ~ ~ 9.357734 
:~Z \[\] 8.793914 ~\[~\]~ IS  9.448947 
~'~ ~3~ 9.121846 ~-~\]~ ~-~ 9.528801 
}~\]~J 9.150412 ,~ ~.  9.599495 
It can been concluded from ruble 1 and table 2 that the mutual information can reflect 
63 
the tightness of semantic relations. 
4. Experiment And Analysis 
We did the experiment on a corpus of 10,000 
characters from People's Dialy. 
Firstly, the corpus is segmented, and then 
the sememe co-occurrence frequecny database 
and mutual information database is created. In 
the mutual-informationdatabase, th re is 
709,496 data items corresponding to different 
sememes pairs. In order to speeding up the 
processing, the mutual-information database 
was sorted and indexed according to the first 
two bytes of each sememe pair. At last the 
experiment of disambiguation of some 
polysemous words was done. Here is two 
examples: 
Example 1: 
Example 2 :~1:~1~1~1~"~1~11~1 
? 1--1 I 
We use the following euqation to access the 
accuracy ratio of disambiguafion: 
accuracy ratio = the number of correctlytagged xample~ 
the total number of examplesin testing se; 
(11) 
the experimental result is shown in table 4. 
Tab~3: Two examples that disambiguate using sememe co-occurrence frequency database 
The definition of 
word " ~ " 
The score of sense items and 
the context of word "~"  in 
example 1 
The score of sense items and 
the context of word "~"  in 
example 2 
~3~'-~ 14. 459068 8. 659968 
Z~'F~ 9. 817648 i0. 817648 
M'I~ -~ 7. 415986 12. 415986 
~ ~ -0. 134779 -0. 134779 
i,~3~: :k~/..W... ~ji,~" ~.i..~$.~ -0. 818518 -0. 818518 
~9k:~ 14. 459068 12. 415986 
Table 4: the experiment result 
Total number of testing The number of correctly Accurracy 
examples tagged examples rat io 
Close test I00 75 75% 
Open test I00 71 71% 
The disambiguation method introduced 
above have the following charatristics: 
(1) The problem of data spraseness i
solved in a large degree. 
(2) This disambiguation method avoids 
the laborious hand tagging of training corpus. 
(3) This method can been easily applied 
to other kind of corpus. 
Reference 
\[1\]. Nancy Ide, Jean Veronis, Introduction to 
the Special Issue on Word Sense 
Disambiguation: The State of the Art, 
Computational Linguistics, 1998, Volume 
24, number 1, pp 1-40 
\[2\]. Philip Resnik, David Yarowsky, A 
Perspective on Word Sense 
Disambiguation Methods and their 
64 
Evaluation, 
http://www.cs.jhu.edu/~yarowsky/pubs.ht 
ml 
\[3\]. Alpha K. Luk, Statistical Sense 
Disambiguation with Relatively Small 
Corpus Using Dictionary Definitions, 33rd 
Annual Meeting of the Association for 
Computational Linguistics,26-30 June, 
1995, Massachusetts Institute of 
Technology, Cambridge, Massachusetts. 
USA, pp.181-188 
\[411 -~'~3}~, i .~?~j~3~j2~II~I l j ,R~j~f~, 
~,~3~/~ff\], 1998 ~lz~ 3 ~, ,~,~ 27 ~, 
pp.76-82 
\[5\]. ~ ,  ~11~, http://www.how-net.com. 
\[6\]. Kenneth Ward Church, Word Association 
Norms, Mutual Information, and 
Lexicography, Computational Linguistics, 
1990,Volume 16, Number 1, pp.22-29 
65 
Proceedings of the 8th Workshop on Asian Language Resources, pages 72?79,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
The Annotation of Event Schema in Chinese 
Hongjian Zou1,  Erhong Yang1, Yan Gao2, Qingqing Zeng1 
1Institute of Applied Linguistics, Beijing Language and Culture University 
2Zhanjiang Normal University 
hongjianzou@gmail.com, yerhong@blcu.edu.cn 
 
Abstract 
We present a strategy for revealing 
event schema in Chinese based on the 
manual annotation of texts. The overall 
event information is divided into three 
levels and events are chosen as the ele-
mentary units in annotation. Event-level 
annotation content and the obtaining of 
events patterns are explored in detail. 
The discourse-level annotation, annota-
tion of relations between events and an-
notation of the functional attributes pro-
vide a simple way to represent event 
schema. 
1 Introduction 
When we want to understand a report on occur-
rences, we need to catch the following informa-
tion: the categorization of events, the relation-
ships between them, the participants and the 
attributes of the events such as polarity and 
modality, the attitudes towards the events and 
the following actions or consequences. Only the 
information above cannot be the precisely de-
scried. Furthermore, we need to form a schema 
which incorporates all of the above, that is, to 
compile all this information together to get the 
integral structure about the report. 
The available annotated corpora concerning 
the different types of information mentioned 
above include: the event-annotated corpora such 
as ACE corpora, the corpora annotating tempor-
al information such as TimeBank, the corpora 
annotating event factuality such as FactBank, 
the corpora annotating various types of dis-
course relations such as RST corpus and Penn 
Discourse TreeBank. Meanwhile, we lack the 
annotation of event schema, which is important 
for providing the integral meaning of the reports. 
Currently for Chinese language, the annota-
tion of event information corpora is just begin-
ning and still far from being sufficient, when 
compared with English, hence it needs further 
exploration. 
2 Related Work 
The work and theories concerning event schema 
annotation can be divided into three categories. 
The first kind is focused on annotation of the 
event argument structure, such as in ACE. The 
second kind is focused on annotation of the 
temporal information and event factuality. The 
last is focused on the annotation of the relations 
among different discourse units such as RST 
corpus and Penn Discourse TreeBank. 
ACE(2005) is an in-depth study of research 
oriented annotated corpus for the purpose of 
textual information extraction. The annotation 
task includes event annotation besides the anno-
tation of entities, values and relations between 
entities. The event annotation is limited to cer-
tain types and subtypes of events, that is, Life, 
Movement, Transaction, Business, Conflict, 
Contact, Personnel, and Justice. The argument 
structure of events including participants and 
other components such as time and place are 
predefined and tagged. Besides these, four kinds 
of attributes of events, polarity, tense, genericity 
and modality, are tagged. The expression cha-
racters of events, including the extent and the 
triggers, are also tagged.  
TimeML(Pustejovsky et al, 2003; TimeML, 
2005) is a system for representing not only all 
events but also temporal information. The 
events tagged are not limited to certain types as 
in ACE, but are classified in a different way. 
Event tokens and event instances are distin-
72
guished and tagged respectively. For each event 
instance, four kinds of attributes, namely, tense, 
aspect, polarity and modality are tagged. 
TimeML defines three kinds of links between 
events and times. TLINK represents temporal 
relationships, including simultaneous, before 
and after. SLINK represents subordinative rela-
tionships. And ALINK represents relationships 
between an aspectual event and its argument 
event. Several TimeML corpora have been 
created now, including TimeBank and 
AQUAINT TimeML Corpus. 
FactBank(Roser and Pustejovsky, 2008, 2009; 
Roser, 2008) is a corpus that adds factuality in-
formation to TimeBank. The factual value of 
events under certain sources is represented by 
two kinds of attributes, modality and polarity.  
Besides the annotation of events and their 
temporal relationships or factuality information, 
there are various types of discourse annotation, 
which can be divided into two trends: one under 
the guidance of a certain discourse theory(such 
as RST) and the one independent of any specific 
theory(such as PDTB). 
RST (Mann and Thompson, 1987; Taboada 
and Mann, 2006) was originally developed as 
part of studies on computer-based text genera-
tion by William Mann and Sandra Thompson in 
1980s. In the RST framework, the discourse 
structure of a text can be represented as a tree. 
The leaves of the tree correspond to text frag-
ments that represent the minimal units of the 
discourse; the internal nodes of the tree corres-
pond to contiguous text spans; each node is cha-
racterized by its nuclearity and by a rhetorical 
relation that holds between two or more non-
overlapping, adjacent text spans. RST chooses 
the clause as the elementary unit of discourse. 
All units are also spans, and spans may be com-
posed of more than one unit. RST relations are 
defined in terms of four fields: (1) Constraints 
on the nucleus; (2) Constraints on the satellite; 
(3) Constraints on the combination of the nuc-
leus and the satellite; and (4) Effects. The num-
ber and the types of relations are not fixed. It 
can be reduced or extended. Carlson et al (2003) 
describes the experience of developing a dis-
course-annotated corpus grounded in the 
framework of Rhetorical Structure Theory. The 
resulting corpus contains 385 documents se-
lected from the Penn Treebank.  
Penn Discourse TreeBank(Miltsakaki et al, 
2004; Webber et al, 2005) is to annotate the 
million-word WSJ corpus in the Penn TreeBank 
with a layer of discourse information. Although 
the idea of annotating connectives and their ar-
guments comes from the theoretical work on 
discourse connectives in the framework of lexi-
calized grammar, the corpus itself is not tied to 
any particular theory. Discourse connectives 
were treated as discourse-level predicates of 
binary discourse relations that take two abstract 
objects such as events, states, and propositions. 
The two arguments to a discourse connective 
were simply labeled Arg1 and Arg2. 
3 The Levels and Elementary Unit of 
Event Schema Annotation 
3.1 The Elementary Unit of Event Schema 
Annotation 
What counts as an elementary unit of Event 
Schema annotation in Chinese? 
It is common to set sentences or clause as the 
basic units in discourse annotation such as RST 
corpus. However, there will be certain limita-
tions if we choose sentences or clauses as the 
elementary units of Chinese event schema anno-
tation: 
First, a Chinese sentence is generally defined 
as a grammatical unit that has pauses before and 
after it, a certain intonation, and expresses a 
complete idea. But the definition is not exact or 
operational. The only notable borders of Chi-
nese sentences in writings are the punctuations 
at the end of the sentences. The same is true of 
clauses in Chinese. 
Second, there is generally more than one 
event in a sentence or a clause in Chinese. 
Hence, if we choose sentences or clauses as the 
basic units of event schema annotation, the rela-
tions between the events in one sentence/clause 
cannot be described in detail. For example: 
1. ?? 24 ??????????????
?????????????? 62 ???
??? (In less than 24 hours, a fire swept 
through an old people?s home in the Black Sea 
coast of southern Russia and killed at least 62 
people.) 
2. ????????? 22 ???????
???????????????????
??????(Earthquakes have hit the Aysen 
73
region in southern Chile frequently since the 
22nd. The government has declared the region 
to be a state of "early warning".) 
In example 1, there are two events in bold 
type: the fire and the death in one sentence. In 
example 2, there are also two events in a single 
sentence: the earthquake and the declaration. 
The ?event? in this paper covers the same 
meaning defined by ACE(2005), which refers to 
"a specific occurrence involving participants".  
Zou and Yang(2007) shows that an average 
of 2.3 times events per sentence are reported in 
Chinese texts and hence chose events as the ba-
sic discourse unit in their annotation. This con-
sideration also fits the elementary unit of event 
schema annotation. 
3.2 Three Levels of Event Schema Annota-
tion 
The overall event information in a report is 
complex and consists of different levels. In or-
der to simplify the annotation task, we first di-
vide the total event information into three levels, 
that is, the discourse level, the event level, and 
the entity level, choosing the event as the ele-
mentary unit of the event schema annotation. 
The event level is defined as the level relat-
ing to atomic events. A report of occurrences 
always has many related events that are very 
easy to recognize. The events are atomic, which 
means the events are divided into small and mi-
nimal events. For example, when reading a re-
port about an earthquake that happened in Haiti, 
the reader will not only know about the earth-
quake itself, but also other relating happenings 
such as the number of casualty or the following 
search and rescue. These things are divided into 
different atomic events, though they are still 
linked closely.  
The entity level means the entities, times, 
and locations that are involved in events. For 
example, in ?China rescues 115 from a flooded 
mine?, ?China? is the agent of the rescue; 
?115(miners)? are the recipients; ?a flooded 
mine? is the location. These three entities are 
the arguments of the rescue event and should be 
annotated before tagging them as the arguments 
of the rescue event.  
The discourse level is the level above the 
event level which creates the integral meaning 
of the event schema. For example, the report 
concerning the rescue of miners from a flooded 
mine involves the rescue, the coalmine accident 
and possibly injuries. These events are linked 
together but have different significances within 
the report. So it is necessary to annotate the dif-
ferent significances of the events, as well as re-
lations between events. 
The following passages discuss in detail the 
event-level and the discourse-level annotation, 
while the entity-level annotation will not be dis-
cussed considering its relative simplicity. 
4 Event-level Annotation 
4.1 Definition of Events 
ACE(2005) defines an event as follows: An 
event is a specific occurrence involving partici-
pants. An event is something that happens. An 
event can frequently be described as a change of 
state. According to ACE?s definition, we define 
event as the following: An Event is an occur-
rence that catches somebody's attention and a 
change of state. 
4.2 Obtainment of  Event Patterns 
The event patterns are the argument structures 
of certain types of events, which are the direc-
tors of argument annotation. They are extracted 
from large-scale texts category by category. The 
above categories are based on the classification 
of sudden events. In other words, sudden events 
are divided into 4 categories: natural disasters, 
accidental disasters, public health incidents, 
and social security incidents, and each category 
includes different types of events, for example, 
the natural disasters includes earthquakes, tsu-
namis, debris flows and so on. In dealing with a 
specific kind of texts, only the closely related 
events that appear frequently are annotated. For 
example, when annotating the events of earth-
quake, only earthquake itself and closely related 
events such as loss, rescue, etc, are annotated.  
The event patterns are manually extracted 
from real texts as follows, taking earthquake for 
instance: 
? A search engine is used to obtain the reports 
whose titles and main bodies contain the key 
word ?earthquake?, and then manually filter out 
those texts whose topics are not; 
? The remaining texts are then split into sen-
tences and only the sentences that narrate an 
earthquake or are closely relate to the earth-
quake are selected; 
74
? Specific entities in these sentences are re-
placed with general tags such as ?<TIME>?, 
?<PER>? and ?<LOC>? to get the patterns for 
earthquake type events; 
? Frequently used patterns for earthquake 
events are extracted from the descriptions; 
? The arguments of the event are numbered in 
sequence, and given corresponding explanations; 
? The arguments are appended to event pat-
terns when new roles are found. 
The following principles should be abided by 
when extracting event patterns: 
? Event triggers are the words or expressions 
that indicate existence of an event or events. If 
there is an event trigger in a sentence, we con-
sider that there exists a corresponding event; 
? Event triggers of different categories indi-
cate different kinds of events; 
? Some arguments of an event can be indis-
tinct in a sentence. In other words, the different 
roles of the same event need to be merged into 
different patterns to get the complete argument 
structure of a certain event. 
Some arguments are common roles in many 
events, such as time, location, and some argu-
ments are specific to some events, such as the 
magnitude, and the focus of an earthquake. Af-
ter the extraction of a certain amount of patterns, 
we can then merge the similar events. So far, we 
have obtained 31 categories of event patterns 
for 4 topics of news events. 
Here is the event pattern corresponding to the 
earthquake event type extracted: 
 
arg0 Time 
arg1 Location 
arg2 Magnitude 
arg3 Epicenter 
arg4 Focus 
arg5 Focal depth 
arg6 Quake-feeling locations 
arg7 Frequency 
Table 1. The earthquake event pattern. 
4.3 Annotation of Types and Arguments 
After obtaining the event patterns, we can anno-
tate the types and the arguments of events ac-
cording to the predefined types and patterns. If a 
certain event is not yet defined, the annotator 
should tag the event as ?Other? and retag it later 
after obtaining the pattern of that category pro-
vided that the category is not too rare in similar 
reports.  
  The annotation of arguments consists of two 
steps. Firstly, we locate the entities and other 
expressions that belong to the arguments of a 
certain event. Then, we locate the roles of fixed 
arguments according to the corresponding event 
pattern. The arguments of an event are sought in 
the scope of the sentence in which the event 
trigger appears. 
  For example, according to the earthquake 
event pattern listed before, the annotation of the 
following sentence would be as follows: 
??????????????????
?? 12??? 4? 53?????????
???????? 16 ?????????
10 ????????? 7.0 ??(The earth-
quake, with a magnitude estimated at 7.0, struck 
Haiti at 4:53 p.m. local time and was centered 
about 16 kilometers southwest of Port-au-
Prince, at a depth of 10 km, the U.S. Geological 
Survey reported. ) 
 
arg0 Time 
???? 12??? 4?
53?(about 4:53 p.m. local 
time) 
arg1 Location  
arg2 Magnitude ?? 7.0? (7.0) 
arg3 Epicenter 
???????????
16??? (16 kilometers 
southwest of Port-au-
Prince) 
arg4 Focus  
arg5 Focal depth 10?? (10 km) 
arg6 Quake feeling 
locations  
arg7 Frequency 1 
Table 2. The annotation of the Haiti Earthquake. 
4.4 Annotation of Event Attributes 
Besides the types and arguments, the attributes 
of events are also tagged, which is necessary for 
a comprehensive description of events. Based 
on the analysis of various attributes in the re-
ports, we decided to annotate the following: 
Polarity, Modality, Tense, Aspect, Level, Fre-
quency, Source, and Fulfillment. Among these 
attributes, Polarity, Modality and Tense are 
adopted by both ACE and TimeML. Aspect, 
Frequency and Source are adopted by TimeML. 
The primary reason for annotating these 
attributes is that they have an important role in 
75
describing events in detail and different values 
of some attributes can even imply a totally dif-
ferent meaning. 
Polarity is whether the event happened or 
would happen. The value of polarity can only be 
one between ?Positive? and ?Negative?. For 
example, in 
???????????????? (Fortu-
nately the fires did not result in any casualties) 
the polarity of event ??? ?(injuries-or-
deaths) is ?negative?. 
Modality is the possibility of the event. Cur-
rently, we divide modality simply into ?As-
serted? and ?Other?. For example, in 
???????????????? (Many 
residents in earthquake-hit areas worry about a 
recurrence of the tsunami) 
the modality of event ??? ?(tsunami) is 
?Other?. 
Tense is the time the event happened com-
pared with the time of the report. It can be 
?Past?, ?Present?, ?Future?, or ?Underspeci-
fied?. For example, in 
????????????(A Police investiga-
tion is under way) 
the tense of event ???? (investigation) is 
?Present?. 
Aspect is whether or not the event is continu-
ing or completed. It can be ?Progressive?, ?Per-
fective? or ?Underspecified?. In the sentence 
above, the aspect of event ???? (investigation) 
is ?Progressive?. 
Level is the extent of the events. It can be 
?Serious?, ?Medium? or ?Slight?. If the annota-
tor cannot make sure, it can also be ignored. For 
example, in 
?????????? (Strong earthquake hits 
Indonesian) 
the level of the event ???? (earthquake) is 
?Serious?. 
Frequency is how many times the event hap-
pened. Usually it is only once, yet sometimes, 
as mentioned above, it may be twice or more. 
Source consists of the source of the informa-
tion about a certain event and the time the in-
formation issued. If not specialized, the source 
is equal to the source of the report itself and the 
time of source is equal to the time that the report 
was issued. For example, in 
????? 10 ???? (according to state-
ments by the Paris police on 10th) 
the source is ??????(the Paris police) 
and the time issued is ?10??(the 10th). 
Fulfillment is an interesting attribute of 
events that deserves further study and will be 
discussed in another paper. This is an attribute 
which is only applicable to man-made events 
with an emphasized intention, in other words, it 
is not applicable to those events occurring natu-
rally. It can be ?Fulfilled?, ?Unfulfilled?, or 
?Underspecified?. For example, a rescue event 
is deliberate and has or will have a result. For 
example, in 
????????? 8??? 115???? 
(China rescues 115 from flooded mine after 8 
days) 
the fulfillment of the event ???(rescue) is 
?Fulfilled?. 
The complete attributes of an event can be 
represented as a complex feature set as shown 
below: 
 
?
?
?
?
?
?
?
?
?
?
?
?
Polarity?Positive/Negative
Modality?Asserted/Other
Tense?Past/Present/Future/Underspecified
Aspect?Perfective/Progressive/Underspecified
Level?Slight/Medium/Serious
Frequency?n?n ? 1?
Source? ?
?time?1?Source?1?
??
?time?m?Source?n?
Fulfillment??Fulfilled/Unfulfilled/Underspecified
??
?
?
?
?
?
?
?
?
?
?
?
?
 
Figure 1. The complex feature set of attributes. 
4.5 Annotation of Indicators 
The recognition of types, arguments and 
attributes of the events not only depends on the 
sense of the annotator, but also depends on lin-
guistic indicators within the text. To locate the 
existences of an event and its types, the annota-
tor should find the lexical evidence that we 
called an Event Word (ACE call it a trigger) 
which clearly indicates something that has hap-
pened. In the following sentence, 
???? 10 ???????????
???? 10 ?????????????
? 2 ?????????????????
????? (According to statements made by 
the Paris police on the 10th, serious fire swept 
through an apartment building in district one in 
76
Paris on the morning of the 10th, killing at least 
2 women, seriously injuring two firemen and 
causing huge property damage.) 
The Event Words ? ? ? ?(fire), ? ?
? ?(killing), ? ? ? ?(injuring) and ? ?
? ?(damage) in the sentence above indicate 
four events respectively. 
Besides annotating Event Words for events, 
the annotator also needs annotating indicators 
from texts to help to locate the attributes of the 
events. The attributes annotated should be clear-
ly indicated by some linguistic hints, so the val-
ue of a certain attribute will not be specified if 
the hints are not so clear. 
5 Discourse-level Annotation 
The purpose of discourse level annotation is to 
integrate the information from the event-level 
into a structure. We annotate two kinds of dis-
course information, the relationships among 
events as annotated before and the functional 
attributes of events, to represent the event 
schema. 
5.1 Annotation of Relations among Events 
The events in the same report are not self-
sufficient or independent, but are linked by var-
ious relationships, such as the causal relation-
ships between an earthquake and an injury. 
Taking into account of both the frequency of 
relationships between events and the ease and 
accuracy of distinguishing them, we have de-
cided to focus on the following: causality, co-
reference, sequential, purpose, part-whole, jux-
taposition and contrast. 
Causality is very common in reports. If event 
A is responsible for the happening of event B, 
then there exists a causal relationship between 
A and B. For example, in 
?????????????? 7.0 ???
???????????????????
????? (A magnitude 7.0 earthquake hit 
Haiti, causing a hospital to collapse and da-
maging government buildings in the capital city 
of Port-au-Prince.)  
there are three events, called ? ?
??(earthquake), ????(collapsing) and ??
??(damaging), and a causal relationship be-
tween ???? and ???? /????. 
Co-reference is not the relationship between 
two different events but the relationship be-
tween two expressions of events that refer to the 
same object.  
Sequential is the relation between A and B 
such that B follows A chronologically but there 
is not necessarily a causal relationship between 
them. For example, in 
???????????????? 22 ?
?? 1? 17??????????????
???????????????? (A 22-
year-old woman died of illness on Jan. 17 in 
Lagos, Nigeria's southern economic hub. After 
being tested by the Nigerian health sector, it 
was found that the woman had died of bird flu.) 
the events ????(death) and ????(testing) 
have sequential relationship. 
Purpose is the relation between A and B that 
A happened for B. For example, in 
  ???????????????????
???????????????????
???? (The Nigerian government has already 
strengthened hygienic supervision and regula-
tion nationwide to control the spread of the 
highly pathogenic avian influenza.) 
the purpose of the event ????(supervision) 
is to ????(control). 
Part-whole relationship between A and B is 
when B is part of A. For example, in 
??????????????????
???????? 138 ?????????
??? 116 ???????? 22 ????
86 ????? (Saomai caused significant ca-
sualties in Fuding: at least 138 people have 
been killed so far, including 116 at sea, and 22 
were on land, with 86 missing.) 
the event ???? (killed) appeared first and 
is part of the event ???? (casualties). 
Juxtaposition relationship means that A and 
B are caused by the same thing, or that A and B 
are simultaneous. For example, in 
??????????????????
???????????????????
????????? (Datong, Zuoyun authori-
ties have made proper arrangements for the 
families of trapped miners. Meanwhile, the de-
partment for environmental protection has been 
monitoring water quality.) 
77
the ????(arrangement) and ???? (moni-
toring) are simultaneous. 
Contrast relationship is when A would 
usually cause B, but here A happened and didn?t 
in fact cause B. For example, in 
????????? 2 ????? 5?3 ?
??????????????????? 
(A 5.3 magnitude earthquake hit the central re-
gion of Salvador on the 2nd, but caused no ca-
sualties or property losses.) 
the ????(earthquake) usually causes ??
??(casualties), but here there is no ????. 
The contrast relationship between A and B is 
not equal to the negation of a causal relationship, 
because in a contrast relationship A is positive 
and B is negative, while in the negation of caus-
al relationship, the A is negative. 
Besides those relationships between events 
described above, the annotator could tag the 
relation as ?Underspecified? if he/she feels that 
relationship belongs to a new kind and deserves 
to be annotated.  
These relations are also annotated with the 
attributes similar to those of events, but only 
including Polarity, Modality, Tense, Aspect 
and Source. 
5.2 Annotation of Functional Attributes 
The annotation of relations among events only 
represents the local discourse structure of the 
report. To represent the overall information it is 
necessary to integrate the event-level informa-
tion globally. We find that the events annotated 
in one text are not owning equal significance, 
and they can be divided into at least two basic 
kinds according to their role in expressing the 
highlight of the text. The two basic kinds of role 
we decide to tag are ?core? and ?related?. We 
call this the functional attribute of the events. 
The core events are the events that are the 
topics of the reports. Other events are the re-
lated events. If core events were removed, the 
elementary topics would change and the remain-
ing events could not easily be organized togeth-
er well. For example, in a report concerning the 
earthquake that happened in Haiti several 
months ago, the report?s core events are the 
events representing the earthquake. The other 
events such as the rescue or the injuries are not 
integral and cannot be meaningful alone. But if 
the other events were removed, the topic and 
logic of the report would still be clear, though 
the details might be somewhat incomplete. 
After annotating the relationships among 
events and functional attributes of these events, 
we can represent a report about an earthquake 
which happened in Kyrgyzstan as follow: 
 
 
Figure 2. Event schema of Kyrgyzstan earthquake. 
Nodes of 1, 3, 6, 7 and 8 represent earthquakes; Nodes 
of 2, 4, and 9 represent damage; Node 5 represents casual-
ty; Nodes 10 represents investigation. 
 
In the graph above, the nodes represent the 
events, and the edges represent the relationships 
between events. The gray nodes represent the 
core events, while the white nodes represent the 
related events. As can be seen from the graph, 
the core events are at the center of the text and 
the related events are attached to the core events. 
6 Preliminary  Results and Discussion 
In order to check the taggability of the annota-
tion strategy mentioned above, three graduate 
students manually annotated about 60 news re-
ports in 3 categories, including earthquake, fire 
and attack, using sina search engine, according 
to the method and principles above. Each text 
was annotated by two annotators and discussed 
jointly if the annotation results were inconsis-
tent or not proper.  
As can be seen from Table 3 below, 1) the 
event patterns extracted can cover the texts well 
because up to 78% sentences have been anno-
tated. 2) There are 1.6 times more annotated 
events than annotated sentences. This shows 
that there is generally more than one event in a 
sentence. So, it is reasonable to assume that the 
annotation method can accomplish the task of a 
detailed description of relationships between 
events. 3) The relevant events are more numer-
causality 
causality 
contrast 
juxtaposition 
causality 
sequence 
coreference 
coreference coreference 
coreference
coreference 
1
3
2 
4 
5 
6
7 
8
9
10 
coreference 
78
ous than the core events. This shows that it is 
necessary to distinguish the core events from 
the relevant events. 
 
C T S NS EV CE RE AR 
C1 20 277 45 361 191 170 588 
C2 20 309 66 394 183 211 515 
C3 20 356 93 401 121 280 605 
C4 60 942 204 1156 495 661 1708
Table 3. The annotation of EVENTs 
C: Sub-category; C1: earthquake; C2: fire;  
C3: terrorist attacks; C4: total 
T: the number of texts; S: the number of sentences 
NS: the number of sentences not annotated 
EV: the number of EVENTs 
CE: the number of core EVENTs 
RE: the number of relevant EVENTs 
AR: the number of arguments 
 
We have also analyzed the event attributes in 
detail(Zou and Yang, 2010). An interesting 
event attribute is Fulfillment, which is only ap-
plicable to those events with intentions whose 
result is often emphasized. Sometimes, readers 
care about the intended results or outcomes as 
much as or more than the events themselves. 
Therefore it would be useful to explore the no-
tion of Fulfillment, and investigate which lin-
guistic categories could play a role in deciding 
the value of Fulfillment. We plan to create a 
Fulfillment corpus in the next stage. 
The annotation of event schema is time-
consuming, partly because it needs to annotate 
all three levels of event information of every 
text, and partly because of the difficulties to 
identify the event information from trivial de-
scriptions, in other words, one question we of-
ten discuss is whether it deserves to annotate 
certain parts of a text. Also, we often need to 
make a balance between obtaining enough event 
patterns to cover various types of related events 
well and omitting low frequent event types to 
simply the obtainment of event patterns. In dis-
course-level annotation, the main difficulty is 
the identification of relations between events 
without lexical hints. This discourse-level anno-
tation is only just underway. We also plan to 
give detailed analysis in the next stage.  
 
Acknowledgements. This paper is sponsored 
by National Philosophy and Social Sciences 
Fund projects of China (No. 06YY047). 
 
References 
ACE. 2005. ACE (Automatic Content Extraction) 
Chinese Annotation Guidelines for Events. 
http://www.ldc.upenn.edu/Projects/ACE/docs/Chi
nese-Events-Guidelines_v5.5.1.pdf  
Carlson L., D. Marcu, M. E. Okurowski. 2003. 
Building a Discourse-Tagged Corpus in the 
Framework of Rhetorical Structure Theory. Cur-
rent Directions in Discourse and Dialogue, Jan 
van Kuppevelt and Ronnie Smith eds., Kluwer 
Academic Publishers.  
Mann W. and S. Thompson, 1987. Rhetorical Struc-
ture Theory: A Theory of Text Organization (No. 
ISI/RS-87-190). Marina del Rey, CA, Information 
Sciences Institute. 
Miltsakaki E., R. Prasad, A. Joshi, and B. Webber. 
2004. Annotating Discourse Connectives and 
their Arguments. Proceedings of the 
HLT/NAACL Workshop on Frontiers in Corpus 
Annotation. Boston, MA. 
Pustejovsky J., J. Casta?o, R. Ingria, S. Roser R. 
Gaizauskas, A. Setzer and G. Katz. 2003. TimeML: 
Robust Specification of Event and Temporal Ex-
pressions in Text.  Fifth International Workshop 
on Computational Semantics.  
Taboada M. and W. Mann. 2006. Rhetorical Struc-
ture Theory: Looking Back and Moving Ahead. 
Discourse Studies 8(3): 423-459. 
TimeML. 2005. Annotation Guidelines Version 1.2. 
http://www.timeml.org/site/publications/timeMLd
ocs/annguide_1.2.1.pdf.  
Webber B., A. Joshi, E. Miltsakaki, et al 2005. A 
Short Introduction to the Penn Discourse Tree-
Bank. Copenhagen Working Papers in Language 
and Speech Processing.  
Roser S. 2008. A Factuality Profiler for Eventuali-
ties in Text. Ph.D. Thesis. Brandeis University.  
Roser S. and J. Pustejovsky. 2008. From Structure to 
Interpretation: A Double-layered Annotation for 
Event Factuality. Prooceedings of the 2nd Lin-
guistic Annotation Workshop.  
Roser S. and J. Pustejovsky. 2009. FactBank: A 
Corpus Annotated with Event Factuality. Lan-
guage Resources and Evaluation.  
Zou H.J. and E.H. Yang. 2007. Event Counts as 
Elementary Unit in Discourse Annotation. Inter-
national Conference on Chinese Computing 2007. 
Zou H.J. and E.H. Yang. 2010. Annotation of Event 
Attributes. The 11th Chinese Lexical Semantics 
Workshop. 
79
