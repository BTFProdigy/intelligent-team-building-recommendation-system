Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 851?856,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Corpus Level MIRA Tuning Strategy for Machine Translation
Ming Tan, Tian Xia, Shaojun Wang
Wright State University
3640 Colonel Glenn Hwy,
Dayton, OH 45435 USA
{tan.6, xia.7, shaojun.wang}
@wright.edu
Bowen Zhou
IBM T.J. Watson Research Center
1101 Kitchawan Rd,
Yorktown Heights, NY 10598 USA
zhou@us.ibm.com
Abstract
MIRA based tuning methods have been
widely used in statistical machine translation
(SMT) system with a large number of fea-
tures. Since the corpus-level BLEU is not de-
composable, these MIRA approaches usually
define a variety of heuristic-driven sentence-
level BLEUs in their model losses. Instead,
we present a new MIRA method, which em-
ploys an exact corpus-level BLEU to com-
pute the model loss. Our method is simpler in
implementation. Experiments on Chinese-to-
English translation show its effectiveness over
two state-of-the-art MIRA implementations.
1 Introduction
Margin infused relaxed algorithm (MIRA) has been
widely adopted for the parameter optimization in
SMT with a large feature size (Watanabe et al, 2007;
Chiang et al, 2008; Chiang et al, 2009; Chiang,
2012; Eidelman, 2012; Cherry and Foster, 2012).
Since BLEU is defined on the corpus, and not de-
composed into sentences, most MIRA approaches
consider a variety of sentence-level BLEUs for the
model losses, many of which are heuristic-driven
(Watanabe et al, 2007; Chiang et al, 2008; Chi-
ang et al, 2009; Chiang, 2012; Cherry and Foster,
2012). The sentence-level BLEU appearing in the
objective is generally based on a pseudo-document,
which may not precisely reflect the corpus-level
BLEU. We believe that this mismatch could poten-
tially harm the performance. To avoid the sentence
BLEU, the work in (Haddow et al, 2011) proposed
to process sentences in small batches. The authors
adopted a Gibbs sampling (Arun et al, 2009) tech-
nique to search the hope and fear hypotheses, and
they did not compare with MIRA. Watanabe (2012)
also tuned the parameters with small batches of sen-
tences and optimized a hinge loss not explicitly re-
lated to BLEU using stochastic gradient descent.
Both approaches introduced additional complexities
over baseline MIRA approaches.
In contrast, we propose a remarkably simple but
efficient batch MIRA approach which exploits the
exact corpus-level BLEU to compute model losses.
We search for a hope and a fear hypotheses for the
corpus with a straightforward approach and mini-
mize the structured hinge loss defined on them. The
experiments show that our method consistently out-
performs two state-of-the-art MIRAs in Chinese-to-
English translation tasks with a moderate margin.
2 Margin Infused Relaxed Algorithm
We optimize the model parameters based on N-best
lists. Our development (dev) set is a set of triples
{(fi, ei , ri)}Mi=1, where fi is a source-language sen-
tence, corresponded by a list of target-language hy-
potheses ei = {eij}
N(fi)
j=1 , with a number of refer-
ences ri. h(e ij ) is a feature vector. Generally, most
decoders return a top-1 candidate as the transla-
tion result, such that e?i(w) = arg maxj w ? h(eij ),
where w are the model parameters. In this paper, we
aim at optimizing the BLEU score (Papineni et al,
2002).
MIRA is an instance of online learning which as-
sumes an overlap of the decoding procedure and the
parameter optimization procedure. For example in
(Crammer et al, 2006; Chiang et al, 2008), MIRA
851
is performed after an input sentence are decoded,
and the next sentence is decoded with the updated
parameters. The objective for each sentence i is,
min
w
1
2
||w ?w?||2 + C ? li(w) (1)
li(w) = max
eij
{b(e?i )? b(eij)
?w ? [h(e?i )? h(eij )]} (2)
where e?i ? ei is a hope candidate, w
? is the pa-
rameter vector from the last sentence. Since MIRA
defines its objective only based on the current sen-
tence, b(?) is a sentence-level BLEU.
Most MIRA algorithms need a deliberate defini-
tion of b(?), since BLEU cannot be decomposed into
sentences. The types of the sentence BLEU calcula-
tion includes: (a) a smoothed version of BLEU for
eij (Liang et al, 2006), (b) fit eij into a pseudo-
document considering the history (Chiang et al,
2008; Chiang, 2012), (c) use eij to replace the corre-
sponding hypothesis in the oracles (Watanabe et al,
2007). The sentence-level BLEU sometimes per-
plexes the algorithms and results in a mismatch with
the corpus-level BLEU.
3 Corpus-level MIRA
3.1 Algorithm
We propose a batch tuning strategy, corpus-level
MIRA (c-MIRA), in which an objective is not built
upon a hinge loss of a single sentence, but upon that
of the entire corpus.
The online MIRAs are difficult to parallelize.
Therefore, similar to the batch MIRA in (Cherry and
Foster, 2012), we conduct the batch tuning by re-
peating the following steps: (a) Decode source sen-
tences (in parallel) and obtain {ei}Mi=1, (b) Merge
{ei}Mi=1 with the one from the previous iteration, (c)
Invoke Algorithm 1.
We define E = (eE,1 , eE,2 , ..., eE,M ) as a corpus
hypothesis, with H (E) =
1
M
M?
i=1
h(eE,i). eE,i is the
hypothesis of the source sentence fi covered by E .
E is corresponded to a corpus-level BLEU, which
we ultimately want to optimize. Following MIRA
formulated in (Crammer et al, 2006; Chiang et al,
2008), c-MIRA repeatedly optimizes,
min
w
1
2
||w ?w?||2 + C ? lcorpus(w) (3)
lcorpus(w) = max
E
{B(E?)? B(E)
?w ? [H(E?)?H(E)]} (4)
where B(?) is a corpus-level BLEU. E? is a hope
hypothesis. E ? L, where L is the hypothesis space
of the entire corpus, and |L| = |e1| ? ? ? |eM|.
Algorithm 1 Corpus-Level MIRA
Require: {(fi, ei , ri)}Mi=1, w0, C
1: for t = 1 ? ? ?T do
2: E? = {} ,E ? = {} . Initialize the hope and fear
3: for i = 1 ? ? ?M do
4: eE?,i = arg max
eij
[wt?1 ? h(eij) + b?(eij )]
5: eE?,i = arg max
eij
[wt?1 ? h(eij)? b?(eij )]
6: E? ? E? + {eE?,i} . Build the hope
7: E ? ? E ? + {eE?,i} . Build the fear
8: end for
9: 4B = B(E?)? B(E ?) . the BLEU difference
10: 4H = H(E ?)?H(E?) . the feature difference
11: ? = min
[
C, 4B+wt?1?4H||4H||2
]
12: wt = wt?1 ? ? ? 4H
13: w?t =
1
t+ 1
t?
t=0
wt
14: end for
15: return w?t with the optimal BLEU on the dev set.
c-MIRA can be regarded as a standard MIRA,
in which there is only one single triple (F ,L,R),
where F and R are the source and reference of
the corpus respectively. Eq. 3 is equivalent to a
quadratic programming with |L| constraints. Cram-
mer et al (2006) show that a single constraint with
one hope E? and one fear E ? admits a closed-form
update and performs well. We denote one execution
of the outer loop as an epoch. The hope and fear
are updated in each epoch. Similar to (Chiang et al,
2008), the hope and fear hypotheses are defined as
following,
E? = max
E
[w ?H(E) + B(E)] (5)
E ? = max
E
[w ?H(E)? B(E)] (6)
Eq. 5 and 6 find the hypotheses with the best and
worse BLEU that the decoder can easily achieve. It
is unnecessary to search the entire space of L for
precise solution E?and E ?, because MIRA only at-
852
tempts to separate the hope from the fear by a mar-
gin proportional to their BLEU differentials (Cherry
and Foster, 2012). We just construct E?and E ? re-
spectively by,
eE?,i = max
ei,j
[w ? h(ei,j) + b?(ei,j)]
eE ?,i = max
ei,j
[w ? h(ei,j)? b?(ei,j)]
where b? is simply a BLEU with add one smoothing
(Lin and Och, 2004). A smoothed BLEU is good
enough to pick up a ?satisfying? pair of hope and
fear. However, the updating step (Line 11) uses the
corpus-level BLEU.
3.2 Justification
c-MIRA treats a corpus as one sentence for decod-
ing, while conventional decoders process sentences
one by one. We show the optimal solutions from the
two methods are equivalent theoretically.
We follow the notations in (Och and Ney,
2002). We search a hypothesis on corpus E =
{e1 ,k1 , e2 ,k2 , ..., eM ,kM } with the highest probabil-
ity given the source corpus F = {f1, f2, ..., fM},
E = arg max
E
logP (E|F)
= arg max
E
(
w ?
M?
i=1
h(ei,ki )?
M?
i=1
log(Zi)
)
(7)
= {arg max
ei,ki
w ? h(ei,ki )}
M
i=1 (8)
where Zi =
?N(fi)
j=1 exp(w ? h(ei ,j )), which is a
constant with respective to E . Eq. 7 shows that
the feature vector of E is determined by the sum of
each candidate?s feature vectors. Also, the model
score can be decomposed into each sentence in Eq.
8, which shows that decoding all sentences together
equals to decoding one by one.
We also show that if the metric is decomposable,
the loss in c-MIRA is actually the sum of the hinge
loss li(w) in structural SVM (Tsochantaridis et al,
2004; Cherry and Foster, 2012). We assume B(eij)
to be the metric of a sentence hypothesis, then the
loss of c-MIRA in Eq. 4 is,
lcorpus(w) ? max
E?
M?
i=1
[B(ei,kE? )?B(ei,kE? )
?w?h(ei,kE? ) + w ? h(ei,kE? )]
=
M?
i=1
max
eij
[B(ei,kE? )?B(eij)
?w?h(ei,kE? ) + w ? h(eij)] =
M?
i=1
li(w)
Instead of adopting a cutting-plane algorithm
(Tsochantaridis et al, 2004), we optimize the same
loss with a MIRA pattern in a simpler way. How-
ever, since BLEU is not decomposable, the struc-
tural SVM (Cherry and Foster, 2012) uses an inter-
polated sentence BLEU (Liang et al, 2006). Al-
though Algorithm 1 has an outlook similar to the
batch-MIRA algorithm in (Cherry and Foster, 2012),
their loss definitions differ fundamentally. Batch
MIRA basically uses a sentence-level loss, and they
also follow the sentence-by-sentence tuning pattern.
In the future work, we will compare structural SVM
and c-MIRA under decomposable metrics like WER
or SSER (Och and Ney, 2002).
4 Experiments and Analysis
We first evaluate c-MIRA in a iterative batch tuning
procedure in a Chinese-to-English machine transla-
tion system with 228 features. Second, we show c-
MIRA is also effective in the re-ranking task with
more than 50,000 features.
In both experiments, we compare c-MIRA and
three baselines: (1) MERT (Och, 2003), (2) Chiang
et al?s MIRA (MIRA1) in (Chiang et al, 2008). (3)
batch-MIRA (MIRA2) in (Cherry and Foster, 2012).
Here, we roughly choose C with the best BLEU on
dev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}.
We convert Chiang et al?s MIRA to the batch mode
described in section 3.1. So the only difference be-
tween MIRA1 and MIRA2 is: MIRA1 obtains mul-
tiple constraints before optimization, while MIRA2
only uses one constraint. We implement MERT
and MIRA1, and directly use MIRA2 from Moses
(Koehn et al, 2007). We conduct experiments in a
server of 8-cores with 2.5GHz Opteron. We set the
maximum number of epochs as we generally do not
observe an obvious increase on the dev set BLEU.
853
MERT MIRA1 MIRA2 c-MIRA
C 0.0001 0.001 0.0001
8 dev 34.80 34.70 34.73 34.70
feat. 04 31.92 31.81 31.73 31.83
05 28.85 28.94 28.71 28.92
C 0.001 0.001 0.001
all dev 34.61 35.24 35.14 35.56
feat. 04 31.76 32.25 32.04 32.57+
05 28.85 29.43 29.37 29.41
06news 30.91 31.43 31.24 31.82+
06others 27.43 28.01 28.13 28.45
08news 25.62 26.11 26.03 26.40
08others 16.22 16.66 16.46 17.10+
Table 1: BLEUs (%) on the dev and test sets with 8 dense
features only and all features. The significant symbols (+
at 0.05 level) are compared with MIRA2
The epoch size for MIRA1 and MIRA2 is 40, while
the one for c-MIRA is 400. c-MIRA runs more
epochs, because we update the parameters by much
fewer times. However, we can implement Line 3?8
in Algorithm 1 in multi-thread (we use eight threads
in the following experiments), which makes our al-
gorithm much faster. Also, we increase the epoch
sizes of MIRA1 and MIRA2 to 400, and find there
is no improvement on their performance.
4.1 Iterative Batch Training
In this experiment, we conduct the batch tuning pro-
cedure shown in section 3. We align the FBIS data
including about 230K sentence pairs with GIZA++
for extracting grammar, and train a 4-gram language
model on the Xinhua portion of Gigaword corpus. A
hierarchical phrase-based model (Chiang, 2007) is
tuned on NIST MT 2002, which has 878 sentences,
and tested on MT 2004, 2005, 2006, and 2008. All
features used here, besides eight basic ones in (Chi-
ang, 2007), consists of an extra 220 group features.
We design such feature templates to group gram-
mar by the length of source side and target side,
(feat type, a ? src side ? b, c ? tgt side ? d) ,
where feat type denotes any of relative frequency,
reversed relative frequency, lexical probability and
reversed lexical probability, and [a, b], [c, d] enumer-
ate all possible subranges of [1, 10], as the maximum
MERT MIRA1 MIRA2 c-MIRA
R. T. 25.8min 16.0min 7.3min 7.8min
Table 2: Running time.
length on each side of a hierarchical grammar is lim-
ited to 10. There are 4? 55 extra group features. We
also set the size of N-best list per sentence before
merge as 200.
All methods use 30 decoding iterations. We se-
lect the iteration with the best BLEU of the dev set
for testing. We present the BLEU scores in Table 1
on two feature settings: (1) 8 basic features only, and
(2) all 228 features. In the first case, due to the small
feature size, MERT can get a better BLEU of the
dev set, and all MIRA algorithms fails to generally
beat MERT on the test set. However, as the feature
size increase to 228, MERT degrades on the dev-set
BLEU, and also become worse on test sets, while
MIRA algorithms improve on the dev set expect-
edly. MIRA1 performs better than MIRA2, proba-
bly because of more constraints. c-MIRA can mod-
erately improve BLEU by 0.2?0.4 from MIRA1
and 0.2?0.6 from MIRA2. This might indicate that
a loss defined on corpus is more accurate than the
one defined on sentence. Table 2 lists the running
time. Only MIRA2 is fairly faster than c-MIRA be-
cause of more epochs in c-MIRA.
4.2 Re-ranking Experiments
The baseline system is a state-of-the-art hierarchi-
cal phrase-based system, and trained on six million
parallel sentences corpora available to the DARPA
BOLT Chinese-English task. This system includes
51 dense features (including translation probabili-
ties, provenance features, etc.) and about 50k sparse
features (mostly lexical and fertility-based). The
language model is a six-gram model trained on a
10 billion words monolingual corpus, including the
English side of our parallel corpora plus other cor-
pora such as Gigaword (LDC2011T07) and Google
News. We use 1275 sentences for tuning and 1239
sentences for testing from the LDC2010E30 corpus
respectively. There are four reference translations
for each input sentence in both tuning and testing
datasets.
We use a N-best list which is an intermediate out-
854
MIRA1 MIRA2 c-MIRA
dense dev 31.90 31.78 32.00
only test 30.89 30.89 31.07
dense dev 32.29 32.20 32.49
+sparse test 31.12 31.00 31.39
Table 3: BLEUs (%) on re-ranking experiments.
MIRA1 MIRA2 c-MIRA
about 1,966,720 35,120 400
Table 4: Times of updating model parameters.
put of the baseline system optimized on TER-BLEU
instead of BLEU. Before the re-ranking task, the ini-
tial BLEUs of the top-1 hypotheses on the tuning
and testing set are 31.45 and 30.56. The average
numbers of hypotheses per sentence are about 200
and 500, respectively for the tuning and testing sets.
Again, we use the best epoch on the tuning set for
testing. The BLEUs on dev and test sets are reported
in Table 3. We observe that the effectiveness of c-
MIRA is not harmed as the feature size is scaled up.
4.3 Analysis
To examine the simple search for hopes and fears
(Line 3?8 in Alg. 1), we use two hope/fear building
strategies to get E? and E ? : (1) simply connect each
e?i and e
?
i in Line 4?5 of Algorithm 1, (2) conduct a
slow beam search among the N-best lists of all for-
eign sentences from e1 to eM and use Eq. 5 and
6 to prune the stack. The stack size is 10. We ob-
serve that there is no significant difference between
the two strategies on the BLEU of the dev set. But
the second strategy is about 10 times slower.
We also consider more constraints in Eq. 3. By
beam search, we obtain one corpus-level oracle and
29 other hypotheses similar to (Chiang et al, 2008),
and optimize with SMO (Platt, 1998). Unfortu-
nately, experiments show that more constraints lead
to an overfitting and no improved performance.
As shown in Table 4, in one execution, our
method updates the parameters by only 400 times;
MIRA2 updates by 40 ? 878 = 35120 times; and
MIRA1 updates much more (about 1,966,720 times)
due to the SMO procedure. We are surprised to find
c-MIRA gets a higher training BLEU with such few
parameter updates. This probably suggests that there
is a gap between sentence-level BLEU and corpus-
level BLEU, so standard MIRAs need to update the
parameters more often.
Regarding simplicity, MIRA1 uses a strongly-
heuristic definition of a sentence BLEU, and
MIRA2 needs a pseudo-document with a decay rate
of ? = 0.9. In comparison, c-MIRA avoids both
the sentence level BLEU and the pseudo-document,
thus needs fewer variables.
5 Conclusion
We present a simple and effective MIRA batch tun-
ing algorithm without the heuristic-driven calcula-
tion of sentence-level BLEU, due to the indecom-
posability of a corpus-level BLEU. Our optimiza-
tion objective is directly defined on the corpus-level
hypotheses. This work simplifies the tuning pro-
cess, and avoid the mismatch between the sentence-
level BLEU and the corpus-level BLEU. This strat-
egy can be potentially applied to other optimiza-
tion paradigms, such as the structural SVM (Cherry
and Foster, 2012), SGD and AROW (Chiang, 2012),
and other forms of samples, such as forests (Chiang,
2012) and lattice (Cherry and Foster, 2012).
6 Acknowledgments
The key idea and a part of the experimental work
of this paper were developed in collaboration with
the IBM researcher when the first author was an in-
tern at IBM T.J. Watson Research Center. This re-
search is partially supported by Air Force Office of
Scientific Research under grant FA9550-10-1-0335,
the National Science Foundation under grant IIS RI-
small 1218863 and a Google research award.
References
A. Arun, C. Dyer, B. Haddow, P. Blunsom, A. Lopez,
and P. Koehn. 2009. Monte Carlo inference and maxi-
mization for phrase-based translation. In Proceedings
of the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL), 102-110.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL-HLT), 427-436.
855
D. Chiang. 2012. Hope and fear for discriminative train-
ing of statistical translation models. Journal of Ma-
chine Learning Research (JMLR), 1159-1187.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL-HLT), 218-226.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), 224-
233.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201-228.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research
(JMLR), 7:551-585.
V. Eidelman. 2012. Optimization strategies for online
large-margin learning in machine translation. Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, 480-489.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank
training for phrase-based machine translation. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics, 261-271.
P. Koehn, H. Hoang, A. Birch, C. Burch, M. Federico, N.
Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C.
Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
177-180.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, 761-768.
C. Lin and F. Och. 2004. Orange: a method for evaluat-
ing automatic evaluation metrics for machine transla-
tion. In Proc. of International Conference on Compu-
tational Linguistics (COLING), No. 501.
F. Och. 2003. Minimum error rate training in statistical
machine translation. Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
(ACL), 160-167.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics (ACL),
295-302.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. Proceedings of the 40th annual meeting on
association for computational linguistics. Association
for Computational Linguistics (ACL), 311-318.
J. Platt. 1998. Sequetial minimal optimization: A fast al-
gorithm for training support vector machines. In Tech-
nical Report MST-TR-98-14. Microsoft Research.
I. Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interde-
pendent and structured output spaces. International
Conference on Machine Learning (ICML), 823-830.
T. Watanabe. 2012. Optimized online rank learning for
machine translation. Proceedings of Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL-HLT), 253-262.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. Proceedings of Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), 764-773.
856
A Scalable Distributed Syntactic, Semantic,
and Lexical Language Model
Ming Tan?
Wright State University
Wenli Zhou??
Wright State University
Lei Zheng?
Wright State University
Shaojun Wang?
Wright State University
This paper presents an attempt at building a large scale distributed composite language model
that is formed by seamlessly integrating an n-gram model, a structured language model, and
probabilistic latent semantic analysis under a directed Markov random field paradigm to simul-
taneously account for local word lexical information, mid-range sentence syntactic structure,
and long-span document semantic content. The composite language model has been trained by
performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm
to improve word prediction power on corpora with up to a billion tokens and stored on a
supercomputer. The large scale distributed composite language model gives drastic perplexity
reduction over n-grams and achieves significantly better translation quality measured by the
Bleu score and ?readability? of translations when applied to the task of re-ranking the N-best list
from a state-of-the-art parsing-based machine translation system.
No rights reserved. This work was authored as part of the Contributor?s official duties as an Employee of
the United States Government and is therefore a work of the United States Government In accordance with
1. Introduction
The Markov chain (n-gram) source models, which predict each word on the basis of the
previous n? 1 words, have been the workhorses of state-of-the-art speech recognizers
and machine translators that help to resolve acoustic or foreign language ambiguities by
placing higher probability on more likely original underlying word strings. Although
the Markov chains are efficient at encoding local word interactions, the n-gram model
? Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton
OH 45435. E-mail: tan.6@wright.edu.
?? Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton
OH 45435. E-mail: zhou.23@wright.edu.
? Kno.e.sis Center, Wright State University, Dayton OH 45435. E-mail: lei.zheng@wright.edu.
? Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton
OH 45435. E-mail: shaojun.wang@wright.edu.
Submission received: 10 October 2010; revised submission received: 17 October 2011; accepted for publication:
16 November 2011.
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
Computational Linguistics Volume 38, Number 3
clearly ignores the rich syntactic and semantic structures that constrain natural lan-
guages. Attempting to increase the order of an n-gram to capture longer range depen-
dencies in natural language immediately runs into the curse of dimensionality (Bengio
et al 2003). The performance of conventional n-gram technology has essentially reached
a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to
improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005;
Zhang, Hildebrand, and Vogel 2006; Brants et al 2007; Emami, Papineni, and Sorensen
2007) have shown that using an immense distributed computing paradigm, up to
6-grams, can be trained on up to billions and trillions of tokens, yielding consistent sys-
tem improvements because of excellent n-gram hit ratios on unseen test data, but Zhang
(2008) did not observe much improvement beyond 6-grams. As the machine translation
(MT) working groups stated in their final report (Lavie et al 2006, page 3), ?These
approaches have resulted in small improvements in MT quality, but have not funda-
mentally solved the problem. There is a dire need for developing novel approaches to
language modeling.?
Over the past two decades, more sophisticated models have been developed that
outperform n-grams; these are mainly the syntactic language models (Della Pietra et al
1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and
Harper 2002; Jelinek 2004; Bened?? and Sa?nchez 2005; Van Uytsel and Compernolle 2005)
that effectively exploit sentence-level syntactic structure of natural language, and the
topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda
2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each
of these language models only targets some specific, distinct linguistic phenomena
(Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects
of natural language regularity. A natural question we should ask is whether/how
we can construct more complex and powerful but computationally tractable language
models by integrating many existing/emerging language model components, with each
component focusing on specific linguistic phenomena like syntactic structure, semantic
topic, morphology, and pragmatics in complementary, supplementary, and coherent
ways (Bellegarda 2001, 2003).
Several techniques for combining language models have been investigated. The
most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek
and Mercer 1980; Goodman 2001), where each individual model is trained separately
and then combined by a weighted linear combination. All of the syntactic structure-
based models have used linear interpolation to combine trigrams to achieve further
improvement over using their own models alone (Charniak 2001; Chelba and Jelinek
2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out
data. Even though this technique is simple and easy to implement, it does not generally
yield very effective combinations (Rosenfeld 1996) because the linear additive form
is a strong assumption in capturing subtleties in each of the component models (see
more explanation and analysis in Section 6.2 and Appendix A). The second method
is based on maximum entropy philosophy, which became very popular in machine
learning and natural language processing communities due to the work in Berger,
Della Pietra, and Della Pietra (1996), Della Pietra, Della Pietra, and Lafferty (1997),
Lau et al (1993) and Rosenfeld (1996). In fact, for a complete data case, maximum
entropy is nothing but maximum likelihood estimation for undirected Markov random
fields (MRFs) (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra,
and Lafferty 1997). As stated in Wang et al (2005b), however, there are two weaknesses
with maximum entropy approach. The first weakness is that this approach can only
model distributions over explicitly observed features, but we know there is hidden
632
Tan et al A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
information in natural language, such as syntactic structure and semantic topic. The
second weakness is that if the statistical model is too complex it becomes intractable to
estimate model parameters; computationally very expensive Markov chain Monte Carlo
sampling methods (Mark, Miller, and Grenander 1996; Rosenfeld 2000b; Rosenfeld,
Chen, and Zhu 2001) would have to be used. One way to overcome the first hurdle
is to use a preprocessing tool to extract hidden features (e.g., Rosenfeld [1996] used
mutual information clustering method to find word pair triggers) then combine these
triggers with trigrams through a maximum conditional entropy approach to allow the
discourse topic to influence word prediction; Khudanpur and Wu (2000) used Chelba
and Jelinek?s structured language model and a word clustering model to extract relevant
grammatical and semantic features, then to again combine these features with trigrams
through a maximum conditional entropy approach to form a syntactic, semantic, and
lexical language model. Wang and colleagues (Wang et al 2005a; Wang, Schuurmans,
and Zhao 2012) have proposed the latent maximum entropy (LME) principle, which
extends standard maximum entropy estimation by incorporating hidden dependency
structure, but still the LME wouldn?t overcome the second hurdle. The third method is
directed Markov random field (Wang et al 2005b) that overcomes both weaknesses in
the maximum entropy approach. Wang et al used this approach to combine trigram,
probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis
(PLSA) models; a generalized inside?outside algorithm is derived that alters the well-
known inside?outside algorithm for PCFG (Baker 1979; Lari and Young 1990) with
modular modification to take into account the effect of n-gram and PLSA while remain-
ing at the same cubic time complexity. When applying this to the Wall Street Journal
corpus with 40 million tokens, they achieved moderate perplexity reduction. Because
the probabilistic dependency structure in a structured language model (SLM) (Chelba
2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG,
Wang et al (2006) studied the stochastic properties for the composite language model
that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al
2005b) and derived another generalized inside?outside algorithm to train a composite n-
gram, SLM, and PLSA language model from a general expectation maximization (EM)
(Dempster, Laird, and Rubin 1977) algorithm by following Jelinek?s ingenious definition
of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized
inside?outside algorithm alters Jelinek?s inside?outside algorithm with modular modi-
fication and has the same sixth order of sentence-length time complexity. Unfortunately,
there are no experimental results reported.
In this article, we study the same composite n-gram, SLM, and PLSA model un-
der the directed MRF framework as in Wang et al (2006). The composite n-gram/
SLM/PLSA language model under the directed MRF paradigm is first introduced in
Section 2. In Section 3, instead of using the sixth order generalized inside?outside
algorithm proposed in Wang et al (2006), we show how to train this composite model
via an N-best list approximate EM algorithm that has linear time complexity and a
follow-up EM algorithm to improve word prediction power. We prove the convergence
of the N-best list approximate EM algorithm. To resolve the data sparseness problem,
we generalize Jelinek and Mercer?s recursive mixing scheme for Markov source (Jelinek
and Mercer 1980) to a mixture of Markov chains. To handle large-scale corpora up to a
billion tokens, we demonstrate how to implement these algorithms under a distributed
computing environment and how to store this language model on a supercomputer. In
Section 4, we describe how to use the model for testing. Related works are then summa-
rized and compared in Section 5. Because language modeling is a data-rich and feature-
rich density estimation problem, there is always a trade-off between approximate error
633
Computational Linguistics Volume 38, Number 3
and estimation error, thus in Section 6 we conduct comprehensive experiments on
corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens, and compare
perplexity results with n-grams (n = 3, 4, 5 respectively) on these three corpora under
various situations; drastic perplexity reductions are obtained. We explain why the com-
posite language models lead to better predictive capacity than linear interpolation. The
proposed composite language models are applied to the task of re-ranking the N-best
list from Hiero (Chiang 2005, 2007), a state-of-the-art parsing-based machine translation
system; we achieve significantly better translation quality measured by the Bleu score
and ?readability? of translations. Finally, we draw our conclusions and propose future
work in Section 7.
The main theme of our approach is ?to exploit information, be it syntactic structure
or semantic fabric, which involves a fairly high degree of cognition. This is precisely
the kind of knowledge that humans naturally and inherently use to process natural
language, so it can be reasonably conjectured to represent a key ingredient for success?
(Bellegarda 2003, p. 105). In that light, the directed MRF framework, ?whose ultimate
goal is to integrate all available knowledge sources, appears most likely to harbor a
potential breakthrough. It is hoped that the on-going effort conducted in this work to
leverage such latent synergies will lead, in the not-too-distant future, to more polyva-
lent, multi-faceted, effective and tractable solutions for language modeling ? this is only
beginning to scratch the surface in developing systems capable of deep understanding
of natural language? (Bellegarda 2003, p. 105).
2. The Composite n-gram/SLM/PLSA Language Model
Let X denote a set of random variables (X?)??? taking values in a (discrete) probability
space (X?)???, where ? is a finite set of states. We define a (discrete) directed Markov
random field to be a probability distribution P , which admits a recursive factorization
if there exist non-negative functions, ??(?, ?), ? ? ? defined on X? ?Xpa(?), such that
?
x?
??(x?, xpa(?) ) = 1 and P has density
p(x) =
?
???
??(x?, xpa(?) ) (1)
Here pa(?) denotes the set of parent states of ?. If the recursive factorization refers to a
graph, then we have a Bayesian network (Lauritzen 1996). Broadly speaking, however,
the recursive factorization can refer to a representation more complicated than a graph
with a fixed set of nodes and edges?for example, PCFG and SLM are examples of
directed MRFs whose parse tree structure is a random object that can?t be described
as a Bayesian network (McAllester, Collins, and Pereira 2004). A key difference be-
tween directed MRFs and undirected MRFs is that a directed MRF requires many
local normalization constraints whereas an undirected MRF has a global normalization
factor.
The n-gram (Jelinek 1998; Jurafsky and Martin 2008) language model is essentially a
WORD-PREDICTOR, that is, given its entire document history, it predicts the next word
wk+1 ? V based on the last n?1 words with probability p(wk+1|wkk?n+2) where w
k
k?n+2 =
wk?n+2, ? ? ? ,wk and V denotes the vocabulary.
The SLM proposed in Chelba and Jelinek (1998, 2000) and Chelba (2000) uses syntac-
tic information beyond the regular n-gram models to capture sentence-level long-range
634
Tan et al A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
dependencies. The SLM is based on statistical parsing techniques that allow syntactic
analysis of sentences; it assigns a probability p(W,T) to every sentence W and every
possible binary parse T. The terminals of T are the words of W with part of speech
(POS) tags, and the nodes of T are annotated with phrase headwords and non-terminal
labels. Let W be a sentence of length n words to which we have prepended the sentence
beginning marker ?s? and appended the sentence end marker ?/s? so that w0 =?s? and
wn+1 =?/s?. Let Wk = w0, ? ? ? ,wk be the word k-prefix of the sentence (the words from
the beginning of the sentence up to the current position k) and WkTk be the word-parse
k-prefix. A word-parse k-prefix has a set of exposed heads h?m, ? ? ? , h?1 ? H, with each
head being a pair (headword, non-terminal label), H = V ?ONT where ONT denotes
the set of non-terminal label (NTlabel), or in the case of a root-only tree (word, POS tag)
H = V ?O where O denotes the set of POS tags. The exposed heads at a given position
k in the input sentence are a function of the word-parse k-prefix.
The SLM operates left-to-right, building up the parse structure in a bottom?up
manner. At any given stage of the word generation by the SLM, the exposed headwords
are those headwords of the current partial parse which are not yet part of a higher
phrase with a head of its own. An mth order SLM (m-SLM) has three operators to
generate a sentence:
 The WORD-PREDICTOR predicts the next word wk+1 ? V based on the m
most recently exposed headwords h?1?m = h?m, ? ? ? , h?1 in the word-parse
k-prefix with probability p(wk+1|h
?1
?m), and then passes control to the
TAGGER.
 The TAGGER predicts the POS tag tk+1 ? O to the next word wk+1 based
on the next word wk+1 and the POS tags of the m most recently exposed
headwords h?1?m (denoted as h
?1
?m.tag = h?m.tag, ? ? ? , h?1.tag) in the
word-parse k-prefix with probability p(tk+1|wk+1, h
?1
?m.tag).
 The CONSTRUCTOR builds the partial parse Tk+1 from Tk, wk+1, and tk+1
in a series of moves ending with NULL, where a parse move a is made
with probability p(a|h?1?m); a ? A={(unary, NTlabel), (adjoin-left, NTlabel),
(adjoin-right, NTlabel), NULL}. Depending on an action a = adjoin-right
or adjoin-left, the headword h?1 or h?2 is percolated up by one tree level,
the indices of the current exposed headwords h?3, h?4, ? ? ? are increased
by 1, and these headwords together with h?1 or h?2 become the new
exposed headwords. Once the CONSTRUCTOR hits NULL, the
headword indexing and current parse structure remain as they are,
and the CONSTRUCTOR passes control to the WORD-PREDICTOR.
SLM is thus essentially a generalization of a shift-reduce parser (Aho and Ullman
1972) with adjoin corresponding to reduce and predict to shift. (See a detailed description
about SLM in Chelba and Jelinek [1998, 2000]; Chelba [2000]; Jelinek [2004]). As an
example taken from Jelinek (2004), Figure 1 shows a complete parse where SB/SE is a
distinguished POS tag for ?s?/?/s? respectively, (?s?,TOP) is the only allowed head, and
(?/s?,TOP?) is the head of any constituent that dominates ?/s? but not ?s?. In Figure 1,
at the time just after the word as is generated, the exposed headwords are ??s? SB,
show np, has vbz.? The subsequent model actions are: ?POStag as, null, predict its,
POStag its, null, predict host, POStag host, adjoin-right-np, adjoin-left-pp, adjoin-left-
pp, null, predict a, ? ? ? .?
635
Computational Linguistics Volume 38, Number 3
Figure 1
A complete parse tree by the structured language model.
A PLSA model (Hofmann 2001) is a generative probabilistic model of word-
document co-occurrences using the bag-of-words assumption described as follows:
 Choose a document d with probability p(d).
 SEMANTIZER selects a semantic class g ? G with probability p(g|d) where
G denotes the set of topics.
 WORD-PREDICTOR picks a word w ? V with probability p(w|g).
Because only one pair of (d,w) is being observed, the joint probability model is a mixture
of log-linear models with the expression p(d,w) = p(d)
?
g p(w|g)p(g|d). Typically, the
number of documents and the vocabulary size are much larger than the size of latent
semantic class variables. Latent semantic class variables therefore function as bottleneck
variables to constrain word occurrences in documents.
When combining n-gram, m-SLM, and PLSA together to build a composite
generative language model under the directed MRF paradigm (Wang et al 2005b,
2006), the composite language model is simply a complicated generative model that has
four operators: WORD-PREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER.
The TAGGER and CONSTRUCTOR in SLM and the SEMANTIZER in PLSA remain
unchanged; the WORD-PREDICTORs in n-gram, m-SLM, and PLSA, however, are
combined to form a stronger WORD-PREDICTOR that generates the next word, wk+1,
not only depending on the m most recently exposed headwords h?1?m in the word-parse
k-prefix but also its n-gram history wkk?n+2 and its semantic content gk+1. The parameter
for WORD-PREDICTOR in the composite n-gram/m-SLM/PLSA language model
becomes p(w|w?1?n+1h
?1
?mg). The resulting composite language model has an even more
complex dependency structure but with more expressive power than the original
SLM. Figure 2 illustrates the structure of a composite n-gram/m-SLM/PLSA language
model.
The composite n-gram/m-SLM/PLSA language model can be formulated as a
rather complex chain-tree-table directed MRF model (Wang et al 2006) with local
636
Tan et al A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
Figure 2
A composite n-gram/m-SLM/PLSA language model where the hidden information is the parse
tree T and semantic content g. The n-gram encodes local word interactions, the m-SLM models
the sentence?s syntactic structure, and the PLSA captures the document?s semantic content;
all interact together to constrain the generation of natural language. The WORD-PREDICTOR
generates the next word wk+1 with probability p(wk+1|wkk?n+2h
?1
?mgk+1) instead of p(wk+1|w
k
k?n+2),
p(wk+1|h
?1
?m), and p(wk+1|gk+1), respectively.
normalization constraints for the parameters of each model component, WORD-
PREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER. That is,
?
w?V
p(w|w?1?n+1h
?1
?mg) = 1 (2)
?
t?O
p(t|wh?1?m.tag) = 1 (3)
?
a?A
p(a|h?1?m) = 1 (4)
?
g?G
p(g|d) = 1 (5)
If we look at the example in Figure 1, for the composite n-gram/m-SLM/PLSA
language model there exists a SEMANTIZER?s action to choose a topic g before
any WORD-PREDICTOR?s action. Moreover, for m-SLM, its WORD-PREDICTOR
predicts the next word, such as a, based on m most recently exposed headwords
??s?-SB, show-np, has-vp,? but for the composite model, the WORD-PREDICTOR
predicts the next word a based on m most recently exposed headwords ??s?-SB,
show-np, has-vp,? n-grams ?as its host,? and a topic g. These are the only differences
between SLM and our proposed composite language model.
3. Training Algorithm
For the composite n-gram/m-SLM/PLSA language model under the directed MRF
paradigm, the likelihood of a training corpus D, a collection of documents, can be
written as
L?(D, p) =
?
d?D
?
?
?
?
?
l
?
?
?
Gl
?
?
?
Tl
Pp(W
l,Tl,Gl|d)
?
?
?
?
?
? p(d)
?
? (6)
637
Computational Linguistics Volume 38, Number 3
where (Wl,Tl,Gl|d) denotes the joint sequence of the lth sentence Wl with its parse struc-
ture Tl and semantic annotation string Gl in document d. This sequence is produced by
a unique sequence of model actions: WORD-PREDICTOR, TAGGER, CONSTRUCTOR,
SEMANTIZER moves; its probability is obtained by chaining the probabilities of these
moves
Pp(W
l,Tl,Gl|d) =
?
g?G
?
?p(g|d)#(g,W
l,Gl,d)
?
h?1,??? ,h?m?H
(7)
?
?
?
w,w?1,??? ,w?n+1?V
p(w|w?1?n+1h
?1
?mg)
#(w?1?n+1wh
?1
?mg,W
l,Tl,Gl,d)
?
t?O
p(t|wh?1?m.tag)
#(t,wh?1?m.tag,W
l,Tl,d)
?
a?A
p(a|h?1?m)
#(a,h?1?m,W
l,Tl,d)
))
where #(g,Wl,Gl, d) is the count of semantic content g in semantic annotation string Gl of
the lth sentence Wl in document d; #(w?1?n+1wh
?1
?mg,W
l,Tl,Gl, d) is the count of n-grams,
its m most recently exposed headwords, and semantic content g in parse Tl and semantic
annotation string Gl of the lth sentence Wl in document d; #(twh?1?m.tag,W
l,Tl, d) is the
count of tag t predicted by word w and the tags of m most recently exposed headwords
in parse tree Tl of the lth sentence Wl in document d; and finally #(ah?1?m,W
l,Tl, d) is the
count of constructor move a conditioning on m exposed headwords h?1?m in parse tree T
l
of the lth sentence Wl in document d.
Let
L(D, p) =
?
d?D
?
?
?
l
?
?
?
Gl
?
?
?
Tl
Pp(W
l,Tl,Gl|d)
?
?
?
?
?
? (8)
then
L?(D, p) = L(D, p)
?
d?D
(
p(d)
)
(9)
Clearly, when maximizing L?(D, p) in Equation (6), p(d) is an ancillary term that is
independent of all other data-generating parameters, it is not critical to anything that
follows; moreover, when a language model is used to find the most likely word se-
quence in machine translation and speech recognition, this term is useless. Thus, similar
to an n-gram language model, we will generally ignore this term and concentrate on
optimizing Equation (8) in the subsequent development.
The objective of maximum likelihood estimation is to maximize the likelihood
L(D, p) with respect to model parameters. For a given sentence, its parse tree and
638
Tan et al A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
semantic content are hidden and the number of parse trees grows faster than expo-
nentially with sentence length; Wang et al (2006) have derived a generalized inside?
outside algorithm by applying the standard EM algorithm and considering the auxiliary
function
Q(p?, p) =
?
d?D
?
l
?
Gl
?
Tl
Pp(T
l,Gl|Wl, d) logPp? (W
l,Tl,Gl|d) (10)
The complexity of this algorithm is sixth order (sentence length), however; thus it is
computationally too expensive to be practical for a large corpus even with the use of
pruning on charts (Jelinek and Chelba 1999; Jelinek 2004).
3.1 N-best List Approximate EM
Similar to SLM (Chelba and Jelinek 1998, 2000; Chelba 2000), we adopt an N-best list
approximate EM re-estimation with modular modifications to seamlessly incorporate
the effect of n-gram and PLSA components. Instead of maximizing the likelihood
L(D, p), we maximize the N-best list likelihood,
max
T ?N
L(D, p, T ?N ) =
?
d?D
?
?
?
l
?
? max
T ? lN?T ?N
?
?
?
Gl
?
?
?
Tl?T ? lN ,||T ?
l
N||=N
Pp(W
l,Tl,Gl|d)
?
?
?
?
?
?
?
? (11)
where T ?lN is a set of N parse trees for sentence W
l in document d, || ? || denotes the
cardinality, and T ?N is a collection of T ?
l
N for sentences over entire corpus D.
The N-best list approximate EM involves two steps:
1. N-best list search: For each sentence W in document d, find N-best
parse trees,
T lN = arg max
T ? lN
{
?
Gl
?
Tl?T ? lN
Pp(W
l,Tl,Gl|d), ||T ?lN|| = N
}
and denote TN as the collection of N-best list parse trees for sentences
over entire corpus D under model parameter p.
2. EM update: Perform one iteration (or several iterations) of the EM
algorithm to estimate model parameters that maximize N-best list
likelihood of the training corpus D,
L?(D, p, TN ) =
?
d?D
?
?
?
l
?
?
?
Gl
?
?
?
Tl?T lN?TN
Pp(W
l,Tl,Gl|d)
?
?
?
?
?
?
639
Computational Linguistics Volume 38, Number 3
That is,
(a) E-step: Compute the auxiliary function of the N-best list likelihood
Q?(p?, p, TN ) =
?
d?D
?
l
?
Gl
?
Tl?T lN?TN
Pp(T
l,Gl|Wl, d) logPp? (W
l,Tl,Gl|d)
(b) M-step: Maximize Q?(p?, p, TN ) with respect to p? to get the new
update for p.
Iterate steps (1) and (2) until the convergence of the N-best list likelihood.
We use Zangwill?s global convergence theorem (Zangwill 1969) to analyze the
behavior of convergence of the N-best list approximate EM.
First, we define two concepts needed for Zangwill?s global convergence theorem.
A map M is from points of ? to subsets of ? is called a point-to-set map on ?. It
is said to be closed at ? if ?i ? ?,?i ? ? and ?i ? ?, ?i ? M(?i) implies ? ? M(?).
For a point-to-point map, continuity implies closedness. Then the global convergence
theorem (Zangwill 1969) states the following.
Theorem
Let M be a point-to-set map (an algorithm) that, given a point ?0 ? ?, generates a
sequence {??i=0} through the iteration ?i+1 =M(?i). Let ? ? ? be the set of fixed points
of M. Suppose (i) M is closed over the complement of ?; (ii) there is a continuous
function ? on ? such that (a) if ? /? ?, ?(?) > ?(?) for all ? ? M(?), and (b) if ? ? ?,
?(?) ? ?(?) for all ? ? M(?).
Then all the limit points of {?i} are in ? and ?(?i) converges monotonically to ?(?)
for some ? ? ?.
Proof
This theorem has been used by Wu (1983) to prove the convergence of a standard EM
algorithm (Dempster, Laird, and Rubin 1977). We now use this theorem to show that
the N-best list approximate EM algorithm globally converges to the stationary points
of the N-best list likelihood. We encounter one difficulty at this point, however, due to
the maximization operator in Equation (11); after each iteration the N-best list may have
been changed, therefore the set of data presented for the estimation of model parameters
may be different from the previous one. Nevertheless, we prove the convergence of the
N-best list approximate EM algorithm by checking whether it satisfies two conditions
in Zangwill?s global convergence theorem. Because the composite model is essentially
a mixture model of a curved exponential family through a complex hierarchy, there
is a closed form solution for the Q?(p?, p, TN ) function irrespective of the N-best list
parse trees, so the N-best list approximate EM algorithm is a one-to-one map. Because
Q?(p?, p, TN ) is continuous in both p? and p, the map is closed, thus condition (i) is
satisfied.
To check condition (ii), we need to verify that the N-best list likelihood as a function
of p satisfies the properties of ?(?) in condition (ii). Let T?N and T?N be the two collections
640
Tan et al A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
of N-best list parse trees for sentences over entire corpus D under two model parameters
p? and p?, respectively:
T?N = arg max
T ?N
L(D, p?, T ?N ) (12)
T?N = arg max
T ?N
L(D, p?, T ?N ) (13)
and let p? be the closed form solution of maximizing Q?(p?, p?, T?N ) with respect to p?, that is,
p? = arg max
p?
Q?(p?, p?, T?N ) (14)
Then
max
T ?N
L(D, p?, T ?N ) ? L?(D, p?, T?N ) (15)
? L?(D, p?, T?N ) (16)
? max
T ?N
L(D, p?, T ?N ) (17)
The inequality in Equation (15) is strict unless T?N = T?N, which results in p? ? M(p?).
Using results proven by Wu (1983), we know that when p? is not a stationary point of the
N-best list likelihood or p? /? M(p?), ?L?(D,p?,TN )?p? =
?Q?(p?,p?,T?N )
?p? Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 201?210,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Large Scale Distributed Syntactic, Semantic and Lexical
Language Model for Machine Translation
Ming Tan Wenli Zhou Lei Zheng Shaojun Wang
Kno.e.sis Center
Department of Computer Science and Engineering
Wright State University
Dayton, OH 45435, USA
{tan.6,zhou.23,lei.zheng,shaojun.wang}@wright.edu
Abstract
This paper presents an attempt at building
a large scale distributed composite language
model that simultaneously accounts for local
word lexical information, mid-range sentence
syntactic structure, and long-span document
semantic content under a directed Markov ran-
dom field paradigm. The composite language
model has been trained by performing a con-
vergent N-best list approximate EM algorithm
that has linear time complexity and a follow-
up EM algorithm to improve word prediction
power on corpora with up to a billion tokens
and stored on a supercomputer. The large
scale distributed composite language model
gives drastic perplexity reduction over n-
grams and achieves significantly better trans-
lation quality measured by the BLEU score
and ?readability? when applied to the task of
re-ranking the N-best list from a state-of-the-
art parsing-based machine translation system.
1 Introduction
The Markov chain (n-gram) source models, which
predict each word on the basis of previous n-1
words, have been the workhorses of state-of-the-art
speech recognizers and machine translators that help
to resolve acoustic or foreign language ambiguities
by placing higher probability on more likely original
underlying word strings. Research groups (Brants et
al., 2007; Zhang, 2008) have shown that using an
immense distributed computing paradigm, up to 6-
grams can be trained on up to billions and trillions
of words, yielding consistent system improvements,
but Zhang (2008) did not observe much improve-
ment beyond 6-grams. Although the Markov chains
are efficient at encoding local word interactions, the
n-gram model clearly ignores the rich syntactic and
semantic structures that constrain natural languages.
As the machine translation (MT) working groups
stated on page 3 of their final report (Lavie et al,
2006), ?These approaches have resulted in small im-
provements in MT quality, but have not fundamen-
tally solved the problem. There is a dire need for de-
veloping novel approaches to language modeling.?
Wang et al (2006) integrated n-gram, structured
language model (SLM) (Chelba and Jelinek, 2000)
and probabilistic latent semantic analysis (PLSA)
(Hofmann, 2001) under the directed MRF frame-
work (Wang et al, 2005) and studied the stochas-
tic properties for the composite language model.
They derived a generalized inside-outside algorithm
to train the composite language model from a gen-
eral EM (Dempster et al, 1977) by following Je-
linek?s ingenious definition of the inside and outside
probabilities for SLM (Jelinek, 2004) with 6th order
of sentence length time complexity. Unfortunately,
there are no experimental results reported.
In this paper, we study the same composite lan-
guage model. Instead of using the 6th order general-
ized inside-outside algorithm proposed in (Wang et
al., 2006), we train this composite model by a con-
vergent N-best list approximate EM algorithm that
has linear time complexity and a follow-up EM al-
gorithm to improve word prediction power. We con-
duct comprehensive experiments on corpora with 44
million tokens, 230 million tokens, and 1.3 billion
tokens and compare perplexity results with n-grams
(n=3,4,5 respectively) on these three corpora, we
obtain drastic perplexity reductions. Finally, we ap-
201
ply our language models to the task of re-ranking
the N-best list from Hiero (Chiang, 2005; Chiang,
2007), a state-of-the-art parsing-based MT system,
we achieve significantly better translation quality
measured by the BLEU score and ?readability?.
2 Composite language model
The n-gram language model is essentially a word
predictor that given its entire document history it
predicts next word wk+1 based on the last n-1 words
with probability p(wk+1|wkk?n+2) where wkk?n+2 =
wk?n+2, ? ? ? , wk .
The SLM (Chelba and Jelinek, 1998; Chelba and
Jelinek, 2000) uses syntactic information beyond
the regular n-gram models to capture sentence level
long range dependencies. The SLM is based on sta-
tistical parsing techniques that allow syntactic anal-
ysis of sentences; it assigns a probability p(W,T ) to
every sentence W and every possible binary parse
T . The terminals of T are the words of W with POS
tags, and the nodes of T are annotated with phrase
headwords and non-terminal labels. Let W be a sen-
tence of length n words to which we have prepended
the sentence beginning marker <s> and appended
the sentence end marker </s> so that w0 =<s>
and wn+1 =</s>. Let Wk = w0, ? ? ? , wk be the
word k-prefix of the sentence ? the words from the
beginning of the sentence up to the current position
k and WkTk the word-parse k-prefix. A word-parse
k-prefix has a set of exposed heads h?m, ? ? ? , h?1,
with each head being a pair (headword, non-terminal
label), or in the case of a root-only tree (word,
POS tag). An m-th order SLM (m-SLM) has
three operators to generate a sentence: WORD-
PREDICTOR predicts the next word wk+1 based
on the m left-most exposed headwords h?1?m =
h?m, ? ? ? , h?1 in the word-parse k-prefix with prob-
ability p(wk+1|h?1?m), and then passes control to the
TAGGER; the TAGGER predicts the POS tag tk+1
to the next word wk+1 based on the next word wk+1
and the POS tags of the m left-most exposed head-
words h?1?m in the word-parse k-prefix with prob-
ability p(tk+1|wk+1, h?m.tag, ? ? ? , h?1.tag); the
CONSTRUCTOR builds the partial parse Tk from
Tk?1, wk, and tk in a series of moves ending with
NULL, where a parse move a is made with proba-
bility p(a|h?1?m); a ? A={(unary, NTlabel), (adjoin-
left, NTlabel), (adjoin-right, NTlabel), null}. Once
the CONSTRUCTOR hits NULL, it passes control
to the WORD-PREDICTOR. See detailed descrip-
tion in (Chelba and Jelinek, 2000).
A PLSA model (Hofmann, 2001) is a gener-
ative probabilistic model of word-document co-
occurrences using the bag-of-words assumption de-
scribed as follows: (i) choose a document d with
probability p(d); (ii) SEMANTIZER: select a se-
mantic class g with probability p(g|d); and (iii)
WORD-PREDICTOR: pick a word w with proba-
bility p(w|g). Since only one pair of (d,w) is being
observed, as a result, the joint probability model is
a mixture of log-linear model with the expression
p(d,w) = p(d)?g p(w|g)p(g|d). Typically, the
number of documents and vocabulary size are much
larger than the size of latent semantic class variables.
Thus, latent semantic class variables function as bot-
tleneck variables to constrain word occurrences in
documents.
When combining n-gram, m order SLM and
PLSA models together to build a composite gen-
erative language model under the directed MRF
paradigm (Wang et al, 2005; Wang et al, 2006),
the TAGGER and CONSTRUCTOR in SLM and
SEMANTIZER in PLSA remain unchanged; how-
ever the WORD-PREDICTORs in n-gram, m-SLM
and PLSA are combined to form a stronger WORD-
PREDICTOR that generates the next word, wk+1,
not only depending on the m left-most exposed
headwords h?1?m in the word-parse k-prefix but also
its n-gram history wkk?n+2 and its semantic con-
tent gk+1. The parameter for WORD-PREDICTOR
in the composite n-gram/m-SLM/PLSA language
model becomes p(wk+1|wkk?n+2h?1?mgk+1). The re-
sulting composite language model has an even more
complex dependency structure but with more ex-
pressive power than the original SLM. Figure 1 il-
lustrates the structure of a composite n-gram/m-
SLM/PLSA language model.
The composite n-gram/m-SLM/PLSA lan-
guage model can be formulated as a directed
MRF model (Wang et al, 2006) with lo-
cal normalization constraints for the param-
eters of each model component, WORD-
PREDICTOR, TAGGER, CONSTRUCTOR,
SEMANTIZER, i.e.,
?
w?V p(w|w?1?n+1h?1?mg) =
1,?t?O p(t|wh?1?m.tag) = 1,
?
a?A p(a|h?1?m) =
1,?g?G p(g|d) = 1.
202
......
............
g
w
g g g...... ...... ...... ......
</s>
d
kk?n+2j+1
......<s> w1 i
i......
......
g1
wk wk+1
gk+1
h
?1h
?2
h
?m
j+1wwj
g j
......k?n+2w
......
Figure 1: A composite n-gram/m-SLM/PLSA language
model where the hidden information is the parse tree
T and semantic content g. The WORD-PREDICTOR
generates the next word wk+1 with probability
p(wk+1|wkk?n+2h?1?mgk+1) instead of p(wk+1|wkk?n+2),
p(wk+1|h?1?m) and p(wk+1|gk+1) respectively.
3 Training algorithm
Under the composite n-gram/m-SLM/PLSA lan-
guage model, the likelihood of a training corpus D,
a collection of documents, can be written as
L(D, p) =
Y
d?D
 
Y
l
 
X
Gl
 
X
T l
Pp(W l, T l, Gl|d)
!!!
(1)
where (W l, T l, Gl, d) denote the joint sequence of
the lth sentence W l with its parse tree structure T l
and semantic annotation string Gl in document d.
This sequence is produced by a unique sequence
of model actions: WORD-PREDICTOR, TAGGER,
CONSTRUCTOR, SEMANTIZER moves, its prob-
ability is obtained by chaining the probabilities of
these moves
Pp(W l, T l, Gl|d)
=
Y
g?G
0
@p(g|d)#(g,W
l,Gl,d) Y
h?1,??? ,h?m?H
Y
w,w?1 ,??? ,w?n+1?V
p(w|w?1?n+1h?1?mg)#(w
?1
?n+1wh
?1
?mg,W
l,T l,Gl,d)
Y
t?O
p(t|wh?1?m.tag)#(t,wh
?1
?m.tag,W
l,T l,d)
Y
a?A
p(a|h?1?m)#(a,h
?1
?m,W
l,T l,d)
!
where #(g,W l, Gl, d) is the count of seman-
tic content g in semantic annotation string
Gl of the lth sentence W l in document d,
#(w?1?n+1wh?1?mg,W l, T l, Gl, d) is the count
of n-grams, its m most recent exposed headwords
and semantic content g in parse T l and semantic
annotation string Gl of the lth sentence W l in
document d, #(twh?1?m.tag,W l, T l, d) is the count
of tag t predicted by word w and the tags of m
most recent exposed headwords in parse tree T l
of the lth sentence W l in document d, and finally
#(ah?1?m,W l, T l, d) is the count of constructor
move a conditioning on m exposed headwords h?1?m
in parse tree T l of the lth sentence W l in document
d.
The objective of maximum likelihood estimation
is to maximize the likelihood L(D, p) respect to
model parameters. For a given sentence, its parse
tree and semantic content are hidden and the num-
ber of parse trees grows faster than exponential with
sentence length, Wang et al (2006) have derived a
generalized inside-outside algorithm by applying the
standard EM algorithm. However, the complexity of
this algorithm is 6th order of sentence length, thus it
is computationally too expensive to be practical for
a large corpus even with the use of pruning on charts
(Jelinek and Chelba, 1999; Jelinek, 2004).
3.1 N-best list approximate EM
Similar to SLM (Chelba and Jelinek, 2000), we
adopt an N -best list approximate EM re-estimation
with modular modifications to seamlessly incorpo-
rate the effect of n-gram and PLSA components.
Instead of maximizing the likelihood L(D, p), we
maximize the N -best list likelihood,
max
T ?N
L(D, p, T ?N ) =
Y
d?D
 
Y
l
 
max
T ?lN?T ?N
X
Gl
0
@
X
T l?T ?lN ,||T ?
l
N ||=N
Pp(W l, T l, Gl|d)
1
A
1
A
1
A
where T ?lN is a set of N parse trees for sentence W l
in document d and || ? || denotes the cardinality and
T ?N is a collection of T ?lN for sentences over entire
corpus D.
The N-best list approximate EM involves two
steps:
1. N-best list search: For each sentence W in doc-
ument d, find N -best parse trees,
T lN = arg max
T ?lN
n
X
Gl
X
T l?T ?lN
Pp(W l, T l, Gl|d), ||T ?lN || = N
o
and denote TN as the collection of N -best list
parse trees for sentences over entire corpus D
under model parameter p.
2. EM update: Perform one iteration (or several
iterations) of EM algorithm to estimate model
203
parameters that maximizes N -best-list likeli-
hood of the training corpus D,
L?(D, p,TN ) =
Y
d?D
(
Y
l
(
X
Gl
(
X
T l?T lN?TN
Pp(W l, T l, Gl|d))))
That is,
(a) E-step: Compute the auxiliary function of
the N -best-list likelihood
Q?(p?, p, TN ) =
X
d?D
X
l
X
Gl
X
T l?T lN?TN
Pp(T l, Gl|W l, d)
logPp?(W l, T l, Gl|d)
(b) M-step: Maximize Q?(p?, p,TN ) with re-
spect to p? to get new update for p.
Iterate steps (1) and (2) until the convergence of the
N -best-list likelihood. Due to space constraints, we
omit the proof of the convergence of the N-best list
approximate EM algorithm which uses Zangwill?s
global convergence theorem (Zangwill, 1969).
N -best list search strategy: To extract the N -
best parse trees, we adopt a synchronous, multi-
stack search strategy that is similar to the one in
(Chelba and Jelinek, 2000), which involves a set
of stacks storing partial parses of the most likely
ones for a given prefix Wk and the less probable
parses are purged. Each stack contains hypotheses
(partial parses) that have been constructed by the
same number of WORD-PREDICTOR and the same
number of CONSTRUCTOR operations. The hy-
potheses in each stack are ranked according to the
log(
?
Gk Pp(Wk, Tk, Gk|d)) score with the highest
on top, where Pp(Wk, Tk, Gk|d) is the joint prob-
ability of prefix Wk = w0, ? ? ? , wk with its parse
structure Tk and semantic annotation string Gk =
g1, ? ? ? , gk in a document d. A stack vector consists
of the ordered set of stacks containing partial parses
with the same number of WORD-PREDICTOR op-
erations but different number of CONSTRUCTOR
operations. In WORD-PREDICTOR and TAGGER
operations, some hypotheses are discarded due to
the maximum number of hypotheses the stack can
contain at any given time. In CONSTRUCTOR
operation, the resulting hypotheses are discarded
due to either finite stack size or the log-probability
threshold: the maximum tolerable difference be-
tween the log-probability score of the top-most hy-
pothesis and the bottom-most hypothesis at any
given state of the stack.
EM update: Once we have the N -best parse trees
for each sentence in document d and N -best topics
for document d, we derive the EM algorithm to esti-
mate model parameters.
In E-step, we compute the expected count of
each model parameter over sentence W l in docu-
ment d in the training corpus D. For the WORD-
PREDICTOR and the SEMANTIZER, the number
of possible semantic annotation sequences is expo-
nential, we use forward-backward recursive formu-
las that are similar to those in hidden Markov mod-
els to compute the expected counts. We define the
forward vector ?l(g|d) to be
?lk+1(g|d) =
X
Glk
Pp(W lk, T lk, wkk?n+2wk+1h?1?mg,Glk|d)
that can be recursively computed in a forward man-
ner, where W lk is the word k-prefix for sentence W l,
T lk is the parse for k-prefix. We define backward
vector ?l(g|d) to be
?lk+1(g|d)
=
X
Glk+1,?
Pp(W lk+1,?, T lk+1,?, Glk+1,?|wkk?n+2wk+1h?1?mg, d)
that can be computed in a backward manner, here
W lk+1,? is the subsequence after k+1th word in sen-
tence W l, T lk+1,? is the incremental parse struc-
ture after the parse structure T lk+1 of word k+1-
prefix W lk+1 that generates parse tree T l, Glk+1,? is
the semantic subsequence in Gl relevant to W lk+1,?.
Then, the expected count of w?1?n+1wh?1?mg for the
WORD-PREDICTOR on sentence W l in document
d is
X
Gl
Pp(T l, Gl|W l, d)#(w?1?n+1wh?1?mg,W l, T l, Gl, d)
=
X
l
X
k
?lk+1(g|d)?lk+1(g|d)p(g|d)
?(wkk?n+2wk+1h?1?mgk+1 = w?1?n+1wh?1?mg)/Pp(W l|d)
where ?(?) is an indicator function and the expected
count of g for the SEMANTIZER on sentence W l
in document d is
X
Gl
Pp(T l, Gl|W l, d)#(g,W l, Gl, d)
=
j?1
X
k=0
?lk+1(g|d)?lk+1(g|d)p(g|d)/Pp(W l|d)
For the TAGGER and the CONSTRUCTOR,
the expected count of each event of twh?1?m.tag
and ah?1?m over parse T l of sentence W l in
204
document d is the real count appeared in parse
tree T l of sentence W l in document d times
the conditional distribution Pp(T l|W l, d) =
Pp(T l,W l|d)/
?
T l?T l Pp(T l,W l|d) respectively.
In M-step, the recursive linear interpolation
scheme (Jelinek and Mercer, 1981) is used
to obtain a smooth probability estimate for
each model component, WORD-PREDICTOR,
TAGGER, and CONSTRUCTOR. The TAGGER
and CONSTRUCTOR are conditional probabilis-
tic models of the type p(u|z1, ? ? ? , zn) where
u, z1, ? ? ? , zn belong to a mixed set of words, POS
tags, NTtags, CONSTRUCTOR actions (u only),
and z1, ? ? ? , zn form a linear Markov chain. The re-
cursive mixing scheme is the standard one among
relative frequency estimates of different orders k =
0, ? ? ? , n as explained in (Chelba and Jelinek, 2000).
The WORD-PREDICTOR is, however, a condi-
tional probabilistic model p(w|w?1?n+1h?1?mg) where
there are three kinds of context w?1?n+1, h?1?m and g,
each forms a linear Markov chain. The model has
a combinatorial number of relative frequency esti-
mates of different orders among three linear Markov
chains. We generalize Jelinek and Mercer?s original
recursive mixing scheme (Jelinek and Mercer, 1981)
and form a lattice to handle the situation where the
context is a mixture of Markov chains.
3.2 Follow-up EM
As explained in (Chelba and Jelinek, 2000), for the
SLM component, a large fraction of the partial parse
trees that can be used for assigning probability to the
next word do not survive in the synchronous, multi-
stack search strategy, thus they are not used in the
N-best approximate EM algorithm for the estima-
tion of WORD-PREDICTOR to improve its predic-
tive power. To remedy this weakness, we estimate
WORD-PREDICTOR using the algorithm below.
The language model probability assignment for
the word at position k+1 in the input sentence of
document d can be computed as
Pp(wk+1|Wk, d) =
X
h?1?m?Tk;Tk?Zk,gk+1?Gd
p(wk+1|wkk?n+2h?1?mgk+1)
Pp(Tk|Wk, d)p(gk+1|d) (2)
where Pp(Tk|Wk, d) =
P
Gk
Pp(Wk,Tk,Gk|d)
P
Tk?Zk
P
Gk
Pp(Wk,Tk,Gk|d)
and Zk is the set of all parses present in the stacks
at the current stage k during the synchronous multi-
stack pruning strategy and it is a function of the word
k-prefix Wk.
The likelihood of a training corpus D under this
language model probability assignment that uses
partial parse trees generated during the process of
the synchronous, multi-stack search strategy can be
written as
L?(D, p) =
Y
d?D
Y
l
?
X
k
Pp(w(l)k+1|W
l
k, d)
?
(3)
We employ a second stage of parameter re-
estimation for p(wk+1|wkk?n+2h?1?mgk+1) and
p(gk+1|d) by using EM again to maximize
Equation (3) to improve the predictive power of
WORD-PREDICTOR.
3.3 Distributed architecture
When using very large corpora to train our compos-
ite language model, both the data and the parameters
can?t be stored in a single machine, so we have to
resort to distributed computing. The topic of large
scale distributed language models is relatively new,
and existing works are restricted to n-grams only
(Brants et al, 2007; Emami et al, 2007; Zhang et
al., 2006). Even though all use distributed archi-
tectures that follow the client-server paradigm, the
real implementations are in fact different. Zhang
et al (2006) and Emami et al (2007) store train-
ing corpora in suffix arrays such that one sub-corpus
per server serves raw counts and test sentences are
loaded in a client. This implies that when comput-
ing the language model probability of a sentence in
a client, all servers need to be contacted for each n-
gram request. The approach by Brants et al (2007)
follows a standard MapReduce paradigm (Dean and
Ghemawat, 2004): the corpus is first divided and
loaded into a number of clients, and n-gram counts
are collected at each client, then the n-gram counts
mapped and stored in a number of servers, result-
ing in exactly one server being contacted per n-gram
when computing the language model probability of
a sentence. We adopt a similar approach to Brants
et al and make it suitable to perform iterations
of N -best list approximate EM algorithm, see Fig-
ure 2. The corpus is divided and loaded into a num-
ber of clients. We use a public available parser to
parse the sentences in each client to get the initial
counts for w?1?n+1wh?1?mg etc., finish the Map part,
and then the counts for a particular w?1?n+1wh?1?mg
at different clients are summed up and stored in one
205
Server 2Server 1 Server L
Client 1 Client 2 Client M
Figure 2: Distributed architecture is essentially a MapRe-
duce paradigm: clients store partitioned data and per-
form E-step: compute expected counts, this is Map;
servers store parameters (counts) for M-step where
counts of w?1
?n+1wh?1?mg are hashed by word w?1 (or
h?1) and its topic g to evenly distribute these model pa-
rameters into servers as much as possible, this is Reduce.
of the servers by hashing through the word w?1 (or
h?1) and its topic g, finish the Reduce part. This
is the initialization of the N -best list approximate
EM step. Each client then calls the servers for pa-
rameters to perform synchronous multi-stack search
for each sentence to get the N -best list parse trees.
Again, the expected count for a particular parameter
of w?1?n+1wh?1?mg at the clients are computed, thus
we finish a Map part, then summed up and stored in
one of the servers by hashing through the word w?1
(or h?1) and its topic g, thus we finish the Reduce
part. We repeat this procedure until convergence.
Similarly, we use a distributed architecture as in
Figure 2 to perform the follow-up EM algorithm to
re-estimate WORD-PREDICTOR.
4 Experimental results
We have trained our language models using three
different training sets: one has 44 million tokens,
another has 230 million tokens, and the other has
1.3 billion tokens. An independent test set which
has 354 k tokens is chosen. The independent check
data set used to determine the linear interpolation
coefficients has 1.7 million tokens for the 44 mil-
lion tokens training corpus, 13.7 million tokens for
both 230 million and 1.3 billion tokens training cor-
pora. All these data sets are taken from the LDC
English Gigaword corpus with non-verbalized punc-
tuation and we remove all punctuation. Table 1 gives
the detailed information on how these data sets are
chosen from the LDC English Gigaword corpus.
The vocabulary sizes in all three cases are:
? word (also WORD-PREDICTOR operation)
1.3 BILLION TOKENS TRAINING CORPUS
AFP 19940512.0003 ? 19961015.0568
AFW 19941111.0001 ? 19960414.0652
NYT 19940701.0001 ? 19950131.0483
NYT 19950401.0001 ? 20040909.0063
XIN 19970901.0001 ? 20041125.0119
230 MILLION TOKENS TRAINING CORPUS
AFP 19940622.0336 ? 19961031.0797
APW 19941111.0001 ? 19960419.0765
NYT 19940701.0001 ? 19941130.0405
44 MILLION TOKENS TRAINING CORPUS
AFP 19940601.0001 ? 19950721.0137
13.7 MILLION TOKENS CHECK CORPUS
NYT 19950201.0001 ? 19950331.0494
1.7 MILLION TOKENS CHECK CORPUS
AFP 19940512.0003 ? 19940531.0197
354 K TOKENS TEST CORPUS
CNA 20041101.0006 ? 20041217.0009
Table 1: The corpora used in our experiments are selected
from the LDC English Gigaword corpus and specified in
this table, AFP, AFW, NYT, XIN and CNA denote the
sections of the LDC English Gigaword corpus.
vocabulary: 60 k, open - all words outside the
vocabulary are mapped to the <unk> token,
these 60 k words are chosen from the most fre-
quently occurred words in 44 millions tokens
corpus;
? POS tag (also TAGGER operation) vocabulary:
69, closed;
? non-terminal tag vocabulary: 54, closed;
? CONSTRUCTOR operation vocabulary: 157,
closed.
Similar to SLM (Chelba and Jelinek, 2000), af-
ter the parses undergo headword percolation and
binarization, each model component of WORD-
PREDICTOR, TAGGER, and CONSTRUCTOR is
initialized from a set of parsed sentences. We use
the ?openNLP? software (Northedge, 2005) to parse
a large amount of sentences in the LDC English Gi-
gaword corpus to generate an automatic treebank,
which has a slightly different word-tokenization
than that of the manual treebank such as the Upenn
Treebank used in (Chelba and Jelinek, 2000). For
the 44 and 230 million tokens corpora, all sentences
are automatically parsed and used to initialize model
parameters, while for 1.3 billion tokens corpus, we
parse the sentences from a portion of the corpus that
206
contain 230 million tokens, then use them to initial-
ize model parameters. The parser at ?openNLP? is
trained by Upenn treebank with 1 million tokens and
there is a mismatch between Upenn treebank and
LDC English Gigaword corpus. Nevertheless, ex-
perimental results show that this approach is effec-
tive to provide initial values of model parameters.
As we have explained, the proposed EM algo-
rithms can be naturally cast into a MapReduce
framework, see more discussion in (Lin and Dyer,
2010). If we have access to a large cluster of
machines with Hadoop installed that are powerful
enough to process a billion tokens level corpus,
we just need to specify a map function and a re-
duce function etc., Hadoop will automatically par-
allelize and execute programs written in this func-
tional style. Unfortunately, we don?t have this kind
of resources available. Instead, we have access to a
supercomputer at a supercomputer center with MPI
installed that has more than 1000 core processors us-
able. Thus we implement our algorithms using C++
under MPI on the supercomputer, where we have to
write C++ codes for Map part and Reduce part, and
the MPI is used to take care of massage passing,
scheduling, synchronization, etc. between clients
and servers. This involves a fair amount of pro-
gramming work, even though our implementation
under MPI is not as reliable as under Hadoop but
it is more efficient. We use up to 1000 core proces-
sors to train the composite language models for 1.3
billion tokens corpus where 900 core processors are
used to store the parameters alone. We decide to use
linearly smoothed trigram as the baseline model for
44 million token corpus, linearly smoothed 4-gram
as the baseline model for 230 million token corpus,
and linearly smoothed 5-gram as the baseline model
for 1.3 billion token corpus. Model size is a big is-
sue, we have to keep only a small set of topics due to
the consideration in both computational time and re-
source demand. Table 2 shows the perplexity results
and computation time of composite n-gram/PLSA
language models that are trained on three corpora
when the pre-defined number of total topics is 200
but different numbers of most likely topics are kept
for each document in PLSA, the rest are pruned. For
composite 5-gram/PLSA model trained on 1.3 bil-
lion tokens corpus, 400 cores have to be used to
keep top 5 most likely topics. For composite tri-
gram/PLSA model trained on 44M tokens corpus,
the computation time increases drastically with less
than 5% percent perplexity improvement. So in the
following experiments, we keep top 5 topics for each
document from total 200 topics and all other 195
topics are pruned.
All composite language models are first trained
by performing N-best list approximate EM algo-
rithm until convergence, then EM algorithm for a
second stage of parameter re-estimation for WORD-
PREDICTOR and SEMANTIZER until conver-
gence. We fix the size of topics in PLSA to be 200
and then prune to 5 in the experiments, where the
unpruned 5 topics in general account for 70% prob-
ability in p(g|d). Table 3 shows comprehensive per-
plexity results for a variety of different models such
as composite n-gram/m-SLM, n-gram/PLSA, m-
SLM/PLSA, their linear combinations, etc., where
we use online EM with fixed learning rate to re-
estimate the parameters of the SEMANTIZER of
test document. The m-SLM performs competitively
with its counterpart n-gram (n=m+1) on large scale
corpus. In Table 3, for composite n-gram/m-SLM
model (n = 3,m = 2 and n = 4,m = 3) trained
on 44 million tokens and 230 million tokens, we cut
off its fractional expected counts that are less than a
threshold 0.005, this significantly reduces the num-
ber of predictor?s types by 85%. When we train
the composite language on 1.3 billion tokens cor-
pus, we have to both aggressively prune the param-
eters of WORD-PREDICTOR and shrink the order
of n-gram and m-SLM in order to store them in a
supercomputer having 1000 cores. In particular, for
composite 5-gram/4-SLM model, its size is too big
to store, thus we use its approximation, a linear com-
bination of 5-gram/2-SLM and 2-gram/4-SLM, and
for 5-gram/2-SLM or 2-gram/4-SLM, again we cut
off its fractional expected counts that are less than a
threshold 0.005, this significantly reduces the num-
ber of predictor?s types by 85%. For composite 4-
SLM/PLSA model, we cut off its fractional expected
counts that are less than a threshold 0.002, again this
significantly reduces the number of predictor?s types
by 85%. For composite 4-SLM/PLSA model or its
linear combination with models, we ignore all the
tags and use only the words in the 4 head words.
In this table, we have three items missing (marked
by ?), since the size of corresponding model is
207
CORPUS n # OF PPL TIME # OF # OF # OF TYPES
TOPICS (HOURS) SERVERS CLIENTS OF ww?1
?n+1g
44M 3 5 196 0.5 40 100 120.1M
3 10 194 1.0 40 100 218.6M
3 20 190 2.7 80 100 537.8M
3 50 189 6.3 80 100 1.123B
3 100 189 11.2 80 100 1.616B
3 200 188 19.3 80 100 2.280B
230M 4 5 146 25.6 280 100 0.681B
1.3B 5 2 111 26.5 400 100 1.790B
5 5 102 75.0 400 100 4.391B
Table 2: Perplexity (ppl) results and time consumed of composite n-gram/PLSA language model trained on three
corpora when different numbers of most likely topics are kept for each document in PLSA.
LANGUAGE MODEL 44M REDUC- 230M REDUC- 1.3B REDUC-
n=3,m=2 TION n=4,m=3 TION n=5,m=4 TION
BASELINE n-GRAM (LINEAR) 262 200 138
n-GRAM (KNESER-NEY) 244 6.9% 183 8.5% ? ?
m-SLM 279 -6.5% 190 5.0% 137 0.0%
PLSA 825 -214.9% 812 -306.0% 773 -460.0%
n-GRAM+m-SLM 247 5.7% 184 8.0% 129 6.5%
n-GRAM+PLSA 235 10.3% 179 10.5% 128 7.2%
n-GRAM+m-SLM+PLSA 222 15.3% 175 12.5% 123 10.9%
n-GRAM/m-SLM 243 7.3% 171 14.5% (125) 9.4%
n-GRAM/PLSA 196 25.2% 146 27.0% 102 26.1%
m-SLM/PLSA 198 24.4% 140 30.0% (103) 25.4%
n-GRAM/PLSA+m-SLM/PLSA 183 30.2% 140 30.0% (93) 32.6%
n-GRAM/m-SLM+m-SLM/PLSA 183 30.2% 139 30.5% (94) 31.9%
n-GRAM/m-SLM+n-GRAM/PLSA 184 29.8% 137 31.5% (91) 34.1%
n-GRAM/m-SLM+n-GRAM/PLSA 180 31.3% 130 35.0% ? ?
+m-SLM/PLSA
n-GRAM/m-SLM/PLSA 176 32.8% ? ? ? ?
Table 3: Perplexity results for various language models on test corpus, where + denotes linear combination, / denotes
composite model; n denotes the order of n-gram and m denotes the order of SLM; the topic nodes are pruned from
200 to 5.
too big to store in the supercomputer. The com-
posite n-gram/m-SLM/PLSA model gives signifi-
cant perplexity reductions over baseline n-grams,
n = 3, 4, 5 and m-SLMs, m = 2, 3, 4. The major-
ity of gains comes from PLSA component, but when
adding SLM component into n-gram/PLSA, there is
a further 10% relative perplexity reduction.
We have applied our composite 5-gram/2-
SLM+2-gram/4-SLM+5-gram/PLSA language
model that is trained by 1.3 billion word corpus for
the task of re-ranking the N -best list in statistical
machine translation. We used the same 1000-best
list that is used by Zhang et al (2006). This
list was generated on 919 sentences from the
MT03 Chinese-English evaluation set by Hiero
(Chiang, 2005; Chiang, 2007), a state-of-the-art
parsing-based translation model. Its decoder uses
a trigram language model trained with modified
Kneser-Ney smoothing (Kneser and Ney, 1995) on
a 200 million tokens corpus. Each translation has
11 features and language model is one of them.
We substitute our language model and use MERT
(Och, 2003) to optimize the BLEU score (Papineni
et al, 2002). We partition the data into ten pieces,
9 pieces are used as training data to optimize the
BLEU score (Papineni et al, 2002) by MERT (Och,
208
2003), a remaining single piece is used to re-rank
the 1000-best list and obtain the BLEU score. The
cross-validation process is then repeated 10 times
(the folds), with each of the 10 pieces used exactly
once as the validation data. The 10 results from the
folds then can be averaged (or otherwise combined)
to produce a single estimation for BLEU score.
Table 4 shows the BLEU scores through 10-fold
cross-validation. The composite 5-gram/2-SLM+2-
gram/4-SLM+5-gram/PLSA language model gives
1.57% BLEU score improvement over the baseline
and 0.79% BLEU score improvement over the
5-gram. This is because there is not much diversity
on the 1000-best list, and essentially only 20 ? 30
distinct sentences are there in the 1000-best list.
Chiang (2007) studied the performance of machine
translation on Hiero, the BLEU score is 33.31%
when n-gram is used to re-rank the N -best list, how-
ever, the BLEU score becomes significantly higher
37.09% when the n-gram is embedded directly into
Hiero?s one pass decoder, this is because there is not
much diversity in the N -best list. It is expected that
putting the our composite language into a one pass
decoder of both phrase-based (Koehn et al, 2003)
and parsing-based (Chiang, 2005; Chiang, 2007)
MT systems should result in much improved BLEU
scores.
SYSTEM MODEL MEAN (%)
BASELINE 31.75
5-GRAM 32.53
5-GRAM/2-SLM+2-GRAM/4-SLM 32.87
5-GRAM/PLSA 33.01
5-GRAM/2-SLM+2-GRAM/4-SLM 33.32
+5-GRAM/PLSA
Table 4: 10-fold cross-validation BLEU score results for
the task of re-ranking the N -best list.
Besides reporting the BLEU scores, we look at the
?readability? of translations similar to the study con-
ducted by Charniak et al (2003). The translations
are sorted into four groups: good/bad syntax crossed
with good/bad meaning by human judges, see Ta-
ble 5. We find that many more sentences are perfect,
many more are grammatically correct, and many
more are semantically correct. The syntactic lan-
guage model (Charniak, 2001; Charniak, 2003) only
improves translations to have good grammar, but
does not improve translations to preserve meaning.
The composite 5-gram/2-SLM+2-gram/4-SLM+5-
gram/PLSA language model improves both signif-
icantly. Bear in mind that Charniak et al (2003) in-
tegrated Charniak?s language model with the syntax-
based translation model Yamada and Knight pro-
posed (2001) to rescore a tree-to-string translation
forest, whereas we use only our language model
for N -best list re-ranking. Also, in the same study
in (Charniak, 2003), they found that the outputs
produced using the n-grams received higher scores
from BLEU; ours did not. The difference between
human judgments and BLEU scores indicate that
closer agreement may be possible by incorporating
syntactic structure and semantic information into the
BLEU score evaluation. For example, semantically
similar words like ?insure? and ?ensure? in the ex-
ample of BLEU paper (Papineni et al, 2002) should
be substituted in the formula, and there is a weight
to measure the goodness of syntactic structure. This
modification will lead to a better metric and such
information can be provided by our composite lan-
guage models.
SYSTEM MODEL P S G W
BASELINE 95 398 20 406
5-GRAM 122 406 24 367
5-GRAM/2-SLM 151 425 33 310
+2-GRAM/4-SLM
+5-GRAM/PLSA
Table 5: Results of ?readability? evaluation on 919 trans-
lated sentences, P: perfect, S: only semantically correct,
G: only grammatically correct, W: wrong.
5 Conclusion
As far as we know, this is the first work of building a
complex large scale distributed language model with
a principled approach that is more powerful than n-
grams when both trained on a very large corpus with
up to a billion tokens. We believe our results still
hold on web scale corpora that have trillion tokens,
since the composite language model effectively en-
codes long range dependencies of natural language
that n-gram is not viable to consider. Of course,
this implies that we have to take a huge amount of
resources to perform the computation, nevertheless
this becomes feasible, affordable, and cheap in the
era of cloud computing.
209
References
L. Bahl and J. Baker,F. Jelinek and R. Mercer. 1977. Per-
plexity?a measure of difficulty of speech recognition
tasks. 94th Meeting of the Acoustical Society of Amer-
ica, 62:S63, Supplement 1.
T. Brants et al. 2007. Large language models in ma-
chine translation. The 2007 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
858-867.
E. Charniak. 2001. Immediate-head parsing for language
models. The 39th Annual Conference on Association
of Computational Linguistics (ACL), 124-131.
E. Charniak, K. Knight and K. Yamada. 2003. Syntax-
based language models for statistical machine transla-
tion. MT Summit IX., Intl. Assoc. for Machine Trans-
lation.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. The 36th Annual
Conference on Association of Computational Linguis-
tics (ACL), 225-231.
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283-332.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. The 43th Annual Con-
ference on Association of Computational Linguistics
(ACL), 263-270.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201-228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. Operating Sys-
tems Design and Implementation (OSDI), 137-150.
A. Dempster, N. Laird and D. Rubin. 1977. Maximum
likelihood estimation from incomplete data via the EM
algorithm. Journal of Royal Statistical Society, 39:1-
38.
A. Emami, K. Papineni and J. Sorensen. 2007. Large-
scale distributed language modeling. The 32nd IEEE
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), IV:37-40.
T. Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177-196.
F. Jelinek and R. Mercer. 1981. Interpolated estimation
of Markov source parameters from sparse data. Pat-
tern Recognition in Practice, 381-397.
F. Jelinek and C. Chelba. 1999. Putting language
into language modeling. Sixth European Confer-
ence on Speech Communication and Technology (EU-
ROSPEECH), Keynote Paper 1.
F. Jelinek. 2004. Stochastic analysis of structured lan-
guage modeling. Mathematical Foundations of Speech
and Language Processing, 37-72, Springer-Verlag.
D. Jurafsky and J. Martin. 2008. Speech and Language
Processing, 2nd Edition, Prentice Hall.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. The 20th IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP), 181-184.
P. Koehn, F. Och and D. Marcu. 2003. Statistical phrase-
based translation. The Human Language Technology
Conference (HLT), 48-54.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, 14(4):355-372.
A. Lavie et al 2006. MINDS Workshops Machine
Translation Working Group Final Report. http://www-
nlpir.nist.gov/MINDS/FINAL/MT.web.pdf
J. Lin and C. Dyer. 2010. Data-Intensive Text Processing
with MapReduce. Morgan and Claypool Publishers.
R. Northedge. 2005. OpenNLP software
http://www.codeproject.com/KB/recipes/englishpar
sing.aspx
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. The 41th Annual meeting of
the Association for Computational Linguistics (ACL),
311-318.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. The 40th Annual meeting of the Associa-
tion for Computational Linguistics (ACL), 311-318.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249-276.
S. Wang et al 2005. Exploiting syntactic, semantic and
lexical regularities in language modeling via directed
Markov random fields. The 22nd International Con-
ference on Machine Learning (ICML), 953-960.
S. Wang et al 2006. Stochastic analysis of lexical and
semantic enhanced structural language model. The 8th
International Colloquium on Grammatical Inference
(ICGI), 97-111.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. The 39th Annual Conference
on Association of Computational Linguistics (ACL),
1067-1074.
W. Zangwill. 1969. Nonlinear Programming: A Unified
Approach. Prentice-Hall.
Y. Zhang, A. Hildebrand and S. Vogel. 2006. Dis-
tributed language modeling for N-best list re-ranking.
The 2006 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), 216-223.
Y. Zhang, 2008. Structured language models for statisti-
cal machine translation. Ph.D. dissertation, CMU.
210
