Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 486?495, Prague, June 2007. c?2007 Association for Computational Linguistics
An Approach to Text Corpus Construction which Cuts Annotation Costs
and Maintains Reusability of Annotated Data
Katrin Tomanek Joachim Wermter Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30
D-07743 Jena, Germany
{tomanek|wermter|hahn}@coling-uni-jena.de
Abstract
We consider the impact Active Learning
(AL) has on effective and efficient text cor-
pus annotation, and report on reduction rates
for annotation efforts ranging up until 72%.
We also address the issue whether a corpus
annotated by means of AL ? using a particu-
lar classifier and a particular feature set ? can
be re-used to train classifiers different from
the ones employed by AL, supplying alter-
native feature sets as well. We, finally, report
on our experience with the AL paradigm un-
der real-world conditions, i.e., the annota-
tion of large-scale document corpora for the
life sciences.
1 Introduction
The annotation of corpora has become a crucial pre-
requisite for NLP utilities which rely on (semi-) su-
pervised machine learning (ML) techniques. While
stability, by and large, has been reached for tagsets
up until the syntax layer, semantic annotations in
terms of (named) entities, semantic roles, proposi-
tions, events, etc. reveal a high degree of variability
due to the inherent domain-dependence of the under-
lying tagsets. This diversity fuels a continuous need
for creating semantic annotation data anew.
Accordingly, annotation activities will persist and
even increase in number as HLT is expanding on
various technical and scientific domains (e.g., the
life sciences) outside the classical general-language
newspaper genre. Since the provision of annota-
tions is a costly, labor-intensive and error-prone pro-
cess the amount of work and time this activity re-
quires should be minimized to the extent that corpus
data could still be used to effectively train ML-based
NLP components on them. The approach we ad-
vocate does exactly this and yields reduction gains
(compared with standard procedures) ranging be-
tween 48% to 72%, without seriously sacrificing an-
notation quality.
Various techniques to minimize the necessary
amount of annotated training material have al-
ready been investigated. In co-training (Blum and
Mitchell, 1998), e.g., from a small initial set of la-
beled data multiple learners mutually provide new
training material for each other by labeling unseen
examples. Pierce and Cardie (2001) have shown,
however, that for tasks which require large numbers
of labeled examples ? such as most NLP tasks ? co-
training might be inadequate because it tends to gen-
erate noisy data. Furthermore, a well compiled ini-
tial training set is a crucial prerequisite for success-
ful co-training. As another alternative for minimiz-
ing annotation work, active learning (AL) is based
on the idea to let the learner have control over the ex-
amples to be manually labeled so as to optimize the
prediction accuracy. Accordingly, AL aims at select-
ing those examples with high utility for the model.
AL (as well as semi-supervised methods) is typi-
cally considered as a learning protocol, i.e., to train
a particular classifier. In contrast, we here propose
to employ AL as a corpus annotation method. A
corpus built on these premises must, however, still
be reusable in a flexible way so that, e.g., train-
ing with modified or improved classifiers is feasible
and reasonable on AL-generated corpora. Baldridge
and Osborne (2004) have already argued that this is
a highly critical requirement because the examples
selected by AL are tuned to one particular classi-
fier. The second major contribution of this paper ad-
486
dresses this issue and provides empirical evidence
that corpora built with one type of classifier (based
on Maximum Entropy) can reasonably be reused by
another, methodologically related type of classifier
(based on Conditional Random Fields) without re-
quiring changes of the corpus data. We also show
that feature sets being used for training classifiers
can be enhanced without invalidating corpus annota-
tions generated on the basis of AL and, hence, with
a poorer feature set.
2 Related Work
There are mainly two methodological strands of
AL research, viz. optimization approaches which
aim at selecting those examples that optimize some
(algorithm-dependent) objective function, such as
prediction variance (Cohn et al, 1996), and heuris-
tic methods with uncertainty sampling (Lewis and
Catlett, 1994) and query-by-committee (QBC) (Se-
ung et al, 1992) just to name the most prominent
ones. AL has already been applied to several NLP
tasks, such as document classification (Schohn and
Cohn, 2000), POS tagging (Engelson and Dagan,
1996), chunking (Ngai and Yarowsky, 2000), statis-
tical parsing (Thompson et al, 1999; Hwa, 2000),
and information extraction (Lewis and Catlett, 1994;
Thompson et al, 1999).
In a more recent study, Shen et al (2004) consider
AL for entity recognition based on Support Vector
Machines. Here, the informativeness of an exam-
ple is estimated by the distance to the hyperplane of
the currently learned SVM. It is assumed that an ex-
ample which lies close to the hyperplane has high
chances to have an effect on training. This approach
is essentially limited to the SVM learning scheme as
it solely relies on SVM-internal selection criteria.
Hachey et al (2005) propose a committee-based
AL approach where the committee?s classifiers con-
stitute multiple views on the data by employing dif-
ferent feature subsets. The authors focus on (pos-
sible) negative side effects of AL on the annota-
tions. They argue that AL annotations are cogni-
tively more difficult to deal with for the annota-
tors (because of the increased complexity of the se-
lected sentences). Hence, lower annotation quality
and higher per-sentence annotation times might be a
concern.
There are controversial findings on the reusabil-
ity of data annotated by means of AL for the prob-
lem of parse tree selection. Whereas Hwa (2001) re-
ports positive results, Baldridge and Osborne (2004)
argue that AL based on uncertainty sampling may
face serious performance degradation when labeled
data is reused for training a classifier different from
the one employed during AL. For committee-based
AL, however, there is a lack of work on reusabil-
ity. Our experiments of committee-based AL for en-
tity recognition, however, reveal that for this task at
least, reusability can be guaranteed to a very large
extent.
3 AL for Corpus Annotation -
Requirements for Practical Use
AL frameworks for real-world corpus annotation
should meet the following requirements:
fast selection time cycles ? AL-based corpus an-
notation is an interactive process in which b
sentences are selected by the AL engine for hu-
man annotation. Once the annotated data is
supplied, the AL engine retrains its underly-
ing classifier(s) on all available annotations and
then re-classifies all unseen corpus items. After
that the most informative (i.e., deviant) b sen-
tences from the set of newly classified data are
selected for the next iteration round. In this ap-
proach the time needed to select the next exam-
ples (which is the idle time of the human an-
notators) has to be kept at an acceptable limit
of a few minutes only. There are various AL
strategies which ? although they yield theoreti-
cally near-optimal sample selection ? turn out
to be actually impractible for real-world use
because of excessively high computation times
(cf. Cohn et al (1996)). Thus, AL-based an-
notation should be based on a computationally
tractable and task-wise feasible and acceptable
selection strategy (even if this might imply a
suboptimal reduction of annotation costs).
reusability ? The examples AL selects for man-
ual annotation are dependent on the model be-
ing used, up to a certain extent (Baldridge and
Osborne, 2004). During annotation time, how-
ever, the best model might not be known and
487
model tuning (especially the choice of features)
is typically performed once a training corpus
is available. Hence, from a practical point of
view, the resulting corpus should be reusable
with modified classifiers as well.
adaptive stopping criterion ? An explicit and
adaptive stopping criterion which is sensitive
towards the already achieved level of quality of
the annotated corpus is clearly preferred over
stopping after an a priori fixed number of an-
notation iterations.
If these requirements, especially the first and the
second one, cannot be guaranteed for a specific an-
notation task one should refrain from using AL. The
efficiency of AL-driven annotation (in terms of the
time needed to compile high quality training mate-
rial) might be worse compared to the annotation of
randomly (or subjectively) selected examples.
4 Framework for AL-based Named Entity
Annotation
For named entity recognition (NER), each change
of the application domain requires a more or less
profound change of the types of semantic categories
(tags) being used for corpus annotation. Hence, one
may encounter a lack of training material for various
relevant (sub)domains. Once this data is available,
however, one might want to modify the features of
the final classifier with respect to the specific entity
types. Thus, a corpus annotated by means of AL has
to provide the flexibility to modify the features of
the final classifier.
To meet the requirements from above under the
constraints of a real-world annotation task, we
decided for QBC-based AL, a heuristic AL ap-
proach, which is computationally less complex and
resource-greedy than objective function AL meth-
ods (the latter explicitly quantify the differences be-
tween the current and an ideal classifier in terms
of some objective function). Accordingly, we ruled
out uncertainty sampling, another heuristic AL ap-
proach, because it was shown before that QBC is
more efficient and robust (Freund et al, 1997).
QBC is based on the idea to select those examples
for manual annotation on which a committee of clas-
sifiers disagree most in their predictions (Engelson
and Dagan, 1996). A committee consists of a num-
ber of k classifiers of the same type (same learning
algorithm, parameters, and features) but trained on
different subsets of the training data. QBC-based
AL is also iterative. In each AL round the com-
mittee?s k classifiers are trained on the already an-
notated data C, then a pool of unannotated data P
is predicted with each classifier resulting in n au-
tomatically labeled versions of P . These are then
compared according to their labels. Those with the
highest variance are selected for manual annotation.
4.1 Selection Strategy
In each iteration, a batch of b examples is selected
for manual annotation. The informativeness of an
example is estimated in terms of the disagreement,
i.e., the uncertainty among the committee?s classi-
fiers on classifying a particular example. This is
measured by the vote entropy (Engelson and Dagan,
1996), i.e., the entropy of the distribution of classi-
fications assigned to an example by the classifiers.
Vote entropy is defined on the token level t as:
Dtok(t) := ?
1
log k
?
li
V (li, t)
k log
V (li, t)
k
where V (li,t)k is the ratio of k classifiers where the
label li is assigned to a token t. As (named) en-
tities often span more than a single text token we
consider complete sentences as a reasonable exam-
ple size unit1 for AL and calculate the disagreement
of a sentence Dsent as the mean vote entropy of
its single tokens. Since the vote entropy is mini-
mal when all classifiers agree in their vote, sentences
with high disagreement are preferred for manual an-
notation. With informed decisions of human anno-
tators made available, the potential for future dis-
agreement of the classifier committee on conflicting
instances should decrease. Thus, each AL iteration
selects the b sentences with the highest disagreement
to focus on the most controversial decision prob-
lems.
Besides informativeness, additional criteria can
be envisaged for the selection of examples, e.g., di-
1Sentence-level examples are but one conceivable grain size
? lower grains (such as clauses or phrases) as well as higher
grains (e.g., paragraphs or abstracts) are equally possible, with
different implications for the AL process.
488
feature class description
orthographical based on regular expressions (e.g. Has-
Dash, IsGreek, ...), token transforma-
tion rule: capital letters replaced by ?A?,
lowercase letters by ?a?, digits by ?0?,
etc. (e.g., IL2 ? AA0, have ? aaaa)
lexical and
morphological
prefix and suffix of length 3, stemmed
version of each token
syntactic the token?s part-of-speech tag
contextual features of neighboring tokens
Table 1: Features used for AL
versity of a batch and representativeness of the re-
spective example (to avoid outliers) (Shen et al,
2004). We experimented with these more sophis-
ticated selection strategies but preliminary experi-
ments did not reveal any significant improvement of
the AL performance. Engelson and Dagan (1996)
confirm this observation that, in general, different
(and even more refined) selection methods still yield
similar results. Moreover, strategies incorporating
more selection criteria often require more parame-
ters to be set. However, proper parametrization is
hard to achieve in real-world applications. Using
disagreement exclusively for selection requires only
one parameter, viz. the batch size b, to be specified.
4.2 Classifier and Features
For our AL framework we decided to employ a Max-
imum Entropy (ME) classifier (Berger et al, 1996).
We employ a rich set of features (see Table 1) which
are general enough to be used in most (sub)domains
for entity recognition. We intentionally avoided us-
ing features such as semantic triggers or external
dictionary look-ups because they depend a lot on
the specific subdomain and entity types being used.
However, one might add them to fin- tune the final
classifier, if needed. ME classifiers outperform their
generative counterparts (e.g., Na??ve Bayesian clas-
sifiers) because they can easily handle overlapping,
probably dependent features which might be con-
tained in rich feature sets. We also favored an ME
classifier over an SVM one because the latter is com-
putationally much more complex on rich feature sets
and multiple classes and is thus not so well suited for
an interactive process like AL.
It has been shown that Conditional Random
Fields (CRF) (Lafferty et al, 2001) achieve higher
performance on many NLP tasks, such as NER, but
on the other hand they are computionally more com-
plex than an ME classifier making them also im-
practical for the interactive AL process. Thus, in
our committee we employ ME classifiers to meet re-
quirement 1 (fast selection time cycles). However,
in the end we want to use the annotated corpora to
train a CRF and will thus examine the reusability
of such an ME-annotated AL corpus for CRFs (cf.
Subsection 5.2).
4.3 Stopping Criterion
A question hardly addressed up until now is when to
actually terminate the AL process. Usually, it gets
stopped when the supervized learning performance
of the specific classifier is achieved. The problem
with such an approach is, however, that in prac-
tice one does not know the performance level which
could possibly be achieved on an unannotated cor-
pus.
An apparent way to monitor the progress of the
annotation process is to periodically (e.g., after each
AL iteration) train a classifier on the data annotated
so far and evaluate it against some randomly se-
lected gold standard. When the relative performance
growth of each AL iteration falls below a certain
threshold this might be a good reason to stop the an-
notation. Though this is probably the most reliable
way, it is impractical for many scenarios since as-
sembling and manually annotating a representative
gold standard may already be quite a laborious task.
Thus, a measure from which we can predict the de-
velopment of the learning curve would be beneficial.
One way to achieve this goal is to monitor the rate
of disagreement among the different classifiers after
each iteration. This rate will descend as the classi-
fiers get more and more robust in their predictions
on unseen data. Thus, an average disagreement ap-
proaching zero can be interpreted as an indication
that additional annotations will not render any fur-
ther improvement. In our experiments, we will show
that this is a valid stopping criterion, indeed.
5 Experiments and Results
For our experiments, we specified the following
three parameters: the batch size b (i.e., the num-
ber of sentences to be selected for each AL itera-
tion), the size and composition of the initial train-
489
ing set, and the number of k classifiers in a com-
mittee. The smaller the batch size, the higher the
AL performance turns out to be. In the special case
of batch size of b = 1 only that example with the
highest disagreement is selected. This is certainly
impractical since after each AL iteration a new com-
mittee of classifiers has to be trained causing unwar-
ranted annotation idle time. We found b = 20 to
be a good compromise between the annotators? idle
time and AL performance. The initial training set
also contains 20 sentences which are randomly se-
lected though. Our committee consists of k = 3
classifiers, which is a good trade-off between com-
putational complexity and diversity. Although the
AL iterations were performed on the sentence level,
we report on the number of annotated tokens. Since
sentences may considerably vary in their length the
number of tokens constitutes a better measure for an-
notation costs.
We ran our experiments on two common entity-
annotated corpora from two different domains (see
Table 2). From the general-language newspaper do-
main, we used the English data set of the CoNLL-
2003 shared task (Tjong Kim Sang and De Meul-
der, 2003). It consists of a collection of newswire
articles from the Reuters Corpus,2 which comes
annotated with three entity types: persons, loca-
tions, and organizations. From the sublanguage
biology domain we used the oncology part of the
PENNBIOIE corpus which consists of some 1150
PubMed abstracts. Originally, this corpus contains
gene, variation event, and malignancy entity annota-
tions. Manual annotation after each AL round was
simulated by moving the selected sentences from
the pool of unannotated sentences P to the train-
ing corpus T . For our simulations, we built two
subcorpora by filtering out entity annotations: the
PENNBIOIE gene corpus (PBgene), including the
three gene entity subtypes generic, protein, and rna,
and the PENNBIOIE variation events corpus (PB-
var) corpus including the variation entity subtypes
type, event, location, state-altered, state-generic,
and state-original. We split all three corpora into
two subsets, viz. AL simulation data and gold stan-
dard data on which we evaluate3 a classifier in terms
2http://trec.nist.gov/
3We use a strict evaluation criterion which only counts exact
matches as true positives because annotations having incorrect
corpus data set sentences tokens
CONLL AL 14,040 203,617
3 entities Gold 3,453 46,435
PBGENE AL 10,050 249,490
3 entities Gold 1,114 27,563
PBVAR AL 10,050 249,490
6 entities Gold 1,114 27,563
Table 2: Corpora used in the Experiments
of f-score trained on the annotated corpus after each
AL iteration (learning curve). As far as the CoNLL
corpus is concerned, we have used CoNLL?s training
set for AL and CoNLL?s test set as gold standard. As
for PBgene and PBvar, we randomly split the cor-
pora into 90% for AL and 10% as gold standard.
In the following experiments we will refer to the
classifiers used in the AL committee as selectors,
and the classifier used for evaluation as the tester.
5.1 Efficiency of AL and the Applicability of
the Stopping Criterion
In a first series of experiments, we evaluated whether
AL-based annotations can significantly reduce the
human effort compared to the standard annotation
procedure where sentences are selected randomly
(or subjectively). We also show that disagreement
is an accurate stopping criterion. As described in
Section 4.2, we here employed a committee of ME
classifiers for AL; a CRF was used as tester for both
the AL and the random selection. Figures 1, 2, and 3
depict the learning curves for AL selection and ran-
dom selection (upper two curves) and the respective
disagreement curves (lower curve). The random se-
lection curves contained in these plots are averaged
over three random selection runs.
With AL, we get a maximum f-score of ? 84.5%
on the CoNLL corpus after about 118,000 tokens. At
about the same number of tokens the disagreement
curve drops down to values of around Dsent = 0.
Comparing AL and random selection, an f-score of
? 84% is reached after 86,000 and 165,000 tokens,
respectively, which means a reduction of annotation
costs of about 48%. On PBgene, the effect of AL is
comparable: a maximum value of 83.5% f-score is
reached first after about 124,000 tokens, a data point
where hardly any disagreement between the com-
mittee?s classifiers occurs. For, e.g., an f-score of
boundaries are insufficient for manual corpus annotation.
490
Figure 1: CoNLL Corpus: Learning/Disagreement Curves
Figure 2: PBgene Corpus: Learning/Disagreement Curves
Figure 3: PBvar Corpus: Learning/Disagreement Curves
corpus selection F tokens reduction
CONLL random 84.0 165,000
AL 84.0 86,000 ? 48%
PBGENE random 83.0 101,000
AL 83.0 213,000 ? 53%
PBVAR random 80.0 56,000
AL 80.0 200,000 ? 72%
Table 3: Reduction of Annotation Costs Achieved
with AL-based Annotation
83%, the annotation effort can be reduced by about
53% using AL. On PBvar, an f-score of about 80%
is reached after ? 56,000 tokens when using AL se-
lection, while 200,000 tokens are needed with ran-
dom selection. For this task, AL reduces the an-
notation effort by of 72%. Here, the disagreement
curve approaches values of zero after approximately
80,000 tokens. At about this point the learning curve
reaches its maximum of about 81% f-score. Ta-
ble 3 summarizes the reduction of annotation costs
achieved on all three corpora.
Comparing both PENNBIOIE simulations, obvi-
ously, the reduction of annotation costs through AL
is much higher for the variation type entities than for
the gene entities. We hypothesize this to be mainly
due to incomparable entity densities. Whereas the
gene entities are quite frequent (about 1.3 per sen-
tence on average), the variation entities are rather
sparse (0.62 per sentence on average) making it an
ideal playground for AL-based annotation. Our ex-
periments also reveal that disagreement approaching
values of zero is a valid stopping criterion. This is,
under all circumstances, definitely the point when
AL-based annotation should stop because then all
classifiers of the committee vote consistently. Any
further selection ? even though AL selection is used
? is then, actually, a random selection. If, due to
reasons whatsoever, further annotations are wanted,
a direct switch to random selection is advisable be-
cause this is computationally less expensive than
AL-based selection.
5.2 Reusability
To evaluate whether the proposed AL framework for
named entity annotation allows for flexible re-use
of the annotated data, we performed experiments
where we varied both the learning algorithms and
the features of the selectors.
491
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
14012010080604020
F-
sc
or
e
K tokens
AL (CRF committee)
AL (ME committee)
AL (NB committee)
random selection
Figure 4: Algorithm Flexibility on PBvar
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
14012010080604020
F-
sc
or
e
K tokens
AL (CRF committee)
AL (ME committee)
AL (NB committee)
random selection
Figure 5: AlgorithmFflexibility on CoNLL
First, we analyzed the effect of different proba-
bilistic classifiers as selectors on the resulting learn-
ing curve of the CRF tester. Figures 4 and 5 show
the learning curves on our original ME committee,
a CRF committee, and also a committee of Na??ve
Bayes (NB) classifiers. It is not surprising that self-
reuse (CRF selectors and CRF tester) yields the best
results. Switching from CRF selectors to ME selec-
tors has almost no negative effect. Even with a com-
mittee of NB selectors (an ML approach which is
essentially less well suited for the NER task), AL-
based selection is still substantially more efficient
than random selection on both corpora. This shows
that our approach to use the less complex ME clas-
sifiers for the AL selection process has the positive
effect of fast selection cycle times at almost no costs.
This is especially interesting as the performance of
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
14012010080604020
F-
sc
or
e
K tokens
all features
sub1
sub2
sub3
random selection
Figure 6: Feature Flexibility on PBvar
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
14012010080604020
F-
sc
or
e
K tokens
all features
sub1
sub2
sub3
random selection
Figure 7: Feature Flexibility on ConLL
an ME classifier trained in supervized manner on
the complete corpus is significantly worse (several
percentage points of f-measure) than a CRF. That
means, even though an ME classifier is less well
suited as the final classifier, it works well as a se-
lector for CRFs.4
Second, we ran experiments on selectors with
only some features and our CRF tester with all fea-
tures (cf. Table 1). Feature subset 1 (sub1) contains
all but the syntactic features. In the second subset
(sub2), also morphological and lexical features are
missing. The third set (sub3) only contains ortho-
graphical features. We ran an AL simulation for
4We have also conducted experiments where we varied the
learning algorithms of the tester (we experimented with NB,
ME, MEMM, and CRFs) ? with comparable results. In a real-
istic scenario, however, on would rather choose a CRF as final
tester over, e.g., a NB.
492
each feature subset with a committee of CRF se-
lectors.5 Figures 6 and 7 show the various learning
curves. Here we see that a corpus that was produced
with AL on sub1 can easily be re-used by a tester
with little more features. This is probably the most
realistic scenario: the core features are kept and
only a few specific features (e.g., POS, a dictionary
look-up, chunk information, etc.) are added. When
adding substantially more features to the tester than
were available during AL time, the respective learn-
ing curves drop down towards the learning curve for
random selection. But even with a selector which
has only orthographical features and a tester with
many more features ? which is actually quite an ex-
treme example and a rather unrealistic scenario for
a real-world application ? AL is more efficient than
random selection. However, the limits of reusability
are taking shape: on PBvar, the AL selection with
sub3 converges with the random selection curve af-
ter about 100,000 tokens.
5.3 Findings with Real AL Annotation
We currently perform AL entity mention annotations
for an information extraction project in the biomedi-
cal subdomain of immunogenetics. For this purpose,
we retrieved about 200,000 abstracts (? 2,000,000
sentences) as our document pool of unlabeled exam-
ples from PUBMED. By means of random subsam-
pling, only about 40,000 sentences are considered in
each round of AL selection. To regularly monitor
classifier performance, we also perform gold stan-
dard (GS) annotations on 250 randomly chosen ab-
stracts (? 2,200 sentences). In all our annotations of
different entity types so far, we found AL learning
curves similar to the ones reported in our simula-
tion experiments, with classifier performance level-
ling off at around 75% - 85% f-score (depending on
the entity type).
Our annotations also reveal that AL is especially
beneficial when entity mentions are very sparse.
Figure 8 shows the cumulated entity density on AL
and gold standard annotations of cytokine receptors
(specialized proteins for which we annotated six dif-
ferent entity subtypes) ? very sparse entity types
with less than one entity mention per PUBMED ab-
stract on the average. As can be seen, after 2,000
5Here, we employed CRF instead of ME selectors to isolate
the effect of feature re-usability.
 0
 500
 1000
 1500
 2000
 2500
 3000
 200  400  600  800  1000  1200  1400  1600  1800  2000
e
n
tit
y 
m
en
tio
ne
s
sentences
GS annotation
AL annotation
Figure 8: Cumulated Entity Density on AL and GS
Annotations of Cytokine Receptors
sentences the entity density in our AL corpus is al-
most 15 times higher than in our GS corpus. Such a
dense corpus may be more appropriate for classifier
training than a sparse one yielded by random or se-
quential annotations, which may just contain lots of
negative training examples. We have observed com-
parable effects with other entity types, too, and thus
conclude that the sparser entity mentions of a spe-
cific type are in texts, the more beneficial AL-based
annotation is. We report on other aspects of AL for
real annotation projects in Tomanek et al (2007).
6 Discussion and Conclusions
We have shown, for the annotation of (named) en-
tities, that AL is well-suited to speed up annotation
work under realistic conditions. In our simulations
we yielded gains (in the number of tokens) up to
72%. We collected evidence that an average dis-
agreement approaching zero may serve as an adap-
tive stopping criterion for AL-driven annotation and
that a corpus compiled by means of QBC-based AL
is to a large extent reusable by modified classifiers.
These findings stand in contrast to those supplied
by Baldridge and Osborne (2004) who focused on
parse selection. Their research indicates that AL on
selectors with different learning algorithms and fea-
ture sets then used by the tester can easily get worse
than random selection. They conclude that it might
not be be advisable to employ AL in environments
where the final classifier is not very stable.
Our evidence leads us to a re-assessment of AL-
493
based annotations. First, we employed a committee-
based (QBC) while Baldridge and Osborne per-
formed uncertainty sampling AL. Committee-based
approaches calculate the uncertainty on an exam-
ple in a more implicit way, i.e., by the disagree-
ment among the committee?s classifiers. With uncer-
tainty sampling, however, the labeling uncertainty
of one classifier is considered directly. In future
work we will directly compare QBC and uncertainty
sampling with respect to data reusability. Second,
whereas Baldridge and Osborne employed AL on a
scoring or ranking problem we focused on classifica-
tion problems. Further research is needed to inves-
tigate whether the problem class (classification with
a fixed and moderate number of classes vs. ranking
large numbers of possible candidates) is responsible
for limited data reusability.
On the basis of our experiments we stipulate that
the proposed AL approach might be applicable with
comparable results to a wider range of corpus anno-
tation tasks, which otherwise would require substan-
tially larger amounts of annotation efforts.
Acknowledgements
This research was funded by the EC within the
BOOTStrep project (FP6-028099), and by the Ger-
man Ministry of Education and Research within the
StemNet project (01DS001A to 1C).
References
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Dekang Lin
and Dekai Wu, editors, EMNLP 2004 ? Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing, pages 9?16. Barcelona, Spain,
July 25-26, 2004. Association for Computational Lin-
guistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT?98 ? Proceedings of the 11th Annual Confer-
ence on Computational Learning Theory, pages 92?
100. Madison, Wisconsin, USA, July 24-26, 1998.
New York, NY: ACM Press.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artifical Intelligence Research, 4:129?145.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In ACL?96 ? Proceedings of the 34th Annual
Meeting of the Association for Computational Linguis-
tics, pages 319?326. University of California at Santa
Cruz, California, U.S.A., 24-27 June 1996. San Fran-
cisco, CA: Morgan Kaufmann.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine Learning, 28(2-
3):133?168.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL-2005 ? Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151. Ann Arbor, MI, USA, June
2005. Association for Computational Linguistics.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In EMNLP/VLC-2000 ? Proceed-
ings of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pages 45?52. Hong Kong, China, Oc-
tober 7-8, 2000.. Association for Computational Lin-
guistics.
Rebecca Hwa. 2001. On minimizing training corpus for
parser acquisition. In Walter Daelemans and Re?mi
Zajac, editors, CoNLL-2001 ? Proceedings of the
5th Natural Language Learning Workshop. Toulouse,
France, 6-7 July 2001. Association for Computational
Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In ICML-2001 ? Proceedings of the 18th In-
ternational Conference on Machine Learning, pages
282?289. Williams College, MA, USA, June 28 - July
1, 2001. San Francisco, CA: Morgan Kaufmann.
David D. Lewis and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learning.
In William W. Cohen and Haym Hirsh, editors, ICML
?94: Proceedings of the 11th International Conference
on Machine Learning, pages 148?156. San Francisco,
CA: Morgan Kaufmann.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In ACL?00 ? Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 117?125. Hong Kong, China,
1-8 August 2000. San Francisco, CA: Morgan Kauf-
mann.
494
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Lillian Lee and Donna Harman, editors,
EMNLP 2001 ? Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1?9. Pittsburgh, PA, USA, June 3-4, 2001.
Association for Computational Linguistics.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In ICML
?00: Proceedings of the 17th International Conference
on Machine Learning, pages 839?846. San Francisco,
CA: Morgan Kaufmann.
H. Sebastian Seung, Manfred Opper, and Haim Som-
polinsky. 1992. Query by committee. In COLT?92 ?
Proceedings of the 5th Annual Conference on Compu-
tational Learning Theory, pages 287?294. Pittsburgh,
PA, USA, July 27-29, 1992. New York, NY: ACM
Press.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In ACL?04 ?
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 589?596.
Barcelona, Spain, July 21-26, 2004. San Francisco,
CA: Morgan Kaufmann.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In ICML
?99: Proceedings of the 16th International Conference
on Machine Learning, pages 406?414. Bled, Slovenia,
June 1999. San Francisco, CA: Morgan Kaufmann.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CONLL-2003 shared
task: Language-independent named entity recogni-
tion. In Walter Daelemans and Miles Osborne, edi-
tors, CoNLL-2003 ? Proceedings of the 7th Confer-
ence on Computational Natural Language Learning,
pages 142?147. Edmonton, Canada, 2003. Association
for Computational Linguistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Efficient annotation with the Jena ANnota-
tion Environment (JANE). In Proceedings of the ACL
2007 ?Linguistic Annotation Workshop ? A Merger of
NLPXML 2007 and FLAC 2007?. Prague, Czech Re-
public, June 28-29, 2007. Association for Computa-
tional Linguistics (ACL).
495
Proceedings of ACL-08: HLT, pages 861?869,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multi-Task Active Learning for Linguistic Annotations
Roi Reichart1? Katrin Tomanek2? Udo Hahn2 Ari Rappoport1
1Institute of Computer Science
Hebrew University of Jerusalem, Israel
{roiri|arir}@cs.huji.ac.il
2Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
We extend the classical single-task active
learning (AL) approach. In the multi-task ac-
tive learning (MTAL) paradigm, we select ex-
amples for several annotation tasks rather than
for a single one as usually done in the con-
text of AL. We introduce two MTAL meta-
protocols, alternating selection and rank com-
bination, and propose a method to implement
them in practice. We experiment with a two-
task annotation scenario that includes named
entity and syntactic parse tree annotations on
three different corpora. MTAL outperforms
random selection and a stronger baseline, one-
sided example selection, in which one task is
pursued using AL and the selected examples
are provided also to the other task.
1 Introduction
Supervised machine learning methods have success-
fully been applied to many NLP tasks in the last few
decades. These techniques have demonstrated their
superiority over both hand-crafted rules and unsu-
pervised learning approaches. However, they re-
quire large amounts of labeled training data for every
level of linguistic processing (e.g., POS tags, parse
trees, or named entities). When, when domains
and text genres change (e.g., moving from common-
sense newspapers to scientific biology journal arti-
cles), extensive retraining on newly supplied train-
ing material is often required, since different do-
mains may use different syntactic structures as well
as different semantic classes (entities and relations).
? Both authors contributed equally to this work.
Consequently, with an increasing coverage of a
wide variety of domains in human language tech-
nology (HLT) systems, we can expect a growing
need for manual annotations to support many kinds
of application-specific training data.
Creating annotated data is extremely labor-
intensive. The Active Learning (AL) paradigm
(Cohn et al, 1996) offers a promising solution to
deal with this bottleneck, by allowing the learning
algorithm to control the selection of examples to
be manually annotated such that the human label-
ing effort be minimized. AL has been successfully
applied already for a wide range of NLP tasks, in-
cluding POS tagging (Engelson and Dagan, 1996),
chunking (Ngai and Yarowsky, 2000), statistical
parsing (Hwa, 2004), and named entity recognition
(Tomanek et al, 2007).
However, AL is designed in such a way that it se-
lects examples for manual annotation with respect to
a single learning algorithm or classifier. Under this
AL annotation policy, one has to perform a separate
annotation cycle for each classifier to be trained. In
the following, we will refer to the annotations sup-
plied for a classifier as the annotations for a single
annotation task.
Modern HLT systems often utilize annotations re-
sulting from different tasks. For example, a machine
translation system might use features extracted from
parse trees and named entity annotations. For such
an application, we obviously need the different an-
notations to reside in the same text corpus. It is not
clear how to apply the single-task AL approach here,
since a training example that is beneficial for one
task might not be so for others. We could annotate
861
the same corpus independently by the two tasks and
merge the resulting annotations, but that (as we show
in this paper) would possibly yield sub-optimal us-
age of human annotation efforts.
There are two reasons why multi-task AL, and
by this, a combined corpus annotated for various
tasks, could be of immediate benefit. First, annota-
tors working on similar annotation tasks (e.g., con-
sidering named entities and relations between them),
might exploit annotation data from one subtask for
the benefit of the other. If for each subtask a sepa-
rate corpus is sampled by means of AL, annotators
will definitely lack synergy effects and, therefore,
annotation will be more laborious and is likely to
suffer in terms of quality and accuracy. Second, for
dissimilar annotation tasks ? take, e.g., a compre-
hensive HLT pipeline incorporating morphological,
syntactic and semantic data ? a classifier might re-
quire features as input which constitute the output
of another preceding classifier. As a consequence,
training such a classifier which takes into account
several annotation tasks will best be performed on
a rich corpus annotated with respect to all input-
relevant tasks. Both kinds of annotation tasks, simi-
lar and dissimilar ones, constitute examples of what
we refer to as multi-task annotation problems.
Indeed, there have been efforts in creating re-
sources annotated with respect to various annotation
tasks though each of them was carried out indepen-
dently of the other. In the general language UPenn
annotation efforts for the WSJ sections of the Penn
Treebank (Marcus et al, 1993), sentences are anno-
tated with POS tags, parse trees, as well as discourse
annotation from the Penn Discourse Treebank (Milt-
sakaki et al, 2008), while verbs and verb arguments
are annotated with Propbank rolesets (Palmer et al,
2005). In the biomedical GENIA corpus (Ohta et
al., 2002), scientific text is annotated with POS tags,
parse trees, and named entities.
In this paper, we introduce multi-task active
learning (MTAL), an active learning paradigm for
multiple annotation tasks. We propose a new AL
framework where the examples to be annotated are
selected so that they are as informative as possible
for a set of classifiers instead of a single classifier
only. This enables the creation of a single combined
corpus annotated with respect to various annotation
tasks, while preserving the advantages of AL with
respect to the minimization of annotation efforts.
In a proof-of-concept scenario, we focus on two
highly dissimilar tasks, syntactic parsing and named
entity recognition, study the effects of multi-task AL
under rather extreme conditions. We propose two
MTAL meta-protocols and a method to implement
them for these tasks. We run experiments on three
corpora for domains and genres that are very differ-
ent (WSJ: newspapers, Brown: mixed genres, and
GENIA: biomedical abstracts). Our protocols out-
perform two baselines (random and a stronger one-
sided selection baseline).
In Section 2 we introduce our MTAL framework
and present two MTAL protocols. In Section 3 we
discuss the evaluation of these protocols. Section
4 describes the experimental setup, and results are
presented in Section 5. We discuss related work in
Section 6. Finally, we point to open research issues
for this new approach in Section 7.
2 A Framework for Multi-Task AL
In this section we introduce a sample selection
framework that aims at reducing the human anno-
tation effort in a multiple annotation scenario.
2.1 Task Definition
To measure the efficiency of selection methods, we
define the training quality TQ of annotated mate-
rial S as the performance p yielded with a reference
learner X trained on that material: TQ(X, S) = p.
A selection method can be considered better than an-
other one if a higher TQ is yielded with the same
amount of examples being annotated.
Our framework is an extension of the Active
Learning (AL) framework (Cohn et al, 1996)). The
original AL framework is based on querying in an it-
erative manner those examples to be manually anno-
tated that are most useful for the learner at hand. The
TQ of an annotated corpus selected by means of AL
is much higher than random selection. This AL ap-
proach can be considered as single-task AL because
it focuses on a single learner for which the examples
are to be selected. In a multiple annotation scenario,
however, there are several annotation tasks to be ac-
complished at once and for each task typically a sep-
arate statistical model will then be trained. Thus, the
goal of multi-task AL is to query those examples for
862
human annotation that are most informative for all
learners involved.
2.2 One-Sided Selection vs. Multi-Task AL
The naive approach to select examples in a multiple
annotation scenario would be to perform a single-
task AL selection, i.e., the examples to be annotated
are selected with respect to one of the learners only.1
In a multiple annotation scenario we call such an ap-
proach one-sided selection. It is an intrinsic selec-
tion for the reference learner, and an extrinsic selec-
tion for all the other learners also trained on the an-
notated material. Obviously, a corpus compiled with
the help of one-sided selection will have a good TQ
for that learner for which the intrinsic selection has
taken place. For all the other learners, however, we
have no guarantee that their TQ will not be inferior
than the TQ of a random selection process.
In scenarios where the different annotation tasks
are highly dissimilar we can expect extrinsic selec-
tion to be rather poor. This intuition is demonstrated
by experiments we conducted for named entity (NE)
and parse annotation tasks2 (Figure 1). In this sce-
nario, extrinsic selection for the NE annotation task
means that examples where selected with respect
to the parsing task. Extrinsic selection performed
about the same as random selection for the NE task,
while for the parsing task extrinsic selection per-
formed markedly worse. This shows that examples
that were very informative for the NE learner were
not that informative for the parse learner.
2.3 Protocols for Multi-Task AL
Obviously, we can expect one-sided selection to per-
form better for the reference learner (the one for
which an intrinsic selection took place) than multi-
task AL selection, because the latter would be a
compromise for all learners involved in the multi-
ple annotation scenario. However, the goal of multi-
task AL is to minimize the annotation effort over all
annotation tasks and not just the effort for a single
annotation task.
For a multi-task AL protocol to be valuable in a
specific multiple annotation scenario, the TQ for all
considered learners should be
1Of course, all selected examples would be annotated w.r.t.
all annotation tasks.
2See Section 4 for our experimental setup.
1. better than the TQ of random selection,
2. and better than the TQ of any extrinsic selec-
tion.
In the following, we introduce two protocols for
multi-task AL. Multi-task AL protocols can be con-
sidered meta-protocols because they basically spec-
ify how task-specific, single-task AL approaches can
be combined into one selection decision. By this,
the protocols are independent of the underlying task-
specific AL approaches.
2.3.1 Alternating Selection
The alternating selection protocol alternates one-
sided AL selection. In sj consecutive AL iterations,
the selection is performed as one-sided selection
with respect to learning algorithm Xj . After that,
another learning algorithm is considered for selec-
tion for sk consecutive iterations and so on. Depend-
ing on the specific scenario, this enables to weight
the different annotation tasks by allowing them to
guide the selection in more or less AL iterations.
This protocol is a straight-forward compromise be-
tween the different single-task selection approaches.
In this paper we experiment with the special case
of si = 1, where in every AL iteration the selection
leadership is changed. More sophisticated calibra-
tion of the parameters si is beyond the scope of this
paper and will be dealt with in future work.
2.3.2 Rank Combination
The rank combination protocol is more directly
based on the idea to combine single-task AL selec-
tion decisions. In each AL iteration, the usefulness
score sXj (e) of each unlabeled example e from the
pool of examples is calculated with respect to each
learner Xj and then translated into a rank rXj (e)
where higher usefulness means lower rank number
(examples with identical scores get the same rank
number). Then, for each example, we sum the rank
numbers of each annotation task to get the overall
rank r(e) = ?nj=1 rXj (e). All examples are sorted
by this combined rank and b examples with lowest
rank numbers are selected for manual annotation.3
3As the number of ranks might differ between the single an-
notation tasks, we normalize them to the coarsest scale. Then
we can sum up the ranks as explained above.
863
10000 20000 30000 40000 50000
0.
65
0.
70
0.
75
0.
80
tokens
f?
sc
or
e
random selection
extrinsic selection (PARSE?AL)
10000 20000 30000 40000
0.
76
0.
78
0.
80
0.
82
0.
84
constituents
f?
sc
or
e
random selection
extrinsic selection (NE?AL)
Figure 1: Learning curves for random and extrinsic selection on both tasks: named entity annotation (left) and syntactic
parse annotation (right), using the WSJ corpus scenario
This protocol favors examples which are good for
all learning algorithms. Examples that are highly in-
formative for one task but rather uninformative for
another task will not be selected.
3 Evaluation of Multi-Task AL
The notion of training quality (TQ) can be used to
quantify the effectiveness of a protocol, and by this,
annotation costs in a single-task AL scenario. To ac-
tually quantify the overall training quality in a multi-
ple annotation scenario one would have to sum over
all the single task?s TQs. Of course, depending on
the specific annotation task, one would not want to
quantify the number of examples being annotated
but different task-specific units of annotation. While
for entity annotations one does typically count the
number of tokens being annotated, in the parsing
scenario the number of constituents being annotated
is a generally accepted measure. As, however, the
actual time needed for the annotation of one exam-
ple usually differs for different annotation tasks, nor-
malizing exchange rates have to be specified which
can then be used as weighting factors. In this paper,
we do not define such weighting factors4, and leave
this challenging question to be discussed in the con-
text of psycholinguistic research.
We could quantify the overall efficiency score E
of a MTAL protocol P by
E(P ) =
n
?
j=1
?j ? TQ(Xj , uj)
where uj denotes the individual annotation task?s
4Such weighting factors not only depend on the annotation
level or task but also on the domain, and especially on the cog-
nitive load of the annotation task.
number of units being annotated (e.g., constituents
for parsing) and the task-specific weights are defined
by ?j . Given weights are properly defined, such a
score can be applied to directly compare different
protocols and quantify their differences.
In practice, such task-specific weights might also
be considered in the MTAL protocols. In the alter-
nating selection protocol, the numbers of consecu-
tive iterations si each single task protocol can be
tuned according to the ? parameters. As for the
rank combination protocol, the weights can be con-
sidered when calculating the overall rank: r(e) =
?n
j=1 ?j ? rXj (e) where the parameters ?1 . . . ?n re-
flect the values of ?1 . . . ?n (though they need not
necessarily be the same).
In our experiments, we assumed the same weight
for all annotation schemata, thus simply setting si =
1, ?i = 1. This was done for the sake of a clear
framework presentation. Finding proper weights for
the single tasks and tuning the protocols accordingly
is a subject for further research.
4 Experiments
4.1 Scenario and Task-Specific Selection
Protocols
The tasks in our scenario comprise one semantic
task (annotation with named entities (NE)) and one
syntactic task (annotation with PCFG parse trees).
The tasks are highly dissimilar, thus increasing the
potential value of MTAL. Both tasks are subject to
intensive research by the NLP community.
The MTAL protocols proposed are meta-
protocols that combine the selection decisions of
the underlying, task-specific AL protocols. In
our scenario, the task-specific AL protocols are
864
committee-based (Freund et al, 1997) selection
protocols. In committee-based AL, a committee
consists of k classifiers of the same type trained
on different subsets of the training data.5 Each
committee member then makes its predictions on
the unlabeled examples, and those examples on
which the committee members disagree most are
considered most informative for learning and are
thus selected for manual annotation. In our scenario
the example grain-size is the sentence level.
For the NE task, we apply the AL approach of
Tomanek et al (2007). The committee consists of
k1 = 3 classifiers and the vote entropy (VE) (Engel-
son and Dagan, 1996) is employed as disagreement
metric. It is calculated on the token-level as
V Etok(t) = ?
1
log k
c
?
i=0
V (li, t)
k log
V (li, t)
k (1)
where V (li,t)k is the ratio of k classifiers where the
label li is assigned to a token t. The sentence level
vote entropy V Esent is then the average over all to-
kens tj of sentence s.
For the parsing task, the disagreement score is
based on a committee of k2 = 10 instances of Dan
Bikel?s reimplementation of Collins? parser (Bickel,
2005; Collins, 1999). For each sentence in the un-
labeled pool, the agreement between the committee
members was calculated using the function reported
by Reichart and Rappoport (2007):
AF (s) = 1N
?
i,l?[1...N ],i6=l
fscore(mi, ml) (2)
Where mi and ml are the committee members and
N = k2?(k2?1)2 is the number of pairs of different
committee members. This function calculates the
agreement between the members of each pair by cal-
culating their relative f-score and then averages the
pairs? scores. The disagreement of the committee on
a sentence is simply 1 ? AF (s).
4.2 Experimental settings
For the NE task we employed the classifier described
by Tomanek et al (2007): The NE tagger is based on
Conditional Random Fields (Lafferty et al, 2001)
5We randomly sampled L = 34 of the training data to create
each committee member.
and has a rich feature set including orthographical,
lexical, morphological, POS, and contextual fea-
tures. For parsing, Dan Bikel?s reimplementation of
Collins? parser is employed, using gold POS tags.
In each AL iteration we select 100 sentences for
manual annotation.6 We start with a randomly cho-
sen seed set of 200 sentences. Within a corpus we
used the same seed set in all selection scenarios. We
compare the following five selection scenarios: Ran-
dom selection (RS), which serves as our baseline;
one-sided AL selection for both tasks (called NE-AL
and PARSE-AL); and multi-task AL selection with
the alternating selection protocol (alter-MTAL) and
the rank combination protocol (ranks-MTAL).
We performed our experiments on three dif-
ferent corpora, namely one from the newspaper
genre (WSJ), a mixed-genre corpus (Brown), and a
biomedical corpus (Bio). Our simulation corpora
contain both entity annotations and (constituent)
parse annotations. For each corpus we have a pool
set (from which we select the examples for annota-
tion) and an evaluation set (used for generating the
learning curves). The WSJ corpus is based on the
WSJ part of the PENN TREEBANK (Marcus et al,
1993); we used the first 10,000 sentences of section
2-21 as the pool set, and section 00 as evaluation set
(1,921 sentences). The Brown corpus is also based
on the respective part of the PENN TREEBANK. We
created a sample consisting of 8 of any 10 consec-
utive sentences in the corpus. This was done as
Brown contains text from various English text gen-
res, and we did that to create a representative sample
of the corpus domains. We finally selected the first
10,000 sentences from this sample as pool set. Every
9th from every 10 consecutive sentences package
went into the evaluation set which consists of 2,424
sentences. For both WSJ and Brown only parse an-
notations though no entity annotations were avail-
able. Thus, we enriched both corpora with entity
annotations (three entities: person, location, and or-
ganization) by means of a tagger trained on the En-
glish data set of the CoNLL-2003 shared task (Tjong
Kim Sang and De Meulder, 2003).7 The Bio corpus
6Manual annotation is simulated by just unveiling the anno-
tations already contained in our corpora.
7We employed a tagger similar to the one presented by Set-
tles (2004). Our tagger has a performance of ? 84% f-score on
the CoNLL-2003 data; inspection of the predicted entities on
865
is based on the parsed section of the GENIA corpus
(Ohta et al, 2002). We performed the same divi-
sions as for Brown, resulting in 2,213 sentences in
our pool set and 276 sentences for the evaluation set.
This part of the GENIA corpus comes with entity an-
notations. We have collapsed the entity classes an-
notated in GENIA (cell line, cell type, DNA, RNA,
protein) into a single, biological entity class.
5 Results
In this section we present and discuss our results
when applying the five selection strategies (RS, NE-
AL, PARSE-AL, alter-MTAL, and ranks-MTAL) to
our scenario on the three corpora. We refrain from
calculating the overall efficiency score (Section 3)
here due to the lack of generally accepted weights
for the considered annotation tasks. However, we
require from a good selection protocol to exceed the
performance of random selection and extrinsic se-
lection. In addition, recall from Section 3 that we
set the alternate selection and rank combination pa-
rameters to si = 1, ?i = 1, respectively to reflect a
tradeoff between the annotation efforts of both tasks.
Figures 2 and 3 depict the learning curves for
the NE tagger and the parser on WSJ and Brown,
respectively. Each figure shows the five selection
strategies. As expected, on both corpora and both
tasks intrinsic selection performs best, i.e., for the
NE tagger NE-AL and for the parser PARSE-AL.
Further, random selection and extrinsic selection
perform worst. Most importantly, both MTAL pro-
tocols clearly outperform extrinsic and random se-
lection in all our experiments. This is in contrast
to NE-AL which performs worse than random se-
lection for all corpora when used as extrinsic selec-
tion, and for PARSE-AL that outperforms the ran-
dom baseline only for Brown when used as extrin-
sic selection. That is, the MTAL protocols suggest a
tradeoff between the annotation efforts of the differ-
ent tasks, here.
On WSJ, both for the NE and the parse annotation
tasks, the performance of the MTAL protocols is
very similar, though ranks-MTAL performs slightly
better. For the parser task, up to 30,000 constituents
MTAL performs almost as good as does PARSE-
AL. This is different for the NE task where NE-AL
WSJ and Brown revealed a good tagging performance.
clearly outperforms MTAL. On Brown, in general
we see the same results, with some minor differ-
ences. On the NE task, extrinsic selection (PARSE-
AL) performs better than random selection, but it is
still much worse than intrinsic AL or MTAL. Here,
ranks-MTAL significantly outperforms alter-MTAL
and almost performs as good as intrinsic selection.
For the parser task, we see that extrinsic and ran-
dom selection are equally bad. Both MTAL proto-
cols perform equally well, again being quite similar
to the intrinsic selection. On the BIO corpus8 we ob-
served the same tendencies as in the other two cor-
pora, i.e., MTAL clearly outperforms extrinsic and
random selection and supplies a better tradeoff be-
tween annotation efforts of the task at hand than one-
sided selection.
Overall, we can say that in all scenarios MTAL
performs much better than random selection and ex-
trinsic selection, and in most cases the performance
of MTAL (especially but not exclusively, ranks-
MTAL) is even close to intrinsic selection. This is
promising evidence that MTAL selection can be a
better choice than one-sided selection in multiple an-
notation scenarios. Thus, considering all annotation
tasks in the selection process (even if the selection
protocol is as simple as the alternating selection pro-
tocol) is better than selecting only with respect to
one task. Further, it should be noted that overall the
more sophisticated rank combination protocol does
not perform much better than the simpler alternating
selection protocol in all scenarios.
Finally, Figure 4 shows the disagreement curves
for the two tasks on the WSJ corpus. As has already
been discussed by Tomanek and Hahn (2008), dis-
agreement curves can be used as a stopping crite-
rion and to monitor the progress of AL-driven an-
notation. This is especially valuable when no anno-
tated validation set is available (which is needed for
plotting learning curves). We can see that the dis-
agreement curves significantly flatten approximately
at the same time as the learning curves do. In the
context of MTAL, disagreement curves might not
only be interesting as a stopping criterion but rather
as a switching criterion, i.e., to identify when MTAL
could be turned into one-sided selection. This would
be the case if in an MTAL scenario, the disagree-
8The plots for the Bio are omitted due to space restrictions.
866
10000 20000 30000 40000 50000
0.
65
0.
70
0.
75
0.
80
0.
85
tokens
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
5000 10000 15000 20000 25000 30000
0.
55
0.
60
0.
65
0.
70
0.
75
0.
80
tokens
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 2: Learning curves for NE task on WSJ (left) and Brown (right)
10000 20000 30000 40000
0.
76
0.
78
0.
80
0.
82
0.
84
constituents
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
5000 10000 15000 20000 25000 30000 35000
0.
65
0.
70
0.
75
0.
80
constituents
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 3: Learning curves for parse task on WSJ (left) and Brown (right)
ment curve of one task has a slope of (close to) zero.
Future work will focus on issues related to this.
6 Related Work
There is a large body of work on single-task AL ap-
proaches for many NLP tasks where the focus is
mainly on better, task-specific selection protocols
and methods to quantify the usefulness score in dif-
ferent scenarios. As to the tasks involved in our
scenario, several papers address AL for NER (Shen
et al, 2004; Hachey et al, 2005; Tomanek et al,
2007) and syntactic parsing (Tang et al, 2001; Hwa,
2004; Baldridge and Osborne, 2004; Becker and Os-
borne, 2005). Further, there is some work on ques-
tions arising when AL is to be used in real-life anno-
tation scenarios, including impaired inter-annotator
agreement, stopping criteria for AL-driven annota-
tion, and issues of reusability (Baldridge and Os-
borne, 2004; Hachey et al, 2005; Zhu and Hovy,
2007; Tomanek et al, 2007).
Multi-task AL is methodologically related to ap-
proaches of decision combination, especially in the
context of classifier combination (Ho et al, 1994)
and ensemble methods (Breiman, 1996). Those ap-
proaches focus on the combination of classifiers in
order to improve the classification error rate for one
specific classification task. In contrast, the focus of
multi-task AL is on strategies to select training ma-
terial for multi classifier systems where all classifiers
cover different classification tasks.
7 Discussion
Our treatment of MTAL within the context of the
orthogonal two-task scenario leads to further inter-
esting research questions. First, future investiga-
tions will have to focus on the question whether
the positive results observed in our orthogonal (i.e.,
highly dissimilar) two-task scenario will also hold
for a more realistic (and maybe more complex) mul-
tiple annotation scenario where tasks are more sim-
ilar and more than two annotation tasks might be
involved. Furthermore, several forms of interde-
pendencies may arise between the single annotation
tasks. As a first example, consider the (functional)
interdependencies (i.e., task similarity) in higher-
level semantic NLP tasks of relation or event recog-
nition. In such a scenario, several tasks including
entity annotations and relation/event annotations, as
well as syntactic parse data, have to be incorporated
at the same time. Another type of (data flow) inter-
867
10000 20000 30000 40000
0.
01
0
0.
01
4
0.
01
8
tokens
di
sa
gr
ee
m
en
t
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
10000 20000 30000 40000
5
10
15
20
25
30
35
40
constituents
di
sa
gr
ee
m
en
t
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 4: Disagreement curves for NE task (left) and parse task (right) on WSJ
dependency occurs in a second scenario where ma-
terial for several classifiers that are data-dependent
on each other ? one takes the output of another clas-
sifier as input features ? has to be efficiently anno-
tated. Whether the proposed protocols are beneficial
in the context of such highly interdependent tasks is
an open issue. Even more challenging is the idea
to provide methodologies helping to predict in an
arbitrary application scenario whether the choice of
MTAL is truly advantageous.
Another open question is how to measure and
quantify the overall annotation costs in multiple an-
notation scenarios. Exchange rates are inherently
tied to the specific task and domain. In practice, one
might just want to measure the time needed for the
annotations. However, in a simulation scenario, a
common metric is necessary to compare the perfor-
mance of different selection strategies with respect
to the overall annotation costs. This requires stud-
ies on how to quantify, with a comparable cost func-
tion, the efforts needed for the annotation of a textual
unit of choice (e.g., tokens, sentences) with respect
to different annotation tasks.
Finally, the question of reusability of the anno-
tated material is an important issue. Reusability in
the context of AL means to which degree corpora
assembled with the help of any AL technique can be
(re)used as a general resource, i.e., whether they are
well suited for the training of classifiers other than
the ones used during the selection process.This is
especially interesting as the details of the classifiers
that should be trained in a later stage are typically
not known at the resource building time. Thus, we
want to select samples valuable to a family of clas-
sifiers using the various annotation layers. This, of
course, is only possible if data annotated with the
help of AL is reusable by modified though similar
classifiers (e.g., with respect to the features being
used) ? compared to the classifiers employed for the
selection procedure.
The issue of reusability has already been raised
but not yet conclusively answered in the context of
single-task AL (see Section 6). Evidence was found
that reusability up to a certain, though not well-
specified, level is possible. Of course, reusability
has to be analyzed separately in the context of var-
ious MTAL scenarios. We feel that these scenarios
might both be more challenging and more relevant
to the reusability issue than the single-task AL sce-
nario, since resources annotated with multiple lay-
ers can be used to the design of a larger number of a
(possibly more complex) learning algorithms.
8 Conclusions
We proposed an extension to the single-task AL ap-
proach such that it can be used to select examples for
annotation with respect to several annotation tasks.
To the best of our knowledge this is the first paper on
this issue, with a focus on NLP tasks. We outlined
a problem definition and described a framework for
multi-task AL. We presented and tested two proto-
cols for multi-task AL. Our results are promising as
they give evidence that in a multiple annotation sce-
nario, multi-task AL outperforms naive one-sided
and random selection.
Acknowledgments
The work of the second author was funded by the
German Ministry of Education and Research within
the STEMNET project (01DS001A-C), while the
work of the third author was funded by the EC
within the BOOTSTREP project (FP6-028099).
868
References
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
EMNLP?04, pages 9?16.
Markus Becker and Miles Osborne. 2005. A two-stage
method for active learning of statistical grammars. In
Proceedings of IJCAI?05, pages 991?996.
Daniel M. Bickel. 2005. Code developed at the Univer-
sity of Pennsylvania, http://www.cis.upenn.
edu/
?
dbikel/software.html.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings of ACL?96, pages 319?326.
Yoav Freund, Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine Learning, 28(2-
3):133?168.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of CoNLL?05, pages
144?151.
Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier sys-
tems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 16(1):66?75.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of ICML?01, pages 282?289.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind K. Joshi. 2008. Sense annotation in the penn
discourse treebank. In Proceedings of CICLing?08,
pages 275?286.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of ACL?00,
pages 117?125.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA corpus: An annotated research abstract
corpus in molecular biology domain. In Proceedings
of HLT?02, pages 82?86.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Roi Reichart and Ari Rappoport. 2007. An ensemble
method for selection of high quality parses. In Pro-
ceedings of ACL?07, pages 408?415, June.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of JNLPBA?04, pages 107?110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In Proceedings
of ACL?04, pages 589?596.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Ac-
tive learning for statistical natural language parsing. In
Proceedings of ACL?02, pages 120?127.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CONLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL?03, pages 142?147.
Katrin Tomanek and Udo Hahn. 2008. Approximating
learning curves for active-learning-driven annotation.
In Proceedings of LREC?08.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains corpus reusabil-
ity of annotated data. In Proceedings of EMNLP-
CoNLL?07, pages 486?495.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of
EMNLP-CoNLL?07, pages 783?790.
869
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1039?1047,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-Supervised Active Learning for Sequence Labeling
Katrin Tomanek and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
While Active Learning (AL) has already
been shown to markedly reduce the anno-
tation efforts for many sequence labeling
tasks compared to random selection, AL
remains unconcerned about the internal
structure of the selected sequences (typ-
ically, sentences). We propose a semi-
supervised AL approach for sequence la-
beling where only highly uncertain sub-
sequences are presented to human anno-
tators, while all others in the selected se-
quences are automatically labeled. For the
task of entity recognition, our experiments
reveal that this approach reduces annota-
tion efforts in terms of manually labeled
tokens by up to 60 % compared to the stan-
dard, fully supervised AL scheme.
1 Introduction
Supervised machine learning (ML) approaches are
currently the methodological backbone for lots of
NLP activities. Despite their success they create a
costly follow-up problem, viz. the need for human
annotators to supply large amounts of ?golden?
annotation data on which ML systems can be
trained. In most annotation campaigns, the lan-
guage material chosen for manual annotation is se-
lected randomly from some reference corpus.
Active Learning (AL) has recently shaped as a
much more efficient alternative for the creation of
precious training material. In the AL paradigm,
only examples of high training utility are selected
for manual annotation in an iterative manner. Dif-
ferent approaches to AL have been successfully
applied to a wide range of NLP tasks (Engel-
son and Dagan, 1996; Ngai and Yarowsky, 2000;
Tomanek et al, 2007; Settles and Craven, 2008).
When used for sequence labeling tasks such as
POS tagging, chunking, or named entity recogni-
tion (NER), the examples selected by AL are se-
quences of text, typically sentences. Approaches
to AL for sequence labeling are usually uncon-
cerned about the internal structure of the selected
sequences. Although a high overall training util-
ity might be attributed to a sequence as a whole,
the subsequences it is composed of tend to ex-
hibit different degrees of training utility. In the
NER scenario, e.g., large portions of the text do
not contain any target entity mention at all. To
further exploit this observation for annotation pur-
poses, we here propose an approach to AL where
human annotators are required to label only uncer-
tain subsequences within the selected sentences,
while the remaining subsequences are labeled au-
tomatically based on the model available from the
previous AL iteration round. The hardness of sub-
sequences is characterized by the classifier?s con-
fidence in the predicted labels. Accordingly, our
approach is a combination of AL and self-training
to which we will refer as semi-supervised Active
Learning (SeSAL) for sequence labeling.
While self-training and other bootstrapping ap-
proaches often fail to produce good results on NLP
tasks due to an inherent tendency of deteriorated
data quality, SeSAL circumvents this problem and
still yields large savings in terms annotation de-
cisions, i.e., tokens to be manually labeled, com-
pared to a standard, fully supervised AL approach.
After a brief overview of the formal underpin-
nings of Conditional Random Fields, our base
classifier for sequence labeling tasks (Section 2),
a fully supervised approach to AL for sequence
labeling is introduced and complemented by our
semi-supervised approach in Section 3. In Section
4, we discuss SeSAL in relation to bootstrapping
and existing AL techniques. Our experiments are
laid out in Section 5 where we compare fully and
semi-supervised AL for NER on two corpora, the
newspaper selection of MUC7 and PENNBIOIE, a
biological abstracts corpus.
1039
2 Conditional Random Fields for
Sequence Labeling
Many NLP tasks, such as POS tagging, chunking,
or NER, are sequence labeling problems where a
sequence of class labels ~y = (y1, . . . ,yn) ? Yn
are assigned to a sequence of input units
~x = (x1, . . . ,xn) ? X n. Input units xj are usually
tokens, class labels yj can be POS tags or entity
classes.
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) are a probabilistic framework for label-
ing structured data and model P~?(~y|~x). We focus
on first-order linear-chain CRFs, a special form of
CRFs for sequential data, where
P~?(~y|~x) =
1
Z~?(~x)
? exp
(
n
?
j=1
m
?
i=1
?ifi(yj?1,yj ,~x, j)
)
(1)
with normalization factor Z~?(~x), feature functions
fi(?), and feature weights ?i.
Parameter Estimation. The model parameters
?i are set to maximize the penalized log-likelihood
L on some training data T :
L(T ) =
?
(~x,~y)?T
log p(~y|~x) ?
m
?
i=1
?2i
2?2 (2)
The partial derivations of L(T ) are
?L(T )
??i
= E?(fi) ? E(fi) ?
?i
?2 (3)
where E?(fi) is the empirical expectation of fea-
ture fi and can be calculated by counting the oc-
currences of fi in T . E(fi) is the model expecta-
tion of fi and can be written as
E(fi) =
?
(~x,~y)?T
?
~y ??Yn
P~?(~y
?|~x)?
n
?
j=1
fi(y?j?1, y?j , ~x,j) (4)
Direct computation of E(fi) is intractable due to
the sum over all possible label sequences ~y ? ? Yn.
The Forward-Backward algorithm (Rabiner, 1989)
solves this problem efficiently. Forward (?) and
backward (?) scores are defined by
?j(y|~x) =
?
y??T?1j (y)
?j?1(y?|~x) ? ?j(~x, y?, y)
?j(y|~x) =
?
y??Tj(y)
?j+1(y?|~x) ? ?j(~x, y, y?)
where ?j(~x,a,b) = exp
(
?m
i=1 ?ifi(a,b,~x, j)
)
,
Tj(y) is the set of all successors of a state y at a
specified position j, and, accordingly, T?1j (y) is
the set of predecessors.
Normalized forward and backward scores
are inserted into Equation (4) to replace
?
~y ??Yn P~?(~y
?|~x) so that L(T ) can be opti-
mized with gradient-based or iterative-scaling
methods.
Inference and Probabilities. The marginal
probability
P~?(yj = y
?|~x) = ?j(y
?|~x) ? ?j(y?|~x)
Z~?(~x)
(5)
specifies the model?s confidence in label y? at po-
sition j of an input sequence ~x. The forward
and backward scores are obtained by applying the
Forward-Backward algorithm on ~x. The normal-
ization factor is efficiently calculated by summing
over all forward scores:
Z~?(~x) =
?
y?Y
?n(y|~x) (6)
The most likely label sequence
~y ? = argmax
~y?Yn
exp
(
n
?
j=1
m
?
i=1
?ifi(yj?1,yj ,~x, j)
)
(7)
is computed using the Viterbi algorithm (Rabiner,
1989). See Equation (1) for the conditional prob-
ability P~?(~y
?|~x) with Z~? calculated as in Equa-
tion (6). The marginal and conditional probabili-
ties are used by our AL approaches as confidence
estimators.
3 Active Learning for Sequence Labeling
AL is a selective sampling technique where the
learning protocol is in control of the data to be
used for training. The intention with AL is to re-
duce the amount of labeled training material by
querying labels only for examples which are as-
sumed to have a high training utility. This section,
first, describes a common approach to AL for se-
quential data, and then presents our approach to
semi-supervised AL.
3.1 Fully Supervised Active Learning
Algorithm 1 describes the general AL framework.
A utility function UM(pi) is the core of each AL
approach ? it estimates how useful it would be for
1040
Algorithm 1 General AL framework
Given:
B: number of examples to be selected
L: set of labeled examples
P : set of unlabeled examples
UM: utility function
Algorithm:
loop until stopping criterion is met
1. learn modelM from L
2. for all pi ? P : upi ? UM(pi)
3. select B examples pi ? P with highest utility upi
4. query human annotator for labels of all B examples
5. move newly labeled examples from P to L
return L
a specific base learner to have an unlabeled exam-
ple labeled and, subsequently included in the train-
ing set.
In the sequence labeling scenario, such an ex-
ample is a stream of linguistic items ? a sentence
is usually considered as proper sequence unit. We
apply CRFs as our base learner throughout this pa-
per and employ a utility function which is based
on the conditional probability of the most likely
label sequence ~y ? for an observation sequence ~x
(cf. Equations (1) and (7)):
U~?(~x) = 1 ? P~?(~y
?|~x) (8)
Sequences for which the current model is least
confident on the most likely label sequence are
preferably selected.1 These selected sentences are
fully manually labeled. We refer to this AL mode
as fully supervised Active Learning (FuSAL).
3.2 Semi-Supervised Active Learning
In the sequence labeling scenario, an example
which, as a whole, has a high utility U~?(~x), can
still exhibit subsequences which do not add much
to the overall utility and thus are fairly easy for the
current model to label correctly. One might there-
fore doubt whether it is reasonable to manually la-
bel the entire sequence. Within many sequences
of natural language data, there are probably large
subsequences on which the current model already
does quite well and thus could automatically gen-
erate annotations with high quality. This might, in
particular, apply to NER where larger stretches of
sentences do not contain any entity mention at all,
or merely trivial instances of an entity class easily
predictable by the current model.
1There are many more sophisticated utility functions for
sequence labeling. We have chosen this straightforward one
for simplicity and because it has proven to be very effective
(Settles and Craven, 2008).
For the sequence labeling scenario, we accord-
ingly modify the fully supervised AL approach
from Section 3.1. Only those tokens remain to be
manually labeled on which the current model is
highly uncertain regarding their class labels, while
all other tokens (those on which the model is suf-
ficiently certain how to label them correctly) are
automatically tagged.
To select the sequence examples the same util-
ity function as for FuSAL (cf. Equation (8)) is ap-
plied. To identify tokens xj from the selected se-
quences which still have to be manually labeled,
the model?s confidence in label y?j is estimated by
the marginal probability (cf. Equation (5))
C~?(y
?
j ) = P~?(yj = y
?
j |~x) (9)
where y?j specifies the label at the respective po-
sition of the most likely label sequence ~y ? (cf.
Equation (7)). If C~?(y?j ) exceeds a certain con-fidence threshold t, y?j is assumed to be the correct
label for this token and assigned to it.2 Otherwise,
manual annotation of this token is required. So,
compared to FuSAL as described in Algorithm 1
only the third step step is modified.
We call this semi-supervised Active Learning
(SeSAL) for sequence labeling. SeSAL joins the
standard, fully supervised AL schema with a boot-
strapping mode, namely self-training, to combine
the strengths of both approaches. Examples with
high training utility are selected using AL, while
self-tagging of certain ?safe? regions within such
examples additionally reduces annotation effort.
Through this combination, SeSAL largely evades
the problem of deteriorated data quality, a limiting
factor of ?pure? bootstrapping approaches.
This approach requires two parameters to be set:
Firstly, the confidence threshold t which directly
influences the portion of tokens to be manually
labeled. Using lower thresholds, the self-tagging
component of SeSAL has higher impact ? presum-
ably leading to larger amounts of tagging errors.
Secondly, a delay factor d can be specified which
channels the amount of manually labeled tokens
obtained with FuSAL before SeSAL is to start.
Only with d = 0, SeSAL will already affect the
first AL iteration. Otherwise, several iterations of
FuSAL are run until a switch to SeSAL will hap-
pen.
2Sequences of consecutive tokens xj for which C~?(y?j ) ?
t are presented to the human annotator instead of single, iso-
lated tokens.
1041
It is well known that the performance of boot-
strapping approaches crucially depends on the size
of the seed set ? the amount of labeled examples
available to train the initial model. If class bound-
aries are poorly defined by choosing the seed set
too small, a bootstrapping system cannot learn
anything reasonable due to high error rates. If, on
the other hand, class boundaries are already too
well defined due to an overly large seed set, noth-
ing to be learned is left. Thus, together with low
thresholds, a delay rate of d > 0 might be crucial
to obtain models of high performance.
4 Related Work
Common approaches to AL are variants of the
Query-By-Committee approach (Seung et al,
1992) or based on uncertainty sampling (Lewis
and Catlett, 1994). Query-by-Committee uses a
committee of classifiers, and examples on which
the classifiers disagree most regarding their pre-
dictions are considered highly informative and
thus selected for annotation. Uncertainty sam-
pling selects examples on which a single classi-
fier is least confident. AL has been successfully
applied to many NLP tasks; Settles and Craven
(2008) compare the effectiveness of several AL
approaches for sequence labeling tasks of NLP.
Self-training (Yarowsky, 1995) is a form of
semi-supervised learning. From a seed set of la-
beled examples a weak model is learned which
subsequently gets incrementally refined. In each
step, unlabeled examples on which the current
model is very confident are labeled with their pre-
dictions, added to the training set, and a new
model is learned. Similar to self-training, co-
training (Blum and Mitchell, 1998) augments the
training set by automatically labeled examples.
It is a multi-learner algorithm where the learners
have independent views on the data and mutually
produce labeled examples for each other.
Bootstrapping approaches often fail when ap-
plied to NLP tasks where large amounts of training
material are required to achieve acceptable perfor-
mance levels. Pierce and Cardie (2001) showed
that the quality of the automatically labeled train-
ing data is crucial for co-training to perform well
because too many tagging errors prevent a high-
performing model from being learned. Also, the
size of the seed set is an important parameter.
When it is chosen too small data quality gets dete-
riorated quickly, when it is chosen too large no im-
provement over the initial model can be expected.
To address the problem of data pollution by tag-
ging errors, Pierce and Cardie (2001) propose cor-
rected co-training. In this mode, a human is put
into the co-training loop to review and, if neces-
sary, to correct the machine-labeled examples. Al-
though this effectively evades the negative side ef-
fects of deteriorated data quality, one may find the
correction of labeled data to be as time-consuming
as annotations from the scratch. Ideally, a human
should not get biased by the proposed label but
independently examine the example ? so that cor-
rection eventually becomes annotation.
In contrast, our SeSAL approach which also ap-
plies bootstrapping, aims at avoiding to deteriorate
data quality by explicitly pointing human annota-
tors to classification-critical regions. While those
regions require full annotation, regions of high
confidence are automatically labeled and thus do
not require any manual inspection. Self-training
and co-training, in contradistinction, select exam-
ples of high confidence only. Thus, these boot-
strapping methods will presumably not find the
most useful unlabeled examples but require a hu-
man to review data points of limited training util-
ity (Pierce and Cardie, 2001). This shortcoming is
also avoided by our SeSAL approach, as we inten-
tionally select informative examples only.
A combination of active and semi-supervised
learning has first been proposed by McCallum and
Nigam (1998) for text classification. Committee-
based AL is used for the example selection. The
committee members are first trained on the labeled
examples and then augmented by means of Expec-
tation Maximization (EM) (Dempster et al, 1977)
including the unlabeled examples. The idea is
to avoid manual labeling of examples whose la-
bels can be reliably assigned by EM. Similarly,
co-testing (Muslea et al, 2002), a multi-view AL
algorithms, selects examples for the multi-view,
semi-supervised Co-EM algorithm. In both works,
semi-supervision is based on variants of the EM
algorithm in combination with all unlabeled ex-
amples from the pool. Our approach to semi-
supervised AL is different as, firstly, we aug-
ment the training data using a self-tagging mech-
anism (McCallum and Nigam (1998) and Muslea
et al (2002) performed semi-supervision to aug-
ment the models using EM), and secondly, we op-
erate in the sequence labeling scenario where an
example is made up of several units each requiring
1042
a label ? partial labeling of sequence examples is
a central characteristic of our approach. Another
work also closely related to ours is that of Krist-
jansson et al (2004). In an information extraction
setting, the confidence per extracted field is cal-
culated by a constrained variant of the Forward-
Backward algorithm. Unreliable fields are high-
lighted so that the automatically annotated corpus
can be corrected. In contrast, AL selection of ex-
amples together with partial manual labeling of the
selected examples are the main foci of our work.
5 Experiments and Results
In this section, we turn to the empirical assessment
of semi-supervised AL (SeSAL) for sequence la-
beling on the NLP task of named entity recogni-
tion. By the nature of this task, the sequences ?
in this case, sentences ? are only sparsely popu-
lated with entity mentions and most of the tokens
belong to the OUTSIDE class3 so that SeSAL can
be expected to be very beneficial.
5.1 Experimental Settings
In all experiments, we employ the linear-chain
CRF model described in Section 2 as the base
learner. A set of common feature functions was
employed, including orthographical (regular ex-
pression patterns), lexical and morphological (suf-
fixes/prefixes, lemmatized tokens), and contextual
(features of neighboring tokens) ones.
All experiments start from a seed set of 20 ran-
domly selected examples and, in each iteration,
50 new examples are selected using AL. The ef-
ficiency of the different selection mechanisms is
determined by learning curves which relate the an-
notation costs to the performance achieved by the
respective model in terms of F1-score. The unit of
annotation costs are manually labeled tokens. Al-
though the assumption of uniform costs per token
has already been subject of legitimate criticism
(Settles et al, 2008), we believe that the number
of annotated tokens is still a reasonable approxi-
mation in the absence of an empirically more ade-
quate task-specific annotation cost model.
We ran the experiments on two entity-annotated
corpora. From the general-language newspaper
domain, we took the training part of the MUC7
corpus (Linguistic Data Consortium, 2001) which
incorporates seven different entity types, viz. per-
3The OUTSIDE class is assigned to each token that does
not denote an entity in the underlying domain of discourse.
corpus entity classes sentences tokens
MUC7 7 3,020 78,305
PENNBIOIE 3 10,570 267,320
Table 1: Quantitative characteristics of the chosen corpora
sons, organizations, locations, times, dates, mone-
tary expressions, and percentages. From the sub-
language biology domain, we used the oncology
part of the PENNBIOIE corpus (Kulick et al,
2004) and removed all but three gene entity sub-
types (generic, protein, and rna). Table 1 summa-
rizes the quantitative characteristics of both cor-
pora.4 The results reported below are averages of
20 independent runs. For each run, we randomly
split each corpus into a pool of unlabeled examples
to select from (90 % of the corpus), and a comple-
mentary evaluation set (10 % of the corpus).
5.2 Empirical Evaluation
We compare semi-supervised AL (SeSAL) with
its fully supervised counterpart (FuSAL), using
a passive learning scheme where examples are
randomly selected (RAND) as baseline. SeSAL
is first applied in a default configuration with a
very high confidence threshold (t = 0.99) with-
out any delay (d = 0). In further experiments,
these parameters are varied to study their impact
on SeSAL?s performance. All experiments were
run on both the newspaper (MUC7) and biological
(PENNBIOIE) corpus. When results are similar to
each other, only one data set will be discussed.
Distribution of Confidence Scores. The lead-
ing assumption for SeSAL is that only a small por-
tion of tokens within the selected sentences consti-
tute really hard decision problems, while the ma-
jority of tokens are easy to account for by the cur-
rent model. To test this stipulation we investigate
the distribution of the model?s confidence values
C~?(y
?
j ) over all tokens of the sentences (cf. Equa-
tion (9)) selected within one iteration of FuSAL.
Figure 1, as an example, depicts the histogram
for an early AL iteration round on the MUC7 cor-
pus. The vast majority of tokens has a confidence
score close to 1, the median lies at 0.9966. His-
tograms of subsequent AL iterations are very sim-
ilar with an even higher median. This is so because
4We removed sentences of considerable over and under
length (beyond +/- 3 standard deviations around the average
sentence length) so that the numbers in Table 1 differ from
those cited in the original sources.
1043
confidence score
fre
qu
en
cy
0.2 0.4 0.6 0.8 1.0
0
50
0
10
00
15
00
Figure 1: Distribution of token-level confidence scores in the
5th iteration of FuSAL on MUC7 (number of tokens: 1,843)
the model gets continuously more confident when
trained on additional data and fewer hard cases re-
main in the shrinking pool.
Fully Supervised vs. Semi-Supervised AL.
Figure 2 compares the performance of FuSAL and
SeSAL on the two corpora. SeSAL is run with
a delay rate of d = 0 and a very high confi-
dence threshold of t = 0.99 so that only those
tokens are automatically labeled on which the cur-
rent model is almost certain. Figure 2 clearly
shows that SeSAL is much more efficient than
its fully supervised counterpart. Table 2 depicts
the exact numbers of manually labeled tokens to
reach the maximal (supervised) F-score on both
corpora. FuSAL saves about 50 % compared to
RAND, while SeSAL saves about 60 % compared
to FuSAL which constitutes an overall saving of
over 80 % compared to RAND.
These savings are calculated relative to the
number of tokens which have to be manually la-
beled. Yet, consider the following gedanken ex-
periment. Assume that, using SeSAL, every sec-
ond token in a sequence would have to be labeled.
Though this comes to a ?formal? saving of 50 %,
the actual annotation effort in terms of the time
needed would hardly go down. It appears that
only when SeSAL splits a sentence into larger
Corpus Fmax RAND FuSAL SeSAL
MUC7 87.7 63,020 36,015 11,001
PENNBIOIE 82.3 194,019 83,017 27,201
Table 2: Tokens manually labeled to reach the maximal (su-
pervised) F-score
0 10000 30000 50000
0.
60
0.
70
0.
80
0.
90
MUC7
manually labeled tokens
F?
sc
or
e
SeSAL
FuSAL
RAND
0 10000 30000 50000
0.
60
0.
70
0.
80
0.
90
PennBioIE
manually labeled tokens
F?
sc
or
e
SeSAL
FuSAL
RAND
Figure 2: Learning curves for Semi-supervised AL (SeSAL),
Fully Supervised AL (FuSAL), and RAND(om) selection
well-packaged, chunk-like subsequences annota-
tion time can really be saved. To demonstrate that
SeSAL comes close to this, we counted the num-
ber of base noun phrases (NPs) containing one or
more tokens to be manually labeled. On the MUC7
corpus, FuSAL requires 7,374 annotated NPs to
yield an F-score of 87%, while SeSAL hit the
same F-score with only 4,017 NPs. Thus, also in
terms of the number of NPs, SeSAL saves about
45% of the material to be considered.5
Detailed Analysis of SeSAL. As Figure 2 re-
veals, the learning curves of SeSAL stop early (on
MUC7 after 12,800 tokens, on PENNBIOIE after
27,600 tokens) because at that point the whole cor-
pus has been labeled exhaustively ? either manu-
ally, or automatically. So, using SeSAL the com-
plete corpus can be labeled with only a small
fraction of it actually being manually annotated
(MUC7: about 18%, PENNBIOIE: about 13%).
5On PENNBIOIE, SeSAL also saves about 45% com-
pared to FuSAL to achieve an F-score of 81%.
1044
Table 3 provides additional analysis results on
MUC7. In very early AL rounds, a large ratio of
tokens has to be manually labeled (70-80 %). This
number decreases increasingly as the classifier im-
proves (and the pool contains fewer informative
sentences). The number of tagging errors is quite
low, resulting in a high accuracy of the created cor-
pus of constantly over 99 %.
labeled tokens
manual automatic ? AR (%) errors ACC
1,000 253 1,253 79.82 6 99.51
5,000 6,207 11,207 44.61 82 99.27
10,000 25,506 34,406 28.16 174 99.51
12,800 57,371 70,171 18.24 259 99.63
Table 3: Analysis of SeSAL on MUC7: Manually and auto-
matically labeled tokens, annotation rate (AR) as the portion
of manually labeled tokens in the total amount of labeled to-
kens, errors and accuracy (ACC) of the created corpus.
The majority of the automatically labeled to-
kens (97-98 %) belong to the OUTSIDE class.
This coincides with the assumption that SeSAL
works especially well for labeling tasks where
some classes occur predominantly and can, in
most cases, easily be discriminated from the other
classes, as is the case in the NER scenario. An
analysis of the errors induced by the self-tagging
component reveals that most of the errors (90-
100 %) are due to missed entity classes, i.e., while
the correct class label for a token is one of the
entity classes, the OUTSIDE class was assigned.
This effect is more severe in early than in later AL
iterations (see Table 4 for the exact numbers).
labeled error types (%)
corpus tokens errors E2O O2E E2E
MUC7 10,000 75 100 ? ?
70,000 259 96 1.3 2.7
Table 4: Distribution of errors of the self-tagging component.
Error types: OUTSIDE class assigned though an entity class
is correct (E2O), entity class assigned but OUTSIDE is cor-
rect (O2E), wrong entity class assigned (E2E).
Impact of the Confidence Threshold. We also
ran SeSAL with different confidence thresholds t
(0.99, 0.95, 0.90, and 0.70) and analyzed the re-
sults with respect to tagging errors and the model
performance. Figure 3 shows the learning and er-
ror curves for different thresholds on the MUC7
corpus. The supervised F-score of 87.7% is only
reached by the highest and most restrictive thresh-
old of t = 0.99. With all other thresholds, SeSAL
0 2000 6000 10000
0.
60
0.
70
0.
80
0.
90
learning curves
manually labeled tokens
F?
sc
or
e
t=0.99
t=0.95
t=0.90
t=0.70
0 20000 40000 60000
0
50
0
10
00
20
00
error curves
all labeled tokens
e
rr
o
rs
t=0.99
t=0.95
t=0.90
t=0.70
Figure 3: Learning and error curves for SeSAL with different
thresholds on the MUC7 corpus
stops at much lower F-scores and produces labeled
training data of lower accuracy. Table 5 contains
the exact numbers and reveals that the poor model
performance of SeSAL with lower thresholds is
mainly due to dropping recall values.
threshold F R P Acc
0.99 87.7 85.9 89.9 99.6
0.95 85.4 82.3 88.7 98.8
0.90 84.3 80.6 88.3 98.1
0.70 69.9 61.8 81.1 96.5
Table 5: Maximum model performance on MUC7 in terms of
F-score (F), recall (R), precision (P) and accuracy (Acc) ? the
labeled corpus obtained by SeSAL with different thresholds
Impact of the Delay Rate. We also measured
the impact of delay rates on SeSAL?s efficiency
considering three delay rates (1,000, 5,000, and
10,000 tokens) in combination with three confi-
dence thresholds (0.99, 0.9, and 0.7). Figure 4 de-
picts the respective learning curves on the MUC7
corpus. For SeSAL with t = 0.99, the delay
1045
0 5000 10000 15000 20000
0.
60
0.
70
0.
80
0.
90
threshold 0.99
manually labeled tokens
F?
sc
or
e
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=0.877
0 5000 10000 15000 20000
0.
60
0.
70
0.
80
0.
90
threshold 0.9
manually labeled tokens
F?
sc
or
e
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=0.843
F=0.877
0 2000 6000 10000
0.
60
0.
70
0.
80
0.
90
threshold 0.7
manually labeled tokens
F?
sc
or
e
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=69.9
F=0.877
Figure 4: SeSAL with different delay rates and thresholds on MUC7. Horizontal lines mark the supervised F-score (upper line)
and the maximal F-score achieved by SeSAL with the respective threshold and d = 0 (lower line).
has no particularly beneficial effect. However,
in combination with lower thresholds, the delay
rates show positive effects as SeSAL yields F-
scores closer to the maximal F-score of 87.7%,
thus clearly outperforming undelayed SeSAL.
6 Summary and Discussion
Our experiments in the context of the NER
scenario render evidence to the hypothesis that
the proposed approach to semi-supervised AL
(SeSAL) for sequence labeling indeed strongly re-
duces the amount of tokens to be manually anno-
tated ? in terms of numbers, about 60% compared
to its fully supervised counterpart (FuSAL), and
over 80% compared to a totally passive learning
scheme based on random selection.
For SeSAL to work well, a high and, by this,
restrictive threshold has been shown to be crucial.
Otherwise, large amounts of tagging errors lead to
a poorer overall model performance. In our ex-
periments, tagging errors in such a scenario were
OUTSIDE labelings, while an entity class would
have been correct ? with the effect that the result-
ing models showed low recall rates.
The delay rate is important when SeSAL is run
with a low threshold as early tagging errors can
be avoided which otherwise reinforce themselves.
Finding the right balance between the delay factor
and low thresholds requires experimental calibra-
tion. For the most restrictive threshold (t = 0.99)
though such a delay is unimportant so that it can
be set to d = 0 circumventing this calibration step.
In summary, the self-tagging component of
SeSAL gets more influential when the confidence
threshold and the delay factor are set to lower val-
ues. At the same time though, under these con-
ditions negative side-effects such as deteriorated
data quality and, by this, inferior models emerge.
These problems are major drawbacks of many
bootstrapping approaches. However, our experi-
ments indicate that as long as self-training is cau-
tiously applied (as is done for SeSAL with restric-
tive parameters), it can definitely outperform an
entirely supervised approach.
From an annotation point of view, SeSAL effi-
ciently guides the annotator to regions within the
selected sentence which are very useful for the
learning task. In our experiments on the NER sce-
nario, those regions were mentions of entity names
or linguistic units which had a surface appearance
similar to entity mentions but could not yet be cor-
rectly distinguished by the model.
While we evaluated SeSAL here in terms of
tokens to be manually labeled, an open issue re-
mains, namely how much of the real annotation
effort ? measured by the time needed ? is saved
by this approach. We here hypothesize that hu-
man annotators work much more efficiently when
pointed to the regions of immediate interest in-
stead of making them skim in a self-paced way
through larger passages of (probably) semantically
irrelevant but syntactically complex utterances ?
a tiring and error-prone task. Future research is
needed to empirically investigate into this area and
quantify the savings in terms of the time achiev-
able with SeSAL in the NER scenario.
Acknowledgements
This work was funded by the EC within the
BOOTStrep (FP6-028099) and CALBC (FP7-
231727) projects. We want to thank Roman Klin-
ger (Fraunhofer SCAI) for fruitful discussions.
1046
References
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT?98 ?
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory, pages 92?100.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
S. Engelson and I. Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In ACL?96 ? Proceedings of the 34th Annual
Meeting of the Association for Computational Lin-
guistics, pages 319?326.
T. Kristjansson, A. Culotta, and P. Viola. 2004. Inter-
active information extraction with constrained Con-
ditional Random Fields. In AAAI?04 ? Proceed-
ings of 19th National Conference on Artificial Intel-
ligence, pages 412?418.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. T. Mc-
Donald, M. S. Palmer, and A. I. Schein. 2004. Inte-
grated annotation for biomedical information extrac-
tion. In Proceedings of the HLT-NAACL 2004 Work-
shop ?Linking Biological Literature, Ontologies and
Databases: Tools for Users?, pages 61?68.
J. D. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional Random Fields: Probabilistic models
for segmenting and labeling sequence data. In
ICML?01 ? Proceedings of the 18th International
Conference on Machine Learning, pages 282?289.
D. D. Lewis and J. Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In
ICML?94 ? Proceedings of the 11th International
Conference on Machine Learning, pages 148?156.
Linguistic Data Consortium. 2001. Message Under-
standing Conference (MUC) 7. LDC2001T02. FTP
FILE. Philadelphia: Linguistic Data Consortium.
A. McCallum and K. Nigam. 1998. Employing EM
and pool-based Active Learning for text classifica-
tion. In ICML?98 ? Proceedings of the 15th Interna-
tional Conference on Machine Learning, pages 350?
358.
I. A. Muslea, S. Minton, and C. A. Knoblock. 2002.
Active semi-supervised learning = Robust multi-
view learning. In ICML?02 ? Proceedings of the
19th International Conference on Machine Learn-
ing, pages 435?442.
G. Ngai and D. Yarowsky. 2000. Rule writing or anno-
tation: Cost-efficient resource usage for base noun
phrase chunking. In ACL?00 ? Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 117?125.
D. Pierce and C. Cardie. 2001. Limitations of co-
training for natural language learning from large
datasets. In EMNLP?01 ? Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?9.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
B. Settles and M. Craven. 2008. An analysis of Active
Learning strategies for sequence labeling tasks. In
EMNLP?08 ? Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1069?1078.
B. Settles, M. Craven, and L. Friedland. 2008. Active
Learning with real annotation costs. In Proceedings
of the NIPS 2008 Workshop on ?Cost-Sensitive Ma-
chine Learning?, pages 1?10.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT?92 ? Proceedings of
the 5th Annual Workshop on Computational Learn-
ing Theory, pages 287?294.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts anno-
tation costs and maintains corpus reusability of an-
notated data. In EMNLP-CoNLL?07 ? Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Language Learning, pages 486?495.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL?95 ?
Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 189?
196.
1047
Proceedings of the Linguistic Annotation Workshop, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
Efficient Annotation with the Jena ANnotation Environment (JANE)
Katrin Tomanek Joachim Wermter Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30
D-07743 Jena, Germany
{tomanek|wermter|hahn}@coling-uni-jena.de
Abstract
With ever-increasing demands on the diver-
sity of annotations of language data, the
need arises to reduce the amount of efforts
involved in generating such value-added lan-
guage resources. We introduce here the Jena
ANnotation Environment (JANE), a platform
that supports the complete annotation life-
cycle and allows for ?focused? annotation
based on active learning. The focus we pro-
vide yields significant savings in annotation
efforts by presenting only informative items
to the annotator. We report on our experi-
ence with this approach through simulated
and real-world annotations in the domain of
immunogenetics for NE annotations.
1 Introduction
The remarkable success of machine-learning meth-
ods for NLP has created, for supervised approaches
at least, a profound need for annotated language cor-
pora. Annotation of language resources, however,
has become a bottleneck since it is performed, with
some automatic support (pre-annotation) though, by
humans. Hence, annotation is a time-costly and
error-prone process.
The demands for annotated language data is in-
creasing at different levels. After the success in syn-
tactic (Penn TreeBank (Marcus et al, 1993)) and
propositional encodings (Penn PropBank (Palmer et
al., 2005)), more sophisticated semantic data (such
as temporal (Pustejovsky et al, 2003) or opinion an-
notations (Wiebe et al, 2005)) and discourse data
(e.g., for anaphora resolution (van Deemter and Kib-
ble, 2000) and rhetorical parsing (Carlson et al,
2003)) are being generated. Once the ubiquitous
area of newswire articles is left behind, different do-
mains (e.g., the life sciences (Ohta et al, 2002)) are
yet another major concern. Furthermore, any new
HLT application (e.g., information extraction, doc-
ument summarization) makes it necessary to pro-
vide appropriate human annotation products. Be-
sides these considerations, the whole field of non-
English languages is desperately seeking to enter
into enormous annotation efforts, at virtually all en-
coding levels, to keep track of methodological re-
quirements imposed by such resource-intensive re-
search activities.
Given this enormous need for high-quality anno-
tations at virtually all levels the question turns up
how to minimize efforts within an acceptable qual-
ity window. Currently, for most tasks several hun-
dreds of thousands of text tokens (ranging between
200,000 to 500,000 text tokens) have to be scruti-
nized unless valid tagging judgments can be learned.
While significant time savings have already been re-
ported on the basis of automatic pre-tagging (e.g.,
for POS and parse tree taggings in the Penn Tree-
Bank (Marcus et al, 1993), or named entity taggings
for the Genia corpus (Ohta et al, 2002)), this kind of
pre-processing does not reduce the number of text
tokens actually to be considered.
We have developed the Jena ANnotation Environ-
ment (JANE) that allows to reduce annotation ef-
forts by means of the active learning (AL) approach.
Unlike random or sequential sampling of linguistic
items to be annotated, AL is an intelligent selective
9
sampling strategy that helps reduce the amount of
data to be annotated substantially at almost no loss
in annotation effectiveness. This is achieved by fo-
cusing on those items particularly relevant for the
learning process.
In Section 2, we review approaches to annota-
tion cost reduction. We turn in Section 3 to the de-
scription of JANE, our AL-based annotation system,
while in Section 4 we report on the experience we
made using the AL component in NE annotations.
2 Related Work
Reduction of efforts for training (semi-) supervised
learners on annotated language data has always been
an issue of concern. Semi-supervised learning pro-
vides methods to bootstrap annotated corpora from a
small number of manually labeled examples. How-
ever, it has been shown (Pierce and Cardie, 2001)
that semi-supervised learning is brittle for NLP tasks
where typically large amounts of high quality anno-
tations are needed to train appropriate classifiers.
Another approach to reducing the human labeling
effort is active learning (AL) where the learner has
direct influence on the examples to be manually la-
beled. In such a setting, those examples are taken
for annotation which are assumed to be maximally
useful for (classifier) training. AL approaches have
already been tried for different NLP tasks (Engelson
and Dagan, 1996; Hwa, 2000; Ngai and Yarowsky,
2000), though such studies usually report on simula-
tions rather than on concrete experience with AL for
real annotation efforts. In their study on AL for base
noun phrase chunking, Ngai and Yarowsky (2000)
compare the costs of rule-writing with (AL-driven)
annotation to compile a base noun phrase chunker.
They conclude that one should rather invest human
labor in annotation than in rule writing.
Closer to our concerns is the study by Hachey et
al. (2005) who apply AL to named entity (NE) an-
notation. There are some differences in the actual
AL approach they chose, while their main idea, viz.
to apply committee-based AL to speed up real anno-
tations, is comparable to our work. They report on
negative side effects of AL on the annotations and
state that AL annotations are cognitively more diffi-
cult for the annotators to deal with (because the sen-
tences selected for annotation are more complex).
As a consequence, diminished annotation quality
and higher per-sentence annotation times arise in
their experiments. By and large, however, they con-
clude that AL selection should still be favored over
random selection because the negative implications
of AL are easily over-compensated by the signifi-
cant reduction of sentences to be annotated to yield
comparable classifier performance as under random
sampling conditions.
Whereas Hatchey et al focus only on one group
of entity mentions (viz. four entity subclasses of the
astrophysics domain), we report on broader experi-
ence when applying AL to annotate several groups
of entity mentions in biomedical subdomains. We
also address practical aspects as to how create the
seed set for the first AL round and how one might
estimate the efficiency of AL. The immense sav-
ings in annotation effort we achieve here (up to
75%) may mainly depend on the sparseness of many
entity types in biomedical corpora. Furthermore,
we here present a general annotation environment
which supports AL-driven annotations for most seg-
mentation problems, not just for NE recognition.
In contrast, annotation editors, such as e.g. Word-
Freak1, typically offer facilities for supervised cor-
rection of automatically annotated text. This, how-
ever, is very different from the AL approach.
3 JANE ? Jena ANnotation Environment
JANE, the Jena ANnotation Environment, supports
the whole annotation life-cycle including the com-
pilation of annotation projects, annotation itself (via
an external editor), monitoring, and the deploy-
ment of annotated material. In JANE, an annota-
tion project consists of a collection of documents
to be annotated, an associated annotation schema
? a specification of what has to be annotated in
which way, according to the accompanying annota-
tion guidelines ? a set of configuration parameters,
and an annotator assigned to it.
We distinguish two kinds of annotation projects:
A default project, on the one hand, contains a prede-
fined and fixed collection of naturally occurring doc-
uments which the annotator handles independently
of each other. In an active learning project, on the
other hand, the annotator has access to exactly one
1http://wordfreak.sourceforge.net
10
(AL-computed pseudo) document at a time. After
such a document has completely been annotated, a
new one is dynamically constructed which contains
those sentences for annotation which are the most
informative ones for training a classifier. Besides
annotators who actually do the annotation, there
are administrators who are in charge of (annota-
tion) project management, monitoring the annota-
tion progress, and deployment, i.e., exporting the
data to other formats.
JANE consists of one central component, the an-
notation repository, where all annotation and project
data is stored centrally, two user interfaces, namely
one for the annotators and one for the administra-
tor, and the active learning component which inter-
actively generates documents to speed up the anno-
tation process. All components communicate with
the annotation repository through a network socket
? allowing JANE to be run in a distributed envi-
ronment. JANE is largely platform-independent be-
cause all components are implemented in Java. A
test version of JANE may be obtained from http:
//www.julielab.de.
3.1 Active Learning Component
One of the most established approaches to active
learning is based on the idea to build an ensemble
of classifiers from the already annotated examples.
Each classifier then makes its prediction on all unla-
beled exampels. Examples on which the classifiers
in the ensemble disagree most in their predictions
are considered informative and are thus requested
for labeling. Obviously, we can expect that adding
these examples to the training corpus will increase
the accuracy of a classifier trained on this data (Se-
ung et al, 1992). A common metric to estimate
the disagreement within an ensemble is the so-called
vote entropy, the entropy of the distribution of labels
li assigned to an example e by the ensemble of k
classifiers (Engelson and Dagan, 1996):
D(e) = ? 1log k
?
li
V (li, e)
k log
V (li, e)
k
Our AL component employs such an ensemble-
based approach (Tomanek et al, 2007). The ensem-
ble consists of k = 3 classifiers2. AL is run on the
2Currently, we incorporate as classifiers Naive Bayes, Max-
imum Entropy, and Conditional Random Fields.
sentence level because this is a natural unit for many
segmentation tasks. In each round, b sentences with
the highest disagreement are selected.3 The pool of
(available) unlabeled examples can be very large for
many NLP tasks; for NE annotations in the biomedi-
cal domain we typically download several hundreds
of thousands of abstracts from PUBMED.4 In or-
der to avoid high selection times, we consider only
a (random) subsample of the pool of unlabeled ex-
amples in each AL round. Both the selection size b
(which we normally set to b = 30), the composition
of the ensemble, and the subsampling ratio can be
configured with the administration component.
AL selects single, non-contiguous sentences from
different documents. Since the context of these sen-
tences is still crucial for many (semantic) annota-
tion decisions, for each selected sentence its origi-
nal context is added (but blocked from annotation).
When AL selection is finished, a new document is
compiled from these sentences (including their con-
texts) and uploaded to the annotation repository. The
annotator can then proceed with annotation.
Although optimized for NE annotations, the AL
component may ? after minor modifications of the
feature sets being used by the classifiers ? also be ap-
plied to other segmentation problems, such as POS
or chunk annotations.
3.2 Administration Component
Administering large-scale annotation projects is a
challenging management task for which we supply
a GUI (Figure 1) to support the following tasks:
User Management Create accounts for adminis-
trators and annotators.
Creation of Projects The creation of an annota-
tion project requires a considerable number of doc-
uments and other files (such as annotation schema
definitions) to be uploaded to the annotation reposi-
tory. Furthermore, several parameters, especially for
AL projects have to be set appropriately.
Editing a Project The administrator can reset a
project (especially when guidelines change, one
3Here, the vote entropy is calculated separately for each to-
ken. The sentence-level vote entropy is then the average over
the respective token sequence.
4http://www.ncbi.nlm.nih.gov/
11
Figure 1: Administration GUI: frame in foreground shows actions that can be performed on an AL project.
might want to start the annotation process anew,
i.e., delete all previous annotations but keep the rest
of the project unchanged), delete a project, copy a
project (which is helpful when several annotators la-
bel the same documents to check the applicability of
the guidelines by inter-annotator agreement calcula-
tion), and change several AL-specific settings.
Monitoring the Annotation Process The admin-
istrator can check which documents of an annotation
project have already been annotated, how long anno-
tation took on the average, when an annotator logged
in last time, etc. Furthermore, the progress of AL
projects can be visualized by learning and disagree-
ment curves and an enumeration of the number of
(unique) entities found so far.
Inter-Annotator Agreement For related projects
(projects sharing the same annotation schema and
documents to be annotated) the degree to which
several annotators mutually agree in their annota-
tions can be calculated. Such an inter-annotator
agreement (IAA) is common to estimate the quality
and applicability of particular annotation guidelines
(Kim and Tsujii, 2006). Currently, several IAA met-
rics of different strictness for NE annotations (and
other segmentation tasks) are incorporated.
Deployment The annotation repository stores the
annotations in a specific XML format (see Sec-
tion 3.3). For deployment, the annotations may be
needed in a different format. Currently, the admin-
istration GUI basically supports export into the IOB
format. Only documents marked by the annotators
as ?completely annotated? are considered.
3.3 Annotation Component
As the annotators are rather domain experts (in our
case graduate students of biology or related life sci-
ences) than computer specialists, we wanted to make
life for them as easy as possible. Hence, we pro-
vide a separate GUI for the annotators. After log-in
the annotator is given an overview of his/her annota-
tion projects along with a short description. Double
clicking on a project, the annotators get a list with
all documents in this project. Documents have dif-
ferent flags (raw, in progress, done) to indicate the
current annotation state as set by each annotator.
Annotation itself is done with MMAX, an external
annotation editor (Mu?ller and Strube, 2003), which
can be customized with respect to the particular an-
notation schema. The document to be annotated, the
annotations, and the configuration parameters are
stored in separate XML files. Our annotation repos-
itory reflects this MMAX-specific data structure.
Double clicking on a specific document directly
opens MMAX for annotation. During annotation,
the annotation GUI is locked to ensure data in-
12
tegrity. When working on an AL project, the anno-
tator can start the AL selection process (which then
runs on a separate high-performance machine) after
having finished the annotation of the current docu-
ment. During the AL selection process (it usually
takes up to several minutes) the current project is
blocked. However, meanwhile the annotator can go
on annotating other projects.
3.4 Annotation Repository
The annotation repository is the heart of our annota-
tion environment. All project, user, and annotation
relevant data is stored here centrally. This is a cru-
cial design criterion because it lets the administrator
access (e.g., for backup or deployment) all annota-
tions from one central site. Furthermore, the anno-
tators do not have to care about how to shift the an-
notated documents to the managerial staff. All state
information related to the entire annotation cycle is
recorded and kept centrally in this repository.
The repository is realized as a relational database5
reflecting largely the data structure of MMAX. Both,
the GUIs and the AL component, communicate with
the repository via the JDBC network driver. Thus,
each component can be run on a different machine
as long as it has a network connection to the annota-
tion repository. This has two main advantages: First,
annotators can work remotely (e.g., from home or
from a physically dislocated lab). Second, resource-
intensive tasks, e.g., AL selection, can be run on sep-
arate machines to which the annotators normally do
not have access. The components communicate with
each other only through the annotation repository. In
particular, there is no direct communication between
the annotation GUI and the AL component.
4 Experience with Real-World Annotations
We are currently conducting NE annotations for
two large-scale information extraction and seman-
tic retrieval projects. Both tasks cover two non-
overlapping biomedical subdomains, viz. one in the
field of hematopoietic stem cell transplantation (im-
munogenetics), the other in the area of gene regu-
lation. Entity types of interest are, e.g., cytokines
and their receptors, antigens, antibodies, immune
5We chose MYSQL, a fast and reliable open source database
with native Java driver support
cells, variation events, chemicals, blood diseases,
etc. In this section, we report on our actual ex-
perience and findings in annotating entity mentions
(drawing mainly on our work in the immunogenetics
subdomain) with JANE, with a focus on methodolog-
ical issues related to active learning.
In the biomedical domain, there is a vast amount
of unlabeled material available for almost any topic
of interest. The most prominent source is probably
PUBMED, a literature database which currently in-
cludes over 16 million citations, mostly abstracts,
from MEDLINE and other life science sources. We
used MESH terms6 and publication date ranges7 to
select relevant documents from the immunogenet-
ics subdomain. Thus, we retrieved about 200,000
abstracts (? 2,000,000 sentences) as our document
pool of unlabeled examples for immunogenetics.
Through random subsampling, only about 40,000
sentences are considered for AL selection.
For several of our entity annotations, we did both
an active learning (AL) annotation and a gold stan-
dard (GS) annotation. The latter is performed in
the default project mode on 250 abstracts randomly
chosen from the entire document pool. We asked
different annotators to annotate the same (subset of
the) GS to calculate inter-annotator agreement in or-
der to make sure that our annotation guidelines were
non-ambiguous. Furthermore, as the annotation pro-
ceeds, we regularly train a classifier on the AL an-
notations and evaluate it against the GS annotations.
From this learning curve, we can estimate the poten-
tial gain of further AL annotation rounds and decide
when to stop AL annotation.
4.1 Reduction of Annotation Effort through AL
In real-world AL annotation projects, the amount of
cost reduction is hard to estimate properly. We have
thus extensively simulated and tested the gain in the
reduction of annotation costs of our AL component
on available entity annotations of the biomedical do-
main (GENIA8 and PENNBIOIE9) and the general-
6MESH (http://www.nlm.nih.gov/mesh/) is the
U.S. National Library of Medicine?s controlled vocabulary used
for indexing PUBMED articles.
7Typically, articles published before 1990 are not considered
to contain relevant information for molecular biology.
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
9http://bioie.ldc.upenn.edu/
13
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  1000  2000  3000  4000  5000  6000  7000  8000
F-
sc
or
e
sentences
reduction of annotation costs (75%)
AL selection
random selection
Figure 2: Learning curves for AL and random selec-
tion on variation event entity mentions.
language newspaper domain (English data set of the
CoNLL-2003 shared task (Tjong Kim Sang and De
Meulder, 2003)). As a metric for annotation costs
we here consider the number of sentences to be an-
notated such that a certain F-score is reached with
our NE tagger.10 We therefore compare the learning
curves of AL and random selection. On almost ev-
ery scenario, we found that AL yields cost savings
of about 50%, sometimes even up to 75%.
As an example, we report on our AL simula-
tion on the PENNBIOIE corpus for variation events.
These entity mentions include the following six sub-
classes: type, event, original state, altered state,
generic state, and location. The learning curves
for AL and random selection are shown in Figure
2. Using random sampling, an F-score of 80% is
reached by random selection after ? 8,000 sentences
(200,000 tokens). In contrast, AL selection yields
the same F-score after ? 2,000 sentences (46,000
tokens). This amounts to a reduction of annotation
costs on the order of 75%.
Our real-world annotations revealed that AL is
especially beneficial when entity mentions are very
sparsely distributed in the texts. After an initializa-
tion phase needed by AL to take off (which can con-
siderably be accelerated when one carefully selects
the sentences of the first AL round, see Section 4.2),
AL selects, by and large, only sentences which con-
tain at least one entity mention of the type of inter-
10The named enatity tagger used throughout in this section
is based on Conditional Random Fields and similar to the one
presented by (Settles, 2004).
 0
 500
 1000
 1500
 2000
 2500
 3000
 200  400  600  800  1000  1200  1400  1600  1800  2000
e
n
tit
y 
m
en
tio
ns
sentences
AL annotation
GS annotation
Figure 3: Cumulated entity density on AL and GS
annotations of cytokine receptors.
est. In contrast, random selection (or in real anno-
tation projects: sequential annotations of abstracts
as in our default project mode), may lead to lots of
negative training examples with no entity mentions
of interest. When there is no simulation data at hand,
the entity density of AL annotations (compared with
the respective GS annotation) is a good estimate of
the effectiveness of AL.
Figure 3 depicts such a cumulated entity density
plot on AL and GS annotations of subtypes of cy-
tokine receptors, really very sparse entity types with
one entity mention per PUBMED abstract on the av-
erage. The 250 abstracts of the GS annotation only
contain 193 cytokine receptor entity mentions. AL
annotation of the same number of sentences resulted
in 2,800 annotated entity mentions of this type. The
entity density in our AL corpus is thus almost 15
times higher than in our GS corpus. Such a dense
corpus is certainly much more appropriate for clas-
sifier training due to the tremendous increase of pos-
itive training instances. We observed comparable ef-
fects with other entity types as well, and thus con-
clude that the sparser entity mentions of a specific
type are in texts, the more benefical AL-based anno-
tation actually is.
4.2 Mind the Seed Set
For AL, the sentences to be annotated in the first AL
round, the seed set, have to be manually selected. As
stated above, the proper choice of this set is crucial
for efficient AL based annotation. One should def-
initely refrain from a randomly generated seed set
14
? especially, when sparse entity mentions are anno-
tated ? because it might take quite a while for AL to
take off. If, in the worst case, the seed set contains
no entity mentions of interest, AL based annotation
resembles (for several rounds in the beginning until
incidentally some entity mentions are found) a ran-
dom selection ? which is, as shown in Section 4.1,
suboptimal. Figure 4 shows the simulated effect of
three different seed sets on variation event annota-
tion (PENNBIOIE). In the tuned seed set, each sen-
tence contains at least one variation entity mention.
On this seed, AL performs significantly better than
the randomly assembled seed or the seed with no en-
tity mentions at all. Of course, in the long run, the
three curves converge. Given this evidence, we stip-
ulate that the sparser an entity type is11 or the larger
the document pool to be selected from is, the later
the point of convergence and, thus, the more rele-
vant an effective seed set is.
We developed a useful three-step heuristic to
compile effective seed sets without excessive man-
ual work. In the first step, a list is compiled
comprised of as many entity mentions (of inter-
est to the current annotation project) as possible.
In knowledge- and expert-intensive domains such
as molecular biology, this can either be done by
consulting a domain expert or by harvesting entity
mentions from online resources (such as biological
databases).12 In a second step, the compiled list
is matched against each sentence of the document
pool. Third, a ranking procedure orders the sen-
tences (in descending order) according to the num-
ber of diverse matches of entity mentions. This en-
sures that textual mentions of all items from the list
are included in the seed set. Depending on the vari-
ety and density of the specific entity types, our seed
sets typically consist of 200 to 500 sentences.
4.3 Portability of Corpora
While we are working in the field of immunogenet-
ics, the PENNBIOIE corpus focuses on the subdo-
main of oncogenetics and provides a sound annota-
11Variation events are not as sparse in PENNBIOIE as, e.g.,
cytokine receptors in our subdomain. Actually, there is a varia-
tion entity in almost every second sentence.
12In an additional step, some spelling variations of such en-
tity mentions could automatically be generated.
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  100  200  300  400  500
F-
sc
or
e
sentences
random seed set
tuned seed set
seed set with no entities
Figure 4: Effect of different seed sets for AL on vari-
ation event annotation.
tion of these entity mentions (PBVAR).13 We did a
GS annotation on 250 randomly chosen abstracts (?
2,000 sentences/65,000 tokens) from our document
pool applying PENNBIOIE?s annotation guidelines
for variation events to the subdomain of immuno-
genetics (IMVAR-Gold). We then evaluated how
well our entity tagger trained on PBVAR would do
on this data. Surprisingly, the performance was dra-
matically low, viz. 31.2% F-score.14
Thus, we did further variation event annotations
for the immunogenetics domain with AL: We anno-
tated ? 58,000 tokens (IMVAR-AL). We trained our
entity tagger on this data and evaluated the tagger on
both IMVAR-Gold and PBVAR. Table 1 summarizes
the results. We conclude that porting training cor-
pora, even from one related subdomain into another,
is only possible to a very limited extent. This may be
because current NE taggers (ours, as well) make ex-
tensive use of lexical features. However, the results
also reveal that annotations made by AL may be
more robust when ported to another domain: a tag-
ger trained on IMVAR-AL still yields about 62.5%
F-score on PBVAR, whereas training the tagger on
the respective GS annotation (IMVAR-Gold), only
about half the performance is yielded (35.8%).
13Although oncogenetics and immunogenetics are different
subdomains, they share topical overlaps ? in particular, with
respect to the types of relevant variation entity mentions (such
as ?single nucleotide polymorphism?, ?translocation?, ?in-frame
deletion?, ?substitution?, etc.). Hence, at least at this level the
two subdomains are related.
14Note that in a 10-fold cross-validation on PBVAR our entity
tagger yielded about 80% F-score.
15
evaluation data
training data PBVAR IMVAR-Gold
PBVAR
(? 200.000 tokens) ? 80% 31.2%
IMVAR-AL
(58.251 tokens) 62.5% 70.2%
IMVAR-Gold
(63.591 tokens) 35.8% ?
Table 1: Corpus portability: PENNBIOIE?s variation
entity annotations (PBVAR) vs. ours for immuno-
genetics (IMVAR-AL and -Gold).
5 Conclusion and Future Work
We introduced JANE, an annotation environment
which supports the whole annotation life-cycle from
annotation project compilation to annotation deploy-
ment. As one of its major contributions, JANE al-
lows for focused annotation based on active learn-
ing, i.e., it automatically presents sentences for an-
notation which are of most use for classifier training.
We have shown that porting annotated training
corpora, even from one subdomain to another and
thus related to a good extent, may severely degrade
classifier performance. Thus, generating new an-
notation data will increasingly become important,
especially under the prospect that there are more
and more real-world information extraction projects
for different (sub)domains and languages. We have
shown that focused, i.e., AL-driven, annotation is a
reasonable choice to significantly reduce the effort
needed to create such annotations ? up to 75% in a
realistic setting. Furthermore, we have highlighted
the positive effects of a high-quality seed set for AL
and outlined a general heuristic for its compilation.
At the moment, the AL component may be used
for most kinds of segmentation problems (e.g. POS
tagging, text chunking, entity recognition). Future
work will focus on the extension of the AL compo-
nent for relation encoding as required for corefer-
ences or role and propositional information.
Acknowledgements
We thank Alexander Klaue for implementing the
GUIs. This research was funded by the EC within
the BOOTStrep project (FP6-028099), and by the
German Ministry of Education and Research within
the StemNet project (01DS001A to 1C).
References
Lynn Carlson, Daniel Marcu, and Mary E. Okurowski. 2003.
Building a discourse-tagged corpus in the framework of
Rhetorical Structure Theory. In J. van Kuppevelt and R.
Smith, editors, Current Directions in Discourse and Dia-
logue, pp. 85?112. Kluwer.
Sean Engelson and Ido Dagan. 1996. Minimizing manual an-
notation cost in supervised training from corpora. In Proc.
of ACL 1996, pp. 319?326.
B. Hachey, B. Alex, and M. Becker. 2005. Investigating the
effects of selective sampling on the annotation task. In Proc.
of CoNLL-2005, pp. 144?151.
Rebecca Hwa. 2000. Sample selection for statistical grammar
induction. In Proc. of EMNLP/VLC-2000, pp. 45?52.
Jin-Dong Kim and Jun?ichi Tsujii. 2006. Corpora and their
annotation. In S. Ananiadou and J. McNaught, editors, Text
Mining for Biology and Biomedicine, pp. 179?211. Artech.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
C. Mu?ller and M. Strube. 2003. Multi-level annotation in
MMAX. In Proc. of the 4th SIGdial Workshop on Discourse
and Dialogue, pp. 198?207.
Grace Ngai and David Yarowsky. 2000. Rule writing or an-
notation: Cost-efficient resource usage for base noun phrase
chunking. In Proc. of ACL 2000, pp. 117?125.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002. The GE-
NIA corpus: An annotated research abstract corpus in molec-
ular biology domain. In Proc. of HLT 2002, pp. 82?86.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proc. of EMNLP 2001, pp. 1?9.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew See,
Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth
Sundheim, David Day, Lisa Ferro, and Marcia Lazo. 2003.
The TIMEBANK corpus. In Proc. of the Corpus Linguistics
2003 Conference, pp. 647?656.
Burr Settles. 2004. Biomedical named entity recognition using
conditional random fields and rich feature sets. In Proc. of
JNLPBA 2004, pp. 107?110.
H. Sebastian Seung, Manfred Opper, and Haim Sompolinsky.
1992. Query by committee. In Proc. of COLT 1992, pp.
287?294.
Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction
to the CONLL-2003 shared task: Language-independent
named entity recognition. In Proc. of CoNLL 2003, pp. 142?
147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. An
approach to downsizing annotation costs and maintaining
corpus reusability. In Proc of EMNLP-CoNLL 2007.
Kees van Deemter and Rodger Kibble. 2000. On coreferring:
Coreference in MUC and related annotation schemes. Com-
putational Linguistics, 26(4):629?637.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. An-
notating expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2/3):165?210.
16
Proceedings of the Linguistic Annotation Workshop, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Annotation Type System for a Data-Driven NLP Pipeline
Udo Hahn Ekaterina Buyko Katrin Tomanek
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
{hahn|buyko|tomanek}@coling-uni-jena.de
Scott Piao John McNaught Yoshimasa Tsuruoka Sophia Ananiadou
NaCTeM and School of Computer Science
University of Manchester
{scott.piao|john.mcnaught|yoshimasa.tsuruoka|sophia.ananiadou}@manchester.ac.uk
Abstract
We introduce an annotation type system for
a data-driven NLP core system. The specifi-
cations cover formal document structure and
document meta information, as well as the
linguistic levels of morphology, syntax and
semantics. The type system is embedded in
the framework of the Unstructured Informa-
tion Management Architecture (UIMA).
1 Introduction
With the maturation of language technology, soft-
ware engineering issues such as re-usability, in-
teroperability, or portability are getting more and
more attention. As dozens of stand-alone compo-
nents such as tokenizers, stemmers, lemmatizers,
chunkers, parsers, etc. are made accessible in vari-
ous NLP software libraries and repositories the idea
sounds intriguing to (re-)use them on an ?as is? basis
and thus save expenditure and manpower when one
configures a composite NLP pipeline.
As a consequence, two questions arise. First, how
can we abstract away from the specific code level of
those single modules which serve, by and large, the
same functionality? Second, how can we build NLP
systems by composing them, at the abstract level
of functional specification, from these already ex-
isting component building blocks disregarding con-
crete implementation matters? Yet another burning
issue relates to the increasing availability of multiple
metadata annotations both in corpora and language
processors. If alternative annotation tag sets are cho-
sen for the same functional task a ?data conversion?
problem is created which should be solved at the ab-
stract specification level as well (Ide et al, 2003).
Software engineering methodology points out that
these requirements are best met by properly identi-
fying input/output capabilities of constituent compo-
nents and by specifying a general data model (e.g.,
based on UML (Rumbaugh et al, 1999)) in or-
der to get rid of the low-level implementation (i.e.,
coding) layer. A particularly promising proposal
along this line of thought is the Unstructured Infor-
mation Management Architecture (UIMA) (Ferrucci
and Lally, 2004) originating from IBM research ac-
tivities.1 UIMA is but the latest attempt in a series
of proposals concerned with more generic NLP en-
gines such as ATLAS (Laprun et al, 2002) or GATE
(Cunningham, 2002). These frameworks have in
common a data-driven architecture and a data model
based on annotation graphs as an adaptation of the
TIPSTER architecture (Grishman, 1997). They suf-
fer, however, from a lack of standards for data ex-
change and abstraction mechanisms at the level of
specification languages.
This can be achieved by the definition of a com-
mon annotation scheme. We propose an UIMA
schema which accounts for a significant part of the
complete NLP cycle ? from the collection of doc-
uments and their internal formal structure, via sen-
tence splitting, tokenization, POS tagging, and pars-
ing, up until the semantic layer (still excluding dis-
course) ? and which aims at the implementation-
independent specification of a core NLP system.
1Though designed for any sort of unstructured data (text,
audio and video data), we here focus on special requirements
for the analysis of written documents.
33
2 Related work
Efforts towards the design of annotation schemata
for language resources and their standardization
have a long-standing tradition in the NLP commu-
nity. In the very beginning, this work often fo-
cused exclusively on subdomains of text analysis
such as document structure meta-information, syn-
tactic or semantic analysis. The Text Encoding Ini-
tiative (TEI)2 provided schemata for the exchange
of documents of various genres. The Dublin Core
Metadata Initiative3 established a de facto standard
for the Semantic Web.4 For (computational) lin-
guistics proper, syntactic annotation schemes, such
as the one from the Penn Treebank (Marcus et al,
1993), or semantic annotations, such as the one un-
derlying ACE (Doddington et al, 2004), are increas-
ingly being used in a quasi standard way.
In recent years, however, the NLP community is
trying to combine and merge different kinds of an-
notations for single linguistic layers. XML formats
play a central role here. An XML-based encod-
ing standard for linguistic corpora XCES (Ide et al,
2000) is based on CES (Corpus Encoding Standard)
as part of the EAGLES Guidelines.5 Work on TIGER
(Brants and Hansen, 2002) is an example for the li-
aison of dependency- and constituent-based syntac-
tic annotations. New standardization efforts such as
the Syntactic Annotation Framework (SYNAF) (De-
clerck, 2006) aim to combine different proposals and
create standards for syntactic annotation.
We also encounter a tendency towards multiple
annotations for a single corpus. Major bio-medical
corpora, such as GENIA (Ohta et al, 2002) or
PennBioIE,6 combine several layers of linguistic
information in terms of morpho-syntactic, syntac-
tic and semantic annotations (named entities and
events). In the meantime, the Annotation Compat-
ibility Working Group (Meyers, 2006) began to con-
centrate its activities on the mutual compatibility of
annotation schemata for, e.g., POS tagging, tree-
banking, role labeling, time annotation, etc.
The goal of these initiatives, however, has never
been to design an annotation scheme for a complete
2http://www.tei-c.org
3http://dublincore.org
4http://www.w3.org/2001/sw
5http://www.ilc.cnr.it/EAGLES96/
6http://bioie.ldc.upenn.edu
NLP pipeline as needed, e.g., for information ex-
traction or text mining tasks (Hahn and Wermter,
2006). This lack is mainly due to missing standards
for specifying comprehensive NLP software archi-
tectures. The MEANING format (Pianta et al, 2006)
is designed to integrate different levels of morpho-
syntactic annotations. The HEART OF GOLD mid-
dleware (Scha?fer, 2006) combines multidimensional
mark-up produced by several NLP components. An
XML-based NLP tool suite for analyzing and anno-
tating medical language in an NLP pipeline was also
proposed by (Grover et al, 2002). All these propos-
als share their explicit linkage to a specific NLP tool
suite or NLP system and thus lack a generic annota-
tion framework that can be re-used in other develop-
mental environments.
Buitelaar et al developed in the context of an in-
formation extraction project an XML-based multi-
layered annotation scheme that covers morpho-
syntactic, shallow parsing and semantic annotation
(Buitelaar et al, 2003). Their scheme borrows con-
cepts from object-oriented programming (e.g., ab-
stract types, polymorphism). The object-oriented
perspective already allows the development of a
domain-independent schema and extensions of core
types without affecting the base schema. This
schema is comprehensive indeed and covers a sig-
nificant part of advanced NLP pipelines but it is also
not connected to a generic framework.
It is our intention to come full circle within a
general annotation framework. Accordingly, we
cover a significant part of the NLP pipeline from
document meta information and formal document
structure, morpho-syntactic and syntactic analysis
up to semantic processing. The scheme we propose
is intended to be compatible with on-going work
in standardization efforts from task-specific annota-
tions and to adhere to object-oriented principles.
3 Data-Driven NLP Architecture
As the framework for our specification efforts, we
adopted the Unstructured Information Management
Architecture (UIMA) (Ferrucci and Lally, 2004). It
provides a formal specification layer based on UML,
as well as a run-time environment for the interpreta-
tion and use of these specifications. This dualism is
going to attract more and more researchers as a basis
34
for proper NLP system engineering.
3.1 UIMA-based Tool Suite
UIMA provides a platfrom for the integration
of NLP components (ANALYSIS ENGINES in the
UIMA jargon) and the deployment of complex
NLP pipelines. It is more powerful than other
prominent software systems for language engineer-
ing (e.g., GATE, ATLAS) as far as its pre- and
post-processing facilities are concerned ? so-called
COLLECTION READERS can be developed to handle
any kind of input format (e.g., WWW documents,
conference proceedings), while CONSUMERS, on
other hand, deal with the subsequent manipulation
of the NLP core results (e.g., automatic indexing).
Therefore, UIMA is a particularly suitable architec-
ture for advanced text analysis applications such as
text mining or information extraction.
We currently provide ANALYSIS ENGINES for
sentence splitting, tokenization, POS tagging, shal-
low and full parsing, acronym detection, named
entity recognition, and mapping from named enti-
ties to database term identifiers (the latter is mo-
tivated by our biological application context). As
we mainly deal with documents taken from the bio-
medical domain, our collection readers process doc-
uments from PUBMED,7 the most important liter-
ature resource for researchers in the life sciences.
PUBMED currently provides more than 16 million
bibliographic references to bio-medical articles. The
outcomes of ANALYSIS ENGINES are input for var-
ious CONSUMERS such as semantic search engines
or text mining tools.
3.2 Common Analysis System
UIMA is based on a data-driven architecture. This
means that UIMA components do not exchange or
share code, they rather exchange data only. The
components operate on common data referred to
as COMMON ANALYSIS SYSTEM (CAS)(Go?tz and
Suhre, 2004). The CAS contains the subject of anal-
ysis (document) and provides meta data in the form
of annotations. Analysis engines receive annotations
through a CAS and add new annotations to the CAS.
An annotation in the CAS then associates meta data
with a region the subject of the analysis occupies
7http://www.pubmed.gov
(e.g., the start and end positions in a document).
UIMA defines CAS interfaces for indexing, ac-
cessing and updating the CAS. CASes are modelled
independently from particular programming lan-
guages. However, JCAS, an object-oriented inter-
face to the CAS, was developed for JAVA. CASes are
crucial for the development and deployment of com-
plex NLP pipelines. All components to be integrated
in UIMA are characterized by abstract input/output
specifications, so-called capabilities. These speci-
fications are declared in terms of descriptors. The
components can be integrated by wrappers conform-
ing with the descriptors. For the integration task, we
define in advance what kind of data each component
may manipulate. This is achieved via the UIMA
annotation type system. This type system follows
the object-oriented paradigm. There are only two
kinds of data, viz. types and features. Features spec-
ify slots within a type, which either have primitive
values such as integers or strings, or have references
to instances of types in the CAS. Types, often called
feature structures, are arranged in an inheritance hi-
erarchy.
In the following section, we propose an ANNO-
TATION TYPE SYSTEM designed and implemented
for an UIMA Tool Suite that will become the back-
bone for our text mining applications. We distin-
guish between the design and implementation lev-
els, talking about the ANNOTATION SCHEME and
the TYPE SYSTEM, respectively.
4 Annotation Type System
The ANNOTATION SCHEME we propose currently
consists of five layers: Document Meta, Document
Structure & Style, Morpho-Syntax, Syntax and Se-
mantics. Accordingly, annotation types fall into five
corresponding categories. Document Meta and Doc-
ument Structure & Style contain annotations about
each document?s bibliography, organisation and lay-
out. Morpho-Syntax and Syntax describe the results
of morpho-syntactic and syntactic analysis of texts.
The results of lemmatisation, stemming and decom-
position of words can be represented at this layer, as
well. The annotations from shallow and full parsing
are represented at the Syntax layer. The appropri-
ate types permit the representation of dependency-
and constituency-based parsing results. Semantics
35
uima.tcas.Annotation
+begin: uima.cas.Integer
+end: uima.cas.Integer
Annotation
+componentId: uima.cas.String
+confidence: uima.cas.Double
Descriptor
pubmed.ManualDescriptor
+MeSHList: uma.cas.FSArray = MeSHHeading
+...
AutoDescriptor
+...
Header
+docType: uima.cas.String
+source: uima.cas.String
+docID: uima.cas.String
+language: uima.cas.String
+copyright: uima.cas.String
+authors: uima.cas.FSArray = AuthorInfo
+title: uima.cas.String
+pubTypeList: uima.cas.FSArray = PubType
+...
pubmed.Header
+citationStatus: uima.cas.String {...}
ManualDescriptor
+keywordList: uima.cas.FSArray = Keyword
+...
PubType
+name: uima.cas.Sting
Journal
+ISSN: uima.cas.String
+volume: uima.cas.String
+journalTitle: uima.cas.String
+impactFactor: uima.cas.String
Keyword
+name: uima.cas.String
+source: uima.cas.String
Token
+posTag: uima.cas.FSArray  = POSTag
+lemma: Lemma
+feats: GrammaticalFeats
+stemmedForm: StemmedForm
+depRelList: uima.cas.FSArray = DependencyRelation
+orthogr: uima.cas.FSArray = String
POSTag
+tagsetId: uima.cas.String
+language: uima.cas.String
+value: uima.cas.String
Lemma
+value: String
Acronym
Abbreviation
+expan: String
StemmedForm
+value: String
GrammaticalFeats
+language: uima.cas.String
DiscontinuousAnnotation
+value: FSArray = Annotation
PennPOSTag
NounFeats
+...
...
+...
...
+...
DependencyRelation
+head: Token
+projective: uima.cas.Boolean
+label: uima.cas.String
Relation
DepRelationSet...
Chunk
PhraseChunk
PTBConstituent
+formFuncDisc: uima.cas.String
+gramRole: uima.cas.String
+adv: uima.cas.String
+misc: uima.cas.String
+map: Constituent
+tpc: uima.cas.Boolean
+nullElement: uima.cas.String
+ref: Constituent
Constituent
+parent: Constituent
+head: Token
+cat: uima.cas.String
GENIAConstituent
+syn: uima.cas.String 
...
+...
...
+...
NP ...PP
Entity
+dbEntry: uima.cas.FSArray = DBEntry
+ontologyEntry: uima.cas.FSArray = OntologyEntry
+specificType: uima.cas.String
BioEntity
Cytokine
Organism VariationGene
...
LexiconEntry OntologyEntryDBEntry
ResourceEntry
+source: uima.cas.String
+entryId: uima.cas.String
+version: uima.cas.String
Zone 
Title TextBody Paragraph Figure
+caption: Caption
Section
+title: Title
+depth: uima.cas.Integer
Misc ... PersonOrganization
MUCEntity
...
2
3
4
5 6
1
CAS Core
Figure 1: Multi-Layered UIMA Annotation Scheme in UML Representation. 1: Basic Feature Structure and
Resource Linking. 2: Document Meta Information. 3: Morpho-Syntax. 4: Syntax. 5: Document Structure
& Style. 6: Semantics.
36
currently covers information about named entities,
events and relations between named entities.
4.1 Basic Feature Structure
All types referring to different linguistic lay-
ers derive from the basic type Annotation,
the root type in the scheme (cf. Figure 1-
1). The Annotation type itself derives infor-
mation from the default UIMA annotation type
uima.tcas.Annotation and, thus, inherits the
basic annotation features, viz. begin and end (mark-
ing spans of annotations in the subject of analysis).
Annotation extends this default feature structure
with additional features. The componentId marks
which NLP component actually computed this an-
notation. This attribute allows to manage multiple
annotations of the same type The unique linkage be-
tween an analysis component and an annotation item
is particularly relevant in cases of parallel annota-
tions. The component from which the annotation
originated also assigns a specific confidence score
to its confidence feature. Each type in the scheme is
at least supplied with these four slots inherited from
their common root type.
4.2 Document Meta Information
The Document Meta layer (cf. Figure 1-2) describes
the bibliographical and content information of a doc-
ument. The bibliographical information, often re-
trieved from the header of the analyzed document,
is represented in the type Header. The source
and docID attributes yield a unique identifier for
each document. We then adopted some Dublin Core
elements, e.g., language, title, docType. We dis-
tinguish between domain-independent information
such as language, title, document type and domain-
dependent information as relevant for text mining
in the bio-medical domain. Accordingly, the type
pubmed.Header was especially created for the
representation of PUBMED document information.
A more detailed description of the document?s pub-
lication data is available from types which specialize
PubType such as Journal. The latter contains
standard journal-specific attributes, e.g., ISSN, vol-
ume, journalTitle.
The description of the document?s content of-
ten comes with a list of keywords, informa-
tion assigned to the Descriptor type. We
clearly distinguish between content descriptors man-
ually provided by an author, indexer or cura-
tor, and items automatically generated by text
analysis components after document processing.
While the first kind of information will be stored
in the ManualDescriptor, the second one
will be represented in the AutoDescriptor.
The generation of domain-dependent descriptors is
also possible; currently the scheme contains the
pubmed.ManualDescriptor which allows to
assign attributes such as chemicals and genes.
4.3 Document Structure & Style
The Document Structure & Style layer (cf. Figure 1-
5) contains information about the organization and
layout of the analyzed documents. This layer en-
ables the marking-up of document structures such
as paragraphs, rhetorical zones, figures and tables,
as well as typographical information, such as italics
and special fonts. The focus of modeling this layer is
on the annotation of scientific documents, especially
in the life sciences. We adopted here the SCIXML8
annotation schema, which was especially developed
for marking-up scientific publications. The Zone
type refers to a distinct division of text and is the par-
ent type for various subtypes such as TextBody,
Title etc. While it seems impossible to predict all
of the potential formal text segments, we first looked
at types of text zones frequently occurring in sci-
entific documents. The type Section, e.g., repre-
sents a straightforward and fairly standard division
of scientific texts into introduction, methods and re-
sults sections. The divisions not covered by current
types can be annotated with Misc. The annotation
of tables and figures with corresponding types en-
ables to link text and additional non-textual infor-
mation, an issue which is gaining more and more
attention in the text mining field.
4.4 Morpho-Syntax
The Morpho-Syntax layer (cf. Figure 1-3) represents
the results of morpho-syntactic analysis such as to-
kenization, stemming, POS tagging. The small-
est annotation unit is Token which consists of five
attributes, including its part-of-speech information
8http://www.cl.cam.ac.uk/?aac10/
escience/sciborg.html
37
(posTag), stemmedForm, lemma, grammatical fea-
tures (feats), and orthographical information (or-
thogr).
With respect to already available POS tagsets,
the scheme allows corresponding extensions of
the supertype POSTag to, e.g., PennPOSTag
(for the Penn Tag Set (Marcus et al, 1993)) or
GeniaPOSTag (for the GENIA Tag Set (Ohta et
al., 2002)). The attribute tagsetId serves as a unique
identifier of the corresponding tagset. The value of
the POS tag (e.g., NN, VVD, CC) can be stored in
the attribute value. The potential values for the in-
stantiation of this attribute are always restricted to
the tags of the associated tagset. These constraints
enforce formal control on annotation processes.
As for morphologically normalized lexical items,
the Lemma type stores the canonical form of a lexi-
cal token which can be retrieved from a lexicon once
it is computed by a lemmatizer. The lemma value,
e.g., for the verb ?activates? would be ?activate?. The
StemmedForm represents a base form of a text to-
ken as produced by stemmers (e.g., ?activat-? for the
noun ?activation?).
Due to their excessive use in life science docu-
ments, abbreviations, acronyms and their expanded
forms have to be considered in terms of appropriate
types, as well. Accordingly, Abbreviation and
Acronym are defined, the latter one being a child
type of the first one. The expanded form of a short
one can easily be accessed from the attribute expan.
Grammatical features of tokens are represented
in those types which specialize the supertype
GrammaticalFeats. Its child types, viz.
NounFeats, VerbFeats, AdjectiveFeats,
PronounFeats (omitted from Figure 1-3) cover
the most important word categories. Attributes
of these types obviously reflect the properties
of particular grammatical categories. While
NounFeats comes with gender, case and num-
ber only, PronounFeats must be enhanced with
person. A more complex feature structure is asso-
ciated with VerbFeats which requires attributes
such as tense, person, number, voice and aspect. We
adapted here specifications from the TEI to allow
compatibility with other annotation schemata.
The type LexiconEntry (cf. Figure 1-1) en-
ables a link to the lexicon of choice. By designing
this type we achieve much needed flexibility in link-
ing text snaps (e.g., tokens, simplex forms, multi-
word terms) to external resources. The attributes
entryId and source yield, in combination, a unique
identifier of the current lexicon entry. Resource ver-
sion control is enabled through an attribute version.
Text annotations often mark disrupted text spans,
so-called discontinuous annotations. In coordinated
structures such as ?T and B cell?, the annotator
should mark two named entities, viz. ?T cell? and ?B
cell?, where the first one results from the combina-
tion of the disjoint parts ?T? and ?cell?. In order to
represent such discontinous annotations, we intro-
duced the type DiscontinuousAnnotation
(cf. Figure 1-1) which links through its attribute
value spans of annotations to an annotation unit.
4.5 Syntax
This layer of the scheme provides the types and at-
tributes for the representation of syntactic structures
of sentences (cf. Figure 1-4). The results from shal-
low and full parsing can be stored here.
Shallow parsing (chunking) aims at dividing
the flow of text into phrases (chunks) in a non-
overlapping and non-recursive manner. The type
Chunk accounts for different chunk tag sets by sub-
typing. Currently, the scheme supports Phrase-
Chunks with subtypes such as NP, VP, PP, or ADJP
(Marcus et al, 1993).
The scheme also reflects the most popular full
parsing approaches in NLP, viz. constituent-based
and dependency-based approaches. The results
from constituent-based parsing are represented in
a parse tree and can be stored as single nodes in
the Constituent type. The tree structure can
be reconstructed through links in the attribute par-
ent which stores the id of the parent constituent.
Besides the attribute parent, Constituent holds
the attributes cat which stores the complex syntac-
tic category of the current constituent (e.g., NP, VP),
and head which links to the head word of the con-
stituent. In order to account for multiple annota-
tions in the constituent-based approach, we intro-
duced corresponding constituent types which spe-
cialize Constituent. This parallels our approach
which we advocate for alternatives in POS tagging
and the management of alternative chunking results.
Currently, the scheme supports three differ-
ent constituent types, viz. PTBConstituent,
38
GENIAConstituent (Miyao and Tsujii, 2005)
and PennBIoIEConstituent. The attributes
of the type PTBConstituent cover the com-
plete repertoire of annotation items contained in
the Penn Treebank, such as functional tags for
form/function dicrepancies (formFuncDisc), gram-
matical role (gramRole), adverbials (adv) and mis-
cellaneous tags (misc). The representation of null
elements, topicalized elements and gaps with corre-
sponding references to the lexicalized elements in a
tree is reflected in attributes nullElement, tpc, map
and ref, respectively. GENIAConstituent and
PennBIoIEConstituent inherit from PTB-
Constituent all listed attributes and provide, in
the case of GENIAConstituent , an additional
attribute syn to specify the syntactic idiosyncrasy
(coordination) of constituents.
Dependency parsing results are directly linked to
the token level and are thus referenced in the Token
type. The DependencyRelation type inherits
from the general Relation type and introduces
additional features which are necessary for describ-
ing a syntactic dependency. The attribute label char-
acterizes the type of the analyzed dependency rela-
tion. The attribute head indicates the head of the
dependency relation attributed to the analyzed to-
ken. The attribute projective relates to the property
of the dependency relation whether it is projective
or not. As different dependency relation sets can be
used for parsing, we propose subtyping similar to
the constituency-based parsing approaches. In order
to account for alternative dependency relation sets,
we aggregate all possible annotations in the Token
type as a list (depRelList).
4.6 Semantics
The Semantics layer comprises currently the repre-
sentation of named entities, particularly for the bio-
medical domain. The entity types are hierarchically
organized. The supertype Entity (cf. Figure 1-
6) links annotated (named) entities to the ontologies
and databases through appropriate attributes, viz. on-
tologyEntry and sdbEntry. The attribute specific-
Type specifies the analyzed entity in a more detailed
way (e.g., Organism can be specified through
the species values ?human?, ?mouse?, ?rat?, etc.)
The subtypes are currently being developed in the
bio-medical domain and cover, e.g., genes, pro-
teins, organisms, diseases, variations. This hierar-
chy can easily be extended or supplemented with
entities from other domains. For illustration pur-
poses, we extended it here by MUC (Grishman
and Sundheim, 1996) entity types such as Person,
Organization, etc.
This scheme is still under construction and will
soon also incorporate the representation of relation-
ships between entities and domain-specific events.
The general type Relation will then be extended
with specific conceptual relations such as location,
part-of, etc. The representation of events will be
covered by a type which aggregates pre-defined re-
lations between entities and the event mention. An
event type such as InhibitionEventwould link
the text spans in the sentence ?protein A inhibits
protein B? in attributes agent (?protein A?), patient
(?protein B?), mention (?inhibits?).
5 Conclusion and Future work
In this paper, we introduced an UIMA annotation
type system which covers the core functionality
of morphological, syntactic and semantic analysis
components of a generic NLP system. It also in-
cludes type specifications which relate to the formal
document format and document style. Hence, the
design of this scheme allows the annotation of the
entire cycle of (sentence-level) NLP analysis (dis-
course phenomena still have to be covered).
The annotation scheme consists mostly of core
types which are designed in a domain-independent
way. Nevertheless, it can easily be extended with
types which fit other needs. The current scheme sup-
plies an extension for the bio-medical domain at the
document meta and structure level, as well as on the
semantic level. The morpho-syntactic and syntactic
levels provide types needed for the analysis of the
English language. Changes of attributes or attribute
value sets will lead to adaptations to other natural
languages.
We implemented the scheme as an UIMA type
system. The formal specifications are implemented
using the UIMA run-time environment. This direct
link of formal and implementational issues is a ma-
jor asset using UIMA unmatched by any previous
specification approach. Furthermore, all annotation
results can be converted to the XMI format within
39
the UIMA framework. XMI, the XML Metadata In-
terchange format, is an OMG9 standard for the XML
representation of object graphs.
The scheme also eases the representation of an-
notation results for the same task with alternative
and often competitive components. The identifica-
tion of the component which provided specific an-
notations can be retrieved from the attribute com-
ponentId. Furthermore, the annotation with alterna-
tive and multiple tag sets is supported as well. We
have designed for each tag set a type representing
the corresponding annotation parameters. The inher-
itance trees at almost all annotation layers support
the parallelism in annotation process (e.g., tagging
may proceed with different POS tagsets).
The user of the scheme can restrict the potential
values of the types or attributes. The current scheme
makes use of the customization capability for POS
tagsets, for all attributes of constituents and chunks.
This yields additional flexibility in the design and,
once specified, an increased potential for automatic
control for annotations.
The scheme also enables a straightforward con-
nection to external resources such as ontologies,
lexicons, and databases as evidenced by the corre-
sponding subtypes of ResourceEntry (cf. Figure
1-1). These types support the specification of a re-
lation between a concrete text span and the unique
item addressed in any of these resources.
With these considerations in mind, we strive for
the elaboration of a common standard UIMA type
system for NLP engines. The advantages of such a
standard include an easy exchange and integration
of different NLP analysis engines, the facilitation
of sophisticated evaluation studies (where, e.g., al-
ternative components for NLP tasks can be plugged
in and out at the spec level), and the reusability of
single NLP components developed in various labs.
Acknowledgments. This research was funded by the EC?s 6th Framework Programme
(4th call) within the BOOTStrep project under grant FP6-028099.
References
S. Brants and S. Hansen. 2002. Developments in the TIGER
annotation scheme and their realization in the corpus. In
Proc. of the 3rd LREC Conference, pages 1643?1649.
P. Buitelaar, T. Declerck, B. Sacaleanu, ?S. Vintar, D. Raileanu,
and C. Crispi. 2003. A multi-layered, XML-based approach
9http://www.omg.org
to the integration of linguistic and semantic annotations. In
Proc. of EACL 2003 Workshop NLPXML-03.
H. Cunningham. 2002. GATE, a general architecture for text
engineering. Computers and the Humanities, 36:223?254.
T. Declerck. 2006. SYNAF: Towards a standard for syntactic
annotation. In Proc. of the 5th LREC Conference.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program. In Proc. of the 4th LREC
Conference, pages 837?840.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural ap-
proach to unstructured information processing in the corpo-
rate research environment. Natural Language Engineering,
10(3-4):327?348.
T. Go?tz and O. Suhre. 2004. Design and implementation of the
UIMA Common Analysis System. IBM Systems Journal,
43(3):476?489.
R. Grishman and B. Sundheim. 1996. Message Understand-
ing Conference ? 6: A brief history. In Proc. of the 16th
COLING, pages 466?471.
R. Grishman. 1997. Tipster architecture design document,
version 2.3. Technical report, Defense Advanced Research
Projects Agency (DARPA), U.S. Departement of Defense.
C. Grover, E. Klein, M. Lapata, and A. Lascarides. 2002.
XML-based NLP tools for analysing and annotating medi-
cal language. In Proc. of the 2nd Workshop NLPXML-2002,
pages 1?8.
U. Hahn and J. Wermter. 2006. Levels of natural language pro-
cessing for text mining. In S. Ananiadou and J. McNaught,
editors, Text Mining for Biology and Biomedicine, pages 13?
41. Artech House.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES: An XML-
based standard for linguistic corpora. In Proc. of the 2nd
LREC Conference, pages 825?830.
N. Ide, L. Romary, and E. de la Clergerie. 2003. International
standard for a linguistic annotation framework. In Proc. of
the HLT-NAACL 2003 SEALTS Workshop, pages 25?30.
C. Laprun, J. Fiscus, J. Garofolo, and S. Pajot. 2002. A prac-
tical introduction to ATLAS. In Proc. of the 3rd LREC Con-
ference, pages 1928?1932.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
A. Meyers. 2006. Annotation compatibility working group re-
port. In Proc. of the COLING-ACL 2006 Workshop FLAC
2006?, pages 38?53.
Y. Miyao and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proc. of the
ACL 2005, pages 83 ? 90.
T. Ohta, Y. Tateisi, and J.-D. Kim. 2002. The GENIA corpus:
An annotated research abstract corpus in molecular biology
domain. In Proc. of the 2nd HLT, pages 82?86.
E. Pianta, L. Bentivogli, C. Girardi, and B. Magnini. 2006.
Representing and accessing multilevel linguistic annotation
using the MEANING format. In Proc. of the 5th EACL-2006
Workshop NLPXML-2006, pages 77?80.
J. Rumbaugh, I. Jacobson, and G. Booch. 1999. The Unified
Modeling Language Reference Manual. Addison-Wesley.
U. Scha?fer. 2006. Middleware for creating and combining
multi-dimensional NLP markup. In Proc. of the 5th EACL-
2006 Workshop NLPXML-2006, pages 81?84.
40
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 138?146,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Intrinsic Stopping Criterion for Committee-Based Active Learning
Fredrik Olsson
SICS
Box 1263
SE-164 29 Kista, Sweden
fredrik.olsson@sics.se
Katrin Tomanek
Jena University Language & Information Engineering Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, D-07743 Jena, Germany
katrin.tomanek@uni-jena.de
Abstract
As supervised machine learning methods are
increasingly used in language technology, the
need for high-quality annotated language data
becomes imminent. Active learning (AL) is
a means to alleviate the burden of annotation.
This paper addresses the problem of knowing
when to stop the AL process without having
the human annotator make an explicit deci-
sion on the matter. We propose and evaluate
an intrinsic criterion for committee-based AL
of named entity recognizers.
1 Introduction
With the increasing popularity of supervised ma-
chine learning methods in language processing, the
need for high-quality labeled text becomes immi-
nent. On the one hand, the amount of readily avail-
able texts is huge, while on the other hand the la-
beling and creation of corpora based on such texts is
tedious, error prone and expensive.
Active learning (AL) is one way of approaching
the challenge of classifier creation and data annota-
tion. Examples of AL used in language engineering
include named entity recognition (Shen et al, 2004;
Tomanek et al, 2007), text categorization (Lewis
and Gale, 1994; Hoi et al, 2006), part-of-speech
tagging (Ringger et al, 2007), and parsing (Thomp-
son et al, 1999; Becker and Osborne, 2005).
AL is a supervised machine learning technique in
which the learner is in control of the data used for
learning ? the control is used to query an oracle, typ-
ically a human, for the correct label of the unlabeled
training instances for which the classifier learned so
far makes unreliable predictions.
The AL process takes as input a set of labeled in-
stances and a larger set of unlabeled instances, and
produces a classifier and a relatively small set of
newly labeled data. The overall goal is to obtain
as good a classifier as possible, without having to
mark-up and supply the learner with more than nec-
essary data. The learning process aims at keeping
the human annotation effort to a minimum, only ask-
ing for advice where the training utility of the result
of such a query is high.
The approaches taken to AL in this paper are
based on committees of classifiers with access to
pools of data. Figure 1 outlines a prototypical
committee-based AL loop. In this paper we focus
on the question when AL-driven annotation should
be stopped (Item 7 in Figure 1).
Usually, the progress of AL is illustrated by
means of a learning curve which depicts how the
classifier?s performance changes as a result of in-
creasingly more labeled training data being avail-
able. A learning curve might be used to address
the issue of knowing when to stop the learning pro-
cess ? once the curve has leveled out, that is, when
additional training data does not contribute (much)
to increase the performance of the classifier, the AL
process may be terminated. While in a random se-
lection scenario, classifier performance can be esti-
mated by cross-validation on the labeled data, AL
requires a held-out annotated reference corpus. In
AL, the performance of the classifier cannot be re-
liably estimated using the data labeled in the pro-
cess since sampling strategies for estimating per-
formance assume independently and identically dis-
tributed examples (Schu?tze et al, 2006). The whole
point in AL is to obtain a distribution of instances
that is skewed in favor of the base learner used.
138
1. Initialize the process by applying EnsembleGeneration-
Method using base learner B on labeled training data set
DL to obtain a committee of classifiers C.
2. Have each classifier inC predict a label for every instance
in the unlabeled data set DU , obtain labeled set DU ?.
3. From DU ?, select the most informative n instances to
learn from, obtaining DU ??.
4. Ask the teacher for classifications of the instances I in
DU ??.
5. Move I , with supplied classifications, from DU to DL.
6. Re-train using EnsembleGenerationMethod and base
learner B on the newly extended DL to obtain a new com-
mittee, C.
7. Repeat steps 2 through 6 until DU is empty or some stop-
ping criterion is met.
8. Output classifier learned using EnsembleGeneration-
Method and base learner B on DL.
Figure 1: A prototypical query by committee algorithm.
In practice, however, an annotated reference cor-
pus is rarely available and its creation would be in-
consistent with the goal of creating a classifier with
as little human effort as possible. Thus, other ways
of deciding when to stop AL are needed. In this pa-
per, we propose an intrinsic stopping-criterion for
committee-based AL of named entity recognizers.
It is intrinsic in that it relies on the characteristics of
the data and the base learner1 rather than on exter-
nal parameters, i.e., the stopping criterion does not
require any pre-defined thresholds.
The paper is structured as follows. Section 2
sketches interpretations of ideal stopping points and
describes the idea behind our stopping criterion.
Section 3 outlines related work. Section 4 describes
the experiments we have conducted concerning a
named entity recognition scenario, while Section 5
presents the results which are then discussed in Sec-
tion 6. Section 7 concludes the paper.
2 A stopping criterion for active learning
What is the ideal stopping point for AL? Obviously,
annotation should be stopped at the latest when the
1The term base learner (configuration) refers to the combi-
nation of base learner, parameter settings, and data representa-
tion.
best classifier for a scenario is yielded. However, de-
pending on the scenario at hand, the ?best? classifier
could have different interpretations. In many papers
on AL and stopping criteria, the best (or optimal)
classifier is the one that yields the highest perfor-
mance on a test set. It is assumed that AL-based
annotation should be stopped as soon as this per-
formance is reached. This could be generalized as
stopping criteria based on maximal classifier perfor-
mance. In practice, the trade-off between annota-
tion effort and classifier performance is related to the
achievable performance given the learner configura-
tion and data under scrutiny: For instance, would we
invest many hours of additional annotation effort just
to possibly increase the classifier performance by a
fraction of a percent? In this context, a stopping cri-
terion may be based on classifier performance con-
vergence, and consequently, we can define the best
possible classifier to be one which cannot learn more
from the remaining pool of data.
The intrinsic stopping criterion (ISC) we propose
here focuses on the latter aspect of the ideal stop-
ping point described above ? exhaustiveness of the
AL pool. We suggest to stop the annotation process
of the data from a given pool when the base learner
cannot learn (much) more from it. The definition of
our intrinsic stopping criterion for committee-based
AL builds on the notions of Selection Agreement
(Tomanek et al, 2007), and Validation Set Agree-
ment (Tomanek and Hahn, 2008).
The Selection Agreement (SA) is the agreement
among the members of a decision committee re-
garding the classification of the most informative in-
stance selected from the pool of unlabeled data in
each AL round. The intuition underlying the SA is
that the committee will agree more on the hard in-
stances selected from the remaining set of unlabeled
data as the AL process proceeds. When the mem-
bers of the committee are in complete agreement,
AL should be aborted since it no longer contributes
to the overall learning process ? in this case, AL is
but a computationally expensive counterpart of ran-
dom sampling. However, as pointed out by Tomanek
et al (2007), the SA hardly ever signals complete
agreement and can thus not be used as the sole in-
dicator of AL having reached the point at which it
should be aborted.
The Validation Set Agreement (VSA) is the agree-
139
ment among the members of the decision commit-
tee concerning the classification of a held-out, unan-
notated data set (the validation set). The validation
set stays the same throughout the entire AL process.
Thus, the VSA is mainly affected by the perfor-
mance of the committee, which in turn, is grounded
in the information contained in the most informative
instances in the pool of unlabeled data. Tomanek
and colleagues argue that the VSA is thus a good
approximation of the (progression of the) learning
curve and can be employed as decision support for
knowing when to stop annotating ? from the slope of
the VSA curve one can read whether further annota-
tion will result in increased classifier performance.
We combine the SA and the VSA into a single
stopping criterion by relating the agreement of the
committee on a held-out validation set with that on
the (remaining) pool of unlabeled data. If the SA
is larger than the VSA, it is a signal that the deci-
sion committee is more in agreement concerning the
most informative instances in the (diminishing) un-
labeled pool than it is concerning the validation set.
This, in turn, implies that the committee would learn
more from a random sample2 from the validation set
(or from a data source exhibiting the same distribu-
tion of instances), than it would from the unlabeled
data pool. Based on this argument, a stopping crite-
rion for committee-based AL can be formulated as:
Active learning may be terminated when
the Selection Agreement is larger than, or
equal to, the Validation Set Agreement.
In relation to the stopping criterion based solely
on SA proposed by Tomanek et al (2007), the above
defined criterion comes into effect earlier in the
AL process. Furthermore, while it was claimed in
(Tomanek and Hahn, 2008) that one can observe the
classifier convergence from the VSA curve (as it ap-
proximated the progression of the learning curve),
that requires a threshold to be specified for the ac-
tual stopping point. The ISC is completely intrinsic
and does thus not require any thresholds to be set.
3 Related work
Schohn and Cohn (2000) report on document clas-
sification using AL with Support Vector Machines.
2The sample has to be large enough to mimic the distribution
of instances in the original unlabeled pool.
If the most informative instance is no closer to the
decision hyperplane than any of the support vectors,
the margin has been exhausted and AL is terminated.
Vlachos (2008) suggests to use classifier confi-
dence to define a stopping criterion for uncertainty-
based sampling. The idea is to stop learning when
the confidence of the classifier, on an external, pos-
sibly unannotated test set, remains at the same level
or drops for a number of consecutive iterations dur-
ing the AL process. Vlachos shows that the criterion
indeed is applicable to the tasks he investigates.
Zhu and colleagues (Zhu and Hovy, 2007;
Zhu et al, 2008a; Zhu et al, 2008b) introduce
max-confidence, min-error, minimum expected er-
ror strategy, overall-uncertainty, and classification-
change as means to terminate AL. They primar-
ily use a single-classifier approach to word sense
disambiguation and text classification in their ex-
periments. Max-confidence seeks to terminate AL
once the classifier is most confident in its predic-
tions. In the min-error strategy, the learning is halted
when there is no difference between the classifier?s
predictions and those labels provided by a human
annotator. The minimum expected error strategy
involves estimating the classification error on fu-
ture unlabeled instances and stop the learning when
the expected error is as low as possible. Overall-
uncertainty is similar to max-confidence, but unlike
the latter, overall-uncertainty takes into account all
data remaining in the unlabeled pool when estimat-
ing the uncertainty of the classifier. Classification-
change builds on the assumption that the most in-
formative instance is the one which causes the clas-
sifier to change the predicted label of the instance.
Classification-change-based stopping is realized by
Zhu and colleagues such that AL is terminated once
no predicted label of the instances in the unlabeled
pool change during two consecutive AL iterations.
Laws and Schu?tze (2008) investigate three ways
of terminating uncertainty-based AL for named en-
tity recognition ? minimal absolute performance,
maximum possible performance, and convergence.
The minimal absolute performance of the system
is set by the user prior to starting the AL process.
The classifier then estimates its own performance
using a held-out unlabeled data set. Once the per-
formance is reached, the learning is terminated. The
maximum possible performance strategy refers to
140
the optimal performance of the classifier given the
data. Once the optimal performance is achieved, the
process is aborted. Finally, the convergence crite-
rion aims to stop the learning process when the pool
of available data does not contribute to the classi-
fier?s performance. The convergence is calculated
as the gradient of the classifier?s estimated perfor-
mance or uncertainty. Laws and Schu?tze conclude
that both gradient-based approaches, that is, conver-
gence, can be used as stopping criteria relative to the
optimal performance achievable on a given pool of
data. They also show that while their method lends
itself to acceptable estimates of accuracy, it is much
harder to estimate the recall of the classifier. Thus,
the stopping criteria based on minimal absolute or
maximum possible performance are not reliable.
The work most related to ours is that of Tomanek
and colleagues (Tomanek et al, 2007; Tomanek and
Hahn, 2008) who define and evaluate the Selection
Agreement (SA) and the Validation Set Agreement
(VSA) already introduced in Section 2. Tomanek
and Hahn (2008) conclude that monitoring the
progress of AL should be based on a separate vali-
dation set instead of the data directly affected by the
learning process ? thus, VSA is preferred over SA.
Further, they find that the VSA curve approximates
the progression of the learning curve and thus clas-
sifier performance convergence could be estimated.
However, to actually find where to stop the annota-
tion, a threshold needs to be set.
Our proposed intrinsic stopping criterion is
unique in several ways: The ISC is intrinsic, relying
only on the characteristics of the base learner and
the data at hand in order to decide when the AL pro-
cess may be terminated. The ISC does not require
the user to set any external parameters prior to ini-
tiating the AL process. Further, the ISC is designed
to work with committees of classifiers, and as such,
it is independent of how the disagreement between
the committee members is quantified. The ISC does
neither rely on a particular base learner, nor on a par-
ticular way of creating the decision committee.
4 Experiments
To challenge the definition of the ISC, we conducted
two types of experiments concerning named entity
recognition. The primary focus of the first type
of experiment is on creating classifiers (classifier-
centric), while the second type is concerned with the
creation of annotated documents (data-centric). In
all experiments, the agreement among the decision
committee members is quantified by the Vote En-
tropy measure (Engelson and Dagan, 1996):
V E(e) = ? 1log k
?
l
V (l, e)
k log
V (l, e)
k (1)
where k is the number of members in the committee,
and V (l, e) is the number of members assigning la-
bel l to instance e. If an instance obtains a low Vote
Entropy value, it means that the committee members
are in high agreement concerning its classification,
and thus also that it is less a informative one.
4.1 Classifier-centric experimental settings
In common AL scenarios, the main goal of us-
ing AL is to create a good classifier with min-
imal label complexity. To follow this idea, we
select sentences that are assumed to be useful
for classifier training. We decided to select
complete sentences ? instead of, e.g., single to-
kens ? as in practice annotators must see the
context of words to decide on their entity labels.
Our experimental setting is based on the AL ap-
proach described by Tomanek et al (2007): The
committee consists of k = 3 Maximum Entropy
(ME) classifiers (Berger et al, 1996). In each AL
iteration, each classifier is trained on a randomly
drawn (sampling without replacement) subset L? ?
L with |L?| = 23L, L being the set of all instances la-
beled so far (cf. EnsembleGenerationMethod in Fig-
ure 1). Usefulness of a sentence is estimated as the
average token Vote Entropy (cf. Equation 1). In each
AL iteration, the 20 most useful sentences are se-
lected (n = 20 in Step 3 in Figure 1). AL is started
from a randomly chosen seed of 20 sentences.
While we made use of ME classifiers during the
selection, we employed an NE tagger based on Con-
ditional Random Fields (CRF) (Lafferty et al, 2001)
during evaluation time to determine the learning
curves. CRFs have a significantly higher tagging
performance, so the final classifier we are aiming
at should be a CRF model. We have shown be-
fore (Tomanek et al, 2007) that MEs are well apt as
selectors with the advantage of much shorter train-
ing times than CRFs. For both MEs and CRFs the
141
same features were employed which comprised or-
thographical (based mainly on regular expressions),
lexical and morphological (suffixed/prefixed, word
itself), syntactic (POS tags), as well as contextual
(features of neighboring tokens) ones.
The experiments on classifier-centric AL have
been performed on the English data set of cor-
pus used in the CoNLL-2003 shared task (Tjong
Kim Sang and Meulder, 2003). This corpus con-
sists of newspaper articles annotated with respect to
person, location, and organisation entities. As AL
pool we took the training set which consists of about
14,000 sentences (? 200, 000 tokens). As valida-
tion set and as gold standard for plotting the learn-
ing curve we used CoNLL?s evaluation corpus which
sums up to 3,453 sentences.
4.2 Data-centric experimental settings
While AL is commonly used to create as good
classifiers as possible, with the amount of human
effort kept to a minimum, it may result in frag-
mented and possibly non re-usable annotations (e.g.,
a collection of documents in which only some of
the names are marked up). This experiment con-
cerns a method of orchestrating AL in a way ben-
eficial for the bootstrapping of annotated data (Ols-
son, 2008). The bootstrapping proper is realized by
means of AL for selecting documents to annotate, as
opposed to sentences. This way the annotated data
set is comprised of entire documents thus promot-
ing data creation. As in the classifier-centric setting,
the task is to recognize names ? persons, organiza-
tions, locations, times, dates, monetary expressions,
and percentages ? in news wire texts. The texts
used are part of the MUC-7 corpus (Linguistic Data
Consortium, 2001) and consists of 100 documents,
3,480 sentences, and 90,790 tokens. The task is ap-
proached using the IOB tagging scheme proposed
by, e.g., Ramshaw and Marcus (1995), turning the
original 7-class task into a 15-class task. Each to-
ken is represented using a fairly standard menagerie
of features, including such stemming from the sur-
face appearance of the token (e.g., Contains dollar?
Length in characters), calculated based on linguis-
tic pre-processing made with the English Functional
Dependency Grammar (Tapanainen and Ja?rvinen,
1997) (e.g., Case, Part-of-speech), fetched from pre-
compiled lists of information (e.g., Is first name?),
and features based on predictions concerning the
context of the token (e.g, Class of previous token).
The decision committee is made up from 10
boosted decision trees using MultiBoostAB (Webb,
2000) (cf. EnsembleGenerationMethod in Figure 1).
Each classifier is created by the REPTree decision
tree learner described by Witten and Frank (2005).
The informativeness of a document is calculated by
means of average token Vote Entropy (cf. Equa-
tion 1). The seed set of the AL process consists of
five randomly selected documents. In each AL iter-
ation, one document is selected for annotation from
the corpus (n = 1 in Step 3 in Figure 1).
5 Results
Two different scenarios were used to illustrate the
applicability of the proposed intrinsic stopping cri-
terion. In the first scenario, we assumed that the
pool of unlabeled data was static and fairly large.
In the second scenario, we assumed that the unla-
beled data would be collected in smaller batches as
it was made available on a stream, for instance, from
a news feed. Both the classifier-centric and the data-
centric experiments were carried out within the first
scenario. Only the classifier-centric experiment was
conducted in the stream-based scenario.
In the classifier-centric setting, the SA is defined
as (1 ? Vote Entropy) for the most informative in-
stances in the unlabeled pool, that is, the per-token
average Vote Entropy on the most informative sen-
tences. Analogously, in the data-centric setting, the
SA is defined as (1 ? Vote Entropy) for the most in-
formative document ? here too, the informativeness
is calculated as the per-token average Vote Entropy.
In both settings, the VSA is the per-token average
Vote Entropy on the validation set.
5.1 AL on static pools
The intersection of the SA and VSA agreement
curves indicates a point at which the AL process
may be terminated without (a significant) loss in
classifier performance. For both AL scenarios (data-
and classifier-centric) we plot both the learning
curves for AL and random selection, as well as the
SA and VSA curve for AL. In both scenarios, these
142
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  25000  50000  75000  100000  125000  150000
Vo
te
 E
nt
ro
py
Number of tokens in the training set
C
Selection agreement
Validation set agreement
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  25000  50000  75000  100000  125000  150000
F-
sc
or
e
Baseline
Query by committee
Figure 2: Classifier-centric AL experiments on the
CoNLL corpus. The intersection, C, corresponds to the
point where (almost) no further improvement in terms
of classifier performance can be expected. The baseline
learning curve shows the results of learning from ran-
domly sampled data.
curves are averages over several runs.3
The results from the classifier-centric experiment
on the CoNLL corpus are presented in Figure 2.
AL clearly outperforms random selection. The AL
curve converges at a maximum performance of F ?
84% after about 125,000 tokens. As expected, the
SA curve drops from high values in the beginning
down to very low values in the end where hardly
any interesting instances are left in the pool. The
intersection (C) with the VSA curve is very close to
the point (125,000 tokens) where no further increase
of performance can be reached by additional anno-
tation making it a good stopping point.
The results from the data-centric experiment are
available in Figure 3. The bottom part shows the
SA and VSA curves. The ISC occurs at the inter-
section of the SA and VSA curves (C), which corre-
sponds to a point well beyond the steepest part of the
learning curve. While stopping the learning at C re-
sults in a classifier with performance inferior what is
maximally achievable, stopping at C arguably corre-
3The classifier-centric experiments are averages over three
independent runs. The data-centric experiments are averages
over ten independent runs.
 0.05
 0.06
 0.07
 0.08
 0.09
 0.1
 0.11
 0.12
 0.13
 0.14
 0  10  20  30  40  50  60  70  80  90
Vo
te
 E
nt
ro
py
Number of documents in the training set
C
Selection agreement
Validation set agreement
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  10  20  30  40  50  60  70  80  90
F-
sc
or
e
Baseline
Query by boosting
Figure 3: Data-centric AL experiments on the MUC-7
corpus. The intersection, C, corresponds to a point at
which the AL curve has almost leveled out. The base-
line learning curve shows the results of learning from ran-
domly sampled data.
sponds to a plausible place to abort the learning. The
optimal performance is F ? 83.5%, while the ISC
corresponds to F ? 82%.
Keep in mind that the learning curves with which
the ISC are compared are not available in a practical
situation, they are included in Figures 2 and 3 for the
sake of clarity only.
5.2 AL on streamed data
One way of paraphrasing the ISC is: Once the in-
tersection between the SA and VSA curves has been
reached, the most informative instances remaining
in the pool of unlabeled data are less informative to
the classifier than the instances in the held-out, unla-
beled validation set are on average. This means that
the classifier would learn more from a sufficiently
large sample taken from the validation set than it
would if the AL process continued on the remain-
ing unlabeled pool.4
As an illustration of the practical applicability of
the ISC consider the following scenario. Assume
4Note however, that the classifier might still learn from the
instances in the unlabeled pool ? applying the ISC only means
that the classifier would learn more from a validation set-like
distribution of instances.
143
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0  2000  4000  6000  8000  10000  12000  14000  16000  18000  20000
F-
sc
or
e
Number of tokens in the training set
C1
C2
C3
Partition 1
Partition 2
Partition 3
Partition 4
Figure 4: AL curves for the four partitions used in the ex-
periments on streamed data. Ci denotes a point at which
the AL is terminated for partition i and a new partition is
employed instead. C1 corresponds to the ISC plotted in
the graph labeled Partition 1 in Figure 5, C2 to the ISC in
Partition 2, and C3 to the ISC in Partition 3.
that we are collecting data from a stream, for in-
stance items taken from a news feed. Thus, the data
is not available on the form of a closed set, but rather
an open one which grows over time. To make the
most of the human annotators in this scenario, we
want them to operate on batches of data instead of
annotating individual news items as they are pub-
lished. The purpose of the annotation is to mark up
names in the texts in order to train a named entity
recognizer. To do so, we wait until there has ap-
peared a given number of sentences on the stream,
and then collect those sentences. The problem is,
how do we know when the AL-based annotation
process for each such batch should be terminated?
We clearly do not want the annotators to annotate
all sentences, and we cannot have the annotators
set new thresholds pertaining to the absolute per-
formance of the named entity recognizer for each
new batch of data available. By using the ISC, we
are able to automatically issue a halting of the AL
process (and thus also the annotation process) and
proceed to the next batch of data without losing too
much in performance, and without having the anno-
tators mark up too much of the available data. To
this end, the ISC seems like a reasonable trade-off
between annotation effort and performance gain.
To carry out this experiment we took a sub sample
of 10% (1,400 sentences) from the original AL pool
of the CoNLL corpus as validation set.5 The rest of
5Note that the original CoNLL test set was not used in this
1000 4000 7000
0.
0
0.
2
Partition 1
Tokens
Vo
te
 E
nt
ro
py
SA
VSA
5000 8000
0.
00
0.
10
0.
20
Partition 2
Tokens
Vo
te
 E
nt
ro
py
SA
VSA
8000 14000
0.
00
0.
10
0.
20
Partition 3
Tokens
Vo
te
 E
nt
ro
py
SA
VSA
14000 18000
0.
00
0.
10
0.
20
Partition 4
Tokens
Vo
te
 E
nt
ro
py
SA
VSA
Figure 5: The SA and VSA curves for the four data par-
titions used in the experiment on streamed data. Each
intersection ? ISC ? corresponds to a point where AL is
terminated.
this pool was split into batches of about 500 con-
secutive sentences. Classifier-centric AL was now
run taking the first batch as pool to select from. At
the point where the SA and VSA curve crossed, we
continued AL selection from the next batch and so
forth. Figure 4 shows the learning curve for a simu-
lation of the scenario described above. The inter-
section between the SA and VSA curves for par-
tition 1 as depicted in Figure 5 corresponds to the
first ?step? (ending in C1) in the stair-like learning
curve in Figure 4. The step occurs after 4,641 to-
kens. Analogously, the other steps (ending in C2 and
C3, respectively) in the learning curve corresponds
the intersection between the SA and VSA curves for
partitions 2 and 3 in Figure 5. The intersection for
partition 4 corresponds to the point were we would
have turned to the next partition. This experiment
was stopped after 4 partitions.
Table 1 shows the accumulated number of sen-
tences and tokens (center columns) that required an-
notation in order to reach the ISC for each partition.
In addition, the last column in the table shows the
number of sentences (of the 500 collected for inclu-
experiment, thus the F-score reported in Figure 4 cannot be
compared to that in Figure 2.
144
Partition Sents Toks Sentences per partition
1 320 4,641 320
2 580 7,932 260
3 840 13,444 260
4 1070 16,751 230
Table 1: The number of tokens and sentences required to
reach the ISC for each partition.
sion in each partition) needed to reach the ISC ? each
new partition contributes less to the increase in per-
formance than the preceding ones.
6 Discussion
We have argued that one interpretation of the ISC
is that it constitutes the point where the informa-
tiveness on the remaining part of the AL pool is
lower than the informativeness on a different and
independent data set with the same distribution. In
the first AL scenario where there is one static pool
to select from, reaching this point can be inter-
preted as an overall stopping point for annotation.
Here, the ISC represents a trade-off between the
amount of data annotated and the classifier perfor-
mance obtained such that the resulting classifier is
nearly optimal with respect to the data at hand. In
the second, stream-based AL scenario where several
smaller partitions are consecutively made available
to the learner, the ISC serves as an indicator that the
annotation of one batch should be terminated, and
that the mark-up should proceed with the next batch.
The ISC constitutes an intrinsic way of determin-
ing when to stop the learning process. It does not
require any external parameters such as pre-defined
thresholds to be set, and it depends only on the char-
acteristics of the data and base learner at hand. The
ISC can be utilized to relate the performance of the
classifier to the performance that is possible to ob-
tain by the data and learner at hand.
The ISC can not be used to estimate the perfor-
mance of the classifier. Consequently, it can not be
used to relate the classifier?s performance to an ex-
ternally set level, such as a particular F-score pro-
vided by the user. In this sense, the ISC may serve as
a complement to stopping criteria requiring the clas-
sifier to achieve absolute performance measures be-
fore the learning process is aborted, for instance the
max-confidence proposed by Zhu and Hovy (2007),
and the minimal absolute performance introduced
by Laws and Schu?tze (2008).
7 Conclusions and Future Work
We have defined and empirically tested an intrinsic
stopping criterion (ISC) for committee-based AL.
The results of our experiments in two named en-
tity recognition scenarios show that the stopping cri-
terion is indeed a viable one, which represents a
fair trade-off between data use and classifier perfor-
mance. In a setting in which the unlabeled pool of
data used for learning is static, terminating the learn-
ing process by means of the ISC results in a nearly
optimal classifier. The ISC can also be used for de-
ciding when the pool of unlabeled data needs to be
refreshed.
We have focused on challenging the ISC with re-
spect to named entity recognition, approached in
two very different settings; future work includes ex-
periments using the ISC for other tasks. We be-
lieve that the ISC is likely to work in AL-based ap-
proaches to, e.g., part-of-speech tagging, and chunk-
ing as well. It should be kept in mind that while
the types of experiments conducted here concern
the same task, the ways they are realized differ in
many respects: the ways the decision committees
are formed, the data sets used, the representation of
instances, the relation between the sample size and
the instance size, as well as the pre-processing tools
used. Despite these differences, which outnumbers
the similarities, the ISC proves a viable stopping cri-
terion.
An assumption underlying the ISC is that the ini-
tial distribution of instances in the pool of unlabeled
data used for learning, and the distribution of in-
stances in the validation set are the same (or at least
very similar). Future work also includes investiga-
tions of automatic ways to ensure that this assump-
tion is met.
Acknowledgements
The first author was funded by the EC project
COMPANIONS (IST-FP6-034434), the second au-
thor was funded by the EC projects BOOTStrep
(FP6-028099) and CALBC (FP7-231727).
145
References
Markus Becker and Miles Osborne. 2005. A Two-Stage
Method for Active Learning of Statistical Grammars.
In Proc 19th IJCAI, Edinburgh, Scotland, UK.
Adam Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistics, 22(1):39?71.
Sean P. Engelson and Ido Dagan. 1996. Minimizing
Manual Annotation Cost In Supervised Training From
Corpora. In Proc 34th ACL, Santa Cruz, California,
USA.
Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006.
Large-Scale Text Categorization by Batch Mode Ac-
tive Learning. In Proc 15th WWW, Edinburgh, Scot-
land.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proc 18th ICML, Williamstown, Massachusetts, USA.
Florian Laws and Hinrich Schu?tze. 2008. Stopping Cri-
teria for Active Learning of Named Entity Recogni-
tion. In Proc 22nd COLING, Manchester, England.
David D. Lewis and William A. Gale. 1994. A Sequen-
tial Algorithm for Training Text Classifiers. In Proc
17th ACM-SIGIR, Dublin, Ireland.
Linguistic Data Consortium. 2001. Message understand-
ing conference (muc) 7. LDC2001T02. FTP FILE.
Philadelphia: Linguistic Data Consortium.
Fredrik Olsson. 2008. Bootstrapping Named Entity An-
notation by means of Active Machine Learning ? A
Method for Creating Corpora. Ph.D. thesis, Depart-
ment of Swedish, University of Gothenburg.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
Chunking using Transformation Based Learning. In
Proc 3rd VLC, Massachusetts Institute of Technology,
Cambridge, Massachusetts, USA.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active Learning for Part-of-
Speech Tagging: Accelerating Corpus Annotation. In
Proc Linguistic Annotation Workshop, Prague, Czech
Republic.
Greg Schohn and David Cohn. 2000. Less is More: Ac-
tive Learning with Support Vector Machines. In Proc
17th ICML, Stanford University, Stanford, California,
USA.
Hinrich Schu?tze, Emre Velipasaoglu, and Jan O. Peder-
sen. 2006. Performance Thresholding in Practical
Text Classification. In Proc 15th CIKM, Arlington,
Virginia, USA.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-Criteria-based Active Learn-
ing for Named Entity Recognition. In Proc 42nd ACL,
Barcelona, Spain.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A Non-
Projective Dependency Parser. In Proc 5th ANLP,
Washington DC, USA.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active Learning for Natural
Language Parsing and Information Extraction. In Proc
16th ICML, Bled, Slovenia.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 Shared Task: Language
Independent Named Entity Recognition. In Proc 7th
CoNLL, Edmonton, Alberta, Canada.
Katrin Tomanek and Udo Hahn. 2008. Approximating
Learning Curves for Active-Learning-Driven Annota-
tion. In Proc 6th LREC, Marrakech, Morocco.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An Approach to Text Corpus Construction
which Cuts Annotation Costs and Maintains Reusabil-
ity of Annotated Data. In Proc Joint EMNLP-CoNLL,
Prague, Czech Republic.
Andreas Vlachos. 2008. A Stopping Criterion for
Active Learning. Computer, Speech and Language,
22(3):295?312, July.
Geoffrey I. Webb. 2000. MultiBoosting: A Tech-
nique for Combining Boosting and Wagging. Machine
Learning, 40(2):159?196, August.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools with Java Implementa-
tions. 2nd Edition. Morgan Kaufmann, San Fransisco.
Jingbo Zhu and Eduard Hovy. 2007. Active Learning
for Word Sense Disambiguation with Methods for Ad-
dressing the Class Imbalance Problem. In Proc Joint
EMNLP-CoNLL, Prague, Czech Republic.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008a.
Learning a Stopping Criterion for Active Learning for
Word Sense Disambiguation and Text Classification.
In Proc 3rd IJCNLP, Hyderabad, India.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008b.
Multi-Criteria-based Strategy to Stop Active Learning
for Data Annotation. In Proc 22nd COLING, Manch-
ester, England.
146
Proceedings of the Workshop on BioNLP, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
How Feasible and Robust is the Automatic Extraction of Gene Regulation
Events ? A Cross-Method Evaluation under Lab and Real-Life Conditions
Udo Hahn1 Katrin Tomanek1 Ekaterina Buyko1 Jung-jae Kim2 Dietrich Rebholz-Schuhmann2
1Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{udo.hahn|katrin.tomanek|ekaterina.buyko}@uni-jena.de
2EMBL-EBI, Wellcome Trust Genome Campus, Hinxton, Cambridge, UK
{kim|rebholz}@ebi.ac.uk
Abstract
We explore a rule system and a machine learn-
ing (ML) approach to automatically harvest
information on gene regulation events (GREs)
from biological documents in two different
evaluation scenarios ? one uses self-supplied
corpora in a clean lab setting, while the other
incorporates a standard reference database of
curated GREs from REGULONDB, real-life
data generated independently from our work.
In the lab condition, we test how feasible
the automatic extraction of GREs really is
and achieve F-scores, under different, not di-
rectly comparable test conditions though, for
the rule and the ML systems which amount
to 34% and 44%, respectively. In the REGU-
LONDB condition, we investigate how robust
both methodologies are by comparing them
with this routinely used database. Here, the
best F-scores for the rule and the ML systems
amount to 34% and 19%, respectively.
1 Introduction
The extraction of binary relations from biomedical
text has caught much attention in the recent years.
Progress on this and other tasks has been monitored
in challenge competitions such as BIOCREATIVE I
and II,1 which dealt with gene/protein names and
and protein-protein interaction.
The BIOCREATIVE challenge and other related
ones have shown at several occasions that partici-
pants continue to use two fundamentally different
1http://biocreative.sourceforge.net/
systems: symbolic pattern-based systems (rule sys-
tems), on the one hand, and feature-based statisti-
cal machine learning (ML) systems, on the other
hand. This has led to some rivalry with regard to the
interpretation of their performance data, the costs
of human efforts still required and their scalability
for the various tasks. While rule systems are of-
ten hand-crafted and fine-tuned to a particular ap-
plication (making a major manual rewrite often nec-
essary when the application area is shifted), ML
systems are trained automatically on manually an-
notated corpora, i.e., without manual intervention,
and thus have the advantage to more easily adapt to
changes in the requested identification tasks. Time
costs (human workload) are thus shifted from rule
design and adaptation to metadata annotation.
Text mining systems as usually delivered by
BioNLP researchers render biologically relevant en-
tities and relations on a limited set of test documents
only. While this might be sufficient for the BioNLP
community, it is certainly insufficient for bioinfor-
maticians and molecular biologists since they re-
quire large-scale data with high coverage and reli-
ability. For our analysis, we have chosen the topic
of gene regulatory events in E. coli, which is a do-
main of very active research and grand challenges.2
Currently the gold standard of the existing body of
knowledge of such events is represented by the fact
database REGULONDB.3 Its content has been man-
2The field of gene regulation is one of the most prominent
topics of research and often mentioned as one of the core fields
of future research in molecular biology (cf, e.g., the Grand
Challenge I-2 described by Collins et al (2003)).
3http://regulondb.ccg.unam.mx/
37
ually gathered from the scientific literature and de-
scribes the curated computational model of mecha-
nisms of transcriptional regulation in E. coli. Having
this gold standard in mind, we face the challenging
task to automatically reproduce this content from the
available literature, to enhance this content with re-
liable additional information and to update this re-
source as part of a regular automatic routine.
Hence, we first explore the feasibility and per-
formance of a rule-based and an ML-based system
against special, independently created corpora that
were generated to enable measurements under clean
experimental lab conditions. This part, due to dif-
ferent experimental settings, is not meant as a com-
parison between both approaches though. We then
move to the even more demanding real-life scenario
where we evaluate and compare these solutions for
the identification of gene regulatory events against
the REGULONDB data resource. This approach tar-
gets the robustness of the proposed text mining so-
lutions from the perspectives of completeness, cor-
rectness and novelty of the generated results.
2 Related Work
Considering relation extraction (RE) in the biomed-
ical domain, there are only few studies which deal
primarily with gene regulation. Yang et al (2008)
focus on the detection of sentences that contain
mentions of transcription factors (proteins regulat-
ing gene expression). They aim at the detection
of new transcription factors, while relations are not
taken into account. In contrast, S?aric? et al (2004)
extract gene regulatory networks and achieve in the
RE task an accuracy of up to 90%. They disregard,
however, ambiguous instances, which may have led
to the low recall around 20%. The Genic Interaction
Extraction Challenge (Ne?dellec, 2005) was orga-
nized to determine the state-of-the-art performance
of systems designed for the detection of gene regula-
tion interactions. The best system achieved a perfor-
mance of about 50% F-score. The results, however,
have to be taken with care as the LLL corpus used in
the challenge is of extremely limited size.
3 Extraction of Gene Regulation Events
Gene regulation is a complex cellular process that
controls the expression of genes. These genes are
then transcribed into their RNA representation and
later translated into proteins, which fulfill various
tasks such as maintaining the cell structure, enabling
the generation of energy and interaction with the en-
vironment.
The analysis of the gene regulatory processes is
ongoing research work in molecular biology and af-
fects a large number of research domains. In par-
ticular the interpretation of gene expression profiles
from microarray analyses could be enhanced using
our understanding of gene regulation events (GREs)
from the literature.
We approach the task of the automatic extraction
of GREs from literature from two different method-
ological angles. On the one hand, we provide a set of
hand-crafted rules ? both for linguistic analysis and
conceptual inference (cf. Section 3.1), the latter be-
ing particularly helpful in unveiling only implicitly
stated biological knowledge. On the other hand, we
supply a machine learning-based system for event
extraction (cf. Section 3.2). No regularities are spec-
ified a priori by a human although, at least in the su-
pervised scenario we have chosen, this approach re-
lies on training data supplied by human (expert) an-
notators who provide sufficiently many instances of
ground truth decisions from which regularities can
automatically be learnt. At the level of system per-
formance, rules tend to foster precision at the cost
of recall and ML systems tend to produce inverse
figures, while there is no conclusive evidence for or
against any of these two approaches.
The extraction of GREs, independent of the ap-
proach one subscribes to, is a complex problem
composed of a series of subtasks. Abstracting away
from lots of clerical and infrastructure services (e.g.,
sentence splitting, tokenization) at the core of any
GRE extraction lie the following basic steps:
? the identification of pairs of gene mentions as
the arguments of a relation ? the well-known
named entity recognition and normalization
task,
? the decision whether the entity pair really con-
stitutes a relation,
? and the identification of the roles of the argu-
ments in the relation which implicitly amounts
to characterize each argument as either agent or
patient.
38
3.1 Rule-based Extraction
The rule-based system extracts GREs from text em-
ploying logical inference. The motivation of using
inference is that the events under scrutiny are often
expressed in text in either a compositional or an in-
complete way. We address this issue by composi-
tionally representing textual semantics and by log-
ically inferring implicit meanings of text over the
compositional representation of textual semantics.
Entity Identification. The system first recognizes
named entities of the types that can be participants of
the target events. We have collected 15,881 E. coli
gene/protein and operon names from REGULONDB
and UNIPROT. Most of the gene/protein names are
associated with UNIPROT identifiers. An operon in
prokaryotes is a DNA sequence with multiple genes
whose expression is controlled by a shared promoter
and which thus express together. We have mapped
the operon names to corresponding gene sets.
Named entity recognition relies on the use of dic-
tionaries. If the system recognizes an operon name,
it then associates the operon with its genes. The
system further recognizes multi-gene object names
(e.g., ?acrAB?), divides them into individual gene
names (e.g., ?acrA?, ?acrB?) and associates the gene
names with the multi-gene object names.
Relation Identification. The system then iden-
tifies syntactic structures of sentences in an in-
put corpus by utilizing the ENJU parser (Sagae et
al., 2007). The ENJU parser generates predicate-
argument structures, and the system converts them
into dependency structures.
The system then analyzes the semantics of the
sentences by matching syntactic-semantic patterns
to the dependency structures. We constructed 1,123
patterns for the event extraction according to the fol-
lowing workflow. We first collected keywords re-
lated to gene regulation, from GENE ONTOLOGY,
INTERPRO, WORDNET, and several papers about
information extraction from biomedical literature
(Hatzivassiloglou and Weng, 2002; Kim and Park,
2004; Huang et al, 2004). Then we collected sub-
categorization frames for each keyword and created
patterns for the frames manually.
Each pattern consists of a syntactic pattern and
a semantic pattern. The syntactic patterns com-
ply with dependency structures. The system tries
to match the syntactic patterns to the dependency
structures of sentences in a bottom-up way, consid-
ering syntactic and semantic restrictions of syntac-
tic patterns. Once a syntactic pattern is successfully
matched to a sub-tree of the available dependency
structure, its corresponding semantic pattern is as-
signed to the sub-tree as one of its semantics. The
semantic patterns are combined according to the de-
pendency structures to form a compositional seman-
tic structure.
The system then performs logical inference over
the semantic structures by using handcrafted infer-
ence rules and extracts target information from the
results of the inference. We have manually created
28 inference rules that reflect the knowledge of the
gene regulation domain. Only relations where the
identified agent is one of those known TFs are kept,
while all others are discarded.
3.2 Generic, ML-based Extraction
Apart from the already mentioned clerical pre-
processing steps, the ML-based extraction of GREs
requires several additional syntactic processing
steps including POS-tagging, chunking, and full
dependency- and constituency-based parsing.4
Entity Identification. To identify gene names in
the documents, we applied GENO, a multi-organism
gene name recognizer and normalizer (Wermter
et al, 2009) which achieved a top-rank perfor-
mance of 86.4% on the gene normalization task
of BIOCREATIVE-II. GENO recognizes gene men-
tions by means of an ML-based named entity tag-
ger trained on publicly available corpora. Subse-
quently, it attempts to map all identified mentions to
organism-specific UNIPROT5 identifiers. Mentions
that cannot be mapped are discarded; only success-
fully mapped mentions are kept. We utilized GENO
in its original version, i.e., without special adjust-
ments to the E. coli organism. However, only those
mentions detected to be genes of E. coli were fed
into the relation extraction component.
4These tasks were performed with the OPENNLP tools
(http://opennlp.sourceforge.net/) and the
MST parser (http://sourceforge.net/projects/
mstparser), both retrained on biomedical corpora.
5http://www.uniprot.de
39
Relation Identification. The ML-based approach
to GRE employs Maximum Entropy models and
constitutes and extension of the system proposed by
Buyko et al (2008) as it also makes use of depen-
dency parse information including dependency tree
level features (Katrenko and Adriaans, 2006) and
shortest dependency path features (Kim et al, 2008).
In short, the feature set consists of:
? word features (covering words before, after and
between both entity mentions);
? entity features (accounting for combinations of
entity types, flags indicating whether mentions
have an overlap, and their mention level);
? chunking and constituency-based parsing fea-
tures (concerned with head words of the
phrases between two entity mentions; this class
of features exploits constituency-based parsing
as well and indicates, e.g., whether mentions
are in the same NP, PP or VP);
? dependency parse features (analyzing both the
dependency levels of the arguments as dis-
cussed by Katrenko and Adriaans (2006) and
dependency path structure between the argu-
ments as described by Kim et al (2008));
? and relational trigger (key)words (accounting
for the connection of trigger words and men-
tions in a full parse tree).
An advantage of ML-based systems is that they
allow for thresholding. To achieve higher recall
values for our system, we may set the confidence
threshold for the negative class (i.e., a pair of en-
tity mentions does not constitute a relation) to values
> 0.5. Clearly, this is at the cost of precision as the
system more readily assigns the positive class.
4 Intrinsic Evaluation of Feasibility
The following two sections aim at evaluating the
rule-based and ML-based GRE extraction systems.
The systems are first ?intrinsically? evaluated, i.e.,
in a cross-validation manner on corpora annotated
with respect to GREs. Second, in a more realistic
scenario, both systems were evaluated against REG-
ULONDB, a database collecting knowledge about
gene regulation in E. coli. This scenario tests which
part of manually accumulated knowledge about gene
regulation in E. coli can automatically be identified
by our systems and at what level of quality.
4.1 Rule-based system
Corpus. For the training and evaluation of the
rule-based system, we annotated 209 MEDLINE ab-
stracts with three types of events: specific events
of gene transcription regulation, general events of
gene expression regulation, and physical events of
binding of transcription factors to gene regulatory
regions. Strictly speaking, only the first type is rele-
vant to REGULONDB. However, biologists often re-
port gene transcription regulation events in the sci-
entific literature as if they are gene expression regu-
lation events, which is a generalization of gene tran-
scription regulation, or the binding event, which it-
self is insufficient evidence for gene transcription
regulation. The two latter types may indicate that
the full-texts contain evidence of the first type.
We asked two curators to annotate the abstracts.
Curator A was trained with example annotations and
interactive discussions. Curator B was trained only
with example annotations and guidelines. For cross-
checking of annotations, we asked them to annotate
an unseen corpus of 97 abstracts and found that Cu-
rator A made 10.8% errors, misjudging three event
additions and, in the other 14 errors, mistaking in
annotating event types, event attributes, and pas-
sage boundaries, while Curator B made 32.4% er-
rors as such. This result indicates that the annotation
of GREs requires intensive and interactive training.
The curators have discussed and agreed on the final
release of the corpora.6
Results. The system has successfully extracted 79
biologically meaningful events among them (21.1%
recall) and incorrectly produced 15 events (84.0%
precision) which constitutes an overall F-score of
33.6%. Among the 79 events, the system has cor-
rectly identified event types of 39 events (49.4% pre-
cision), polarity of 46 events (58.2% precision), and
directness of 51 events (64.6% precision). Note that
the system employed a fully automatic module for
named entity recognition. The event type recogni-
tion is impaired, because it often fails to recognize
6The resultant annotated corpora are available at http://
www.ebi.ac.uk/?kim/eventannotation/.
40
the specific event type of transcription regulation,
but only identifies the general event type of gene ex-
pression regulation due to the lack of identified evi-
dence.
4.2 ML-based system
GeneReg corpus. The GENEREG corpus (Buyko
et al, 2008) constitutes a selection of 314 MED-
LINE abstracts related to gene regulation in E. coli.
These abstracts were randomly drawn from a set of
32,155 selected by MESH term queries from MED-
LINE using keywords such as Escherichia coli, Gene
Expression and Transcription Factors. These 314
abstracts were manually annotated for named enti-
ties involved in gene regulatory processes (such as
transcription factor, including co-factors and regu-
lators, and genes) and pairwise relations between
transcription factors (TFs) and genes, as well as trig-
gers (e.g., clue verbs) essential for the description of
gene regulation relations. As for the relation types,
the GENEREG corpus distinguishes between (a) un-
specified regulation of gene expression, (b) positive,
and (c) negative regulation of gene expression. Out
of the 314 abstracts a set of 65 were randomly se-
lected and annotated by a second annotator to iden-
tify inter-annotator agreement (IAA) values. For the
task of correct identification of the pair of interacting
named entities in gene regulation processes, an IAA
of 78.4% (R), 77.3% (P ), 77.8% (F) was measured ,
while 67% (R), 67.9% (P), 67.4% (F) were achieved
for the identification of interacting pairs plus the 3-
way classification of the interaction relation.
Experimental Setting. The ML-based extraction
system merges all of the above mentioned three
types (unspecific, negative and positive) into one
common type ?relation of gene expression?. So, it
either finds that there is a relation of interest be-
tween a pair of gold entity mentions or not. We
evaluated our system by a 5-fold cross-validation on
the GENEREG corpus. The fold splits were done
on the abstract-level to avoid the otherwise unrealis-
tic scenario where a system is trained on sentences
from an abstract and evaluated on other sentences
but from the same abstract (Pyysalo et al, 2008).
As our focus here is only on the performance of the
GRE extraction component, gold entity mentions as
annotated in the respective corpus were used.
Results. For the experimental settings given
above, the system achieved an F-score of 42% with
a precision of 59% and a recall of 33%. Increasing
the confidence threshold for the negative class in-
creases recall as shown for two different thresholds
in Table 1. As expected this is at the cost of preci-
sion. It shows, that using an extremely high thresh-
old of 0.95 results in a dramatically increased recall
of 73% compared to 33% with the default threshold.
Although at the cost of diminished precision of 32%
compared to originally 59%, the lifted threshold in-
creases the overall F-score (44%) by 2 points.
threshold R P F
default (0.5) 0.33 0.59 0.42
0.80 0.54 0.43 0.48
0.95 0.73 0.32 0.44
Table 1: Different confidence thresholds for the ML-
based system achieved by intrinsic evaluation
5 Extrinsic Evaluation of Robustness
REGULONDB is the primary and largest reference
database providing manually curated knowledge of
the transcriptional regulatory network of E. coli
K12. On K12, approximately for one-third of K12?s
genes, information about their regulation is avail-
able. REGULONDB is updated with content from
recent research papers on this issue. While REG-
ULONDB contains much more, for this paper our
focus was solely on REGULONDB?s information
about gene regulation events in E. coli. In the fol-
lowing, the term REGULONDB refers to this part of
the REGULONDB database. REGULONDB includes
e.g., the following information for each regulation
event: regulatory gene (the ?agent? in such an event,
a transcription factor), the regulated gene (the ?pa-
tient?), the regulatory effect on the regulated gene
(activating, suppression, dual, unknown), and evi-
dence that supports the existence of the regulatory
interaction.
Evaluation against REGULONDB constitutes a
real-life scenario. Thus, the complete extraction sys-
tems were run, including gene name recognition and
normalization as well as relation detection. Hence,
the systems? overall recall values are highly affected
by the gene name identification. REGULONDB is
here taken as a ?true? gold standard and thus as-
41
sumed to be correct and exhaustive with respect to
the GREs contained. As, however, every manu-
ally curated database is likely to be incomplete and
might contain some errors, we supplement our eval-
uation against REGULONDB with a manual analy-
sis of false positives errors caused by our system (cf.
Section 5.4).
5.1 Evaluation Scenario and Experimental
Settings
To evaluate our extraction systems against REG-
ULONDB we first processed a set of input docu-
ments (see below), collected all unique gene reg-
ulation events extracted and compared this set of
events against the full set of known events in REG-
ULONDB. A true positive (TP) hit is obtained, when
an event found automatically corresponds to one in
REGULONDB, i.e., having the same agent and pa-
tient. The type of regulation is not considered. A
false positive (FP) hit is counted, if an event was
found which does not occur in the same way in
REGULONDB, i.e., either patient or agent (or both)
are wrong. False negatives (FN) are those events
covered by REGULONDB but not found by a sys-
tem automatically. From these hit values, standard
precision, recall, and F-score values are calculated.
Of course, the systems? performance largely depend
on the size of the base corpus collection processed.
Thus, for both systems and all three document sets
we got separate performance scores.
Table 2 gives an overview to the document col-
lections used for evaluating the robustness of our
systems: The ?ecoli-tf? variants are documents fil-
tered both with E. coli TF names and with relevance
to E. coli. Abstracts are taken from Medline cita-
tions, while full texts are from a corpus of different
biomedical journals. The third document set, ?regu-
lon ra?, is a set containing abstracts from the REG-
ULONDB references.
name type # documents
ecoli-tf.abstracts abstract 4,347
ecoli-tf.fulltext full texts 1,812
regulon ra abstracts 2,704
Table 2: Document sets for REGULONDB evaluation
5.2 Rule-based-System
Table 3 shows the evaluation results of the rule-
based system against REGULONDB. Though the
system distinguishes the three types of events, we
have considered them all as events of gene tran-
scription regulation for the evaluation. For instance,
the system has extracted 718 unique events with
single-unit participants (i.e., excluding operons), not
considering event types and attributes (e.g., polar-
ity), from the ?ecoli-tf.fulltext? corpus. Among the
events, 347 events are found in Regulon (9.7% re-
call, 48.3% precision). If we only consider the
events that are specifically identified as gene tran-
scription regulation, the system has extracted 379
unique events among which 201 are also found in
Regulon (5.6% recall, 53.0% precision).
participant document set R P F
single-unit ecoli-tf.abstracts 0.09 0.60 0.15
multi-unit ecoli-tf.abstracts 0.24 0.61 0.34
single-unit ecoli-tf.fulltext 0.10 0.48 0.16
multi-unit ecoli-tf.fulltext 0.25 0.49 0.33
single-unit regulon ra 0.07 0.73 0.13
multi-unit regulon ra 0.18 0.70 0.28
Table 3: Results of evaluation against REGULONDB of
rule-based system.
When we split multi-unit participants into individ-
ual genes, the rule-based system shows better per-
formance, as shown in Table 3 with the participant
type ?multi-unit?. This may indicate that the gene
regulatory events of E. coli are often described as
interactions of operons. At best, the system shows
34% F-score with the ?ecoli-tf.abstracts? corpus.
5.3 ML-based System
The ML-based system was designed to recognize
all types of gene regulation events. REGULONDB,
however, contains only the subtype, i.e., regulation
of transcription. Thus, the ML-based system was
evaluated against REGULONDB in two modes: by
default, all events extracted by the systems are con-
sidered; in the ?TF-filtered? mode, only relations
with an agent from the list of all known TFs in E.
coli are considered (as done for the rule-based sys-
tem by default). Thus, comparing to the rule-based
system, only the results obtained in the ?TF-filtered?
mode should be considered.
42
5.3.1 Raw performance scores
The results for the ML-based system are shown in
Table 4. Recall values here range between 7 and
10%, while precision is between 29 and 78% de-
pending on both the document set as well as the
application of the TF filter. The low recall of the
ML-based system is partially due to the fact that the
system does not recognize multi-gene object names
(e.g., ?acrAB?), in this configuration the recall is
similar to the recall of the rule-based system in a
?single-unit modus? (see Table 3).
mode document set R P F
TF-filtered ecoli-tf.abstracts 0.09 0.70 0.16
default ecoli-tf.abstracts 0.09 0.45 0.15
TF-filtered ecoli-relevant.fulltext 0.10 0.54 0.17
default ecoli-relevant.fulltext 0.10 0.29 0.15
TF-filtered regulon ra 0.07 0.78 0.13
default regulon ra 0.07 0.47 0.12
Table 4: Results of evaluation against REGULONDB of
ML-based system
As already shown in the intrinsic evaluation,
application of different confidence thresholds in-
creases the recall of the ML-based system. This was
also done for the evaluation against REGULONDB.
Table 5 shows the impact of increased confidence
thresholds for the negative class on the ?regulon ra?
set for the ?TF-filtered? evaluation mode. Given an
extremely high threshold of 0.95, the recall is in-
creased from 7 to 11% which constitutes a relative
increase of over 60%. Precision obviously drops,
however, the overall F-score has improved from 13
to 19%. These results emphasize that an ML-based
system has an important handle which allows to ad-
just recall according to the application needs.
threshold R P F
default (0.5) 0.07 0.78 0.13
0.8 0.09 0.70 0.16
0.95 0.11 0.63 0.19
Table 5: Different confidence thresholds for the ML-
based system tested on the ?regulon ra? set
5.4 Manual analysis of false positives
REGULONDB was taken as an absolute gold stan-
dard in this evaluation. If a system correctly extracts
an event which is not contained in REGULONDB
for some reason, this constitutes a FP. Moreover, all
kinds of error (e.g., agent and patient mixed up) were
subsumed as FP errors. To analyze the cause and
distribution of FPs in more detail, a manual analysis
of the FP errors was performed and original FP hits
were assigned to one out of four FP error categories:
Cat1: Not a GRE This is really an FP error, as the
extracted relation does not at all constitute a
gene regulation event.
Cat2a: GRE but other than transcription
Unlike REGULONDB which contains only one
subtype of GREs, namely transcriptions, the
ML-based system identifies all kinds of GREs.
Therefore, the ML-based system clearly
identifies events which cannot be contained in
REGULONDB and, therefore, are not really
FPs.
Cat 3: Partially correct transcription event This
category deals with incorrect arguments of
GREs. We distinguish three types of FPs: (a)
the patient and the agent role are interchanged,
(b) the patient is wrong, while the agent is
right, and (c) the agent is wrong, while the
patient is right. In all these three cases, though
errors were committed human curators might
find the partially incorrect information useful
to speed up the curation process.
Cat4: Relation missing in REGULONDB Those
are relations which should be contained in
REGULONDB but are missing for some
reason. The agent is a correctly identified
transcription factor and the sentence contains
a mention of a transcription event. There are
several reasons why this relation was not found
in REGULONDB as we will discuss in the
following.
Table 6 shows the results of the manual FP anal-
ysis of the ML-based system (no TF filter applied)
on the ?ecoli-tf-abstracts? and ?ecoli-tf-fulltexts?.
It shows that the largest source of error is due
to Cat1, i.e., an identified relation is completely
wrong. As fulltext documents are generally more
complex, the relative amount of this kind of errors
is higher here than on abstracts (54.5 % compared
43
category abstracts (%) fulltexts (%)
Cat 1 44.5 54.5
Cat 2 11.2 10.9
Cat 3a 3.8 3.9
Cat 3b 8.5 4.4
Cat 3c 8.2 5.4
Cat 4 23.8 21.0
Table 6: Manual analysis of false positive errors (FP).
Percentages of FPs by category are reported on ?ecoli-tf-
abstracts? and ?ecoli-tf-fulltexts?
to 44.5 %). However, on abstracts and fulltexts, a
bit more than 10 % of the FP are because the sys-
tem found too general GREs which, by definition,
are not contained in REGULONDB (Cat2). Iden-
tified GREs that were partially correct constitute
20.5 % (abstracts) or 13.7 % (fulltexts) of the FP er-
rors (Cat3).
Finally, 23.8% and 21.0% of the FPs for abstracts
and fulltext, respectively, are correct transcription
events but could not be found in REGULONDB
(Cat4). This is due to several reasons. For instance,
identified gene names were incorrectly normalized
so that they could not be found in REGULONDB,
REGULONDB curators have not yet added a relation
or simply overlooked it; relations are correctly iden-
tified as such in the narrow context of a paragraph of
a document but were actually of speculative nature
only (this includes relations whose status is unsure,
often indicated by ?likely? or ?possibly?).
Summarizing, the manual FP analysis shows that
about 50% of all FPs are not completely erroneous.
These numbers must clearly be kept in mind when
interpreting the raw numbers (especially for preci-
sion) reported on in the previous subsection.
5.5 Integration of text mining results
We have integrated the results of the two different
text mining systems and found that both systems are
complementary to each other such that their result
sets do not heavily overlap. For instance, from the
?ecoli-tf.abstract? corpus, the rule-based system ex-
tracts 992 events, while the ML-based system ex-
tracts 705 events. For the integration, we have con-
sidered only the events whose participants are as-
sociated with UNIPROT identifiers. Among the ex-
tracted events, only 285 events are extracted by both
systems. We might speculate that the overlapping
events are more reliable than the rest of the extracted
events. It also leaves 71.3% of the results from
the rule-based system and 59.6% of results from the
ML-based system as unique contributions from each
of the approaches for the integration.
6 Conclusions
We have explored a rule-based and a machine
learning-based approach to the automatic extrac-
tion of gene regulation events. Both approaches
were evaluated under well-defined lab conditions us-
ing self-supplied corpora, and under real-life condi-
tions by comparing our results with REGULONDB,
a well-curated reference data set. While the re-
sults for the first evaluation scenario are state of the
art, performance figures in the real-life scenario are
not so shiny (the best F-scores for the rule-based
and the ML-based system are on the order of 34%
and 19%, respectively). This holds, in particular,
for the comparison with the work of Rodr??guez-
Penagos et al (2007). Still, at least the ML-based
approach is much more general than the very specifi-
cally tuned manual rule set from Rodr??guez-Penagos
et al (2007) and has potential for increases in perfor-
mance. Also, this has been the first extra-mural eval-
uation of automatically generating content for REG-
ULONDB.
Still, the analysis of false positives reveals that
the strict criteria we applied for our evaluation may
appear in another light for human curators. Con-
founded agents and patients (21% on the abstracts,
14% on full texts) and information not contained in
REGULONDB (24% on the abstracts, 21% on full
texts) might be useful from a heuristic perspective to
focus on interesting data during the curation process.
Acknowledgements
This work was funded by the EC within the BOOT-
Strep (FP6-028099) and the CALBC (FP7-231727)
projects. We want to thank Tobias Wagner (Centre
for Molecular Biomedicine, FSU Jena) for perform-
ing the manual FP analysis.
44
References
Ekaterina Buyko, Elena Beisswanger, and Udo Hahn.
2008. Testing different ACE-style feature sets for
the extraction of gene regulation relations from MED-
LINE abstracts. In Proceedings of the 3rd Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM 2008), pages 21?28.
Francis Collins, Eric Green, Alan Guttmacher, and Mark
Guyer. 2003. A vision for the future of genomics re-
search. Nature, 422(6934 (24 Feb)):835?847.
Vasileios Hatzivassiloglou and Wubin Weng. 2002.
Learning anchor verbs for biological interaction pat-
terns from published text articles. International Jour-
nal of Medical Informatics, 67:19?32.
Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-
bin Qu, and Ming Li. 2004. Discovering patterns
to extract protein-protein interactions from full texts.
Bioinformatics, 20(18):3604?3612.
Sophia Katrenko and P. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency trees.
In KDECB 2006 ? Knowledge Discovery and Emer-
gent Complexity in Bioinformatics, pages 61?80.
Jung-jae Kim and Jong C. Park. 2004. BioIE: retar-
getable information extraction and ontological anno-
tation of biological interactions from the literature.
Journal of Bioinformatics and Computational Biology,
2(3):551?568.
Seon-Ho Kim, Juntae Yoon, and Jihoon Yang. 2008.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Learning
language in logic - genic interaction extraction LLL?
2005, pages 31?37.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein interac-
tion corpora. BMC Bioinformatics, 9(3), April.
Carlos Rodr??guez-Penagos, Heladia Salgado, Irma
Mart??nez-Flores, and Julio Collado-Vides. 2007. Au-
tomatic reconstruction of a bacterial regulatory net-
work using natural language processing. BMC Bioin-
formatics, 8(293).
Kenji Sagae, Yusuke Miyao, and Junichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Annual Meeting of Association for Computational
Linguistics, pages 624?631.
Jasmin S?aric?, Lars J. Jensen, Rossitza Ouzounova, Isabel
Rojas, and Peer Bork. 2004. Extracting regulatory
gene expression networks from pubmed. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 191, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Joachim Wermter, Katrin Tomanek, and Udo Hahn.
2009. High-performance gene name normalization
with GeNo. Bioinformatics, 25(6):815?821.
Hui Yang, Goran Nenadic, and John Keane. 2008. Iden-
tification of transcription factor contexts in literature
using machine learning approaches. BMC Bioinfor-
matics, 9(Supplement 3: S11).
45
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 9?17,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On Proper Unit Selection in Active Learning:
Co-Selection Effects for Named Entity Recognition
Katrin Tomanek1? Florian Laws2? Udo Hahn1 Hinrich Schu?tze2
1Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
2Institute for Natural Language Processing, Universita?t Stuttgart, Germany
{fl|hs999}@ifnlp.org
Abstract
Active learning is an effective method for cre-
ating training sets cheaply, but it is a biased
sampling process and fails to explore large
regions of the instance space in many appli-
cations. This can result in a missed cluster
effect, which signficantly lowers recall and
slows down learning for infrequent classes.
We show that missed clusters can be avoided
in sequence classification tasks by using sen-
tences as natural multi-instance units for label-
ing. Co-selection of other tokens within sen-
tences provides an implicit exploratory com-
ponent since we found for the task of named
entity recognition on two corpora that en-
tity classes co-occur with sufficient frequency
within sentences.
1 Introduction
Active learning (AL) has been shown to be an effec-
tive approach to reduce the amount of data needed
to train an accurate statistical classifier. AL selects
highly informative examples from a pool of unla-
beled data and prompts a human annotator for the
labels of these examples. The newly labeled exam-
ples are added to a training set used to build a statis-
tical classifier. This classifier is in turn used to assess
the informativeness of further examples. Thus, a
select-label-retrain loop is formed that quickly se-
lects hard to classify examples, honing in on the de-
cision boundary (Cohn et al, 1996).
A fundamental characteristic of AL is the fact that
it constitutes a biased sampling process. This is so
? Both authors contributed equally to this work.
by design, but the bias can have an undesirable con-
sequence: partial coverage of the instance space. As
a result, classes or clusters within classes may be
completely missed, resulting in low recall or slow
learning progress. This has been called the missed
cluster effect (Schu?tze et al, 2006). While AL has
been studied for a range of NLP tasks, the missed
cluster problem has hardly been addressed.
This paper studies the missed class effect, a spe-
cial case of the missed cluster effect where complete
classes are overlooked by an active learner. The
missed class effect is the result of insufficient ex-
ploration before or during a mainly exploitative AL
process. In AL approaches where exploration is only
addressed by an initial seed set, poor seed set con-
struction gives rise to the missed class effect.
We focus on the missed class effect in the con-
text of a common NLP task: named entity recogni-
tion (NER). We show that for this task the missed
class effect is avoided by increasing the sampling
granularity from single-instance units (i.e., tokens)
to multi-instance units (i.e., sentences). For AL ap-
proaches to NER, sentence selection recovers better
from unfavorable seed sets than token selection due
to what we call the co-selection effect. Under this
effect, a non-targeted entity class co-occurs in sen-
tences that were originally selected because of un-
certainty on tokens of a different entity class.
The rest of the paper is structured as follows: Sec-
tion 2 introduces the missed class effect in detail.
Experiments which demonstrate the co-selection ef-
fect achieved by sentence selection for NER are de-
scribed in Section 3 and their results presented in
Section 4. We draw conclusions in Section 5.
9
2 The Missed Class Effect
This section first describes the missed class ef-
fect. Then, we discuss several factors influencing
this effect, focusing on co-selection, a natural phe-
nomenon in common NLP applications of AL.
2.1 Sampling bias and misguided AL
The distribution of the labeled data points obtained
with an active learner deviates from the true data
distribution. While this sampling bias is intended
and accounts for the effectiveness of AL, it also
poses challenges as it leads to classifiers that per-
form poorly in some regions, or clusters, of the ex-
ample space. In the literature, this phenomenon has
been described as the missed cluster effect (Schu?tze
et al, 2006; Dasgupta and Hsu, 2008)
In this context, we must distinguish between ex-
ploration and exploitation. By design, AL is a
highly exploitative strategy: regions around decision
boundaries are inspected thoroughly so that decision
boundaries are learned well, but regions far from any
of the initial decision boundaries remain unexplored.
An exploitative sampling approach thus has to be
combined with some kind of exploratory strategy to
make sure the example space is adequately covered.
A common approach is to start an AL process with
an initial seed set that accounts for the exploration
step. However, a seed set which is not represen-
tative of the example space may completely mis-
guide AL ? at least when no other explorative tech-
niques are applied as a remedy. While approaches
to balancing exploration and exploitation (Baram et
al., 2003; Dasgupta and Hsu, 2008; Cebron and
Berthold, 2009) have been discussed, we here fo-
cus on a ?pure? AL scenario where exploration takes
only place in the beginning by a seed set. In sum-
mary, the missed clusters are the result of a sce-
nario where poor exploration is combined with ex-
clusively exploitative sampling.
Why is AL an exploitative sampling strategy? AL
selects data points based on the confidence of the ac-
tive learner. Assume an initial seed set that does not
contain examples of a specific cluster. This leads to
an initial active learner that is mistakenly overconfi-
dent about the class membership of instances in this
missed cluster. Far away from the decision bound-
ary, the active learner assumes a high confidence for
A
B
C
(a)
A
B
C
(b)
Figure 1: Illustration of the missed cluster effect in a 1-
d scenario. Shaded points are contained in the seed set,
vertical lines are final decision boundaries, and dashed
rectangles mark the explored regions
all instances in that cluster, even if they are in fact
misclassified. Consequently, the active learner will
fail to select these instances for long until some re-
direction impulse is received (if at all).
To give an example, let us consider a simple 1-
d toy scenario with examples from three clusters A,
B, and C as shown in Figure 1. In scenario (a), AL
is started from a seed set including one example of
clusters A and B only. In subsequent rounds, AL
will select examples in these clusters only (shown as
the dashed box in the figure). Examples in cluster
C are ignored as they are far from the initial deci-
sion boundary. Eventually, a decision boundary is
fixed as shown by the vertical line which indicates
that this AL process has completely overlooked ex-
amples from cluster C.
Assuming that the examples fall in two classes
X1 = {A ? C} and X2 = {B} the learned clas-
sifier has low recall for class X1 and relatively low
precision for class X2 as it erroneously assigns ex-
amples of cluster C to class X2. In a related sce-
nario with three classes X1 = {A}, X2 = {B}, and
X3 = {C} this would even mean that the classifier
is not at all aware about the third class resulting in
the missed class problem.
A more representative seed set circumvents this
problem. Given a seed set including one example
of each cluster, AL might find a second decision
boundary1 between clusters B and C because it is
now aware of examples from C. Figure 1(b) shows
a possible result of AL on this seed set.
The missed cluster effect can be understood as
the generalized problem. A special case of it is the
1Assuming a classifier that can learn several boundaries.
10
missed class effect as shown in the previous exam-
ple. In general, it has the same causes (insufficient
exploration and misguided exploitation), but is eas-
ier to test. Often we know (at least the number of) all
classes under scrutiny, while we usually cannot as-
sume all clusters in the feature space to be known. In
this paper, we focus on the missed class effect, i.e.,
scenarios where classes are overlooked by a mis-
guided AL process resulting in a slow (active) learn-
ing progress.
2.2 Factors influcencing the missed class effect
AL in a practical scenario is subject to several fac-
tors which mitigate or intensify the missed class ef-
fect described before. In the following, we describe
three such factors, with a special focus on the co-
selection effect, which we claim to significantly mit-
igate the missed class effect in a specific type of NLP
tasks, sequence learning problems such as NER or
POS tagging.
Class imbalance Many studies on AL for NLP
tasks assume that AL is started from a randomly
drawn seed set. Such a seed set can be problem-
atic when the class distribution in the data is highly
skewed. In this case, ?rare? classes might not be
represented in the seed set, increasing the chance to
completely miss out such a class using AL. When
classes are relatively frequent, an active learner ?
even when started from an unfavorable seed set ?
might still mistake an example of one class for an
uncertain example of a different class and conse-
quently select it. Thereby, it can acquire information
about the former class ?by accident? leading to sud-
den and rapid discovery of the newly-found class.
However, in the case of extreme class imbalance this
is very unlikely. Severe class imbalance intensifies
the missed cluster effect.
Similarity of considered classes If, e.g., two of
the classes to be learned, say Xi and Xj , are harder
to discriminate than others, or if the data contains
lots of noise, an active learner is more likely to select
some instances of Xi if at least its ?similar? coun-
terpart Xj was represented in the seed set. Hence,
it may mistake the instances of Xi and Xj before it
has acquired enough information to discriminate be-
tween them. So, under certain situations similarity
of classes can mitigate the missed class effect.
The co-selection effect Many NLP tasks are se-
quence learning problems including, e.g., POS tag-
ging, and named entity recognition. Sequences are
consecutive text tokens constituting linguistically
plausible chunks, e.g., sentences. Algorithms for se-
quence learning obviously work on sequence data,
so respective AL approaches need to select complete
sequences instead of single text tokens (Settles and
Craven, 2008). Furthermore, sentence selection has
been preferred over token selection in other works
with the argument that the manual annotation of sin-
gle, possibly isolated tokens is almost impossible or
at least extremely time-consuming (Ringger et al,
2007; Tomanek et al, 2007).
Within such sequences, instances of different
classes often co-occur. Thus, an active learner that
selects uncertain examples of one class gets exam-
ples of a second class as an unintended, yet pos-
itive side effect. We call this the co-selection ef-
fect. As a result, AL for sequence labeling is not
?pure? exploitative AL, but implicitly comprises an
exploratory aspect which can substantially reduce
the missed class problem. In scenarios where we
cannot hope for such a co-selection, we are much
more likely to have decreased AL performance due
to missed clusters or classes.
3 Experiments
We ran several experiments to investigate how the
sampling granularity, i.e. the size of the selection
unit, influences the missed class effect. AL based
on token selection (T-AL) is compared to AL based
on sentence selection (S-AL). Although our experi-
ments are certainly also subject to the other factors
mitigating the missed class effect (e.g. similarity of
classes), the main focus of the experiments is on the
co-selection effect that we expected to observe in
S-AL. Several scenarios of initial exploration were
simulated by seed sets of different characteristics.
The experiments were run on synthetic and real data
in the context of named entity recognition (NER).
3.1 Classifiers and active learning setup
The active learning approach used for both S-AL
and T-AL is based on uncertainty sampling (Lewis
and Gale, 1994) with the margin metric (Schein and
Ungar, 2007) as uncertainty measure. Let c and c?
11
be the two most likely classes predicted for token
xj with p?c,xj and p?c?,xj being the associated class
probabilities. The per-token margin is calculated as
M = |p?c,xj ? p?c?,xj |.
For T-AL, the sampling granularity is the token,
while in S-AL, complete sentences are selected. For
S-AL, the margins of all tokens in a sentence are
averaged and the aggregate margin is used to select
sentences. We chose this uncertainty measure for S-
AL for better comparison with T-AL. In either case,
examples (tokens or sentences) with a small margin
are preferred for selection. In every iteration, a batch
of examples is selected: 20 sentences for S-AL, 200
tokens for T-AL.
Bayesian logistic regression as implemented in
the BBR classification package (Genkin et al, 2007)
with out-of-the-box parameter settings was used as
base learner for T-AL. For S-AL, a linear-chain
Conditional Random Field (Lafferty et al, 2001) is
employed as implemented in MALLET (McCallum,
2002). Both base learners employ standard features
for NER including the lexical token itself, various
orthographic features such as capitalization, the oc-
currence of special characters like hyphens, and con-
text information in terms of features of neighboring
tokens to the left and right of the current token.
3.2 Data sets
We used three data sets in our experiments. Two of
them (ACE and PBIO) are standard data sets. The
third (SYN) is a synthetic set constructed to have
specific characteristics. For simplicity, we consider
only scenarios with two entity classes, a majority
class (MAJ) and a minority class (MIN). We dis-
carded all other entity annotations originally con-
tained in the corpus assigning the OUTSIDE class.2
The first data set (PBIO) is based on the annota-
tions of the PENNBIOIE corpus for biomedical en-
tity extraction (Kulick et al, 2004). As PENNBIOIE
makes fine-grained and subtle distinctions between
various subtypes of classes irrelevant for this study,
we combined several of the original classes into two
entity classes: The majority class consists of the
three original classes ?gene-protein?, ?gene-generic?,
and ?gene-rna?. The minority class consists of
the original and similar classes ?variation-type? and
2The OUTSIDE class marks that a token is not part of an
named entity.
?variation-event?. All other entity labels were re-
placed by the OUTSIDE class.
The second data set (ACE) is based on the
newswire section of the ACE 2005 Multilingual
Training Corpus (Walker et al, 2006). We chose
the ?person? class as majority class and the ?organi-
zation? class as the minority class. Again, all other
classes are mapped to OUTSIDE.
The synthetic data set (SYN) was constructed by
combining the sentences from the original ACE and
PENNBIOIE corpora. The ?person? class consti-
tutes the minority class, the very similar classes
?malignancy? and ?malignancy-type? were merged
to form the majority class. All other class la-
bels were set to OUTSIDE. SYN?s construction
was motivated by the following characteristics of
the new data set which would make the appear-
ance of the missed class effect very likely for
insufficient exploration scenarios:
(i) absence of inner-sentence entity class correlation
to ensure that sentences contain either mentions of
only a single entity class or no mentions at all.
(ii) marked entity class imbalance between the ma-
jority and minority classes
(iii) dissimilar surface patterns of entity mentions of
the two entity classes with the rationale that class
similarity will be low.
Table 1 summarizes characteristics of the data
sets. While SYN exhibits high imbalance (e.g., 1:9.4
on the token level), PBIO and ACE are moderately
skewed. In PBIO, the number of sentences contain-
ing any entity mention is relatively high compared
to ACE or SYN. For our experiments, the corpora
were randomly split in a pool for AL and a test set
for performance evaluation.
Inner-sentence entity class co-occurrence We
have described co-selection as a potential mitigat-
ing factor for the missed class effect in Section 2.
For this effect to occur, there must be some corre-
lation between the occurrence of entity mentions of
the MAJ class with those from MIN.
Table 2 shows correlation statistics based on the
?2 measure. We found strong correlation in all three
corpora3: For ACE and PBIO, the correlation is pos-
itive; for SYN it is negative so when a sentence in
SYN contains a majority class entity mention, it is
3All correlations are statistically significant (p < 0.01).
12
PBIO ACE SYN
sentences (all) 11,164 2,642 13,804
sentences (MAJ) 7,075 767 5,667
sentences (MIN) 2,156 974 974
MIN-MAJ ratio 1 : 3.3 1 : 1.3 1 : 5.8
tokens (all) 277,053 66,752 343,773
tokens (MAJ) 17,928 2,008 18,959
tokens (MIN) 4,079 1,822 2,008
MIN-MAJ ratio 1 : 4.4 1 : 1.1 1 : 9.4
Table 1: Characteristics of the data sets; ?sentences
(MAJ)?, e.g., specifies the number of sentences contain-
ing mentions of the majority class.
PBIO ACE SYN
?2 132.34 6.07 727
P (MIN |MAJ) 0.26 0.31 0.0
Table 2: Co-occurrence of entity classes in sentences
highly unlikely that it also contains a minority entity.
In fact, it is impossible by construction of the data
set. Further, this table shows the probability that a
sentence containing the majority class also contains
the minority class. As expected, this is exactly 0 for
SYN, but significantly above 0 for PBIO and ACE.
3.3 Seed sets
Selection of an appropriate seed set for the start of an
AL process is important to the success of AL. This is
especially relevant in the case of imbalanced classes
because a typically small random sample will pos-
sibly not contain any example of the rare class. We
constructed different types of seed sets (whose nam-
ing intentionally reflects the use of the entity classes
from Section 3.2) to simulate different scenarios of
ill-managed initial exploration. All seed sets have
a size of 20 sentences. The RANDOM set was ran-
domly sampled, the MAJ set is made of sentences
containing at least one majority class entity, but no
minority class entity. Accordingly, MIN is densely
populated with minority entities. Finally, OUTSIDE
contains only sentences without entity mentions.
One could think of the OUTSIDE and MAJ seed
sets of cases where a random seed set selection has
unluckily produced an especially bad seed set. MIN
serves to demonstrate the opposite case. For each
type of seed set, we sampled ten independent ver-
sions to calculate averages over several AL runs.
3.4 Cost measure
The success of AL is usually measured as reduc-
tion of annotation effort according to some cost mea-
sure. Traditionally, the most common cost measure
considers a unit cost per annotated token, which fa-
vors AL systems that select individual tokens. In
a real annotation setting, however, it is unnatural,
and therefore hard for humans to annotate single,
possibly isolated tokens, leading to bad annotation
quality (Hachey et al, 2005; Ringger et al, 2007).
When providing context, the question arises whether
the annotator can label several tokens present in the
context (e.g., an entire multi-token entity or even
the whole sentence) at little more cost than anno-
tating a single token. Thus, assigning a linear cost
of n to a sentence where n is the sentence?s length
in tokens seems to unfairly disadvantage sentence-
selection AL setups.
However, more work is needed to find a more re-
alistic cost measure. At present there is no other
generally accepted cost measure than unit cost per
token, so we report costs using the token measure.
4 Results
This section presents the results of our experiments
on the missed class effect in two different AL
scenarios, i.e., sentence selection (S-AL) and to-
ken selection (T-AL). The AL runs were stopped
when convergence on the minority class F-score was
achieved. This was done because early AL iterations
before the convergence point are most important and
representative for a real-life scenario where the pool
is extremely large, so that absolute convergence of
the classifier?s performance will never be reached.
The learning curves in Figures 2, 3, and 4 reveal
general characteristics of S-AL compared to T-AL.
For S-AL, the number of tokens on the x-axis is the
total number of tokens in the sentences labeled so
far. While S-AL generally yields higher F-scores, T-
AL converges much earlier when counted in terms
of tokens. The reason for this is that T-AL can se-
lect uncertain data more specifically. In contrast, S-
AL also selects tokens that the classifier can already
classify reliably ? these tokens are selected because
they co-occur in a sentence that also contains an un-
certain token. Whether T-AL is really more efficient
clearly depends on the cost-metric applied (cf. Sec-
13
0 5000 10000 150000.0
0.2
0.4
0.6
0.8
tokens
F?score
MIN classMAJ class
(a) T-AL learning curve, single run
with OUTSIDE seed
0 5000 10000 150000.5
0.6
0.7
0.8
0.9
1.0
tokens
mean
 margin
MIN classMAJ class
(b) T-AL mean margin curve, single
run with OUTSIDE seed
0 5000 10000 150000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(c) T-AL learning curves, minority
class, all seeds, 10 runs
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
tokens
per clas
s F?sco
re
MIN classMAJ class
(d) S-AL learning curve, single run
with OUTSIDE seed
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
1.0
tokens
mean
 margin
MIN classMAJ class
(e) S-AL mean margin curve, single
run with OUTSIDE seed
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(f) S-AL learning curves, minority
class, all seeds, 10 runs
Figure 2: Results on SYN corpus for token selection (a,b,c) and sentence selection (d,e,f)
tion 3.4). Since the focus of this paper is on compar-
ing the missed class effect in a sentence and a token
selection AL setting (T-AL and S-AL) we apply the
straight-forward token measure.
4.1 The pathological case
Figure 2 shows results on the SYN corpus for T-AL
(upper row) and S-AL (lower row). Figures 2(a)
and 2(d) show the minority and majority class learn-
ing curves for a single run starting from the OUT-
SIDE seed set, which was particularly problematic
on SYN. (We show single runs to give a better pic-
ture of what happens during the selection process.)
The figures show that for both AL scenarios, the
OUTSIDE seed set caused the active learner to focus
exclusively on the majority class and to completely
ignore the minority class for many AL iterations (al-
most 30,000 tokens for S-AL and over 4,000 tokens
for T-AL). Had we stopped the AL process before
this turning point, the classifier?s performance on
the majority entity class would have been reason-
ably high while the minority class would not have
been learned at all ? which is precisely the defini-
tion of an (initially) missed class.
Figures 2(b) and 2(e) show the corresponding
mean margin plots of these AL runs, indicating the
confidence of the classifier on each class. The mean
margin is calculated as the average margin over to-
kens in the remaining pool, separately for each true
class label.4 As expected, the active learner is over-
confident but wrong on instances of the minority
class (assigning them to the OUTSIDE class, we
assume). Only after some time, margin scores on
minority class tokens start decreasing. This hap-
pens because from time to time minority class ex-
amples are mistakenly considered as majority class
examples with low confidence and thus selected by
accident. Lowered minority class confidence then
causes the selection of further minority class exam-
ples, resulting in a turning point with a steep slope
of the minority class learning curve.
Consequences of seed set selection We compare
the minority class learning curves for all types of
4Note that in a real, non-simulation active learning task, the
true class labels would be unknown.
14
0 5000 10000 15000 200000.0
0.2
0.4
0.6
0.8
tokens
F?score
MIN classMAJ class
(a) T-AL learning curve, single run
with MAJ seed
0 5000 10000 15000 200000.5
0.6
0.7
0.8
0.9
1.0
tokens
mean
 margin
MIN classMAJ class
(b) T-AL mean margin curve, single
run with MAJ seed
0 5000 10000 15000 200000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(c) T-AL learning curves, minority
class, all seeds, 10 runs
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
tokens
per clas
s F?sco
re
MIN classMAJ class
(d) S-AL learning curve, single run
with MAJ seed
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
1.0
tokens
mean
 margin
MIN classMAJ class
(e) S-AL mean margin curve, single
run with MAJ seed
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(f) S-AL learning curves, minority
class, all seeds, 10 runs
Figure 3: Results on PBIO corpus for token selection (a,b,c) and sentence selection (d,e,f)
seed sets and for random selection (cf. Figures 2(c)
and 2(f)), now averaged over 10 runs. On S-AL all
but the MIN seed set were inferior to random selec-
tion. Even the commonly used random seed set se-
lection is problematic because the minority class is
so rare that there are random seed sets without any
example of the minority class.
On T-AL, all seed sets are better than random se-
lection. This, however, is because random selec-
tion is an extremely weak baseline for T-AL due to
the token distribution (cf. Table 1). Still, the RAN-
DOM, MAJ, and OUTSIDE seed sets are signifi-
cantly worse than a seed set which covers the minor-
ity class well. Note that the majority class learning
curves are relatively invariant against different seed
sets. The minority class seed set does have some
negative impact on initial learning progress on the
majority class (not shown here), but the impact is
rather small. Because of the higher frequency of
the majority class, the classifier soon finds major-
ity class examples to compensate for the seed set by
chance or class similarity.
4.2 Missed class effect mitigated by co-selection
Results on PBIO corpus On the PBIO corpus,
where minority and majority class entity mentions
naturally co-occur on the sentence level, we get
a different picture. Figure 3 shows the learning
(3(a), 3(d)) and mean margin (3(b), 3(e)) curves for
the MAJ seed set. T-AL still exhibits the missed
class effect on this seed set. The minority class
learning curve again has a delayed slope and high
mean margin scores of minority tokens at the be-
ginning, resulting in insufficient selection and slow
learning. S-AL, on the other hand, does not re-
ally suffer from the missed class effect: minor-
ity class entity mentions are co-selected in sen-
tences which were chosen due to uncertainty on
majority class tokens. Minority class mean mar-
gin scores quickly fall, reinforcing selection for mi-
nority class entities. Learning curves for minority
and majority classes run approximately in parallel.
Figure 3(f) shows that all seed sets perform quite
similar for S-AL. MIN unsurprisingly is a bit better.
With the other seed sets, S-AL performance is com-
15
0 2000 4000 6000 80000.0
0.2
0.4
0.6
0.8
tokens
minority 
class F?
score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(a) T-AL
0 5000 15000 250000.0
0.2
0.4
0.6
0.8
tokens
minority 
class F?
score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(b) S-AL
Figure 4: Minority class learning curves for all seeds on
ACE averaged over 10 runs
parable to random selection. On the PBIO corpus,
random selection is a strong baseline as almost every
sentence contains an entity mention ? which is not
the case for SYN and ACE (cf. Table 1). As there is
no co-selection effect for T-AL, the MAJ and OUT-
SIDE seed sets also here are subject to the missed
class problem (Figure 3(c)), although not as severely
as on the SYN corpus.
Results on ACE corpus Figure 4 shows learning
curves averaged over 10 runs on ACE. Overall, the
missed class effect is less pronounced on ACE com-
pared to PBIO. Still, co-selection avoids a good por-
tion of the missed class effect on S-AL ? all seed
sets yield results much better than random selection
right from the beginning.
On T-AL, the OUTSIDE seed set has a marked
negative effect. However, while different seed
sets still have visible differences in learning perfor-
mance, the magnitude of the effect is smaller than
on PBIO. It is difficult to find the exact reasons
in a non-synthetic, natural language corpus where a
lot of different effects are intermingled. One might
assume higher class similarity between the major-
ity (?persons?) and the minority (?organizations?)
classes on the ACE corpus than, e.g., on the PBIO
corpus. Moreover, there is hardly any imbalance
in frequency between the two entity classes on the
ACE corpus. We briefly discussed such influencing
factors possibly mitigating the missed class effect in
Section 2.2.
4.3 Discussion
To summarize, on a synthetic corpus (SYN) the
missed class effect can be well studied in both
AL scenarios, i.e., S-AL and T-AL. Moving from
a relatively controlled, synthetic corpus (extreme
class imbalance, no inner-sentence co-occurrence
between entity classes, quite different entity classes)
to more realistic corpora, effects generally mix a bit
due to different degrees of class imbalance and prob-
ably higher similarity between entity classes.
Our experiments unveil that co-selection in S-AL
effectively helps avoid dysfunctional classifiers that
insufficiently explore the instance space due to a
disadvantageous seed set. In contrast, AL based
on token-selection (T-AL) cannot recover from in-
sufficient exploration as easy as AL with sentence-
selection and is thus more sensitive to the missed
class effect.
5 Conclusion
We have shown that insufficient exploration in the
initial stages of active learning gives rise to regions
of the sample space that contain missed classes that
are incorrectly classified. This results in low clas-
sification performance and slow learning progress.
Comparing two sampling granularities, tokens vs.
sentences, we found that the missed class effect is
more severe when isolated tokens instead of sen-
tences are selected for labeling.
The missed class problem in sequence classifica-
tion tasks can be avoided using sentences as natural
multi-instance units for selection and labeling. Us-
ing multi-instance units, co-selection of other tokens
within sentences provides an implicit exploratory
component. This solution is effective if classes co-
occur sufficiently within sentences which is the case
for many real-life entity recognition tasks.
While other work has proposed sentence selection
in AL for sequence labeling as a means to ease and
speed up annotation, we have gathered here addi-
tional motivation from the perspective of robustness
of learning. Future work will compare the beneficial
effect introduced by co-selection with other forms of
exploration-enabled active learning.
Acknowledgements
The first and the third author were funded by the
German Ministry of Education and Research within
the StemNet project (01DS001A-C) and by the EC
within the BOOTStrep project (FP6-028099).
16
References
Yoram Baram, Ran El-Yaniv, and Kobi Luz. 2003. On-
line choice of active learning algorithms. In ICML
?03: Proceedings of the 20th International Conference
on Machine Learning, pages 19?26.
Nicolas Cebron and Michael R. Berthold. 2009. Active
learning for object classification: From exploration to
exploitation. Data Mining and Knowledge Discovery,
18(2):283?299.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
sampling for active learning. In ICML ?08: Proceed-
ings of the 25th International Conference on Machine
Learning, pages 208?215.
Alexander Genkin, David D. Lewis, and David Madigan.
2007. Large-scale Bayesian logistic regression for text
categorization. Technometrics, 49(3):291?304.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ?05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan T. McDonald, Martha S. Palmer, and Andrew Ian
Schein. 2004. Integrated annotation for biomedical
information extraction. In Proceedings of the HLT-
NAACL 2004 Workshop ?Linking Biological Litera-
ture, Ontologies and Databases: Tools for Users?,
pages 61?68.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML ?01: Proceedings of the 18th International
Conference on Machine Learning, pages 282?289.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3?12.
Andrew McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.
umass.edu.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101?108.
Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235?265.
Hinrich Schu?tze, Emre Velipasaoglu, and Jan Pedersen.
2006. Performance thresholding in practical text clas-
sification. In CIKM ?06: Proceedings of the 15th ACM
International Conference on Information and Knowl-
edge Management, pages 662?671.
Burr Settles and Mark Craven. 2008. An analysis of ac-
tive learning strategies for sequence labeling tasks. In
EMNLP ?08: Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 1070?1079.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486?495.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium,
Philadelphia.
17
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 45?48,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Web Survey on the Use of Active Learning
to Support Annotation of Text Data
Katrin Tomanek
Jena University Language & Information Engineering Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, D-07743 Jena, Germany
katrin.tomanek@uni-jena.de
Fredrik Olsson
SICS
Box 1263
SE-164 29 Kista, Sweden
fredrik.olsson@sics.se
Abstract
As supervised machine learning methods for
addressing tasks in natural language process-
ing (NLP) prove increasingly viable, the fo-
cus of attention is naturally shifted towards the
creation of training data. The manual annota-
tion of corpora is a tedious and time consum-
ing process. To obtain high-quality annotated
data constitutes a bottleneck in machine learn-
ing for NLP today. Active learning is one way
of easing the burden of annotation. This pa-
per presents a first probe into the NLP research
community concerning the nature of the anno-
tation projects undertaken in general, and the
use of active learning as annotation support in
particular.
1 Introduction
Supervised machine learning methods have been
successfully applied to many NLP tasks in the last
few decades. While these techniques have shown
to work well, they require large amounts of labeled
training data in order to achieve high performance.
Creating such training data is a tedious, time con-
suming and error prone process. Active learning
(AL) is a supervised learning technique that can be
used to reduce the annotation effort. The main idea
in AL is to put the machine learner in control of the
data from which it learns; the learner can ask an or-
acle (typically a human) about the labels of the ex-
amples for which the model learned so far makes
unreliable predictions. The active learning process
takes as input a set of labeled examples, as well as
a larger set of unlabeled examples, and produces a
classifier and a relatively small set of newly labeled
data. The overall goal is to create as good a classifier
as possible, without having to mark-up and supply
the learner with more data than necessary. AL aims
at keeping the human annotation effort to a mini-
mum, only asking the oracle for advice where the
training utility of the result of such a query is high.
Settles (2009) gives a detailed overview of the liter-
ature on AL.
It has been experimentally shown that AL can in-
deed be successfully applied to a range of NLP tasks
including, e.g., text categorization (Lewis and Gale,
1994), part-of-speech tagging (Dagan and Engelson,
1995; Ringger et al, 2007), parsing (Becker and Os-
borne, 2005), and named entity recognition (Shen et
al., 2004; Tomanek et al, 2007). Despite that some-
what impressive results in terms of reduced anno-
tation effort have been achieved by such studies, it
seems that AL is rarely applied in real-life annota-
tion endeavors.
This paper presents the results from a web survey
we arranged to analyze the extent to which AL has
been used to support the annotation of textual data in
the context of NLP, as well as addressing the reasons
to why or why not AL has been found applicable to a
specific task. Section 2 describes the survey in gen-
eral, Section 3 introduces the questions and presents
the answers received. Finally, the answers received
are discussed in Section 4.
2 The Survey
The survey was realized in the form of a web-based
questionnaire; the primary reason for this approach,
as opposed to reading and compiling information
45
from academic publications, was that we wanted to
free ourselves and the participants from the dos and
don?ts common to the discourse of scientific papers.
The survey targeted participants who were in-
volved in the annotation of textual data intended for
machine learning for all kinds of NLP tasks. It was
announced on the following mailing lists: BioNLP,
Corpora, UAI List, ML-news, SIG-IRlist, Linguist
list, as well as lists reaching members of SIGANN,
SIGNLL, and ELRA. By utilizing these mailing
lists, we expect to have reached a fairly large por-
tion of the researchers likely to participate in anno-
tation projects for NLP. The questionnaire was open
February 6?23, 2009.
After an introductory description and one initial
question, the questionnaire was divided into two
branches. The first branch was answered by those
who had used AL to support their annotation, while
the second branch was answered by those who had
not. Both branches shared a common first part about
the general set-up of the annotation project under
scrutiny. The second part of the AL-branch focused
on experiences made with applied AL. The second
part of the non AL-branch asked questions about the
reasons why AL had not been used. Finally, the
questionnaire was concluded by a series of questions
targeting the background of the participant.
The complete survey can be downloaded from
http://www.julielab.de/ALSurvey.
3 Questions and answers
147 people participated in the survey. 54 completed
the survey while 93 did not, thus the overall comple-
tion rate was 37 %. Most of the people who did not
complete the questionnaire answered the first couple
of questions but did not continue. Their answers are
not part of the discussion below. We refrain from a
statistically analysis of the data but rather report on
the distribution of the answers received.
Of the people that finished the survey, the ma-
jority (85 %) came from academia, with the rest
uniformly split between governmental organizations
and industry. The educational background of the
participants were mainly computational linguistics
(46 %), general linguistics (22 %), and computer sci-
ence (22 %).
3.1 Questions common to both branches
Both the AL and the non-AL branch were asked
several questions about the set-up of the annotation
project under scrutiny. The questions concerned,
e.g., whether AL had been used to support the anno-
tation process, the NLP tasks addressed, the size of
the project, the constitution of the corpus annotated,
and how the decision when to stop the annotation
process was made.
The use of AL as annotation support. The first
question posed was whether people had used AL as
support in their annotation projects. 11 participants
(20 %) answered this question positively, while 43
(80 %) said that they had not used AL.
The task addressed. Most AL-based annotation
projects concerned the tasks information extraction
(IE) (52 %), document classification (17.6 %), and
(word sense) disambiguation (17.6 %). Also in non
AL-based projects, most participants had focused on
IE tasks (36.8 %). Here, syntactic tasks including
part-of-speech tagging, shallow, and deep parsing
were also often considered (19.7 %). Textual phe-
nomena, such as coreferences and discourse struc-
ture (9.6 %), and word sense disambiguation (5.5 %)
formed two other answer groups. Overall, the non
AL-based annotation projects covered a wider vari-
ety of NLP tasks than the AL-based ones. All AL-
based annotation projects concerned English texts,
whereas of the non-AL projects only 62.8 % did.
The size of the project. The participants were also
asked for the size of the annotation project in terms
of number of units annotated, number of annotators
involved and person months per annotator. The av-
erage number of person months spent on non AL-
projects was 21.2 and 8.7 for AL-projects. However,
these numbers are subject to a high variance.
The constitution of the corpus. Further, the par-
ticipants were asked how the corpus of unlabeled
instances was selected.1 The answer options in-
cluded (a) taking all available instances, (b) a ran-
dom subset of them, (c) a subset based on key-
words/introspection, and (d) others. In the AL-
branch, the answers were uniformly distributed be-
1The unlabeled instances are used as a pool in AL, and as a
corpus in non AL-based annotation.
46
tween the alternatives. In the non AL-branch, the
majority of participants had used alternatives (a)
(39.5 %) and (b) (34.9 %).
The decision to stop the annotation process. A
last question regarding general annotation project
execution concerned the stopping of the annotation
process. In AL-based projects, evaluation on a held-
out gold standard (36.5 %) and the exhaustion of
money or time (36.5 %) were the major stopping cri-
teria. Specific stopping criteria based on AL-internal
aspects were used only once, while in two cases the
annotation was stopped because the expected gains
in model performance fell below a given threshold.
In almost half (47.7 %) of the non AL-based
projects the annotation was stopped since the avail-
able money or time had been used up. Another ma-
jor stopping criterion was the fact that the complete
corpus was annotated (36 %). Only in two cases an-
notation was stopped based on an evaluation of the
model achievable from the corpus.
3.2 Questions specific to the AL-branch
The AL-specific branch of the questionnaire was
concerned with two aspects: the learning algorithms
involved, and the experiences of the participants re-
garding the use of AL as annotation support. Per-
centages presented below are all related to the 11
persons who answered this branch.
Learning algorithms used. As for the AL meth-
ods applied, there was no single most preferred
approach. 27.3 % had used uncertainty sampling,
18.2 % query-by-committee, another 18.2% error
reduction-based approaches, and 36.4 % had used
an ?uncanonical? or totally different approach which
was not covered by any of these categories. As
base learners, maximum-entropy based approaches
as well as Support-Vector machines were most fre-
quently used (36.4 % each).
Experiences. When asked about their experi-
ences, the participants reported that their expecta-
tions with respect to AL had been partially (54.4 %)
or fully (36.3 %) met, while one of the participants
was disappointed. The AL participants did not leave
many experience reports in the free text field. From
the few received, it was evident that the sampling
complexity and the resulting delay or idle time of
the annotators, as well as the interface design are
critical issues in the practical realization of AL as
annotation support.
3.3 Question specific to the non-AL branch
The non AL-specific branch of the questionnaire
was basically concerned with why people did not use
AL as annotation support and whether this situation
could be changed. The percentages given below are
related to the 43 people who answered this particular
part of the questionnaire.
Why was not AL used? Participants could give
multiple answers to this question. Many partici-
pants had either never heard of AL (11 %) or did
not use AL due to insufficient knowledge or exper-
tise (26 %). The implementational overhead to de-
velop an AL-enabled annotation editor kept 17.8 %
of the participants from using AL. Another 19.2 %
of the participants stated that their project specific
requirements did not allow them to use AL. Given
the comments given in the free text field, it can be
deduced that this was often the case when people
wanted to create a corpus that could be used for a
multitude of purposes (such as building statistics on,
cross-validation, learning about the annotation task
per se, and so forth) and not just for classifier train-
ing. In such scenarios, the sampling bias introduced
by AL is certainly disadvantageous. Finally, about
20.5 % of the participants were not convinced that
AL would work well in their scenario or really re-
duce annotation effort. Some participants stated in
their free form comments that while they believed
AL would reduce the amount of instances to be an-
notated it would probably not reduce the overall an-
notation time.
Would you consider using AL in future projects?
According to the answers of another question of the
survey, 40 % would in general use AL, while 56 %
were sceptical but stated that they would possibly
use a technique such as AL.
4 Discussion
Although it cannot be claimed that the data collected
in this survey is representative for the NLP research
community as a whole, and the number of partic-
ipants was too low to draw statistically firm con-
clusions, some interesting trends have indeed been
47
discovered within the data itself. The conclusions
drawn in this section are related to the answers pro-
vided in light of the questions posed in the survey.
The questionnaire was open to the public and was
not explicitly controlled with respect to the distribu-
tion of characteristics of the sample of the commu-
nity that partook in it. One effect of this, coupled
with the fact that the questionnaire was biased to-
wards those familiar with AL, is that we believe that
the group of people that have used AL are overrep-
resented in the data at hand. However, this cannot
be verified. Nevertheless, given this and the poten-
tial reach of the mailing lists used for announcing
the survey, it is remarkable that not more than 20 %
(11 out of 54) of the participants had used AL as
annotation support.
The doubts of the participants who did not use
AL towards considering the technique as a poten-
tial aid in annotation in essence boil down to the
absence of an AL-based annotation editor, as well
as the difficulty in estimating the effective reduction
in effort (such as time, money, labor) that the use
of AL imply. Put simply: Can AL for NLP really
cut annotation costs? Can AL for NLP be practi-
cally realized without too much overhead in terms
of implementation and education of the annotator?
Research addressing the former question is ongo-
ing which is shown, e.g., by the recent Workshop on
Cost-Sensitive Learning held in conjunction with the
Neural Information Processing Systems Conference
2008. As for the latter question, there is evidently a
need of a general framework for AL in which (spe-
cialized) annotation editors can be realized. Also,
hand-in-hand with the theoretical aspects of AL and
their practical realizations in terms of available soft-
ware packages, there clearly is a need for usage and
user studies concerning the effort required by human
annotators operating under AL-based data selection
schemes in real annotation tasks.
Two things worth noticing among the answers
from participants of the survey that had used AL in-
clude that most of these participants had positive ex-
periences from using AL, although turn-around time
and consequently the idle time of the annotator re-
mains a critical issue; and that English was the only
language addressed. This is somewhat surprising
given that AL seems to be a technique well suited
for bootstrapping language resources for, e.g., so
called ?under resourced? languages. Also we were
surprised by the fact that both in AL and non-AL
projects rather ?unsophisticated? criteria were used
to decide about the stopping of annotation projects.
Acknowledgements
The first author was funded by the German Min-
istry of Education and Research within the STEM-
NET project (01DS001A-C) and the EC within the
BOOTStrep project (FP6-028099). We wish to
thank Bjo?rn Gamba?ck for commenting and proof-
reading drafts of this paper.
References
Markus Becker and Miles Osborne. 2005. A two-stage
method for active learning of statistical grammars. In
Proc. of the 19th International Joint Conference on Ar-
tificial Intelligence, pages 991?996.
Ido Dagan and Sean P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers. In
Proc. of the 12th International Conference on Machine
Learning, pages 150?157.
David D. Lewis and William A. Gale. 1994. A Sequen-
tial Algorithm for Training Text Classifiers. In Proc.
of the 17th Annual International ACM-SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3?12.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proc. of the Linguistic Annotation Workshop, pages
101?108.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, University
of Wisconsin-Madison.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proc. of the 42nd
Annual Meeting of the Association for Computational
Linguistics, pages 589?596.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proc. of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 486?495.
48
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 112?115,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Timed Annotations ? Enhancing MUC7 Metadata
by the Time It Takes to Annotate Named Entities
Katrin Tomanek and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
We report on the re-annotation of selected
types of named entities from the MUC7
corpus where our focus lies on record-
ing the time it takes to annotate these
entities given two basic annotation units
? sentences vs. complex noun phrases.
Such information may be helpful to lay
the empirical foundations for the develop-
ment of cost measures for annotation pro-
cesses based on the investment in time for
decision-making per entity mention.
1 Introduction
Manually supplied annotation metadata is at the
heart of (semi)supervised machine learning tech-
niques which have become very popular in NLP
research. At their flipside, they create an enor-
mous bottleneck because major shifts in the do-
main of discourse, the basic entities of interest, or
the text genre often require new annotation efforts.
But annotations are costly in terms of getting well-
trained and intelligible human resources involved.
Surprisingly, cost awareness has not been a pri-
mary concern in most of the past linguistic anno-
tation initiatives. Only recently, annotation strate-
gies (such as Active Learning (Cohn et al, 1996))
which strive for minimizing the annotation load
have gained increasing attention. Still, when it
comes to the empirically plausible assessment of
annotation costs even proponents of Active Learn-
ing make overly simplistic and empirically ques-
tionable assumptions, e.g., the uniformity of an-
notation costs over the number of linguistic units
(e.g., tokens) to be annotated.
We here consider the time it takes to annotate
a particular entity mention as a natural indicator
of effort for named entity annotations. In order to
lay the empirical foundations for experimentally
grounded annotation cost models we couple com-
mon named entity annotation metadata with a time
stamp reflecting the time measured for decision
making.1
Previously, two studies ? one dealing with POS
annotation (Haertel et al, 2008), the other with
named entity and relation annotation (Settles et al,
2008) ? have measured the time needed to anno-
tate sentences on small data sets and attempted to
learn predictive models of annotation cost. How-
ever, these data sets do not meet our requirements
as we envisage a large, coherent, and also well-
known newspaper entity corpus extended by an-
notation costs on a fine-grained level. Especially
size and coherence of such a corpus are not only
essential for building accurate cost models but also
as a reference baseline for cost-sensitive annota-
tion strategies. Moreover, the annotation level for
which cost information is available is crucial be-
cause document- or sentence-level data might be
too coarse for several applications. Accordingly,
this paper introduces MUC7T , our extension to the
entity annotations of the MUC7 corpus (Linguis-
tic Data Consortium, 2001) where time stamps are
added to two levels of annotation granularity, viz.
sentences and complex noun phrases.
2 Corpus Annotation
2.1 Annotation Task
Our annotation initiative constitutes an extension
to the named entity annotations (ENAMEX) of the
English part of the MUC7 corpus covering New
York Times articles from 1996. ENAMEX annota-
tions cover three types of named entities, viz. PER-
SONS, LOCATIONS, and ORGANIZATIONS. We in-
structed two human annotators, both advanced stu-
dents of linguistics with good English language
skills, to re-annotate the MUC7 corpus for the
ENAMEX subtask. To be as consistent as possi-
1These time stamps should not be confounded with the an-
notation of temporal expressions (TIMEX in MUC7, or even
more advanced metadata using TIMEML for the creation of
the TIMEBANK (Pustejovsky et al, 2003)).
112
ble with the existing MUC7 annotations, the an-
notators had to follow the original guidelines of
the MUC7 named entity task. For ease of re-
annotation, we intentionally ignored temporal and
number expressions (TIMEX and NUMEX).
MUC7 covers three distinct document sets for
the named entity task. We used one of these sets
to train the annotators and develop the annotation
design, and another one for our actual annotation
initiative which consists of 100 articles reporting
on airplane crashes. We split lengthy documents
(27 out of 100) into halves to fit on the annota-
tion screen without the need for scrolling. Further-
more, we excluded two documents due to over-
length which would have required overly many
splits. Our final corpus contains 3,113 sentences
(76,900 tokens) (see Section 3.1 for more details).
Time-stamped ENAMEX annotation of this cor-
pus constitutes MUC7T , our extension of MUC7.
Annotation time measurements were taken on two
syntactically different annotation units of single
documents: (a) complete sentences and (b) com-
plex noun phrases. The annotation task was de-
fined such as to assign an entity type label to each
token of an annotation unit. Sentence-level anno-
tation units where derived by the OPENNLP2 sen-
tence splitter. The use of complex noun phrases
(CNPs) as an alternative annotation unit is mo-
tivated by the fact that in MUC7 the syntactic
encoding of named entity mentions basically oc-
curs through nominal phrases. CNPs were derived
from the sentences? constituency structure using
the OPENNLP parser (trained on PENNTREE-
BANK data) to determine top-level noun phrases.
To avoid overly long phrases, CNPs dominating
special syntactic structures, such as coordinations,
appositions, or relative clauses, were split up at
discriminative functional elements (e.g., a relative
pronoun) and these elements were eliminated. An
evaluation of our CNP extractor on ENAMEX an-
notations in MUC7 showed that 98.95% of all en-
tities where completely covered by automatically
identified CNPs. For the remaining 1.05% of the
entity mentions, parsing errors were the most com-
mon source of incomplete coverage.
2.2 Annotation and Time Measurement
While the annotation task itself was ?officially?
declared to yield only annotations of named en-
tity mentions within the different annotation units,
2http://opennlp.sourceforge.net
we were primarily interested in the time needed
for these annotations. For precise time measure-
ments, single annotation examples were shown to
the annotators, one at a time. An annotation exam-
ple consists of the chosen MUC7 document with
one annotation unit (sentence or CNP) selected
and highlighted. Only the highlighted part of the
document could be annotated and the annotators
were asked to read only as much of the context sur-
rounding the annotation unit as necessary to make
a proper annotation decision. To present the an-
notation examples to annotators and allow for an-
notation without extra time overhead for the ?me-
chanical? assignment of entity types, our annota-
tion GUI is controlled by keyboard shortcuts. This
minimizes annotation time compared to mouse-
controlled annotation such that the measured time
reflects only the amount of time needed for taking
an annotation decision.
In order to avoid learning effects at the annota-
tors? side on originally consecutive syntactic sub-
units, we randomly shuffled all annotation exam-
ples so that subsequent annotation examples were
not drawn from the same document. Hence, an-
notation times were not biased by the order of ap-
pearance of the annotation examples.
Annotators were given blocks of either 500
CNP- or 100 sentence-level annotation examples.
They were asked to annotate each block in a single
run under noise-free conditions, without breaks
and disruptions. They were also instructed not to
annotate for too long stretches of time to avoid tir-
ing effects making time measurements unreliable.
All documents were first annotated with respect
to CNP-level examples within 2-3 weeks, with
only very few hours per day of concrete annota-
tion work. After completion of the CNP-level an-
notation, the same documents had to be annotated
on the sentence level as well. Due to randomiza-
tion and rare access to surrounding context during
the CNP-level annotation, annotators credibly re-
ported that they had indeed not remembered the
sentences from the CNP-level round. Thus, the
time measurements taken on the sentence level do
not seem to exhibit any human memory bias.
Both annotators went through all annotation ex-
amples so that we have double annotations of the
complete data set. Prior to coding, they indepen-
dently got used to the annotation guidelines and
were trained on several hundred examples. For the
annotators? performance see Section 3.2.
113
3 Analysis
3.1 Corpus Statistics
Table 1 summarizes statistics on the time-stamped
MUC7 corpus. About 60% of all tokens are cov-
ered by CNPs (45,097 out of 76,900 tokens) show-
ing that sentences are made up from CNPs to a
large extent. Still, removing the non-CNP to-
kens markedly reduces the amount of tokens to
be considered for entity annotation. CNPs cover
slightly less entities (3,937) than complete sen-
tences (3,971), a marginal loss only.
sentences 3,113
sentence tokens 76,900
chunks 15,203
chunk tokens 45,097
entity mentions in sentences 3,971
entity mentions in CNPs 3,937
sentences with entity mentions 63%
CNPs with entity mentions 23%
Table 1: Descriptive statistics of time-stamped MUC7 corpus
On the average, sentences have a length of
24.7 tokens, while CNPs are rather short with
3.0 tokens, on the average. However, CNPs vary
tremendously in their length, with the shortest
ones having only one token and the longest ones
(mostly due to parsing errors) spanning over 30
(and more) tokens. Figure 1 depicts the length
distribution of sentences and CNPs showing that
a reasonable portion of CNPs have less than five
tokens, while the distribution of sentence lengths
almost follows a normal distribution in the interval
[0, 50]. While 63% of all sentences contain at least
one entity mention, only 23% of CNPs contain en-
tity mentions. These statistics show that CNPs are
generally rather short and a large fraction of CNPs
does not contain entity mentions at all. We may
hypothesize that this observation will be reflected
by annotation times.
sentence length
tokens
freq
uenc
y
0 20 40 60 80
0
100
300
500
CNP length
tokens
freq
uenc
y
0 5 10 15 20
0
200
0
600
0
Figure 1: Length distribution of sentences and CNPs
3.2 Annotation Performance
To test the validity of the guidelines and the gen-
eral performance of our annotators A and B, we
compared their annotation results on 5 blocks of
sentence-level annotation examples created dur-
ing training. Annotation performance was mea-
sured in terms of Cohen?s kappa coefficient ? on
the token level and entity-segment F -score against
MUC7 annotations. The annotators achieved
?A = 0.95 and ?B = 0.96, and FA = 0.92
and FB = 0.94, respectively.3 Moreover, they ex-
hibit an inter-annotator agreement of ?A,B = 0.94
and an averaged mutual F-score of FA,B = 0.90.
These numbers reveal that the task is well-defined
and the annotators have sufficiently internalized
the annotation guidelines to produce valid results.
Figure 2 shows the annotators? scores against
the original MUC7 annotations for the 31 blocks
of sentence-level annotations (3,113 sentences)
which range from ? = 0.89 to ? = 0.98. Largely,
annotation performance is similar for both anno-
tators and shows that they consistently found a
block either rather hard or easy to annotate. More-
over, annotation performance seems stationary ?
no general trend in annotation performance over
time can be observed.
l l l l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l l
l
l l
l l
l l
l l
0 5 10 15 20 25 300
.80
0.85
0.90
0.95
1.00
sentence?level annotation
blocks
kapp
a l
l l l l l l
l
l l l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l annotator Aannotator B
Figure 2: Average kappa coefficient per block
3.3 Time Measurements
Figure 3 shows the average annotation time per
block (CNPs and sentences). Considering the
CNP-level annotations, there is a learning effect
for annotator B during the first 9 blocks. Af-
ter that, both annotators are approximately on a
par regarding the annotation time. For sentence-
level annotations, both annotators again yield sim-
ilar annotation times per block, without any learn-
ing effects. Similar to annotation performance,
3Entity-specific F-scores against MUC7 annotations for
A and B are 0.90 and 0.92 for LOCATION, 0.92 and 0.93
for ORGANIZATION, and 0.96 and 0.98 for PERSON, respec-
tively.
114
l l l l l
l l l
l l
l
l l l l l l
l
l l
l l l l
l
l l l
l
l
0 5 10 15 20 25 30
1.0
1.2
1.4
1.6
1.8
2.0
CNP?level annotation
blocks
sec
onds
l
l l
l
l l
l
l l
l l
l l l l l l
l l l l
l
l
l
l
l
l annotator Aannotator B
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l
l l
l
l l
l
l
0 5 10 15 20 25 30
4.0
4.5
5.0
5.5
6.0
sentence?level annotation
blocks
sec
onds
l
l l l
l
l
l
l
l
l
l
l
l l
l
l
l l l l
l
l
l l
l l
l
l
l
l
l annotator Aannotator B
Figure 3: Average annotation times per block
analysis of annotation time shows that the annota-
tion behavior is largely stationary (excluding first
rounds of CNP-level annotation) which allows sin-
gle time measurements to be interpreted indepen-
dently of previous time measurements. Both, time
and performance plots exhibit that there are blocks
which were generally harder or easier than other
ones because both annotators operated in tandem.
3.4 Easy and Hard Annotation Examples
As we have shown, inter-annotator variation of
annotation performance is moderate. Intra-block
performance, in contrast, is subject to high vari-
ance. Figure 4 shows the distribution of annota-
tor A?s CNP-level annotation times for block 20.
A?s average annotation time on this block amounts
to 1.37 seconds per CNP, the shortest time be-
ing 0.54, the longest one amounting 10.2 seconds.
The figure provides ample evidence for an ex-
tremely skewed time investment for coding CNPs.
A preliminary manual analysis revealed CNPs
with very low annotation times are mostly short
and consist of stop words and pronouns only, or
CNP?level annotation
annotation time
freq
uenc
y
2 4 6 8 10
05
0
150
250
Figure 4: Distribution of annotation times in one block
are otherwise simple noun phrases with a sur-
face structure incompatible with entity mentions
(e.g., all tokens are lower-cased). Here, humans
can quickly exclude the occurrence of entity men-
tions which results in low annotation times. CNPs
which took desparately long (more than 6 seconds)
were outliers indicating distraction or loss of con-
centration. Times between 3 and 5 seconds were
basically caused by semantically complex CNPs.
4 Conclusions
We have created a time-stamped version of MUC7
entity annotations, MUC7T , on two levels of anno-
tation granularity ? sentences and complex noun
phrases. Especially the phrase-level annotations
allow for fine-grained time measurement. We will
use this corpus for studies on (time) cost-sensitive
Active Learning. MUC7T can also be used to de-
rive or learn accurate annotation cost models al-
lowing to predict annotation time on new data. We
are currently investigating causal factors of anno-
tation complexity for named entity annotation on
the basis of MUC7T .
Acknowledgements
This work was funded by the EC within the
BOOTStrep (FP6-028099) and CALBC (FP7-
231727) projects. We want to thank Oleg Lichten-
wald (JULIE Lab) for implementing the noun
phrase extractor for our experiments.
References
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129?145.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
costs of sampling methods in active learning for an-
notation. In Proceedings of the ACL-08: HLT, Short
Papers, pages 65?68.
Linguistic Data Consortium. 2001. Message Under-
standing Conference 7. LDC2001T02. FTP file.
James Pustejovsky, Patrick Hanks, Roser Saur??, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
corpus. In Proceedings of the Corpus Linguistics
2003 Conference, pages 647?656.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Pro-
ceedings of the NIPS?08 Workshop on Cost Sensitive
Learning, pages 1?10.
115
Coling 2010: Poster Volume, pages 1247?1255,
Beijing, August 2010
A Comparison of Models for Cost-Sensitive Active Learning
Katrin Tomanek and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
http://www.julielab.de
Abstract
Active Learning (AL) is a selective sam-
pling strategy which has been shown to
be particularly cost-efficient by drastically
reducing the amount of training data to be
manually annotated. For the annotation
of natural language data, cost efficiency
is usually measured in terms of the num-
ber of tokens to be considered. This mea-
sure, assuming uniform costs for all to-
kens involved, is, from a linguistic per-
spective at least, intrinsically inadequate
and should be replaced by a more ade-
quate cost indicator, viz. the time it takes
to manually label selected annotation ex-
amples. We here propose three differ-
ent approaches to incorporate costs into
the AL selection mechanism and evaluate
them on the MUC7T corpus, an extension
of the MUC7 newspaper corpus that con-
tains such annotation time information.
Our experiments reveal that using a cost-
sensitive version of semi-supervised AL,
up to 54% of true annotation time can be
saved compared to random selection.
1 Introduction
Active Learning (AL) is a selective sampling strat-
egy for determining those annotation examples
which are particularly informative for classifier
training, while discarding those that are already
easily predictable for the classifier given previous
training experience. While the efficiency of AL
has already been shown for many NLP tasks based
on measuring the number of tokens or sentences
that are saved in comparison to random sampling
(e.g., Engelson and Dagan (1996), Tomanek et al
(2007) or Settles and Craven (2008)), it is obvious
that just counting tokens under the assumption of
uniform annotation costs for each token is empir-
ically questionable, from a linguistic perspective,
at least.
As an alternative, we here explore annotation
costs that incur for AL based on an empirically
more plausible cost metric, viz. the time it takes
to annotate selected linguistic examples. We in-
vestigate three approaches to incorporate costs
into the AL selection mechanism by modifying
the standard (fully supervised) mode of AL and
a non-standard semi-supervised one according to
cost considerations. The empirical backbone of
this comparison is constituted by MUC7T , a re-
annotation of a part of the MUC7 newspaper
corpus that contains annotation time information
(Tomanek and Hahn, 2010).
2 Active Learning
Unlike random sampling, AL is a selective sam-
pling technique where the learner is in control of
the data to be chosen for training. By design, the
intention behind AL is to reduce annotation costs,
usually considered as the amount of labeled train-
ing material required to achieve a particular target
performance of the model. The latter is yielded
by querying labels only for those examples which
are assumed to have a high training utility. In this
section, we introduce different AL frameworks ?
the default, fully supervised AL approach (Sec-
tion 2.1), as well as a semi-supervised variant of
it (Section 2.2). In Section 2.3 we then propose
three methods how these approaches to AL can be
made cost-sensitive without further modifications.
1247
2.1 Fully Supervised AL (FuSAL)
As we consider AL for the NLP task of Named
Entity Recognition (NER), some design decisions
have to be made. Firstly, the selection granular-
ity is set to complete sentences ? a reasonable lin-
guistic annotation unit which still allows for fairly
precise selection. Second, a batch of examples in-
stead of a single example is selected per AL iter-
ation to reduce the computational overhead of the
sampling process.
We base our approach to AL on Conditional
Random Fields (CRFs), which we employ as base
learners (Lafferty et al, 2001). For observation
sequences ~x = (x1, . . . , xn) and label sequences
~y = (y1, . . . , yn), a linear-chain CRF is defined as
P?(~y|~x) =
1
Z?(~x)
?
n?
i=1
exp
k?
j=1
?jfj
(
yi?1, yi, ~x, i
)
where Z?(~x) is the normalization factor, and k
feature functions fj(?) with feature weights ? =
(?1, . . . , ?k) appear.
The core of any AL approach is a utility func-
tion u(p, ?) which estimates the informativeness
of each example p, a complete sentence p = (~x),
drawn from the pool P of all unlabeled examples,
for model induction. For our experiments, we em-
ploy two alternative utility functions which have
produced the best results in previous experiments
(Tomanek, 2010, Chapter 4). The first utility func-
tion is based on the confidence of a CRF model ?
in the predicted label sequence ~y? which is given
by the probability distribution P?(~y?|~x). The util-
ity function based on this probability boils down
to
uLC(p, ?) = 1? P?(~y ?|~x)
so that sentences for which the predicted label se-
quence ~y? has a low probability is granted a high
utility. Instead of calculating the model?s con-
fidence on the complete sequence, we might al-
ternatively calculate the model?s confidence in its
predictions on single tokens. To obtain an overall
confidence for the complete sequence, the aver-
age over the single token-confidence values can be
computed by the marginal probability P?(yi|~x).
Now that we are calculating the confidence on the
token level, we might also obtain the performance
of the second best label and calculate the margin
between the first and second best label as a con-
fidence score so that the final utility function is
obtained by
uMA(p, ?) = ?
1
n
n?
i=1
(
max
y?inY
P?(yi = y?|~x)?
max
y??inY
y? 6=y??
P?(yi = y??|~x)
)
Algorithm 1 formalizes our AL framework.
Depending on the utility function, the best b ex-
amples are selected per round, manually labeled,
and then added to the set of labeled data L which
feeds the classifier for the next training round.
Algorithm 1 NER-specific AL Framework
Given:
b: number of examples to be selected in each iteration
L: set of labeled examples l = (~x, ~y) ? Xn ? Yn
P: set of unlabeled examples p = (~x) ? Xn
T (L): a learning algorithm
u(p, ?): utility function
Algorithm:
loop until stopping criterion is met
1. learn model: ? ? T (L)
2. sort p ? P: let S ? (p1, . . . , pm) : u(pi, ?) ?
u(pi+1, ?), i ? [1,m], p ? P
3. select b examples pi with highest utility from S: B ?
{p1, . . . , pb}, b ? m, pi ? S
4. query labels for all p ? B: B? ? {l1, . . . , lb}
5. L ? L ? B?, P ? P \ B
return L? ? L and ?? ? T (L?)
The specification is still not cost-sensitive as the
selection of examples depends only on the utility
function. Using uLC will result in a reduction of
the number of examples (i.e., sentences) selected
irrespective of the sentence length so that a model
learns the most from it. As a result, we observed
that the selected sentences are quite long which
might even cause higher annotation costs per sen-
tence (Tomanek, 2010, Chapter 4). As for uMA
there is at least a slight normalization sensitive
to costs since the sum over all token-level utility
scores is normalized by the length of the selected
sentence.
1248
2.2 Semi-supervised AL (SeSAL)
Tomanek and Hahn (2009) extendeded this stan-
dard fully supervised AL framework by a semi-
supervised variant (SeSAL). The selection of sen-
tences is performed in a standard manner, i.e.,
similarly to the procedure in Algorithm 1. How-
ever, once selected, rather than manually annotat-
ing the complete sentence, only (uncertain) sub-
sequences of each selected sentence are manually
labeled, while the remaining (certain) ones are au-
tomatically annotated using the current version of
the classifier.
After the selection of an informative example
p = (~x) with ~x = (x1, . . . , xn), the subsequences
~x? = (xa, . . . , xb), 1 ? a ? b ? n, with low local
uncertainty have to be identified. For reasons of
simplicity, only sequences of length 1, i.e., single
tokens, are considered. For a token xi from a se-
lected sequence ~x the model?s confidence C?(y?i )
in label y?i is estimated. Token-level confidence
for a CRF is calculated as the marginal probabil-
ity so that
C?(y?i ) = P?(yi = y?i |~x)
where y?i specifies the label at the respective posi-
tion of the predicted label sequence ~y ? (the one
which is obtained by the Viterbi algorithm). If
C?(y?i ) exceeds a confidence threshold t, y?i is as-
signed as the putatively correct label. Otherwise,
manual annotation of this token is required.
Employing SeSAL, savings of over 80 % of the
tokens compared to random sampling are reported
by Tomanek and Hahn (2009). Even when com-
pared to FuSAL, still 60 % of the number of to-
kens are eliminated. A crucial question, however,
not answered in these experiments, is whether this
method actually reduces the overall annotation ex-
penses in time rather than just in the number of to-
kens. Also SeSAL does not incorporate labeling
costs in the selection process.
2.3 Cost-Sensitive AL (CoSAL)
In this section, we turn to an extension of FuSAL
and SeSAL which incorporates cost sensitivity
into the AL selection process (CoSAL). Three
different approaches of CoSAL will be explored.
The challenge we now face is that two contradic-
tory criteria ? utility and costs ? have to be bal-
anced.
2.3.1 Cost-Constrained Sampling
CoSAL can be realized in the most straight-
forward way by simply constraining the sampling
to a particular maximum cost cmax per example.
Therefore, in a pre-processing step all examples
p ? P for which cost(p) > cmax are removed from
P . The unmodified NER-specific AL framework
can then be applied.
An obvious shortcoming of Cost-Constrained
Sampling (CCS) is that it precludes any form of
compensation between utility and costs. Thus, an
exceptionally useful example with a cost factor
slightly above cmax will be rejected. Another crit-
ical issue is how to fix cmax. If chosen too low,
the pre-filtering of P results in a much too strong
restriction of selection options when only few ex-
amples remain inside P . If chosen too high, the
cost constraint becomes ineffective.
2.3.2 Linear Rank Combination
A general solution to fit different criteria into
a single one is by way of linear combination.
If, however, different units of measurement are
used, a transformation function for the alignment
of benefit, or utility, and costs must be found. This
can be difficult to determine. In our scenario, ben-
efits measured by utility scores and costs mea-
sured in seconds are clearly incommensurable. As
it is not immediately evident how to express utility
in monetary terms (or vice versa), we transform
utility and cost information into ranks R(u(p, ?))
andR?(cost(p)) instead. As for utility, higher util-
ity leads to higher ranks. As for costs, lower costs
lead to higher ranks. The linear rank combination
(LRK) is defined as
?LRK(~v(p)) = ?R
(
u(p, ?)
)
+(1??)R?
(cost(p))
where ? is a weighting term. In a CoSAL sce-
nario, where utility is the primary criterion, ? >
0.5 seems a reasonable choice. Alternatively, as
costs and utility are contradictory, allowing equal
influence for both criteria, as with ? = 0.5, it
may be difficult to find appropriate examples in
a medium-sized corpus. Thus, the choice of ? de-
pends on size and diversity with respect to combi-
nations of utility and costs within P .
1249
2.3.3 Benefit-Cost Ratio
Our third approach to CoSAL is based on the
Benefit-Cost Ratio (BCR). Given equal units of
measurement for benefits and costs, the benefit-
cost ratio indicates whether a scenario is profitable
(ratio > 1). BCR can also be applied when units
are incommensurable and a transformation func-
tion is available, as is the case for the combination
of utility and cost. This holds as long as bene-
fit and costs can be expressed in the same units
by a linear transformation function, i.e., u(p, ?) =
? ? cost(p) + b. If such a transformation function
exists, one can refrain from finding proper values
for the above variables b and ? and instead calcu-
late BCR as
?BCR(p) =
u(p, ?)
cost(p)
Since annotation costs are usually expressed on
a linear scale, this is also required for utility, if
we want to use BCR. But when utility is based
on model confidence as we do it here, this prop-
erty gets lost.1 Hence a non-linear transforma-
tion function is needed to fit the scales of utility
and costs. Assuming a linear relationship between
utility and costs, BCR has already been applied
by Haertel et al (2008) and Settles et al (2008).
Our approach provides a crucial extension as we
explicitly consider scenarios where such a linear
relationship is not given and a non-linear transfor-
mation function is required instead.
In a direct comparison of LRK with BCR, LRK
may be used when such a transformation function
would be needed but is unknown and hard to find.
Choosing LRK over BCR is also motivated by
findings in the context of data fusion in informa-
tion retrieval where Hsu and Taksa (2005) remark
that, given incommensurable units and scales, one
would do better when ranks rather than the actual
scores or values were combined.
3 Experiments
In the following, we study possible benefits of
CoSAL, relative to FuSAL and SeSAL, in the
1Though normalized to [0, 1], confidence estimates, es-
pecially for sequence classification, are often not on a linear
scale so that confidence values that are twice as high do not
necessarily mean that the benefit in training a model on such
an example is doubled.
light of real annotation times as a cost measure
(instead of the standard, yet inadequate one, viz.
the number of tokens being selected). Such timing
data is available in the MUC7T corpus (Tomanek
and Hahn, 2010), a re-annotation of the MUC7
corpus containing the ENAMEX types (persons,
locations, and organizations) and a time stamp re-
flecting the time it took annotators to decide on
each entity type. The MUC7T corpus contains
3,113 sentences (76,900 tokens).
The results we report on are averaged over 20
independent runs. For each run, we split the
MUC7T corpus randomly into a pool to select
from (90%) and an evaluation set (10%). AL was
started from a random seed set of 20 sentences.
As utility scores to estimate benefits we applied
uMA and uLC as defined in Section 2.1.
The plots in the following sections depict costs
in terms of annotation time (in seconds) relative
to annotation quality (expressed via F1-scores).
Learning curves are only shown for early AL it-
erations. Later on, in the convergence phase, due
to the two conflicting criteria now considered si-
multaneously, selection options become more and
more scarce so that CoSAL necessarily performs
sub-optimally.
3.1 Parametrization of CoSAL Approaches
Preparatory experiments were run to analyze how
different parameters affected different CoSAL set-
tings. For the CCS and LRK experiments, we
used the uLC utility function.
For CCS, we tested three cmax values, viz. 7.5,
10, and 15, to determine the maximum perfor-
mance attainable on MUC7T when only examples
below the chosen threshold were included. Our
choices of the maximum were based on the dis-
tributions of annotation times over the sentences
(see Figure 1) where 7.5s marks the 75% quantile
and 15s is just above the 90% quantile. For 7.5s,
we peaked at Fmax = 0.84, for 10s at Fmax =
0.86, and for 15s at Fmax = 0.88. Figure 2
(top) shows the learning curves of CoSAL with
CCS and different cmax values. With cmax = 15,
as could be expected from the boxplot in Fig-
ure 1, no difference can be observed compared
to cost-insensitive FuSAL. CCS with lower val-
ues for cmax stagnates at the maximum perfor-
1250
seconds
freq
uen
cy
0 5 10 15 20 25 30
0
200
400
600
800
Figure 1: Distribution of annotation times per sen-
tence in MUC7T .
mance reported above, but still improves upon
cost-insensitive FuSAL in early AL iterations.
At some point in time all economical exam-
ples, with costs below cmax but high utility, have
been consumed from the corpus. Even in a cor-
pus much larger than MUC7T this effect will only
occur with some delay. Indeed, any choice of a re-
strictive value for cmax will cause similar exhaus-
tion effects. Unfortunately, it is unclear how to
tune cmax suitably in a real-life annotation sce-
nario where pretests for maximum performance
for a particular cmax are not possible. For further
experiments, we chose cmax = 10.
For LRK, we tested three different weights ?,
viz. 0.5, 0.75, and 0.9. Figure 2 (bottom) shows
their effects on the learning curves. Similar ten-
dencies as for cmax for CCS can be observed.
With ? = 0.9, CoSAL does not fall below default
FuSAL, at least in the observed range. A lower
weight of ? = 0.75 results in larger improve-
ments in earlier AL iterations but then falls back
to FuSAL and in later AL iterations (not shown
here) even below FuSAL. If the time parameter
is granted too much influence, as with ? = 0.5,
performance even drops to random selection level.
This might also be due to corpus exhaustion. For
further experiments, we chose ? = 0.75 because
of its potential to improve upon FuSAL in early
iterations.
For BCR with uMA, we change this utility func-
tion to n ? uMA to compensate for the normaliza-
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for CCS
seconds
F?s
cor
e
CCS 15s
CCS 10s
CCS 7.5s
FuSAL : uLC
RS
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for LRK
seconds
F?s
cor
e
LRK 0.9
LRK 0.75
LRK 0.5
FuSAL : uLC
RS
Figure 2: Different parameter settings for CCS
and LRK based on FuSAL with uLC as utility
function. FuSAL: uLC refers to cost-insensitive
FuSAL, CCS and LRK to the cost-sensitive ver-
sions of FuSAL with the respective parameters.
tion by token length which is otherwise already
contained in uMA(n is the length of the respective
sentence). For uLC, the preparatory experiments
already showed that this utility function does not
behave on a linear scale. This is so because uLC is
based on P?(~y|~x) for confidence estimation of the
complete label sequence ~y. Hence, a uLC score
twice as high does not indicate doubled benefit for
classifier training. Thus, we need a non-linear cal-
ibration function to transform uLC into a proper
utility estimator on a linear scale so that BCR can
be applied.
To determine such a non-linear calibration
function, the true benefit of an example p would
1251
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
ll
l l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
lll
l
ll
l
l
l
l
l
l
l
l
l
l
0.5 0.6 0.7 0.8 0.9 1.0
0
2
4
6
8
uLC
n?
u M
A
corr: 0.6494
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l l
l
l l
l
ll
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
0e+00 1e+08 2e+08 3e+08 4e+08 5e+08
0
2
4
6
8
e??uLC
n?
u M
A
corr: 0.8959
Figure 3: Scatter plots for (a) uLC versus n?uMA and (b) e??uLC versus n?uMA
be needed. In the absence of such informa-
tion, we consider n ? uMA as a good approxima-
tion. To identify the relationship between uLC and
n ? uMA, we trained a model on a random subsam-
ple from P ? ? P and used this model to obtain
the scores for uLC and n ? uMA for each example
from the test set T .2 Figure 3 (left) shows a scat-
ter plot of these scores which provides ample evi-
dence that the relationship between uLC and ben-
efit is indeed non-linear. As calibration function
for uLC we propose f(p) = e??uLC(p). Experi-
mentally, we determined ? = 20 as a good value.
Figure 3 (right) reveals that e??uLC(p) is a better
utility estimator; the correlation with n ? uMA is
now corr = 0.8959 and the relationship is close
to being linear.
In Figure 4, learning curves for BCR with the
utility function uLC and the calibrated function
e??uLC(p) are compared. BCR with the uncali-
brated utility function uLC fails miserably (the
performance falls even below random selection).
This adds credibility to our claim that while uLC
may be appropriate for ranking examples (as for
standard, cost-insensitive AL), it is inappropriate
for estimating true benefit/utility which is needed
when costs are to be incorporated with the BCR
method. BCR with the calibrated utility e??uLC(p),
in contrast, outperforms cost-insensitive FuSAL.
For further experiments with BCR, we either ap-
ply n?uMA or e??uLC(p) as utility functions.
2We experimented with different sizes forP ?, with almost
identical results.
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for BCR
seconds
F?s
cor
e
BCR : e20?uLC
BCR : uLC
FuSAL : uLC
RS
Figure 4: Different parameter settings for BCR
3.2 Comparison of CoSAL Approaches
We compared all three approaches to CoSAL in
the parametrization chosen above for the utility
functions uMA and uLC. Learning curves are
shown in Figure 5. Improvements over cost-
insensitive AL are only achieved in early AL iter-
ations up to 2,500s (for CoSAL based on uMA) or
4,000s (for CoSAL based on uLC) of annotation
time. This exclusiveness of early improvements
can be explained by the size of the corpus and, by
this, the limited number of good selection options.
Since AL selects with respect to two conflicting
criteria, the pool P should be much larger to in-
crease the chance for examples that are favorable
with respect to both criteria.
1252
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
utility function : uMA
seconds
F?s
cor
e
CCS (10s)
LRK (0.75)
BCR : n ? uMA
FuSAL : uMA
RS
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
utility function : uLC
seconds
F?s
cor
e
CCS (10s)
LRK (0.75)
BCR : e20?uLC
FuSAL : uLC
RS
Figure 5: Comparison of CoSAL approaches for the utility functions uMA and uLC. Baseline given by
random selection (RS) and standard FuSAL with either uMA or uLC.
Improvements for CoSAL based on uLC are
generally higher than for uMA. Moreover, cost-
insensitive AL based on uLC does not exhibit any
normalization where, in contrast, uMA is normal-
ized at least to the number of tokens per example.
In CoSAL, both uLC and uMA are normalized by
costs, which is methodologically a more substan-
tial enhancement for uLC than for uMA.
For CoSAL based on uMA we cannot proclaim
a clear winner among the different approaches.
All three CoSAL approaches improve upon cost-
insensitive AL. For CoSAL based on uLC, LRK
performs best, while CCS and BCR perform simi-
larly well. Given this result, we might prefer LRK
or CCS over BCR. A disadvantage of the first two
approaches is that they require corpus-specific pa-
rameters which may be difficult to find for a new
learning problem for which no data for experi-
mentation is at hand. Though not the best per-
former, BCR does not require further parametriza-
tion and appears more appropriate for real-life an-
notation projects ? as long as utility is an appro-
priate estimator for benefit. CoSAL with BCR has
already been studied by Settles et al (2008). They
also applied a utility function based on sequence-
confidence estimation which presumably, as with
our uLC utility function, is not a good benefit esti-
mator. The fact that Settles et al did not explicitly
treat this issue might explain why cost-sensitive
AL based on BCR often performed worse than
cost-insensitive AL in their experiments.
3.3 CoSAL Applied to SeSAL
We looked at a cost-sensitive version of SeSAL by
applying the cost-sensitive FuSAL approach to-
gether with BCR and the transformation function
for the utility as discussed above. On top of this
selection, we ran the standard SeSAL approach ?
only tokens below a confidence threshold were se-
lected for annotation. The following experiments
are all based on the uLC utility function (and the
transformation function of it).
Figure 6 depicts learning curves for cost-
insensitive and cost-sensitive SeSAL and FuSAL
which reveal that cost-sensitive SeSAL consid-
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
seconds
F?s
cor
e
SeSAL BCR
FuSAL BCR
SeSAL
FuSAL
RS
Figure 6: Cost-sensitive (BCR variants) vs. cost-
insensitive FuSAL and SeSAL with uLC as utility
function.
1253
erably outperforms cost-sensitive FuSAL. Cost-
sensitive SeSAL attains a target performance of
F=0.85 with only 2806s, while cost-sensitive
FuSAL needs 3410s, and random selection con-
sumes over 6060s. Thus, cost-sensitive SeSAL
here reduces true annotation time by about 54 %
compared to random selection, whereas cost-
sensitive FuSAL reduces annotation time by only
44 %.
4 Related Work
Although the reduction of data acquisition costs
that result from human labeling efforts have al-
ways been the main driver for AL studies, cost-
sensitive AL is a new branch of AL. In an early
study on cost metrics for AL, Becker and Osborne
(2005) examined whether AL, while decreasing
the sample size on the one hand, on the other
hand increased annotation efforts. For a real-
world AL annotation project, they demonstrated
that the actual sampling efficiency measure for
an AL approach depends on the cost metric be-
ing applied. In a companion paper, Hachey et al
(2005) studied how sentences selected by AL af-
fected the annotators? performance both in terms
of the time needed and the annotation accuracy
achieved. They found that selectively sampled ex-
amples are, on the average, more difficult to anno-
tate than randomly sampled ones. This observa-
tion, for the first time, questioned the widespread
assumption that all annotation examples can be as-
signed a uniform cost factor.
Making a standard AL approach cost-sensitive
by normalizing utility in terms of annotation time
has been proposed before by Haertel et al (2008),
Settles et al (2008), and Donmez and Carbonell
(2008). CoSAL based on the net-benefit (costs
subtracted from utility) was proposed by Vijaya-
narasimhan and Grauman (2009) for object recog-
nition in images and Kapoor et al (2007) for voice
message classification.
5 Conclusions
We investigated three approaches to incorporate
the notion of cost into the AL selection mecha-
nism, including a fixed maximal cost budget per
example, a linear rank combination to express net-
benefit, and a benefit-cost ratio. The cost metric
we applied was the time needed by human coders
for annotating particular annotation examples.
Among the three approaches to cost-sensitive
AL, we see a slight advantage for benefit cost ra-
tios in real-world settings because they do not re-
quire additional corpus-specific parametrization,
once a proper calibration function is found.
Another observation is that advantages of
the three cost-sensitive AL models over cost-
insensitive ones consistently occur only in early
iteration rounds ? a result we attribute to corpus
exhaustion effects since cost-sensitive AL selects
for two criteria (utility and cost) and thus requires
a extremely large pool to be able to pick up really
advantageous examples. Consequently, applied
to real-world annotation settings where the pools
may be extremely large, we expect cost-sensitive
approaches to be even more effective in terms of
the reduction of annotation time.
To be applicable in real-world scenarios, anno-
tation costs which, in our experiments, were di-
rectly traceable in the MUC7T corpus have to be
estimated since they are not known prior to anno-
tation. In Tomanek et al (2010), we investigated
the reading behavior during named entity annota-
tion using eye-tracking technology. With the in-
sights gained from this study on crucial factors in-
fluencing annotation time we were able to induce
such a much needed predictive model of annota-
tion costs. In future work, we plan to incorporate
this empirically founded cost model into our ap-
proaches to cost-sensitive AL and to investigate
whether our positive findings can be reproduced
with estimated costs as well.
Acknowledgements
This work was partially funded by the EC within
the CALBC (FP7-231727) project.
References
Becker, Markus and Miles Osborne. 2005. A two-
stage method for active learning of statistical gram-
mars. In IJCAI?05 ? Proceedings of the 19th Inter-
national Joint Conference on Artificial Intelligence,
pages 991?996. Edinburgh, Scotland, UK, July 31 -
August 5, 2005.
1254
Donmez, Pinar and Jaime Carbonell. 2008. Proactive
learning: Cost-sensitive active learning with mul-
tiple imperfect oracles. In CIKM?08 ? Proceed-
ing of the 17th ACM conference on Information
and Knowledge Management, pages 619?628. Napa
Valley, CA, USA, October 26-30, 2008.
Engelson, Sean and Ido Dagan. 1996. Minimizing
manual annotation cost in supervised training from
corpora. In ACL?96 ? Proceedings of the 34th An-
nual Meeting of the Association for Computational
Linguistics, pages 319?326. Santa Cruz, CA, USA,
June 24-27, 1996.
Hachey, Ben, Beatrice Alex, and Markus Becker.
2005. Investigating the effects of selective sampling
on the annotation task. In CoNLL?05 ? Proceed-
ings of the 9th Conference on Computational Natu-
ral Language Learning, pages 144?151. Ann Arbor,
MI, USA, June 29-30, 2005.
Haertel, Robbie, Kevin Seppi, Eric Ringger, and James
Carroll. 2008. Return on investment for active
learning. In Proceedings of the NIPS 2008 Work-
shop on Cost-Sensitive Machine Learning. Whistler,
BC, Canada, December 13, 2008.
Hsu, Frank and Isak Taksa. 2005. Comparing rank and
score combination methods for data fusion in infor-
mation retrieval. Information Retrieval, 8(3):449?
480.
Kapoor, Ashish, Eric Horvitz, and Sumit Basu. 2007.
Selective supervision: Guiding supervised learning
with decision-theoretic active learning. In IJCAI?07
? Proceedings of the 20th International Joint Con-
ference on Artifical Intelligence, pages 877?882.
Hyderabad, India, January 6-12, 2007.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML?01 ? Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289. Williamstown, MA, USA, June
28 - July 1, 2001.
Settles, Burr and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP?08 ? Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1069?1078. Waikiki, Hon-
olulu, Hawaii, USA, October 25-27, 2008.
Settles, Burr, Mark Craven, and Lewis Friedland.
2008. Active learning with real annotation costs. In
Proceedings of the NIPS 2008 Workshop on Cost-
Sensitive Machine Learning. Whistler, BC, Canada,
December 13, 2008.
Tomanek, Katrin and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
ACL/IJCNLP?09 ? Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natu-
ral Language Processing, pages 1039?1047. Singa-
pore, August 2-7, 2009.
Tomanek, Katrin and Udo Hahn. 2010. Annotation
time stamps: Temporal metadata from the linguistic
annotation process. In LREC?10 ? Proceedings of
the 7th International Conference on Language Re-
sources and Evaluation. La Valletta, Malta, May 17-
23, 2010.
Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains cor-
pus reusability of annotated data. In EMNLP-
CoNLL?07 ? Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Language Learning,
pages 486?495. Prague, Czech Republic, June 28-
30, 2007.
Tomanek, Katrin, Udo Hahn, Steffen Lohmann, and
Ju?rgen Ziegler. 2010. A cognitive cost model of an-
notations based on eye-tracking data. In ACL?10 ?
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics. Uppsala,
Sweden, July 11-16, 2010.
Tomanek, Katrin. 2010. Resource-Aware Annotation
through Active Learning. Ph.D. thesis, Technical
University of Dortmund.
Vijayanarasimhan, Sudheendra and Kristen Grauman.
2009. What?s it going to cost you? predicting ef-
fort vs. informativeness for multi-label image anno-
tations. CVPR?09 ? Proceedings of the 2009 IEEE
Computer Vision and Pattern Recognition Confer-
ence.
1255
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1158?1167,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Cognitive Cost Model of Annotations Based on Eye-Tracking Data
Katrin Tomanek
Language & Information
Engineering (JULIE) Lab
Universita?t Jena
Jena, Germany
Udo Hahn
Language & Information
Engineering (JULIE) Lab
Universita?t Jena
Jena, Germany
Steffen Lohmann
Dept. of Computer Science &
Applied Cognitive Science
Universita?t Duisburg-Essen
Duisburg, Germany
Ju?rgen Ziegler
Dept. of Computer Science &
Applied Cognitive Science
Universita?t Duisburg-Essen
Duisburg, Germany
Abstract
We report on an experiment to track com-
plex decision points in linguistic meta-
data annotation where the decision behav-
ior of annotators is observed with an eye-
tracking device. As experimental con-
ditions we investigate different forms of
textual context and linguistic complexity
classes relative to syntax and semantics.
Our data renders evidence that annotation
performance depends on the semantic and
syntactic complexity of the decision points
and, more interestingly, indicates that full-
scale context is mostly negligible ? with
the exception of semantic high-complexity
cases. We then induce from this obser-
vational data a cognitively grounded cost
model of linguistic meta-data annotations
and compare it with existing non-cognitive
models. Our data reveals that the cogni-
tively founded model explains annotation
costs (expressed in annotation time) more
adequately than non-cognitive ones.
1 Introduction
Today?s NLP systems, in particular those rely-
ing on supervised ML approaches, are meta-data
greedy. Accordingly, in the past years, we have
witnessed a massive quantitative growth of anno-
tated corpora. They differ in terms of the nat-
ural languages and domains being covered, the
types of linguistic meta-data being solicited, and
the text genres being served. We have seen large-
scale efforts in syntactic and semantic annotations
in the past related to POS tagging and parsing,
on the one hand, and named entities and rela-
tions (propositions), on the other hand. More re-
cently, we are dealing with even more challeng-
ing issues such as subjective language, a large
variety of co-reference and (e.g., RST-style) text
structure phenomena, Since the NLP community
is further extending their work into these more and
more sophisticated semantic and pragmatic analyt-
ics, there seems to be no end in sight for increas-
ingly complex and diverse annotation tasks.
Yet, producing annotations is pretty expensive.
So the question comes up, how we can rationally
manage these investments so that annotation cam-
paigns are economically doable without loss in an-
notation quality. The economics of annotations are
at the core of Active Learning (AL) where those
linguistic samples are focused on in the entire doc-
ument collection, which are estimated as being
most informative to learn an effective classifica-
tion model (Cohn et al, 1996). This intentional
selection bias stands in stark contrast to prevailing
sampling approaches where annotation examples
are randomly chosen.
When different approaches to AL are compared
with each other, or with standard random sam-
pling, in terms of annotation efficiency, up until
now, the AL community assumed uniform annota-
tion costs for each linguistic unit, e.g. words. This
claim, however, has been shown to be invalid in
several studies (Hachey et al, 2005; Settles et al,
2008; Tomanek and Hahn, 2010). If uniformity
does not hold and, hence, the number of annotated
units does not indicate the true annotation efforts
required for a specific sample, empirically more
adequate cost models are needed.
Building predictive models for annotation costs
has only been addressed in few studies for now
(Ringger et al, 2008; Settles et al, 2008; Arora
et al, 2009). The proposed models are based
on easy-to-determine, yet not so explanatory vari-
ables (such as the number of words to be anno-
tated), indicating that accurate models of anno-
tation costs remain a desideratum. We here, al-
ternatively, consider different classes of syntac-
tic and semantic complexity that might affect the
cognitive load during the annotation process, with
1158
the overall goal to find additional and empirically
more adequate variables for cost modeling.
The complexity of linguistic utterances can be
judged either by structural or by behavioral crite-
ria. Structural complexity emerges, e.g., from the
static topology of phrase structure trees and pro-
cedural graph traversals exploiting the topology
of parse trees (see Szmrecsa?nyi (2004) or Cheung
and Kemper (1992) for a survey of metrics of this
type). However, structural complexity criteria do
not translate directly into empirically justified cost
measures and thus have to be taken with care.
The behavioral approach accounts for this prob-
lem as it renders observational data of the an-
notators? eye movements. The technical vehicle
to gather such data are eye-trackers which have
already been used in psycholinguistics (Rayner,
1998). Eye-trackers were able to reveal, e.g.,
how subjects deal with ambiguities (Frazier and
Rayner, 1987; Rayner et al, 2006; Traxler and
Frazier, 2008) or with sentences which require
re-analysis, so-called garden path sentences (Alt-
mann et al, 2007; Sturt, 2007).
The rationale behind the use of eye-tracking de-
vices for the observation of annotation behavior is
that the length of gaze durations and behavioral
patterns underlying gaze movements are consid-
ered to be indicative of the hardness of the lin-
guistic analysis and the expenditures for the search
of clarifying linguistic evidence (anchor words) to
resolve hard decision tasks such as phrase attach-
ments or word sense disambiguation. Gaze dura-
tion and search time are then taken as empirical
correlates of linguistic complexity and, hence, un-
cover the real costs. We therefore consider eye-
tracking as a promising means to get a better un-
derstanding of the nature of the linguistic annota-
tion processes with the ultimate goal of identifying
predictive factors for annotation cost models.
In this paper, we first describe an empirical
study where we observed the annotators? reading
behavior while annotating a corpus. Section 2
deals with the design of the study, Section 3 dis-
cusses its results. In Section 4 we then focus on
the implications this study has on building cost
models and compare a simple cost model mainly
relying on word and character counts and addi-
tional simple descriptive characteristics with one
that can be derived from experimental data as pro-
vided from eye-tracking. We conclude with ex-
periments which reveal that cognitively grounded
models outperform simpler ones relative to cost
prediction using annotation time as a cost mea-
sure. Based on this finding, we suggest that cog-
nitive criteria are helpful for uncovering the real
costs of corpus annotation.
2 Experimental Design
In our study, we applied, for the first time ever to
the best of our knowledge, eye-tracking to study
the cognitive processes underlying the annotation
of linguistic meta-data, named entities in particu-
lar. In this task, a human annotator has to decide
for each word whether or not it belongs to one of
the entity types of interest.
We used the English part of the MUC7 corpus
(Linguistic Data Consortium, 2001) for our study.
It contains New York Times articles from 1996 re-
porting on plane crashes. These articles come al-
ready annotated with three types of named entities
considered important in the newspaper domain,
viz. ?persons?, ?locations?, and ?organizations?.
Annotation of these entity types in newspaper
articles is admittedly fairly easy. We chose this
rather simple setting because the participants in
the experiment had no previous experience with
document annotation and no serious linguistic
background. Moreover, the limited number of
entity types reduced the amount of participants?
training prior to the actual experiment, and posi-
tively affected the design and handling of the ex-
perimental apparatus (see below).
We triggered the annotation processes by giving
our participants specific annotation examples. An
example consists of a text document having one
single annotation phrase highlighted which then
had to be semantically annotated with respect to
named entity mentions. The annotation task was
defined such that the correct entity type had to be
assigned to each word in the annotation phrase. If
a word belongs to none of the three entity types a
fourth class called ?no entity? had to be assigned.
The phrases highlighted for annotation were
complex noun phrases (CNPs), each a sequence of
words where a noun (or an equivalent nominal ex-
pression) constitutes the syntactic head and thus
dominates dependent words such as determin-
ers, adjectives, or other nouns or nominal expres-
sions (including noun phrases and prepositional
phrases). CNPs with even more elaborate inter-
nal syntactic structures, such as coordinations, ap-
positions, or relative clauses, were isolated from
1159
their syntactic host structure and the intervening
linguistic material containing these structures was
deleted to simplify overly long sentences. We also
discarded all CNPs that did not contain at least
one entity-critical word, i.e., one which might be a
named entity according to its orthographic appear-
ance (e.g., starting with an upper-case letter). It
should be noted that such orthographic signals are
by no means a sufficient condition for the presence
of a named entity mention within a CNP.
The choice of CNPs as stimulus phrases is mo-
tivated by the fact that named entities are usually
fully encoded by this kind of linguistic structure.
The chosen stimulus ? an annotation example with
one phrase highlighted for annotation ? allows for
an exact localization of the cognitive processes
and annotation actions performed relative to that
specific phrase.
2.1 Independent Variables
We defined two measures for the complexity of
the annotation examples: The syntactic complex-
ity was given by the number of nodes in the con-
stituent parse tree which are dominated by the an-
notation phrase (Szmrecsa?nyi, 2004).1 According
to a threshold on the number of nodes in such a
parse tree, we classified CNPs as having either
high or low syntactic complexity.
The semantic complexity of an annotation ex-
ample is based on the inverse document frequency
df of the words in the annotation phrase according
to a reference corpus.2 We calculated the seman-
tic complexity score of an annotation phrase as
max i 1df (wi) , where wi is the i-th word of the anno-
tation phrase. Again, we empirically determined a
threshold classifying annotation phrases as having
either high or low semantic complexity. Addition-
ally, this automatically generated classification
was manually checked and, if necessary, revised
by two annotation experts. For instance, if an an-
notation phrase contained a strong trigger (e.g., a
social role or job title, as with ?spokeswoman? in
the annotation phrase ?spokeswoman Arlene?), it
was classified as a low-semantic-complexity item
even though it might have been assigned a high
inverse document frequency (due to the infrequent
word ?Arlene?).
1Constituency parse structure was obtained from the
OPENNLP parser (http://opennlp.sourceforge.
net/) trained on PennTreeBank data.
2We chose the English part of the Reuters RCV2 corpus
as the reference corpus for our experiments.
Two experimental groups were formed to study
different contexts. In the document context con-
dition the whole newspaper article was shown as
annotation example, while in the sentence context
condition only the sentence containing the annota-
tion phrase was presented. The participants3 were
randomly assigned to one of these groups. We de-
cided for this between-subjects design to avoid any
irritation of the participants caused by constantly
changing contexts. Accordingly, the participants
were assigned to one of the experimental groups
and corresponding context condition already in the
second training phase that took place shortly be-
fore the experiment started (see below).
2.2 Hypotheses and Dependent Variables
We tested the following two hypotheses:
Hypothesis H1: Annotators perform differently
in the two context conditions.
H1 is based on the linguistically plausible
assumption that annotators are expected to
make heavy use of the surrounding context
because such context could be helpful for the
correct disambiguation of entity classes. Ac-
cordingly, lacking context, an annotator is ex-
pected to annotate worse than under the con-
dition of full context. However, the availabil-
ity of (too much) context might overload and
distract annotators, with a presumably nega-
tive effect on annotation performance.
Hypothesis H2: The complexity of the annota-
tion phrases determines the annotation per-
formance.
The assumption is that high syntactic or se-
mantic complexity significantly lowers the
annotation performance.
In order to test these hypotheses we collected data
for the following dependent variables: (a) the an-
notation accuracy ? we identified erroneous enti-
ties by comparison with the original gold annota-
tions in the MUC7 corpus, (b) the time needed per
annotation example, and (c) the distribution and
duration of the participants? eye gazes.
320 subjects (12 female) with an average age of 24 years
(mean = 24, standard deviation (SD) = 2.8) and normal or
corrected-to-normal vision capabilities took part in the study.
All participants were students with a computing-related study
background, with good to very good English language skills
(mean = 7.9, SD = 1.2, on a ten-point scale with 1 = ?poor?
and 10 = ?excellent?, self-assessed), but without any prior
experience in annotation and without previous exposure to
linguistic training.
1160
2.3 Stimulus Material
According to the above definition of complex-
ity, we automatically preselected annotation ex-
amples characterized by either a low or a high de-
gree of semantic and syntactic complexity. After
manual fine-tuning of the example set assuring an
even distribution of entity types and syntactic cor-
rectness of the automatically derived annotation
phrases, we finally selected 80 annotation exam-
ples for the experiment. These were divided into
four subsets of 20 examples each falling into one
of the following complexity classes:
sem-syn: low semantic/low syntactic complexity
SEM-syn: high semantic/low syntactic complexity
sem-SYN: low semantic/high syntactic complexity
SEM-SYN: high semantic/high syntactic complexity
2.4 Experimental Apparatus and Procedure
The annotation examples were presented in a
custom-built tool and its user interface was kept
as simple as possible not to distract the eye move-
ments of the participants. It merely contained one
frame showing the text of the annotation example,
with the annotation phrase being highlighted. A
blank screen was shown after each annotation ex-
ample to reset the eyes and to allow a break, if
needed. The time the blank screen was shown was
not counted as annotation time. The 80 annotation
examples were presented to all participants in the
same randomized order, with a balanced distribu-
tion of the complexity classes. A variation of the
order was hardly possible for technical and ana-
lytical reasons but is not considered critical due to
extensive, pre-experimental training (see below).
The limitation on 80 annotation examples reduces
the chances of errors due to fatigue or lack of at-
tention that can be observed in long-lasting anno-
tation activities.
Five introductory examples (not considered in
the final evaluation) were given to get the subjects
used to the experimental environment. All anno-
tation examples were chosen in a way that they
completely fitted on the screen (i.e., text length
was limited) to avoid the need for scrolling (and
eye distraction). The position of the CNP within
the respective context was randomly distributed,
excluding the first and last sentence.
The participants used a standard keyboard to as-
sign the entity types for each word of the annota-
tion example. All but 5 keys were removed from
the keyboard to avoid extra eye movements for fin-
ger coordination (three keys for the positive en-
tity classes, one for the negative ?no entity? class,
and one to confirm the annotation). Pre-tests had
shown that the participants could easily issue the
annotations without looking down at the keyboard.
We recorded the participant?s eye movements
on a Tobii T60 eye-tracking device which is in-
visibly embedded in a 17? TFT monitor and com-
paratively tolerant to head movements. The partic-
ipants were seated in a comfortable position with
their head in a distance of 60-70 cm from the mon-
itor. Screen resolution was set to 1280 x 1024 px
and the annotation examples were presented in the
middle of the screen in a font size of 16 px and a
line spacing of 5 px. The presentation area had no
fixed height and varied depending on the context
condition and length of the newspaper article. The
text was always vertically centered on the screen.
All participants were familiarized with the
annotation task and the guidelines in a pre-
experimental workshop where they practiced an-
notations on various exercise examples (about 60
minutes). During the next two days, one after the
other participated in the actual experiment which
took between 15 and 30 minutes, including cali-
bration of the eye-tracking device. Another 20-30
minutes of training time directly preceded the ex-
periment. After the experiment, participants were
interviewed and asked to fill out a questionnaire.
Overall, the experiment took about two hours for
each participant for which they were financially
compensated. Participants were instructed to fo-
cus more on annotation accuracy than on annota-
tion time as we wanted to avoid random guess-
ing. Accordingly, as an extra incentive, we re-
warded the three participants with the highest an-
notation accuracy with cinema vouchers. None of
the participants reported serious difficulties with
the newspaper articles or annotation tool and all
understood the annotation task very well.
3 Results
We used a mixed-design analysis of variance
(ANOVA) model to test the hypotheses, with the
context condition as between-subjects factor and
the two complexity classes as within-subject fac-
tors.
3.1 Testing Context Conditions
To test hypothesis H1 we compared the number
of annotation errors on entity-critical words made
1161
above before anno phrase after below
percentage of participants looking at a sub-area 35% 32% 100% 34% 16%
average number of fixations per sub-area 2.2 14.1 1.3
Table 1: Distribution of annotators? attention among sub-areas per annotation example.
by the annotators in the two contextual conditions
(complete document vs. sentence). Surprisingly,
on the total of 174 entity-critical words within
the 80 annotation examples, we found exactly the
same mean value of 30.8 errors per participant in
both conditions. There were also no significant
differences in the average time needed to annotate
an example in both conditions (means of 9.2 and
8.6 seconds, respectively, with F (1, 18) = 0.116,
p = 0.74).4 These results seem to suggest that it
makes no difference (neither for annotation accu-
racy nor for time) whether or not annotators are
shown textual context beyond the sentence that
contains the annotation phrase.
To further investigate this finding we analyzed
eye-tracking data of the participants gathered for
the document context condition. We divided the
whole text area into five sub-areas as schemat-
ically shown in Figure 1. We then determined
the average proportion of participants that directed
their gaze at least once at these sub-areas. We con-
sidered all fixations with a minimum duration of
100 ms, using a fixation radius (i.e., the smallest
distance that separates fixations) of 30 px and ex-
cluded the first second (mainly used for orientation
and identification of the annotation phrase).
Figure 1: Schematic visualization of the sub-areas
of an annotation example.
Table 1 reveals that on average only 35% of the
4In general, we observed a high variance in the number of
errors and time values between the subjects. While, e.g., the
fastest participant handled an example in 3.6 seconds on the
average, the slowest one needed 18.9 seconds; concerning
the annotation errors on the 174 entity-critical words, these
ranged between 21 and 46 errors.
participants looked in the textual context above the
annotation phrase embedding sentence, and even
less perceived the context below (16%). The sen-
tence parts before and after the annotation phrase
were, on the average, visited by one third (32%
and 34%, respectively) of the participants. The
uneven distribution of the annotators? attention be-
comes even more apparent in a comparison of the
total number of fixations on the different text parts:
14 out of an average of 18 fixations per example
were directed at the annotation phrase and the sur-
rounding sentence, the text context above the an-
notation chunk received only 2.2 fixations on the
average and the text context below only 1.3.
Thus, the eye-tracking data indicates that the
textual context is not as important as might have
been expected for quick and accurate annotation.
This result can be explained by the fact that par-
ticipants of the document-context condition used
the context whenever they thought it might help,
whereas participants of the sentence-context con-
dition spent more time thinking about a correct an-
swer, overall with the same result.
3.2 Testing Complexity Classes
To test hypothesis H2 we also compared the av-
erage annotation time and the number of errors
on entity-critical words for the complexity subsets
(see Table 2). The ANOVA results show highly
significant differences for both annotation time
and errors.5 A pairwise comparison of all sub-
sets in both conditions with a t-test showed non-
significant results only between the SEM-syn and
syn-SEM subsets.6
Thus, the empirical data generally supports hy-
pothesis H2 in that the annotation performance
seems to correlate with the complexity of the an-
notation phrase, on the average.
5Annotation time results: F (1, 18) = 25, p < 0.01 for
the semantic complexity and F (1, 18) = 76.5, p < 0.01
for the syntactic complexity; Annotation complexity results:
F (1, 18) = 48.7, p < 0.01 for the semantic complexity and
F (1, 18) = 184, p < 0.01 for the syntactic complexity.
6t(9) = 0.27, p = 0.79 for the annotation time in the
document context condition, and t(9) = 1.97, p = 0.08 for
the annotation errors in the sentence context condition.
1162
experimental complexity e.-c. time errors
condition class words mean SD mean SD rate
sem-syn 36 4.0s 2.0 2.7 2.1 .075
document SEM-syn 25 9.2s 6.7 5.1 1.4 .204
condition sem-SYN 51 9.6s 4.0 9.1 2.9 .178
SEM-SYN 62 14.2s 9.5 13.9 4.5 .224
sem-syn 36 3.9s 1.3 1.1 1.4 .031
sentence SEM-syn 25 7.5s 2.8 6.2 1.9 .248
condition sem-SYN 51 9.6s 2.8 9.0 3.9 .176
SEM-SYN 62 13.5s 5.0 14.5 3.4 .234
Table 2: Average performance values for the 10 subjects of each experimental condition and 20 anno-
tation examples of each complexity class: number of entity-critical words, mean annotation time and
standard deviations (SD), mean annotation errors, standard deviations, and error rates (number of errors
divided by number of entity-critical words).
3.3 Context and Complexity
We also examined whether the need for inspect-
ing the context increases with the complexity of
the annotation phrase. Therefore, we analyzed the
eye-tracking data in terms of the average num-
ber of fixations on the annotation phrase and on
its embedding contexts for each complexity class
(see Table 3). The values illustrate that while the
number of fixations on the annotation phrase rises
generally with both the semantic and the syntactic
complexity, the number of fixations on the context
rises only with semantic complexity. The num-
ber of fixations on the context is nearly the same
for the two subsets with low semantic complexity
(sem-syn and sem-SYN, with 1.0 and 1.5), while
it is significantly higher for the two subsets with
high semantic complexity (5.6 and 5.0), indepen-
dent of the syntactic complexity.7
complexity fix. on phrase fix. on context
class mean SD mean SD
sem-syn 4.9 4.0 1.0 2.9
SEM-syn 8.1 5.4 5.6 5.6
sem-SYN 18.1 7.7 1.5 2.0
SEM-SYN 25.4 9.3 5.0 4.1
Table 3: Average number of fixations on the anno-
tation phrase and context for the document condi-
tion and 20 annotation examples of each complex-
ity class.
These results suggest that the need for context
mainly depends on the semantic complexity of the
annotation phrase, while it is less influenced by its
syntactic complexity.
7ANOVA result of F (1, 19) = 19.7, p < 0.01 and sig-
nificant differences also in all pairwise comparisons.
phrase antecedent
Figure 2: Annotation example with annotation
phrase and the antecedent for ?Roselawn? in the
text (left), and gaze plot of one participant show-
ing a scanning-for-coreference behavior (right).
This finding is also qualitatively supported by
the gaze plots we generated from the eye-tracking
data. Figure 2 shows a gaze plot for one partici-
pant that illustrates a scanning-for-coreference be-
havior we observed for several annotation phrases
with high semantic complexity. In this case, words
were searched in the upper context, which accord-
ing to their orthographic signals might refer to a
named entity but which could not completely be
resolved only relying on the information given by
the annotation phrase itself and its embedding sen-
tence. This is the case for ?Roselawn? in the an-
notation phrase ?Roselawn accident?. The con-
text reveals that Roselawn, which also occurs in
the first sentence, is a location. A similar proce-
dure is performed for acronyms and abbreviations
which cannot be resolved from the immediate lo-
cal context ? searches mainly visit the upper con-
text. As indicated by the gaze movements, it also
became apparent that texts were rather scanned for
hints instead of being deeply read.
1163
4 Cognitively Grounded Cost Modeling
We now discuss whether the findings on dependent
variables from our eye-tracking study are fruitful
for actually modeling annotation costs. There-
fore, we learn a linear regression model with time
(an operationalization of annotation costs) as the
dependent variable. We compare our ?cognitive?
model against a baseline model which relies on
some simple formal text features only, and test
whether the newly introduced features help predict
annotation costs more accurately.
4.1 Features
The features for the baseline model, character- and
word-based, are similar to the ones used by Ring-
ger et al (2008) and Settles et al (2008).8 Our
cognitive model, however, makes additional use
of features based on linguistic complexity, and in-
cludes syntactic and semantic criteria related to the
annotation phrases. These features were inspired
by the insights provided by our eye-tracking ex-
periments. All features are designed such that they
can automatically be derived from unlabeled data,
a necessary condition for such features to be prac-
tically applicable.
To account for our findings that syntactic and
semantic complexity correlates with annotation
performance, we added three features based on
syntactic, and two based on semantic complex-
ity measures. We decided for the use of multiple
measures because there is no single agreed-upon
metric for either syntactic or semantic complex-
ity. This decision is further motivated by find-
ings which reveal that different measures are often
complementary to each other so that their combi-
nation better approximates the inherent degrees of
complexity (Roark et al, 2007).
As for syntactic complexity, we use two mea-
sures based on structural complexity including (a)
the number of nodes of a constituency parse tree
which are dominated by the annotation phrase
(cf. Section 2.1), and (b) given the dependency
graph of the sentence embedding the annotation
phrase, we consider the distance between words
for each dependency link within the annotation
phrase and consider the maximum over such dis-
8In preliminary experiments our set of basic features com-
prised additional features providing information on the usage
of stop words in the annotation phrase and on the number
of paragraphs, sentences, and words in the respective annota-
tion example. However, since we found these features did not
have any significant impact on the model, we removed them.
tance values as another metric for syntactic com-
plexity. Lin (1996) has already shown that human
performance on sentence processing tasks can be
predicted using such a measure. Our third syn-
tactic complexity measure is based on the prob-
ability of part-of-speech (POS) 2-grams. Given
a POS 2-gram model, which we learned from
the automatically POS-tagged MUC7 corpus, the
complexity of an annotation phrase is defined by
?n
i=2 P (POSi|POSi?1) where POSi refers to the
POS-tag of the i-th word of the annotation phrase.
A similar measure has been used by Roark et al
(2007) who claim that complex syntactic struc-
tures correlate with infrequent or surprising com-
binations of POS tags.
As far as the quantification of semantic com-
plexity is concerned, we use (a) the inverse docu-
ment frequency df (wi) of each word wi (cf. Sec-
tion 2.1), and a measure based on the semantic
ambiguity of each word, i.e., the number of mean-
ings contained in WORDNET,9 within an annota-
tion phrase. We consider the maximum ambigu-
ity of the words within the annotation phrase as
the overall ambiguity of the respective annotation
phrase. This measure is based on the assumption
that annotation phrases with higher semantic am-
biguity are harder to annotate than low-ambiguity
ones. Finally, we add the Flesch-Kincaid Read-
ability Score (Klare, 1963), a well-known metric
for estimating the comprehensibility and reading
complexity of texts.
As already indicated, some of the hardness of
annotations is due to tracking co-references and
abbreviations. Both often cannot be resolved lo-
cally so that annotators need to consult the con-
text of an annotation chunk (cf. Section 3.3).
Thus, we also added features providing informa-
tion whether the annotation phrases contain entity-
critical words which may denote the referent of an
antecedent of an anaphoric relation. In the same
vein, we checked whether an annotation phrase
contains expressions which can function as an ab-
breviation by virtue of their orthographical appear-
ance, e.g., consist of at least two upper-case letters.
Since our participants were sometimes scanning
for entity-critical words, we also added features
providing information on the number of entity-
critical words within the annotation phrase. Ta-
ble 4 enumerates all feature classes and single fea-
tures used for determining our cost model.
9http://wordnet.princeton.edu/
1164
Feature Group # Features Feature Description
characters (basic) 6 number of characters and words per annotation phrase; test whether
words in a phrase start with capital letters, consist of capital letters only,
have alphanumeric characters, or are punctuation symbols
words 2 number of entity-critical words and percentage of entity-critical words
in the annotation phrase
complexity 6 syntactic complexity: number of dominated nodes, POS n-gram proba-
bility, maximum dependency distance;
semantic complexity: inverse document frequency, max. ambiguity;
general linguistic complexity: Flesch-Kincaid Readability Score
semantics 3 test whether entity-critical word in annotation phrase is used in docu-
ment (preceding or following current phrase); test whether phrase con-
tains an abbreviation
Table 4: Features for cost modeling.
4.2 Evaluation
To test how well annotation costs can be mod-
eled by the features described above, we used the
MUC7T corpus, a re-annotation of the MUC7 cor-
pus (Tomanek and Hahn, 2010). MUC7T has time
tags attached to the sentences and CNPs. These
time tags indicate the time it took to annotate the
respective phrase for named entity mentions of the
types person, location, and organization. We here
made use of the time tags of the 15,203 CNPs in
MUC7T . MUC7T has been annotated by two an-
notators (henceforth called A and B) and so we
evaluated the cost models for both annotators. We
learned a simple linear regression model with the
annotation time as dependent variable and the fea-
tures described above as independent variables.
The baseline model only includes the basic feature
set, whereas the ?cognitive? model incorporates all
features described above.
Table 5 depicts the performance of both mod-
els induced from the data of annotator A and B.
The coefficient of determination (R2) describes
the proportion of the variance of the dependent
variable that can be described by the given model.
We report adjusted R2 to account for the different
numbers of features used in both models.
model R2 on A?s data R2 on B?s data
baseline 0.4695 0.4640
cognitive 0.6263 0.6185
Table 5: Adjusted R2 values on both models and
for annotators A and B.
For both annotators, the baseline model is sig-
nificantly outperformed in terms of R2 by our
?cognitive? model (p < 0.05). Considering the
features that were inspired from the eye-tracking
study, R2 is increased from 0.4695 to 0.6263 on
the timing data of annotator A, and from 0.464 to
0.6185 on the data of annotator B. These numbers
clearly demonstrate that annotation costs are more
adequately modelled by the additional features we
identified through our eye-tracking study.
Our ?cognitive? model now consists of 21 co-
efficients. We tested for the significance of this
model?s regression terms. For annotator A we
found all coefficients to be significant with respect
to the model (p < 0.05), for annotator B all coeffi-
cients except one were significant. Figure 6 shows
the coefficients of annotator A?s ?cognitive? model
along with the standard errors and t-values.
5 Summary and Conclusions
In this paper, we explored the use of eye-tracking
technology to investigate the behavior of human
annotators during the assignment of three types of
named entities ? persons, organizations and loca-
tions ? based on the eye-mind assumption. We
tested two main hypotheses ? one relating to the
amount of contextual information being used for
annotation decisions, the other relating to differ-
ent degrees of syntactic and semantic complex-
ity of expressions that had to be annotated. We
found experimental evidence that the textual con-
text is searched for decision making on assigning
semantic meta-data at a surprisingly low rate (with
1165
Feature Group Feature Name/Coefficient Estimate Std. Error t value Pr(>|t|)
(Intercept) 855.0817 33.3614 25.63 0.0000
characters (basic) token number -304.3241 29.6378 -10.27 0.0000
char number 7.1365 2.2622 3.15 0.0016
has token initcaps 244.4335 36.1489 6.76 0.0000
has token allcaps -342.0463 62.3226 -5.49 0.0000
has token alphanumeric -197.7383 39.0354 -5.07 0.0000
has token punctuation -303.7960 50.3570 -6.03 0.0000
words number tokens entity like 934.3953 13.3058 70.22 0.0000
percentage tokens entity like -729.3439 43.7252 -16.68 0.0000
complexity sem compl inverse document freq 392.8855 35.7576 10.99 0.0000
sem compl maximum ambiguity -13.1344 1.8352 -7.16 0.0000
synt compl number dominated nodes 87.8573 7.9094 11.11 0.0000
synt compl pos ngram probability 287.8137 28.2793 10.18 0.0000
syn complexity max dependency distance 28.7994 9.2174 3.12 0.0018
flesch kincaid readability -0.4117 0.1577 -2.61 0.0090
semantics has entity critical token used above 73.5095 24.1225 3.05 0.0023
has entity critical token used below -178.0314 24.3139 -7.32 0.0000
has abbreviation 763.8605 73.5328 10.39 0.0000
Table 6: ?Cognitive? model of annotator A.
the exception of tackling high-complexity seman-
tic cases and resolving co-references) and that an-
notation performance correlates with semantic and
syntactic complexity.
The results of these experiments were taken as
a heuristic clue to focus on cognitively plausi-
ble features of learning empirically rooted cost
models for annotation. We compared a simple
cost model (basically taking the number of words
and characters into account) with a cognitively
grounded model and got a much higher fit for the
cognitive model when we compared cost predic-
tions of both model classes on the recently re-
leased time-stamped version of the MUC7 corpus.
We here want to stress the role of cognitive evi-
dence from eye-tracking to determine empirically
relevant features for the cost model. The alterna-
tive, more or less mechanical feature engineering,
suffers from the shortcoming that is has to deal
with large amounts of (mostly irrelevant) features
? a procedure which not only requires increased
amounts of training data but also is often compu-
tationally very expensive.
Instead, our approach introduces empirical,
theory-driven relevance criteria into the feature
selection process. Trying to relate observables
of complex cognitive tasks (such as gaze dura-
tion and gaze movements for named entity anno-
tation) to explanatory models (in our case, a time-
based cost model for annotation) follows a much
warranted avenue in research in NLP where fea-
ture farming becomes a theory-driven, explanatory
process rather than a much deplored theory-blind
engineering activity (cf. ACL-WS-2005 (2005)).
In this spirit, our focus has not been on fine-
tuning this cognitive cost model to achieve even
higher fits with the time data. Instead, we aimed at
testing whether the findings from our eye-tracking
study can be exploited to model annotation costs
more accurately.
Still, future work will be required to optimize
a cost model for eventual application where even
more accurate cost models may be required. This
optimization may include both exploration of ad-
ditional features (such as domain-specific ones)
as well as experimentation with other, presum-
ably non-linear, regression models. Moreover,
the impact of improved cost models on the effi-
ciency of (cost-sensitive) selective sampling ap-
proaches, such as Active Learning (Tomanek and
Hahn, 2009), should be studied.
1166
References
ACL-WS-2005. 2005. Proceedings of the ACL Work-
shop on Feature Engineering for Machine Learn-
ing in Natural Language Processing. accessible
via http://www.aclweb.org/anthology/
W/W05/W05-0400.pdf.
Gerry Altmann, Alan Garnham, and Yvette Dennis.
2007. Avoiding the garden path: Eye movements
in context. Journal of Memory and Language,
31(2):685?712.
Shilpa Arora, Eric Nyberg, and Carolyn Rose?. 2009.
Estimating annotation cost for active learning in a
multi-annotator environment. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 18?26.
Hintat Cheung and Susan Kemper. 1992. Competing
complexity metrics and adults? production of com-
plex sentences. Applied Psycholinguistics, 13:53?
76.
David Cohn, Zoubin Ghahramani, and Michael Jordan.
1996. Active learning with statistical models. Jour-
nal of Artificial Intelligence Research, 4:129?145.
Lyn Frazier and Keith Rayner. 1987. Resolution of
syntactic category ambiguities: Eye movements in
parsing lexically ambiguous sentences. Journal of
Memory and Language, 26:505?526.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL 2005 ? Proceedings of
the 9th Conference on Computational Natural Lan-
guage Learning, pages 144?151.
George Klare. 1963. The Measurement of Readability.
Ames: Iowa State University Press.
Dekang Lin. 1996. On the structural complexity of
natural language sentences. In COLING 1996 ? Pro-
ceedings of the 16th International Conference on
Computational Linguistics, pages 729?733.
Linguistic Data Consortium. 2001. Message Under-
standing Conference (MUC) 7. Philadelphia: Lin-
guistic Data Consortium.
Keith Rayner, Anne Cook, Barbara Juhasz, and Lyn
Frazier. 2006. Immediate disambiguation of lex-
ically ambiguous words during reading: Evidence
from eye movements. British Journal of Psychol-
ogy, 97:467?482.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 126:372?422.
Eric Ringger, Marc Carmen, Robbie Haertel, Kevin
Seppi, Deryle Lonsdale, Peter McClanahan, James
Carroll, and Noel Ellison. 2008. Assessing the
costs of machine-assisted corpus annotation through
a user study. In LREC 2008 ? Proceedings of the 6th
International Conference on Language Resources
and Evaluation, pages 3318?3324.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for
detecting mild cognitive impairment. In Proceed-
ings of the Workshop on BioNLP 2007: Biological,
Translational, and Clinical Language Processing,
pages 1?8.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In
Proceedings of the NIPS 2008 Workshop on Cost-
Sensitive Machine Learning, pages 1?10.
Patrick Sturt. 2007. Semantic re-interpretation and
garden path recovery. Cognition, 105:477?488.
Benedikt M. Szmrecsa?nyi. 2004. On operationalizing
syntactic complexity. In Proceedings of the 7th In-
ternational Conference on Textual Data Statistical
Analysis. Vol. II, pages 1032?1039.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
ACL 2009 ? Proceedings of the 47th Annual Meet-
ing of the ACL and the 4th IJCNLP of the AFNLP,
pages 1039?1047.
Katrin Tomanek and Udo Hahn. 2010. Annotation
time stamps: Temporal metadata from the linguistic
annotation process. In LREC 2010 ? Proceedings of
the 7th International Conference on Language Re-
sources and Evaluation.
Matthew Traxler and Lyn Frazier. 2008. The role of
pragmatic principles in resolving attachment ambi-
guities: Evidence from eye movements. Memory &
Cognition, 36:314?328.
1167
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 235?242,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Proposal for a Configurable Silver Standard
Udo Hahn, Katrin Tomanek, Elena Beisswanger and Erik Faessler
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
http://www.julielab.de
Abstract
Among the many proposals to promote al-
ternatives to costly to create gold stan-
dards, just recently the idea of a fully au-
tomatically, and thus cheaply, to set up sil-
ver standard has been launched. However,
the current construction policy for such a
silver standard requires crucial parameters
(such as similarity thresholds and agree-
ment cut-offs) to be set a priori, based on
extensive testing though, at corpus com-
pile time. Accordingly, such a corpus is
static, once it is released. We here propose
an alternative policy where silver stan-
dards can be dynamically optimized and
customized on demand (given a specific
goal function) using a gold standard as an
oracle.
1 Introduction
Training natural language systems which rely on
(semi-)supervised machine learning algorithms,
or measuring the systems? performance requires
some standardized ground truth from which one
can learn or against which one evaluate, respec-
tively. Usually, a manually crafted gold stan-
dard is provided that is generated by human lan-
guage or domain experts after lots of iterative,
guideline-based training rounds. This procedure is
expensive, slow and yields only small, yet highly
trustable, amounts of meta data ? because human
experts are in the loop.
In the CALBC project,1 an alternative ap-
proach is currently under investigation (Rebholz-
Schuhmann et al, 2010a). The basic idea is to
generate the much needed ground truth automati-
cally. This is achieved by letting a flock of named
entity taggers run on a corpus, without impos-
ing any restriction on the type(s) being annotated.
1http://www.calbc.eu
The (most likely) heterogeneous results are auto-
matically homogenized subsequently, thus yield-
ing a consensus-based, machine-generated ground
truth. Considering the possible benefits (e.g., the
positive experience from boosting-style machine
learners (Freund, 1990)), but also being aware of
the possible drawbacks (varying quality of the dif-
ferent systems, skewed coverage of entity types,
different types of guidelines on which they were
trained, etc.), the CALBC consortium refers to
the outcome of this process as a silver standard
(Rebholz-Schuhmann et al, 2010a). This proce-
dure is inexpensive, fast, yields huge amounts of
meta data ? because computers are in the loop ?
but after all its applicability and validity has yet to
be determined experimentally.
The first silver standard corpus (SSC) that came
out of the CALBC project was generated by the
four main partners? named entity taggers.2 The
various contributions covered, among others, an-
notations for genes and proteins, chemicals, dis-
eases, etc (Rebholz-Schuhmann et al, 2010b). Af-
ter the submission of their runs, the SSC was gen-
erated by, first, harmonizing stretches of text in
terms of entity mention identification and, second,
by mapping these normalized mentions to agreed-
upon type systems (such as the MESH Semantic
Groups as described by Bodenreider and McCray
(2003) for entity type normalization). Basically,
the harmonization steps included rules when en-
tity mentions were considered to match or overlap
(using a cosine-based similarity criterion) and en-
tity types referred to the same class. For consensus
generation, finally, simple rules for majority votes
were established.
The CALBC consortium is fully aware of the
fact that the value of an SSC can only be assessed
2The CALBC consortium consists the Rebholz Group
from EBI (Hinxton, U.K.), the Biosemantics Group from
Erasmus (Rotterdam, The Netherlands), the JULIE Lab (Jena,
Germany), and LINGUAMATICS (Cambridge, U.K.).
235
by comparing, e.g., systems trained on such a sil-
ver standard with systems trained on a gold stan-
dard (preferably, though not necessarily, one that
is a subset of the document set which makes up the
SSC).
In the absence of such a gold standard, the
CALBC consortium has spent enormous efforts to
find out the most reasonable parameter settings
for, e.g., the cosine threshold (setting similar men-
tions apart from dissimilar ones) or the consen-
sus constraint (where a certain number of entity
types equally assigned by different taggers makes
one type the consensual silver one and discards all
alternative annotations). Once these criteria are
made effective, the SSC is completely fixed.
As an alternative, we are looking for a more
flexible solution. Our investigation was fuelled by
the following observations:
? The idiosyncrasies of guidelines (on which
(some) taggers were trained) do not necessar-
ily lead to semantically totally different enti-
ties although they differ literally to some de-
gree. Some guidelines prefer, e.g., ?human
IL-7 protein?, others favor ?human IL-7?,
and some lean towards ?IL-7?. As the cosine
measure tends to penalize a pair such as ?hu-
man IL-7 protein? and ?IL-7?, we intended
to avoid such a prescriptive mode and just
look at the type assignment for single tokens
as (parts of) entity mentions. thus avoiding
inconclusive mention boundary discussions.
? While we were counting, for all tokens of
the document set, the votes a single token re-
ceived from different taggers in terms of an-
notating this token with respect to some type,
we generated confidence data for meta data
assignments. Incorporating the distribution
of confidence values into the configuration
process, this allows us to get rid of a pri-
ori fixed majority criteria (e.g., two or three
out of five systems must agree on this token)
which are hard to justify in an absolute way.
Summarizing, we believe that the nature of di-
verging tasks to be solved, the levels of entity type
specificity to be reached, the sort of guidelines be-
ing preferred, etc. should allow prospective users
of a silver standard to customize one on their own
and not stick to one that is already prefabricated
without concrete application in mind.3
3There may be tasks where a ?long? entity such as ?hu-
As such an enterprise would be quite arbitrary
without a reference standard, we even go one step
further. We determine the suitability of, say, dif-
ferent voting scores and varying lexical extensions
of mentions by comparison to a gold standard so
that the ?optimal? configuration of a silver stan-
dard, given a set of goal-derived requirements,
can be automatically learned. In real-world ap-
plications, such gold standard annotations would
be delivered only for a fraction of the documents
contained in the entire corpus being tagged by a
flock of taggers. The gold standard is used to op-
timize parameters which are subsequently applied
to the aggregation of automatically annotated data.
Note that the gold standard is used for optimiza-
tion only, not for training. We call such a flexible,
dynamically adjustable silver standard a config-
urable Silver Standard Corpus (conSSC). In a sec-
ond step, we split the various conSSCs, re-trained
our NER tagger on these data sets and, by compar-
ison with the gold standard, were able to identify
the optimal conSSC for this task (which is not the
one (SSC I) made available by the CALBC consor-
tium for the first challenge round).4
2 Optimizing Silver Standards
In this section, we describe the constituent param-
eters of a wide spectrum of SSCs. Mostly, these
parameters were taken over from the design of the
SSC as developed by the CALBC project members.
Differing from that fixed SSC, we investigate the
impact of different parameter settings on the con-
struction of a collection of SSCs, and, first, eval-
uate their direct usefulness on a gold standard for
protein-gene annotations. Second, we also assess
their indirect usefulness by training NER classi-
fiers on these SSCs and evaluate the NERs? perfor-
mance on the gold standard. Thus, our approach
is entirely data-driven without the need for human
intervention in terms of choosing suitable param-
eter settings.
Technically, we first aggregate the votes from
the flock of taggers (in our experiments, we used
the four taggers from the CALBC project members
plus a second tagger of one of the members) for
each text token (for confidence-based decisions)
or at the entity level (for cosine-based decisions),
then we determine the confidence values of these
man IL-7 protein? may be appropriate, while for another task
a short one such as ?IL-7? is entirely sufficient.
4http://www.ebi.ac.uk/Rebholz-srv/
CALBC/challenge.html
236
aggregated votes, and, finally, we compute the
similarity of the various SSCs with the gold stan-
dard data in terms of F-scores (both exact and open
boundaries) and accuracy on the token level.
2.1 Calibrating Consensus
The metrical interpretation of consensus will be
based on thresholded votes for semantic groups at
the token level (cf. Section 2.1.1) and a cosine-
based measure to determine contiguous stretches
of entity mentions in the text (cf. Section 2.1.2).
2.1.1 Type Confidence and Type Voting
For each text token, we determine the entity type
assignment as generated by each NER tagger
which is part of the flock of CALBC taggers.5 We
count and aggregate these votes such that each en-
tity type has an associated type count value.
We then compute the ratio of systems agree-
ing on the same single type assignment and call
this the confidence attributed to a particular type
for some token. The confidence value will sub-
sequently be interpreted against the confidence
threshold [0, 1] that defines a measure of certainty
a type assignment should have in order to be ac-
cepted as consensual.
2.1.2 Cosine-based Similarity of Phrasal
Entity Mentions
As the above policy of token-wise annotation de-
couples contiguous entity mentions spanning over
more than one token, we also want to restitute this
phrasal structure. This is achieved by constructing
contiguous sequences of tokens that characterize a
phrasal entity mention at the text level to which the
same type label has been assigned. Since differ-
ent taggers tend to identify different spans of text
for the same entity type (as shown in the exam-
ple from Section 1) we have to account for similar
phrasal forms of named entity mentions.
This is achieved by constructing vectors which
represent entity mentions and by computing the
cosine between the different entity mention vec-
tors. Let E1 = T1T2T3 be an entity mention com-
prised of three tokens T1 to T3. Let E2 = T2T3 be
5Due to time constraints when we performed our experi-
ments, we make an extremely simplifying assumption: From
the whole range of possible entity types NER taggers may as-
sign to some token (cf. (Bodenreider and McCray, 2003)) we
have chosen the PRotein/GEne group for testing. Still, this
assumption does not do harm to the core of our hypotheses.
See also our discussion in Section 5.
an entity mention overlapping with E1 in the to-
kens T2 and T3. To decide whether E1 and E2 are
considered similar, we first construct two vectors
representing the entity mentions:
v(E1) = (f1, f2, f3)T
with fi = IDF (Ti) being the inverse document
frequency of the token Ti. We compute the in-
verse document frequency of tokens based on the
corpus which is subject to analysis. Analogously,
we construct the vector for E2
v(E2) = (0, f2, f3)T
filling in a zero for the IDF of T1 since it is not
covered by E2. The entity mentions E1 and E2
are considered equal or similar, if the cosine of
the two vectors is greater or equal a given thresh-
old, cos(v(E1), v(E2)) ? threshold.6 We then
compute the number of systems considering an en-
tity annotation as similar in the manner described
above. The annotation is accepted and thus en-
tered into the SSC, if a particular number of sys-
tems agree on one annotation. This approach was
previously developed by the CALBC project part-
ners (Rebholz-Schuhmann et al, 2010a).
The number of agreeing systems and the thresh-
old are the free parameters of this method and thus
subject to optimization.
2.2 Optimization of Silver Standard Corpora
In the experiments described in the next section,
we will consider alternative parametrizations for
Silver Standard Corpora, i.e., the required confi-
dence threshold or cosine threshold and the num-
ber of agreeing systems. We will then discuss two
variants for optimizing this collection of SSCs.
The first one directly uses the gold standard for op-
timization. The task will be to find that particular
parameter setting for an SSC which best fits the
data contained in the gold standard. Once these
parameters are determined they can be applied to
the complete CALBC document set (composed of
100,000 documents) to produce the final, quasi-
optimal SSC.
In another variant, we insert a classifier into this
loop. First, we train a classifier on a particular
6For final corpus creation, it must be decided which of the
matching entity mentions is entered into the reference SSC,
e.g. the longest or shortest entity annotation. In our exper-
iments, we always chose the shortest entity mention. How-
ever, preliminary experiments showed that the differences to
taking the longest entity mention were marginal.
237
SSC that is built from a particular parameter com-
bination. Next, this classifier is tested against the
gold standard. This is iterated through all parame-
ter combinations. Obviously, the best performing
classifier relative to the gold standard selects the
optimal SSC.
3 Experimental Setting
3.1 Gold Standard
We generated a new broad-coverage corpus com-
posed of 3,236 MEDLINE abstracts (35,519 sen-
tences or 941,890 tokens) dealing with gene
and protein mentions. Altogether, it comprises
57,889 named entity type annotations annotated
by one expert biologist. We created this new re-
source to have a consistent and (as far as pos-
sible) subdomain-independent protein-annotated
corpus.7
MEDLINE abstracts were annotated with (pro-
tein coding) genes, mRNAs and proteins. A
distinction was made between dedicated proteins
as they are recorded in the protein database
UNIPROT,8 protein complexes consisting of sev-
eral protein subunits (e.g., IL-2 receptor consist-
ing of ?, ?, and ? chain), and protein families or
groups (e.g., ?transcription factors?). Also enu-
merations of proteins and protein variants were an-
notated. Discontinuous annotations were avoided
as well as nested annotations (annotations embed-
ded in other annotations). However, gene/protein
mentions nested in terms other than gene/protein
mentions were annotated (e.g., protein mentions
nested in protein function descriptions such as
?ligase? in ?ligase activity?). Modifiers such as
species designators were excluded from annota-
tions whenever possible. Gene segments or pro-
tein fragments were also not annotated.
For our experiments, we did not distinguish be-
tween the different annotation classes (see Table
1) but merged all available annotations into one
class, viz. PRotein/GEne (PRGE).
3.2 Automatic Annotation of the Gold Standard
We then asked all four sites participating in the
CALBC project to automatically annotate the given
gold standard (made available without gold data,
7We are aware of other gene/protein-annotated corpora
such as PENNBIOIE (http://bioie.ldc.upenn.
edu/) or GENIA (http://www-tsujii.is.s.
u-tokyo.ac.jp/GENIA/home/wiki.cgi) that will
have to be taken into account in future studies as well.
8http://www.uniprot.org/
semantic type description
T028 Gene or Genome
T086 Nucleotide Sequence
T087 Amino Acid Sequence,
Amino Acid, Peptide
T116 Protein
T126 Enzyme
T192 Receptor
Table 1: Semantic types defining the PRGE group
(semantic type codes refer to the UMLS).
of course) using the same type of named entity tag-
ging machinery as was used to annotate CALBC?s
canonical SSC. The performance results of each
group?s system evaluated against the gold standard
are reported in Table 2. The data of each system
constitute the reference data sets and raw data for
all subsequent experiments on the configuration
and optimization of the silver standard.
The resulting raw material does thus not only
contain gene/protein annotations but also any
other entity types as supplied by the partners.
For our experiments on the gold standard, how-
ever, only the entity types subsumed by the PRGE
group (see Table 1) were considered and annota-
tions of all other types were discarded. The def-
inition of the PRGE group is identical to the one
proposed by Rebholz-Schuhmann et al (2010a).
For the experiments, the specific semantic types
(e.g., the UMLS concepts)9 were not considered,
only the semantic group PRGE was.
3.3 Evaluation Metrics
The following metrics were used to evaluate how
good the silver standard(s) fit(s) the provided gold
standard:
? segment-level recall, precision, and F-score
values with exact boundaries, the standard
way to evaluate NER taggers,
? segment-level recall, precision, and F-score,
but with relaxed boundary constraints. This
means that two entity mentions are consid-
ered to match when they overlap with at least
one token and have the same entity type as-
signed to them,
? accuracy measured on the token level.
These metrics can be considered as optimization
criteria.
9http://www.nlm.nih.gov/research/umls/
238
3.4 Tokenization
The CALBC partners? data do not necessarily
come with tokenization information and, more-
over, different partners/systems might have differ-
ent tokenizations. Since a common ground for
comparison is thus lacking we added a new, con-
sistent tokenization based on the JULIE Lab tok-
enizer (Tomanek et al, 2007b). This tokenizer is
optimized for biomedical documents with intrinsic
focus to keep complex biological terminological
units (such as ?IL-2?) unsegmented, but to split
up tokens that are not terminologically connected
(such as dividing ?IL-2-related? up into ?IL-2?,
?-? and ?related?). As a matter of fact, entity
boundaries do not necessarily coincide with token
boundaries. Our solution to this problem is as fol-
lows: Whenever a token partially overlaps with an
entity name, the full form of that token is consid-
ered to be associated with this entity. All data on
which we report here (silver and gold standards)
obey to this tokenization scheme.
3.5 Parameters Being Tested
The following parameter settings were considered
in our experiments:
? Four different values for confidence thresh-
olds indicating that 20% (0.2), 40% (0.4),
60% (0.6) or 80% (0.8) of all taggers agreed
on the same type annotation, viz. PRGE,
? Five different values for cosine thresholds
to identify overlapping entity mentions, viz.
(0.7, 0.8, 0.9, 0.95, 0.975), and two different
values for the number n of agreeing taggers,
viz. n ? 2 and n ? 3,
? Two tagger crowd scenarios, viz. one where
all five systems were involved, the other
where subsets of cardinality 2 of these
crowds were re-combined.10
4 Results
As already described in Section 2.2, we performed
two types of experiments. In the first experiment
(Section 4.1), we intend to find proper calibrations
of parameters for an optimal SSC as described in
Section 3.5. In the second experiment (Section
4.2), we incorporate an extrinsic task, training an
NER classifier on different parameter settings, as
a selector for the optimal SSC.
10We refrained from also testing combinations of 3 and 4
systems due to time constraints.
4.1 Intrinsic Calibration of Parameters
Full Merger of All Taggers. In this scenario,
we tested the merged results of the entire crowd of
CALBC taggers when compared to the gold stan-
dard and determined their performance scores (see
Table 3). We will discuss the results with respect
to the overlapping F-score, if not explicitly stated
otherwise.
Looking at the results of the runs involving dif-
ferent cosine thresholds, we witness a systematic
drawback when more than two systems are re-
quired to agree. Although precision is boosted in
this setting, recall is decreasing strongly which re-
sults in overall lower F-scores. When only two
systems are required to agree a comparatively
higher recall comes at the cost of lower preci-
sion. Yet, the F-score (both under exact as well
as overlap conditions) is always superior (ranging
between 75% and 73%) when compared to the 3-
agreement scenario. Note that the 2-agreement
condition for the highest threshold being tested
yields, without exception, better scores than the
best single system (cf. Table 2).
The best performing run in terms of F-score for
the confidence method results from a threshold of
0.2 with an F-score of 76%. Note that this F-
score lies 4 percentage points above the best per-
formance of a single system (cf. Table 2).
A threshold of 0.2 with five contributing sys-
tems results in a union of all annotations. Conse-
quently, this run benefits from a high recall com-
pared with the other runs. However, the run ex-
hibits the lowest precision rating (both for the ex-
act and overlap condition), which is due to the low
threshold being chosen. As can also be seen with
the confidence method at a threshold of 0.80, a
very high precision can be reached (99%) but at
the cost of extremely low recall.11 The methods
performing best in terms of overlapping F-score
also perform best in terms of exact F-score.
Selected Tagger Combinations: Twin Taggers.
In this scenario, we evaluated all twin combina-
tions of taggers against the gold standard regard-
ing the confidence criterion. In Table 4 we contrast
the two best performing and the two worst per-
forming tagger pairs for the confidence method.
The table reveals that there are some cases where
the taggers seem to complement each other, e.g.,
the twins SYS-1 and SYS-3, as well as SYS-3 and
11Exactly these kinds of alternatives offer flexibility for
choosing the most appropriate SSC given a specific task.
239
exactR exactP exactF overlapR overlapP overlapF systems
0.55 0.74 0.63 0.63 0.84 0.72 SYS-1
0.36 0.53 0.43 0.46 0.68 0.55 SYS-2
0.48 0.77 0.59 0.59 0.95 0.72 SYS-3
0.44 0.83 0.58 0.49 0.91 0.64 SYS-4
0.34 0.61 0.44 0.41 0.74 0.53 SYS-5
Table 2: Performance of single systems (SYS-1 to SYS-5) as evaluated against the gold standard (best
performance scores in bold face). Measurements are taken both for exact as well as overlapping recall
(R), precision (P) and F-score (F).
method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr. systems
cosine 0.94 0.53 0.71 0.61 0.66 0.87 0.75 0.70 2.00
cosine 0.93 0.40 0.79 0.53 0.49 0.96 0.65 0.70 3.00
cosine 0.94 0.54 0.71 0.61 0.65 0.87 0.74 0.80 2.00
cosine 0.93 0.41 0.80 0.54 0.48 0.95 0.64 0.80 3.00
cosine 0.94 0.54 0.72 0.62 0.65 0.86 0.74 0.90 2.00
cosine 0.93 0.41 0.81 0.54 0.48 0.95 0.64 0.90 3.00
cosine 0.94 0.54 0.73 0.62 0.64 0.86 0.74 0.95 2.00
cosine 0.93 0.41 0.83 0.55 0.47 0.95 0.63 0.95 3.00
cosine 0.94 0.55 0.75 0.64 0.64 0.86 0.73 0.97 2.00
cosine 0.93 0.42 0.85 0.56 0.47 0.95 0.63 0.97 3.00
confidence 0.95 0.58 0.73 0.65 0.68 0.85 0.76 0.20
confidence 0.94 0.44 0.83 0.58 0.50 0.94 0.66 0.40
confidence 0.93 0.32 0.88 0.47 0.35 0.97 0.52 0.60
confidence 0.91 0.16 0.91 0.27 0.17 0.99 0.30 0.80
Table 3: Merged annotations of the entire crowd of CALBC taggers (best performance scores per param-
eter setting in bold face). Parameters: threshold (confidence or cosine) and number of agreeing systems
(agr. systems).
SYS-4. In both cases, a confidence threshold of
0.2 yields the best F-score. Additionally, these F-
scores (81% and 78%) are even higher than the
single system?s F-scores (+9% up to +14%). This
comes with a significant increase in recall over
both systems (+13% to +28%) though at the cost
of lowered precision relative to the system with
the higher precision (?1% to ?10%). These re-
sults also outperform the best results of the exper-
imental runs where all systems were involved (see
Table 3). This indicates that a subset of all systems
might yield a better SSC than a combination of all
systems? outputs.
4.2 Extrinsic Calibration of Parameters
We employed a standard named entity tagger to as-
sess the impact of the different merging strategies
on a scenario near to a real-world application.12
12This tagger is based on Conditional Random Fields (Laf-
ferty et al, 2001) and employs a standard feature set used for
Each SSC variant (and thus each parameter com-
bination) was evaluated with this tagger in a 10-
fold cross validation. The SSC and the gold corpus
were split into ten parts of equal size. Nine parts of
the SSC constituted the training data of one cross
validation round, the corresponding tenth part of
the gold standard was used for evaluation. This
way, we tested how adequate a merged corpus was
with respect to the training of a classifier. Because
the cross validation has been very time consum-
ing, we did not consider specific combinations of
systems but always merged the annotations of all
five systems. The results are displayed in Table 5.
Interestingly, the highest recall, precision, and
F-score values (both for the exact and overlap con-
dition) are shared by the same parameter combi-
nations which also performed best in Section 4.1.
Hence, the use of a named entity tagger supports
the evaluation results when comparing the various
biomedical entity recognition (Settles, 2004).
240
ACC exactR exactP exactF overlapR overlapP overlapF systems threshold
0.95 0.62 0.69 0.65 0.76 0.85 0.81 SYS-1 + SYS-3 0.20
0.92 0.22 0.69 0.34 0.26 0.81 0.39 SYS-2 + SYS-5 0.60
0.95 0.55 0.75 0.63 0.67 0.91 0.78 SYS-3 + SYS-4 0.20
0.92 0.30 0.85 0.45 0.34 0.94 0.50 SYS-4 + SYS-5 0.60
Table 4: Twin pairs of taggers, contrasting the two best (in bold face) and the two worst performing pairs
obtained by the confidence method.
method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr. systems
cosine 0.94 0.46 0.69 0.56 0.58 0.86 0.69 0.70 2.00
cosine 0.93 0.32 0.77 0.45 0.39 0.94 0.55 0.70 3.00
cosine 0.94 0.46 0.69 0.56 0.57 0.86 0.69 0.80 2.00
cosine 0.93 0.32 0.78 0.46 0.39 0.94 0.55 0.80 3.00
cosine 0.94 0.46 0.70 0.56 0.57 0.85 0.68 0.90 2.00
cosine 0.93 0.32 0.79 0.46 0.38 0.93 0.54 0.90 3.00
cosine 0.94 0.47 0.71 0.56 0.56 0.85 0.68 0.95 2.00
cosine 0.93 0.33 0.80 0.47 0.38 0.93 0.54 0.95 3.00
cosine 0.94 0.47 0.73 0.57 0.56 0.85 0.67 0.97 2.00
cosine 0.93 0.33 0.82 0.47 0.38 0.93 0.54 0.97 3.00
confidence 0.94 0.50 0.72 0.59 0.60 0.85 0.70 0.20
confidence 0.93 0.36 0.82 0.50 0.41 0.93 0.56 0.40
confidence 0.92 0.25 0.87 0.39 0.28 0.95 0.43 0.60
confidence 0.91 0.12 0.89 0.20 0.12 0.96 0.22 0.80
Table 5: Performance of an NER tagger trained on an SSC, 10-fold cross validation, and all systems.
Parameters: threshold (confidence or cosine) and number of agreeing systems (agr. systems).
SSCs directly to the gold standard corpus. How-
ever, this result may be due to our particular exper-
imental setting and should not be taken as a gen-
eral rule. Instead, this issue should be studied on
additional gold standard corpora (cf. Section 5).
5 Discussion and Conclusions
The experiments reported in this paper strengthen
the empirical basis of the novel idea of a silver
standard corpus (SSC). While the originators of
the SSC have come up with a fixed SSC, our ex-
periments show that different parametrizations of
SSCs allow to dynamically configure or select an
optimal one given a gold standard for comparison
during this optimization.
Our experimental data reveals that the boosting
hypothesis (the combination of several classifiers
outperforms weaker single ones in terms of perfor-
mance) is confirmed for complete mergers as well
as selected twin pairs of taggers. We also have
evidence that boosting within the SSC paradigm
tends to increase precision whereas it seems to de-
crease recall. This general observation becomes
stronger and stronger when the size of the commit-
tees (i.e., the number of submitting classifiers) in-
creases. It is also particularly interesting that both
the intrinsic evaluation (groups of classifiers vs.
gold standard), as well as the extrinsic evaluation
of SSCs (groups of classifiers trained and tested on
mutually exclusive partitions of the gold standard)
reveal parallel patterns in terms of performance ?
this indicates a surprising level of stability of the
entire SSC approach.
In our view, the strongest finding from our ex-
periments is the possibility to calibrate an SSC ac-
cording to requirements derived from the goal of
annotation campaigns. In particular, one can adapt
parameters to a specific use case, e.g., building a
corpus with high precision when compared to the
gold standard. Through the evaluation of the pa-
rameter space, one can assess the costs of reach-
ing a specific goal. For instance, a precision of
99% can be reached, yet at the cost of the F-score
plunging to 30%; only slightly lowering the preci-
sion to 97% boosts the F-score by 22 points (see
last two rows in Table 3).
241
Also, when increasingly more annotation sets
become available (e.g., through the CALBC chal-
lenges) the problem of adversarial or extremely
bad performing systems is no longer a pressing is-
sue since with the optimization approach such sys-
tems are automatically sorted out when optimizing
over the set of possible system combinations.
While our experiments are but a first step to-
wards the consolidation of the SSC paradigm
some obvious limitations of our work have to be
overcome:
? experiments with different gold standards
have to be run as one might hypothesize that
different gold standards require different pa-
rameter settings for the optimal SSC,
? experiments with different NER taggers have
to be run (e.g., we plan to use an NER tag-
ger which prefers recall over precision, while
the one used for these experiments generally
yields higher precision than recall scores),
? test with crowds of taggers which generate
higher recall than precision.13
In our approach, a gold standard is needed to
find good parameters to build an SSC. A ques-
tion not addressed so far is how huge such a gold
standard must be to offer an appropriate size for
the optimization step. Finally, it might be particu-
larly rewarding to join efforts in reducing the de-
velopment costs for such a gold standards ? Active
Learning (e.g., Tomanek et al (2007a)) might be
one promising approach to break this bottleneck.
Since effective calibration of SSCs is in need of
reasonably sized and densely populated gold stan-
dards, by combining these lines of research we
claim that additional benefits for SSCs become vi-
able.
6 Acknowledgments
We wish to thank Kerstin Hornbostel for stim-
ulating and corrective remarks on the biological
grounding of this investigation. This research was
partially funded by the EC?s 7th Framework Pro-
gramme within the CALBC project (FP7-231727)
and the GERONTOSYS research initiative from the
13We used a gold standard in which some unusual entities
(e.g., protein families) had been annotated for which most
named entity taggers have not been trained. This might also
explain the generally overall low recall among the crowd of
taggers yielded in our experiments.
German Federal Ministry of Education and Re-
search (BMBF) under grant 0315581D within the
JENAGE project.
References
Olivier Bodenreider and Alexa T. McCray. 2003. Ex-
ploring semantic groups through visual approaches.
Journal of Biomedical Informatics, 36(6):414?432.
Yoav Freund. 1990. Boosting a weak learning algo-
rithm by majority. In COLT?90 ? Proceedings of the
3rd Annual Workshop on Computational Learning
Theory, pages 202?216.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML?01 ? Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno
Yepes, Erik van Mulligen, Ning Kang, Jan Kors,
David Milward, Peter Corbett, Ekaterina Buyko,
Elena Beisswanger, and Udo Hahn. 2010a. CALBC
Silver Standard Corpus. Journal of Bioinformatics
and Computational Biology, 8:163?179.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno
Yepes, Erik M. van Mulligen, Ning Kang, Jan
Kors, Peter Milward, David Corbett, Ekaterina
Buyko, Katrin Tomanek, Elena Beisswanger, and
Udo Hahn. 2010b. The CALBC Silver Standard
Corpus for biomedical named entities: A study in
harmonizing the contributions from four indepen-
dent named entity taggers. In LREC 2010 ? Pro-
ceedings of the 7th International Conference on
Language Resources and Evaluation.
Burr Settles. 2004. Biomedical named entity recog-
nition using conditional random fields and rich fea-
ture sets. In NLPBA/BioNLP 2004 ? COLING
2004 International Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applica-
tions, pages 107?110.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007a. An approach to text corpus construction
which cuts annotation costs and maintains corpus
reusability of annotated data. In EMNLP-CoNLL?07
? Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Language Learning, pages 486?
495.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007b. A reappraisal of sentence and token splitting
for life sciences documents. In K. A. Kuhn, J. R.
Warren, and T. Y. Leong, editors, MEDINFO?07 ?
Proceedings of the 12th World Congress on Medical
Informatics, number 129 in Studies in Health Tech-
nology and Informatics, pages 524?528. IOS Press.
242
