Adaptive Chinese Word Segmentation1  
Jianfeng Gao*, Andi Wu*, Mu Li*, Chang-Ning Huang*, Hongqiao Li**, Xinsong Xia$, Haowei Qin&  
*
 Microsoft Research. {jfgao, andiwu, muli, cnhuang}@microsoft.com 
**
 Beijing Institute of Technology, Beijing. lhqtxm@bit.edu.cn 
$
 Peking University, Beijing. xia_xinsong@founder.com 
&
 Shanghai Jiaotong university, Shanghai. haoweiqin@sjtu.edu.cn 
                                                   
1
 This work was done while Hongqiao Li, Xinsong Xia and Haowei Qin were visiting Microsoft Research (MSR) Asia. We thank 
Xiaodan Zhu for his early contribution, and the three reviewers, one of whom alerted us the related work of (Uchimoto et al, 2001). 
Abstract 
This paper presents a Chinese word segmen-
tation system which can adapt to different 
domains and standards. We first present a sta-
tistical framework where domain-specific 
words are identified in a unified approach to 
word segmentation based on linear models. 
We explore several features and describe how 
to create training data by sampling. We then 
describe a transformation-based learning 
method used to adapt our system to different 
word segmentation standards. Evaluation of 
the proposed system on five test sets with dif-
ferent standards shows that the system 
achieves state- of-the-art performance on all of 
them. 
1 Introduction 
Chinese word segmentation has been a long- 
standing research topic in Chinese language proc-
essing. Recent development in this field shows that, 
in addition to ambiguity resolution and unknown 
word detection, the usefulness of a Chinese word 
segmenter also depends crucially on its ability to 
adapt to different domains of texts and different 
segmentation standards.  
The need of adaptation involves two research 
issues that we will address in this paper. The first is 
new word detection. Different domains/applications 
may have different vocabularies which contain new 
words/terms that are not available in a general 
dictionary. In this paper, new words refer to OOV 
words other than named entities, factoids and mor-
phologically derived words. These words are 
mostly domain specific terms (e.g. ??? ?cellular?) 
and time-sensitive political, social or cultural terms 
(e.g. ???Three Links?, ?? ?SARS?).  
The second issue concerns the customizable 
display of word segmentation. Different Chinese 
NLP-enabled applications may have different re-
quirements that call for different granularities of 
word segmentation. For example, speech recogni-
tion systems prefer ?longer words? to achieve 
higher accuracy whereas information retrieval 
systems prefer ?shorter words? to obtain higher 
recall rates, etc. (Wu, 2003). Given a word seg-
mentation specification (or standard) and/or some 
application data used as training data, a segmenter 
with customizable display should be able to provide 
alternative segmentation units according to the 
specification which is either pre-defined or implied 
in the data. 
In this paper, we first present a statistical 
framework for Chinese word segmentation, where 
various problems of word segmentation are solved 
simultaneously in a unified approach.  Our ap-
proach is based on linear models where component 
models are inspired by the source-channel models 
of Chinese sentence generation. We then describe in 
detail how the new word identification (NWI) 
problem is handled in this framework. We explore 
several features and describe how to create training 
data by sampling. We evaluate the performance of 
our segmentation system using an annotated test set, 
where new words are simulated by sampling. We 
then describe a transformation-based learning (TBL, 
Brill, 1995) method that is used to adapt our system 
to different segmentation standards. We compare 
the adaptive system to other state-of-the-art systems 
using four test sets in the SIGHAN?s First Interna-
tional Chinese Word Segmentation Bakeoff, each of 
which is constructed according to a different seg-
mentation standard. The performance of our system 
is comparable to the best systems reported on all 
four test sets. It demonstrates the possibility of 
having a single adaptive Chinese word segmenter 
that is capable of supporting multiple user applica-
tions. 
Word Class2 Model Feature Functions, f(S,W) 
Context Model Word class based trigram, P(W). -log(P(W)) 
Lexical Word (LW) --- 1 if S forms a word lexicon entry, 0 otherwise. 
Morphological Word (MW) --- 1 if S forms a morph lexicon entry, 0 otherwise. 
Named Entity (NE) Character/word bigram, P(S|NE). -log(P(S|NE)) 
Factoid (FT) --- 1 if S can be parsed using a factoid grammar, 0 otherwise 
New Word (NW) --- Score of SVM classifier 
Figure 1: Context model, word classes, and class models, and feature functions. 
                                                   
2
 In our system, we define three types of named entity: person name (PN), location name (LN), organization (ON) and translit-
eration name (TN); ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone number, 
and WWW; and five types of morphologically derived words (MDW): affixation, reduplication, merging, head particle and split. 
2 Chinese Word Segmentation with 
Linear Models 
Let S be a Chinese sentence which is a character 
string. For all possible word segmentations W, we 
will choose the most likely one W* which achieves 
the highest conditional probability P(W|S): W* = 
argmaxw P(W|S). According to Bayes? decision rule 
and dropping the constant denominator, we can 
equivalently perform the following maximization: 
)|()(maxarg* WSPWPW
W
=
. 
(1) 
Equation (1) represents a source-channel approach 
to Chinese word segmentation. This approach 
models the generation process of a Chinese sen-
tence: first, the speaker selects a sequence of con-
cepts W to output, according to the probability 
distribution P(W); then he attempts to express each 
concept by choosing a sequence of characters, 
according to the probability distribution P(S|W).  
We define word class as a group of words that 
are supposed to be generated according to the same 
distribution (or in the same manner). For instance, 
all Chinese person names form a word class. We 
then have multiple channel models, each for one 
word class. Since a channel model estimates the 
likelihood that a character string is generated given 
a word class, it is also referred to as class model. 
Similarly, source model is referred to as context 
model because it indicates the likelihood that a word 
class occurs in a context. We have only one context 
model which is a word-class-based trigram model. 
Figure 1 shows word classes and class models that 
we used in our system. We notice that different 
class models are constructed in different ways (e.g. 
name entity models are n-gram models trained on 
corpora whereas factoid models use derivation rules 
and have binary values). The dynamic value ranges 
of different class models can be so different that it is 
improper to combine all models through simple 
multiplication as Equation (1). 
In this study we use linear models. The method 
is derived from linear discriminant functions widely 
used for pattern classification (Duda et al, 2001), 
and has been recently introduced into NLP tasks by 
Collins and Duffy (2001). It is also related to log- 
linear models for machine translation (Och, 2003).  
In this framework, we have a set of M+1 feature 
functions fi(S,W), i = 0,?,M. They are derived from 
the context model (i.e. f0(W)) and M class models, 
each for one word class, as shown in Figure 1: For 
probabilistic models such as the context model or 
person name model, the feature functions are de-
fined as the negative logarithm of the corresponding 
probabilistic models. For each feature function, 
there is a model parameter ?i. The best word seg-
mentation W* is determined by the decision rule as 
?
=
==
M
i
ii
W
M
W
WSfWSScoreW
0
0
* ),(maxarg),,(maxarg ??  (2) 
Below we describe how to optimize ?s. Our 
method is a discriminative approach inspired by the 
Minimum Error Rate Training method proposed in 
Och (2003). Assume that we can measure the 
number of segmentation errors in W by comparing it 
with a reference segmentation R using a function 
Er(R,W). The training criterion is to minimize the 
count of errors over the training data as 
?=
RWS
M
M
SWREr
M
,,
11
^
)),(,(minarg
1
??
?
, (3) 
where W is detected by Equation (2). However, we 
cannot apply standard gradient descent to optimize  
Initialization: ?0=?, ?i=1, i = 1,?,M. 
For t = 1 ? T,  j = 1 ? N 
 Wj = argmax ? ?i fi(Sj,W) 
 For i = 1? M 
  ?i = ?i + ?(Score(?,S,W)-Score(?,S,R))(fi(R) - fi(W)),  
where ?={?0, ?1,?,?M} and ? =0.001. 
Figure 2: The training algorithm for model parameters 
model parameters according to Equation (3) be-
cause the gradient cannot be computed explicitly 
(i.e., Er is not differentiable), and there are many 
local minima in the error surface. We then use a 
variation called stochastic gradient descent (or 
unthresholded perceptron, Mitchell, 1997). As 
shown in Figure 2, the algorithm takes T passes over 
the training set (i.e. N sentences). All parameters are 
initially set to be 1, except for the context model 
parameter ?0 which is set to be a constant ? during 
training, and is estimated separately on held-out 
data. Class model parameters are updated in a sim-
ple additive fashion. Notice that Score(?,S,W) is not 
less than Score(?,S,R). Intuitively the updated rule 
increases the parameter values for word classes 
whose models were ?underestimated? (i.e. expected 
feature value f(W) is less than observed feature 
value f(R)), and decreases the parameter values 
whose models were ?overestimated? (i.e. f(W) is 
larger than f(R)).  Although the method cannot 
guarantee a global optimal solution, it is chosen for 
our modeling because of its efficiency and the best 
results achieved in our experiments. 
Given the linear models, the procedure of word 
segmentation in our system is as follows: First, all 
word candidates (lexical words and OOV words of 
certain types) are generated, each with its word 
class tag and class model score. Second, Viterbi 
search is used to select the best W according to 
Equation (2). Since the resulting W* is a sequence of 
segmented words that are either lexical words or 
OOV words with certain types (e.g. person name, 
morphological words, new words) we then have a 
system that can perform word segmentation and 
OOV word detection simultaneously in a unified 
approach. Most previous works treat OOV word 
detection as a separate step after word segmentation. 
Compared to these approaches, our method avoids 
the error propagation problem and can incorporate a 
variety of knowledge to achieve a globally optimal 
solution. The superiority of the unified approach 
has been demonstrated empirically in Gao et al 
(2003), and will also be discussed in Section 5. 
3 New Word Identification 
New words in this section refer to OOV words that 
are neither recognized as named entities or factoids 
nor derived by morphological rules. These words 
are mostly domain specific and/or time-sensitive. 
The identification of such new words has not been 
studied extensively before. It is an important issue 
that would have substantial impact on the per-
formance of word segmentation. For example, 
approximately 30% of OOV words in the 
SIGHAN?s PK corpus (see Table 1) are new words 
of this type. There has been previous work on de-
tecting Chinese new words from a large corpus in 
an off-line manner and updating the dictionary 
before word segmentation. However, our approach 
is able to detect new words on-line, i.e. to spot new 
words in a sentence on the fly during the process of 
word segmentation where widely-used statistical 
features such as mutual information or term fre-
quency are not available. 
For brevity of discussion, we will focus on the 
identification of 2-character new words, denoted as 
NW_11. Other types of new words such as NW_21 
(a 2-character word followed with a character) and 
NW_12 can be detected similarly (e.g. by viewing 
the 2-character word as an inseparable unit, like a 
character). Below, we shall describe the class model 
and context model for NWI, and the creation of 
training data by sampling. 
3.1 Class Model 
We use a classifier (SVM in our experiments) to 
estimate the likelihood of two adjacent characters to 
form a new word. Of the great number of features 
we experimented, three linguistically-motivated 
features are chosen due to their effectiveness and 
availability for on-line detection. They are Inde-
pendent Word Probability (IWP), Anti-Word Pair 
(AWP), and Word Formation Analogy (WFA). 
Below we describe each feature in turn. In Section 
3.2, we shall describe the way the training data (new 
word list) for the classifier is created by sampling. 
IWP is a real valued feature. Most Chinese 
characters can be used either as independent words 
or component parts of multi-character words, or 
both. The IWP of a single character is the likelihood 
for this character to appear as an independent word 
in texts (Wu and Jiang, 2000): 
)(
) ,()(
xC
WxC
xIWP = . (4) 
where C(x, W) is the number of occurrences of the 
character x as an independent word in training data, 
and C(x) is the total number of x in training data. We 
assume that the IWP of a character string is the 
product of the IWPs of the component characters. 
Intuitively, the lower the IWP value, the more likely 
the character string forms a new word. In our im-
plementation, the training data is word-segmented. 
AWP is a binary feature derived from IWP. For 
example, the value of AWP of an NW_11 candidate 
ab is defined as: AWP(ab)=1 if IWP(a)>? or IWP(b) 
>?, 0 otherwise. ? ? [0, 1] is a pre-set threshold. 
Intuitively, if one of the component characters is 
very likely to be an independent word, it is unlikely 
to be able to form a word with any other characters. 
While IWP considers all component characters in a 
new word candidate, AWP only considers the one 
with the maximal IWP value. 
WFA is a binary feature. Given a character pair 
(x, y), a character (or a multi-character string) z is 
called the common stem of (x, y) if at least one of the 
following two conditions hold: (1) character strings 
xz and yz are lexical words (i.e. x and y as prefixes); 
and (2) character strings zx and zy are lexical words 
(i.e. x and y as suffixes). We then collect a list of 
such character pairs, called affix pairs, of which the 
number of common stems is larger than a pre-set 
threshold. The value of WFA for a given NW_11 
candidate ab is defined as: WFA(ab) = 1 if there 
exist an affix pair (a, x) (or (b, x)) and the string xb 
(or ax) is a lexical word, 0 otherwise. For example, 
given an NW_11 candidate ?? (xia4-gang3, ?out of 
work?), we have WFA(??) = 1 because (?, ?) is 
an affix pair (they have 32 common stems such as _
?,  ?,  ?,  ?,  ?,  ?,  ?) and ?? 
(shang4-gang3, ?take over a shift?) is a lexical word. 
3.2 Context Model 
The motivations of using context model for NWI 
are two-fold. The first is to capture useful contex-
tual information. For example, new words are more 
likely to be nouns than pronouns, and the POS 
tagging is context-sensitive. The second is more 
important. As described in Section 2, with a context 
model, NWI can be performed simultaneously with 
other word segmentation tasks (e.g.: word break, 
named entity recognition and morphological analy-
sis) in a unified approach. 
However, it is difficult to develop a training 
corpus where new words are annotated because ?we 
usually do not know what we don?t know?. Our 
solution is Monte Carlo simulation. We sample a set 
of new words from our dictionary according to the 
distribution ? the probability that any lexical word 
w would be a new word P(NW|w). We then generate 
a new-word-annotated corpus from a word-seg-
mented text corpus.  
Now we describe the way P(NW|w) is estimated. 
It is reasonable to assume that new words are those 
words whose probability to appear in a new docu-
ment is lower than general lexical words. Let Pi(k) 
be the probability of word wi that occurs k times in a 
document. In our experiments, we assume that 
P(NW|wi) can be approximated by the probability of 
wi occurring less than K times in a new document:  
??
=
?
1
0
)()|(
K
k
ii kPwNWP , (5) 
where the constant K is dependent on the size of the 
document: The larger the document, the larger the 
value. Pi(k) can be estimated using several term 
distribution models (see Chapter 15.3 in Manning 
and Sch?tze, 1999). Following the empirical study 
in (Gao and Lee, 2000), we use K-Mixture (Katz, 
1996) which estimate Pi(k) as 
k
ki kP )1(1)1()( 0, +++?= ?
?
?
??? , (6) 
where ?k,0=1 if  k=0, 0 otherwise. ? and ? are pa-
rameters that can be fit using the observed mean ? 
and the observed inverse document frequency IDF 
as follow: 
N
cf
=? , 
df
NIDF log= , 
df
dfcfIDF ?
=??= 12?? , and ?
?
? = , 
where cf is the total number of occurrence of word 
wi in training data, df is the number of documents in 
training data that wi occurs in, and N is the total 
number of documents. In our implementation, the 
training data contain approximately 40 thousand 
documents that have been balanced among domain, 
style and time. 
4 Adaptation to Different Standards 
The word segmentation standard (or standard for 
brevity) varies from system to system because there 
is no commonly accepted definition of Chinese
   
Condition: ?Affixation? Condition: ?Date? Condition: ?PersonName? 
Actions: Insert a boundary 
between ?Prefix? and ?Stem?? 
Actions: Insert a boundary between 
?Year? and ?Mon? ? 
Actions: Insert a boundary be-
tween ?FamilyName? and ?Given-
Name?? 
Figure 3: Word internal structure and class-type transformation templates. 
words and different applications may have different 
requirements that call for different granularities of 
word segmentation. 
It is ideal to develop a single word segmentation 
system that is able to adapt to different standards. 
We consider the following standard adaptation 
paradigm. Suppose we have a ?general? standard 
pre-defined by ourselves. We have also created a 
large amount of training data which are segmented 
according to this general standard. We then develop 
a generic word segmenter, i.e. the system described 
in Sections 2 and 3. Whenever we deploy the seg-
menter for any application, we need to customize 
the output of the segmenter according to an appli-
cation-specific standard, which is not always ex-
plicitly defined. However, it is often implicitly 
defined in a given amount of application data 
(called adaptation data) from which the specific 
standard can be partially learned. 
In our system, the standard adaptation is con-
ducted by a postprocessor which performs an or-
dered list of transformations on the output of the 
generic segmenter ? removing extraneous word 
boundaries, and inserting new boundaries ? to 
obtain a word segmentation that meets a different 
standard. 
The method we use is transformation-based 
learning (Brill, 1995), which requires an initial 
segmentation, a goal segmentation into which we 
wish to transform the initial segmentation and a 
space of allowable transformations (i.e. transfor-
mation templates). Under the abovementioned 
adaptation paradigm, the initial segmentation is the 
output of the generic segmenter. The goal segmen-
tation is adaptation data. The transformation tem-
plates can make reference to words (i.e. lexicalized 
templates) as well as some pre-defined types (i.e. 
class-type based templates), as described below. 
We notice that most variability in word seg-
mentation across different standards comes from 
those words that are not typically stored in the 
dictionary. Those words are dynamic in nature and 
are usually formed through productive morpho-
logical processes. In this study, we focus on three 
categories: morphologically derived words (MDW), 
named entities (NE) and factoids. 
For each word class that belongs to these cate-
gories2, we define an internal structure similar to 
(Wu, 2003). The structure is a tree with ?word class? 
as the root, and ?component types? as the other 
nodes. There are 30 component types. As shown in 
Figure 3, the word class Affixation has three 
component types: Prefix, Stem and Suffix. 
Similarly, PersonName has two component types 
and Date has nine ? 3 as non-terminals and 6 as 
terminals. These internal structures are assigned to 
words by the generic segmenter at run time. 
The transformation templates for words of the 
above three categories are of the form: 
Condition: word class 
Actions:  
z Insert ? place a new boundary 
between two component types. 
z Delete ? remove an existing 
boundary between two component 
types. 
Since the application of the transformations de-
rived from the above templates are conditioned on 
word class and make reference to component types, 
we call the templates class-type transformation 
templates. Some examples are shown in Figure 3. 
In addition, we also use lexicalized transforma-
tion templates as: 
z Insert ? place a new boundary 
between two lemmas. 
Mon Day
Pre_Y Pre_MDig_M Dig_D
Year 
Date
PersonName 
FamilyName GivenName
Affixation 
Prefix Stem Suffix
Pre_DDig_Y 
z Delete ? remove an existing 
boundary between two lemmas. 
Here, lemmas refer to those basic lexical words 
that cannot be formed by any productive morpho-
logical process. They are mostly single characters, 
bi-character words, and 4-character idioms. 
In short, our adaptive Chinese word segmenter 
consists of two components: (1) a generic seg-
menter that is capable of adapting to the vocabu-
laries of different domains and (2) a set of output 
adaptors, learned from application data, for adapt-
ing to different ?application-specific? standards  
5 Evaluation 
We evaluated the proposed adaptive word seg-
mentation system (henceforth AWS) using five 
different standards. The training and test corpora of 
these standards are detailed in Table 1, where MSR 
is defined by ourselves, and the other four are stan-
dards used in SIGHAN?s First International Chi-
nese Word Segmentation Bakeoff (Bakeoff test sets 
for brevity, see Sproat and Emperson (2003) for 
details). 
Corpus Abbrev. # Tr. Word # Te. Word 
?General? standard  MSR 20M 226K 
Beijing University PK 1.1M 17K 
U. Penn Chinese 
Treebank 
CTB 250K 40K 
Hong Kong City U. HK 240K 35K 
Academia Sinica AS 5.8M 12K 
Table 1: standards and corpora. 
MSR is used as the general standard in our ex-
periments, on the basis of which the generic seg-
menter has been developed. The training and test 
corpora were annotated manually, where there is 
only one allowable word segmentation for each 
sentence. The training corpus contains approxi-
mately 35 million Chinese characters from various 
domains of text such as newspapers, novels, maga-
zines etc. 90% of the training corpus are used for 
context model training, and 10% are held-out data 
for model parameter training as shown in Figure 2. 
The NE class models, as shown in Figure 1, were 
trained on the corresponding NE lists that were 
collected separately. The test set contains a total of 
225,734 tokens, including 205,162 lexi-
con/morph-lexicon words, 3,703 PNs, 5,287 LNs, 
3,822 ONs, and 4,152 factoids. In Section 5.1, we 
will describe some simulated test sets that are de-
rived from the MSR test set by sampling NWs from 
a 98,686-entry dictionary. 
The four Bakeoff standards are used as ?specific? 
standards into which we wish to adapt the general 
standard. We notice in Table 1 that the sizes of 
adaptation data sets (i.e. training corpora of the four 
Bakeoff standards) are much smaller than that of the 
MSR training set. The experimental setting turns 
out to be a good simulation of the adaptation para-
digm described in Section 4. 
The performance of word segmentation is 
measured through test precision (P), test recall (R), 
F score (which is defined as 2PR/(P+R)), the OOV 
rate for the test corpus (on Bakeoff corpora, OOV is 
defined as the set of words in the test corpus not 
occurring in the training corpus.), the recall on 
OOV words (Roov), and the recall on in-vocabulary 
(Riv) words. We also tested the statistical signifi-
cance of results, using the criterion proposed by 
Sproat and Emperson (2003), and all results re-
ported in this section are significantly different 
from each other. 
5.1 NWI Results 
This section discusses two factors that we believe 
have the most impact on the performance of NWI. 
First, we compare methods where we use the NWI 
component (i.e. an SVM classifier) as a post- 
processor versus as a feature function in the linear 
models of Equation (2). Second, we compare dif-
ferent sampling methods of creating simulated 
training data for context model. Which sampling 
method is best depends on the nature of P(NW|w). 
As described in Section 3.2, P(NW|w) is unknown 
and has to be approximated by Pi(k) in our study, so 
it is expected that the closer P(NW|w) and Pi(k) are, 
the better the resulting context model. We compare 
three estimates of Pi(k) in Equation (5) using term 
models based on Uniform, Possion, and K- Mixture 
distributions, respectively. 
Table 2 shows the results of the generic seg-
menter on three test sets that are derived from the 
MSR test set using the above three different sam-
pling methods, respectively. For all three distribu-
tions, unified approaches (i.e. using NWI compo-
nent as a feature function) outperform consecutive 
approaches (i.e. using NWI component as a post- 
processor). This demonstrates empirically the 
benefits of using context model for NWI and the 
unified approach to Chinese word segmentation, as 
described in 3.2. We also perform NWI on Bakeoff 
AWS w/o NW AWS w/ NW (post-processor) AWS w/ NW (unified approach) 
word segmentation word segmentation NW word segmentation NW  # of NW 
P% R% P% R% P% R% P% R% P% R% 
Uniform 5,682 92.6 94.5 94.7 95.2 64.1 66.8 95.1 95.5 68.1 78.4 
Poisson 3,862 93.4 95.6 94.5 95.9 61.4 45.6 95.0 95.7 57.2 60.6 
K-Mixture 2,915 94.7 96.4 95.1 96.2 44.1 41.5 95.6 96.2 46.2 60.4 
Table 2: NWI results on MSR test set, NWI as post-processor versus unified approach 
PK CTB 
 P R F OOV Roov Riv P R F OOV Roov Riv 
1. AWS w/o adaptation .824 .854 .839 .069 .320 .861 .799 .818 .809 .181 .624 .861 
2. AWS .952 .959 .955 .069 .781 .972 .895 .914 .904 .181 .746 .950 
3. AWS w/o NWI .949 .963 .956 .069 .741 .980 .875 .910 .892 .181 .690 .959 
4. FMM w/ adaptation .913 .946 .929 .069 .524 .977 .805 .874 .838 .181 .521 .952 
5. Rank 1 in Bakeoff .956 .963 .959 .069 .799 .975 .907 .916 .912 .181 .766 .949 
6. Rank 2 in Bakeoff .943 .963 .953 .069 .743 .980 .891 .911 .901 .181 .736 .949 
Table 3: Comparison scores for PK open and CTB open. 
HK AS 
 
P R F OOV Roov Riv P R F OOV Roov Riv 
1. AWS w/o adaptation .819 .822 .820 .071 .593 .840 .832 .838 .835 .021 .405 .847 
2. AWS .948 .960 .954 .071 .746 .977 .955 .961 .958 .021 .584 .969 
3. AWS w/o NWI .937 .958 .947 .071 .694 .978 .958 .943 .951 .021 .436 .969 
4. FMM w/ adaptation .818 .823 .821 .071 .591 .841 .930 .947 .939 .021 .160 .964 
5. Rank 1 in Bakeoff .954 .958 .956 .071 .788 .971 .894 .915 .904 .021 .426 .926 
6. Rank 2 in Bakeoff .863 .909 .886 .071 .579 .935 .853 .892 .872 .021 .236 .906 
Table 4: Comparison scores for HK open and AS open. 
test sets. As shown in Tables 3 and 4 (Rows 2 and 3), 
the use of NW functions (via the unified approach) 
substantially improves the word segmentation per-
formance. 
We find in our experiments that NWs sampled 
by Possion and K-Mixture are mostly specific and 
time-sensitive terms, in agreement with our intui-
tion, while NWs sampled by Uniform include more 
common words and lemmas that are easier to detect. 
Consequently, by Uniform sampling, the P/R of 
NWI is the highest but the P/R of the overall word 
segmentation is the lowest, as shown in Table 2. 
Notice that the three sampling methods are not 
comparable in terms of P/R of NWI in Table 2 
because of different sampling result in different sets 
of new words in the test set. We then perform NWI 
on Bakeoff test sets where the sets of new words are 
less dependent on specific sampling methods. The 
results however do not give a clear indication which 
sampling method is the best because the test sets are 
too small to show the difference. We then leave it to 
future work a thorough empirical comparison 
among different sampling methods. 
5.2 Standard Adaptation Results 
The results of standard adaptation on four Bakeoff 
test sets are shown in Tables 3 and 4. A set of 
transformations for each standard is learnt using 
TBL from the corresponding Bakeoff training set. 
For each test set, we report results using our system 
with and without standard adaptation (Rows 1 and 
2). It turns out that performance improves dra-
matically across the board in all four test sets. 
For comparison, we also include in each table 
the results of using the forward maximum matching 
(FMM) greedy segmenter as a generic segmenter 
(Row 4), and the top 2 scores (sorted by F) that are 
reported in SIGHAN?s First International Chinese 
Word Segmentation Bakeoff (Rows 5 and 6). We 
can see that with adaptation, our generic segmenter 
can achieve state-of-the-art performance on dif-
ferent standards, showing its superiority over other 
systems. For example, there is no single segmenter 
in SIGHAN?s Bakeoff, which achieved top-2 ranks 
in all four test sets (Sproat and Emperson, 2003). 
We notice in Table 3 and 4 that the quality of 
adaptation seems to depend largely upon the size of 
adaptation data: we outperformed the best bakeoff 
systems in the AS set because the size of the adap-
tation data is big while we are worse in the CTB set 
because of the small size of the adaptation data. To 
verify our speculation, we evaluated the adaptation 
results using subsets of the AS training set of dif-
ferent sizes, and observed the same trend. However, 
even with a much smaller adaptation data set (e.g. 
250K), we still outperform the best bakeoff results. 
6 Related Work 
Many methods of Chinese word segmentation have 
been proposed (See Wu and Tseng, 1993; Sproat 
and Shih, 2001 for reviews). However, it is difficult 
to compare systems due to the fact that there is no 
widely accepted standard. There has been less work 
on dealing with NWI and standard adaptation. 
All feature functions in Figure 1, except the NW 
function, are derived from models presented in 
(Gao et al, 2003). The linear models are similar to 
what was presented in Collins and Duffy (2001). An 
alternative to linear models is the log-linear models 
suggested by Och (2003). See Collins (2002) for a 
comparison of these approaches. 
The features for NWI were studied in Wu & 
Jiang (2000) and Li et al (2004). The use of sam-
pling was proposed in Della Pietra et al (1997) and 
Rosenfeld et al (2001). There is also a related work 
on this line in Japanese (Uchimoto et al, 2001). 
A detailed discussion on differences among the 
four Bakeoff standards is presented in Wu (2003), 
which also proposes an adaptive system where the 
display of the output can be customized by users. 
The method described in Section 4 can be viewed as 
an improved version in that the transformations are 
learnt automatically from adaptation data. The use 
of TBL for Chinese word segmentation was first 
suggested in Palmer (1997). 
7 Conclusion 
This paper presents a statistical approach to adap-
tive Chinese word segmentation based on linear 
models and TBL. The system has two components: 
A generic segmenter that can adapt to the vocabu-
laries of different domains, and a set of output 
adaptors, learned from application data, for adapt-
ing to different ?application-specific? standards. 
We evaluate our system on five test sets, each cor-
responding to a different standard. We achieve 
state-of-the-art performance on all test sets. 
References 
Brill, Eric. 1995. Transformation-based error-driven 
learning and natural language processing: a case study 
in Part-of-Speech tagging. In: Computational Linguis-
tics, 21(4). 
Collins, Michael and Nigel Duffy. 2001. Convolution 
kernels for natural language. In: Advances in Neural 
Information Processing Systems (NLPS 14). 
Collins, Michael. 2002. Parameter estimation for statis-
tical parsing models: theory and practice of distribu-
tion-free methods. To appear. 
Della Pietra, S., Della Pietra, V., and Lafferty, J. 1997. 
Inducing features of random fields. In: IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 19, 
380-393. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 
2001. Pattern classification. John Wiley & Sons, Inc. 
Gao, Jianfeng and Kai-Fu Lee. 2000. Distribution based 
pruning of backoff language models. In: ACL2000. 
Gao, Jianfeng, Mu Li and Chang-Ning Huang. 2003. 
Improved source-channel model for Chinese word 
segmentation. In: ACL2003.  
Katz, S. M. 1996. Distribution of content words and 
phrases in text and language modeling, In: Natural 
Language Engineering, 1996(2): 15-59 
Li, Hongqiao, Chang-Ning Huang, Jianfeng Gao and 
Xiaozhong Fan. 2004. The use of SVM for Chinese 
new word identification. In: IJCNLP2004. 
Manning, C. D. and H. Sch?tze, 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press.  
Mitchell, Tom M. 1997. Machine learning. The 
McGraw-Hill Companies, Inc. 
Och, Franz. 2003. Minimum error rate training in statis-
tical machine translation. In: ACL2003. 
Palmer, D. 1997. A trainable rule-based algorithm for 
word segmentation. In: ACL '97. 
Rosenfeld, R., S. F. Chen and X. Zhu. 2001. Whole 
sentence exponential language models: a vehicle for 
linguistic statistical integration. In: Computer Speech 
and Language, 15 (1). 
Sproat, Richard and Chilin Shih. 2002. Corpus-based 
methods in Chinese morphology and phonology. In: 
COLING 2002.  
Sproat, Richard and Tom Emerson. 2003. The first 
international Chinese word segmentation bakeoff. In: 
SIGHAN 2003. 
Uchimoto, K., S. Sekine and H. Isahara. 2001. The 
unknown word problem: a morphological analysis of 
Japanese using maximum entropy aided by a diction-
ary. In: EMNLP2001. 
Wu, Andi and Zixin Jiang. 2000. Statistically-enhanced 
new word identification in a rule-based Chinese system. 
In: Proc of the 2rd ACL Chinese Processing Workshop. 
Wu, Andi. 2003. Customizable segmentation of mor-
phologically derived words in Chinese. In: Interna-
tional Journal of Computational Linguistics and Chi-
nese Language Processing, 8(1): 1-27. 
Wu, Zimin and Gwyneth Tseng. 1993. Chinese text 
segmentation for text retrieval achievements and prob-
lems. In: JASIS, 44(9): 532-542. 
Chinese Chunking with another Type of Spec  
Hongqiao Li 
Beijing Institute of 
Technology 
Beijing 100081 China 
lhqtxm@bit.edu.cn 
Chang-Ning Huang 
Microsoft Research Asia 
Beijing 100080 China 
cnhuang@msrchina. 
research.microsoft.com 
Jianfeng Gao  
Microsoft Research Asia 
Beijing 100080 China 
 jfgao@microsoft.com 
Xiaozhong Fan 
Beijing Institute of 
Technology 
Beijing 100081 China 
fxz@bit.edu.cn 
 
Abstract 
Spec is a critical issue for automatic chunking. 
This paper proposes a solution of Chinese 
chunking with another type of spec, which is 
not derived from a complete syntactic tree but 
only based on the un-bracketed, POS tagged 
corpus. With this spec, a chunked data is built 
and HMM is used to build the chunker. TBL-
based error correction is used to further 
improve chunking performance. The average 
chunk length is about 1.38 tokens, F measure 
of chunking achieves 91.13%, labeling 
accuracy alone achieves 99.80% and the ratio 
of crossing brackets is 2.87%. We also find 
that the hardest point of Chinese chunking is 
to identify the chunking boundary inside 
noun-noun sequences1.  
1 Introduction 
Abney (1991) has proposed chunking as a useful 
and relative tractable median stage that is to divide 
sentences into non-overlapping segments only 
based on superficial analysis and local information. 
(Ramshaw and Marcus, 1995) represent chunking 
as tagging problem and the CoNLL2000 shared 
task (Kim Sang and Buchholz, 2000) is now the 
standard evaluation task for chunking English. 
Their work has inspired many others to study 
chunking for other human languages.  
Besides the chunking algorithm, spec (the 
detailed definitions of all chunk types) is another 
critical issue for automatic chunking development. 
The well-defined spec can induce the chunker to 
perform well. Currently chunking specs are 
defined as some rules or one program to extract 
phrases from Treebank such as (Li, 2003) and (Li, 
2004) in order to save the cost of manual 
annotation. We name it as Treebank-derived spec. 
However, we find that it is more valuable to 
compile another type of chunking spec according 
to the observation from un-bracketed corpus 
instead of Treebank. 
                                                     
1
 This work was done while Hongqiao Li was visiting 
Microsoft Research Asia. 
Based on the problems of chunking Chinese that 
are found with our observation, we explain the 
reason why another type of spec is needed and then 
propose our spec in which the shortening and 
extending strategies are used to resolve these 
problems. We also compare our spec with a 
Treebank-derived spec which is derived from 
Chinese Treebank (CTB) (Xue and Xia, 2000). An 
annotated chunking corpus is built with the spec 
and then a chunker is also constructed accordingly. 
For annotation, we adopt a two-stage processing, 
in which text is first chunked manually and then 
the potential inconsistent annotations are checked 
semi-automatically with a tool. For the chunker, 
we use HMM model and TBL (Transform-based 
Learning) (Brill, 1995) based error correction to 
further improve chunking performance. With our 
spec the overall average length of chunks arrives 
1.38 tokens, in open test, the chunking F measure 
achieves 91.13% and 95.45% if under-combining 
errors are not counted. We also find the hardest 
point of Chinese chunking is to identify the 
chunking boundary inside a noun-noun sequence. 
In the remainder of this paper section 2 describes 
some problems in chunking Chinese text, section 3 
discusses the reason why another type of spec is 
needed and proposes our chunking spec, section 4 
discusses the annotation of our chunking corpus, 
section 5 describes chunking model, section 6 
gives experiment results, section 7, 8 recall some 
related work and give our conclusions respectively. 
2 Problems of Chunking Chinese Text 
The purpose of Chinese chunking is to divide 
sentence into syntactically correlated parts of 
words after word segmentation and part-of-speech 
(POS) tagging. For example: 
[NP ?? /ns ?Zhuhai?] ? /u ?of? [NP ?? /a 
?solid? ??/n ?traffic? ??/n ?frame?] [VP ?/d 
?already? ? ?? ? /v ?achieve considerable 
scale ? ?/u]   ?Zhuhai has achieved considerable 
scale in solid traffic frame.? 
According to Abney?s definition, most chunks 
are modifier-head structures and non-overlapping. 
However, some syntactic structures in Chinese are 
very hard to be chunked correctly due to 
characteristics of Chinese language, for example, 
less using of function words and less inflection 
formats. Table 1 shows the most common 
structural ambiguities occurred during Chinese 
chunking. Their occurrences and distributions of 
each possible structure are also reported. As can be 
seen in Table 1, only 77% neighboring nouns can 
be grouped inside one chunk; if the left word is ??
/of? or a verb, this figure will ascend to 80% and 
94% respectively; but if the left word is an 
adjective or a numeral, it will descend to 70% and 
59% respectively; for ?n_c_n?, only 52%  are word 
level coordination. In contrast with English 
chunking, several hard problems are described in 
detail as following.  
(1) Noun-noun compounds 
Compounds formed by more than two 
neighboring nouns are very common in Chinese 
and not always all the left nouns modify the head 
of the compound. Some compounds consist of 
several shorter sub-compounds. For example: 
( ? ? /younger ??? /volunteer ? ?
/science and technology ???/service team)    
?young volunteer service team of science and 
technology? 
??? ???? and ??? ???? are two sub-
compounds and the former modifies the latter. 
But sometimes it is impossible to distinguish the 
inner structures, for example: 
??/world ??/peace ??/career  
It is impossible to distinguish whether it is {{?
? ??} ??} or {?? {?? ??}}. 
English chunking also shows such problem, and 
the common solution for English is not to identify 
their inner structure and treat them as a flat noun 
phrase. Following is an example in CoNLL2000 
shared task: 
[NP employee assistance program directors] 
(2) Coordination 
Coordination in all cases can be divided into two 
types: with conjunctions and without conjunctions. 
The former can be further divided into two 
subcategories: word-level and phrase-level 
coordinations. For example: 
{ ? ?? /policy ?? /bank ? /and ??
/commercial ? ? /bank} ? /of { ? ?
/relationship ?/and  ??/cooperation}    ?the 
relationship and cooperation between policy 
banks and commercial banks?. 
The former coordination is phrase-level and the 
latter is word-level. Unfortunately, sometimes it is 
difficult or even impossible to distinguish whether 
it is word-level or phrase-level at all, for example: 
?? /least ?? /salary ? /and ??? /living 
maintenance ?the least salary and living 
maintenance? 
It is impossible to distinguish ???? is a shared 
modifier or not. English chunking also has such 
kind of problems. The solution of CoNLL2000 is 
to leave the conjunctions outside chunks for 
phrase-level coordinations and to group the 
conjunction inside a chunk when it is word-level or 
impossibly distinguished phrase-level. For 
example: 
[NP enough food and water] 
In Chinese, some coordinate construction has no 
conjunction or punctuation inside, and also could 
not be distinguished from a modifier-head 
construction with syntactic knowledge only. For 
example: 
??/order (??/police wagon ??/caution 
light ???/alarm whistle)   ?Order the police 
Pattern1 No.2 Distributions Examples 
n_n 951 
77% (modifier head) 
7% (coordination) 
16% (others) 
(??/society ??/phenomenon) ?social phenomena? 
(??/language ??/wordage) ?language and wordage? 
(??/capital ??/art ??/stage) ? the stage of capital art? 
v_n_n 154 6% (v_n modify the last noun) 94 % (others) 
?/enter?/factory??/worker 
??/avoid  ??/law ??/duty ?avoid legal duties? 
?_n_n 98 80% ( n_n is modifier_head) 20% (others) 
??/watch ?/of??/traffic ??/cop ?a orderly traffic cop? 
??/paralytic?/of??/body??/function  
a_n_n 27 70% ( a modify the first n) 30% (others) 
?/high ??/technology ??/company ?high-tech company? 
?/old ??/news ???/worker ?old news worker? 
m_n_n 17 41% ( m modify the first n) 59% (others) 
?/two ?/nation ??/people ?our two peoples? 
??/some ??/country ??/area ?some rural areas? 
n_c_n 88 52%(word level coordination) 48%(others) 
??/economy ?/and ??/society ?economy and society? 
??/quality  ?/and  ??/technology ??/requirement 
 
1
 n, v, a, d, m, q,  p, f , c are the POS tags of noun, verb, adjective, adverb, number, measure, preposition, localizer, 
conjunction respectively, ?_? means neighboring, ??/of? is a common auxiliary word in Chinese.  
2This statistical work is done on our test corpus whose setting is shown in Table 3. 
Table 1: The observation of several common structural ambiguities during Chinese chunking 
wagons, caution lights and alarm whistles? 
Such problem does not exist in English because 
almost all coordinations have certain conjunctions 
or punctuations between words or phrases of the 
same syntactic categories in formal English. 
(3) Structural ambiguities 
In Chinese, some structural ambiguities in 
phrase level are impossible or unnecessary to be 
distinguished during chunking. There is an 
example of ?a_n_n?: 
?? /a ?modern? ?? /n ?industry? ?? /n 
?system? 
{?? {?? ??}} or {{?? ??} ??} are 
identically acceptable. English also has such 
problem. The solution of CoNLL2000 is not to 
distinguish inner structure and group the given 
sequence as a single chunk. For example, the inner 
structure of ?[NP heavy truck production]? is ?{{heavy 
truck} production}?, whereas one reading of ?[NP 
heavy quake damage]? is ?{heavy {quake damage}}?. 
Besides, ?a_n_n?, ?m_n_n? and ?m_q_n_n? also 
have the similar problem. 
3 Chinese Chunking Spec 
As a kind of shallow parsing, the principles of 
chunking are to make chunking much more 
efficient and precise than full parsing. Obviously, 
one can shorten the length of chunks to leave 
ambiguities outside of chunks. For example, if we 
let noun-noun sequences always chunk into single 
word, those ambiguities listed in Table 1 would not 
be encountered and the performance would be 
greatly improved. In fact, there is an implicit 
requirement in chunking, no matter which 
language it is, the average length of chunks is as 
longer as possible without violating the general 
principle of chunking. So a trade-off between the 
average chunk length and the chunking 
performance exists. 
3.1 Why another type of spec is needed 
A convenient spec is to extract the lowest non-
terminal nodes from a Treebank (e.g. CTB) as 
Chinese chunked data. But there are some 
problems. The trees are designed for full parsing 
instead of shallow parsing, thus some of these 
problems listed in section 2 could not be resolved 
well in chunking. Maybe we can compile some 
rules to prune the tree or break some non-terminal 
nodes in order to properly resolve these problems 
just like CoNLL2000. However, just as (Kim Sang 
and Buchholz, 2000) noted: ?some trees are very 
complex and some annotations are inconsistent?. 
So these rules are complex, the extracted data are 
inconsistent and manual check is also needed. In 
addition, the resource of Chinese Treebank is 
limited and the extracted data is not enough for 
chunking. 
So we compile another type of chunking spec 
according to the observation from un-bracket 
corpus instead of Treebank. The only shortcoming 
is the cost of annotation, but there are some 
advantages for us to explore.  
1) It coincides with auto chunking procedure, 
and we can select proper solutions to these 
problems without constraints of the exist Treebank. 
The purpose of drafting another type of chunking 
spec is to keep chunking consistency as high as 
possible without hurting the performance of auto-
chunking in whole. 
2) Through spec drafting and text 
annotating most frequent and significant syntactic 
ambiguities could be studied, and those 
observations are in turn described in the spec 
carefully.  
3) With a proper spec and certain mechanical 
approaches, a large-scale chunked data could be 
produced without supporting from the Treebank. 
3.2 Our spec 
Our spec and chunking annotation are based on 
PK corpus2 (Yu et al 1996). The PK corpus is un-
bracketed, but in which all words are segmented 
and only one POS tag is assigned to each word. 
We define 11 chunk types that are similar with 
CoNLL2000. They are NP (noun chunk), VP (verb 
chunk), ADJP (adjective chunk), ADVP (adverb 
chunk), PP (prepositional chunk), CONJP 
(conjunction), MP (numerical chunk), TP 
(temporal chunk), SP (spatial chunk), INTJP 
(interjection) and INDP (independent chunk).  
During spec drafting we try to find a proper 
chunk spec to solve these problems by two ways: 
either merging neighboring chunks into one chunk 
or shortening them. Besides those structural 
ambiguities, we also extend boundary of the 
chunks with minor structural ambiguities in order 
to make the chunks close to the constituents. 
3.2.1 Shortening 
The auxiliary ??/of? is one of the most frequent 
words in Chinese and used to connect a pre-
modifier with its nominal head. However the left 
boundary of such a ? -construction is quite 
complicated: almost all kinds of preceding clauses, 
phrases and words can be combined with it to form 
such a pre-modifier, and even one ?-construction 
can embed into another. So we definitely leave it 
outside any chunk. Similarly, conjunctions, ??
/and?, ?? /or? and ?? /and? et al, are also left 
outside any chunk no matter they are word-level or 
                                                     
2
 Can be downloaded from www.icl.pku.edu.cn  
phrase-level coordinations. For instances, the 
examples in Section 2 are chunked as ?[NP ??? 
??] ? [NP?? ??] ? [NP ??] ?  [NP?
?]? and ?[ADJP ??] [NP ??]  ?  [NP ??
?]? 
3.2.2 Extending 
(1) NP 
Similar with the shared task of CoNLL2000, 
we define noun compound that is formed by a 
noun-sequence: ?a_n_n?, ?m_n_n? or ?m_q_n_n?, 
as one chunk, even if there are sub-compounds, 
sub-phrase or coordination relations inside it. For 
instances, ?[NP ?? ??? ?? ???]?, 
?[NP ?? ?? ??]?, ?[VP??] [NP?? ?
? ???]?, ?[NP ?? ?? ??] and ?[NP ?
? ?? ??]? are grouped into single chunks 
respectively. 
However, it does not mean that we blindly bind 
all neighboring nouns into a flat NP. If those 
neighboring nouns are not in one constituent or 
cross the phrase boundary, they will be chunked 
separately, such as following two examples in 
Table 1: ?[VP?] [NP ?] [NP??]? and ?[ADJP
??] ?/u [NP ??] [NP ??]?. So our solution 
does not break the grammatical phrase structure in 
a given sentence. 
With this chunking strategy, we not only 
properly resolved these problems, but also get 
longer chunks. Longer chunks can make 
successive parsing easier based on chunking. For 
example, if we chunked the sentence as: 
[NP ??] ? [NP ?? ??] [NP ??] [VP ? 
???? ?]  ?/w 
There would be three possible syntactic trees 
which are difficult to be distinguished: 
1a) {{ [NP ??] ? { [NP ?? ??] [NP ?
?]}} [VP ? ????]} 
1b) {{{ [NP ??] ? [NP ?? ??]} [NP ?
?]}  [VP ? ????]} 
1c) {{ [NP ??] ? [NP ?? ??]} { [NP ?
?]  [VP ? ????]}} 
Whereas with above chunking strategy of our 
spec, there is only one syntactic tree remained: 
{{[NP ??] ? [NP ?? ?? ??]} [VP ?  
???? ?]}  ?/w 
Another reason of the chunking strategy is that 
for some NLP applications such as IR, IE or QA, it 
is unnecessary to analyze these ambiguities at the 
early stage of text analysis. 
(2) PP 
Most PP consists of only the preposition itself 
because the right boundary of a preposition phrase 
is hard to identify or far from the preposition. But 
certain prepositional phrases in Chinese are formed 
with a frame-like construction, such as [PP ?/p 
?at? ??/f ?middle?], [PP ?/p ??/f ?top?], etc. 
Statistics shows that more than 90% of those 
frame-like PPs are un-ambiguous, and others 
commonly have certain formal features such as an 
auxiliary ?  or a conjunction immediately 
following the localizer. Table 2 shows the statistic 
result. Thus with those observations, those frame-
like constructions could be chunked as PP. The 
length of such kind of PP frames is restricted to be 
at most two words inside in order to keep the 
distribution of chunk length more even and the 
chunking annotation more consistent.  
 
Pattern1 No.of occurrence Ratio as a chunk 
p_*_f 45 93.33% 
P_*_*_f 36 97.22% 
*_f 40 92.50% 
*_*_f 9 77.78% 
  
1
 This statistical work is also done on our test 
corpus and ?*? means a wildcard for a POS tag. 
Table 2: The ration of grouping these patterns 
as a chunk without any ambiguity  
(3) SP 
Most spatial chunks consist of only the 
localizer(with POS tag ?/s? or ?/f?). But if the 
spatial phrase is in the beginning of a sentence, or 
there is a punctuation (except ???) in front of it, 
then the localizer and its preceding words could be 
chunked as a SP. And the number of words in front 
of the localizer is also restricted to at most two for 
the same reason. 
(4) VP 
Commonly, a verb chunk VP is a pre-modifier 
verb construction, or a head-verb with its following 
verb-particles which form a morphologically 
derived word sequence. The pre-modifier is 
formed by adverbial phrases and/or auxiliary verbs. 
In order to keep the annotation consistent those 
verb particles and auxiliary verbs could be found in 
a closed list respectively only. Post-modifiers of a 
verb such as object and complement should be 
excluded in a verb chunk. 
We find that although a head verb groups more 
than one preceding adverbial phrases, auxiliary 
verbs and following verb-particles into one VP, its 
chunking performance is still high. For example: 
[CONJP ??/c ?if?] [VP ??/d ?lately? 
?/d ?not? ?/v ?can? ??/v ?build? ?/v 
?up?] [NP ??/n ?diplomat ??/n ?relation?] 
?If we could not build up the foreign relations 
soon? 
3.3 Spec Comparison 
We compare our spec with the Treebank-derived 
spec, named as S1, which is to extract the lowest 
non-terminal nodes from CTB as chunks from the 
aspect of the solutions of these problems in section 
2. Noun-noun compound and the coordination 
which has no conjunction are chunked identically 
in both specs. But for others, there are different. In 
S1, the conjunctions of phrase-level coordination 
are outside of chunks and the ones of word-level 
are inside a chunk, all adjective or numerical 
modifiers are separate from noun head. According 
to S1, the example in 3.2.1 should be chunked as 
following. 
[ADJP ???] [NP ??] ? [NP??] [NP ?
?] ? [NP ?? ?  ??] 
But these phrases that are impossible to 
distinguish inner structures during the early stage 
of text analysis are hard to be chunked and would 
cause some inconsistency. ?[ADJP ??] [NP ??] 
?  [NP ???]? or ?[ADJP ??] [NP ?? ? ?
??]?, ?[ADJP ??] [NP ??] [NP ??]? or 
?[ADJP ??] [NP ?? ??]?, are hard to make 
decisions with S1.  
In addition, with our spec outside words are only 
punctuations, structural auxiliary ? ? /of?, or 
conjunctions, whereas with S1, outside words are 
defined as all left words after lowest non-terminal 
extraction. 
4 Chunking Annotation 
Four graduate students of linguistics were 
assigned to annotate manually the PK corpus with 
the proposed chunking spec. Many discussions 
between authors and those annotators were 
conducted in order to define a better chunking spec 
for Chinese. Through the spec drafting and text 
annotating most significant syntactic ambiguities 
in Chinese, such as those structural ambiguities 
discussed in section 2 and 3, have been studied, 
and those observations are carefully described in 
the spec in turn.  
Consistency control is another important issue 
during annotation. Besides the common methods: 
manual checking, double annotation, post 
annotation checking, we explored a new 
consistency measure to help us find the potential 
inconsistent annotations, which is hinted by 
(Kenneth and Ryszard. 2000), who defined 
consistency gain as a measure of a rule in learning 
from noisy data.  
The consistency of an annotated corpus in whole 
could be divided down into consistency of each 
chunk. If the same chunks appear in the same 
context, they should be identically annotated. So 
we define the consistency of one special chunk as 
the ratio of identical annotation in the same context.  
 
corpusin  ))context( ,( of  No.
)context(in  annotation same of No.
        
 ) )context( ,cons(   
PP
P
PP
=
 
(1) 
?
=
=
N
i
ii PcontextPconsN 1
))(,(1cons(S)  (2) 
Where P represents a pattern of the chunk (POS 
or/and lexical sequence), context(P) represents the 
needed context to annotate this chunk, N represents 
the number of chunks in the whole corpus S. 
In order to improve the efficiency we also 
develop a semi-automatic tool that not only check 
mechanical errors but also detect those potential 
inconsistent annotations. For example, one inputs a 
POS pattern: ?a_n_n?, and an expected annotation 
result: ?B-NP_I-NP_E-NP3?, the tool will list all 
the consistent and inconsistent sentences in the 
annotated text respectively. Based on the output 
one can revise those inconsistent results one by one, 
and finally the consistency of the chunked text will 
be improved step by step. 
5 Chunking Model 
After annotating the corpus, we could use 
various learning algorithms to build the chunking 
model. In this paper, HMM is selected because not 
only its training speed is fast, but also it has 
comparable performance (Xun and Huang, 2000). 
Automatic chunking with HMM should conduct 
the following two steps. 1) Identify boundaries of 
each chunk. It is to assign each word a chunk mark, 
named M, which contains 5 classes: B, I, E, S (a 
single word chunk) and O (outside all chunks). 2) 
Tag the chunk type, named X, which contains 11 
types defined in Section 3.  
So each word will be tagged with two tags: M 
and X (the words excluding from any chunk only 
have M). So the result after chunking is a sequence 
of triples (t, m, x), where t, m, x represent POS tag, 
chunk mark and chunk type respectively. All the 
triples of a chunk are combined as an item ni, 
which also could be named as a chunk rule. Let W 
as the word segmentation result of a given sentence, 
T as POS tagging result and C (C= n1 n2?nj) as the 
chunking result. The statistical chunking model 
could be described as following: 
),(),|(maxarg      
),(/),(),|(maxarg      
),|(maxarg
TCPTCWP
TWPTCPTCWP
TWCPC
C
C
C
=
=
=
?
 
(3) 
Independent assumption is used to approximate 
P(W|C,T), that is: 
                                                     
3
 B, E, I represent the left/right boundary of a chunk 
and inside a chunk respectively, B-NP means this word 
is the beginning of NP. 
?
=
?
m
i
iiii xmtwPTCWP
1
),,|(),|(  (4) 
If the triple is unseen, formula 5 is used. 
2
,
)),,((max
),,(),,|(
kjikj
iii
iiii
xmtcount
xmtcount
xmtwP =  
(5) 
For P(C, T), tri-grams among chunks and outside 
words are used to approximate, that is: 
?
=
??
?
k
i
iii nnnPnnPnPTCP
3
12121 )|()|()(),(
 
(6) 
Smoothing follows the method of (Gao et al, 
2002).  
In order to improve the performance we use N-
fold error correction (Wu, 2004) technique to 
reduce the error rate and TBL is used to learn the 
error correction rules based on the output of HMM. 
6 Data and Evaluation 
The performance of chunking is commonly 
measured with three figures: precision (P), recall 
(R) and F measure that are defined in CoNLL2000. 
Besides these, we also use two other measurements 
to evaluate the performance of bracketing and 
labeling respectively: RCB(ratio of crossing 
brackets), that is the percentage of the found 
brackets which cross the correct brackets; 
LA(labeling accuracy), that is the percentage of the 
found chunks which have the correct labels.   
 
datain test  chunks  of  No.
boundarieschunk  crossed chunks  theof No.
       
  RCB   =
 
 
 boundariescorrect  with chunks   theof  No.
chunkscorrect  of No.
       
LA     =
 
The average length (ALen) of chunks for each 
type is the average number of tokens in each chunk 
of given type. The overall average length is the 
average number of tokens in each chunk. To be 
more disinterested, outside tokens (including 
outside punctuations) are also concerned and each 
of them is counted as one chunk.  
6.1 Chunking performance with our spec 
Training and test was done on the PK corpus. 
Table 3 shows the detail information. We use the 
uni-gram of chunk POS rules as the baseline.  
Data No. of tokens 
No. of 
chunks 
No. of 
outside 
ALen 
(include O) 
Train 444,777 229,989 92,839 1.377 
Test 28,382 13,879 5,493 1.363 
 
Table 3:The information of data set 
Table 4 shows the chunking performance of 
close test and open test when HMM and ten folds 
TBL based error correction (EC) are done 
respectively.  
 
Close Test (%) Open Test (%) 
 
F RCB LA F RCB LA 
Baseline 81.95 6.55 99.46 81.44 6.58 99.47 
HMM 94.79 2.62 99.78 88.39 3.18 99.65 
HMM+EC 95.11 2.38 99.91 91.13 2.87 99.80 
Table 4:The overall performance of chunking 
As can be seen, the performance of open test 
doesn?t drop much. For open test, HMM achieves 
6.9% F improvement, 3.4% RCB reduction on 
baseline; error correction gets another 2.7% F 
improvement, 0.3% RCB reduction. Labeling 
accuracy is so high even with the baseline, which 
indicates that the hard point of chunking is to 
identify the boundaries of each chunk.  
Table 5 shows the performance of each type of 
chunks respectively. NP and VP amount to 
approximately 76% of all chunks, so their 
chunking performance dominates the overall 
performance. Although we extend VP and PP, their 
performances are much better than overall. The 
performance of INDP can arrive 99% although it is 
much longer than other types. Because its surface 
evidences are clear and complete owing to its 
definition: the meta-data of a document, all the 
descriptions inside a pair of parenthesis, and also 
certain fixed phrases which do not act as a 
syntactic constituent in a sentence. From the 
relative lower performance of NP, but the most 
part of all chunks, we can conclude that the hardest 
issue of Chinese chunking is to identify boundaries 
of NPs.  
 
Percent
age(%) 
ALen 
(tokens) 
P 
(%) 
R 
(%) 
F 
(%) 
NP 45.94 1.649 88.82 86.25 87.52 
VP 29.82 1.416 96.60 96.49 96.55 
PP 6.59 1.221 93.67 93.58 93.63 
MP 3.69 1.818 89.51 86.33 87.89 
ADJP 3.77 1.308 86.11 89.43 87.74 
SP 2.71 1.167 84.70 84.03 84.36 
TP 2.59 1.251 93.23 94.30 93.76 
CONJP 2.22 1.000 97.20 98.73 97.96 
INDP 1.41 4.297 99.06 99.06 99.06 
ADVP 1.06 1.117 85.48 85.03 85.25 
INTJP 0.23 1.016 68.75 95.65 80.00 
ALL 100 1.507 91.70 90.55 91.13 
 
Table 5:The result of each type with our spec 
All the chunking errors could be classified into 
four types: wrong labeling, under-combining, over-
combining and overlapping. Table 6 lists the 
number and percentage of each type of errors. 
Under-combining errors count about a half  
number of overall chunking errors, however it is 
not a problem in certain applications because they 
does not cross the brackets, thus there are still 
opportunities to combine them later with additional 
knowledge. If we evaluate the chunking result 
without counting those under-combining errors, the 
F score of the proposed chunker achieves 95.45%. 
Error type No.of the Errors Percentage 
Wrong labeling 22 2.56% 
Under-combine 418 48.71% 
Over-combining 339 39.51% 
Overlapping 59 6.88% 
 
Table 6:The distribution of chunking errors 
With comparison we also use some other 
learning methods, MBL(Bosch and Buchholz, 
2002), SVM(Kudoh and Matsumoto, 2001) and 
TBL to build the chunker. The features for MBL 
and SVM are the POS of current, left two and right 
two words, lexical of current, left one and right one 
word. TiMBL 4  and SVM-light 5  are used as the 
tools. For SVM, we convert the chunk marks 
BIOES to BI and the binary class SVM is used to 
classifier the chunk boundary, then some rules are 
used to identify its label. For TBL, the rule 
templates are all the possible combinations of the 
features and the initial state is that each word is a 
chunk. Table 7 shows the result. As seen, without 
error correction all these models do not perform 
well and our HMM gets the best performance. 
 MBL SVM TBL HMM 
F(%) 85.31 86.25 86.92 88.39 
 
Table 7:Comparison with different algorithms 
6.2 Further applications 
The length of chunks with our spec (AoL is 1.38) 
is longer than other Treebank-derived specs (AoL 
of S1 is 1.239) and closer to the constituents of 
sentence. Thus there are several applications 
benefit from the fact, such as: 
1) The longest/full noun phrase identification. 
According to our statistics, due to including noun-
noun compounds, ?a_n_n? and ?m_n_n? inside NPs, 
65% noun chunks are already the longest/full  noun 
phrases and other 22% could become the longest 
/full noun phrases by only one next combining step. 
2) The predicate-verb identification. 
By extending the average length of VPs, the main 
verb (or predicate-verb, also called tensed verb in 
English) of a given sentence could be identified 
based on certain surface evidences with a relatively 
high accuracy. With certain definition our statistics 
based on our test set show that 84.88% of those 
main verbs are located in the first longest VPs 
among all VPs in a sentence. 
                                                     
4
 http://ilk.kub.nl/software.html  
5
 http://svmlight.joachims.org/ 
7 Related Work 
For chunking spec, the CoNLL2000 shared task 
defines a program chunklink to extract chunks 
from English Treebank. (Li, 2003) defines the 
similar Treebank-derived spec for Chinese and she 
reports manual check is also needed to make data 
consistent. Part of the Sparkle project has 
concentrates on a spec based on un-bracketed 
corpus of English, Italian, French and 
German(Carroll et al, 1997). (Zhou, 2002) defines 
base phrase which is similar as chunk for Chinese, 
but his annotation and experiment are on his own 
corpus. 
For chunking algorithm, many machine learning 
(ML) methods have been applied and got 
promising results after chunking is represented as 
tagging problem, such as: SVM (Kudoh and 
Matsumoto, 2001), Memory-based (Bosch and 
Buchholz, 2002), SNoW (Li and Roth), et al. 
Some rule-base chunking (Kinyon, 2003) and 
combining rules with learning (Park and Zhang, 
2003) are also reported.  
For annotation, (Brants, 2000) reports the inter-
annotator agreement of part-of-speech annotations 
is 98.57%, the one of structural annotations is 
92.43% and some consistency measures. (Xue et 
al., 2002) also address some issues related to 
building a large-scale Chinese corpus. 
8 Conclusion 
We propose a solution of Chinese chunking with 
another type of spec that is based on un-bracketed 
corpus rather than derived from a Treebank. 
Through spec drafting and annotating, most 
significant syntactic ambiguous patterns have been 
studied, and those observations in turn have been 
described in the spec carefully. The proposed 
method of defining a chunking spec helps us find a 
proper solution for the hard problems of chunking 
Chinese. The experiments show that with our spec, 
the overall Chinese chunking F-measure achieves 
91.13% and 95.45% if under-combining errors are 
not counted. 
9 Acknowledgements 
We would like to thank the members of the 
Natural Language Computing Group at Microsoft 
Research Asia. Especial acknowledgement is to the 
anonymous reviewers for their insightful 
comments and suggestions. Based on that we have 
revised the paper accordingly.  
References  
S. Abney. 1991. Parsing by chunks. In Principle-
Based Parsing. Kluwer Academic Publishers, 
Dordrecht: 257?278. 
L.A. Ramshaw and M.P. Marcus. 1995. Text 
chunkingusing transformation-based learning. In 
Proceedings of the 3rd ACL/SIGDAT Workshop, 
Cambridge, Massachusetts, USA: 82?94. 
E. Tjong Kim Sang and S. Buchholz. 2000. 
Introduction to the CoNLL-2000 shared task: 
Chunking. In Proceedings of CoNLL-2000 and 
LLL-2000, Lisbon, Portugal: 127?132.  
Sujian Li, Qun Liu and Zhifeng Yang. 2003. 
Chunking based on maximum entropy. Chinese 
Journal of Computer, 25(12): 1734?1738. 
Heng Li, Jingbo Zhu and Tianshun Yao. 2004. 
SVM based Chinese text chunking. Journal of 
Chinese Information Processing, 18(2): 1?7. 
Nianwen Xue and Fei Xia. 2000. The Bracketing 
Guidelines for the Penn Chinese Treebank(3.0). 
Technical report ,University of Pennsylvania, 
URL http:// www.cis.upenn.edu/~chinese/. 
Eric Brill. Transformation-based error-driven 
learning and natural language processing: A case 
study in part of speech tagging. Computational 
Linguistics,21 (4):543?565. 
Shiwen Yu, Huiming Duan, Xuefeng Zhu, et al 
2002. The basic processing of contemporary 
Chinese corpus at Peking University. Journal of 
Chinese Information Processing, 16(6): 58?65. 
K.A. Kaufman and R.S. Michalski. 1999. Learning 
from inconsistent and noisy data: the AQ18 
approach, Proceedings of the Eleventh 
International Symposium on Methodologies for 
Intelligent Systems, Warsaw: 411?419. 
Endong Xun and Changning Huang. 2000. A 
unified statistical model for the identification of 
English baseNP, In Proceedings of the 38th ACL: 
109?117. 
Jianfeng Gao, Joshua Goodman, Mingjing Li, Kai-
Fu Lee. Toward a unified approach to statistical 
language modeling for Chinese. ACM 
Transactions on Asian Language Information 
Processing, Vol. 1, No. 1, 2002: 3-33.  
Dekai WU, Grace NGAI, Marine CARPUAT. N-
fold Templated Piped Correction. Proceedings of 
the First International Joint Conference on 
Natural Language Processing, SANYA: 632?
637. 
Antal van den Bosch and S. Buchholz. 2002. 
Shallow parsing on the basis of words only: a 
case study, In Proceedings of the 40th ACL: 433?
440. 
Taku Kudo and Yuji Matsumoto. 2000. Use of 
support vector learning for chunk identification, 
In Proceedings of the 4th CoNLL: 142?144. 
J. Carroll, T. Briscoe, G. Carroll et al 1997. 
Phrasal parsing software. Sparkle Work 
Package 3, Deliverable D3.2.  
Yuqi Zhang and Qiang Zhou. 2002. Automatic 
identification of Chinese base phrases. Journal 
of Chinese Information Processing, 16(6):1?8. 
X. Li and D. Roth. 2001. Exploring evidence for 
shallow parsing. In Proceedings of the 5th 
CoNLL. 
Alexandra Kinyon. 2003. A language-independent 
shallow-parser compiler. In Proceedings of 10th 
EACL Conference, Toulouse, France: 322-329. 
S.-B. Park, B.-T. Zhang. 2003. Text chunking by 
combining hand-crafted rules and memory-based, 
In Proceedings of the 41th ACL: 497?504. 
Thorsten Brants. 2000. Inter-annotator agreement 
for a German newspaper corpus, In Second 
International Conference on Language 
Resources and Evaluation LREC-2000, Athens, 
Greece: 69?76. 
Nianwen Xue, Fu-Dong Chiou and M. Palmer. 
2002. Building a large-scale annotated Chinese 
corpus, In Proceedings of COLING.  
 
 
