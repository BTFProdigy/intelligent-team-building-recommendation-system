Proceedings of the 12th Conference of the European Chapter of the ACL, pages 148?156,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
EM Works for Pronoun Anaphora Resolution
Eugene Charniak and Micha Elsner
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{ec,melsner}@cs.brown.edu
Abstract
We present an algorithm for pronoun-
anaphora (in English) that uses Expecta-
tion Maximization (EM) to learn virtually
all of its parameters in an unsupervised
fashion. While EM frequently fails to find
good models for the tasks to which it is
set, in this case it works quite well. We
have compared it to several systems avail-
able on the web (all we have found so far).
Our program significantly outperforms all
of them. The algorithm is fast and robust,
and has been made publically available for
downloading.
1 Introduction
We present a new system for resolving (per-
sonal) pronoun anaphora1. We believe it is of
interest for two reasons. First, virtually all of
its parameters are learned via the expectation-
maximization algorithm (EM). While EM has
worked quite well for a few tasks, notably ma-
chine translations (starting with the IBM models
1-5 (Brown et al, 1993), it has not had success in
most others, such as part-of-speech tagging (Meri-
aldo, 1991), named-entity recognition (Collins
and Singer, 1999) and context-free-grammar in-
duction (numerous attempts, too many to men-
tion). Thus understanding the abilities and limi-
tations of EM is very much a topic of interest. We
present this work as a positive data-point in this
ongoing discussion.
Secondly, and perhaps more importantly, is the
system?s performance. Remarkably, there are very
few systems for actually doing pronoun anaphora
available on the web. By emailing the corpora-
list the other members of the list pointed us to
1The system, the Ge corpus, and the
model described here can be downloaded from
http://bllip.cs.brown.edu/download/emPronoun.tar.gz.
four. We present a head to head evaluation and find
that our performance is significantly better than
the competition.
2 Previous Work
The literature on pronominal anaphora is quite
large, and we cannot hope to do justice to it here.
Rather we limit ourselves to particular papers and
systems that have had the greatest impact on, and
similarity to, ours.
Probably the closest approach to our own is
Cherry and Bergsma (2005), which also presents
an EM approach to pronoun resolution, and ob-
tains quite successful results. Our work improves
upon theirs in several dimensions. Firstly, they
do not distinguish antecedents of non-reflexive
pronouns based on syntax (for instance, subjects
and objects). Both previous work (cf. Tetreault
(2001) discussed below) and our present results
find these distinctions extremely helpful. Sec-
ondly, their system relies on a separate prepro-
cessing stage to classify non-anaphoric pronouns,
and mark the gender of certain NPs (Mr., Mrs.
and some first names). This allows the incorpo-
ration of external data and learning systems, but
conversely, it requires these decisions to be made
sequentially. Our system classifies non-anaphoric
pronouns jointly, and learns gender without an
external database. Next, they only handle third-
person pronouns, while we handle first and sec-
ond as well. Finally, as a demonstration of EM?s
capabilities, its evidence is equivocal. Their EM
requires careful initialization ? sufficiently care-
ful that the EM version only performs 0.4% better
than the initialized program alone. (We can say
nothing about relative performance of their system
vs. ours since we have been able to access neither
their data nor code.)
A quite different unsupervised approach is
Kehler et al (2004a), which uses self-training of a
discriminative system, initialized with some con-
148
servative number and gender heuristics. The sys-
tem uses the conventional ranking approach, ap-
plying a maximum-entropy classifier to pairs of
pronoun and potential antecedent and selecting the
best antecedent. In each iteration of self-training,
the system labels the training corpus and its de-
cisions are treated as input for the next training
phase. The system improves substantially over a
Hobbs baseline. In comparison to ours, their fea-
ture set is quite similar, while their learning ap-
proach is rather different. In addition, their system
does not classify non-anaphoric pronouns,
A third paper that has significantly influenced
our work is that of (Haghighi and Klein, 2007).
This is the first paper to treat all noun phrase (NP)
anaphora using a generative model. The success
they achieve directly inspired our work. There are,
however, many differences between their approach
and ours. The most obvious is our use of EM
rather than theirs of Gibbs sampling. However, the
most important difference is the choice of training
data. In our case it is a very large corpus of parsed,
but otherwise unannotated text. Their system is
trained on the ACE corpus, and requires explicit
annotation of all ?markables? ? things that are or
have antecedents. For pronouns, only anaphoric
pronouns are so marked. Thus the system does
not learn to recognize non-anaphoric pronouns ?
a significant problem. More generally it follows
from this that the system only works (or at least
works with the accuracy they achieve) when the
input data is so marked. These markings not only
render the non-anaphoric pronoun situation moot,
but also significantly restrict the choice of possible
antecedent. Only perhaps one in four or five NPs
are markable (Poesio and Vieira, 1998).
There are also several papers which treat
coference as an unsupervised clustering problem
(Cardie and Wagstaff, 1999; Angheluta et al,
2004). In this literature there is no generative
model at all, and thus this work is only loosely
connected to the above models.
Another key paper is (Ge et al, 1998). The data
annotated for the Ge research is used here for test-
ing and development data. Also, there are many
overlaps between their formulation of the problem
and ours. For one thing, their model is genera-
tive, although they do not note this fact, and (with
the partial exception we are about to mention) they
obtain their probabilities from hand annotated data
rather than using EM. Lastly, they learn their gen-
der information (the probability of that a pronoun
will have a particular gender given its antecedent)
using a truncated EM procedure. Once they have
derived all of the other parameters from the train-
ing data, they go through a larger corpus of unla-
beled data collecting estimated counts of how of-
ten each word generates a pronoun of a particular
gender. They then normalize these probabilities
and the result is used in the final program. This is,
in fact, a single iteration of EM.
Tetreault (2001) is one of the few papers that
use the (Ge et al, 1998) corpus used here. They
achieve a very high 80% correct, but this is
given hand-annotated number, gender and syntac-
tic binding features to filter candidate antecedents
and also ignores non-anaphoric pronouns.
We defer discussion of the systems against
which we were able to compare to Section 7 on
evaluation.
3 Pronouns
We briefly review English pronouns and their
properties. First we only concern ourselves with
?personal? pronouns: ?I?, ?you?, ?he?, ?she?, ?it?,
and their variants. We ignore, e.g., relative pro-
nouns (?who?, ?which?, etc.), deictic pronouns
(?this?, ?that?) and others.
Personal pronouns come in four basic types:
subject ?I?, ?she?, etc. Used in subject position.
object ?me?, ?her? etc. Used in non-subject po-
sition.
possessive ?my? ?her?, and
reflexive ?myself?, ?herself? etc. Required by
English grammar in certain constructions ?
e.g., ?I kicked myself.?
The system described here handles all of these
cases.
Note that the type of a pronoun is not connected
with its antecedent, but rather is completely deter-
mined by the role it plays in it?s sentence.
Personal pronouns are either anaphoric or non-
anaphoric. We say that a pronoun is anaphoric
when it is coreferent with another piece of text in
the same discourse. As is standard in the field we
distinguish between a referent and an antecedent.
The referent is the thing in the world that the pro-
noun, or, more generally, noun phrase (NP), de-
notes. Anaphora on the other hand is a relation be-
149
tween pieces of text. It follows from this that non-
anaphoric pronouns come in two basic varieties ?
some have a referent, but because the referent is
not mentioned in the text2 there is no anaphoric
relation to other text. Others have no referent (ex-
pletive or pleonastic pronouns, as in ?It seems that
. . . ?). For the purposes of this article we do not
distinguish the two.
Personal pronouns have three properties other
than their type:
person first (?I?,?we?), second (?you?) or third
(?she?,?they?) person,
number singular (?I?,?he?) or plural (?we?,
?they?), and
gender masculine (?he?), feminine (?she?) or
neuter (?they?).
These are critical because it is these properties
that our generative model generates.
4 The Generative Model
Our generative model ignores the generation of
most of the discourse, only generating a pronoun?s
person, number,and gender features along with the
governor of the pronoun and the syntactic relation
between the pronoun and the governor. (Infor-
mally, a word?s governor is the head of the phrase
above it. So the governor of both ?I? and ?her? in
?I saw her? is ?saw?.
We first decide if the pronoun is anaphoric
based upon a distribution p(anaphoric). (Actu-
ally this is a bit more complex, see the discus-
sion in Section 5.3.) If the pronoun is anaphoric
we then select a possible antecedent. Any NP
in the current or two previous sentences is con-
sidered. We select the antecedent based upon a
distribution p(anaphora|context). The nature of
the ?context? is discussed below. Then given
the antecedent we generative the pronoun?s person
according to p(person|antecedent), the pronoun?s
gender according to p(gender|antecedent), num-
ber, p(number|antecedent) and governor/relation-
to-governor from p(governor/relation|antecedent).
To generate a non-anaphoric third person singu-
lar ?it? we first guess that the non-anaphoric pro-
nouns is ?it? according to p(?it?|non-anaphoric).
2Actually, as in most previous work, we only consider ref-
erents realized by NPs. For more general approaches see By-
ron (2002).
and then generate the governor/relation according
to p(governor/relation|non-anaphoric-it);
Lastly we generate any other non-anaphoric
pronouns and their governor with a fixed probabil-
ity p(other). (Strictly speaking, this is mathemati-
cally invalid, since we do not bother to normalize
over all the alternatives; a good topic for future re-
search would be exploring what happens when we
make this part of the model truly generative.)
One inelegant part of the model is the need
to scale the p(governor/rel|antecedent) probabili-
ties. We smooth them using Kneser-Ney smooth-
ing, but even then their dynamic range (a factor of
106) greatly exceeds those of the other parameters.
Thus we take their nth root. This n is the last of
the model parameters.
5 Model Parameters
5.1 Intuitions
All of our distributions start with uniform val-
ues. For example, gender distributions start with
the probability of each gender equal to one-third.
From this it follows that on the first EM iteration
all antecedents will have the same probability of
generating a pronoun. At first glance then, the EM
process might seem to be futile. In this section we
hope to give some intuitions as to why this is not
the case.
As is typically done in EM learning, we start
the process with a much simpler generative model,
use a few EM iterations to learn its parameters,
and gradually expose the data to more and more
complex models, and thus larger and larger sets of
parameters.
The first model only learns the probability of
an antecedent generating the pronoun given what
sentence it is in. We train this model through four
iterations before moving on to more complex ones.
As noted above, all antecedents initially have
the same probability, but this is not true after the
first iteration. To see how the probabilities diverge,
and diverge correctly, consider the first sentence of
a news article. Suppose it starts ?President Bush
announced that he ...? In this situation there is
only one possible antecedent, so the expectation
that ?he? is generated by the NP in the same sen-
tence is 1.0. Contrast this with the situation in the
third and subsequent sentences. It is only then that
we have expectation for sentences two back gener-
ating the pronoun. Furthermore, typically by this
point there will be, say, twenty NPs to share the
150
probability mass, so each one will only get an in-
crease of 0.05. Thus on the first iteration only the
first two sentences have the power to move the dis-
tributions, but they do, and they make NPs in the
current sentence very slightly more likely to gener-
ate the pronoun than the sentence one back, which
in turn is more likely than the ones two back.
This slight imbalance is reflected when EM
readjusts the probability distribution at the end of
the first iteration. Thus for the second iteration ev-
eryone contributes to subsequent imbalances, be-
cause it is no longer the case the all antecedents are
equally likely. Now the closer ones have higher
probability so forth and so on.
To take another example, consider how EM
comes to assign gender to various words. By the
time we start training the gender assignment prob-
abilities the model has learned to prefer nearer
antecedents as well as ones with other desirable
properties. Now suppose we consider a sentence,
the first half of which has no pronouns. Consider
the gender of the NPs in this half. Given no fur-
ther information we would expect these genders to
distribute themselves accord to the prior probabil-
ity that any NP will be masculine, feminine, etc.
But suppose that the second half of the sentence
has a feminine pronoun. Now the genders will be
skewed with the probability of one of them being
feminine being much larger. Thus in the same way
these probabilities will be moved from equality,
and should, in general be moved correctly.
5.2 Parameters Learned by EM
Virtually all model parameters are learned by EM.
We use the parsed version of the North-American
News Corpus. This is available from the (Mc-
Closky et al, 2008). It has about 800,000 articles,
and 500,000,000 words.
The least complicated parameter is the proba-
bility of gender given word. Most words that have
a clear gender have this reflected in their probabil-
ities. Some examples are shown in Table 1. We
can see there that EM gets ?Paul?, ?Paula?, and
?Wal-mart? correct. ?Pig? has no obvious gender
in English, and the probabilities reflect this. On
the other hand ?Piggy? gets feminine gender. This
is no doubt because of ?Miss Piggy? the puppet
character. ?Waist? the program gets wrong. Here
the probabilities are close to gender-of-pronoun
priors. This happens for a (comparatively small)
class of pronouns that, in fact, are probably never
Word Male Female Neuter
paul 0.962 0.002 0.035
paula 0.003 0.915 0.082
pig 0.445 0.170 0.385
piggy 0.001 0.853 0.146
wal-mart 0.016 0.007 0.976
waist 0.380 0.155 0.465
Table 1: Words and their probabilities of generat-
ing masculine, feminine and neuter pronouns
antecedent p(singular|antecedent)
Singular 0.939048
Plural 0.0409721
Not NN or NNP 0.746885
Table 2: The probability of an antecedent genera-
tion a singular pronoun as a function of its number
an antecedent, but are nearby random pronouns.
Because of their non-antecedent proclivities, this
sort of mistake has little effect.
Next consider p(number|antecedent), that is the
probability that a given antecedent will generate a
singular or plural pronoun. This is shown in Table
2. Since we are dealing with parsed text, we have
the antecedent?s part-of-speech, so rather than the
antecedent we get the number from the part of
speech: ?NN? and ?NNP? are singular, ?NNS?
and ?NNPS? are plural. Lastly, we have the prob-
ability that an antecedent which is not a noun will
have a singular pronoun associated with it. Note
that the probability that a singular antecedent will
generate a singular pronoun is not one. This is
correct, although the exact number probably is too
low. For example, ?IBM? may be the antecedent
of both ?we? and ?they?, and vice versa.
Next we turn to p(person|antecedent), predict-
ing whether the pronoun is first, second or third
person given its antecedent. We simplify this
by noting that we know the person of the an-
tecedent (everything except ?I? and ?you? and
their variants are third person), so we compute
p(person|person). Actually we condition on one
further piece of information, if either the pronoun
or the antecedent is being quoted. The idea is that
an ?I? in quoted material may be the same person
as ?John Doe? outside of quotes, if Mr. Doe is
speaking. Indeed, EM picks up on this as is il-
lustrated in Tables 3 and 4. The first gives the
situation when neither antecedent nor pronoun is
within a quotation. The high numbers along the
151
Person of Pronoun
Person of Ante First Second Third
First 0.923 0.076 0.001
Second 0.114 0.885 0.001
Third 0.018 0.015 0.967
Table 3: Probability of an antecedent generating a
first,second or third person pronoun as a function
of the antecedents person
Person of Pronoun
Person of Ante First Second Third
First 0.089 0.021 0.889
Second 0.163 0.132 0.705
Third 0.025 0.011 0.964
Table 4: Same, but when the antecedent is in
quoted material but the pronoun is not
diagonal (0.923, 0.885, and 0.967) show the ex-
pected like-goes-to-like preferences. Contrast this
with Table 4 which gives the probabilities when
the antecedent is in quotes but the pronoun is not.
Here we see all antecedents being preferentially
mapped to third person (0.889, 0.705, and 0.964).
We save p(antecedent|context) till last because
it is the most complicated. Given what we know
about the context of the pronoun not all antecedent
positions are equally likely. Some important con-
ditioning events are:
? the exact position of the sentence relative to
the pronoun (0, 1, or 2 sentences back),
? the position of the head of the antecedent
within the sentence (bucketed into 6 bins).
For the current sentence position is measured
backward from the pronoun. For the two pre-
vious sentences it is measure forward from
the start of the sentence.
? syntactic positions ? generally we expect
NPs in subject position to be more likely an-
tecedents than those in object position, and
those more likely than other positions (e.g.,
object of a preposition).
? position of the pronoun ? for example the
subject of the previous sentence is very likely
to be the antecedent if the pronoun is very
early in the sentence, much less likely if it is
at the end.
? type of pronoun ? reflexives can only be
bound within the same sentence, while sub-
Part of Speech pron proper common
0.094 0.057 0.032
Word Position bin 0 bin 2 bin 5
0.111 0.007 0.0004
Syntactic Type subj other object
0.068 0.045 0.037
Table 5: Geometric mean of the probability of
the antecedent when holding everything expect the
stated feature of the antecedent constant
ject and object pronouns may be anywhere.
Possessives may be in previous sentences but
this is not as common.
? type of antecedent. Intuitively other pro-
nouns and proper nouns are more likely to
be antecedents than common nouns and NPs
headed up by things other than nouns.
All told this comes to 2592 parameters (3 sen-
tences, 6 antecedent word positions, 3 syntactic
positions, 4 pronoun positions, 3 pronoun types,
and 4 antecedent types). It is impossible to say
if EM is setting all of these correctly. There are
too many of them and we do not have knowledge
or intuitions about most all of them. However, all
help performance on the development set, and we
can look at a few where we do have strong intu-
itions. Table 5 gives some examples. The first two
rows are devoted to the probabilities of particular
kind of antecedent (pronouns, proper nouns, and
common nouns) generating a pronoun, holding ev-
erything constant except the type of antecedent.
The numbers are the geometric mean of the prob-
abilities in each case. The probabilities are or-
dered according to, at least my, intuition with pro-
noun being the most likely (0.094), followed by
proper nouns (0.057), followed by common nouns
(0.032), a fact also noted by (Haghighi and Klein,
2007). When looking at the probabilities as a func-
tion of word position again the EM derived proba-
bilities accord with intuition, with bin 0 (the clos-
est) more likely than bin 2 more likely than bin
5. The last two lines have the only case where we
have found the EM probability not in accord with
our intuitions. We would have expected objects
of verbs to be more likely to generate a pronoun
than the catch-all ?other? case. This proved not to
be the case. On the other hand, the two are much
closer in probabilities than any of the other, more
intuitive, cases.
152
5.3 Parameters Not Set by EM
There are a few parameters not set by EM.
Several are connected with the well known syn-
tactic constraints on the use of reflexives. A simple
version of this is built in. Reflexives must have an
antecedent in same sentence, and generally cannot
be coreferent-referent with the subject of the sen-
tence.
There are three system parameters that we set
by hand to optimize performance on the develop-
ment set. The first is n. As noted above, the distri-
bution p(governor/relation|antecedent) has a much
greater dynamic range than the other probability
distributions and to prevent it from, in essence,
completely determining the answer, we take its
nth root. Secondly, there is a probability of gen-
erating a non-anaphoric ?it?. Lastly we have a
probability of generating each of the other non-
monotonic pronouns along with (the nth root of)
their governor. These parameters are 6, 0.1, and
0.0004 respectively.
6 Definition of Correctness
We evaluate all programs according to Mitkov?s
?resolution etiquette? scoring metric (also used
in Cherry and Bergsma (2005)), which is defined
as follows: if N is the number of non-anaphoric
pronouns correctly identified, A the number of
anaphoric pronouns correctly linked to their an-
tecedent, and P the total number of pronouns, then
a pronoun-anaphora program?s percentage correct
is N+AP .
Most papers dealing with pronoun coreference
use this simple ratio, or the variant that ignores
non-anaphoric pronouns. It has appeared under
a number of names: success (Yang et al, 2006),
accuracy (Kehler et al, 2004a; Angheluta et al,
2004) and success rate (Tetreault, 2001). The
other occasionally-used metric is the MUC score
restricted to pronouns, but this has well-known
problems (Bagga and Baldwin, 1998).
To make the definition perfectly concrete, how-
ever, we must resolve a few special cases. One
is the case in which a pronoun x correctly says
that it is coreferent with another pronoun y. How-
ever, the program misidentifies the antecedent of
y. In this case (sometimes called error chaining
(Walker, 1989)), both x and y are to be scored as
wrong, as they both end up in the wrong corefer-
ential chain. We believe this is, in fact, the stan-
dard (Mitkov, personal communication), although
there are a few papers (Tetreault, 2001; Yang et
al., 2006) which do the opposite and many which
simply do not discuss this case.
One more issue arises in the case of a system
attempting to perform complete NP anaphora3. In
these cases the coreferential chains they create
may not correspond to any of the original chains.
In these cases, we call a pronoun correctly re-
solved if it is put in a chain including at least one
correct non-pronominal antecedent. This defini-
tion cannot be used in general, as putting all NPs
into the same set would give a perfect score. For-
tunately, the systems we compare against do not
do this ? they seem more likely to over-split than
under-split. Furthermore, if they do take some
inadvertent advantage of this definition, it helps
them and puts our program at a possible disadvan-
tage, so it is a more-than-fair comparison.
7 Evaluation
To develop and test our program we use the dataset
annotated by Niyu Ge (Ge et al, 1998). This
consists of sections 0 and 1 of the Penn tree-
bank. Ge marked every personal pronoun and all
noun phrases that were coreferent with these pro-
nouns. We used section 0 as our development
set, and section 1 for testing. We reparsed the
sentences using the Charniak and Johnson parser
(Charniak and Johnson, 2005) rather than using
the gold-parses that Ge marked up. We hope
thereby to make the results closer to those a user
will experience. (Generally the gold trees perform
about 0.005 higher than the machine parsed ver-
sion.) The test set has 1119 personal pronouns
of which 246 are non-anaphoric. Our selection of
this dataset, rather than the widely used MUC-6
corpus, is motivated by this large number of pro-
nouns.
We compared our results to four currently-
available anaphora programs from the web. These
four were selected by sending a request to a com-
monly used mailing list (the ?corpora-list?) ask-
ing for such programs. We received four leads:
JavaRAP, Open-NLP, BART and GuiTAR. Of
course, these systems represent the best available
work, not the state of the art. We presume that
more recent supervised systems (Kehler et al,
2004b; Yang et al, 2004; Yang et al, 2006) per-
3Of course our system does not attempt NP coreference
resolution, nor does JavaRAP. The other three comparison
systems do.
153
form better. Unfortunately, we were unable to ob-
tain a comparison unsupervised learning system at
all.
Only one of the four is explicitly aimed
at personal-pronoun anaphora ? RAP (Resolu-
tion of Anaphora Procedure) (Lappin and Le-
ass, 1994). It is a non-statistical system orig-
inally implemented in Prolog. The version we
used is JavaRAP, a later reimplementation in Java
(Long Qiu and Chua, 2004). It only handles third
person pronouns.
The other three are more general in that they
handle all NP anaphora. The GuiTAR system
(Poesio and Kabadjov, 2004) is designed to work
in an ?off the shelf? fashion on general text GUI-
TAR resolves pronouns using the algorithm of
(Mitkov et al, 2002), which filters candidate an-
tecedents and then ranks them using morphosyn-
tactic features. Due to a bug in version 3, GUI-
TAR does not currently handle possessive pro-
nouns.GUITAR also has an optional discourse-
new classification step, which cannot be used as
it requires a discontinued Google search API.
OpenNLP (Morton et al, 2005) uses a
maximum-entropy classifier to rank potential an-
tecedents for pronouns. However despite being
the best-performing (on pronouns) of the existing
systems, there is a remarkable lack of published
information on its innards.
BART (Versley et al, 2008) also uses a
maximum-entropy model, based on Soon et al
(2001). The BART system also provides a more
sophisticated feature set than is available in the
basic model, including tree-kernel features and a
variety of web-based knowledge sources. Unfor-
tunately we were not able to get the basic version
working. More precisely we were able to run the
program, but the results we got were substantially
lower than any of the other models and we believe
that the program as shipped is not working prop-
erly.
Some of these systems provide their own pre-
processing tools. However, these were bypassed,
so that all systems ran on the Charniak parse trees
(with gold sentence segmentation). Systems with
named-entity detectors were allowed to run them
as a preprocess. All systems were run using the
models included in their standard distribution; typ-
ically these models are trained on annotated news
articles (like MUC-6), which should be relatively
similar to our WSJ documents.
System Restrictions Performance
GuiTAR No Possessives 0.534
JavaRap Third Person 0.529
Open-NLP None 0.593
Our System None 0.686
Table 6: Performance of Evaluated Systems on
Test Data
The performance of the remaining systems is
given in Table 6. The two programs with restric-
tions were only evaluated on the pronouns the sys-
tem was capable of handling.
These results should be approached with some
caution. In particular it is possible that the re-
sults for the systems other than ours are underes-
timated due to errors in the evaluation. Compli-
cations include the fact all of the four programs
all have different output conventions. The better
to catch such problems the authors independently
wrote two scoring programs.
Nevertheless, given the size of the difference
between the results of our system and the others,
the conclusion that ours has the best performance
is probably solid.
8 Conclusion
We have presented a generative model of pronoun-
anaphora in which virtually all of the parameters
are learned by expectation maximization. We find
it of interest first as an example of one of the few
tasks for which EM has been shown to be effec-
tive, and second as a useful program to be put in
general use. It is, to the best of our knowledge, the
best-performing system available on the web. To
down-load it, go to (to be announced).
The current system has several obvious limita-
tion. It does not handle cataphora (antecedents
occurring after the pronoun), only allows an-
tecedents to be at most two sentences back, does
not recognize that a conjoined NP can be the an-
tecedent of a plural pronoun, and has a very lim-
ited grasp of pronominal syntax. Perhaps the
largest limitation is the programs inability to rec-
ognize the speaker of a quoted segment. The result
is a very large fraction of first person pronouns are
given incorrect antecedents. Fixing these prob-
lems would no doubt push the system?s perfor-
mance up several percent.
However the most critical direction for future
research is to push the approach to handle full NP
154
anaphora. Besides being of the greatest impor-
tance in its own right, it would also allow us to
add one piece of information we currently neglect
in our pronominal system ? the more times a doc-
ument refers to an entity the more likely it is to do
so again.
9 Acknowledgements
We would like to thank the authors and main-
tainers of the four systems against which we did
our comparison, especially Tom Morton, Mijail
Kabadjov and Yannick Versley. Making your sys-
tem freely available to other researchers is one of
the best ways to push the field forward. In addi-
tion, we thank three anonymous reviewers.
References
Roxana Angheluta, Patrick Jeuniaux, Rudradeb Mi-
tra, and Marie-Francine Moens. 2004. Clustering
algorithms for noun phrase coreference resolution.
In Proceedings of the 7es Journes internationales
d?Analyse statistique des Donnes Textuelles, pages
60?70, Louvain La Neuve, Belgium, March 10?12.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2).
Donna K. Byron. 2002. Resolving pronominal
reference to abstract entities. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL2002), pages 80?
87, Philadelphia, PA, USA, July 6?12.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase
coreference as clustering. In In Proceedings of
EMNLP, pages 82?89.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 2005 Meeting of the
Assoc. for Computational Linguistics (ACL), pages
173?180.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning (CoNLL-2005),
pages 88?95, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Michael Collins and Yorav Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora (EMNLP 99).
Niyu Ge, John Hale, and Eugene Charniak. 1998. A
statistical approach to anaphora resolution. In Pro-
ceedings of the Sixth Workshop on Very Large Cor-
pora, pages 161?171, Orlando, Florida. Harcourt
Brace.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 848?855. Association for Computational
Linguistics.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004a. Competitive self-trained
pronoun interpretation. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Short Papers, pages 33?36, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computa-
tional Linguistics.
Andrew Kehler, Douglas E. Appelt, Lara Taylor, and
Aleksandr Simma. 2004b. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Proceedings of the 2004 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 289?296.
Shalom Lappin and Herber J. Leass. 1994. An algo-
rithm for pronouminal anaphora resolution. Compu-
tational Linguistics, 20(4):535?561.
Min-Yen Kan Long Qiu and Tat-Seng Chua. 2004.
A public reference implementation of the RAP
anaphora resolution algorithm. In Proceedings of
the Fourth International Conference on Language
Resources and Evaluation, volume I, pages 291?
294.
David McClosky, Eugene Charniak, and MarkJohnson.
2008. BLLIP North American News Text, Complete.
Linguistic Data Consortium. LDC2008T13.
Bernard Merialdo. 1991. Tagging text with a prob-
abilistic model. In International Conference on
Speech and Signal Processing, volume 2, pages
801?818.
Ruslan Mitkov, Richard Evans, and Constantin Ora?san.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In
Proceedings of the Third International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2002), Mexico City, Mexico,
February, 17 ? 23.
Thomas Morton, Joern Kottmann, Jason Baldridge, and
Gann Bierner. 2005. Opennlp: A java-based nlp
toolkit. http://opennlp.sourceforge.net.
155
Massimo Poesio and Mijail A. Kabadjov. 2004.
A general-purpos, of-the-shelf anaphora resolution
module: implementataion and preliminary evalu-
ation. In Proceedings of the 2004 international
Conference on Language Evaluation and Resources,
pages 663,668.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Joel R. Tetreault. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computational
Linguistics, 27(4):507?520.
Yannick Versley, Simone Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008.
Bart: A modular toolkit for coreference resolution.
In Companion Volume of the Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 9?12.
Marilyn A. Walker. 1989. Evaluating discourse pro-
cessing algorithms. In ACL, pages 251?261.
Xiaofeng Yang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Improving pronoun res-
olution by incorporating coreferential information
of candidates. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL2004), pages 127?134, Barcelona,
Spain, July 21?26.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 41?48, Sydney,
Australia, July. Association for Computational Lin-
guistics.
156
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 168?175,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilevel Coarse-to-fine PCFG Parsing
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil,
David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths,
Jeremy Moore, Michael Pozar, and Theresa Vu
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
ec@cs.brown.edu
Abstract
We present a PCFG parsing algorithm
that uses a multilevel coarse-to-fine
(mlctf) scheme to improve the effi-
ciency of search for the best parse.
Our approach requires the user to spec-
ify a sequence of nested partitions or
equivalence classes of the PCFG non-
terminals. We define a sequence of
PCFGs corresponding to each parti-
tion, where the nonterminals of each
PCFG are clusters of nonterminals of
the original source PCFG. We use the
results of parsing at a coarser level
(i.e., grammar defined in terms of a
coarser partition) to prune the next
finer level. We present experiments
showing that with our algorithm the
work load (as measured by the total
number of constituents processed) is
decreased by a factor of ten with no de-
crease in parsing accuracy compared to
standard CKY parsing with the origi-
nal PCFG. We suggest that the search
space over mlctf algorithms is almost
totally unexplored so that future work
should be able to improve significantly
on these results.
1 Introduction
Reasonably accurate constituent-based parsing
is fairly quick these days, if fairly quick means
about a second per sentence. Unfortunately, this
is still too slow for many applications. In some
cases researchers need large quantities of parsed
data and do not have the hundreds of machines
necessary to parse gigaword corpora in a week
or two. More pressingly, in real-time applica-
tions such as speech recognition, a parser would
be only a part of a much larger system, and
the system builders are not keen on giving the
parser one of the ten seconds available to pro-
cess, say, a thirty-word sentence. Even worse,
some applications require the parsing of multi-
ple candidate strings per sentence (Johnson and
Charniak, 2004) or parsing from a lattice (Hall
and Johnson, 2004), and in these applications
parsing efficiency is even more important.
We present here a multilevel coarse-to-fine
(mlctf) PCFG parsing algorithm that reduces
the complexity of the search involved in find-
ing the best parse. It defines a sequence of in-
creasingly more complex PCFGs, and uses the
parse forest produced by one PCFG to prune
the search of the next more complex PCFG.
We currently use four levels of grammars in our
mlctf algorithm. The simplest PCFG, which we
call the level-0 grammar, contains only one non-
trivial nonterminal and is so simple that min-
imal time is needed to parse a sentence using
it. Nonetheless, we demonstrate that it identi-
fies the locations of correct constituents of the
parse tree (the ?gold constituents?) with high
recall. Our level-1 grammar distinguishes only
argument from modifier phrases (i.e., it has two
nontrivial nonterminals), while our level-2 gram-
mar distinguishes the four major phrasal cate-
gories (verbal, nominal, adjectival and preposi-
tional phrases), and level 3 distinguishes all of
the standard categories of the Penn treebank.
168
The nonterminal categories in these grammars
can be regarded as clusters or equivalence classes
of the original Penn treebank nonterminal cat-
egories. (In fact, we obtain these grammars by
relabeling the node labels in the treebank and
extracting a PCFG from this relabelled treebank
in the standard fashion, but we discuss other ap-
proaches below.) We require that the partition
of the nonterminals defined by the equivalence
classes at level l + 1 be a refinement of the par-
tition defined at level l. This means that each
nonterminal category at level l+1 is mapped to a
unique nonterminal category at level l (although
in general the mapping is many to one, i.e., each
nonterminal category at level l corresponds to
several nonterminal categories at level l + 1).
We use the correspondence between categories
at different levels to prune possible constituents.
A constituent is considered at level l + 1 only
if the corresponding constituent at level l has
a probability exceeding some threshold.. Thus
parsing a sentence proceeds as follows. We first
parse the sentence with the level-0 grammar to
produce a parse forest using the CKY parsing
algorithm. Then for each level l + 1 we reparse
the sentence with the level l + 1 grammar us-
ing the level l parse forest to prune as described
above. As we demonstrate, this leads to consid-
erable efficiency improvements.
The paper proceeds as follows. We next dis-
cuss previous work (Section 2). Section 3 out-
lines the algorithm in more detail. Section
4 presents some experiments showing that the
work load (as measured by the total number of
constituents processed) is decreased by a fac-
tor of ten over standard CKY parsing at the
final level. We also discuss some fine points of
the results therein. Finally in section 5 we sug-
gest that because the search space of mlctf al-
gorithms is, at this point, almost totally unex-
plored, future work should be able to improve
significantly on these results.
2 Previous Research
Coarse-to-fine search is an idea that has ap-
peared several times in the literature of com-
putational linguistics and related areas. The
first appearance of this idea we are aware of is
in Maxwell and Kaplan (1993), where a cover-
ing CFG is automatically extracted from a more
detailed unification grammar and used to iden-
tify the possible locations of constituents in the
more detailed parses of the sentence. Maxwell
and Kaplan use their covering CFG to prune the
search of their unification grammar parser in es-
sentially the same manner as we do here, and
demonstrate significant performance improve-
ments by using their coarse-to-fine approach.
The basic theory of coarse-to-fine approxima-
tions and dynamic programming in a stochastic
framework is laid out in Geman and Kochanek
(2001). This paper describes the multilevel
dynamic programming algorithm needed for
coarse-to-fine analysis (which they apply to de-
coding rather than parsing), and show how
to perform exact coarse-to-fine computation,
rather than the heuristic search we perform here.
A paper closely related to ours is Goodman
(1997). In our terminology, Goodman?s parser
is a two-stage ctf parser. The second stage is a
standard tree-bank parser while the first stage is
a regular-expression approximation of the gram-
mar. Again, the second stage is constrained by
the parses found in the first stage. Neither stage
is smoothed. The parser of Charniak (2000)
is also a two-stage ctf model, where the first
stage is a smoothed Markov grammar (it uses
up to three previous constituents as context),
and the second stage is a lexicalized Markov
grammar with extra annotations about parents
and grandparents. The second stage explores
all of the constituents not pruned out after the
first stage. Related approaches are used in Hall
(2004) and Charniak and Johnson (2005).
A quite different approach to parsing effi-
ciency is taken in Caraballo and Charniak (1998)
(and refined in Charniak et al (1998)). Here
efficiency is gained by using a standard chart-
parsing algorithm and pulling constituents off
the agenda according to (an estimate of) their
probability given the sentence. This probability
is computed by estimating Equation 1:
p(nki,j | s) =
?(nki,j)?(nki,j)
p(s) . (1)
169
It must be estimated because during the
bottom-up chart-parsing algorithm, the true
outside probability cannot be computed. The
results cited in Caraballo and Charniak (1998)
cannot be compared directly to ours, but are
roughly in the same equivalence class. Those
presented in Charniak et al (1998) are superior,
but in Section 5 below we suggest that a com-
bination of the techniques could yield better re-
sults still.
Klein and Manning (2003a) describe efficient
A? for the most likely parse, where pruning is
accomplished by using Equation 1 and a true
upper bound on the outside probability. While
their maximum is a looser estimate of the out-
side probability, it is an admissible heuristic and
together with an A? search is guaranteed to find
the best parse first. One question is if the guar-
antee is worth the extra search required by the
looser estimate of the true outside probability.
Tsuruoka and Tsujii (2004) explore the frame-
work developed in Klein and Manning (2003a),
and seek ways to minimize the time required
by the heap manipulations necessary in this
scheme. They describe an iterative deepening
algorithm that does not require a heap. They
also speed computation by precomputing more
accurate upper bounds on the outside proba-
bilities of various kinds of constituents. They
are able to reduce by half the number of con-
stituents required to find the best parse (com-
pared to CKY).
Most recently, McDonald et al (2005) have
implemented a dependency parser with good
accuracy (it is almost as good at dependency
parsing as Charniak (2000)) and very impres-
sive speed (it is about ten times faster than
Collins (1997) and four times faster than Char-
niak (2000)). It achieves its speed in part be-
cause it uses the Eisner and Satta (1999) algo-
rithm for n3 bilexical parsing, but also because
dependency parsing has a much lower grammar
constant than does standard PCFG parsing ?
after all, there are no phrasal constituents to
consider. The current paper can be thought of
as a way to take the sting out of the grammar
constant for PCFGs by parsing first with very
few phrasal constituents and adding them only
Level: 0 1 2 3
S1
{
S1
{
S1
{
S1
P
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
S
?
?
?
?
?
?
?
?
?
?
?
?
?
S
VP
UCP
SQ
SBAR
SBARQ
SINV
N
?
?
?
?
?
?
?
?
?
?
?
?
?
NP
NAC
NX
LST
X
UCP
FRAG
MP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
A
?
?
?
?
?
?
?
?
?
?
?
?
?
ADJP
QP
CONJP
ADVP
INTJ
PRN
PRT
P
?
?
?
?
?
?
?
?
?
?
?
?
?
PP
PRT
RRC
WHADJP
WHADVP
WHNP
WHPP
Figure 1: The levels of nonterminal labels
after most constituents have been pruned away.
3 Multilevel Course-to-fine Parsing
We use as the underlying parsing algorithm a
reasonably standard CKY parser, modified to
allow unary branching rules.
The complete nonterminal clustering is given
in Figure 1. We do not cluster preterminals.
These remain fixed at all levels to the standard
Penn-tree-bank set Marcus et al (1993).
Level-0 makes two distinctions, the root node
and everybody else. At level 1 we make one
further distinction, between phrases that tend
to be heads of constituents (NPs, VPs, and Ss)
and those that tend to be modifiers (ADJPs,
PPs, etc.). Level-2 has a total of five categories:
root, things that are typically headed by nouns,
those headed by verbs, things headed by prepo-
sitions, and things headed by classical modifiers
(adjectives, adverbs, etc.). Finally, level 3 is the
170
S1
P
P
PRP
He
P
VBD
ate
P
IN
at
P
DT
the
NN
mall
.
.
S1
HP
HP
PRP
He
HP
VBD
ate
MP
IN
at
HP
DT
the
NN
mall
.
.
S1
S_
N_
PRP
He
S_
VBD
ate
P_
IN
at
N_
DT
the
NN
mall
.
.
S1
S
NP
PRP
He
VP
VBD
ate
PP
IN
at
NP
DT
the
NN
mall
.
.
Figure 2: A tree represented at levels 0 to 3
classical tree-bank set. As an example, Figure 2
shows the parse for the sentence ?He ate at the
mall.? at levels 0 to 3.
During training we create four grammars, one
for each level of granularity. So, for example, at
level 1 the tree-bank rule
S ?NP VP .
would be translated into the rule
HP ?HP HP .
That is, each constituent type found in ?S ?NP
VP .? is mapped into its generalization at level 1.
The probabilities of all rules are computed us-
ing maximum likelihood for constituents at that
level.
The grammar used by the parser can best be
described as being influenced by four compo-
nents:
1. the nonterminals defined at that level of
parsing,
2. the binarization scheme,
3. the generalizations defined over the bina-
rization, and
4. extra annotation to improve parsing accu-
racy.
The first of these has already been covered. We
discuss the other three in turn.
In anticipation of eventually lexicalizing the
grammar we binarize from the head out. For
example, consider the rule
A ?a b c d e
where c is the head constituent. We binarize
this as follows:
A ?A1 e
A1 ?A2 d
A2 ?a A3
A3 ?b c
Grammars induced in this way tend to be
too specific, as the binarization introduce a very
large number of very specialized phrasal cat-
egories (the Ai). Following common practice
Johnson (1998; Klein and Manning (2003b) we
Markovize by replacing these nonterminals with
ones that remember less of the immediate rule
context. In our version we keep track of only the
parent, the head constituent and the constituent
immediately to the right or left, depending on
which side of the constituent we are processing.
With this scheme the above rules now look like
this:
A ?Ad,c e
Ad,c ?Aa,c d
Aa,c ?a Ab,c
Ab,c ?b c
So, for example, the rule ?A ?Ad,c e? would
have a high probability if constituents of type
A, with c as their head, often have d followed
by e at their end.
Lastly, we add parent annotation to phrasal
categories to improve parsing accuracy. If we
assume that in this case we are expanding a rule
for an A used as a child of Q, and a, b, c, d, and
e are all phrasal categories, then the above rules
become:
AQ ?Ad,c eA
Ad,c ?Aa,c dA
Aa,c ?aA Ab,c
Ab,c ?bA cA
171
10?8 10?7 10?6 10?5 10?4 10?3
0.0001
0.001
0.01
0.1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 3: Probability of a gold constituent being
pruned as a function of pruning thresholds for
the first 100 sentences of the development corpus
Once we have parsed at a level, we evaluate
the probability of a constituent p(nki,j | s) ac-
cording to the standard inside-outside formula
of Equation 1. In this equation nki,j is a con-
stituent of type k spanning the words i to j, and
?(?) and ?(?) are the outside and inside proba-
bilities of the constituent, respectively. Because
we prune at the end each granularity level, we
can evaluate the equation exactly; no approxi-
mations are needed (as in, e.g., Charniak et al
(1998)).
During parsing, instead of building each con-
stituent allowed by the grammar, we first test
if the probability of the corresponding coarser
constituent (which we have from Equation 1 in
the previous round of parsing) is greater than
a threshold. (The threshold is set empirically
based upon the development data.) If it is below
the threshold, we do not put the constituent in
the chart. For example, before we can use a NP
and a VP to create a S (using the rule above),
we would first need to check that the probability
in the coarser grammar of using the same span
HP and HP to create a HP is above the thresh-
old. We use the standard inside-outside for-
mula to calculate this probability (Equation 1).
The empirical results below justify our conjec-
ture that there are thresholds that allow signifi-
cant pruning while leaving the gold constituents
untouched.
10?8 10?7 10?6 10?5 10?4 10?3
0.001
0.01
0.1
1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 4: Fraction of incorrect constituents kept
as a function of pruning thresholds for the first
100 sentences of the development corpus
4 Results
In all experiments the system is trained on the
Penn tree-bank sections 2-21. Section 23 is used
for testing and section 24 for development. The
input to the parser are the gold-standard parts
of speech, not the words.
The point of parsing at multiple levels of gran-
ularity is to prune the results of rough levels be-
fore going on to finer levels. In particular, it is
necessary for any pruning scheme to retain the
true (gold-standard WSJ) constituents in the
face of the pruning. To gain an idea of what
is possible, consider Figure 3. According to the
graph, at the zeroth level of parsing and a the
pruning level 10?4 the probability that a gold
constituent is deleted due to pruning is slightly
more than 0.001 (or 0.1%). At level three it is
slightly more that 0.01 (or 1.0%).
The companion figure, Figure 4 shows the
retention rate of the non-gold (incorrect) con-
stituents. Again, at pruning level 10?4 and pars-
ing level 0 we retain about .3 (30%) of the bad
constituents (so we pruned 70%), whereas at
level 3 we retain about .004 (0.4%). Note that
in the current paper we do not actually prune
at level 3, instead return the Viterbi parse. We
include pruning results here in anticipation of
future work in which level 3 would be a precur-
sor to still more fine-grained parsing.
As noted in Section 2, there is some (implicit)
172
Level Constits Constits % Pruned
Produced Pruned
?106 ?106
0 8.82 7.55 86.5
1 9.18 6.51 70.8
2 11.2 9.48 84.4
3 11,8 0 0.0
total 40.4 ? ?
3-only 392.0 0 0
Figure 5: Total constituents pruned at all levels
for WSJ section 23, sentences of length ? 100
debate in the literature on using estimates of
the outside probability in Equation 1, or instead
computing the exact upper bound. The idea is
that an exact upper bound gives one an admis-
sible search heuristic but at a cost, since it is a
less accurate estimator of the true outside prob-
ability. (Note that even the upper bound does
not, in general, keep all of the gold constituents,
since a non-perfect model will assign some of
them low probability.) As is clear from Figure
3, the estimate works very well indeed.
On the basis of this graph, we set the lowest
allowable constituent probability at ? 5 ? 10?4,
? 10?5, and ? 10?4 for levels 0,1, and 2, re-
spectively. No pruning is done at level 3, since
there is no level 4. After setting the pruning
parameters on the development set we proceed
to parse the test set (WSJ section 23). Figure 5
shows the resulting pruning statistics. The to-
tal number of constituents created at level 0, for
all sentences combined, is 8.82 ? 106. Of those
7.55 ? 106 (or 86.5%) are pruned before going on
to level 1. At level 1, the 1.3 million left over
from level 0 expanded to a total of 9.18 ? 106.
70.8% of these in turn are pruned, and so forth.
The percent pruned at, e.g., level 1 in Figure 3
is much higher than that shown here because it
considers all of the possible level-1 constituents,
not just those left unpruned after level 0.
There is no pruning at level 3. There we sim-
ply return the Viterbi parse. We also show that
with pruning we generate a total of 40.4 ? 106
constituents. For comparison exhaustively pars-
ing using the tree-bank grammar yields a total
of 392 ? 106 constituents. This is the factor-of-10
Level Time for Level Running Total
0 1598 1598
1 2570 4168
2 4303 8471
3 1527 9998
3-only 114654 ?
Figure 6: Running times in seconds on WSJ sec-
tion 23, with and without pruning
workload reduction mentioned in Section 1.
There are two points of interest. The first is
that each level of pruning is worthwhile. We do
not get most of the effect from one or the other
level. The second point is that we get signif-
icant pruning at level 0. The reader may re-
member that level 0 distinguishes only between
the root node and the rest. We initially ex-
pected that it would be too coarse to distinguish
good from bad constituents at this level, but it
proved as useful as the other levels. The expla-
nation is that this level does use the full tree-
bank preterminal tags, and in many cases these
alone are sufficient to make certain constituents
very unlikely. For example, what is the proba-
bility of any constituent of length two or greater
ending in a preposition? The answer is: very
low. Similarly for constituents of length two or
greater ending in modal verbs, and determiners.
Not quite so improbable, but nevertheless less
likely than most, would be constituents ending
in verbs, or ending just short of the end of the
sentence.
Figure 6 shows how much time is spent at each
level of the algorithm, along with a running to-
tal of the time spent to that point. (This is for
all sentences in the test set, length ? 100.) The
number for the unpruned parser is again about
ten times that for the pruned version, but the
number for the standard CKY version is prob-
ably too high. Because our CKY implementa-
tion is quite slow, we ran the unpruned version
on many machines and summed the results. In
all likelihood at least some of these machines
were overloaded, a fact that our local job dis-
tributer would not notice. We suspect that the
real number is significantly lower, though still
173
No pruning 77.9
With pruning 77.9
Klein and Manning (2003b) 77.4
Figure 7: Labeled precision/recall f-measure,
WSJ section 23, all sentences of length ? 100
much higher than the pruned version.
Finally Figure 7 shows that our pruning is ac-
complished without loss of accuracy. The results
with pruning include four sentences that did not
receive any parses at all. These sentences re-
ceived zeros for both precision and recall and
presumably lowered the results somewhat. We
allowed ourselves to look at the first of these,
which turned out to contain the phrase:
(NP ... (INTJ (UH oh) (UH yes)) ...)
The training data does not include interjections
consisting of two ?UH?s, and thus a gold parse
cannot be constructed. Note that a different
binarization scheme (e.g. the one used in Klein
and Manning (2003b) would have smoothed over
this problem. In our case the unpruned version
is able to patch together a lot of very unlikely
constituents to produce a parse, but not a very
good one. Thus we attribute the problem not to
pruning, but to binarization.
We also show the results for the most similar
Klein and Manning (2003b) experiment. Our
results are slightly better. We attribute the dif-
ference to the fact that we have the gold tags
and they do not, but their binarization scheme
does not run into the problems that we encoun-
tered.
5 Conclusion and Future Research
We have presented a novel parsing algorithm
based upon the coarse-to-fine processing model.
Several aspects of the method recommend it.
First, unlike methods that depend on best-first
search, the method is ?holistic? in its evalua-
tion of constituents. For example, consider the
impact of parent labeling. It has been repeat-
edly shown to improve parsing accuracy (John-
son, 1998; Charniak, 2000; Klein and Manning,
2003b), but it is difficult if not impossible to
integrate with best-first search in bottom-up
chart-parsing (as in Charniak et al (1998)). The
reason is that when working bottom up it is diffi-
cult to determine if, say, ssbar is any more or less
likely than ss, as the evidence, working bottom
up, is negligible. Since our method computes
the exact outside probability of constituents (al-
beit at a coarser level) all of the top down in-
formation is available to the system. Or again,
another very useful feature in English parsing
is the knowledge that a constituent ends at the
right boundary (minus punctuation) of a string.
This can be included only in an ad-hoc way when
working bottom up, but could be easily added
here.
Many aspects of the current implementation
that are far from optimal. It seems clear to
us that extracting the maximum benefit from
our pruning would involve taking the unpruned
constituents and specifying them in all possible
ways allowed by the next level of granularity.
What we actually did is to propose all possi-
ble constituents at the next level, and immedi-
ately rule out those lacking a corresponding con-
stituent remaining at the previous level. This
was dictated by ease of implementation. Before
using mlctf parsing in a production parser, the
other method should be evaluated to see if our
intuitions of greater efficiency are correct.
It is also possible to combine mlctf parsing
with queue reordering methods. The best-first
search method of Charniak et al (1998) esti-
mates Equation 1. Working bottom up, estimat-
ing the inside probability is easy (we just sum
the probability of all the trees found to build
this constituent). All the cleverness goes into
estimating the outside probability. Quite clearly
the current method could be used to provide a
more accurate estimate of the outside probabil-
ity, namely the outside probability at the coarser
level of granularity.
There is one more future-research topic to add
before we stop, possibly the most interesting of
all. The particular tree of coarser to finer con-
stituents that governs our mlctf algorithm (Fig-
ure 1) was created by hand after about 15 min-
utes of reflection and survives, except for typos,
with only two modifications. There is no rea-
174
son to think it is anywhere close to optimal. It
should be possible to define ?optimal? formally
and search for the best mlctf constituent tree.
This would be a clustering problem, and, for-
tunately, one thing statistical NLP researchers
know how to do is cluster.
Acknowledgments
This paper is the class project for Computer
Science 241 at Brown University in fall 2005.
The faculty involved were supported in part
by DARPA GALE contract HR0011-06-2-0001.
The graduate students were mostly supported
by Brown University fellowships. The under-
graduates were mostly supported by their par-
ents. Our thanks to all.
References
Sharon Caraballo and Eugene Charniak. 1998. Fig-
ures of merit for best-first probabalistic parsing.
Computational Linguistics, 24(2):275?298.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 2005 Meeting of
the Association for Computational Linguistics.
Eugene Charniak, Sharon Goldwater, and Mark
Johnson. 1998. Edge-based best-first chart pars-
ing. In Proceedings of the Sixth Workshop on
Very Large Corpora, pages 127?133. Morgan Kauf-
mann.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 132?139.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, San Francisco. Mor-
gan Kaufmann.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 457?464.
Stuart Geman and Kevin Kochanek. 2001. Dy-
namic programming and the representation of
soft-decodable codes. IEEE Transactions on In-
formation Theory, 47:549?568.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 1997).
Keith Hall and Mark Johnson. 2004. Attention shift-
ing for parsing speech. In The Proceedings of the
42th Annual Meeting of the Association for Com-
putational Linguistics, pages 40?46.
Keith Hall. 2004. Best-first Word-lattice Pars-
ing: Techniques for Integrated Syntactic Language
Modeling. Ph.D. thesis, Brown University.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 33?
39.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003a. A* parsing:
Fast exact viterbi parse selection. In Proceedings
of HLT-NAACL?03.
Dan Klein and Christopher Manning. 2003b. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?
330.
John T. Maxwell and Ronald M. Kaplan. 1993.
The interface between phrasal and functional con-
straints. Computational Linguistics, 19(4):571?
590.
Ryan McDonald, Toby Crammer, and Fernando
Pereira. 2005. Online large margin training of
dependency parsers. In Proceedings of the 43rd
Meeting of the Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. It-
erative cky parsing for probabilistic context-free
grammars. In International Joint Conference on
Natural-Language Processing.
175
Proceedings of NAACL HLT 2007, pages 436?443,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Unified Local and Global Model for Discourse Coherence
Micha Elsner, Joseph Austerweil, and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@cs.brown.edu, joseph.austerweil@gmail.com
Abstract
We present a model for discourse co-
herence which combines the local entity-
based approach of (Barzilay and Lapata,
2005) and the HMM-based content model
of (Barzilay and Lee, 2004). Unlike the
mixture model of (Soricut and Marcu,
2006), we learn local and global features
jointly, providing a better theoretical ex-
planation of how they are useful. As the
local component of our model we adapt
(Barzilay and Lapata, 2005) by relaxing
independence assumptions so that it is ef-
fective when estimated generatively. Our
model performs the ordering task compet-
itively with (Soricut and Marcu, 2006),
and significantly better than either of the
models it is based on.
1 Introduction
Models of coherent discourse are central to several
tasks in natural language processing: such mod-
els have been used in text generation (Kibble and
Power, 2004) and evaluation of human-produced
text in educational applications (Miltsakaki and Ku-
kich, 2004; Higgins et al, 2004). Moreover, an ac-
curate model can reveal information about document
structure, aiding in such tasks as supervised summa-
rization (Barzilay and Lapata, 2005).
Models of coherence tend to fall into two classes.
Local models (Lapata, 2003; Barzilay and Lapata,
2005; Foltz et al, 1998) attempt to capture the gen-
eralization that adjacent sentences often have similar
content, and therefore tend to contain related words.
Models of this type are good at finding sentences
that belong near one another in the document. How-
ever, they have trouble finding the beginning or end
of the document, or recovering from sudden shifts in
topic (such as occur at paragraph boundaries). Some
local models also have trouble deciding which of a
pair of related sentences ought to come first.
In contrast, the global HMM model of Barzilay
and Lee (2004) tries to track the predictable changes
in topic between sentences. This gives it a pro-
nounced advantage in ordering sentences, since it
can learn to represent beginnings, ends and bound-
aries as separate states. However, it has no local
features; the particular words in each sentence are
generated based only on the current state of the doc-
ument. Since information can pass from sentence
to sentence only in this restricted manner, the model
sometimes fails to place sentences next to the correct
neighbors.
We attempt here to unify the two approaches by
constructing a model with both sentence-to-sentence
dependencies providing local cues, and a hidden
topic variable for global structure. Our local fea-
tures are based on the entity grid model of (Barzilay
and Lapata, 2005; Lapata and Barzilay, 2005). This
model has previously been most successful in a con-
ditional setting; to integrate it into our model, we
first relax its independence assumptions to improve
its performance when used generatively. Our global
model is an HMM like that of Barzilay and Lee
(2004), but with emission probabilities drawn from
the entity grid. We present results for two tasks,
the ordering task, on which global models usually
do well, and the discrimination task, on which lo-
cal models tend to outperform them. Our model im-
proves on purely global or local approaches on both
436
tasks.
Previous work by Soricut and Marcu (2006) has
also attempted to integrate local and global fea-
tures using a mixture model, with promising results.
However, mixture models lack explanatory power;
since each of the individual component models is
known to be flawed, it is difficult to say that the com-
bination is theoretically more sound than the parts,
even if it usually works better. Moreover, since the
model we describe uses a strict subset of the fea-
tures used in the component models of (Soricut and
Marcu, 2006), we suspect that adding it to the mix-
ture would lead to still further improved results.
2 Naive Entity Grids
Entity grids, first described in (Lapata and Barzilay,
2005), are designed to capture some ideas of Cen-
tering Theory (Grosz et al, 1995), namely that ad-
jacent utterances in a locally coherent discourses are
likely to contain the same nouns, and that important
nouns often appear in syntactically important roles
such as subject or object. An entity grid represents
a document as a matrix with a column for each en-
tity, and a row for each sentence. The entry ri,j de-
scribes the syntactic role of entity j in sentence i:
these roles are subject (S), object (O), or some other
role (X)1. In addition there is a special marker (-)
for nouns which do not appear at all in a given sen-
tence. Each noun appears only once in a given row
of the grid; if a noun appears multiple times, its grid
symbol describes the most important of its syntac-
tic roles: subject if possible, then object, or finally
other. An example text is figure 1, whose grid is fig-
ure 2.
Nouns are also treated as salient or non-salient,
another important concern of Centering Theory. We
condition events involving a noun on the frequency
of that noun. Unfortunately, this way of representing
salience makes our model slightly deficient, since
the model conditions on a particular noun occurring
e.g. 2 times, but assigns nonzero probabilities to
documents where it occurs 3 times. This is theo-
1Roles are determined heuristically using trees produced by
the parser of (Charniak and Johnson, 2005). Following previous
work, we slightly conflate thematic and syntactic roles, marking
the subject of a passive verb as O.
2The numeric token ?1300? is removed in preprocessing,
and ?Nuevo Laredo? is marked as ?PROPER?.
0 [The commercial pilot]O , [sole occupant of [the airplane]X]X
, was not injured .
1 [The airplane]O was owned and operated by [a private
owner]X .
2 [Visual meteorological conditions]S prevailed for [the per-
sonal cross country flight for which [a VFR flight plan]O was
filed]X .
3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at
[approximately 1300]X .
Figure 1: A section of a document, with syntactic
roles of noun phrases marked.
0 1 2 3
PLAN - - O -
AIRPLANE X O - -
CONDITION - - S -
FLIGHT - - X S
PILOT O - - -
PROPER - - - X
OWNER - X - -
OCCUPANT X - - -
Figure 2: The entity grid for figure 12.
retically quite unpleasant but in comparing different
orderings of the same document, it seems not to do
too much damage.
Properly speaking entities may be referents of
many different nouns and pronouns throughout the
discourse, and both (Lapata and Barzilay, 2005) and
(Barzilay and Lapata, 2005) present models which
use coreference resolution systems to group nouns.
We follow (Soricut and Marcu, 2006) in dropping
this component of the system, and treat each head
noun as having an individual single referent.
To model transitions in this entity grid model,
Lapata and Barzilay (2005) takes a generative ap-
proach. First, the probability of a document is de-
fined as P (D) = P (Si..Sn), the joint probability of
all the sentences. Sentences are generated in order
conditioned on all previous sentences:
P (D) =
?
i
P (Si|S0..(i?1)). (1)
We make a Markov assumption of order h (in our
experiments h = 2) to shorten the history. We repre-
sent the truncated history as ~Shi?1 = S(i?h)..S(i?1).
Each sentence Si can be split up into a set of
nouns representing entities, Ei, and their corre-
sponding syntactic roles Ri, plus a set of words
which are not entities, Wi. The model treats Wi as
independent of the previous sentences. For any fixed
437
set of sentences Si,
?
i P (Wi) is always constant,
and so cannot help in finding a coherent ordering.
The probability of a sentence is therefore dependent
only on the entities:
P (Si|~Sh(i?1)) = P (Ei, Ri|~Sh(i?1)). (2)
Next, the model assumes that each entity ej ap-
pears in sentences and takes on syntactic roles in-
dependent of all the other entities. As we show
in section 3, this assumption can be problem-
atic. Once we assume this, however, we can sim-
plify P (Ei, Ri|~Sh(i?1)) by calculating for each en-
tity whether it occurs in sentence i and if so, which
role it takes. This is equivalent to predicting ri,j .
We represent the history of the specific entity ej as
~r h(i?1),j = r(i?h),j ..r(i?1),j , and write:
P (Ei, Ri|~Sh(i?1)) ?
?
j
P (ri,j|~r h(i?1),j). (3)
For instance, in figure 2, the probability of S3 with
horizon 1 is the product of P (S|X) (for FLIGHT),
P (X|?) (for PROPER), and likewise for each other
entity, P (?|O), P (?|S), P (?|?)3.
Although this generative approach outperforms
several models in correlation with coherence ratings
assigned by human judges, it suffers in comparison
with later systems. Barzilay and Lapata (2005) uses
the same grid representation, but treats the transi-
tion probabilities P (ri,j |~ri,j) for each document as
features for input to an SVM classifier. Soricut and
Marcu (2006)?s implementation of the entity-based
model also uses discriminative training.
The generative model?s main weakness in com-
parison to these conditional models is its assump-
tion of independence between entities. In real doc-
uments, each sentence tends to contain only a few
nouns, and even fewer of them can fill roles like
subject and object. In other words, nouns compete
with each other for the available syntactic positions
in sentences; once one noun is chosen as the sub-
ject, the probability that any other will also become
a subject (of a different subclause of the same sen-
tence) is drastically lowered. Since the generative
entity grid does not take this into account, it learns
that in general, the probability of any given entity
appearing in a specific sentence is low. Thus it gen-
erates blank sentences (those without any nouns at
all) with overwhelmingly high probability.
It may not be obvious that this misallocation of
probability mass also reduces the effectiveness of
the generative entity grid in ordering fixed sets of
sentences. However, consider the case where an en-
tity has a history ~r h, and then does not appear in
the next sentence. The model treats this as evidence
that entities generally do not occur immediately af-
ter ~r h? but it may also happen that the entity was
outcompeted by some other word with even more
significance.
3 Relaxed Entity Grid
In this section, we relax the troublesome assump-
tion of independence between entities, thus mov-
ing the probability distribution over documents away
from blank sentences. We begin at the same point as
above: sequential generation of sentences: P (D) =
?
i P (Si|S0..(i?1)). We similarly separate the words
into entities and non-entities, treat the non-entities as
independent of the history ~S and omit them. We also
distinguish two types of entities. Let the known set
Ki = ej : ej ? ~S(i?1), the set of all entities which
have appeared before sentence i. Of the entities ap-
pearing in Si, those in Ki are known entities, and
those which are not are new entities. Since each en-
tity in the document is new precisely once, we treat
these as independent and omit them from our calcu-
lations as we did the non-entities. We return to both
groups of omitted words in section 4 below when
discussing our topic-based models.
To model a sentence, then, we generate the set of
known entities it contains along with their syntac-
tic roles, given the history and the known set Ki.
We truncate the history, as above, with horizon h;
note that this does not make the model Markovian,
since the known set has no horizon. Finally, we con-
sider only the portion of the history which relates to
known nouns (since all non-known nouns have the
same history - -). In all the equations below, we re-
strict Ei to known entities which actually appear in
sentence i, and Ri to roles filled by known entities.
The probability of a sentence is now:
P (Si|~Sh(i?1)) = P (Ei, Ri|~Rh(i?1)). (4)
We make one further simplification before begin-
ning to approximate: we first generate the set of syn-
tactic slots Ri which we intend to fill with known en-
tities, and then decide which entities from the known
438
set to select. Again, we assume independence from
the history, so that the contribution of P (Ri) for any
ordering of a fixed set of sentences is constant and
we omit it:
P (Ei, Ri|~Rh(i?1),j) = P (Ei|Ri, ~Rh(i?1),j). (5)
Estimating P (Ei|Ri, ~Rh(i?1),j) proves to be dif-
ficult, since the contexts are very sparse. To con-
tinue, we make a series of approximations. First let
each role be filled individually (where r ? e is the
boolean indicator function ?noun e fills role r?):
P (Ei|Ri, ~Rh(i?1),j) ?
?
r?Ri
P (r ? ej |r, ~Rh(i?1),j).
(6)
Notice that this process can select the same noun ej
to fill multiple roles r, while the entity grid cannot
represent such an occurrence. The resulting distri-
bution is therefore slightly deficient.
Unfortunately, we are still faced with the sparse
context ~Rh(i?1),j , the set of histories of all currently
known nouns. It is much easier to estimate P (r ?
ej |r,~r h(i?1),j), where we condition only on the his-
tory of the particular noun which is chosen to fill
slot r. However, in this case we do not have a proper
probability distribution: i.e. the probabilities do not
sum to 1. To overcome this difficulty we simply nor-
malize by force3:
P (r ? ej|r, ~Rh(i?1),j) ? (7)
P (r ? ej |r,~r h(i?1),j)
?
j?Ki P (r ? ej|r,~r h(i?1),j)
The individual probabilities P (r ? ej |r,~r h(i?1),j)
are calculated by counting situations in the train-
ing documents in which a known noun has his-
tory ~r h(i?1),j and fills slot r in the next sentence,
versus situations where the slot r exists but is
filled by some other noun. Some rare contexts are
still sparse, and so we smooth by adding a pseu-
docount of 1 for all events. Our model is ex-
pressed by equations (1),(4),(5),(6) and (7). In
3Unfortunately this estimator is not consistent (that is, given
infinite training data produced by the model, the estimated pa-
rameters do not converge to the true parameters). We are in-
vestigating maximum entropy estimation as a solution to this
problem.
figure 2, the probability of S3 with horizon 1 is
now calculated as follows: the known set con-
tains PLAN, AIRPLANE, CONDITION, FLIGHT,
PILOT, OWNER and OCCUPANT. There is one syn-
tactic role filled by a known noun, S. The proba-
bility is then calculated as P (+|S,X) (the proba-
bility of selecting a noun with history X to fill the
role of S) normalized by P (+|S,O)+P (+|S,S)+
P (+|S,X) + 4? P (+|S,?).
Like Lapata and Barzilay (2005), our relaxed
model assigns low probability to sentences where
nouns with important-seeming histories do not ap-
pear. However, in our model, the penalty is less
severe if there are many competitor nouns. On the
other hand, if the sentence contains many slots, giv-
ing the noun more opportunity to fill one of them,
the penalty is proportionally greater if it does not
appear.
4 Topic-Based Model
The model we describe above is a purely local one,
and moreover it relies on a particular set of local fea-
tures which capture the way adjacent sentences tend
to share lexical choices. Its lack of any global struc-
ture makes it impossible for the model to recover at
a paragraph boundary, or to accurately guess which
sentence should begin a document. Its lack of lexi-
calization, meanwhile, renders it incapable of learn-
ing dependences between pairs of words: for in-
stance, that a sentence discussing a crash is often
followed by a casualty report.
We remedy both these problems by extending our
model of document generation. Like Barzilay and
Lee (2004), we learn an HMM in which each sen-
tence has a hidden topic qi, which is chosen con-
ditioned on the previous state qi?1. The emission
model of each state is an instance of the relaxed en-
tity grid model as described above, but in addition
to conditioning on the role and history, we condi-
tion also on the state and on the particular set of
lexical items lex(Ki) which may be selected to fill
the role: P (r ? ej |r, ~Rh(i?1),j , qi, lex(Ki)). This
distribution is approximated as above by the nor-
malized value of P (r ? ej |r,~r h(i?1),j , qi, lex(ej)).
However, due to our use of lexical information,
even this may be too sparse for accurate estima-
tion, so we back off by interpolating with the pre-
439
Figure 3: A single time-slice of our HMM.
Wi ? PY (?|qi; ?LM , discountLM )
Ni ? PY (?|qi; ?NN , discountNN )
Ei ? EGrid(?|R, ~R2i?1, qi, lex(Ki); ?EG)
qi ? DP (?|qi?1)
In the equations above, only the manually set inter-
polation hyperparameters are indicated.
vious model. In each context, we introduce ?EG
pseudo-observations, split fractionally according to
the backoff distribution: if we abbreviate the context
in the relaxed entity grid as C and the event as e, this
smoothing corresponds to:
P (e|C, qi, ej) =
#(e,C, qi, ej) + ?EGP (e|C)
#(e,C, qi, ej) + ?EG
.
This is equivalent to defining the topic-based entity
grid as a Dirichlet process with parameter ?EG sam-
pling from the relaxed entity grid.
In addition, we are now in a position to gener-
ate the non-entity words Wi and new entities Ni in
an informative way, by conditioning on the sentence
topic qi. Since they are interrupted by the known
entities, they do not form contiguous sequences of
words, so we make a bag-of-words assumption. To
model these sets of words, we use unigram ver-
sions of the hierarchical Pitman-Yor processes of
(Teh, 2006), which implement a Bayesian version
of Kneser-Ney smoothing.
To represent the HMM itself, we adapt the non-
parametric HMM of (Beal et al, 2001). This is
a Bayesian alternative to the conventional HMM
model learned using EM, chosen mostly for conve-
nience. Our variant of it, unlike (Beal et al, 2001),
has no parameter ? to control self-transitions; our
emission model is complex enough to make it un-
necessary.
The actual number of states found by the model
depends mostly on the backoff constants, the ?s
(and, for Pitman-Yor processes, discounts) chosen
for the emission models (the entity grid, non-entity
word model and new noun model), and is relatively
insensitive to particular choices of prior for the other
hyperparameters. As the backoff constants decrease,
the emission models become more dependent on the
state variable q, which leads to more states (and
eventually to memorization of the training data). If
instead the backoff rate increases, the emission mod-
els all become close to the general distribution and
the model prefers relatively few states. We train with
interpolations which generally result in around 40
states.
Once the interpolation constants are set, the
model can be trained by Gibbs sampling. We also
do inference over the remaining hyperparameters of
the model by Metropolis sampling from uninforma-
tive priors. Convergence is generally very rapid; we
obtain good results after about 10 iterations. Unlike
Barzilay and Lee (2004), we do not initialize with
an informative starting distribution.
When finding the probability of a test document,
we do not do inference over the full Bayesian model,
because the number of states, and the probability of
different transitions, can change with every new ob-
servation, making dynamic programming impossi-
ble. Beal et al (2001) proposes an inference algo-
rithm based on particle filters, but we feel that in
this case, the effects are relatively minor, so we ap-
proximate by treating the model as a standard HMM,
using a fixed transition function based only on the
training data. This allows us to use the conventional
Viterbi algorithm. The backoff rates we choose at
training time are typically too small for optimal in-
ference in the ordering task. Before doing tests, we
set them to higher values (determined to optimize
ordering performance on held-out data) so that our
emission distributions are properly smoothed.
5 Experiments
Our experiments use the popular AIRPLANE cor-
pus, a collection of documents describing airplane
crashes taken from the database of the National
440
Transportation Safety Board, used in (Barzilay and
Lee, 2004; Barzilay and Lapata, 2005; Soricut and
Marcu, 2006). We use the standard division of
the corpus into 100 training and 100 test docu-
ments; for development purposes we did 10-fold
cross-validation on the training data. The AIRPLANE
documents have some advantages for coherence re-
search: they are short (11.5 sentences on average)
and quite formulaic, which makes it easy to find lex-
ical and structural patterns. On the other hand, they
do have some oddities. 46 of the training documents
begin with a standard preamble: ?This is prelimi-
nary information, subject to change, and may con-
tain errors. Any errors in this report will be corrected
when the final report has been completed,? which
essentially gives coherence models the first two sen-
tences for free. Others, however, begin abruptly with
no introductory material whatsoever, and sometimes
without even providing references for their definite
noun phrases; one document begins: ?At V1, the
DC-10-30?s number 1 engine, a General Electric
CF6-50C2, experienced a casing breach when the
2nd-stage low pressure turbine (LPT) anti-rotation
nozzle locks failed.? Even humans might have trou-
ble identifying this sentence as the beginning of a
document.
5.1 Sentence Ordering
In the sentence ordering task, (Lapata, 2003; Barzi-
lay and Lee, 2004; Barzilay and Lapata, 2005; Sori-
cut and Marcu, 2006), we view a document as an
unordered bag of sentences and try to find the or-
dering of the sentences which maximizes coherence
according to our model. This type of ordering pro-
cess has applications in natural language generation
and multi-document summarization. Unfortunately,
finding the optimal ordering according to a prob-
abilistic model with local features is NP-complete
and non-approximable (Althaus et al, 2004). More-
over, since our model is not Markovian, the relax-
ation used as a heuristic for A? search by Soricut
and Marcu (2006) is ineffective. We therefore use
simulated annealing to find a high-probability order-
ing, starting from a random permutation of the sen-
tences. Our search system has few Estimated Search
Errors as defined by Soricut and Marcu (2006); it
rarely proposes an ordering which has lower proba-
? Discr. (%)
(Barzilay and Lapata, 2005) - 90
(Barzilay and Lee, 2004) .44 745
(Soricut and Marcu, 2006) .50 -6
Topic-based (relaxed) .50 94
Table 1: Results for AIRPLANE test data.
bility than the original ordering4 .
To evaluate the quality of the orderings we predict
as optimal, we use Kendall?s ? , a measurement of
the number of pairwise swaps needed to transform
our proposed ordering into the original document,
normalized to lie between ?1 (reverse order) and 1
(original order). Lapata (2006) shows that it corre-
sponds well with human judgements of coherence
and reading times. A slight problem with ? is that
it does not always distinguish between proposed or-
derings of a document which disrupt local relation-
ships at random, and orderings in which paragraph-
like units move as a whole. In longer documents, it
may be worth taking this problem into account when
selecting a metric; however, the documents in the
AIRPLANE corpus are mostly short and have little
paragraph structure, so ? is an effective metric.
5.2 Discrimination
Our second task is the discriminative test used by
(Barzilay and Lapata, 2005). In this task we gen-
erate random permutations of a test document, and
measure how often the probability of a permutation
is higher than that of the original document. This
task bears some resemblance to the task of discrim-
inating coherent from incoherent essays in (Milt-
sakaki and Kukich, 2004), and is also equivalent
in the limit to the ranking metric of (Barzilay and
Lee, 2004), which we cannot calculate because our
model does not produce k-best output. As opposed
to the ordering task, which tries to measure how
close the model?s preferred orderings are to the orig-
inal, this measurement assesses how many orderings
the model prefers. We use 20 random permutations
per document, for 2000 total tests.
441
? Discr. (%)
Naive Entity Grid .17 81
Relaxed Entity Grid .02 87
Topic-based (naive) .39 85
Topic-based (relaxed) .54 96
Table 2: Results for 10-fold cross-validation on AIR-
PLANE training data.
6 Results
Since the ordering task requires a model to propose
the complete structure for a set of sentences, it is
very dependent on global features. To perform ad-
equately, a model must be able to locate the begin-
ning and end of the document, and place intermedi-
ate sentences relative to these two points. Without
any way of doing this, our relaxed entity grid model
has ? of approximately 0, meaning its optimal or-
derings are essentially uncorrelated with the correct
orderings7 . The HMM content model of (Barzilay
and Lee, 2004), which does have global structure,
performs much better on ordering, at ? of .44. How-
ever, local features can help substantially for this
task, since models which use them are better at plac-
ing related sentences next to one another. Using both
sets of features, our topic-based model achieves state
of the art performance (? = .5) on the ordering task,
comparable with the mixture model of (Soricut and
Marcu, 2006).
The need for good local coherence features is es-
pecially clear from the results on the discrimination
task (table 1). Permuting a document may leave ob-
vious ?signposts? like the introduction and conclu-
sion in place, but it almost always splits up many
pairs of neighboring sentences, reducing local co-
herence. (Barzilay and Lee, 2004), which lacks lo-
cal features, does quite poorly on this task (74%),
while our model performs extremely well (94%).
It is also clear from the results that our relaxed en-
tity grid model (87%) improves substantially on the
generative naive entity grid (81%). When used on
40 times on test data, 3 times in cross-validation.
5Calculated on our test permutations using the code at
http://people.csail.mit.edu/regina/code.html.
6Soricut and Marcu (2006) do not report results on this task,
except to say that their implementation of the entity grid per-
forms comparably to (Barzilay and Lapata, 2005).
7Barzilay and Lapata (2005) do not report ? scores.
its own, it performs much better on the discrimina-
tion task, which is the one for which it was designed.
(The naive entity grid has a higher ? score, .17, es-
sentially by accident. It slightly prefers to generate
infrequent nouns from the start context rather than
the context - -, which happens to produce the correct
placement for the ?preliminary information? pream-
ble.) When used as the emission model for known
entities in our topic-based system, the relaxed en-
tity grid shows its improved performance even more
strongly (table 2); its results are about 10% higher
than the naive version under both metrics.
Our combined model uses only entity-grid fea-
tures and unigram language models,a strict subset of
the feature set of (Soricut and Marcu, 2006). Their
mixture includes an entity grid model and a version
of the HMM of (Barzilay and Lee, 2004), which
uses n-gram language modeling. It also uses a model
of lexical generation based on the IBM-1 model for
machine translation, which produces all words in the
document conditioned on words from previous sen-
tences. In contrast, we generate only entities con-
ditioned on words from previous sentences; other
words are conditionally independent given the topic
variable. It seems likely therefore that using our
model as a component of a mixture might improve
on the state of the art result.
7 Future Work
Ordering in the AIRPLANE corpus and similar con-
strained sets of short documents is by no means a
solved problem, but the results so far show a good
deal of promise. Unfortunately, in longer and less
formulaic corpora, the models, inference algorithms
and even evaluation metrics used thus far may prove
extremely difficult to scale up. Domains with more
natural writing styles will make lexical prediction a
much more difficult problem. On the other hand,
the wider variety of grammatical constructions used
may motivate more complex syntactic features, for
instance as proposed by (Siddharthan et al, 2004) in
sentence clustering.
Finding optimal orderings is a difficult task even
for short documents, and will become exponen-
tially more challenging in longer ones. For multi-
paragraph documents, it is probably impractical to
use full-scale coherence models to find optimal or-
442
derings directly. A better approach may be a coarse-
to-fine or hierarchical system which cuts up longer
documents into more manageable chunks that can be
ordered as a unit.
Multi-paragraph documents also pose a problem
for the ? metric itself. In documents with clear the-
matic divisions between their different sections, a
good ordering metric should treat transposed para-
graphs differently than transposed sentences.
8 Acknowledgements
We are extremely grateful to Regina Barzilay, for her
code, data and extensive support, Mirella Lapata for
code and advice, and the BLLIP group, especially
Tom Griffiths, Sharon Goldwater and Mark Johnson,
for comments and criticism. We were supported by
DARPA GALE contract HR0011-06-2-0001 and the
Karen T. Romer Foundation. Finally we thank three
anonymous reviewers for their comments.
References
Ernst Althaus, Nikiforos Karamanis, and Alexander
Koller. 2004. Computing locally coherent discourses.
In Proceedings of the 42nd ACL, Barcelona.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The infinite Hidden Markov
Model. In NIPS, pages 577?584.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173?180.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In HLT-NAACL, pages 185?
192.
Roger Kibble and Richard Power. 2004. Optimising ref-
erential coherence in text generation. Computational
Linguistics, 30(4):401?416.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall?s tau. Computational Linguis-
tics, 32(4):1?14.
E. Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25?55.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2004. Syntactic simplification for improving
content selection in multi-document summarization.
In COLING04, pages 896?902.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
Y.W. Teh. 2006. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, Na-
tional University of Singapore.
443
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164?172,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Structured Generative Models for Unsupervised Named-Entity Clustering
Micha Elsner, Eugene Charniak and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec,mj}@cs.brown.edu
Abstract
We describe a generative model for clustering
named entities which also models named en-
tity internal structure, clustering related words
by role. The model is entirely unsupervised;
it uses features from the named entity itself
and its syntactic context, and coreference in-
formation from an unsupervised pronoun re-
solver. The model scores 86% on the MUC-7
named-entity dataset. To our knowledge, this
is the best reported score for a fully unsuper-
vised model, and the best score for a genera-
tive model.
1 Introduction
Named entity clustering is a classic task in NLP, and
one for which both supervised and semi-supervised
systems have excellent performance (Mikheev et al,
1998; Chinchor, 1998). In this paper, we describe a
fully unsupervised system (using no ?seed rules? or
initial heuristics); to our knowledge this is the best
such system reported on the MUC-7 dataset. In ad-
dition, the model clusters the words which appear
in named entities, discovering groups of words with
similar roles such as first names and types of orga-
nization. Finally, the model defines a notion of con-
sistency between different references to the same en-
tity; this component of the model yields a significant
increase in performance.
The main motivation for our system is the re-
cent success of unsupervised generative models for
coreference resolution. The model of Haghighi
and Klein (2007) incorporated a latent variable for
named entity class. They report a named entity score
of 61.2 percent, well above the baseline of 46.4, but
still far behind existing named-entity systems.
We suspect that better models for named entities
could aid in the coreference task. The easiest way to
incorporate a better model is simply to run a super-
vised or semi-supervised system as a preprocess. To
perform joint inference, however, requires an unsu-
pervised generative model for named entities. As far
as we know, this work is the best such model.
Named entities also pose another problem with
the Haghighi and Klein (2007) coreference model;
since it models only the heads of NPs, it will fail to
resolve some references to named entities: (?Ford
Motor Co.?, ?Ford?), while erroneously merging
others: (?Ford Motor Co.?, ?Lockheed Martin Co.?).
Ng (2008) showed that better features for match-
ing named entities? exact string match and an ?alias
detector? looking for acronyms, abbreviations and
name variants? improve the model?s performance
substantially. Yet building an alias detector is non-
trivial (Uryupina, 2004). English speakers know that
?President Clinton? is the same person as ?Bill Clin-
ton? , not ?President Bush?. But this cannot be im-
plemented by simple substring matching. It requires
some concept of the role of each word in the string.
Our model attempts to learn this role information by
clustering the words within named entities.
2 Related Work
Supervised named entity recognition now performs
almost as well as human annotation in English
(Chinchor, 1998) and has excellent performance on
other languages (Tjong Kim Sang and De Meul-
der, 2003). For a survey of the state of the art,
164
see Nadeau and Sekine (2007). Of the features
we explore here, all but the pronoun information
were introduced in supervised work. Supervised ap-
proaches such as Black et al (1998) have used clus-
tering to group together different nominals referring
to the same entity in ways similar to the ?consis-
tency? approach outlined below in section 3.2.
Semi-supervised approaches have also achieved
notable success on the task. Co-training (Riloff and
Jones, 1999; Collins and Singer, 1999) begins with
a small set of labeling heuristics and gradually adds
examples to the training data. Various co-training
approaches presented in Collins and Singer (1999)
all score about 91% on a dataset of named entities;
the inital labels were assigned using 7 hand-written
seed rules. However, Collins and Singer (1999)
show that a mixture-of-naive-Bayes generative clus-
tering model (which they call an EM model), initial-
ized with the same seed rules, performs much more
poorly at 83%.
Much later work (Evans, 2003; Etzioni et al,
2005; Cucerzan, 2007; Pasca, 2004) relies on the
use of extremely large corpora which allow very
precise, but sparse features. For instance Etzioni
et al (2005) and Pasca (2004) use web queries to
count occurrences of ?cities such as X? and simi-
lar phrases. Although our research makes use of a
fairly large amount of data, our method is designed
to make better use of relatively common contextual
features, rather than searching for high-quality se-
mantic features elsewhere.
Models of the internal structure of names have
been used for cross-document coreference (Li et al,
2004; Bhattacharya and Getoor, 2006) and a goal in
their own right (Charniak, 2001). Li et al (2004)
take named entity classes as a given, and develops
both generative and discriminative models to detect
coreference between members of each class. Their
generative model designates a particular mention of
a name as a ?representative? and generates all other
mentions from it according to an editing process.
Bhattacharya and Getoor (2006) operates only on
authors of scientific papers. Their model accounts
for a wider variety of name variants than ours, in-
cluding misspellings and initials. In addition, they
confirm our intuition that Gibbs sampling for infer-
ence has insufficient mobility; rather than using a
heuristic algorithm as we do (see section 3.5), they
use a data-driven block sampler. Charniak (2001)
uses a Markov chain to generate 6 different com-
ponents of people?s names, again assuming that the
class of personal names can be pre-distinguished us-
ing a name list. He infers coreference relationships
between similar names appearing in the same docu-
ment, using the same notion of consistency between
names as our model. As with our model, the clusters
found are relatively good, although with some mis-
takes even on frequent items (for example, ?John? is
sometimes treated as a descriptor like ?Secretary?).
3 System Description
Like Collins and Singer (1999), we assume that the
named entities have already been correctly extracted
from the text, and our task is merely to label them.
We assume that all entities fit into one of the three
MUC-7 categories, LOC (locations), ORG (organi-
zations), and PER (people). This is an oversimplifi-
cation; Collins and Singer (1999) show that about
12% of examples do not fit into these categories.
However, while using the MUC-7 data, we have no
way to evaluate on such examples.
As a framework for our models, we adopt adap-
tor grammars (Johnson et al, 2007), a framework
for non-parametric Bayesian inference over context-
free grammars. Although our system does not re-
quire the full expressive power of PCFGs, the adap-
tor grammar framework allows for easy develop-
ment of structured priors, and supplies a flexible
generic inference algorithm. An adaptor grammar is
a hierarchical Pitman-Yor process (Pitman and Yor,
1997). The grammar has two parts: a base PCFG
and a set of adapted nonterminals. Each adapted
nonterminal is a Pitman-Yor process which expands
either to a previously used subtree or to a sample
from the base PCFG. The end result is a posterior
distribution over PCFGs and over parse trees for
each example in our dataset.
Each of our models is an adaptor grammar based
on a particular base PCFG where the top nonter-
minal of each parse tree represents a named entity
class.
3.1 Core NP Model
We begin our analysis by reducing each named-
entity reference to the contiguous substring of
165
ROOT ?NE 0|NE 1|NE 2
NE 0 ?(NE 00)(NE 10)(NE 20)(NE 30)(NE 40)
?NE 00 ?Words
?Words ?Word (Words)
Word ?Bill . . .
Figure 1: Part of the grammar for core phrases. (Paren-
theses) mark optional nonterminals. *Starred nontermi-
nals are adapted.
proper nouns which surrounds its head, which we
call the core (Figure 1). To analyze the core, we use
a grammar with three main symbols (NEx), one for
each named entity class x. Each class has an asso-
ciated set of lexical symbols, which occur in a strict
order (NE ix is the ith symbol for class x). We can
think of the NE i as the semantic parts of a proper
name; for people, NE 0PER might generate titles and
NE 1PER first names. Each NE i is adapted, and can
expand to any string of words; the ability to gen-
erate multiple words from a single symbol is use-
ful both because it can learn to group collocations
like ?New York? and because it allows the system to
handle entities longer than four words. However, we
set the prior on multi-word expansions very low, to
avoid degenerate solutions where most phrases are
analyzed with a single symbol. The system learns
a separate probability for each ordered subset of the
NE i (for instance the rule NE 0 ? NE 00 NE 20 NE 40),
so that it can represent constraints on possible refer-
ences; for instance, a last name can occur on its own,
but not a title.
3.2 Consistency Model
This system captures some of our intuitions about
core phrases, but not all: our representation for ?Bill
Clinton? does not share any information with ?Presi-
dent Bill Clinton? except the named-entity class. To
remedy this, we introduce a set of ?entity? nonter-
minals Ek, which enforce a weak notion of consis-
tency. We follow Charniak (2001) in assuming that
two names are consistent (can be references to the
same entity) if they do not have different expansions
for any lexical symbol. In other words, a particu-
lar entity EPER,Clinton has a title E0PER,Clinton =
ROOT ?NE 0|NE 1|NE 2
NE 0 ?E00|E01 . . . E0k
E00 ?(E000)(E100)(E200)(E300)(E400)
? ? E000 ?NE 00
?NE 00 ?Words . . .
Figure 2: Part of the consistency-enforcing grammar for
core phrases. There are an infinite number of entities
Exk, all with their own lexical symbols. Each lexical
symbol Eixk expands to a single NE ix.
?President?, a first name E1PER,Clinton = ?Bill? etc.
These are generated from the class-specific distribu-
tions, for instance E0PER,Clinton ? E0PER, which
we intend to be a distribution over titles in general.
The resulting grammar is shown in Figure 2; the
prior parameters for the entity-specific symbols Eixk
are fixed so that, with overwhelming probability,
only one expansion occurs. We can represent any
fixed number of entities Ek with a standard adap-
tor grammar, but since we do not know the correct
number, we must extend the adaptor model slightly
to allow for an unbounded number. We generate the
Ek from a Chinese Restaurant process prior. (Gen-
eral grammars with infinite numbers of nonterminals
were studied by (Liang et al, 2007b)).
3.3 Modifiers, Prepositions and Pronouns
Next, we introduce two types of context information
derived from Collins and Singer (1999): nominal
modifiers and prepositional information. A nominal
modifier is either the head of an appositive phrase
(?Maury Cooper, a vice president?) or a non-proper
prenominal (?spokesman John Smith?)1. If the en-
tity is the complement of a preposition, we extract
the preposition and the head of the governing NP (?a
federally funded sewage plant in Georgia?). These
are added to the grammar at the named-entity class
level (separated from the core by a special punctua-
tion symbol).
Finally, we add information about pronouns and
wh-complementizers (Figure 3). Our pronoun infor-
mation is derived from an unsupervised coreference
algorithm which does not use named entity informa-
1We stem modifiers with the Porter stemmer.
166
ROOT ?Modifiers0 # NE 0 #
Prepositions0 # Pronouns0 #
. . .
Pronouns0 ?Pronoun0 Pronouns0
Pronouns0 ?
Pronoun0 ?pers|loc|org |any
pers ?i |he|she|who|me . . .
loc ?where|which|it |its
org ?which|it |they |we . . .
Figure 3: A fragment of the full grammar. The symbol
# represents punctuation between different feature types.
The prior for class 0 is concentrated around personal pro-
nouns, although other types are possible.
tion (Charniak and Elsner, 2009). This algorithm
uses EM to learn a generative model with syntactic,
number and gender parameters. Like Haghighi and
Klein (2007), we give our model information about
the basic types of pronouns in English. By setting
up the base grammar so that each named-entity class
prefers to associate to a single type of pronoun, we
can also determine the correspondence between our
named-entity symbols and the actual named-entity
labels? for the models without pronoun information,
this matching is arbitrary and must be inferred dur-
ing the evaluation process.
3.4 Data Preparation
To prepare data for clustering with our system, we
first parse it with the parser of Charniak and Johnson
(2005). We then annotate pronouns with Charniak
and Elsner (2009). For the evaluation set, we use the
named entity data from MUC-7. Here, we extract
all strings in <ne> tags and determine their cores,
plus any relevant modifiers, governing prepositions
and pronouns, by examining the parse trees. In addi-
tion, we supply the system with additional data from
the North American News Corpus (NANC). Here
we extract all NPs headed by proper nouns.
We then process our data by merging all exam-
ples with the same core; some merged examples
from our dataset are shown in Figure 4. When two
examples are merged, we concatenate their lists of
attack airlift airlift rescu # wing # of-commander
of-command with-run # #
# air-india # # #
# abels # # it #
# gaudreau # # they he #
# priddy # # he #
spokesman bird bird bird director bird ford clin-
ton director bird # johnson # before-hearing
to-happened of-cartoon on-pressure under-medicare
to-according to-allied with-stuck of-government of-
photographs of-daughter of-photo for-embarrassing
under-instituted about-allegations for-worked
before-hearing to-secretary than-proposition of-
typical # he he his he my himself his he he he he i
he his his i i i he his #
Figure 4: Some merged examples from an input file. (#
separates different feature types.)
modifiers, prepositions and pronouns (capping the
length of each list at 20 to keep inference tractable).
For instance, ?air-india? has no features outside the
core, while ?wing? has some nominals (?attack?
&c.) and some prepositions (?commander-of? &c.).
This merging is useful because it allows us to do in-
ference based on types rather than tokens (Goldwa-
ter et al, 2006). It is well known that, to interpo-
late between types and tokens, Hierarchical Dirich-
let Processes (including adaptor grammars) require
a deeper hierarchy, which slows down inference and
reduces the mobility of sampling schemes. By merg-
ing examples, we avoid using this more complicated
model. Each merged example also represents many
examples from the training data, so we can summa-
rize features (such as modifiers) observed through-
out a large input corpus while keeping the size of
our input file small.
To create an input file, we first add all the MUC-
7 examples. We then draw additional examples
from NANC, ranking them by how many features
they have, until we reach a specified number (larger
datasets take longer, but without enough data, results
tend to be poor).
3.5 Inference
Our implementation of adaptor grammars is a mod-
ified version of the Pitman-Yor adaptor grammar
167
sampler2, altered to deal with the infinite number of
entities. It carries out inference using a Metropolis-
within-Gibbs algorithm (Johnson et al, 2007), in
which it repeatedly parses each input line using the
CYK algorithm, samples a parse, and proposes this
as the new tree.
To do Gibbs sampling for our consistency-
enforcing model, we would need to sample a parse
for an example from the posterior over every pos-
sible entity. However, since there are thousands of
entities (the number grows roughly linearly with the
number of merged examples in the data file), this is
not tractable. Instead, we perform a restricted Gibbs
sampling search, where we enumerate the posterior
only for entities which share a word in their core
with the example in question. In fact, if the shared
word is very common (occuring in more than .001 of
examples), we compute the posterior for that entity
only .05 of the time3. These restrictions mean that
we do not compute the exact posterior. In particular,
the actual model allows entities to contain examples
with no words in common, but our search procedure
does not explore these solutions.
For our model, inference with the Gibbs algo-
rithm seems to lack mobility, sometimes falling into
very poor local minima from which it does not seem
to escape. This is because, if there are several ref-
erences to the same named entity with slightly dif-
ferent core phrases, once they are all assigned to
the wrong class, it requires a low-probability se-
ries of individual Gibbs moves to pull them out.
Similarly, the consistency-enforcing model gener-
ally does not fully cluster references to common en-
tities; there are usually several ?Bill Clinton? clus-
ters which it would be best to combine, but the se-
quence of moves that does so is too improbable. The
data-merging process described above is one attempt
to improve mobility by reducing the number of du-
plicate examples. In addition, we found that it was a
better use of CPU time to run multiple samplers with
different initialization than to perform many itera-
tions. In the experiments below, we use 20 chains,
initializing with 50 iterations without using consis-
tency, then 50 more using the consistency model,
and evaluate the last sample from each. We discard
2Available at http://www.cog.brown.edu/ mj/Software.htm
3We ignore the corresponding Hastings correction, as in
practice it leads to too many rejections.
the 10 samples with worst log-likelihood and report
the average score for the other 10.
3.6 Parameters
In addition to the base PCFG itself, the system re-
quires a few hyperparameter settings: Dirichlet pri-
ors for the rule weights of rules in the base PCFG.
Pitman-Yor parameters for the adapted nonterminals
are sampled from vague priors using a slice sam-
pler (Neal, 2003). The prior over core words was
set to the uniform distribution (Dirichlet 1.0) and the
prior for all modifiers, prepositions and pronouns to
a sparse value of .01. Beyond setting these param-
eters to a priori reasonable values, we did not opti-
mize them. To encourage the system to learn that
some lexical symbols were more common than oth-
ers, we set a sparse prior over expansions to sym-
bols4. There are two really important hyperparame-
ters: an extremely biased prior on class-to-pronoun-
type probabilities (1000 for the desired class, .0001
for everything else), and a prior of .0001 for the
Word ?Word Words rule to discourage symbols
expanding to multiword strings.
4 Experiments
We performed experiments on the named entity
dataset from MUC-7 (Chinchor, 1998), using the
training set as development data and the formal test
set as test data. The development set has 4936
named entities, of which 1575 (31.9%) are locations,
2096 (42.5%) are organizations and 1265 (25.6%)
people. The test set has 4069 named entities, 1321
(32.5%) locations, 1862 (45.8%) organizations and
876 (21.5%) people5. We use a baseline which
gives all named entities the same label; this label is
mapped to ?organization?.
In most of our experiments, we use an input file of
40000 lines. For dev experiments, the labeled data
contributes 1585 merged examples; for test experi-
ments, only 1320. The remaining lines are derived
4Expansions that used only the middle three symbols
NE1,2,3x got a prior of .005, expansions whose outermost sym-
bol was NE0,4x got .0025, and so forth. This is not so impor-
tant for our final system, which has only 5 symbols, but was
designed during development to handle systems with up to 10
symbols.
510 entities are labeled location|organization; since this
fraction of the dataset is insignificant we score them as wrong.
168
Model Accuracy
Baseline (All Org) 42.5
Core NPs (no consistency) 45.5
Core NPs (consistency) 48.5
Context Features 83.3
Pronouns 87.1
Table 1: Accuracy of various models on development
data.
Model Accuracy
Baseline (All Org) 45.8
Pronouns 86.0
Table 2: Accuracy of the final model on test data.
using the process described in section 3.4 from 5
million words of NANC.
To evaluate our results, we map our three induced
labels to their corresponding gold label, then count
the overlap; as stated, this mapping is predictably
encoded in the prior when we use the pronoun fea-
tures. Our experimental results are shown in Table
1. All models perform above baseline, and all fea-
tures contribute significantly to the final result. Test
results for our final model are shown in Table 2.
A confusion matrix for our highest-likelihood test
solution is shown as Figure 5. The highest confusion
class is ?organization?, which is confused most often
with ?location? but also with ?person?. ?location? is
likewise confused with ?organization?. ?person? is
the easiest class to identify? we believe this explains
the slight decline in performance from dev to test,
since dev has proportionally more people.
Our mapping from grammar symbols to words ap-
pears in Table 3; the learned prepositional and mod-
ifier information is in Table 4. Overall the results
are good, but not perfect; for instance, the Pers
states are mostly interpretable as a sequence of ti-
tle - first name - middle name or initial - last name -
loc org per
LOC 1187 97 37
ORG 223 1517 122
PER 36 20 820
Figure 5: Confusion matrix for highest-likelihood test
run. Gold labels in CAPS, induced labels italicized. Or-
ganizations are most frequently confused.
last name or post-title (similar to (Charniak, 2001)).
The organization symbols tend to put nationalities
and other modifiers first, and end with institutional
types like ?inc.? or ?center?, although there is a sim-
ilar (but smaller) cluster of types at Org2, suggest-
ing the system has incorrectly found two analyses
for these names. Location symbols seem to put en-
tities with a single, non-analyzable name into Loc2,
and use symbols 0, 1 and 3 for compound names.
Loc4 has been recruited for time expressions, since
our NANC dataset includes many of these, but we
failed to account for them in the model. Since
they appear in a single class here, we are optimistic
that they could be clustered separately if another
class and some appropriate features were added to
the prior. Some errors do appear (?supreme court?
and ?house? as locations, ?minister? and ?chairman?
as middle names, ?newt gingrich? as a multiword
phrase). The table also reveals an unforeseen issue
with the parser: it tends to analyze the dateline be-
ginning a news story along with the following NP
(?WASHINGTON Bill Clinton said...?). Thus com-
mon datelines (?washington?, ?new york? and ?los
angeles?) appear in state 0 for each class.
5 Discussion
As stated above, we aim to build an unsupervised
generative model for named entity clustering, since
such a model could be integrated with unsupervised
coreference models like Haghighi and Klein (2007)
for joint inference. To our knowledge, the closest
existing system to such a model is the EM mix-
ture model used as a baseline in Collins and Singer
(1999). Our system improves on this EM system
in several ways. While they initialize with minimal
supervision in the form of 7 seed heuristics, ours is
fully unsupervised. Their results cover only exam-
ples which have a prepositional or modifier feature;
we adopt these features from their work, but label
all entities in the predefined test set, including those
that appear without these features. Finally, as dis-
cussed, we find the ?person? category to be the eas-
iest to label. 33% of the test items in Collins and
Singer (1999) were people, as opposed to 21% of
ours. However, even without the pronoun features,
that is, using the same feature set, our system scores
equivalently to the EM model, at 83% (this score is
169
Pers0 Pers1 Pers2 Pers3 Pers4
rep. john (767) minister brown jr.
sen. (256) robert (495) j. smith (97) a
washington david john (242) b smith (111)
dr. michael l. johnson iii
los angeles james chairman newt gingrich williams
senate president e. king wilson
house richard m. miller brown
new york william (317) william (173) kennedy clinton
president sen. (236) robert (155) martin simpson
republican george r. davis b
Org0 Org1 Org2 Org3 Org4
american (137) national university research association
washington american (182) inc. (166) medical center
washington the new york corp. (156) news inc. (257)
national international (136) college health corp. (252)
first public institute (87) services co.
los angeles united group communications committee
new house hospital development institute
royal federal museum policy council
british home press affairs fund
california world international (61) defense act
Loc0 Loc1 Loc2 Loc3 Loc4
washington (92) the texas county monday
los angeles st. new york city thursday
south new washington (22) beach river (57)
north national (69) united states valley tuesday
old east (65) baltimore island wednesday
grand mount california river (71) hotel
black fort capitol park friday
west (22) west (56) christmas bay hall
east (21) lake bosnia house center
haiti great san juan supreme court building
Table 3: 10 most common words for each grammar symbol. Words which appear in multiple places have observed
counts indicated in parentheses.
170
Pers-gov Pers-mod Org-gov Org-mod Loc-gov Loc-mod
according-to (1044) director president-of $ university-of calif.
played-by spokesman chairman-of giant city-of newspap[er]
directed-by leader director-of opposit[e] from-to state
led-by presid[ent] according-to (786) group town-of downtown
meeting-with attorney professor-at pp state-of n.y.
from-to candid[ate] head-of compan[y] center-in warrant
met-with lawyer department-of journal out-of va.
letter-to chairman member-of firm is-in fla.
secretary-of counsel members-of state house-of p.m.
known-as actor spokesman-for agenc[y] known-as itself
Table 4: 10 most common prepositional and modifier features for each named entity class. Modifiers were Porter-
stemmed; for clarity a reconstructed stem is shown in brackets.
on dev, 25% people). When the pronoun features are
added, our system?s performance increases to 86%,
significantly better than the EM system.
One motivation for our use of a structured model
which defined a notion of consistency between en-
tities was that it might allow the construction of
an unsupervised alias detector. According to the
model, two entities are consistent if they are in the
same class, and do not have conflicting assignments
of words to lexical symbols. Results here are at
best equivocal. The model is reasonable at pass-
ing basic tests? ?Dr. Seuss? is not consistent with
?Dr. Strangelove?, ?Dr. Quinn? etc, despite their
shared title, because the model identifies the sec-
ond element of each as a last name. Also correctly,
?Dr. William F. Gibson? is judged consistent with
?Dr. Gibson? and ?Gibson? despite the missing el-
ements. But mistakes are commonplace. In the
?Gibson? case, the string ?William F.? is misana-
lyzed as a multiword string, making the name in-
consistent with ?William Gibson?; this is probably
the result of a search error, which, as we explained,
Gibbs sampling is unlikely to correct. In other cases,
the system clusters a family group together under
a single ?entity? nonterminal by forcing their first
names into inappropriate states, for instance assign-
ing Pers1 Bruce, Pers2 Ellen, Pers3 Jarvis, where
Pers2 (usually a middle name) actually contains the
first name of a different individual. To improve this
aspect of our system, we might incorporate name-
specific features into the prior, such as abbreviations
and the concept of a family name. The most critical
improvement, however, would be integration with a
generative coreference system, since the document
context probably provides hints about which entities
are and are not coreferent.
The other key issue with our system is inference.
Currently we are extremely vulnerable to falling into
local minima, since the complex structure of the
model can easily lock a small group of examples
into a poor configuration. (The ?William F. Gibson?
case above seems to be one of these.) In addition to
the block sampler used by Bhattacharya and Getoor
(2006), we are investigating general-purpose split-
merge samplers (Jain and Neal, 2000) and the per-
mutation sampler (Liang et al, 2007a). One inter-
esting question is how well these samplers perform
when faced with thousands of clusters (entities).
Despite these issues, we clearly show that it is
possible to build a good model of named entity class
while retaining compatibility with generative sys-
tems and without supervision. In addition, we do a
reasonable job learning the latent structure of names
in each named entity class. Our system improves
over the latent named-entity tagging in Haghighi
and Klein (2007), from 61% to 87%. This sug-
gests that it should indeed be possible to improve
on their coreference results without using a super-
vised named-entity model. How much improvement
is possible in practice, and whether joint inference
can also improve named-entity performance, remain
interesting questions for future work.
Acknowledgements
We thank three reviewers for their comments, and
NSF for support via grants 0544127 and 0631667.
171
References
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
dirichlet model for unsupervised entity resolution. In
The SIAM International Conference on Data Mining
(SIAM-SDM), Bethesda, MD, USA.
William J. Black, Fabio Rinaldi, and David Mowatt.
1998. Facile: Description of the ne system used for
muc-7. In In Proceedings of the 7th Message Under-
standing Conference.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-09),
Athens, Greece.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173?180.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In NAACL-01.
Nancy A. Chinchor. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-7)
named entity task definition. In Proceedings of the
Seventh Message Understanding Conference (MUC-
7), page 21 pages, Fairfax, VA, April. version 3.5,
http://www.itl.nist.gov/iaui/894.02/related projects/muc/.
Michael Collins and Yorav Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP 99.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
EMNLP-CoNLL, pages 708?716, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana
maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.
Weld, and Er Yates. 2005. Unsupervised named-
entity extraction from the web: An experimental study.
Artificial Intelligence, 165:91?134.
Richard Evans. 2003. A framework for named en-
tity recognition in the open domain. In Proceedings
of Recent Advances in Natural Language Processing
(RANLP-2003), pages 137 ? 144, Borovetz, Bulgaria,
September.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006. Interpolating between types and tokens by es-
timating power-law generators. In Advances in Neural
Information Processing Systems (NIPS) 18.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
848?855. Association for Computational Linguistics.
Sonia Jain and Radford M. Neal. 2000. A split-merge
markov chain monte carlo procedure for the dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13:158?182.
Mark Johnson, Tom L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings of NAACL 2007.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In AAAI, pages 419?424.
Percy Liang, Michael I. Jordan, and Ben Taskar. 2007a.
A permutation-augmented sampler for DP mixture
models. In Proceedings of ICML, pages 545?552,
New York, NY, USA. ACM.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007b. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of EMNLP-CoNLL, pages
688?697, Prague, Czech Republic, June. Association
for Computational Linguistics.
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the LTG System Used for MUC-7. In Pro-
ceedings of the 7th Message Understanding Confer-
ence (MUC-7), Fairfax, Virginia.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Journal
of Linguisticae Investigationes, 30(1).
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 640?649, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In CIKM ?04: Proceedings of
the thirteenth ACM international conference on Infor-
mation and knowledge management, pages 137?145,
New York, NY, USA. ACM.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Ann. Probab., 25:855?900.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, pages 472?479.
AAAI.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003, pages 142?147. Edmonton, Canada.
Olga Uryupina. 2004. Evaluating name-matching for
coreference resolution. In Proceedings of LREC 04,
Lisbon.
172
Proceedings of ACL-08: HLT, pages 834?842,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
You talking to me? A Corpus and Algorithm for Conversation
Disentanglement
Micha Elsner and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@@cs.brown.edu
Abstract
When multiple conversations occur simultane-
ously, a listener must decide which conversa-
tion each utterance is part of in order to inter-
pret and respond to it appropriately. We refer
to this task as disentanglement. We present a
corpus of Internet Relay Chat (IRC) dialogue
in which the various conversations have been
manually disentangled, and evaluate annota-
tor reliability. This is, to our knowledge, the
first such corpus for internet chat. We pro-
pose a graph-theoretic model for disentangle-
ment, using discourse-based features which
have not been previously applied to this task.
The model?s predicted disentanglements are
highly correlated with manual annotations.
1 Motivation
Simultaneous conversations seem to arise naturally
in both informal social interactions and multi-party
typed chat. Aoki et al (2006)?s study of voice con-
versations among 8-10 people found an average of
1.76 conversations (floors) active at a time, and a
maximum of four. In our chat corpus, the average is
even higher, at 2.75. The typical conversation, there-
fore, is one which is interrupted? frequently.
Disentanglement is the clustering task of dividing
a transcript into a set of distinct conversations. It is
an essential prerequisite for any kind of higher-level
dialogue analysis: for instance, consider the multi-
party exchange in figure 1.
Contextually, it is clear that this corresponds to
two conversations, and Felicia?s1 response ?excel-
1Real user nicknames are replaced with randomly selected
(Chanel) Felicia: google works :)
(Gale) Arlie: you guys have never worked
in a factory before have you
(Gale) Arlie: there?s some real unethical
stuff that goes on
(Regine) hands Chanel a trophy
(Arlie) Gale, of course ... thats how they
make money
(Gale) and people lose limbs or get killed
(Felicia) excellent
Figure 1: Some (abridged) conversation from our corpus.
lent? is intended for Chanel and Regine. A straight-
forward reading of the transcript, however, might in-
terpret it as a response to Gale?s statement immedi-
ately preceding.
Humans are adept at disentanglement, even in
complicated environments like crowded cocktail
parties or chat rooms; in order to perform this task,
they must maintain a complex mental representation
of the ongoing discourse. Moreover, they adapt their
utterances to some degree to make the task easier
(O?Neill and Martin, 2003), which suggests that dis-
entanglement is in some sense a ?difficult? discourse
task.
Disentanglement has two practical applications.
One is the analysis of pre-recorded transcripts in
order to extract some kind of information, such as
question-answer pairs or summaries. These tasks
should probably take as as input each separate con-
versation, rather than the entire transcript. Another
identifiers for ethical reasons.
834
application is as part of a user-interface system for
active participants in the chat, in which users target a
conversation of interest which is then highlighted for
them. Aoki et al (2003) created such a system for
speech, which users generally preferred to a conven-
tional system? when the disentanglement worked!
Previous attempts to solve the problem (Aoki et
al., 2006; Aoki et al, 2003; Camtepe et al, 2005;
Acar et al, 2005) have several flaws. They clus-
ter speakers, not utterances, and so fail when speak-
ers move from one conversation to another. Their
features are mostly time gaps between one utterance
and another, without effective use of utterance con-
tent. Moreover, there is no framework for a prin-
cipled comparison of results: there are no reliable
annotation schemes, no standard corpora, and no
agreed-upon metrics.
We attempt to remedy these problems. We present
a new corpus of manually annotated chat room data
and evaluate annotator reliability. We give a set of
metrics describing structural similarity both locally
and globally. We propose a model which uses dis-
course structure and utterance contents in addition
to time gaps. It partitions a chat transcript into dis-
tinct conversations, and its output is highly corre-
lated with human annotations.
2 Related Work
Two threads of research are direct attempts to solve
the disentanglement problem: Aoki et al (2006),
Aoki et al (2003) for speech and Camtepe et al
(2005), Acar et al (2005) for chat. We discuss
their approaches below. However, we should em-
phasize that we cannot compare our results directly
with theirs, because none of these studies publish re-
sults on human-annotated data. Although Aoki et al
(2006) construct an annotated speech corpus, they
give no results for model performance, only user sat-
isfaction with their conversational system. Camtepe
et al (2005) and Acar et al (2005) do give perfor-
mance results, but only on synthetic data.
All of the previous approaches treat the problem
as one of clustering speakers, rather than utterances.
That is, they assume that during the window over
which the system operates, a particular speaker is
engaging in only one conversation. Camtepe et al
(2005) assume this is true throughout the entire tran-
script; real speakers, by contrast, often participate
in many conversations, sequentially or sometimes
even simultaneously. Aoki et al (2003) analyze each
thirty-second segment of the transcript separately.
This makes the single-conversation restriction some-
what less severe, but has the disadvantage of ignor-
ing all events which occur outside the segment.
Acar et al (2005) attempt to deal with this prob-
lem by using a fuzzy algorithm to cluster speakers;
this assigns each speaker a distribution over conver-
sations rather than a hard assignment. However, the
algorithm still deals with speakers rather than utter-
ances, and cannot determine which conversation any
particular utterance is part of.
Another problem with these approaches is the in-
formation used for clustering. Aoki et al (2003) and
Camtepe et al (2005) detect the arrival times of mes-
sages, and use them to construct an affinity graph be-
tween participants by detecting turn-taking behavior
among pairs of speakers. (Turn-taking is typified by
short pauses between utterances; speakers aim nei-
ther to interrupt nor leave long gaps.) Aoki et al
(2006) find that turn-taking on its own is inadequate.
They motivate a richer feature set, which, however,
does not yet appear to be implemented. Acar et
al. (2005) adds word repetition to their feature set.
However, their approach deals with all word repe-
titions on an equal basis, and so degrades quickly
in the presence of noise words (their term for words
which shared across conversations) to almost com-
plete failure when only 1/2 of the words are shared.
To motivate our own approach, we examine some
linguistic studies of discourse, especially analysis of
multi-party conversation. O?Neill and Martin (2003)
point out several ways in which multi-party text chat
differs from typical two-party conversation. One key
difference is the frequency with which participants
mention each others? names. They hypothesize that
mentioning is a strategy which participants use to
make disentanglement easier, compensating for the
lack of cues normally present in face-to-face dia-
logue. Mentions (such as Gale?s comments to Ar-
lie in figure 1) are very common in our corpus, oc-
curring in 36% of comments, and provide a useful
feature.
Another key difference is that participants may
create a new conversation (floor) at any time, a pro-
cess which Sacks et al (1974) calls schisming. Dur-
835
ing a schism, a new conversation is formed, not
necessarily because of a shift in the topic, but be-
cause certain participants have refocused their atten-
tion onto each other, and away from whoever held
the floor in the parent conversation.
Despite these differences, there are still strong
similarities between chat and other conversations
such as meetings. Our feature set incorporates infor-
mation which has proven useful in meeting segmen-
tation (Galley et al, 2003) and the task of detect-
ing addressees of a specific utterance in a meeting
(Jovanovic et al, 2006). These include word rep-
etitions, utterance topic, and cue words which can
indicate the bounds of a segment.
3 Dataset
Our dataset is recorded from the IRC (Internet Re-
lay Chat) channel ##LINUX at freenode.net, using
the freely-available gaim client. ##LINUX is an un-
official tech support line for the Linux operating sys-
tem, selected because it is one of the most active chat
rooms on freenode, leading to many simultaneous
conversations, and because its content is typically
inoffensive. Although it is notionally intended only
for tech support, it includes large amounts of social
chat as well, such as the conversation about factory
work in the example above (figure 1).
The entire dataset contains 52:18 hours of chat,
but we devote most of our attention to three anno-
tated sections: development (706 utterances; 2:06
hr) and test (800 utts.; 1:39 hr) plus a short pilot sec-
tion on which we tested our annotation system (359
utts.; 0:58 hr).
3.1 Annotation
Our annotators were seven university students with
at least some familiarity with the Linux OS, al-
though in some cases very slight. Annotation of the
test dataset typically took them about two hours. In
all, we produced six annotations of the test set2.
Our annotation scheme marks each utterance as
part of a single conversation. Annotators are in-
structed to create as many, or as few conversations as
they need to describe the data. Our instructions state
that a conversation can be between any number of
2One additional annotation was discarded because the anno-
tator misunderstood the task.
people, and that, ?We mean conversation in the typ-
ical sense: a discussion in which the participants are
all reacting and paying attention to one another. . . it
should be clear that the comments inside a conver-
sation fit together.? The annotation system itself is a
simple Java program with a graphical interface, in-
tended to appear somewhat similar to a typical chat
client. Each speaker?s name is displayed in a differ-
ent color, and the system displays the elapsed time
between comments, marking especially long pauses
in red. Annotators group sentences into conversa-
tions by clicking and dragging them onto each other.
3.2 Metrics
Before discussing the annotations themselves, we
will describe the metrics we use to compare differ-
ent annotations; these measure both how much our
annotators agree with each other, and how well our
model and various baselines perform. Comparing
clusterings with different numbers of clusters is a
non-trivial task, and metrics for agreement on su-
pervised classification, such as the ? statistic, are not
applicable.
To measure global similarity between annota-
tions, we use one-to-one accuracy. This measure de-
scribes how well we can extract whole conversations
intact, as required for summarization or information
extraction. To compute it, we pair up conversations
from the two annotations to maximize the total over-
lap3, then report the percentage of overlap found.
If we intend to monitor or participate in the con-
versation as it occurs, we will care more about lo-
cal judgements. The local agreement metric counts
agreements and disagreements within a context k.
We consider a particular utterance: the previous
k utterances are each in either the same or a dif-
ferent conversation. The lock score between two
annotators is their average agreement on these k
same/different judgements, averaged over all utter-
ances. For example, loc1 counts pairs of adjacent
utterances for which two annotations agree.
836
Mean Max Min
Conversations 81.33 128 50
Avg. Conv. Length 10.6 16.0 6.2
Avg. Conv. Density 2.75 2.92 2.53
Entropy 4.83 6.18 3.00
1-to-1 52.98 63.50 35.63
loc 3 81.09 86.53 74.75
M-to-1 (by entropy) 86.70 94.13 75.50
Table 1: Statistics on 6 annotations of 800 lines of chat
transcript. Inter-annotator agreement metrics (below the
line) are calculated between distinct pairs of annotations.
3.3 Discussion
A statistical examination of our data (table 1) shows
that that it is eminently suitable for disentanglement:
the average number of conversations active at a time
is 2.75. Our annotators have high agreement on
the local metric (average of 81.1%). On the 1-to-
1 metric, they disagree more, with a mean overlap
of 53.0% and a maximum of only 63.5%. This level
of overlap does indicate a useful degree of reliabil-
ity, which cannot be achieved with naive heuristics
(see section 5). Thus measuring 1-to-1 overlap with
our annotations is a reasonable evaluation for com-
putational models. However, we feel that the major
source of disagreement is one that can be remedied
in future annotation schemes: the specificity of the
individual annotations.
To measure the level of detail in an annotation, we
use the information-theoretic entropy of the random
variable which indicates which conversation an ut-
terance is in. This quantity is non-negative, increas-
ing as the number of conversations grow and their
size becomes more balanced. It reaches its maxi-
mum, 9.64 bits for this dataset, when each utterance
is placed in a separate conversation. In our anno-
tations, it ranges from 3.0 to 6.2. This large vari-
ation shows that some annotators are more specific
than others, but does not indicate how much they
agree on the general structure. To measure this, we
introduce the many-to-one accuracy. This measure-
ment is asymmetrical, and maps each of the conver-
sations of the source annotation to the single con-
3This is an example of max-weight bipartite matching, and
can be computed optimally using, eg, max-flow. The widely
used greedy algorithm is a two-approximation, although we
have not found large differences in practice.
(Lai) need money
(Astrid) suggest a paypal fund or similar
(Lai) Azzie [sic; typo for Astrid?]: my
shack guy here said paypal too but i have
no local bank acct
(Felicia) second?s Azzie?s suggestion
(Gale) we should charge the noobs $1 per
question to [Lai?s] paypal
(Felicia) bingo!
(Gale) we?d have the money in 2 days max
(Azzie) Lai: hrm, Have you tried to set
one up?
(Arlie) the federal reserve system conspir-
acy is keeping you down man
(Felicia) Gale: all ubuntu users .. pay up!
(Gale) and susers pay double
(Azzie) I certainly would make suse users
pay.
(Hildegard) triple.
(Lai) Azzie: not since being offline
(Felicia) it doesn?t need to be ?in state?
either
Figure 2: A schism occurring in our corpus (abridged):
not all annotators agree on where the thread about charg-
ing for answers to techical questions diverges from the
one about setting up Paypal accounts. Either Gale?s or
Azzie?s first comment seems to be the schism-inducing
utterance.
versation in the target with which it has the great-
est overlap, then counts the total percentage of over-
lap. This is not a statistic to be optimized (indeed,
optimization is trivial: simply make each utterance
in the source into its own conversation), but it can
give us some intuition about specificity. In partic-
ular, if one subdivides a coarse-grained annotation
to make a more specific variant, the many-to-one
accuracy from fine to coarse remains 1. When we
map high-entropy annotations (fine) to lower ones
(coarse), we find high many-to-one accuracy, with a
mean of 86%, which implies that the more specific
annotations have mostly the same large-scale bound-
aries as the coarser ones.
By examining the local metric, we can see even
more: local correlations are good, at an average of
81.1%. This means that, in the three-sentence win-
dow preceding each sentence, the annotators are of-
837
ten in agreement. If they recognize subdivisions of
a large conversation, these subdivisions tend to be
contiguous, not mingled together, which is why they
have little impact on the local measure.
We find reasons for the annotators? disagreement
about appropriate levels of detail in the linguistic
literature. As mentioned, new conversations of-
ten break off from old ones in schisms. Aoki et
al. (2006) discuss conversational features associated
with schisming and the related process of affiliation,
by which speakers attach themselves to a conversa-
tion. Schisms often branch off from asides or even
normal comments (toss-outs) within an existing con-
versation. This means that there is no clear begin-
ning to the new conversation? at the time when it
begins, it is not clear that there are two separate
floors, and this will not become clear until distinct
sets of speakers and patterns of turn-taking are es-
tablished. Speakers, meanwhile, take time to ori-
ent themselves to the new conversation. An example
schism is shown in Figure 2.
Our annotation scheme requires annotators to
mark each utterance as part of a single conversation,
and distinct conversations are not related in any way.
If a schism occurs, the annotator is faced with two
options: if it seems short, they may view it as a mere
digression and label it as part of the parent conver-
sation. If it seems to deserve a place of its own, they
will have to separate it from the parent, but this sev-
ers the initial comment (an otherwise unremarkable
aside) from its context. One or two of the annota-
tors actually remarked that this made the task con-
fusing. Our annotators seem to be either ?splitters?
or ?lumpers?? in other words, each annotator seems
to aim for a consistent level of detail, but each one
has their own idea of what this level should be.
As a final observation about the dataset, we test
the appropriateness of the assumption (used in pre-
vious work) that each speaker takes part in only one
conversation. In our data, the average speaker takes
part in about 3.3 conversations (the actual number
varies for each annotator). The more talkative a
speaker is, the more conversations they participate
in, as shown by a plot of conversations versus utter-
ances (Figure 3). The assumption is not very accu-
rate, especially for speakers with more than 10 utter-
ances.
0 10 20 30 40 50 60
Utterances
0
1
2
3
4
5
6
7
8
9
10
T
h
r
e
a
d
s
Figure 3: Utterances versus conversations participated in
per speaker on development data.
4 Model
Our model for disentanglement fits into the general
class of graph partitioning algorithms (Roth and Yih,
2004) which have been used for a variety of tasks in
NLP, including the related task of meeting segmen-
tation (Malioutov and Barzilay, 2006). These algo-
rithms operate in two stages: first, a binary classifier
marks each pair of items as alike or different, and
second, a consistent partition is extracted.4
4.1 Classification
We use a maximum-entropy classifier (Daume? III,
2004) to decide whether a pair of utterances x and
y are in same or different conversations. The most
likely class is different, which occurs 57% of the
time in development data. We describe the classi-
fier?s performance in terms of raw accuracy (cor-
rect decisions / total), precision and recall of the
same class, and F-score, the harmonic mean of pre-
cision and recall. Our classifier uses several types
of features (table 2). The chat-specific features yield
the highest accuracy and precision. Discourse and
content-based features have poor accuracy on their
own (worse than the baseline), since they work best
on nearby pairs of utterances, and tend to fail on
more distant pairs. Paired with the time gap fea-
ture, however, they boost accuracy somewhat and
produce substantial gains in recall, encouraging the
model to group related utterances together.
The time gap, as discussed above, is the most
widely used feature in previous work. We exam-
4Our first attempt at this task used a Bayesian generative
model. However, we could not define a sharp enough posterior
over new sentences, which made the model unstable and overly
sensitive to its prior.
838
Chat-specific (Acc 73: Prec: 73 Rec: 61 F: 66)
Time The time between x and y in sec-
onds, bucketed logarithmically.
Speaker x and y have the same speaker.
Mention x mentions y (or vice versa),
both mention the same name, ei-
ther mentions any name.
Discourse (Acc 52: Prec: 47 Rec: 77 F: 58)
Cue words Either x or y uses a greeting
(?hello? &c), an answer (?yes?,
?no? &c), or thanks.
Question Either asks a question (explicitly
marked with ???).
Long Either is long (> 10 words).
Content (Acc 50: Prec: 45 Rec: 74 F: 56)
Repeat(i) The number of words shared
between x and y which have
unigram probability i, bucketed
logarithmically.
Tech Whether both x and y use tech-
nical jargon, neither do, or only
one does.
Combined (Acc 75: Prec: 73 Rec: 68 F: 71)
Table 2: Feature functions with performance on develop-
ment data.
ine the distribution of pauses between utterances in
the same conversation. Our choice of a logarithmic
bucketing scheme is intended to capture two char-
acteristics of the distribution (figure 4). The curve
has its maximum at 1-3 seconds, and pauses shorter
than a second are less common. This reflects turn-
taking behavior among participants; participants in
the same conversation prefer to wait for each others?
responses before speaking again. On the other hand,
the curve is quite heavy-tailed to the right, leading
us to bucket long pauses fairly coarsely.
Our discourse-based features model some pair-
0 10 100 1000
0
20
40
seconds
Fr
eq
ue
nc
y
Figure 4: Distribution of pause length (log-scaled) be-
tween utterances in the same conversation.
wise relationships: questions followed by answers,
short comments reacting to longer ones, greetings at
the beginning and thanks at the end.
Word repetition is a key feature in nearly every
model for segmentation or coherence, so it is no sur-
prise that it is useful here. We bucket repeated words
by their unigram probability5 (measured over the en-
tire 52 hours of transcript). The bucketing scheme
allows us to deal with ?noise words? which are re-
peated coincidentally.
The point of the repetition feature is of course to
detect sentences with similar topics. We also find
that sentences with technical content are more likely
to be related than non-technical sentences. We label
an utterance as technical if it contains a web address,
a long string of digits, or a term present in a guide
for novice Linux users 6 but not in a large news cor-
pus (Graff, 1995)7. This is a light-weight way to
capture one ?semantic dimension? or cluster of re-
lated words, in a corpus which is not amenable to
full LSA or similar techniques. LSA in text corpora
yields a better relatedness measure than simple rep-
etition (Foltz et al, 1998), but is ineffective in our
corpus because of its wide variety of topics and lack
of distinct document boundaries.
Pairs of utterances which are widely separated
in the discourse are unlikely to be directly related?
even if they are part of the same conversation, the
link between them is probably a long chain of in-
tervening utterances. Thus, if we run our classifier
on a pair of very distant utterances, we expect it to
default to the majority class, which in this case will
be different, and this will damage our performance
in case the two are really part of the same conver-
sation. To deal with this, we run our classifier only
on utterances separated by 129 seconds or less. This
is the last of our logarithmic buckets in which the
classifier has a significant advantage over the major-
ity baseline. For 99.9% of utterances in an ongoing
conversation, the previous utterance in that conver-
sation is within this gap, and so the system has a
5We discard the 50 most frequent words entirely.
6
?Introduction to Linux: A Hands-on Guide?. Machtelt
Garrels. Edition 1.25 from http://tldp.org/LDP/intro-
linux/html/intro-linux.html .
7Our data came from the LA times, 94-97? helpfully, it pre-
dates the current wide coverage of Linux in the mainstream
press.
839
chance of correctly linking the two.
On test data, the classifier has a mean accuracy of
68.2 (averaged over annotations). The mean preci-
sion of same conversation is 53.3 and the recall is
71.3, with mean F-score of 60. This error rate is
high, but the partitioning procedure allows us to re-
cover from some of the errors, since if nearby utter-
ances are grouped correctly, the bad decisions will
be outvoted by good ones.
4.2 Partitioning
The next step in the process is to cluster the utter-
ances. We wish to find a set of clusters for which the
weighted accuracy of the classifier would be max-
imal; this is an example of correlation clustering
(Bansal et al, 2004), which is NP-complete8. Find-
ing an exact solution proves to be difficult; the prob-
lem has a quadratic number of variables (one for
each pair of utterances) and a cubic number of tri-
angle inequality constraints (three for each triplet).
With 800 utterances in our test set, even solving the
linear program with CPLEX (Ilog, Inc., 2003) is too
expensive to be practical.
Although there are a variety of approximations
and local searches, we do not wish to investigate
partitioning methods in this paper, so we simply
use a greedy search. In this algorithm, we as-
sign utterance j by examining all previous utter-
ances i within the classifier?s window, and treat-
ing the classifier?s judgement pi,j ? .5 as a vote for
cluster(i). If the maximum vote is greater than 0,
we set cluster(j) = argmaxc votec. Otherwise j
is put in a new cluster. Greedy clustering makes at
least a reasonable starting point for further efforts,
since it is a natural online algorithm? it assigns each
utterance as it arrives, without reference to the fu-
ture.
At any rate, we should not take our objective func-
tion too seriously. Although it is roughly correlated
with performance, the high error rate of the classifier
makes it unlikely that small changes in objective will
mean much. In fact, the objective value of our output
solutions are generally higher than those for true so-
8We set up the problem by taking the weight of edge i, j as
the classifier?s decision pi,j ? .5. Roth and Yih (2004) use log
probabilities as weights. Bansal et al (2004) propose the log
odds ratio log(p/(1 ? p)). We are unsure of the relative merit
of these approaches.
lutions, which implies we have already reached the
limits of what our classifier can tell us.
5 Experiments
We annotate the 800 line test transcript using our
system. The annotation obtained has 63 conversa-
tions, with mean length 12.70. The average density
of conversations is 2.9, and the entropy is 3.79. This
places it within the bounds of our human annota-
tions (see table 1), toward the more general end of
the spectrum.
As a standard of comparison for our system, we
provide results for several baselines? trivial systems
which any useful annotation should outperform.
All different Each utterance is a separate conversa-
tion.
All same The whole transcript is a single conversa-
tion.
Blocks of k Each consecutive group of k utterances
is a conversation.
Pause of k Each pause of k seconds or more sepa-
rates two conversations.
Speaker Each speaker?s utterances are treated as a
monologue.
For each particular metric, we calculate the best
baseline result among all of these. To find the best
block size or pause length, we search over multiples
of 5 between 5 and 300. This makes these baselines
appear better than they really are, since their perfor-
mance is optimized with respect to the test data.
Our results, in table 3, are encouraging. On aver-
age, annotators agree more with each other than with
any artificial annotation, and more with our model
than with the baselines. For the 1-to-1 accuracy met-
ric, we cannot claim much beyond these general re-
sults. The range of human variation is quite wide,
and there are annotators who are closer to baselines
than to any other human annotator. As explained
earlier, this is because some human annotations are
much more specific than others. For very specific
annotations, the best baselines are short blocks or
pauses. For the most general, marking all utterances
the same does very well (although for all other an-
notations, it is extremely poor).
840
Other Annotators Model Best Baseline All Diff All Same
Mean 1-to-1 52.98 40.62 34.73 (Blocks of 40) 10.16 20.93
Max 1-to-1 63.50 51.12 56.00 (Pause of 65) 16.00 53.50
Min 1-to-1 35.63 33.63 28.62 (Pause of 25) 6.25 7.13
Mean loc 3 81.09 72.75 62.16 (Speaker) 52.93 47.07
Max loc 3 86.53 75.16 69.05 (Speaker) 62.15 57.47
Min loc 3 74.75 70.47 54.37 (Speaker) 42.53 37.85
Table 3: Metric values between proposed annotations and human annotations. Model scores typically fall between
inter-annotator agreement and baseline performance.
For the local metric, the results are much clearer.
There is no overlap in the ranges; for every test an-
notation, agreement is highest with other annota-
tor, then our model and finally the baselines. The
most competitive baseline is one conversation per
speaker, which makes sense, since if a speaker
makes two comments in a four-utterance window,
they are very likely to be related.
The name mention features are critical for our
model?s performance. Without this feature, the clas-
sifier?s development F-score drops from 71 to 56.
The disentanglement system?s test performance de-
creases proportionally; mean 1-to-1 falls to 36.08,
and mean loc 3 to 63.00, essentially baseline per-
formance. On the other hand, mentions are not
sufficient; with only name mention and time gap
features, mean 1-to-1 is 38.54 and loc 3 is 67.14.
For some utterances, of course, name mentions pro-
vide the only reasonable clue to the correct deci-
sion, which is why humans mention names in the
first place. But our system is probably overly depen-
dent on them, since they are very reliable compared
to our other features.
6 Future Work
Although our annotators are reasonably reliable, it
seems clear that they think of conversations as a hi-
erarchy, with digressions and schisms. We are in-
terested to see an annotation protocol which more
closely follows human intuition and explicitly in-
cludes these kinds of relationships.
We are also interested to see how well this feature
set performs on speech data, as in (Aoki et al, 2003).
Spoken conversation is more natural than text chat,
but when participants are not face-to-face, disentan-
glement remains a problem. On the other hand, spo-
ken dialogue contains new sources of information,
such as prosody. Turn-taking behavior is also more
distinct, which makes the task easier, but according
to (Aoki et al, 2006), it is certainly not sufficient.
Improving the current model will definitely re-
quire better features for the classifier. However, we
also left the issue of partitioning nearly completely
unexplored. If the classifier can indeed be improved,
we expect the impact of search errors to increase.
Another issue is that human users may prefer more
or less specific annotations than our model provides.
We have observed that we can produce lower or
higher-entropy annotations by changing the classi-
fier?s bias to label more edges same or different. But
we do not yet know whether this corresponds with
human judgements, or merely introduces errors.
7 Conclusion
This work provides a corpus of annotated data for
chat disentanglement, which, along with our pro-
posed metrics, should allow future researchers to
evaluate and compare their results quantitatively9.
Our annotations are consistent with one another, es-
pecially with respect to local agreement. We show
that features based on discourse patterns and the
content of utterances are helpful in disentanglement.
The model we present can outperform a variety of
baselines.
Acknowledgements
Our thanks to Suman Karumuri, Steve Sloman, Matt
Lease, David McClosky, 7 test annotators, 3 pilot
annotators, 3 anonymous reviewers and the NSF
PIRE grant.
9Code and data for this project will be available at
http://cs.brown.edu/people/melsner.
841
References
Evrim Acar, Seyit Ahmet Camtepe, Mukkai S. Kr-
ishnamoorthy, and Blent Yener. 2005. Model-
ing and multiway analysis of chatroom tensors. In
Paul B. Kantor, Gheorghe Muresan, Fred Roberts,
Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen,
and Ralph C. Merkle, editors, ISI, volume 3495 of
Lecture Notes in Computer Science, pages 256?268.
Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter?s cocktail party: a
social mobile audio space supporting multiple simul-
taneous conversations. In CHI ?03: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 425?432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke D.
Plurkowski, James D. Thornton, Allison Woodruff,
and Weilie Yi. 2006. Where?s the ?party? in ?multi-
party??: analyzing the structure of small-group socia-
ble talk. In CSCW ?06: Proceedings of the 2006 20th
anniversary conference on Computer supported coop-
erative work, pages 393?402, New York, NY, USA.
ACM Press.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learning, 56(1-
3):89?113.
Seyit Ahmet Camtepe, Mark K. Goldberg, Malik
Magdon-Ismail, and Mukkai Krishnamoorty. 2005.
Detecting conversing groups of chatters: a model, al-
gorithms, and tests. In IADIS AC, pages 89?96.
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name#daume04cg-bfgs, implemen-
tation available at http://hal3.name/megam/, August.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In ACL ?03: Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 562?569, Morristown, NJ,
USA. Association for Computational Linguistics.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Ilog, Inc. 2003. Cplex solver.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006. Addressee identification in face-to-face
meetings. In EACL. The Association for Computer
Linguistics.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In ACL.
The Association for Computer Linguistics.
Jacki O?Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP ?03: Proceedings of the 2003 inter-
national ACM SIGGROUP conference on Supporting
group work, pages 40?49, New York, NY, USA. ACM
Press.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL-2004, pages
1?8. Boston, MA, USA.
Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.
1974. A simplest systematics for the organization of
turn-taking for conversation. Language, 50(4):696?
735.
842
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41?44,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Coreference-inspired Coherence Modeling
Micha Elsner and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@cs.brown.edu
Abstract
Research on coreference resolution and sum-
marization has modeled the way entities are
realized as concrete phrases in discourse. In
particular there exist models of the noun
phrase syntax used for discourse-new versus
discourse-old referents, and models describ-
ing the likely distance between a pronoun and
its antecedent. However, models of discourse
coherence, as applied to information ordering
tasks, have ignored these kinds of information.
We apply a discourse-new classifier and pro-
noun coreference algorithm to the information
ordering task, and show significant improve-
ments in performance over the entity grid, a
popular model of local coherence.
1 Introduction
Models of discourse coherence describe the relation-
ships between nearby sentences, in which previous
sentences help make their successors easier to un-
derstand. Models of coherence have been used to
impose an order on sentences for multidocument
summarization (Barzilay et al, 2002), to evaluate
the quality of human-authored essays (Miltsakaki
and Kukich, 2004), and to insert new information
into existing documents (Chen et al, 2007).
These models typically view a sentence either as
a bag of words (Foltz et al, 1998) or as a bag of en-
tities associated with various syntactic roles (Lapata
and Barzilay, 2005). However, a mention of an en-
tity contains more information than just its head and
syntactic role. The referring expression itself con-
tains discourse-motivated information distinguish-
ing familiar entities from unfamiliar and salient from
non-salient. These patterns have been studied ex-
tensively, by linguists (Prince, 1981; Fraurud, 1990)
and in the field of coreference resolution. We draw
on the coreference work, taking two standard models
from the literature and applying them to coherence
modeling.
Our first model distinguishes discourse-new from
discourse-old noun phrases, using features based
on Uryupina (2003). Discourse-new NPs are those
whose referents have not been previously mentioned
in the discourse. As noted by studies since Hawkins
(1978), there are marked syntactic differences be-
tween the two classes.
Our second model describes pronoun coreference.
To be intelligible, pronouns must be placed close to
appropriate referents with the correct number and
gender. Centering theory (Grosz et al, 1995) de-
scribes additional constraints about which entities in
a discourse can be pronominalized: if there are pro-
nouns in a segment, they must include the backward-
looking center. We use a model which probabilisti-
cally attempts to describe these preferences (Ge et
al., 1998).
These two models can be combined with the en-
tity grid described by Lapata and Barzilay (2005)
for significant improvement. The magnitude of the
improvement is particularly interesting given that
Barzilay and Lapata (2005) do use a coreference sys-
tem but are unable to derive much advantage from it.
2 Discourse-new Model
In the task of discourse-new classification, the model
is given a referring expression (as in previous work,
we consider only NPs) from a document and must
41
determine whether it is a first mention (discourse-
new) or a subsequent mention (discourse-old). Fea-
tures such as full names, appositives, and restrictive
relative clauses are associated with the introduction
of unfamiliar entities into discourse (Hawkins, 1978;
Fraurud, 1990; Vieira and Poesio, 2000). Classi-
fiers in the literature include (Poesio et al, 2005;
Uryupina, 2003; Ng and Cardie, 2002). The sys-
tem of Nenkova and McKeown (2003) works in the
opposite direction. It is designed to rewrite the ref-
erences in multi-document summaries, so that they
conform to the common discourse patterns.
We construct a maximum-entropy classifier us-
ing syntactic and lexical features derived from
Uryupina (2003), and a publicly available learning
tool (Daume? III, 2004). Our system scores 87.4%
(F-score of the disc-new class on the MUC-7 for-
mal test set); this is comparable to the state-of-the-
art system of Uryupina (2003), which scores 86.91.
To model coreference with this system, we assign
each NP in a document a label Lnp ? {new, old}.
Since the correct labeling depends on the coref-
erence relationships between the NPs, we need
some way to guess at this; we take all NPs with
the same head to be coreferent, as in the non-
coreference version of (Barzilay and Lapata, 2005)2.
We then take the probability of a document as
?
np:NPs P (Lnp|np).
We must make several small changes to the model
to adapt it to this setting. For the discourse-new clas-
sification task, the model?s most important feature
is whether the head word of the NP to be classified
has occurred previously (as in Ng and Cardie (2002)
and Vieira and Poesio (2000)). For coherence mod-
eling, we must remove this feature, since it depends
on document order, which is precisely what we are
trying to predict. The coreference heuristic will also
fail to resolve any pronouns, so we discard them.
Another issue is that NPs whose referents are
familiar tend to resemble discourse-old NPs, even
though they have not been previously mentioned
(Fraurud, 1990). These include unique objects like
the FBI or generic ones like danger or percent. To
1Poesio et al (2005) score 90.2%, but on a different corpus.
2Unfortunately, this represents a substantial sacrifice; as
Poesio and Vieira (1998) show, only about 2/3 of definite de-
scriptions which are anaphoric have the same head as their an-
tecedent.
avoid using these deceptive phrases as examples of
discourse-newness, we attempt to heuristically re-
move them from the training set by discarding any
NP whose head occurs only once in the document3.
The labels we apply to NPs in our test data are
systematically biased by the ?same head? heuristic
we use for coreference. This is a disadvantage for
our system, but it has a corresponding advantage?
we can use training data labeled using the same
heuristic, without any loss in performance on the
coherence task. NPs we fail to learn about during
training are likely to be mislabeled at test time any-
way, so performance does not degrade by much. To
counter this slight degradation, we can use a much
larger training corpus, since we no longer require
gold-standard coreference annotations.
3 Pronoun Coreference Model
Pronoun coreference is another important aspect of
coherence? if a pronoun is used too far away from
any natural referent, it becomes hard to interpret,
creating confusion. Too many referents, however,
create ambiguity. To describe this type of restriction,
we must model the probability of the text containing
pronouns (denoted ri), jointly with their referents
ai. (This takes more work than simply resolving the
pronouns conditioned on the text.) The model of Ge
et al (1998) provides the requisite probabilities:
P (ai, ri|ai?1i ) =P (ai|h(ai), m(ai))
Pgen(ai, ri)Pnum(ai, ri)
Here h(a) is the Hobbs distance (Hobbs, 1976),
which measures distance between a pronoun and
prospective antecedent, taking into account various
factors, such as syntactic constraints on pronouns.
m(a) is the number of times the antecedent has
been mentioned previously in the document (again
using ?same head? coreference for full NPs, but
also counting the previous antecedents ai?1i ). Pgen
and Pnum are distributions over gender and num-
ber given words. The model is trained using a small
hand-annotated corpus first used in Ge et al (1998).
3Bean and Riloff (1999) and Uryupina (2003) construct
quite accurate classifiers to detect unique NPs. However, some
preliminary experiments convinced us that our heuristic method
worked well enough for the purpose.
42
Disc. Acc Disc. F Ins.
Random 50.00 50.00 12.58
Entity Grid 76.17 77.55 19.57
Disc-New 70.35 73.47 16.27
Pronoun 55.77 62.27 13.95
EGrid+Disc-New 78.88 80.31 21.93
Combined 79.60 81.02 22.98
Table 1: Results on 1004 WSJ documents.
Finding the probability of a document using this
model requires us to sum out the antecedents a. Un-
fortunately, because each ai is conditioned on the
previous ones, this cannot be done efficiently. In-
stead, we use a greedy search, assigning each pro-
noun left to right. Finally we report the probability
of the resulting sequence of pronoun assignments.
4 Baseline Model
As a baseline, we adopt the entity grid (Lapata and
Barzilay, 2005). This model outperforms a variety
of word overlap and semantic similarity models, and
is used as a component in the state-of-the-art system
of Soricut and Marcu (2006). The entity grid rep-
resents each entity by tracking the syntactic roles in
which it appears throughout the document. The in-
ternal syntax of the various referring expressions is
ignored. Since it also uses the ?same head? corefer-
ence heuristic, it also disregards pronouns.
Since the three models use very different feature
sets, we combine them by assuming independence
and multiplying the probabilities.
5 Experiments
We evaluate our models using two tasks, both based
on the assumption that a human-authored document
is coherent, and uses the best possible ordering of
its sentences (see Lapata (2006)). In the discrimina-
tion task (Barzilay and Lapata, 2005), a document
is compared with a random permutation of its sen-
tences, and we score the system correct if it indicates
the original as more coherent4.
4Since the model might refuse to make a decision by scor-
ing a permutation the same as the original, we also report
F-score, where precision is correct/decisions and recall is
correct/total.
Discrimination becomes easier for longer docu-
ments, since a random permutation is likely to be
much less similar to the original. Therefore we also
test our systems on the task of insertion (Chen et al,
2007), in which we remove a sentence from a doc-
ument, then find the point of insertion which yields
the highest coherence score. The reported score is
the average fraction of sentences per document rein-
serted in their original position (averaged over doc-
uments, not sentences, so that longer documents do
not disproportionally influence the results)5.
We test on sections 14-24 of the Penn Treebank
(1004 documents total). Previous work has fo-
cused on the AIRPLANE corpus (Barzilay and Lee,
2004), which contains short announcements of air-
plane crashes written by and for domain experts.
These texts use a very constrained style, with few
discourse-new markers or pronouns, and so our sys-
tem is ineffective; the WSJ corpus is much more
typical of normal informative writing. Also unlike
previous work, we do not test the task of completely
reconstructing a document?s order, since this is com-
putationally intractable and results on WSJ docu-
ments6 would likely be dominated by search errors.
Our results are shown in table 5. When run alone,
the entity grid outperforms either of our models.
However, all three models are significantly better
than random. Combining all three models raises dis-
crimination performance by 3.5% over the baseline
and insertion by 3.4%. Even the weakest compo-
nent, pronouns, contributes to the joint model; when
it is left out, the resulting EGrid + Disc-New model
is significantly worse than the full combination. We
test significance using Wilcoxon?s signed-rank test;
all results are significant with p < .001.
6 Conclusions
The use of these coreference-inspired models leads
to significant improvements in the baseline. Of the
two, the discourse-new detector is by far more ef-
fective. The pronoun model?s main problem is that,
although a pronoun may have been displaced from
its original position, it can often find another seem-
ingly acceptable referent nearby. Despite this issue
5Although we designed a metric that distinguishes near
misses from random performance, it is very well correlated with
exact precision, so, for simplicity?s sake, we omit it.
6Average 22 sentences, as opposed to 11.5 for AIRPLANE.
43
it performs significantly better than chance and is
capable of slightly improving the combined model.
Both of these models are very different from the lex-
ical and entity-based models currently used for this
task (Soricut and Marcu, 2006), and are probably
capable of improving the state of the art.
As mentioned, Barzilay and Lapata (2005) uses a
coreference system to attempt to improve the entity
grid, but with mixed results. Their method of com-
bination is quite different from ours; they use the
system?s judgements to define the ?entities? whose
repetitions the system measures7. In contrast, we do
not attempt to use any proposed coreference links;
as Barzilay and Lapata (2005) point out, these links
are often erroneous because the disorded input text
is so dissimilar to the training data. Instead we ex-
ploit our models? ability to measure the probability
of various aspects of the text.
Acknowledgements
Chen and Barzilay, reviewers, DARPA, et al
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In ACL
2005.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004, pages 113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Ar-
tificial Intelligence Results (JAIR), 17:35?55.
David L. Bean and Ellen Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
ACL?99, pages 373?380.
Erdong Chen, Benjamin Snyder, and Regina Barzilay.
2007. Incremental text structuring with online hier-
archical ranking. In Proceedings of EMNLP.
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regressio n. Paper available
at http://pub.hal3.name#daume04cg-bfgs, implemen-
tation available at http://hal3.name/megam/, August.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
7We attempted this method for pronouns using our model,
but found it ineffective.
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Kari Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Seman-
tics, 7(4):395?433.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?171.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: a study in reference and grammaticality predic-
tion. Croom Helm Ltd.
Jerry R. Hobbs. 1976. Pronoun resolution. Technical
Report 76-1, City College New York.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall?s tau. Computational Linguis-
tics, 32(4):1?14.
E. Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25?55.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In NAACL
?03, pages 70?72.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In COLING.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata
Vieira, Rodrigo Goulart, and Olga Uryupina. 2005.
Does discourse-new detection help definite description
resolution? In Proceedings of the Sixth International
Workshop on Computational Semantics, Tillburg.
Ellen Prince. 1981. Toward a taxonomy of given-new in-
formation. In Peter Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In ACL-
2006.
Olga Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proceed-
ings of the ACL Student Workshop, Sapporo.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
44
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Bounding and Comparing Methods for Correlation Clustering Beyond ILP
Micha Elsner and Warren Schudy
Department of Computer Science
Brown University
Providence, RI 02912
{melsner,ws}@cs.brown.edu
Abstract
We evaluate several heuristic solvers for corre-
lation clustering, the NP-hard problem of par-
titioning a dataset given pairwise affinities be-
tween all points. We experiment on two prac-
tical tasks, document clustering and chat dis-
entanglement, to which ILP does not scale.
On these datasets, we show that the cluster-
ing objective often, but not always, correlates
with external metrics, and that local search al-
ways improves over greedy solutions. We use
semi-definite programming (SDP) to provide a
tighter bound, showing that simple algorithms
are already close to optimality.
1 Introduction
Correlation clustering is a powerful technique for
discovering structure in data. It operates on the
pairwise relationships between datapoints, partition-
ing the graph to minimize the number of unrelated
pairs that are clustered together, plus the number
of related pairs that are separated. Unfortunately,
this minimization problem is NP-hard (Ailon et al,
2008). Practical work has adopted one of three
strategies for solving it. For a few specific tasks, one
can restrict the problem so that it is efficiently solv-
able. In most cases, however, this is impossible. In-
teger linear programming (ILP) can be used to solve
the general problem optimally, but only when the
number of data points is small. Beyond a few hun-
dred points, the only available solutions are heuristic
or approximate.
In this paper, we evaluate a variety of solu-
tions for correlation clustering on two realistic NLP
tasks, text topic clustering and chat disentangle-
ment, where typical datasets are too large for ILP
to find a solution. We show, as in previous work
on consensus clustering (Goder and Filkov, 2008),
that local search can improve the solutions found by
commonly-used methods. We investigate the rela-
tionship between the clustering objective and exter-
nal evaluation metrics such as F-score and one-to-
one overlap, showing that optimizing the objective
is usually a reasonable aim, but that other measure-
ments like number of clusters found should some-
times be used to reject pathological solutions. We
prove that the best heuristics are quite close to op-
timal, using the first implementation of the semi-
definite programming (SDP) relaxation to provide
tighter bounds.
The specific algorithms we investigate are, of
course, only a subset of the large number of pos-
sible solutions, or even of those proposed in the lit-
erature. We chose to test a few common, efficient
algorithms that are easily implemented. Our use of
a good bounding strategy means that we do not need
to perform an exhaustive comparison; we will show
that, though the methods we describe are not per-
fect, the remaining improvements possible with any
algorithm are relatively small.
2 Previous Work
Correlation clustering was first introduced by Ben-
Dor et al (1999) to cluster gene expression pat-
terns. The correlation clustering approach has sev-
eral strengths. It does not require users to specify
a parametric form for the clusters, nor to pick the
number of clusters. Unlike fully unsupervised clus-
19
tering methods, it can use training data to optimize
the pairwise classifier, but unlike classification, it
does not require samples from the specific clusters
found in the test data. For instance, it can use mes-
sages about cars to learn a similarity function that
can then be applied to messages about atheism.
Correlation clustering is a standard method for
coreference resolution. It was introduced to the
area by Soon et al (2001), who describe the first-
link heuristic method for solving it. Ng and Cardie
(2002) extend this work with better features, and de-
velop the best-link heuristic, which finds better solu-
tions. McCallum and Wellner (2004) explicitly de-
scribe the problem as correlation clustering and use
an approximate technique (Bansal et al, 2004) to
enforce transitivity. Recently Finkel and Manning
(2008) show that the optimal ILP solution outper-
forms the first and best-link methods. Cohen and
Richman (2002) experiment with various heuristic
solutions for the cross-document coreference task of
grouping references to named entities.
Finally, correlation clustering has proven useful in
several discourse tasks. Barzilay and Lapata (2006)
use it for content aggregation in a generation system.
In Malioutov and Barzilay (2006), it is used for topic
segmentation?since segments must be contiguous,
the problem can be solved in polynomial time. El-
sner and Charniak (2008) address the related prob-
lem of disentanglement (which we explore in Sec-
tion 5.3), doing inference with the voting greedy al-
gorithm.
Bertolacci and Wirth (2007), Goder and Filkov
(2008) and Gionis et al (2007) conduct experiments
on the closely related problem of consensus cluster-
ing, often solved by reduction to correlation cluster-
ing. The input to this problem is a set of clusterings;
the output is a ?median? clustering which minimizes
the sum of (Rand) distance to the inputs. Although
these papers investigate some of the same algorithms
we use, they use an unrealistic lower bound, and so
cannot convincingly evaluate absolute performance.
Gionis et al (2007) give an external evaluation on
some UCI datasets, but this is somewhat unconvinc-
ing since their metric, the impurity index, which is
essentially precision ignoring recall, gives a perfect
score to the all-singletons clustering. The other two
papers are based on objective values, not external
metrics.1
A variety of approximation algorithms for corre-
lation clustering with worst-case theoretical guar-
antees have been proposed: (Bansal et al, 2004;
Ailon et al, 2008; Demaine et al, 2006; Charikar
et al, 2005; Giotis and Guruswami, 2006). Re-
searchers including (Ben-Dor et al, 1999; Joachims
and Hopcroft, 2005; Mathieu and Schudy, 2008)
study correlation clustering theoretically when the
input is generated by randomly perturbing an un-
known ground truth clustering.
3 Algorithms
We begin with some notation and a formal definition
of the problem. Our input is a complete, undirected
graph G with n nodes; each edge in the graph has
a probability pij reflecting our belief as to whether
nodes i and j come from the same cluster. Our goal
is to find a clustering, defined as a new graph G?
with edges xij ? {0, 1}, where if xij = 1, nodes
i and j are assigned to the same cluster. To make
this consistent, the edges must define an equivalence
relationship: xii = 1 and xij = xjk = 1 implies
xij = xik.
Our objective is to find a clustering as consistent
as possible with our beliefs?edges with high proba-
bility should not cross cluster boundaries, and edges
with low probability should. We define w+ij as the
cost of cutting an edge whose probability is pij and
w?ij as the cost of keeping it. Mathematically, this
objective can be written (Ailon et al, 2008; Finkel
and Manning, 2008) as:
min ?
ij:i<j
xijw?ij + (1? xij)w+ij . (1)
There are two plausible definitions for the costs w+
and w?, both of which have gained some support in
the literature. We can take w+ij = pij and w?ij =1 ? pij (additive weights) as in (Ailon et al, 2008)
and others, or w+ij = log(pij), w?ij = log(1 ? pij)
(logarithmic weights) as in (Finkel and Manning,
2008). The logarithmic scheme has a tenuous math-
ematical justification, since it selects a maximum-
likelihood clustering under the assumption that the
1Bertolacci and Wirth (2007) gave normalized mutual infor-
mation for one algorithm and data set, but almost all of their
results study objective value only.
20
pij are independent and identically distributed given
the status of the edge ij in the true clustering. If
we obtain the pij using a classifier, however, this as-
sumption is obviously untrue?some nodes will be
easy to link, while others will be hard?so we eval-
uate the different weighting schemes empirically.
3.1 Greedy Methods
We use four greedy methods drawn from the lit-
erature; they are both fast and easy to implement.
All of them make decisions based on the net weight
w?ij = w+ij ?w?ij .
These algorithms step through the nodes of the
graph according to a permutation pi. We try 100 ran-
dom permutations for each algorithm and report the
run which attains the best objective value (typically
this is slightly better than the average run; we dis-
cuss this more in the experimental sections). To sim-
plify the pseudocode we label the vertices 1, 2, . . . n
in the order specified by pi. After this relabeling
pi(i) = i so pi need not appear explicitly in the al-
gorithms.
Three of the algorithms are given in Figure 1. All
three algorithms start with the empty clustering and
add the vertices one by one. The BEST algorithm
adds each vertex i to the cluster with the strongest
w? connecting to i, or to a new singleton if none of
the w? are positive. The FIRST algorithm adds each
vertex i to the cluster containing the most recently
considered vertex j with w?ij > 0. The VOTE algo-
rithm adds each vertex to the cluster that minimizes
the correlation clustering objective, i.e. to the cluster
maximizing the total net weight or to a singleton if
no total is positive.
Ailon et al (2008) introduced the PIVOT algo-
rithm, given in Figure 2, and proved that it is a 5-
approximation if w+ij + w?ij = 1 for all i, j and
pi is chosen randomly. Unlike BEST, VOTE and
FIRST, which build clusters vertex by vertex, the
PIVOT algorithm creates each new cluster in its fi-
nal form. This algorithm repeatedly takes an unclus-
tered pivot vertex and creates a new cluster contain-
ing that vertex and all unclustered neighbors with
positive weight.
3.2 Local Search
We use the straightforward local search previously
used by Gionis et al (2007) and Goder and Filkov
k ? 0 // number of clusters created so far
for i = 1 . . . n do
for c = 1 . . . k do
if BEST then
Qualityc ? maxj?C[c] w?ij
else if FIRST then
Qualityc ? maxj?C[c]:w?ij>0 j
else if VOTE then
Qualityc ?
?
j?C[c] w?ij
c? ? argmax1?c?k Qualityc
if Qualityc? > 0 then
C[c?]? C[c?] ? {i}
else
C[k++]? {i} // form a new cluster
Figure 1: BEST/FIRST/VOTE algorithms
k ? 0 // number of clusters created so far
for i = 1 . . . n do
P ? ?1?c?k C[c] // Vertices already placed
if i 6? P then
C[k++] ? {i} ? { i < j ? n :
j 6? P and w?ij > 0 }
Figure 2: PIVOT algorithm by Ailon et al (2008)
(2008). The allowed one element moves consist
of removing one vertex from a cluster and either
moving it to another cluster or to a new singleton
cluster. The best one element move (BOEM) al-
gorithm repeatedly makes the most profitable best
one element move until a local optimum is reached.
Simulated Annealing (SA) makes a random single-
element move, with probability related to the dif-
ference in objective it causes and the current tem-
perature. Our annealing schedule is exponential and
designed to attempt 2000n moves for n nodes. We
initialize the local search either with all nodes clus-
tered together, or at the clustering produced by one
of our greedy algorithms (in our tables, the latter is
written, eg. PIVOT/BOEM, if the greedy algorithm
is PIVOT).
4 Bounding with SDP
Although comparing different algorithms to one an-
other gives a good picture of relative performance, it
is natural to wonder how well they do in an absolute
sense?how they compare to the optimal solution.
21
For very small instances, we can actually find the
optimum using ILP, but since this does not scale be-
yond a few hundred points (see Section 5.1), for re-
alistic instances we must instead bound the optimal
value. Bounds are usually obtained by solving a re-
laxation of the original problem: a simpler problem
with the same objective but fewer constraints.
The bound used in previous work (Goder and
Filkov, 2008; Gionis et al, 2007; Bertolacci and
Wirth, 2007), which we call the trivial bound, is
obtained by ignoring the transitivity constraints en-
tirely. To optimize, we link (xij = 1) all the pairs
where w+ij is larger than w?ij ; since this solution is
quite far from being a clustering, the bound tends
not to be very tight.
To get a better idea of how good a real clustering
can be, we use a semi-definite programming (SDP)
relaxation to provide a better bound. Here we moti-
vate and define this relaxation.
One can picture a clustering geometrically by as-
sociating cluster c with the standard basis vector
ec = (0, 0, . . . , 0,? ?? ?
c?1
1, 0, . . . , 0? ?? ?
n?c
) ? Rn. If object i is
in cluster c then it is natural to associate i with the
vector ri = ec. This gives a nice geometric picture
of a clustering, with objects i and j in the same clus-
ter if and only if ri = rj . Note that the dot product
ri ? rj is 1 if i and j are in the same cluster and 0
otherwise. These ideas yield a simple reformulation
of the correlation clustering problem:
minr?i,j:i<j(ri ? rj)w?ij + (1? rj ? rj)w+ij
s.t. ?i ?c : ri = ec
To get an efficiently computable lower-bound we
relax the constraints that the ris are standard basis
vectors, replacing them with two sets of constraints:
ri ? ri = 1 for all i and ri ? rj ? 0 for all i, j.
Since the ri only appear as dot products, we can
rewrite in terms of xij = ri ? rj . However, we
must now constrain the xij to be the dot products
of some set of vectors in Rn. This is true if and
only if the symmetric matrix X = {xij}ij is posi-
tive semi-definite. We now have the standard semi-
definite programming (SDP) relaxation of correla-
tion clustering (e.g. (Charikar et al, 2005; Mathieu
and Schudy, 2008)):
minx?i,j:i<j xijw?ij + (1? xij)w+ij
s.t.
?
?
?
xii = 1 ?i
xij ? 0 ?i, j
X = {xij}ij PSD
.
This SDP has been studied theoretically by a
number of authors; we mention just two here.
Charikar et al (2005) give an approximation al-
gorithm based on rounding the SDP which is a
0.7664 approximation for the problem of maximiz-
ing agreements. Mathieu and Schudy (2008) show
that if the input is generated by corrupting the
edges of a ground truth clustering B independently,
then the SDP relaxation value is within an additive
O(n?n) of the optimum clustering. They further
show that using the PIVOT algorithm to round the
SDP yields a clustering with value at most O(n?n)
more than optimal.
5 Experiments
5.1 Scalability
Using synthetic data, we investigate the scalability
of the linear programming solver and SDP bound.
To find optimal solutions, we pass the complete ILP2
to CPLEX. This is reasonable for 100 points and
solvable for 200; beyond this point it cannot be
solved due to memory exhaustion. As noted below,
despite our inability to compute the LP bound on
large instances, we can sometimes prove that they
must be worse than SDP bounds, so we do not in-
vestigate LP-solving techniques further.
The SDP has fewer constraints than the ILP
(O(n2) vs O(n3)), but this is still more than many
SDP solvers can handle. For our experiments we
used one of the few SDP solvers that can handle such
a large number of constraints: Christoph Helmberg?s
ConicBundle library (Helmberg, 2009; Helmberg,
2000). This solver can handle several thousand data-
points. It produces loose lower-bounds (off by a few
percent) quickly but converges to optimality quite
slowly; we err on the side of inefficiency by run-
ning for up to 60 hours. Of course, the SDP solver
is only necessary to bound algorithm performance;
our solvers themselves scale much better.
2Consisting of the objective plus constraints 0 ? xij ? 1
and triangle inequality (Ailon et al, 2008).
22
5.2 Twenty Newsgroups
In this section, we test our approach on a typi-
cal benchmark clustering dataset, 20 Newsgroups,
which contains posts from a variety of Usenet
newsgroups such as rec.motorcycles and
alt.atheism. Since our bounding technique
does not scale to the full dataset, we restrict our at-
tention to a subsample of 100 messages3 from each
newsgroup for a total of 2000?still a realistically
large-scale problem. Our goal is to cluster messages
by their newsgroup of origin. We conduct exper-
iments by holding out four newsgroups as a train-
ing set, learning a pairwise classifier, and applying it
to the remaining 16 newsgroups to form our affinity
matrix.4
Our pairwise classifier uses three types of fea-
tures previously found useful in document cluster-
ing. First, we bucket al words5 by their log doc-
ument frequency (for an overview of TF-IDF see
(Joachims, 1997)). For a pair of messages, we create
a feature for each bucket whose value is the propor-
tion of shared words in that bucket. Secondly, we
run LSA (Deerwester et al, 1990) on the TF-IDF
matrix for the dataset, and use the cosine distance
between each message pair as a feature. Finally, we
use the same type of shared words features for terms
in message subjects. We make a training instance for
each pair of documents in the training set and learn
via logistic regression.
The classifier has an average F-score of 29% and
an accuracy of 88%?not particularly good. We
should emphasize that the clustering task for 20
newsgroups is much harder than the more com-
mon classification task?since our training set is en-
tirely disjoint with the testing set, we can only learn
weights on feature categories, not term weights. Our
aim is to create realistic-looking data on which to
test our clustering methods, not to motivate correla-
tion clustering as a solution to this specific problem.
In fact, Zhong and Ghosh (2003) report better results
using generative models.
We evaluate our clusterings using three different
3Available as mini newsgroups.tar.gz from the UCI
machine learning repository.
4The experiments below are averaged over four disjoint
training sets.
5We omit the message header, except the subject line, and
also discard word types with fewer than 3 occurrences.
Logarithmic Weights
Obj Rand F 1-1
SDP bound 51.1% - - -
VOTE/BOEM 55.8% 93.80 33 41
SA 56.3% 93.56 31 36
PIVOT/BOEM 56.6% 93.63 32 39
BEST/BOEM 57.6% 93.57 31 38
FIRST/BOEM 57.9% 93.65 30 36
VOTE 59.0% 93.41 29 35
BOEM 60.1% 93.51 30 35
PIVOT 100% 90.85 17 27
BEST 138% 87.11 20 29
FIRST 619% 40.97 11 8
Additive Weights
Obj Rand F 1-1
SDP bound 59.0% - - -
SA 63.5% 93.75 32 39
VOTE/BOEM 63.5% 93.75 32 39
PIVOT/BOEM 63.7% 93.70 32 39
BEST/BOEM 63.8% 93.73 31 39
FIRST/BOEM 63.9% 93.58 31 37
BOEM 64.6% 93.65 31 37
VOTE 67.3% 93.35 28 34
PIVOT 109% 90.63 17 26
BEST 165% 87.06 20 29
FIRST 761% 40.46 11 8
Table 1: Score of the solution with best objective for each
solver, averaged over newsgroups training sets, sorted by
objective.
metrics (see Meila (2007) for an overview of cluster-
ing metrics). The Rand measure counts the number
of pairs of points for which the proposed clustering
agrees with ground truth. This is the metric which
is mathematically closest to the objective. However,
since most points are in different clusters, any so-
lution with small clusters tends to get a high score.
Therefore we also report the more sensitive F-score
with respect to the minority (?same cluster?) class.
We also report the one-to-one score, which mea-
sures accuracy over single points. For this metric,
we calculate a maximum-weight matching between
proposed clusters and ground-truth clusters, then re-
port the overlap between the two.
When presenting objective values, we locate them
within the range between the trivial lower bound dis-
23
cussed in Section 4 and the objective value of the
singletons clustering (xij = 0, i 6= j). On this scale,
lower is better; 0% corresponds to the trivial bound
and 100% corresponds to the singletons clustering.
It is possible to find values greater than 100%, since
some particularly bad clusterings have objectives
worse than the singletons clustering. Plainly, how-
ever, real clusterings will not have values as low as
0%, since the trivial bound is so unrealistic.
Our results are shown in Table 1. The best re-
sults are obtained using logarithmic weights with
VOTE followed by BOEM; reasonable results are
also found using additive weights, and annealing,
VOTE or PIVOT followed by BOEM. On its own,
the best greedy scheme is VOTE, but all of them are
substantially improved by BOEM. First-link is by
far the worst. Our use of the SDP lower bound rather
than the trivial lower-bound of 0% reduces the gap
between the best clustering and the lower bound by
over a factor of ten. It is easy to show that the LP
relaxation can obtain a bound of at most 50%6?the
SDP beats the LP in both runtime and quality!
We analyze the correlation between objective val-
ues and metric values, averaging Kendall?s tau7 over
the four datasets (Table 2). Over the entire dataset,
correlations are generally good (large and negative),
showing that optimizing the objective is indeed a
useful way to find good results. We also examine
correlations for the solutions with objective values
within the top 10%. Here the correlation is much
poorer; selecting the solution with the best objective
value will not necessarily optimize the metric, al-
though the correspondence is slightly better for the
log-weights scheme. The correlations do exist, how-
ever, and so the solution with the best objective value
is typically slightly better than the median.
In Figure 3, we show the distribution of one-to-
one scores obtained (for one specific dataset) by the
best solvers. From this diagram, it is clear that log-
weights and VOTE/BOEM usually obtain the best
scores for this metric, since the median is higher
than other solvers? upper quartile scores. All solvers
have quite high variance, with a range of about 2%
between quartiles and 4% overall. We omit the F-
6The solution xij = 121
`
w?ij > w+ij
?
for i < j is feasible
in the LP.
7The standard Pearson correlation coefficient is less robust
to outliers, which causes problems for this data.
B
O
E
M
b
e
s
t
/
B
L
b
e
s
t
/
B
f
i
r
s
t
/
B
p
i
v
o
t
/
B
L
p
i
v
o
t
/
B
S
A
L
-
S
A
v
o
t
e
/
B
L
v
o
t
e
/
B
0.32
0.34
0.36
0.38
0.40
0.42
0.44
Figure 3: Box-and-whisker diagram (outliers as +) for
one-to-one scores obtained by the best few solvers on a
particular newsgroup dataset. L means using log weights.
B means improved with BOEM.
Rand F 1-1
Log-wt -.60 -.73 -.71
Top 10 % -.14 -.22 -.24
Add-wt -.60 -.67 -.65
Top 10 % -.13 -.15 -.14
Table 2: Kendall?s tau correlation between objective and
metric values, averaged over newsgroup datasets, for all
solutions and top 10% of solutions.
score plot, which is similar, for space reasons.
5.3 Chat Disentanglement
In the disentanglement task, we examine data from a
shared discussion group where many conversations
are occurring simultaneously. The task is to partition
the utterances into a set of conversations. This task
differs from newsgroup clustering in that data points
(utterances) have an inherent linear order. Ordering
is typical in discourse tasks including topic segmen-
tation and coreference resolution.
We use the annotated dataset and pairwise classi-
24
fier made available by Elsner and Charniak (2008);8
this study represents a competitive baseline, al-
though more recently Wang and Oard (2009) have
improved it. Since this classifier is ineffective at
linking utterances more than 129 seconds apart, we
treat all decisions for such utterances as abstentions,
p = .5. For utterance pairs on which it does make
a decision, the classifier has a reported accuracy of
75% with an F-score of 71%.
As in previous work, we run experiments on the
800-utterance test set and average metrics over 6 test
annotations. We evaluate using the three metrics re-
ported by previous work. Two node-counting met-
rics measure global accuracy: one-to-one match as
explained above, and Shen?s F (Shen et al, 2006):
F = ?i nin maxj(F (i, j)). Here i is a gold con-
versation with size ni and j is a proposed conver-
sation with size nj , sharing nij utterances; F (i, j)
is the harmonic mean of precision (nijnj ) and recall
(nijni ). A third metric, the local agreement, counts
edgewise agreement for pairs of nearby utterances,
where nearby means ?within three utterances.?
In this dataset, the SDP is a more moderate im-
provement over the trivial lower bound, reducing
the gap between the best clustering and best lower
bound by a factor of about 3 (Table 3).
Optimization of the objective does not correspond
to improvements in the global metrics (Table 3);
for instance, the best objectives are attained with
FIRST/BOEM, but VOTE/BOEM yields better one-
to-one and F scores. Correlation between the ob-
jective and these global metrics is extremely weak
(Table 5). The local metric is somewhat correlated.
Local search does improve metric results for each
particular greedy algorithm. For instance, when
BOEM is added to VOTE (with log weights), one-
to-one increases from 44% to 46%, local from 72%
to 73% and F from 48% to 50%. This represents a
moderate improvement on the inference scheme de-
scribed in Elsner and Charniak (2008). They use
voting with additive weights, but rather than per-
forming multiple runs over random permutations,
they process utterances in the order they occur. (We
experimented with processing in order; the results
are unclear, but there is a slight trend toward worse
performance, as in this case.) Their results (also
8Downloaded from cs.brown.edu/?melsner
shown in the table) are 41% one-to-one, 73% local
and .44% F-score.9 Our improvement on the global
metrics (12% relative improvement in one-to-one,
13% in F-score) is modest, but was achieved with
better inference on exactly the same input.
Since the objective function fails to distinguish
good solutions from bad ones, we examine the types
of solutions found by different methods in the hope
of explaining why some perform better than others.
In this setting, some methods (notably local search
run on its own or from a poor starting point) find far
fewer clusters than others (Table 4; log weights not
shown but similar to additive). Since the classifier
abstains for utterances more than 129 seconds apart,
the objective is unaffected if very distant utterances
are linked on the basis of little or no evidence; this
is presumably how such large clusters form. (This
raises the question of whether abstentions should
be given weaker links with p < .5. We leave this
for future work.) Algorithms which find reasonable
numbers of clusters (VOTE, PIVOT, BEST and lo-
cal searches based on these) all achieve good metric
scores, although there is still no reliable way to find
the best solution among this set of methods.
6 Conclusions
It is clear from these results that heuristic methods
can provide good correlation clustering solutions on
datasets far too large for ILP to scale. The particular
solver chosen10 has a substantial impact on the qual-
ity of results obtained, in terms of external metrics
as well as objective value.
For general problems, our recommendation is to
use log weights and run VOTE/BOEM. This algo-
rithm is fast, achieves good objective values, and
yields good metric scores on our datasets. Although
objective values are usually only weakly correlated
with metrics, our results suggest that slightly bet-
ter scores can be obtained by running the algorithm
many times and returning the solution with the best
objective. This may be worth trying even when the
datapoints are inherently ordered, as in chat.
9The F-score metric is not used in Elsner and Charniak
(2008); we compute it ourselves on the result produced by their
software.
10Our C++ correlation clustering software and SDP
bounding package are available for download from
cs.brown.edu/?melsner.
25
Log Weights
Obj 1-1 Loc3 Shen F
SDP bound 13.0% - - -
FIRST/BOEM 19.3% 41 74 44
VOTE/BOEM 20.0% 46 73 50
SA 20.3% 42 73 45
BEST/BOEM 21.3% 43 73 47
BOEM 21.5% 22 72 21
PIVOT/BOEM 22.0% 45 72 50
VOTE 26.3% 44 72 48
BEST 37.1% 40 67 44
PIVOT 44.4% 39 66 44
FIRST 58.3% 39 62 41
Additive Weights
Obj 1-1 Loc3 Shen F
SDP bound 16.2% - - -
FIRST/BOEM 21.7% 40 73 44
BOEM 22.3% 22 73 20
BEST/BOEM 22.7% 44 74 49
VOTE/BOEM 23.3% 46 73 50
SA 23.8% 41 72 46
PIVOT/BOEM 24.8% 46 73 50
VOTE 30.5% 44 71 49
EC ?08 - 41 73 44
BEST 42.1% 43 69 47
PIVOT 48.4% 38 67 44
FIRST 69.0% 40 59 41
Table 3: Score of the solution with best objective found
by each solver on the chat test dataset, averaged over 6
annotations, sorted by objective.
Whatever algorithm is used to provide an initial
solution, we advise the use of local search as a post-
process. BOEM always improves both objective
and metric values over its starting point.
The objective value is not always sufficient to se-
lect a good solution (as in the chat dataset). If pos-
sible, experimenters should check statistics like the
number of clusters found to make sure they conform
roughly to expectations. Algorithms that find far
too many or too few clusters, regardless of objec-
tive, are unlikely to be useful. This type of problem
can be especially dangerous if the pairwise classifier
abstains for many pairs of points.
SDP provides much tighter bounds than the trivial
bound used in previous work, although how much
Num clusters
Max human annotator 128
PIVOT 122
VOTE 99
PIVOT/BOEM 89
VOTE/BOEM 86
Mean human annotator 81
BEST 70
FIRST 70
Elsner and Charniak (2008) 63
BEST/BOEM 62
SA 57
FIRST/BOEM 54
Min human annotator 50
BOEM 7
Table 4: Average number of clusters found (using addi-
tive weights) for chat test data.
1-1 Loc3 Shen F
Log-wt -.40 -.68 -.35
Top 10 % .14 -.15 .15
Add-wt -.31 -.67 -.25
Top 10 % -.07 -.22 .13
Table 5: Kendall?s tau correlation between objective and
metric values for the chat test set, for all solutions and top
10% of solutions.
tighter varies with dataset (about 12 times smaller
for newsgroups, 3 times for chat). This bound can
be used to evaluate the absolute performance of our
solvers; the VOTE/BOEM solver whose use we rec-
ommend is within about 5% of optimality. Some of
this 5% represents the difference between the bound
and optimality; the rest is the difference between the
optimum and the solution found. If the bound were
exactly optimal, we could expect a significant im-
provement on our best results, but not a very large
one?especially since correlation between objective
and metric values grows weaker for the best solu-
tions. While it might be useful to investigate more
sophisticated local searches in an attempt to close
the gap, we do not view this as a priority.
Acknowledgements
We thank Christoph Helmberg, Claire Mathieu and
three reviewers.
26
References
Nir Ailon, Moses Charikar, and Alantha Newman. 2008.
Aggregating inconsistent information: Ranking and
clustering. Journal of the ACM, 55(5):Article No. 23.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learning, 56(1-
3):89?113.
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
HLT-NAACL.
Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. 1999.
Clustering gene expression patterns. Journal of Com-
putational Biology, 6(3-4):281?297.
Michael Bertolacci and Anthony Wirth. 2007. Are
approximation algorithms for consensus clustering
worthwhile? In SDM ?07: Procs. 7th SIAM Inter-
national Conference on Data Mining.
Moses Charikar, Venkatesan Guruswami, and Anthony
Wirth. 2005. Clustering with qualitative information.
J. Comput. Syst. Sci., 71(3):360?383.
William W. Cohen and Jacob Richman. 2002. Learn-
ing to match and cluster large high-dimensional data
sets for data integration. In KDD ?02, pages 475?480.
ACM.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Erik D. Demaine, Dotan Emanuel, Amos Fiat, and Nicole
Immorlica. 2006. Correlation clustering in general
weighted graphs. Theor. Comput. Sci., 361(2):172?
187.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of ACL-08: HLT,
pages 834?842, Columbus, Ohio, June. Association
for Computational Linguistics.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers, pages 45?
48, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Aristides Gionis, Heikki Mannila, and Panayiotis
Tsaparas. 2007. Clustering aggregation. ACM Trans.
on Knowledge Discovery from Data, 1(1):Article 4.
Ioannis Giotis and Venkatesan Guruswami. 2006. Corre-
lation clustering with a fixed number of clusters. The-
ory of Computing, 2(1):249?266.
Andrey Goder and Vladimir Filkov. 2008. Consensus
clustering algorithms: Comparison and refinement. In
ALENEX ?08: Procs. 10th Workshop on Algorithm En-
ginering and Experiments, pages 109?117. SIAM.
Cristoph Helmberg. 2000. Semidefinite programming
for combinatorial optimization. Technical Report ZR-
00-34, Konrad-Zuse-Zentrum fu?r Informationstechnik
Berlin.
Cristoph Helmberg, 2009. The ConicBundle Li-
brary for Convex Optimization. Ver. 0.2i from
http://www-user.tu-chemnitz.de
/?helmberg/ConicBundle/.
Thorsten Joachims and John Hopcroft. 2005. Error
bounds for correlation clustering. In ICML ?05, pages
385?392, New York, NY, USA. ACM.
Thorsten Joachims. 1997. A probabilistic analysis of
the Rocchio algorithm with TFIDF for text categoriza-
tion. In International Conference on Machine Learn-
ing (ICML), pages 143?151.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In ACL.
The Association for Computer Linguistics.
Claire Mathieu and Warren Schudy. 2008.
Correlation clustering with noisy input.
Unpublished manuscript available from
http://www.cs.brown.edu/?ws/papers/clustering.pdf.
Andrew McCallum and Ben Wellner. 2004. Condi-
tional models of identity uncertainty with application
to noun coreference. In Proceedings of the 18th An-
nual Conference on Neural Information Processing
Systems (NIPS), pages 905?912. MIT Press.
Marina Meila. 2007. Comparing clusterings?an infor-
mation based distance. Journal of Multivariate Analy-
sis, 98(5):873?895, May.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42, New York, NY,
USA. ACM.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In Proceedings of NAACL-09 (to
appear).
Shi Zhong and Joydeep Ghosh. 2003. Model-based clus-
tering with soft balancing. In ICDM ?03: Proceedings
of the Third IEEE International Conference on Data
Mining, page 459, Washington, DC, USA. IEEE Com-
puter Society.
27
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 198?199,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Online Statistics for a Unification-Based Dialogue Parser
Micha Elsner, Mary Swift, James Allen, and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
{melsner,swift,allen,gildea}@cs.rochester.edu
Abstract
We describe a method for augmenting
unification-based deep parsing with statis-
tical methods. We extend and adapt the
Bikel parser, which uses head-driven lex-
ical statistics, to dialogue. We show that
our augmented parser produces signifi-
cantly fewer constituents than the baseline
system and achieves comparable brack-
eting accuracy, even yielding slight im-
provements for longer sentences.
1 Introduction
Unification parsers have problems with efficiency
and selecting the best parse. Lexically-conditioned
statistics as used by Collins (1999) may provide a
solution. They have been used in three ways: as
a postprocess for parse selection (Toutanova et al,
2005; Riezler et al, 2000; Riezler et al, 2002), a
preprocess to find more probable bracketing struc-
tures (Swift et al, 2004), and online to rank each
constituent produced, as in Tsuruoka et al (2004)
and this experiment.
The TRIPS parser (Allen et al, 1996) is a unifi-
cation parser using an HPSG-inspired grammar and
hand-tuned weights for each rule. In our augmented
system (Aug-TRIPS), we replaced these weights
with a lexically-conditioned model based on the
adaptation of Collins used by Bikel (2002), allowing
more efficiency and (in some cases) better selection.
Aug-TRIPS retains the same grammar and lexicon
as TRIPS, but uses its statistical model to determine
the order in which unifications are attempted.
2 Experiments
We tested bracketing accuracy on the Monroe cor-
pus (Stent, 2001), which contains collaborative
emergency-management dialogues. Aug-TRIPS is
comparable to TRIPS in accuracy, but produces
fewer constituents (Table 1). The Bikel parser has
slightly higher precision/recall than either TRIPS
or Aug-TRIPS, since it can choose any bracketing
structure regardless of semantic coherence, while
the TRIPS systems must find a legal pattern of fea-
ture unifications. Aug-TRIPS also has better preci-
sion/recall when parsing the longer sentences (Ta-
ble 2).
(training=9282) Bikel Aug-TRIPS TRIPS
Recall 79.40 76.09 76.77
Precision 79.40 77.08 78.20
Complete Match 42.00 46.00 65.00
% Constit. Reduction - 36.96 0.00
Table 1: Bracketing accuracy for 100 random sen-
tences ? 2 words.
> 7 Aug-TRIPS > 7 TRIPS
Recall 73.25 71.00
Precision 74.78 73.44
Complete Match 22.50 37.50
Table 2: Bracketing accuracy for the 40 sentences >
7 words.
Since our motivation for unification parsing is to
reveal semantics as well as syntax, we next evalu-
ated Aug-TRIPS?s production of correct interpreta-
tions at the sentence level, which require complete
correctness not only of the bracketing structure but
of the sense chosen for each word and the thematic
198
roles of each argument (Tetreault et al, 2004).
For this task, we modified the probability model
to condition on the senses in our lexicon rather than
words. For instance, the words ?two thousand dol-
lars? are replaced with the senses ?number number-
unit money-unit?. This allows us to model lexi-
cal disambiguation explicitly. The model generates
one or more senses from each word with probability
P (sense|word, tag), and then uses sense statistics
rather than word statistics in all other calculations.
Similar but more complex models were used in the
PCFG-sem model of Toutanova et al (2005) and us-
ing WordNet senses in Bikel (2000).
We used the Projector dialogues (835 sentences),
which concern purchasing video projectors. In this
domain, Aug-TRIPS makes about 10% more inter-
pretation errors than TRIPS (Table 3), but when
parsing sentences on which TRIPS itself makes er-
rors, it can correct about 10% (Table 4).
(training=310) TRIPS Aug-TRIPS
Correct 26 21
Incorrect 49 54
% Reduction in Constituents 0% 45%
Table 3: Sentence-level accuracy on 75 random sen-
tences.
(training=396) TRIPS Aug-TRIPS
Correct 0 8
Incorrect 54 46
% Reduction in Constituents 0% 46%
Table 4: Sentence-level accuracy on 54 TRIPS error
sentences
Our parser makes substantially fewer constituents
than baseline TRIPS at only slightly lower accu-
racy. Tsuruoka et al (2004) achieved a much higher
speedup (30 times) than we did; this is partly due to
their use of the Penn Treebank, which contains much
more data than our corpora. In addition, however,
their baseline system is a classic HPSG parser with
no efficiency features, while our baseline, TRIPS, is
designed as a real-time dialogue parser which uses
hand-tuned weights to guide its search and imposes
a maximum chart size.
Acknowledgements Our thanks to Will DeBeau-
mont and four anonymous reviewers.
References
James F. Allen, Bradford W. Miller, Eric K. Ringger, and
Teresa Sikorski. 1996. A robust system for natural
spoken dialogue. In Proceedings of the 1996 Annual
Meeting of the Association for Computational Linguis-
tics (ACL?96).
Daniel Bikel. 2000. A statistical model for parsing
and word-sense disambiguation. In Proceedings of
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, Hong Kong.
Daniel Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Human Lan-
guage Technology Conference (HLT), San Diego.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of the 38th Annual
Meeting of the ACL, Hong Kong.
Stefan Riezler, Tracy H. King, Richard Crouch, and
John T. Maxwell. 2002. Parsing the Wall Street Jour-
nal using a Lexical-Functional Grammar and discrim-
inative estimation. In Proceedings of the 40th Annual
Meeting of the ACL, Philadelphia.
Amanda J. Stent. 2001. Dialogue Systems as Conversa-
tional Partners. Ph.D. thesis, University of Rochester.
Mary Swift, James Allen, and Daniel Gildea. 2004.
Skeletons in the parser: Using a shallow parser to im-
prove deep parsing. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), Geneva, Switzerland, August.
Joel Tetreault, Mary Swift, Preethum Prithviraj, My-
roslava Dzikovska, and James Allen. 2004. Discourse
annotation in the Monroe corpus. In ACL workshop on
Discourse Annotation, Barcelona, Spain, July.
Kristina Toutanova, Christopher D. Manning, Dan
Flickinger, and Stephan Oepen. 2005. Stochastic
HPSG parse disambiguation using the Redwoods cor-
pus. Journal of Logic and Computation.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2004. Towards efficient probabilistic HPSG parsing:
Integrating semantic and syntactic preference to guide
the parsing. In Proceedings of IJCNLP-04 Workshop:
Beyond Shallow Analyses- Formalisms and Statistical
Modeling for Deep Analyses, Sanya City, China.
199
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 42?54,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Joint Learning Model of Word Segmentation, Lexical Acquisition,
and Phonetic Variability
Micha Elsner
melsner0@gmail.com
Dept. of Linguistics
The Ohio State University
Sharon Goldwater
sgwater@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Naomi H. Feldman
nhf@umd.edu
Dept. of Linguistics
University of Maryland
Frank Wood
fwood@robots.ox.ac.uk
Dept. of Engineering
University of Oxford
Abstract
We present a cognitive model of early lexi-
cal acquisition which jointly performs word
segmentation and learns an explicit model of
phonetic variation. We define the model as a
Bayesian noisy channel; we sample segmen-
tations and word forms simultaneously from
the posterior, using beam sampling to control
the size of the search space. Compared to a
pipelined approach in which segmentation is
performed first, our model is qualitatively more
similar to human learners. On data with vari-
able pronunciations, the pipelined approach
learns to treat syllables or morphemes as words.
In contrast, our joint model, like infant learners,
tends to learn multiword collocations. We also
conduct analyses of the phonetic variations that
the model learns to accept and its patterns of
word recognition errors, and relate these to de-
velopmental evidence.
1 Introduction
By the end of their first year, infants have acquired
many of the basic elements of their native language.
Their sensitivity to phonetic contrasts has become
language-specific (Werker and Tees, 1984), and they
have begun detecting words in fluent speech (Jusczyk
and Aslin, 1995; Jusczyk et al, 1999) and learn-
ing word meanings (Bergelson and Swingley, 2012).
These developmental cooccurrences lead some re-
searchers to propose that phonetic and word learning
occur jointly, each one informing the other (Swingley,
2009; Feldman et al, 2013). Previous computational
models capture some aspects of this joint learning
problem, but typically simplify the problem consid-
erably, either by assuming an unrealistic degree of
phonetic regularity for word segmentation (Goldwa-
ter et al, 2009) or assuming pre-segmented input
for phonetic and lexical acquisition (Feldman et al,
2009; Feldman et al, in press; Elsner et al, 2012).
This paper presents, to our knowledge, the first broad-
coverage model that learns to segment phonetically
variable input into words, while simultaneously learn-
ing an explicit model of phonetic variation that allows
it to cluster together segmented tokens with different
phonetic realizations (e.g., [ju] and [jI]) into lexical
items (/ju/).
We base our model on the Bayesian word segmen-
tation model of Goldwater et al (2009) (henceforth
GGJ), using a noisy-channel setup where phonetic
variation is introduced by a finite-state transducer
(Neubig et al, 2010; Elsner et al, 2012). This in-
tegrated model allows us to examine how solving
the word segmentation problem should affect infants?
strategies for learning about phonetic variability and
how phonetic learning can allow word segmentation
to proceed in ways that mimic the idealized input
used in previous models.
In particular, although the GGJ model achieves
high segmentation accuracy on phonemic (non-
variable) input and makes errors that are qualitatively
similar to human learners (tending to undersegment
the input), its accuracy drops considerably on phonet-
ically noisy data and it tends to oversegment rather
than undersegment. Here, we demonstrate that when
the model is augmented to account for phonetic vari-
ability, it is able to learn common phonetic changes
42
and by doing so, its accuracy improves and its errors
return to the more human-like undersegmentation
pattern. In addition, we find small improvements
in lexicon accuracy over a pipeline model that seg-
ments first and then performs lexical-phonetic learn-
ing (Elsner et al, 2012). We analyze the model?s
phonetic and lexical representations in detail, draw-
ing comparisons to experimental results on adult and
infant speech processing. Taken together, our results
support the idea that a Bayesian model that jointly
performs word segmentation and phonetic learning
provides a plausible explanation for many aspects of
early phonetic and word learning in infants.
2 Related Work
Nearly all computational models used to explore the
problems addressed here have treated the learning
tasks in isolation. Examples include models of word
segmentation from phonemic input (Christiansen et
al., 1998; Brent, 1999; Venkataraman, 2001; Swing-
ley, 2005) or phonetic input (Fleck, 2008; Rytting,
2007; Daland and Pierrehumbert, 2011; Boruta et
al., 2011), models of phonetic clustering (Vallabha
et al, 2007; Varadarajan et al, 2008; Dupoux et al,
2011) and phonological rule learning (Peperkamp et
al., 2006; Martin et al, 2013).
Elsner et al (2012) present a model that is similar
to ours, using a noisy channel model implemented
with a finite-state transducer to learn about phonetic
variability while clustering distinct tokens into lexi-
cal items. However (like the earlier lexical-phonetic
learning model of Feldman et al (2009; in press))
their model assumes known word boundaries, so
to perform both segmentation and lexical-phonetic
learning, they use a pipeline that first segments using
GGJ and then applies their model to the results.
Neubig et al (2010) also present a transducer-
based noisy channel model that performs joint in-
ference on two out of the three tasks we consider
here; their model assumes fixed probabilities for pho-
netic changes (the noise model) and jointly infers
the word segmentation and lexical items, as in our
?oracle? model below (though unlike our system their
model learns from phone lattices rather than a single
transcription). They evaluate only on phone recogni-
tion, not scoring the inferred lexical items.
Recently, Bo?rschinger et al (2013) did present a
? Geom a, b, ..., ju, ... want, ... juwant, ...
Generator for possible words
Probabilities for each word(sparse)p(?i) = .1, p(a) = .05, p(want) = .01...
? contexts
Conditional probabilitiesfor each word after each wordp(?i | want) = .3, p(a | want) = .1, p(want | want) = .0001...
G
Gx?1
n utterances
x1 x2 ... Intended formsju want ? kukiju want ?t...s1 s2 ...
00
Surface formsj? wan ? kukiju wand ?t...T
GGJ09
Figure 1: The graphical model for our system (Eq. 1-
4). Note that the si are not distinct observations; they
are concatenated together into a continuous sequence of
characters which constitute the observations.
joint learner for segmentation, phonetic learning, and
lexical clustering, but the model and inference are
tailored to investigate word-final /t/-deletion, rather
than aiming for a broad coverage system as we do.
3 Model
We follow several previous models of lexical acquisi-
tion in adopting a Bayesian noisy channel framework
(Eq. 1-4; Fig. 1). The model has two components:
a source distribution P (X) over utterances without
phonetic variability X , i.e., intended forms (Elsner et
al., 2012) and a channel or noise distribution T (S|X)
that translates them into the observed surface forms
S. The boundaries between surface forms are then
deterministically removed so that the actual observa-
tions are just the unsegmented string of characters in
the surface forms.
G0|?0, pstop ? DP (?0, Geom(pstop)) (1)
Gx|G0, ?1 ? DP (?1, G0) (2)
Xi|Xi?1 ? GXi?1 (3)
S|X; ? ? T (S|X; ?) (4)
The source model is an exact copy of GGJ1: to
generate the intended-form word sequences X , we
1We use their best reported parameter values: ?0 =
3000, ?1 = 100, pstop = .2 and for unigrams, ?0 = 20.
43
sample a random language model from a hierarchi-
cal Dirichlet process (Teh et al, 2006) with char-
acter strings as atoms. To do so, we first draw a
unigram distribution G0 from a Dirichlet process
prior whose base distribution generates intended form
word strings by drawing each phone in turn until the
stop character is drawn (with probability pstop). Then,
for each possible context word x, we draw a condi-
tional distribution on words following that context
Gx = P (Xi = ?|Xi?1 = x) using G0 as a prior.
Finally, we sample word sequences x1 . . . xn from
the bigram model.
The channel model is a finite transducer with pa-
rameters ? which independently rewrites single char-
acters from the intended string into characters of the
surface string. We use MAP point estimates of these
parameters; single characters (without n-gram con-
text) are used for computational efficiency. Also for
efficiency, the transducer can insert characters into
the surface string, but cannot delete characters from
the intended string. As in several previous phonolog-
ical models (Dreyer et al, 2008; Hayes and Wilson,
2008), the probabilities are learned using a feature-
based log-linear model. For features, we use all the
unigram features from Elsner et al (2012), which
check faithfulness to voicing, place and manner of
articulation (for example, for k ? g, active features
are faith-manner, faith-place, output-g and voiceless-
to-voiced).
Below, we present two methods for learning the
transducer parameters ?. The oracle transducer is es-
timated using the gold-standard word segmentations
and intended forms for the dataset; it represents the
best possible approximation under our model of the
actual phonetics of the dataset. We can also estimate
the transducer using the EM algorithm. We first ini-
tialize a simple transducer by putting small weights
on the faithfulness features to encourage phonologi-
cally plausible changes. With this initial model, we
begin running the sampler used to learn word segmen-
tations. After several hundred sampler iterations, we
start re-estimating the transducer by maximum likeli-
hood after each iteration. We regularize our estimates
by adding 200 pseudocounts for the rewrite x ? x
during training (rather than regularizing the weights
for particular features). We also show segment only
results for a model without the transducer component
(i.e., S = X); this recovers the GGJ baseline.
4 Inference
Inference for this model is complicated for two rea-
sons. First, the hypothesis space is extremely large.
Since we allow the input string to be probabilistically
lengthened, we cannot be sure how long it is, nor
which characters it contains. Second, our hypothe-
ses about nearby characters are highly correlated due
to lexical effects. When deciding how to interpret
[w@nt], if we posit that the intended vowel is /2/, the
word is likely to be /w2n/ ?one? and the next word
begins with /t/ ; if instead we posit that the vowel
is /O/, the word is probably /wOnt/ ?want?. Thus,
inference methods that change only one character at
a time are unlikely to mix well. Since they cannot
simultaneously change the vowel and resegment the
/t/, they must pass through a low-probability inter-
mediate state to get from one state to the other, so
will tend to get stuck in a bad local minimum. A
Gibbs sampler which inserts or deletes a single seg-
ment boundary in each step (Goldwater et al, 2009)
suffers from this problem.
Mochihashi et al (2009) describe an inference
method with higher mobility: a block sampler for
the GGJ model that samples from the posterior over
analyses of a whole utterance at once. This method
encodes the model as a large HMM, using dynamic
programming to select an analysis. We encode our
own model in the same way, constructing the HMM
and composing it with the transducer (Mohri, 2004)
to form a larger finite-state machine which is still
amenable to forward-backward sampling.
4.1 Finite-state encoding
Following Mochihashi et al (2009) and Neubig et
al. (2010), we can write the original GGJ model
as a Hidden Semi-Markov model. States in the
HMM, written ST:[w][C], are labeled with the
previous word w and the sequence of characters C
which have so far been incorporated into the current
word. To produce a word boundary, we transition
from ST:[w][C] to ST:[C][] with probability
P (xi = C|xi?1 = w). We can also add the next
character s to the current word, transitioning from
ST:[w][C] to ST:[w][C : s], at no cost (since
the full cost of the word is paid at its boundary, there
44
? word j?p(j?|[s])d
j u word ju[s]
word j u word up(j|[s]) p(u|j)
p(ju|[s])j/jd/j ?/u
u/u
u/u
Figure 2: A fragment of the composed finite-state machine
for word segmentation and character replacement for the
surface string ju. The start state [s] is followed by a word
boundary (filled circle); the next intended character is
probably j but can be d or others with lower probability.
After j can be a word boundary (forming the intended
word j), or another character such as u, @ or other (not
shown) alternatives.
is no cost for the individual characters)2.
In addition to analyses using known words, we
can also encode the uniform-geometric prior over
unknown words using a finite-state machine. We
can choose to select a word from the prior by tran-
sitioning to a state ST:[Geom][] with probability
P (new word|xi?1 = w) immediately after a word
boundary. While inGeom, we can transition to a new
Geom state and produce any character with uniform
probability P (c) = (1?Pstop) 1|C| ; otherwise, we can
end the word, transitioning to ST:[unk .word][],
with probability Pstop.
This construction is also approximate; it ignores
the possibility that the prior will generate a known
word w, in which case our final transition ought to
be to ST:[w][] instead of ST:[unk .word][]. This
approximation means we do not need to add context
to the Geom state to remember the sequence of char-
acters it produced, which allows us to keep only a
single Geom state on the chart at each timestep.
When we compose this model with the channel
model, the number of states expands. Each state must
now keep track of the previous word, what intended
charactersC have been posited and what surface char-
acters S have been recognized, ST:[w][C][S].
2Though not mentioned by Mochihashi et al (2009) or Neu-
big et al (2010), this construction is not exact, since transitions
in a Bayesian HMM are exchangeable but not independent (Beal
et al, 2001): if a word occurs twice in an utterance, its probabil-
ity is slightly higher the second time. For single utterances, this
bias is small and easy to correct for using a Metropolis-Hastings
acceptance check (Bo?rschinger and Johnson, 2012) using the
path probability from the HMM as the proposal.
To recognize the current word, we transition to
ST:[C][][] with probability P (xi = C|xi?1 =
w). To parse a new surface character s by positing
intended character x (note that x might be ), we
transition to ST:[w][C : x][S : s] with probabil-
ity T (s|x). (As above, we pay no cost for our choice
of x, which is paid for when we recognize the word;
however, we must pay for s.) For efficiency, we do
not allow the G0 states to hypothesize different sur-
face and intended characters, so when we initially
propose an unknown word, it must surface as itself.3
4.2 Beam sampler
This machine has too many states to fully fill the chart
before backward sampling, so we restrict the set of
trajectories under consideration using beam sampling
(Van Gael et al, 2008) and simulated annealing.
The beam sampler is closely related to the standard
beam search technique, which uses a probability cut-
off to discard parts of the FST which are unlikely to
figure in the eventual solution. Unlike conventional
beam search, the sampler explores using stochastic
cutoffs, so that all trajectories are explored, but most
of the bad ones are explored infrequently, leading to
higher efficiency.
We design our beam sampler to restrict the set
of potential intended characters at each timestep.
In particular, given a stream of input characters
S = s1 . . . sn, we introduce a set of auxiliary cutoff
variables U = u1 . . . un. The ui variables represent
limits on the probability of the emission of surface
character si; we exclude any hypothesized xi whose
probability of generating si, T (si|xi), is less than
ui. To create a beam sampling scheme, we must de-
vise a distribution for U given a state sequence Q (as
discussed above, the sequence of states encodes the
intended character sequence and the segmentation
of the surface string), Pu(U |Q) and then incorporate
the probability of U into the forward messages.
If qi is the state in Q at which si is generated, and
xi the corresponding intended character, we require
that Pu < T (si|xi); that is, the cutoffs must not
exclude any states in the sequence Q. We define Pu
3Again, this approximation is corrected for by the Metropolis-
Hastings step.
45
as a ?-mixture of two distributions:
Pu(u|si, xi) = ?U [0,min(.05, T (si|xi))]+
(1? ?)T (si|xi)Beta(5, 1e? 5)
The former distribution is quite unrestrictive, while
the latter prefers to prune away nearly all the states.
Thus, for most characters in the string, we do not
permit radical changes, while for a fraction, we do.
We follow Huggins and Wood (2013), who ex-
tended Van Gael et al (2008) to the case of a non-
uniform Pu, to define our forward message ? as:
?(qi, i) ? P (qi, S0..i, U0..i) (5)
=
?
qi?1
Pu(ui|si, xi)T (si|xi)?(qi?1, i? 1)
This is the standard HMM forward message, aug-
mented with the probability of u. Since Pu(?|si, xi)
is required to be less than T (si|xi), it will be 0 when-
ever T (si|xi) < u; this is how the u variables func-
tion as cutoffs. In practice, we use the u variables to
filter the lexical items that begin at each position i
in advance, using a simple 0/1 edit distance Markov
model which runs faster than our full model. (For ex-
ample, we can quickly check if the current U allows
want as the intended form for wOlk at i; if not, we can
avoid constructing the prefix ST:[xi?1][wa][wO]
since the continuation will fail.)
The algorithm?s speed depends on the size and
uncertainty of the inferred LM: large numbers of
plausible words mean more states to explore. When
inference starts, and the system is highly uncertain
about word boundaries, it is therefore reasonable to
limit the exploration of the character sequence. We
do so by annealing in two ways: as in Goldwater
et al (2009), we raise P (X) (Eq. 3) to a power t
which increases linearly from .3. To sample from
the posterior, we would want to end with t = 1, but
as in previous noisy-channel models (Elsner et al,
2012; Bahl et al, 1980) we get better results when we
emphasize the LM at the expense of the channel and
so end at t = 2. Meanwhile, as t rises and we explore
fewer implausible lexical sequences, we can explore
the character sequence more. We begin by setting
the ? interpolation parameter of Pu to 0 to minimize
exploration and increase it linearly to .3 (allowing
the system to change about a third of the characters
on each sweep). This is similar to the scheme for
altering Pu in Huggins and Wood (2013).
4.3 Dataset and metrics
We use the corpus released by Elsner et al (2012),
which contains 9790 child-directed English utter-
ances originally from the Bernstein-Ratner corpus
(Bernstein-Ratner, 1987) and later transcribed phone-
mically (Brent, 1999). This standard word segmenta-
tion dataset was modified by Elsner et al (2012) to
include phonetic variation by assigning each token a
pronunciation independently selected from the empir-
ical distribution of pronunciations of that word type
in the closely-transcribed Buckeye Speech Corpus
(Pitt et al, 2007). Following previous work, we hold
out the last 1790 utterances as unseen test data during
development. In the results presented here, we run
the model on all 9790 utterances but score only these
1790. We average results over 5 runs of the model
with different random seeds.
We use standard metrics for segmentation and lex-
icon recovery. For segmentation, we report precision,
recall and F-score for word boundaries (bds), and for
the positions of word tokens in the surface string (srf ;
both boundaries must be correct).
For normalization of the pronunciation variation,
we follow Elsner et al (2012) in measuring how well
the system clusters together variant pronunciations
of the same lexical item, without insisting that the
intended form the system proposes for them match
the one in our corpus. For example, if the system
correctly clusters [ju] and [jI] together but assigns
them the incorrect intended form /jI/, we can still
give credit to this cluster if it is the one that overlaps
best with the gold-standard /ju/ cluster. To compute
these scores, we find the optimal one-to-one map-
ping between our clusters of pronunciations and the
true lexical entries, then report scores for mapped to-
kens (mtk; boundaries and mapping to gold standard
cluster must be correct) and mapped types4 (mlx).
4Elsner et al (2012) calls the mlx metric lexicon F, which
is possibly confusing. We map the clusters to a gold-standard
lexicon (plus potentially some words that don?t correspond to
anything in the gold standard) and compute a type-level F-score
on this lexicon.
46
Prec Rec F-score
Pipeline (segment, then cluster): (Elsner et al, 2012)
Bds 70.4 93.5 80.3
Srf 56.5 69.7 62.4
Mtk 44.2 54.5 48.8
Mlx 48.6 43.1 45.7
Bigram model, segment only
Bds 73.9 (-0.6:0.7) 91.0 (-0.6:0.4) 81.6 (-0.5:0.6)
Srf 60.8 (-0.7:1.1) 70.8 (-0.8:0.9) 65.4 (-0.6:1.0)
Mtk 41.6 (-0.6:1.2) 48.4 (-0.5:1.2) 44.8 (-0.6:1.2)
Mlx 36.6 (-0.7:0.8) 49.8 (-1.0:0.8) 42.2 (-0.9:0.8)
Unigram model, oracle transducer
Bds 81.4 (-0.8:0.4) 72.1 (-0.9:0.8) 76.4 (-0.5:0.7)
Srf 63.6 (-1.0:1.1) 58.5 (-1.2:1.2) 60.9 (-0.9:1.2)
Mtk 46.8 (-1.0:1.1) 43.0 (-1.1:1.2) 44.8 (-1.0:1.2)
Mlx 56.7 (-1.1:1.0) 47.6 (-1.4:0.8) 51.7 (-1.2:0.8)
Bigram model, oracle transducer
Bds 76.1 (-0.6:0.6) 83.8 (-0.9:1.0) 79.8 (-0.8:0.4)
Srf 62.2 (-0.9:1.0) 66.7 (-1.2:1.1) 64.4 (-1.1:0.8)
Mtk 47.2 (-0.7:0.9) 50.6 (-1.0:0.8) 48.8 (-0.8:0.7)
Mlx 40.1 (-1.0:1.2) 43.7 (-0.6:0.7) 41.8 (-0.8:0.6)
Bigram model, EM transducer
Bds 80.1 (-0.5:0.8) 83.0 (-1.4:1.3) 81.5 (-0.5:0.7)
Srf 66.1 (-0.8:1.4) 67.8 (-1.4:1.7) 66.9 (-0.9:1.4)
Mtk 49.0 (-0.9:0.7) 50.3 (-1.1:1.4) 49.6 (-1.0:1.0)
Mlx 43.0 (-1.0:1.4) 49.5 (-1.5:1.1) 46.0 (-1.0:1.3)
Table 1: Mean segmentation (bds, srf ) and normalization
(mtk, mlx) scores on the test set over 5 runs. Parentheses
show min and max scores as differences from the mean.
5 Results and discussion
In the following sections, we analyze how our model
with variability compares to GGJ on noisy data. We
give quantitative scores and also show that qualitative
patterns of errors are often similar to those of human
learners and listeners.
5.1 Clean versus variable input
We begin by evaluating our model as a word seg-
mentation system. (Table 1 gives segmentation and
normalization scores for various models and base-
lines on the 1790 test utterances.) We first confirm
that our inference method is reasonable. The bigram
model without variability (?segment only?) should
have the same segmentation performance as the stan-
dard dpseg implementation of GGJ. This is the case:
dpseg has boundary F of 80.3 and token F of 62.4;
we get 81.6 and 65.4. Thus, our sampler is finding
good solutions, at least for the no-variability model.
We compare segmentation scores between the
?segment only? system and the two bigram models
with transducers (?oracle? and ?EM?). While these
systems all achieve similar segmentation scores, they
do so in different ways. ?Segment only? finds a so-
lution with boundary precision 73.9% and boundary
recall 91.0% for a total F of 81.6%. The low pre-
cision and high recall here indicate a tendency to
oversegment; when the analysis of a given subse-
quence is unclear, the system prefers to chop it into
small chunks. The bigram models which incorporate
transducers score P : 76.1, R: 83.8 (oracle) and P :
80.1,R: 83.0 (EM), indicating that they prefer to find
longer sequences (undersegment) more.
In previous experiments on datasets without varia-
tion, GGJ also has a strong tendency to undersegment
the data (boundary P : 90.1, R: 80.3), which Gold-
water et al argue is rational behavior for an ideal
learner seeking a parsimonious explanation for the
data. Undersegmentation occurs especially when ig-
noring lexical context (a unigram model), but to some
extent even in bigram models. Human learners also
tend to learn collocations as single words (Peters,
1983; Tomasello, 2000), and the GGJ model has been
shown to capture several other effects seen in labora-
tory segmentation tasks (Frank et al, 2010). Together,
these findings support the idea that human learners
may behave in important respects like the Bayesian
ideal learners that Goldwater et al presented.
However, experiments on data with variation have
called these conclusions into question. In particu-
lar, GGJ has previously been shown to oversegment
rather than undersegment as the input grows noisier
(Fleck, 2008), and our results replicate this finding
(oversegmentation for the ?segment only? model).
In addition, the GGJ bigram model, which achieves
much higher segmentation accuracy than the unigram
model on clean data, actually performs worse on very
noisy data (Jansen et al, 2013). Infants are known to
track statistical dependencies across words (Go?mez
and Maye, 2005), so it is worrisome that these de-
pendencies hurt GGJ?s segmentation accuracy when
learning from noisy data.
Our results show that modeling phonetic variabil-
ity reverses the problematic trends described above.
Although the models with phonetic variability show
similar overall segmentation accuracy on noisy data
to the original GGJ model, the pattern of errors
changes, with less oversegmentation and more un-
47
dersegmentation. Thus, their qualitative performance
on variable data resembles GGJ?s on clean data, and
therefore the behavior of human learners.
5.2 Phonetic variability
We next analyze the model?s ability to normalize vari-
ations in the pronunciation of tokens, by inspecting
the mtk score. The ?segment only? baseline is pre-
dictably poor, F : 44.8. The pipeline model scores
48.8, and our oracle transducer model matches this
exactly. The EM transducer scores better, F : 49.6.
Although the confidence intervals overlap slightly,
the EM system also outperforms the pipeline on the
other F -measures; altogether, these results suggest
at least a weak learning synergy (Johnson, 2008) be-
tween segmentation and phonetic learning.
It is interesting that EM can perform better than
the oracle. However, EM is more conservative about
which sound changes it will allow, and thus tends to
avoid mistakes caused by the simplicity of the trans-
ducer model. Since the transducer works segment-
by-segment, it can apply rare contextual variations
out of context. EM benefits from not learning these
variations to begin with.
We can also compare the bigram and unigram ver-
sions of the model. The unigram model is a rea-
sonable segmenter, though not quite as good as the
bigram model, with boundary F of 76.4 and token
F of 60.9 (compared to 79.8 and 64.4 using the bi-
gram model). However, it is not good at normalizing
variation; its mtk score is comparable to the baseline
at 44.8%5. Although bigram context is only moder-
ately effective for telling where words are, the model
seems heavily reliant on lexical context to decide
what words it is hearing.
5.3 Error analysis
To gain more insight into the differing behavior of
our model versus a pipelined system, we inspect the
intended word strings X proposed by each one in
detail. Below, we categorize the kinds of intended
word strings that the model might propose to span a
given gold-standard word token:
Correct Correctly segmented, mapped to the correct
lexical item (e.g., gold intended /ju/, surface
5Elsner et al (2012) show a similar result for a unigram
version of their pipelined system.
EM-learned Segment only
Correct 49.88 47.61
Wrong form 17.96 23.73
Collocation 14.25 7.59
Split 8.26 15.18
One bound 7.11 15.18
Corr. colloc. 1.35 < 0.01
Other 0.75 0.22
Corr. split 0.43 0.66
Table 2: Distribution (%) of error types (see text) in a
single run on the full dataset.
segmentation [ju], intended /ju/)
Wrong form Correctly segmented, mapped to the
wrong lexical item (/ju/, surf. [ju], int. /jEs/)
Colloc Missegmented as part of a sequence whose
boundaries correspond to real word boundaries
(/ju?want/, surf. [juwant], int. /juwant/)
Corr. colloc As above, but proposed lexical item
maps to this word (/ar?ju/, surf. [arj@] int.
/ju/)
Split Missegmented with a word-internal boundary
(/dOgiz/, surf. [dO?giz], int. /dO?giz/)
Corr. split As above, but one proposed word maps
correctly (/dOgi/, surf. [dOg?i], int. /dOgi?@/)
One boundary One boundary correct, the other
wrong (/ju?wa. . ./, surf. [juw], int. /juw/)
Other Not a collocation, both boundaries are wrong
(/du?ju?wa. . ./, surf. [ujuw], int. /ujuw/)
Table 2 shows the distribution over intended word
strings proposed by the ?segment only? baseline and
the EM-learned transducer. Both systems propose
a large number of correct forms, and the most com-
mon error category is ?wrong form? (lexical error
without segmentation error), an error which could
potentially be repaired in a pipeline system. How-
ever, the remaining errors represent segmentation
mistakes which a pipeline could not repair. Here
the two systems behave quite differently. The EM-
learned transducer analyses 14% of real tokens as
parts of multiword collocations like ?doyou?; in an-
other 1.35%, the underlying content word is even
correctly detected. The non-variable system, on the
other hand, analyses 15% of real tokens by splitting
them into pieces. Since infant learners tend to learn
collocations, this supports our analysis that the model
with variation better models human behavior.
48
EM ju: 805, duju: 239, juwan: 88, jI: 58, e~ju: 54, judu:
47, j?: 39, jul2k: 39, Su: 30, u: 23, Zu: 18, j: 17,
je~: 16, tSu: 15, aj:15, Derjugo: 12, dZu: 12
GGJ ju: 498, jI: 280, j@: 165, ji: 119, duju: 106, dujI: 44,
kInju: 39, i: 32, u: 29, kInjI: 29, jul2k: 24, juwan:
23, j: 22, Su: 19, jU: 18, e~ju: 18, I:16, Zu: 15, dZ?u:
13, jE: 12, SI: 11, T?Nkju: 11
Table 3: Forms proposed with frequency > 10 for
gold-standard tokens of ?you? in one sample from EM-
transducer and segment-only (GGJ) system.
To illustrate this behavior anecdotally, we present
the distribution of intended word strings spanning
tokens whose gold intended form is /ju/ ?you? (Table
3). The EM-learned solution proposes 805 tokens
of /ju/, which is the correct analysis6; the ?segment
only? system instead finds varying forms like /jI/,
/j?/ etc. This is unsurprising and could be repaired
by a suitable pipelined system. However, the EM
system also proposes 239 instances of ?doyou?, 88
instances of ?youwant?, 54 instances of ?areyou? and
several other collocations. The ?segment only? sys-
tem finds some of these collocations, split into dif-
ferent versions: for instance 106 instances of /duju/
and 44 of /dujI/. In a pipelined system, we could
combine these variants to find 150 instances? but
this is still 89 instances short of the 239 found when
allowing for variability. The same pattern holds for
?youlike? and ?youwant?. Because the non-variable
system must learn each variant separately, it learns
only the most common instances of these long collo-
cations, and analyzes infrequent variants differently.
We also perform this analysis specifically for
words beginning with vowels. Infants show a delay
in their ability to segment these words from continu-
ous speech (Mattys and Jusczyk, 2001; Nazzi et al,
2005; Seidl and Johnson, 2008), and Seidl and John-
son (2008) suggest a perceptual explanation? initial
vowels can be hard to hear and often exhibit variation
due to coarticulation or resyllabification. Although
our dataset does not contain coarticulation as such, it
should show this pattern of greater variation, which
we hypothesize might lead to difficulty in segmenting
and recognizing vowel-initial words.
The model?s behavior is consistent with this hy-
pothesis (Table 4). Both the ?segment only? and
EM transducer models find approximately the same
6Not all the variants are merged, however. jI, j?, Su etc. are
still occasionally analyzed as separate lexical items.
Segment only Vow. init Cons. init
Correct 47.5 51.7
Wrong form 18.6 15.7
Collocation 14.6 12.2
Split 6.2 10.8
Right bd. corr. 5.8 3.6
Left bd. corr. 4.6 3.8
EM transducer Vow. init Cons. init
Correct 41.5 52.1
Wrong form 20.4 17.3
Collocation 19.2 12.5
Split 5.2 9.1
Right bd. corr. 6.2 2.7
Left bd. corr. 2.7 3.1
Table 4: Most common error types (%; see text) for in-
tended forms beginning with vowels or consonants. Rare
error types are not shown. ?One bound? errors are split up
by which boundary is correct.
proportion of vowel-initial tokens, and both systems
do somewhat better on consonant-initial words than
vowel-initial words. The advantage is stronger for
the transducer model, which gets only 41.5% of
vowel-initial tokens correct as opposed to 52.1% of
consonant-initial words. It proposes more colloca-
tions for vowel-initial words (19.2%) than for conso-
nants (12.5%). In cases where they do not propose a
collocation, both systems are somewhat more likely
to find the right boundary of a vowel-initial token
than the left boundary (although again this difference
is larger for the EM system); this suggests that the
problem is indeed caused by the initial segment.
5.4 Phonetic Learning
We next compare phonetic variations learned by the
model to characteristics of infant speech perception.
Infants show an asymmetry between consonants and
vowels, losing sensitivity to non-native vowel con-
trasts by eight months (Kuhl et al, 1992; Bosch
and Sebastia?n-Galle?s, 2003) but to non-native con-
sonant contrasts only by 10-12 months (Werker and
Tees, 1984). The observed ordering is somewhat
puzzling when one considers the availability for dis-
tributional information (Maye et al, 2002), which is
much stronger for stop consonants than for vowels
(Lisker and Abramson, 1964; Peterson and Barney,
1952). Infants are also conservative in generalizing
across phonetic variability, showing a delayed abil-
49
ity to generalize across talkers, affects, and dialects.
They have difficulty recognizing word tokens that are
spoken by a different talker or in a different tone of
voice until 11 months (Houston and Jusczyk, 2000;
Singh et al, 2004), and the ability to adapt to unfa-
miliar dialects appears to develop even later, between
15 and 19 months (Best et al, 2009; Heugten and
Johnson, in press; White and Aslin, 2011).
Similar to infants, our model shows both a vowel-
consonant asymmetry and a reluctance to accept the
full range of adult phonetic variability. Table 5 shows
some segment-to-segment alternations learned in var-
ious transducers. The oracle learns a large amount
of variation (u surfaces as itself only 68% of the
time) involving many different segments, whereas
EM is similar to infant learners in learning a more
conservative solution with fewer alternations over-
all. Moreover, EM appears to identify patterns of
variability in vowels before consonants. It learns a
similar range of alternations for u as in the oracle,
although it treats the sound as less variable than it
actually is. It learns much less variability for con-
sonants; it picks up the alternation of D with s and
z, but predicts that D will surface as itself 91% of
the time when the true figure is only 69%. And it
fails to learn any meaningful alternations involving
k. These results suggest that patterns of variability in
vowels are more evident than patterns of variability
in consonants when infants are beginning to solve the
word segmentation problem.
To investigate the effect of data size on this con-
servativism, we ran the system on 1000 utterances
instead of 9790. This leads to an even more conser-
vative solution, with variations for u but none of the
others (although i and D still vary more than k).
5.5 Segmentation and recognition errors
A particularly interesting set of errors are those that
involve both a missegmentation and a simultaneous
misrecognition, since the joint model is prone to
such errors while the pipelined model is not. Rel-
atively little is known about infants? misrecognitions
of words in fluent speech, although it is clear that they
find words in medial position harder (Plunkett, 2005;
Seidl and Johnson, 2006). However, adults make
missegmentation/misrecognition errors fairly often,
especially when listening to noisy audio (Butterfield
and Cutler, 1988). Such errors are more common
System x top 4 outputs s
Oracle
u u .68 @ .05 a .04 U .04
i i .85 I .03 @ .03 E .02
D D .69 s .07 [?] .07 z .04
k k .93 d .02 g .02
[?] r .21 h .11 d .01 @ .07
EM
(full)
u u .75 @ .08 I .04 U .03
i i .90 I .04 E .02
D D .91 s .03 z 0.1
k k .98
[?] @ .32 I .14 n .13 t .13
EM
(only
1000
utts)
u u .82 I .04 @ .04 a .02
i i .97
D D .95
k k .99
[?] @ .21 I .18 t .12 s .12
Table 5: Learned phonetic alternations: top 4 outputs s
with p > .001 for inputs x = uw (/u/ ), iy (/i/ ), dh (/D/ ),
k (/k/) and [?], the null character. Outputs from [?] are
insertions. The oracle allows [?] as an output (deletion)
but for computational reasons, the model does not.
when the misrecognized word belongs to a prosod-
ically rare class and when the incorrectly hypothe-
sized string contains frequent words (Cutler, 1990);
phonetically ambiguous words are also more com-
monly recognized as the more frequent of two op-
tions (Connine et al, 1993). For the indefinite article
?a? (often reduced to [@]), lexical context is the main
factor in deciding between ambiguous interpretations
(Kim et al, 2012). In rapid speech, listeners have few
phonetic cues to indicate whether it is present at all
(Dilley and Pitt, 2010). Below, we analyze various
misrecognitions made by our system (using the EM
transducer), and find some similar effects.
The easiest cases to analyze are those with no mis-
segmentation: the proposed boundaries are correct,
and the proposed lexical entry corresponds to a real
word7, but not the correct one. Most of them corre-
spond to homophones (Table 6).
Common cases with a missegmentation include it
and is, a and is, it?s and is, who, who?s and whose,
that?s and what?s, and there and there?s. In general,
these errors involve words which sometimes appear
7The one-to-one mapping can be misleading, as it may map
a large cluster to a real word on the basis of one or two tokens if
all other tokens correspond to a different word already used for
another cluster. We manually filter out a few cases like this.
50
Actual proposed count
/tu/ ?two? /t@/ ?to? 95
/kin/ ?can? /k?nt/ ?can?t? 67
/En/ ?and? /?n/ ?an? 61
/hIz/ ?his? /Iz/ ?is? 57
/D@/ ?the? /@/ ?ah? 51
/w@ts/ ?what?s? /wants/ ?wants? 40
/wan/ ?want? /won/ ?won?t? 39
/yu/ ?you? /y?/ ?yeah? 39
/f@~/ ?for? /fOr/ ?four? 30
/hir/ ?here? /hil/ ?he?ll? 28
Table 6: Top ten errors involving confusion between real,
correctly segmented words: the most common pronunci-
ation of the actual token and its orthographic form, the
same for the proposed token, and the frequency.
with a morpheme or clitic (which can easily be mis-
segmented as part of something else), words which
differ by one segment, and frequent function words
which often appear in similar contexts. These tenden-
cies match those shown by adult human listeners.
A particularly distinctive set of joint recognition
and segmentation errors are those where an entire
real token is treated as phonetic ?noise?? that is, it
is segmented along with an adjacent word, and the
system clusters the whole sequence as a token of
that word. The most common examples are ?that?s a?
identified as ?that?s?, ?have a? identified as ?have?,
?sees a? identified as ?sees? and other examples in-
volving ?a?, a word which also frequently confuses
humans (Kim et al, 2012; Dilley and Pitt, 2010).
However, there are also instances of ?who?s in? as
?who?s?, ?does it? as ?does?, and ?can you? as ?can?.
6 Conclusion
We have presented a model that jointly infers word
segmentation, lexical items, and a model of phonetic
variability; we believe this is the first model to do
so on a broad-coverage naturalistic corpus8. Our re-
sults show a small improvement in both segmentation
and normalization over a pipeline model, providing
evidence for a synergistic interaction between these
learning tasks and supporting claims of interactive
learning from the developmental literature on infants.
We also reproduced several experimental findings;
our results suggest that two vowel-consonant asym-
8Software is available from the ACL archive; updated
versions may be posted at https://bitbucket.org/
melsner/beamseg.
metries, one from the word segmentation literature
and another from the phonetic learning literature, are
linked to the large variability in vowels found in nat-
ural corpora. The model?s correspondence with hu-
man behavioral results is by no means exact, but we
believe these kinds of predictions might help guide
future research on infant phonetic and word learning.
Acknowledgements
Thanks to Mary Beckman for comments. This work
was supported by EPSRC grant EP/H050442/1 to the
second author.
References
Lalit Bahl, Raimo Bakis, Frederick Jelinek, and Robert
Mercer. 1980. Language-model/acoustic-channel-
model balance mechanism. Technical disclosure bul-
letin Vol. 23, No. 7b, IBM, December.
Matthew J. Beal, Zoubin Ghahramani, and Carl Edward
Rasmussen. 2001. The infinite Hidden Markov Model.
In NIPS, pages 577?584.
Elika Bergelson and Daniel Swingley. 2012. At 6-9
months, human infants know the meanings of many
common nouns. Proceedings of the National Academy
of Sciences, 109:3253?3258.
Nan Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Catherine T. Best, Michael D. Tyler, Tiffany N. Good-
ing, Corey B. Orlando, and Chelsea A. Quann. 2009.
Development of phonological constancy: Toddlers? per-
ception of native- and jamaican-accented words. Psy-
chological Science, 20(5):539?542.
Benjamin Bo?rschinger and Mark Johnson. 2012. Using
rejuvenation to improve particle filtering for Bayesian
word segmentation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 85?89, Jeju Island,
Korea, July. Association for Computational Linguistics.
Benjamin Bo?rschinger, Mark Johnson, and Katherine De-
muth. 2013. A joint model of word segmentation
and phonological variation for English word-final /t/-
deletion. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
Luc Boruta, Sharon Peperkamp, Beno??t Crabbe?, and Em-
manuel Dupoux. 2011. Testing the robustness of online
word segmentation: Effects of linguistic diversity and
51
phonetic variation. In Proceedings of the 2nd Workshop
on Cognitive Modeling and Computational Linguistics,
pages 1?9.
Laura Bosch and Nu?ria Sebastia?n-Galle?s. 2003. Simulta-
neous bilingualism and the perception of a language-
specific vowel contrast in the first year of life. Lan-
guage and Speech, 46(2-3):217?243.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105, February.
Sally Butterfield and Anne Cutler. 1988. Segmentation
errors by human listeners: Evidence for a prosodic
segmentation strategy. In Proceedings of SPEECH
?88: Seventh Symposium of the Federation of Acoustic
Societies of Europe, vol. 3, pages 827?833, Edinburgh.
Morten H. Christiansen, Joseph Allen, and Mark S. Sei-
denberg. 1998. Learning to Segment Speech Using
Multiple Cues: A Connectionist Model. Language and
Cognitive Processes, 13(2/3):221?269.
C. M. Connine, D. Titone, and J. Wang. 1993. Audi-
tory word recognition: Extrinsic and intrinsic effects of
word frequency. Journal of Experimental Psychology:
Learning, Memory and Cognition, 19:81?94.
Anne Cutler. 1990. Exploiting prosodic probabilities in
speech segmentation. In G. A. Altmann, editor, Cog-
nitive models of speech processing: Psycholinguistic
and computational perspectives, pages 105?121. MIT
Press, Cambridge, MA.
Robert Daland and Janet B. Pierrehumbert. 2011. Learn-
ing diphone-based segmentation. Cognitive Science,
35(1):119?155.
Laura C. Dilley and Mark Pitt. 2010. Altering context
speech rate can cause words to appear or disappear.
Psychological Science, 21(11):1664?1670.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 1080?1089, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Emmanuel Dupoux, Guillaume Beraud-Sudreau, and
Shigeki Sagayama. 2011. Templatic features for mod-
eling phoneme acquisition. In Proceedings of the 33rd
Annual Cognitive Science Society.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and pho-
netic acquisition. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 184?193, Jeju
Island, Korea, July. Association for Computational Lin-
guistics.
Naomi Feldman, Thomas Griffiths, and James Morgan.
2009. Learning phonetic categories by learning a lexi-
con. In Proceedings of the 31st Annual Conference of
the Cognitive Science Society.
Naomi H. Feldman, Emily B. Myers, Katherine S. White,
Thomas L. Griffiths, and James L. Morgan. 2013.
Word-level information influences phonetic learning
in adults and infants. Cognition, 127(3):427?438.
Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-
water, and James L. Morgan. in press. A role for the
developing lexicon in phonetic category acquisition.
Psychological Review.
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of ACL-08: HLT, pages
130?138, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Michael C. Frank, Sharon Goldwater, Thomas L. Griffiths,
and Joshua B. Tenenbaum. 2010. Modeling human per-
formance in statistical word segmentation. Cognition,
117(2):107?125.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Rebecca Go?mez and Jessica Maye. 2005. The develop-
mental trajectory of nonadjacent dependency learning.
Infancy, 7:183?206.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learning.
Linguistic Inquiry, 39(3):379?440.
Marieke van Heugten and Elizabeth K. Johnson. in press.
Learning to contend with accents in infancy: Benefits
of brief speaker exposure. Journal of Experimental
Psychology: General.
Derek M. Houston and Peter W. Jusczyk. 2000. The role
of talker-specific information in word segmentation by
infants. Journal of Experimental Psychology: Human
Perception and Performance, 26:1570?1582.
Jonathan Huggins and Frank Wood. 2013. Infinite struc-
tured hidden semi-Markov models. Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), to
appear, September.
Aren Jansen, Emmanuel Dupoux, Sharon Goldwater,
Mark Johnson, Sanjeev Khudanpur, Kenneth Church,
Naomi Feldman, Hynek Hermansky, Florian Metze,
Richard Rose, Mike Seltzer, Pascal Clark, Ian McGraw,
Balakrishnan Varadarajan, Erin Bennett, Benjamin
Borschinger, Justin Chiu, Ewan Dunbar, Abdellah Four-
tassi, David Harwath, Chia-ying Lee, Keith Levin,
Atta Norouzian, Vijay Peddinti, Rachael Richardson,
Thomas Schatz, and Samuel Thomas. 2013. A sum-
mary of the 2012 JHU CLSP workshop on zero re-
source speech technologies and early language acqui-
sition. Proceedings of the IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing.
52
Mark Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguis-
tic structure. In Proceedings of ACL-08: HLT, pages
398?406, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Peter W. Jusczyk and Richard N. Aslin. 1995. Infants? de-
tection of the sound patterns of words in fluent speech.
Cognitive Psychology, 29:1?23.
Peter W. Jusczyk, Derek M. Houston, and Mary Newsome.
1999. The beginnings of word segmentation in English-
learning infants. Cognitive Psychology, 39:159?207.
Dahee Kim, Joseph D.W. Stephens, and Mark A. Pitt.
2012. How does context play a part in splitting words
apart? Production and perception of word boundaries
in casual speech. Journal of Memory and Language,
66(4):509 ? 529.
Patricia K. Kuhl, Karen A. Williams, Francisco Lacerda,
Kenneth N. Stevens, and Bjorn Lindblom. 1992. Lin-
guistic experience alters phonetic perception in infants
by 6 months of age. Science, 255(5044):606?608.
Leigh Lisker and Arthur S. Abramson. 1964. A cross-
language study of voicing in initial stops: Acoustical
measurements. Word, 20:384?422.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37:103?124.
Sven L. Mattys and Peter W. Jusczyk. 2001. Do infants
segment words or recurring contiguous patterns? Jour-
nal of Experimental Psychology: Human Perception
and Performance, 27(3):644?655+.
Jessica Maye, Janet F. Werker, and LouAnn Gerken. 2002.
Infant sensitivity to distributional information can affect
phonetic discrimination. Cognition, 82(3):B101?11.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
100?108, Suntec, Singapore, August. Association for
Computational Linguistics.
Mehryar Mohri, 2004. Weighted Finite-State Transducer
Algorithms: An Overview, chapter 29, pages 551?564.
Physica-Verlag.
Thierry Nazzi, Laura C. Dilley, Ann Marie Jusczyk, Ste-
fanie Shattuck-Hufnagel, and Peter W. Jusczyk. 2005.
English-learning infants? segmentation of verbs from
fluent speech. Language and Speech, 48(3):279?298+.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language model
from continuous speech. In 11th Annual Conference
of the International Speech Communication Associa-
tion (InterSpeech 2010), pages 1053?1056, Makuhari,
Japan, 9.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre Nadal,
and Emmanuel Dupoux. 2006. The acquisition of
allophonic rules: Statistical learning with linguistic
constraints. Cognition, 101(3):B31?B41.
Ann M. Peters. 1983. The Units of Language Acquisi-
tion. Cambridge Monographs and Texts in Applied
Psycholinguistics. Cambridge University Press.
Gordon E. Peterson and Harold L. Barney. 1952. Control
methods used in a study of the vowels. Journal of the
Acoustical Society of America, 24(2):175?184.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech (2nd release).
Kim Plunkett. 2005. Learning how to be flexible with
words. Attention and Performance, XXI:233?248.
Anton Rytting. 2007. Preserving Subsegmental Varia-
tion in Modeling Word Segmentation (Or, the Raising
of Baby Mondegreen). Ph.D. thesis, The Ohio State
University.
Amanda Seidl and Elizabeth Johnson. 2006. Infant word
segmentation revisited: Edge alignment facilitates tar-
get extraction. Developmental Science, 9:565?573.
Amanda Seidl and Elizabeth Johnson. 2008. Perceptual
factors influence infants? extraction of onsetless words
from continuous speech. Journal of Child Language,
34.
Leher Singh, James Morgan, and Katherine White. 2004.
Preference and processing: The role of speech affect
in early spoken word recognition. Journal of Memory
and Language, 51:173?189.
Daniel Swingley. 2005. Statistical clustering and the con-
tents of the infant vocabulary. Cognitive Psychology,
50:86?132.
Daniel Swingley. 2009. Contributions of infant word
learning to language development. Philosophical
Transactions of the Royal Society B: Biological Sci-
ences, 364(1536):3617?3632, December.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101(476):1566?1581.
Michael Tomasello. 2000. The item-based nature of chil-
dren?s early syntactic development. Trends in Cognitive
Sciences, 4(4):156 ? 163.
Gautam K. Vallabha, James L. McClelland, Ferran Pons,
Janet F. Werker, and Shigeaki Amano. 2007. Unsuper-
vised learning of vowel categories from infant-directed
speech. Proceedings of the National Academy of Sci-
ences, 104(33):13273?13278.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite Hidden Markov model. In Proceedings of the
25th International Conference on Machine learning,
53
ICML ?08, pages 1088?1095, New York, NY, USA.
ACM.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of the As-
sociation for Computational Linguistics: Short Papers,
pages 165?168.
Anand Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for perceptual
reorganization during the first year of life. Infant Be-
havior and Development, 7(1):49 ? 63.
Katherine S. White and Richard N. Aslin. 2011. Adap-
tation to novel accents by toddlers. Developmental
Science, 14(2):372?384.
54
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 634?644,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Character-based Kernels for Novelistic Plot Structure
Micha Elsner
Institute for Language, Cognition and Computation (ILCC)
School of Informatics
University of Edinburgh
melsner0@gmail.com
Abstract
Better representations of plot structure
could greatly improve computational meth-
ods for summarizing and generating sto-
ries. Current representations lack abstrac-
tion, focusing too closely on events. We
present a kernel for comparing novelistic
plots at a higher level, in terms of the
cast of characters they depict and the so-
cial relationships between them. Our kernel
compares the characters of different nov-
els to one another by measuring their fre-
quency of occurrence over time and the
descriptive and emotional language associ-
ated with them. Given a corpus of 19th-
century novels as training data, our method
can accurately distinguish held-out novels
in their original form from artificially dis-
ordered or reversed surrogates, demonstrat-
ing its ability to robustly represent impor-
tant aspects of plot structure.
1 Introduction
Every culture has stories, and storytelling is one
of the key functions of human language. Yet while
we have robust, flexible models for the structure
of informative documents (for instance (Chen et
al., 2009; Abu Jbara and Radev, 2011)), current
approaches have difficulty representing the nar-
rative structure of fictional stories. This causes
problems for any task requiring us to model
fiction, including summarization and generation
of stories; Kazantseva and Szpakowicz (2010)
show that state-of-the-art summarizers perform
extremely poorly on short fictional texts1. A ma-
jor problem with applying models for informative
1Apart from Kazantseva, we know of one other at-
tempt to apply a modern summarizer to fiction, by the
artist Jason Huff, using Microsoft Word 2008?s extrac-
tive summary feature: http://jason-huff.com/
text to fiction is that the most important struc-
ture underlying the narrative?its plot?occurs at
a high level of abstraction, while the actual narra-
tion is of a series of lower-level events.
A short synopsis of Jane Austen?s novel Pride
and Prejudice, for example, is that Elizabeth Ben-
net first thinks Mr. Darcy is arrogant, but later
grows to love him. But this is not stated straight-
forwardly in the text; the reader must infer it from
the behavior of the characters as they participate
in various everyday scenes.
In this paper, we present the plot kernel, a
coarse-grained, but robust representation of nov-
elistic plot structure. The kernel evaluates the
similarity between two novels in terms of the
characters and their relationships, constructing
functional analogies between them. These are in-
tended to correspond to the labelings produced by
human literary critics when they write, for exam-
ple, that Elizabeth Bennet and Emma Woodhouse
are protagonists of their respective novels. By fo-
cusing on which characters and relationships are
important, rather than specifically how they inter-
act, our system can abstract away from events and
focus on more easily-captured notions of what
makes a good story.
The ability to find correspondences between
characters is key to eventually summarizing or
even generating interesting stories. Once we can
effectively model the kinds of people a romance
or an adventure story is usually about, and what
kind of relationships should exist between them,
we can begin trying to analyze new texts by com-
parison with familiar ones. In this work, we eval-
uate our system on the comparatively easy task
projects/autosummarize. Although this cannot be
treated as a scientific experiment, the results are unusably
bad; they consist mostly of short exclamations containing
the names of major characters.
634
of recognizing acceptable novels (section 6), but
recognition is usually a good first step toward
generation?a recognition model can always be
used as part of a generate-and-rank pipeline, and
potentially its underlying representation can be
used in more sophisticated ways. We show a de-
tailed analysis of the character correspondences
discovered by our system, and discuss their po-
tential relevance to summarization, in section 9.
2 Related work
Some recent work on story understanding has fo-
cused on directly modeling the series of events
that occur in the narrative. McIntyre and Lapata
(2010) create a story generation system that draws
on earlier work on narrative schemas (Chambers
and Jurafsky, 2009). Their system ensures that
generated stories contain plausible event-to-event
transitions and are coherent. Since it focuses only
on events, however, it cannot enforce a global no-
tion of what the characters want or how they relate
to one another.
Our own work draws on representations that
explicitly model emotions rather than events. Alm
and Sproat (2005) were the first to describe sto-
ries in terms of an emotional trajectory. They an-
notate emotional states in 22 Grimms? fairy tales
and discover an increase in emotion (mostly posi-
tive) toward the ends of stories. They later use this
corpus to construct a reasonably accurate clas-
sifier for emotional states of sentences (Alm et
al., 2005). Volkova et al(2010) extend the hu-
man annotation approach using a larger number of
emotion categories and applying them to freely-
defined chunks instead of sentences. The largest-
scale emotional analysis is performed by Moham-
mad (2011), using crowd-sourcing to construct a
large emotional lexicon with which he analyzes
adult texts such as plays and novels. In this work,
we adopt the concept of emotional trajectory, but
apply it to particular characters rather than works
as a whole.
In focusing on characters, we follow Elson et
al. (2010), who analyze narratives by examining
their social network relationships. They use an
automatic method based on quoted speech to find
social links between characters in 19th century
novels. Their work, designed for computational
literary criticism, does not extract any temporal
or emotional structure.
A few projects attempt to represent story struc-
ture in terms of both characters and their emo-
tional states. However, they operate at a very de-
tailed level and so can be applied only to short
texts. Scheherazade (Elson and McKeown, 2010)
allows human annotators to mark character goals
and emotional states in a narrative, and indicate
the causal links between them. AESOP (Goyal et
al., 2010) attempts to learn a similar structure au-
tomatically. AESOP?s accuracy, however, is rel-
atively poor even on short fables, indicating that
this fine-grained approach is unlikely to be scal-
able to novel-length texts; our system relies on a
much coarser analysis.
Kazantseva and Szpakowicz (2010) summarize
short stories, although unlike the other projects
we discuss here, they explicitly try to avoid giving
away plot details?their goal is to create ?spoiler-
free? summaries focusing on characters, settings
and themes, in order to attract potential readers.
They do find it useful to detect character men-
tions, and also use features based on verb aspect to
automatically exclude plot events while retaining
descriptive passages. They compare their genre-
specific system with a few state-of-the-art meth-
ods for summarizing news, and find it outper-
forms them substantially.
We evaluate our system by comparing real nov-
els to artificially produced surrogates, a procedure
previously used to evaluate models of discourse
coherence (Karamanis et al 2004; Barzilay and
Lapata, 2005) and models of syntax (Post, 2011).
As in these settings, we anticipate that perfor-
mance on this kind of task will be correlated with
performance in applied settings, so we use it as an
easier preliminary test of our capabilities.
3 Dataset
We focus on the 19th century novel, partly fol-
lowing Elson et al(2010) and partly because
these texts are freely available via Project Guten-
berg. Our main dataset is composed of romances
(which we loosely define as novels focusing on a
courtship or love affair). We select 41 texts, tak-
ing 11 as a development set and the remaining
30 as a test set; a complete list is given in Ap-
pendix A. We focus on the novels used in Elson
et al(2010), but in some cases add additional ro-
mances by an already-included author. We also
selected 10 of the least romantic works as an out-
of-domain set; experiments on these are in section
8.
635
4 Preprocessing
In order to compare two texts, we must first ex-
tract the characters in each and some features of
their relationships with one another. Our first step
is to split the text into chapters, and each chapter
into paragraphs; if the text contains a running di-
alogue where each line begins with a quotation
mark, we append it to the previous paragraph.
We segment each paragraph with MXTerminator
(Reynar and Ratnaparkhi, 1997) and parse it with
the self-trained Charniak parser (McClosky et al
2006). Next, we extract a list of characters, com-
pute dependency tree-based unigram features for
each character, and record character frequencies
and relationships over time.
4.1 Identifying characters
We create a list of possible character references
for each work by extracting all strings of proper
nouns (as detected by the parser), then discarding
those which occur less than 5 times. Grouping
these into a useful character list is a problem of
cross-document coreference.
Although cross-document coreference has been
extensively studied (Bhattacharya and Getoor,
2005) and modern systems can achieve quite high
accuracy on the TAC-KBP task, where the list
of available entities is given in advance (Dredze
et al 2010), novelistic text poses a significant
challenge for the methods normally used. The
typical 19th-century novel contains many related
characters, often named after one another. There
are complicated social conventions determining
which titles are used for whom?for instance,
the eldest unmarried daughter of a family can be
called ?Miss Bennet?, while her younger sister
must be ?Miss Elizabeth Bennet?. And characters
often use nicknames, such as ?Lizzie?.
Our system uses the multi-stage clustering
approach outlined in Bhattacharya and Getoor
(2005), but with some features specific to 19th
century European names. To begin, we merge all
identical mentions which contain more than two
words (leaving bare first or last names unmerged).
Next, we heuristically assign each mention a gen-
der (masculine, feminine or neuter) using a list of
gendered titles, then a list of male and female first
names2. We then merge mentions where each is
longer than one word, the genders do not clash,
2The most frequent names from the 1990 US census.
reply left-of-[name] 17
right-of-[name] feel 14
right-of-[name] look 10
right-of-[name] mind 7
right-of-[name] make 7
Table 1: Top five stemmed unigram dependency fea-
tures for ?Miss Elizabeth Bennet?, protagonist of
Pride and Prejudice, and their frequencies.
and the first and last names are consistent (Char-
niak, 2001). We then merge single-word mentions
with matching multiword mentions if they appear
in the same paragraph, or if not, with the multi-
word mention that occurs in the most paragraphs.
When this process ends, we have resolved each
mention in the novel to some specific character.
As in previous work, we discard very infrequent
characters and their mentions.
For the reasons stated, this method is error-
prone. Our intuition is that the simpler method
described in Elson et al(2010), which merges
each mention to the most recent possible coref-
erent, must be even more so. However, due to
the expense of annotation, we make no attempt to
compare these methods directly.
4.2 Unigram character features
Once we have obtained the character list, we use
the dependency relationships extracted from our
parse trees to compute features for each charac-
ter. Similar feature sets are used in previous work
in word classification, such as (Lin and Pantel,
2001). A few example features are shown in Table
1.
To find the features, we take each mention in
the corpus and count up all the words outside the
mention which depend on the mention head, ex-
cept proper nouns and stop words. We also count
the mention?s own head word, and mark whether
it appears to the right or the left (in general, this
word is a verb and the direction reflects the men-
tion?s role as subject or object). We lemmatize
all feature words with the WordNet (Miller et al
1990) stemmer. The resulting distribution over
words is our set of unigram features for the char-
acter. (We do not prune rare features, although
they have proportionally little influence on our
measurement of similarity.)
636
0 10 20 30 40 500.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Freq of Miss Elizabeth Bennet
Emotions of Miss Elizabeth Bennet
Cross freq x Mr. Darcy
Figure 1: Normalized frequency and emotions associated with ?Miss Elizabeth Bennet?, protagonist of Pride
and Prejudice, and frequency of paragraphs about her and ?Mr. Darcy?, smoothed and projected onto 50 basis
points.
4.3 Temporal relationships
We record two time-varying features for each
character, each taking one value per chapter. The
first is the character?s frequency as a proportion
of all character mentions in the chapter. The sec-
ond is the frequency with which the character is
associated with emotional language?their emo-
tional trajectory (Alm et al 2005). We use the
strong subjectivity cues from the lexicon of Wil-
son et al(2005) as a measurement of emotion.
If, in a particular paragraph, only one character
is mentioned, we count all emotional words in
that paragraph and add them to the character?s
total. To render the numbers comparable across
works, each paragraph subtotal is normalized by
the amount of emotional language in the novel as
a whole. Then the chapter score is the average
over paragraphs.
For pairwise character relationships, we count
the number of paragraphs in which only two char-
acters are mentioned, and treat this number (as a
proportion of the total) as a measurement of the
strength of the relationship between that pair3. El-
son et al(2010) show that their method of find-
ing conversations between characters is more pre-
cise in showing whether a relationship exists, but
the co-occurrence technique is simpler, and we
3We tried also counting emotional language in these para-
graphs, but this did not seem to help in development experi-
ments.
care mostly about the strength of key relationships
rather than the existence of infrequent ones.
Finally, we perform some smoothing, by taking
a weighted moving average of each feature value
with a window of the three values on either side.
Then, in order to make it easy to compare books
with different numbers of chapters, we linearly in-
terpolate each series of points into a curve and
project it onto a fixed basis of 50 evenly spaced
points. An example of the final output is shown in
Figure 1.
5 Kernels
Our plot kernel k(x, y) measures the similarity
between two novels x and y in terms of the fea-
tures computed above. It takes the form of a
convolution kernel (Haussler, 1999) where the
?parts? of each novel are its characters u ? x,
v ? y and c is a kernel over characters:
k(x, y) =
?
u?x
?
v?y
c(u, v) (1)
We begin by constructing a first-order ker-
nel over characters, c1(u, v), which is defined in
terms of a kernel d over the unigram features and
a kernel e over the single-character temporal fea-
tures. We represent the unigram feature counts as
distributions pu(w) and pv(w), and compute their
similarity as the amount of shared mass, times a
small penalty of .1 for mismatched genders:
637
d(pu, pv) = exp(??(1?
?
wmin(pu(w), pv(w))))
?.1 I{genu = genv}
We compute similarity between a pair of time-
varying curves (which are projected onto 50
evenly spaced points) using standard cosine dis-
tance, which approximates the normalized inte-
gral of their product.
e(u, v) =
(
u ? v
?
?u??v?
)?
(2)
The weights ? and ? are parameters of the sys-
tem, which scale d and e so that they are compa-
rable to one another, and also determine how fast
the similarity scales up as the feature sets grow
closer; we set them to 5 and 10 respectively.
We sum together the similarities of the char-
acter frequency and emotion curves to measure
overall temporal similarity between the charac-
ters. Thus our first-order character kernel c1 is:
c1(u, v) = d(pu, pv)(e(ufreq, vfreq)+e(uemo, vemo))
We use c1 and equation 1 to construct a first-
order plot kernel (which we call k1), and also as
an ingredient in a second-order character kernel
c2 which takes into account the curve of pairwise
frequencies u?, u? between two characters u and u?
in the same novel.
c2(u, v) = c1(u, v)
?
u??x
?
v??y
e(u?, u?, v?, v?)c1(u
?, v?)
In other words, u is similar to v if, for some
relationships of u with other characters u?, there
are similar characters v? who serves the same role
for v. We use c2 and equation 1 to construct our
full plot kernel k2.
5.1 Sentiment-only baseline
In addition to our plot kernel systems, we imple-
ment a simple baseline intended to test the effec-
tiveness of tracking the emotional trajectory of
the novel without using character identities. We
give our baseline access to the same subjectiv-
ity lexicon used for our temporal features. We
compute the number of emotional words used in
each chapter (regardless of which characters they
co-occur with), smoothed and normalized as de-
scribed in subsection 4.3. This produces a single
time-varying curve for each novel, representing
the average emotional intensity of each chapter.
We use our curve kernel e (equation 2) to mea-
sure similarity between novels.
6 Experiments
We evaluate our kernels on their ability to distin-
guish between real novels from our dataset and
artificial surrogate novels of three types. First, we
alter the order of a real novel by permuting its
chapters before computing features. We construct
one uniformally-random permutation for each test
novel. Second, we change the identities of the
characters by reassigning the temporal features
for the different characters uniformally at random
while leaving the unigram features unaltered. (For
example, we might assign the frequency, emotion
and relationship curves for ?Mr. Collins? to ?Miss
Elizabeth Bennet? instead.) Again, we produce
one test instance of this type for each test novel.
Third, we experiment with a more difficult order-
ing task by taking the chapters in reverse.
In each case, we use our kernel to perform
a ranking task, deciding whether k(x, y) >
k(x, yperm). Since this is a binary forced-choice
classification, a random baseline would score
50%. We evaluate performance in the case where
we are given only a single training document x,
and for a whole training set X , in which case we
combine the decisions using a weighted nearest
neighbor (WNN) strategy:
?
x?X
k(x, y) >
?
x?X
k(x, yperm)
In each case, we perform the experiment in
a leave-one-out fashion; we include the 11 de-
velopment documents in X , but not in the test
set. Thus there are 1200 single-document compar-
isons and 30 with WNN. The results of our three
systems (the baseline, the first-order kernel k1 and
the second-order kernel k2) are shown in Table
2. (The sentiment-only baseline has no character-
specific features, and so cannot perform the char-
acter task.)
Using the full dataset and second-order kernel
k2, our system?s performance on these tasks is
quite good; we are correct 90% of the time for
order and character examples, and 67% for the
638
order character reverse
sentiment only 46.2 - 51.5
single doc k1 59.5 63.7 50.7
single doc k2 61.8 67.7 51.6
WNN sentiment 50 - 53
WNN k1 77 90 63
WNN k2 90 90 67
Table 2: Accuracy of kernels ranking 30 real novels
against artificial surrogates (chance accuracy 50%).
more difficult reverse cases. Results of this qual-
ity rely heavily on the WNN strategy, which trusts
close neighbors more than distant ones.
In the single training point setup, the system
is much less accurate. In this setting, the sys-
tem is forced to make decisions for all pairs of
texts independently, including pairs it considers
very dissimilar because it has failed to find any
useful correspondences. Performance for these
pairs is close to chance, dragging down overall
scores (52% for reverse) even if the system per-
forms well on pairs where it finds good correspon-
dences, enabling a higher WNN score (67%).
The reverse case is significantly harder than
order. This is because randomly permuting a
novel actually breaks up the temporal continuity
of the text?for instance, a minor character who
appeared in three adjacent chapters might now ap-
pear in three separate places. Reversing the text
does not cause this kind of disruption, so correctly
detecting a reversal requires the system to repre-
sent patterns with a distinct temporal orientation,
for instance an intensification in the main char-
acter?s emotions, or in the number of paragraphs
focusing on pairwise relationships, toward the end
of the text.
The baseline system is ineffective at detecting
either ordering or reversals4. The first-order ker-
nel k1 is as good as k2 in detecting character per-
mutations, but less effective on reorderings and
reversals. As we will show in section 9, k1 places
more emphasis on correspondences between mi-
nor characters and between places, while k2 is
more sensitive to protagonists and their relation-
ships, which carry the richest temporal informa-
4The baseline detects reversals as well as the plot kernels
given only a single point of comparison, but these results do
not transfer to the WNN strategy. This suggests that unlike
the plot kernels, the baseline is no more accurate for docu-
ments it considers similar than for those it judges are distant.
tion.
7 Significance testing
In addition to using our kernel as a classifier, we
can directly test its ability to distinguish real from
altered novels via a non-parametric two-sample
significance test, the Maximum Mean Discrep-
ancy (MMD) test (Gretton et al 2007). Given
samples from a pair of distributions p and q and
a kernel k, this test determines whether the null
hypothesis that p and q are identically distributed
in the kernel?s feature space can be rejected. The
advantage of this test is that, since it takes all
pairwise comparisons (except self-comparisons)
within and across the classes into account, it uses
more information than our classification experi-
ments, and can therefore be more sensitive.
As in Gretton et al(2007), we find an unbiased
estimate of the test statistic MMD2 for sample
sets x ? p, y ? q, each with m samples, by pair-
ing the two as z = (xi, yi) and computing:
MMD2(x, y) =
1
(m)(m? 1)
m?
i 6=j
h(zi, zj)
h(zi, zj) = k(xi, xj)+k(yi, yj)?k(xi, yj)?k(xj , yi)
Intuitively, MMD2 approaches 0 if the ker-
nel cannot distinguish x from y and is positive
otherwise. The null distribution is computed by
the bootstrap method; we create null-distributed
samples by randomly swapping xi and yi in ele-
ments of z and computing the test statistic. We
use 10000 test permutations. Using both k1 and
k2, we can reject the null hypothesis that the dis-
tribution of novels is equal to order or characters
with p < .001; for reversals, we cannot reject the
null hypothesis.
8 Out-of-domain data
In our main experiments, we tested our kernel
only on romances; here we investigate its ability
to generalize across genres. We take as our train-
ing set X the same romances as above, but as our
test set Y a disjoint set of novels focusing mainly
on crime, children and the supernatural.
Our results (Table 3) are not appreciably differ-
ent from those of the in-domain experiments (Ta-
ble 2) considering the small size of the dataset.
This shows our system to be robust, but shallow;
639
order character reverse
sentiment only 33.0 - 53.4
single doc k1 59.5 61.7 52.7
single doc k2 63.7 62.0 57.3
WNN sentiment 20 - 70
WNN k1 80 90 80
WNN k2 100 80 70
Table 3: Accuracy of kernels ranking 10 non-romance
novels against artificial surrogates, with 41 romances
used for comparison.
the patterns it can represent generalize acceptably
across domains, but this suggests it is describing
broad concepts like ?main character? rather than
genre-specific ones like ?female romantic lead?.
9 Character-level analysis
To gain some insight into exactly what kinds of
similarities the system picks up on when compar-
ing two works, we sorted the characters detected
by our system into categories and measured their
contribution to the kernel?s overall scores. We
selected four Jane Austen works from the devel-
opment set5 and hand-categorized each character
detected by our system. (We performed the cate-
gorization based on the most common full name
mention in each cluster. This name is usually a
good identifier for all the mentions in the cluster,
but if our coreference system has made an error, it
may not be.)
Our categorization for characters is intended to
capture the stereotypical plot dynamics of liter-
ary romance, sorting the characters according to
their gender and a simple notion of their plot func-
tion. The genders are female, male, plural (?the
Crawfords?) or not a character (?London?). The
functional classes are protagonist (used for the
female viewpoint character and her eventual hus-
band), marriageable (single men and women
who are seeking to marry within the story) and
other (older characters, children, and characters
married before the story begins).
We evaluate the pairwise kernel similarities
among our four works, and add up the propor-
tional contribution made by character pairs of
each type to the eventual score. (For instance,
the similarity between ?Elizabeth Bennet? and
5Pride and Prejudice, Emma, Mansfield Park and Per-
suasion.
?Emma Woodhouse?, both labeled ?female pro-
tagonist?, contributes 26% of the kernel similarity
between the works in which they appear.) We plot
these as Hinton-style diagrams in Figure 2. The
size of each black rectangle indicates the magni-
tude of the contribution. (Since kernel functions
are symmetric, we show only the lower diagonal.)
Under the kernel for unigram features, d
(top), the most common character types?non-
characters (almost always places) and non-
marriageable women?contribute most to the ker-
nel scores; this is especially true for places, since
they often occur with similar descriptive terms.
The diagram also shows the effect of the kernel?s
penalty for gender mismatches, since females pair
more strongly with females and males with males.
Character roles have relatively little impact.
The first-order kernel c1 (middle), which takes
into account frequency and emotion as well as un-
igrams, is much better than d at distinguishing
places from real characters, and assigns somewhat
more weight to protagonists.
Finally, c2 (bottom), which takes into account
second-order relationships, places much more
emphasis on female protagonists and much less
on places. This is presumably because the female
protagonists of Jane Austen?s novels are the view-
point characters, and the novels focus on their re-
lationships, while characters do not tend to have
strong relationships with places. An increased
tendency to match male marriageable characters
with marriageable females, and ?other? males
with ?other? females, suggests that c2 relies more
on character function and less on unigrams than
c1 when finding correspondences between char-
acters.
As we concluded in the previous section, the
frequent confusion between categories suggests
that the analogies we construct are relatively non-
specific. We might hope to create role-based sum-
mary of novels by finding their nearest neighbors
and then propagating the character categories (for
example, ? is the protagonist of this novel. She
lives at . She eventually marries , her other
suitors are and her older guardian is .?)
but the present system is probably not adequate
for the purpose. We expect that detecting a fine-
grained set of emotions will help to separate char-
acter functions more clearly.
640
F ProtM Prot
F Marr.
M Marr.
Pl Marr.
F Other
M Other
Pl Other
Non-char Character frequency by category
Types
Token
s
F Prot M Prot F Marr.M Marr.Pl Marr.F OtherM OtherPl OtherNon-char Unigram features (d)
Non-cha
r P
l Other
M Other
F Other
Pl Marr.
M Marr.
F Marr.
M Prot
F Prot
F Prot M Prot F Marr.M Marr.Pl Marr.F OtherM OtherPl OtherNon-char First-order (c1)
Non-cha
r P
l Other
M Other
F Other
Pl Marr.
M Marr.
F Marr.
M Prot
F Prot
F Prot M Prot F Marr.M Marr.Pl Marr.F OtherM OtherPl OtherNon-char Second-order (c2)
Non-cha
r P
l Other
M Other
F Other
Pl Marr.
M Marr.
F Marr.
M Prot
F Prot
Figure 2: Affinity diagrams showing character types
contributing to the kernel similarity between four
works by Jane Austen.
10 Conclusions
This work presents a method for describing nov-
elistic plots at an abstract level. It has three main
contributions: the description of a plot in terms
of analogies between characters, the use of emo-
tional and frequency trajectories for individual
characters rather than whole works, and evalua-
tion using artificially disordered surrogate novels.
In future work, we hope to sharpen the analogies
we construct so that they are useful for summa-
rization, perhaps by finding an external standard
by which we can make the notion of ?analogous?
characters precise. We would also like to investi-
gate what gains are possible with a finer-grained
emotional vocabulary.
Acknowledgements
Thanks to Sharon Goldwater, Mirella Lapata, Vic-
toria Adams and the ProbModels group for their
comments on preliminary versions of this work,
Kira Moura?o for suggesting graph kernels, and
three reviewers for their comments.
References
Amjad Abu Jbara and Dragomir Radev. 2011. Coher-
ent citation-based summarization of scientific pa-
pers. In Proceedings of ACL 2011, Portland, Ore-
gon.
Cecilia Ovesdotter Alm and Richard Sproat. 2005.
Emotional sequencing and development in fairy
tales. In ACII, pages 668?674.
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: Machine learn-
ing for text-based emotion prediction. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 579?586, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2005. Model-
ing local coherence: an entity-based approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?05).
Indrajit Bhattacharya and Lise Getoor. 2005. Rela-
tional clustering for multi-type entity resolution. In
Proceedings of the 4th international workshop on
Multi-relational mining, MRDM ?05, pages 3?12,
New York, NY, USA. ACM.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
641
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 602?610,
Suntec, Singapore, August. Association for Com-
putational Linguistics.
Eugene Charniak. 2001. Unsupervised learning of
name structure from coreference data. In Second
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NACL-01).
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of docu-
ment structure using latent permutations. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 371?379, Boulder, Colorado, June.
Association for Computational Linguistics.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambigua-
tion for knowledge base population. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 277?
285, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
David K. Elson and Kathleen R. McKeown. 2010.
Building a bank of semantically encoded narratives.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh con-
ference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).
David Elson, Nicholas Dames, and Kathleen McKe-
own. 2010. Extracting social networks from liter-
ary fiction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 138?147, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daume III. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77?86, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
Arthur Gretton, Karsten M. Borgwardt, Malte Rasch,
Bernhard Schlkopf, and Alexander J. Smola. 2007.
A kernel method for the two-sample-problem. In
B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems
19, pages 513?520. MIT Press, Cambridge, MA.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, Computer Science Department, UC Santa Cruz.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence. In ACL, pages 391?
398.
Anna Kazantseva and Stan Szpakowicz. 2010. Sum-
marizing short stories. Computational Linguistics,
pages 71?109.
Dekang Lin and Patrick Pantel. 2001. Induction of
semantic classes from natural language text. In
Proceedings of the seventh ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, KDD ?01, pages 317?322, New York, NY,
USA. ACM.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1562?1572, Uppsala, Sweden, July. Association for
Computational Linguistics.
G. Miller, A.R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: an on-
line lexical database. International Journal of Lexi-
cography, 3(4).
Saif Mohammad. 2011. From once upon a time
to happily ever after: Tracking emotions in novels
and fairy tales. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 105?114, Portland, OR, USA, June. Associa-
tion for Computational Linguistics.
Matt Post. 2011. Judging grammaticality with tree
substitution grammar derivations. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 217?222, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997.
A maximum entropy approach to identifying sen-
tence boundaries. In Proceedings of the Fifth Con-
ference on Applied Natural Language Processing,
pages 16?19, Washington D.C.
Ekaterina P. Volkova, Betty Mohler, Detmar Meur-
ers, Dale Gerdemann, and Heinrich H. Bu?lthoff.
2010. Emotional perception of fairy tales: Achiev-
ing agreement in emotion annotation of text. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, pages 98?106, Los Ange-
les, CA, June. Association for Computational Lin-
guistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
642
Processing, pages 347?354, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
643
A List of texts
Dev set (11 works)
Austen Emma, Mansfield Park, Northanger
Abbey, Persuasion, Pride and Prej-
udice, Sense and Sensibility
Bronte?, Emily Wuthering Heights
Burney Cecilia (1782) Hardy Tess of the D?Urbervilles
James The Ambassadors Scott Ivanhoe
Test set (30 works)
Braddon Aurora Floyd Bronte?, Anne The Tenant of Wildfell Hall
Bronte?, Charlotte Jane Eyre, Villette Bulwer-Lytton Zanoni
Disraeli Coningsby, Tancred Edgeworth The Absentee, Belinda, Helen
Eliot Adam Bede, Daniel Deronda, Mid-
dlemarch
Gaskell Mary Barton, North and South
Gissing In the Year of Jubilee, New Grub
Street
Hardy Far From the Madding Crowd, Jude
the Obscure, Return of the Native,
Under the Greenwood Tree
James The Wings of the Dove Meredith The Egoist, The Ordeal of Richard
Feverel
Scott The Bride of Lammermoor Thackeray History of Henry Esmond, History
of Pendennis, Vanity Fair
Trollope Doctor Thorne
Out-of-domain set (10 works)
Ainsworth The Lancashire Witches Bulwer-Lytton Paul Clifford
Dickens Oliver Twist, The Pickwick Papers Collins The Moonstone
Conan-Doyle A Study in Scarlet, The Sign of the
Four
Hughes Tom Brown?s Schooldays
Stevenson Treasure Island Stoker Dracula
Table 4: 19th century novels used in our study.
644
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 520?529,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Information Structure Prediction for Visual-world Referring Expressions
Micha Elsner
Department of Linguistics,
The Ohio State University
melsner@ling.osu.edu
Hannah Rohde
Linguistics & English Language,
University of Edinburgh
hannah.rohde@ed.ac.uk
Alasdair D. F. Clarke
School of Informatics,
University of Edinburgh
a.clarke@ed.ac.uk
Abstract
We investigate the order of mention for
objects in relational descriptions in visual
scenes. Existing work in the visual do-
main focuses on content selection for text
generation and relies primarily on tem-
plates to generate surface realizations from
underlying content choices. In contrast,
we seek to clarify the influence of visual
perception on the linguistic form (as op-
posed to the content) of descriptions, mod-
eling the variation in and constraints on
the surface orderings in a description. We
find previously-unknown effects of the vi-
sual characteristics of objects; specifically,
when a relational description involves a vi-
sually salient object, that object is more
likely to be mentioned first. We conduct
a detailed analysis of these patterns using
logistic regression, and also train and eval-
uate a classifier. Our methods yield signif-
icant improvement in classification accu-
racy over a naive baseline.
1 Introduction
Visual-world referring expression generation
(REG) is the task of instructing a listener how
to find an object (the target) in a visual scene.
In complicated scenes, people often produce
relational descriptions, in which the target object
is described relative to another (a landmark)
(Viethen and Dale, 2008). While existing REG
systems can generate relational descriptions,
they tend to focus on content selection (that is,
choosing an appropriate set of landmarks for
each object). Surface realization (turning the
selected content into a string of words) is handled
by simple heuristics, such as sets of templates.
Complex descriptions, however, have a non-trivial
information structure? objects are not mentioned
in an arbitrary order. Numerous studies in
non-visual domains show that English speakers
favor constructions that place familiar (given)
information before unfamiliar (new) (Bresnan et
al., 2007; Ward and Birner, 2001; Prince, 1981).
We show that this pattern also holds for visual-
world referring expressions (REs), and moreover,
that objects with sufficient visual prominence are
treated as given. Thus, we argue that the concept
of salience used in surface realization should
incorporate metrics from visual perception.
In this study, we create a model of information
ordering in complex relational descriptions. Us-
ing a discriminative classifier, we learn to predict
the information structuring strategies used in our
corpus. We compare these strategies to the typical
given/new pattern of English discourse. Experi-
ments on a corpus of descriptions of cartoon peo-
ple in the childrens? book ?Where?s Wally? (Hand-
ford, 1987), corpus described in (Clarke et al.,
2013), show that our approach significantly out-
performs a naive baseline, improving especially
on prediction of non-canonical orderings.
This study has three main contributions. First,
it demonstrates that humans use sophisticated in-
formation ordering strategies for REG, and there-
fore that the template strategies used in previous
work do not adequately model human production.
Second, it makes a practical proposal for an im-
proved model which is capable of predicting these
orderings; while this model is not a full-scale sur-
face realizer, we view it as an important interme-
diate step towards one. Finally, it makes a the-
oretical contribution: By linking the information
structures observed in the data to the existing re-
520
search on salience and information structure, we
show that visually prominent objects are treated
as part of common ground despite the lack of pre-
vious mention.
2 Related work
Computational models of REG (Krahmer and van
Deemter, 2012) focus mainly on content selection:
Given a list of objects in the scene and their visual
attributes, such models decide what information to
include in a description so as to specify the tar-
get object. Early systems (with the exception of
Dale and Haddock (1991)) did not produce rela-
tional descriptions. Nor did these systems model
the visual salience of the objects or attributes un-
der discussion.
Later models (Kelleher et al., 2005; Kelleher
and Kruijff, 2006; Duckham et al., 2010) intro-
duce simple models of visual salience, prompted
by psycholinguistic research which shows that ob-
jects are more likely to be selected as landmarks
when they are easy for an observer to find (Beun
and Cremers, 1998). Clarke et al. (2013) extend
these results with a more complicated model of
visual salience (Torralba et al., 2006). Fang et al.
(2013) similarly note that generated REs should
avoid information that is perceptually expensive to
obtain. However, these results focus on content se-
lection rather than surface realization.
In comparison to selection, surface realization
for REG has received little attention. Many re-
searchers do not even perform realization, but sim-
ply compare their systems? selected content with
the gold standard under metrics like the Dice co-
efficient. The TUNA challenges (Gatt et al., 2008;
Gatt et al., 2009; Gatt and Belz, 2010) are an ex-
ception; participants were required to provide sur-
face realizations, which were evaluated via NIST,
BLEU and string edit distance. Many partici-
pants used a template-based realizer written by
Irene Langkilde-Geary, which imposes a fixed or-
dering on attributes like ?size? and ?color? but
has no provisions for relational descriptions. A
few participants created their own realizers. Brug-
man et al. (2009) describe a system with multi-
ple hand-written templates. Di Fabbrizio et al.
(2008) propose several learning-based systems;
the most effective were a dependency-based ap-
proach which learned precedence relationships be-
tween pairs of words, and a template-based ap-
proach which learned global orderings over sets of
attributes. Neither approach is designed to handle
relational descriptions, nor do they incorporate vi-
sual information. Duan et al. (2013), also studying
the Wally corpus, demonstrates that visual features
affect determiner choice for NPs, but do not study
information structure.
Several studies give basic principles for infor-
mation structure in English discourse. Prince
(1981) introduces the key distinctions between
discourse-old and new entities (previously men-
tioned vs not mentioned) and hearer-old and new
entities (familiar to the listener vs not familiar).
Clark and Wilkes-Gibbs (1986) extends the latter
distinction to a notion of common ground; entities
in the common ground are familiar to both par-
ticipants in the discourse, and each participant is
in turn aware of the other?s familiarity. As noted
by Prince (1981) and expanded on by Ward and
Birner (2001) and in Centering Theory (Grosz et
al., 1995), the first element in an English sentence
is generally reserved for old information, while
new information is usually placed at the end. For
instance, see these (contrived) examples:
(1) a. Obama adopted a dog named Bo.
b. #A dog named Bo was adopted by
Obama.
Ex. (1-a) demonstrates the standard order (un-
der the assumption that Obama is familiar to a
reader of this paper while Bo may not be). (1-b)
violates the ordering principles and is likely to
be judged less felicitous. Importantly, Obama is
hearer-old not because of a preceding discourse
mention but due to (assumed) general knowl-
edge; it is an unused (Prince, 1981), or existential
(Bean and Riloff, 1999) entity. General knowl-
edge shared by speakers of a community is one
way in which an entity enters the common ground.
Along with this shared socio-cultural background,
speakers may also share physical co-presence and
linguistic co-presence (Clark, 1996). They can in-
dicate salient entities, individuals, or entire events
by engaging their listener in joint attention via
pointing or gaze cueing (Baldwin, 1995; Carpen-
ter et al., 1998); in this paper, we demonstrate that
visual prominence is also sufficient.
Maienborn (2001) explicitly suggests that this
topic-comment structure principle is the motiva-
tion for the frequent appearance of locative modi-
fiers in clause-initial position; however, she gives
no felicity conditions on when this leftward move-
ment is expected. Since most of the modifiers in
521
this study are locatives, our data should be taken as
endorsing this theoretical position, but supplying
felicity conditions in terms of common ground.
These principles have been applied to compu-
tational surface realization in non-visual domains
(Webber, 2004; Nakatsu and White, 2010, and
others). Freer-word-order languages such as Ger-
man also have predictable information structures
which have been employed in surface realization
systems, but these require a different structural
analysis than in English (Zarrie? et al., 2012; Fil-
ippova and Strube, 2007).
3 Information structures in our corpus
In this section, we define the particular ordering
strategies which we investigate in the rest of the
paper. We begin by defining some terms: A re-
lational description includes two objects, the an-
chor, which is the object being located, and the
landmark, an object which is mentioned to make
it easier to locate the anchor. The anchor may be
the target of the entire expression, or it may in turn
serve as a landmark in another relational descrip-
tion (as in ?the man next to the horse next to the
building? where ?horse? serves as both a landmark
for ?man? and an anchor for ?building?.
1
The
REs in this corpus reflect the variation in the way
speakers constructed their descriptions: Some pro-
duced multiple complete sentences; others used
abbreviated language and compacted their expres-
sion into a single sentence or phrase. In this pa-
per we use the term ?ordering? to refer to speak-
ers? decisions of whether to precede or postpose a
reference to one object relative to their reference
to another. In this way, the ?syntax? of the de-
scription is built out of references to particular ob-
jects (the noun phrases) and the relationships be-
tween those references. Note that the references
may consist of a short phrase (?the man with the
sword?) or an entire clause (?he is standing and
holding a sword?)
In our corpus, speakers use three primary strate-
gies to order anchors and landmarks, exemplified
by the following REs from our corpus (shown with
bold for text describing the anchor and italics for
text for landmarks):
(2) Near the hut that is burning, there is a man
holding a lit torch in one hand, and a
sword in the other.
1
In our examples below, the anchor is the target of the
overall expression, i.e., the intended referent in the REG task.
(3) Man closest to the rear tyre of the van.
(4) There is a person standing in the water
wearing a blue shirt and yellow hat
Ex. (2) places the landmark so that it precedes
the anchor; Ex. (3) shows the landmark follow-
ing it. Ex. (4) shows a more complex structure,
which we refer to as interleaved, where informa-
tion about the anchor is given in multiple phrases
and the landmark phrase appears between them.
2
(These orders are determined with respect to the
first mention of the landmark.) We denote these
ordering strategies as PRECEDE, FOLLOW and IN-
TER respectively.
We also distinguish between landmarks which
are only mentioned in relation to an anchor and
those which are first introduced in a non-relative
construction such as ?look at the X? or ?there?s an
X?:
(5) There is a horse rearing up on its hind legs.
Behind the horse is a man laying down on
his back completely flat and straight.
Since these constructions establish the existence
of a landmark without immediately incorporating
it into the description, we denote these as ESTAB-
LISH constructions.
Finally, our annotation scheme distinguishes
between genuine landmarks (visible objects or
groups of objects in the scene) and image regions
like ?the left? or ?bottom center?:
(6) Bottom center, man looking left
4 Dataset
We use a collection of referring expressions
elicited on Mechanical Turk, previously described
in (Clarke et al., 2013).
3
The dataset contains
descriptions of targets in 11 images from the
childrens? book Where?s Wally
4
(Handford, 1987;
Handford, 1988); in each image, 16 people were
designated as targets. Each participant saw each
scene only once. An example scene is shown in
Figure 1. The participant was instructed to type a
description of the person in the red box so that an-
other person viewing the same scene (but without
the box) would be able to find them; to make sure
2
This structure is not syntactically discontinuous, but vi-
sually it is; if the listener wants to confirm these details visu-
ally, they must first look at the person, then look away at the
water and then look back at the person.
3
Via http://datashare.is.ed.ac.uk/
handle/10283/336
4
Published in the USA as Where?s Waldo.
522
this was clear, as part of the study instructions,
they completed a few visual searches based on text
descriptions. The image in the figure also contains
a black box (not part of the initial stimulus), which
the annotator has added to designate the landmark
object ?burning hut?). The dataset contains 1672
descriptions, contributed by 152 different partici-
pants (152 participants ? 11 scenes).
The REs are annotated for visual and linguistic
content. The annotation scheme indicates which
substrings of the RE describe the target object, an-
other mentioned object or an image region. Ref-
erences to parts or attributes of objects are not
treated as separate objects; ?a man holding torch
and sword? in Figure 1 is a single object. The
mentioned objects are linked to bounding boxes
(or for very large objects, bounding polygons) in
the image.
For each mention of a non-target object, the an-
notation indicates whether it is part of a relational
description of a specific anchor, and if so which; if
it is not, it receives an ESTABLISH tag. These an-
notations are used to determine the ordering strate-
gies used in this study. In some cases, the linkage
between objects is implicit:
(7) ...there are 4 men smoking... the man you
are looking for is the one [=of the 4 men]
leaning against a crate
In the above RE, 4 men is first introduced in an
ESTABLISH construction. The word ?one? refers
implicitly to part of this set of men, so the annota-
tor marks a relational link from ?4 men? to ?one?.
In our analysis in this study, we treat the entity
?crates? as anchored to the target (?one?) on the
basis of this implicit link (so that this is an instance
of the PRECEDE-ESTABLISH pattern), but we do
not treat the hidden link itself as a mention or try
to predict its nonexistent ?position? in the string.
5 Distribution of ordering strategies
We first describe the distribution of these strate-
gies across the corpus as a whole. As shown in Ta-
ble 1, landmarks are ordered about equally to the
FOLLOW or PRECEDE of the objects they help to
locate. Regions, on the other hand, prefer the PRE-
CEDE ordering. The INTER ordering is less com-
mon, but still quite well-represented. The ESTAB-
LISH construction (initial ?there is? or ?look at?)
occurs only with PRECEDE ordering, and indeed
can be viewed as a syntactic strategy for achieving
such an order. We will explain these characteristic
The <targ>man</targ> just to the left
of the <lmark rel=?targ? obj=?imgID?>
burning hut</lmark> <targ>holding a
torch and a sword</targ>.
Figure 1: Example scene (red box indicates tar-
get) with annotated referring expression. Words in
<targ> tags describe the target. A single land-
mark (the burning hut, indicated by the rel at-
tribute) is mentioned in a relational description
whose anchor is the target; the annotator has
marked it with a black box.
patterns in linguistic terms in Section 7.
As in most discourse tasks (Ford and Olson,
1975; Pechmann, 2009), speakers display a fair
amount of variability. To measure this, we exam-
ine each anchor/landmark pair which is mentioned
by more than one speaker, and compute how often
these speakers use the same strategy. There are
664 such pairs,
5
appearing a total of 2361 times
in the corpus.
6
Of these, 66% agree on the direc-
tional strategy.
7
Separately, 14% of the expres-
sions use an ESTABLISH construction, and 43% of
these are agreed on by the majority. (The remain-
ing variation could in principle have two sources:
The content of the expression as a whole could af-
fect the realization of a particular pair of objects,
or individual speakers might simply differ in their
usage patterns.) Nonetheless, there is a good deal
of regularity in speakers? decisions. In the rest of
the paper, we attempt to model and predict this
regularity.
5
286 of these pairs are mentioned by exactly two speakers.
6
This is more than the total number of referring expres-
sions in the corpus, because many of the REs contain multiple
pairs of entities.
7
If strategies were assigned randomly using the overall
marginals, we would expect only 34% agreement. Using this
method of calculating chance agreement, we would obtain a
Cohen?s ? of .48.
523
PRECEDE INTER FOLLOW
Region 60 (440) 21 (160) 19 (138)
L-mark 38 (977) 25 (632) 37 (945)
ESTABLISH NON-EST.
PRECEDE landmark 51 (495) 49 (482)
Table 1: Distribution of ordering strategies for all
landmarks and regions in the corpus: % (count).
An additional 24 landmarks occur with no associ-
ated anchor (and therefore no discernible order).
6 Visual and non-visual information
Since visual properties are known to affect land-
mark selection (Kelleher et al., 2005; Viethen and
Dale, 2008), we expect them to influence informa-
tion structure as well. Our system uses three visual
properties to predict information structure; we se-
lect properties that are known from previous work
to help predict whether a landmark will be men-
tioned. These properties are the area of the an-
chor and landmark, the distance between them
(Golland et al., 2010, among others) and their cen-
trality (centr.) (distance from the center of the
screen) (Kelleher et al., 2005).
8
These properties
are all indicators of visual salience (Toet, 2011),
the property which makes objects in a scene easy
to find quickly (Wolfe, 2012) and tends to draw
initial gaze fixations (Itti and Koch, 2000). We
also include indicators for whether the anchor is
the target object, and whether the landmark is an
image region (reg) (see section 3).
In addition, we give a few non-visual features
derived from the content structure. These include
the number of dependents (landmarks which re-
late to each object in the description) and the num-
ber of descendants (the direct dependents, their
dependents and so forth). When the speaker has
to arrange a large number of landmarks, they tend
to vary the ordering more, because of heavy-shift
effects (White and Rajkumar, 2012) and the diffi-
culty of preposing more than one constituent.
7 Regression analysis
To gain some insight into the influence of differ-
ent features, we conduct a logistic regression anal-
ysis. For each pair of (anchor, landmark) occur-
8
Following Clarke et al. (2013), we attempted to also
measuring distinctiveness from the background using a per-
ceptual model of visual salience (Torralba et al., 2006). Al-
though this measure is effective in predicting landmark selec-
tion, it proves uninformative here for predicting information
structure, yielding no significant effects in any analyses.
ring in a relational description, we attempt to pre-
dict the manner of realization (direction and ES-
TABLISH). We performed a logistic regression for
each class (one-vs-all); thus there are four regres-
sors in total, making 0-1 predictions for PRECEDE,
PRECEDE-ESTABLISH, INTER and FOLLOW.
Because their distributions are heavily skewed,
area is transformed to square root area and dis-
tance/centrality values are log-transformed as in
Clarke et al. (2013).
9
Features are scaled to zero
mean and unit variance. Finally, centrality values
are negated so that higher values indicate more
central objects; this is for ease of interpretation.
We fit models using random intercepts for speaker
and image using the LME4 package (Bates et al.,
2011), then removed all fixed effects which were
never significant for any class and reran the anal-
ysis until a minimal model was reached (Crawley,
2007). This minimization removed the number of
descendants features (but kept number of direct
dependents). Table 2 shows the significant coef-
ficients, standard deviations and Z-scores. (Note
that as the regressions are separate, the coefficients
are comparable reading down columns, but not
across rows).
The regression analysis shows that as landmarks
get larger, they are more likely to be realized with
the PRECEDE (? = 3.27) or INTER (? = 1.28)
strategies (but not PRECEDE-ESTABLISH) and less
likely (? = ?3.76) to be placed following. (This
does not appear to be the case for landmarks that
are central; these are slightly more likely to be
ordered FOLLOW (? = .81).) The PRECEDE-
ESTABLISH construction is neither favored nor
disfavored by landmark area. It does, however,
have a strong preference for landmarks with many
dependents (? = 2.38), since these are more nat-
urally realized in the clause-final position intro-
duced by a ?There is X?-type construction. In con-
trast, landmarks with many dependents disfavor
the INTER strategy (? = ?1.07), since this would
require placing a heavy NP in a central rather than
rightward position.
There are also a few effects of visual features
of the anchor objects. Larger anchors (which are
easier to see in their own right) prefer landmarks
to FOLLOW (? = .35). This presumably reflects
the fact that, since the listener is more likely to
see them quickly, such anchors are more often re-
9
We use these continuous values in our analysis; our clas-
sifier model (below) uses discretized area, distance and cen-
trality.
524
Feature PRECEDE Z PREC.-EST. Z INTER Z FOLLOW Z
intercept -4.18 ? .37 -11.2 -2.66 ? .50 -5.3 -2.51 ? .32 -7.7 2.72 ? .32 8.5
anch area -.27 ? .06 -4.6 -.19 ? .09 -2.2 - - .35 ? .05 6.9
anch centr .11 ? .05 2.0 - - - - - -
anch deps - - -.74 ? .12 -6.2 .22 ? .06 3.6 - -
anch=targ .30 ? .13 2.3 - - .55 ? .14 4.0 -.71 ? .13 -5.7
distance - - -.24 ? .09 -2.6 - - - -
lmk=reg 11.46 ? 1.35 8.5 - - 3.01 ? 1.19 2.5 -12.62 ? 1.17 -10.8
lmk area 3.27 ? .38 8.7 - - 1.28 ? .32 4.0 -3.76 ? .32 -11.7
lmk centr - - - - - - .81 ? .32 2.6
lmk deps - - 2.38 ? .14 16.9 -1.07 ? .13 -8.3 -1.37 ? .12 -11.5
Table 2: Regression coefficients, standard deviations and Z-scores from one-vs-all logistic regressions
with direction/ESTABLISH status as output variable. Only effects significant at p < .05 level are shown;
other effects are displayed as -.
alized at the start of an expression. (Clarke et al.
(2013) show that they have fewer landmarks over-
all.) Again, the effect of centrality is counterin-
tuitive, but weak (? = .81). Anchors with more
dependents are slightly more likely to use the IN-
TER slot (? = .22), suggesting that the various
dependents are spread syntactically throughout the
expression.
Although distance and centrality are weak in-
dicators in this dataset, area shows strong effects
which support our conclusion that visual salience
behaves like discourse salience. The standard in-
formation order of English clauses places given in-
formation first and new information later (Prince,
1981). Thus, we observe that the non-right or-
ders are used for larger objects, which is what we
would expect if their visual perceptibility is suffi-
cient to place them in common ground despite the
lack of a previous mention.
10
On the other hand,
the FOLLOW order is used for smaller objects that
cannot be assumed to be part of common ground
(and are therefore treated as new).
The use of ESTABLISH constructions for mid-
sized objects also makes sense on theoretical
grounds. ESTABLISH constructions are a way
of achieving the PRECEDE information structure,
which places the landmark first? and this makes
sense primarily if the landmark is reasonably
salient, since otherwise it will not be found any
faster than the target. On the other hand, most
of the constructions we discuss as ESTABLISH,
10
Prince (1981) discusses other discourse-new items that
are nonetheless treated as familiar, like ?The FBI?, under the
name unused (that is, available, but not previously in use in
the discourse).
such as existential ?there is?, require their object
to be discourse-new (Ward and Birner, 1995); it
would be infelicitous to start a description by stat-
ing the existence of something already in the com-
mon ground ?there is a sky, and it is blue. . . ?
Thus, it makes sense that neither large or small
objects favor the use of this construction; it can be
used to foreground an object which is not salient
enough to be assumed in common ground, but is
salient enough to find without a great deal of vi-
sual search.
8 Information structure prediction
In this section, we experiment with an idealized
version of the information structuring task. We
provide our system with gold standard content
selection? we know which objects will be men-
tioned, and if they serve as landmarks, we know
the anchor they describe. However, we do not
know which information strategies will be used to
order them; our task is to predict this. In doing
so, we are working with an idealized version of
the standard generation pipeline, which often op-
erates as a two-stage process, with content selec-
tion followed by surface realization. Information
structure prediction is intermediate between these
two stages; once we have decided which objects
to mention (or in concert), we would like to de-
cide what order to mention them in.
We set up the prediction task as in the pre-
vious section: Given an anchor/landmark pair,
our system must decide what direction and ES-
TABLISH status to assign it. However, here we
evaluate the system as a classifier. We treat an-
chor/landmark pair as independent from the others
525
Feat type # features
type (targ/lmark/region) of anchor 3
type (targ/lmark/region) of dep 3
quartile of anchor area 4
quartile of lmark area 4
quartile of anchor? lmark dist 4
quartile of dist anchor? screen ctr 4
quartile of dist lmark? screen ctr 4
# direct dependents of anchor 6
# descendents of anchor 6
Table 3: Feature templates and number of instan-
tiations in our discriminative system.
(including other pairs from the same description);
during development, we investigated a parser-like
structured classifier based on (Socher et al., 2011;
Salakhutdinov and Hinton, 2009) that jointly clas-
sified all the relational descriptions in a single ut-
terance at once, but results did not improve over
the classifier system, perhaps because on average
the trees are fairly shallow.
8.1 Discriminative comparison
We train a discriminative multilabel classifier us-
ing maximum entropy.
11
We predict EST-DIR pairs
given a set of discrete features shown in Table
3. This setup differs slightly from the previous
section (which used one-vs-all); we are attempt-
ing to conform to the standard practices of psy-
cholinguistics and computational linguistics re-
spectively. Area, salience, distance to center and
inter-object distance values are discretized by de-
termining in which quartile of the training set each
value falls (lowest 25%, mid-low, mid-high, high-
est 25%). Our initial model used continuous val-
ues as in the previous section, but results were
somewhat poorer, suggesting some of these fea-
tures may have nonlinear effects.
8.2 Experiments
We hold out three images (vikings, airport,
blackandwhite) as a development set. In test, we
exclude these 3 documents and use the other 8
for evaluation. In both development and test, we
conduct experiments by crossvalidation, testing on
one document at a time and training on the other
ten.
12
11
Learned using the Theano neural-network package
(Bergstra et al., 2010) and stochastic gradient descent code
from deeplearning.net/tutorial (Bengio, 2009).
12
This means we always use 10 of the 11 documents for
training, whether in dev or test, but we didn?t do error anal-
We report two trivial baseline strategies, all
landmarks following (the best baseline for over-
all accuracy) and all landmarks preceding (the best
baseline for predicting the direction, but not as
good overall because the PRECEDE predictions are
split between ESTABLISH and not ESTABLISH).
Our preliminary analysis shows that regions have
a strong tendency to precede their anchors, so we
also report results for a baseline using this pat-
tern (regions preceding, everything else follow-
ing). We believe this baseline pattern is the one
which would be learned as a template by previ-
ous systems like Di Fabbrizio et al. (2008), since
this system can learn relationships between broad
types of entities (target, landmark and region) but
does not use visual features of the actual entities
in the scene to make any finer distinctions.
We also provide two ?inter-subject? oracle
scores intended to estimate the performance ceil-
ing imposed by human variability. This oracle
assigns each anchor/landmark pair the direction
and ESTABLISH status assigned by the majority
of speakers who mentioned that pair. The ?mul-
tiple mentions? estimate of agreement is the one
mentioned in Section 5; it was based only on pairs
mentioned by multiple speakers. The ?all? esti-
mate is based on all objects; it is higher because,
for pairs mentioned by only one speaker, it is by
definition perfect. Our system?s use of the num-
ber of descendants feature is not captured by this
oracle? these features capture information about
a particular speaker?s content plan beyond their
decision to mention a particular pair? but we sus-
pect that the oracle?s performance will nonetheless
be hard for any practical system to beat.
We report gross accuracy (correctly predicting
both DIR and ESTABLISH) for relational pairs (Ta-
ble 5), and also decompose by direction (Table 4)
and ESTABLISH status (Table 6).
The baseline correctly predicts 43% of pairs,
implying that this pattern (regions precede, land-
marks follow) covers a bit under half the data. The
classifier improves this to 52%. When predicting
the direction alone, the best baseline (PRECEDE)
scores 42%; the classifier scores 57%. All sys-
tem scores are significantly better than the base-
line (sign test on pairs, p < 0.01). In predictions
of ESTABLISH tags, our result is a 60% f-score,
which is indistinguishable from the lower bound
ysis on the training examples. Data size does appear to mat-
ter; training on 8 documents at a time and testing on 3 yields
poorer results.
526
System PRECEDE INTER FOLLOW Dir Acc
Prec Rec F Prec Rec F Prec Rec F
Follow 0 0 0 0 0 0 32 100 49 32
Precede 44 100 62 0 0 0 0 0 0 44
Regions precede 61 32 42 0 0 0 37 87 52 42
Discr 66 69 68 39 23 29 53 65 58 57
Inter-subj (multiple mentions) 77 61 68 54 62 58 67 76 71 66
Inter-subj (all) 84 75 79 65 69 67 74 83 78 76
Table 4: Direction scores (p/r/f per direction and total pair directions correctly predicted) in 2382 pairs
in test set. Overall accuracy differences between system and baselines are significant (p < .01).
System Pair accuracy
Follow 36
Precede 29
Regions precede 43
Discr 52
Inter-subj (mult) 64
Inter-subj (all) 74
Table 5: Gross accuracy (%) for 2382 test pairs.
System ESTABLISH
Prec Rec F
Follow 0 0 0
Precede 0 0 0
Regions precede 0 0 0
Discr 55 67 60
Inter-subj (mult) 68 43 53
Inter-subj (all) 82 66 73
Table 6: ESTABLISH scores (p/r/f for EST=TRUE)
in 2382 pairs in test set.
estimate of interannotator agreement.
9 Conclusions
The results of this study show that the information
structure of relational descriptions is highly vari-
able, and depends on notions of salience and com-
mon ground that are difficult to capture with tem-
plates or simple case-based rules. This suggests
that the question of realization for visual-word re-
ferring expressions may need to be reopened. A
data-driven approach not only allows better pre-
diction of which strategy will be used (reducing
error by 9% absolute, 16% relative) but also en-
ables us to analyze the pattern and conclude that
the visual salience of an object acts in the same
way as discourse salience.
Several open questions remain. One is the fail-
ure of the Torralba et al. (2006) visual distinctive-
ness model to make any difference: Is this actually
a perceptual fact, or does it merely demonstrate
that the model is not as predictive of human atten-
tional patterns as we would like? More important
is the question of what lies behind the substantial
variations we observe across individuals. These
may reflect truly different strategies; for instance,
some speakers may generate REs incrementally as
they scan the image (Pechmann, 2009) while oth-
ers perform a more complete scan before begin-
ning (Gatt et al., 2012). We suspect answering
this question is beyond the scope of corpus stud-
ies, and intend to investigate via psycholinguistic
experiments using an eyetracker.
Another question is to what extend the patterns
we observe are intended to facilitate listeners? vi-
sual search (an audience design hypothesis) ver-
sus speakers? efficient construction of utterances.
This study focused on predicting speaker behavior,
while acknowledging that the utterances speakers
produce are not always optimal for listeners (Belz
and Gatt, 2008). However, we suspect that in this
case, putting easy-to-see objects early really does
help listeners; we are currently planning percep-
tion experiments to test this hypothesis.
Finally, we intend to incorporate the visual fea-
tures used in this study into a full-scale realization
system. This will enable us to create more human-
like REs for visual domains. Such REs can be in-
corporated into natural language systems for a va-
riety of interactive visual-world tasks.
Acknowledgements
The third author was supported by EPSRC grant
EP/H050442/1 and ERC grant 203427 ?Syn-
chronous Linguistic and Visual Processing?. We
also thank Marie-Catherine de Marneffe, Craige
Roberts, the OSU Pragmatics group and our
anonymous reviewers for their helpful comments.
527
References
D. A. Baldwin. 1995. Understanding the link between
joint attention and language. In Joint attention: its
origins and role in development. Lawrence Erlbaum
Assoc., Hillsdale, NJ.
D. Bates, M. Maechler, and B. Bolker. 2011.
lme4: Linear mixed-effects models using s4
classes. Comprehensive R Archive Network:
cran.r-project.org.
David L. Bean and Ellen Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics (ACL?99),
pages 373?380, Morristown, NJ, USA. Association
for Computational Linguistics.
Anja Belz and Albert Gatt. 2008. Intrinsic vs. ex-
trinsic evaluation measures for referring expression
generation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
on Human Language Technologies: Short Papers,
pages 197?200. Association for Computational Lin-
guistics.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1?127. Also published as a book. Now Pub-
lishers, 2009.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
Robbert-Jan Beun and Anita H.M. Cremers. 1998.
Object reference in a shared domain of conversation.
Pragmatics and Cognition, 6(1-2):121?152.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and
R. Harald Baayen. 2007. Predicting the dative al-
ternation. Cognitive Foundations of Interpretation,
pages 69?94.
Ivo Brugman, Mari?et Theune, Emiel Krahmer, and
Jette Viethen. 2009. Realizing the costs: template-
based surface realisation in the graph approach to
referring expression generation. In Proceedings of
the 12th European Workshop on Natural Language
Generation, ENLG ?09, pages 183?184, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
M. Carpenter, K. Nagell, and M. Tomasello. 1998. So-
cial cognition, joint attention, and communicative
competence from 9 to 15 months of age. Mono-
graphs of the Society for Research in Child Devel-
opment, 63(4).
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22(1):1?39.
Herbert H. Clark. 1996. Using language. Cambridge
University Press, Cambridge.
Alasdair D. F. Clarke, Micha Elsner, and Hannah Ro-
hde. 2013. Where?s Wally: The influence of
visual salience on referring expression generation.
Frontiers in Psychology (Perception Science), Issue
on Scene Understanding: Behavioral and computa-
tional perspectives.
Michael Crawley. 2007. The R Book. Wiley-
Blackwell, Hoboken, NJ.
Robert Dale and Nicholas J. Haddock. 1991. Gen-
erating referring expressions involving relations. In
EACL, pages 161?166.
Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas
Bangalore. 2008. Referring expression generation
using speaker-based attribute selection and trainable
realization (ATTR). In Proceedings of the 5th Inter-
national Conference on Natural Language Genera-
tion (INLG), Salt Fork, OH.
Manjuan Duan, Micha Elsner, and Marie-Catherine
de Marneffe. 2013. Visual and linguistic predictors
for the definiteness of referring expressions. In Pro-
ceedings of the 17th Workshop on the Semantics and
Pragmatics of Dialogue (SemDial), Amsterdam.
Matt Duckham, Stephan Winter, and Michelle Robin-
son. 2010. Including landmarks in routing instruc-
tions. Journal of Location Based Services, 4(1):28?
52.
Rui Fang, Changsong Liu, Lanbo She, and Joyce Y.
Chai. 2013. Towards situated dialogue: Revisiting
referring expression generation. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 392?402, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 320?327,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
William Ford and David Olson. 1975. The elabora-
tion of the noun phrase in children?s description of
objects. Journal of Experimental Child Psychology,
19:371?382.
Albert Gatt and Anja Belz. 2010. Introducing shared
task evaluation to NLG: The TUNA shared task
evaluation challenges. In E. Krahmer and M. The-
une, editors, Empirical Methods in Natural Lan-
guage Generation. Springer, Berlin and Heidelberg.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA-REG challenge 2008: Overview and eval-
uation results. In Proceedings of the 5th Interna-
tional Conference on Natural Language Generation
(INLG), Salt Fork, OH.
528
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th Euro-
pean Workshop on Natural Language Generation
(ENLG), Athens.
A. Gatt, E. Krahmer, R. P. G. van Gompel, and K. van
Deemter. 2012. Does domain size impact speech
onset time during reference production? In Pro-
ceedings of the 34th Annual Meeting of the Cog-
nitive Science Society, pages 1584?1589, Sapporo,
Japan.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 410?419, Cambridge, MA, October.
Association for Computational Linguistics.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
M. Handford. 1987. Where?s Wally? Walker Books,
London, 3 edition.
M. Handford. 1988. Where?s Wally Now? Walker
Books, London, 4 edition.
L. Itti and C. Koch. 2000. A saliency-based search
mechanism for overt and covert shifts of visual at-
tention. Vision research, 40(10-12):1489?1506.
John D. Kelleher and Geert-Jan M. Kruijff. 2006. In-
cremental generation of spatial referring expressions
in situated dialog. In ACL.
J. Kelleher, F. Costello, and J. van Genabith. 2005.
Dynamically structuring, updating and interrelating
representations of visual and linguistic discourse
context. Artificial Intelligence, 167(12):62 ? 102.
Connecting Language to the World.
Emiel Krahmer and Kees van Deemter. 2012. Com-
putational generation of referring expressions: A
survey. Computational Linguistics, 38(1):173?218,
March.
Claudia Maienborn. 2001. On the position and inter-
pretation of locative modifiers. Natural Language
Semantics, 9(2):191?240.
Crystal Nakatsu and Michael White. 2010. Generat-
ing with discourse combinatory categorial grammar.
Linguistic Issues in Language Technology, 4(1).
T. Pechmann. 2009. Incremental speech produc-
tion and referential overspecification. Linguistics,
27(1):89?110.
Ellen Prince. 1981. Toward a taxonomy of given-new
information. In Peter Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press, New York.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Replicated softmax: an undirected topic model. In
Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I.
Williams, and A. Culotta, editors, Advances in Neu-
ral Information Processing Systems 22, pages 1607?
1614.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural
networks. In Proceedings of the 26th International
Conference on Machine Learning (ICML).
A. Toet. 2011. Computational versus psychophysi-
cal bottom-up image saliency: A comparative eval-
uation study. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 33(11):2131 ?2146.
A. Torralba, A. Oliva, M. Castelhano, and J. M. Hen-
derson. 2006. Contextual guidance of attention in
natural scenes: The role of global features on object
search. Psychological Review, 113:766?786.
Jette Viethen and Robert Dale. 2008. The use of spa-
tial relations in referring expressions. In Proceed-
ings of the 5th International Conference on Natural
Language Generation, Salt Fork, Ohio, USA.
Gregory Ward and Betty Birner. 1995. Definiteness
and the English existential. Language, 71(4):722?
742, December.
Gregory Ward and Betty Birner. 2001. Discourse and
information structure. In Deborah Schiffrin, Debo-
rah Tannen, and Heidi Hamilton, editors, Handbook
of discourse analysis, pages 119?137. Basil Black-
well, Oxford.
Bonnie L. Webber. 2004. D-ltag: extending lexical-
ized tag to discourse. Cognitive Science, 28(5):751?
779.
Michael White and Rajakrishnan Rajkumar. 2012.
Minimal dependency length in realization ranking.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 244?255, Jeju Island, Korea, July. Association
for Computational Linguistics.
Jeremy M. Wolfe. 2012. Visual search. In P. Todd,
T. Holls, and T. Robbins, editors, Cognitive Search:
Evolution, Algorithms and the Brain, pages 159 ?
175. MIT Press, Cambridge, MA, USA.
Sina Zarrie?, Aoife Cahill, and Jonas Kuhn. 2012.
To what extent does sentence-internal realisation re-
flect discourse context? a study on word order. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 767?776, Avignon, France, April.
Association for Computational Linguistics.
529
Disentangling Chat
Micha Elsner?
Brown Laboratory for Linguistic
Information Processing (BLLIP)
Eugene Charniak??
Brown Laboratory for Linguistic
Information Processing (BLLIP)
When multiple conversations occur simultaneously, a listener must decide which conversation
each utterance is part of in order to interpret and respond to it appropriately. We refer to this task
as disentanglement. We present a corpus of Internet Relay Chat dialogue in which the various
conversations have been manually disentangled, and evaluate annotator reliability. We propose
a graph-based clustering model for disentanglement, using lexical, timing, and discourse-based
features. The model?s predicted disentanglements are highly correlated with manual annotations.
We conclude by discussing two extensions to the model, specificity tuning and conversation start
detection, both of which are promising but do not currently yield practical improvements.
1. Motivation
Simultaneous conversations seem to arise naturally in both informal social interactions
and multi-party typed chat. Aoki et al?s (2006) study of voice conversations among
8?10 people found an average of 1.76 conversations (floors) active at a time, and a
maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical
conversation, therefore, does not form a contiguous segment of the chatroom transcript,
but is frequently broken up by interposed utterances from other conversations.
Disentanglement (also called thread detection [Shen et al 2006], thread extraction
[Adams and Martell 2008], and thread/conversation management [Traum 2004]) is
the clustering task of dividing a transcript into a set of distinct conversations. It is
an essential prerequisite for any kind of higher-level dialogue analysis. For instance,
consider the multi-party exchange in Figure 1.
Contextually, it is clear that this corresponds to two conversations, and Felicia?s1
response excellent is intended for Chanel and Regine. A straightforward reading of the
transcript, however, might interpret it as a response to Gale?s statement immediately
preceding.
? Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.
E-mail: melsner@cs.brown.edu.
?? Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.
E-mail: ec@cs.brown.edu.
1 Real user nicknames are replaced with randomly selected identifiers for ethical reasons.
Submission received: 27 January 2009; revised submission received: 1 November 2009; accepted for
publication: 3 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
Figure 1
Some (abridged) conversation from our corpus.
Humans are adept at disentanglement, even in complicated environments like
crowded cocktail parties or chat rooms; in order to perform this task, they must maintain
a complex mental representation of the ongoing discourse. Moreover, they adjust their
conversational behavior to make the task easier, mentioning names more frequently
than in spoken or two-party typed dialogues (O?Neill and Martin 2003). This need for
adaptation suggests that disentanglement can be challenging even for humans, and
therefore can serve as a useful stress test for computational models of discourse.
Disentanglement has two practical applications. One is the analysis of pre-recorded
transcripts in order to extract some kind of information, such as question?answer pairs
or summaries. These tasks should probably take as input each separate conversation,
rather than the entire transcript. Another application is as part of a user-interface system
for active participants in the chat, in which users target a conversation of interest
which is then highlighted for them. Aoki et al (2003) created such a system for speech,
which users generally preferred to a conventional system?when the disentanglement
worked!
We begin in Section 2 with an overview of related work. In Section 3, we present a
new corpus of manually annotated chat room data and evaluate annotator reliability. We
give a set of metrics (Section 3.2) describing structural similarity both locally and glob-
ally. In Section 4, we propose a model which uses supervised pairwise classification to
link utterances from the same conversation, followed by a greedy inference stage which
clusters the utterances into conversations. Our system uses time gap and utterance con-
tent features. Experimental results (Section 5) show that its output is highly correlated
with human annotations. Finally, in Sections 6 and 7, we investigate two extensions to
the basic model, specificity tuning and automatic detection of conversation starts.
To our knowledge, this is the first work to evaluate interannotator agreement for
the disentanglement task. It is also the first to use a supervised method to learn weights
for different feature types, rather than relying on cosine distance with uniform or hand-
tuned feature weights. It supplements standard word repetition and time gap features
with other feature types, including very powerful features based on name mentioning,
which is common in Internet Relay Chat.
2. Related Work
Several threads of research are direct attempts to solve the disentanglement problem.
The closest to our own work is that of Shen et al (2006), which performs conversation
disentanglement on an online chat corpus. Aoki et al (2003, 2006) disentangle speech,
rather than chat. Other work has slightly different goals than ours: Adams and Martell
(2008) attempt to find all utterances of a specific single conversation in Internet and
Navy tactical chat. Camtepe et al (2005) and Acar et al (2005) perform social network
390
Elsner and Charniak Disentangling Chat
analysis, extracting groups of speakers who talk to one another. This can be considered
a disentanglement task, although, as we will see (Section 5), the assumption that each
speaker participates in only one conversation is flawed.
Adams and Martell (2008) and Shen et al (2006) publish results on human-
annotated data; although we do not have their corpora, we discuss their evaluation
metrics (Section 3.2) and give a comparison to our own results herein (Section 5). Aoki
et al (2006) construct an annotated speech corpus, but they give no results for model
performance, only user satisfaction with their conversational system. Camtepe et al
(2005) and Acar et al (2005) do give performance results, but only on synthetic data.
Adams and Martell (2008) and Shen et al (2006) treat disentanglement in the same
way we do, as a clustering task where the objects to be clustered are the individual
utterances. Both their algorithms define a notion of distance (based on the cosine) and
a threshold parameter determining how close to the cluster center an utterance must be
before the clustering algorithm adds it. This stands in contrast to our own supervised
approach, where the distance metric is explicitly tuned on training data. The super-
vised method has the advantage that it can weigh individual feature types based on
their predictivity?unsupervised methods combine features either uniformly or using
heuristic methods. There is also no need for a separate tuning phase to determine the
threshold. On the other hand, supervised methods require labeled training data, and
may be more difficult to adapt to novel domains or corpora.
The remaining papers treat the problem as one of clustering speakers, rather than
utterances. That is, they assume that during the window over which the system oper-
ates, a particular speaker is engaging in only one conversation. Camtepe et al (2005)
state an explicit assumption that this is true throughout the entire transcript; real speak-
ers, by contrast, often participate in many conversations, sequentially or sometimes
even simultaneously. Aoki et al (2003) analyze each 30-second segment of the transcript
separately. This makes the single-conversation restriction somewhat less severe, but has
the disadvantage of ignoring all events which occur outside the segment.
Acar et al (2005) attempt to deal with this problem by using a fuzzy algorithm to
cluster speakers; this assigns each speaker a distribution over conversations rather than
making a hard assignment. However, the algorithm still deals with speakers rather than
utterances, and cannot determine which conversation any particular utterance is part of.
Another problem with these two approaches is the information used for clustering.
Aoki et al (2003) and Camtepe et al (2005) detect the arrival times of messages, and
use them to construct an affinity graph between participants by detecting turn-taking
behavior among pairs of speakers. (Turn-taking is typified by short pauses between
utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al (2006) find
that turn-taking on its own is inadequate. They motivate a richer feature set, which,
however, does not yet appear to be implemented. Acar et al (2005) add word repetition
to their feature set. However, their approach deals with all word repetitions on an equal
basis, and so degrades quickly in the presence of ?noise words? (their term for words
which are shared across conversations) to almost complete failure when only half of the
words are shared.
Adams and Martell (2008) and Shen et al (2006) use a more robust representation
for lexical features: Term frequency?inverse document frequency (TF?IDF) weighted
unigrams, as often used in information extraction. This feature set works fairly well
alone, although time is also a key feature as in the other studies. Adams and Martell
(2008) also investigate WordNet hypernyms (Miller et al 1990) as a measure of semantic
relatedness, and use the identity of the speaker of a particular utterance as a feature. It
is unclear from their results whether these latter two features are effective or not.
391
Computational Linguistics Volume 36, Number 3
To motivate our own approach, we examine some linguistic studies of discourse,
especially analysis of multi-party conversation. O?Neill and Martin (2003) point out
several ways in which multi-party text chat differs from typical two-party conversation.
One key difference is the frequency with which participants mention each others?
names. They hypothesize that name mentioning is a strategy which participants use
to make disentanglement easier, compensating for the lack of cues normally present in
face-to-face dialogue. Mentions (such as Gale?s comments to Arlie in Figure 1) are very
common in our corpus, occurring in 36% of comments, and provide a useful feature.
Another key difference is that participants may create a new conversation (floor) at
any time, a process which Sacks, Schegloff, and Jefferson (1974) calls schisming. During
a schism, a new conversation is formed, not necessarily because of a shift in the topic,
but because certain participants have refocused their attention onto each other, and
away from whoever held the floor in the parent conversation.
Despite these differences, there are still strong similarities between chat and other
conversations such as meetings. Meetings do not typically allow multiple simultaneous
conversations, but analogues to schisms do exist, in the form of digressions or ?sub-
ordinate conversations,? in which the speaker addresses someone specific, who is then
expected to answer. Some meeting analysis systems attempt to discover where these
digressions begin, who is involved in them, and who has the floor when they end;
features used in this work are relevant to disentanglement.
The task of automatically determining the intended recipient of an utterance in a
meeting is called addressee identification. It requires detecting digressions and identi-
fying their participants. Several studies attempt this task. Jovanovic and op den Akker
(2004) and Jovanovic, op den Akker, and Nijholt (2006) perform addressee identification
using a complex feature set including linguistic cues like pronouns and discourse mark-
ers, temporal information, and gaze direction. They also find that addressee identity
can be annotated with high reliability (? = .7 for one set and .8 for another). Traum
(2004) discusses the necessity for addressee identification and disentanglement in the
design of a system for military dialogues involving virtual agents. Subsequent work
(Traum, Robinson, and Stephan 2004) develops a rule-based system with high accuracy
on addressee identification.
Another meeting-related task is floor tracking, which attempts to determine which
speaker has the floor after each utterance. This task involves modeling the coordination
strategies which speakers use to acquire or give up the floor, and so provides a good
model of an ongoing conversation. A detailed analysis is given in Chen et al (2006);
Chen (2008) also gives a model for detecting floor shifts. Hawes, Lin, and Resnik (2008)
use a conditional random fields (CRF) model to predict the next speaker in Supreme
Court oral argument transcripts.
A somewhat related area of research involves environments with higher latency
than real-time chat: message boards and e-mail. Content matching approaches tend
to work better in these settings, because while many chat messages are backchannel
responses or discourse requests (Yes, Why? and so forth), longer posts tend to be
contentful. Of the two tasks, e-mail is easier; Yeh and Harnly (2006) find that heuristic
information from message headers can be useful, as can content-based matching such
as detecting quotes from earlier messages. Wang et al (2008), an analysis of a student
discussion group, is the work on which Adams and Martell (2008) is based, and uses
very similar methodology based on TF-IDF.
Our two-stage classification and partitioning algorithm draws on work on corefer-
ence resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001)
use such an approach, building global clusters based on pairwise decisions made by a
392
Elsner and Charniak Disentangling Chat
classifier. The global partitioning problem was identified as correlation clustering, an
NP-hard problem, by McCallum and Wellner (2004).
Finally, we briefly mention some work which appeared after we developed the
system described here. Wang and Oard (2009) is another system that uses TF?IDF
unigrams, but augments these feature vectors using the information retrieval technique
of message expansion. They report results on our corpus which improve on our own.
Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to
describe semantic relatedness. He finds both techniques ineffective. In addition, he
annotates a large corpus of Internet Relay Chat and similarly finds that annotators have
trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies,
improving on the greedy algorithm that we present.
3. Data Set
Our data set is recorded from IRC channel ##LINUX at free?node.net, using the freely
available gaim client. ##LINUX is an unofficial tech support line for the Linux operating
system, selected because it is one of the most active chat rooms on freenode, leading
to many simultaneous conversations, and because its content is typically inoffensive.
Although it is notionally intended only for tech support, it includes large amounts of
social chat as well, such as the conversation about factory work in Figure 1.
The entire data set contains over 52 hours of chat, but we devote most of our
attention to three annotated sections: development (706 utterances; 2:06 hr) and test
(800 utterances; 1:39 hr), plus a short pilot section on which we tested our annotation
system (359 utterances; 0:58 hr).
3.1 Annotation
We recruited and paid seven university students to annotate the test section. All had
at least some familiarity with the Linux OS, although in some cases very slight. Anno-
tation of the test data set typically took them about two hours. In all, we produced six
annotations of the test set.2
We have four annotations of the pilot set, by three volunteers and the experimenters.
The pilot set was used to prototype our annotation software, and also as a validation cor-
pus for our system. The development set was annotated only once, by the experimenter.
This data set is used for training.
Our annotation scheme marks each utterance as part of a single conversation.
Annotators are instructed to create as many or as few conversations as they need to
describe the data. Our instructions state that a conversation can be between any number
of people, and that, ?We mean conversation in the typical sense: a discussion in which
the participants are all reacting and paying attention to one another . . . it should be clear
that the comments inside a conversation fit together.? The annotation system itself is a
simple Java program with a graphical interface, intended to appear somewhat similar
to a typical chat client. Each speaker?s name is displayed in a different color, and the
system displays the elapsed time between comments, marking especially long pauses
2 One additional annotation was discarded because the annotator misunderstood the task.
393
Computational Linguistics Volume 36, Number 3
in red. Annotators group utterances into conversations by clicking and dragging them
onto each other.
3.2 Metrics
Before discussing the annotations themselves, we will describe the metrics we use to
compare different annotations; these measure both how much our annotators agree
with each other, and how well our model and various baselines perform. Comparing
clusterings with different numbers of clusters is a non-trivial task, and metrics for
agreement on supervised classification, such as the ? statistic, are not applicable.
To measure global similarity between annotations, we use one-to-one accuracy.
This measure describes how well we can extract whole conversations intact, as required
for summarization or information extraction. To compute it, we pair up conversations
from the two annotations to maximize the total overlap by computing an optimal max-
weight bipartite matching, then report the percentage of overlap found.3 One-to-one
accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi
and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference
resolution.
If we intend to monitor or participate in the conversation as it occurs, we will care
more about local judgments. The local agreement metric is a constrained form of the
Rand index for clusterings (Rand 1971) which counts agreements and disagreements for
pairs within a context k. We consider a particular utterance: The previous k utterances
are each in either the same or a different conversation. The lock score between two
annotators is their average agreement on these k same/different judgments, averaged
over all utterances. For example, loc1 counts pairs of adjacent utterances for which two
annotations agree.
Several related papers use some variant of the F-score metric to measure accuracy.
The most complete treatment is given in Shen et al (2006). They use a micro-averaged
F-score, which is defined by constructing a multiway matching between conversations
in the two annotations. For a gold conversation i with size ni, and a proposed conver-
sation j with size nj, with overlap of size nij, they define precision and recall (plus the
standard balanced F-score). The F-score of an entire annotation is a weighted sum over
the matching:
P =
nij
nj
R =
nij
ni
F(i, j) = 2PR
P+ R
F =
?
i
ni
n maxjF(i, j) (1)
This is the F-score we report for comparative purposes. Because the match is multiway,
the score is not symmetric; when measuring agreement between pairs of human anno-
tators (where there is no reason for one to be considered gold), we map the high-entropy
transcript to the lower one (the entropy of a transcript is defined subsequently, in
Equation 2). Micro-averaged F-scores are also popular in work on document clustering.
In general, scores using this metric are correlated with our other measurement of global
consistency, the one-to-one accuracy.
3 The matching can be computed efficiently with the so-called Hungarian algorithm or by reduction to max
flow. The widely used greedy algorithm is a two-approximation, although we have not found large
differences in practice.
394
Elsner and Charniak Disentangling Chat
Adams and Martell (2008) also report F-score, but using a somewhat different
definition. They define F-score only between a particular pair of conversations, and
report the score for a single selected conversation. They do not describe how this
reference conversation is chosen. It is also unclear how they determine which proposed
conversation to match to it?the one with the best F-score, or the one which contains
the first (or ?root?) utterance of the reference conversation. (The latter, although it may
be more useful for some applications, has an obvious problem?if the conversation is
retrieved perfectly except for the root utterance, the score will be zero.) For these reasons
we do not evaluate their metric.
3.3 Discussion
A statistical examination of our data (Table 1) shows that there is a substantial amount
of disentanglement to do: the average number of conversations active at a time (the
density) is 2.75. Our annotators have high agreement on the local metric (average of
81.1%). On the one-to-one metric, they disagree more, with a mean overlap of 53.0% and
a maximum of only 63.5%. Though this level of agreement is low, naive baselines score
even lower (see Section 5). Therefore the metric does indeed distinguish human-like
from baseline performance. Thus measuring one-to-one overlap with our annotations
is a reasonable evaluation for computational models. However, we feel that the major
source of disagreement is one that can be remedied in future annotation schemes: the
specificity of the individual annotations.
To measure the level of detail in an annotation, we use the information-theoretic
entropy of the random variable, which indicates which conversation an utterance is
in. This variable has as many potential values as the number of conversations in the
transcript, each value having probability proportional to its size. Thus, for a transcript
of length n, with conversations i each having size ni, the entropy is:
H(c) =
?
i
ni
n log2
ni
n (2)
This quantity is non-negative, increasing as the number of conversations grow and their
size becomes more balanced. It reaches its maximum, 9.64 bits for this data set, when
Table 1
Statistics on 6 annotations of 800 utterances of chat transcript. Inter-annotator agreement metrics
(below the line) are calculated between distinct pairs of annotations.
Mean Max Min
Conversations 81.33 128 50
Average conversation length 10.6 16.0 6.2
Average conversation density 2.75 2.92 2.53
Entropy 4.83 6.18 3.00
one-to-one 52.98 63.50 35.63
loc3 81.09 86.53 74.75
Many-to-1 (by entropy) 86.70 94.13 75.50
Shen F (by entropy) 53.87 66.08 35.43
395
Computational Linguistics Volume 36, Number 3
each utterance is placed in a separate conversation. In our annotations, it ranges from
3.0 to 6.2. This large variation shows that some annotators are more specific than others,
but does not indicate how much they agree on the general structure. To measure this,
we introduce the many-to-one accuracy. This measurement is asymmetrical, and maps
each of the conversations of the source annotation to the single conversation in the
targetwith which it has the greatest overlap, then counts the total percentage of overlap.
This is not a statistic to be optimized (indeed, optimization is trivial: Simply make each
utterance in the source into its own conversation), but it can give us some intuition
about specificity. In particular, if one subdivides a coarse-grained annotation to make a
more specific variant, the many-to-one accuracy from fine to coarse remains 1. When we
map high-entropy annotations (fine) to lower ones (coarse), we find high many-to-one
accuracy, with a mean of 86%, which implies that the more specific annotations have
mostly the same large-scale boundaries as the coarser ones.
By examining the local metric, we can see even more: Local correlations are good,
at an average of 81.1%. This means that, in the three-sentence window preceding each
sentence, the annotators are often in agreement. If they recognize subdivisions of a large
conversation, these subdivisions tend to be contiguous, not mingled together, which is
why they have little impact on the local measure.
We find reasons for the annotators? disagreement about appropriate levels of detail
in the linguistic literature. As mentioned, new conversations often break off from old
ones in schisms. Aoki et al (2006) discuss conversational features associated with
schisming and the related process of affiliation, by which speakers attach themselves to
a conversation. Schisms often branch off from asides or even normal comments (toss-
outs) within an existing conversation. This means that there is no clear beginning to the
new conversation?at the time when it begins, it is not clear that there are two separate
floors, and this will not become clear until distinct sets of speakers and patterns of turn-
taking are established. Speakers, meanwhile, take time to orient themselves to the new
conversation. Example schisms are shown in Figures 2 and 3.
Our annotation scheme requires annotators to mark each utterance as part of a
single conversation, and distinct conversations are not related in any way. If a schism
occurs, the annotator is faced with two options: If it seems short, they may view it as
a mere digression and label it as part of the parent conversation. If it seems to deserve
a place of its own, they will have to separate it from the parent, but this severs the
initial comment (an otherwise unremarkable aside) from its context. One or two of the
annotators actually remarked that this made the task confusing. Our annotators seem
Figure 2
A schism occurring in our corpus (abridged). The schism-inducing turn is Kandra?s comment,
marked by arrows. Annotators 0 and 2 begin a new conversation with this turn; 1, 4, and 5 group
it with the other utterances shown; 3 creates new conversations for both this turn and Madison?s
question immediately following.
396
Elsner and Charniak Disentangling Chat
Figure 3
A schism occurring in our corpus (abridged): not all annotators agree on where the thread about
charging for answers to technical questions diverges from the one about setting up Paypal
accounts. The schism begins just after Lai?s second comment (marked with arrows), to which
Gale and Azzie both respond (marked with double arrows). Annotators 1, 2, 4, and 5 begin a new
conversation with Gale?s response. Annotator 0 starts a new conversation with Azzie?s response.
Annotator 3 makes an error, linking the two responses to each other, but not to the parent.
Figure 4
Utterances versus conversations participated in per speaker on development data.
to be either ?splitters? or ?lumpers??in other words, each annotator seems to aim for
a consistent level of detail, but each one has their own idea of what this level should be.
As a final observation about the data set, we test the appropriateness of the assump-
tion (used in previous work) that each speaker takes part in only one conversation. In
our data, the average speaker takes part in about 3.3 conversations (the actual number
varies for each annotator). The more talkative a speaker is, the more conversations they
participate in, as shown by a plot of conversations versus utterances (Figure 4). The
assumption is not very accurate, especially for speakers with more than 10 utterances.
4. Model
Our model for disentanglement fits into the general class of graph partitioning algo-
rithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including
coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting seg-
mentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First,
397
Computational Linguistics Volume 36, Number 3
Table 2
Feature functions with performance on development data.
Chat-specific (Acc: 73 Prec: 73 Rec: 61 F: 66)
Time The time between x and y in seconds, discretized into logarithmically sized bins.
Speaker x and y have the same speaker.
Mention x-y x mentions the speaker of y (or vice versa). For example, this feature is true for
a pair such as: Felicia ?Gale: ... and any utterance spoken by Gale.
Mention same Both x and y mention the same name.
Mention other either x or y mentions a third person?s name.
Discourse (Acc: 52 Prec: 47 Rec: 77 F: 58)
Cue words Either x or y uses a greeting (hello etc.), an answer (yes, no etc.), or thanks.
Question Either asks a question (explicitly marked with ?).
Long Either is long (> 10 words).
Content (Acc: 50 Prec: 45 Rec: 74 F: 56)
Repeat(i) The number of words shared between x and y which have unigram
probability i, binned logarithmically.
Tech Whether both x and y use technical jargon, neither do, or only one does.
Combined (Acc: 75 Prec: 73 Rec: 68 F: 71)
a binary classifier marks each pair of items as alike or different, and second, a consistent
partition is extracted.4
4.1 Classification
We use a maximum-entropy classifier (Daume? 2004) to decide whether a pair of utter-
ances x and y are in same or different conversations. The most likely class is different,
which occurs 57% of the time in the development data. We describe the classifier?s per-
formance in terms of raw accuracy (correct decisions/total), precision and recall of the
same class, and F-score, the harmonic mean of precision and recall. Our classifier uses
several types of features (Table 2). The chat-specific features yield the highest accuracy
and precision. Discourse and content-based features have poor accuracy on their own
(worse than the baseline), because they work best on nearby pairs of utterances, and
tend to fail on more distant pairs. Paired with the time gap feature, however, they boost
accuracy somewhat and produce substantial gains in recall, encouraging the model to
group related utterances together.
The classifier is trained on our single annotation of the 706-utterance development
section and validated against the 359-utterance pilot section.
4 Our first attempt at this task used a Bayesian generative model. However, we could not define a sharp
enough posterior over new sentences, which made the model unstable and overly sensitive to its prior.
398
Elsner and Charniak Disentangling Chat
Figure 5
Distribution of pause length (log-scaled) between utterances in the same conversation.
The time gap, as discussed earlier, is the most widely used feature in previous work.
Our choice of a logarithmic binning scheme is intended to capture two characteristics of
the distribution of pause lengths (shown in Figure 5). The curve has its maximum at 1?
3 seconds, and pauses shorter than a second are less common. This reflects turn-taking
behavior among participants; participants in the same conversation prefer to wait for
each others? responses before speaking again. On the other hand, the curve is quite
heavy-tailed to the right, leading us to bucket long pauses fairly coarsely. The specific
discretization we adopt for a time gap ? is bin(?) = floor(log1.5(?+ 1)). The particular
choice of 1.5 was chosen by hand to fit the observed scale of the curve.
Our discourse-based features model some pairwise relationships: questions fol-
lowed by answers, short comments reacting to longer ones, greetings at the beginning,
and thanks at the end.
Word repetition is a key feature in nearly every model for segmentation or coher-
ence, so it is no surprise that it is useful here. We discard the 50 most frequent words.
Then we bin all words by their unigram probability (bin(w) = floor(log10(p(w))) and
create an integer-valued feature for each bin, equal to the number of repeated words in
that bin. Unigram probabilities are calculated over the entire 52 hours of transcript. The
binning scheme allows us to deal with ?noise words? which are repeated coincidentally,
because these occur in high-probability bins where repetitions are given less weight.
The point of the repetition feature is of course to detect sentences with similar
topics. We also find that sentences with technical content are more likely to be related
than non-technical sentences. We label an utterance as technical if it contains a Web
address, a long string of digits, or a term present in a guide for novice Linux users5
but not in a large news corpus (Graff 1995).6 This is a lightweight way to capture
one ?semantic dimension? or cluster of related words. The technical word feature was
included because it improves our development classification score slightly, but it does
not have a significant effect on overall performance. Adams (2008) attempts to add
more semantic dimensions learned via Latent Dirichlet Allocation, and similarly finds
no improvement.
Pairs of utterances which are widely separated in the discourse are unlikely to be
directly related?even if they are part of the same conversation, the link between them
is probably a long chain of intervening utterances. Thus, if we run our classifier on a
pair of very distant utterances, we expect it to default to the majority class, which in this
case will be different, and this will damage our performance in case the two are really
part of the same conversation. To deal with this, we run our classifier only on utterances
5 Introduction to Linux: A Hands-on Guide. Machtelt Garrels. Edition 1.25 from
http://tldp.org/LDP/intro-linux/html/intro-linux.html.
6 Our data came from the LA Times, 1994?1997 ? helpfully, this corpus predates the current wide coverage
of Linux in the mainstream press.
399
Computational Linguistics Volume 36, Number 3
Figure 6
VOTE algorithm.
separated by 129 seconds or less. The cutoff of 129 seconds was chosen because, for
utterances further apart than this, the classifier has no significant advantage over the
majority baseline. For 99.9% of utterances in an ongoing conversation, the previous
utterance in that conversation is within this gap, and so the system has a chance of
correctly linking the two.
On test data, the classifier has a mean accuracy of 68.2 (averaged over annotations).
The mean precision of same conversation is 53.3 and the recall is 71.3, with a mean
F-score of 60. This error rate is high, but the partitioning procedure allows us to recover
from some of the errors, because if nearby utterances are grouped correctly, the bad
decisions will be outvoted by good ones.
4.2 Partitioning
The next step in the process is to cluster the utterances. We wish to find a set of clusters
for which the weighted accuracy of the classifier would be maximal; this is an example
of correlation clustering (Bansal, Blum, and Chawla 2004), which is NP-complete. The
input to our partitioning procedure is a graph with a node for each utterance; if the
classifier connects utterances i and j with probability p, we take the weight wij of edge ij
to be the log odds log(pij/(1 ? pij)).7 We create a variable xij for each pair of utterances,
which is 1 if the utterances are placed in the same conversation, and 0 if they are
separated. The log probability of the clustering, treating the edges as independent, is
?
ij:i<j wijxij. We attempt to maximize this quantity, subject to the constraint that the xij
must form a legitimate clustering such that xij = xjk = 1 implies xij = xik.
Finding an exact solution proves to be difficult; the problem has a quadratic number
of variables (one for each pair of utterances) and a cubic number of triangle inequality
constraints (three for each triplet).8 With 800 utterances in our test set, even solving
the linear relaxation of the problem with CPLEX (Ilog, Inc. 2003) is too expensive to be
practical.
Experiments on a variety of heuristic algorithms (Elsner and Schudy 2009) show
that a relatively good solution can be obtained using a greedy voting algorithm (Fig-
ure 6). In this algorithm, we assign utterance j by examining all previously assigned
7 The original version of our system used a different weighting scheme, wij = pij ? .5. The log-odds ratio
behaves similarly for our basic algorithm, but appears to be more robust to other partitioning algorithms
or tuning (see Section 6), so, for simplicity, we present it here as well.
8 There is a triangle inequality constraint for each triplet i, j, k: (1 ? xik ) ? (1 ? xij )+ (1 ? xjk ).
400
Elsner and Charniak Disentangling Chat
utterances i, and treating the classifier?s judgment wij as a vote for cluster(i). If the
maximum vote is greater than 0, we set cluster( j) = argmaxc votec. Otherwise j is put
in a new cluster.
If the utterances are considered in order, this is a natural on-line algorithm?it
assigns each utterance as it arrives, without reference to the future. Elsner and Schudy
(2009) show that performance can be improved by approximately 6% on the one-to-one
and F-score metrics using offline randomized and local search methods. The loc3 metric
is insensitive to these more complex search procedures.
5. Experiments
We annotate the 800-line test transcript using our system. The annotation obtained has
62 conversations, with mean length 12.90. The average density of conversations is 2.86,
and the entropy is 3.72. This places it within the bounds of our human annotations (see
Table 1), toward the more general end of the spectrum.
As a standard of comparison for our system, we provide results for several
baselines?trivial systems which any useful annotation should outperform.
All different Each utterance is a separate conversation.
All same The whole transcript is a single conversation.
Blocks of k Each consecutive group of k utterances is a conversation.
Pause of k Each pause of k seconds or more separates two conversations.
Speaker Each speaker?s utterances are treated as a monologue.
For each particular metric, we calculate the best baseline result among all of these.
To find the best block size or pause length, we search over multiples of five between
5 and 300. This makes these baselines appear better than they really are, because their
performance is optimized with respect to the test data. (A complete table of baseline
results is shown in Figure 7.)
We also calculate results for two more systems. One is a non-trivial baseline:
Time/mention Our system, using only time gap and mention-based features.
The other is an oracle, designed to test how well a segmentation system designed for
meeting or lecture data might possibly do on this task. If no conversation were ever
interrupted, such a system would be perfect (up to the limit of annotator agreement).
Figure 7
Metric values for all baselines.
401
Computational Linguistics Volume 36, Number 3
Table 3
Metric values between proposed annotations and human annotations. Model scores typically fall
between inter-annotator agreement and baseline performance.
Annotators Model Time/ment. Perf. Seg. Best Baseline
Mean one-to-one 52.98 41.23 38.62 26.20 35.08 (Pause 35)
Max one-to-one 63.50 52.12 44.12 36.50 56.00 (Pause 65)
Min one-to-one 35.63 31.62 30.62 15.38 27.50 (Blocks 80)
Mean loc3 81.09 72.94 68.69 75.98 62.16 (Speaker)
Max loc3 86.53 74.70 70.93 85.40 69.05 (Speaker)
Min loc3 74.75 70.77 66.37 69.05 54.37 (Speaker)
Mean Shen F 53.87 43.47 41.31 35.50 36.58 (Speaker)
Max Shen F 66.08 57.57 48.85 46.70 46.79 (Speaker)
Min Shen F 35.43 32.97 32.07 21.83 29.09 (Blocks 65)
Perfect segments The transcript is divided into contiguous segments, where all utter-
ances in a segment belong to the same conversation. The conversation assign-
ments are determined by the human annotation whose agreement with the others
is highest.
Our results, in Table 3, are encouraging. On average, annotators agree more with
each other than with any artificial annotation, and more with our model than with
the baselines. For the one-to-one accuracy metric, we cannot claim much beyond these
general results. The range of human variation is quite wide, and there are annotators
who are closer to baselines than to any other human annotator. As explained earlier,
this is because some human annotations are much more specific than others. For very
specific annotations, the best baselines are short blocks or pauses. For the most general,
marking all utterances the same does very well (although for all other annotations, it is
extremely poor).
For the local metric, the results are much clearer. There is no overlap in the ranges;
for every test annotation, agreement is highest with other annotators, then our model,
and finally the baselines. The most competitive baseline is one conversation per speaker,
which makes sense, since if a speaker makes two comments in a four-utterance win-
dow, they are very likely to be related.
The Shen F-score metric seems to perform similarly to the one-to-one accuracy,
which is unsurprising because they are both measures of global consistency. The largest
difference between them is that the speaker baseline outperforms blocks and pauses in
F-score (although not by very much), perhaps because it is more precise.
Shen et al (2006) report higher F-scores for their own best model: It obtains an
F-score of 61.2, whereas our model?s mean score is only 43.4. Because of the different
corpora, we are unable to explain this difference. Better results are also reported in Wang
and Oard (2009) and Elsner and Schudy (2009) (see Table 4).
Mention information alone is not sufficient for disentanglement; with only name
mention and time gap features, mean one-to-one is 38 and loc3 is 69. However, name
mention features are critical for our model. Without them, the classifier?s development
F-score drops from 71 to 56. The disentanglement system?s test performance decreases
proportionally; mean one-to-one falls to 36, and mean loc3 to 63, essentially baseline per-
formance. For some utterances, of course, name mentions provide the only reasonable
402
Elsner and Charniak Disentangling Chat
Table 4
Results reported by others on the same task.
Result F-score Notes
this model 43.4
Elsner and Schudy (2009) 50 improved partitioning inference
Wang and Oard (2009) 54 message expansion features
Shen et al (2006) 61.2 different corpus
clue to the correct decision, which is why humans mention names in the first place.
But our system is probably overly dependent on them, because they are very reliable
compared to our other features.
Because of the frequency with which conversations interleave, perfect segmenta-
tion alone is not sufficient to optimize either global metric, and generally does not
outperform the baselines. For the local metric, however, it generally does better than the
model. Here, performance depends mainly on whether the system can find the bound-
aries between one conversation and another, and it is less important to link the segments
of a particular conversation to one another, since these different segments often lie
outside the three-utterance horizon. Systems designed to detect segment boundaries,
like those for meetings, might contribute to improvement of this metric.
6. Specificity Tuning
Although our analysis shows that individual annotators can produce more or less
specific annotations of the same conversation, the system described here can produce
only a single annotation (for any given set of training data) with a fixed specificity. Now
we attempt to control the specificity parametrically, producing more and less specific
annotations on demand, without retraining the classifier.
The parameter we choose to alter is the bias of our pairwise classifier. A maximum-
entropy classifier has the form:
y(x) = 1
1+ exp(?(w ? x+ b))
(3)
Here w represents the vector of feature weights and b is the bias term; a positive b shifts
all judgments toward high-confidence same conversation decisions and a negative b
shifts them away. To alter the classifier, we add a constant ? to b. In general, increasing
the number and confidence of same decisions leads to larger, coarser partitionings,
and decreasing it creates smaller, finer ones. We measure specificity by examining the
entropy of the output annotation. Although entropy is generally an increasing function
of ?, the relationship is not always smooth, nor is it completely monotonic. Figure 8
plots entropy as a function of ?.
In Figure 9, we plot the one-to-one match between each test annotation and the
altered annotations produced by this method, as a function of the entropy. The unbiased
system creates an annotation with entropy 3.7. Although this yields reasonable results
for all human annotations, each of the annotations has a point of higher performance
at a different bias level. For instance, the line uppermost on the left side of the plot
403
Computational Linguistics Volume 36, Number 3
Figure 8
Entropy of the output annotation produced with bias factor ? on test data. ? = 0 corresponds to
the unbiased system.
shows overlap with a human transcript whose entropy is 3.0 bits; lower-entropy system
annotations correspond better with this annotator?s judgments.
For each human annotation, we evaluate the tuned system?s performance at the
entropy level of the original annotation. (This point is marked by the large dot on
Figure 9
One-to-one accuracy between biased system annotations and each test annotation, as a function
of entropy. The vertical line (at 3.72 bits) marks the scores obtained by the unbiased system with
? = 0. The large dot on each line is the score obtained at the entropy of the human annotation.
404
Elsner and Charniak Disentangling Chat
Table 5
Metric values between proposed annotations and human annotations on test data. The tuned
model (evaluated at the entropy of the human annotations) improves on one-to-one accuracy but
not on loc3.
Unbiased Model Tuned Model
Mean one-to-one 41.23 48.52
Max one-to-one 52.13 58.75
Min one-to-one 31.66 40.88
Mean loc3 72.94 73.64
Max loc3 74.70 75.87
Min loc3 70.77 69.95
each line in the figure.) To do this, we perform a line search over ? until we produce
a clustering whose entropy is within .25 bits of the original?s, then evaluate. In other
words, we measure performance given an additional piece of supervision?the annota-
tor?s preferred specificity level.
Results on the one-to-one metric are fairly good: Extreme and average scores are
listed in Table 5. The effects of this technique on the local metric are small (and in
many cases negative). This is not entirely surprising, as the local metric is less sensitive
to specificity of annotations. Slight positive effects occur only for the most and least
specific annotation, which are presumably so extreme that specificity begins to have a
slight effect even on local decisions.
Despite fairly large performance increases on the test set, we do not consider this
technique really reliable, because the relationship between the bias parameter and final
score is not smooth. Small changes in the bias can cause large shifts in entropy, and small
changes in entropy can have large effects on quality. (For instance, two annotations have
a sharp decline in score at about entropy 5.7, losing about 5% of performance with a
change of just over .1 bit.) Therefore it is not clear exactly how to choose a bias parameter
which will yield good performance. Matching the entropy of a human annotation seems
to work on the test data, but fails to improve scores on our development data. Moreover,
although for methodological simplicity we assume access to the exact target entropy for
each annotation, it is unlikely that a real user could express their desired specificity
so precisely. Figuring out a way to let the user select the desired entropy remains a
challenge.
7. Detecting Conversation Starts
In this section, we investigate better ways to find the beginnings of new conversations.
In the pairwise-linkage representation presented earlier, a new conversation is begun
when none of the previous utterances is strongly linked to the current utterance. This
representation spreads out the responsibility for detecting a new conversation over
many pairwise decisions. We are inspired by the use of discourse-new classifiers (also
called anaphoricity detectors) in coreference classification (Ng and Cardie 2002) to find
NPs which begin coreferential chains. Oracle experiments show that a similar detector
for utterances which begin conversations could improve disentanglement scores if it
were available. We attempt to develop such a detector, but without much success.
405
Computational Linguistics Volume 36, Number 3
Table 6
Metric values using an oracle new-conversation detector on test data.
Original Model +Oracle New Conversations
Mean one-to-one 41.23 46.75
Max one-to-one 52.13 53.50
Min one-to-one 31.66 42.13
Mean loc3 72.94 73.90
Max loc3 74.70 76.49
Min loc3 70.77 70.72
As a demonstration of the gains possible if a good classifier could be developed,
we show the oracle improvements possible on the test data, using an optimal new-
conversation detector as a hard constraint on inference (Table 6). The oracle detector
detects a conversation start if it occurs in the majority of human annotations, and the
inference algorithm is forced to start a new conversation if and only if the oracle has
detected one. Good conversation detection is capable of improving not only one-to-one
accuracy but local accuracy as well.
We can track the performance of realistic, non-oracle new-conversation detection
via the precision, recall, and F-score of the new conversation class (Table 7). As a
starting point, we report the accuracy obtained by the pairwise-linkage model and
greedy inference already presented. At 49% F-score, it is clearly not doing a good job.
It is possible to do better than this using information already represented in the
pairwise classifier: The time since the speaker of the utterance last spoke (logarithmi-
cally bucketed), and whether the utterance mentions a name. A better representation for
the problem allows the classifier to make somewhat more effective use of these features.
For reasons we cannot explain, adding discourse features like the presence of a question
or greeting does not improve performance. The simple classifier does improve slightly
on the baseline, up to 51%. These test results, however, are somewhat surprising to us.
On our development corpus, the corresponding scores are 69% and 75%. Because that
corpus contains an average (over three annotations) of 34 conversations, it is likely that
we were misled by coincidentally good results.
On the development set, where the classifier works well, its decisions can be inte-
grated with inference to yield substantial improvements in actual system performance.
Mean loc3 increases from 72% to about 78% and mean one-to-one accuracy from 41%
to about 66%. However, we find no improvement at all on the test data, because
the classifier has very low recall, and the resulting test annotations have far too few
conversations.
Table 7
Precision, recall, and F-score of the new conversation class on test data (average 81
conversations).
Precision Recall F-score
Pairwise system 56.08 43.44 48.96
Time/Mention Features 68.06 40.16 50.52
Human Annotators 64.30 61.70 61.14
406
Elsner and Charniak Disentangling Chat
8. Future Work
Although our annotators are reasonably reliable, it seems clear that they think of con-
versations as a hierarchy, with digressions and schisms. We are interested to see an an-
notation protocol which more closely follows human intuition. One suggestion (David
Traum, personal communication) is to drop the idea of partitioning entirely and have
annotators mark the data as a graph, linking each utterance to its parents and children
with links of various strengths. Such a scheme might yield more reliable annotations
than our current one, although testing this hypothesis would require new annotation
software and a different set of metrics. Any new annotation project should also investi-
gate whether annotators can define their desired specificity, and with what precision.
Our results on new conversation detection suggest that a high-performance clas-
sifier for this task could improve results substantially. It is also interesting to consider,
given the weakness of our technical words feature and the disappointing results using
Latent Dirichlet Allocation from Adams (2008), how semantic similarity might be use-
fully modeled.
Finally, we are interested to see how well this feature set performs on speech data,
as in Aoki et al (2003). Spoken conversation is more natural than text chat, but even
when participants are face-to-face, disentanglement remains a problem. On the other
hand, spoken dialogue contains new sources of information, such as prosody and gaze
direction. Turn-taking behavior is also more distinct, which makes the task easier, but
according to Aoki et al (2006), it is certainly not sufficient.
9. Conclusion
This work provides a corpus of annotated data for chat disentanglement, which, along
with our proposed metrics, should allow future researchers to evaluate and compare
their results quantitatively.9 Our annotations are consistent with one another, especially
with respect to local agreement. We show that features based on discourse patterns and
the content of utterances are helpful in disentanglement. The model we present can
outperform a variety of baselines.
Acknowledgments
Our thanks to Suman Karumuri, Steve
Sloman, Matt Lease, David McClosky, seven
test annotators, three pilot annotators, three
anonymous conference reviewers, three
anonymous journal reviewers, and the NSF
PIRE grant. We would also like to thank
Craig Martell and David Traum for their
very useful comments at the conference
presentation.
References
Acar, Evrim, Seyit Ahmet Camtepe,
Mukkai S. Krishnamoorthy, and Bu?lent
Yener. 2005. Modeling and multiway
analysis of chatroom tensors. In Paul B.
Kantor, Gheorghe Muresan, Fred Roberts,
Daniel Dajun Zeng, Fei-Yue Wang,
Hsinchun Chen, and Ralph C. Merkle,
editors, ISI, volume 3495 of Lecture Notes
in Computer Science. Springer, Berlin,
pages 256?268.
Adams, Paige H. 2008. Conversation Thread
Extraction and Topic Detection in Text-based
Chat. Ph.D. thesis, Naval Postgraduate
School.
Adams, Paige H. and Craig H. Martell. 2008.
Topic detection and extraction in chat.
International Conference on Semantic
Computing, 2:581?588.
Aoki, Paul M., Matthew Romaine,
Margaret H. Szymanski, James D.
Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter?s
cocktail party: A social mobile audio
9 Our software and data set are publicly available from cs.brown.edu/?melsner.
407
Computational Linguistics Volume 36, Number 3
space supporting multiple simultaneous
conversations. In CHI ?03: Proceedings of
the SIGCHI Conference on Human Factors
in Computing Systems, pages 425?432,
New York, NY.
Aoki, Paul M., Margaret H. Szymanski,
Luke D. Plurkowski, James D. Thornton,
Allison Woodruff, and Weilie Yi. 2006.
Where?s the ?party? in ?multi-party??:
Analyzing the structure of small-group
sociable talk. In CSCW ?06: Proceedings of
the 2006 20th Anniversary Conference on
Computer Supported Cooperative Work,
pages 393?402, New York, NY.
Bansal, Nikhil, Avrim Blum, and Shuchi
Chawla. 2004. Correlation clustering.
Machine Learning, 56(1-3):89?113.
Camtepe, Seyit Ahmet, Mark K. Goldberg,
Malik Magdon-Ismail, and Mukkai
Krishnamoorty. 2005. Detecting
conversing groups of chatters: A model,
algorithms, and tests. In IADIS AC,
pages 89?96, Algarve.
Chen, Lei. 2008. Incorporating Nonverbal
Features into Multimodal Models of
Human-to-Human Communication.
Ph.D. thesis, Purdue University.
Chen, Lei, Mary Harper, Amy Franklin,
Travis R. Rose, Irene Kimbara,
Zhongqiang Huang, and Francis
Quek. 2006. A multimodal analysis
of floor control in meetings. In
Proceedings of MLMI 06, pages 36?49,
Bethesda, MD.
Daume?, III, Hal. 2004. Notes on CG and
LM-BFGS optimization of logistic
regression. Paper available at http://
pub.hal3.name#daume04cg-bfgs.
Implementation available at http://
hal3.name/megam/.
Elsner, Micha and Warren Schudy. 2009.
Bounding and comparing methods for
correlation clustering beyond ILP. In
Proceedings of ILP-NLP, pages 19?27,
Boulder, CO.
Graff, David. 1995. North American News
Text Corpus. Linguistic Data Consortium.
LDC95T21.
Haghighi, Aria and Dan Klein. 2006.
Prototype-driven learning for sequence
models. In Proceedings of HLT-NAACL,
pages 320?327, New York, NY.
Hawes, Timothy, Jimmy Lin, and
Philip Resnik. 2008. Elements of a
computational model for multi-party
discourse: The turn-taking behavior
of supreme court justices. Technical
Report LAMP-TR-147/HCIL-2008-02,
University of Maryland, College Park.
Ilog, Inc. 2003. CPLEX solver. Available at
www-01.ibm.com/software/websphere/
ilog migration.html.
Jovanovic, Natasa and Rieks op den
Akker. 2004. Towards automatic
addressee identification in multi-party
dialogues. In Proceedings of the
5th SIGdial Workshop, pages 89?92,
Cambridge, MA.
Jovanovic, Natasa, Rieks op den Akker,
and Anton Nijholt. 2006. Addressee
identification in face-to-face meetings. In
Proceedings of EACL, Trento.
Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics. In
Proceedings of HLT-EMNLP, pages 25?32,
Morristown, NJ.
Malioutov, Igor and Regina Barzilay. 2006.
Minimum cut model for spoken lecture
segmentation. In Proceedings of ACL,
pages 25?32, Sydney.
McCallum, Andrew and Ben Wellner.
2004. Conditional models of identity
uncertainty with application to noun
coreference. In Proceedings of the 18th
Annual Conference on Neural Information
Processing Systems (NIPS), pages 905?912,
Vancouver.
Miller, G., A. R. Beckwith, C. Fellbaum,
D. Gross, and K. Miller. 1990. Introduction
to Wordnet: An on-line lexical database.
International Journal of Lexicography,
3(4):235?244.
Ng, Vincent and Claire Cardie. 2002.
Identifying anaphoric and non-anaphoric
noun phrases to improve coreference
resolution. In COLING, Taipei.
O?Neill, Jacki and David Martin. 2003. Text
chat in action. In GROUP ?03: Proceedings
of the 2003 International ACM SIGGROUP
Conference on Supporting Group Work,
pages 40?49, New York, NY.
Rand, William M. 1971. Objective criteria for
the evaluation of clustering methods.
Journal of the American Statistical
Association, 66(336):846?850.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks. In
Proceedings of CoNLL-2004, pages 1?8,
Boston, MA.
Sacks, Harvey, Emanuel A. Schegloff,
and Gail Jefferson. 1974. A simplest
systematics for the organization of
turn-taking for conversation. Language,
50(4):696?735.
Shen, Dou, Qiang Yang, Jian-Tao Sun,
and Zheng Chen. 2006. Thread detection
in dynamic text message streams. In
408
Elsner and Charniak Disentangling Chat
SIGIR ?06: Proceedings of the 29th Annual
International ACM SIGIR Conference,
pages 35?42, New York, NY.
Soon, Wee Meng, Hwee Tou Ng, and
Daniel Chung Yong Lim. 2001. A
machine learning approach to
coreference resolution of noun
phrases. Computational Linguistics,
27(4):521?544.
Traum, D. 2004. Issues in multi-party
dialogues. In F. Dignum, editor, Advances
in Agent Communication. Springer Verlag,
Berlin, pages 201?211.
Traum, David R., Susan Robinson, and
Jens Stephan. 2004. Evaluation of
multi-party virtual reality dialogue
interaction. In Proceedings of the Fourth
International Conference on Language
Resources and Evaluation (LREC),
pages 1699?1702, Lisbon.
Wang, Lidan and Douglas W. Oard. 2009.
Context-based message expansion for
disentanglement of interleaved text
conversations. In Proceedings of
HLT-NAACL, pages 200?208, Boulder, CO.
Wang, Yi-Chia, Mahesh Joshi, William
Cohen, and Carolyn Rose?. 2008.
Recovering implicit thread structure in
newsgroup style conversations. In
Proceedings of the 2nd International
Conference on Weblogs and Social Media
(ICWSM II), Seattle, WA.
Yeh, Jen-Yuan and Aaron Harnly. 2006.
Email thread reassembly using similarity
matching. In Conference on Email and
Anti-Spam, Mountain View, CA.
409

Proceedings of the ACL 2010 Conference Short Papers, pages 33?37,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Same-head Heuristic for Coreference
Micha Elsner and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@cs.brown.edu
Abstract
We investigate coreference relationships
between NPs with the same head noun.
It is relatively common in unsupervised
work to assume that such pairs are
coreferent? but this is not always true, es-
pecially if realistic mention detection is
used. We describe the distribution of non-
coreferent same-head pairs in news text,
and present an unsupervised generative
model which learns not to link some same-
head NPs using syntactic features, improv-
ing precision.
1 Introduction
Full NP coreference, the task of discovering which
non-pronominal NPs in a discourse refer to the
same entity, is widely known to be challenging.
In practice, however, most work focuses on the
subtask of linking NPs with different head words.
Decisions involving NPs with the same head word
have not attracted nearly as much attention, and
many systems, especially unsupervised ones, op-
erate under the assumption that all same-head
pairs corefer. This is by no means always the case?
there are several systematic exceptions to the rule.
In this paper, we show that these exceptions are
fairly common, and describe an unsupervised sys-
tem which learns to distinguish them from coref-
erent same-head pairs.
There are several reasons why relatively little
attention has been paid to same-head pairs. Pri-
marily, this is because they are a comparatively
easy subtask in a notoriously difficult area; Stoy-
anov et al (2009) shows that, among NPs headed
by common nouns, those which have an exact
match earlier in the document are the easiest to
resolve (variant MUC score .82 on MUC-6) and
while those with partial matches are quite a bit
harder (.53), by far the worst performance is on
those without any match at all (.27). This effect
is magnified by most popular metrics for coref-
erence, which reward finding links within large
clusters more than they punish proposing spu-
rious links, making it hard to improve perfor-
mance by linking conservatively. Systems that
use gold mention boundaries (the locations of NPs
marked by annotators)1 have even less need to
worry about same-head relationships, since most
NPs which disobey the conventional assumption
are not marked as mentions.
In this paper, we count how often same-head
pairs fail to corefer in the MUC-6 corpus, show-
ing that gold mention detection hides most such
pairs, but more realistic detection finds large num-
bers. We also present an unsupervised genera-
tive model which learns to make certain same-
head pairs non-coreferent. The model is based
on the idea that pronoun referents are likely to
be salient noun phrases in the discourse, so we
can learn about NP antecedents using pronom-
inal antecedents as a starting point. Pronoun
anaphora, in turn, is learnable from raw data
(Cherry and Bergsma, 2005; Charniak and Elsner,
2009). Since our model links fewer NPs than the
baseline, it improves precision but decreases re-
call. This tradeoff is favorable for CEAF, but not
for b3.
2 Related work
Unsupervised systems specify the assumption of
same-head coreference in several ways: by as-
1Gold mention detection means something slightly differ-
ent in the ACE corpus, where the system input contains every
NP annotated with an entity type.
33
sumption (Haghighi and Klein, 2009), using
a head-prediction clause (Poon and Domingos,
2008), and using a sparse Dirichlet prior on word
emissions (Haghighi and Klein, 2007). (These
three systems, perhaps not coincidentally, use gold
mentions.) An exception is Ng (2008), who points
out that head identity is not an entirely reliable cue
and instead uses exact string match (minus deter-
miners) for common NPs and an alias detection
system for proper NPs. This work uses mentions
extracted with an NP chunker. No specific results
are reported for same-head NPs. However, while
using exact string match raises precision, many
non-matching phrases are still coreferent, so this
approach cannot be considered a full solution to
the problem.
Supervised systems do better on the task, but
not perfectly. Recent work (Stoyanov et al, 2009)
attempts to determine the contributions of various
categories of NP to coreference scores, and shows
(as stated above) that common NPs which partially
match an earlier mention are not well resolved by
the state-of-the-art RECONCILE system, which
uses pairwise classification. They also show that
using gold mention boundaries makes the corefer-
ence task substantially easier, and argue that this
experimental setting is ?rather unrealistic?.
3 Descriptive study: MUC-6
We begin by examining how often non-same-head
pairs appear in the MUC-6 coreference dataset.
To do so, we compare two artificial coreference
systems: the link-all strategy links all, and only,
full (non-pronominal) NP pairs with the same head
which occur within 10 sentences of one another.
The oracle strategy links NP pairs with the same
head which occur within 10 sentences, but only if
they are actually coreferent (according to the gold
annotation)2 The link-all system, in other words,
does what most existing unsupervised systems do
on the same-head subset of NPs, while the oracle
system performs perfectly.
We compare our results to the gold standard us-
ing two metrics. b3(Bagga and Baldwin, 1998)
is a standard metric which calculates a precision
and recall for each mention. The mention CEAF
(Luo, 2005) constructs a maximum-weight bipar-
2The choice of 10 sentences as the window size captures
most, but not all, of the available recall. Using nouns mention
detection, it misses 117 possible same-head links, or about
10%. However, precision drops further as the window size
increases.
tite matching between gold and proposed clusters,
then gives the percentage of entities whose gold
label and proposed label match. b3 gives more
weight to errors involving larger clusters (since
these lower scores for several mentions at once);
for mention CEAF, all mentions are weighted
equally.
We annotate the data with the self-trained Char-
niak parser (McClosky et al, 2006), then extract
mentions using three different methods. The gold
mentions method takes only mentions marked by
annotators. The nps method takes all base noun
phrases detected by the parser. Finally, the nouns
method takes all nouns, even those that do not
head NPs; this method maximizes recall, since it
does not exclude prenominals in phrases like ?a
Bush spokesman?. (High-precision models of the
internal structure of flat Penn Treebank-style NPs
were investigated by Vadas and Curran (2007).)
For each experimental setting, we show the num-
ber of mentions detected, and how many of them
are linked to some antecedent by the system.
The data is shown in Table 1. b3 shows a large
drop in precision when all same-head pairs are
linked; in fact, in the nps and nouns settings, only
about half the same-headed NPs are actually coref-
erent (864 real links, 1592 pairs for nps). This
demonstrates that non-coreferent same-head pairs
not only occur, but are actually rather common in
the dataset. The drop in precision is much less
obvious in the gold mentions setting, however;
most unlinked same-head pairs are not annotated
as mentions in the gold data, which is one reason
why systems run in this experimental setting can
afford to ignore them.
Improperly linking same-head pairs causes a
loss in precision, but scores are dominated by re-
call3. Thus, reporting b3 helps to mask the impact
of these pairs when examining the final f-score.
We roughly characterize what sort of same-
headed NPs are non-coreferent by hand-
examining 100 randomly selected pairs. 39
pairs denoted different entities (?recent employ-
ees? vs ?employees who have worked for longer?)
disambiguated by modifiers or sometimes by
discourse position. The next largest group (24)
consists of time and measure phrases like ?ten
miles?. 12 pairs refer to parts or quantities
3This bias is exaggerated for systems which only link
same-head pairs, but continues to apply to real systems; for
instance (Haghighi and Klein, 2009) has a b3 precision of 84
and recall of 67.
34
Mentions Linked b3 pr rec F mention CEAF
Gold mentions
Oracle 1929 1164 100 32.3 48.8 54.4
Link all 1929 1182 80.6 31.7 45.5 53.8
Alignment 1929 495 93.7 22.1 35.8 40.5
NPs
Oracle 3993 864 100 30.6 46.9 73.4
Link all 3993 1592 67.2 29.5 41.0 62.2
Alignment 3993 518 87.2 24.7 38.5 67.0
Nouns
Oracle 5435 1127 100 41.5 58.6 83.5
Link all 5435 2541 56.6 40.9 45.7 67.0
Alignment 5435 935 83.0 32.8 47.1 74.4
Table 1: Oracle, system and baseline scores on MUC-6 test data. Gold mentions leave little room
for improvement between baseline and oracle; detecting more mentions widens the gap between
them. With realistic mention detection, precision and CEAF scores improve over baselines, while recall
and f-scores drop.
(?members of...?), and 12 contained a generic
(?In a corporate campaign, a union tries...?). 9
contained an annotator error. The remaining 4
were mistakes involving proper noun phrases
headed by Inc. and other abbreviations; this case
is easy to handle, but apparently not the primary
cause of errors.
4 System
Our system is a version of the popular IBM model
2 for machine translation. To define our generative
model, we assume that the parse trees for the en-
tire document D are given, except for the subtrees
with root nonterminal NP, denoted ni, which our
system will generate. These subtrees are related
by a hidden set of alignments, ai, which link each
NP to another NP (which we call a generator) ap-
pearing somewhere before it in the document, or
to a null antecedent. The set of potential genera-
tors G (which plays the same role as the source-
language text in MT) is taken to be all the NPs
occurring within 10 sentences of the target, plus a
special null antecedent which plays the same role
as the null word in machine translation? it serves
as a dummy generator for NPs which are unrelated
to any real NP in G.
The generative process fills in all the NP nodes
in order, from left to right. This process ensures
that, when generating node ni, we have already
filled in all the NPs in the set G (since these all
precede ni). When deciding on a generator for
NP ni, we can extract features characterizing its
relationship to a potential generator gj . These fea-
tures, which we denote f(ni, gj , D), may depend
on their relative position in the document D, and
on any features of gj , since we have already gener-
ated its tree. However, we cannot extract features
from the subtree under ni, since we have yet to
generate it!
As usual for IBM models, we learn using EM,
and we need to start our alignment function off
with a good initial set of parameters. Since an-
tecedents of NPs and pronouns (both salient NPs)
often occur in similar syntactic environments, we
use an alignment function for pronoun corefer-
ence as a starting point. This alignment can be
learned from raw data, making our approach un-
supervised.
We take the pronoun model of Charniak and El-
sner (2009)4 as our starting point. We re-express
it in the IBM framework, using a log-linear model
for our alignment. Then our alignment (parame-
terized by feature weights w) is:
p(ai = j|G,D) ? exp(f(ni, gj , D) ? w)
The weights w are learned by gradient descent
on the log-likelihood. To use this model within
EM, we alternate an E-step where we calculate
the expected alignments E[ai = j], then an M-
step where we run gradient descent. (We have also
had some success with stepwise EM as in (Liang
and Klein, 2009), but this requires some tuning to
work properly.)
4Downloaded from http://bllip.cs.brown.edu.
35
As features, we take the same features as Char-
niak and Elsner (2009): sentence and word-count
distance between ni and gj , sentence position of
each, syntactic role of each, and head type of gj
(proper, common or pronoun). We add binary fea-
tures for the nonterminal directly over gj (NP, VP,
PP, any S type, or other), the type of phrases mod-
ifying gj (proper nouns, phrasals (except QP and
PP), QP, PP-of, PP-other, other modifiers, or noth-
ing), and the type of determiner of gj (possessive,
definite, indefinite, deictic, other, or nothing). We
designed this feature set to distinguish prominent
NPs in the discourse, and also to be able to detect
abstract or partitive phrases by examining modi-
fiers and determiners.
To produce full NPs and learn same-head coref-
erence, we focus on learning a good alignment
using the pronoun model as a starting point. For
translation, we use a trivial model, p(ni|gai) = 1
if the two have the same head, and 0 otherwise,
except for the null antecedent, which draws heads
from a multinomial distribution over words.
While we could learn an alignment and then
treat all generators as antecedents, so that only
NPs aligned to the null antecedent were not la-
beled coreferent, in practice this model would
align nearly all the same-head pairs. This is
true because many words are ?bursty?; the prob-
ability of a second occurrence given the first is
higher than the a priori probability of occurrence
(Church, 2000). Therefore, our model is actually a
mixture of two IBM models, pC and pN , where pC
produces NPs with antecedents and pN produces
pairs that share a head, but are not coreferent. To
break the symmetry, we allow pC to use any pa-
rameters w, while pN uses a uniform alignment,
w ? ~0. We interpolate between these two models
with a constant ?, the single manually set parame-
ter of our system, which we fixed at .9.
The full model, therefore, is:
p(ni|G,D) =?pT (ni|G,D)
+ (1? ?)pN (ni|G,D)
pT (ni|G,D) =
1
Z
?
j?G
exp(f(ni, gj , D) ? w)
? I{head(ni) = head(j)}
pT (ni|G,D) =
?
j?G
1
|G|
I{head(ni) = head(gj)}
NPs for which the maximum-likelihood gener-
ator (the largest term in either of the sums) is from
pT and is not the null antecedent are marked as
coreferent to the generator. Other NPs are marked
not coreferent.
5 Results
Our results on the MUC-6 formal test set are
shown in Table 1. In all experimental settings,
the model improves precision over the baseline
while decreasing recall? that is, it misses some le-
gitimate coreferent pairs while correctly exclud-
ing many of the spurious ones. Because of the
precision-recall tradeoff at which the systems op-
erate, this results in reduced b3 and link F. How-
ever, for the nps and nouns settings, where the
parser is responsible for finding mentions, the
tradeoff is positive for the CEAF metrics. For in-
stance, in the nps setting, it improves over baseline
by 57%.
As expected, the model does poorly in the gold
mentions setting, doing worse than baseline on
both metrics. Although it is possible to get very
high precision in this setting, the model is far too
conservative, linking less than half of the available
mentions to anything, when in fact about 60% of
them are coreferent. As we explain above, this ex-
perimental setting makes it mostly unnecessary to
worry about non-coreferent same-head pairs be-
cause the MUC-6 annotators don?t often mark
them.
6 Conclusions
While same-head pairs are easier to resolve than
same-other pairs, they are still non-trivial and de-
serve further attention in coreference research. To
effectively measure their effect on performance,
researchers should report multiple metrics, since
under b3 the link-all heuristic is extremely diffi-
cult to beat. It is also important to report results
using a realistic mention detector as well as gold
mentions.
Acknowledgements
We thank Jean Carletta for the SWITCHBOARD
annotations, and Dan Jurafsky and eight anony-
mous reviewers for their comments and sugges-
tions. This work was funded by a Google graduate
fellowship.
36
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In LREC Workshop on
Linguistics Coreference, pages 563?566.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution.
In Proceedings of CoNLL, pages 88?95, Ann Arbor,
Michigan.
Kenneth W. Church. 2000. Empirical estimates of
adaptation: the chance of two Noriegas is closer to
p/2 than p2. In Proceedings of ACL, pages 180?186.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of ACL, pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic fea-
tures. In Proceedings of EMNLP, pages 1152?1161.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In HLT-NAACL.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25?32, Morristown, NJ, USA. Association for
Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL, pages 152?159.
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of EMNLP, pages
640?649, Honolulu, Hawaii. Association for Com-
putational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of EMNLP, pages 650?659, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656?664, Suntec, Singapore, August. Association
for Computational Linguistics.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of ACL, pages 240?247, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
37
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1179?1189,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Disentangling Chat with Local Coherence Models
Micha Elsner
School of Informatics
University of Edinburgh
melsner0@gmail.com
Eugene Charniak
Department of Computer Science
Brown University, Providence, RI 02912
ec@cs.brown.edu
Abstract
We evaluate several popular models of local
discourse coherence for domain and task gen-
erality by applying them to chat disentangle-
ment. Using experiments on synthetic multi-
party conversations, we show that most mod-
els transfer well from text to dialogue. Co-
herence models improve results overall when
good parses and topic models are available,
and on a constrained task for real chat data.
1 Introduction
One property of a well-written document is coher-
ence, the way each sentence ts into its context? sen-
tences should be interpretable in light of what has
come before, and in turn make it possible to inter-
pret what comes after. Models of coherence have
primarily been used for text-based generation tasks:
ordering units of text for multidocument summariza-
tion or inserting new text into an existing article.
In general, the corpora used consist of informative
writing, and the tasks used for evaluation consider
different ways of reordering the same set of textual
units. But the theoretical concept of coherence goes
beyond both this domain and this task setting? and
so should coherence models.
This paper evaluates a variety of local coher-
ence models on the task of chat disentanglement or
?threading?: separating a transcript of a multiparty
interaction into independent conversations
1
. Such
simultaneous conversations occur in internet chat
1
A public implementation is available via https://
bitbucket.org/melsner/browncoherence.
rooms, and on shared voice channels such as push-
to-talk radio. In these situations, a single, correctly
disentangled, conversational thread will be coherent,
since the speakers involved understand the normal
rules of discourse, but the transcript as a whole will
not be. Thus, a good model of coherence should be
able to disentangle sentences as well as order them.
There are several differences between disentan-
glement and the newswire sentence-ordering tasks
typically used to evaluate coherence models. Inter-
net chat comes from a different domain, one where
topics vary widely and no reliable syntactic annota-
tions are available. The disentanglement task mea-
sures different capabilities of a model, since it com-
pares documents that are not permuted versions of
one another. Finally, full disentanglement requires
a large-scale search, which is computationally dif-
cult. We move toward disentanglement in stages,
carrying out a series of experiments to measure the
contribution of each of these factors.
As an intermediary between newswire and inter-
net chat, we adopt the SWITCHBOARD (SWBD) cor-
pus. SWBD contains recorded telephone conversa-
tions with known topics and hand-annotated parse
trees; this allows us to control for the performance
of our parser and other informational resources. To
compare the two algorithmic settings, we use SWBD
for ordering experiments, and also articially entan-
gle pairs of telephone dialogues to create synthetic
transcripts which we can disentangle. Finally, we
present results on actual internet chat corpora.
On synthetic SWBD transcripts, local coherence
models improve performance considerably over our
baseline model, Elsner and Charniak (2008b). On
1179
internet chat, we continue to do better on a con-
strained disentanglement task, though so far, we are
unable to apply these improvements to the full task.
We suspect that, with better low-level annotation
tools for the chat domain and a good way of integrat-
ing prior information, our improvements on SWBD
could transfer fully to IRC chat.
2 Related work
There is extensive previous work on coherence mod-
els for text ordering; we describe several specic
models below, in section 2. This study focuses on
models of local coherence, which relate text to its
immediate context. There has also been work on
global coherence, the structure of a document as a
whole (Chen et al, 2009; Eisenstein and Barzilay,
2008; Barzilay and Lee, 2004), typically modeled
in terms of sequential topics. We avoid using them
here, because we do not believe topic sequences are
predictable in conversation and because such models
tend to be algorithmically cumbersome.
In addition to text ordering, local coherence mod-
els have also been used to score the uency of texts
written by humans or produced by machine (Pitler
and Nenkova, 2008; Lapata, 2006; Miltsakaki and
Kukich, 2004). Like disentanglement, these tasks
provide an algorithmic setting that differs from or-
dering, and so can demonstrate previously unknown
weaknesses in models. However, the target genre is
still informative writing, so they reveal little about
cross-domain exibility.
The task of disentanglement or ?threading? for
internet chat was introduced by Shen et al (2006).
Elsner and Charniak (2008b) created the publicly
available #LINUX corpus; the best published re-
sults on this corpus are those of Wang and Oard
(2009). These two studies use overlapping unigrams
to measure similarity between two sentences; Wang
and Oard (2009) use a message expansion tech-
nique to incorporate context beyond a single sen-
tence. Unigram overlaps are used to model coher-
ence, but more sophisticated methods using syntax
(Lapata and Barzilay, 2005) or lexical features (La-
pata, 2003) often outperform them on ordering tasks.
This study compares several of these methods with
Elsner and Charniak (2008b), which we use as a
baseline because there is a publicly available imple-
mentation
2
.
Adams (2008) also created and released a disen-
tanglement corpus. They use LDA (Blei et al, 2001)
to discover latent topics in their corpus, then measur-
ing similarity by looking for shared topics. These
features fail to improve their performance, which is
puzzling in light of the success of topic modeling for
other coherence and segmentation problems (Eisen-
stein and Barzilay, 2008; Foltz et al, 1998). The
results of this study suggest that topic models can
help with disentanglement, but that it is difcult to
nd useful topics for IRC chat.
A few studies have attempted to disentangle con-
versational speech (Aoki et al, 2003; Aoki et al,
2006), mostly using temporal features. For the most
part, however, this research has focused on auditory
processing in the context of the cocktail party prob-
lem, the task of attending to a specic speaker in
a noisy room (Haykin and Chen, 2005). Utterance
content has some inuence on what the listener per-
ceives, but only for extremely salient cues such as
the listener's name (Moray, 1959), so cocktail party
research does not typically use lexical models.
3 Models
In this section, we briey describe the models we in-
tend to evaluate. Most of them are drawn from pre-
vious work; one, the topical entity grid, is a novel
extension of the entity grid. For the experiments be-
low, we train the models on SWBD, sometimes aug-
mented with a larger set of automatically parsed con-
versations from the FISHER corpus. Since the two
corpora are quite similar, FISHER is a useful source
for extra data; McClosky et al (2010) uses it for
this purpose in parsing experiments. (We continue
to use SWBD/FISHER even for experiments on IRC,
because we do not have enough disentangled train-
ing data to learn lexical relationships.)
3.1 Entity grid
The entity grid (Lapata and Barzilay, 2005; Barzilay
and Lapata, 2005) is an attempt to model some prin-
ciples of Centering Theory (Grosz et al, 1995) in a
statistical manner. It represents a document in terms
of entities and their syntactic roles: subject (S), ob-
ject (O), other (X) and not present (-). In each new
2
cs.brown.edu/
?
melsner
1180
utterance, the grid predicts the role in which each
entity will appear, given its history of roles in the
previous sentences, plus a salience feature counting
the total number of times the entity occurs. For in-
stance, for an entity which is the subject of sentence
1, the object of sentence 2, and occurs four times in
total, the grid predicts its role in sentence 3 accord-
ing to the conditional P (jS;O; sal = 4).
As in previous work, we treat each noun in a doc-
ument as denoting a single entity, rather than using
a coreference technique to attempt to resolve them.
In our development experiments, we noticed that
coreferent nouns often occur farther apart in conver-
sation than in newswire, since they are frequently
referred to by pronouns and deictics in the interim.
Therefore, we extend the history to six previous ut-
terances. For robustness with this long history, we
model the conditional probabilities using multilabel
logistic regression rather than maximum likelihood.
This requires the assumption of a linear model, but
makes the estimator less vulnerable to overtting
due to sparsity, increasing performance by about 2%
in development experiments.
3.2 Topical entity grid
This model is a variant of the generative entity
grid, intended to take into account topical informa-
tion. To create the topical entity grid, we learn a
set of topic-to-word distributions for our corpus us-
ing LDA (Blei et al, 2001)
3
with 200 latent top-
ics. This model embeds our vocabulary in a low-
dimensional space: we represent each word w as
the vector of topic probabilities p(t
i
jw). We ex-
perimented with several ways to measure relation-
ships between words in this space, starting with the
standard cosine. However, the cosine can depend on
small variations in probability (for instance, if w has
most of its mass in dimension 1, then it is sensitive
to the exact weight of v for topic 1, even if this es-
sentially never happens).
To control for this tendency, we instead use the
magnitude of the dimension of greatest similarity:
sim(w; v) = max
i
min(w
i
; v
i
)
Tomodel coherence, we generalize the binary his-
3
www.cs.princeton.edu/
?
blei/
topicmodeling.html
tory features of the standard entity grid, which de-
tect, for example, whether entity e is the subject of
the previous sentence. In the topical entity grid, we
instead compute a real-valued feature which sums
up the similarity between entity e and the subject(s)
of the previous sentence.
These features can detect a transition like: ?The
House voted yesterday. The Senate will consider the
bill today.?. If ?House? and ?Senate? have a high
similarity, then the feature will have a high value,
predicting that ?Senate? is a good subject for the cur-
rent sentence. As in the previous section, we learn
the conditional probabilities with logistic regression;
we train in parallel by splitting the data and averag-
ing (Mann et al, 2009). The topics are trained on
FISHER, and on NANC for news.
3.3 IBM-1
The IBM translation model was rst considered for
coherence by Soricut and Marcu (2006), although a
less probabilistically elegant version was proposed
earlier (Lapata, 2003). This model attempts to gen-
erate the content words of the next sentence by trans-
lating them from the words of the previous sentence,
plus a null word; thus, it will learn alignments be-
tween pairs of words that tend to occur in adjacent
sentences. We learn parameters on the FISHER cor-
pus, and on NANC for news.
3.4 Pronouns
The use of a generative pronoun resolver for co-
herence modeling originates in Elsner and Char-
niak (2008a). That paper used a supervised model
(Ge et al, 1998), but we adapt a newer, unsuper-
vised model which they also make publicly available
(Charniak and Elsner, 2009)
4
. They model each pro-
noun as generated by an antecedent somewhere in
the previous two sentences. If a good antecedent is
found, the probability of the pronoun's occurrence
will be high; otherwise, the probability is low, sig-
naling that the text is less coherent because the pro-
noun is hard to interpret correctly.
We use the model as distributed for news text. For
conversation, we adapt it by running a few iterations
of their EM training algorithm on the FISHER data.
4
bllip.cs.brown.edu/resources.shtml\
#software
1181
3.5 Discourse-newness
Building on work from summarization (Nenkova
and McKeown, 2003) and coreference resolution
(Poesio et al, 2005), Elsner and Charniak (2008a)
use a model which recognizes discourse-new versus
old NPs as a coherence model. For instance, the
model can learn that ?President Barack Obama? is
a more likely rst reference than ?Obama?. Follow-
ing their work, we score discourse-newness with a
maximum-entropy classier using syntactic features
counting different types of NP modiers, and we use
NP head identity as a proxy for coreference.
3.6 Chat-specic features
Most disentanglement models use non-linguistic in-
formation alongside lexical features; in fact, times-
tamps and speaker identities are usually better cues
than words are. We capture three essential non-
linguistic features using simple generative models.
The rst feature is the time gap between one utter-
ance and the next within the same thread. Consistent
short gaps are a sign of normal turn-taking behavior;
long pauses do occur, but much more rarely (Aoki et
al., 2003). We round all time gaps to the nearest sec-
ond and model the distribution of time gaps using a
histogram, choosing bucket sizes adaptively so that
each bucket contains at least four datapoints.
The second feature is speaker identity; conver-
sations usually involve a small subset of the to-
tal number of speakers, and a few core speakers
make most of the utterances. We model the distri-
bution of speakers in each conversation using a Chi-
nese Restaurant Process (CRP) (Aldous, 1985) (tun-
ing the dispersion  to maximize development pe-
formance). The CRP's ?rich-get-richer? dynamics
capture our intuitions, favoring conversations domi-
nated by a few vociferous speakers.
Finally, we model name mentioning. Speakers
in IRC chat often use their addressee's names to co-
ordinate the chat (O'Neill and Martin, 2003), and
this is a powerful source of information (Elsner and
Charniak, 2008b). Our model classies each utter-
ance into either the start or continuation of a conver-
sational turn, by checking if the previous utterance
had the same speaker. Given this status, it computes
probabilities for three outcomes: no name mention,
a mention of someone who has previously spoken
in the conversation, or a mention of someone else.
(The third option is extremely rare; this accounts
for most of the model's predictive power). We learn
these probabilities from IRC training data.
3.7 Model combination
To combine these different models, we adopt the
log-linear framework of Soricut and Marcu (2006).
Here, each model P
i
is assigned a weight 
i
, and the
combined score P (d) is proportional to:
X
i

i
log(P
i
(d))
The weights  can be learned discriminatively,
maximizing the probability of d relative to a task-
specic contrast set. For ordering experiments, the
contrast set is a single random permutation of d; we
explain the training regime for disentanglement be-
low, in subsection 4.1.
4 Comparing orderings of SWBD
To measure the differences in performance caused
by moving from news to a conversational domain,
we rst compare our models on an ordering task,
discrimination (Barzilay and Lapata, 2005; Karama-
nis et al, 2004). In this task, we take an original
document and randomly permute its sentences, cre-
ating an articial incoherent document. We then test
to see if our model prefers the coherent original.
For SWBD, rather than compare permutations
of the individual utterances, we permute conversa-
tional turns (sets of consecutive utterances by each
speaker), since turns are natural discourse units in
conversation. We take documents numbered 2000?
3999 as training/development and the remainder as
test, yielding 505 training and 153 test documents;
we evaluate 20 permutations per document. As a
comparison, we also show results for the same mod-
els on WSJ, using the train-test split from Elsner and
Charniak (2008a); the test set is sections 14-24, to-
talling 1004 documents.
Purandare and Litman (2008) carry out similar ex-
periments on distinguishing permuted SWBD doc-
uments, using lexical and WordNet features in a
model similar to Lapata (2003). Their accuracy for
this task (which they call ?switch-hard?) is roughly
68%.
1182
WSJ SWBD
EGrid 76.4z 86.0
Topical EGrid 71.8z 70.9z
IBM-1 77.2z 84.9y
Pronouns 69.6z 71.7z
Disc-new 72.3z 55.0z
Combined 81.9 88.4
-EGrid 81.0 87.5
-Topical EGrid 82.2 90.5
-IBM-1 79.0z 88.9
-Pronouns 81.3 88.5
-Disc-new 82.2 88.4
Table 1: Discrimination F scores on news and dialogue.
z indicates a signicant difference from the combined
model at p=.01 and y at p=.05.
In Table 1, we show the results for individual
models, for the combined model, and ablation re-
sults for mixtures without each component. WSJ is
more difcult than SWBD overall because, on av-
erage, news articles are shorter than SWBD con-
versations. Short documents are harder, because
permuting disrupts them less. The best SWBD re-
sult is 91%; the best WSJ result is 82% (both for
mixtures without the topical entity grid). The WSJ
result is state-of-the-art for the dataset, improving
slightly on Elsner and Charniak (2008a) at 81%. We
test results for signicance using the non-parametric
Mann-Whitney U test.
Controlling for the fact that discrimination is eas-
ier on SWBD, most of the individual models perform
similarly in both corpora. The strongest models in
both cases are the entity grid and IBM-1 (at about
77% for news, 85% for dialogue). Pronouns and the
topical entity grid are weaker. The major outlier is
the discourse-new model, whose performance drops
from 72% for news to only 55%, just above chance,
for conversation.
The model combination results show that all the
models are quite closely correlated, since leaving
out any single model does not degrade the combi-
nation very much (only one of the ablations is sig-
nicantly worse than the combination). The most
critical in news is IBM-1 (decreasing performance
by 3% when removed); in conversation, it is the
entity grid (decreasing by about 1%). The topical
entity grid actually has a (nonsignicant) negative
impact on combined performance, implying that its
predictive power in this setting comes mainly from
information that other models also capture, but that
it is noisier and less reliable. In each domain, the
combined models outperform the best single model,
showing the information provided by the weaker
models is not completely redundant.
Overall, these results suggest that most previ-
ously proposed local coherence models are domain-
general; they work on conversation as well as
news. The exception is the discourse-newness
model, which benets most from the specic con-
ventions of a written style. Full names with titles
(like ?President Barack Obama?) are more common
in news, while conversation tends to involve fewer
completely unfamiliar entities and more cases of
bridging reference, in which grounding information
is given implicitly (Nissim, 2006). Due to its poor
performance, we omit the discourse-newness model
in our remaining experiments.
5 Disentangling SWBD
We now turn to the task of disentanglement, test-
ing whether models that are good at ordering also
do well in this new setting. We would like to hold
the domain constant, but we do not have any disen-
tanglement data recorded from naturally occurring
speech, so we create synthetic instances by merging
pairs of SWBD dialogues. Doing so creates an arti-
cial transcript in which two pairs of people appear
to be talking simultaneously over a shared channel.
The situation is somewhat contrived in that each
pair of speakers converses only with each other,
never breaking into the other pair's dialogue and
rarely using devices like name mentioning to make
it clear who they are addressing. Since this makes
speaker identity a perfect cue for disentanglement,
we do not use it in this section. The only chat-
specic model we use is time.
Because we are not using speaker information, we
remove all utterances which do not contain a noun
before constructing synthetic transcripts? these are
mostly backchannels like ?Yeah?. Such utterances
cannot be correctly assigned by our coherence mod-
els, which deal with content; we suspect most of
them could be dealt with by associating them with
the nearest utterance from the same speaker.
1183
Once the backchannels are stripped, we can cre-
ate a synthetic transcript. For each dialogue, we rst
simulate timestamps by sampling the number of sec-
onds between each utterance and the next from a dis-
cretized Gaussian: bN(0; 2:5)c. The interleaving of
the conversations is dictated by the timestamps. We
truncate the longer conversation at the length of the
shorter; this ensures a baseline score of 50% for the
degenerate model that assigns all utterances to the
same conversation.
We create synthetic instances of two types? those
where the two entangled conversations had differ-
ent topical prompts and those where they were the
same. (Each dialogue in SWBD focuses on a prese-
lected topic, such as shing or movies.) We entangle
dialogues from our ordering development set to use
for mixture training and validation; for testing, we
use 100 instances of each type, constructed from di-
alogues in our test set.
When disentangling, we treat each thread as inde-
pendent of the others. In other words, the probability
of the entire transcript is the product of the probabil-
ities of the component threads. Our objective is to
nd the set of threads maximizing this. As a com-
parison, we use the model of Elsner and Charniak
(2008b) as a baseline. To make their implementa-
tion comparable to ours, in this section we constrain
it to nd only two threads.
5.1 Disentangling a single utterance
Our rst disentanglement task is to correctly assign
a single utterance, given the true structure of the rest
of the transcript. For each utterance, we compare
two versions of the transcript, the original, and a
version where it is swapped into the other thread.
Our accuracy measures how often our models prefer
the original. Unlike full-scale disentanglement, this
task does not require a computationally demanding
search, so it is possible to run experiments quickly.
We also use it to train our mixture models for disen-
tanglement, by construct a training example for each
utterance i in our training transcripts. Since the El-
sner and Charniak (2008b) model maximizes a cor-
relation clustering objective which sums up indepen-
dent edge weights, we can also use it to disentangle
a single sentence efciently.
Our results are shown in Table 2. Again, re-
sults for individual models are above the line, then
Different Same Avg.
EGrid 80.2 72.9 76.6
Topical EGrid 81.7 73.3 77.5
IBM-1 70.4 66.7 68.5
Pronouns 53.1 50.1 51.6
Time 58.5 57.4 57.9
Combined 86.8 79.6 83.2
-EGrid 86.0 79.1 82.6
-Topical EGrid 85.2 78.7 81.9
-IBM-1 86.2 78.7 82.4
-Pronouns 86.8 79.4 83.1
-Time 84.5 76.7 80.6
E+C `08 78.2 73.5 75.8
Table 2: Average accuracy for disentanglement of a sin-
gle utterance on 200 synthetic multiparty conversations
from SWBD test.
our combined model, and nally ablation results for
mixtures omitting a single model. The results show
that, for a pair of dialogues that differ in topic, our
best model can assign a single sentence with 87%
accuracy. For the same topic, the accuracy is 80%.
In each case, these results improve on (Elsner and
Charniak, 2008b), which scores 78% and 74%.
Changing to this new task has a substantial im-
pact on performance. The topical model, which per-
formed poorly for ordering, is actually stronger than
the entity grid in this setting. IBM-1 underperforms
either grid model (69% to 77%); on ordering, it was
nearly as good (85% to 86%).
Despite their ordering performance of 72%, pro-
nouns are essentially useless for this task, at 52%.
This decline is due partly to domain, and partly
to task setting. Although SWBD contains more
pronominals than WSJ, many of them are rst
and second-person pronouns or deictics, which our
model does not attempt to resolve. Since the disen-
tanglement task involves moving only a single sen-
tence, if moving this sentence does not sever a re-
solvable pronoun from its antecedent, the model will
be unable to make a good decision.
As before, the ablation results show that all the
models are quite correlated, since removing any sin-
gle model from the mixture causes only a small de-
crease in performance. The largest drop (83% to
81%) is caused by removing time; though time is
a weak model on its own, it is completely orthogo-
1184
nal to the other models, since unlike them, it does
not depend on the words in the sentences.
Comparing results between ?different topic? and
?same topic? instances shows that ?same topic? is
harder? by about 7% for the combined model. The
IBM model has a relatively small gap of 3.7%, and
in the ablation results, removing it causes a larger
drop in performance for ?same? than ?different?;
this suggests it is somewhat more robust to similar-
ity in topic than entity grids.
Disentanglement accuracy is hard to predict given
ordering performance; the two tasks plainly make
different demands on models. One difference is that
the models which use longer histories (the two entity
grids) remain strong, while the models considering
only one or two previous sentences (IBM and pro-
nouns) do not do as well. Since the changes being
considered here affect only a single sentence, while
permutation affects the entire transcript, more his-
tory may help by making the model more sensitive
to small changes.
5.2 Disentangling an entire transcript
We now turn to the task of disentangling an entire
transcript at once. This is a practical task, motivated
by applications such as search and information re-
trieval. However, it is more difcult than assign-
ing only a single utterance, because decisions are
interrelated? an error on one utterance may cause
a cascade of poor decisions further down. It is also
computationally harder.
We use tabu search (Glover and Laguna, 1997) to
nd a good solution. The search repeatedly nds and
moves the utterance which would most improve the
model score if swapped from one thread to the other.
Unlike greedy search, tabu search is constrained not
to repeat a solution that it has recently visited; this
forces it to keep exploring when it reaches a local
maximum. We run 500 iterations of tabu search
(usually nding the rst local maximum after about
100) and return the best solution found.
We measure performance with one-to-one over-
lap, which maps the two clusters to the two gold
dialogues, then measures percent correct
5
. Our re-
sults (Table 3) show that, for transcripts with dif-
ferent topics, our disentanglement has 68% over-
5
The other popular metrics, F and loc
3
, are correlated.
Different Same Avg.
EGrid 60.3 57.1 58.7
Topical EGrid 62.3 56.8 59.6
IBM-1 56.5 55.2 55.9
Pronouns 54.5 54.4 54.4
Time 55.4 53.8 54.6
Combined 67.9 59.8 63.9
E+C `08 59.1 57.4 58.3
Table 3: One-to-one overlap between disentanglement re-
sults and truth on 200 synthetic multiparty conversations
from SWBD test.
lap with truth, extracting about two thirds of the
structure correctly; this is substantially better than
Elsner and Charniak (2008b), which scores 59%.
Where the entangled conversations have the same
topic, performance is lower, about 60%, but still bet-
ter than the comparison model with 57%. Since cor-
relations with the previous section are fairly reliable,
and the disentanglement procedure is computation-
ally intensive, we omit ablation experiments.
As we expect, full disentanglement is more dif-
cult than single-sentence disentanglement (com-
bined scores drop by about 20%), but the single-
sentence task is a good predictor of relative perfor-
mance. Entity grid models do best, the IBM model
remains useful, but less so than for discrimination,
and pronouns are very weak. The IBM model per-
forms similarly under both metrics (56% and 57%),
while other models perform worse on loc
3
. This
supports our suggestion that IBM's decline in per-
formance from ordering is indeed due to its using a
single sentence history; it is still capable of getting
local structures right, but misses global ones.
6 IRC data
In this section, we move from synthetic data to
real multiparty discourse recorded from internet chat
rooms. We use two datasets: the #LINUX corpus
(Elsner and Charniak, 2008b), and three larger cor-
pora, #IPHONE, #PHYSICS and #PYTHON (Adams,
2008). We use the 1000-line ?development? sec-
tion of #LINUX for tuning our mixture models and
the 800-line ?test? section for development experi-
ments. We reserve the Adams (2008) corpora for
testing; together, they consist of 19581 lines of chat,
with each section containing 500 to 1000 lines.
1185
Chat-specic 74.0
+EGrid 79.3
+Topical EGrid 76.8
+IBM-1 76.3
+Pronouns 73.9
+EGrid/Topic/IBM-1 78.3
E+C `08b 76.4
Table 4: Accuracy for single utterance disentanglement,
averaged over annotations of 800 lines of #LINUX data.
In order to use syntactic models like the entity
grid, we parse the transcripts using (McClosky et
al., 2006). Performance is bad, although the parser
does identify most of the NPs; poor results are typi-
cal for a standard parser on chat (Foster, 2010). We
postprocess the parse trees to retag ?lol?, ?haha? and
?yes? as UH (rather than NN, NNP and JJ).
In this section, we use all three of our chat-
specic models (sec. 2.0.6; time, speaker andmen-
tion) as a baseline. This baseline is relatively strong,
so we evaluate our other models in combination with
it.
6.1 Disentangling a single sentence
As before, we show results on correctly disentan-
gling a single sentence, given the correct structure
of the rest of the transcript. We average perfor-
mance on each transcript over the different annota-
tions, then average the transcripts, weighing them by
length to give each utterance equal weight.
Table 4 gives results on our development corpus,
#LINUX. Our best result, for the chat-specic fea-
tures plus entity grid, is 79%, improving on the com-
parison model, Elsner and Charniak (2008b), which
gets 76%. (Although the table only presents an av-
erage over all annotations of the dataset, this model
is also more accurate for each individual annota-
tor than the comparison model.) We then ran the
same model, chat-specic features plus entity grid,
on the test corpora from Adams (2008). These re-
sults (Table 5) are also better than Elsner and Char-
niak (2008b), at an average of 93% over 89%.
As pointed out in Elsner and Charniak (2008b),
the chat-specic features are quite powerful in this
domain, and it is hard to improve over them. Elsner
and Charniak (2008b), which has simple lexical fea-
tures, mostly based on unigram overlap, increases
#IPHONE #PHYSICS #PYTHON
+EGrid 92.3 96.6 91.1
E+C `08b 89.0 90.2 88.4
Table 5: Average accuracy for disentanglement of a sin-
gle utterance for 19581 total lines from Adams (2008).
performance over baseline by 2%. Both IBM and
the topical entity grid achieve similar gains. The en-
tity grid does better, increasing performance to 79%.
Pronouns, as before for SWBD, are useless.
We believe that the entity grid's good perfor-
mance here is due mostly to two factors: its use of
a long history, and its lack of lexicalization. The
grid looks at the previous six sentences, which dif-
ferentiates it from the IBM model and from Elsner
and Charniak (2008b), which treats each pair of sen-
tences independently. Using this long history helps
to distinguish important nouns from unimportant
ones better than frequency alone. We suspect that
our lexicalized models, IBM and the topical entity
grid, are hampered by poor parameter settings, since
their parameters were learned on FISHER rather than
IRC chat. In particular, we believe this explains why
the topical entity grid, which slightly outperformed
the entity grid on SWBD, is much worse here.
6.2 Full disentanglement
Running our tabu search algorithm on the full disen-
tanglement task yields disappointing results. Accu-
racies on the #LINUX dataset are not only worse than
previous work, but also worse than simple baselines
like creating one thread for each speaker. The model
nds far too many threads? it detects over 300, when
the true number is about 81 (averaging over annota-
tions). This appears to be related to biases in our
chat-specic models as well as in the entity grid;
the time model (which generates gaps between adja-
cent sentences) and the speaker model (which uses
a CRP) both assign probability 1 to single-utterance
conversations. The entity grid also has a bias toward
short conversations, because unseen entities are em-
pirically more likely to occur toward the beginning
of a conversation than in the middle.
A major weakness in our model is that we aim
only to maximize coherence of the individual con-
versations, with no prior on the likely length or num-
ber of conversations that will appear in the tran-
1186
script. This allows the model to create far too many
conversations. Integrating a prior into our frame-
work is not straightforward because we currently
train our mixture to maximize single-utterance dis-
entanglement performance, and the prior is not use-
ful for this task.
We experimented with xing parts of the tran-
script to the solution obtained by Elsner and Char-
niak (2008b), then using tabu search to ll in the
gaps. This constrains the number of conversations
and their approximate positions. With this structure
in place, we were able to obtain scores comparable
to Elsner and Charniak (2008b), but not improve-
ments. It appears that our performance increase on
single-sentence disentanglement does not transfer to
this task because of cascading errors and the neces-
sity of using external constraints.
7 Conclusions
We demonstrate that several popular models of lo-
cal coherence transfer well to the conversational do-
main, suggesting that they do indeed capture coher-
ence in general rather than specic conventions of
newswire text. However, their performance across
tasks is not as stable; in particular, models which
use less history information are worse for disentan-
glement.
Our results study suggest that while sophisticated
coherence models can potentially contribute to dis-
entanglement, they would benet greatly from im-
proved low-level resources for internet chat. Bet-
ter parsing, or at least NP chunking, would help for
models like the entity grid which rely on syntactic
role information. Larger training sets, or some kind
of transfer learning, could improve the learning of
topics and other lexical parameters. In particular,
our results on SWBD data conrm the conjecture of
(Adams, 2008) that LDA topic modeling is in prin-
ciple a useful tool for disentanglement? we believe a
topic-based model could also work on IRC chat, but
would require a better set of extracted topics. With
better parameters for these models and the integra-
tion of a prior, we believe that our good performance
on SWBD and single-utterance disentanglement for
IRC can be extended to full-scale disentanglement
of IRC.
Acknowledgements
We are extremely grateful to Regina Barzilay, Mark
Johnson, Rebecca Mason, Ben Swanson and Neal
Fox for their comments, to Craig Martell for the
NPS chat datasets and to three anonymous review-
ers. This work was funded by a Google Fellowship
for Natural Language Processing.
References
Paige H. Adams. 2008. Conversation Thread Extraction
and Topic Detection in Text-based Chat. Ph.D. thesis,
Naval Postgraduate School.
David Aldous. 1985. Exchangeability and related top-
ics. In Ecole d'Ete de Probabilities de Saint-Flour
XIII 1983, pages 1?198. Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter's cocktail party: a
social mobile audio space supporting multiple simul-
taneous conversations. In CHI '03: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 425?432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke D.
Plurkowski, James D. Thornton, Allison Woodruff,
and Weilie Yi. 2006. Where's the ?party? in ?multi-
party??: analyzing the structure of small-group socia-
ble talk. In CSCW '06: Proceedings of the 2006 20th
anniversary conference on Computer supported coop-
erative work, pages 393?402, New York, NY, USA.
ACM Press.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL'05).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:2003.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of document
structure using latent permutations. In Proceedings
of Human Language Technologies: The 2009 Annual
1187
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 371?
379, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP, pages
334?343.
Micha Elsner and Eugene Charniak. 2008a.
Coreference-inspired coherence modeling. In
Proceedings of ACL-08: HLT, Short Papers, pages
41?44, Columbus, Ohio, June. Association for
Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008b. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of ACL-08: HLT,
pages 834?842, Columbus, Ohio, June. Association
for Computational Linguistics.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Jennifer Foster. 2010. ?cba to check the spelling?: In-
vestigating parser performance on discussion forum
posts. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
381?384, Los Angeles, California, June. Association
for Computational Linguistics.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?171, Orlando, Florida. Harcourt Brace.
Fred Glover and Manuel Laguna. 1997. Tabu Search.
University of Colorado at Boulder.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Simon Haykin and Zhe Chen. 2005. The Cocktail Party
Problem. Neural Computation, 17(9):1875?1902.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence. In ACL, pages 391?398.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall's tau. Computational Linguis-
tics, 32(4):1?14.
Gideon Mann, Ryan McDonald, Mehryar Mohri, Nathan
Silberman, and Dan Walker. 2009. Efcient large-
scale distributed training of conditional maximum en-
tropy models. In Y. Bengio, D. Schuurmans, J. Laf-
ferty, C. K. I. Williams, and A. Culotta, editors, Ad-
vances in Neural Information Processing Systems 22,
pages 1231?1239.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 28?36,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Eleni Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25?55.
Neville Moray. 1959. Attention in dichotic listening: Af-
fective cues and the inuence of instructions. Quar-
terly Journal of Experimental Psychology, 11(1):56?
60.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In NAACL
'03, pages 70?72.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of EMNLP, pages
94?102, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Jacki O'Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP '03: Proceedings of the 2003 inter-
national ACM SIGGROUP conference on Supporting
group work, pages 40?49, New York, NY, USA. ACM
Press.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unied framework for predicting text qual-
ity. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages
186?195, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata
Vieira, Rodrigo Goulart, and Olga Uryupina. 2005.
Does discourse-new detection help denite description
resolution? In Proceedings of the Sixth International
Workshop on Computational Semantics, Tillburg.
Amruta Purandare and Diane J. Litman. 2008. Analyz-
ing dialog coherence using transition patterns in lexi-
cal and semantic features. In FLAIRS Conference'08,
pages 195?200.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
1188
streams. In SIGIR '06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 35?42,
New York, NY, USA. ACM.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In Proceedings of NAACL-09.
1189
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125?129,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Extending the Entity Grid with Entity-Specic Features
Micha Elsner
School of Informatics
University of Edinburgh
melsner0@gmail.com
Eugene Charniak
Department of Computer Science
Brown University, Providence, RI 02912
ec@cs.brown.edu
Abstract
We extend the popular entity grid representa-
tion for local coherence modeling. The grid
abstracts away information about the entities it
models; we add discourse prominence, named
entity type and coreference features to distin-
guish between important and unimportant en-
tities. We improve the best result for WSJ doc-
ument discrimination by 6%.
1 Introduction
A well-written document is coherent (Halliday and
Hasan, 1976)? it structures information so that each
new piece of information is interpretable given the
preceding context. Models that distinguish coherent
from incoherent documents are widely used in gen-
eration, summarization and text evaluation.
Among the most popular models of coherence is
the entity grid (Barzilay and Lapata, 2008), a sta-
tistical model based on Centering Theory (Grosz et
al., 1995). The grid models the way texts focus
on important entities, assigning them repeatedly to
prominent syntactic roles. While the grid has been
successful in a variety of applications, it is still a
surprisingly unsophisticated model, and there have
been few direct improvements to its simple feature
set. We present an extension to the entity grid which
distinguishes between different types of entity, re-
sulting in signicant gains in performance
1
.
At its core, the grid model works by predicting
whether an entity will appear in the next sentence
1
A public implementation is available via https://
bitbucket.org/melsner/browncoherence.
(and what syntactic role it will have) given its his-
tory of occurrences in the previous sentences. For
instance, it estimates the probability that ?Clinton?
will be the subject of sentence 2, given that it was
the subject of sentence 1. The standard grid model
uses no information about the entity itself? the prob-
ability is the same whether the entity under discus-
sion is ?Hillary Clinton? or ?wheat?. Plainly, this
assumption is too strong. Distinguishing important
from unimportant entity types is important in coref-
erence (Haghighi and Klein, 2010) and summariza-
tion (Nenkova et al, 2005); our model applies the
same insight to the entity grid, by adding informa-
tion from syntax, a named-entity tagger and statis-
tics from an external coreference corpus.
2 Related work
Since its initial appearance (Lapata and Barzilay,
2005; Barzilay and Lapata, 2005), the entity grid
has been used to perform wide variety of tasks. In
addition to its rst proposed application, sentence
ordering for multidocument summarization, it has
proven useful for story generation (McIntyre and
Lapata, 2010), readability prediction (Pitler et al,
2010; Barzilay and Lapata, 2008) and essay scor-
ing (Burstein et al, 2010). It also remains a criti-
cal component in state-of-the-art sentence ordering
models (Soricut and Marcu, 2006; Elsner and Char-
niak, 2008), which typically combine it with other
independently-trained models.
There have been few attempts to improve the en-
tity grid directly by altering its feature representa-
tion. Filippova and Strube (2007) incorporate se-
mantic relatedness, but nd no signicant improve-
125
1 [Visual meteorological conditions]
S
prevailed for [the
personal cross country ight for which [a VFR ight
plan]
O
was led]
X
.
2 [The ight]
S
originated at [Nuevo Laredo , Mexico]
X
,
at [approximately 1300]
X
.
s conditions plan flight laredo
1 S O X -
2 - - S X
Figure 1: A short text (using NP-only mention detection),
and its corresponding entity grid. The numeric token
?1300? is removed in preprocessing.
ment over the original model. Cheung and Penn
(2010) adapt the grid to German, where focused con-
stituents are indicated by sentence position rather
than syntactic role. The best entity grid for English
text, however, is still the original.
3 Entity grids
The entity grid represents a document as a matrix
(Figure 1) with a row for each sentence and a column
for each entity. The entry for (sentence i, entity j),
which we write r
i;j
, represents the syntactic role that
entity takes on in that sentence: subject (S), object
(O), or some other role (X)
2
. In addition, there is a
special marker (-) for entities which do not appear at
all in a given sentence.
To construct a grid, we must rst decide which
textual units are to be considered ?entities?, and how
the different mentions of an entity are to be linked.
We follow the -COREFERENCE setting from Barzi-
lay and Lapata (2005) and perform heuristic coref-
erence resolution by linking mentions which share a
head noun. Although some versions of the grid use
an automatic coreference resolver, this often fails
to improve results; in Barzilay and Lapata (2005),
coreference improves results in only one of their tar-
get domains, and actually hurts for readability pre-
diction. Their results, moreover, rely on running
coreference on the document in its original order; in
a summarization task, the correct order is not known,
which will cause even more resolver errors.
To build a model based on the grid, we treat the
columns (entities) as independent, and look at lo-
cal transitions between sentences. We model the
2
Roles are determined heuristically using trees produced by
the parser of (Charniak and Johnson, 2005).
transitions using the generative approach given in
Lapata and Barzilay (2005)
3
, in which the model
estimates the probability of an entity's role in the
next sentence, r
i;j
, given its history in the previ-
ous two sentences, r
i 1;j
; r
i 2;j
. It also uses a sin-
gle entity-specic feature, salience, determined by
counting the total number of times the entity is men-
tioned in the document. We denote this feature vec-
tor F
i;j
. For example, the vector for ?ight? after the
last sentence of the example would be F
3;f light
=
hX;S; sal = 2i. Using two sentences of context
and capping salience at 4, there are only 64 possi-
ble vectors, so we can learn an independent multino-
mial distribution for each F . However, the number
of vectors grows exponentially as we add features.
4 Experimental design
We test our model on two experimental tasks, both
testing its ability to distinguish between correct
and incorrect orderings for WSJ articles. In doc-
ument discrimination (Barzilay and Lapata, 2005),
we compare a document to a random permutation of
its sentences, scoring the system correct if it prefers
the original ordering
4
.
We also evaluate on the more difcult task of sen-
tence insertion (Chen et al, 2007; Elsner and Char-
niak, 2008). In this task, we remove each sentence
from the article and test whether the model prefers to
re-insert it at its original location. We report the av-
erage proportion of correct insertions per document.
As in Elsner and Charniak (2008), we test on sec-
tions 14-24 of the Penn Treebank, for 1004 test doc-
uments. We test signicance using the Wilcoxon
Sign-rank test, which detects signicant differences
in the medians of two distributions
5
.
5 Mention detection
Our main contribution is to extend the entity grid
by adding a large number of entity-specic features.
Before doing so, however, we add non-head nouns
to the grid. Doing so gives our feature-based model
3
Barzilay and Lapata (2005) give a discriminative model,
which relies on the same feature set as discussed here.
4
As in previous work, we use 20 random permutations of
each document. Since the original and permutation might tie,
we report both accuracy and balanced F-score.
5
Our reported scores are means, but to test signicance of
differences in means, we would need to use a parametric test.
126
Disc. Acc Disc. F Ins.
Random 50.0 50.0 12.6
Grid: NPs 74.4 76.2 21.3
Grid: all nouns
y
77.8 79.7 23.5
Table 1: Discrimination scores for entity grids with dif-
ferent mention detectors onWSJ development documents.
y
indicates performance on both tasks is signicantly dif-
ferent from the previous row of the table with p=.05.
more information to work with, but is benecial
even to the standard entity grid.
We alter our mention detector to add all nouns
in the document to the grid
6
, even those which do
not head NPs. This enables the model to pick up
premodiers in phrases like ?a Bush spokesman?,
which do not head NPs in the Penn Treebank. Find-
ing these is also necessary to maximize coreference
recall (Elsner and Charniak, 2010). We give non-
head mentions the role X. The results of this change
are shown in Table 1; discrimination performance
increases about 4%, from 76% to 80%.
6 Entity-specic features
As we mentioned earlier, the standard grid model
does not distinguish between different types of en-
tity. Given the same history and salience, the same
probabilities are assigned to occurrences of ?Hillary
Clinton?, ?the airlines?, or ?May 25th?, even though
we know a priori that a document is more likely to
be about Hillary Clinton than it is to be about May
25th. This problem is exacerbated by our same-head
coreference heuristic, which sometimes creates spu-
rious entities by lumping together mentions headed
by nouns like ?miles? or ?dollars?. In this section,
we add features that separate important entities from
less important or spurious ones.
Proper Does the entity have a proper mention?
Named entity The majority OPENNLP Morton et
al. (2005) named entity label for the coreferen-
tial chain.
Modiers The total number of modiers in all men-
tions in the chain, bucketed by 5s.
Singular Does the entity have a singular mention?
6
Barzilay and Lapata (2008) uses NPs as mentions; we are
unsure whether all other implementations do the same, but we
believe we are the rst to make the distinction explicit.
News articles are likely to be about people and
organizations, so we expect these named entity tags,
and proper NPs in general, to be more important to
the discourse. Entities with many modiers through-
out the document are also likely to be important,
since this implies that the writer wishes to point
out more information about them. Finally, singular
nouns are less likely to be generic.
We also add some features to pick out entities
that are likely to be spurious or unimportant. These
features depend on in-domain coreference data, but
they do not require us to run a coreference resolver
on the target document itself. This avoids the prob-
lem that coreference resolvers do not work well for
disordered or automatically produced text such as
multidocument summary sentences, and also avoids
the computational cost associated with coreference
resolution.
Linkable Was the head word of the entity ever
marked as coreferring in MUC6?
Unlinkable Did the head word of the entity occur 5
times in MUC6 and never corefer?
Has pronouns Were there 5 or more pronouns
coreferent with the head word of the entity in
the NANC corpus? (Pronouns in NANC are
automatically resolved using an unsupervised
model (Charniak and Elsner, 2009).)
No pronouns Did the head word of the entity occur
over 50 times in NANC, and have fewer than 5
coreferent pronouns?
To learn probabilities based on these features,
we model the conditional probability p(r
i;j
jF ) us-
ing multilabel logistic regression. Our model has
a parameter for each combination of syntactic role
r, entity-specic feature h and feature vector F :
rhF . This allows the old and new features to in-
teract while keeping the parameter space tractable
7
.
In Table 2, we examine the changes in our esti-
mated probability in one particular context: an entity
with salience 3 which appeared in a non-emphatic
role in the previous sentence. The standard entity
grid estimates that such an entity will be the sub-
ject of the next sentence with a probability of about
7
We train the regressor using OWLQN (Andrew and Gao,
2007), modied and distributed by Mark Johnson as part of
the Charniak-Johnson parse reranker (Charniak and Johnson,
2005).
127
Context P(next role is subj)
Standard egrid .045
Head coref in MUC6 .013
...and proper noun .025
...and NE type person .037
...and 5 modiers overall .133
Never coref in MUC6 .006
...and NE type date .001
Table 2: Probability of an entity appearing as subject of
the next sentence, given the history - X, salience 3, and
various entity-specic features.
.04. For most classes of entity, we can see that this
is an overestimate; for an entity described by a com-
mon noun (such as ?the airline?), the probability as-
signed by the extended grid model is .01. If we
suspect (based on MUC6 evidence) that the noun
is not coreferent, the probability drops to .006 (?an
increase?)? if it is a date, it falls even further, to .001.
However, given that the entity refers to a person, and
some of its mentions are modied, suggesting the ar-
ticle gives a title or description (?Obama's Secretary
of State, Hillary Clinton?), the chance that it will be
the subject of the next sentence more than triples.
7 Experiments
Table 3 gives results for the extended grid model
on the test set. This model is signicantly better
than the standard grid on discrimination (84% ver-
sus 80%) and has a higher mean score on insertion
(24% versus 21%)
8
.
The best WSJ results in previous work are those of
Elsner and Charniak (2008), who combine the entity
grid with models based on pronoun coreference and
discourse-new NP detection. We report their scores
in the table. This comparison is unfair, however,
because the improvements from adding non-head
nouns improve our baseline grid sufciently to equal
their discrimination result. State-of-the-art results
on a different corpus and task were achieved by Sori-
cut and Marcu (2006) using a log-linear mixture of
an entity grid, IBM translation models, and a word-
correspondence model based on Lapata (2003).
8
For insertion using the model on its own, the median
changes less than the mean, and the change in median score is
not signicant. However, using the combined model, the change
is signicant.
Disc. Acc Disc. F Ins.
Random 50.00 50.00 12.6
Elsner+Charniak 79.6 81.0 23.0
Grid 79.5 80.9 21.4
Extended Grid 84.0
y
84.5 24.2
Grid+combo 82.6 84.0 24.3
ExtEGrid+combo 86.0
y
86.5 26.7
y
Table 3: Extended entity grid and combination model
performance on 1004 WSJ test documents. Combination
models incorporate pronoun coreference, discourse-new
NP detection, and IBM model 1.
y
indicates an extended
model score better than its baseline counterpart at p=.05.
To perform a fair comparison of our extended
grid with these model-combining approaches, we
train our own combined model incorporating an en-
tity grid, pronouns, discourse-newness and the IBM
model. We combine models using a log-linear mix-
ture as in Soricut and Marcu (2006), training the
weights to maximize discrimination accuracy.
The second section of Table 3 shows these model
combination results. Notably, our extended entity
grid on its own is essentially just as good as the com-
bined model, which represents our implementation
of the previous state of the art. When we incorpo-
rate it into a combination, the performance increase
remains, and is signicant for both tasks (disc. 86%
versus 83%, ins. 27% versus 24%). Though the im-
provement is not perfectly additive, a good deal of
it is retained, demonstrating that our additions to the
entity grid are mostly orthogonal to previously de-
scribed models. These results are the best reported
for sentence ordering of English news articles.
8 Conclusion
We improve a widely used model of local discourse
coherence. Our extensions to the feature set involve
distinguishing simple properties of entities, such as
their named entity type, which are also useful in
coreference and summarization tasks. Although our
method uses coreference information, it does not re-
quire coreference resolution to be run on the target
documents. Given the popularity of entity grid mod-
els for practical applications, we hope our model's
improvements will transfer to summarization, gen-
eration and readability prediction.
128
Acknowledgements
We are most grateful to Regina Barzilay, Mark John-
son and three anonymous reviewers. This work was
funded by a Google Fellowship for Natural Lan-
guage Processing.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML '07.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL'05).
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in stu-
dent essays. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 681?684, Los Angeles, California, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
ne n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173?180.
Erdong Chen, Benjamin Snyder, and Regina Barzilay.
2007. Incremental text structuring with online hier-
archical ranking. In Proceedings of EMNLP.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
elds. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 186?195, Uppsala, Sweden, July. Association
for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of ACL-
08: HLT, Short Papers, pages 41?44, Columbus, Ohio,
June. Association for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2010. The same-
head heuristic for coreference. In Proceedings of ACL
10, Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 139?142, Saarbr?ucken, Germany, June. DFKI
GmbH. Document D-07-01.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June. Association for Computational
Linguistics.
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman, London.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Neil McIntyre and Mirella Lapata. 2010. Plot induction
and evolutionary search for story generation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1562?1572,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Thomas Morton, Joern Kottmann, Jason Baldridge, and
Gann Bierner. 2005. Opennlp: A java-based nlp
toolkit. http://opennlp.sourceforge.net.
Ani Nenkova, Advaith Siddharthan, and Kathleen McK-
eown. 2005. Automatically learning cognitive status
for multi-document summarization of newswire. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 241?248, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 544?554, Uppsala, Sweden, July.
Association for Computational Linguistics.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
129
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 184?193,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Bootstrapping a Unified Model of Lexical and Phonetic Acquisition
Micha Elsner
melsner0@gmail.com
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Sharon Goldwater
sgwater@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Jacob Eisenstein
jacobe@gmail.com
School of Interactive Computing
Georgia Institute of Technology
Atlanta, GA, 30308, USA
Abstract
During early language acquisition, infants must
learn both a lexicon and a model of phonet-
ics that explains how lexical items can vary
in pronunciation?for instance ?the? might be
realized as [Di] or [D@]. Previous models of ac-
quisition have generally tackled these problems
in isolation, yet behavioral evidence suggests
infants acquire lexical and phonetic knowledge
simultaneously. We present a Bayesian model
that clusters together phonetic variants of the
same lexical item while learning both a lan-
guage model over lexical items and a log-linear
model of pronunciation variability based on ar-
ticulatory features. The model is trained on
transcribed surface pronunciations, and learns
by bootstrapping, without access to the true
lexicon. We test the model using a corpus of
child-directed speech with realistic phonetic
variation and either gold standard or automati-
cally induced word boundaries. In both cases
modeling variability improves the accuracy of
the learned lexicon over a system that assumes
each lexical item has a unique pronunciation.
1 Introduction
Infants acquiring their first language confront two
difficult cognitive problems: building a lexicon of
word forms, and learning basic phonetics and phonol-
ogy. The two tasks are closely related: knowing what
sounds can substitute for one another helps in clus-
tering together variant pronunciations of the same
word, while knowing the environments in which par-
ticular words can occur helps determine which sound
changes are meaningful and which are not (Feldman
(a) intended: /ju want w2n/ /want e kUki/
(b) surface: [j@ waP w2n] [wan @ kUki]
(c) unsegmented: [j@waPw2n] [wan@kUki]
(d) idealized: /juwantw2n/ /wantekUki/
Figure 1: The utterances you want one? want a cookie?
represented (a) using a canonical phonemic encoding for
each word and (b) as they might be pronounced phoneti-
cally. Lines (c) and (d) remove the word boundaries (but
not utterance boundaries) from (b) and (a), respectively.
et al, 2009). For instance, if an infant who already
knows the word [ju] ?you? encounters a new word
[j@], they must decide whether it is a new lexical item
or a variant of the word they already know. Evidence
for the correct conclusion comes from the pronun-
ciation (many English vowels are reduced to [@] in
unstressed positions) and the context?if the next
word is ?want?, ?you? is a plausible choice.
To date, most models of infant language learn-
ing have focused on either lexicon-building or pho-
netic learning in isolation. For example, many mod-
els of word segmentation implicitly or explicitly
build a lexicon while segmenting the input stream
of phonemes into word tokens; in nearly all cases
the phonemic input is created from an orthographic
transcription using a phonemic dictionary, thus ab-
stracting away from any phonetic variability (Brent,
1999; Venkataraman, 2001; Swingley, 2005; Gold-
water et al, 2009, among others). As illustrated
in Figure 1, these models attempt to infer line (a)
from line (d). However, (d) is an idealization: real
speech has variability, and behavioral evidence sug-
gests that infants are still learning about the phonetics
and phonology of their language even after beginning
to segment words, rather than learning to neutralize
184
the variations first and acquiring the lexicon after-
wards (Feldman et al, 2009, and references therein).
Based on this evidence, a more realistic model of
early language acquisition should propose a method
of inferring the intended forms (Figure 1a) from the
unsegmented surface forms (1c) while also learning a
model of phonetic variation relating the intended and
surface forms (a) and (b). Previous models with sim-
ilar goals have learned from an artificial corpus with
a small vocabulary (Driesen et al, 2009; Ra?sa?nen,
2011) or have modeled variability only in vowels
(Feldman et al, 2009); to our knowledge, this paper
is the first to use a naturalistic infant-directed corpus
while modeling variability in all segments, and to
incorporate word-level context (a bigram language
model). Our main contribution is a joint lexical-
phonetic model that infers intended forms from seg-
mented surface forms; we test the system using in-
put with either gold standard word boundaries or
boundaries induced by an existing unsupervised seg-
mentation model (Goldwater et al, 2009). We show
that in both cases modeling variability improves the
accuracy of the learned lexicon over a system that
assumes each intended form has a unique surface
form.
Our model is conceptually similar to those used
in speech recognition and other applications: we
assume the intended tokens are generated from a bi-
gram language model and then distorted by a noisy
channel, in particular a log-linear model of phonetic
variability. But unlike speech recognition, we have
no ?intended-form, surface-form? training pairs to
train the phonetic model, nor even a dictionary of
intended-form strings to train the language model.
Instead, we initialize the noise model using feature
weights based on universal linguistic principles (e.g.,
a surface phone is likely to share articulatory features
with the intended phone) and use a bootstrapping
process to iteratively infer the intended forms and
retrain the language model and noise model. While
we do not claim that the particular inference mech-
anism we use is cognitively plausible, our positive
results further support the claim that infants can and
do acquire phonetics and the lexicon in concert.
2 Related work
Our work is inspired by the lexical-phonetic model
of Feldman et al (2009). They extend a model for
clustering acoustic tokens into phonetic categories
(Vallabha et al, 2007) by adding a lexical level that
simultaneously clusters word tokens (which contain
the acoustic tokens) into lexical entries. Including
the lexical level improves the model?s phonetic cat-
egorization, and a follow-up study on artificial lan-
guage learning (Feldman, 2011) supports the claim
that human learners use lexical knowledge to distin-
guish meaningful from unimportant phonetic con-
trasts. Feldman et al (2009) use a real-valued rep-
resentation for vowels (formant values), but assume
no variability in consonants, and treat each word to-
ken independently. In contrast, our model uses a
symbolic representation for sounds, but models vari-
ability in all segment types and incorporates a bigram
word-level language model.
To our knowledge, the only other lexicon-building
systems that also learn about phonetic variability are
those of Driesen et al (2009) and Ra?sa?nen (2011).
These systems learn to represent lexical items and
their variability from a discretized representation of
the speech stream, but they are tested on an artifi-
cial corpus with only 80 vocabulary items that was
constructed so as to ?avoid strong word-to-word de-
pendencies? (Ra?sa?nen, 2011). Here, we use a natu-
ralistic corpus, demonstrating that lexical-phonetic
learning is possible in this more general setting and
that word-level context information is important for
doing so.
Several other related systems work directly from
the acoustic signal and many of these do use natu-
ralistic corpora. However, they do not learn at both
the lexical and phonetic/acoustic level. For example,
Park and Glass (2008), Aimetti (2009), Jansen et al
(2010), and McInnes and Goldwater (2011) present
lexicon-building systems that use hard-coded acous-
tic similarity measures rather than learning about
variability, and they only extract and cluster a few
frequent words. On the phonetic side, Varadarajan et
al. (2008) and Dupoux et al (2011) describe systems
that learn phone-like units but without the benefit of
top-down information.
A final line of related work is on word segmenta-
tion. In addition to the models mentioned in Section
1, which use phonemic input, a few models of word
segmentation have been tested using phonetic input
(Fleck, 2008; Rytting, 2007; Daland and Pierrehum-
bert, 2010). However, they do not cluster segmented
185
Figure 2: Our generative model of the surface tokens s
from intended tokens x, which occur with left and right
contexts l and r.
word tokens into lexical items (none of these mod-
els even maintains an explicit lexicon), nor do they
model or learn from phonetic variation in the input.
3 Lexical-phonetic model
Our lexical-phonetic model is defined using the stan-
dard noisy channel framework: first a sequence of
intended word tokens is generated using a language
model, and then each token is transformed by a proba-
bilistic finite-state transducer to produce the observed
surface sequence. In this section, we present the
model in a hierarchical Bayesian framework to em-
phasize its similarity to existing models, in particu-
lar those of Feldman et al (2009) and Goldwater et
al. (2009). In our actual implementation, however,
we use approximation and MAP point estimates to
make our inference process more tractable; we dis-
cuss these simplifications in Section 4.
Our observed data consists of a (segmented) se-
quence of surface words s1 . . . sn. We wish to re-
cover the corresponding sequence of intended words
x1 . . . xn. As shown in Figure 2, si is produced from
xi by a transducer T : si ? T (xi), which models
phonetic changes. Each xi is sampled from a dis-
tribution ? which represents word frequencies, and
its left and right context words, li and ri, are drawn
from distributions conditioned on xi, in order to cap-
ture information about the environments in which
xi appears: li ? PL(xi), ri ? PR(xi). Because the
number of word types is not known in advance, ? is
drawn from a Dirichlet process DP (?), and PL(x)
and PR(x) have Pitman-Yor priors with concentra-
tion parameter 0 and discount d (Teh, 2006).
Our generative model of xi is unusual for two rea-
sons. First, we treat each xi independently rather
than linking them via a Markov chain. This makes
the model deficient, since li overlaps with xi?1 and
so forth, generating each token twice. During in-
ference, however, we will never compute the joint
probability of all the data at once, only the prob-
abilities of subsets of the variables with particular
intended word forms u and v. As long as no two of
these words are adjacent, the deficiency will have no
effect. We make this independence assumption for
computational reasons?when deciding whether to
merge u and v into a single lexical entry, we compute
the change in estimated probability for their contexts,
but not the effect on other words for which u and v
themselves appear as context words.
Also unusual is that we factor the joint probabil-
ity (l, x, r) as p(x)p(l|x)p(r|x) rather than as a left-
to-right chain p(l)p(x|l)p(r|x). Given our indepen-
dence assumption above, these two quantities are
mathematically equivalent, so the difference matters
only because we are using smoothed estimates. Our
factorization leads to a symmetric treatment of left
and right contexts, which simplifies implementation:
we can store all the context parameters locally as
PL(?|x) rather than distributed over various P (x|?).
Next, we explain our transducer T . A weighted
finite-state transducer (WFST) is a variant of a finite-
state automaton (Pereira et al, 1994) that reads an
input string symbol-by-symbol and probabilistically
produces an output string; thus it can be used to
specify a conditional probability on output strings
given an input. Our WFST (Figure 3) computes a
weighted edit distance, and is implemented using
OpenFST (Allauzen et al, 2007). It contains a state
for each triplet of (previous, current, next) phones;
conditioned on this state, it emits a character out-
put which can be thought of as a possible surface
realization of current in its particular environment.
The output can be the empty string , in which case
current is deleted. The machine can also insert char-
acters at any point in the string, by transitioning to an
insert state (previous, , current) and then returning
while emitting some new character.
The transducer is parameterized by the probabil-
ities of the arcs. For instance, all arcs leaving the
state (?, D, i) consume the character D and emit some
character c with probability p(c|?, D, i). Following
186
Figure 3: The fragment of the transducer responsible for
input string [Di] ?the?. ?...? represents an output arc for
each possible character, including the empty string ; ? is
the word boundary marker.
Dreyer et al (2008), we parameterize these distribu-
tions with a log-linear model. The model features are
based on articulatory phonetics and distinguish three
dimensions of sound production: voicing, place of
articulation and manner of articulation.
Features are generated from four positional tem-
plates (Figure 4): (curr)?out, (prev, curr)?out,
(curr, next)?out and (prev, curr, next)?out. Each
template is instantiated once per articulatory dimen-
sion, with prev, curr, next and out replaced by their
values for that dimension: for instance, there are
two voicing values, voiced and unvoiced1 and the
(curr)?out template for [D] producing [d] would
be instantiated as (voiced)?voiced. To capture
trends specific to particular sounds, each template
is instantiated again using the actual symbol for
curr and articulatory values for everything else (e.g.,
[D]?unvoiced). An additional template,?out, cap-
tures the marginal frequency of the output symbol.
There are also faithfulness features, same-sound,
same-voice, same-place and same-manner which
check if curr is exactly identical to out or shares
the exact value of a particular feature.
Our choice of templates and features is based on
standard linguistic principles: we expect that chang-
ing only a single articulatory dimension will be more
acceptable than changing several, and that the artic-
ulatory dimensions of context phones are important
because of assimilatory and dissimilatory processes
(Hayes, 2011). In modern phonetics and phonology,
these generalizations are usually expressed as Opti-
mality Theory constraints; log-linear models such as
ours have previously been used to implement stochas-
1We use seven place values and five manner values (stop,
nasal stop, fricative, vowel, other). Empty segments like  and ?
are assigned a special value ?no-value? for all features.
Figure 4: Some features generated for (?, D, i)? d. Each
black factor node corresponds to a positional template.
The features instantiated for the (curr)?out and ?out
template are shown in full, and we show some of the
features for the (curr,next)?out template.
tic Optimality Theory models (Goldwater and John-
son, 2003; Hayes and Wilson, 2008).
4 Inference
Global optimization of the model posterior is diffi-
cult; instead we use Viterbi EM (Spitkovsky et al,
2010; Allahverdyan and Galstyan, 2011). We begin
with a simple initial transducer and alternate between
two phases: clustering together surface forms, and
reestimating the transducer parameters. We iterate
this procedure until convergence (when successive
clustering phases find nearly the same set of merges);
this tends to take about 5 or 6 iterations.
In our clustering phase, we improve the model
posterior as much as possible by greedily making
type merges, where, for a pair of intended word forms
u and v, we replace all instances of xi = u with
xi = v. We maintain the invariant that each intended
word form?s most common surface form must be
itself; this biases the model toward solutions with
low distortion in the transducer.
4.1 Scoring merges
We write the change in the log posterior probability
of the model resulting from a type merge of u to v as
?(u, v), which factors into two terms, one depending
on the surface string and the transducer, and the other
depending on the string of intended words. In order to
ensure that each intended word form?s most common
surface form is itself, we define ?(u, v) = ?? if u
is more common than v.
We write the log probability of x being transduced
to s as T (s|x). If we merge u into v, we no longer
187
need to produce any surface forms from u, but instead
we must derive them from v. If #(?) counts the
occurrences of some event in the current state of the
model, the transducer component of ? is:
?T =
?
s
#(xi=u, si=s)(T (s|v)? T (s|u)) (1)
This term is typically negative, voting against a
merge, since u is more similar to itself than to v.
The language modeling term relating to the in-
tended string again factors into multiple components.
The probability of a particular li, xi, ri can be broken
into p(xi)p(li|xi)p(ri|xi) according to the model.
We deal first with the p(xi) unigram term, consid-
ering all tokens where xi ? {u, v} and computing
the probability pu = p(xi = u|xi ? {u, v}). By
definition of a Dirichlet process, the marginal over a
subset of the variables will be Dirichlet, so for ? > 1
we have the MAP estimate:
pu =
#(xi=u) + ?? 1
#(xi ? {u, v}) + 2(?? 1)
(2)
pv = p(xi = v|xi ? {u, v}) is computed similarly.
If we decide to merge u into v, however, the proba-
bility p(xi = v|xi ? {u, v}) becomes 1. The change
in log-probability resulting from the merge is closely
related to the entropy of the distribution:
?U = ?#(xi=u) log(pu)?#(xi=v) log(pv) (3)
This change must be positive and favors merging.
Next, we consider the change in probability from
the left contexts (the derivations for right contexts are
equivalent). If u and v are separate words, we gen-
erate their left contexts from different distributions
p(l|u) and p(l|v), while if they are merged, we must
generate all the contexts from the same distribution
p(l|{u, v}). This change is:
?L =
?
l
#(l, u){log(p(l|{u, v}))? log(p(l|u)}
+
?
l
#(l, v){log(p(l|{u, v}))? log(p(l|v)}
In a full Bayesian model, we would integrate over
the parameters of these distributions; instead, we
use Kneser-Ney smoothing (Kneser and Ney, 1995)
which has been shown to approximate the MAP solu-
tion of a hierarchical Pitman-Yor model (Teh, 2006;
Goldwater et al, 2006). The Kneser-Ney discount2
d is a tunable parameter of our system, and con-
trols whether the term favors merging or not. If d is
small, p(l|u) and p(l|v) are close to their maximum-
likelihood estimates, and ?L is similar to a Jensen-
Shannon divergence; it is always negative and dis-
courages mergers. As d increases, however, p(l|u)
for rare words approaches the prior distribution; in
this case, merging two words may result in better
posterior parameters than estimating both separately,
since the combined estimate loses less mass to dis-
counting.
Because neither the transducer nor the language
model are perfect models of the true distribution,
they can have incompatible dynamic ranges. Often,
the transducer distribution is too peaked; to remedy
this, we downweight the transducer probability by
?, a parameter of our model, which we set to .5.
Downweighting of the acoustic model versus the LM
is typical in speech recognition (Bahl et al, 1980).
To summarize, the full change in posterior is:
?(u, v) = ?U + ?L + ?R + ??T (4)
There are four parameters. The transducer regular-
ization r = 1 and unigram prior ? = 2, which we
set ad-hoc, have little impact on performance. The
Kneser-Ney discount d = 2 and transducer down-
weight ? = .5 have more influence and were tuned
on development data.
4.2 Clustering algorithm
In the clustering phase, we start with an initial solu-
tion in which each surface form is its own intended
pronunciation and iteratively improve this solution
by merging together word types, picking (approxi-
mately) the best merger at each point.
We begin by computing a set of candidate mergers
for each surface word type u. This step saves time
by quickly rejecting mergers which are certain to get
very low transducer scores. We reject a pair u, v if
the difference in their length is greater than 4, or if
both words are longer than 4 segments, but, when
we consider them as unordered bags of segments, the
Dice coefficient between them is less than .5.
For each word u and all its candidates v, we com-
pute ?(u, v) as in Equation 4. We keep track of the
2We use one discount, rather than several as in modified KN.
188
Input: vocabulary of surface forms u
Input: C(u): candidate intended forms of u
Output: intend(u): intended form of u
foreach u ? vocab do
// initialization
v?(u)? argmaxv ?C(u) ?(u, v);
??(u)? ?(u, v?(u))
intend(u)? u
add u to queue Q with priority ??(u))
while top(Q) > ?? do
u? pop(Q)
recompute v?(u),??(u)
if ??(u) > 0 then
// merge u with best merger
intend(u)? v?(u)
update ?(x, u) ?x : v?(x) = u
remove u from C(x) ?x
update ?(x, v) ?x : v?(x) = v
update ?(v, x) ?x ? C(v)
if updated ? > ?? for any words then
reset ??, v? for those words
// (these updates can
increase a word?s priority
from ??)
else if ??(u) 6= ?? then
// reject but leave in queue
??(u)? ??
Algorithm 1: Our clustering phase.
current best target v?(u) and best score ??(u), using
a priority queue. At each step of the algorithm, we
pop the u with the current best ??(u), recompute
its scores, and then merge it with v?(u) if doing so
would improve the model posterior. In an exact al-
gorithm, we would then need to recompute most of
the other scores, since merging u and v?(u) affects
other words for which u and v?(u) are candidates,
and also words for which they appear in the context
set. However, recomputing all these scores would be
extremely time-consuming.3 Therefore, we recom-
pute scores for only those words where the previous
best merger was either u or v?(u). (If the best merge
would not improve the probability, we reject it, but
since its score might increase if we merge v?(u), we
leave u in the queue, setting its ? score to ??; this
score will be updated if we merge v?(u).)
Since we recompute the exact scores ?(u, v) im-
mediately before merging u, the algorithm is guaran-
3The transducer scores can be cached since they depend only
on surface forms, but the language model scores cannot.
teed never to reduce the posterior probability. It can
potentially make changes in the wrong order, since
not all the ?s are recomputed in each step, but most
changes do not affect one another, so performing
them out of order has no impact. Empirically, we
find that mutually exclusive changes (usually of the
form (u, v) and (v, w)) tend to differ enough in initial
score that they are evaluated in the correct order.
4.3 Training the transducer
To train the transducer on a set of mappings between
surface and intended forms, we find the maximum-
probability state sequence for each mapping (another
application of Viterbi EM) and extract features for
each state and its output. Learning weights is then
a maximum-entropy problem, which we solve using
Orthant-wise Limited-memory Quasi-Newton.4
To construct our initial transducer, we first learn
weights for the marginal distribution on surface
sounds by training the max-ent system with only the
bias features active. Next, we manually set weights
(Table 1) for insertions and deletions, which do not
appear on the surface, and for faithfulness features.
Other features get an initial weight of 0.
5 Experiments
5.1 Dataset
Our corpus is a processed version of the Bernstein-
Ratner corpus (Bernstein-Ratner, 1987) from
CHILDES (MacWhinney, 2000), which contains or-
thographic transcriptions of parent-child dyads with
infants aged 13-23 months. Brent and Cartwright
(1996) created a phonemic version of this corpus
by extracting all infant-directed utterances and con-
verted them to a phonemic transcription using a dic-
tionary. This version, which contains 9790 utterances
(33399 tokens, 1321 types), is now standard for word
segmentation, but contains no phonetic variability.
Since producing a close phonetic transcription of
this data would be impractical, we instead construct
an approximate phonetic version using information
from the Buckeye corpus (Pitt et al, 2007). Buckeye
is a corpus of adult-directed conversational Ameri-
can English, and has been phonetically transcribed
4We use the implementation of Andrew and Gao (2007) with
an l2 regularizer and regularization parameter r = 1; although
this could be tuned, in practice it has little effect on results.
189
Feature Weight
output-is-x marginal p(x)
output-is- 0
same-sound 5
same-{place,voice, manner} 2
insertion -3
Table 1: Initial transducer weights.
?about? ahbawt:15, bawt:9, ihbawt:4, ahbawd:4, ih-
bawd:4, ahbaat:2, baw:1, ahbaht:1, erbawd:1,
bawd:1, ahbaad:1, ahpaat:1, bah:1, baht:1,
ah:1, ahbahd:1, ehbaat:1, ahbaed:1, ihbaht:1,
baot:1
?wanna? waanah:94, waanih:37, wahnah:16, waan:13,
wahneh:8, wahnih:5, wahney:3, waanlih:3,
wehnih:2, waaneh:2, waonih:2, waaah:1,
wuhnih:1, wahn:1, waantah:1, waanaa:1,
wowiy:1, waaih:1, wah:1, waaniy:1
Table 2: Empirical distribution of pronunciations of
?about? and ?wanna? in our dataset.
by hand to indicate realistic pronunciation variability.
To create our phonetic corpus, we replace each phone-
mic word in the Bernstein-Ratner-Brent corpus with
a phonetic pronunciation of that word sampled from
the empirical distribution of pronunciations in Buck-
eye (Table 2). If the word never occurs in Buckeye,
we use the original phonemic version.
Our corpus is not completely realistic as a sam-
ple of child-directed speech. Since each pronuncia-
tion is sampled independently, it lacks coarticulation
and prosodic effects, and the distribution of pronun-
ciations is derived from adult-directed rather than
child-directed speech. Nonetheless, it represents pho-
netic variability more realistically than the Bernstein-
Ratner-Brent corpus, while still maintaining the lexi-
cal characteristics of infant-directed speech (as com-
pared to the Buckeye corpus, with its much larger
vocabulary and more complex language model).
We conduct our development experiments on the
first 8000 input utterances, holding out the remain-
ing 1790 for evaluation. For evaluation experiments,
we run the system on all 9790 utterances, reporting
scores on only the last 1790.
5.2 Metrics
We evaluate our results by generalizing the three
segmentation metrics from Goldwater et al (2009):
word boundary F-score, word token F-score, and
lexicon (word type) F-score.
0 1 2 3 4 5Iteration
75
76
77
78
79
80
81
82
Token F
Lexicon F
Figure 5: System scores over 5 iterations.
In our first set of experiments we evaluate how
well our system clusters together surface forms de-
rived from the same intended form, assuming gold
standard word boundaries. We do not evaluate the
induced intended forms directly against the gold stan-
dard intended forms?we want to evaluate cluster
memberships and not labels. Instead we compute
a one-to-one mapping between our induced lexical
items and the gold standard, maximizing the agree-
ment between the two (Haghighi and Klein, 2006).
Using this mapping, we compute mapped token F-
score5 and lexicon F-score.
In our second set of experiments, we use unknown
word boundaries and evaluate the segmentations. We
report the standard word boundary F and unlabeled
word token F as well as mapped F. The unlabeled to-
ken score counts correctly segmented tokens, whether
assigned a correct intended form or not.
5.3 Known word boundaries
We first run our system with known word boundaries
(Table 3). As a baseline, we treat every surface token
as its own intended form (none). This baseline has
fairly high accuracy; 65% of word tokens receive
the most common pronunciation for their intended
form.6 As an upper bound, we find the best intended
form for each surface type (type ubound). This cor-
rectly resolves 91% of tokens; the remaining error is
due to homophones (surface types corresponding to
more than one intended form). We also test our sys-
5When using the gold word boundaries, the precision and
recall are equal and this is is the same as the accuracy; in seg-
mentation experiments the two differ, because with fewer seg-
mentation boundaries, the system proposes fewer tokens. Only
correctly segmented tokens which are also mapped to the correct
form count as matches.
6The lexicon recall is not quite 100% because one rare word
appears only as a homophone of another word.
190
System Tok F Lex P Lex R Lex F
none 65.4 50.2 99.7 66.7
initializer 75.2 83.2 73.3 78.0
system 79.2 87.1 75.9 81.1
oracle trans. 82.7 88.7 83.8 86.2
type ubound 91.0 97.5 98.0 97.7
Table 3: Results on 1790 utterances (known boundaries).
Boundaries Unlabeled Tokens
P R F P R F
no var. 90.1 80.3 84.9 74.5 68.7 71.5
w/var. 70.4 93.5 80.3 56.5 69.7 62.4
Table 4: Degradation in dpseg segmentation perfor-
mance caused by pronunciation variation.
Mapped Tokens Lexicon (types)
P R F P R F
none 39.8 49.0 43.9 37.7 49.1 42.6
init 42.2 52.0 56.5 50.1 40.8 45.0
sys 44.2 54.5 48.8 48.6 43.1 45.7
Table 5: Results on 1790 utterances (induced boundaries).
tem using an oracle transducer (oracle trans.)?the
transducer estimated from the upper-bound mapping.
This scores 83%, showing that our articulatory fea-
ture set captures most, but not all, of the available
information. At the beginning of bootstrapping, our
system (init) scores 75%, but this improves to 79%
after five iterations of reestimation (system). Most
learning occurs in the first two or three iterations
(Figure 5).
To determine the importance of different parts of
our system, we run a few ablation tests on develop-
ment data. Context information is critical to obtain
a good solution; setting ?L and ?R to 0 lowers our
dev token F-score from 83% to 75%. Initializing
all feature weights to 0 yields a poor initial solution
(18% dev token F instead of 75%), but after learn-
ing the result is only slightly lower than using the
weights in Table 1 (78% rather than 80%), showing
that the system is quite robust to initialization.
5.4 Unknown word boundaries
As a simple extension of our model to the case of
unknown word boundaries, we interleave it with an
existing model of word segmentation, dpseg (Gold-
water et al, 2009).7 In each iteration, we run the
segmenter, then bootstrap our model for five itera-
tions on the segmented output. We then concatenate
the intended word sequence proposed by our model
to produce the next iteration?s segmenter input.
Phonetic variation is known to reduce the perfor-
mance of dpseg (Fleck, 2008; Boruta et al, 2011)
and our experiments confirm this (Table 4). Using
induced word boundaries also makes it harder to
recover the lexicon (Table 5), lowering the baseline
F-score from 67% to 43%. Nevertheless, our system
improves the lexicon F-score to 46%, with token F
rising from 44% to 49%, demonstrating the system?s
ability to work without gold word boundaries. Un-
fortunately, performing multiple iterations between
the segmenter and lexical-phonetic learner has little
further effect; we hope to address this issue in future.
6 Conclusion
We have presented a noisy-channel model that si-
multaneously learns a lexicon, a bigram language
model, and a model of phonetic variation, while us-
ing only the noisy surface forms as training data.
It is the first model of lexical-phonetic acquisition
to include word-level context and to be tested on an
infant-directed corpus with realistic phonetic variabil-
ity. Whether trained using gold standard or automati-
cally induced word boundaries, the model recovers
lexical items more effectively than a system that as-
sumes no phonetic variability; moreover, the use of
word-level context is key to the model?s success. Ul-
timately, we hope to extend the model to jointly infer
word boundaries along with lexical-phonetic knowl-
edge, and to work directly from acoustic input. How-
ever, we have already shown that lexical-phonetic
learning from a broad-coverage corpus is possible,
supporting the claim that infants acquire lexical and
phonetic knowledge simultaneously.
Acknowledgements
This work was supported by EPSRC grant
EP/H050442/1 to the second author.
7dpseg1.2 from http://homepages.inf.ed.ac.
uk/sgwater/resources.html
191
References
Guillaume Aimetti. 2009. Modelling early language
acquisition skills: Towards a general statistical learning
mechanism. In Proceedings of the Student Research
Workshop at EACL.
Armen Allahverdyan and Aram Galstyan. 2011. Compar-
ative analysis of Viterbi training and ML estimation for
HMMs. In Advances in Neural Information Processing
Systems (NIPS).
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Galen Andrew and Jianfeng Gao. 2007. Scalable training
of L1-regularized log-linear models. In ICML ?07.
Lalit Bahl, Raimo Bakis, Frederick Jelinek, and Robert
Mercer. 1980. Language-model/acoustic-channel-
model balance mechanism. Technical disclosure bul-
letin Vol. 23, No. 7b, IBM, December.
Nan Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
L. Boruta, S. Peperkamp, B. Crabbe?, E. Dupoux, et al
2011. Testing the robustness of online word segmenta-
tion: effects of linguistic diversity and phonetic varia-
tion. ACL HLT 2011, page 1.
Michael Brent and Timothy Cartwright. 1996. Distribu-
tional regularity and phonotactic constraints are useful
for segmentation. Cognition, 61:93?125.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105, February.
R. Daland and J.B. Pierrehumbert. 2010. Learning
diphone-based segmentation. Cognitive Science.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 1080?1089, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Joris Driesen, Louis ten Bosch, and Hugo Van hamme.
2009. Adaptive non-negative matrix factorization in
a computational model of language acquisition. In
Proceedings of Interspeech.
E. Dupoux, G. Beraud-Sudreau, and S. Sagayama. 2011.
Templatic features for modeling phoneme acquisition.
In Proceedings of the 33rd Annual Cognitive Science
Society.
Naomi Feldman, Thomas Griffiths, and James Morgan.
2009. Learning phonetic categories by learning a lexi-
con. In Proceedings of the 31st Annual Conference of
the Cognitive Science Society (CogSci).
Naomi Feldman. 2011. Interactions between word and
speech sound categorization in language acquisition.
Ph.D. thesis, Brown University.
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of ACL-08: HLT, pages
130?138, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Sharon Goldwater and Mark Johnson. 2003. Learning OT
constraint rankings using a maximum entropy model.
In J. Spenader, A. Eriksson, and Osten Dahl, editors,
Proceedings of the Stockholm Workshop on Variation
within Optimality Theory, pages 111?120, Stockholm.
Stockholm University.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006. Interpolating between types and tokens by esti-
mating power-law generators. In Advances in Neural
Information Processing Systems (NIPS) 18.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. In In 46th
Annual Meeting of the ACL, pages 398?406.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learning.
Linguistic Inquiry, 39(3):379?440.
Bruce Hayes. 2011. Introductory Phonology. John Wiley
and Sons.
A. Jansen, K. Church, and H. Hermansky. 2010. Towards
spoken term discovery at scale with zero resources. In
Proceedings of Interspeech, pages 1676?1679.
R. Kneser and H. Ney. 1995. Improved backing-off for M-
gram language modeling. In Proc. ICASSP ?95, pages
181?184, Detroit, MI, May.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Vol 2: The Database. Lawrence
Erlbaum Associates, Mahwah, NJ, 3rd edition.
Fergus R. McInnes and Sharon Goldwater. 2011. Un-
supervised extraction of recurring words from infant-
directed speech. In Proceedings of the 33rd Annual
Conference of the Cognitive Science Society.
A. S. Park and J. R. Glass. 2008. Unsupervised pat-
tern discovery in speech. IEEE Transactions on Audio,
Speech and Language Processing, 16:186?197.
192
Fernando Pereira, Michael Riley, and Richard Sproat.
1994. Weighted rational transductions and their ap-
plication to human language processing. In HLT.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech (2nd release).
Okko Ra?sa?nen. 2011. A computational model of word
segmentation from continuous speech using transitional
probabilities of atomic acoustic events. Cognition,
120(2):28.
Anton Rytting. 2007. Preserving Subsegmental Varia-
tion in Modeling Word Segmentation (Or, the Raising
of Baby Mondegreen). Ph.D. thesis, The Ohio State
University.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D. Manning. 2010. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, pages 9?17, Up-
psala, Sweden, July. Association for Computational
Linguistics.
D. Swingley. 2005. Statistical clustering and the contents
of the infant vocabulary. Cognitive Psychology, 50:86?
132.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 985?992, Sydney,
Australia, July. Association for Computational Linguis-
tics.
G.K. Vallabha, J.L. McClelland, F. Pons, J.F. Werker, and
S. Amano. 2007. Unsupervised learning of vowel
categories from infant-directed speech. Proceedings
of the National Academy of Sciences, 104(33):13273?
13278.
B. Varadarajan, S. Khudanpur, and E. Dupoux. 2008. Un-
supervised learning of acoustic sub-word units. In Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics on Human Language
Technologies: Short Papers, pages 165?168. Associa-
tion for Computational Linguistics.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
193
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1084?1093,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Bootstrapping into Filler-Gap: An Acquisition Story
Marten van Schijndel Micha Elsner
The Ohio State University
{vanschm,melsner}@ling.ohio-state.edu
Abstract
Analyses of filler-gap dependencies usu-
ally involve complex syntactic rules or
heuristics; however recent results suggest
that filler-gap comprehension begins ear-
lier than seemingly simpler constructions
such as ditransitives or passives. Therefore,
this work models filler-gap acquisition as a
byproduct of learning word orderings (e.g.
SVO vs OSV), which must be done at a
very young age anyway in order to extract
meaning from language. Specifically, this
model, trained on part-of-speech tags, rep-
resents the preferred locations of semantic
roles relative to a verb as Gaussian mix-
tures over real numbers.
This approach learns role assignment in
filler-gap constructions in a manner con-
sistent with current developmental findings
and is extremely robust to initialization
variance. Additionally, this model is shown
to be able to account for a characteristic er-
ror made by learners during this period (A
and B gorped interpreted as A gorped B).
1 Introduction
The phenomenon of filler-gap, where the argument
of a predicate appears outside its canonical posi-
tion in the phrase structure (e.g. [the apple]
i
that
the boy ate t
i
or [what]
i
did the boy eat t
i
), has long
been an object of study for syntacticians (Ross,
1967) due to its apparent processing complexity.
Such complexity is due, in part, to the arbitrary
length of the dependency between a filler and its
gap (e.g. [the apple]
i
that Mary said the boy ate t
i
).
Recent studies indicate that comprehension of
filler-gap constructions begins around 15 months
(Seidl et al, 2003; Gagliardi et al, 2014). This
finding raises the question of how such a complex
phenomenon could be acquired so early since chil-
dren at that age do not yet have a very advanced
grasp of language (e.g. ditransitives do not seem
to be generalized until at least 31 months; Gold-
berg et al 2004, Bello 2012). This work shows
that filler-gap comprehension in English may be
Age
Wh-S
Wh-O
1-1
13mo
No
No
15mo
Yes
(Yes)
20mo
Yes
Yes
Yes
25mo
Yes
Yes
No
Figure 1: The developmental timeline of subject
(Wh-S) and object (Wh-O) wh-clause extraction
comprehension suggested by experimental results
(Seidl et al, 2003; Gagliardi et al, 2014). Paren-
theses indicate weak comprehension. The final row
shows the timeline of 1-1 role bias errors (Naigles,
1990; Gertner and Fisher, 2012). Missing nodes de-
note a lack of studies.
acquired through learning word orderings rather
than relying on hierarchical syntactic knowledge.
This work describes a cognitive model of the de-
velopmental timecourse of filler-gap comprehension
with the goal of setting a lower bound on the mod-
eling assumptions necessary for an ideal learner
to display filler-gap comprehension. In particular,
the model described in this paper takes chunked
child-directed speech as input and learns orderings
over semantic roles. These orderings then permit
the model to successfully resolve filler-gap depen-
dencies.
1
Further, the model presented here is also
shown to initially reflect an idiosyncratic role as-
signment error observed in development (e.g. A
and B kradded interpreted as A kradded B ; Gert-
ner and Fisher, 2012), though after training, the
model is able to avoid the error. As such, this work
may be said to model a learner from 15 months to
between 25 and 30 months.
1
This model does not explicitly learn gap positions,
but rather assigns thematic roles to arguments based
on where those arguments are expected to manifest.
This approach to filler-gap comprehension is supported
by findings that show people do not actually link fillers
to gap positions but instead link the filler to a verb
with missing arguments (Pickering and Barry, 1991)
1084
2 Background
The developmental timeline during which children
acquire the ability to process filler-gap construc-
tions is not well-understood. Language comprehen-
sion precedes production, and the developmental
literature on the acquisition of filler-gap construc-
tions is sparsely populated due to difficulties in de-
signing experiments to test filler-gap comprehen-
sion in preverbal infants. Older studies typically
looked at verbal children and the mistakes they
make to gain insight into the acquisition process
(de Villiers and Roeper, 1995).
Recent studies, however, indicate that filler-
gap comprehension likely begins earlier than pro-
duction (Seidl et al, 2003; Gagliardi and Lidz,
2010; Gagliardi et al, 2014). Therefore, studies
of verbal children are probably actually testing
the acquisition of production mechanisms (plan-
ning, motor skills, greater facility with lexical ac-
cess, etc) rather than the acquisition of filler-
gap. Note that these may be related since filler-
gap could introduce greater processing load which
could overwhelm the child?s fragile production ca-
pacity (Phillips, 2010).
Seidl et al (2003) showed that children are able
to process wh-extractions from subject position
(e.g. [who]
i
t
i
ate pie) as young as 15 months
while similar extractions from object position (e.g.
[what]
i
did the boy eat t
i
) remain unparseable until
around 20 months of age.
2
This line of investiga-
tion has been reopened and expanded by Gagliardi
et al (2014) whose results suggest that the ex-
perimental methodology employed by Seidl et al
(2003) was flawed in that it presumed infants have
ideal performance mechanisms. By providing more
trials of each condition and controlling for the prag-
matic felicity of test statements, Gagliardi et al
(2014) provide evidence that 15-month old infants
can process wh-extractions from both subject and
object positions. Object extractions are more diffi-
cult to comprehend than subject extractions, how-
ever, perhaps due to additional processing load in
object extractions (Gibson, 1998; Phillips, 2010).
Similarly, Gagliardi and Lidz (2010) show that rel-
ativized extractions with a wh-relativizer (e.g. find
[the boy]
i
who t
i
ate the apple) are easier to com-
prehend than relativized extractions with that as
the relativizer (e.g. find [the boy]
i
that t
i
ate the
apple).
Yuan et al (2012) demonstrate that 19-month
olds use their knowledge of nouns to learn both
verbs and their associated argument structure. In
2
Since the wh-phrase is in the same (or a very simi-
lar) position as the original subject when the wh-phrase
takes subject position, it is not clear that these con-
structions are true extractions (Culicover, 2013), how-
ever, this paper will continue to refer to them as such
for ease of exposition.
their study, infants were shown video of a person
talking on a phone using a nonce verb with ei-
ther one or two nouns (e.g. Mary kradded Susan).
Under the assumption that infants look longer at
things that correspond to their understanding of
a prompt, the infants were then shown two im-
ages that potentially depicted the described action
? one picture where two actors acted independently
(reflecting an intransitive proposition) and one pic-
ture where one actor acted on the other (reflecting
a transitive proposition).
3
Even though the infants
had no extralinguistic knowledge about the verb,
they consistently treated the verb as transitive if
two nouns were present and intransitive if only one
noun was present.
Similarly, Gertner and Fisher (2012) show that
intransitive phrases with conjoined subjects (e.g.
John and Mary gorped) are given a transitive in-
terpretation (i.e. John gorped Mary) at 21 months
(henceforth termed ?1-1 role bias?), though this ef-
fect is no longer present at 25 months (Naigles,
1990). This finding suggests both that learners
will ignore canonical structure in favor of using
all possible arguments and that children have a
bias to assign a unique semantic role to each argu-
ment. It is important to note, however, that cross-
linguistically children do not seem to generalize be-
yond two arguments until after at least 31 months
of age (Goldberg et al, 2004; Bello, 2012), so a
predicate occurring with three nouns would still
likely be interpreted as merely transitive rather
than ditransitive.
Computational modeling provides a way to test
the computational level of processing (Marr, 1982).
That is, given the input (child-directed speech,
adult-directed speech, and environmental experi-
ences), it is possible to probe the computational
processes that result in the observed output. How-
ever, previous computational models of grammar
induction (Klein and Manning, 2004), including in-
fant grammar induction (Kwiatkowski et al, 2012),
have not addressed filler-gap comprehension.
4
The closest work to that presented here is the
work on BabySRL (Connor et al, 2008; Connor et
al., 2009; Connor et al, 2010). BabySRL is a com-
putational model of semantic role acquistion using
a similar set of assumptions to the current work.
BabySRL learns weights over ordering constraints
(e.g. preverbal, second noun, etc.) to acquire se-
mantic role labelling while still exhibiting 1-1 role
bias. However, no analysis has evaluated the abil-
3
There were two actors in each image to avoid bias-
ing the infants to look at the image with more actors.
4
As one reviewer notes, Joshi et al (1990) and sub-
sequent work show that filler-gap phenomena can be
formally captured by mildly context-sensitive grammar
formalisms; these have the virtue of scaling up to adult
grammar, but due to their complexity, do not seem to
have been described as models of early acquisition.
1085
Susan said John gave girl book
-3 -2 -1 0 1 2
Table 1: An example of a chunked sentence (Su-
san said John gave the girl a red book) with the
sentence positions labelled. Nominal heads of noun
chunks are in bold.
ity of BabySRL to acquire filler-gap constructions.
Further comparison to BabySRL may be found in
Section 6.
3 Assumptions
The present work restricts itself to acquiring filler-
gap comprehension in English. The model pre-
sented here learns a single, non-recursive ordering
for the semantic roles in each sentence relative to
the verb since several studies have suggested that
early child grammars may consist of simple lin-
ear grammars that are dictated by semantic roles
(Diessel and Tomasello, 2001; Jackendoff and Wit-
tenberg, in press). This work assumes learners can
already identify nouns and verbs, which is sup-
ported by Shi et al (1999) who show that chil-
dren at an extremely young age can distinguish be-
tween content and function words and by Waxman
and Booth (2001) who show that children can dis-
tinguish between different types of content words.
Further, since Waxman and Booth (2001) demon-
strate that, by 14 months, children are able to dis-
tinguish nouns from modifiers, this work assumes
learners can already chunk nouns and access the
nominal head. To handle recursion, this work as-
sumes that children treat the final verb in each
sentence as the main verb (implicitly assuming sen-
tence segmentation), which ideally assigns roles to
each of the nouns in the sentence.
Due to the findings of Yuan et al (2012),
this work adopts a ?syntactic bootstrapping? the-
ory of acquisition (Gleitman, 1990), where struc-
tural properties (e.g. number of nouns) inform the
learner about semantic properties of a predicate
(e.g. how many semantic roles it confers). Since
infants infer the number of semantic roles, this
work further assumes they already have expecta-
tions about where these roles tend to be realized
in sentences, if they appear. These positions may
correspond to different semantic roles for different
predicates (e.g. the subject of run and of melt);
however, the role for predicates with a single argu-
ment is usually assigned to the noun that precedes
the verb while a second argument is usually as-
signed after the verb. The semantic properties of
these roles may be learned lexically for each pred-
icate, but that is beyond the scope of this work.
Therefore, this work uses syntactic and semantic
roles interchangeably (e.g. subject and agent).
? ? pi
G
SC
-1 0.5 .999
G
SN
-1 3 .001
G
OC
1 0.5 .999
G
ON
1 3 .001
? .00001
Table 2: Initial values for the mean (?), standard
deviation (?), and prior (pi) of each Gaussian as
well as the skip penalty (?) used in this paper.
Finally, following the finding by Gertner and
Fisher (2012) that children interpret intransitives
with conjoined subjects as transitives, this work as-
sumes that semantic roles have a one-to-one corre-
spondence with nouns in a sentence (similarly used
as a soft constraint in the semantic role labelling
work of Titov and Klementiev, 2012).
4 Model
The model represents the preferred locations of
semantic roles relative to the verb as distribu-
tions over real numbers. This idea is adapted from
Boersma (1997) who uses it to learn constraint
rankings in optimality theory.
In this work, the final (main) verb is placed at
position 0; words (and chunks) before the verb are
given progressively more negative positions, and
words after the verb are given progressively more
positive positions (see Table 1). Learner expecta-
tions of where an argument will appear relative
to the verb are modelled as two-component Gaus-
sian mixtures: one mixture of Gaussians (G
S?
) cor-
responds to the subject argument, another (G
O?
)
corresponds to the object argument. There is no
mixture for a third argument since children do not
generalize beyond two arguments until later in de-
velopment (Goldberg et al, 2004; Bello, 2012).
One component of each mixture learns to repre-
sent the canonical position for the argument (G
?C
)
while the other (G
?N
) represents some alternate,
non-canonical position such as the filler position
in filler-gap constructions. To reflect the fact that
learners have had 15 months of exposure to their
language before acquiring filler-gap, the mixture is
initialized so that there is a stronger probability
associated with the canonical Gaussian than with
the non-canonical Gaussian of each mixture.
5
Fi-
nally, the one-to-one role bias is explicitly encoded
such that the model cannot use a label that has
already been used elsewhere in the sentence.
5
Akhtar (1999) finds that learners may not have
strong expectations of canonical argument positions
until four years of age, but the results of the current
study are extremely robust to changes in initialization,
as discussed in Section 7 of this paper, so this assump-
tion is mostly adopted for ease of exposition.
1086
P
r
o
b
a
b
i
l
i
t
y
Position relative to verb
P
r
o
b
a
b
i
l
i
t
y
Position relative to verb
Figure 2: Visual representations of (Left) the initial model?s expectations of where arguments will appear,
given the initial parameters in Table 2 and (Right) the converged model?s expectations of where arguments
will appear.
Thus, the initial model conditions (see Figure 2)
are most likely to realize an SVO ordering, al-
though it is possible to obtain SOV (by sampling
a negative number from the blue curve) or even
OSV (by also sampling the red curve very close
to 0). The model is most likely to hypothesize a
preverbal object when it has already assigned the
subject role to something and, in addition, there is
no postverbal noun competing for the object label.
In other words, the model infers that an object ex-
traction may have occurred if there is a ?missing?
postverbal argument.
Finally, the probability of a given sequence is the
product of the label probabilities for the compo-
nent argument positions (e.g. G
SC
generating an
argument at position -2, etc). Since many sentences
have more than two nouns, the model is allowed to
skip nouns by multiplying a penalty term (?) into
the product for each skipped noun; the cost is set
at 0.00001 for this study, though see Section 7 for a
discussion of the constraints on this parameter. See
Table 2 for initialization parameters and Figure 2
for a visual representation of the initial expecta-
tions of the model.
This work uses a model with 2-component mix-
tures for both subjects and objects (termed the
symmetric model). This formulation achieves the
best fit to the training data according to the
Bayesian Information Criterion (BIC).
6
However,
follow-up experiments find that the non-canonical
subject Gaussian only improves the likelihood of
the data by erroneously modeling postverbal nouns
in imperative statements. The lack of a canonical
subject in English imperatives allows the model to
improve the likelihood of the data by using the
non-canonical subject Gaussian to capture ficti-
6
The BIC rewards improved log-likelihood but pe-
nalizes increased model complexity.
tious postverbal arguments. When imperatives are
filtered out of the training corpus, the symmetric
model obtains a worse BIC fit than a model that
lacks the non-canonical subject Gaussian. There-
fore, if one makes the assumption that impera-
tives are prosodically-marked for learners (e.g. the
learner is the implicit subject), the best model is
one that lacks a non-canonical subject.
7
The re-
mainder of this paper assumes a symmetric model
to demonstrate what happens if such an assump-
tion is not made; for the evaluations described in
this paper, the results are similar in either case.
This model differs from other non-recursive
computational models of grammar induction (e.g.
Goldwater and Griffiths, 2007) since it is not based
on Hidden Markov Models. Instead, it determines
the best ordering for the sentence as a whole. This
approach bears some similarity to a Generalized
Mallows model (Chen et al, 2009), but the current
formulation was chosen due to being independently
posited as cognitively plausible (Boersma, 1997).
Figure 2 (Right) shows the converged, final state
of the model. The model expects the first argu-
ment (usually agent) to be assigned preverbally
and expects the second (say, patient) to be assigned
postverbally; however, there is now a larger chance
that the second argument will appear preverbally.
5 Evaluation
The model in this work is trained using transcribed
child-directed speech (CDS) from the BabySRL
portions (Connor et al, 2008) of CHILDES
(MacWhinney, 2000). Chunking is performed us-
7
This finding suggests that a Dirichlet Process or
other means of dynamically determining the number
of components in each mixture would converge to a
model that lacks non-canonical subjects if imperative
filtering were employed.
1087
Eve (n = 4820) Adam (n = 4461)
P R F P R F
Initial .54 .64 .59 .53 .60 .56
Trained .52 .69 .59
?
.51 .65 .57
?
Initial
c
.56 .66 .60 .55 .62 .58
Trained
c
.54 .71 .61
?
.53 .67 .59
?
Table 3: Overall accuracy on the Eve and Adam
sections of the BabySRL corpus. Bottom rows re-
flect accuracy when non-agent roles are collapsed
into a single role. Note that improvements are nu-
merically slight since filler-gap is relatively rare
(Schuler, 2011).
?
p << .01
ing a basic noun-chunker from NLTK (Bird et al,
2009). Based on an initial analysis of chunker per-
formance, yes is hand-corrected to not be a noun.
Poor chunker perfomance is likely due to a mis-
match in chunker training and testing domains
(Wall Street Journal text vs transcribed speech),
but chunking noise may be a good estimation of
learner uncertainty, so the remaining text is left
uncorrected. All noun phrase chunks are then re-
placed with their final noun (presumed the head)
to approximate the ability of children to distin-
guish nouns from modifiers (Waxman and Booth,
2001). Finally, for each sentence, the model assigns
sentence positions to each word with the final verb
at zero.
Viterbi Expectation-Maximization is performed
over each sentence in the corpus to infer the pa-
rameters of the model. During the Expectation
step, the model uses the current Gaussian param-
eters to label the nouns in each sentence with ar-
gument roles. Since the model is not lexicalized,
these roles correspond to the semantic roles most
commonly associated with subject and object. The
model then chooses the best label sequence for each
sentence.
These newly labelled sentences are used during
the Maximization step to determine the Gaussian
parameters that maximize the likelihood of that
labelling. The mean of each Gaussian is updated
to the mean position of the words it labels. Sim-
ilarly, the standard deviation of each Gaussian is
updated with the standard deviation of the posi-
tions it labels. A learning rate of 0.3 is used to
prevent large parameter jumps. The prior proba-
bility of each Gaussian is updated as the ratio of
that Gaussian?s labellings to the total number of
labellings from that mixture in the corpus:
pi
??
=
| G
??
|
| G
??
|
(1)
where ? ? {S,O} and ? ? {C,N}.
Best results seem to be obtained when the skip-
penalty is loosened by an order of magnitude dur-
Subject Extraction filter: S x V . . .
Object Extraction filter: O . . . V . . .
Eve (n = 1345) Adam (n = 1287)
P R F P R F
Initial
c
.53 .57 .55 .53 .52 .52
Trained
c
.55 .67 .61
?
.54 .63 .58
?
Table 4: (Above) Filters to extract filler-gap con-
structions: A) the subject and verb are not ad-
jacent, B) the object precedes the verb. (Below)
Filler-gap accuracy on the Eve and Adam sections
of the BabySRL corpus when non-agent roles are
collapsed into a single role.
?
p << .01
ing testing. Essentially, this forces the model to
tightly adhere to the perceived argument struc-
ture during training to learn more rigid parame-
ters, but the model is allowed more leeway to skip
arguments it has less confidence in during testing.
Convergence (see Figure 2) tends to occur after
four iterations but can take up to ten iterations
depending on the initial parameters.
Since the model is unsupervised, it is trained on
a given corpus (e.g. Eve) before being tested on
the role annotations of that same corpus. The Eve
corpus was used for development purposes,
8
and
the Adam data was used only for testing.
For testing, this study uses the semantic role
annotations in the BabySRL corpus. These anno-
tations were obtained by automatically semantic
role labelling portions of CHILDES with the sys-
tem of Punyakanok et al (2008) before roughly
hand-correcting them (Connor et al, 2008). The
BabySRL corpus is annotated with 5 different
roles, but the model described in this paper only
uses 2 roles. Therefore, overall accuracy results (see
Table 3) are presented both for the raw BabySRL
corpus and for a collapsed BabySRL corpus where
all non-agent roles are collapsed into a single role
(denoted by a subscript
c
in all tables).
Since children do not generalize above two ar-
guments during the modelled age range (Goldberg
et al, 2004; Bello, 2012), the collapsed numbers
more closely reflect the performance of a learner
at this age than the raw numbers. The increase in
accuracy obtained from collapsing non-agent ar-
guments indicates that children may initially gen-
eralize incorrectly to some verbs and would need
to learn lexically-specific role assignments (e.g.
double-object constructions of give). Since the cur-
rent work is interested in general filler-gap com-
prehension at this age, including over unknown
verbs, the remaining analyses in this paper con-
8
This is included for transparency, though the ini-
tial parameters have very little bearing on the final re-
sults as stated in Section 7, so the danger of overfitting
to development data is very slight.
1088
P R F P R F
Eve Subj (n = 691) Obj (n = 654)
Initial
c
.66 .83 .74 .35 .31 .33
Trained
c
.64 .84 .72
?
.45 .52 .48
?
Adam Subj (n = 886) Obj (n = 1050)
Initial
c
.69 .81 .74 .33 .27 .30
Trained
c
.66 .81 .73 .44 .48 .46
?
P R F P R F
Eve Wh- (n = 689) That (n = 125)
Initial
c
.63 .45 .53 .43 .48 .45
Trained
c
.73 .75 .74
?
.44 .57 .50
?
Adam Wh- (n = 748) That (n = 189)
Initial
c
.50 .37 .42 .50 .50 .50
Trained
c
.61 .65 .63
?
.47 .56 .51
?
Table 5: (Left) Subject-extraction accuracy and object-extraction accuracy and (Right) Wh-relative ac-
curacy and that-relative accuracy; calculated over the Eve and Adam sections of the BabySRL corpus
with non-agent roles collapsed into a single role.
?
p = .02
?
p << .01
sider performance when non-agent arguments are
collapsed.
9
Next, a filler-gap version of the BabySRL cor-
pus is created using a coarse filtering process: the
new corpus is comprised of all sentences where an
associated object precedes the final verb and all
sentences where the relevant subject is not imme-
diately followed by the final verb (see Table 4). For
these filler-gap evaluations, the model is trained on
the full version of the corpus in question (e.g. Eve)
before being tested on the filler-gap subset of that
corpus. The overall results of the filler-gap evalua-
tion (see Table 4) indicate that the model improves
significantly at parsing filler-gap constructions af-
ter training.
The performance of the model on role-
assignment in filler-gap constructions may be
analyzed further in terms of how the model
performs on subject-extractions compared with
object-extractions and in terms of how the model
performs on that-relatives compared with wh-
relatives (see Table 5).
The model actually performs worse at subject-
extractions after training than before training.
This is unsurprising because, prior to training,
subjects have little-to-no competition for prever-
bal role assignments; after training, there is a pre-
verbal extracted object category, which the model
can erroneously use. This slight, though signifi-
cant in Eve, deficit is counter-balanced by a very
substantial and significant improvement in object-
extraction labelling accuracy.
Similarly, training confers a large and significant
improvement for role assignment in wh-relative
constructions, but it yields less of an improve-
ment for that-relative constructions. This differ-
ence mimics a finding observed in the developmen-
tal literature where children seem slower to ac-
quire comprehension of that-relatives than of wh-
relatives (Gagliardi and Lidz, 2010).
9
Though performance is slightly worse when argu-
ments are not collapsed, all the same patterns emerge.
6 Comparison to BabySRL
The acquisition of semantic role labelling (SRL) by
the BabySRL model (Connor et al, 2008; Connor
et al, 2009; Connor et al, 2010) bears many sim-
ilarities to the current work and is, to our knowl-
edge, the only comparable line of inquiry to the
current one. The primary function of BabySRL is
to model the acquisition of semantic role labelling
while making an idiosyncratic error which infants
also make (Gertner and Fisher, 2012), the 1-1 role
bias error (John and Mary gorped interpreted as
John gorped Mary). Similar to the model presented
in this paper, BabySRL is based on simple ordering
features such as argument position relative to the
verb and argument position relative to the other
arguments.
This section will demonstrate that the model in
this paper initially reflects 1-1 role bias comparably
to BabySRL, though it progresses beyond this bias
after training.
10
Further, the model in this paper is
able to reflect the concurrent acquisition of filler-
gap whereas BabySRL does not seem well-suited
to such a task. Finally, BabySRL performs unde-
sirably in intransitive settings whereas the model
in this paper does not.
Connor et al (2008) demonstrate that a super-
vised perceptron classifier, based on positional fea-
tures and trained on the silver role label annota-
tions of the BabySRL corpus, manifests 1-1 role
bias errors. Follow-up studies show that supervi-
sion may be lessened (Connor et al, 2009) or re-
moved (Connor et al, 2010) and BabySRL will still
reflect a substantial 1-1 role bias.
Connor et al (2008) and Connor et al (2009)
run direct analyses of how frequently their mod-
els make 1-1 role bias errors. A comparable eval-
uation may be run on the current model by
generating 1000 sentences with a structure of
NNV and reporting how many times the model
chooses a subject-first labelling (see Table 6).
11
10
All evaluations in this section are preceded by
training on the chunked Eve corpus.
11
While Table 6 analyzes erroneous labellings of
NNV structure, the ?Obj? column of Table 5 (Left)
1089
Error rate
Initial .36
Trained .11
Initial (given 2 args) .66
Trained (given 2 args) .13
2008 arg-arg position .65
2008 arg-verb position 0
2009 arg-arg position .82
2009 arg-verb position .63
Table 6: 1-1 role bias error in this model compared
to the models of Connor et al (2008) and Connor
et al (2009). That is, how frequently each model
labelled an NNV sentence SOV. Since the Connor
et al models are perceptron-based, they require
both arguments be labelled. The model presented
in this paper does not share this restriction, so the
raw error rate for this model is presented in the
first two lines; the error rate once this additional
restriction is imposed is given in the second two
lines.
The results of Connor et al (2008) and Connor
et al (2009) depend on whether BabySRL uses
argument-argument relative position as a feature
or argument-verb relative position as a feature
(there is no combined model). Further, the model
presented here from Connor et al (2009) has a
unique argument constraint, similar to the model
in this paper, in order to make comparison as di-
rect as possible.
The 1-1 role bias error rate (before training) of
the model presented in this paper is comparable
to that of Connor et al (2008) and Connor et al
(2009), which shows that the current model pro-
vides comparable developmental modeling benefits
to the BabySRL models. Further, similar to real
children (see Figure 1) the model presented in this
paper develops beyond this error by the end of its
training,
12
whereas the BabySRL models still make
this error after training.
Connor et al (2010) look at how frequently
their model correctly labels the agent in transitive
and intransitive sentences with unknown verbs (to
demonstrate that it exhibits an agent-first bias).
This evaluation can be replicated for the current
study by generating 1,000 sentences with the tran-
sitive form of NVN and a further 1,000 sentences
with the intransitive form of NV (see Table 7).
Since Connor et al (2010) investigate the effects
shows model accuracy on NNV structures.
12
It is important to note that the unique argument
constraint prevents the current model from actually
getting the correct, conjoined-subject parse, but it no
longer exhibits agent-first bias, an important step for
acquiring passives, which occurs between 3 and 4 years
(Thatcher et al, 2008).
NVN NV
Sents in Eve 1173 1513
Sents in Adam 1029 1353
Initial .67 1
Trained .65 .96
Weak (10) lexical .71 .59
Strong (365) lexical .74 .41
Gold Args .77 .58
Table 7: Agent-prediction recall accuracy in tran-
sitive (NVN) and intransitive (NV) settings of the
model presented in this paper (middle) and the
combined model of Connor et al (2010) (bottom),
which has features for argument-argument relative
position as well as argument-predicate relative po-
sition and so is closest to the model presented in
this paper.
of different initial lexicons, this evaluation com-
pares against the resulting BabySRL from each ini-
tializer: they initially seed their part-of-speech tag-
ger with either the 10 or 365 most frequent nouns
in the corpus or they dispense with the tagger and
use gold part-of-speech tags.
As with subject extraction, the model in this
paper gets less accurate after training because of
the newly minted extracted object category that
can be mistakenly used in these canonical settings.
While the model of Connor et al (2010) outper-
forms the model presented here when in a tran-
sitive setting, their model does much worse in an
intransitive setting. The difference in transitive set-
tings stems from increased lexicalization, as is ap-
parent from their results alone; the model pre-
sented here initially performs close to their weakly
lexicalized model, though training impedes agent-
prediction accuracy due to an increased probability
of non-canonical objects.
For the intransitive case, however, whereas the
model presented in this paper is generally able to
successfully label the lone noun as the subject, the
model of Connor et al (2010) chooses to label lone
nouns as objects about 40% of the time. This likely
stems from their model?s reliance on argument-
argument relative position as a feature; when there
is no additional argument to use for reference, the
model?s accuracy decreases. This is borne out by
their model (not shown in Table 7) that omits
the argument-argument relative position feature
and solely relies on verb-argument position, which
achieves up to 70% accuracy in intransitive set-
tings. Even in that case, however, BabySRL still
chooses to label lone nouns as objects 30% of the
time. The fact that intransitive sentences are more
common than transitive sentences in both the Eve
and Adam sections of the BabySRL corpus sug-
gests that learners should be more likely to assign
1090
correct roles in an intransitive setting, which is not
reflected in the BabySRL results.
The overall reason for the different results be-
tween the current work and BabySRL is that
BabySRL relies on positional features that mea-
sure the relative position of two individual ele-
ments (e.g. where a given noun is relative to the
verb). Since the model in this paper operates over
global orderings, it implicitly takes into account
the positions of other nouns as it models argument
position relative to the verb; object and subject
are in competition as labels for preverbal nouns,
so a preverbal object is usually only assigned once
a subject has already been detected.
Further, while BabySRL consistently reflects 1-
1 role bias (corresponding to a pre 25-month old
learner), it also learns to productively label five
roles, which developmental studies have shown
does not take place until at least 31 months (Gold-
berg et al, 2004; Bello, 2012). Finally, it does not
seem likely that BabySRL could be easily extended
to capture filler-gap acquisition. The argument-
verb position features impede acquisition of filler-
gap by classifying preverbal arguments as agents,
and the argument-argument position features in-
hibit accurate labelling in intransitive settings and
result in an agent-first bias which would tend to
label extracted objects as agents. In fact, these ob-
servations suggest that any linear classifier which
relies on positioning features will have difficulties
modeling filler-gap acquisition.
In sum, the unlexicalized model presented in this
paper is able to achieve greater labelling accuracy
than the lexicalized BabySRL models in intran-
sitive settings, though this model does perform
slightly worse in the less common transitive set-
ting. Further, the unsupervised model in this pa-
per initially reflects developmental 1-1 role bias as
well as the supervised BabySRL models, and it
is able to progress beyond this bias. Finally, un-
like BabySRL, the model presented here provides a
cognitive model of the acquisition of filler-gap com-
prehension, which BabySRL does not seem well-
suited to model.
7 Discussion
This paper has presented a simple cognitive model
of filler-gap acquisition, which is able to capture
several findings from developmental psychology.
Training significantly improves role labelling in
the case of object-extractions, which improves the
overall accuracy of the model. This boost is ac-
companied by a slight decrease in labelling ac-
curacy in subject-extraction settings. The asym-
metric ease of subject versus object comprehen-
sion is well-documented in both children and
adults (Gibson, 1998), and while training improves
the model?s ability to process object-extractions,
there is still a gap between object-extraction and
subject-extraction comprehension even after train-
ing.
Further, the model exhibits better comprehen-
sion of wh-relatives than that-relatives similar to
children (Gagliardi and Lidz, 2010). This could
also be an area where a lexicalized model could
do better. As Gagliardi and Lidz (2010) point
out, whereas wh-relatives such as who or which
always signify a filler-gap construction, that can
occur for many different reasons (demonstrative,
determiner, complementizer, etc) and so is a much
weaker filler-gap cue. A lexical model could poten-
tially pick up on clues which could indicate when
that is a relativizer or simply improve on its com-
prehension of wh-relatives even more.
It is interesting to note that the cuurent model
does not make use of that as a cue at all and
yet is still slower at acquiring that-relatives than
wh-relatives. This fact suggests that the findings
of Gagliardi and Lidz (2010) may be partially ex-
plained by a frequency effect: perhaps the input to
children is simply biased such that wh-relatives are
much more common than that-relatives (as shown
in Table 5).
This model also initially reflects the 1-1 role bias
observed in children (Gertner and Fisher, 2012) as
well as previous models (Connor et al, 2008; Con-
nor et al, 2009; Connor et al, 2010) without sac-
rificing accuracy in canonical intransitive settings.
Finally, this model is extremely robust to differ-
ent initializations. The canonical Gaussian expec-
tations can begin far from the verb (?3) or close
to the verb (?0.1), and the standard deviations
of the distributions and the skip-penalty can vary
widely; the model always converges to give compa-
rable results to those presented here. The only con-
straint on the initial parameters is that the proba-
bility of the extracted object occurring preverbally
must exceed the skip-penalty (i.e. extraction must
be possible). In short, this paper describes a sim-
ple, robust cognitive model of the development of
a learner between 15 months until somewhere be-
tween 25- and 30-months old (since 1-1 role bias is
no longer present but no more than two arguments
are being generalized).
In future, it would be interesting to incorporate
lexicalization into the model presented in this pa-
per, as this feature seems likely to bridge the gap
between this model and BabySRL in transitive set-
tings. Lexicalization should also help further dis-
tinguish modifiers from arguments and improve the
overall accuracy of the model.
It would also be interesting to investigate how
well this model generalizes to languages besides
English. Since the model is able to use the verb
position as a semi-permeable boundary between
canonical subjects and objects, it may not work as
1091
well in verb-final languages, and thus makes the
prediction that filler-gap comprehension may be
acquired later in development in such languages
due to a greater reliance on hierarchical syntax.
Ordering is one of the definining characteris-
tics of a language that must be acquired by learn-
ers (e.g. SVO vs SOV), and this work shows that
filler-gap comprehension can be acquired as a by-
product of learning orderings rather than having to
resort to higher-order syntax. Note that this model
cannot capture the constraints on filler-gap usage
which require a hierarchical grammar (e.g. subja-
cency), but such knowledge is really only needed
for successful production of filler-gap construc-
tions, which occurs much later (around 5 years;
de Villiers and Roeper, 1995). Further, the kind of
ordering system proposed in this paper may form
an initial basis for learning such grammars (Jack-
endoff and Wittenberg, in press).
8 Acknowledgements
Thanks to Peter Culicover, William Schuler, Laura
Wagner, and the attendees of the OSU 2013 Fall
Linguistics Colloquium Fest for feedback on this
work. This work was partially funded by an OSU
Dept. of Linguistics Targeted Investment for Ex-
cellence (TIE) grant for collaborative interdisci-
plinary projects conducted during the academic
year 2012-13.
References
Nameera Akhtar. 1999. Acquiring basic word or-
der: evidence for data-driven learning of syn-
tactic structure. Journal of Child Language,
26:339?356.
Sophia Bello. 2012. Identifying indirect objects
in French: An elicitation task. In Proceedings
of the 2012 annual conference of the Canadian
Linguistic Association.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python:
Analyzing Text with the Natural Language
Toolkit. O?Reilly, Beijing.
Paul Boersma. 1997. How we learn variation, op-
tionality, and probability. Proceedings of the In-
stitute of Phonetic Sciences of the University of
Amsterdam, 21:43?58.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using
latent permutations. Journal of Artificial Intel-
ligence Research, 36:129?163.
Michael Connor, Yael Gertner, Cynthia Fisher, and
Dan Roth. 2008. Baby srl: Modeling early lan-
guage acquisition. In Proceedings of the Twelfth
Conference on Computational Natural Language
Learning.
Michael Connor, Yael Gertner, Cynthia Fisher, and
Dan Roth. 2009. Minimally supervised model of
early language acquisition. In Proceedings of the
Thirteenth Conference on Computational Natu-
ral Language Learning.
Michael Connor, Yael Gertner, Cynthia Fisher, and
Dan Roth. 2010. Starting from scratch in se-
mantic role labelling. In Proceedings of ACL
2010.
Peter Culicover. 2013. Explaining syntax: repre-
sentations, structures, and computation. Oxford
University Press.
Jill de Villiers and Thomas Roeper. 1995. Bar-
riers, binding, and acquisition of the dp-np dis-
tinction. Language Acquisition, 4(1):73?104.
Holger Diessel and Michael Tomasello. 2001. The
acquisition of finite complement clauses in en-
glish: A corpus-based analysis. Cognitive Lin-
guistics, 12:1?45.
Annie Gagliardi and Jeffrey Lidz. 2010. Mor-
phosyntactic cues impact filler-gap dependency
resolution in 20- and 30-month-olds. In Poster
session of BUCLD35.
Annie Gagliardi, Tara M. Mease, and Jeffrey
Lidz. 2014. Discontinuous development
in the acquisition of filler-gap dependen-
cies: Evidence from 15- and 20-month-
olds. Harvard unpublished manuscript:
http://www.people.fas.harvard.edu/?gagliardi.
Yael Gertner and Cynthia Fisher. 2012. Predicted
errors in children?s early sentence comprehen-
sion. Cognition, 124:85?94.
Edward Gibson. 1998. Linguistic complexity:
Locality of syntactic dependencies. Cognition,
68(1):1?76.
Lila R. Gleitman. 1990. The structural sources of
verb meanings. Language Acquisition, 1:3?55.
Adele E. Goldberg, Devin Casenhiser, and Nitya
Sethuraman. 2004. Learning argument struc-
ture generalizations. Cognitive Linguistics,
14(3):289?316.
Sharon Goldwater and Tom Griffiths. 2007. A
fully Bayesian approach to unsupervised part-
of-speech tagging. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics.
Ray Jackendoff and Eva Wittenberg. in press.
What you can say without syntax: A hierarchy
of grammatical complexity. In Fritz Newmeyer
and Lauren Preston, editors, Measuring Linguis-
tic Complexity. Oxford University Press.
Aravind K. Joshi, K. Vijay Shanker, and David
Weir. 1990. The convergence of mildly context-
sensitive grammar formalisms. Technical Report
MS-CIS-90-01, Department of Computer and In-
formation Science, University of Pennsylvania.
1092
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure:
Models of dependency and constituency. In Pro-
ceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics.
Tom Kwiatkowski, Sharon Goldwater, Luke S.
Zettlemoyer, and Mark Steedman. 2012. A
probabilistic model of syntactic and semantic
acquisition from child-directed utterances and
their meanings. In Proceedings of EACL 2012.
Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk. Lawrence Elrbaum As-
sociates, Mahwah, NJ, third edition.
David Marr. 1982. Vision. A Computational In-
vestigation into the Human Representation and
Processing of Visual Information. W.H. Free-
man and Company.
Letitia R. Naigles. 1990. Children use syntax to
learn verb meanings. The Journal Child Lan-
guage, 17:357?374.
Colin Phillips. 2010. Some arguments and non-
arguments for reductionist accounts of syntactic
phenomena. Language and Cognitive Processes,
28:156?187.
Martin Pickering and Guy Barry. 1991. Sentence
processing without empty categories. Language
and Cognitive Processes, 6(3):229?259.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008. The importance of syntactic parsing and
inference in semantic role labeling. Computa-
tional Linguistics, 34(2):257?287.
John R. Ross. 1967. Constraints on Variables in
Syntax. Ph.D. thesis, Massachusetts Institute of
Technology.
William Schuler. 2011. Effects of filler-gap de-
pendencies on working memory requirements for
parsing. In Proceedings of COGSCI, pages 501?
506, Austin, TX. Cognitive Science Society.
Amanda Seidl, George Hollich, and Peter W.
Jusczyk. 2003. Early understanding of subject
and object wh-questions. Infancy, 4(3):423?436.
Rushen Shi, Janet F. Werker, and James L. Mor-
gan. 1999. Newborn infants? sensitivity to per-
ceptual cues to lexical and grammatical words.
Cognition, 72(2):B11?B21.
Katherine Thatcher, Holly Branigan, Janet
McLean, and Antonella Sorace. 2008. Chil-
dren?s early acquisition of the passive: Evidence
from syntactic priming. In Proceedings of the
Child Language Seminar 2007, pages 195?205,
University of Reading.
Ivan Titov and Alexandre Klementiev. 2012.
Crosslingual induction of semantic roles. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
2011).
Sandra R. Waxman and Amy E. Booth. 2001. See-
ing pink elephants: Fourteen-month-olds? inter-
pretations of novel nouns and adjectives. Cogni-
tive Psychology, 43:217?242.
Sylvia Yuan, Cynthia Fisher, and Jesse Snedeker.
2012. Counting the nouns: Simple structural
cues to verb meaning. Child Development,
83(4):1382?1399.
1093
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265?271,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
POS induction with distributional and morphological information
using a distance-dependent Chinese restaurant process
Kairit Sirts
Institute of Cybernetics at
Tallinn University of Technology
sirts@ioc.ee
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Micha Elsner
Department of Linguistics
The Ohio State University
melsner0@gmail.com
Sharon Goldwater
ILCC, School of Informatics
University of Edinburgh
sgwater@inf.ed.ac.uk
Abstract
We present a new approach to inducing the
syntactic categories of words, combining
their distributional and morphological prop-
erties in a joint nonparametric Bayesian
model based on the distance-dependent
Chinese Restaurant Process. The prior
distribution over word clusterings uses a
log-linear model of morphological similar-
ity; the likelihood function is the probabil-
ity of generating vector word embeddings.
The weights of the morphology model
are learned jointly while inducing part-of-
speech clusters, encouraging them to co-
here with the distributional features. The
resulting algorithm outperforms competi-
tive alternatives on English POS induction.
1 Introduction
The morphosyntactic function of words is reflected
in two ways: their distributional properties, and
their morphological structure. Each information
source has its own advantages and disadvantages.
Distributional similarity varies smoothly with syn-
tactic function, so that words with similar syntactic
functions should have similar distributional proper-
ties. In contrast, there can be multiple paradigms
for a single morphological inflection (such as past
tense in English). But accurate computation of
distributional similarity requires large amounts of
data, which may not be available for rare words;
morphological rules can be applied to any word
regardless of how often it appears.
These observations suggest that a general ap-
proach to the induction of syntactic categories
should leverage both distributional and morpho-
logical features (Clark, 2003; Christodoulopoulos
et al, 2010). But these features are difficult to
combine because of their disparate representations.
Distributional information is typically represented
in numerical vectors, and recent work has demon-
strated the utility of continuous vector represen-
tations, or ?embeddings? (Mikolov et al, 2013;
Luong et al, 2013; Kim and de Marneffe, 2013;
Turian et al, 2010). In contrast, morphology is
often represented in terms of sparse, discrete fea-
tures (such as morphemes), or via pairwise mea-
sures such as string edit distance. Moreover, the
mapping between a surface form and morphology
is complex and nonlinear, so that simple metrics
such as edit distance will only weakly approximate
morphological similarity.
In this paper we present a new approach for in-
ducing part-of-speech (POS) classes, combining
morphological and distributional information in a
non-parametric Bayesian generative model based
on the distance-dependent Chinese restaurant pro-
cess (ddCRP; Blei and Frazier, 2011). In the dd-
CRP, each data point (word type) selects another
point to ?follow?; this chain of following links
corresponds to a partition of the data points into
clusters. The probability of word w
1
following w
2
depends on two factors: 1) the distributional simi-
larity between all words in the proposed partition
containing w
1
and w
2
, which is encoded using a
Gaussian likelihood function over the word embed-
dings; and 2) the morphological similarity between
w
1
and w
2
, which acts as a prior distribution on the
induced clustering. We use a log-linear model to
capture suffix similarities between words, and learn
the feature weights by iterating between sampling
and weight learning.
We apply our model to the English section of
the the Multext-East corpus (Erjavec, 2004) in or-
der to evaluate both against the coarse-grained and
265
fine-grained tags, where the fine-grained tags en-
code detailed morphological classes. We find that
our model effectively combines morphological fea-
tures with distributional similarity, outperforming
comparable alternative approaches.
2 Related work
Unsupervised POS tagging has a long history in
NLP. This paper focuses on the POS induction
problem (i.e., no tag dictionary is available), and
here we limit our discussion to very recent sys-
tems. A review and comparison of older systems
is provided by Christodoulopoulos et al (2010),
who found that imposing a one-tag-per-word-type
constraint to reduce model flexibility tended to
improve system performance; like other recent
systems, we impose that constraint here. Recent
work also shows that the combination of morpho-
logical and distributional information yields the
best results, especially cross-linguistically (Clark,
2003; Berg-Kirkpatrick et al, 2010). Since then,
most systems have incorporated morphology in
some way, whether as an initial step to obtain pro-
totypes for clusters (Abend et al, 2010), or as
features in a generative model (Lee et al, 2010;
Christodoulopoulos et al, 2011; Sirts and Alum?ae,
2012), or a representation-learning algorithm (Yat-
baz et al, 2012). Several of these systems use a
small fixed set of orthographic and/or suffix fea-
tures, sometimes obtained from an unsupervised
morphological segmentation system (Abend et al,
2010; Lee et al, 2010; Christodoulopoulos et al,
2011; Yatbaz et al, 2012). Blunsom and Cohn?s
(2011) model learns an n-gram character model
over the words in each cluster; we learn a log-
linear model, which can incorporate arbitrary fea-
tures. Berg-Kirkpatrick et al (2010) also include
a log-linear model of morphology in POS induc-
tion, but they use morphology in the likelihood
term of a parametric sequence model, thereby en-
couraging all elements that share a tag to have the
same morphological features. In contrast, we use
pairwise morphological similarity as a prior in a
non-parametric clustering model. This means that
the membership of a word in a cluster requires only
morphological similarity to some other element in
the cluster, not to the cluster centroid; which may
be more appropriate for languages with multiple
morphological paradigms. Another difference is
that our non-parametric formulation makes it un-
necessary to know the number of tags in advance.
3 Distance-dependent CRP
The ddCRP (Blei and Frazier, 2011) is an extension
of the CRP; like the CRP, it defines a distribution
over partitions (?table assignments?) of data points
(?customers?). Whereas in the regular CRP each
customer chooses a table with probability propor-
tional to the number of customers already sitting
there, in the ddCRP each customer chooses another
customer to follow, and sits at the same table with
that customer. By identifying the connected compo-
nents in this graph, the ddCRP equivalently defines
a prior over clusterings.
If c
i
is the index of the customer followed by
customer i, then the ddCRP prior can be written
P (c
i
= j) ?
{
f(d
ij
) if i 6= j
? if i = j,
(1)
where d
ij
is the distance between customers i and j
and f is a decay function. A ddCRP is sequential if
customers can only follow previous customers, i.e.,
d
ij
=? when i > j and f(?) = 0. In this case,
if d
ij
= 1 for all i < j then the ddCRP reduces to
the CRP.
Separating the distance and decay function
makes sense for ?natural? distances (e.g., the num-
ber of words between word i and j in a document,
or the time between two events), but they can also
be collapsed into a single similarity function. We
wish to assign higher similarities to pairs of words
that share meaningful suffixes. Because we do not
know which suffixes are meaningful a priori, we
use a maximum entropy model whose features in-
clude all suffixes up to length three that are shared
by at least one pair of words. Our prior is then:
P (c
i
= j|w, ?) ?
{
e
w
T
g(i,j)
if i 6= j
? if i = j,
(2)
where g
s
(i, j) is 1 if suffix s is shared by ith and
jth words, and 0 otherwise.
We can create an infinite mixture model by com-
bining the ddCRP prior with a likelihood function
defining the probability of the data given the cluster
assignments. Since we are using continuous-valued
vectors (word embeddings) to represent the distri-
butional characteristics of words, we use a multi-
variate Gaussian likelihood. We will marginalize
over the mean ? and covariance ? of each clus-
ter, which in turn are drawn from Gaussian and
inverse-Wishart (IW) priors respectively:
? ? IW (?
0
,?
0
) ? ? N (?
0
,
?
/
?
0
) (3)
266
The full model is then:
P (X,c,?,?|?,w, ?) (4)
=
K
?
k=1
P (?
k
|?)p(?
k
|?
k
,?)
?
n
?
i=1
(P (c
i
|w, ?)P (x
i
|?
z
i
,?
z
i
)),
where ? are the hyperparameters for (?,?) and z
i
is the (implicit) cluster assignment of the ith word
x
i
. With a CRP prior, this model would be an infi-
nite Gaussian mixture model (IGMM; Rasmussen,
2000), and we will use the IGMM as a baseline.
4 Inference
The Gibbs sampler for the ddCRP integrates over
the Gaussian parameters, sampling only follower
variables. At each step, the follower link c
i
for a
single customer i is sampled, which can implicitly
shift the entire block of n customers fol(i) who fol-
low i into a new cluster. Since we marginalize over
the cluster parameters, computing P (c
i
= j) re-
quires computing the likelihood P (fol(i),X
j
|?),
where X
j
are the k customers already clustered
with j. However, if we do not merge fol(i)
with X
j
, then we have P (X
j
|?) in the overall
joint probability. Therefore, we can decompose
P (fol(i),X
j
|?) = P (fol(i)|X
j
,?)P (X
j
|?) and
need only compute the change in likelihood due to
merging in fol(i):
1
:
P (fol(i)|X
j
,?) = pi
?nd/2
?
d/2
k
|?
k
|
?
k
/2
?
d/2
n+k
|?
n+k
|
?
n+k
/2
?
d
?
i=1
?
(
?
n+k
+1?i
2
)
?
(
?
k
+1?i
2
)
, (5)
where the hyperparameters are updated as ?
n
=
?
0
+ n, ?
n
= ?
0
+ n, and
?
n
=
?
0
?
0
+ x?
?
0
+ n
(6)
?
n
= ?
0
+Q+ ?
0
?
0
?
0
T
? ?
n
?
n
?
T
n
, (7)
where Q =
?
n
i=1
x
i
x
T
i
.
Combining this likelihood term with the prior,
the probability of customer i following j is
P (c
i
= j|X
,
?,w, ?)
? P (fol(i)|X
j
,?)P (c
i
= j|w, ?). (8)
1
http://www.stats.ox.ac.uk/
?
teh/re-
search/notes/GaussianInverseWishart.pdf
Our non-sequential ddCRP introduces cycles
into the follower structure, which are handled in the
sampler as described by Socher et al (2011). Also,
the block of customers being moved around can po-
tentially be very large, which makes it easy for the
likelihood term to swamp the prior. In practice we
found that introducing an additional parameter a
(used to exponentiate the prior) improved results?
although we report results without this exponent as
well. This technique was also used by Titov and
Klementiev (2012) and Elsner et al (2012).
Inference also includes optimizing the feature
weights for the log-linear model in the ddCRP
prior (Titov and Klementiev, 2012). We interleave
L-BFGS optimization within sampling, as in Monte
Carlo Expectation-Maximization (Wei and Tanner,
1990). We do not apply the exponentiation parame-
ter a when training the weights because this proce-
dure affects the follower structure only, and we do
not have to worry about the magnitude of the like-
lihood. Before the first iteration we initialize the
follower structure: for each word, we choose ran-
domly a word to follow from amongst those with
the longest shared suffix of up to 3 characters. The
number of clusters starts around 750, but decreases
substantially after the first sampling iteration.
5 Experiments
Data For our experiments we used the English
word embeddings from the Polyglot project (Al-
Rfou? et al, 2013)
2
, which provides embeddings
trained on Wikipedia texts for 100,000 of the most
frequent words in many languages.
We evaluate on the English part of the Multext-
East (MTE) corpus (Erjavec, 2004), which provides
both coarse-grained and fine-grained POS labels
for the text of Orwell?s ?1984?. Coarse labels con-
sist of 11 main word classes, while the fine-grained
tags (104 for English) are sequences of detailed
morphological attributes. Some of these attributes
are not well-attested in English (e.g. gender) and
some are mostly distinguishable via semantic anal-
ysis (e.g. 1st and 2nd person verbs). Many tags are
assigned only to one or a few words. Scores for the
fine-grained tags will be lower for these reasons,
but we argue below that they are still informative.
Since Wikipedia and MTE are from different
domains their lexicons do not fully overlap; we
2
https://sites.google.com/site/rmyeid/
projects/polyglot
267
Wikipedia tokens 1843M
Multext-East tokens 118K
Multext-East types 9193
Multext-East & Wiki types 7540
Table 1: Statistics for the English Polyglot word embeddings
and English part of MTE: number of Wikipedia tokens used
to train the embeddings, number of tokens/types in MTE, and
number of types shared by both datasets.
take the intersection of these two sets for training
and evaluation. Table 1 shows corpus statistics.
Evaluation With a few exceptions (Biemann,
2006; Van Gael et al, 2009), POS induction sys-
tems normally require the user to specify the num-
ber of desired clusters, and the systems are evalu-
ated with that number set to the number of tags in
the gold standard. For corpora such as MTE with
both fine-grained and coarse-grained tages, pre-
vious evaluations have scored against the coarse-
grained tags. Though coarse-grained tags have
their place (Petrov et al, 2012), in many cases
the distributional and morphological distinctions
between words are more closely aligned with the
fine-grained tagsets, which typically distinguish
between verb tenses, noun number and gender,
and adjectival scale (comparative, superlative, etc.),
so we feel that the evaluation against fine-grained
tagset is more relevant here. For better comparison
with previous work, we also evaluate against the
coarse-grained tags; however, these numbers are
not strictly comparable to other scores reported on
MTE because we are only able to train and evalu-
ate on the subset of words that also have Polyglot
embeddings. To provide some measure of the dif-
ficulty of the task, we report baseline scores using
K-means clustering, which is relatively strong base-
line in this task (Christodoulopoulos et al, 2011).
There are several measures commonly used for
unsupervised POS induction. We report greedy
one-to-one mapping accuracy (1-1) (Haghighi and
Klein, 2006) and the information-theoretic score V-
measure (V-m), which also varies from 0 to 100%
(Rosenberg and Hirschberg, 2007). In previous
work it has been common to also report many-to-
one (m-1) mapping but this measure is particularly
sensitive to the number of induced clusters (more
clusters yield higher scores), which is variable for
our models. V-m can be somewhat sensitive to the
number of clusters (Reichart and Rappoport, 2009)
but much less so than m-1 (Christodoulopoulos
et al, 2010). With different number of induced
and gold standard clusters the 1-1 measure suffers
because some induced clusters cannot be mapped
to gold clusters or vice versa. However, almost half
the gold standard clusters in MTE contain just a
few words and we do not expect our model to be
able to learn them anyway, so the 1-1 measure is
still useful for telling us how well the model learns
the bigger and more distinguishable classes.
In unsupervised POS induction it is standard to
report accuracy on tokens even when the model it-
self works on types. Here we report also type-based
measures because these can reveal differences in
model behavior even when token-based measures
are similar.
Experimental setup For baselines we use K-
means and the IGMM, which both only learn from
the word embeddings. The CRP prior in the IGMM
has one hyperparameter (the concentration param-
eter ?); we report results for ? = 5 and 20. Both
the IGMM and ddCRP have four hyperparameters
controlling the prior over the Gaussian cluster pa-
rameters: ?
0
, ?
0
, ?
0
and ?
0
. We set the prior scale
matrix ?
0
by using the average covariance from
a K-means run with K = 200. When setting the
average covariance as the expected value of the IW
distribution the suitable scale matrix can be com-
puted as ?
0
= E [X] (?
0
? d? 1), where ?
0
is the
prior degrees of freedom (which we set to d + 10)
and d is the data dimensionality (64 for the Poly-
glot embeddings). We set the prior mean ?
0
equal
to the sample mean of the data and ?
0
to 0.01.
We experiment with three different priors for the
ddCRP model. All our ddCRP models are non-
sequential (Socher et al, 2011), allowing cycles
to be formed. The simplest model, ddCRP uni-
form, uses a uniform prior that sets the distance
between any two words equal to one.
3
The second
model, ddCRP learned, uses the log-linear prior
with weights learned between each two Gibbs iter-
ations as explained in section 4. The final model,
ddCRP exp, adds the prior exponentiation. The ?
parameter for the ddCRP is set to 1 in all experi-
ments. For ddCRP exp, we report results with the
exponent a set to 5.
Results and discussion Table 2 presents all re-
sults. Each number is an average of 5 experiments
3
In the sequential case this model would be equivalent to
the IGMM (Blei and Frazier, 2011). Due to the nonsequen-
tiality this equivalence does not hold, but we do expect to see
similar results to the IGMM.
268
Fine types Fine tokens Coarse tokens
Model K Model K-means Model K-means Model K-means
K-means 104 or 11 16.1 / 47.3 - 39.2 / 62.0 - 44.4 / 45.5 -
IGMM, ? = 5 55.6 41.0 / 45.9 23.1 / 49.5 48.0 / 64.8 37.2 / 61.0 48.3 / 58.3 40.8 / 55.0
IGMM, ? = 20 121.2 35.0 / 47.1 14.7 / 46.9 50.6 / 67.8 44.7 / 65.5 48.7 / 60.0 48.3 / 57.9
ddCRP uniform 80.4 50.5 / 52.9 18.6 / 48.2 52.4 / 68.7 35.1 / 60.3 52.1 / 62.2 40.3 / 54.2
ddCRP learned 89.6 50.1 / 55.1 17.6 / 48.0 51.1 / 69.7 39.0 / 63.2 48.9 / 62.0 41.1 / 55.1
ddCRP exp, a = 5 47.2 64.0 / 60.3 25.0 / 50.3 55.1 / 66.4 33.0 / 59.1 47.8 / 55.1 36.9 / 53.1
Table 2: Results of baseline and ddCRP models evaluated on word types and tokens using fine-grained tags, and on tokens
using coarse-grained tags. For each model we present the number of induced clusters K (or fixed K for K-means) and 1-1 / V-m
scores. The second column under each evaluation setting gives the scores for K-means with K equal to the number of clusters
induced by the model in that row.
with different random initializations. For each eval-
uation setting we provide two sets of scores?first
are the 1-1 and V-m scores for the given model,
second are the comparable scores for K-means run
with the same number of clusters as induced by the
non-parametric model.
These results show that all non-parametric mod-
els perform better than K-means, which is a strong
baseline in this task (Christodoulopoulos et al,
2011). The poor performace of K-means can be
explained by the fact that it tends to find clusters
of relatively equal size, although the POS clus-
ters are rarely of similar size. The common noun
singular class is by far the largest in English, con-
taining roughly a quarter of the word types. Non-
parametric models are able to produce cluster of
different sizes when the evidence indicates so, and
this is clearly the case here.
From the token-based evaluation it is hard to
say which IGMM hyperparameter value is better
even though the number of clusters induced differs
by a factor of 2. The type-base evaluation, how-
ever, clearly prefers the smaller value with fewer
clusters. Similar effects can be seen when com-
paring IGMM and ddCRP uniform. We expected
these two models perform on the same level, and
their token-based scores are similar, but on the type-
based evaluation the ddCRP is clearly superior. The
difference could be due to the non-sequentiality,
or becuase the samplers are different?IGMM en-
abling resampling only one item at a time, ddCRP
performing blocked sampling.
Further we can see that the ddCRP uniform and
learned perform roughly the same. Although the
prior in those models is different they work mainly
using the the likelihood. The ddCRP with learned
prior does produce nice follower structures within
each cluster but the prior is in general too weak
compared to the likelihood to influence the cluster-
ing decisions. Exponentiating the prior reduces the
number of induced clusters and improves results,
as it can change the cluster assignment for some
words where the likelihood strongly prefers one
cluster but the prior clearly indicates another.
The last column shows the token-based evalua-
tion against the coarse-grained tagset. This is the
most common evaluation framework used previ-
ously in the literature. Although our scores are not
directly comparable with the previous results, our
V-m scores are similar to the best published 60.5
(Christodoulopoulos et al, 2010) and 66.7 (Sirts
and Alum?ae, 2012).
In preliminary experiments, we found that di-
rectly applying the best-performing English model
to other languages is not effective. Different lan-
guages may require different parametrizations of
the model. Further study is also needed to verify
that word embeddings effectively capture syntax
across languages, and to determine the amount of
unlabeled text necessary to learn good embeddings.
6 Conclusion
This paper demonstrates that morphology and dis-
tributional features can be combined in a flexi-
ble, joint probabilistic model, using the distance-
dependent Chinese Restaurant Process. A key ad-
vantage of this framework is the ability to include
arbitrary features in the prior distribution. Future
work may exploit this advantage more thoroughly:
for example, by using features that incorporate
prior knowledge of the language?s morphological
structure. Another important goal is the evaluation
of this method on languages beyond English.
Acknowledgments: KS was supported by the
Tiger University program of the Estonian Infor-
mation Technology Foundation for Education. JE
was supported by a visiting fellowship from the
Scottish Informatics & Computer Science Alliance.
We thank the reviewers for their helpful feedback.
269
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2010.
Improved unsupervised pos induction through pro-
totype discovery. In Proceedings of the 48th An-
nual Meeting of the Association of Computational
Linguistics, pages 1298?1307.
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proceedings of the Thir-
teenth Annual Conference on Natural Language
Learning, pages 183?192, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre B. C?ot?e, John
DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proceedings of Hu-
man Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582?590.
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7?12.
David M Blei and Peter I Frazier. 2011. Distance
dependent chinese restaurant processes. Journal of
Machine Learning Research, 12:2461?2488.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part
of speech induction. In Proceedings of the 49th An-
nual Meeting of the Association of Computational
Linguistics, pages 865?874.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for part-of-speech induction using multiple features.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the European chapter of the
ACL.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association of Computational
Linguistics.
Toma?z Erjavec. 2004. MULTEXT-East version 3:
Multilingual morphosyntactic specifications, lexi-
cons and corpora. In LREC.
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Joo-Kyung Kim and Marie-Catherine de Marneffe.
2013. Deriving adjectival scales from continuous
space word representations. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. 2010. Simple type-level unsupervised pos tag-
ging. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
853?861.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representations
with recursive neural networks for morphology. In
Proceedings of the Thirteenth Annual Conference on
Natural Language Learning.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of Human
Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 746?751.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Carl Rasmussen. 2000. The infinite Gaussian mixture
model. In Advances in Neural Information Process-
ing Systems 12, Cambridge, MA. MIT Press.
Roi Reichart and Ari Rappoport. 2009. The nvi cluster-
ing evaluation measure. In Proceedings of the Ninth
Annual Conference on Natural Language Learning,
pages 165?173.
A. Rosenberg and J. Hirschberg. 2007. V-measure:
A conditional entropy-based external cluster evalua-
tion measure. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 410?42.
Kairit Sirts and Tanel Alum?ae. 2012. A hierarchi-
cal Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of Hu-
man Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 407?416.
Richard Socher, Andrew L Maas, and Christopher D
Manning. 2011. Spectral chinese restaurant pro-
cesses: Nonparametric clustering based on similar-
ities. In Proceedings of the Fifteenth International
Conference on Artificial Intelligence and Statistics,
pages 698?706.
270
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 678?687, Singapore.
Greg CG Wei and Martin A Tanner. 1990. A
monte carlo implementation of the em algorithm
and the poor man?s data augmentation algorithms.
Journal of the American Statistical Association,
85(411):699?704.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 940?951.
271
Workshop on Monolingual Text-To-Text Generation, pages 54?63,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 54?63,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Learning to Fuse Disparate Sentences
Micha Elsner
School of Informatics
University of Edinburgh
melsner0@gmail.com
Deepak Santhanam
Brown Lab for
Linguistic Information Processing (BLLIP)
Department of Computer Science
Brown University, Providence, RI 02912
dsanthan@microsoft.com
Abstract
We present a system for fusing sentences
which are drawn from the same source docu-
ment but have different content. Unlike previ-
ous work, our approach is supervised, training
on real-world examples of sentences fused by
professional journalists in the process of edit-
ing news articles. Like Filippova and Strube
(2008), our system merges dependency graphs
using Integer Linear Programming. However,
instead of aligning the inputs as a preprocess,
we integrate the tasks of finding an alignment
and selecting a merged sentence into a joint
optimization problem, and learn parameters
for this optimization using a structured online
algorithm. Evaluation by human judges shows
that our technique produces fused sentences
that are both informative and readable.
1 Introduction
Sentence fusion is the process by which content from
two or more original sentences is transformed into a
single output sentence. It is usually studied in the
context of multidocument summarization, since fus-
ing similar sentences can avoid repetition of material
which is shared by more than one input. However,
human editors and summarizers do not restrict them-
selves to combining sentences which share most of
their content. This paper extends previous work on
fusion to the case in which the input sentences are
drawn from the same document and express funda-
mentally different content, while still remaining re-
lated enough to make fusion sensible1.
1Unfortunately, we cannot release our corpus due to li-
censing agreements. Our system is available at https://
Our data comes from a corpus of news articles for
which we have un-edited and edited versions. We
search this corpus for sentences which were fused
(or separated) by the editor; these constitute natu-
rally occurring data for our system. One example
from our dataset consists of input sentences (1) and
(2) and output (3). We show corresponding regions
of the input and output in boldface.
(1) The bodies showed signs of torture.
(2) They were left on the side of a highway in
Chilpancingo, about an hour north of the
tourist resort of Acapulco in the southern
state of Guerrero, state police said.
(3) The bodies of the men, which showed signs
of torture, were left on the side of a highway
in Chilpancingo, which is about an hour
north of the tourist resort of Acapulco, state
police told Reuters.
While the two original sentences are linked by a
common topic and reference to a shared entity, they
are not paraphrases of one another. This could cre-
ate a problem for traditional fusion systems which
first find an alignment between similar dependency
graphs, then extract a shared structure. While our
system has the same basic framework of alignment
and extraction, it performs the two jointly, as parts
of a global optimization task. This makes it robust
to uncertainty about the hidden correspondences be-
tween the sentences. We use structured online learn-
ing to find parameters for the system, allowing it to
bitbucket.org/melsner/sentencefusion.
54
discover good ways to piece together input sentences
by examining examples from our corpus.
Sentence fusion is a common strategy in human-
authored summaries of single documents? 36% of
sentences in the summaries investigated by Jing
and McKeown (1999) contain content from multiple
sentences in the original document. This suggests
that a method to fuse dissimilar sentences could
be useful for single-document summarization. Our
dataset is evidence that editing also involves fusing
sentences, and thus that models of this task could
contribute to systems for automatic editing.
In the remainder of the paper, we first give an
overview of related work (Section 2). We next de-
scribe our dataset and preprocessing in more detail
(Section 3), describe the optimization we perform
(Section 4), and explain how we learn parameters for
it (Section 5). Finally, we discuss our experimental
evaluation and give results (Section 6).
2 Related work
Previous work on sentence fusion examines the task
in the context of multidocument summarization, tar-
geting groups of sentences with mostly redundant
content. The pioneering work on fusion is Barzilay
and McKeown (2005), which introduces the frame-
work used by subsequent projects: they represent
the inputs by dependency trees, align some words to
merge the input trees into a lattice, and then extract
a single, connected dependency tree as the output.
Our work most closely follows Filippova and
Strube (2008), which proposes using Integer Lin-
ear Programming (ILP) for extraction of an output
dependency tree. ILP allows specification of gram-
maticality constraints in terms of dependency rela-
tionships (Clarke and Lapata, 2008), as opposed to
previous fusion methods (Barzilay and McKeown,
2005; Marsi and Krahmer, 2005) which used lan-
guage modeling to extract their output.
In their ILP, Filippova and Strube (2008) optimize
a function based on syntactic importance scores
learned from a corpus of general text. While similar
methods have been used for the related task of sen-
tence compression, improvements can be obtained
using supervised learning (Knight and Marcu, 2000;
Turner and Charniak, 2005; Cohn and Lapata, 2009)
if a suitable corpus of compressed sentences can be
obtained. This paper is the first we know of to adopt
the supervised strategy for sentence fusion.
For supervised learning to be effective, it is nec-
essary to find or produce example data. Previous
work does produce some examples written by hu-
mans, though these are used during evaluation, not
for learning (a large corpus of fusions (McKeown et
al., 2010) was recently compiled as a first step to-
ward a supervised fusion system). However, they
elicit these examples by asking experimental sub-
jects to fuse selected input sentences? the choice
of which sentences to fuse is made by the system,
not the subjects. In contrast, our dataset consists of
sentences humans actually chose to fuse as part of a
practical writing task. Moreover, our sentences have
disparate content, while previous work focuses on
sentences whose content mostly overlaps.
Input sentences with differing content present a
challenge to the models used in previous work.
All these models use deterministic node alignment
heuristics to merge the input dependency graphs.
Filippova and Strube (2008) align all content words
with the same lemma and part of speech; Barzi-
lay and McKeown (2005) and Marsi and Krahmer
(2005) use syntactic methods based on tree simi-
larity. Neither method is likely to work well for
our data. Lexical methods over-align, since there
are many potential points of correspondence be-
tween our sentences, only some of which should
be merged? ?the Doha trade round? and ?U.S. trade
representative? share a word, but probably ought to
remain separate regardless. Syntactic methods, on
the other hand, are unlikely to find any alignments
since the input sentences are not paraphrases and
have very different trees. Our system selects the set
of nodes to merge during ILP optimization, allowing
it to choose correspondences that lead to a sensible
overall solution.
3 Data and preprocessing
Our sentence fusion examples are drawn from a cor-
pus of 516 pre- and post-editing articles from the
Thomson-Reuters newswire, collected over a period
of three months in 2008. We use a simple greedy
method based on bigram count overlaps to align the
sentences of each original article to sentences in the
edited version, allowing us to find fused sentences.
55
Since these sentences are relatively rare, we use both
merges (where the editor fused two input sentences)
and splits (where the editor splits an input sentence
into multiple outputs) as examples for our system.
In the case of a split, we take the edited sentences
as input for our method and attempt to produce the
original through fusion2. This is suboptimal, since
the editor?s decision to split the sentences probably
means the fused version is too long, but is required
in this small dataset to avoid sparsity.
Out of a total of 9007 sentences in the corpus,
our bigram method finds that 175 were split and 132
were merged, for a total of 307. We take 92 exam-
ples for testing and 189 for training3.
Following previous work (Barzilay and McKe-
own, 2005), we adopt a labeled dependency format
for our system?s input. To produce this, we segment
sentences with MXTerminator (Reynar and Ratna-
parkhi, 1997) and parse the corpus with the self-
trained Charniak parser (McClosky et al, 2006). We
then convert to dependencies and apply rules to sim-
plify and label the graph. An example dependency
graph is shown in Figure 1.
We augment the dependency tree by adding a
potential dependency labeled ?relative clause? be-
tween each subject and its verb. This allows our
system to transform main clauses, like ?the bodies
showed signs of torture?, into NPs like ?the bod-
ies, which showed signs of torture?, a common para-
phrase strategy in our dataset.
We also add correspondences between the two
sentences to the graph, marking nodes which the
system might decide to merge while fusing the two
sentences. We introduce correspondence arcs be-
tween pairs of probable synonyms4. We also anno-
tate pronoun coreference by hand and create a cor-
respondence between each pronoun and the heads of
all coreferent NPs. The example sentence has only a
single correspondence arc (?they? and ?bodies?) be-
2In a few cases, this creates two examples which share a
sentence, since the editor sometimes splits content off from one
sentence and merges it into another.
3We originally had 100 testing and 207 training examples,
but found 26 of our examples were spurious, caused by faulty
sentence segmentation.
4Words with the same part of speech whose similarity is
greater than 3.0 according to the information-theoretic Word-
Net based similarity measure of Resnik (1995), using the im-
plementation of (Pedersen et al, 2004).
cause input sentence (1) is extremely short, but most
sentences have more.
bodies showed
signs torture
said
left
were
they
side highway chilpancingo
policestate
north hour resort acapulco
root
root
rel
sbj
obj
pp of
rel
sbj
pp by
pp of pp in
pp about
pp of pp of
thean
aux
obj
sbj
rel
merge?
Figure 1: The labeled dependency graph for sentences (1)
and (2). Dashed lines show a correspondence arc (?bod-
ies? and ?they?) and potential relative clauses between
subjects and VPs.
3.1 Retained information
Sentence fusion can be thought of as a two-part
process: first, the editor decides which information
from the input sentences to retain, and then they gen-
erate a sentence incorporating it. In this paper, we
focus on the generation stage. To avoid having to
perform content selection5 , we provide our system
with the true information selected by the editor. To
do this, we align the input sentences with the output
by repeatedly finding the longest common substring
(LCS) until a substring containing a matching con-
tent word can no longer be found. (The LCS is com-
puted by a dynamic program similar to that for edit
distance, but unlike edit distance, repeated LCS can
handle reordering.) We provide our system with the
boundaries of the retained regions as part of the in-
put. For the example above, these are the regions
of sentences (1) and (2) marked in boldface. Al-
though this helps the system select the correct infor-
mation, generating a grammatical and easy-to-read
fused sentence is still non-trivial (see examples in
section 7).
4 Fusion via optimization
Like Filippova and Strube (2008), we model our
fusion task as a constrained optimization problem,
which we solve using Integer Linear Programming
(ILP). For each dependency from word w to head
5As pointed out by Daume III and Marcu (2004) and Krah-
mer et al (2008), content selection is not only difficult, but also
somewhat ill-defined without discourse context information.
56
h in the input sentences, we have a binary variable
xh,w, which is 1 if the dependency is retained in the
output and 0 otherwise. However, unlike Filippova
and Strube (2008), we do not know the points of cor-
respondence between the inputs, only a set of possi-
ble points. Therefore, we also introduce 0-1 integer
variables ms,t for each correspondence arc, which
indicate whether word s in one sentence should be
merged with word t in another. If the words are
merged, they form a link between the two sentences,
and only one of the pair appears in the output.
Each dependency x, each word w, and each
merger m have an associated weight value v, which
is assigned based on its features and the learned pa-
rameters of our system (explained in Section 5). Our
objective function (4) sums these weight values for
the structures we retain:
max
?
h,w
vh,w ? vw ? xh,w +
?
s,t
vs,t ?ms,t (4)
We use structural constraints to require the output
to form a single connected tree. (In the following
equations, W denotes the set of words, X denotes
the set of dependencies and M denotes the poten-
tial correspondence pairs.) Constraint (5) requires a
word to have at most one parent and (6) allows it to
be merged with at most one other word. (7) and (8)
require each merged node to have a single parent:
?w ? W,
?
h
xh,w ? 1 (5)
?w ? W,
?
t
ms,t ? 1 (6)
?s, t ? M, ms,t ?
?
h
xh,s +
?
h
xh,t (7)
?s, t ? M, ms,t +
?
h
xh,s +
?
h
xh,t ? 2 (8)
(9) forces the output to be connected by ensuring
that if a node has children, it either has a parent or is
merged.
?w ? W,
?
c
xc,w?
|W |
?
h
xh,w ? |W |
?
u
mu,w ? 0
(9)
Certain choices of nodes to merge or dependen-
cies to follow can create a cycle, so we also intro-
duce a rank variable rw ? R for each word and con-
strain each word (except the root) to have a higher
rank than its parent (10). Merged nodes must have
equal ranks (11).
?w,h ? X,|X|xh,w + rh ? rw ? |X| ? 1 (10)
?s,t ? M,|X|ms,t + rs ? rt ? |X| (11)
We also apply syntactic constraints to make sure
we supply all the required arguments for each word
we select. We hand-write rules to prevent the sys-
tem from pruning determiners, auxiliary verbs, sub-
jects, objects, verbal particles and the word ?not?
unless their head word is also pruned or it can find
a replacement argument of the same type. We learn
probabilities for prepositional and subclause argu-
ments using the estimation method described in Fil-
ippova and Strube (2008), which counts how often
the argument appears with the head word in a large
corpus. While they use these probabilities in the ob-
jective function, we threshold them and supply con-
straints to make sure all argument types with proba-
bility > 10% appear if the head is chosen.
Word merging makes it more difficult to write
constraints for required arguments, because a word
s might be merged with some other word t which is
attached to the correct argument type (for instance, if
s and t are both verbs and they are merged, only one
of them must be attached to a subject). This condi-
tion is modeled by the expression ms,t ?xt,a, where a
is a argument word of the appropriate type. This ex-
pression is non-linear and cannot appear directly in
a constraint, but we can introduce an auxiliary vari-
able gs,t,A which summarizes it for a set of poten-
tial arguments A, while retaining a polynomial-sized
program:
?s,t ? M,
?
a?A
xa,s+
?
a?A
xa,t + |W |ms,t ? |W + 1|gs,t,A ? 0
(12)
(13) then requires a word s to be connected to an
argument in set A, either via a link or directly:
57
?h
xs,h ? 2
?
t:{s,t?M}
gs,t,A ? 2
?
a?A
xa,s ? 0 (13)
The resulting resulting ILP is usually solvable
within a second using CPLEX (Ilog, Inc., 2003).
4.1 Linearization
The output of the ILP is a dependency tree, not an
ordered sentence. We determine the final ordering
mostly according to the original word order of the
input. In the case of a merged node, however, we
must also interleave modifiers of the merged heads,
which are not ordered with respect to one another.
We use a simple heuristic, trying to place dependen-
cies with the same arc label next to one another; this
can cause errors. We must also introduce conjunc-
tions between arguments of the same syntactic type;
our system always inserts ?and?. Finally, we choose
a realization for the dummy relative pronoun THAT
using a trigram language model (Stolcke, 2002). A
more sophisticated approach (Filippova and Strube,
2009) might lead to better results.
5 Learning
The solution which the system finds depends on the
weights v which we provide for each dependency,
word and merger. We set the weights based on a dot
product of features ? and parameters ?, which we
learn from data using a supervised structured tech-
nique (Collins, 2002). To do so, we define a loss
function L(s, s?) ? R which measures how poor
solution s is when the true solution is s?. For each of
our training examples, we compute the oracle so-
lution, the best solution accessible to our system,
by minimizing the loss. Finally, we use the struc-
tured averaged perceptron update rule to push our
system?s parameters away from bad solutions and
towards the oracle solutions for each example.
Our loss function is designed to measure the high-
level similarity between two dependency trees con-
taining some aligned regions. (For our system, these
are the regions found by LCS alignment of the in-
put strings with the output.) For two sentences to be
similar, they should have similar links between the
regions. Specifically, we define the paths P (s,C) in
a tree s with a set of regions C as the set of word
pairs w,w? where w is in one region, w? is in an-
other, and the dependency path between w and w?
lies entirely outside all the regions. An example is
given in figure 2.
left on the side of a highway...were
bodies showedof the men, which signs of torture
state police told Reuters root
Figure 2: Paths between retained regions in sentence (3).
Boxes indicate the retained regions.
Our loss (equation 14) is defined as the number of
paths in s and s? which do not match, plus a penalty
K1 for keeping extra words, minus a bonus K2 for
retaining words inside aligned regions:
L(s,s?;C,K) =
|(P (s,C) ? P (s?, C)) \ (P (s,C) ? P (s?, C))|
+K1|w ? s \ C| ?K2|w ? s ? C|
(14)
To compute the oracle s?, we must minimize this
loss function with respect to the human-authored
reference sentence r over the space S of fused de-
pendency trees our system can produce.
s? = argmins?S L(s, r) (15)
We perform the minimization by again using ILP,
keeping the constraints from the original program
but setting the objective to minimize the loss. This
cannot be done directly, since the existence of a path
from s to t must be modeled as a product of x vari-
ables for the dependencies forming the path. How-
ever, we can again introduce a polynomial number
of auxiliary variables to solve the problem. We in-
troduce a 0-1 variable qsh,w for each path start word
s and dependency h,w, indicating whether the de-
pendency from h to w is retained and forms part of
a path from s. Likewise, we create variables qsw for
each word and qsu,v for mergers6. Using these vari-
ables, we can state the loss function linearly as (16),
6The q variables are constrained to have the appropriate val-
ues in the same way as (12) constrains g. We will print the
specific equations in a technical report when this work is pub-
lished.
58
where P (r, C) is the set of paths extracted from the
reference solution.
min
?
s,t
qsh,t ? 2
?
s,t?P (r,C)
qsh,t (16)
The oracle fused sentence for the example (1) and
(2) is (17). The reference has a path from bodies
to showed, so the oracle includes one as well. To
do so, follows a relative clause arc, which was not
in the original dependency tree but was created as
an alternative by our syntactic analysis. (At this
stage of processing, we show the dummy relative
pronoun as THAT.) It creates a path from left to bod-
ies by choosing to merge the pronoun they with its
antecedent. Other options, such as linking the two
original sentences with ?and?, are penalized because
they would create erroneous paths? since there is
no direct path between root and showed, the oracle
should not make showed the head of its own clause.
(17) the bodies THAT showed signs of torture were
left on the side of a highway in Chilpancingo
about an hour north of the tourist resort of
Acapulco state police said
The features which represent each merger, word
and dependency are listed in Table 1. We use the first
letters of POS tags (in the Penn Treebank encoding)
to capture coarse groupings such as all nouns and all
verbs. For mergers, we use two measures of seman-
tic similarity, one based on Roget?s Thesaurus (Jar-
masz and Szpakowicz, 2003) and another based on
WordNet (Resnik, 1995). As previously stated, we
hand-annotate the corpus with true pronoun corefer-
ence relationships (about 30% of sentences contain
a coreferent pronoun). Finally, we provide the LCS
retained region boundaries as explained above.
Once we have defined the feature representation
and the loss function, and can calculate the oracle
for each datapoint, we can easily apply any struc-
tured online learning algorithm to optimize the pa-
rameters. We adopt the averaged perceptron, applied
to structured learning by (Collins, 2002). For each
example, we extract a current solution st by solving
the ILP (with weights v dependent on our parame-
ters ?), then perform an update to ? which forces
the system away from st and towards the oracle so-
lution s?. The update at each timestep t (18) de-
pends on the loss, the global feature vectors ?, and
COMPONENT FEATURES
MERGER SAME WORD
SAME POS TAGS
SAME FIRST LETTER OF THE POS TAGS
POS TAG IF WORD IS SAME
COREFERENT PRONOUN
SAME DEPENDENCY ARC LABEL TO PARENT
ROGET?S SIMILARITY
WORDNET SIMILARITY
FIRST LETTER OF BOTH POS TAGS
WORD POS TAG AND ITS FIRST LETTER
WORD IS PART OF RETAINED CHUNK IN EDITOR?S FUSION
DEPENDENCY POS TAGS OF THE PARENT AND CHILD
FIRST LETTER OF THE POS TAGS
TYPE OF THE DEPENDENCY
DEPENDENCY IS AN INSERTED RELATIVE CLAUSE ARC
PARENT IS RETAINED IN EDITOR?S SENTENCE
CHILD IS RETAINED IN EDITOR?S SENTENCE
Table 1: List of Features.
a learning rate parameter ?. (Note that the update
leaves the parameters unchanged if the loss relative
to the oracle is 0, or if the two solutions cannot be
distinguished in terms of their feature vectors.)
?t+1 = ?t + ?(L(st, r)?L(s?, r))(?(s?)??(st))
(18)
We do 100 passes over the training data, with ?
decaying exponentially toward 0. At the end of each
pass over the data, we set ?? to the average of all the
?t for that pass (Freund and Schapire, 1999). Fi-
nally, at the end of training, we select the committee
of 10 ?? which achieved lowest overall loss and av-
erage them to derive our final weights (Elsas et al,
2008). Since the loss function is nonsmooth, loss
does not decrease on every pass, but it declines over-
all as the algorithm proceeds.
6 Evaluation
Evaluating sentence fusion is a notoriously difficult
task (Filippova and Strube, 2008; Daume III and
Marcu, 2004) with no accepted quantitative metrics,
so we have to depend on human judges for evalu-
ation. We compare sentences produced by our sys-
tem to three alternatives: the editor?s fused sentence,
a readability upper-bound and a baseline formed by
splicing the input sentences together by inserting the
word ?and? between each one. The readability upper
59
bound is the output of parsing and linearization on
the editor?s original sentence (Filippova and Strube,
2008); it is designed to measure the loss in gram-
maticality due to our preprocessing.
Native English speakers rated the fused sentences
with respect to readability and content on a scale of
1 to 5 (we give a scoring rubric based on (Nomoto,
2009)). 12 judges participated in the study, for a
total of 1062 evaluations7 . Each judge saw the each
pair of inputs with the retained regions boldfaced,
plus a single fusion drawn randomly from among the
four systems. Results are displayed in Table 2.
System Readability Content
Editor 4.55 4.56
Readability UB 3.97 4.27
?And?-splice 3.65 3.80
Our System 3.12 3.83
Table 2: Results of human evaluation.
7 Discussion
Readability scores indicate that the judges prefer
human-authored sentences, then the readability up-
per bound, then ?and?-splicing and finally our sys-
tem. This ordering is unsuprising considering that
our system is abstractive and can make grammatical
errors, while the remaining systems are all based on
grammatical human-authored text. The gap of .58
between human sentences and the readability upper
bound represents loss due to poor linearization; this
accounts for over half the gap between our system
and human performance.
For content, the human-authored sentences
slightly outperform the readability upper bound?
this indicates that poor linearization has some ef-
fect on content as well as readability. Our system is
slightly better than ?and?-splicing. The distribution
of scores is shown in Table 3. The system gets more
scores of 5 (perfect), but it occasionally fails drasti-
cally and receives a very low score; ?and?-splicing
shows less variance.
Both metrics show that, while our system does not
achieve human performance, it does not lag behind
7One judge completed only the first 50 evaluations; the rest
did all 92.
1 2 3 4 5 Total
?And?-splice 3 43 60 57 103 266
System 24 24 39 58 115 260
Table 3: Number of times each Content score was as-
signed by human judges.
by that much. It performs quite well on some rel-
atively hard sentences and gets easy fusions right
most of the time. For instance, the output on our
example sentence is (19), matching the oracle (17).
(19) The bodies who showed signs of torture were
left on the side of a highway in Chilpancingo
about an hour north of the tourist resort of
Acapulco state police said.
In some cases, the system output corresponds
to the ?and?-splice baseline, but in many cases,
the ?and?-splice baseline adds extraneous content.
While the average length of a human-authored fu-
sion is 34 words, the average splice is 49 words long.
Plainly, editors often prefer to produce compact fu-
sions rather than splices. Our own system?s out-
put has an average length of 33 words per sentence,
showing that it has properly learned to trim away ex-
traneous information from the input. We instructed
participants to penalize the content score when fused
sentences lost important information or added extra
details.
Our integration of node alignment into our solu-
tion procedure helps the system to find good corre-
spondences between the inputs. For inputs (20) and
(21), the system was allowed to match ?company?
to ?unit?, but could also match ?terrorism? to ?ad-
ministration? or to ?lawsuit?. Our system correctly
merges ?company? and ?unit?, but not the other two
pairs, to form our output (22); the editor makes the
same decision in their fused sentence (23).
(20) The suit claims the company helped fly
terrorism suspects abroad to secret prisons.
(21) Holder?s review was disclosed the same day
as Justice Department lawyers repeated a
Bush administration state-secret claim in a
lawsuit against a Boeing Co unit.
60
(22) Review was disclosed the same day as Justice
Department lawyers repeated a Bush
administration claim in a lawsuit against a
Boeing Co unit that helped fly terrorism
suspects abroad to secret prisons.
(23) The review was disclosed the same day that
Justice Department lawyers repeated Bush
administration claims of state secrets in a
lawsuit against a Boeing Co <BA.N> unit
claiming it helped fly terrorism suspects
abroad to secret prisons.
In many cases, even when the result is awkward
or ungrammatical, the ILP system makes reason-
able choices of mergers and dependencies to retain.
For inputs (24) and (25), the system (26) decides
?Secretary-General? belongs as a modifier on ?de
Mello?, which is in fact the choice made by the ed-
itor (27). In order to add the relative clause, the
editor paraphrased ?de Mello?s death? as ?de Mello
was killed?. Our system, without this paraphrase op-
tion, is forced to produce the improper phrase ?de
Mello?s death who?; a wider array of paraphrase op-
tions might lead to better results.
This example also demonstrates that the system
does not simply keep the LCS-aligned retained re-
gions and throw away everything else, since the re-
sult would be ungrammatical. Here it links the se-
lected content by also choosing to keep ?could have
been?, ?an account? and ?death?.
(24) Barker mixes an account of Vieira de
Mello?s death with scenes from his career,
which included working in countries such
as Mozambique, Cyprus, Cambodia,
Bangladesh, and the former Yugoslavia.
(25) Had he lived, he could have been a future
U.N. Secretary-General.
(26) Barker mixes an account of Vieira de Mello?s
death who could been a future U.N.
secretary-general with scenes from career
which included working in countries as such
Mozambique Cyprus Cambodia and
Bangladesh
(27) Barker recounted the day Vieira de Mello, a
Brazilian who was widely tipped as a future
U.N. Secretary-General, was killed and mixes
in the story of the 55-year-old?s career, which
included working in countries such as
Mozambique, Cyprus, Cambodia, Bangladesh,
and Yugoslavia.
Many of our errors are due to our simplistic lin-
earization. For instance, we produce a sentence be-
ginning ?Biden a veteran Democratic senator from
Delaware that Vice president-elect and Joe...?, where
a correct linearization of the output tree would have
begun ?Vice President-elect Joe Biden, a veteran
Democratic senator from Delaware that...?. Some
errors also occur during the ILP tree extraction pro-
cess. In (28), the system fails to mark the arguments
of ?took? and ?position? as required, leading to their
omission, which makes the output ungrammatical.
(28) The White House that took when Israel
invaded Lebanon in 2006 showed no signs of
preparing to call for restraint by Israel and the
stance echoed of the position.
8 Conclusion
We present a supervised method for learning to fuse
disparate sentences. To the best of our knowl-
edge, it is the first attempt at supervised learning
for this task. We apply our method to naturally oc-
curring sentences from editing data. Despite using
text generation, our system is comparable to a non-
abstractive baseline.
Our technique is general enough to apply to con-
ventional fusion of similar sentences as well? all that
is needed is a suitable training dataset. We hope
to make use of the new corpus of McKeown et al
(2010) for this purpose. We are also interested in
evaluating our approach on the fused sentences in
abstractive single-document summaries.
The performance of our readability upper bound
suggests we could improve our results using bet-
ter tree linearization techniques and parsing. Al-
though we show results for our system using hand-
annotated pronoun coreference, it should be possible
to use automatic coreference resolution instead.
Paraphrase rules would help our system repli-
cate some output structures it is currently unable
to match (for instance, it cannot convert between
the copular ?X is Y? and appositive ?X, a Y? con-
structions). Currently, the system has just one such
61
rule, which converts main clauses to relatives. Oth-
ers could potentially be learned from a corpus, as in
(Cohn and Lapata, 2009).
Finally, in this study, we deliberately avoid in-
vestigating the way editors choose which sentences
to fuse and what content from each of them to re-
tain. This is a challenging discourse problem that
deserves further study.
Acknowledgements
We are very grateful to Alan Elsner, Howard Goller
and Thomas Kim at Thomson-Reuters for giving us
access to this dataset. We thank Eugene Charniak for
his supervision, our colleagues in BLLIP for their
comments, Kapil Thadani and Kathy McKeown for
discussing the project with us, and our human eval-
uators for completing a task which turned out to be
extremely tedious. Part of this work was funded by a
Google Fellowship in Natural Language Processing.
References
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. J. Artif. Intell. Res. (JAIR),
31:399?429.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. J. Artif. Intell. Res.
(JAIR), 34:637?674.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics, July.
Hal Daume III and Daniel Marcu. 2004. Generic
sentence fusion is an ill-defined summarization task.
In Stan Szpakowicz Marie-Francine Moens, editor,
Text Summarization Branches Out: Proceedings of the
ACL-04 Workshop, pages 96?103, Barcelona, Spain,
July. Association for Computational Linguistics.
Jonathan L. Elsas, Vitor R. Carvalho, and Jaime G. Car-
bonell. 2008. Fast learning of document ranking func-
tions with the committee perceptron. In WSDM, pages
55?64.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 177?185, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, pages 225?228, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277?296.
Ilog, Inc. 2003. Cplex solver.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s the-
saurus and semantic similarity. In Conference on Re-
cent Advances in Natural Language Processing, pages
212?219.
Hongyan Jing and Kathleen McKeown. 1999. The de-
composition of human-written summary sentences. In
SIGIR, pages 129?136.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: sentence compression. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence, pages 703?71.
Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 193?196, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the 10th Eu-
ropean Workshop on Natural Language Generation,
pages 109?117.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159.
Kathleen McKeown, Sara Rosenthal, Kapil Thadani, and
Coleman Moore. 2010. Time-efficient creation of
an accurate sentence fusion corpus. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 317?320, Los An-
geles, California, June. Association for Computational
Linguistics.
Tadashi Nomoto. 2009. A comparison of model free
versus model intensive approaches to sentence com-
pression. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 391?399, Singapore, August. Association for
Computational Linguistics.
62
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::Similarity - measuring the re-
latedness of concepts. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In IJCAI?95:
Proceedings of the 14th international joint conference
on Artificial intelligence, pages 448?453, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 16?19,
Washington D.C.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, pages
257?286, November.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proc. Assoc. for Computational Linguistics (ACL),
pages 290?297.
63
