Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1060?1068, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Extending Machine Translation Evaluation Metrics with Lexical Cohesion
To Document Level
Billy T. M. Wong and Chunyu Kit
Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong SAR, P. R. China
{tmwong,ctckit}@cityu.edu.hk
Abstract
This paper proposes the utilization of lexical
cohesion to facilitate evaluation of machine
translation at the document level. As a linguis-
tic means to achieve text coherence, lexical
cohesion ties sentences together into a mean-
ingfully interwoven structure through words
with the same or related meaning. A compar-
ison between machine and human translation
is conducted to illustrate one of their critical
distinctions that human translators tend to use
more cohesion devices than machine. Various
ways to apply this feature to evaluate machine-
translated documents are presented, including
one without reliance on reference translation.
Experimental results show that incorporating
this feature into sentence-level evaluation met-
rics can enhance their correlation with human
judgements.
1 Introduction
Machine translation (MT) has benefited a lot from
the advancement of automatic evaluation in the past
decade. To a certain degree, its progress is also con-
fined to the limitations of evaluation metrics in use.
Most efforts devoted to evaluate the quality of MT
output so far have still focused on the sentence level
without sufficient attention to how a larger text is
structured. This is notably reflected in the represen-
tative MT evaluation metrics, such as BLEU (Pap-
ineni et al 2002), METEOR (Banerjee and Lavie,
2005) and TER (Snover et al 2006), that adopt a
sentence-by-sentence fashion to score MT outputs.
The evaluation result for a document by any of them
is usually a simple average of its sentence scores. A
drawback of this kind of sentence-based evaluation
is the neglect of document structure. There is no
guarantee for the coherence of a text if it is produced
by simply putting together stand-alone sentences, no
matter how well-translated, without adequate inter-
sentential connection. As a consequence, MT sys-
tem optimized this way to any of these metrics can
only have a very dim chance of producing translated
document that reads as natural as human writing.
The accuracy of MT output at the document level
is particularly important to MT users, for they care
about the overall meaning of a text in question more
than the grammatical correctness of each sentence
(Visser and Fuji, 1996). Post-editors particularly
need to ensure the quality of a whole document of
MT output when revising its sentences. The con-
nectivity of sentences is surely a significant factor
contributing to the understandability of a text as a
whole.
This paper studies the inter-sentential linguistic
features of cohesion and coherence and presents
plausible ways to incorporate them into the
sentence-based metrics to support MT evaluation at
the document level. In the Framework for MT Eval-
uation in the International Standards of Language
Engineering (FEMTI) (King et al 2003), coherence
is defined as ?the degree to which the reader can de-
scribe the role of each individual sentence (or group
of sentences) with respect to the text as a whole?.
The measurement of coherence has to rely on cohe-
sion, referring to the ?relations of meaning that exist
within the text? (Halliday and Hasan, 1976). Cohe-
sion is realized via the interlinkage of grammatical
and lexical elements across sentences. Grammatical
1060
cohesion refers to the syntactic links between text
items, while lexical cohesion is achieved through the
word choices in a text. This paper focuses on the
latter. A quantitative comparison of lexical cohesion
devices between MT output and human translation
is first conducted, to examine the weakness of cur-
rent MT systems in handling this feature. Different
ways of exploiting lexical cohesion devices for MT
evaluation at the document level are then illustrated.
2 Related Works
Cohesion and coherence are both necessary mono-
lingual features in a target text. They can hardly
be evaluated in isolation and have to be conjoined
with other quality criteria such as adequacy and flu-
ency. A survey of MT post-editing (Vasconcellos,
1989) suggests that cohesion and coherence serve
as higher level quality criteria beyond many others
such as syntactic well-formedness. Post-editors tend
to correct syntactic errors first before any amend-
ment for improving the cohesion and coherence of
an MT output. Also, as Wilks (1978)1 noted, it
is rather unlikely for a sufficiently large sample of
translations to be coherent and totally wrong at the
same time. Cohesion and coherence are appropri-
ate to serve as criteria for the overall quality of MT
output.
Previous researches in MT predominantly focus
on specific types of cohesion devices. For grammat-
ical cohesion, a series of works, including Nakaiwa
and Ikehara (1992), Nakaiwa et al(1995), and
Nakaiwa and Shirai (1996), present approaches to
resolving Japanese zero pronouns and to integrat-
ing them into a Japanese-English transferred-based
MT system. Peral et al(1999) propose an inter-
lingual mechanism for pronominal anaphora gen-
eration by exploiting a rich set of lexical, syntac-
tic, morphologic and semantic information. Mu-
rata and Nagao (1993) and Murata et al(2001) de-
velop a rule base to identify the referential prop-
erties of Japanese noun phrases, so as to facilitate
anaphora resolution for Japanese and article gen-
eration for English during translation. A recent
COMTIS project (Cartoni et al 2011) begins to ex-
ploit inter-sentential information for statistical MT.
A phase of its work is to have grammatical devices,
1As cited in van Slype (1979).
such as verbal tense/aspect/mode, discourse connec-
tives and pronouns, manually annotated in multilin-
gual corpora, in hopes of laying a foundation for the
development of automatic labelers for them that can
be integrated into an MT model.
For lexical cohesion, it has been only partially and
indirectly addressed in terms of translation consis-
tency in MT output. Different approaches to main-
taining consistency in target word choices are pro-
posed (Itagaki et al 2007; Gong et al 2011; Xiao
et al 2011). Carpuat (2009) also observes a general
tendency in human translation that a given sense is
usually lexicalized in a consistent manner through-
out the whole translation.
Nevertheless there are only a few evaluation
methods explicitly targeting on the quality of a docu-
ment. Miller and Vanni (2001) devise a human eval-
uation approach to measure the comprehensibility
of a text as a whole, based on the Rhetorical Struc-
ture Theory (Mann and Thompson, 1988), a theory
of text organization specifying coherence relations
in an authentic text. Snover et al(2006) proposes
HTER to assess post-editing effort through human
annotation. Its automatic versions TER and TERp
(Snover et al 2009), however, remain sentence-
based metrics. Comelles et al(2010) present a
family of automatic MT evaluation measures, based
on the Discourse Representation Theory (Kamp and
Reyle, 1993), that generate semantic trees to put to-
gether different text entities for the same referent ac-
cording to their contexts and grammatical connec-
tions. Apart from MT evaluation, automated essay
scoring programs such as E-rater (Burstein, 2003)
also employ a rich set of discourse features for as-
sessment. However, the parsing process needed for
these linguistic-heavy approaches may suffer seri-
ously from grammatical errors, which are unavoid-
able in MT output. Hence their accuracy and reli-
ability inevitably fluctuate in accord with different
evaluation data.
Lexical cohesion has far been neglected in both
MT and MT evaluation, even though it is the single
most important form of cohesion devices, account-
ing for nearly half of the cohesion devices in En-
glish (Halliday and Hasan, 1976). It is also a signif-
icant feature contributing to translation equivalence
of texts by preserving their texture (Lotfipour-Saedi,
1997). The lexical cohesion devices in a text can be
1061
represented as lexical chains conjoining related en-
tities. There are many methods of computing lexical
chains for various purposes, e.g., Morris and Hirst
(1991), Barzilay and Elhadad (1997), Chan (2004),
Li et al(2007), among many others. Contrary to
grammatical cohesion highly depending on syntac-
tic well-formedness of a text, lexical cohesion is less
affected by grammatical errors. Its computation has
to rely on a thesaurus, which is usually available for
almost every language. In this research, a number
of formulations of lexical cohesion, with or without
reliance on external language resource, will be ex-
plored for the purpose of MT evaluation.
3 Lexical Cohesion in Machine and
Human Translation
This section presents a comparative study of MT and
human translation (HT) in terms of the use of lexi-
cal cohesion devices. It is an intuition that more co-
hesion devices are used by humans than machines
in translation, as part of the superior quality of HT.
Two different datasets are used to ensure the relia-
bility and generality of the comparison. The results
confirm the incapability of MT in handling this fea-
ture and the necessity of using lexical cohesion in
MT evaluation.
3.1 Data
The MetricsMATR 2008 development set (Przy-
bocki et al 2009) and the Multiple-Translation Chi-
nese (MTC) part 4 (Ma, 2006) are used for this
study. They consist of MT outputs of different
source languages in company with reference trans-
lations. The data of MetricsMATR is selected from
the NIST Open MT 2006 evaluation, while MTC4 is
from the TIDES 2003 MT evaluation. Both datasets
include human assessments of MT output, from
which the part of adequacy assessment is selected
for this study. Table 1 provides overall statistics of
the datasets.
3.2 Identification of Lexical Cohesion Devices
Lexical cohesion is achieved through word choices
of two major types: reiteration and collocation. Re-
iteration can be realized in a continuum or a cline of
specificity, with repetition of the same lexical item at
one end and the use of a general noun to point to the
MetricsMATR MTC4
Number of systems 8 6
Number of documents 25 100
Number of segments 249 919
Number of references 4 4
Source language Arabic Chinese
Genre Newswire Newswire
Table 1: Information about the datasets in use
same referent at the other. In between the two ends
is to use a synonym (or near-synonym) and superor-
dinate. Collocation refers to those lexical items that
share the same or similar semantic relations, includ-
ing complementarity, antonym, converse, coordinate
term, meronym, troponym, and so on.
In this study, lexical cohesion devices are defined
as content words (i.e., tokens after stopword having
been removed) that reiterate once or more times in
a document, including synonym, near-synonym and
superordinate, besides those repetition and colloca-
tion. Repetition refers to the same words or stems
in a document. Stems are identified with the aid of
Porter stemmer (1980).
To classify the semantic relationships of words,
WordNet (Fellbaum, 1998) is used as a lexical re-
source, which clusters words of the same sense (i.e.,
synonyms) into a semantic group, namely a synset.
Synsets are interlinked in WordNet according to
their semantic relationships. Superordinate and col-
location are formed by words in a proximate se-
mantic relationship, such as bicycle and vehicle (hy-
pernym), bicycle and wheel (meronym), bicycle and
car (coordinate term), and so on. They are defined
as synset pairs with a distance of 1 in WordNet.
The measure of semantic distance (Wu and Palmer,
1994) is also applied to identify near-synonyms, i.e.,
words that are synonyms in a broad sense but not
grouped in the same synset. It quantifies the seman-
tic similarity of word pairs as a real number in be-
tween 0 and 1 (the higher the more similar) as
sim(c1, c2) =
2 d(lcs(c1, c2))
d(c1) + d(c2)
where c1 and c2 are the concepts (synsets) that the
two words in question belong to, d is the distance
in terms of the shortest path from a concept to the
1062
Word type
MetricsMATR MTC4
MT HT Difference (%) MT HT Difference (%)
Content word 4428 4636 208 (4.7) 16162 16982 830 (5.1)
- Not lexical cohesion device 2403 2381 -22 (-1.0) 8657 8814 157 (1.8)
- Lexical cohesion device 2025 2255 230 (11.4) 7505 8168 663 (8.9)
- Repetition 1297 1445 148 (11.4) 4888 5509 621 (12.7)
- Synonym and near-synonym 318 350 32 (10.1) 1323 1311 -12 (-0.9)
- Superordinate and collocation 410 460 50 (12.4) 1294 1348 54 (4.2)
Table 2: Statistics of lexical cohesion devices in machine versus human translation (average frequencies per version
of MT/HT)
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
MetricsMATR 
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
MTC4 
RC (MT)
RC (HT)
LC (MT)
LC (HT)
V
al
ue
s 
of
 R
C
 &
 L
C
 
Figure 1: Use of lexical cohesion devices in machine versus human translation
global root node in WordNet, and lcs is the least
common subsumer (i.e., the most specific ancestor
concept) of c1 and c2. A threshold is set to 0.96
for words to be considered near-synonyms of each
other, based on the empirical observation in a previ-
ous study (Wong, 2010).
3.3 Results
The difference between MT and HT (reference
translation) in terms of the frequencies of lexical co-
hesion devices in MetricsMATR and MTC4 datasets
is presented in Table 2. The frequencies are aver-
aged by the number of MT/HT versions. A further
categorization breaks down content words into lex-
ical cohesion devices and those that are not. The
count of each type of lexical cohesion device is also
provided. In general the two datasets provide highly
similar statistics. There are 4.7?5.1% more content
words in HT than in MT. The numbers of ordinary
content words (i.e., not lexical cohesion devices) are
close in MT and HT. The difference of content words
in HT and MT is mostly due to that of lexical co-
hesion devices, which are mostly repetition. 8.9?
11.4% more lexical cohesion devices are found in
HT than in MT in the datasets.
A further analysis is carried out to investigate into
the use of lexical cohesion devices in each version
of MT and HT in terms of the following two ratios,
LC = lexical cohesion devices / content words,
RC = repetition / content words.
A higher LC or RC ratio means that a greater pro-
portion of content words are used as lexical cohesion
devices.
Figure 1 illustrates the RC and LC ratios in the
two datasets. The ratios of different MT systems
are presented in an ascending order in each graph
from left to right, according to their human assess-
ment results. The distributions of these values show
a strong similarity between the two datasets. First,
most of the RC and LC ratios are within an observ-
able range, i.e., 0.25?0.35 for the former and 0.40?
0.50 for the latter, except a particularly low LC for
1063
MT 1
1 Chine scrambled research on 16 key technical
2 These techniques are from within headline everyones boosting science and technology and achiev-
ing goals and contend of delivered on time bound through achieving breakthroughs in essential
technology and complimentarity resources . national
BLEU: 0.224 (1-gram:7, 2-gram:0, 3-gram:2, 4-gram:1)
LC: 0.107 (number of lexical cohesion devices: 5)
Human assessment: 2.67
MT 2
1 China is accelerating research 16 main technologies
2 These technologies are within the important realm to promote sciences and technology and
achieve national goals and must be completed in a timely manner through achieving main dis-
coveries in technology and integration of resources .
BLEU: 0.213 (1-gram:5, 2-gram:3, 3-gram:2, 4-gram:1)
LC: 0.231 (number of lexical cohesion devices: 9)
Human assessment: 4.33
Reference
1 China Accelerates Research on 16 Main Technologies
2 These technologies represent a significant part in the development of science and technology and
the achievement of national goals. They must be accomplished within a fixed period of time by
realizing breakthroughs in essential technologies and integration of resources.
Table 3: An example of MT outputs of different quality (underlined: matched n-grams; italic: lexical cohesion devices)
one MT system. Second, the ratios in those differ-
ent HT versions are very stable in comparison with
those of MT. Especially, all four HT versions in the
MetricsMATR dataset share the sameRC ratio 0.31.
This shows a typical level of the use of lexical cohe-
sion device. Third, the ratios in MT are lower than or
at most equal to those in HT, suggesting their corre-
lation with translation quality: the closer their RC
and LC ratios to those in HT, the better the MT.
These results verify our assumption that lexical co-
hesion can serve as an effective proxy of the level of
translation quality.
4 MT Evaluation at Document Level
As a feature at the discourse level, lexical cohesion
is a good complement to current evaluation met-
rics focusing on features at the sentence level. Ta-
ble 3 illustrates an example selected from the Met-
ricsMATR dataset, consisting two versions of MT
output for a short document of two segments only.
The n-grams matched with the reference are under-
lined, while the lexical cohesion devices are itali-
cized. The two MT outputs have a similar num-
ber of matched n-grams and hence receive similar
BLEU scores. These scores, however, do not reflect
their real difference in quality: the second version is
better, according to human assessment of adequacy.
Instead, their LC ratios seem to represent such a
variation more accurately. The theme of the second
output is also highlighted through the lexical chains,
including main/important, technology/technologies
and achieve/achieving, which create a tight texture
between the two sentences, a crucial factor of text
quality.
To perform MT evaluation at the document level,
the LC and RC ratios can be used alone or in-
tegrated into a sentence-level metric. The former
way has an advantage that it does not have to rely
on any reference translation. LC mainly requires
a thesaurus for computing semantic relation, while
RC only needs a morphological processor such as
stemmer, both of which are available for most lan-
1064
guages. Its drawback, however, lies in the risk of
relying on a single discourse feature. Although lex-
ical cohesion gives a strong indication of text co-
herence, it is not indispensable, because a text can
be coherent without any surface cohesive clue. Fur-
thermore, the quality of a document is also reflected
in that of its sentences. A coherent translation may
be mistranslated, and on the other hand, a text con-
taining lots of sentence-level errors would make it
difficult to determine its document-level quality. A
previous study comparing MT evaluation at the sen-
tence versus document level (Wong et al 2011) re-
ports a poor consistency in the evaluation results at
these two levels when the sentence-level scores of
MT output are low. In regard of these, how to inte-
grate these two levels of MT evaluation is particu-
larly worth studying.
5 Experiments
We examine, through experiments, the effectiveness
of using LC and RC ratios alone and integrating
them into other evaluation metrics for MT evalua-
tion at the document and system levels. Three evalu-
ation metrics, namely BLEU, TER and METEOR,2
are selected for testing. They represent three dis-
tinctive types of evaluation metrics: n-gram, edit-
distance, and unigram with external language re-
sources, respectively. These metrics are evaluated in
terms of their correlation with human assessments,
using Pearson?s r correlation coefficient. The Met-
ricsMATR and MTC4 datasets and their adequacy
assessments are used as evaluation data. Note that
the adequacy assessment is in fact an evaluation
method for the sentence level. We have to rely on
an assumption that this evaluation data may emulate
document-level quality, since its MT outputs were
assessed sentence by sentence in sequence as in a
document. All experiments are performed under a
setting of multiple reference translations.
The integration of the two ratios into an evaluation
metric follows a simple weighted average approach.
A hybrid metric H is formulated as
H = ? mdoc + (1? ?) mseg
where mdoc refers to the document-level feature in
2METEOR 1.0 with default parameters optimized over the
adequacy assessments.
use (i.e., LC or RC), mseg to a sentence-level met-
ric, and ? to a weight controlling their proportion.
The MetricsMATR dataset is used as training data to
optimize the values of ? for different metrics, while
the MTC4 is used as evaluation data. Table 4 shows
the optimized weights for the metrics for evaluation
at the document level.
Metrics RC LC
BLEU 0.28 0.29
TER 0.40 0.38
METEOR 0.19 0.18
Table 4: Optimized weights for the integration of dis-
course feature into sentence-level metrics
Table 5 presents the correlation rates of evalua-
tion metrics obtained in our experiments under dif-
ferent settings, with their 95% conference intervals
(CI) provided. The LC and RC ratios are found to
have strong correlations with human assessments at
the system level even when used alone, highly com-
parable to BLEU and TER. At the document level,
however, they are not as good as the others. They
show their advantages when integrated into other
metrics, especially BLEU and TER. LC raises the
correlation of BLEU from 0.447 to 0.472 and from
0.861 to 0.905 at the document and system levels,
respectively. It improves TER even more signifi-
cantly, in that the correlation rates are boosted up
from -0.326 to -0.390 at the document level, and
even from -0.601 to -0.763 at the system level. Since
there are only six systems in the MTC4 data, such a
dramatic change may not be as meaningful as the
smooth improvement at the document level. ME-
TEOR is a special case in this experiment. Its corre-
lation cannot be improved by integrating LC orRC,
and is even slightly dropped at the document level.
The cause for this is yet to be identified. Neverthe-
less, these results confirm the close relationship of
an MT system?s capability to appropriately generate
lexical cohesion devices with the quality of its out-
put.
Table 6 presents the Pearson correlations between
evaluation results at the document level using dif-
ferent evaluation metrics in the MTC4 data. It il-
lustrates the homogeneity/heterogeneity of different
metrics and helps explain the performance change
1065
Document System
Metrics Correlation 95% CI Correlation 95% CI
RC 0.243 (0.167, 0.316) 0.873 (0.211, 0.985)
LC 0.267 (0.192, 0.339) 0.818 (0.020, 0.979)
BLEU 0.447 (0.381, 0.508) 0.861 (0.165, 0.984)
BLEU+RC 0.463 (0.398, 0.523) 0.890 (0.283, 0.987)
BLEU+LC 0.472 (0.408, 0.531) 0.905 (0.352, 0.989)
TER -0.326 (-0.253, -0.395) -0.601 (-0.411, -0.949)
TER+RC -0.370 (-0.299, -0.437) -0.740 (-0.179, -0.969)
TER+LC -0.390 (-0.320, -0.455) -0.763 (-0.127, -0.972)
METEOR 0.557 (0.500, 0.609) 0.961 (0.679, 0.995)
METEOR+RC 0.555 (0.498, 0.608) 0.960 (0.672, 0.995)
METEOR+LC 0.556 (0.499, 0.609) 0.962 (0.687, 0.995)
Table 5: Correlation of different metrics with adequacy assessment in MTC4 data
BLEU 1
TER -0.699 1
METEOR 0.834 -0.510 1
RC 0.287 -0.204 0.405 1
LC 0.263 -0.097 0.437 0.736 1
BLEU TER METEOR RC LC
Table 6: Correlation between the evaluation results of different metrics
by combining sentence- and document-level met-
rics. The table shows that the two ratios LC and
RC highly correlate with each other, as if they are
two variants of quantifying lexical cohesion devices.
The three sentence-level metrics, BLEU, TER and
METEOR, also show strong correlations with each
other, especially between BLEU and METEOR. The
correlations are generally weaker between sentence-
and document-level metrics, for instance, 0.263 be-
tween BLEU and LC and only -0.097 between TER
and LC, showing that they are quite heterogeneous
in nature. This accounts for the significant perfor-
mance gain from their combination: their difference
allows them to complement each other. It is also
worth noting that between METEOR and LC the
correlation of 0.437 is mildly strong, explaining the
negative result of their integration. On the one hand,
lexical cohesion is word choice oriented, which is
only sensitive to the reiteration and semantic relat-
edness of words in MT output. On the other hand,
METEOR is strong in unigram matching, with mul-
tiple strategies to maximize the matching rate be-
tween MT output and reference translation. In this
sense they are homogeneous to a certain extent, ex-
plaining the null effect of their combination.
6 Discussion and Conclusion
In this study we have attempted to address the prob-
lem that most existing MT evaluation metrics dis-
regard the connectivity of sentences in a document.
By focusing on a typical type of cohesion, i.e., lexi-
cal cohesion, we have shown that its use frequency is
a significant factor to differentiate HT from MT and
MT outputs of different quality from each other. The
high correlation rate of its use with translation ade-
quacy also suggests that the more lexical cohesion
devices in use, the better the quality of MT output.
Accordingly we have used two ratios, LC and RC,
to capture such correlativity. Our experimental re-
sults have confirmed the effectiveness of this feature
in accounting for the document-level quality of MT
output. The performance of two evaluation metrics,
BLEU and TER, is highly improved through incor-
porating this document-level feature, in terms of the
1066
change of their correlation with human assessments.
This finding is positive and sheds light on a region
of MT research that is still severely under-explored.
Our approach to extending the granularity of MT
evaluation from sentence to document through lex-
ical cohesion is highly applicable to different lan-
guages. It has a relatively weak demand for lan-
guage resource in comparison with the processing of
other discourse features like grammatical cohesion.
It is also much unaffected by grammatical problems
or errors commonly seen in natural languages and,
in particular, MT outputs.
Our future work will continue to explore the re-
lationship of lexical cohesion to translation quality,
so as to identify, apart from its use frequency, other
significant aspects for MT evaluation at the docu-
ment level. A frequent use of cohesion devices in
a text is not necessarily appropriate, because an ex-
cess of them may decrease the quality and readabil-
ity of a text. Human writers can strategically change
the ways of expression to achieve appropriate coher-
ence and also avoid overuse of the same lexical item.
To a certain extent, this is one of the causes for the
unnaturalness of MT output: it may contain a large
number of lexical cohesion devices which are sim-
ply direct translation of those in a source text that
do not fit in the target context. How to use lexical
cohesion devices appropriately instead of frequently
is thus an important issue to tackle before we can
adopt them in MT and MT evaluation by a suitable
means.
Acknowledgments
The research described in this paper was substan-
tially supported by the Research Grants Council
(RGC) of Hong Kong SAR, P. R. China through
GRF grant 144410.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17.
Jill Burstein. 2003. The E-rater scoring engine: Auto-
mated essay scoring with natural language processing.
In Mark D. Shermis and Jill Burstein, editors, Auto-
mated Essay Scoring: A Cross-Disciplinary Perspec-
tive, chapter 7, pages 113?122. Lawrence Erlbaum As-
sociates.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the NAACL HLT Workshop on Se-
mantic Evaluations: Recent Achievements and Future
Directions, pages 19?27, Boulder, Colorado.
Bruno Cartoni, Andrea Gesmundo, James Henderson,
Cristina Grisot, Paola Merlo, Thomas Meyer, Jacques
Moeschler, Sandrine Zufferey, and Andrei Popescu-
Belis. 2011. Improving MT coherence through text-
level processing of input texts: The COMTIS project.
In Tralogy, Paris.
Samuel W. K. Chan. 2004. Extraction of sailent tex-
tual patterns: Synergy between lexical cohesion and
contextual coherence. IEEE Transactions on Systems,
Man and Cybernetics, Part A: Systems and Humans,
34(2):205?218.
Elisabet Comelles, Jesus Gime?nez, Llu?`s Ma?rquez, Irene
Castello`n, and Victoria Arranz. 2010. Document-
level automatic MT evaluation based on discourse rep-
resentations. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 333?338, Uppsala.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In EMNLP 2011, pages 909?919, Edinburgh,
Scotland.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. London: Longman.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology translation
consistency with statistical method. In MT Summit XI,
pages 269?274.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and Discourse Rep-
resentation Theory. Dordrecht: Kluwer.
Margaret King, Andrei Popescu-Belis, and Eduard Hovy.
2003. FEMTI: Creating and using a framework for
MT evaluation. In MT Summit IX, pages 224?231,
New Orleans.
Jing Li, Le Sun, Chunyu Kit, and Jonathan Webster.
2007. A query-focused multi-document summarizer
based on lexical chains. In DUC 2007, Rochester,
New York.
1067
Kazem Lotfipour-Saedi. 1997. Lexical cohesion and
translation equivalence. Meta, 42(1):185?192.
Xiaoyi Ma. 2006. Multiple-Translation Chinese (MTC)
part 4. Linguistic Data Consortium.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Keith J. Miller and Michelle Vanni. 2001. Scaling
the ISLE taxonomy: Development of metrics for the
multi-dimensional characterisation of machine trans-
lation quality. In MT Summit VIII, pages 229?238.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21?48.
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In TMI 1993, pages 218?225, Kyoto.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and Hi-
toshi Isahara. 2001. A machine-learning approach to
estimating the referential properties of Japanese noun
phrases. In CICLING 2001, pages 142?153, Mexico-
City.
Hiromi Nakaiwa and Satoru Ikehara. 1992. Zero pro-
noun resolution in a machine translation system by us-
ing Japanese to English verbal semantic attributes. In
ANLP 1992, pages 201?208.
Hiromi Nakaiwa and Satoshi Shirai. 1996. Anaphora
resolution of Japanese zero pronouns with deictic ref-
erence. In COLING 1996, pages 812?817, Copen-
hagen.
Hiromi Nakaiwa, Satoshi Shirai, Satoru Ikehara, and
Tsukasa Kawaok. 1995. Extrasentential resolution
of Japanese zero pronouns using semantic and prag-
matic constraints. In Proceedings of the AAAI 1995
Spring Symposium Series: Empirical Methods in Dis-
course Interpretation and Generation, pages 99?105,
Stanford.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL 2002, pages
311?318.
Jesu?s Peral, Manuel Palomar, and Antonio Ferra`ndez.
1999. Coreference-oriented interlingual slot structure
and machine translation. In Proceedings of the ACL
Workshop on Coreference and its Applications, pages
69?76, College Park, MD.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Mark Przybocki, Kay Peterson, and Se?bastien Bronsart.
2009. 2008 NIST metrics for machine translation
(MetricsMATR08) development data. Linguistic Data
Consortium.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA 2006, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the 4th Work-
shop on Statistical Machine Translation, pages 259?
268, Athens.
Georges van Slype. 1979. Critical Study of Methods for
Evaluating the Quality of Machine Translation. Tech-
nical report, Bureau Marcel van Dijk / European Com-
mission, Brussels.
Muriel Vasconcellos. 1989. Cohesion and coherence in
the presentation of machine translation products. In
James E. Alatis, editor, Georgetown University Round
Table on Languages and Linguistics 1989: Language
Teaching, Testing, and Technology: Lessons from the
Past with a View Toward the Future, pages 89?105.
Georgetown University Press.
Eric M. Visser and Masaru Fuji. 1996. Using sentence
connectors for evaluating MT output. In COLING
1996, pages 1066?1069.
Yorick Wilks. 1978. The Value of the Monolingual
Component in MT Evaluation and its Role in the Bat-
telle. Report on Systran, Luxembourg CEC Memoran-
dum.
Billy T. M. Wong, Cecilia F. K. Pun, Chunyu Kit, and
Jonathan J. Webster. 2011. Lexical cohesion for eval-
uation of machine translation at document level. In
NLP-KE 2011, pages 238?242, Tokushima.
Billy Tak-Ming Wong. 2010. Semantic evaluation of ma-
chine translation. In LREC 2010, pages 2884?2888,
Valletta.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In ACL 1994, pages 133?138,
Las Cruces.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In MT summit XIII, pages 131?138,
Xiamen.
1068
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 360?364,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Parameter-optimized ATEC Metric for MT Evaluation 
 
 
Billy T-M Wong               Chunyu Kit 
Department of Chinese, Translation and Linguistics 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong 
{ctbwong, ctckit}@cityu.edu.hk 
 
  
 
Abstract 
This paper describes the latest version of the 
ATEC metric for automatic MT evaluation, 
with parameters optimized for word choice 
and word order, the two fundamental features 
of language that the metric relies on. The 
former is assessed by matching at various 
linguistic levels and weighting the informa-
tiveness of both matched and unmatched 
words. The latter is quantified in term of 
word position and information flow. We also 
discuss those aspects of language not yet 
covered by other existing evaluation metrics 
but carefully considered in the formulation of 
our metric.  
1 Introduction 
It is recognized that the proposal of the BLEU 
metric (Papineni et al, 2002) has piloted a para-
digm evolution to MT evaluation. It provides a 
computable solution to the task and turns it into 
an engineering problem of measuring text simi-
larity and simulating human judgments of trans-
lation quality. Related studies in recent years 
have extensively revealed more essential charac-
teristics of BLEU, including its strengths and 
weaknesses. This has aroused the proposal of 
different new evaluation metrics aimed at ad-
dressing such weaknesses so as to find some oth-
er hopefully better alternatives for the task. Ef-
fort in this direction brings up some advanced 
metrics such as METEOR (Banerjee and Lavie, 
2005) and TERp (Snover et al, 2009) that seem 
to have already achieved considerably strong 
correlations with human judgments. Nevertheless, 
few metrics have really nurtured our understand-
ing of possible parameters involved in our lan-
guage comprehension and text quality judgment. 
This inadequacy limits, inevitably, the applica-
tion of the existing metrics. 
The ATEC metric (Wong and Kit, 2008) was 
developed as a response to this inadequacy, with 
a focus to account for the process of human 
comprehension of sentences via two fundamental 
features of text, namely word choice and word 
order. It integrates various explicit measures for 
these two features in order to provide an intuitive 
and informative evaluation result. Its previous 
version (Wong and Kit, 2009b) has already illu-
strated a highly comparable performance to the 
few state-of-the-art evaluation metrics, showing 
a great improvement over its initial version for 
participation in MetricsMATR081. It is also ap-
plied to evaluate online MT systems for legal 
translation, to examine its applicability for lay 
users? use to select appropriate MT systems 
(Wong and Kit, 2009a). 
In this paper we describe the formulation of 
ATEC, including its new features and optimiza-
tion of parameters. In particular we will discuss 
how the design of this metric can complement 
the inadequacies of other metrics in terms of its 
treatment of word choice and word order and its 
utilization of multiple references in the evalua-
tion process. 
2 The ATEC Metric 
2.1 Word Choice 
In general, word is the basic meaning bearing 
unit of language. In a semantic theory such as 
Latent Semantic Analysis (LSA) (Landauer et al, 
1998), lexical selection is even the sole consider-
ation of the meaning of a text. A recent study of 
the major errors in MT outputs by Vilar et al 
(2006) also reveals that different kinds of error 
related to word choices constitute a majority of 
error types. It is therefore of prime importance 
                                                 
1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2008/ 
360
for MT evaluation metrics to diagnose the ade-
quacy of word selection by an MT system. 
It is a general consensus that the performance 
of an evaluation metric can be improved by 
matching more words between MT outputs and 
human references. Linguistic resources like 
stemmer and WordNet are widely applied by 
many metrics for matching word stems and syn-
onyms. ATEC is equipped with these two mod-
ules as well, and furthermore, with two measures 
for word similarity, including a WordNet-based 
(Wu and Palmer, 1994) and a corpus-based 
measure (Landauer et al, 1998) for matching 
word pairs of similar meanings. Our previous 
work (Wong, 2010) shows that the inclusion of 
semantically similar words results in a positive 
correlation gain comparable to the use of Word-
Net for synonym identification. 
In addition to increasing the number of legiti-
mate matches, we also consider the importance 
of each match. Although most metrics score 
every matched word with equal weight, different 
words indeed contribute different amount of in-
formation to the meaning of a sentence. In Ex-
ample 1 below, both C1 and C2 contain the same 
number of words matched with Ref, but the 
matches in C1 are more informative and there-
fore should be assigned higher weights. 
 
Example 1 
C1: it was not first time that prime minister con-
fronts northern league ? 
C2: this is not the prime the operation with the 
north ? 
Ref: this is not the first time the prime minister 
has faced the northern league ? 
 
The informativeness of a match is weighted by 
the tf-idf measure, which has been widely used in 
information retrieval to assess the relative impor-
tance of a word as an indexing term for a docu-
ment. A word is more important to a document 
when it occurs more frequently in this document 
and less in others. In ATEC, we have ?document? 
to refer to ?sentence?, the basic text unit in MT 
evaluation. This allows a more sensitive measure 
for words in different sentences, and gets around 
the problem of an evaluation dataset containing 
only one or a few long documents. Accordingly, 
the tf-idf measure is formulated as: 
)log(),( ,
i
ji sf
N
tfjitfidf ?=  
where tfi,j is the occurrences of word wi in sen-
tence sj, sfi the number of sentences containing 
word wi, and N the total number of sentences in 
the evaluation set. In case of a high-frequency 
word whose tf-idf weight is less than 1, it is then 
rounded up to 1. 
In addition to matched words, unmatched 
words are also considered to have a role to play 
in determining the quality of word choices of an 
MT output. As illustrated in Example 1, the un-
matched words in Ref for C1 and C2 are [this | is 
| the | the | has | faced | the] and [first | time | mi-
nister | has | faced | northern | league] respective-
ly. One can see that the words missing in C2 are 
more significant. It is therefore necessary to ap-
ply the tf-idf weighting to unmatched reference 
words so as to quantify the information missed in 
the MT outputs in question.  
2.2 Word Order 
In MT evaluation, word order refers to the extent 
to which an MT output is interpretable following 
the information flow of its reference translation. 
It is not rare that an MT output has many 
matched words but does not make sense because 
of a problematic word order. Currently it is ob-
served that consecutive matches represent a legi-
timate local ordering, causing some metrics to 
extend the unit of matching from word to phrase. 
Birch et al (2010) show, however, that the cur-
rent metrics including BLEU, METEOR and 
TER are highly lexical oriented and still cannot 
distinguish between sentences of different word 
orders. This is a serious problem in MT evalua-
tion, for many MT systems have become capable 
of generating more and more suitable words in 
translations, resulting in that the quality differ-
ence of their outputs lies more and more crucial-
ly in the variances of word order.  
ATEC uses three explicit features for word or-
der, namely position distance, order distance and 
phrase size. Position distance refers to the diver-
gence of the locations of matches in an MT out-
put and its reference. Example 2 illustrates two 
candidates with the same match, whose position 
in C1 is closer to its corresponding position in 
Ref than that in C2. We conceive this as a signif-
icant indicator of the accuracy of word order: the 
closer the positions of a matched word in the 
candidate and reference, the better match it is. 
 
Example 2 
C1: non-signatories these acts victims but it 
caused to incursion transcendant 
C2: non-signatories but it caused to incursion 
transcendant these acts victims 
Ref: there were no victims in this incident but 
they did cause massive damage 
361
The calculation of position distance is based 
on the position indices of words in a sentence. In 
particular, we align every word in a candidate to 
its closest counterpart in a reference. In Example 
3, all the candidate words have a match in the 
reference. As illustrated by the two ?a? in the 
candidate, the shortest alignments (strict lines) 
are preferred over any farther alternatives (dash 
lines). In a case like this, only two matches, i.e., 
thief and police, vary in position by a distance of 
3. 
 
Example 3 
Candidate: a thief chases a police 
Pos distance:        0     3         0      0      3 
Pos index: 1    2         3      4      5  
 
Reference: a police chases a thief 
Pos index: 1     2         3        4       5  
 
This position distance is sensitive to sentence 
length as it simply makes use of word position 
indices without any normalization. Example 4 
illustrates two cases of different lengths. The po-
sition distance of the bold matched words is 3 in 
C1 but 14 in C2. Indeed, the divergence of word 
order in C1 does not hinder our understanding, 
but in C2 it poses a serious problem. This exces-
sive length inevitably magnifies the interference 
effect of word order divergence. 
 
Example 4 
C1: Short1 and2 various3 international4 news5 
R1: International1 news2 brief3 
C2: Is1 on2 a3 popular4 the5 very6 in7 Iraq8 to9 
those10 just11 like12 other13 world14 in15 
which16 young17 people18 with19 the20 and21 
flowers22 while23 awareness24 by25 other26 
times27 of28 the29 countries30 of31 the32 
R2: Valentine?s1 day2 is3 a4 very5 popular6 day7 
in8 Iraq9 as10 it11 is12 in13 the14 other15 coun-
tries16 of17 the18 world19. Young20 men21 ex-
change22 with23 their24 girlfriends25 sweets26, 
flowers27, perfumes28 and29 other30 gifts31. 
 
Another feature, the order distance, concerns 
the information flow of a sentence in the form of 
the sequence of matches. Each match in a candi-
date and a reference is first assigned an order 
index in a sequential manner. Then, the differ-
ence of two counterpart indices is measured, so 
as to see if a variance exists. Examples 5a and 5b 
exemplify two kinds of order distance and their 
corresponding position distance. Both cases have 
two matches with the same sum of position dis-
tance. However, the matches are in an identical 
sequence in 5a but cause a cross in 5b, resulting 
in a larger order distance for the latter.  
 
Example 5a 
Position index 
Order index 
Candidate: 
 
Reference: 
Order index 
Position index 
 
Position distance 
Order distance 
1      2     3     4 
1            2 
A    B    C    D 
 
B    E    D    F 
1           2 
1     2     3     4 
 
(2-1) + (4-3) = 2 
(1-1) + (2-2) = 0 
 
Example 5b 
Position index 
Order index 
Candidate: 
 
Reference: 
Order index 
Position index 
 
Position distance 
Order distance 
1      2     3     4 
        1     2 
A    B    C    D 
 
C    B    E    F 
1      2 
1      2     3     4 
 
(2-2) + (3-1) = 2 
(2-1) + (2-1) = 2 
 
In practice, ATEC operates on phrases like 
many other metrics. But unlike these metrics that 
count only the number of matched phrases, 
ATEC gives extra credit to a longer phrase to 
reward its valid word sequence. In Example 6, 
C1 and C2 represent two MT outputs of the same 
length, with matched words underlined. Both 
have 10 matches in 3 phrases, and will receive 
the same evaluation score from a metric like 
METEOR or TERp, ignoring the subtle differ-
ence in the sizes of the matched phrases, which 
are [8,1,1] and [4,3,3] words for C1 and C2 re-
spectively. In contrast, ATEC uses the size of a 
phrase as a reduction factor to its position dis-
tance, so as to raise the contribution of a larger 
phrase to the metric score.  
 
Example 6 
C1: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 
C2: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 
2.3 Multiple References 
The availability of multiple references allows 
more legitimate word choices and word order of 
an MT output to be accounted. Some existing 
metrics only compute the scores of a candidate 
against each reference and select the highest one. 
362
This deficit can be illustrated by a well-known 
example from Papineni et al (2002), as repli-
cated in Example 7 with slight modification. It 
shows that nearly all candidate words can find 
their matches in either reference. However, if we 
resort to single reference, only around half of 
them can have a match, which would seriously 
underrate the quality of the candidate.  
 
Example 7 
C:   It is a guide to action which ensures that the 
military always obeys the commands of the 
party. 
 
R1: It is a guide to action that ensures that the 
military will forever heed Party commands. 
 
R2: It is the guiding principle which guarantees 
the military forces always being under the 
commands of the party. 
 
ATEC exploits multiple references in this fa-
shion to maximize the number of matches in a 
candidate. It begins with aligning the longest 
matches with either reference. The one with the 
shortest position distance is preferred if more 
than one alternative available in the same phrase 
size. This process repeats until no more candi-
date word can find a match. 
2.4 Formulation of ATEC 
The computation of an ATEC score begins with 
alignment of phrases, as described above. For 
each matched phase, we first sum up the score of 
each word i in the phrase as 
?
?
?=
}{
)(
phrasei i
match
typematch tfidf
Info
wW  
where wtype refers to a basic score of a matched 
word depending on its match type. It is then 
minus its information load, i.e., the tf-idf score of 
the matched word with a weight factor, Infomatch. 
There is also a distance penalty for a phrase, 
orderorder
e
pospos diswc
p
diswDis +?= )
||
||
1(
 ,
 
where dispos and disorder refer to the position 
distance and order distance, and wpos and worder 
are their corresponding weight factors, 
respectively. The position distance is further 
weighted according to the size of phrase |p| with 
an exponential factor e, in proportion to the 
length of candidate |c|.  
The score of a matched phrase is then 
computed by 
??
?
?
?=
,
,
DisW
LimitW
Phrase
match
dismatch  if  Dis > Wmatch?Limitdis; 
otherwise, 
Limitdis is an upper limit for the distance penalty. 
Accordingly, the score C of all phrases in a can-
didate is  
?
?
=
}{candidatej
jPhraseC
. 
Then, we move on to calculating the informa-
tion load of unmatched reference words Wunmatch, 
approximated as 
)(
}{
?
?
?=
unmatchk k
unmatch
typeunmatch tfidf
Info
wW
.
 
The overall score M accounting for both the 
matched and unmatched is defined as 
??
?
?
?=
,
,
unmatch
Info
WC
LimitC
M
if  Wunmatch > C?LimitInfo; 
otherwise, 
LimitInfo is an upper limit for the information 
penalty of the unmatched words. 
Finally, the ATEC score is computed using the 
conventional F-measure in terms of precision P 
and recall R as  
RP
PR
ATEC
)1( ?? ?+=  
where             || c
M
P =
,
 
|| r
M
R =
.
 
The parameter ? adjusts the weights of P and R, 
and |c| and |r| refer to the length of candidate and 
reference, respectively. In the case of multiple 
references, |r| refers to the average length of ref-
erences. 
We have derived the optimized values for the 
parameters involved in ATEC calculation using 
the development data of NIST MetricsMATR10 
with adequacy assessments by a simple hill 
climbing approach. The optimal parameter set-
ting is presented in Table 1 below. 
3 Conclusion 
In the above sections we have presented the lat-
est version of our ATEC metric with particular 
emphasis on word choice and word order as two 
fundamental features of language. Each of these 
features contains multiple parameters intended to 
363
have a comprehensive coverage of different tex-
tual factors involved in our interpretation of a 
sentence. The optimal offsetting for the parame-
ters is expected to report an empirical observa-
tion of the relative merits of each factor in ade-
quacy assessment. We are currently exploring 
their relation with the errors of MT outputs, to 
examine the potential of automatic error analysis. 
The ATEC package is obtainable at:  
http://mega.ctl.cityu.edu.hk/ctbwong/ATEC/ 
 
Acknowledgments 
The research work described in this paper was 
supported by City University of Hong Kong 
through the Strategic Research Grant (SRG) 
7002267. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Pro-
ceedings of Workshop on Intrinsic and Extrinsic 
Evaluation Measures for MT and/or Summariza-
tion at the 43th Annual Meeting of the Association 
of Computational Linguistics (ACL), pages 65-72, 
Ann Arbor, Michigan, June 2005. 
Alexandra Birch, Miles Osborne and Phil Blunsom. 
2010. Metrics for MT Evaluation: Evaluating 
Reordering. Machine Translation (forthcoming). 
Thomas Landauer, Peter W. Foltz and Darrell Laham. 
1998. Introduction to Latent Semantic Analysis. 
Discourse Processes 25: 259?284. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. Proceed-
ings of 40th Annual Meeting of the Association for 
Computational Linguistics (ACL), pages 311?318, 
Philadelphia, PA, July 2002. 
Matthew Snover, Nitin Madnani, Bonnie Dorr, and 
Richard Schwartz. 2009. Fluency, Adequacy, or 
HTER? Exploring Different Human Judgments 
with a Tunable MT Metric. Proceedings of the 
Fourth Workshop on Statistical Machine Transla-
tion at the 12th Meeting of the European Chapter 
of the Association for Computational Linguistics 
(EACL), pages 259-268, Athens, Greece, March, 
2009. 
David Vilar, Jia Xu, Luis Fernando D'Haro and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation (LREC), pages 697-702, Genova, 
Italy, May 2006. 
Billy T-M Wong. 2010. Semantic Evaluation of Ma-
chine Translation. Proceedings of the 7th Interna-
tional Conference on Language Resources and 
Evaluation (LREC), Valletta, Malta, May, 2010. 
Billy T-M Wong and Chunyu Kit. 2008. Word choice 
and Word Position for Automatic MT Evaluation.  
AMTA 2008 Workshop: MetricsMATR, 3 pages, 
Waikiki, Hawai'i, October, 2008. 
Billy T-M Wong and Chunyu Kit. 2009a. Meta-
Evaluation of Machine Translation on Legal Texts. 
Proceedings of the 22nd International Conference 
on the Computer Processing of Oriental Languag-
es (ICCPOL), pages 343-350, Hong Kong, March, 
2009. 
Billy Wong and Chunyu Kit. 2009b. ATEC: Automat-
ic Evaluation of Machine Translation via Word 
Choice and Word Order. Machine Translation, 
23(2):141-155. 
Zhibiao Wu and Martha Palmer. 1994. Verb Seman-
tics and Lexical Selection. Proceedings of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics, pages 133-138, Las Cruces, 
New Mexico. 
Parameters Values 
wtype 1        (exact match), 
 0.95  (stem / synonym / 
semantically close),
 0.15  (unmatch) 
Infomatch 0.34 
Infounmatch 0.26 
wpos 0.02 
worder 0.15 
e 1.1 
Limitdis 0.95 
LimitInfo 0.5 
? 0.5 
Table 1  Optimal parameter values for ATEC
 
364
