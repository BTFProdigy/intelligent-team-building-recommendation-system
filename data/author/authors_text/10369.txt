R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 612 ? 623, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Machine Translation Based on Constraint-Based 
Synchronous Grammar 
Fai Wong1, Dong-Cheng Hu1, Yu-Hang Mao1, 
Ming-Chui Dong2, and Yi-Ping Li2 
1
 Speech and Language Processing Research Center, 
Department of Automation, Tsinghua University, 100084 Beijing 
huangh01@mails.tsinghua.edu.cn  
{hudc, myh-dau}@mail.tsinghua.edu.cn 
2
 Faculty of Science and Technology of University of Macao, 
Av. Padre Tom?s Pereira S.J., Taipa, Macao 
{dmc, ypli}@umac.mo 
Abstract. This paper proposes a variation of synchronous grammar based on 
the formalism of context-free grammar by generalizing the first component of 
productions that models the source text, named Constraint-based Synchronous 
Grammar (CSG). Unlike other synchronous grammars, CSG allows multiple 
target productions to be associated to a single source production rule, which can 
be used to guide a parser to infer different possible translational equivalences 
for a recognized input string according to the feature constraints of symbols in 
the pattern. Furthermore, CSG is augmented with independent rewriting that al-
lows expressing discontinuous constituents in the inference rules. It turns out 
that such grammar is more expressive to model the translational equivalences of 
parallel texts for machine translation, and in this paper, we propose the use of 
CSG as a basis for building a machine translation (MT) system for Portuguese 
to Chinese translation. 
1   Introduction 
In machine translation, to analyze the structure deviations of languages pair hence to 
carry out the transformation from one language into another as the target translation is 
the kernel part in a translation system, and this requires a large amount of structural 
transformations in both grammatical and concept level. The problems of syntactic com-
plexity and word sense ambiguity have been the major obstacles to produce promising 
quality of translation. In order to overcome the obstacles and hence to improve the qual-
ity of translation systems, several alternative approaches have been proposed.  
As stated in [1], much of the theoretical linguistics can be formulated in a very 
natural manner as stating correspondences between layers of representations. In simi-
lar, many problems in natural language processing, in particular language translation 
and grammar rewriting systems, can be expressed as transduction through the use of 
synchronous formalisms [2,3,4,5,6]. Recently, synchronous grammars are becoming 
more and more popular for the formal description of parallel texts representing trans-
lations for the same document. The underlying idea of such formalisms is to combine 
two generative devices through a pairing of their productions in such a way that right 
 Machine Translation Based on Constraint-Based Synchronous Grammar 613 
hand side non-terminal symbols in the paired productions are linked. However, such 
formalisms are less expressive and unable to express mutual translations that have 
different lengths and crossing dependencies. Moreover, synchronous formalisms do 
not deal with unification and feature structures, as in unification-based formalisms, 
that give patterns additional power for describing constraints on features. For exam-
ples, Multiple Context-Free Grammar [4], where functions are engaged to the non-
terminal symbols in the productions to further interpreting the symbols in target gen-
eration. In [7], Inversion Transduction Grammar (ITG) has been proposed for simul-
taneously bracketing parallel corpora as a variant of Syntax Directed translation 
schema [8]. But these formalisms are lacked of expressive to describe discontinuous 
constituents in linguistic expression. Generalized Multitext Grammar (GMTG) pro-
posed by [5,9] is constructed by maintaining two sets of productions as components, 
one for each language, for modeling parallel texts. Although GMTG is more expres-
sive and can be used to express as independent rewriting, the lack of flexibility in the 
way to describe constraints on the features associated with a non-terminal makes it 
difficult to the development of practical MT system. 
In this paper, a variation of synchronous grammar, Constraint-Based Synchronous 
Grammar (CSG), is proposed based on the formalism of context-free grammar. 
Through the use of feature structures as that in unification-based grammar, the first 
component of productions in CSG, that describes the sentential patterns for source 
text, is generalized while the corresponding target rewriting rules for each production 
are grouped in a vector representing the possible translation patterns for source pro-
duction. The choice of rule for target generation is based on the constraints on fea-
tures of non-terminal symbols in pattern. Our motivation is three-fold. First, synchro-
nous formalisms have been proposed for modeling of parallel text, and such algo-
rithms can infer the synchronous structures of texts for two different languages 
through the grammar representation of their syntactic deviations. That is quite suitable 
for use in the analysis of languages pair in the development of MT system. Secondly, 
by augmented the synchronous models with feature structures can enhance the pattern 
with additional power in describing gender, number, agreement, etc. Since the de-
scriptive power of unification-based grammars is considerably greater than that of 
classical CFG [10,11]. Finally, by retaining the notational and intuitive simplicity of 
CFG, we can enjoy both a grammar formalism with better descriptive power than 
CFG and more efficient parsing and generation algorithm controlled by the feature 
constraints of symbols hence to achieve the purposes of word sense and syntax dis-
ambiguation. 
2   Constraint-Based Synchronous Grammars 
Constraint-Based Synchronous Grammars (CSG) is defined by means of the syntax of 
context-free grammar (CFG) to the case of synchronous. The formalism consists of a 
set of generative productions and each production is constructed by a pair of CFG 
rules with zero and more syntactic head and link constraints for the non-terminal 
symbols in patterns. In a similar way, the first component (in right hand side of pro-
ductions) represents the sentential patterns of source language, while the second com-
ponent represents the translation patterns in target language, called source and target 
component respectively in CSG. Unlike other synchronous formalisms, the target 
614 F. Wong et al 
component of production consists of one or more generative rules associated with 
zero or more controlled conditions based on the features of non-terminal symbols of 
source rule for describing the possible generation correspondences in target transla-
tion. In such a way, the source components in CSG are generalized by leaving the task 
of handling constraints on features in target component, so this also helps to reduce 
the grammar size. For example, following is one of the productions used in the MT 
system for Portuguese to Chinese translation: 
S ? NP1 VP* NP2 PP NP3 {[NP1 VP1 NP3 VP2 NP2;VPcate=vb1, 
VPs:sem = NP1sem, VPio:sem=NP2sem,VPo:sem=NP3sem], 
[NP1 VP NP3 NP2 ;VP =vb0,VPs:sem =NP1sem, 
VPio:sem=NP2sem]} 
(1) 
The production has two components beside the reduced syntactic symbol on left 
hand side, the first modeling Portuguese and the second Chinese. The target compo-
nent in this production consists of two generative rules maintained in vector, and each 
of which is engaged with control conditions based on the features of symbols from the 
source component, and this is used as the selectional preferences in parsing. These 
constraints, in the parsing/generation algorithm, are used for inferring, not only, the 
structure of input to dedicate what structures are possible or probable, but also the 
structure of output text for target translation. For example, the condition expression: 
VPcate=vb1, VPs:sem=NP1sem, VPio:sem=NP2sem, VPo:sem=NP3sem, specifies if the senses of 
the first, second and the third nouns (NPs) in the input strings matched to that of the 
subject, direct and indirect objects governed by the verb, VP, with the category type 
of vb1. Once the condition gets satisfied, the source structure is successfully recog-
nized and the corresponding structure of target language, NP1 VP1 NP3 VP2 NP2, is 
determined also. 
Non-terminal symbols in source and target rules are linked if they are given the 
same index ?subscripts? for case of multiple occurrences, such as NPs in the produc-
tion: S ? NP1 VP NP2 PP NP3 [NP1 VP* NP3 NP2], otherwise symbols that appear 
only once in both the source and target rules, such as VPs, are implicitly linked to 
give the synchronous rewriting. Linked non-terminal must be derived from a se-
quence of synchronized pairs. Consider the production: S ? NP1 VP NP2 PP NP3 
[NP1 VP* NP3 NP2], the second NP (NP2) in the source rule corresponds to the third 
NP (NP2) in the target rule, the third NP (NP3) in source rule corresponds to the sec-
ond NP (NP3) in target pattern, while the first NP (NP1) and VP correspond to each 
other in both source and target rules. The symbol marked by an ?*? is designated as 
head element in pattern, this allows the features of designated head symbol propagate 
to the reduced non-terminal symbol in the left hand side of production rule, hence to 
achieve the property of features inheritance in CSG formalism. The use of features 
structures associated to non-terminal symbols will be discussed in the later section in 
this paper.  
In modeling of natural language, in particular for the process of languages-pair, the 
treatment for non-standard linguistic phenomena, i.e. crossing dependencies, discon-
tinuous constituents, etc., is very important due to the structure deviations of two 
different languages, in particular for languages from different families such as Portu-
guese and Chinese [12,13]. Linguistic expressions can vanish and appear in transla-
tion. For example, the preposition (PP) in the source rule does not show up in any of 
 Machine Translation Based on Constraint-Based Synchronous Grammar 615 
the target rules in Production (1). In contrast, Production (2) allows the Chinese char-
acters of ??? and ??? to appear in the target rules for purpose to modify the noun 
(NP) together with the quantifier (num) as the proper translation for the source text. 
This explicitly relaxes the synchronization constraint, so that the two components can 
be rewritten independently.  
NP ? num NP* {[num ? NP; NPsem=SEM_book],  
[num ? NP; NPsem=SEM_automobile]} (2) 
A remarkable strength of CSG is its expressive power to the description of discon-
tinuous constituents. In Chinese, the use of combination words that discontinuously 
distributed in a sentence is very common. For example, take the sentences pair [?Ele 
vendeu-me todas as uvas. (He sell me all the grapes.)?, ???????????? ?]. 
The Chinese preposition ??? and the verb ????? should be paired with the Portu-
guese verb ?vendeu?, and this causes a fan-out1 and discontinuous constituent in the 
Chinese component. The following fragment of CSG productions represents such 
relationships. 
S ? NP1 VP* NP2 NP3 {[NP1 VP1 NP3 VP2 NP2 ; VPcate=vb0,?],..} 
VP ? vendeu* {[?, ??? ;?]} 
(3) 
(4) 
In Production (3), the corresponding discontinuous constituents of VP (from source 
rule) are represented by VP1 and VP2 respectively in the target rule, where the ?super-
scripts? are added to indicate the pairing of the VP in target component. The corre-
sponding translation constituents in the lexicalized production are separated by com-
mas representing the discontinuity between constituents ??? and ????? in target 
translation. During the rewriting phase, the corresponding constituents will be used to 
replace the syntactic symbols in pattern rule. 
3   Definitions 
Let L be a context-free language defined over terminal symbol VT and generated by a 
context-free grammar G using non-terminal symbol VN disjointed with VT, starting 
symbol S, and productions of the form A ? w where A is in VN and w in (VN?VT)*. 
Let Z as a set of integers, each non-terminal symbol in VN is assigned with an integer, 
?(VN) = {W? | W ? VN, ? ? Z}. The elements of ?(VN) are indexed non-terminal sym-
bols. Now, we extend to include the set of terminal symbols VT? as the translation in 
target language, disjoint from VT, (VT VT?=?). Let R = {r1, ?, rn | ri? (?(VN)?VT?), 1 ? 
i ? n} be a finite set of rules, and C = {c1, ?, cm} be a finite set of constraints over the 
associated features of (?(VN)?VT), where the features of non-terminal ?(VN), the syn-
tactic symbols, are inherited from the designated head element during rule reduction. 
A target rule is defined as pair [r?R*, c?C*] in ?, where ? = R*?C* in form of [r, c]. 
Now, we define ?(?i) to denote the number of conjunct features being considered in 
                                                          
1 We use this term for describing a word where its translation is paired of discontinuous words 
in target language, e.g. ?vendeu[-pro] [NP]? in Portuguese gives similar English translation as 
?sell [NP] to [pro]?, so ?vendeu?, in this case, is corresponding to ?sell? and ?to?. 
616 F. Wong et al 
the associated constraint, hence to determine the degree of generalization for a con-
straint. Therefore, the rules, ?i and ?j, are orderable, ?i p  ?j, if ?(?i) ? ?(?j) (or ?i f  ?j, 
if ?(?i) < ?(?j)). For ?i p  ?j (?(?i) ? ?(?j)), we say, the constraint of the rule, ?i, is 
more specific, while the constraint of ?j is more general. In what follows, we consider 
a set of related target rules working over the symbols, w?, on the RHS of production A 
? w?, the source rule, where w? ? ?(VN)?VT. All of these non-terminals are co-
indexed as link. 
Definition 1: A target component is defined as a ordered vector of target rules in ?  
having the form ? = {?1, ?, ? q}, where 1 ? i ? q to denote the i-th tuple of ?. The 
rules are being arranged in the order of ?1 p ?2p  ?p ?q. 
In rule reduction, the association conditions of the target rules are used for investi-
gating the features of corresponding symbols in source rules, similar to that of feature 
unification, to determine if the active reduction successes or not. At the mean while, 
this helps in determining the proper structure as the target correspondence.  
Definition 2: A Constraint-Based Synchronous Grammar (CSG) is defined to be  
5-tuple G = (VN, VT, P, CT, S) which satisfies the following conditions: 
? VN is a finite set of non-terminal symbols; 
? VT is a finite set of terminal symbols which is disjoint with VN; 
? CT is a finite set of target components; 
? P is a finite set of productions of the form A ? ? ?, where ? ? (?(VN)?VT)* 
and, ? ? CT, the non-terminal symbols that occur from both the source and target 
rules are linked under the index given by ?(VN)2. 
? S ? VN is the initial symbol. 
For example, the following CSG productions can generate both of the parallel texts 
[?Ele deu um livro ao Jos?. (He gave a book to Jos?)?, ??????????] and [?Ele 
comprou um livro ao Jos?. (He bought a book from Jos?)?, ???????????]: 
S ? NP1 VP* NP2 PP NP3 {[NP1 VP1 NP3 VP2 NP2;VPcate=vb1, 
VPs:sem = NP1sem, Pio:sem=NP2sem,VPo:sem=NP3sem], 
[NP1 VP NP3 NP2 ;VP =vb0,VPs:sem =NP1sem, 
VPio:sem=NP2sem]} 
(5) 
 
VP ? v3 {[v ; ?]} (6) 
NP ? det NP* {[NP ; ?]} (7) 
NP ? num NP* {[num ?NP; NPsem=SEM_book]} (8) 
                                                          
2
  Link constraints are dedicated by the symbols indices, which is trivially for connecting the 
corresponding symbols between the source and target rules. Hence, we assume, without loss 
of generality, that index is only given to the non-terminal symbols that have multiple occur-
rences in the production rules. It is assumed that ?S ? NP1 VP2 PP3 NP4 {NP1 VP21 NP4 
VP22}? implies ?S ? NP1 VP PP NP2 {NP1 VP1 NP2 VP2}?. 
3
  Similar for the designation of head element in productions, the only symbol from the RHS of 
production will inherently be the head element. Thus, no head mark ?*? is given for such 
rules, and we assume that ?VP ? v*? implies ?VP ? v?. 
 Machine Translation Based on Constraint-Based Synchronous Grammar 617 
NP ? n {[n ; ?]} (9) 
NP ? pro {[pro ; ?]} (10) 
PP ? p {[p ; ?]} (11) 
n ? Jos? {[?? ; ?]}| livro {[? ; ?]} (12) 
pro ? ele {[? ; ?]} (13) 
v ? deu{[?? ; ?]} | comprou {[?, ?? ;?]} (14) 
num ? um {[? ; ?]} (15) 
p ? a {?} (16) 
det ? o {?} (17) 
A set P of productions is said to accept an input string s iff there is a derivation se-
quence Q for s using source rules of P, and any of the constraint associated with every 
target component in Q is satisfied4. Similarly, P is said to translate s iff there is a 
synchronized derivation sequence Q for s such that P accepts s, and the link con-
straints of associated target rules in Q is satisfied. The derivation Q then produces a 
translation t as the resulting sequence of terminal symbols included in the determined 
target rules in Q. The translation of an input string s essentially consists of three steps. 
First, the input string is parsed by using the source rules of productions. Secondly, the 
link constraints are propagated from source rule to target component to determine and 
build a target derivation sequence. Finally, translation of input string is generated 
from the target derivation sequence. 
3.1   Feature Representation 
In CSG, linguistic entities are modeled as feature structures which give patterns addi-
tional power for describing gender, number, semantic, attributes and number of the 
arguments required by a verb, and so on. These information are encoded in the com-
monly used attribute value matrices (AVMs), attached to each of the lexical and syn-
tactic symbols in CSG. This allows us to specify such as syntactic dependencies as 
agreement and sub-categorization in patterns. Unlike other unification-based gram-
mars [11,14], we do not carry out the unification in full, only interested conditions 
that are explicitly expressed in the rule constraints are tested and unified. Such unifi-
cation process can perform in constant time. The use of feature constraints has to be 
restricted to maintain the efficiency of parsing and generating algorithms, especially 
to the prevention from generating a large number of ambiguous structure candidates. 
The word selection in the target language can also be achieved by checking features. 
In the parsing and generating algorithm, the features information are propagated to the 
reduced symbol from the designated head element in pattern, hence to realize the 
mechanism of features inheritance. Features can either be put in lexical dictionary 
isolated from the formalism to make the work simpler to the construction of analytical 
grammar, or explicitly encoded in the pre-terminal rules as:  
                                                          
4
  If there is no any constraint associated to a target rule, during the parsing phase, the reduction 
of the source rule is assumed to be valid all the time.  
618 F. Wong et al 
Pro ? Jos?:[CAT:pro;NUM:sg;GEN:masc,SEM:hum] {[?? ; ?]} (18) 
n ? livro:[CAT:n;NUM:sg;GEN:masc;SEM:artifact+book] {[? ; ?]} (19) 
Where the features set is being bracketed, and separated by a semi-colon, the name 
and the value of a feature are delimited by a colon to represent the feature pair. An-
other way to enhance the CSG formalism is to apply the soft preferences other than 
hard constraints in the process of features unification. Our consideration is two-fold: 
first, we found that more than one combination of feature values engaged to a single 
lexical item is very common in the process of natural language, i.e. one word may 
have several translations according to the different senses and the pragmatic uses of 
the word, and this has been the problem of word senses disambiguation [15]. Sec-
ondly, the conventional feature unification method can only tell us if the process suc-
cesses or not. In case of a minor part of conditions get failed during the unification, all 
the related candidates are rejected without any flexibility to choosing the next prefer-
able or probable candidate. In order to resolve these problems, each feature structure 
is associates with a weight. It is then possible to rank the matching features according 
to the linear ordering of the weights rather than the order of lexical items expressed in 
grammars or dictionary. In our prototyping system, each symbol has its original 
weight, and according to preference measurement at the time in checking the feature 
constraints, a penalty is used to reduce from the weight to give the effective weight of 
associated features in a particular context. Features with the largest weight are to be 
chosen as the most preferable content. 
4   Application to Portuguese-Chinese MT 
CSG formalism can be parsed by any known CFG parsing algorithm including the 
Earley [16] and generalized LR algorithms [17] augmented by taking into account the 
features constraints and the inference of target structure. In the prototyping system, 
the parsing algorithm for our formalism is based on the generalized LR algorithm that 
we have development for MT system, since the method uses a parse table, it achieves 
a considerable efficiency over the Earley?s non-complied method which has to com-
pute a set of LR items at each stage of parsing [17]. Generalized LR algorithm was 
first introduced by Tomita for parsing the augmented Context-Free grammar that can 
ingeniously handle non-determinism and ambiguity through the use of graph-
structured stack while retaining much of the advantages of standard LR parsing5. It 
takes a shift-reduce approach using an extended LR parse table to guide its actions by 
allowing the multiple actions entries such as shift/reduce and reduce/reduce hence to 
handle the nondeterministic parse with pseudo-parallelism. In order to adapt to our 
formalism, we further extend the parse table by engaging with the features constraints 
and the target rules into the actions table. Our strategy is thus to parse the source rules 
of CSG productions through the normal shift actions proposed by the parsing table, 
while at the time reduce action to be fired, the associated conditions are checked to 
determine if the active reduction is a valid action or not depending on if the working 
symbols of patterns fulfill the constraints on features.  
                                                          
5
  Especially when the grammar is close to the LR grammars. 
 Machine Translation Based on Constraint-Based Synchronous Grammar 619 
4.1   The CSG Parse Table 
Fig. 1 shows an extended LR(1) parsing table for Productions (5)-(17)6 as constructed 
using the LR table construction method described in [18] extended to consider the 
rule components of productions by associating the corresponding target rules with 
constraints, which are explicitly expressed in table. The parsing table consists of two 
parts: a compact ACTION-GOTO table 7  and CSONTRAINT-RULE table. The 
ACTION-GOTO table s indexed by a state symbol s (row) and a symbols x ?VN?VT, 
including the end marker ???. The entry ACTION[s, x] can be one of the following: s 
n, r m, acc or blank. s n denotes a shift action representing GOTO[s, x]=n, defining 
the next state the parser should go to; r m means a reduction by the mth production 
located in the entry of CONSTRAINT-RULE in state s, and acc denotes the accept 
action and blank indicates a parsing error. The CONSTRAINT-RULE table is in-
dexed by state symbol s (row) and the number of productions m that may be applied 
for reduction in state s. The entry CONSTRAINT-RULE[s, m] consists of a set of 
involved productions together with the target rules and features constraints that are 
used for validating if the active parsing node can be reduced or not, then try to iden-
tify the corresponding target generative rule for reduced production. 
4.2   The CSG Parser  
In the parsing process, the algorithm operates by maintaining a number of parsing 
processes in parallel, each of which represents an individual parsed result, hence to 
handle the case of non-deterministic. In general, there are two major components in 
the process, shift(i) and reduce(i), which are called at each position i=0, 1, ?, n in 
an input string I = x1x2?xn. The shift(i) process with top of stack vertex v shifts on xi 
from its current state s to some successor state s? by creating a new leaf v?; estab-
lishing edge from v? to the top of stack v; and making v? as the new top of  
stack vertex.  
The reduce(i) executes a reduce action on a production p by following the chain 
ofparent links down from the top of stack vertex v to the ancestor vertex from which 
the process began scanning for p earlier, then popping intervening vertices off the 
stack. Now, for every reduction action in reduce(i), there exists a set C of ordered 
constraints, c1p ?p cm, with the production, each of which is associated with a target 
rule that may be the probable corresponding target structure for the production, de-
pending on whether the paired constraint gets satisfied or not according to the features 
of the parsed string p. Before reduction takes place, the constraints cj (1 ? j ? m) are 
tested in order started from the most specific one, the evaluation process stops once a 
positive result is obtained from evaluation. The corresponding target rule for the 
parsed string is determined and attached to the reduced syntactic symbol, which will 
be used for rewriting the target translation in phase of generation. At the mean while, 
the features information will be inherited from the designated head element of pro-
duction. The parsing algorithm for CSG formalism is given in Fig. 2. 
 
                                                          
6
  For simplicity, the productions used for building the parse table are deterministic, so no con-
flict actions such as shift/reduce and reduce/reduce appear in the parse table in Fig.1. 
7
  Original version introduced in [17] maintains two tables, ACTION and GOTO. 
620 F. Wong et al 
ACTIONs/GOTOs 
Ste 
p r
o  
n
u
m
 
n
 
v  d e
t 
p NP
 
VP
 
PP
 
S ? o a um
 
el
e 
Jo
s?
 
liv
ro
 
de
u 
co
m
pr
o
u
 Reduced Rules 
Constraints/Target 
Rules 
0 s8 s9 s10  s11  s7   s6  s5  s2 s1 s4 s3    
1               r1     (1) pro ? ele   {[? ; ?]} 
2              r1      (1) num ? um 
3                 r1   (1) n ? livro   {[? ; ?]} 
4                r1    (1) n ? Jos?   {[?? ; ?]} 
5            r1        (1) det ? o 
6           acc          
7    s14    s15          s12 s13  
8 r1                   (1) NP ? pro 
9 s8 s9 s10  s11  s16     s5  s2 s1 s4 s3    
10   r1                 (1) NP ? n 
11 s8 s9 s10  s11  s17     s5  s2 s1 s4 s3    
12                  r1  (1) v ? deu   {[?? ; ?]} 
13                   r1 (1) v ? comprou {[?, ?? ;?]} 
14    r1                (1) VP ? v 
15 s8 s9 s10  s11  s18     s5  s2 s1 s4 s3    
16       r1             (1) NP ? num NP* {[num ? NP; NPsem=SEM_book]} 
17       r1             (1) NP ? det NP*   {[NP ; ?]} 
18      s21   s20    s19        
19             r1       (1) p ? a 
20 s8 s9 s10  s11  s22     s5  s2 s1 s4 s3    
21      r1              (1) PP ? p 
22          r1          (1) S ? NP1 VP* NP2 PP NP3 {[...]} 
 
Fig. 1. Extended LR(1) parse table 
PARSE(grammar,x1 ? xn) 
x
n+1? ? 
Ui?? (0 ? i ? n) 
U0?v0 
for each terminal symbol xi (1 ? i ? n) 
P?? 
for each node v ? Ui-1 
P?P?v 
if ACTION[STATE(v),xi] = ?shift s??, SHIFT(v,s?) 
for each ?reduce p??ACTION[STATE(v),xi], REDUCE(v,p) 
if ?acc??ACTION[STATE(v),xi], accept 
if Ui=?, reject 
 
SHIFT(v,s) 
if v??Ui s.t. STATE(v?)=s and ANCESTOR(v?,1)=v and state 
transition ?(v,x)=v? 
do nothing 
 
 Machine Translation Based on Constraint-Based Synchronous Grammar 621 
else 
create a new node v? 
s.t. STATE(v?)=s and ANCESTOR(v?,1)=v and state tran-
sition ?(v,x)=v? 
Ui?Ui?v? 
 
REDUCE(v,p) 
for each possible reduced parent v1??ANCESTOR(v,RHS(p)) 
if UNIFY(v,p)=?success? 
s? ? GOTO(v1?,LHS(p)) 
if node v??Ui-1 s.t. STATE(v?)=s? 
if ?(v1?, LHS(p))=v? 
do nothing 
else 
if node v2??ANCESTOR(v?,1) 
let v
c
? s.t. ANCESTOR(v
c
?,1)=v1? and STATE(vc?)=s? 
for each ?reduce p? ? ACTION[STATE(v
c
?),xi] 
REDUCE(v
c
?,p) 
else 
if v??P 
let v
c
? st. ANCESTOR(v
c
?,1)=v1? and STATE(vc?)=s? 
for each ?reduce p? ? ACTION[STATE(v
c
?),xi] 
REDUCE(v
c
?,p) 
else 
create a new node v
n
 
s.t. STATE(v
n
)=s? and ANCESTOR(v
n
,1)=v1? and  
state transition ?(v
n
,x)=v1? 
Ui-1?Ui-1?vn 
else current reduction failed 
 
UNIFY(v,p) 
for ?constraint cj? ? CONSTRAINT(STATE(v)) (1 ? j ? m, 
c1p ?p cm) 
if ?(cj,p)=?true?  (?(?,p)=?true?) 
TARGET(v)?j 
return ?success? 
Fig. 2. Modified generalized LR Parsing algorithm 
The parser is a function of two arguments PARSE(grammar, x1 ? xn), where the 
grammar is provided in form of parsing table. It calls upon the functions SHIFT(v, s) 
and REDUCE(v, p) to process the shifting and rule reduction as described. The 
UNIFY(v, p) function is called for every possible reduction in REDUCE(v, p) to ver-
ify the legal reduction and select the target rule for the source structure for synchroni-
zation. The function TARGET(v) after unification passed is to dedicate the jth target 
rule as correspondence. 
622 F. Wong et al 
4.3   Translation as Parsing 
Our Portuguese-to-Chinese translation (PCT) system is a transfer-based translation 
system by using the formalism of Constraint-Based Synchronous Grammar (CSG) as its 
analytical grammar. Unlike other transfer-based MT systems that the major compo-
nents: analysis, transfer and generation are carried out individually in pipeline by using 
different sets of representation rules to achieve the tasks of structure analysis and trans-
formation [19], in PCT, only a single set of CS grammar is used to dominate the transla-
tion task. Since the structures of parallel languages are synchronized in formalism, as 
well as the deviations of their structures are also captured and described by the gram-
mar. Hence, to the translation of an input text, it essentially consists of three steps. First, 
for an input sentence s, the structure of string is analyzed by using the rules of source 
components from the CSG productions; by using the augmented generalized LR parsing 
algorithm as described. Secondly, the link constraints that are determined during the rule 
reduction process are propagated to the corresponding target rules R (as selection of 
target rules) to construct a target derivation sequence Q. And finally, based on the deri-
vation sequence Q, translation of the input sentence s is generated by referencing the set 
of generative rules R that attached to the corresponding constituent nodes in the parsed 
tree, hence to realize the translation in target language. 
5   Conclusion 
In this paper, we have proposed a variation of synchronous grammar based on the 
syntax of context-free grammar, called Constraint-based Synchronous Grammar 
(CSG). The source components of CSG are being generalized for representing the 
common structure of language. Different from other synchronous grammars, each 
source rule is associated with a set of target productions, where each of the target 
rules is connected with a constraint over the features of source patterns. The set of 
target rules are grouped and maintained in a vector ordered by the specificity of con-
straints. The objective of this formalism is to allow parsing and generating algorithms 
to inference different possible translation equivalences for an input sentence being 
analyzed according to the linguistic features. We have presented a modified general-
ized LR parsing algorithm that has been adapted to the parsing our formalism that we 
have developed for analyzing the syntactic structure of Portuguese in the machine 
translation system.  
References 
1. Rambow, O., Satta, G.: Synchronous Models of Language. In Proceedings of 34th Annual 
Meeting of the Association for Computational Linguistics, University of California, Santa 
Cruz, California, USA, Morgan Kaufmann (1996) 116-123. 
2. Lewis, P.M., Stearns, R.E.: Syntax-directed transduction. Journal of the Association for 
Computing Machinery, 15(3), (1968) 465-488. 
3. Shieber, S.M., Schabes, Y.: Synchronous Tree Adjoining Grammar. Proceedings of the 
13th International Conference on Computational Linguistic, Helsinki (1990) 
4. Seki, H., Matsumura, T., Fujii, M., Kasami, T.: On multiple context-free grammars. Theo-
retical Computer Science, 88(2) (1991) 191-229 
 Machine Translation Based on Constraint-Based Synchronous Grammar 623 
5. Melamed, I.D.: Multitext Grammars and Synchronous Parsers. In Proceedings of 
NAACL/HLT 2003, Edmonton, (2003) 79-86 
6. Wong, F., Hu, D.C., Mao, Y.H., Dong, M.C. A Flexible Example Annotation Schema: 
Translation Corresponding Tree Representation. In Proceedings of the 20th International 
Conference on Computational Linguistics, Switzerland, Geneva (2004) 1079-1085 
7. Wu, D.: Grammarless extraction of phrasal translation examples from parallel texts. In 
Proceedings of TMI-95, Sixth International Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, v2, Leuven Belgium (1995) 354-372 
8. Aho, A.V., Ullman, J.D.: Syntax Directed Translations and the Pushdown Assembler. 
Journal of Computer and System Sciences, 3, (1969) 37-56 
9. Melamed, I.D., Satta. G., Wellington, B.: Generalized Multitext Grammars. In Proceed-
ings of 42th Annual Meeting of the Association for Computational Linguistics, Barcelona, 
Spain (2004) 661-668 
10. Kaplan, R.M., Bresnan, J.: Lexical-Functional Grammar: A Formal System for Grammati-
cal Representation. In Joan Bresnan, The Mental Representation of Grammatical Rela-
tions, Cambridge, Mass, MIT Press, (1982) 173-281 
11. Kaplan, R.M.: The Formal Architecture of Lexical-Functional Grammar. Information Sci-
ence and Engineering, 5, (1989) 30-322 
12. Wong, F., Mao, Y.H.: Framework of Electronic Dictionary System for Chinese and Ro-
mance Languages. Automatique des Langues (TAL), 44(2), (2003) 225-245 
13. Wong, F., Mao, Y.H., Dong, Q.F., Qi, Y.H.: Automatic Translation: Overcome the Barri-
ers between European and Chinese Languages. In Proceedings (CD Version) of First In-
ternational UNL Open Conference, SuZhou China (2001) 
14. Pollard, C., Sag, I.: Head-Driven Phrase Structure Grammar. University of Chicago Press, 
(1994) 
15. Ide, N., Veronis, J.: Word Sense Disambiguation: The State of the Art. Computational 
Linguistics, 24, (1), (1998) 1-41 
16. Earley, J.: An Efficient Context-Free Parsing Algorithm. CACM, 13(2), (1970) 94-102 
17. Tomita, M.: Computational Linguistics, 13(1-2), (1987) 31-46 
18. Aho, A.V., Sethi, R., Ullman, J.D.: Compiler: Principles, Techniques and Tools. Addison-
Wesley, (1986) 
19. Hutchins, W.J., Somers, H.L.: An Introduction to Machine Translation. Academic  
Press, (1992) 
Chinese Tagging Based on Maximum Entropy Model 
Ka Seng Leong 
Faculty of Science and Technology of 
University of Macau 
Av. Padre Tom?s Pereira, Taipa,  
Macau, China 
ma56538@umac.mo 
Fai Wong 
Faculty of Science and Technology of 
University of Macau, INESC Macau 
Av. Padre Tom?s Pereira, Taipa,  
Macau, China 
derekfw@umac.mo 
Yiping Li 
Faculty of Science and Technology of 
University of Macau 
Av. Padre Tom?s Pereira, Taipa,  
Macau, China 
ypli@umac.mo 
Ming Chui Dong 
Faculty of Science and Technology of 
University of Macau, INESC Macau 
Av. Padre Tom?s Pereira, Taipa,  
Macau, China 
dmc@inesc-macau.org.mo 
 
 
Abstract 
In the Fourth SIGHAN Bakeoff, we took 
part in the closed tracks of the word 
segmentation, part of speech (POS)  
tagging and named entity recognition (NER) 
tasks. Particularly, we evaluated our word 
segmentation model on all the corpora, 
namely Academia Sinica (CKIP), City 
University of Hong Kong (CITYU), 
University of Colorado (CTB), State 
Language Commission of P.R.C. (NCC) 
and Shanxi University (SXU). For POS 
tagging and NER tasks, our models were 
evaluated on CITYU corpus only. Our 
models for the evaulation are based on the 
maximum entropy approach, we 
concentrated on the word segmentation 
task for the bakeoff and our best official 
results on all the corpora for this task are 
0.9083 F-score on CITYU, 0.8985 on 
CKIP, 0.9077 on CTB, 0.8995 on NCC and 
0.9146 on SXU. 
1 Introduction 
In the Fourth SIGHAN Bakeoff, besides providing 
the evaluation tasks for the word segmentation and 
NER, it also introduced another important evalua-
tion task, POS tagging for Chinese language. In 
this bakeoff, our models built for the tasks are sim-
ilar to that in the work of Ng and Low (2004). The 
models are based on a maximum entropy frame-
work (Ratnaparkhi, 1996; Xue and Shen, 2003). 
They are trained on the corpora for the tasks from 
the bakeoff. To understand the model, the imple-
mentation of the models is wholly done ourselves. 
We used Visual Studio .NET 2003 and C++ as the 
implementation language. The Improved Iterative 
Scaling (IIS) (Pietra et al, 1997) is used as the pa-
rameter estimation algorithm for the models. We 
tried all the closed track tests of the word segmen-
tation, the CITYU closed track tests for POS tag-
ging and NER. 
2 Maximum Entropy 
In this bakeoff, our basic model is based on the 
framework described in the work of Ratnaparkhi 
(1996) which was applied for English POS tagging. 
The conditional probability model of the 
framework is called maximum entropy (Jaynes, 
1957). Maximum entropy model is a feature-based, 
probability model which can include arbitrary 
number of features that other generative models 
like N-gram model, hidden Markov model (HMM) 
(Rabiner, 1989) cannot do. The probability model 
can be defined over X ? Y, where X is the set of 
138
Sixth SIGHAN Workshop on Chinese Language Processing
possible histories and Y is the set of allowable 
futures or classes. The conditional probability of 
the model of a history x and a class y is defined as 
 
 ( , )
( | ) ( )
if x y
iip y x Z x? ?
?
??   (1) 
 
 
( , )( ) if x yi
y i
Z x? ????
  (2) 
 
where ? is a parameter which acts as a weight for 
the feature in the particular history. The equation 
(1) states that the conditional probability of the 
class given the history is the product of the weight-
ings of all features which are active under the con-
sideration of (x, y) pair, normalized over the sum 
of the products of all the classes. The normaliza-
tion constant is determined by the requirement that 
( | ) 1
y
p y x? ??
 for all x. 
To find the optimized parameters ? of the condi-
tional probability is one of the important processes 
in building the model. This can be done through a 
training process. The parameter estimation algo-
rithm used for training is Improved Iterative Scal-
ing (IIS) (Pietra et al, 1997) in our case. In train-
ing the models for this bakeoff, the training data is 
given in the form of a sequence of characters (for 
the tasks of word segmentation and NER) or words 
(POS tagging) and their classes (tags), the parame-
ters ? can be chosen to maximize the likelihood of 
the training data using p: 
 
( , )
1 1 1
1( ) ( , ) ( )
j i if x yn n m
i i j
i i j
L p p x y Z x? ?
?
? ? ?
? ?? ? ?
(3) 
 
But of course, the success of the model depends 
heavily on the selection of features for a particular 
task. This will be described in Section 5. 
3 Chinese Word Segmenter 
We concentrated on the word segmentation task in 
this bakeoff. For the Chinese word segmenter, it is 
based on the work that treats Chinese word seg-
mentation as tagging (Xue and Shen, 2003; Ng and 
Low, 2004). Given a Chinese sentence, it assigns a 
so-called boundary tag to each Chinese character 
in the sentence. There are four possible boundary 
tags: S for a character which is a single-character 
word, B for a character that is the first character of 
a multi-character word, E for a character that is the 
last character of a multi-character word and M for 
a character that is neither the first nor last of a mul-
ti-character word. With these boundary tags, the 
word segmentation becomes a tagging problem 
where each character in Chinese sentences is given 
one of the boundary tags which is the most proba-
ble one according to the conditional probability 
calculated by the model. And then sequences of 
characters are converted into sequences of words 
according to the tags. 
4 POS Tagger and Named Entity Recog-
nizer 
For the POS tagging task, the tagger is built based 
on the work of Ratnaparkhi (1996) which was ap-
plied for English POS tagging. Because of the time 
limitation, we could only try to port our imple-
mented maximum entropy model to this POS tag-
ging task by using the similar feature set (discussed 
in Section 5) for a word-based POS tagger as in the 
work of Ng and Low (2004). By the way, besides 
porting the model to the POS tagging task, it was 
even tried in the NER task by using the same fea-
ture set (discussed in Section 5) as used for the 
word segmentation in order to test the performance 
of the implemented model. 
The tagging algorithm for these two tasks is bas-
ically the same as used in word segmentation. Giv-
en a word or a character, the model will try to as-
sign the most probable POS or NE tag for the word 
or character respectively. 
5 Features 
To achieve a successful model for any task by us-
ing the maximum entropy model, an important step 
is to select a set of useful features for the task. In 
the following, the feature sets used in the tasks of 
the bakeoff are discussed. 
5.1 Word Segmentation Features 
The feature set used in this task is discussed in our 
previous work (Leong et al, 2007) which is cur-
rently the best in our implemented model. They are 
the unigram features: C-2, C-1, C0, C1 and C2, bi-
gram features: C-2C-1, C-1C0, C0C1, C1C2 and C-1C1 
where C0 is the current character, Cn (C-n) is the 
139
Sixth SIGHAN Workshop on Chinese Language Processing
character at the nth position to the right (left) of the 
current character. For example, given the character 
sequence ??????? (Victoria Harbour), while 
taking the character ??? as C0, then C-2 = ???, C-
1C1 = ????, etc. The boundary tag (S, B, M or E) 
feature T-1 is also applied, i.e., the boundary tag 
assigned to the previous character of C0. And the 
last feature WC0: This feature captures the word 
context in which the current character is found. It 
has the format ?W_C0?. For example, the character 
??? is a character of the word ???????. 
Then this will give the feature WC0 = ??????
_??. 
5.2 POS Tagging Features 
For this task, because of the time limitation as 
mentioned in the previous section, we could only 
port our implemented model by using a part of the 
feature set which was used in the word-based tag-
ger discussed in the work of Ng and Low (2004). 
The feature set includes: Wn (n = -2 to 2), WnWn+1 
(n = -2, -1, 0, 1), W-1W1, POS(W-2), POS(W-1), 
POS(W-2)POS(W-1) where W refers to a word, POS 
refers to the POS assigned to the word and n refers 
to the position of the current word being consi-
dered. For example, while considering this sen-
tence taken from the POS tagged corpus of CITYU: 
???/Ng  ??/Ac  ???/Nc  ??/Dc  ??
/Vt? (Hong Kong S.A.R. is established), taking ??
??? as W0, then W-2 = ????, W-1W1 = ??? ?
??, POS(W-2) = ?Ng?, POS(W-2)POS(W-1) = ?Ac 
Dc?, etc. 
5.3 Named Entity Recognition Features 
For the NER task, we directly used the same fea-
ture set as for the word segmentation basically. 
However, because the original NE tagged corpus is 
presented in two-column format, where the first 
column consists of the character and the second is 
a tag, a transformation which is to transform the 
original corpus to a sentence per line format before 
collecting the features or other training data is 
needed. This transformation actually continues to 
read the lines from the original corpus, whenever a 
blank line is found, a sentence of characters with 
NE tags can be formed. 
After that, the features collected are the unigram 
features: C-2, C-1, C0, C1 and C2, bigram features: 
C-2C-1, C-1C0, C0C1, C1C2 and C-1C1, NE tag fea-
tures: T-1, WC0 (this feature captures the NE con-
text in which the current character is found) where 
T-1 refers to the NE tag assigned to the previous 
character of C0, W refers to the named entity. So 
similar to the explanation of features of word seg-
mentation, for example, given the sequence from 
the NER tagged corpus of CITYU:  ??/N ?/N ?
/B-LOC ? /I-LOC ? /N? (One Chinese), while 
taking the character ??? as C0, then C-2 = ???, C-
1C1 = ????, WC0 = ??????, etc. 
For all the experiments conducted, training was 
done with a feature cutoff of 1. 
6 Testing 
For word segmentation task, during testing, given a 
character sequence C1 ? Cn, the trained model will 
try to assign a boundary tag to each character in the 
sequence based on the probability of the boundary 
tag calculated. Then the sequence of characters is 
converted into sequence of words according to the 
tag sequence t1 ? tn. But if each character was just 
assigned the boundary tag with the highest proba-
bility, invalid boundary tag sequences would be 
produced and wrong word segmentation results 
would be obtained. In particular, known words that 
are in the dictionary of the training corpus are 
segmented wrongly because of these invalid tag 
sequences. In order to correct these, the invalid 
boundary tag sequences are collected, such as for 
two-character words, they are ?B B?, ?B S?, ?M S?, 
?E E?, etc., for three-character words, they are ?B 
E S?, ?B M S?, etc., and for four-character words, 
they are ?B M M S?, ?S M M E?, etc. With these 
invalid boundary tag sequences, some post correc-
tion to the word segmentation result can be tried. 
That is after the model tagger has done the tagging 
for a Chinese sentence every time, the invalid 
boundary tag sequences will be searched within the 
preliminary result given by the tagger. When the 
invalid boundary tag sequence is found, the charac-
ters corresponding to that invalid boundary tag se-
quence will be obtained. After, the word formed by 
these characters is looked up to see if it is indeed a 
word in the dictionary, if it is, then the correction is 
carried out. 
Another kind of post correction to the word 
segmentation result is to make some guessed cor-
rection for some invalid boundary tag sequences 
such as ?B S?, ?S E?, ?B B?, ?E E?, ?B M S?, etc. 
That is, whenever those tag sequences are met 
140
Sixth SIGHAN Workshop on Chinese Language Processing
within the preliminary result given by the model 
tagger, they will be corrected no matter if there is 
word in the dictionary formed by the characters 
corresponding to the invalid boundary tag se-
quence. 
We believe that similar post correction can be 
applied to the NER task. For example, if such NE 
tag sequences ?B-PER N?, ?N I-PER N?, etc. oc-
cur in the result, then the characters corresponding 
to the invalid NE tag sequence can be obtained 
again and looked up in the named entity dictionary 
to see if they really form a named entity. However, 
we did not have enough time to adapt this for the 
NER task finally. Therefore, no such post correc-
tion was applied for the NER task in this bakeoff 
finally. 
7 Evaluation Results 
We evaluated our models in the closed tracks of 
the word segmentation, part of speech (POS)  
tagging and named entity recognition (NER) tasks. 
Particularly, our word segmentation model was 
evaluated on all the corpora, namely Academia 
Sinica (CKIP), City University of Hong Kong 
(CITYU), University of Colorado (CTB), State 
Language Commission of P.R.C. (NCC) and 
Shanxi University (SXU). For POS tagging and 
NER tasks, our models were evaluated on the 
CITYU corpus only. Table 1 shows our official 
results for the word segmentation task in the 
bakeoff. The columns R, P and F show the recall, 
precision and F-score respectively. 
 
Run_ID R P F 
cityu_a 0.9221 0.8947 0.9082 
cityu_b 0.9219 0.8951 0.9083 
ckip_a 0.9076 0.8896 0.8985 
ckip_b 0.9074 0.8897 0.8985 
ctb_a 0.9078 0.9073 0.9075 
ctb_b 0.9077 0.9078 0.9077 
ncc_a 0.8997 0.8992 0.8995 
ncc_b 0.8995 0.8992 0.8994 
sxu_a 0.9186 0.9106 0.9145 
sxu_b 0.9185 0.9107 0.9146 
Table 1. Official Results in the Closed Tracks of 
the Word Segmentation Task on all Corpora 
 
We submitted a few runs for each of the tests of 
the corpora. Table 1 shows the best two runs for 
each of the tests of the corpora for discussion here. 
The run (a) applied only the post correction to the 
known words that are in the dictionary of the train-
ing corpus but are segmented wrongly because of 
the invalid boundary tag sequences. The run (b) 
applied also the guessed post correction for some 
invalid boundary tag sequences in the results as 
mentioned in Section 6. From the results above, it 
can be seen that the runs with the guessed post cor-
rection generally gave a little bit better perfor-
mance than those that did not apply. This shows 
that the guess somehow made some good guesses 
for some unknown words that appear in the testing 
corpora. 
Table 2 shows our official results for the POS 
tagging task. The columns A shows the accuracy. 
The columns IV-R, OOV-R and MT-R show the 
recall on in-vocabulary words, out-of-vocabulary 
words and multi-POS words (multi-POS words are 
the words in the training corpus and have more 
than one POS-tag in either the training corpus or 
testing corpus) respectively. The run (a) used the 
paramters set which was observed to be the 
optimal ones for the model in the training phase. 
The run (b) used the parameters set of the model in 
the last iteration of the training phase. 
 
Run_ID A IV-R OOV-R MT-R 
cityu_a 0.1890 0.2031 0.0550 0.1704 
cityu_b 0.2793 0.2969 0.1051 0.2538 
Table 2. Official Results in the Closed Track of the 
POS Tagging Task on the CITYU Corpus 
 
It can be seen that our results were unexpectedly 
low in accuracy. After releasing the results, we 
found that the problem was due to the encoding 
problem of our submitted result files. The problem 
probably occurred after the conversion from our 
Big5 encoded results to the UTF-16 encoded 
results which are required by the bakeoff. 
Therefore, we did the evaluation ourselves by 
running our POS tagger again, using the official 
evaluation program and the truth test set. Finally, 
our best result was 0.7436 in terms of accuracy but 
this was still far lower than the baseline (0.8425) of 
the CITYU corpus. This shows that the direct 
porting of English word-based POS tagging to 
Chinese is not effective. 
Table 3 shows our official results for the NER 
task. The columns R, P and F show the recall, 
precision and F-score respectively. Again, similar 
to the POS tagging task, the run (a) used the 
141
Sixth SIGHAN Workshop on Chinese Language Processing
paramters set which was observed to be the 
optimal ones for the model in the training phase. 
The run (b) used the parameters set of the model in 
the last iteration of the training phase. 
 
Run_ID R P F 
cityu_a 0.0874 0.1058 0.0957 
cityu_b 0.0211 0.0326 0.0256 
Table 3. Official Results in the Closed Track of the 
NER Task on the CITYU Corpus 
 
It can be seen that our results were again 
unexpectedly low in accuracy. The cause of such 
low accuracy results was due to parts of the wrong 
format of the submitted result files compared with 
the correct format of the result file. So like the 
POS tagging task, we did the evaluation ourselves 
by running our NE recognizer again. Finally, our 
best result was 0.5198 in terms of F-score but this 
was again far lower than the baseline (0.5955) of 
the CITYU corpus. This shows that the similar 
feature set for the word segmentation task is not 
effective for the NER task. 
8 Conclusion 
This paper reports the use of maximum entropy 
approach for implementing models for the three 
tasks in the Fourth SIGHAN Bakeoff and our re-
sults in the bakeoff. From the results, we got good 
experience and knew the weaknesses of our mod-
els. These help to improve the performance of our 
models in the future. 
Acknowledgements 
The research work reported in this paper was par-
tially supported by ?Fundo para o 
Desenvolvimento das Ci?ncias e da Tecnologia? 
under grant 041/2005/A. 
References 
Adwait Ratnaparkhi. 1996. A maximum entropy model 
for part-of-speech tagging, in Proceedings of Confe-
rence on Empirical Methods in Natural Language 
Processing, Philadelphia, USA, pages 133-142. 
Edwin Thompson Jaynes. 1957. Information Theory and 
Statistical Mechanics, The Physical Review, 106(4): 
620-630. 
Hwee Tou Ng, and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? Word-
based or character-based? In Proceedings of the 2004 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), Barcelona, Spain, 
pages 277-284. 
Ka Seng Leong, Fai Wong, Yiping Li, and Ming Chui 
Dong. 2007. Chinese word boundaries detection 
based on maximum entropy model, in Proceedings of 
the 11th International Conference on Enhancement 
and Promotion of Computational Methods in Engi-
neering and Science (EPMESC-XI), Kyoto, Japan. 
Lawrence Rabiner. 1989. A tutorial on hidden Markov 
models and selected applications in speech recogni-
tion, Proceedings of the IEEE, 77(2): 257?286. 
Nianwen Xue, and Libin Shen. 2003. Chinese word 
segmentation as LMR tagging, in Proceedings of the 
2nd SIGHAN Workshop on Chinese Language 
Processing, Sapporo, Japan, pages 176-179.  
Steven Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE 
transactions on pattern analysis and machine intelli-
gence, 19(4): 380?393. 
 
142
Sixth SIGHAN Workshop on Chinese Language Processing
