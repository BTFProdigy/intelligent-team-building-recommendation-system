NewsInEssence: A System For Domain-Independent,
Real-Time News Clustering and Multi-Document
Summarization
Dragomir R. Radev?y, Sasha Blair-Goldensohn?, Zhu Zhang?, Revathi Sundara Raghavany
?School of Information
yDepartment of EECS
University of Michigan
Ann Arbor, MI 48109
fradev,sashabg,zhuzhang,rsundarag@umich.edu
1. INTRODUCTION
NEWSINESSENCE is a system for finding, visualizing and sum-
marizing a topic-based cluster of news stories. In the generic sce-
nario for NEWSINESSENCE, a user selects a single news story from
a news Web site. Our system then searches other live sources of
news for other stories related to the same event and produces sum-
maries of a subset of the stories that it finds, according to parame-
ters specified by the user.
2. THE NEWSINESSENCE SYSTEM
NewsInEssence?s search agent, NewsTroll, runs in two phases.
First, it looks for related articles by traversing links from the page
containing the seed article. Using the seed article and any related
articles it finds in this way, the agent then decides on a set of key-
words for further search. In the second phase, it attempts to add
to the cluster of related articles by going to the search engines of
various news websites and using the keywords which it found in
the first phase as search terms.
In both phases, NewsTroll selectively follows hyperlinks with
the aim of reaching pages which contain related stories and/or fur-
ther hyperlinks to related stories pages.
Both general and site-specific rules help NewsTroll determine
which URLs are likely to be useful. Only if NewsTroll determines
that a URL is ?interesting?, will it go to the Internet to fetch the new
page. A more stringent set of rules are applied to determine whether
the URL is likely to be a news story itself. If so, the similarity
of its text to that of the original seed page is computed using an
IDF-weighted vector measure. If the similarity is above a certain
threshold, the page is considered to contain a related article and
added to the cluster. The user may use our web interface (Figure 2)
to adjust the similarity threshold used in a given search.
Using several levels of filtering, NewsTroll is able to screen out
large numbers web pages quite efficiently. The expensive opera-
tion of testing lexical similarity is reserved for the small number of
.
pages which NewsTroll finds interesting. Consequently, the agent
can return useful results in real time.
3. ANNOTATED SAMPLE RUN
The example begins when we find a news article we would like
to read more about. In this case we pick a story is about a break-
ing story regarding one of President-Elect Bush?s cabinet nominees
(see Figure 1).
We input the URL using the web interface of the NEWSINESSENCE
system, then select our search options, click ?Proceed? and wait for
our results (see Figure 2).
In response to the user query, NewsTroll begins looking for re-
lated articles linked from the chosen start page. In a selection from
the agent?s output log in Figure 3, we can see that it extracts and
tests links from the page, and decides to test one which looks like a
news article. We then see that it tests this article and determines it
to be related. This article is added to the initial cluster, from which
the list of top keywords is drawn.
In its secondary phase, NewsTroll inputs its keywords to the
search engines of news sites and lets them do the work of find-
ing stories. Since we have selected good keywords, most of the
links seen by NewsTroll in this part of the search are indeed related
articles (see Figure 4). Upon exiting, NewsTroll reports the num-
ber of links it has considered, followed, tested, and retrieved (see
Figure 4).
The system?s web interface reports its progress to the user in real
time and provides a link to the visualization GUI once the cluster
is complete (Figure 5). Using the GUI, the user can select which of
the articles to summarize (see Figures 6 and 7). Figure 8 shows the
output of the cluster summarizer.
4. FUTURE WORK
We are currently working on the integration of Cross-Document
structure theory (CST) [1] with NEWSINESSENCE. CST is used to
describe relations between textual units in multi-document clusters.
It is used for example to identify which portions of a cluster contain
background information, which sections are redundant, and which
ones contain additional information about an event.
5. REFERENCES
[1] Dragomir Radev. A common theory of information fusion
from multiple text sources, step one: Cross-document
structure. In Proceedings, 1st ACL SIGDIAL Workshop on
Discourse and Dialogue, Hong Kong, October 2000.
Figure 1: Seed article.
Figure 2: User interface.
Figure 3: Run-time log (part I).
Figure 4: Run-time log (part II).
Figure 5: System progress.
Figure 6: Cluster visualization.
Figure 7: Selected articles.
Figure 8: Summarization interface.
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 514?522,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Sentiment Summarization: Evaluating and Learning User Preferences
Kevin Lerman
Columbia University
New York, NY
klerman@cs.columbia.edu
Sasha Blair-Goldensohn
Google, Inc.
New York, NY
sasha@google.com
Ryan McDonald
Google, Inc.
New York, NY
ryanmcd@google.com
Abstract
We present the results of a large-scale,
end-to-end human evaluation of various
sentiment summarization models. The
evaluation shows that users have a strong
preference for summarizers that model
sentiment over non-sentiment baselines,
but have no broad overall preference be-
tween any of the sentiment-based models.
However, an analysis of the human judg-
ments suggests that there are identifiable
situations where one summarizer is gener-
ally preferred over the others. We exploit
this fact to build a new summarizer by
training a ranking SVM model over the set
of human preference judgments that were
collected during the evaluation, which re-
sults in a 30% relative reduction in error
over the previous best summarizer.
1 Introduction
The growth of the Internet as a commerce
medium, and particularly the Web 2.0 phe-
nomenon of user-generated content, have resulted
in the proliferation of massive numbers of product,
service and merchant reviews. While this means
that users have plenty of information on which to
base their purchasing decisions, in practice this is
often too much information for a user to absorb.
To alleviate this information overload, research on
systems that automatically aggregate and summa-
rize opinions have been gaining interest (Hu and
Liu, 2004a; Hu and Liu, 2004b; Gamon et al,
2005; Popescu and Etzioni, 2005; Carenini et al,
2005; Carenini et al, 2006; Zhuang et al, 2006;
Blair-Goldensohn et al, 2008).
Evaluating these systems has been a challenge,
however, due to the number of human judgments
required to draw meaningful conclusions. Of-
ten systems are evaluated piecemeal, selecting
pieces that can be evaluated easily and automati-
cally (Blair-Goldensohn et al, 2008). While this
technique produces meaningful evaluations of the
selected components, other components remain
untested, and the overall effectiveness of the entire
system as a whole remains unknown. When sys-
tems are evaluated end-to-end by human judges,
the studies are often small, consisting of only a
handful of judges and data points (Carenini et
al., 2006). Furthermore, automated summariza-
tion metrics like ROUGE (Lin and Hovy, 2003)
are non-trivial to adapt to this domain as they re-
quire human curated outputs.
We present the results of a large-scale, end-to-
end human evaluation of three sentiment summa-
rization models applied to user reviews of con-
sumer products. The evaluation shows that there
is no significant difference in rater preference be-
tween any of the sentiment summarizers, but that
raters do prefer sentiment summarizers over non-
sentiment baselines. This indicates that even sim-
ple sentiment summarizers provide users utility.
An analysis of the rater judgments also indicates
that there are identifiable situations where one sen-
timent summarizer is generally preferred over the
others. We attempt to learn these preferences by
training a ranking SVM that exploits the set of
preference judgments collected during the evalu-
ation. Experiments show that the ranking SVM
summarizer?s cross-validation error decreases by
as much as 30% over the previous best model.
Human evaluations of text summarization have
been undertaken in the past. McKeown et al
(2005) presented a task-driven evaluation in the
news domain in order to understand the utility of
different systems. Also in the news domain, the
Document Understanding Conference1 has run a
number of multi-document and query-driven sum-
marization shared-tasks that have used a wide
1http://duc.nist.gov/
514
iPod Shuffle: 4/5 stars
?In final analysis the iPod Shuffle is a decent player that offers a sleek
compact form factor an excessively simple user interface and a low
price? ... ?It?s not good for carrying a lot of music but for a little bit of
music you can quickly grab and go with this nice little toy? ... ?Mine came
in a nice bright orange color that makes it easy to locate.?
Figure 1: An example summary.
range of automatic and human-based evaluation
criteria. This year, the new Text Analysis Con-
ference2 is running a shared-task that contains an
opinion component. The goal of that evaluation is
to summarize answers to opinion questions about
entities mentioned in blogs.
Our work most closely resembles the evalua-
tions in Carenini et al (2006, 2008). Carenini et
al. (2006) had raters evaluate extractive and ab-
stractive summarization systems. Mirroring our
results, they show that both extractive and abstrac-
tive summarization outperform a baseline, but that
overall, humans have no preference between the
two. Again mirroring our results, their analysis in-
dicates that even though there is no overall differ-
ence, there are situations where one system gener-
ally outperforms the other. In particular, Carenini
and Cheung (2008) show that an entity?s contro-
versiality, e.g., mid-range star rating, is correlated
with which summary has highest value.
The study presented here differs from Carenini
et al in many respects: First, our evaluation is
over different extractive summarization systems in
an attempt to understand what model properties
are correlated with human preference irrespective
of presentation; Secondly, our evaluation is on a
larger scale including hundreds of judgments by
hundreds of raters; Finally, we take a major next
step and show that it is possible to automatically
learn significantly improved models by leveraging
data collected in a large-scale evaluation.
2 Sentiment Summarization
A standard setting for sentiment summarization
assumes a set of documents D = {d1, . . . , dm}
that contain opinions about some entity of interest.
The goal of the system is to generate a summary S
of that entity that is representative of the average
opinion and speaks to its important aspects. An
example summary is given in figure 1. For sim-
plicity we assume that all opinions in D are about
the entity being summarized. When this assump-
tion fails, one can parse opinions at a finer-level
2http://www.nist.gov/tac/
(Jindal and Liu, 2006; Stoyanov and Cardie, 2008)
In this study, we look at an extractive summa-
rization setting where S is built by extracting rep-
resentative bits of text from the set D, subject to
pre-specified length constraints. Specifically, as-
sume each document di is segmented into can-
didate text excerpts. For ease of discussion we
will assume all excerpts are sentences, but in prac-
tice they can be phrases or multi-sentence groups.
Viewed this way, D is a set of candidate sentences
for our summary, D = {s1, . . . , sn}, and summa-
rization becomes the following optimization:
argmax
S?D
L(S) s.t.: LENGTH(S) ? K (1)
where L is some score over possible summaries,
LENGTH(S) is the length of the summary and K
is the pre-specified length constraint. The defini-
tion of L will be the subject of much of this sec-
tion and it is precisely different forms of L that
will be compared in our evaluation. The nature of
LENGTH is specific to the particular use case.
Solving equation 1 is typically NP-hard, even
under relatively strong independence assumptions
between the sentences selected for the summary
(McDonald, 2007). In cases where solving L is
non-trivial we use an approximate hill climbing
technique. First we randomly initialize the sum-
mary S to length ?K. Then we greedily in-
sert/delete/swap sentences in and out of the sum-
mary to maximize L(S) while maintaining the
bound on length. We run this procedure until no
operation leads to a higher scoring summary. In
all our experiments convergence was quick, even
when employing random restarts.
Alternate formulations of sentiment summa-
rization are possible, including aspect-based sum-
marization (Hu and Liu, 2004a), abstractive sum-
marization (Carenini et al, 2006) or related tasks
such as opinion attribution (Choi et al, 2005). We
choose a purely extractive formulation as it makes
it easier to develop baselines and allows raters to
compare summaries with a simple, consistent pre-
sentation format.
2.1 Definitions
Before delving into the details of the summariza-
tion models we must first define some useful func-
tions. The first is the sentiment polarity func-
tion that maps a lexical item t, e.g., word or short
phrase, to a real-valued score,
LEX-SENT(t) ? [?1, 1]
515
The LEX-SENT function maps items with positive
polarity to higher values and items with negative
polarity to lower values. To build this function we
constructed large sentiment lexicons by seeding a
semantic word graph induced from WordNet with
positive and negative examples and then propagat-
ing this score out across the graph with a decaying
confidence. This method is common among sen-
timent analysis systems (Hu and Liu, 2004a; Kim
and Hovy, 2004; Blair-Goldensohn et al, 2008).
In particular, we use the lexicons that were created
and evaluated by Blair-Goldensohn et al (2008).
Next we define sentiment intensity,
INTENSITY(s) =
?
t?s
|LEX-SENT(t)|
which simply measures the magnitude of senti-
ment in a sentence. INTENSITY can be viewed as a
measure of subjectiveness irrespective of polarity.
A central function in all our systems is a sen-
tences normalized sentiment,
SENT(s) =
?
t?s LEX-SENT(t)
?+ INTENSITY(s)
This function measures the (signed) ratio of lexical
sentiment to intensity in a sentence. Sentences that
only contain lexical items of the same polarity will
have high absolute normalized sentiment, whereas
sentences with mixed polarity items or no polar-
ity items will have a normalized sentiment near
zero. We include the constant ? in the denomi-
nator so that SENT gives higher absolute scores to
sentences containing many strong sentiment items
of the same polarity over sentences with a small
number of weak items of the same polarity.
Most sentiment summarizers assume that as in-
put, a system is given an overall rating of the en-
tity it is attempting to summarize, R ? [?1, 1],
where a higher rating indicates a more favorable
opinion. This rating may be obtained directly from
user provided information (e.g., star ratings) or au-
tomatically derived by averaging the SENT func-
tion over all sentences in D. Using R, we can de-
fine a mismatch function between the sentiment of
a summary and the known sentiment of the entity,
MISMATCH(S) = (R?
1
|S|
?
si?S
SENT(si))
2
Summaries with a higher mismatch are those
whose sentiment disagrees most with R.
Another key input many sentiment summarizers
assume is a list of salient entity aspects, which are
specific properties of an entity that people tend to
rate when expressing their opinion. For example,
aspects of a digital camera could include picture
quality, battery life, size, color, value, etc. Find-
ing such aspects is a challenging research problem
that has been addressed in a number of ways (Hu
and Liu, 2004b; Gamon et al, 2005; Carenini et
al., 2005; Zhuang et al, 2006; Branavan et al,
2008; Blair-Goldensohn et al, 2008; Titov and
McDonald, 2008b; Titov and McDonald, 2008a).
We denote the set of aspects for an entity as A and
each aspect as a ? A. Furthermore, we assume
that given A it is possible to determine whether
some sentence s ? D mentions an aspect in A.
For our experiments we use a hybrid supervised-
unsupervised method for finding aspects as de-
scribed and evaluated in Blair-Goldensohn et al
(2008).
Having defined what an aspect is, we next de-
fine a summary diversity function over aspects,
DIVERSITY(S) =
?
a?A
COVERAGE(a)
where COVERAGE(a) ? R is a function that
weights how well the aspect is covered in the
summary and is proportional to the importance of
the aspect as some aspects are more important to
cover than others, e.g., ?picture quality? versus
?strap? for digital cameras. The diversity func-
tion rewards summaries that cover many important
aspects and plays the redundancy reducing role
that is common in most extractive summarization
frameworks (Goldstein et al, 2000).
2.2 Systems
For our evaluation we developed three extractive
sentiment summarization systems. Each system
models increasingly complex objectives.
2.2.1 Sentiment Match (SM)
The first system that we look at attempts to ex-
tract sentences so that the average sentiment of the
summary is as close as possible to the entity level
sentiment R, which was previously defined in sec-
tion 2.1. In this case L can be simply defined as,
L(S) = ?MISMATCH(S)
Thus, the model prefers summaries with average
sentiment as close as possible to the average sen-
timent across all the reviews.
516
There is an obvious problem with this model.
For entities that have a mediocre rating, i.e., R ?
0, the model could prefer a summary that only
contains sentences with no opinion whatsoever.
There are two ways to alleviate this problem. The
first is to include the INTENSITY function into L,
L(S) = ? ? INTENSITY(S)? ? ? MISMATCH(S)
Where the coefficients allow one to trade-off sen-
timent intensity versus sentiment mismatch.
The second method, and the one we chose based
on initial experiments, was to address the problem
at inference time. This is done by prohibiting the
algorithm from including a given positive or nega-
tive sentence in the summary if another more pos-
itive/negative sentence is not included. Thus the
summary is forced to consist of only the most pos-
itive and most negative sentences, the exact mix
being dependent upon the overall star rating.
2.2.2 Sentiment Match + Aspect Coverage
(SMAC)
The SM model extracts sentences for the summary
without regard to the content of each sentence rel-
ative to the others in the summary. This is in con-
trast to standard summarization models that look
to promote sentence diversity in order to cover as
many important topics as possible (Goldstein et
al., 2000). The sentiment match + aspect cov-
erage system (SMAC) attempts to model diver-
sity by building a summary that trades-off max-
imally covering important aspects with matching
the overall sentiment of the entity. The model does
this through the following linear score,
L(S) = ? ? INTENSITY(S)? ? ? MISMATCH(S)
+? ? DIVERSITY(S)
This score function rewards summaries for be-
ing highly subjective (INTENSITY), reflecting the
overall product rating (MISMATCH), and covering
a variety of product aspects (DIVERSITY). The co-
efficients were set by inspection.
This system has its roots in event-based summa-
rization (Filatova and Hatzivassiloglou, 2004) for
the news domain. In that work an optimization
problem was developed that attempted to maxi-
mize summary informativeness while covering as
many (weighted) sub-events as possible.
2.2.3 Sentiment-Aspect Match (SAM)
Because the SMAC model only utilizes an entity?s
overall sentiment when calculating MISMATCH, it
is susceptible to degenerate solutions. Consider a
product with aspects A and B, where reviewers
overwhelmingly like A and dislike B, resulting in
an overall SENT close to zero. If the SMAC model
finds a very negative sentence describing A and
a very positive sentence describing B, it will as-
sign that summary a high score, as the summary
has high intensity, has little overall mismatch, and
covers both aspects. However, in actuality, the
summary is entirely misleading.
To address this issue, we constructed the
sentiment-aspect match model (SAM), which not
only attempts to cover important aspects, but cover
them with appropriate sentiment. There are many
ways one might design a model to do this, includ-
ing linear combinations of functions similar to the
SMAC model. However, we decided to employ a
probabilistic approach as it provided performance
benefits based on development data experiments.
Under the SAM model, each sentence is treated as
a bag of aspects and their corresponding mentions?
sentiments. For a given sentence s, we define As
as the set of aspects mentioned within it. For a
given aspect a ? As, we denote SENT(as) as the
sentiment associated with the textual mention of a
in s. The probability of a sentence is defined as,
p(s) = p(a1, . . . , an, SENT(a1s), . . . , SENT(a
n
s ))
which can be re-written as,
?
a?As
p(a, SENT(as)) =
?
a?As
p(a)p(SENT(as)|a)
if we assume aspect mentions are generated inde-
pendently of one another. Thus we need to esti-
mate both p(a) and p(SENT(as)|a). The probabil-
ity of seeing an aspect, p(a), is simply set to the
maximum likelihood estimates over the data set
D. Furthermore, we assume that p(SENT(as)|a)
is normal about the mean sentiment for the as-
pect ?a with a constant standard deviation, ?a.
The mean and standard deviation are estimated
straight-forwardly using the data set D. Note that
the number of parameters our system must es-
timate is very small. For every possible aspect
a ? A we need three values: p(a), ?a, and ?a.
Since |A| is typically small ? on the order of 5-10
? it is not difficult to estimate these models even
from small sets of data.
Having constructed this model, one logical ap-
proach to summarization would be to select sen-
tences for the summary that have highest proba-
bility under the model trained on D. We found,
517
however, that this produced very redundant sum-
maries ? if one aspect is particularly prevalent in
a product?s reviews, this approach will select all
sentences about that aspect, and discuss nothing
else. To combat this we developed a technique that
scores the summary as a whole, rather than by in-
dividual components. First, denote SAM(D) as the
previously described model learned over the set of
entity documents D. Next, denote SAM(S) as an
identical model, but learned over a candidate sum-
mary S, i.e., given a summary S, compute p(a),
ma, and ?a for all a ? A using only the sentences
from S. We can then measure the difference be-
tween these models using KL-divergence:
L(S) = ?KL(SAM(D), SAM(S))
In our case we have 1 + |A| distributions ? p(a),
and p(?|a) for all a ? A ? so we just sum the KL-
divergence of each. The key property of the SAM
system is that it naturally builds summaries where
important aspects are discussed with appropriate
sentiment, since it is precisely these aspects that
will contribute the most to the KL-divergence. It
is important to note that the short length of a can-
didate summary S can make estimates in SAM(S)
rather crude. But we only care about finding the
?best? of a set of crude models, not about finding
one that is ?good? in absolute terms. Between the
few parameters we must learn and the specific way
we use these models, we generally get models use-
ful for our purposes.
Alternatively we could have simply incorpo-
rated the DIVERSITY measure into the objec-
tive function or used an inference algorithm that
specifically accounts for redundancy, e.g., maxi-
mal marginal relevance (Goldstein et al, 2000).
However, we found that this solution was well
grounded and required no tuning of coefficients.
Initial experiments indicated that the SAM sys-
tem, as described above, frequently returned sen-
tences with low intensity when important aspects
had luke-warm sentiment. To combat this we re-
moved low intensity sentences from consideration,
which had the effect of encouraging important
luke-warm aspects to mentioned multiple times in
order to balance the overall sentiment.
Though the particulars of this model are unique,
fundamentally it is closest to the work of Hu and
Liu (2004a) and Carenini et al (2006).
3 Experiments
We evaluated summary performance for reviews
of consumer electronics. In this setting an entity
to be summarized is one particular product, D is
a set of user reviews about that product, and R is
the normalized aggregate star ratings left by users.
We gathered reviews for 165 electronics products
from several online review aggregators. The prod-
ucts covered a variety of electronics, such as MP3
players, digital cameras, printers, wireless routers,
and video game systems. Each product had a min-
imum of four reviews and up to a maximum of
nearly 3000. The mean number of reviews per
product was 148, and the median was 70. We
ran each of our algorithms over the review corpus
and generated summaries for each product with
K = 650. All summaries were roughly equal
length to avoid length-based rater bias3. In total
we ran four experiments for a combined number of
1980 rater judgments (plus additional judgments
during the development phase of this study).
Our initial set of experiments were over the
three opinion-based summarization systems: SM,
SMAC, and SAM. We ran three experiments com-
paring SMAC to SM, SAM to SM, and SAM to
SMAC. In each experiment two summaries of the
same product were placed side-by-side in a ran-
dom order. Raters were also shown an overall rat-
ing, R, for each product (these ratings are often
provided in a form such as ?3.5 of 5 stars?). The
two summaries on either side were shown below
this information with links to the full text of the
reviews for the raters to explore.
Raters were asked to express their preference
for one summary over the other. For two sum-
maries SA and SB they could answer,
1. No preference
2. Strongly preferred SA (or SB)
3. Preferred SA (or SB)
4. Slightly preferred SA (or SB)
Raters were free to choose any rating, but were
specifically instructed that their rating should ac-
count for a summaries representativeness of the
overall set of reviews. Raters were also asked
to provide a brief comment justifying their rat-
ing. Over 100 raters participated in each study,
and each comparison was evaluated by three raters
with no rater making more than five judgments.
3In particular our systems each extracted four text ex-
cerpts of roughly 160-165 characters.
518
Comparison (A v B) Agreement (%) No Preference (%) Preferred A (%) Preferred B (%) Mean Numeric
SM v SMAC 65.4 6.0 52.0 42.0 0.01
SAM v SM 69.3 16.8 46.0 37.2 0.01
SAM v SMAC? 73.9 11.5 51.6 36.9 0.08
SMAC v LT? 64.1 4.1 70.4 25.5 0.24
Table 1: Results of side-by-side experiments. Agreement is the percentage of items for which all raters
agreed on a positive/negative/no-preference rating. No Preference is the percentage of agreement items
in which the raters had no preference. Preferred A/B is the percentage of agreement items in which the
raters preferred either A or B respectively. Mean Numeric is the average of the numeric ratings (converted
from discreet preference decisions) indicating on average the raters preferred system A over B on a scale
of -1 to 1. Positive scores indicate a preference for system A. ? significant at a 95% confidence interval
for the mean numeric score.
We chose to have raters leave pairwise prefer-
ences, rather than evaluate each candidate sum-
mary in isolation, because raters can make a pref-
erence decisions more quickly than a valuation
judgment, which allowed for collection of more
data points. Furthermore, there is evidence that
rater agreement is much higher in preference deci-
sions than in value judgments (Ariely et al, 2008).
Results are shown in the first three rows of ta-
ble 1. The first column of the table indicates the
experiment that was run. The second column indi-
cates the percentage of judgments for which the
raters were in agreement. Agreement here is a
weak agreement, where three raters are defined to
be in agreement if they all gave a no preference rat-
ing, or if there was a preference rating, but no two
preferences conflicted. The next three columns in-
dicate the percentage of judgments for each pref-
erence category, grouped here into three coarse as-
signments. The final column indicates a numeric
average for the experiment. This was calculated
by converting users ratings to a scale of 1 (strongly
preferred SA) to -1 (strongly preferred SB) at 0.33
intervals. Table 1 shows only results for items in
which the raters had agreement in order to draw
reliable conclusions, though the results change lit-
tle when all items are taken into account.
Ultimately, the results indicate that none of the
sentiment summarizers are strongly preferred over
any other. Only the SAM v SMAC model has a
difference that can be considered statistically sig-
nificant. In terms of order we might conclude that
SAM is the most preferred, followed by SM, fol-
lowed by SMAC. However, the slight differences
make any such conclusions tenuous at best. This
leads one to wonder whether raters even require
any complex modeling when summarizing opin-
ions. To test this we took the lowest scoring model
overall, SMAC, and compared it to a leading text
baseline (LT) that simply selects the first sentence
from a ranked list of reviews until the length con-
straint is violated. The results are given in the last
row of 1. Here there is a clear distinction as raters
preferred SMAC to LT, indicating that they did
find usefulness in systems that modeled aspects
and sentiment. However, there are still 25.5%
of agreement items where the raters did choose a
simple leading text baseline.
4 Analysis
Looking more closely at the results we observed
that, even though raters did not strongly prefer
any one sentiment-aware summarizer over another
overall, they mostly did express preferences be-
tween systems on individual pairs of comparisons.
For example, in the SAM vs SM experiment, only
16.8% of the comparisons yielded a ?no prefer-
ence? judgment from all three raters ? by far the
highest percentage of any experiment. This left
83.2% ?slight preference? or higher judgments.
With this in mind we began examining the com-
ments left by raters throughout all our experi-
ments, including a set of additional experiments
used during development of the systems. We ob-
served several trends: 1) Raters tended to pre-
fer summaries with lists, e.g., pros-cons lists; 2)
Raters often did not like text without sentiment,
hence the dislike of the leading text system where
there is no guarantee that the first sentence will
have any sentiment; 3) Raters disliked overly gen-
eral comments, e.g., ?The product was good?.
These statements carry no additional information
over a product?s overall star rating; 4) Raters did
recognize (and strongly disliked) when the overall
sentiment of the summary was inconsistent with
the star rating; 5) Raters tended to prefer different
519
systems depending on what the star rating was. In
particular, the SMAC system was generally pre-
ferred for products with neutral overall ratings,
whereas the SAM system is preferred for products
with ratings at the extremes. We hypothesize that
SAM?s low performance on neutral rated products
is because the system suffers from the dual imper-
atives of selecting high intensity snippets and of
selecting snippets that individually reflect partic-
ular sentiment polarities. When the desired senti-
ment polarity is neutral, it is difficult to find a snip-
pet with lots of sentiment, whose overall polarity
is still neutral, thus SAM may either ignore that
aspect or include multiple mentions of that aspect
at the expense of others; 6) Raters also preferred
summaries with grammatically fluent text, which
benefitted the leading text baseline.
These observations suggest that we could build
a new system that takes into account all these
factors (weighted accordingly) or we could build
a rule-based meta-classifier that selects a single
summary from the four systems described in this
paper based on the global characteristics of each.
The problem with the former is that it will require
hand-tuning of coefficients for many different sig-
nals that are all, for the most part, weakly corre-
lated to summary quality. The problem with the
latter is inefficiency, i.e., it will require the main-
tenance and output of all four systems. In the next
section we explore an alternate method that lever-
ages the data gathered in the evaluation to auto-
matically learn a new model. This approach is
beneficial as it will allow any coefficients to be au-
tomatically tuned and will result in a single model
that can be used to build new summaries.
5 Summarization with Ranking SVMs
Besides allowing us to assess the relative perfor-
mance of our summarizers, our evaluation pro-
duced several hundred points of empirical data in-
dicating which among two summaries raters pre-
fer. In this section we explore how to build im-
proved summarizers with this data by learning
preference ranking SVMs, which are designed to
learn relative to a set of preference judgments
(Joachims, 2002).
A ranking SVM typically assumes as input a set
of queries and associated partial ordering on the
items returned by the query. The training data is
defined as pairs of points, T = {(xki , x
k
j )t}
|T |
t=1,
where each pair indicates that the ith item is pre-
ferred over the jth item for the kth query. Each
input point xki ? R
m is a feature vector repre-
senting the properties of that particular item rel-
ative to the query. The goal is to learn a scoring
function s(xki ) ? R such that s(x
k
i ) > s(x
k
j ) if
(xki , x
k
j ) ? T . In other words, a ranking SVM
learns a scoring function whose induced ranking
over data points respects all preferences in the
training data. The most straight-forward scoring
function, and the one used here, is a linear classi-
fier, s(xki ) = w ? x
k
i , making the goal of learning
to find an appropriate weight vector w ? Rm.
In its simplest form, the ranking SVM opti-
mization problem can be written as the following
quadratic programming problem,
min
1
2
||w||2 s.t.: ?(xki , x
k
j ) ? T ,
s(xki )? s(x
k
j ) ? PREF(x
k
i , x
k
j )
where PREF(xki , x
k
j ) ? R is a function indicating
to what degree item xki is preferred over x
k
j (and
serves as the margin of the classifier). This opti-
mization is well studied and can be solved with a
wide variety of techniques. In our experiments we
used the SVM-light software package4.
Our summarization evaluation provides us with
precisely a large collection of preference points
over different summaries for different product
queries. Thus, we naturally have a training set T
where each query is analogous to a specific prod-
uct of interest and training points are two possi-
ble summarizations produced by two different sys-
tems with corresponding rater preferences. As-
suming an appropriate choice of feature represen-
tation it is straight-forward to then train the model
on our data using standard techniques for SVMs.
To train and test the model we compiled 1906
pairs of summary comparisons, each judged by
three different raters. These pairs were extracted
from the four experiments described in section 3
as well as the additional experiments we ran dur-
ing development. For each pair of summaries
(Ski , S
k
j ) (for some product query indexed by k),
we recorded how many raters preferred each of the
items as vki and v
k
j respectively, i.e., v
k
i is the num-
ber of the three raters who preferred summary Si
over Sj for product k. Note that vki + v
k
j does not
necessarily equal 3 since some raters expressed no
preference between them. We set the loss function
PREF(Ski , S
k
j ) = v
k
i ? v
k
j , which in some cases
4http://svmlight.joachims.org/
520
could be zero, but never negative since the pairs
are ordered. Note that this training set includes all
data points, even those in which raters disagreed.
This is important as the model can still learn from
these points as the margin function PREF encodes
the fact that these judgments are less certain.
We used a variety of features for a candidate
summary: how much capitalization, punctuation,
pros-cons, and (unique) aspects a summary had;
the overall intensity, sentiment, min sentence sen-
timent, and max sentence sentiment in the sum-
mary; the overall ratingR of the product; and con-
junctions of these. Note that none of these fea-
tures encode which system produced the summary
or which experiment it was drawn from. This is
important, as it allows the model to be used as
standalone scoring function, i.e., we can set L to
the learned linear classifier s(S). Alternatively
we could have included features like what system
was the summary produced from. This would have
helped the model learn things like the SMAC sys-
tem is typically preferred for products with mid-
range overall ratings. Such a model could only be
used to rank the outputs of other summarizers and
cannot be used standalone.
We evaluated the trained model by measuring
its accuracy on predicting a single preference pre-
diction, i.e., given pairs of summaries (Ski , S
k
j ),
how accurate is the model at predicting that Si is
preferred to Sj for product query k? We measured
10-fold cross-validation accuracy on the subset of
the data for which the raters were in agreement.
We measure accuracy for both weak agreement
cases (at least one rater indicated a preference and
the other two raters were in agreement or had no
preference) and strong agreement cases (all three
raters indicated the same preference). We ignored
pairs in which all three raters made a no preference
judgment as both summaries can can be consid-
ered equally valid. Furthermore, we ignored pairs
in which two raters indicated conflicting prefer-
ences as there is no gold standard for such cases.
Results are given in table 2. We compare the
ranking SVM summarizer to a baseline system
that always selects the overall-better-performing
summarization system from the experiment that
the given datapoint was drawn from, e.g., for all
the data points drawn from the SAM versus SMAC
experiment, the baseline always chooses the SAM
summary as its preference. Note that in most ex-
periments the two systems emerged in a statistical
Preference Prediction Accuracy
Weak Agr. Strong Agr.
Baseline 54.3% 56.9%
Ranking SVM 61.8% 69.9%
Table 2: Accuracies for learned summarizers.
tie, so this baseline performs only slightly better
than chance. Table 2 clearly shows that the rank-
ing SVM can predict preference accuracy much
better than chance, and much better than that ob-
tained by using only one summarizer (a reduction
in error of 30% for strong agreement cases).
We can thus conclude that the data gathered
in human preference evaluation experiments, such
as the one presented here, have a beneficial sec-
ondary use as training data for constructing a new
and more accurate summarizer. This raises an
interesting line of future research: can we iter-
ate this process to build even better summariz-
ers? That is, can we use this trained summarizer
(and variants of it) to generate more examples for
raters to judge, and then use that data to learn even
more powerful summarizers, which in turn could
be used to generate even more training judgments,
etc. This could be accomplished using Mechani-
cal Turk5 or another framework for gathering large
quantities of cheap annotations.
6 Conclusions
We have presented the results of a large-scale eval-
uation of different sentiment summarization algo-
rithms. In doing so, we explored different ways
of using sentiment and aspect information. Our
results indicated that humans prefer sentiment in-
formed summaries over a simple baseline. This
shows the usefulness of modeling sentiment and
aspects when summarizing opinions. However,
the evaluations also show no strong preference be-
tween different sentiment summarizers. A detailed
analysis of the results led us to take the next step
in this line of research ? leveraging preference
data gathered in human evaluations to automati-
cally learn new summarization models. These new
learned models show large improvements in pref-
erence prediction accuracy over the previous sin-
gle best model.
Acknowledgements: The authors would like to
thank Kerry Hannan, Raj Krishnan, Kristen Parton
and Leo Velikovich for insightful discussions.
5http://www.mturk.com
521
References
D. Ariely, G. Loewenstein, and D. Prelec. 2008. Co-
herent arbitrariness: Stable demand curves without
stable preferences. The Quarterly Journal of Eco-
nomics, 118:73105.
S. Blair-Goldensohn, K. Hannan, R. McDonald,
T. Neylon, G.A. Reis, and J. Reynar. 2008. Building
a sentiment summarizer for local service reviews. In
WWW Workshop on NLP in the Information Explo-
sion Era.
S.R.K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic prop-
erties from free-text annotations. In Proceedings of
the Annual Conference of the Association for Com-
putational Linguistics (ACL).
G. Carenini and J. Cheung. 2008. Extractive vs. nlg-
based abstractive summarization of evaluative text:
The effect of corpus controversiality. In Interna-
tional Conference on Natural Language Generation
(INLG).
G. Carenini, R.T. Ng, and E. Zwart. 2005. Extract-
ing knowledge from evaluative text. In Proceedings
of the International Conference on Knowledge Cap-
ture.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-
document summarization of evaluative text. In Pro-
ceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL).
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005.
Identifying sources of opinions with conditional ran-
dom fields and extraction patterns. In Proceedings
the Joint Conference on Human Language Technol-
ogy and Empirical Methods in Natural Language
Processing (HLT-EMNLP).
E. Filatova and V. Hatzivassiloglou. 2004. A formal
model for information selection in multi-sentence
text extraction. In Proceedings of the International
Conference on Computational Linguistics (COL-
ING).
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free
text. In Proceedings of the 6th International Sympo-
sium on Intelligent Data Analysis (IDA).
J. Goldstein, V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Proceedings
of the ANLP/NAACL Workshop on Automatic
Summarization.
M. Hu and B. Liu. 2004a. Mining and summariz-
ing customer reviews. In Proceedings of the Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD).
M. Hu and B. Liu. 2004b. Mining opinion features in
customer reviews. In Proceedings of National Con-
ference on Artificial Intelligence (AAAI).
N. Jindal and B. Liu. 2006. Mining comprative sen-
tences and relations. In Proceedings of 21st Na-
tional Conference on Artificial Intelligence (AAAI).
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
S.M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of Conference on
Computational Linguistics (COLING).
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram cooccurrence statistics.
In Proceedings of the Conference on Human Lan-
guage Technologies and the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL).
R. McDonald. 2007. A Study of Global Inference
Algorithms in Multi-document Summarization. In
Proceedings of the European Conference on Infor-
mation Retrieval (ECIR).
K. McKeown, R.J. Passonneau, D.K. Elson,
A. Nenkova, and J. Hirschberg. 2005. Do
Summaries Help? A Task-Based Evaluation of
Multi-Document Summarization. In Proceedings
of the ACM SIGIR Conference on Research and
Development in Information Retrieval.
A.M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings of
the Conference on Computational Linguistics (COL-
ING).
I. Titov and R. McDonald. 2008a. A joint model of
text and aspect ratings. In Proceedings of the An-
nual Conference of the Association for Computa-
tional Linguistics (ACL).
I. Titov and R. McDonald. 2008b. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the Annual World Wide Web Conference
(WWW).
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings
of the International Conference on Information and
Knowledge Management (CIKM).
522
Proceedings of NAACL HLT 2007, pages 428?435,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Building and Rening Rhetorical-Semantic Relation Models
Sasha Blair-Goldensohn and
Google, Inc.
76 Ninth Avenue
New York, NY
sasha@google.com
Kathleen R. McKeown? and Owen C. Rambow?
? Department of Computer Science
? Center for Computational Learning Systems
Columbia University
{kathy,rambow}@cs.columbia.edu
Abstract
We report results of experiments which
build and refine models of rhetorical-
semantic relations such as Cause and Con-
trast. We adopt the approach of Marcu
and Echihabi (2002), using a small set of
patterns to build relation models, and ex-
tend their work by refining the training
and classification process using parame-
ter optimization, topic segmentation and
syntactic parsing. Using human-annotated
and automatically-extracted test sets, we
find that each of these techniques results in
improved relation classification accuracy.
1 Introduction
Relations such as Cause and Contrast, which we call
rhetorical-semantic relations (RSRs), may be sig-
naled in text by cue phrases like because or how-
ever which join clauses or sentences and explicitly
express the relation of constituents which they con-
nect (Example 1). In other cases the relation may be
implicitly expressed (2).1
Example 1 Because of the recent accounting scan-
dals, there have been a spate of executive resigna-
tions.
Example 2 The administration was once again be-
set by scandal. After several key resignations ...
1The authors would like to thank the four anonymous re-
viewers for helpful comments. This work was supported by the
Defense Advanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023. Any opinions, findings and
conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the views of
DARPA.
The first author performed most of the research reported in
this paper while at Columbia University.
In this paper, we examine the problem of detect-
ing such relations when they are not explicitly sig-
naled. We draw on and extend the work of Marcu
and Echihabi (2002). Our baseline model directly
implements Marcu and Echihabi?s approach, opti-
mizing a set of basic parameters such as smoothing
weights, vocabulary size and stoplisting. We then
focus on improving the quality of the automatically-
mined training examples, using topic segmenta-
tion and syntactic heuristics to filter out training
instances which may be wholly or partially in-
valid. We find that the parameter optimization and
segmentation-based filtering techniques achieve sig-
nificant improvements in classification performance.
2 Related Work
Rhetorical and discourse theory has a long tradition
in computational linguistics (Moore and Wiemer-
Hastings, 2003). While there are a number of differ-
ent relation taxonomies (Hobbs, 1979; McKeown,
1985; Mann and Thompson, 1988; Martin, 1992;
Knott and Sanders, 1998), many researchers have
found that, despite small differences, these theories
have wide agreement in terms of the core phenom-
ena for which they account (Hovy and Maier, 1993;
Moser and Moore, 1996).
Work on automatic detection of rhetorical and dis-
course relations falls into two categories. Marcu
and Echihabi (2002) use a pattern-based approach
in mining instances of RSRs such as Contrast and
Elaboration from large, unannotated corpora. We
discuss this work in detail in Section 3. Other
work uses human-annotated corpora, such as the
RST Bank (Carlson et al, 2001), used by Soricut
and Marcu (2003), the GraphBank (Wolf and Gib-
son, 2005), used by Wellner et al (2006), or ad-
hoc annotations, used by (Girju, 2003; Baldridge
and Lascarides, 2005). In the past year, the ini-
428
tial public release of the Penn Discourse TreeBank
(PDTB) (Prasad et al, 2006) has significantly ex-
panded the discourse-annotated corpora available to
researchers, using a comprehensive scheme for both
implicit and explicit relations.
Some work in RSR detection has enlisted syntac-
tic analysis as a tool. Marcu and Echihabi (2002) fil-
ter training instances based on Part-of-Speech (POS)
tags, and Soricut and Marcu (2003) use syntac-
tic features to identify sentence-internal RST struc-
ture. Lapata and Lascarides (2004) focus their
work syntactically, analyzing temporal links be-
tween main and subordinate clauses. Sporleder and
Lascarides (2005) extend Marcu and Echihabi?s ap-
proach with the addition of a number of features,
including syntactic features based on POS and ar-
gument structure, as well as lexical and other sur-
face features. They report that, when working with
sparse training data, this richer feature set, combined
with a boosting-based algorithm, achieves more ac-
curate classification than Marcu and Echihabi?s sim-
pler, word-pair based approach (we describe the lat-
ter in the next section).
3 The M&E Framework
We model two RSRs, Cause and Contrast, adopt-
ing the definitions of Marcu and Echihabi (2002)
(henceforth M&E) for their Cause-Explanation-
Evidence and Contrast relations, respectively. In
particular, we follow their intuition that in building
an automated model it is best to adopt a higher-level
view of relations (cf. (Hovy and Maier, 1993)),
collapsing the finer-grained distinctions that hold
within and across relation taxonomies.
M&E use a three-stage approach common in cor-
pus linguistics: collect a large set of class instances
(instance mining), analyze them to create a model
of differentiating features (model building), and use
this model as input to a classication step which
determines the most probable class of unknown in-
stances.
The intuition of the M&E model is to apply a set
of RSR-associated cue phrase patterns over a large
text corpus to compile a training set without the cost
of human annotation. For instance, Example 1 will
match the Cause-associated pattern ?Because of W1
, W2 .?, where W1 and W2 stand for non-empty
strings containing word tokens. In the aggregate,
such instances increase the prior belief that, e.g.,
a text span containing the word scandals and one
containing resignations are in a Cause relation. A
critical point is that the cue words themselves (e.g.,
because) are discarded before extracting these word
pairs; otherwise these cue phrases themselves would
likely be the most distinguishing features learned.
More formally, M&E build up their model
through the three stages mentioned above as fol-
lows: In instance mining, for each RSR r they com-
pile an instance set Ir of (W1,W2) spans which
match a set of patterns associated with r. In
model building, features are extracted from these in-
stances; M&E extract a single feature, namely the
frequency of token pairs derived from taking the
cartesian product of W1 = {w1...wn} ? W2 =
{wn+1...wm} = {(w1, wn+1)...(wn, wm)} over
each span pair instance (W1,W2) ? I; these pair
frequencies are tallied for each RSR into a frequency
table Fr. Then in classication, the most likely re-
lation r between two unknown-relation spans W1
and W2 can be determined by a na??ve Bayesian
classifier as argmaxr?R P (r|W1,W2), where the
probability P (r|W1,W2) is simplified by assum-
ing the independence of the individual token pairs
to: ?(wi,wj)?W1,W2 P ((wi, wj)|r). The frequencycounts Fr are used as maximum likelihood estima-
tors of P ((wi, wj)|r).
4 TextRels
TextRels is our implementation of the M&E frame-
work, and serves as our platform for the experiments
which follow.
For instance mining, we use a set of cue phrase
patterns derived from published lists (e.g., (Marcu,
1997; Prasad et al, 2006)) to mine the Gigaword
corpus of 4.7 million newswire documents2 for re-
lation instances. We mine instances of the Cause
and Contrast RSRs discussed earlier, as well as a
NoRel ?relation?. NoRel is proposed by M&E as
a default model of same-topic text across which no
specific RSR holds; instances are extracted by tak-
ing text span pairs which are simply sentences from
the same document separated by at least three inter-
vening sentences. Table 1 lists a sample of our ex-
2distributed by the Linguistic Data Consortium
429
Type Sample Patterns Instances Instances, M&E
Cause BOS Because W1 , W2 EOS
BOS W1 EOS BOS Therefore , W2 EOS.
926,654 889,946
Contrast BOS W1 , but W2 EOS
BOS W1 EOS BOS However , W2 EOS.
3,017,662 3,881,588
NoRel BOS W1 EOS (BOS EOS){3,} BOS W2 EOS 1,887,740 1,000,000
Table 1: RSR types, sample extraction patterns, number of training instances used in TextRels, and number
of training instances used by M&E. BOS and EOS are sentence beginning/end markers.
traction patterns and the total number of training in-
stances per relation; in addition, we hold out 10,000
instances of each type, which we divide evenly into
development and training sets.
For model building, we compile the training in-
stances into token-pair frequencies. We implement
several parameters which control the way these fre-
quencies are computed; we discuss these parameters
and their optimization in the next section.
For classication, we implement three binary
classifiers (for Cause vs Contrast, Cause vs NoRel
and Contrast vs NoRel) using the nai?ve Bayesian
framework of the M&E approach. We implement
several classification parameters, which we discuss
in the next section.
5 Parameter Optimization
Our first set of experiments examine the impact of
various parameter settings in TextRels, using classi-
fication accuracy on a development set as our heuris-
tic. We find that the following parameters have
strong impacts on classification:
? Tokenizing our training instances using stem-
ming slightly improves accuracy and also reduces
model size.
? Laplace smoothing is as accurate as Good-
Turing, but is simpler to implement. Our experi-
ments find peak performance with 0.25 ? value, i.e.
the frequency assumed for unseen pairs.
? Vocabulary size of 6,400 achieves peak perfor-
mance; tokens which are not in the most frequent
6,400 stems (computed over Gigaword) are replaced
by an UNK pseudo-token before F is computed.
? Stoplisting has a negative impact on accuracy;
we find that even the most frequent tokens contribute
useful information to the model; a stoplist size of
zero achieves peak performance.
? Minimum Frequency cutoff is imposed to dis-
card from F token pair counts with a frequency of
< 4; results degrade slightly below this value, and
discarding this long tail of rare pair counts signifi-
cantly shrinks model size.
Classif.
/
Pdtb Auto Auto-
S
M&E
TestSet Opt Seg Opt Seg Opt Seg
Cau/Con 59.1 61.1 69.8 69.7 70.3 70.6 87
Cau/NR 75.2 74.3 72.7 73.5 71.2 72.3 75
Con/NR 67.4 69.7 70.7 71.3 68.2 70.0 64
Table 2: Classifier accuracy across PDTB, Auto
and Auto-S test sets for the parameter-optimized
classifier (?Opt?) and the same classifier trained on
segment-constrained instances (?Seg?). Accuracy
from M&E is reported for reference, but we note that
they use a different test set so the comparison is not
exact. Baseline in all cases is 50%.
To evaluate the performance of our three binary
classifiers using these optimizations, we follow the
protocol of M&E. We present the classifier for, e.g.,
Cause vs NoRel with an equal number of span-pair
instances for each RSR (as in training, any pattern
text has been removed). We then determine the ac-
curacy of the classifier in predicting the actual RSR
of each instance; in all cases we use an equal num-
ber of input pairs for each RSR so random baseline
is 50 %. We carry out this evaluation over two dif-
ferent test sets.
The first set (?PDTB?) is derived from the Penn
Discourse TreeBank (Prasad et al, 2006). We ex-
tract ?Implicit? relations, i.e. text spans from adja-
cent sentences between which annotators have in-
ferred semantics not marked by any surface lexi-
cal item. To extract test instances for our Cause
RSR, we take all PDTB Implicit relations marked
with ?Cause? or ?Consequence? semantics (344 to-
tal instances); for our Contrast RSR, we take in-
stances marked with ?Contrast? semantics (293 to-
430
tal instances).3 PDTB marks the two ?Arguments?
of these relationship instances, i.e. the text spans to
which they apply; these are used as test (W1,W2)
span pairs for classification. We test the perfor-
mance on PDTB data using 280 randomly selected
instances each from the PDTB Cause and Contrast
sets, as well as 280 randomly selected instances
from our test set of automatically extracted NoRel
instances (while there is a NoRel relation included
in PDTB, it is too sparse to use in this testing, with
53 total examples).
The second test set (?Auto?) uses the 5,000 test
instances of each RSR type automatically extracted
in our instance mining process.
Table 2 lists the accuracy for the optimized
(?Opt?) classifier over the Auto and PDTB test sets4.
(The ?Seg? columns and ?Auto-S? test set are ex-
plained in the next section.)
We also list for reference the accuracy reported
by M&E; however, their training and test sets are
not the same so this comparison is inexact, al-
though their test set is extracted automatically in the
same manner as ours. In the Cause versus Contrast
case, their reported performance exceeds ours sig-
nificantly; however, in a subset of their experiments
which test Cause versus Contrast on instances from
the human annotated RSTBank corpus (Carlson et
al., 2001) where no cue phrase is present, they re-
port only 63% accuracy over a 56% baseline (the
baseline is > 50% because the number of input ex-
amples is unbalanced).
Since we also experience a drop in performance
from the automatically derived test set to the human-
annotated test set (the PDTB in our case), we fur-
ther examined this issue. Our goal was to see if the
lower accuracy on the PDTB examples is due to (1)
the inherent difficulty of identifying implicit rela-
tion spans or (2) something else, such as the corpus-
switching effect due to our model being trained and
3Note that we are using the initial PDTB release, in which
only three of 24 data sections have marked Implicit relations, so
that the number of such examples will presumably grow in the
next release.
4We do not provide pre-optimization baseline accuracy be-
cause this would be arbitrarily depend on how sub-optimally we
select values select parameter values. For instance, by using a
Vocabulary Size of 3,200 (rather than 6,400) and a Laplace ?
value of 1, the mean accuracy of the classifiers on the Auto test
set drops from 71.6 to 70.5; using a Stoplist size of 25 (rather
than 0) drops this number to 67.3.
tested on different corpora (Gigaword and PDTB,
respectively). To informally test this, we tested
against explicitly cue-phrase marked examples gath-
ered from PDTB. That is, we used the M&E-style
method for mining instances, but we gathered them
from the PDTB corpus. Interestingly, we found that
(1) appears to be the case: for the Cause vs. Contrast
(68.7%), Cause vs. NoRel (73.0%) and (Contrast vs.
NoRel (71.0%) classifiers, the performance patterns
with the Auto test set rather than the results from the
PDTB Implicit test set. This bolsters the argument
that ?synthetic? implicit relations, i.e. those created
by stripping of originally present cue phrases, can-
not be treated as fully equivalent to ?organic? ones
annotated by a human judge but which are not ex-
plicitly indicated by a cue phrase. Sporleder and
Lascarides (To Appear) recently investigated this is-
sue in greater detail, and indeed found that such syn-
thetic and organic instances appear to have impor-
tant differences.
6 Using Topic Segmentation
In our experiments with topic segmentation, we aug-
mented the instance mining process to take account
of topic segment boundaries. The intuition here is
that all sentence boundaries should not be treated
equally during RSR instance mining. That is, we
would like to make our patterns recognize that some
sentence boundaries indicate merely an orthographic
break without a switch in topic, while others can
separate quite distinct topics. Sometimes the latter
type are marked by paragraph boundaries, but these
are unreliable markers since they may be used quite
differently by different authors.
Instead, we take the approach of adding topic seg-
ment boundary markers to our corpus, which we can
then integrate into our RSR extraction patterns. In
the case of NoRel, our assumption in our original
patterns is that the presence of at least three inter-
vening sentences is a sufficient heuristic for finding
spans which are not joined by one of the other RSRs;
we add the constraint that sentences in a NoRel re-
lation be in distinct topical segments, we can in-
crease model quality. Conversely, for two-sentence
Cause and Contrast instances, we add the constraint
that there must not be an intervening topic segment
boundary between the two sentences.
431
Before applying these segment-augmented pat-
terns, we must add boundary markers to our cor-
pus. While the concept of a topic segment can
be defined at various granularities, we take a goal-
oriented view and aim to identify segments with a
mean length of approximately four sentences, rea-
soning that these will be long enough to exclude
some candidate NoRel instances, yet short enough to
exclude a non-trivial number of Contrasts and Cause
instances. We use an automatic topic segmentation
tool, LCSeg (Galley et al, 2003) setting parame-
ters so that the derived segments are of the approx-
imate desired length. Using these parameters, LC-
Seg produces topic segments with a mean length of
3.51 sentences over Gigaword, as opposed to 1.54
sentences for paragraph boundaries. Using a sim-
ple metric that assumes ?correct? segment bound-
aries always occur at paragraph boundaries, LCSeg
achieves 76% precision.
We rerun the instance mining step of TextRels
over the segmented training corpus, after adding the
segment-based constraints mentioned above to our
pattern set. Although our constraints reduce the
overall number of instances available in the corpus,
we extract for training the same number of instances
per RSR as listed in Table 1 (our non-segment-
constrained training set does not use all instances
in the corpus). Using the optimal parameter set-
tings determined in the previous section, we build
our models and classifiers based on these segment-
constrained instances.
To evaluate the classifiers built on the segment-
constrained instances, we can essentially follow the
same protocol as in our Parameter Optimization ex-
periments. However, we must choose whether to
use a held-out test set taken from the segment-
constrained instances (?Auto-S?) or the same test
set as used to evaluate our parameter optimization,
i.e. the (?Auto?) test set from unsegmented training
data. We decide to test on both. On the one hand,
segmentation is done automatically, so it is realistic
that given a ?real world? document, we can compute
segment boundaries to help our classification judg-
ments. On the other hand, testing on unsegmented
input allows us to compare more directly to the num-
bers from our previous section. Further, for tasks
which would apply RSR models outside of a single-
document context (e.g., for assessing coherence of
a synthesized abstract), a test on unsegmented input
may be more relevant. Table 2 shows the results for
the ?Seg? classifiers on both Auto test sets, as well
as the PDTB test set.
We observe that the performance of the classi-
fiers is indeed impacted by training on the segment-
constrained instances. On the PDTB test data, per-
formance using the segment-trained classifiers im-
proves in two of three cases, with a mean improve-
ment of 1.2%. However, because of the small size
of this set, this margin is not statistically significant.
On the automatically-extracted test data, the
segment-trained classifier is the best performer in
all three cases when using the segmented test data;
while the margin is not statistically significant for a
single classifier, the overall accurate-inaccurate im-
provement is significant (p < .05) using a Chi-
squared test. On the unsegmented test data, the
segment-trained classifiers are best in two of three
cases, but the overall accurate-inaccurate improve-
ment does not achieve statistical significance. We
conclude tentatively that a classifier trained on ex-
amples gleaned with topic-segment-augmented pat-
terns performs more accurately than our baseline
classifier.
7 Using Syntax
Whether or not we use topic segmentation to con-
strain our training instances, our patterns rely on
sentence boundaries and cue phrase anchors to de-
marcate the extents of the text spans which form
our RSR instances. However, an instance which
matches such a pattern often contains some amount
of text which is not relevant to the relation in ques-
tion. Consider:
Example 3 Wall Street investors, citing a drop in
oil prices because weakness in the automotive
sector, sold off shares in GM today.
In this case, a syntactically informed analysis
could be used to extract the constituents in the cause-
effect relationship from within the boldfaced nomi-
nal clause only, i.e. as ?a drop in oil prices? and
?weakness in the automotive sector.? However, the
output of our instance mining process simply splits
the string around the cue phrase ?because of? and
extracts the entire first and second parts of the sen-
tence as the constituents. Of course, this may be for
432
the best; in this case there is an implicit Cause rela-
tionship between the NP headed by drop and the sold
VP which our pattern-based rules inadvertently cap-
ture; our experiments here test whether such noise is
more helpful than hurtful.
Recognizing the potential complexity of using
syntactic phenomena, we reduce the dimensions of
the problem. First, we focus on single-sentence in-
stances; this means we analyze only Cause and Con-
trast patterns, since NoRel uses only multi-sentence
patterns. Second, within the Cause and Contrast in-
stances, we narrow our investigation to the most pro-
ductive pattern of each type (in terms of training in-
stances extracted), given that different syntactic phe-
nomena may be in play for different patterns. The
two patterns we use are ?W1 because W2? for Cause
(accounts for 54% of training instances) and ?W1
, but W2? for Contrast (accounts for 41% of train-
ing instances). Lastly, we limit the size of our train-
ing set because of parsing time demands. We use
the Collins parser (Collins, 1996) to parse 400,000
instances each of Cause and Contrast for our fi-
nal results. Compared with our other models, this
is approximately 43% of our total Cause instances
and 13% of our total Contrast instances. For the
NoRel model, we use a randomly selected subset of
400,000 instances from our training set. For all rela-
tions, we use the non-segment-constrained instance
set as the source of these instances.
7.1 Analyzing and Classifying Syntactic Errors
To analyze the possible syntactic bases for the type
of over-capturing behavior shown in Example 3, we
create a small development set of 100 examples each
from Cause and Contrast training examples which fit
the criteria just mentioned. We then manually iden-
tify and categorize any instances of over-capturing,
labeling the relation-relevant and irrelevant spans.
We find that 75% of Cause and 58% of Contrast
examples contain at least some over-capturing; we
observe several common reasons for over-capturing
that we characterize syntactically. For example, a
matrix clause with a verb of saying should not be
part of the RSR. Using automatic parses of these in-
stances created by we then design syntactic filtering
heuristics based on a manual examination of parse
trees of several examples from our development set.
For Contrast, we find that using the coordinat-
ing conjunction (CC) analysis of but, we can use a
straightforward rule which limits the extent of RSR
spans captured to the conjuncts/children of the CC
node, e.g. by capturing only the boldfaced clauses
in the following example:
Example 4 For the past six months, management
has been revamping positioning and strategy, but
also scaling back operations.
This heuristic successfully cuts out the irrelevant
temporal relative clause, retaining the relevant VPs
which are being contrasted. Note that the heuris-
tic is not perfect; ideally the adverb also would be
filtered here, but this is more difficult to generalize
since contentful adverbials, e.g. strategically should
not be filtered out.
For the because pattern, we capture the right-
hand span as any text in child(ren) nodes of the be-
cause IN node. We extend the left-hand span only
as far as the first phrasal (e.g. VP) or finite clause
(e.g. SBAR) node above the because node. Analyz-
ing Example 3, the heuristic correctly captures the
right-hand span; however, to the left of because, the
heuristic cuts too much, and misses the key noun
drop.
7.2 Error Analysis: Evaluating the Heuristics
The first question we ask is, how well do our
heuristics work in identifying the actual correct
RSR extents? We evaluate this against the Penn
Discourse TreeBank (PDTB), restricting ourselves
to discourse-annotated but and because sentences
which match the RSR patterns which are the sub-
ject of our syntactic filtering. Since the PDTB
is annotated on the same corpus as Penn Tree-
Bank (PTB), we separately evaluate the perfor-
mance of our heuristics using gold-standard PTB
parses (?PDTB-Gold?) versus the trees generated by
Collins? parser (?PDTB-Prs?). We extract our test
data from the PDTB data corresponding to section
23 of PTB, i.e. the standard testing section, so that
the difference between the gold-standard and real
parse trees is meaningful. Section 23 contains 60
annotated instances of but and 52 instances of be-
cause which we can use for this purpose. We define
the measurement of accuracy here in terms of word-
level precision/recall. That is, the set of words fil-
tered by our heuristics are compared to the ?correct?
433
Heuristic PDTB-Prs PDTB-Gold
Contrast 89.6 / 73.0 / 80.5 79.0 / 80.6 / 79.8
Cause 78.5 / 78.8 / 78.6 87.3 / 79.5 / 83.2
Table 3: Precision/Recall/F-measure of syntactic
heuristics under various data sets and settings as de-
scribed in Section 7.2.
words to cut, i.e. those which the annotated RSR ex-
tents exclude. The results of this analysis are shown
in Table 3.
We performed an analysis of our heuristics on
Section 24 of the PDTB. In that section, there are 74
relevant sentences: 20 sentences with because, and
54 sentences with but. Exactly half of all sentences
(37) have no problems in the application of the
heuristics (7 because sentences, 30 but sentences).
Among the remaining sentences, the main source of
problems is that our heuristics do not always remove
matrix clauses with verbs of saying (15 cases total, 8
of which are because sentences). For the but clauses,
our heuristics removed the subject in 12 cases where
the PDTB did not do so. Additionally, the heuristic
for but sentences does not correctly identify the sec-
ond conjunct in five cases (choosing instead a paren-
thetical, for instance).
In looking at our syntactic heuristics for the
Cause relationship, we see that they indeed elimi-
nate the most frequent source of discrepancies with
the PDTB, namely the false inclusion of a matrix
clause of saying, resulting in 15 out of 20 perfect
analyses.
We also evaluate the difference in performance
between the PDTB-Gold and PDTB-Prs perfor-
mance to determine to what extent using a parser
(as opposed to the Gold Standard) degrades the per-
formance of our heuristics. We find that in Sec-
tion 24, 13 out of 74 sentences contain a parsing
error in the relevant aspects, but the effects are typ-
ically small and result from well-known parser is-
sues, mainly attachment errors. As we can see in Ta-
ble 3, the heuristic performance using an automatic
parser degrades only slightly, and as such we can ex-
pect an automatic parser to contribute to improving
RSR classification (as indeed it does).
Pdtb Test Set Auto Test Set
U Syn P U Syn P
Cau/Con 59.6 60.5 54.5 66.3 65.8 60.8
Cau/NR 72.2 74.9 52.6 70.3 70.2 57.3
Con/NR 61.6 60.2 52.2 69.4 69.8 56.8
Table 4: Classifier accuracy for the Unfiltered (U),
Syntactically Filtered (Syn), and POS (P) models
described in Section 7.3, over PDTB and Auto test
sets. Baseline in all cases is 50%.
7.3 Classification Evaluation
We evaluate the impact of our syntactic heuristics on
classification over the Auto and PDTB test sets using
the same instance set of 400,000 training instances
per relation. However, each applies different filters
to the instances I before computing the frequencies
F (all other parameters use the same values; these
are set slightly differently than the optimized val-
ues discussed earlier because of the smaller train-
ing sets). In addition to an Unfiltered baseline, we
evaluate Filtered models obtained with our syntac-
tic heuristics for Cause and Contrast. To provide an
additional point of comparison, we also evaluate the
Part-of-Speech based filtering heuristic described by
Marcu and Echihabi, which retains only nouns and
verbs. Unlike the other filters, the POS-based filter-
ing is applied to the NoRel instances as well as the
Cause and Contrast instances. Table 4 summarizes
the results of the classifying the PDTB and Auto test
sets with these different models.
Before we examine the results, we note that the
syntactic heuristic cuts a large portion of training
data out. In terms of the total sum of frequencies in
Fcause, i.e. the word pairs extracted from all cause
instances, the syntactic filtering cuts out nearly half.
With this in mind, we see that while the syntac-
tic filtering achieves slightly lower mean accuracy as
compared to the Unfiltered baseline on the Auto test
set, the pairs it does keep appear to be used more ef-
ficiently (the differences are significant). Even with
this reduced training set, the syntactic heuristic im-
proves performance in two out of three cases on the
PDTB test set, including a 2.7 percent improvement
for the Cause vs NoRel classifier. However, due to
the small size of the PDTB test set, none of these
differences is statistically significant.
We posit that bias in the Auto set may cause this
434
difference in performance across training sets; spans
in the Auto set are not true arguments of the rela-
tion in the PDTB sense, but nonetheless occur reg-
ularly with the cue phrases used in instance mining
and thus are more likely to be present in the test set.
Lastly, we observe that the POS-based filtering
described by M&E performs uniformly poorly. We
have no explanation for this at present, given that
M&E?s results with this filter appear promising.
8 Conclusion
In this paper, we analyzed the problem of learning a
model of rhetorical-semantic relations. Building on
the work of Marcu and Echihabi, we first optimized
several parameters of their model, which we found
to have significant impact on classification accuracy.
We then focused on the quality of the automatically-
mined training examples, analyzing two techniques
for data filtering. The first technique, based on au-
tomatic topic segmentation, added additional con-
straints on the instance mining patterns; the sec-
ond used syntactic heuristics to cut out irrelevant
portions of extracted training examples. While the
topic-segmentation filtering approach achieves sig-
nificant improvement and the best results overall,
our analysis of the syntactic filtering approach indi-
cates that refined heuristics and a larger set of parsed
data can further improve those results. We would
also like to experiment with combining the two ap-
proaches, i.e. by applying the syntactic heuristics
to an instance set extracted using topic segmenta-
tion constraints. We conclude that our experiments
show that these techniques can successfully refine
RSR models and improve our ability to classify un-
known relations.
References
Jason Baldridge and Alex Lascarides. 2005. Probabilistic head-
driven parsing for discourse structure. In CoNLL 2005.
L. Carlson, D. Marcu, and M.E. Okurowski. 2001. Building a
discourse-tagged corpus in the framework of rhetorical struc-
ture theory. In Eurospeech 2001 Workshops.
M. Collins. 1996. A new statistical parser based on bigram
lexical dependencies. In ACL 1996.
M. Galley, K.R. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conversation.
In ACL 2003.
R. Girju. 2003. Automatic detection of causal relations for
question answering. In ACL 2003 Workshops.
Jerry R. Hobbs. 1979. Coherence and coreference. Cognitive
Science, 3(1):67?90.
E. Hovy and E. Maier. 1993. Parsimonious or profligate: How
many and which discourse structure relations? Unpublished
Manuscript.
A. Knott and T. Sanders. 1998. The classification of coherence
relations and their linguistic markers: An exploration of two
languages. Journal of Pragmatics, 30(2):135?175.
M. Lapata and A. Lascarides. 2004. Inferring sentence-internal
temporal relations. In HLT 2004.
W.C. Mann and S.A. Thompson. 1988. Rhetorical structure
theory: Towards a functional theory of text organization.
Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An unsupervised approach to
recognizing discourse relations. In ACL 2002.
D. Marcu. 1997. The Rhetorical Parsing, Summarization and
Generation of Natural Language Texts. Ph.D. thesis, Uni-
versity of Toronto, Department of Computer Science.
J. Martin. 1992. English Text: System and Structure. John
Benjamins.
K.R. McKeown. 1985. Text generation: Using discourse
strategies and focus constraints to generate natural lan-
guage text. Cambridge University Press.
J.D. Moore and P. Wiemer-Hastings. 2003. Discourse in
computational linguistics and artificial intelligence. In
M.A. Gernbacher A.G. Graesser and S.R. Goldman, ed-
itors, Handbook of Discourse Processes, pages 439?487.
Lawrence Erlbaum Associates.
M.G. Moser and J.D. Moore. 1996. Toward a synthesis of two
accounts of discourse structure. Computational Linguistics,
22(3):409?420.
R. Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi, and
B. Webber. 2006. The penn discourse treebank 1.0. anno-
tation manual. Technical Report IRCS-06-01, University of
Pennsylvania.
R. Soricut and D. Marcu. 2003. Sentence level discourse pars-
ing using syntactic and lexical information. In HLT-NAACL
2003.
C. Sporleder and A. Lascarides. 2005. Exploiting linguistic
cues to classify rhetorical relations. In RANLP 2005.
C. Sporleder and A. Lascarides. To Appear. Using automat-
ically labelled examples to classify rhetorical relations: An
assessment. Natural Language Engineering.
B. Wellner, J. Pustejovsky, C. Havasi, R. Sauri, and
A. Rumshisky. 2006. Classification of discourse coherence
relations: An exploratory study using multiple knowledge
sources. In SIGDial 2006.
F. Wolf and E. Gibson. 2005. Representing discourse coher-
ence: A corpus-based analysis. Computational Linguistics,
31(2):249?287.
435
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 777?785,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The viability of web-derived polarity lexicons
Leonid Velikovich Sasha Blair-Goldensohn Kerry Hannan Ryan McDonald
Google Inc., New York, NY
{leonidv|sasha|khannan|ryanmcd}@google.com
Abstract
We examine the viability of building large
polarity lexicons semi-automatically from the
web. We begin by describing a graph propa-
gation framework inspired by previous work
on constructing polarity lexicons from lexi-
cal graphs (Kim and Hovy, 2004; Hu and
Liu, 2004; Esuli and Sabastiani, 2009; Blair-
Goldensohn et al, 2008; Rao and Ravichan-
dran, 2009). We then apply this technique
to build an English lexicon that is signifi-
cantly larger than those previously studied.
Crucially, this web-derived lexicon does not
require WordNet, part-of-speech taggers, or
other language-dependent resources typical of
sentiment analysis systems. As a result, the
lexicon is not limited to specific word classes
? e.g., adjectives that occur in WordNet ?
and in fact contains slang, misspellings, multi-
word expressions, etc. We evaluate a lexicon
derived from English documents, both qual-
itatively and quantitatively, and show that it
provides superior performance to previously
studied lexicons, including one derived from
WordNet.
1 Introduction
Polarity lexicons are large lists of phrases that en-
code the polarity of each phrase within it ? either
positive or negative ? often with some score rep-
resenting the magnitude of the polarity (Hatzivas-
siloglou and McKeown, 1997; Wiebe, 2000; Turney,
2002). Though classifiers built with machine learn-
ing algorithms have become commonplace in the
sentiment analysis literature, e.g., Pang et al (2002),
the core of many academic and commercial senti-
ment analysis systems remains the polarity lexicon,
which can be constructed manually (Das and Chen,
2007), through heuristics (Kim and Hovy, 2004;
Esuli and Sabastiani, 2009) or using machine learn-
ing (Turney, 2002; Rao and Ravichandran, 2009).
Often lexicons are combined with machine learning
for improved results (Wilson et al, 2005). The per-
vasiveness and sustained use of lexicons can be as-
cribed to a number of reasons, including their inter-
pretability in large-scale systems as well as the gran-
ularity of their analysis.
In this work we investigate the viability of polar-
ity lexicons that are derived solely from unlabeled
web documents. We propose a method based on
graph propagation algorithms inspired by previous
work on constructing polarity lexicons from lexical
graphs (Kim and Hovy, 2004; Hu and Liu, 2004;
Esuli and Sabastiani, 2009; Blair-Goldensohn et al,
2008; Rao and Ravichandran, 2009). Whereas past
efforts have used linguistic resources ? e.g., Word-
Net ? to construct the lexical graph over which prop-
agation runs, our lexicons are constructed using a
graph built from co-occurrence statistics from the
entire web. Thus, the method we investigate can
be seen as a combination of methods for propagat-
ing sentiment across lexical graphs and methods for
building sentiment lexicons based on distributional
characteristics of phrases in raw data (Turney, 2002).
The advantage of breaking the dependence on Word-
Net (or related resources like thesauri (Mohammad
et al, 2009)) is that it allows the lexicons to include
non-standard entries, most notably spelling mistakes
and variations, slang, and multiword expressions.
The primary goal of our study is to understand the
characteristics and practical usefulness of such a lex-
icon. Towards this end, we provide both a qualitative
and quantitative analysis for a web-derived English
777
lexicon relative to two previously published lexicons
? the lexicon used in Wilson et al (2005) and the
lexicon used in Blair-Goldensohn et al (2008). Our
experiments show that a web-derived lexicon is not
only significantly larger, but has improved accuracy
on a sentence polarity classification task, which is
an important problem in many sentiment analysis
applications, including sentiment aggregation and
summarization (Hu and Liu, 2004; Carenini et al,
2006; Lerman et al, 2009). These results hold true
both when the lexicons are used in conjunction with
string matching to classify sentences, and when they
are included within a contextual classifier frame-
work (Wilson et al, 2005).
Extracting polarity lexicons from the web has
been investigated previously by Kaji and Kitsure-
gawa (2007), who study the problem exclusively for
Japanese. In that work a set of positive/negative sen-
tences are first extracted from the web using cues
from a syntactic parser as well as the document
structure. Adjectives phrases are then extracted from
these sentences based on different statistics of their
occurrence in the positive or negative set. Our work,
on the other hand, does not rely on syntactic parsers
or restrict the set of candidate lexicon entries to spe-
cific syntactic classes, i.e., adjective phrases. As a
result, the lexicon built in our study is on a different
scale than that examined in Kaji and Kitsuregawa
(2007). Though this hypothesis is not tested here, it
also makes our techniques more amenable to adap-
tation for other languages.
2 Constructing the Lexicon
In this section we describe a method to construct po-
larity lexicons using graph propagation over a phrase
similarity graph constructed from the web.
2.1 Graph Propagation Algorithm
We construct our lexicon using graph propagation
techniques, which have previously been investigated
in the construction of polarity lexicons (Kim and
Hovy, 2004; Hu and Liu, 2004; Esuli and Sabas-
tiani, 2009; Blair-Goldensohn et al, 2008; Rao and
Ravichandran, 2009). We assume as input an undi-
rected edge weighted graph G = (V,E), where
wij ? [0, 1] is the weight of edge (vi, vj) ? E. The
node set V is the set of candidate phrases for inclu-
sion in a sentiment lexicon. In practice,G should en-
code semantic similarities between two nodes, e.g.,
for sentiment analysis one would hope that wij >
wik if vi=good, vj=great and vk=bad. We also as-
sume as input two sets of seed phrases, denoted P
for the positive seed set and N for the negative seed
set. The common property among all graph propaga-
tion algorithms is that they attempt to propagate in-
formation from the seed sets to the rest of the graph
through its edges. This can be done using machine
learning, graph algorithms or more heuristic means.
The specific algorithm used in this study is given
in Figure 1, which is distinct from common graph
propagation algorithms, e.g., label propagation (see
Section 2.3). The output is a polarity vector pol ?
R|V | such that poli is the polarity score for the i
th
candidate phrase (or the ith node inG). In particular,
we desire pol to have the following semantics:
poli =
?
??
??
> 0 ith phrase has positive polarity
< 0 ith phrase has negative polarity
= 0 ith phrase has no sentiment
Intuitively, the algorithm works by computing both
a positive and a negative polarity magnitude for
each node in the graph, call them pol+i and pol
-
i.
These values are equal to the sum over the max
weighted path from every seed word (either posi-
tive or negative) to node vi. Phrases that are con-
nected to multiple positive seed words through short
yet highly weighted paths will receive high positive
values. The final polarity of a phrase is then set to
poli = pol
+
i ? ?pol
-
i, where ? a constant meant to
account for the difference in overall mass of positive
and negative flow in the graph. Thus, after the al-
gorithm is run, if a phrase has a higher positive than
negative polarity score, then its final polarity will be
positive, and negative otherwise.
There are some implementation details worth
pointing out. First, the algorithm in Figure 1 is writ-
ten in an iterative framework, where on each itera-
tion, paths of increasing lengths are considered. The
input variable T controls the max path length con-
sidered by the algorithm. This can be set to be a
small value in practice, since the multiplicative path
weights result in long paths rarely contributing to
polarity scores. Second, the parameter ? is a thresh-
old that defines the minimum polarity magnitude a
778
Input: G = (V,E), wij ? [0, 1],
P , N , ? ? R, T ? N
Output: pol ? R|V |
Initialize: poli,pol
+
i ,pol
-
i = 0, for all i
pol+i = 1.0 for all vi ? P and
pol-i = 1.0 for all vi ? N
1. set ?ij = 0 for all i, j
2. for vi ? P
3. F = {vi}
4. for t : 1 . . . T
5. for (vk, vj) ? E such that vk ? F
6. ?ij = max{?ij , ?ik ? wkj}
F = F ? {vj}
7. for vj ? V
8. pol+j =
?
vi?P
?ij
9. Repeat steps 1-8 using N to compute pol-
10. ? =
?
i pol
+
i /
?
i pol
-
i
11. poli = pol
+
i ? ?pol
-
i, for all i
12. if |poli| < ? then poli = 0.0, for all i
Figure 1: Graph Propagation Algorithm.
phrase must have to be included in the lexicon. Both
T and ? were tuned on held-out data.
To construct the final lexicon, the remaining
nodes ? those with polarity scores above ? ? are ex-
tracted and assigned their corresponding polarity.
2.2 Building a Phrase Graph from the Web
Graph propagation algorithms rely on the existence
of graphs that encode meaningful relationships be-
tween candidate nodes. Past studies on building po-
larity lexicons have used linguistic resources like
WordNet to define the graph through synonym and
antonym relations (Kim and Hovy, 2004; Esuli and
Sabastiani, 2009; Blair-Goldensohn et al, 2008;
Rao and Ravichandran, 2009). The goal of this study
is to examine the size and quality of polarity lexi-
cons when the graph is induced automatically from
documents on the web.
Constructing a graph from web-computed lexi-
cal co-occurrence statistics is a difficult challenge
in and of itself and the research and implementa-
tion hurdles that arise are beyond the scope of this
work (Alfonseca et al, 2009; Pantel et al, 2009).
For this study, we used an English graph where the
node set V was based on all n-grams up to length
10 extracted from 4 billion web pages. This list was
filtered to 20 million candidate phrases using a num-
ber of heuristics including frequency and mutual in-
formation of word boundaries. A context vector for
each candidate phrase was then constructed based
on a window of size six aggregated over all men-
tions of the phrase in the 4 billion documents. The
edge set E was constructed by first, for each po-
tential edge (vi, vj), computing the cosine similar-
ity value between context vectors. All edges (vi, vj)
were then discarded if they were not one of the 25
highest weighted edges adjacent to either node vi or
vj . This serves to both reduce the size of the graph
and to eliminate many spurious edges for frequently
occurring phrases, while still keeping the graph rela-
tively connected. The weight of the remaining edges
was set to the corresponding cosine similarity value.
Since this graph encodes co-occurrences over a
large, but local context window, it can be noisy for
our purposes. In particular, we might see a number
of edges between positive and negative sentiment
words as well as sentiment words and non-sentiment
words, e.g., sentiment adjectives and all other adjec-
tives that are distributionally similar. Larger win-
dows theoretically alleviate this problem as they en-
code semantic as opposed to syntactic similarities.
We note, however, that the graph propagation al-
gorithm described above calculates the sentiment of
each phrase as the aggregate of all the best paths to
seed words. Thus, even if some local edges are erro-
neous in the graph, one hopes that, globally, positive
phrases will be influenced more by paths from pos-
itive seed words as opposed to negative seed words.
Section 3, and indeed this paper, aims to measure
whether this is true or not.
2.3 Why Not Label Propagation?
Previous studies on constructing polarity lexicons
from lexical graphs, e.g., Rao and Ravichandran
(2009), have used the label propagation algorithm,
which takes the form in Figure 2 (Zhu and Ghahra-
mani, 2002). Label propagation is an iterative algo-
rithm where each node takes on the weighted aver-
age of its neighbour?s values from the previous iter-
ation. The result is that nodes with many paths to
seeds get high polarities due to the influence from
their neighbours. The label propagation algorithm
is known to have many desirable properties includ-
ing convergence, a well defined objective function
779
Input: G = (V,E), wij ? [0, 1], P , N
Output: pol ? R|V |
Initialize: poli = 1.0 for all vi ? P and
poli = ?1.0 for all vi ? N and
poli = 0.0 ?vi /? P ?N
1. for : t .. T
2. poli =
P
(vi,vj)?E
wij?polj
P
(vi,vj)
wij
, ?vi ? V
3. reset poli = 1.0 ?vi ? P
reset poli = ?1.0 ?vi ? N
Figure 2: The label propagation algorithm (Zhu and
Ghahramani, 2002).
(minimize squared error between values of adjacent
nodes), and an equivalence to computing random
walks through graphs.
The primary difference between standard label
propagation and the graph propagation algorithm
given in Section 2.1, is that a node with multiple
paths to a seed will be influenced by all these paths
in the label propagation algorithm, whereas only the
single path from a seed will influence the polarity
of a node in our proposed propagation algorithm ?
namely the path with highest weight. The intuition
behind label propagation seems justified. That is, if
a node has multiple paths to a seed, it should be re-
flected in a higher score. This is certainly true when
the graph is of high quality and all paths trustwor-
thy. However, in a graph constructed from web co-
occurrence statistics, this is rarely the case.
Our graph consisted of many dense subgraphs,
each representing some semantic entity class, such
as actors, authors, tech companies, etc. Problems
arose when polarity flowed into these dense sub-
graphs with the label propagation algorithm. Ulti-
mately, this flow would amplify since the dense sub-
graph provided exponentially many paths from each
node to the source of the flow, which caused a re-
inforcement effect. As a result, the lexicon would
consist of large groups of actor names, companies,
etc. This also led to convergence issues since the
polarity is divided proportional to the size of the
dense subgraph. Additionally, negative phrases in
the graph appeared to be in more densely connected
regions, which resulted in the final lexicons being
highly skewed towards negative entries due to the
influence of multiple paths to seed words.
For best path propagation, these problems were
less acute as each node in the dense subgraph would
only get the polarity a single time from each seed,
which is decayed by the fact that edge weights are
smaller than 1. Furthermore, the fact that edge
weights are less than 1 results in most long paths
having weights near zero, which in turn results in
fast convergence.
3 Lexicon Evaluation
We ran the best path graph propagation algorithm
over a graph constructed from the web using manu-
ally constructed positive and negative seed sets of
187 and 192 words in size, respectively. These
words were generated by a set of five humans and
many are morphological variants of the same root,
e.g., excel/excels/excelled. The algorithm produced
a lexicon that contained 178,104 entries. Depending
on the threshold ? (see Figure 1), this lexicon could
be larger or smaller. As stated earlier, our selection
of ? and all hyperparameters was based on manual
inspection of the resulting lexicons and performance
on held-out data.
In the rest of this section we investigate the prop-
erties of this lexicon to understand both its general
characteristics as well as its possible utility in sen-
timent applications. To this end we compare three
different lexicons:
1. Wilson et al: Described in Wilson et al
(2005). Lexicon constructed by combining the
lexicon built in Riloff and Wiebe (2003) with
other sources1. Entries are are coarsely rated
? strong/weak positive/negative ? which we
weighted as 1.0, 0.5, -0.5, and -1.0 for our ex-
periments.
2. WordNet LP: Described in Blair-Goldensohn
et al (2008). Constructed using label propaga-
tion over a graph derived from WordNet syn-
onym and antonym links. Note that label prop-
agation is not prone to the kinds of errors dis-
cussed in Section 2.3 since the lexical graph is
derived from a high quality source.
3. Web GP: The web-derived lexicon described
in Section 2.1 and Section 2.2.
1See http://www.cs.pitt.edu/mpqa/
780
3.1 Qualitative Evaluation
Table 1 breaks down the lexicon by the number of
positive and negative entries of each lexicon, which
clearly shows that the lexicon derived from the web
is more than an order of magnitude larger than pre-
viously constructed lexicons.2 This in and of it-
self is not much of an achievement if the additional
phrases are of poor quality. However, in Section 3.2
we present an empirical evaluation that suggests that
these terms provide both additional and useful in-
formation. Table 1 also shows the recall of the each
lexicon relative to the other. Whereas the Wilson
et al (2005) and WordNet lexicon have a recall of
only 3% relative to the web lexicon, the web lexi-
con has a recall of 48% and 70% relative to the two
other lexicons, indicating that it contains a signifi-
cant amount of information from the other lexicons.
However, this overlap is still small, suggesting that
a combination of all the lexicons could provide the
best performance. In Section 3.2 we investigate this
empirically through a meta classification system.
Table 2 shows the distribution of phrases in the
web-derived lexicon relative to the number of to-
kens in each phrase. Here a token is simply defined
by whitespace and punctuation, with punctuation
counting as a token, e.g., ?half-baked? is counted as
3 tokens. For the most part, we see what one might
expect, as the number of tokens increases, the num-
ber of corresponding phrases in the lexicon also de-
creases. Longer phrases are less frequent and thus
will have both fewer and lower weighted edges to
adjacent nodes in the graph. There is a single phrase
of length 9, which is ?motion to dismiss for failure
to state a claim?. In fact, the lexicon contains quite
a number of legal and medical phrases. This should
not be surprising, since in a graph induced from the
web, a phrase like ?cancer? (or any disease) should
be distributionally similar to phrases like ?illness?,
?sick?, and ?death?, which themselves will be simi-
lar to standard sentiment phrases like ?bad? and ?ter-
rible?. These terms are predominantly negative in
the lexicon representing the broad notion that legal
and medical events are undesirable.
2This also includes the web-derived lexicon of (Kaji and Kit-
suregawa, 2007), which has 10K entries. A recent study by
Mohammad et al (2009) generated lexicons from thesauri with
76K entries.
Phrase length 1 2 3
# of phrases 37,449 108,631 27,822
Phrase length 4 5 6 7 8 9
# of phrases 3,489 598 71 29 4 1
Table 2: Number of phrases by phrase length in lexicon
built from the web.
Perhaps the most interesting characteristic of the
lexicon is that the most frequent phrase length is 2
and not 1. The primary reason for this is an abun-
dance of adjective phrases consisting of an adverb
and an adjective, such as ?more brittle? and ?less
brittle?. Almost every adjective of length 1 is fre-
quently combined in such a way on the web, so it
not surprising that we see many of these phrases
in the lexicon. Ideally we would see an order on
such phrases, e.g., ?more brittle? has a larger neg-
ative polarity than ?brittle?, which in turn has a
larger negative polarity than ?less brittle?. However,
this is rarely the case and usually the adjective has
the highest polarity magnitude. Again, this is eas-
ily explained. These phrases are necessarily more
common and will thus have more edges with larger
weights in the graph and thus a greater chance of ac-
cumulating a high sentiment score. The prominence
of such phrases suggests that a more principled treat-
ment of them should be investigated in the future.
Finally, Table 3 presents a selection of phrases
from both the positive and negative lexicons cate-
gorized into revealing verticals. For both positive
and negative phrases we present typical examples of
phrases ? usually adjectives ? that one would expect
to be in a sentiment lexicon. These are phrases not
included in the seed sets. We also present multiword
phrases for both positive and negative cases, which
displays concretely the advantage of building lexi-
cons from the web as opposed to using restricted lin-
guistic resources such as WordNet. Finally, we show
two special cases. The first is spelling variations
(and mistakes) for positive phrases, which were far
more prominent than for negative phrases. Many of
these correspond to social media text where one ex-
presses an increased level of sentiment by repeat-
ing characters. The second is vulgarity in negative
phrases, which was far more prominent than for pos-
itive phrases. Some of these are clearly appropri-
781
Recall wrt other lexicons
All Phrases Pos. Phrases Neg. Phrases Wilson et al WordNet LP Web GP
Wilson et al 7,628 2,718 4,910 100% 37% 2%
WordNet LP 12,310 5,705 6,605 21% 100% 3%
Web GP 178,104 90,337 87,767 70% 48% 100%
Table 1: Lexicon statistics. Wilson et al is the lexicon used in Wilson et al (2005), WordNet LP is the lexicon
constructed by Blair-Goldensohn et al (2008) that uses label propagation algorithms over a graph constructed through
WordNet, and Web GP is the web-derived lexicon from this study.
POSITIVE PHRASES NEGATIVE PHRASES
Typical Multiword expressions Spelling variations Typical Multiword expressions Vulgarity
cute once in a life time loveable dirty run of the mill fucking stupid
fabulous state - of - the - art nicee repulsive out of touch fucked up
cuddly fail - safe operation niice crappy over the hill complete bullshit
plucky just what the doctor ordered cooool sucky flash in the pan shitty
ravishing out of this world coooool subpar bumps in the road half assed
spunky top of the line koool horrendous foaming at the mouth jackass
enchanting melt in your mouth kewl miserable dime a dozen piece of shit
precious snug as a bug cozy lousy pie - in - the - sky son of a bitch
charming out of the box cosy abysmal sick to my stomach sonofabitch
stupendous more good than bad sikk wretched pain in my ass sonuvabitch
Table 3: Example positive and negative phrases from web lexicon.
ate, e.g., ?shitty?, but some are clearly insults and
outbursts that are most likely included due to their
co-occurrence with angry texts. There were also a
number of derogatory terms and racial slurs in the
lexicon, again most of which received negative sen-
timent due to their typical disparaging usage.
3.2 Quantitative Evaluation
To determine the practical usefulness of a polarity
lexicon derived from the web, we measured the per-
formance of the lexicon on a sentence classifica-
tion/ranking task. The input is a set of sentences and
the output is a classification of the sentences as be-
ing either positive, negative or neutral in sentiment.
Additionally, the system outputs two rankings, the
first a ranking of the sentence by positive polarity
and the second a ranking of the sentence by negative
polarity. Classifying sentences by their sentiment is
a subtask of sentiment aggregation systems (Hu and
Liu, 2004; Gamon et al, 2005). Ranking sentences
by their polarity is a critical sub-task in extractive
sentiment summarization (Carenini et al, 2006; Ler-
man et al, 2009).
To classify sentences as being positive, negative
or neutral, we used an augmented vote-flip algo-
rithm (Choi and Cardie, 2009), which is given in
Figure 3. This intuition behind this algorithm is sim-
ple. The number of matched positive and negative
phrases from the lexicon are counted and whichever
has the most votes wins. The algorithm flips the de-
cision if the number of negations is odd. Though this
algorithm appears crude, it benefits from not relying
on threshold values for neutral classification, which
is difficult due to the fact that the polarity scores in
the three lexicons are not on the same scale.
To rank sentences we defined the purity of a sen-
tence X as the normalized sum of the sentiment
scores for each phrase x in the sentence:
purity(X) =
?
x?X polx
? +
?
x?X |polx|
This is a normalized score in the range [?1, 1]. In-
tuitively, sentences with many terms of the same po-
larity will have purity scores at the extreme points of
the range. Before calculating purity, a simple nega-
tion heuristic was implemented that reversed the
sentiment scores of terms that were within the scope
of negations. The term ? helps to favor sentences
with multiple phrase matches. Purity is a common
metric used for ranking sentences for inclusion in
sentiment summaries (Lerman et al, 2009). Purity
and negative purity were used to rank sentences as
being positive and negative sentiment, respectively.
The data used in our initial English-only experi-
782
Lexicon Classifier Contextual Classifier
Positive Negative Positive Negative
P R AP P R AP P R AP P R AP
Wilson et al 56.4 61.8 60.8 58.1 39.0 59.7 74.5 70.3 76.2 80.7 70.1 81.2
WordNet LP 50.9 61.7 62.0 54.9 36.4 59.7 72.0 72.5 75.7 78.0 69.8 79.3
Web GP 57.7 65.1? 69.6? 60.3 42.9 68.5? 74.1 75.0? 79.9? 80.5 72.6? 82.9?
Meta Classifier - - - - - - 76.6? 74.7 81.2? 81.8? 72.2 84.1?
Table 4: Positive and negative precision (P), recall (R), and average precision (AP) for three lexicons using either
lexical matching or contextual classification strategies. ?Web GP is statistically significantly better than Wilson et al
and WordNet LP (p < 0.05). ?Meta Classifier is statistically significantly better than all other systems (p < 0.05).
Input: Scored lexicon pol, negation list NG,
input sentence X
Output: sentiment ? {POS, NEG, NEU}
1. set p, n, ng = 0
2. for x ? X
3. if polx > 0 then p++
4. else if polx < 0 then n++
5. else if x ? NG then ng++
6. flip = (ng % 2 == 1) //ng is odd
7. if (p > n & ?flip) ? (n > p & flip)
return POS
8. else if (p > n & flip) ? (n > p & ?flip)
return NEG
19. return NEU
Figure 3: Vote-flip algorithm (Choi and Cardie, 2009).
ments were a set of 554 consumer reviews described
in (McDonald et al, 2007). Each review was sen-
tence split and annotated by a human as being pos-
itive, negative or neutral in sentiment. This resulted
in 3,916 sentences, with 1,525, 1,542 and 849 posi-
tive, negative and neutral sentences, respectively.
The first six columns of Table 4 shows: 1) the pos-
itive/negative precision-recall of each lexicon-based
system where sentence classes were determined us-
ing the vote-flip algorithm, and 2) the average preci-
sion for each lexicon-based system where purity (or
negative purity) was used to rank sentences. Both
the Wilson et al and WordNet LP lexicons perform
at a similar level, with the former slightly better, es-
pecially in terms of precision. The web-derived lex-
icon, Web GP, outperforms the other two lexicons
across the board, in particular when looking at av-
erage precision, where the gains are near 10% ab-
solute. If we plot the precision-recall graphs using
purity to classify sentences ? as opposed to the vote-
flip algorithm, which only provides an unweighted
classification ? we can see that at almost all recall
levels the web-derived lexicon has superior preci-
sion to the other lexicons (Figure 4). Thus, even
though the web-derived lexicon is constructed from
a lexical graph that contains noise, the graph prop-
agation algorithms appear to be fairly robust to this
noise and are capable of producing large and accu-
rate polarity lexicons.
The second six columns of Table 4 shows the per-
formance of each lexicon as the core of a contextual
classifier (Wilson et al, 2005). A contextual classi-
fier is a machine learned classifier that predicts the
polarity of a sentence using features of that sentence
and its context. For our experiments, this was a max-
imum entropy classifier trained and evaluated us-
ing 10-fold cross-validation on the evaluation data.
The features included in the classifier were the pu-
rity score, the number of positive and negative lex-
icon matches, and the number of negations in the
sentence, as well as concatenations of these features
within the sentence and with the same features de-
rived from the sentences in a window of size 1.
For each sentence, the contextual classifier pre-
dicted either a positive, negative or neutral classifi-
cation based on the label with highest probability.
Additionally, all sentences were placed in the posi-
tive and negative sentence rankings by the probabil-
ity the classifier assigned to the positive and negative
classes, respectively. Mirroring the results of Wil-
son et al (2005), we see that contextual classifiers
improve results substantially over lexical matching.
More interestingly, we see that the a contextual clas-
sifier over the web-derived lexicons maintains the
performance edge over the other lexicons, though
the gap is smaller. Figure 5 plots the precision-recall
curves for the positive and negative sentence rank-
783
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et alWordNet LPWeb GP
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et alWordNet LPWeb GP
Figure 4: Lexicon classifier precision/recall curves for positive (left) and negative (right) classes.
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et al CCWordNet LP CCWeb GP CCMeta Classifier
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et al CCWordNet LP CCWeb GP CCMeta Classifier
Figure 5: Contextual classifier precision/recall curves for positive (left) and negative (right) classes
ings, again showing that at almost every level of re-
call, the web-derived lexicon has higher precision.
For a final English experiment we built a meta-
classification system that is identical to the contex-
tual classifiers, except it is trained using features de-
rived from all lexicons. Results are shown in the
last row of Table 4 and precision-recall curves are
shown in Figure 5. Not surprisingly, this system has
the best performance in terms of average precision
as it has access to the largest amount of information,
though its performance is only slightly better than
the contextual classifier for the web-derived lexicon.
4 Conclusions
In this paper we examined the viability of senti-
ment lexicons learned semi-automatically from the
web, as opposed to those that rely on manual anno-
tation and/or resources such as WordNet. Our quali-
tative experiments indicate that the web derived lex-
icon can include a wide range of phrases that have
not been available to previous systems, most no-
tably spelling variations, slang, vulgarity, and multi-
word expressions. Quantitatively, we observed that
the web derived lexicon had superior performance
to previously published lexicons for English clas-
sification. Ultimately, a meta classifier that incor-
porates features from all lexicons provides the best
performance. In the future we plan to investigate the
construction of web-derived lexicons for languages
other than English, which is an active area of re-
search (Mihalcea et al, 2007; Jijkoun and Hofmann,
2009; Rao and Ravichandran, 2009). The advantage
of the web-derived lexicons studied here is that they
do not rely on language specific resources besides
unlabeled data and seed lists. A primary question is
whether such lexicons improve performance over a
translate-to-English strategy (Banea et al, 2008).
Acknowledgements: The authors thank Andrew
Hogue, Raj Krishnan and Deepak Ravichandran for
insightful discussions about this work.
784
References
E. Alfonseca, K. Hall, and S. Hartmann. 2009. Large-
scale computation of distributional similarities for
queries. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT).
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. 2008.
Multilingual subjectivity analysis using machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
S. Blair-Goldensohn, K. Hannan, R. McDonald, T. Ney-
lon, G.A. Reis, and J. Reynar. 2008. Building a senti-
ment summarizer for local service reviews. In NLP in
the Information Explosion Era.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-document
summarization of evaluative text. In Proceedings of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Y. Choi and C. Cardie. 2009. Adapting a polarity lexicon
using integer linear programming for domain-specific
sentiment classification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
S.R. Das and M.Y. Chen. 2007. Yahoo! for Amazon:
Sentiment extraction from small talk on the web. Man-
agement Science, 53(9):1375?1388.
A Esuli and F. Sabastiani. 2009. SentiWordNet: A pub-
licly available lexical resource for opinion mining. In
Proceedings of the Language Resource and Evaluation
Conference (LREC).
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free text.
In Proceedings of the 6th International Symposium on
Intelligent Data Analysis (IDA).
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics (EACL).
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proceedings of the International
Conference on Knowledge Discovery and Data Min-
ing (KDD).
V.B. Jijkoun and K. Hofmann. 2009. Generating a non-
english subjectivity lexicon: Relations that matter. In
Proceedings of the European Chapter of the Associa-
tion for Computational Linguistics (EACL).
N. Kaji and M. Kitsuregawa. 2007. Building lexicon for
sentiment analysis from massive collection of HTML
documents. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL).
S.M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of the International
Conference on Computational Linguistics (COLING).
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: Evaluat-
ing and learning user preferences. In Proceedings of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured models for fine-to-coarse
sentiment analysis. In Proceedings of the Annual Con-
ference of the Association for Computational Linguis-
tics (ACL).
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Annual Conference of
the Association for Computational Linguistics (ACL).
S. Mohammad, B. Dorr, and C. Dunne. 2009. Generat-
ing high-coverage semantic orientation lexicons from
overtly marked words and a thesaurus. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learn-
ing techniques. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
D. Rao and D. Ravichandran. 2009. Semi-Supervised
Polarity Lexicon Induction. In Proceedings of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL).
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
P. Turney. 2002. Thumbs up or thumbs down? Sentiment
orientation applied to unsupervised classification of re-
views. In Proceedings of the Annual Conference of the
Association for Computational Linguistics (ACL).
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In Proceedings of the National Conference on
Artificial Intelligence (AAAI).
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
report, CMU CALD tech report CMU-CALD-02.
785
