Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 176?182,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Bio-inspired Approach for Multi-Word Expression Extraction
Jianyong Duan, Ruzhan Lu
Weilin Wu, Yi Hu
Department of Computer Science
Shanghai Jiao Tong University
Shanghai, 200240, P.R. China
duanjy@hotmail.com
{lu-rz,wl-wu,huyi}@cs.sjtu.edu.cn
Yan Tian
School of Foreign Languages
Department of Computer Science
Shanghai Jiao Tong University
Shanghai, 200240, P.R. China
tianyan@sjtu.edu.cn
Abstract
This paper proposes a new approach for
Multi-word Expression (MWE)extraction
on the motivation of gene sequence align-
ment because textual sequence is simi-
lar to gene sequence in pattern analy-
sis. Theory of Longest Common Subse-
quence (LCS) originates from computer
science and has been established as affine
gap model in Bioinformatics. We per-
form this developed LCS technique com-
bined with linguistic criteria in MWE ex-
traction. In comparison with traditional
n-gram method, which is the major tech-
nique for MWE extraction, LCS approach
is applied with great efficiency and per-
formance guarantee. Experimental results
show that LCS-based approach achieves
better results than n-gram.
1 Introduction
Language is under continuous development. Peo-
ple enlarge vocabulary and let words carry more
meanings. Meanwhile the language also devel-
ops larger lexical units to carry specific meanings;
specifically MWE?s, which include compounds,
phrases, technical terms, idioms and collocations,
etc. The MWE has relatively fixed pattern because
every MWE denotes a whole concept. In compu-
tational view, the MWE repeats itself constantly in
corpus(Taneli,2003).
The extraction of MWE plays an important role
in several areas, such as machine translation (Pas-
cale,1997), information extraction (Kalliopi,2000)
etc. On the other hand, there is also a need
for MWE extraction in a much more widespread
scenario namely that of human translation and
technical writing. Many efforts have been de-
voted to the study of MWE extraction (Beat-
rice,2003; Ivan,2002; Jordi,2001). These statis-
tical methods detect MWE by frequency of can-
didate patterns. Linguistic information as a filter-
ing strategy is also performed to improve precision
by ranking their candidates (Violeta,2003; Ste-
fan,2004; Arantza,2002). Some measures based
on advance statistical methods are also used,
such as mutual expectation with single statis-
tic model (Paul,2005),C-value/NC-value method
(Katerina,2000),etc.
Frequent information is the original data for
further MWE extraction. Most approaches adopt
n-gram technique(Daniel,1977; Satanjeev,2003;
Makoto,1994). n-gram concerns about one se-
quence for each time. Every sequence can be
cut into some segments with varied lengths be-
cause any length of segment has the possibility to
become candidate MWE. The larger the context
window is, the more difficulty its parameters ac-
quire. Thus data sparseness problem deteriorates.
Another problem arises from the flexible MWE
which can be separated by an arbitrary number of
blanks, for instance, ?make. . . . . . decision?. These
models cannot effectively distinguish all kinds of
variations in flexible MWE.
On the consideration of relations between tex-
tual sequence and gene sequence, we propose a
new bio-inspired approach for MWE identifica-
tion. Both statistical and linguistic information are
incorporated into this model.
2 Multi-word Expression
Multi-word Expression( in general, term) as the
linguistic representation of concepts, also has
some special statistical features. The component
words of terms co-occur in the same context fre-
176
quently. MWE extraction can be viewed as a prob-
lem of pattern extraction. It has two major phases.
The first phase is to search the candidateMWEs by
their frequent occurrence in the corpus. The sec-
ond phase is to filter true MWEs from noise candi-
dates. Filtering process involves linguistic knowl-
edge and some intelligent observations.
MWE can be classified into strict patterns and
flexible patterns by structures of their component
words(Joaquim,1999). For example, a textual se-
quence s = w1w2 ? ? ?wi ? ? ?wn, may contain two
kinds of patterns:
Strict pattern: pi = wiwi+1wi+2
Flexible pattern: pj = wiunionsqwi+2unionsqwi+4, pk =
wi unionsq unionsqwi+3wi+4
where unionsq denotes the variational or active ele-
ment in pattern. The flexible pattern extraction is
always a bottleneck for MWE extraction for lack
of good knowledge of global solution.
3 Algorithms for MWE Extraction
3.1 Pure Mathematical Method
Although sequence alignment algorithm has been
well-developed in bioinformatics (Michael,2003),
(Knut,2000), (Hans,1999), it was rarely reported
in MWE extraction. In fact, it also applies to
MWE extraction especially for complex struc-
tures.
Algorithm.1.
1. Input:tokenlized textual sequences Q =
{s1, s2, ? ? ? , sn}
2. Initionalization : pool, ? = {?k},?
3. Computation:
I. Pairwise sequence alignment
for all si, sj ? Q, si 6= sj
Similarity(si, sj)
Align(si, sj)
path(li,lj)?? {li, lj , ck}
pool ? pool ? {(li, ck), (lj , ck)}
? ? ? ? ck
II. Creation of consistent set
for all ck ? ?, (li, ck) ? pool
?k ? ?k + {li}
pool ? pool ? (li, ck)
III. Multiple sequence alignment
for all ?k
star align(?k) ? MWU ? ?
? ?MWU
4. Output: ?
Our approach is directly inspired by gene se-
quence alignment as algorithm. 1. showed. The
textual sequence should be preprocessed before in-
put. For example, plurals recognition is a rela-
tively simple task for computers which just need
to check if the word accord with the general rule
including rule (+s) and some alternative rules (-y +
ies), etc. A set of tense forms, such as past, present
and future forms, are also transformed into origi-
nal forms. These tokenlized sequences will im-
prove extraction quality.
Pairwise sequence alignment is a crucial step.
Our algorithm uses local alignment for textual se-
quences. The similarity score between s[1 . . . i]
and t[1 . . . i] can be computed by three arrays
G[i, j], E[i, j] ,F[i, j] and zero, where entry ?(x, y)
means word x matches with word y; V[i, j] de-
notes the best score of entry ?(x, y); G[i, j] de-
notes s[i] matched with t[j]:?(s[i], t[j]); E[i, j]
denotes a blank of string s matched with t[j] :
?(unionsq, t[j]); F [i, j] denotes s[i] matched with a
blank of string t : ?(s[i],unionsq).
Initialization:
V [0, 0] = 0; V [i, 0] = E[i, 0] = 0; 1 ? i ?
m. V [0, j] = F [0, j] = 0; 1 ? j ? n.
A dynamic programming solution:
V [i, j] = max{G[i, j], E[i, j], G[i, j], 0};
G[i, j] = ?(i, j) + max
?
?
?
?
?
?
?
?
?
G[i? 1, j ? 1]
E[i? 1, j ? 1]
F [i? 1, j ? 1]
0
E[i, j] = max
?
?
?
?
?
?
?
?
?
?(h + g) + G[i, j ? 1]
? g + E[i, j ? 1]
?(h + g) + F [i, j ? 1]
0
F [i, j] = max
?
?
?
?
?
?
?
?
?
?(h + g) + G[i? 1, j]
?(h + g) + E[i? 1, j]
? g + F [i? 1, j]
0
Here we explain the meaning of these arrays:
I. G[i, j] includes the entry ?(i, j), it denotes
the sum score is the last row plus the max-
imal score between prefix s[1 . . . i ? 1] and
t[1 . . . j ? 1].
177
II. Otherwise the related prefixes s[1 . . . i] and
t[1 . . . j ? 1] are needed1. They are used to
check the first blank or additional blank in or-
der to give appropriate penalty.
a. ForG[i, j?1] and F [i, j?1], they don?t
end with a blank in string s. The blank
s[i] is the first blank. Its score isG[i, j?
1] (or F [i, j ? 1]) minus (h + g).
b. For E[i, j ? 1],The blank is the addi-
tional blank which should be only sub-
tracted g.
In the maximum entry, it records the best score
of optimum local alignment. This entry can be
viewed as the started point of alignment. Then
we backtrack entries by checking arrays which are
generated from dynamic programming algorithm.
When the score decrease to zero, alignment exten-
sion terminates. Finally, the similarity and align-
ment results are easily acquired.
Lots of aligned segments are extracted from
pairwise alignment. Those segments with com-
mon component words (ck) will be collected into
the same set. It is called as consistent set for
further multiple sequence alignment. These con-
sistent sets collect similar sequences with score
greater than certain threshold.
We perform star-alignment in multiple se-
quence alignment. The center sequence in the con-
sistent set which has the highest score in com-
parison with others, is picked out from this set.
Then all the other sequences gather to the cen-
ter sequence with the technique of ?once a blank,
always a blank?. These aligned sequences form
common regions with n-column or a column. Ev-
ery column contains one or more words. Calcula-
tion of dot-matrices is a widespread tool for com-
mon region analysis. Dot-plot agreement is de-
veloped to identify common patterns and reliably
aligned regions in a set of related sequences. If
several plots calculate consistently in a sequence
set, it displays the similarity among them. It in-
creases credibility of extracted pattern in this con-
sistent set. Finally MWE with detailed pattern
emerges from this aligned sequence set.
1Analysis approaches for F [i, j] and E[i, j] are the same,
here only E[i, j] is given its detailed explanation.
3.2 Linguistic Knowledge Combination
3.2.1 Heuristic Knowledge
Original candidate set is noise. Many meaning-
less patterns are extracted from corpus. Some lin-
guistic rules (Argamon,1999) are introduced into
our model. It is observed that candidate pattern
should contain content words. Some patterns are
only organized by pure function words, such as the
most frequent patterns ?the to?, ?of the?. These
patterns should be moved out from the candidate
set. Filter table with certain words is also per-
formed. For example, some words, like ?then?,
cannot occur in the beginning position of MWE.
These filters will reduce the number of noise pat-
terns in great extent.
3.2.2 Embedded Base Phrase detection
Short textual sequence is apt to produce frag-
ments of MWE because local alignment ends pat-
tern extension when similarity score reduces to
zero. The matched component words increase
similarity score while unmatched words decrease
it. The similarity scores of candidates in textual
sequences are lower for lack of matched compo-
nent words. Without accumulation of higher sim-
ilarity score, pattern extension terminates quickly.
Pattern extension becomes especially sensitive to
unmatched words. Some isolated fragments are
generated in this circumstance. One solution is to
give higher scores for matched component words.
It strengthens pattern extension ability at the ex-
pense of introducing noise.
We propose Embedded base phrase(EBP) de-
tection as algorithm.2. It improves pattern ex-
traction by giving lower penalty for longer base
phrase. EBP is the base phrase in a gap (Changn-
ing,2000). It does not contain other phrase recur-
sively. Good quality of MWE should avoid irrela-
tive unit in its gap. The penalty function discerns
the true EBP and irrelative unit in a gap only by
length information. Longer gap means more irrel-
ative unit. It builds a rough penalty model for lack
of semantic information. We improve this model
by POS information. POS tagged textual sequence
is convenient to grammatical analysis. True EBP2
gives comparatively lower penalty.
Algorithm.2.
1. Input: LCS of sl, sk
2The performance of our EBP tagger is 95% accuracy for
base noun phrase and 90% accuracy for general use.
178
2. Check breakpoint in LCS
i. Anchor neighbored common words and
denote gaps
for all ws = wp, wt = wq
if ws ? ls, wt ? lt, ls 6= lt
denote gst, gpq
ii. Detect EBP in gaps
gst
EBP?? g?st, gpq
EBP?? g?pq
iii. Compute new similariy matrix in gaps
similarity(g?st, g?pq)
3. Link broken segment
if path(g?st, g?pq)
lst = ls + lt, lpq = lp + lq
For textual sequence: w1w2 ? ? ?wn, and its
corresponding POS tagged sequence: t1t2 ? ? ? tn,
we suppose [wi ? ? ?wj ] is a gap from wi to wj
in sequence ? ? ? wi?1 [wi ? ? ?wj ]wj ? ? ? . The
corresponding tag sequence is [ti ? ? ? tj ] . We
only focus on EBP analysis in a gap instead of
global sequence. Context Free Grammar (CFG)
is employed in EBP detection. CFG rules follow
this form:
(1)EBP ? adj. + noun
(2)EBP ? noun + ?of? + noun
(3)EBP ? adv. + adj.
(4)EBP ? art. + adj. + noun
? ? ?
The sequences inside breakpoint of LCS are an-
alyzed by EBP detection. True base phrase will
be given lower penalty. When the gap penalty for
breakpoint is lower than threshold, the broken seg-
ment reunites. Based on experience knowledge,
when the length of a gap is less than four words,
EBP detection using CFG can gain good results.
Lower penalty for true EBP will help MWE to
emerge from noise pattern easily.
4 Experiments
4.1 Resources
A large amount of free texts are collected in order
to meet the need of MWE extraction. These texts
are downloaded from internet with various aspects
including art, entertainment, military, business,
etc. Our corpus size is 200, 000 sentences. The
average sentence length is 15 words in corpus.
In addition, result evaluation is a hard job. Its
difficulty comes from two aspects. Firstly, MWE
identification for test corpus is a kind of labor-
intensive business. The judgment of MWEs re-
quires great efforts of domain expert. It is hard and
boring to make a standard test corpus for MWE
identification use. It is a bottleneck for large scales
use. Secondly it relates to human cognition in psy-
chological world. It is proved by experience that
various opinions cannot simply be judged true or
false. As a compromise way, gold standard set
can be established by some accepted resources, for
example, WordNet, as an online lexical reference
system, including many compounds and phrases.
Some terms extracted from dictionaries are also
employed in our experiments. There are nearly
70,000 MWEs in our list.
4.2 Results and Discussion
4.2.1 Close Test
We created a closed test set of 8,000 sen-
tences. MWEs in corpus are extracted by man-
ual work. Every measure in both n-gram and LCS
approaches complies with the same threshold, for
example threshold for frequency is five times.Two
conclusions are drawn from Tab.1.
Firstly, LCS has higher recall than n-gram but
lower precision on the contrary. In close test set,
LCS recall is higher than n-gram. LCS unifies all
the cases of flexible patterns by GAM. However
n-gram only considers limited flexible patterns be-
cause of model limitation. LCS nearly includes
all the n-gram results. Higher recall decreases its
precision to a certain extent because some flexible
patterns are noisier than strict patterns. Flexible
patterns tend to be more irrelevant than strict pat-
terns. The GAM just provides a wiser choice for
all flexible patterns by its gap penalty function. N-
gram gives up analysis on many flexible patterns
without further ado. N-gram ensures its precision
by taking risk of MWE loss .
Secondly, advanced evaluation criterion can
place more MWEs in the front rank of candi-
date list. Evaluation metrics for extracted pat-
terns play an important role in MWE extraction.
Many criteria, which are reported with better per-
formances, are tested. MWE identification is sim-
ilar to IR task. These measures have their own
advantages to move interested patterns forward
in the candidate list. For example, Frequency
data contains much noise. True mutual infor-
179
Table 1: Close Test for N-gram and LCS Approaches
Measure N-Gram LCS
Precision Recall F-Measure Precision Recall F-Measure
(%) (%) (%) (%) (%) (%)
Frequency 35.2 38.0 36.0 32.1 48.2 38.4
TMI 44.7 56.2 49.1 43.2 62.1 51.4
ME 51.6 52.6 51.2 44.7 65.2 52.0
Rankratio 62.1 61.5 61.1 57.0 83.1 68.5
mation (TMI) concerns mutual information with
logarithm(Manning,1999). Mutual expectation
(ME) takes into account the relative probability of
each word compared to the phrase(Joaquim,1999).
Rankratio performs the best on both n-gram and
LCS approaches because it provides all the con-
texts which associated with each word in the cor-
pus and ranks them(Paul,2005). With the help of
advanced statistic measures, the scores of MWEs
are high enough to be detected from noisy pat-
terns.
4.2.2 Open Test
In open test, we just show the extracted MWE
numbers in different given corpus sizes. Two phe-
nomena are observed in Fig.1.








FRUSXVVL]H
0:8
QX
PEH
U
      Coling 2010: Demonstration Volume, pages 25?28,
Beijing, August 2010
YanFa: An Online Automatic Scoring and Intelligent Feedback 
System of Student English-Chinese Translation 
Yan Tian 
School of Foreign Languages 
Shanghai Jiao Tong University 
tianyan@sjtu.edu.cn 
 
Abstract 
Online learning calls for instant assess-
ment and feedback. YanFa is a system 
developed to score online English-
Chinese translation exercises with intel-
ligent feedback for Chinese non-English 
majors. With the aid of HowNet and 
Cilin?Chinese Synonym Set (Extended 
Version), the system adopts the hybrid 
approach to scoring student translation 
semantically. It compares student trans-
lation with model translation by Syno-
nym Matching, Sentence-pattern Match-
ing and Word Similarity Calculating re-
spectively. The experiment results show 
that the correlation ratio between the 
scores given by the system and by hu-
man raters is 0.58, which indicates that 
the algorithm is able to fulfill the task of 
automated scoring. YanFa is also able to 
provide feedback on syntactic mistakes 
made by students through interacting 
with them. It asks students to analyze 
the English sentence elements. Then it 
compares the student analyses with 
those of the parser and points out the 
parts which might lead to their wrong 
understanding as well as their wrong 
translating. 
1 Introduction 
Online language learning and instructing are 
popular in the era of the Internet which calls for 
instant automated assessment and intelligent 
feedback. How to provide online translation 
exercises with immediate scoring and intelligent 
feedback is a challenging task. Although some 
researchers (Wang & Chang, 2009; Wen, et al, 
2009) have investigated ways to score student 
translation, they did not aim at fully automated 
scoring of translation, nor did they try to serve 
online exercise scoring. Wang & Chang 
discussed methods of the human-aided 
automated assessment of translation tests in 
final exams, and Wen adopted bilingual 
alignment technology to score translation in 
language testing. However, online fully 
automated scoring of translation exercises has 
its own characteristics. Besides, providing 
online instant intelligent feedback for students 
presents another challenge to natural language 
processing. Up to now very little research, if 
any, has addressed this topic. In order to meet 
the demand of online automated scoring of 
translation exercises and to help students with 
intelligent feedback, an online automated 
scoring and intelligent feedback system, called 
YanFa, has been developed.  
This paper aims to outline the framework of 
YanFa. The paper addresses this by explaining 
two modules of YanFa, namely, the automatic 
scoring module and the intelligent feedback 
25
module. In order to test the accuracy of YanFa, 
a study with 200 college students was carried 
out at Shanghai Jiao Tong University. The re-
search intends to verify whether YanFa is able 
to undertake the task of online automated scor-
ing of student English-Chinese translation as 
well as the task of providing students with feed-
back on the mistakes in their comprehending of 
English sentences, which might lead to their 
wrong Chinese translation. This paper begins 
with an introduction, followed by the explana-
tion of the two modules. The experiment is also 
described. The research findings suggest that 
YanFa is eligible not only to score student 
online translation, but also to provide feedback 
on student syntactic mistakes in their under-
standing. 
2 Automatic Scoring Module 
?Translating means translating meaning.? (Nida, 
1986) Thus, ideally, automated translation 
scoring should be done at semantic level. 
Namely, the system should be able to judge 
whether the student translation is correct in 
conveying the original meaning to the target 
language. Therefore, the scoring module should 
be able to analyze the meaning of student 
translation which includes word meaning, 
phrase meaning as well as sentence meaning 
because translation involves two kinds of 
transfer: lexical transfer and structural transfer 
(Hutchins, 1992). Another consideration of 
building the module is to simulate the manual 
translation scoring practice in which the 
sentences are scored according to the correct 
translation of language points (words and 
phrases) and that of sentence structures. 
Usually, 3/4 scores are given to language points 
and 1/4 to sentence structures. 
The automatic scoring module is composed 
of two parts: the databases and the automatic 
scoring system. The databases are English Pas-
sage Pool, English Sentence Pool, Model Trans-
lation Pool, Model Sentence Pattern Pool, Stu-
dent Translation Pool. The automatic scoring 
system is composed of a Chinese Parser 
(SharpICTCLAS.net with precision rate of 
97.58 % and recall rate of 90%), a Word Ana-
lyzer, a Sentence Pattern Analyzer, a Rater. Be-
sides, Chinese resources, HowNet and Cilin?
Chinese Synonym Set (Extended Version by the 
Lab of Information Retrieval at Harbin Institute 
of Technology), are also adopted. 
First, student translations are parsed by 
SharpICTCLAS. Then the parsed sentences are 
sent to Word Analyzer to be compared with the 
pre-parsed model translations by the same par-
ser. Three different approaches are taken to deal 
with different parts of speech respectively: 
nouns are compared with the synonyms in Cilin, 
of which the seed nouns are from the model 
translations; verbs, adjectives and adverbs are 
compared by calculating the word similarity 
with the aid of HowNet. Similarly, the seed 
verbs, adjectives and adverbs also come from 
the model translations. The rest parts of speech, 
including idioms, are dealt with by key word 
matching method. After word processing, Sen-
tence Pattern Analyzer compares the sentence 
patterns of student translations with the model 
sentence patterns. Last, the results of both ana-
lyzers are sent to the Rater which calculates the 
final score of a student translation. The formulas 
are as follows: 
The formula for Word Analyzer: 
Processing of nouns with Cilin: 
??
?
?
?
?
??
)(,0
)(,)(_
sk
sk
k
WW
WCWlWclsem ??
 
where )(_ kWclsem refers to the score of a noun 
in student translation, kW stands for a noun in 
student translation, l  is the number of  parsed 
parts of speech in model translation, C is the 
26
synonym set of Cilin which embraces the noun 
appeared in student translation, sW is a noun in 
model translation, ?  is the total score of the 
sentences, ? is a constant.
 Processing of Verbs, Adjectives and Ad-
verbs with HowNet: 
)),(()( maxarg
1
ki
mi
k WWsimWsimhn
??
?  
where )( kWsimhn  is the maximum value of a 
primitive, iW  is the primitive in HowNet, kW  is 
a word in student translation, 1<i <m means i is 
bigger than 1, but less than m (m is the number 
of primitives). 
??
)()(_ kk WsimhnWhnsem ? 
where )(_ kWhnsem refers to the score of a 
word.
         Processing of other parts of speech: 
     
??
?
?
?
?
??
)(,0
)(,)(_
sk
sk
k
WTW
WTWlWstsem ??
 
where )(_ kWstsem means the score of other 
parts of speech, T refers to the set of other parts 
of speech. 
The formula for Sentence Pattern Analyzer: 
      
(1 ) , ( Re )_ 0, ( Re )
AnsTran reg Std gsim pat AnsTran reg Std g
? ?? ??? ? ??
 where patsim_  stands for the score of the 
sentence pattern of a sentence, )Re( gStdreg  
refers to the set of model translation (standard 
version) annotated by regular expression, 
AnsTran  means student translation. 
The formula for each sentence: 
_ ( ) (1 ) _kscore sim sem w sim pat?? ? ?? ? ?
The formula for the total score of a passage 
(with 5 sentences to be translated as in 
YanFa system): 
5)_)1()(_( ???? patsimWsemsimTotalscore k ????
 
3 Intelligent Feedback Module 
It is believed that comprehending of a source 
language plays a crucial role in its translation, 
especially when the source language is a foreign 
language to a translator. Accordingly, correct 
understanding of English sentences is essential 
to its translating into Chinese. Therefore, the 
intelligent feedback module focuses on whether 
students could correctly understand the English 
sentences. Specifically, feedback on correct 
understanding of clauses is provided rather than 
that of phrases because wrong translation occurs 
frequently on linguistic units larger than phrases 
when complex sentences are to be translated by 
Chinese college students. 
The Intelligent Feedback Module is 
composed of three parts: parsing of the original 
English sentences, comparing student parsing 
results with those of the parser, providing 
feedback to students. 
3.1 Parsing 
The module employs the English parser by 
Carnegie Mellon University (free online parser) 
to parse the original English sentences. It takes 
the advantage of the ?SBAR? sign as the marks 
of clauses. For example, following is the parsed 
result of a sentence: ?Because I believe that love 
begins at home and if we can create a home for 
the poor, I think that more and more love will 
spread.?  
27
 3.2 Comparing 
The module asks students to mark clauses of the 
English sentences. Then it compares the marked 
clauses with the results parsed by the parser 
through string matching. If the matching fails, 
the module comes to the decision that a wrong 
understanding happens. 
3.3 Providing feedback 
The module is able to provide students with the 
comparison of their parsing with the right pars-
ing of the whole sentences. If requested, the 
module is also able to present students with the 
comparison of their wrongly parsed clauses with 
the right ones. 
4 Experiment 
In order to test the accuracy of the automatic 
scoring system, 200 non-English majors at 
Shanghai Jiao Tong University were invited to 
try the online scoring system at ?Shanghai Jiao 
Tong University English Learning Center? 
(http://english.sjtu.edu.cn). Then the scores 
were compared with those of two human raters. 
The correlation ratio is around 0.58 (the correla-
tion ratio between the human raters is 0.67). 
Also, an online questionnaire was delivered to 
those who have tried the system to learn their 
opinions on the scores and the feedback they got. 
The statistics show that most of the students 
gave positive responses. 
5 Conclusion 
YanFa has been developed to score Chinese 
college students? online English-Chinese trans-
lation exercises as well as to provide feedback 
on their mistakes of understanding the English 
sentences. Semantic scoring has been explored 
on lexical level with such resources as HowNet 
and Cilin. While the scoring on sentence level 
has to yield to sentence pattern matching due to 
the unsatisfactory performance of Chinese syn-
tactic parsers. Although this pilot research has 
achieved its initial purpose, yet it is far from 
satisfactory. Further efforts should be made in 
increasing the scoring accuracy and more de-
tailed feedback. 
  
References 
Waard, J.D. & Nida, E.A., 1986. From One Lan-
guage to Another. Thomas Nelson Publishers, 
Tennessee, U.S.A  
Wang, L. & Chang, B.B., 2009. Research on the 
Human-aided Auto-assessment for Translation 
Tests in College English. CAFLEC, No. 128,17-
21 
Wen, Q.F. et al, Application of Bilingual Alignment 
Technology to Automatic Translation Scoring of 
English Test. CAFLEC, No. 125, 3-8 
Hutchins,W. J. & Somers H. L. ,1992. An Introduc-
tion to Machine Translation, ACADEMIC 
PRESS LIMITED, Printed in Great Britain at the 
University Press, Cambridge. 110-111 
D. Callear, J. Jerrams-Smith, and V. Soh, ?CAA of 
Short Non-MCQ Answers?, Fifth Internation-
al Computer Assisted Assessment Conference, 
Loughborough University, 2001. 
Brown, H.D.1987. Principles of Language Learning 
and Teaching [M]. Egnlewood Cliffs, NJ: Pren-
tice Hall. 
28
