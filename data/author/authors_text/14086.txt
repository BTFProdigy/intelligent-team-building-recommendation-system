Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 21?29,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Clustering dictionary definitions using Amazon Mechanical Turk  
Gabriel Parent Maxine Eskenazi 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
15213 Pittsburgh, USA 
 
{gparent,max}@cs.cmu.edu 
 
 
 
Abstract 
Vocabulary tutors need word sense disambig-
uation (WSD) in order to provide exercises 
and assessments that match the sense of words 
being taught. Using expert annotators to build 
a WSD training set for all the words supported 
would be too expensive. Crowdsourcing that 
task seems to be a good solution.  However, a 
first required step is to define what the possi-
ble sense labels to assign to word occurrence 
are.  This can be viewed as a clustering task 
on dictionary definitions. This paper evaluates 
the possibility of using Amazon Mechanical 
Turk (MTurk) to carry out that prerequisite 
step to WSD.  We propose two different ap-
proaches to using a crowd to accomplish clus-
tering: one where the worker has a global 
view of the task, and one where only a local 
view is available.  We discuss how we can 
aggregate multiple workers? clusters together, 
as well as pros and cons of our two approach-
es.  We show that either approach has an inte-
rannotator agreement with experts that 
corresponds to the agreement between ex-
perts, and so using MTurk to cluster dictio-
nary definitions appears to be a reliable 
approach. 
1 Introduction 
For some applications it is useful to disambiguate 
the meanings of a polysemous word. For example, 
if we show a student a text containing a word like 
?bank? and then automatically generate questions 
about the meaning of that word as it appeared in 
the text (say as the bank of a river), we would like 
to have the meaning of the word in the questions 
match the text meaning. Teachers do this each time 
they assess a student on vocabulary knowledge.  
For intelligent tutoring systems, two options are 
available. The first one is to ask a teacher to go 
through all the material and label each appearance 
of a polysemous word with its sense.  This option 
is used only if there is a relatively small quantity of 
material. Beyond that, automatic processing, 
known as Word Sense Disambiguation (WSD) is 
essential. Most approaches are supervised and need 
large amounts of data to train the classifier for each 
and every word that is to be taught and assessed.  
Amazon Mechanical Turk (MTurk) has been 
used for the purpose of word sense disambiguation 
(Snow et al 2008). The results show that non-
experts do very well (100% accuracy) when asked 
to identify the correct sense of a word out of a fi-
nite set of labels created by an expert. It is there-
fore possible to use MTurk to build a training 
corpus for WSD. In order to extend the Snow et al
crowdsourced disambiguation to a large number of 
words, we need an efficient way to create the set of 
senses of a word. Asking an expert to do this is 
costly in time and money. Thus it is necessary to 
have an efficient Word Sense Induction (WSI) sys-
tem. A WSI system induces the different senses of 
a word and provides the corresponding sense la-
bels.  This is the first step to crowdsourcing WSD 
on a large scale. 
While many studies have shown that MTurk 
can be used for labeling tasks (Snow et al 2008), 
to rate automatically constructed artifacts 
(Callison-Burch, 2009, Alonso et al 2008) and to 
transcribe speech (Ledlie et al 2009, Gruenstein et 
al, 2009), to our knowledge, there has not been 
much work on evaluating the use of MTurk for 
21
clustering tasks. The goal of this paper is to inves-
tigate different options available to crowdsource a 
clustering task and evaluate their efficiency in the 
concrete application of word sense induction. 
2 Background 
2.1 WSD for vocabulary tutoring  
Our interest in the use of MTurk for disambigua-
tion comes from work on a vocabulary tutor; 
REAP (Heilman et al 2006). The tutor searches for 
documents from the Web that are appropriate for a 
student to use to learn vocabulary from context 
(appropriate reading level, for example). Since the 
system finds a large number of documents, making 
a rich repository of learning material, it is impossi-
ble to process all the documents manually. When a 
document for vocabulary learning is presented to a 
student, the system should show the definition of 
the words to be learned (focus words). In some 
cases a word has several meanings for the same 
part of speech and thus it has several definitions. 
Hence the need for WSD to be included in vocabu-
lary tutors. 
2.2 WSI and WSD 
The identification of a list of senses for a given 
word in a corpus of documents is called word 
sense induction (WSI). SemEval 2007 and 2010 
(SigLex, 2008) both evaluate WSI systems. The 
I2R system achieved the best results in 2007 with 
an F-score of 81.6% (I2R by Niu (2007)).  Snow et 
al (2007) have a good description of the inherent 
problem of WSI where the appropriate granularity 
of the clusters varies for each application. They try 
to solve this problem by building hierarchical-like 
word sense structures. In our case, each dictionary 
definition for a word could be considered as a 
unique sense for that word. Then, when using 
MTurk as a platform for WSD, we could simply 
ask the workers to select which of the dictionary 
definitions best expresses the meaning of the 
words in a document.  The problem here is that 
most dictionaries give quite several definitions for 
a word.    Defining one sense label per dictionary 
definition would result in too many labels, which 
would, in turn, make the MTurk WSD less effi-
cient and our dataset sparser, thus decreasing the 
quality of the classifier.  Another option, investi-
gated by Chklovski and Mihalcea (2003), is to use 
WordNet sense definitions as the possible labels.  
They obtained more than 100,000 labeled instances 
from a crowd of volunteers.  They conclude that 
WordNet senses are not coarse enough to provide 
high interannotator agreement, and exploit workers 
disagreement on the WSD task to derive coarser 
senses. 
The granularity of the senses for each word is a 
parameter that is dependent on the application. In 
our case, we want to be able to assess a student on 
the sense of a word that the student has just been 
taught. Learners have the ability to generalize the 
context in which a word is learned.  For example, 
if a student learns the meaning of the word ?bark? 
as the sound of a dog, they can generalize that this 
can also apply to human shouting. Hence, there is 
no need for two separate senses here. However, a 
student could not generalize the meaning ?hard 
cover of a tree? from that first meaning of ?bark?.  
This implies that students should be able to distin-
guish coarse word senses. (Kulkarni et al, 2007) 
have looked at automatic clustering of dictionary 
definitions. They compared K-Means clustering 
with Spectral Clustering. Various features were 
investigated: raw, normalized word overlap with 
and without stop words. The best combination re-
sults in 74% of the clusters having no misclassified 
definitions. If those misclassified definitions end 
up being used to represent possible sense labels in 
WSD, wrong labels might decrease the quality of 
the disambiguation stage. If a student is shown a 
definition that does not match the sense of a word 
in a particular context, they are likely to build the 
wrong conceptual link. Our application requires 
higher accuracy than that achieved by automatic 
approaches, since students? learning can be directly 
affected by the error rate.  
2.3 Clustering with MTurk 
The possible interaction between users and cluster-
ing algorithms has been explored in the past.  
Huang and Mitchell (2006) present an example of 
how user feedback can be used to improve cluster-
ing results.  In this study, the users were not asked 
to provide clustering solutions. Instead, they fine 
tuned the automatically generated solution. 
With the advent of MTurk, we can use human 
judgment to build clustering solutions. There are 
multiple approaches for combining workforce: pa-
rallel with aggregation (Snow et al 2008), iterative 
22
(Little et al 2009) and collaboration between 
workers (Horton, Turker Talk, 2009). These strate-
gies have been investigated for many applications, 
most of which are for labeling, a few for cluster-
ing. The Deneme blog presents an experiment 
where website clustering is carried out using 
MTurk (Little, Website Clustering, 2009). The 
workers? judgments on the similarity between two 
websites are used to build a distance matrix for the 
distance between websites. Jagadeesan and others 
(2009) asked workers to identify similar objects in 
a pool of 3D CAD models. They then used fre-
quently co-occurring objects to build a distance 
matrix, upon which they then applied hierarchical 
clustering. Those two approaches are different: the 
first gives the worker only two items of the set (a 
local view of the task), while the latter offers the 
worker a global view of the task. In the next sec-
tions we will measure the accuracy of these ap-
proaches and their advantages and disadvantages. 
3 Obtaining clusters from a crowd 
REAP is used to teach English vocabulary and to 
conduct learning studies in a real setting, in a local 
ESL school. The vocabulary tutor provides instruc-
tions for the 270 words on the school?s core voca-
bulary list, which has been built using the 
Academic Word List (Coxhead, 2000). In order to 
investigate how WSI could be accomplished using 
Amazon Mechanical Turk, 50 words were random-
ly sampled from the 270, and their definitions were 
extracted from the Longman Dictionary of Con-
temporary English (LDOCE) and the Cambridge 
Advanced Learner's Dictionary (CALD).  There 
was an average of 6.3 definitions per word. 
The problem of clustering dictionary definitions 
involves solving two sub-problems: how many 
clusters there are, and which definitions belong to 
which clusters.  We could have asked workers to 
solve both problems at the same time by having 
them dynamically change the number of clusters in 
our interface.  We decided not to do this due to the 
fact that some words have more than 12 defini-
tions. Since the worker already needs to keep track 
of the semantics of each cluster, we felt that having 
them modify the number of sense boxes would 
increase their cognitive load to the point that we 
would see a decrease in the accuracy of the results. 
Thus the first task involved determining the 
number of general meanings (which in our case 
determines the number of clusters) that there are in 
a list of definitions. The workers were shown the 
word and a list of its definitions, for example, for 
the word ?clarify?:  
 
 to make something clearer and easier to    
understand 
 to make something clear or easier to under-
stand by giving more details or a simpler explana-
tion 
 to remove water and unwanted substances 
from fat, such as butter, by heating it 
 
They were then asked: ?How many general 
meanings of the word clarify are there in the fol-
lowing definitions??  We gave a definition of what 
we meant by general versus specific meanings, 
along with several examples.  The worker was 
asked to enter a number in a text box (in the above 
example the majority answered 2).  This 2-cent 
HIT was completed 13 times for every 50 words, 
for a total of 650 assignments and $13.00. A ma-
jority vote was used to aggregate the workers? re-
sults, giving us the number of clusters in which the 
definitions were grouped.  In case of a tie, the low-
est number of clusters was retained, since our ap-
plication requires coarse-grained senses. 
The number of ?general meanings? we obtained 
in this first HIT1 was then used in two different 
HITs.  We use these two HITs to determine which 
definitions should be clustered together. In the first 
setup, which we called ?global-view? the workers 
had a view of the entire task. They were shown the 
word and all of its definitions. They were then 
prompted to drag-and-drop the definitions into dif-
ferent sense boxes, making sure to group the defi-
nitions that belong to the same general meaning 
together (Figure 3, Appendix). Once again, an ex-
plicit definition of what was expected for ?general 
meaning? along with examples was given. Also, a 
flash demo of how to use the interface was pro-
vided. The worker got 3 cents for this HIT. It was 
completed 5 times for each of the 50 words, for a 
total cost of $7.50. We created another HIT where 
the workers were not given all of the definitions; 
we called this setup ?local-view?.  The worker was 
asked to indicate if two definitions of a word were 
related to the same meaning or different meanings 
                                                          
1 The code and data used for the different HITs are available at 
http://www.cs.cmu.edu/~gparent/amt/wsi/ 
23
(Figure 4, Appendix).  For each word, we created 
all possible pairs of definitions. This accounts for 
an average of 21 pairs for all of the 50 words. For 
each pair, 5 different workers voted on whether it 
contained the same or different meanings, earning 
1 cent for each answer. The total cost here was 
$52.50. The agreement between workers was used 
to build a distance matrix: if the 5 workers agreed 
that the two definitions concerned the same sense, 
the distance was set to 0. Otherwise, it was set to 
the number of workers who thought they con-
cerned different senses, up to a distance of 5. Hie-
rarchical clustering was then used to build 
clustering solutions from the distance matrices. We 
used complete linkage clustering, with Ward?s cri-
terion. 
4 Evaluation of global-view vs. local-view 
approaches 
In order to evaluate our two approaches, we 
created a gold-standard (GS). Since the task of 
WSI is strongly influenced by an annotator?s grain 
size preference for the senses, four expert annota-
tors were asked to create the GS. The literature 
offers many metrics to compare two annotators? 
clustering solutions (Purity and Entropy (Zhao and 
Karypis, 2001), clustering F-Measure (Fung et al, 
2003) and many others).  SemEval-2 includes a 
WSI task where V-Measure (Rosenberg and Hir-
schberg, 2007) is used to evaluate the clustering 
solutions. V-Measure involves two metrics, homo-
geneity and completeness, that can be thought of as 
precision and recall.  Perfect homogeneity is ob-
tained if the solutions have clusters whose data 
points belong to a single cluster in the GS. Perfect 
completeness is obtained if the clusters in the GS 
contain data points that belong to a single cluster in 
the evaluated solution. The V-Measure is a 
(weighted) harmonic mean of the homogeneity and 
of the completeness metrics. Table 1 shows inter-
annotator agreement (ITA) among four experts on 
the test dataset, using the average V-Measure over 
all the 50 sense clusters. 
 
 
 
 
 
  GS #1 GS #2 GS #3 GS #4 
GS #1 1,000 0,850 0,766 0,770 
GS #2 0,850 1,000 0,763 0,796 
GS #3 0,766 0,763 1,000 0,689 
GS #4 0,770 0,796 0,689 1,000 
Table 1 - ITA on WSI task for four annotators 
 
We can obtain the agreement between one ex-
pert and the three others by averaging the three V-
Measures. We finally obtain an ?Experts vs. Ex-
perts? ITA of 0.772 by averaging this value for all 
of our experts. The standard deviation for this ITA 
is 0.031.To be considered reliable, non-expert clus-
tering would have to agree with the 4 experts with 
a similar result. 
5 Aggregating clustering solutions from 
multiple workers 
Using a majority vote with the local-view HIT is 
an easy way of taking advantage of the ?wisdom of 
crowd? principle. In order to address clustering 
from a local-view perspective, we need to build all 
possible pairs of elements. The number of those 
pairs is O(n2) on the number of elements to cluster. 
Thus the cost grows quickly for large clustering 
problems. For 100 elements to cluster there are 
4950 pairs of elements to show to workers. For 
large problems, a better approach would be to give 
the problem to multiple workers through global-
view, and then find a way to merge all of the clus-
tering solutions to benefit from the wisdom of 
crowd. Consensus clustering (Topchy et al 2005) 
has emerged as a way of combining multiple weak 
clusterings into a better one. The cluster-based si-
milarity partitioning algorithm (CSPA) (Strehl and 
Ghosh, 2002) uses the idea that elements that are 
frequently clustered together have high similarity. 
With MTurk, this involves asking multiple workers 
to provide full clusterings, and then, for each pair 
of elements, counting the number of times they co-
occur in the same clusters. This count is used as a 
similarity measure between elements, which then 
is used to build a distance matrix. We can then use 
it to recluster elements. The results from this tech-
nique on our word sense induction problem are 
shown in the next section. 
24
Another possibility is to determine which clus-
tering solution is the centroid of the set of cluster-
ings obtained from the worker. Finding centroid 
clustering (Hu and Sung, 2006) requires a be-
tween-cluster distance metric. We decided to use 
the entropy-based V-Measure for this purpose. For 
every pair of workers? solutions, we obtain their 
relative distance by calculating 
 1-VMeasure(cluster #1,cluster #2). 
Then, for each candidate?s clusters, we average the 
distance with every other candidate?s.  The candi-
date with the lowest average distance, the centroid, 
is picked as the ?crowd solution?. Results from this 
technique are also shown in the next section. 
6 Results 
For the first HIT the goal was to determine the 
number of distinct senses in a list of definitions. 
The Pearson correlation between the four annota-
tors on the number of clusters they used for the 50 
words was computed. These correlations can be 
viewed as how much the different annotators had 
the same idea of the grain size to be used to define 
senses. While experts 1, 2 and 4 seem to agree on 
grain size (correlation between 0.71 and 0.75), ex-
pert 3 had a different opinion. Correlations be-
tween that expert and the three others are between 
0.53 and 0.58. The average correlation between 
experts is 0.63. On the other hand, the crowd solu-
tion does not agree as well with experts #1,#2 and 
#4 (Pearson correlation of 0.64, 0.68, 0.66), while 
it better approaches expert 3, with a correlation of 
0.68. The average correlation between the non-
expert solution and the experts? solutions is 0.67.  
Another way to analyze the agreement on grain 
size of the word sense between annotators is to 
sum the absolute difference of number of clusters 
for the 50 words (Table 3).  In this way, we can 
specifically examine the results for the four anno-
tators and for the non-expert crowd (N-E) solution, 
averaging that difference for each annotator versus 
all of the others (including the N-E solution). 
To determine how a clustering solution com-
pared to our GS, we computed the V-Measure for 
all 50 words between the solution and each GS.  
By averaging the score on the four GSs, we get an 
averaged ITA score between the clustering solution 
and the experts. For the sake of comparison, we 
first computed the score of a random solution, 
where definitions are randomly assigned to any 
one cluster. We also implemented K-means clus-
tering using normalized word-overlap (Kulkarni et 
al., 2007), which has the best score on their test set.   
The resulting averaged ITA of our local-view 
approaches that of all 4 experts. We did the same 
with the global-view after applying CSPA and our 
centroid identification algorithm to the 5 clustering 
solutions the workers submitted. Table 2 shows the 
agreement between each expert and those ap-
proaches, as well as the averaged ITA. 
For the local-view and global-view ?centroid?, 
we looked at how the crowd size would affect the 
accuracy.  We first computed the averaged ITA by 
considering the answers from the first worker.  
Then, step by step, we added the answers from the 
second, third, fourth and fifth workers, each time 
computing the averaged ITA. Figure 1 shows the 
ITA as a function of the workers.   
  
Random K-Means local 
global  
CSPA 
global  
centroid 
GS #1 0,387 0,586 0,737 0,741 0,741 
GS #2 0,415 0,613 0,765 0,777 0,777 
GS #3 0,385 0,609 0,794 0,805 0,809 
GS #4 0,399 0,606 0,768 0,776 0,776 
Avg. ITA 0.396 ? 0.014 0.603 ? 0.012 0.766 ? 0.023 0.775 ? 0.026 0.776 ? 0.028 
 
Table 2 - Interannotator agreement for our different approaches (bold numbers are within one standard 
deviation of the Expert vs. Expert ITA of 0.772 ? 0.031 described in section 4) 
 
 GS #1 GS #2 GS #3 GS #4 N-E 
GS #1 0 24 26 29 26 
GS #2 24 0 30 27 26 
GS #3 26 30 0 37 20 
GS #4 29 27 37 0 27 
N-E 26 26 20 27 0 
Average 26.25 26.75 28.25 30 24.75 
Table 3 - Absolute difference of number of clusters 
between annotators 
 
25
7 Discussion 
Since our two approaches are based on the result of 
the first HIT, which determines the number of 
clusters, the accuracy of that first task is extremely 
important. It turns out that the correlation between 
the crowd solution and the experts (0.67) is actual-
ly higher than the average correlation between ex-
perts (0.63). One way to explain this is that of the 4 
experts, 3 had a similar opinion on what the grain 
size should be, while the other one had a different 
opinion. The crowd picked a grain size that was 
actually between those two opinions, thus resulting 
in a higher correlation. This hypothesis is also sup-
ported by Table 3. The average difference in the 
number of clusters is lower for the N-E solution 
than for any expert solution. The crowd of 13 was 
able to come up with a grain size that could be 
seen as a good consensus of the four annotators? 
grain size. This allows us to believe that using the 
crowd to determine the number of clusters for our 
two approaches is a reliable technique.  
As expected, Table 3 indicates that our two set-
ups behave better than randomly assigning defini-
tions to clusters.  This is a good indication that the 
workers did not complete our tasks randomly. The 
automatic approach (K-Means) clearly behaves 
better than the random baseline. However, the 
clusters obtained with this approach agree less with 
the experts than any of our crowdsourced ap-
proaches. This confirms the intuition that humans 
are better at distinguishing word senses than an 
automatic approach like K-Means.  
Our first hypothesis was that global-view would 
give us the best results: since the worker complet-
ing a global-view HIT has an overall view of the 
task, they should be able to provide a better solu-
tion. The results indicate that the local-view and 
global-view approaches give similar results in 
terms of ITA. Both of those approaches have clos-
er agreement with the experts, than the experts 
have with each other (all ITAs are around 77%).   
Here is an example of a solution that the crowd 
provided through local-view for the verb ?tape? 
with the definitions; 
 
A. To record something on tape 
B. To use strips of sticky material, especially to fix 
two things together or to fasten a parcel 
C. To record sound or picture onto a tape 
D. To tie a bandage firmly around an injured part of 
someone?s body, strap 
E. To fasten a package, box etc with tape 
 
The crowd created two clusters: one by group-
ing A and C to create a ?record audio/video? sense, 
and another one by grouping B,D and E to create a 
?fasten? sense. This solution was also chosen by 
two of the four experts. One of the other experts 
grouped definitions E with A and C, which is 
clearly an error since there is no shared meaning.  
The last expert created three clusters, by assigning 
D to a different cluster than B and E. This decision 
can be considered valid since there is a small se-
mantic distinction between D and B/E from the 
fact that D is ?fasten? for the specific case of in-
jured body parts. However, a student could gene-
ralize D from B and E. So that expert?s grain size 
does not correspond to our specifications. 
We investigated two different aggregation tech-
niques for clustering solutions, CSPA and centroid 
identification. In this application, both techniques 
give very similar results with only 2 clusters out of 
50 words differing between the two techniques. 
Centroid identification is easier to implement, and 
doesn?t require reclustering the elements. Figure 1 
shows the impact of adding more workers to the 
crowd. While it seems advantageous to use 3 
workers? opinions rather than only 1, (gain of 
0.04), adding a fourth and fifth worker does not 
improve the average ITA.   
Local-view is more tolerant to errors than glob-
al-view.  If a chaotic worker randomly answers one 
pair of elements, the entire final clustering will not 
be affected. If a chaotic (or cheating) worker an-
swers randomly in global-view, the entire cluster-
ing solution will be random. Thus, while a policy 
of using only one worker?s answer for a local-view 
 
Figure 1 - Impact of the crowd size on the ITA of the 
local and global approaches 
26
HIT could be adopted, the same policy might result 
in poor clustering if used for the global-view HIT.   
However, global-view has the advantage over 
local-view of being cheaper. Figure 2 shows the 
distribution of the number of definitions extracted 
from both LDOCE and CALD per word (starting at 
word with more than 6 definitions). Since the lo-
cal-view cost increases in a quadratic manner as 
the number of elements to cluster increases it 
would cost more than $275,000 to group the defi-
nitions of 30,000 words coming from the two dic-
tionaries (using the parameters described in 3).  It 
would be possible to modify it to only ask workers 
for the similarity of a subset of pairs of elements 
and then reconstruct the incomplete distance ma-
trix (Hathaway and Bezdek, 2002). A better option 
for clustering a very large amount of elements is to 
use global-view. For the same 30,000 words above, 
the cost of grouping definitions using this tech-
nique would be around $4,500.  This would imply 
that worker would have to create clusters from set 
of over 22 definitions.  Keeping the cost constant 
while increasing the number of elements to cluster 
might decrease the workers? motivation. Thus scal-
ing up a global-view HIT requires increasing the 
reward. It also requires vigilance on how much 
cognitive load the workers have to handle. Cogni-
tive load can be seen as a function of the number 
of elements to cluster and of the number of clusters 
that a new element can be assigned to. If a worker 
only has to decide if an element should be in A or 
B, the cognitive load is low. But if the worker has 
to decide among many more classes, the cognitive 
load may increase to a point where the worker is 
hampered from providing a correct answer.  
8 Conclusion 
We evaluated two different approaches for crowd-
sourcing dictionary definition clustering as a 
means of achieving WSI. Global-view provides an 
interface to the worker where all the elements to 
be clustered are displayed, while local-view dis-
plays only two elements at a time and prompts the 
worker for their similarity. Both approaches show 
as much agreement with experts as the experts do 
with one another. Applying either CSPA or centro-
id identification allows the solution to benefit from 
the wisdom of crowd effect, and shows similar 
results. While global-view is cheaper than local-
view, it is also strongly affected by worker error, 
and sensitive to the effect of increased cognitive 
load.  
It appears that the task of clustering definitions 
to form word senses is a subjective one, due to dif-
ferent ideas of what the grain size of the senses 
should be. Thus, even though it seems that our two 
approaches provide results that are as good as 
those of an expert, it would be interesting to try 
crowdsourced clustering on a clustering problem 
where an objective ground truth exists. For exam-
ple, we could take several audio recordings from 
each of several different persons. After mixing up 
the recordings from the different speakers, we 
could ask workers to clusters all the recordings 
from the same person. This would provide an even 
stronger evaluation of local-view against global-
view since we could compare them to the true so-
lution, the real identity of the speaker.  
There are several interesting modifications that 
could also be attempted. The local-view task could 
ask for similarity on a scale of 1 to 5, instead of a 
binary choice of same/different meaning. Also, 
since using global-view with one large problem 
causes high cognitive load, we could partition a 
bigger problem, e.g., with 30 definitions, into 3 
problems including 10 definitions. Using the same 
interface as global-view, the workers could cluster 
the sub-problems. We could then use CSPA to 
merge local clusters into a final cluster with the 30 
definitions.   
In this paper we have examined clustering word 
sense definitions. Two approaches were studied, 
and their advantages and disadvantages were de-
scribed. We have shown that the use of human 
computation for WSI, with an appropriate crowd 
 
Figure 2- Distribution of the number of definitions 
 
0
250
500
750
1000
N
u
m
b
e
r 
o
f 
w
o
rd
s
Number of definitions
27
size and mean of aggregation, is as reliable as us-
ing expert judgments.   
Acknowledgements 
Funding for this research is provided by the Na-
tional Science Foundation, Grant Number SBE-
0836012 to the Pittsburgh Science of Learning 
Center (PSLC, http://www.learnlab.org). 
References 
Alonso, O., Rose, D. E., & Stewart, B. (2008). Crowd-
sourcing for relevance evaluation. ACM SIGIR Fo-
rum , 42 (2), pp. 9-15. 
Callison-Burch, C. (2009). Fast, Cheap, and Creative: 
Evaluating Translation Quality Using Amazon?s Me-
chanical Turk. Proceedings of EMNLP 2009.  
Coxhead, A. (2000). A new academic word list. TESOL 
quarterly , 34 (2), 213-238. 
Chklovski, T. & Mihalcea, R. (2003). Exploiting 
agreement and disagreement of human annotators for 
word sense disambiguation.  Proceedings of RANLP 
2003. 
Fung, B. C., Wang, K., & Ester, M. (2003). Hierarchical 
document clustering using frequent itemsets. Proc. of 
the SIAM International Conference on Data Mining.  
Gruenstein, A., McGraw, I., & Sutherland, A. (2009). 
"A self-transcribing speech corpus: collecting conti-
nuous speech with an online educational game". 
SLaTE Workshop.  
Hathaway, R. J., & Bezdek, J. C. (2001). Fuzzy c-means 
clustering of incomplete data. IEEE Transactions on 
Systems, Man, and Cybernetics, Part B , 31 (5), 735-
744. 
Heilman, M. Collins-Thompson, K., Callan, J. & Eske-
nazi M. (2006).  Classroom success of an Intelligent 
Tutoring System for lexical practice and reading 
comprehension. Proceedings of the Ninth Interna-
tional Conference on Spoken Language. 
Horton, J. (2009, 12 11). Turker Talk. Retrieved 01 
2010, from Deneme: 
http://groups.csail.mit.edu/uid/deneme/?p=436 
Hu, T., & Sung, S. Y. (2006). Finding centroid cluster-
ings with entropy-based criteria. Knowledge and In-
formation Systems , 10 (4), 505-514. 
Huang, Y., & Mitchell, T. M. (2006). Text clustering 
with extended user feedback. Proceedings of the 29th 
annual international ACM SIGIR conference on Re-
search and development in information retrieval (p. 
420). ACM. 
Jagadeesan, A., Lynn, A., Corney, J., Yan, X., Wenzel, 
J., Sherlock, A., et al (2009). Geometric reasoning 
via internet CrowdSourcing. 2009 SIAM/ACM Joint 
Conference on Geometric and Physical Modeling 
(pp. 313-318). ACM. 
Kulkarni, A., Callan, J., & Eskenazi, M. (2007). Dictio-
nary Definitions: The Likes and the Unlikes. Pro-
ceedings of the SLaTE Workshop on Speech and 
Language Technology in Education. Farmington, PA, 
USA. 
Ledlie, J., Odero, B., Minkow, E., Kiss, I., & Polifroni, 
J. (2009). Crowd Translator: On Building Localized 
Speech Recognizers through Micropayments. Nokia 
Research Center. 
Little, G. (2009, 08 22). Website Clustering. Retrieved 
01 2010, from Deneme: 
http://groups.csail.mit.edu/uid/deneme/?p=244 
Little, G., Chilton, L. B., Goldman, M., & Miller, R. C. 
(2009). TurKit: tools for iterative tasks on mechani-
cal Turk. Proceedings of the ACM SIGKDD Work-
shop on Human Computation (pp. 29-30). ACM. 
Niu, Z.-Y., Dong-Hong, J., & Chew-Lim, T. (2007). I2r: 
Three systems for word sense discrimination, chinese 
word sense disambiguation, and english word sense 
disambiguation. Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007) (pp. 177-182). Prague, Czech Republic: Asso-
ciation for Computational Linguistics. 
Rosenberg, A., & Hirschberg, J. (2007). V-measure: A 
conditional entropy-based external cluster evaluation 
measure. Proceedings of the 2007 Joint Conference 
EMNLP-CoNLL, (pp. 410-420). 
SigLex, A. (2008). Retrieved 01 2010, from SemEval-2, 
Evaluation Exercises on Semantic Evaluation: 
http://semeval2.fbk.eu/semeval2.php 
Snow, R., O'Connor, B., Jurafsky, D., & Ng, A. Y. 
(2008). Cheap and fast---but is it good? Evaluating 
non-expert annotations for natural language tasks. 
Proceedings of the Conference on Empirical Methods 
in Natural Language Processing (pp. 254-263). Asso-
ciation for Computational Linguistics. 
Snow, R., Prakash, S., Jurafsky, D., & Ng, A. Y. (2007). 
Learning to Merge Word Senses. Proceedings of the 
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL) (pp. 1005-
1014). Prague, Czech Republic: Association for 
Computational Linguistics. 
Strehl, A., & Ghosh, J. (2003). Cluster ensembles---a 
knowledge reuse framework for combining multiple 
partitions. The Journal of Machine Learning Re-
search , 3, 583-617. 
Topchy, A., Jain, A. K., & Punch, W. (2005). Clustering 
ensembles: Models of consensus and weak partitions. 
IEEE Transactions on Pattern Analysis and Machine 
Intelligence , 27 (12), 1866-1881. 
Zhao, Y., & Karypis, G. (2001). Criterion functions for 
document clustering: Experiments and analysis. Re-
port TR 01?40, Department of Computer Science, 
University of Minnesota. 
28
Figure 3: Example of a global-view HIT for the word ?code? (not all of the instructions are shown) 
Appendix 
 
Figure 4: Example of a local-view HIT for the word ?aid? (not all of the instructions are shown) 
29
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialog Challenge 2010: 
 Comparison of Live and Control Test Results 
Alan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  Oliver 
Lemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, Jason 
D. Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi1 
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA 
2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK 
3Engineering Department, Cambridge University, Cambridge, UK 
4AT&T Labs ? Research, Florham Park, NJ, USA 
awb@cs.cmu.edu 
 
 
Abstract 
The Spoken Dialog Challenge 2010 was an 
exercise to investigate how different spo-
ken dialog systems perform on the same 
task.  The existing Let?s Go Pittsburgh Bus 
Information System was used as a task and 
four teams provided systems that were first 
tested in controlled conditions with speech 
researchers as users. The three most stable 
systems were then deployed to real callers.  
This paper presents the results of the live 
tests, and compares them with the control 
test results. Results show considerable 
variation both between systems and be-
tween the control and live tests.  Interest-
ingly, relatively high task completion for 
controlled tests did not always predict 
relatively high task completion for live 
tests.  Moreover, even though the systems 
were quite different in their designs, we 
saw very similar correlations between word 
error rate and task completion for all the 
systems.  The dialog data collected is 
available to the research community. 
1 Background 
The goal of the Spoken Dialog Challenge (SDC) is 
to investigate how different dialog systems per-
form on a similar task.  It is designed as a regularly 
recurring challenge. The first one took place in 
2010. SDC participants were to provide one or 
more of three things: a system; a simulated user, 
and/or an evaluation metric.   The task chosen for 
the first SDC was one that already had a large 
number of real callers. This had several advan-
tages. First, there was a system that had been used 
by many callers. Second, there was a substantial 
dataset that participants could use to train their sys-
tems.  Finally, there were real callers, rather than 
only lab testers.  Past work has found systems 
which appear to perform well in lab tests do not 
always perform well when deployed to real callers, 
in part because real callers behave differently than 
lab testers, and usage conditions can be considera-
bly different [Raux et al2005, Ai et al2008].  De-
ploying systems to real users is an important trait 
of the Spoken Dialog Challenge. 
The CMU Let?s Go Bus Information system 
[Raux et al2006] provides bus schedule informa-
tion for the general population of Pittsburgh.  It is 
directly connected to the local Port Authority, 
whose evening calls for bus information are redi-
rected to the automated system.  The system has 
been running since March 2005 and has served 
over 130K calls. 
The software and the previous years of dialog 
data were released to participants of the challenge 
to allow them to construct their own systems.  A 
number of sites started the challenge, and four sites 
successfully built systems, including the original 
CMU system. 
An important aspect of the challenge is that 
the quality of service to the end users (people in 
Pittsburgh) had to be maintained and thus an initial 
robustness and quality test was carried out on con-
tributed systems.  This control test provided sce-
narios over a web interface and required 
researchers from the participating sites to call each 
of the systems.  The results of this control test were 
published in [Black et al 2010] and by the individ-
ual participants [Williams et al 2010, Thomson et 
al. 2010, Hastie et al 2010] and they are repro-
2
duced below to give the reader a comparison with 
the later live tests. 
Important distinctions between the control 
test callers and the live test callers were that the 
control test callers were primarily spoken dialog 
researchers from around the world.  Although they 
were usually calling from more controlled acoustic 
conditions, most were not knowledgeable about 
Pittsburgh geography.     
As mentioned above, four systems took part 
in the SDC.  Following the practice of other chal-
lenges, we will not explicitly identify the sites 
where these systems were developed. We simply 
refer to them as SYS1-4 in the results.  We will, 
however, state that one of the systems is the system 
that has been running for this task for several 
years. The architectures of the systems cover a 
number of different techniques for building spoken 
dialog systems, including agenda based systems, 
VoiceXML and statistical techniques. 
2 Conditions of Control and Live tests 
For this task, the caller needs to provide the depar-
ture stop, the arrival stop and the time of departure 
or arrival in order for the system to be able to per-
form a lookup in the schedule database. The route 
number can also be provided and used in the 
lookup, but it is not necessary. The present live 
system covers the East End of Pittsburgh.  Al-
though the Port Authority message states that other 
areas are not covered, callers may still ask for 
routes that are not in the East End; in this case, the 
live system must say it doesn?t have information 
available.  Some events that affect the length of the 
dialog include whether the system uses implicit or 
explicit confirmation or some combination of both, 
whether the system has an open-ended first turn or 
a directed one, and whether it deals with requests 
for the previous and/or following bus (this latter 
should have been present in all of the systems). 
Just before the SDC started, the Port Author-
ity had removed some of its bus routes. The sys-
tems were required to be capable of informing the 
caller that the route had been canceled, and then 
giving them a suitable alternative. 
SDC systems answer live calls when the Port 
Authority call center is closed in the evening and 
early morning.  There are quite different types and 
volumes of calls over the different days of the 
week.  Weekend days typically have more calls, in 
part because the call center is open fewer hours on 
weekends.  Figure 1 shows a histogram of average 
calls per hour for the evening and the early morn-
ing of each day of the week. 
 
calls per weekday / ave per hour
0
1
2
3
4
5
6
7
8
9
10
Fr-
19-
0
Sa-
0-8
Sa-
16
-
0
Su-
0-8
Su-
16
-
0
Mo
-
0-7
Mo
-
19
-
0
Tu
-
0-7
Tu
-
19-
0
We
-
0-7
We
-
19
-
0
Th
-
0-7
Th
-
19-
0
Fr-
0-7
 
Figure 1: average number of calls per hour on weekends 
(dark bars) and weekdays. Listed are names of days and 
times before and after midnight when callers called the 
system. 
 
The control tests were set up through a simple 
web interface that presented 8 different scenarios 
to callers. Callers were given a phone number to 
call; each caller spoke to each of the 4 different 
systems twice.  A typical scenario was presented 
with few words, mainly relying on graphics in or-
der to avoid influencing the caller?s choice of vo-
cabulary.  An example is shown in Figure 2. 
 
 
 
Figure 2: Typical scenario for the control tests.  This 
example requests that the user find a bus from the cor-
ner of Forbes and Morewood (near CMU) to the airport, 
using bus route 28X, arriving by 10:45 AM. 
 
3
3 Control Test Results 
The logs from the four systems were labeled for 
task success by hand.  A call is successful if any of 
the following outputs are correctly issued: 
 
? Bus schedule for the requested departure and 
arrival stops for the stated bus number (if giv-
en). 
? A statement that there is no bus available for 
that route. 
? A statement that there is no scheduled bus at 
that time. 
 
We additionally allowed the following boundary 
cases: 
 
? A departure/arrival stop within 15 minutes 
walk. 
? Departure/arrival times within one hour of re-
quested time. 
? An alternate bus number that serves the re-
quested route. 
 
In the control tests, SYS2 had system connection 
issues that caused a number of calls to fail to con-
nect, as well as a poorer task completion.  It was 
not included in the live tests.  It should be pointed 
out that SYS2 was developed by a single graduate 
student as a class project while the other systems 
were developed by teams of researchers.  The re-
sults of the Control Tests are shown in Table 1 and 
are discussed further below. 
 
Table 1. Results of hand analysis of the four systems in 
the control test 
 The three major classes of system response 
are as follows.  no_info: this occurs when the sys-
tem gives neither a specific time nor a valid excuse 
(bus not covered, or none at that time).  no_info 
calls can be treated as errors (even though there 
maybe be valid reasons such as the caller hangs up 
because the bus they are waiting for arrives).  
donthave: identifies calls that state the requested 
bus is not covered by the system or that there is no 
bus at the requested time. pos_out: identifies calls 
where a specific time schedule is given.  Both 
donthave and pos_out calls may be correct or er-
roneous (e.g the given information is not for the 
requested bus,  the departure stop is wrong, etc). 
4 Live Tests Results 
In the live tests the actual Pittsburgh callers had 
access to three systems: SYS1, SYS3, and SYS4.  
Although engineering issues may not always be 
seen to be as relevant as scientific results, it is im-
portant to acknowledge several issues that had to 
be overcome in order to run the live tests. 
Since the Pittsburgh Bus Information System 
is a real system, it is regularly updated with new 
schedules from the Port Authority. This happens 
about every three months and sometimes includes 
changes in bus routes as well as times and stops. 
The SDC participants were given these updates 
and were allowed the time to make the changes to 
their systems. Making things more difficult is the 
fact that the Port Authority often only releases the 
schedules a few days ahead of the change. Another 
concern was that the live tests be run within one 
schedule period so that the change in schedule 
would not affect the results.   
The second engineering issue concerned 
telephony connectivity. There had to be a way to 
transfer calls from the Port Authority to the par-
ticipating systems (that were run at the participat-
ing sites, not at CMU) without slowing down or 
perturbing service to the callers.  This was 
achieved by an elaborate set of call-forwarding 
mechanisms that performed very reliably.  How-
ever, since one system was in Europe, connections 
to it were sometimes not as reliable as to the US-
based systems.  
 
 SYS1 SYS3 SYS4 
Total Calls 678 451 742 
Non-empty calls 633 430 670 
no_ info 18.5% 14.0% 11.0% 
donthave 26.4% 30.0% 17.6% 
donthave_corr 47.3% 40.3% 37.3% 
donthave_incorr 52.7% 59.7% 62.7% 
pos_out 55.1% 56.0% 71.3% 
pos_out_corr 86.8% 93.8% 91.6% 
pos_out_incorr 13.2% 6.2% 8.4% 
 
Table 2. Results of hand analysis of the three systems in 
the live tests.  Row labels are the same as in Table 1. 
 SYS1 SYS2 SYS3 SYS4 
Total Calls 91 61 75 83 
no_ info 3.3% 37.7% 1.3% 9.6% 
donthave 17.6% 24.6% 14.7% 9.6% 
donthave_corr 68.8% 33.3% 100.0% 100.0% 
donthave_incorr 31.3% 66.7% 0.0% 0.0% 
pos_out 79.1% 37.7% 84.0% 80.7% 
pos_out_corr 66.7% 78.3% 88.9% 80.6% 
pos_out_incorr 33.3% 21.7% 11.1% 19.4% 
4
We ran each of the three systems for multiple two 
day periods over July and August 2010.  This de-
sign gave each system an equal distribution of 
weekdays and weekends, and also ensured that 
repeat-callers within the same day experienced the 
same system. 
One of the participating systems (SYS4) 
could support simultaneous calls, but the other two 
could not and the caller would receive a busy sig-
nal if the system was already in use.  This, how-
ever, did not happen very often. 
Results of hand analysis of real calls are 
shown in Table 4 alongside the results for the Con-
trol Test for easy comparison.  In the live tests we 
had an additional category of call types ? empty 
calls (0-turn calls) ? which are calls where there 
are no user turns, for example because the caller 
hung up or was disconnected before saying any-
thing.  Each system had 14 days of calls and exter-
nal daily factors may change the number of calls. 
We do suspect that telephony issues may have pre-
vented some calls from getting through to SYS3 on 
some occasions.   
Table 3 provides call duration information for 
each of the systems in both the control and live 
tests. 
 
 
 Length (s) Turns/call Words/turn 
SYS1 control 155 18.29 2.87 (2.84) 
SYS1 live 111 16.24 2.15 (1.03) 
SYS2 control 147 17.57 1.63 (1.62) 
SYS3 control 96 10.28 2.73 (1.94) 
SYS3 live 80 9.56 2.22 (1.14) 
SYS4 control 154 14.70 2.25 (1.78) 
SYS4 live 126 11.00 1.63 (0.77) 
 
Table 3: For live tests, average length of each call, aver-
age number of turns per call, and average number of 
words per turn (numbers in brackets are standard devia-
tions). 
 
Each of the systems used a different speech 
recognizer.  In order to understand the impact of 
word error rate on the results, all the data were 
hand transcribed to provide orthographic transcrip-
tions of each user turn.   Summary word error sta-
tistics are shown in Table 4.   However, summary 
statistics do not show the correlation between word 
error rate and dialogue success.  To achieve this, 
following Thomson et al(2010), we computed a 
logistic regression of success against word error 
rate (WER) for each of the systems. Figure 3 
shows the regressions for the Control Tests and 
Figure 4 for the Live Tests.  
 
 SYS1 SYS3 SYS4 
Control 38.4 27.9 27.5 
Live 43.8 42.5 35.7 
 
Table 4: Average dialogue word error rate (WER). 
 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
es
s 
Ra
te
Sys4
Sys3
Sys1
 
Figure 3: Logistic regression of control test success vs 
WER for the three fully tested systems 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
e
ss
Sys1
Sys3
Sys4
 
Figure 4: Logistic regression of live success vs WER for 
the three fully tested systems 
 
5
In order to compare the control and live tests, 
we can calculate task completion as the percentage 
of calls that gave a correct result.  We include only 
non-empty calls (excluding 0-turn calls), and treat 
all no_info calls as being incorrect, even though 
some may be due to extraneous reasons such as the 
bus turning up (Table 5). 
 
 SYS1 SYS3 SYS4 
Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%) 
Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%) 
 
Table 5: Live and control test task completion (std. err).  
 
5 Discussion 
All systems had lower WER and higher task com-
pletion in the controlled test vs. the live test.  This 
agrees with past work [Raux et al2005, Ai et al
2008], and underscores the challenges of deploying 
real-world systems. 
For all systems, dialogs with controlled sub-
jects were longer than with live callers ? both in 
terms of length and number of turns.  In addition, 
for all systems, live callers used shorter utterances 
than controlled subjects.  Controlled subjects may 
be more patient than live callers, or perhaps live 
callers were more likely to abandon calls in the 
face of higher recognition error rates.   
Some interesting differences between the sys-
tems are evident in the live tests.  Looking at dia-
log durations, SYS3 used confirmations least often, 
and yielded the fastest dialogs (80s/call).  SYS1 
made extensive use of confirmations, yielding the 
most turns of any system and slightly longer dia-
logs (111s/call).  SYS4 was the most system-
directed, always collecting information one ele-
ment at a time.  As a result it was the slowest of the 
systems (126s/call), but because it often used im-
plicit confirmation instead of explicit confirmation, 
it had fewer turns/call than SYS1.   
For task completion, SYS3 performed best in 
the controlled trials, with SYS1 worst and SYS4 in 
between.  However in the live test, SYS4 per-
formed best, with SYS3 and SYS1 similar and 
worse.  It was surprising that task completion for 
SYS3 was the highest for the controlled tests yet 
among the lowest for the live tests.  Investigating 
this, we found that much of the variability in task 
completion for the live tests appears to be due to 
WER.  In the control tests SYS3 and SYS4 had 
similar error rates but the success rate of SYS3 was 
higher.  The regression in Figure 3 shows this 
clearly.   In the live tests SYS3 had a significantly 
higher word error rate and average success rate 
was much lower than in SYS4.   
It is interesting to speculate on why the rec-
ognition rates for SYS3 and SYS4 were different 
in the live tests, but were comparable in the control 
tests.  In a spoken dialogue system the architecture 
has a considerable impact on the measured word 
error rate.  Not only will the language model and 
use of dialogue context be different, but the dia-
logue design and form of system prompts will in-
fluence the form and content of user inputs.   Thus, 
word error rates do not just depend on the quality 
of the acoustic models ? they depend on the whole 
system design.  As noted above, SYS4 was more 
system-directed than SYS3 and this probably con-
tributed to the comparatively better ASR perform-
ance with live users.   In the control tests, the 
behavior of users (research lab workers) may have 
been less dependent on the manner in which users 
were prompted for information by the system.  
Overall, of course, it is user satisfaction and task 
success which matter. 
6 Corpus Availability and Evaluation 
The SDC2010 database of all logs from all systems 
including audio plus hand transcribed utterances, 
and hand defined success values is released 
through CMU?s Dialog Research Center 
(http://dialrc.org). 
One of the core goals of the Spoken Dialog 
Challenge is to not only create an opportunity for 
researchers to test their systems on a common plat-
form with real users, but also create common data 
sets for testing evaluation metrics.  Although some 
work has been done on this for the control test data 
(e.g. [Zhu et al2010]), we expect further evalua-
tion techniques will be applied to these data. 
One particular issue which arose during this 
evaluation concerned the difficulty of defining pre-
cisely what constitutes task success.  A precise de-
finition is important to developers, especially if 
reinforcement style learning is being used to opti-
mize the success.  In an information seeking task 
of the type described here, task success is straight-
forward when the user?s requirements can be satis-
fied but more difficult if some form of constraint 
relaxation is required.   For example, if the user 
6
asks if there is a bus from the current location to 
the airport ? the answer ?No.? may be strictly cor-
rect but not necessarily helpful.  Should this dia-
logue be scored as successful or not?  The answer 
?No, but there is a stop two blocks away where 
you can take the number 28X bus direct to the air-
port.? is clearly more useful to the user.  Should 
success therefore be a numeric measure rather than 
a binary decision?  And if a measure, how can it be 
precisely defined?  A second and related issue is 
the need for evaluation algorithms which deter-
mine task success automatically.   Without these, 
system optimization will remain an art rather than 
a science. 
7 Conclusions 
This paper has described the first attempt at an ex-
ercise to investigate how different spoken dialog 
systems perform on the same task.  The existing 
Let?s Go Pittsburgh Bus Information System was 
used as a task and four teams provided systems 
that were first tested in controlled conditions with 
speech researchers as users. The three most stable 
systems were then deployed ?live? with real call-
ers. Results show considerable variation both be-
tween systems and between the control and live 
tests.  Interestingly, relatively high task completion 
for controlled tests did not always predict rela-
tively high task completion for live tests.  This 
confirms the importance of testing on live callers, 
not just usability subjects. 
 The general organization and framework 
of the evaluation worked well.  The ability to route 
audio telephone calls to anywhere in the world us-
ing voice over IP protocols was critical to the suc-
cess of the challenge since it provides a way for 
individual research labs to test their in-house sys-
tems without the need to port them to a central co-
ordinating site. 
 Finally, the critical role of precise evalua-
tion metrics was noted and the need for automatic 
tools to compute them.  Developers need these at 
an early stage in the cycle to ensure that when sys-
tems are subsequently evaluated, the results and 
system behaviors can be properly compared.  
Acknowledgments 
Thanks to AT&T Research for providing telephony 
support for transporting telephone calls during the 
live tests.  This work was in part supported by the 
US National Science foundation under the project 
?Dialogue Research Center?.   
References  
Ai, H., Raux, A., Bohus, D., Eskenzai, M., and Litman, 
D.  (2008)  ?Comparing spoken dialog corpora col-
lected with recruited subjects versus real users?, Proc 
SIGDial, Columbus, Ohio, USA.  
Black, A., Burger, S., Langner, B., Parent, G., and Es-
kenazi, M. (2010) ?Spoken Dialog Challenge 2010?, 
SLT 2010, Berkeley, CA.  
Hastie, H., Merigaud, N., Liu, X and Oliver Lemon. 
(2010) ? ?Let?s Go Dude?, Using The Spoken Dia-
logue Challenge to Teach Spoken Dialogue Devel-
opment?, SLT 2010, Berkeley, CA. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M.  (2005)  ?Let?s go public! Taking a spoken dialog 
system to the real world?, Interspeech 2005, Lisbon, 
Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. (2006) ?Doing Research on a Deployed 
Spoken Dialogue System: One Year of Let's Go! Ex-
perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.  
Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,  
Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-
tem for the Let?s Go Spoken Dialogue Challenge?, 
SLT 2010, Berkeley, CA. 
Williams, J., Arizmendi, I., and Conkie, A. ?Demonstra-
tion of AT&T ?Let?s Go?: A Production-Grade Statis-
tical Spoken Dialog System.? SLT 2010, Berkeley, 
CA. 
Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., and 
King, I. (2010) ?Using Finite State Machines for 
Evaluating Spoken Dialog Systems?, SLT 2010, 
Berkeley, CA. 
7
