Proceedings of the 8th International Conference on Computational Semantics, pages 346?350,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Concept and Relation Extraction in the Finance
Domain
Mihaela Vela
DFKI Saarbr?cken
Mihaela.Vela@dfki.de
Thierry Declerck
DFKI Saarbr?cken
Thierry.Declerck@dfki.de
Abstract
In this paper, we describe the state of our work on the possible deriva-
tion of ontological structures from textual analysis. We propose an ap-
proach to semi-automatic generation of domain ontologies from scratch,
on the basis of heuristic rules applied to the result of a multi-layered
processing of textual documents.
1 Introduction
In the context of the MUSING R&D European project
1
, which is dedi-
cated to the development of Business Intelligence (BI) tools and modules
founded on semantic-based knowledge and content systems, we are investi-
gating among others the integration of semantic web and human language
technologies for enhancing the technological foundations of knowledge ac-
quisition and reasoning in BI applications.
In the first phase of the project many efforts have been dedicated to
the manual creation of domain related ontologies and their integration in
upper level ontologies. The creation of those ontologies was guided by do-
main experts, who were submitting so-called competency questions to the
ontology engineers in order to support they work on ontology design and
implementation. Since this approach to ontology creation is very time and
resource consuming, we wanted to investigate the possible automation of
ontology building directly from textual documents, with a special focus on
the financial domain.
2 The Approach
We consider the semi-automatic ontology derivation from text as a linguistic
rule-based approach, which on the basis of lexical and syntactic properties
1
See www.musing.eu for more details
346
can suggest potential ontology classes and properties that can be used for
building an ontology.
We suggest a multi-layered approach, which starts with a very shallow
analysis of certain lexical properties of words or very short combination of
words, going from there to Part-of-Speech (POS) Tagging and morphological
analysis, before using, in a next step, deeper syntactic analysis and taking
into account larger parts of text, up to the level of sentences or even para-
graphs. The idea behind this: at the shallow level it is possible to detect
possible classes and relations, which can then be consolidated, refined or re-
jected at further stages of textual analysis, with the help of domain experts
and ontology engineers.
Our final goal is to clearly state what kind of ontological resource can be
extracted from financial documents (annual reports of companies, financial
newspapers) at various level of textual processing. As a data source we work
first with a corpus of economical news articles from the German newspaper
Wirtschaftswoche.
3 String-Based Processing
As a first step in the task of extracting ontology concepts and relations from
(German) textual documents, we decided to look in the corpus for words
occurring alone (and starting with a capital letter) and in the context of
larger strings (which we assume to be mostly nominal compounds). We
call the words that show this property "anchor-words". We consider them
potential labels of ontology classes and the compounds, in which they occur,
as expressing potential relations for the labels of ontology classes.
Take for example the anchor-word konzern (corporation), which is also
occurring as part of the compound medienkonzern (media corporation). At
this very shallow and pattern-based processing level, we tentatively derive
that from the compound construction PREFIX + ANCHOR we can extract
medienkonzern ISA_SUBCLASS_OF konzern. Another example of com-
pound is konzernverwaltung (corporation management). Here we derive
from the compound construction ANCHOR + SUFFIX the relation: konzern
HAS verwaltung (corporation HAS management);
Although the examples demonstrate that a string analysis can to some
extent propose some guidelines for ontology extraction, there are for sure
major limitations, due to the lack of well-defined domain and range spec-
ifications in the proposed relations, the constraint relative to the number
of extracted relations and classes, the multiple appearance of morphological
variations, the lack of textual context etc.
In order to reduce the limitations just mentioned, we started by looking
for alternative formulations of the compounds, which can help in establishing
some filters and validation steps for relation extraction. So for example the
347
expression Chef vom Konzern (chief of the corporation) is validating the
property relation Konzern HAS Chef (corporation HAS chief ), due to the
meaning we can associate with the preposition von (part-of, belonging-to).
Concerning compound reformulations expressed by genitive post- modifi-
cation, like mitarbeiter einer deutschen bank (employees of a german bank),
we can see that they validate the relation extracted from the corresponding
compounds, since the genitive constructions have here a part-of/belonging-
to meaning.
4 Morphology and Lexical Semantics for Ontology
Derivation
A way to reduce some of the limitations described in Section 3 lies in the
use of morpho-syntactic information. So for example the word Firmenchef
(the boss of the firm) would be analyzed as follows:
(1) <W INFL="[17 18 19]" POS="1" STEM="chef" COMP="firmenchef"
TC="22">Firmenchef</W>
This annotation is to read like this: the word Firmenchef has the stem
chef, has POS noun, is the result of combining the word Firmen and the
word Chef, and has certain morphological properties (here encoded with
numbers). We can then describe certain morphological constraints for filter-
ing out some suggested relations from Section 3. For example, Chemiekonz-
ern is introducing a subclass relation between Chemiekonzern and Konzern,
whereas for Grosskonzern (large corporation) the subclass relation between
Grosskonzern and Konzern does not apply. The constraint proposed for
solving this kind of ambiguities is: the compound should consists of two
nouns.
Lexical semantics can also improve the quality of relation extraction.
For the compound Chefdenker (chief thinker), we want to ensure that no
HAS-relation between an ontology class labeled by chief and thinker is sug-
gested. For this purpose we use lexical semantic resources, like WordNet,
and formulate a rule that states that if the word occurring in the prefix
position of a compound is a person, and the second part of the compound
is also denoting a person, then the HAS-relation can not be derived.
Despite of the improvements made possible by morphology and lexical
semantics a major limitation remains: ontology extraction is proposed only
on the basis of word analysis and not on the basis of phrases and sentences,
which offer more context.
348
5 Syntactic Information for the Generation of On-
tologies
By combining the processing steps described above, we were able to extract
possible relevant ontology classes, relations and properties. For further im-
provement we need to consider both the linguistic context and some infor-
mation available in ontologies so far. The syntactic analysis of the sentence
below is a good example to show how a larger linguistic context can help
improving the method described in this paper.
(2) [NP-Subj Er] [VG soll] [PP im Konzern] [NP-Ind-Obj Finanzchef [NE-Pers
Gerhard Liener] ] [VG folgen]
Through the syntactic structuring of the sentence, we can semantically
group the items, so that we can extract the fact that a financial chief is
within a corporation, since the description of job succession is within a cor-
poration (marked by the prepositional phrase im Konzern). This aspect of
ontology learning is being currently investigated and implemented.
6 Conclusions and Further Work
In this paper we have been describing a multi-layer textual analysis strat-
egy that can help in building up ontologies from scratch, or integrate new
suggested ontology classes (or relations and properties) into existing ontolo-
gies. Since this is work in progress we also intended to get a clearer picture
on what kind of ontological knowledge can be extracted from the different
layers of textual processing.
For the "shallowest" parts of our suggested approach we could see that
proposed labels for ontology classes and relations seem to be appropriate.
For sure, some evaluations of this work has to be done. Nevertheless, we
see a big potential in a combination of suggestions generated by linguistic
analysis, domain experts and ontology engineers.
References
Massimiliano Ciaramita, Aldo Gangemi, Esther Ratsch, Jasmin Saric, and Isabel
Rojas. Unsupervised learning of semantic relations for molecular biology ontolo-
gies. In Paul Buitelaar and Philipp Cimiano, editors, Ontology Learning and
Population: Bridging the Gap between Text and Knowledge, pages 91?107. IOS
Press, Amsterdam, 2008.
Philipp Cimiano, Siegfried Handschuh, and Steffen Staab. Towards the self-
annotating web. In Proceedings of the 13th World Wide Web Conference, 2004.
Thierry Declerck and Mihaela Vela. A generic nlp tool for supporting shallow
ontology building. In Proceedings of LREC, Genoa, May 2006.
349
Thierry Declerck, Hans-Ulrich Krieger, Bernd Kiefer, Marcus Spies, and Christian
Leibold. Integration of semantic resources and tools for business intelligence. In
International Workshop on Semantic-Based Software Development held at OP-
SLA 2007, 2007.
Roberto Navigli and Paola Velardi. From glossaries to Ontologies: Extracting Se-
mantic Structure from Textual Definitions, pages 71?91. IOS Press, 2008.
Patrick Pantel and Marco Pennacchiotti. Automatically Harvesting and Ontologiz-
ing Semantic Relations, pages 171?199. IOS Press, 2008.
350
Multi-dimensional Annotation and Alignment in an English-German 
Translation Corpus 
 
 
Silvia Hansen-Schirra 
Computational Linguistics & 
Applied Linguistics,  
Translation and Interpreting 
Saarland University, 
Germany 
hansen@coli.uni-
sb.de 
Stella Neumann 
Applied Linguistics, 
Translation and Interpreting 
 
Saarland University,  
Germany 
st.neumann@mx.uni-
saarland.de 
Mihaela Vela 
Applied Linguistics, 
Translation and Interpreting 
 
Saarland University, 
Germany 
m.vela@mx.uni-
saarland.de 
 
 
Abstract 
This paper presents the compilation of 
the CroCo Corpus, an English-German 
translation corpus. Corpus design, anno-
tation and alignment are described in de-
tail. In order to guarantee the searchabil-
ity and exchangeability of the corpus, 
XML stand-off mark-up is used as repre-
sentation format for the multi-layer anno-
tation. On this basis it is shown how the 
corpus can be queried using XQuery. 
Furthermore, the generalisation of results 
in terms of linguistic and translational re-
search questions is briefly discussed. 
1 Introduction 
In translation studies the question of how trans-
lated texts differ systematically from original 
texts has been an issue for quite some time with a 
surge of research in the last ten or so years. Ex-
ample-based contrastive analyses of small num-
bers of source texts and their translations had 
previously described characteristic features of 
the translated texts, without the availability of 
more large-scale empirical testing. Blum-Kulka 
(1986), for instance, formulates the hypothesis 
that explicitation is a characteristic phenomenon 
of translated versus original texts on the basis of 
linguistic evidence from individual sample texts 
showing that translators explicitate optional co-
hesive markers in the target text not realised in 
the source text. In general, explicitation covers 
all features that make implicit information in the 
source text clearer and thus explicit in the trans-
lation (cf. Steiner 2005).  
Building on example-based work like Blum-
Kulka?s, Baker put forward the notion of trans-
lation universals (cf. Baker 1996) which can be 
analysed in corpora of translated texts regardless 
of the source language in comparison to original 
texts in the target language. Olohan and Baker 
(2000) therefore analyse explicitation in English 
translations concentrating on the frequency of the 
optional that versus zero-connector in combina-
tion with the two verbs say and tell. While being 
extensive enough for statistical interpretation, 
corpus-driven research like Olohan and Baker's 
is limited in its validity to the selected strings.  
More generally speaking, there is a gap be-
tween the abstract research object and the low le-
vel features used as indicators. This gap can be 
reduced by operationalising notions like explicit-
tation into syntactic and semantic categories, 
which can be annotated and aligned in a corpus. 
Intelligent queries then produce linguistic evi-
dence with more explanatory power than low le-
vel data obtained from raw corpora. The results 
are not restricted to the queried strings but extend 
to more complex units sharing the syntactic and/ 
or semantic properties obtained by querying the 
annotation.  
This methodology serves as a basis for the 
CroCo project, in which the assumed translation 
property of explicitation is investigated for the 
language pair English ? German. The empirical 
evidence for the investigation consists in a cor-
pus of English originals, their German transla-
tions as well as German originals and their Eng-
lish translations. Both translation directions are 
represented in eight registers. Biber?s calcula-
tions, i.e. 10 texts per register with a length of at 
least 1,000 words, serve as an orientation for the 
size of the sub-corpora (cf. Biber 1993). Alto-
35
gether the CroCo Corpus comprises one million 
words. Additionally, reference corpora are in-
cluded for German and English. The reference 
corpora are register-neutral including 2,000 word 
samples from 17 registers (see Neumann & Han-
sen-Schirra 2005 for more details on the CroCo 
corpus design).  
The CroCo Corpus is tokenised and annotated 
for part-of-speech, morphology, phrasal catego-
ries and grammatical functions. Furthermore, the 
following (annotation) units are aligned: words, 
grammatical functions, clauses and sentences. 
The annotation and alignment steps are described 
in section 2.  
Each annotation and alignment layer is stored 
separately in a multi-layer stand-off XML repre-
sentation format. In order to empirically investi-
gate the parallel corpus (e.g. to find evidence for 
explicitation in translations), XQuery is used for 
posing linguistic queries. The query process it-
self works on each layer separately, but can also 
be applied across different annotation and align-
ment layers. It is described in more detail in sec-
tion 3. This way, parallel text segments and/or 
parallel annotation units can be extracted and 
compared for translations and originals in Ger-
man and English. 
2 CroCo XML 
The annotation in CroCo extends to different 
levels in order to cover possible linguistic evi-
dence on each level. Thus, each kind of annota-
tion (part-of-speech, morphology, phrase struc-
ture, grammatical functions) is realised in a sepa-
rate layer. An additional layer is included which 
contains comprehensive metainformation in se-
parate header files for each text in the corpus. 
The file containing the indexed tokens (see sec-
tion 2.1) includes an xlink attribute referring to 
this header file as depicted in Figure 2.1. The 
metadata are based on the TEI guidelines1 and 
include register information. The complex multi-
lingual structure of the corpus in combination 
with the multi-layer annotation requires indexing 
the corpus. The indexing is carried out on the ba-
sis of the tokenised corpus. Index and annotation 
layers are kept separate using XML stand-off 
mark-up. The mark-up builds on XCES2. Differ-
ent formats of the multiple annotation and align-
ment outputs are converted with Perl scripts. 
Each annotation and alignment unit is indexed. 
                                                 
1 http://www.tei-c.org 
2 http://www.xml-ces.org 
The respective annotations and alignments are 
linked to the indexed units via XPointers.  
The following sections describe the different 
annotation layers and are exemplified for the 
German original sentence in (1) and its English 
translation in (2)3. 
 
(1) Ich spielte viele M?glichkeiten 
durch, stellte mir den T?ter in 
verschiedenen Posen vor, ich und 
die Pistole, ich und die Giftfla-
sche, ich und der Kn?ppel, ich und 
das Messer. 
 
(2) I ran through numerous possibi-
lities, pictured the perpetrator in 
various poses, me with the gun, me 
with the bottle of poison, me with 
the bludgeon, me with the knife. 
2.1 Tokenisation and indexing 
The first layer to be presented here is the tokeni-
sation layer. Tokenisation is performed in CroCo 
for both German and English by TnT (Brants 
2000), a statistical part-of-speech tagger. As 
shown in Figure 2.1 each token annotated with 
the attribute strg has also an id attribute, which 
indicates the position of the word in the text. 
This id represents the anchor for all XPointers 
pointing to the tokenisation file by an id starting 
with a ?t?. The file is identified by the name at-
tribute. The xml:lang attribute indicates the lan-
guage of the file, docType provides information 
on whether the present file is an original or a 
translation.  
 
<document 
xmlns:xlink="http://www.w3.org/1999/ 
xlink" name="GO.tok.xml" xml:lang="de"  
docType="ori"> 
<header xlink:href="GO.header.xml"/> 
 <tokens> 
  <token id="t64" strg="Ich"/> 
  <token id="t65" strg="spielte"/> 
  <token id="t66" strg="viele"/> 
  <token id="t67" 
   strg="M?glichkeiten"/> 
  <token id="t68" strg="durch"/> 
  <token id="t69" strg=","/> 
</tokens> 
</document> 
 
Figure 2.1. Tokenisation and indexing 
 
Similar index files necessary for the alignment of 
the respective levels are created for the units 
chunk, clause and sentence. These units stand in 
                                                 
3 All examples are taken from the CroCo Corpus. 
36
a hierarchical relation with sentences consisting 
of clauses, clauses consisting of chunks etc.  
2.2 Part-of-speech tagging 
The second layer annotated for both languages is 
the part-of-speech layer, which is provided again 
by TnT4. The token annotation of the part-of-
speech layer starts with the xml:base attribute, 
which indicates the index file it refers to. The 
part-of-speech information for each token is an-
notated in the pos attribute, as shown in Figure 
2.2. The attribute strg in the token index file and 
pos in the tag annotation are linked by an xlink 
attribute pointing to the id attribute in the index 
file. For example, the German token pointing to 
"t65" in the token index file whose strg value is 
stellte is a finite verb (with the PoS tag vvfin).  
 
<document 
xmlns:xlink="http://www.w3.org/1999/ 
xlink" name="GO.tag.xml"> 
 <tokens xml:base="GO.tok.xml"> 
  <token pos="pper" xlink:href="#t64"/> 
  <token pos="vvfin" 
   xlink:href="#t65"/> 
  <token pos="pidat" 
   xlink:href="#t66"/> 
  <token pos="nn" xlink:href="#t67"/> 
  <token pos="ptkvz"  
   xlink:href="#68"/> 
  <token pos="yc" xlink:href="#t69"/> 
</tokens> 
</document> 
 
Figure 2.2. PoS tagging  
2.3 Morphological annotation 
Morphological information is particularly rele-
vant for German due to the fact that this lan-
guage carries much syntactic information within 
morphemes rather than in separate function 
words like English. Morphology is annotated in 
CroCo with MPro, a rule-based morphology tool 
(Maas 1998). This tool works on both languages. 
As shown in Figure 2.3 each token has morpho-
logical attributes such as person, case, gender, 
number and lemma. As before, the xlink attrib-
ute refers back to the index file, thus providing 
the connection between the morphological attri-
butes and the strg information in the index file.  
For the morphological annotation of the Ger-
man token "t65" in Figure 2.3 the strg value is 
determined by following the XPointer "t65" to 
the token index file, i.e. spielte. The pos value is 
retrieved by searching in the tag annotation for 
                                                 
4 For German we use the STTS tag set (Schiller et al 1999), 
and for English the Susanne tag set (Sampson 1995). 
the file with the same xml:base value. The 
matching tag, in this case vvfin, carries the same 
XPointer ?t65?. 
 
<document 
xmlns:xlink="http://www.w3.org/1999/ 
xlink" name="GO.morph.xml"> 
 <tokens xml:base="GO.tok.xml"> 
  <token strg="Ich" per="1" case="nom"  
   nb="sg" gender="f;m" lemma="ich"  
   lb="ich" xlink:href="#t64"/> 
  <token strg="spielte" vtype="fiv"  
   tns="past" per="3" nb="sg"  
   lemma="spielen" lb="spielen" comp= 
   "spielen" xlink:href="#t65"/> 
  <token strg="viele" case="nom;acc"  
   nb="plu" gender="f" lemma="viel"  
   lb="viel" comp="viel" deg="base"  
   xlink:href="#t66"/> 
  <token strg="M?glichkeiten" case= 
   "nom;acc" nb="plu" gender="f" lemma= 
   "m?glichkeit" lb="m?glich" comp= 
   "m?glichkeit" xlink:href="#t67"/> 
  <token strg="durch" lemma="durch"  
   lb="durch" pref="vzs"  
   xlink:href="#t68"/> 
  <token strg="," lemma="," lb=","     
   xlink:href="#t69"/> 
 </tokens> 
</document> 
 
Figure 2.3. Morphological annotation 
2.4 Phrase chunking and annotation of 
grammatical functions 
Moving up from the token unit to the chunk unit, 
first we have to index these units again before we 
can annotate them. The chunk index file assigns 
an id attribute to each chunk within the file. The 
problem of discontinuous phrase chunks is 
solved by listing child tags referring to the indi-
vidual tokens which make up the chunk via xlink 
attributes. Figure 2.4 shows that the VP ?ch14? 
in the German phrase annotation consists of 
?t70? (stellte) and ?t77? (vor). 
 
<document xmlns:xlink= 
"http://www.w3.org/1999/xlink" 
name="GO.chunk.xml"> 
 <chunks xml:base="GO.tok.xml"> 
  <chunk id="ch13"> 
   <tok xlink:href="#t66"/> 
   <tok xlink:href="#t67"/> 
  </chunk> 
  <chunk id="ch14"> 
   <tok xlink:href="#t70"/> 
  </chunk> 
  <chunk id="ch15"> 
   <tok xlink:href="#t71"/> 
  </chunk> 
  <chunk id="ch16"> 
   <tok xlink:href="#t72"/> 
   <tok xlink:href="#t73"/> 
  </chunk> 
37
  <chunk id="ch17"> 
   <tok xlink:href="#t74"/> 
   <chunk id="ch18"> 
    <tok xlink:href="#t75"/> 
    <tok xlink:href="#t76"/> 
   </chunk> 
  </chunk> 
  <chunk id="ch19"> 
    <tok xlink:href="#t77"/> 
  </chunk> 
 </chunks> 
</document> 
 
Figure 2.4. Chunk indexing 
 
The phrase structure annotation (see Figure 2.5) 
assigns the ps attribute to each phrase chunk 
identified by MPro. XPointers link the phrase 
structure annotation to the chunk index file. It 
should be noted that in CroCo the phrase struc-
ture analysis is limited to higher chunk nodes, as 
our focus within this layer is more on complete 
phrase chunks and their grammatical functions. 
 
<document   
 xmlns:xlink="http://www.w3.org/1999/ 
 xlink" name="GO.ps.xml"> 
 <chunks xml:base="GO.chunk.xml"> 
  <chunk ps="NP" xlink:href="#ch13"/> 
  <chunk ps="VPFIN"  
   xlink:href="#ch14"/> 
  <chunk ps="NP" xlink:href="#ch15"/> 
  <chunk ps="NP" xlink:href="#ch16"/> 
  <chunk ps="PP" xlink:href="#ch17"/> 
  <chunk ps="NP" xlink:href="#ch18"/> 
  <chunk ps="VPPRED"  
   xlink:href="#ch19"/> 
 </chunks> 
</document> 
 
Figure 2.5. Phrase structure annotation 
 
The annotation of grammatical functions is again 
kept in a separate file (see Figure 2.6). Only the 
highest phrase nodes are annotated for their 
grammatical function with the attribute gf. The 
XPointer links the annotation of each function to 
the chunk id in the chunk index file. From this 
file in turn the string can be retrieved in the token 
annotation. For example, the English chunk 
?ch13? carries the grammatical function of direct 
object (DOBJ). It is identified as an NP in the 
phrase structure annotation by comparing the 
xml:base attribute value of the two files and the 
XPointers.  
 
<document    
xmlns:xlink="http://www.w3.org/1999/ 
xlink" name="GO.gf.xml"> 
 <chunks xml:base="GO.chunk.xml"> 
  <chunk gf="DOBJ" xlink:href="#ch13"/> 
  <chunk gf="FIN" link:href="#ch14"/> 
  <chunk gf="IOBJ" xlink:href="#ch15"/> 
  <chunk gf="DOBJ" xlink:href="#ch16"/> 
  <chunk gf="ADV" xlink:href="#ch17"/> 
  <chunk gf="PRED" xlink:href="#ch19"/> 
 </chunks> 
</document> 
 
Figure 2.6. Annotation of grammatical functions  
2.5 Alignment 
In the examples shown so far, the different anno-
tation layers linked to each other all belonged to 
the same language. By aligning words, gram-
matical functions, clauses and sentences, the 
connection between original and translated text is 
made visible. The use of this multi-layer align-
ment will become clearer from the discussion of 
a sample query in section 3. 
For the purpose of the CroCo project word 
alignment is realised with GIZA++ (Och & Ney 
2003), a statistical alignment tool. Chunks and 
clauses are aligned manually with the help of 
MMAX II (M?ller & Strube 2003), a tool allow-
ing assignment of own categories and linking 
units. Finally, sentences are aligned using Win-
Align, an alignment tool within the Translator?s 
Workbench by Trados (Heyn 1996). 
The alignment procedure produces four new 
layers. It follows the XCES standard. Figure 2.7 
shows the chunk alignment of (1) and (2). In this 
layer, we align on the basis of grammatical func-
tions instead of phrases since this annotation in-
cludes the information of the phrase chunking as 
well as on the semantic relations of the chunks. 
The grammatical functions are mapped onto each 
other cross-linguistically and then aligned ac-
cording to our annotation and alignment scheme. 
The trans.loc attribute locates the chunk index 
file for the aligned texts in turn. Furthermore, the 
respective language as well as the n attribute or-
ganising the order of the aligned texts are given. 
We thus have an alignment tag for each language 
in each chunk pointing to the chunk index file. 
As can be seen from Figure 2.7, chunks which do 
not have a matching equivalent receive the value 
?#undefined?, a phenomenon that will be of in-
terest in the linguistic interpretation on the basis 
of querying the corpus. 
 
<document    
xmlns:xlink="http://www.w3.org/1999/ 
xlink" name="gfAlign.xml"> 
<translations xml:base="/CORPUS/"> 
<translation trans.loc="GO.chunk.xml"  
xml:lang="de" n="1"/> 
<translation 
trans.loc="ETrans.chunk.xml" 
xml:lang="en" n="2"/> 
38
</translations> 
 <chunks> 
  <chunk> 
   <align xlink:href="#ch14"/> 
   <align xlink:href="#ch16"/> 
  </chunk> 
  <chunk> 
   <align xlink:href="#ch15"/> 
   <align xlink:href="#undefined"/> 
  </chunk> 
  <chunk> 
   <align xlink:href="#ch16"/> 
   <align xlink:href="#ch17"/> 
  </chunk> 
  <chunk> 
   <align xlink:href="#ch17"/> 
   <align xlink:href="#ch18"/> 
  </chunk> 
  <chunk> 
   <align xlink:href="#ch19"/> 
   <align xlink:href="#undefined"/> 
  </chunk> 
 </chunks> 
</document> 
 
Figure 2.7. Chunk alignment 
3 Querying the CroCo Corpus 
The comprehensive annotation including the 
alignment described in section 2 is the basis for 
the interpretation to be presented in what fol-
lows. We concentrate on two types of queries 
into the different alignment layers that are as-
sumed relevant in connection with our research 
question.  
3.1 Crossing lines and empty links 
From the linguistic point of view we are inter-
ested in those units in the target text which do 
not have matches in the source text and vice 
versa, i.e. empty links, or whose alignment 
crosses the alignment of a higher level, i.e. 
crossing lines. We analyse for instance stretches 
of text contained in one sentence in the source 
text but spread over two sentences in the target 
text, as this probably has implications for the 
overall information contained in the target text. 
We would thus pose a query retrieving all in-
stances where the alignment of the lower level is 
not parallel to the higher level alignment but 
points into another higher level unit. In the ex-
ample below the German source sequence (3) as 
well as the English target sequence (4) both con-
sist of three sentences. These sentences are each 
aligned as illustrated by dashed boxes in Figure 
3.1.  
(3) Aus dem Augenwinkel sah ich, 
wie eine Schwester dem Bettnachbarn 
das Nachthemd wechselte. Sie rieb 
den R?cken mit Franzbranntwein ein 
und massierte den etwas j?ngeren 
Mann, dessen Adern am ganzen K?rper 
bl?ulich hervortraten. Ihre H?nde 
lie?en ihn leise wimmern.  
(4) Out of the corner of my eye I 
watched a nurse change his 
neighbor?s nightshirt and rub his 
back with alcoholic liniment. She 
massaged the slightly younger man, 
whose veins stood out blue all over 
his body. He whimpered softly under 
her hands.  
 
In German the first two sentences are subdivided 
into two clauses each. The English target sen-
tences are co-extensive with the clauses con-
tained in each sentence. This means that two 
English clauses have to accommodate four Ger-
man clauses. Figure 3.1 shows that the German 
clause 3 (Sie rieb den R?cken mit Franzbrannt-
wein ein) in sentence 2 is part of the bare infini-
tive complementation (?and rub his back with 
alcoholic liniment) in the English sentence 1. 
The alignment of this clause points out of the 
aligned first sentence, thus constituting crossing 
lines.  
 
Sentence 3Sentence 2Sentence 1
Clause 1 Clause 2 Clause 3 Clause 4 Clause 5
Sentence 3Sentence 2Sentence 1
Clause 1 Clause 2 Clause 3
German
source
English
target  
Figure 3.1. Sentence and clause alignment  
 
The third sentence also contains a crossing line, 
in this case on the levels of chunk and word 
alignment: The words Ihre H?nde in the German 
subject chunk are aligned with the words her 
hands in the English adverbial chunk. However, 
this sentence is particularly interesting in view of 
empty links. The query asks for units not match-
ing any unit in the parallel text, i.e. for xlink at-
tributes whose values are ?#undefined? (cf. sec-
tion 2.5). In Figure 3.2, the empty links are 
marked by a black dot.  
DOBJ ADVFINSUBJ
word 1 word 2 word 3 word 4 word 5
German
source
English
target
PRED
word 6
SUBJ
word 1
FIN
word 2
ADV
word 3
ADV
word 4 word 5 word 6
Figure 3.2. Chunk and word alignment  
 
39
Our linguistic interpretation is based on a func-
tional view of language. Hence, the finite lie?en 
(word 3) in the German sentence is interpreted as 
a semi-auxiliary and thus as the finite part of the 
verbal group. Therefore, wimmern (word 6) re-
ceives the label PRED, i.e. the non-finite part of 
the verb phrase, in the functional analysis. This 
German word is linked to word 2 (whimpered) in 
the target sentence, which is assigned FIN, i.e. 
the finite verb in the layer of grammatical func-
tions. As FIN exists both in the source and in the 
target sentences, this chunk is aligned. The Ger-
man functional unit PRED does not have an 
equivalent in the target text and gets an empty 
link. Consequently, word 3 in the source sen-
tence (lie?en) receives an empty link as well. 
This mismatch will be interpreted in view of our 
translation-oriented research. In the following 
subsection we will see how these two phenom-
ena can be retrieved automatically.  
3.2 Corpus exploitation using XQuery 
Since the multi-dimensional annotation and 
alignment is realised in XML, the queries are 
posed using XQuery5. This query language is 
particularly suited to retrieve information from 
different sources like for instance individual an-
notation and alignment files. The use for multi-
layer annotation is shown in (Teich et al 2001).  
The query for determining an empty link at 
word level can be formulated as follows: find all 
words which do not have an aligned correspon-
dent, i.e. which carry the xlink attribute value 
?#undefined?. The same query can be applied on 
the chunk level, the query returning the gram-
matical functions that do not have an equivalent 
in the other language.  
 
(5)Ihre H?nde lie?en ihn leise wim-
mern.  
(6) He whimpered softly under her 
hands.  
 
Applied to the sentences in (5) and (6) the 
XQuery in Figure 3.3 returns all German and 
English words, which receive an empty link due 
to a missing equivalent in alignment (lie?en and 
under). This query can be used analogously in all 
other alignment layers. It implies the call of a 
self-defined XQuery function (see Figure 3.4), 
which looks in the correspondent index file for 
words not aligned. 
 
                                                 
5 http://www.w3.org/TR/xquery 
let $doc := . 
for $k in $doc//tokens/token 
return 
 if ($k/align[1][@xlink:href="#un- 
  defined"] and $k/align[2]  
  [@xlink:href!="#undefined"]) 
 then local:getString($k/align[1]/ 
  @xlink:href,$k/align[2]/@xlink:href, 
  $doc//translations/translation 
  [@n='2']/@trans.loc) 
 else if ($k/align[1][@xlink:href 
  !="#undefined"] and $k/align[2] 
  [@xlink:href="#undefined"]) 
 then local:getString($k/align[1]/ 
  @xlink:href,$k/align[2]/@xlink:href, 
  $doc//translations/translation 
  [@n='1']/@trans.loc) 
 else () 
 
Figure 3.3. XQuery for empty links  
 
declare function local:getString 
($firstToken as xs:string,$secondTo- 
ken as xs:string,$fileName as 
xs:string) as element() 
 {let $res:=(if(($firstToken eq  
  "#undefined") and ($lang eq doc  
  ($fileName)//document/@xml:lang))  
  then doc($fileName)//tokens/token[@id  
  eq substring-after($secondToken,"#")] 
  else if (($secondToken eq "#unde- 
  fined") and ($lang eq doc($fileName) 
  //document/@xml:lang))  
  then doc($fileName)//tokens/token[@id  
  eq substring-after($firstToken,"#")] 
  else ()) 
return  
 <token>{$res/@strg}</token>}; 
 
Figure 3.4. XQuery function for missing align-
ment 
 
Querying crossing lines in the German source 
sentence in (5) and the English target sentence in 
(6) is based on the annotation at word level as 
well as on the annotation at the chunk level. As 
mentioned in section 3.1, crossing lines are iden-
tified in (5) and (6) if the words contained in the 
chunks aligned on the grammatical function layer 
are not aligned on the word level. This means 
that the German subject is aligned with the Eng-
lish subject, but the words within the subject 
chunk are aligned with words in other grammati-
cal functions instead. 
In a first step, the query for determining a 
crossing line requires information about all 
aligned German chunks with a xlink attribute 
whose value is not ?#undefined? and all aligned 
German words with a xlink attribute whose 
value is not ?#undefined?. Then all German 
words that are not aligned on the word level but 
are aligned as part of chunks on the chunk level 
40
are filtered out. Figure 3.6 reflects the respective 
XQuery. 
 
let $doc := . 
for $k in $doc//chunks/chunk 
let $ch1:=(if($k/align[1][@xlink:href  
 !="#undefined"] and $k/align[2]  
 [@xlink:href!="#undefined"]) 
 then doc($doc//translations/trans- 
 lation[@n='1']/@trans.loc)//chunks 
 /chunk[@id eq substring-after  
 ($k/align[1]/@xlink:href,"#")] 
 else ()) 
let $ch2:=(if($k/align[1][@xlink:href  
 !="#undefined"] and $k/align[2] 
 [@xlink:href!="#undefined"]) 
 then (doc($doc//translations/transla- 
 tion[@n='2']/@trans.loc)//chunks/chunk 
 [@id eq substring-after($k/align[2]/ 
 @xlink:href,"#")]) 
 else ()) 
for $i in doc("g2e.tokenAlign.xml") 
 //tokens/token 
let $tok1:=(if($i/align[1][@xlink:href  
 !="#undefined"] and $i/align[2] 
 [@xlink:href!="#undefined"]) 
 then(doc(doc("g2e.tokenAlign.xml") 
 //translations/translation[@n='1'] 
 /@trans.loc)//tokens/token[@id eq  
 substring-after($i/align[1] 
 /@xlink:href,"#")]) 
 else ()) 
let $tok2:=(if($i/align[1][@xlink:href  
 !="#undefined"] and $i/align[2] 
 [@xlink:href!="#undefined"]) 
 then(doc(doc("g2e.tokenAlign.xml") 
 //translations/translation[@n='2'] 
 /@trans.loc)//tokens/token[@id eq  
 substring-after($i/align[2] 
 /@xlink:href,"#")]) 
 else ()) 
where(local:containsToken($ch1/tok 
 [position()=1],$ch1/tok[last()],$tok1 
 /@id) and not(local:containsToken 
 ($ch2/tok[position()=1],  
 $ch2/tok[last()],$tok2/@id))) 
return $tok1 
  
Figure 3.6. XQuery for crossing lines 
 
First, the aligned chunks ($ch1 and $ch2) are 
saved into variables. These values are important 
in order to detect the span for each of the chunks 
($ch1/tok[position()=1], $ch1/tok[last()] and 
$ch2/tok[position()=1], $ch2/tok[last()]), and to 
identify the words making up the source chunks 
as well as their German or English equivalents. 
In the second step all words that do not have 
empty links are saved ($tok1 and $tok2). The 
last step filters the crossing lines, i.e. word align-
ments pointing out of the chunk alignment. For 
this purpose, we define a new function (local:-
containsToken) which tests whether a word be-
longs to a chunk or not. By applying local:con-
tainsToken for the German original and not-
(local:containsToken) for the English transla-
tion, all words in the German chunks whose 
aligned English equivalent words do not belong 
to the aligned English chunks are retrieved. The 
example query returns the German words Ihre 
H?nde that are part of the German subject chunk 
and which are aligned with the English words 
her hands that again are part of the second ad-
verbial chunk. 
4 Summary and conclusions 
In a broader view, it can be observed that there is 
an increasing need in richly annotated corpora 
across all branches of linguistics. The same holds 
for linguistically interpreted parallel corpora in 
translation studies. Usually, though, the problem 
with large-scale corpora is that they do not re-
flect the complexity of linguistic knowledge we 
are used to dealing with in linguistic theory. 
Simple research questions can of course be an-
swered on the basis of raw corpora or with the 
help of an automatic part-of-speech tagging. 
Most linguistics and translation scholars are, 
however, interested in more complex questions 
like the interaction of syntax and semantics 
across languages.  
The research described here shows the use of 
comprehensive multi-layer annotation across 
languages. By relating a highly abstract research 
question to multiple layers of lexical and gram-
matical realisations, characteristic patterns of 
groups of texts, e.g. explicitation in translations 
and originals in the case of the CroCo project, 
can be identified on the basis of statistically rele-
vant linguistic evidence. 
If we want to enrich corpora with multiple 
kinds of linguistic information, we need a lin-
guistically motivated model of the linguistic 
units and relations we would like to extract and 
draw conclusions based on an annotated and 
aligned corpus. So the first step for the compila-
tion of a parallel translation corpus is to provide 
a classification of linguistic units and relations 
and their mappings across source and target lan-
guages. The classification of English and Ger-
man linguistic units and relations chosen for the 
CroCo project (i.e. for the investigation of ex-
plicitation in translations and originals) is re-
flected in the CroCo annotation and alignment 
schemes and thus in the CroCo Corpus annota-
tion and alignment.  
From a technical point of view, the representa-
tion of a multilingual resource comprehensively 
41
annotated and aligned is to be realised in such a 
way that 
? multiple linguistic perspectives on the corpus 
are possible since different annotations and 
alignments can be investigated independ-
ently or in combination, 
? the corpus format guarantees best possible 
accessibility and exchangeability, and 
? the exploitation of the corpus is possible us-
ing easily available tools for search and 
analysis. 
We coped with this challenge by introducing a 
multi-layer stand-off corpus representation for-
mat in XML (see section 2), which takes into ac-
count not only the different annotation layers 
needed from a linguistic point of view, but also 
multiple alignment layers necessary to investi-
gate different translation relations.  
We also showed how the CroCo resource can 
be applied to complex research questions in lin-
guistics and translation studies using XQuery to 
retrieve multi-dimensional linguistic information 
(see section 3). Based on the stand-off storage of 
annotation and alignment layers combined with 
the possibility to exploit the required layers 
through intelligent queries, parallel text segments 
and/or parallel annotation units can be extracted 
and compared across languages. 
In order to make the CroCo resource available 
to researchers not familiar with the complexities 
of XML mark-up and the XQuery language, a 
graphical user interface will be implemented in 
Java which allows formulating queries without 
knowledge of the XQuery syntax.  
Acknowledgement 
The authors would like to thank the reviewers for 
their excellent comments and helpful feedback 
on previous versions of this paper.  
The research described here is sponsored by 
the German Research Foundation as project no. 
STE 840/5-1.  
References  
Mona Baker. 1996. Corpus-based translation studies: 
The challenges that lie ahead. In Harold Somers 
(ed.). Terminology, LSP and Translation. Benja-
mins, Amsterdam:175-186. 
Douglas Biber. 1993. Representativeness in Corpus 
Design. Literary and Linguistic Computing 
8/4:243-257. 
Shoshana Blum-Kulka. 1986. Shifts of cohesion and 
coherence in Translation. In Juliane House and 
Shoshana Blum-Kulka (eds.). Interlingual and In-
tercultural Communication. Gunter Narr, T?bin-
gen:17-35. 
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. Proceedings of the Sixth Applied 
Natural Language Processing Conference ANLP-
2000, Seattle, WA.  
Matthias Heyn. 1996. Integrating machine translation 
into translation memory systems. European Asso-
ciation for Machine Translation - Workshop Pro-
ceedings, ISSCO, Geneva:111-123. 
Heinz Dieter Maas. 1998. Multilinguale Textverarbei-
tung mit MPRO. Europ?ische Kommunikationsky-
bernetik heute und morgen '98, Paderborn. 
Christoph M?ller and Michael Strube. 2003. Multi-
Level Annotation in MMAX. Proceedings of the 
4th SIGdial Workshop on Discourse and Dialogue, 
Sapporo, Japan:198-107. 
Stella Neumann and Silvia Hansen-Schirra. 2005. The 
CroCo Project: Cross-linguistic corpora for the in-
vestigation of explicitation in translations. In Pro-
ceedings from the Corpus Linguistics Conference 
Series, Vol. 1, no. 1, ISSN 1747-9398. 
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Journal of Computational Linguistics 
Nr.1, vol. 29:19-51. 
Maeve Olohan and Mona Baker. 2000. Reporting that 
in Translated English. Evidence for Subconscious 
Processes of Explicitation? Across Languages and 
Cultures 1(2):141-158. 
Geoffrey Sampson. 1995. English for the Computer. 
The Susanne Corpus and Analytic Scheme. Claren-
don Press, Oxford. 
Anne Schiller, Simone Teufel and Christine St?ckert. 
1999. Guidelines f?r das Tagging deutscher Text-
korpora mit STTS, University of Stuttgart and Se-
minar f?r Sprachwissenschaft, University of T?-
bingen. 
Erich Steiner. 2005. Explicitation, its lexicogram-
matical realization, and its determining (independ-
ent) variables ? towards an empirical and corpus-
based methodology. SPRIKreports 36:1-43. 
Elke Teich, Silvia Hansen, and Peter Fankhauser. 
2001. Representing and querying multi-layer anno-
tated corpora. Proceedings of the IRCS Workshop 
on Linguistic Databases. Philadelphia: 228-237.  
42
Workshop on Humans and Computer-assisted Translation, pages 47?56,
Gothenburg, Sweden, 26 April 2014.
c
?2014 Association for Computational Linguistics
Beyond Linguistic Equivalence. An Empirical Study of Translation
Evaluation in a Translation Learner Corpus
Mihaela Vela Anne-Kathrin Schumann
Department of Applied Linguistics, Translation and Interpreting
Saarland University, Saarbr?cken, Germany
{m.vela, anne.schumann, a.wurm}@mx.uni-saarland.de
Andrea Wurm
Abstract
The realisation that fully automatic trans-
lation in many settings is still far from
producing output that is equal or superior
to human translation has lead to an in-
tense interest in translation evaluation in
the MT community. However, research in
this field, by now, has not only largely ig-
nored the tremendous amount of relevant
knowledge available in a closely related
discipline, namely translation studies, but
also failed to provide a deeper understand-
ing of the nature of "translation errors" and
"translation quality". This paper presents
an empirical take on the latter concept,
translation quality, by comparing human
and automatic evaluations of learner trans-
lations in the KOPTE corpus. We will
show that translation studies provide so-
phisticated concepts for translation qual-
ity estimation and error annotation. More-
over, by applying well-established MT
evaluation scores, namely BLEU and Me-
teor, to KOPTE learner translations that
were graded by a human expert, we hope
to shed light on properties (and potential
shortcomings) of these scores.
1 Translation quality assessment
In recent years, researchers in the field of MT
evaluation have proposed a large variety of meth-
ods for assessing the quality of automatically pro-
duced translations. Approaches range from fully
automatic quality scoring to efforts aimed at the
development of "human" evaluation scores that try
to exploit the (often tacit) linguistic knowledge of
human evaluators. The criteria according to which
quality is estimated often include adequacy, the
degree of meaning preservation, and fluency, tar-
get language correctness (Callison-Burch et al.,
2007). The goals of both "human" evaluation and
fully automatic quality scoring are manifold and
cover system optimisation as well as benchmark-
ing and comparison.
In translation studies, the scientific (and pre-
scientific) discussion on how to assess the quality
of human translations has been going on for cen-
turies. In recent years, the development of appro-
priate concepts and tools has become even more
vital to the discipline due to the pressing needs
of the language industry. However, different from
the belief, typical to MT, that the "goodness" of a
translation can be scored on the basis of linguistic
criteria alone, the notion of "translation quality",
in translation studies, has assumed a multi-faceted
shape, distancing itself from a simple strive for
equivalence and embracing concepts such as func-
tional, stylistic and pragmatic appropriateness as
well as textual coherence. In this section, we pro-
vide an overview over approaches to translation
quality assessment developed in MT and transla-
tion studies to specify how "quality" is being de-
fined in both fields and which methods and fea-
tures are used. Due to the amount of available
literature, this overview is necessarily incomplete,
but still insightful with respect to differences and
commonalities between MT and human transla-
tion evaluation.
1.1 Automatic MT quality scores
MT output is usually evaluated by automatic
language-independent metrics which can be ap-
plied to any language produced by an MT sys-
tem. The use of automatic metrics for MT eval-
uation is legitimate, since MT systems deal with
large amounts of data, on which manual evaluation
would be very time-consuming and expensive.
Automatic metrics typically compute the close-
ness (adequacy) of a "hypothesis" to a "reference"
translation and differ from each other by how this
closeness is measured. The most popular MT eval-
47
uation metrics are IBM BLEU (Papineni et al.,
2002) and NIST (Doddington, 2002) which are
used not only for tuning MT systems, but also as
evaluation metrics for shared tasks, such as the
Workshop on Statistical Machine Translation (Bo-
jar et al., 2013).
IBM BLEU uses n-gram precision by match-
ing machine translation output against one or more
reference translations. It accounts for adequacy
and fluency by calculating word precision, respec-
tively the n-gram precision. In order to deal with
the over generation of common words, precision
counts are clipped, meaning that a reference word
is exhausted after it is matched. This is then the
modified n-gram precision. For N=4 the modified
n-gram precision is calculated and the results are
combined by using the geometric mean. Instead of
recall, the brevity penalty (BP) is used. It penal-
izes candidate translations which are shorter than
the reference translations.
The NIST metric is derived from IBM BLEU.
The NIST score is the arithmetic mean of modi-
fied n-gram precision for N=5 scaled by BP. Addi-
tionally, NIST also considers the information gain
of each n-gram, giving more weight to more infor-
mative (less frequent) n-grams and less weight to
less informative (more frequent) n-grams.
Another often used machine translation eval-
uation metric is Meteor (Denkowski and Lavie,
2011). Different from IBM BLEU and NIST, Me-
teor evaluates a candidate translation by calcu-
lating precision and recall on the unigram level
and combining them into a parametrized harmonic
mean. The result from the harmonic mean is then
scaled by a fragmentation penalty which penalizes
gaps and differences in word order.
Besides these evaluation metrics, several other
metrics are sometimes used for the evaluation
of MT output. Some of these are the WER
(word error-rate) metric based on the Levens-
thein distance (Levenshtein, 1966), the position-
independent error rate metric PER (Tillmann et
al., 1997) and the translation edit rate metric
TER (Snover et al., 2006) with its newer version
TERp (Snover et al., 2009).
1.2 Human MT quality evaluation
Human evaluation of MT output is performed in
different ways. The most frequently used evalua-
tion method seems to be a simple ranking of trans-
lated sentences by a "reasonable number of eval-
uators" (Farr?s et al., 2010). According to Birch
et al. (2013), this form of evaluation was used,
among others, during the last STATMT workshops
and can thus be considered rather popular. AP-
PRAISE (Federmann, 2012) is a tool that can be
used for such as task, since it allows for the man-
ual ranking of sentences, quality estimation, error
annotation and post-editing.
Other forms of evaluation, however, exist. For
example, Birch et al. (2013) propose HMEANT,
an evaluation score based on MEANT (Lo and
Wu, 2011), a semi-automatic MT quality score
that measures the degree of meaning preservation
by comparing verb frames and semantic roles of
hypothesis translations to their respective coun-
terparts in the reference translation(s). Unfor-
tunately, Birch et al. (2013) report difficulty in
producing coherent role alignments between hy-
potheses and translations, a problem that affects
the final HMEANT score calculation. This, how-
ever, seems hardly surprising given the difficulty
of the annotation task (although, following the au-
thors? description, some familiarity of the anno-
tators with the linguistic key concepts can be as-
sumed) and the fact that guidelines and training
are meant to be minimal.
Another (indirect) human evaluation method for
MT that is also employed for error analysis are
reading comprehension tests (e.g. Maney et al.
(2012), Weiss and Ahrenberg (2012)). More-
over, HTER (Snover et al., 2006) is a TER-based
repair-oriented metric which uses human annota-
tors (the only apparent qualificational requirement
being fluency in the target language) to generate
"targeted" reference translations by post-editing
the MT output or the existing reference trans-
lations, following the goal to find the shortest
path between the hypothesis and a "correct" refer-
ence. Snover et al. (2006) report a high correlation
between evaluation with HTER and traditional hu-
man adequacy and fluency judgements. Last but
not least, Somers (2011) mentions other repair-
oriented measures such as post-editing effort mea-
sured by the amount of key-strokes or time spent
on producing a "correct" translation on the basis
of MT output.
1.3 The notion of quality in translation
studies
Discussions of translation "quality", in translation
studies, for a long time focused on equivalence
48
which, in its oldest and simplest form, used to
echo adequacy as understood by today?s MT re-
searchers: "good" translation was viewed as an
optimal compromise between meaning preserva-
tion and target language correctness, which was
especially relevant to the translation of religious
texts. For example, Ku?maul (2000) emphatically
cites Martin Luther?s famous Bible translation into
German as an example of "good" translation be-
cause Luther, according to his own testimony and
following his reformative ambition, focused on
producing fluent, easily understandable text rather
than mimicking the linguistic structures of the He-
brew, Aramaic and Greek originals (see also Win-
dle and Pym (2011) for a further discussion).
More recent work in translation studies has
abandoned one-dimensional views of the relation
between source and target text and postulates that,
depending on the communicative context within
and for which a translation is produced, this re-
lation can vary greatly. That is, the degree of lin-
guistic or semantic "fidelity" of a good translation
towards the source text depends on functional cri-
teria. This view is echoed in the concepts of "pri-
mary vs. secondary", "documentary vs. instru-
mental" and "covert vs. overt" translation (H?nig,
2003). The consequence of this shift in paradigms
is that, since different translation strategies may
be appropriately adopted in different situations,
evaluation criteria become essentially dependent
on the function that the translation is going to play
in the target language and culture. This view is
most prominently advocated by the so-called sko-
pos theory (cf. Dizdar (2003)). Translation errors,
then, are not just simple violations of the target
language system or outright failures to translate
words or segments, but violations of the transla-
tion task that can manifest themselves on all levels
of text production (Nord, 2003). It is important
to point out that, in this framework, linguistic er-
rors are just one type of error covering not only
one of the favourite MT error categories, namely
un- and mistranslated words (compare, for ex-
ample, Stymne and Ahrenberg (2012), Weiss and
Ahrenberg (2012), Popovi
?
c et al. (2013)), but also
phraseological, idiomatic, syntactic, grammatical,
modal, temporal, stylistic, cohesion and other
kinds of errors. Moreover, translation-specific er-
rors occur when the translation does not fulfill its
function because of pragmatic (e.g. text-type spe-
cific forms of address), cultural (e.g. text con-
ventions, proper names, or other conventions) or
formal (e. g. layout) defects (Nord, 2003). De-
pending on the appropriate translation strategy for
a given translation task, these error types may be
weighted differently. Furthermore, the commu-
nicative and functional view on translation also
dictates a change in the concept of equivalence
which is no longer considered to be adequately
described by the notions of "meaning preserva-
tion" or "fidelity", but becomes dependent on aes-
thetic, connotational, textual, communicative, sit-
uational, functional and cognitive aspects (for a
detailed discussion see Horn-Helf (1999)). In MT
evaluation, most of these aspects have not yet or
only in part been considered.
Last but not least, the translation industry has
developed normative standards and proofreading
schemes. For example, the DIN EN 15038:2006-
08 (Deutsches Institut f?r Normung, 2006) dis-
cusses translation errors, quality management and
qualificational requirements for translators and
proofreaders, while the SAE J2450 standard (So-
ciety of Automotive Engineers, 2005) presents a
weighted "translation quality metric". An appli-
cation perspective is given by Mertin (2006) who
discusses translation quality management proce-
dures in a big automotive company and, among
other things, develops a weighted translation error
scheme for proofreading.
1.4 Discussion
The above discussion shows that, while the object
of evaluation is the same for both MT and trans-
lation studies, namely translation, the differences
between evaluation approaches developed in both
fields are considerable. Most importantly, in trans-
lation studies, translation evaluation is considered
an expert task for which fluency in one or several
languages is certainly not enough, but for which
translation-specific expert knowledge is required.
Another important distinction is that evaluation,
again in translation studies, is normally not car-
ried out on the sentence level, since sentences are
usually split up into several "units of translation"
and can certainly contain more than one "trans-
lation problem". Consequently, the popular MT
practice of ranking whole sentences according to
some automatic score, by anonymous evaluators
or even users of Amazon Turk (e.g. in the intro-
duction to Bojar et al. (2013)), from a translation
studies point of view, is unlikely to provide reason-
49
able evaluations. Last but not least, the MT com-
munity?s strive for adequacy or meaning preser-
vation does not match the notions of weighting
translation errors, of adopting different translation
strategies and, consequently, does not fit the com-
plicated source/target text relations that have been
acknowledged by translation studies. Evaluation
methods that are based on simple measures of lin-
guistic equality such as n-gram overlap (BLEU)
or, just slightly more complicated, the preservation
of syntactic frames and semantic roles (MEANT)
fail to provide straightforward criteria for distin-
guishing between legitimate and illegitimate vari-
ation. Moreover, semantic and pragmatic criteria
as well as the notion of "reference translation" re-
main, at best, rather unclear.
On the other hand, the MT community has
recognised translation evaluation as an unresolved
research problem. For example, Birch et al. (2013)
state that ranking judgements are difficult to gen-
eralise, while Callison-Burch et al. (2007) carry
out extensive correlation tests of a whole range
of automatic MT evaluation metrics in compar-
ison to human judgements, showing that BLEU
does not rank highest, but still remains in the top
segment. It still needs to be shown how MT re-
search can benefit from more sophisticated evalu-
ation measures and whether all the parameters that
are considered relevant to the evaluation of human
translations are relevant for MT usage scenarios,
too. In the remainder of this paper, we present a
study on how much and possibly for which reasons
automatic MT evaluation scores (namely BLEU
and Meteor) differ from translation expert quality
judgements on extracts of a French-German trans-
lation learner corpus.
2 The KOPTE corpus
2.1 General corpus design
The KOPTE project (Wurm, 2013) was designed
to enable research on translation evaluation in
a university training course (master?s level) for
translators and to enlighten students? translation
problems as well as their problem solving strate-
gies. To achieve this goal, a corpus of student
translations was compiled. The corpus consists of
several translations of the same source texts pro-
duced by student translators in a classroom set-
ting. As a whole, it covers 985 translations of
77 source texts amounting to a total of 318,467
tokens. Source texts were taken from French
newspapers and translated into German in class
over a span of several years, the translation brief
calling for a ready-to-publish text to be printed
in a German national newspaper. Consequently,
all translation tasks include the use of idiomatic
language, explanations of culture-specific items,
changes in the explicitness of macrotextual cohe-
sive elements, etc.
1
2.2 Annotation of translation features and
translation evaluation in KOPTE
Student translations were evaluated by one of the
authors, an experienced translation teacher, with
the aim of giving feedback to students. All trans-
lations were graded and errors as well as good
solutions were marked in the text according to a
fine-grained evaluation scheme. In this scheme,
the weight of evaluated items is indicated through
numbers ranging from plus/minus 1 (minor) to
plus/minus 8 (major). Based on these evaluations,
each translation was assigned a final grade accord-
ing to the German grading system on a scale rang-
ing from 1 ("very good") to 6 ("highly erroneous")
with in-between intervals at the levels of .0, .3 and
.7. To calculate this grade, positive and negative
evaluations were summed up separately, before the
negative score was subtracted from the positive
one. A score of around zero corresponds to the
grade "good" (=2), to achieve "very good" (=1) the
student needs a surplus of positive evaluations.
The evaluation scheme based on which student
translations are graded is divided into external
and internal factors. External characteristics de-
scribe the communicative situation given by the
source text and the translation brief (author, re-
cipient, medium, location, time). Internal fac-
tors, on the other hand, comprise eight categories:
form, structure, cohesion, stylistics/register, gram-
mar, lexis/semantics, translation-specific prob-
lems, function. These categories are containers for
more fine-grained criteria which can be applied to
segments of the (source or target) text or even to
the whole text, depending on the nature of the cri-
terion. Some internal subcriteria of the scheme are
summarised in Table 1. A quantitative analysis of
error types in KOPTE shows that semantic/lexical
errors are by far the most common error in the stu-
dent translations (Wurm, 2013).
Evaluations in KOPTE were carried out by just
1
More information about KOPTE is available from
http://fr46.uni-saarland.de/index.php?id=3702&L=%2524L.
50
one evaluator for the reason that, in a classroom
setting, multiple evaluations are not feasible. Al-
though multiple evaluations would have been con-
sidered highly valuable, the data available from
KOPTE was evaluated by an experienced trans-
lation scholar with long-standing experience in
teaching translation. Moreover, the evaluation
scheme is much more detailed than error annota-
tion schemes that are normally described in the lit-
erature and it is theoretically well-motivated. An
analysis of the median grades in our data sample
(compare Tables 2?4) shows that grading varies
only slightly between different texts, considering
the maximum variation potential ranging from 1
to 6, and thus can be considered consistent.
Criteria Examples of
subcriteria
author, recipients,
medium, topic, ?
location, time
form paragraphs, formatting
structure thematic, progression,
macrostructure, illustrations
cohesion reference, connections
stylistics style, genre
grammar determiners, modality, syntax
semantics textual semantics, idioms,
numbers, terminology
translation erroneous source
problems text, proper names, culture-specific
items, ideology, math. units,
pragmatics, allusions
function goal dependence
Table 1: Internal evaluation criteria in the KOPTE annotation
scheme.
3 Experiments
The goal of our experiments was to study
whether the human translation expert judgements
in KOPTE can be mimicked using simple au-
tomatic quality metrics as used in MT, namely
BLEU and Meteor. More specifically, we aim at:
? studying how automatic evaluation scores re-
late to fine-grained human expert evaluations,
? investigating whether a higher number of ref-
erences improves the automatic scores and
why (or why not),
? examining whether a higher number of ref-
erences provides more reliable evaluation
scores as measured by an improved correla-
tion with the human expert judgments.
In order to study the behaviour of automatic MT
evaluation scores, we conducted three experiments
by applying IBM BLEU (Papineni et al., 2002)
and Meteor 1.4 (Denkowski and Lavie, 2011) to
a sample of KOPTE translations that were pro-
duced by translation students preparing for their
final master?s exams. Scores were calculated on
the complete texts. To evaluate the overall perfor-
mance of the automatic evaluation scores on these
texts, we calculated Kendall?s rank correlation co-
efficient for each text following the procedure de-
scribed in Sachs and Hedderich (2009). Correla-
tions were calculated for:
? the human expert grades and BLEU scores
for each translation,
? the human expert grades and Meteor scores
for each translation,
? BLEU and Meteor scores for each transla-
tion.
3.1 Experimental setup and results
In a first experiment, we applied the automatic
evaluation scores to the source texts given in Ta-
ble 2, choosing, for each text, the student transla-
tion with the best human grade as reference trans-
lation. The median human grades as well as mean
BLEU and Meteor and correlation scores obtained
for each text (excluding the reference translation)
are included in Table 2. In a second experiment,
we repeated this procedure, however, using a set
of three reference translations. Results are given
in Table 3. Finally, in a last experiment we used
five reference translations selected according to
their human expert grade (Table 4). In both steps,
source texts for which less than four hypotheses
were available were excluded from the data sets.
3.2 Discussion
The tables show that in the first experiment a set of
152 translations was evaluated, whereas in the sec-
ond and third experiment these numbers were re-
duced to 108 and 68 respectively due to the selec-
tion of more references. The human expert eval-
uations rated most of these translations at least as
acceptable, as can be seen from the median grade
for each experiment which was 2.3 in the first ex-
periment and consecutively decreased to 3.0 for
the third experiment, again due to the selection
of more "good" translations as references. The
51
Source Human trans./ Median Mean Mean Correlation Correlation Correlation
text source text grades BLEU Meteor Human-BLEU Human-Meteor BLEU-Meteor
AT001 7 2. 7 0. 15 0. 33 ?0. 39 ?0. 73 0. 24
AT002 12 2. 3 0. 15 0. 35 ?0. 20 ?0. 43 0. 49
AT004 12 2. 7 0. 19 0. 37 0. 14 0. 11 0. 63
AT005 12 2. 3 0. 20 0. 36 0. 32 0. 45 0. 45
AT008 10 2. 15 0. 23 0. 38 ?0. 43 ?0. 29 0. 78
AT010 11 2. 7 0. 25 0. 41 0. 06 ?0. 10 0. 56
AT012 9 2. 0 0. 22 0. 40 ?0. 30 ?0. 36 0. 50
AT015 5 2. 0 0. 11 0. 28 0. 36 0. 12 0. 60
AT017 7 2. 3 0. 22 0. 38 ?0. 20 0. 06 0. 71
AT021 4 3. 0 0. 18 0. 39 ?0. 55 ?0. 55 1. 00
AT023 6 2. 3 0. 22 0. 38 0. 50 ?0. 07 ?0. 20
AT025 4 2. 15 0. 13 0. 36 0. 33 0. 0 0. 00
AT026 21 3. 0 0. 12 0. 26 ?0. 19 ?0. 35 0. 67
AT039 13 3. 0 0. 10 0. 29 ?0. 08 0. 03 0. 49
AT052 7 2. 0 0. 17 0. 31 ?0. 32 0. 05 0. 00
AT053 7 2. 3 0. 18 0. 32 0. 62 0. 39 0. 33
AT059 5 2. 0 0. 24 0. 36 0. 00 0. 22 0. 80
Table 2: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the
BLEU and Meteor scores per source text and Kendall?s rank correlation coefficients for the first experiment.
Source Human trans./ Median Mean Mean Correlation Correlation Correlation
text source text grades BLEU Meteor Human?BLEU Human-Meteor BLEU-Meteor
AT001 5 3. 0 0. 17 0. 36 ?0. 12 0. 36 0. 60
AT002 10 2. 3 0. 17 0. 36 ?0. 14 0. 05 0. 38
AT004 10 2. 85 0. 20 0. 37 0. 39 0. 16 0. 51
AT005 10 2. 3 0. 20 0. 40 ?0. 10 0. 05 0. 47
AT008 8 2. 5 0. 25 0. 45 ?0. 67 ?0. 15 0. 00
AT010 9 2. 7 0. 23 0. 41 ?0. 10 ?0. 50 0. 28
AT012 7 2. 3 0. 23 0. 43 0. 00 0. 11 0. 52
AT017 5 2. 3 0. 21 0. 43 0. 12 0. 36 0. 60
AT023 4 2. 5 0. 21 0. 38 0. 41 0. 81 0. 67
AT026 19 3. 3 0. 10 0. 26 ?0. 31 ?0. 41 0. 77
AT039 11 3. 0 0. 11 0. 34 0. 06 0. 14 0. 74
AT052 5 2. 0 0. 18 0. 40 0. 12 0. 36 0. 20
AT053 5 2. 3 0. 17 0. 35 0. 36 ?0. 12 0. 40
Table 3: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the
BLEU and Meteor scores per source text and Kendall?s rank correlation coefficients for the second experiment.
grades for the best translations selected as refer-
ences range for the first and second experiment
between 1.0 and 2.3, whereas for the third exper-
iment the selected references were evaluated with
grades between 1.0 and 2.7. Nevertheless, the me-
dian grade for the references in all three exper-
iments is always 1.7. From the overall median
grade and the median grade of the selected trans-
lations as reference we can notice, that the trans-
lations selected as references were indeed "better"
than the remaining ones.
The BLEU and Meteor scores given in the ta-
bles are mean values over the individual transla-
tions? scores for each source text. These scores
are very low, reaching a maximum of 0.25 over
all three experiments for BLEU and 0.45 for Me-
teor. However, given the human expert grades
the translations cannot be considered unreadable.
In fact, the correlation coefficients show that nei-
ther BLEU nor Meteor (except a few exceptional
cases) correlate with the human quality judge-
ments, however, they show a (weak) tendency to
correlate with each other. Moreover, the data
shows that the addition of reference translations
results neither in significantly higher BLEU or
Meteor scores nor in improved correlation.
3.3 Qualitative analysis
Our finding that human quality judgements do not
correlate with automatic scores if the object of
evaluation is a translation produced by a human
(as opposed to a machine) matches earlier results
presented by Doddington (2002) within the con-
text of evaluating NIST. Doddington (2002) pro-
poses the explanation that "differences between
professional translators are far more subtle [than
differences between machine-produced transla-
tions, the authors] and thus less well characterized
52
Source Human trans./ Median Mean Mean Correlation Correlation Correlation
text source text grades BLEU Meteor Human-BLEU Human-Meteor BLEU-Meteor
AT002 8 2. 5 0. 17 0. 36 ?0. 08 0. 00 0. 43
AT004 8 3. 0 0. 20 0. 36 0. 00 0. 23 0. 71
AT005 8 2. 3 0. 20 0. 42 0. 00 0. 08 0. 43
AT008 6 2. 85 0. 26 0. 45 ?0. 55 ?0. 14 0. 33
AT010 7 2. 7 0. 23 0. 41 0. 00 ?0. 12 0. 05
AT012 5 2. 3 0. 23 0. 43 0. 22 0. 22 0. 40
AT026 17 3. 3 0. 11 0. 31 ?0. 24 ?0. 34 0. 62
AT039 9 3. 0 0. 10 0. 37 0. 22 0. 55 0. 22
Table 4: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the
BLEU and Meteor scores per source text and Kendall?s rank correlation coefficients for the third experiment.
by N-gram statistics." We conducted a qualitative
analysis of some KOPTE translations in order to
check whether the differences between individual
translations are indeed as subtle as suggested by
Doddington and to come up at least with hypothe-
ses that could explain the poor performance of the
automatic scores. We selected three source texts
used in the second experiment, namely AT008,
AT023 and AT053 and compared their respective
reference translations to selected hypothesis trans-
lations. This analysis was conducted on the lex-
ical level alone, that is, most of the features of
KOPTE?s elaborated evaluation scheme were not
even considered. The analysis, however, shows
that the amount of variation that can be found just
on the lexical level is almost overwhelming. Some
examples are listed in Appendix A.
A common phenomenon is simple variation due
to synonyms or the use of phrasal variants or
paraphrases. Moreover, the listed examples show
that lexical variation can be triggered by differ-
ent source text elements. The phenomena shown
in the tables are well-known translation prob-
lems, e.g. proper names, colloquial or figurative
speech or numbers. The other categories in the
table are less clear-cut, that is, they can overlap.
In our analysis, source text elements that cannot
be translated literally, but instead call for a cre-
ative solution were classified as translation prob-
lems. Different translation strategies can be ap-
plied to different kinds of problems, most impor-
tantly to the translation of culture-specific items,
proper names, underspecified source text elements
or culture-specific arguments. The respective table
and other examples that we analysed show that for
this category some translators chose to add addi-
tional information, to adapt the perspective to the
German target audience (for example, by adapt-
ing pronouns or deictic elements) or to adapt the
formatting choices to the variant preferred by the
target culture (e.g. commas instead of fullstops,
different types of quotation marks), whereas other
translators chose to translate literally. Both strate-
gies are legitimate under certain circumstances,
however, it can be assumed that adaptations re-
quire a greater cognitive effort. Source ambigu-
ities, according to our preliminary typology, are
source text features that can be interpreted in dif-
ferent ways - at least for a translator translating
from a foreign language (as opposed to a native
speaker). Obviously, the line between this cat-
egory and outright translation errors is not very
clear.
However, it needs to be stated that also for the
other categories - while many variants are correct
and legitimate - not all are equally good. Best
solutions for given problems are distributed un-
equally across the translations studied. Beyond
the purely lexical level, extensive variation can be
witnessed on the syntactic, but also the grammat-
ical level. For example, some translators chose to
break the rather complicated syntax of the French
original into simpler, easily readable sentences,
producing, in some cases, considerable shifts in
the information structure of the text - often a legit-
imate strategy.
With respect to the performance of the auto-
matic scores, our preliminary study - that still calls
for larger-scale and in-depth verification - suggests
that neither BLEU nor Meteor are able to cope
with the amount of variation found in the data.
More specifically, they cannot distinguish between
legitimate and illegitimate variation or grave and
slight errors respectively, but seem to fail to match
acceptable variants because of lexical and phrasal
variation or divergent grammatical structures re-
sulting in different verb frames, word sequences
and text lengths, not to talk even about acceptable
variation on higher linguistic levels. Therefore,
automatic scores seem to overrate surface differ-
53
ences and thus assign very low scores to many
translations that were found to be at least accept-
able by a human expert.
Considering the impact of these findings for MT
evaluation purposes, it is not straightforward to as-
sume that the differences that we have observed
between the human translations are more "subtle"
(in the sense of being unimportant) than the ones
produced by machine translation systems. On the
contrary, our analysis suggests that "good" trans-
lations are characterised by creative solutions that
are not easily reproducible but that help to achieve
target language readability and comprehensibility.
This is a fundamental quality aspect of translation
independently of its production mode. Moreover,
it is difficult to see why some of the variants that
we observed in the human translations selected
from KOPTE, once the context shifts from human
to machine translation, should be found valid in
one situation and invalid in another, depending on
the training and test data used for developing an
MT system: A high amount of the variation found
in the human translations goes back to the legiti-
mate use of the creative and constructive powers
of natural language, and it is, among others, these
powers that should be mimicked by MT output.
4 Conclusion and future work
In this paper, we have studied the performance
of two fully automatic MT evaluation metrics,
namely BLEU and Meteor, in comparison to hu-
man translation expert evaluations on a sample of
learner translations from the KOPTE corpus. The
automatic scores were tested in three experiments
with a varying number of reference translations
and their performance was compared to the hu-
man evaluations by means of Kendall?s rank cor-
relation coefficient. The experiments suggest that
both BLEU and Meteor systematically underesti-
mate the quality of the translations tested, that is,
they assign scores that, given the human expert
evaluations, seem to be by far too low. Moreover,
they do not consistently correlate with the human
expert evaluations. Coming up with explanations
for this failure is not straightforward, however, the
results of our qualitative and explorative analysis
suggest that lexical similarity scores are not able
to cope satisfactorily neither with standard lexical
variation (paraphrases etc.) nor with dissimilari-
ties that can be traced back to the specific nature
of the translation process, leave alone linguistic
levels beyond the lexicon. For Meteor, this short-
coming may partly be alleviated by the provision
of richer sets of synonyms and paraphrases, how-
ever, the amount of uncovered variation is still im-
mense. In fact, it seems that many more reference
translations would be needed in order to cover the
whole range of legitimate variants that can be used
to translate a given source text - a scenario that
seems hardly feasible! So how can BLEU or Me-
teor scores be interpreted when they are given in
MT papers? Based on our analyses, it seems clear
that these scores are based on a data-driven no-
tion of translation quality, that is, they measure
the degree of compliance of a hypothesis transla-
tion with some reference set. This is insofar prob-
lematic as studies based on different reference sets
cannot be compared, neither can BLEU or Me-
teor scores be generalised to other domains. Even
more importantly, BLEU or Meteor scores cannot
be used to measure a data-independent concept of
quality or even the usability of a translation for
a target audience which, as we have shown, de-
pends on many more factors than just lexical sur-
face overlap.
However, our study also leads to some open
research questions. One of these questions is
whether automatic evaluation scores can still be
used for more coarse-grained distinctions, that is,
to distinguish "really bad" translations from "re-
ally good" ones. The fine-grained distinctions
made by the evaluator of KOPTE on generally
rather good translations do not allow us to answer
this question. Future work will also deal with a
comparison of mistakes made by MT systems as
opposed to human translators as well as with the
question how (and which) translation-specific as-
pects can be applied to the evaluation of MT sys-
tems.
References
Alexandra Birch, Barry Haddow, Ulrich Germann,
Maria Nadejde, Christian Buck, and Philipp Koehn.
2013. The feasibility of HMEANT as a human MT
evaluation metric. In Proceedings of the 8th Work-
shop on SMT, pages 52?61.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Barry Haddow, Philipp Koehn, Christof Monz, Matt
Post, Herv? Saint-Amand, Radu Soricut, and Lucia
Specia, editors. 2013. Proceedings of the 8th Work-
shop on SMT. ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
54
(Meta-) evaluation of machine translation. In Pro-
ceedings of the 2nd Workshop on SMT, pages 136?
158.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the 6th Workshop on SMT, pages 85?91.
Deutsches Institut f?r Normung. 2006. DIN
EN 15038:2006-08: ?bersetzungsdienstleistungen-
Dienstleistungsanforderungen. Beuth.
Dilek Dizdar. 2003. Skopostheorie. In Handbuch
Translation, pages 104?107. Stauffenburg.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the 2nd In-
ternational Conference on HLT, pages 138?145.
Mireia Farr?s, Marta R. Costa-Juss?, Jos? B. Mar-
i?o, and Jos? A. R. Fonollosa. 2010. Linguistic-
based evaluation criteria to identify statistical ma-
chine translation errors. In Proceedings of the 14th
Annual Conference of the EAMT, pages 167?173.
Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. PBML, 98:25?35, 9.
Hans H?nig. 2003. Human?bersetzung (therapeutisch
vs. diagnostisch). In Handbuch Translation, pages
378?381. Stauffenburg.
Brigitte Horn-Helf. 1999. Technisches ?bersetzen in
Theorie und Praxis. Franke.
Paul Ku?maul. 2000. Kreatives ?bersetzen. Stauffen-
burg.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions and rever-
sals. Soviet Physics Doklady, 10(8):707?710.
Chi-Kiu Lo and Dekai Wu. 2011. MEANT: An
inexpensive, high-accuracy, semi-automatic metric
for evaluating translation utility based on semantic
roles. In Proceedings of the 49th Annual Meeting of
the ACL, pages 220?229.
Tucker Maney, Linda Sibert, Dennis Perzanowski,
Kalyan Gupta, and Astrid Schmidt-Nielsen. 2012.
Toward determining the comprehensibility of ma-
chine translations. In Proceedings of the 1st PITR,
pages 1?7.
Elvira Mertin. 2006. Prozessorientiertes Qualit?ts-
management im Dienstleistungsbereich ?bersetzen.
Peter Lang.
Christiane Nord. 2003. Transparenz der Korrektur.
In Handbuch Translation, pages 384?387. Stauffen-
burg.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the ACL, pages 311?318.
Maja Popovi?c, Eleftherios Avramidis, Aljoscha Bur-
chardt, Sabine Hunsicker, Sven Schmeier, Cindy
Tscherwinka, David Vilar, and Hans Uszkoreit.
2013. Learning from human judgements of machine
translation output. In MT Summit, pages 231?238.
Lothar Sachs and J?rgen Hedderich. 2009. Ange-
wandte Statistik. Methodensammlung mit R.
Springer.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the 4th Work-
shop on SMT, pages 259?268.
Society of Automotive Engineers. 2005. SAE
J2450:2005-08: Translation Quality Metric. SAE.
Harold Somers. 2011. Machine translation: History,
development, and limitations. In The Oxford Hand-
book of Translation Studies, pages 427?440. Oxford
University Press.
Sara Stymne and Lars Ahrenberg. 2012. On the prac-
tice of error analysis for machine translation evalua-
tion. In Proceedings of the 8th LREC, pages 1785?
1790.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alexander Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated DP based search for statistical translation.
In Proceedings of the EUROSPEECH, pages 2667?
2670.
Sandra Weiss and Lars Ahrenberg. 2012. Error pro-
filing for evaluation of machine-translated text: a
polish-english case study. In Proceedings of the
Eighth LREC, pages 1764?1770.
Kevin Windle and Anthony Pym. 2011. European
thinking on secular translation. In The Oxford
Handbook of Translation Studies, pages 7?22. Ox-
ford University Press.
Andrea Wurm. 2013. Eigennamen und Re-
alia in einem Korpus studentischer ?bersetzungen
(KOPTE). trans-kom, 6(2):381?419.
55
A Examples of lexical variation in human translation
In the examples below, bold face indicates the French source.
A.1 Proper names
pr?sident gabonais
Pr?sidenten von Gabon
Pr?sidenten Gabuns
Pr?sidenten von Gabun
Pr?sident des afrikanischen Landes Gabon
gabunesischen Pr?sidenten
la Commission nationale de l?informatique et des libert?s (CNIL)
Commission nationale de l?informatique et des libert?s (CNIL)
franz?sische Datenschutzbeh?rde (CNIL)
franz?sische Datenschutzkommission CNIL
franz?sische Datenschutzbeh?rde CNIL
franz?sische Kommission f?r Datenschutz (CNIL)
A.2 Problematic source text elements (translation problems)
pivot de l?influence fran?aise
St?tzpunkt des Einflusses Frankreichs
zentralen Figur des franz?sischen Einfluss
St?tze f?r den Einfluss Frankreichs
Schl?sselfigur f?r den Einfluss Frankreichs
Garant f?r den franz?sischen Einflu?
"doyen de l?Afrique"
obersten W?rdentr?gers Afrikas
"Alten Herrn von Afrika"
"Abtes von Afrika"
"?ltesten von Afrika"
"doyen de l?Afrique"
A.3 Paraphrases
sera-t-elle capable
es schaffen
f?hig sein
in der Lage sein
sich als f?hig erweisen
se tenir ? la bonne distance
auf angemessener Distanz zu bleiben
sich nicht einzumischen
sich herauszuhalten
die geb?hrende Neutralit?t zu wahren
A.4 Culture-specific elements and underspecified source text items
la "Fran?afrique"
"Fran?afrique"
Franz?sisch-Afrika ("Fran?afrique")
?Franzafrika?
"Frankafrika"
"Fran?afrique" d.h. der franz?sisch beeinflussten Gebiete Afrikas
les "voitures Google", ?quip?es de cam?ras ? 360 degr?s
mit 360-Grad-Kameras ausgestatteten "Google-Kamerawagen"
Kamera-Autos
Street-View-Wagen mit ihren 360?-Kameras
"Google-Autos", die auf dem Dach eine 360-Grad-Kamera montiert haben,
mit 360-Grad-Kameras ausgestatteten "Street View-Autos"
A.5 Source text ambiguities (syntactic and semantic)
la France a soutenu un r?gime autoritaire et pr?dateur, sans piti? pour les opposants
autorit?ren Systems [...], das kein Mitleid mit seinen Gegnern zeigte
hat Frankreich ohne R?cksicht auf Regimekritiker ein autorit?res Gewaltregime unterst?tzt
autorit?re und ausbeutende Regime [...], welches keine Gnade f?r seine Gegner kannte
autorit?res und angriffslustiges Regime [...], das kein Mitleid mit seinen Gegnern hatte
hat Frankreich dieses autorit?re und ausbeuterische System, ohne Mitleid mit dessen Gegnern, gest?tzt
justes paroles
hat die Wahrheit gesagt
hat [...] die richtigen Worte gefunden
hat die richtigen Worte gefunden
Aussage [...] war nichts als Worte
hat genau das Richtige gesagt
A.6 Numbers
une amende de 100 000 euros
Geldstrafe in H?he von 100 000 Euro
Strafe von 100 000C
Geldstrafe von 100.000,- EUR
Geldstrafe in H?he von 100.000 Euro
Bu?geld in H?he von 100 000C
photographe Yann Arthus-Bertrand, 63 ans
63j?hrigen Fotografen Yann Arthus-Betrand
Fotographen Yann Arthus-Bertrand (63 Jahre)
Fotografen Yann Arthus-Bertrand (63)
63-j?hrigen Fotografen Y.A.B.
Fotografen Yann Arthus-Bertrand, 63
A.7 Colloquial or figurative speech
Je vais vite
Ich beeile mich
Ich mache es schnell
Ich bewege mich schnell
Ich hab?s eilig
Ich beeile mich
r?sultats des petits fr?res
Einnahmen der Vorg?nger
Verdienste zus?tzlicher kleiner Artikel
Einnahmen durch andere Produkte
Erl?se von Merchandising
Einnahmen aus dem Merchandising
A.8 Source text element triggering correct and incorrect translations
65 cha?nes de t?l?vision, dont France 2 et 23 cha?nes en Afrique
65 Fernsehsendern, darunter auch France 2 und 23 afrikanische Sender
65 Fernsehsendern, unter anderem France 2 und 23 Sender in Afrika
65 Fernsehsender, darunter der franz?sische Sender France 2 und 23 afrikanische Sender
65 Fernsehkan?len, u.a. 2 in Frankreich und 23 in Afrika
65 Fernsehkan?len, darunter France 2 und 23 afrikanische Sender
56
Workshop on Humans and Computer-assisted Translation, pages 93?98,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Quantifying the Influence of MT Output in the Translators? Performance:
A Case Study in Technical Translation
Marcos Zampieri
Saarland University
Saarbr?ucken, Germany
mzampier@uni-koeln.de
Mihaela Vela
Saarland University
Saarbr?ucken, Germany
m.vela@mx.uni-saarland.de
Abstract
This paper presents experiments on the use
of machine translation output for technical
translation. MT output was used to pro-
duced translation memories that were used
with a commercial CAT tool. Our exper-
iments investigate the impact of the use
of different translation memories contain-
ing MT output in translations? quality and
speed compared to the same task without
the use of translation memory. We evalu-
ated the performance of 15 novice transla-
tors translating technical English texts into
German. Results suggest that translators
are on average over 28% faster when us-
ing TM.
1 Introduction
Professional translators use a number of tools to
increase the consistency, quality and speed of their
work. Some of these tools include spell checkers,
text processing software, terminological databases
and others. Among all tools used by professional
translators the most important of them nowadays
are translation memory (TM) software. TM soft-
ware use parallel corpora of previously translated
examples to serve as models for new transla-
tions. Translators then validate or correct previ-
ously translated segments and translate new ones
increasing the size of the memory after each new
translated segment.
One of the great issues in working with TMs is
to produce the TM itself. This can be time con-
suming and the memory should ideally contain a
good amount of translated segments to be consid-
ered useful and accurate. For this reason, many
novice translators do not see the benefits of the
use of TM right at the beginning, although it is
consensual that on the long run the use of TMs in-
crease the quality and speed of their work. To cope
with this limitation, more TM software have pro-
vided interface to machine translation (MT) soft-
ware. MT output can be used to suggest new seg-
ments that were not previously translated by a hu-
man translator but generated automatically from
an MT software. But how helpful are these trans-
lations?
To answer this question, the experiments pro-
posed in this paper focus on the translator?s per-
formance when using TMs produced by MT out-
put within a commercial CAT tool interface. We
evaluate the quality of the translation output as
well as the time and effort taken to accomplish
each task. The impact of MT and TM in trans-
lators? performance has been explored and quan-
tified in different settings (Bowker, 2005; Guer-
berof, 2009; Guerberof, 2012; Morado Vazquez
et al., 2013). We believe this paper constitutes
another interesting contribution to the interface
between the study of the performance of human
translators, CAT tools and machine translation.
2 Related Work
CAT tools have become very popular in the last
20 years. They are used by freelance transla-
tors as well as by companies and language ser-
vice providers to increase translation?s quality and
speed (Somers and Diaz, 2004; Lagoudaki, 2008).
The use of CAT tools is part of the core curricu-
lum of most translation studies degrees and a rea-
sonable level of proficiency in the use of these
tools is expected from all graduates. With the im-
provement of state-of-the-art MT software, a re-
cent trend in CAT research is its integration with
machine translation tools as for example the Mate-
Cat
1
project (Cettolo et al., 2013).
There is considerable amount of studies on MT
post-editing published in the last years (Specia,
2011; Green et al., 2013). Due to the scope of our
1
www.matecat.com
93
paper (and space limitation) we will deliberately
not discuss the findings of these experiments and
instead focus on those that involve the use of trans-
lation memories. Post-editing tools are substan-
tially different than commercial CAT tools (such
as the one used here) and even though the TMs
used in our experiments were produced using MT
output, we believe that our experiment setting has
more in common with similar studies that investi-
gate TMs than MT post-editing.
The study by Bowker (2005) was one of the
first to quantify the influence of TM in transla-
tors work. The experiment divided translators in
three groups: A, B and C. Translators in Group
A did not use a TM, translators in Group B used
an unmodified TM and finally translators in group
C used a TM that had been deliberately mod-
ified with a number of translation errors. The
study concluded that when faced with time pres-
sure, translators using TMs tend not to be criti-
cal enough about the suggestions presented by the
software.
Another similar experiment (Guerberof, 2009)
compared productivity and quality of human trans-
lations using MT and TM output. The experiment
was conducted starting with the hypothesis that the
time invested in post-editing one string of machine
translated text will correspond to the same time in-
vested in editing a fuzzy matched string located in
the 80-90 percent range. This study quantified the
performance of 8 translators using a post-editing
tool. According to the author, the results indicate
that using a TM with 80 to 90 fuzzy matches pro-
duces more errors than using MT segments or hu-
man translation.
The aforementioned recent work by Morado
Vazquez et al. (2013) investigates the performance
of twelve human translators (students) using the
ACCEPT post-editing tool. Researchers provided
MT and TM output and compared time, quality
and keystroke effort. Findings of this study indi-
cate that the use of a specific MT has a great im-
pact in the translation activity in all three aspects.
In the context of software localization, productiv-
ity was also tested by Plitt and Masselot (2010)
combining MT output and a post-editing tool. An-
other study compared the performance of human
translators in a scenario using TMs and a com-
mercial CAT tool (Across) with a second scenario
using post-editing (L?aubli et al., 2013).
As to our study, we used instead of a post-
editing tool, a commercial CAT tool, the SDL Tra-
dos Studio 2014 version. A similar setting to ours
was explored by Federico et al. (2012) using SDL
Trados Studio integrating a commercial MT soft-
ware. We took the decision of working a commer-
cial CAT tool for two reasons: first, because this
is the real-world scenario faced by translators in
most companies and language service providers
2
and second, because it allows us to explore a dif-
ferent variable that the aforementioned studies did
not substantially explore, namely: MT output as
TM segments.
3 Setting the Experiment
In our experiments we provided short texts from
the domain of software development containing up
to 343 tokens each to 15 beginner translators. The
average length of these texts ranges between 210
tokens in experiment 1 to 264 tokens in experi-
ment 3 divided in 15 to 17 segments (average) (see
table 2). Translators were given English texts and
were asked to translate them into German, their
mother tongue. One important remark is that all
15 participants were not aware that the TMs we
made available were produced using MT output.
The 15 translators who participated in these
experiments are all 3
rd
semester master degree
students who have completed a bachelors degree
in translation studies and are familiar with CAT
tools. All of them attended at least 20 class
hours about TM software and related technologies.
Translators who participated in this study were all
proficient in English and they have studied it as a
foreign language at bachelor level.
As previously mentioned, the CAT tool used in
these experiments is the most recent version of
SDL Trados, the Studio 2014
3
version. Transla-
tors were given three different short texts to be
translated in three different scenarios:
1. Using no translation memory.
2. Using a translation memory collected with
modified MT examples.
3. Using translation memory collected with un-
modified MT examples.
In experiment number two we performed a
number of modifications in the TM segments. As
2
Although the use of MT and post-editing software has
been growing, commercial TM software is still the most pop-
ular alternative.
3
http://www.sdl.com/campaign/lt/sdl-trados-studio-2014/
94
can be seen in table 1, these modifications were
sufficient to alter the coverage of the TM, but did
not introduce translation errors to the memory.
4
The alterations we performed along with an exam-
ple of each of them can be summarized as follows:
? Deletion: ?To paste the text currently in the
clipboard, use the Edit Paste menu item.? -
?To paste the text, use the Edit Paste menu
item.?
? Modification: ?Persistent Selection is dis-
abled by default.? - ?Persistent Selection is
enabled by default.?
? Substitution: ?The editor is composed of the
following components:? - ?The editor is com-
posed of the following elements:?
Three texts were available per scenario, each of
them with different TM coverage scores (see table
1). Students were asked to translate the texts at
their own pace without time limitation and were
allowed to use external linguistic resources such
as dictionaries, lexica, parallel concordancers, etc.
3.1 Corpus and TM
The corpus used for these experiments is the KDE
corpus obtained from the Opus
5
repository (Tiede-
mann, 2012). The corpus contains texts from the
domain of software engineering, hence the title: ?a
case study in technical translation?. We are con-
vinced that technical translation contains a sub-
stantial amount of fixed expressions and techni-
cal terms different from, for example, news texts.
This makes technical translation, to our under-
standing, an interesting domain for the use of TM
by professional translators and for experiments of
this kind.
In scenarios 1, 2 and 3 we measured different
aspects of translation such as time and edited seg-
ments. One known shortcoming of our experiment
design is that unlike most post-editing software
the reports available in CAT tools are quite poor
(e.g. no information about keystrokes is provided).
Even so, we stick to our decision of using a TM
software and tried to compensate this shortcoming
by a careful qualitative and quantitative data anal-
ysis after the experiments.
4
Modifications were carried out in the source and target
languages
5
http://opus.lingfil.uu.se/
Table number 1 presents the coverage scores for
the different TMs and texts used in the experi-
ments. Coverage scores were calculated based on
the information provided by SDL Trados Studio.
We provided 9 different texts to be translated to
German (3 for each scenario), the 6 texts provided
for experiments 2 and 3 are presented next.
Text Experiment TM Coverage
Text D 2 61.23%
Text E 2 78.16%
Text F 2 59.15%
Average 2 66,18%
Text G 3 88.27%
Text H 3 59.92%
Text I 3 65.16%
Average 3 71,12%
Table 1: TM Coverage
We provided different texts and levels of coverage
to investigate the impact of this variable. We as-
sured an equal distribution of texts among trans-
lators: each text was translated by 5 translators.
This allowed us to calculate average results and
to consider the average TM coverage difference of
4,93% between experiment 2 and 3.
4 Results
We observed performance gain when using any of
the two TMs, which was expectable. The results
varied according to the coverage of the TM. In
experiment number 3, texts contained on average
over 7 segments with 100% matches
6
and exper-
iment number 2 only 2.68. This allowed transla-
tors to finish the task faster in experiment number
3. The average results obtained in the different ex-
periments are presented in table number 2.
7
Criteria Exp. 1 Exp. 2 Exp. 3
Number of Segments 15.85 15.47 17.29
Number of Tokens 209.86 202.89 264.53
Context Matches 6.58 6.06
Repetitions 0.18
100% 2.68 7.18
95% to 99% 0.42 0.12
85% to 94% 0.21
75% to 84% 2.11 0.18
50% to 75% 0.19
New Segments 15.86 5.89 3.24
Time Elapsed (mins.) 37m45s 26m3s 19m21s
Table 2: Average Scores
6
Translators were allowed to modify 100% and context
matches.
7
According to the Trados Studio documentation, a repeti-
tion occurs every time the tool finds the exact same segment
in another (or the same) file the user is translating
95
As to the time spent per segment, experiments
indicate a performance gain of over 52% in ex-
periment number 3 and over 28% in experiment
number 2.
Criteria Exp.1 Exp. 2 Exp. 3
Time Segment (mins.) 2m22s 1m41s 1m07s
Average gain to 1 +28.87% +52.82%
Average gain to 2 +33.77%
Table 3: Time per Segment
Apart from the expectable performance gain when
using TM, we also found a considerable difference
between the use of the modified and unmodified
TM. Translators completed segments in experi-
ment number 3, on average, 33.77% faster than
experiment two. The difference of coverage be-
tween the two TMs was 4,93%, which suggests
that a few percentage points of TM coverage re-
sults on a greater performance boost.
We also have to acknowledge that the experi-
ments were carried out by translators in the same
order in which they are presented in this paper.
This may, of course, influence performance in all
three experiments as translators were more used
to the task towards the end of the experiment. One
hypothesis is that the poor performance in exper-
iment 1, could be improved if this task was done
for last and conversely, the performance boost ob-
served in experiment 3, could be a bit lower if
this experiment was done first. This variable was
not explored in similar productivity studies such
as those presented in section two and, to our un-
derstanding, inverting the order of tasks could be
an interesting variable to be tested in future exper-
iments.
As a general remark, although all translators
had experience with the 2014 version of Trados
Studio, we observed a great difficulty in perform-
ing simple tasks with Windows for at least half of
the group. Simple operations such as copying, re-
naming and moving files or creating folders in the
file system were very time consuming. Trados in-
terface also posed difficulties to translators. For
example, the generation of reports through batch
tasks in a different window was for most transla-
tors confusing. These operations could be simpli-
fied as it is in other CAT tools such as memoQ.
8
8
http://kilgray.com/products/memoq
4.1 A Glance at Quality Estimation
One of the future directions that this work will take
is to investigate the quality of human translations.
Our initial hypothesis is that it is possible to apply
state-of-the-art metrics such as BLEU (Papineni
et al., 2002) or METEOR (Denkowski and Lavie,
2011) to estimate the quality of these translations
regardless of how they are produced.
For machine translation output, quality nowa-
days is measured by automatic evaluation met-
rics such as the aforementioned IBM BLEU (Pap-
ineni et al., 2002), NIST (Doddington, 2002), ME-
TEOR (Denkowski and Lavie, 2011), the Leven-
sthein (1966) distance based WER (word error-
rate) metric, the position-independent error rate
metric PER (Tillmann et al., 1997) and the trans-
lation error rate metric TER (Snover et al., 2006)
with its newer version TERp (Snover et al., 2009).
The most frequently used one is IBM
BLEU (Papineni et al., 2002). It is easy to
use, language-independent, fast and requires
only the candidate and reference translation.
IBM BLEU is based on the n-gram precision by
matching the machine translation output against
one or more reference translations. It accounts
for adequacy and fluency through word precision,
respectively the n-gram precision, by calculating
the geometric mean. Instead of recall, in IBM
BLEU the brevity penalty (BP) was introduced.
Different from IBM BLEU, METEOR evalu-
ates a candidate translation by calculating the pre-
cision and recall on unigram level and combining
them in a parametrized harmonic mean. The result
from the harmonic mean is than scaled by a frag-
mentation penalty which penalizes gaps and dif-
ferences in word order.
For our investigation we applied METEOR on
the human translated text. Our intention is to test
whether we can reproduce the observations from
the experiments: is the experiment setting 3 bet-
ter than the setting of experiment 2? Therefore,
METEOR is used here to investigate whether we
can correlate it with our experiments and not to
evaluate the produced translations. Table number
4 presents the scores obtained with METEOR.
Exp. 2 Exp. 3
Average Score (mean) 0.14 0.41
Best Result 0.35 0.58
Worst Result 0.11 0.25
Table 4: METEOR Scores
96
In experiment number 3 we have previously ob-
served that the translators? performance was sig-
nificantly better and that translators could translate
each segment on average 33.77% faster than ex-
periment 2 and 52.82% faster than experiment 1.
By applying METEOR scores we can also observe
that experiment 3 achieved higher scores which
seems to indicate more suitable translations than
experiment number 2. Quality estimation is one
of the aspects we would like to explore in future
work.
5 Conclusion
This paper is a first step towards the comparison
of different TMs produced with MT output and
their direct impact in human translation. Our study
shows a substantial improvement in performance
with the use of translation memories containing
MT output used trough commercial CAT software.
To our knowledge this experiment setting was not
tested in similar studies, which makes our paper a
new contribution in the study of translators? per-
formance. Although the performance gain seems
intuitive, the quantification of these aspects within
a controlled experiment was not substantially ex-
plored.
We opted for the use of a state-of-the-art com-
mercial CAT tool as this is the real-world scenario
that most translators face everyday. In compari-
son to translating without TM, translators were on
average 28.87% faster using a modified TM and
52.82% using an unmodified one. Between the
two TMs we observed that translators were on av-
erage 33.77% faster when using the unmodified
TM. As previously mentioned, the order in which
this tasks were carried out should be also taken
into account. The performance boost of 33.77%
when using a TM that is only 4,93% better is also
an interesting outcome of our experiments that
should be looked at in more detail.
Finally, in this paper we used METEOR scores
to assess whether it is possible to correlate trans-
lations? speed, quality and TM coverage. The av-
erage score for experiment number 2 was 0.14 and
for experiment number 3 was 0.41. Our initial
analysis suggests that a relation between the two
variables exists for our dataset. Whether this rela-
tion can be found in other scenarios is still an open
question and we wish to investigate this variable
more carefully in future work.
5.1 Future Work
We consider these experiments as a pilot study that
was carried out to provide us a set of variables that
we wish to investigate further. There are a number
of aspects that we wish to look in more detail in
future work.
Future experiments include the aforementioned
quality estimation analysis by applying state-of-
the-art metrics used in machine translation. Using
these metrics we would like to explore the extent
to which it is possible to use automatic methods
to study the interplay between quality and perfor-
mance in computer assisted translation. Further-
more, we would like to perform a qualitative anal-
ysis of the produced translations using human an-
notators and inter annotator agreement (Carletta,
1996).
The performance boost observed between sce-
narios 2 and 3 should be looked in more detail
in future experiments. We would like to replicate
these experiments using other different TMs and
explore this variable more carefully. Another as-
pect that we would like to explore in the future is
the direct impact of the use of different CAT tools.
Does the same TM combined with different CAT
tools produce different results? When conducting
these experiments, we observed that a simplified
interface may speed up translators? work consid-
erably.
Other directions that our work will take include
controlling other variables not taken into account
in this pilot study such as: the use of termino-
logical databases, spelling correctors, etc. How
and to which extent do they influence performance
and quality? Finally, we would also like to use
eye-tracking to analyse the focus of attention of
translators as it was done in previous experiments
(O?brien, 2006).
Acknowledgments
We thank the students who participated in these
experiments for their time. We would also like
to thank the detailed feedback provided by the
anonymous reviewers who helped us to increase
the quality of this paper.
References
Lynne Bowker. 2005. Productivity vs quality? a pilot
study on the impact of translation memory systems.
Localisation Reader, pages 133?140.
97
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Mauro Cettolo, Christophe Servan, Nicola Bertoldi,
Marcello Federico, Loic Barrault, and Holger
Schwenk. 2013. Issues in incremental adaptation of
statistical mt from human post-edits. In Proceedings
of the MT Summit XIV Workshop on Post-editing
Technology and Practice (WPTP-2), Nice, France.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT 2002, pages 138?145,
San Francisco, CA, USA. Morgan Kaufmann Pub-
lishers Inc.
Marcello Federico, Alessandro Cattelan, and Marco
Trombetti. 2012. Measuring user productivity
in machine translation enhanced computer assisted
translation. In Proceedings of Conference of the As-
sociation for Machine Translation in the Americas
(AMTA).
Spence Green, Jeffrey Heer, and Christopher D Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems.
Ana Guerberof. 2009. Productivity and quality in
the post-editing of outputs from translation memo-
ries and machine translation. Localisation Focus,
7(1):133?140.
Ana Guerberof. 2012. Productivity and Quality in the
Post-Edition of Outputs from Translation Memories
and Machine Translation. Ph.D. thesis, Rovira and
Virgili University Tarragona.
Elina Lagoudaki. 2008. The value of machine transla-
tion for the professional translator. In Proceedings
of the 8th Conference of the Association for Ma-
chine Translation in the Americas, pages 262?269,
Waikiki, Hawaii.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen
Ehrensberger-Dow, and Martin Volk. 2013. Assess-
ing post-editing efficiency in a realistic translation
environment. In Proceedings of MT Summit XIV
Workshop on Post-editing Technology and Practice.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions and rever-
sals. Soviet Physics Doklady, (8):707?710, Febru-
ary.
Lucia Morado Vazquez, Silvia Rodriguez Vazquez, and
Pierrette Bouillon. 2013. Comparing forum data
post-editing performance using translation memory
and machine translation output: a pilot study. In
Proceedings of the Machine Translation Summit
XIV), Nice, France.
Sharon O?brien. 2006. Eye-tracking and translation
memory matches. Perspectives: Studies in Transla-
tology, 14:185?204.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Mirko Plitt and Franc?ois Masselot. 2010. A productiv-
ity test of statistical machine translation post-editing
in a typical localisation context. The Prague Bul-
letin of Mathematical Linguistics, 93:7?16.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, AMTA.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
hter? exploring different human judgments with a
tunable mt metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation at the
12th Meeting of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL 2009.
Harold Somers and Gabriela Fernandez Diaz. 2004.
Translation memory vs. example-based mt: What is
the difference? International Journal of Transla-
tion, 16(2):5?33.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of the 15th Conference of the European
Association for Machine Translation, pages 73?80,
Leuven, Belgium.
J?org Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey. European Lan-
guage Resources Association (ELRA).
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alexander Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated dp based search for statistical translation.
In European Conference on Speech Communication
and Technology, EUROSPEECH 1977, pages 2667?
2670.
98
