Proceedings of the ACL 2007 Student Research Workshop, pages 67?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Towards a Computational Treatment of Superlatives 
Silke Scheible 
Institute for Communicating and Collaborative Systems (ICCS) 
School of Informatics 
University of Edinburgh 
S.Scheible@sms.ed.ac.uk 
 
Abstract 
I propose a computational treatment of su-
perlatives, starting with superlative con-
structions and the main challenges in 
automatically recognising and extracting 
their components. Initial experimental evi-
dence is provided for the value of the pro-
posed work for Question Answering. I also 
briefly discuss its potential value for Sen-
timent Detection and Opinion Extraction. 
1 Introduction 
Although superlatives are frequently found in 
natural language, with the exception of recent work 
by Bos and Nissim (2006) and Jindal and Liu 
(2006), they have not yet been investigated within 
a computational framework. And within the 
framework of theoretical linguistics, studies of su-
perlatives have mainly focused on particular se-
mantic properties that may only rarely occur in 
natural language (Szabolcsi, 1986; Heim, 1999). 
My goal is a comprehensive computational 
treatment of superlatives. The initial question I ad-
dress is how useful information can be automati-
cally extracted from superlative constructions. Due 
to the great semantic complexity and the variety of 
syntactic structures in which superlatives occur, 
this is a major challenge. However, meeting it will 
benefit NLP applications such as Question An-
swering, Sentiment Detection and Opinion Extrac-
tion, and Ontology Learning. 
2 What are Superlatives? 
In linguistics, the term ?superlative? describes a 
well-defined class of word forms which (in Eng-
lish) are derived from adjectives or adverbs in two 
different ways: Inflectionally, where the suffix -est 
is appended to the base form of the adjective or 
adverb (e.g. lowest, nicest, smartest), or analyti-
cally, where the base adjective/adverb is preceded 
by the markers most/least (e.g. most interesting, 
least beautiful). Certain adjectives and adverbs 
have irregular superlative forms: good (best), bad 
(worst), far (furthest/farthest), well (best), badly 
(worst), much (most), and little (least).  
In order to be able to form superlatives, adjec-
tives and adverbs must be gradable, which means 
that it must be possible to place them on a scale of 
comparison, at a position higher or lower than the 
one indicated by the adjective/adverb alone. In 
English, this can be done by using the comparative 
and superlative forms of the adjective or adverb:  
[1] (a) Maths is more difficult than Physics. 
      (b) Chemistry is less difficult than Physics.  
[2] (a) Maths is the most difficult subject at school. 
      (b) History is the least difficult subject at school.  
The comparative form of an adjective or adverb is 
commonly used to compare two entities to one an-
other with respect to a certain quality. For exam-
ple, in [1], Maths is located at a higher point on the 
difficulty scale than Physics, and Chemistry at a 
lower point. The superlative form of an adjective 
is usually used to compare one entity to a set of 
other entities, and expresses the end spectrum of 
the scale: In [2], Maths and History are located at 
the highest and lowest points of the difficulty 
scale, respectively, while all the other subjects at 
school range somewhere in between. 
3 Why are Superlatives Interesting? 
From a computational perspective, superlatives 
are of interest because they express a comparison 
67
between a target entity (indicated in bold) and its 
comparison set (underlined), as in: 
[3] The blue whale is the largest mammal. 
Here, the target blue whale is compared to the 
comparison set of mammals. Milosavljevic (1999) 
has investigated the discourse purpose of different 
types of comparisons. She classifies superlatives as 
a type of set complement comparison, whose pur-
pose is to highlight the uniqueness of the target 
entity compared to its contrast set. 
My initial investigation of superlative forms 
showed that there are two types of relation that 
hold between a target and its comparison set: 
Relation 1: Superlative relation 
Relation 2: IS-A relation 
The superlative relation specifies a property which 
all members of the set share, but which the target 
has the highest (or lowest) degree or value of. The 
IS-A (or hypernymy) relation expresses the mem-
bership of the target in the comparison class (e.g. 
its parent class in a generalisation hierarchy). Both 
of these relations are of great interest from a rela-
tion extraction point of view, and in Section 6, I 
discuss their use in applications such as Question 
Answering (QA) and Sentiment Detection and 
Opinion Extraction. That a computational treat-
ment of superlatives is a worthwhile undertaking is 
also supported by the frequency of superlative 
forms in ordinary text: In a 250,000 word subcor-
pus of the WSJ corpus 1  I found 602 instances 
(which amounts to roughly one superlative form in 
every 17 sentences), while in the corpus of animal 
encyclopaedia entries used by Milosavljevic 
(1999), there were 1059 superlative forms in 
250,000 words (about one superlative form in 
every 11 sentences).2 These results show signifi-
cant variation in the distribution of superlatives 
across different text genres. 
4 Elements of a Computational Treat-
ment of Superlatives 
For an interpretation of comparisons, two things 
are generally of interest: What is being compared, 
and with respect to what this comparison is made. 
Given that superlatives express set comparisons, a 
                                                 
1
 www.ldc.upenn.edu/Catalog/LDC2000T43.html 
2
 In the following, these 250,000 word subcorpora will 
be referred to as SubWSJ and SubAC. 
computational treatment should therefore help to 
identify: 
a) The target and comparison set 
b) The type of superlative relation that holds be-
tween them (cf. Relation 1 in Section 3)  
However, this task is far from straightforward, 
firstly because superlatives occur in a variety of 
different constructions. Consider for example: 
[4] The pipe organ is the largest instrument.     
[5] Of all the musicians in the brass band, Peter plays 
the largest instrument. 
[6] The human foot is narrowest at the heel. 
[7] First Class mail usually arrives the fastest. 
[8] This year, Jodie Foster was voted best actress. 
[9] I will get there at 8 at the earliest. 
[10] I am most tired of your constant moaning. 
[11] Most successful bands are from the U.S. 
All these examples contain a superlative form 
(bold italics). However, they differ not only in their 
syntactic structure, but also in the way in which 
they express a comparison. Example [4] contains a 
clear-cut comparison between a target item and its 
comparison set: The pipe organ is compared to all 
other instruments with respect to its size. However, 
although the superlative form in [4] occurs in the 
same noun phrase as in [5], the comparisons differ: 
What is being compared in [5] is not just the in-
struments, but the musicians in the brass band with 
respect to the size of the instrument that they play. 
In example [6], the target and comparison set are 
even less easy to identify. What is being compared 
here is not the human foot and a set of other enti-
ties, but rather different parts of the human foot. In 
contrast to the first two examples, this superlative 
form is not incorporated in a noun phrase, but oc-
curs freely in the sentence. The same applies to 
fastest in example [7], which is an adverbial super-
lative. The comparison here is between First Class 
mail and other mail delivery services. Finally, ex-
amples [8] to [11] are not proper comparisons: best 
actress in [8] is an idiomatic expression, earliest in 
[9] is part of a so-called PP superlative construc-
tion (Corver and Matushansky, 2006), and [10] and 
[11] describe two non-comparative uses of most, as 
an intensifier and a proportional quantifier, respec-
tively (Huddleston and Pullum, 2002). 
Initially, I will focus on cases like [4], which I 
call IS-A superlatives because they make explicit 
the IS-A relation that holds between target and 
comparison set (cf. Relation 2 in Section 3). They 
68
are a good initial focus for a computational ap-
proach because both their target and comparison 
set are explicitly realised in the text (usually, 
though not necessarily, in the same sentence). 
Common surface forms of IS-A superlatives in-
volve the verb ?to be? ([12]-[14]), appositive posi-
tion [15], and other copula verbs or expressions 
([16] and [17]): 
[12] The blue whale is the largest mammal. 
[13] The blue whale is the largest of all mammals. 
[14] Of all mammals, the blue whale is the largest. 
[15] The largest mammal, the blue whale, weighs... 
[16] The ostrich is considered the largest bird. 
[17] Mexico claimed to be the most peaceful country 
in the Americas. 
IS-A superlatives are also the most frequent type of 
superlative comparison, with 176 instances in 
SubWSJ (ca. 30% of all superlative forms), and 
350 instances in SubAC (ca. 33% of all superlative 
forms).  
The second major problem in a computational 
treatment of superlatives is to correctly identify 
and interpret the comparison set. The challenge lies 
in the fact that it can be restricted in a variety of 
ways, for example by preceding possessives and 
premodifiers, or by postmodifiers such as PPs and 
various kinds of clauses. Consider for example: 
[18] VW is [Europe?s largest maker of cars]. 
[19] VW is [the largest European car maker with this 
product range]. 
[20] VW is [the largest car maker in Europe] with an 
impressive product range. 
[21] In China, VW is by far [the largest car maker]. 
The phrases of cars and car in [18] and [19] 
both have the role of specifying the type of maker 
that constitutes the comparison set. The phrases 
Europe?s, European and in Europe occur in deter-
minative, premodifying, and postmodifying posi-
tion, respectively, but all have the role of restrict-
ing the set of car makers to the ones in Europe. 
And finally, the ?with? PP phrases in [19] and [20] 
both occur in postmodifying position, but differ in 
that the one in [19] is involved in the comparison, 
while the one in [20] is non-restrictive. In addition, 
restrictors of the comparison can also occur else-
where in the sentence, as shown by the PP phrase 
and adverbial in [21]. It is evident that in order to 
extract useful and reliable information, a thorough 
syntactic and semantic analysis of superlative con-
structions is required. 
5 Previous Approaches 
5.1 Jindal and Liu (2006) 
Jindal and Liu (2006) propose the study of com-
parative sentence mining, by which they mean the 
study of sentences that express ?an ordering 
relation between two sets of entities with respect to 
some common features? (2006). They consider 
three kinds of relations: non-equal gradable (e.g. 
better), equative (e.g. as good as) and superlative 
(e.g. best). Having identified comparative sen-
tences in a given text, the task is to extract com-
parative relations from them, in form of a vector 
like (relationWord, features, entityS1, entityS2), 
where relationWord represents the keyword used 
to express a comparative relation, features are a set 
of features being compared, and entityS1 and enti-
tyS2 are the sets of entities being compared, where 
entityS1 appears to the left of the relation word and 
entityS2 to the right. Thus, for a sentence like 
?Canon?s optics is better than those of Sony and 
Nikon?, the system is expected to extract the vector 
(better, {optics}, {Canon}, {Sony, Nikon}). 
For extracting the comparative relations, Jindal 
and Liu use what they call label sequential rules 
(LSR), mainly based on POS tags. Their overall F-
score for this extraction task is 72%, a big im-
provement to the 58% achieved by their baseline 
system. Although this result suggests that their sys-
tem represents a powerful way of dealing with su-
perlatives computationally, a closer inspection of 
their approach, and in particular of the gold stan-
dard data set, reveals some serious problems.  
Jindal and Liu claim that for superlatives, the 
entityS2 slot is ?normally empty? (2006). Assum-
ing that the members of entityS2 usually represent 
the comparison set, this is somewhat counter-
intuitive. A look at the data shows that even in 
cases where the comparison set is explicitly men-
tioned in the sentence, the entityS2 slot remains 
empty. For example, although the comparison set 
in [22] is represented by the string these 2nd gen-
eration jukeboxes ( ipod , archos , dell , samsung ), 
it is not annotated as entityS2 in the gold standard: 
[22] all reviews i 've seen seem to in-
dicate that the creative mp3 jukeboxes 
have the best sound quality of these 
2nd generation jukeboxes ( ipod , ar-
chos , dell , samsung ) .  
(best, {sound quality}, {creative mp3 jukeboxes}, {--}) 
Jindal and Liu (2006) 
69
Furthermore, Jindal and Liu do not distinguish 
between different types of superlatives. In con-
structions where the superlative form is incorpo-
rated into an NP, Jindal and Liu consistently inter-
pret the string following the superlative form as a 
?feature?, which is appropriate for cases like [22], 
but does not apply to superlative sentences involv-
ing the copula verb ?to be? (as e.g. in [4]), where 
the NP head denotes the comparison set rather than 
a feature. A further major problem is that restric-
tions on the comparison set as the ones discussed 
in Section 4 and negation are not considered at all. 
Therefore, the reliability of the output produced by 
the system is questionable. 
5.2 Bos and Nissim (2006) 
In contrast to Jindal and Liu (2006), Bos and 
Nissim?s (2006) approach to superlatives is explic-
itly semantic. They describe an implementation of 
a system that can automatically detect superlatives, 
and determine the correct comparison set for at-
tributive cases, where the superlative form is in-
corporated into an NP. For example in [23], the 
comparison set of the superlative oldest spans from 
word 3 to word 7: 
[23]  wsj00 1690 [...] Scope: 3-7 
The oldest bell-ringing group in the 
country , the Ancient Society of Col-
lege Youths , founded in 1637 , re-
mains male-only , [...] .  
(Bos and Nissim 2006) 
Bos and Nissim?s system, called DLA (Deep Lin-
guistic Analysis), uses a wide-coverage parser to 
produce semantic representations of superlative 
sentences, which are then exploited to select the 
comparison set among attributive cases. Compared 
with a baseline result, the results for this are very 
good, with an accuracy of 69%-83%. 
The results are clearly very promising and show 
that comparison sets can be identified with high 
accuracy. However, this only represents a first step 
towards the goal of the present work. Apart from 
the superlative keyword oldest, the only informa-
tion example [23] provides is that the comparison 
set spans from word 3 to word 7. However, what 
would be interesting to know is that the target of 
the comparison appears in the same sentence and 
spans from word 9 to word 14 (the Ancient Society 
of College Youths). Furthermore, no analysis of the 
semantic roles of the constituents of the resulting 
string is carried out: We lose the information that 
the Ancient Society of College Youths IS-A kind of 
bell-ringing group, and that the set of bell-ringing 
groups is restricted in location (in the country). 
6 Applications 
The proposed work will be beneficial for a vari-
ety of areas in NLP, for example Question An-
swering (QA), Sentiment Detection/Opinion Ex-
traction, Ontology Learning, or Natural Language 
Generation. In this section I will discuss applica-
tions in the first two areas. 
6.1 Question Answering 
In open-domain QA, the proposed work will be 
useful for answering two question types. A super-
lative sentence like [24], found in a corpus, can be 
used to answer both a factoid question [25] and a 
definition question [26]:  
[24] A: The Nile is the longest river in the world. 
[25] Q: What is the world?s longest river?  
[26] Q: What is the Nile? 
Here I will focus on the latter. The common as-
sumption that superlatives are useful with respect 
to answering definition questions is based on the 
observation that superlatives like the one in [24] 
both place an entity in a generalisation hierarchy, 
and distinguish it from its contrast set. 
To investigate this assumption, I carried out a 
study involving the TREC QA ?other? question 
nuggets3, which are snippets of text that contain 
relevant information for the definition of a specific 
topic. In a recent study of judgement consistency 
(Lin and Demner-Fushman, 2006), relevant nug-
gets were judged as either 'vital' or 'okay' by 10 
different judges rather than the single assessor 
standardly used in TREC. For example, the first 
three nuggets for the topic ?Merck & Co.? are: 
[27] Qid 75.8: 'other' question for target Merck & Co. 
75.8  1   vital   World's largest drug company. 
75.8  2   okay   Spent $1.68 billion on RandD in 
1997. 
75.8  3   okay   Has experience finding new uses 
for established drugs. 
(taken from TREC 2005; 'vital' and 'okay' reflect 
the opinion of the TREC evaluator.) 
My investigation of the nugget judgements in 
Lin and Demner-Fushman's study yielded two in-
                                                 
3
 http://trec.nist.gov/data/qa.html 
70
teresting results: First of all, a relatively high pro-
portion of relevant nuggets contains superlatives: 
On average, there is one superlative nugget for at 
least half of the TREC topics. Secondly, of 69 
superlative nuggets altogether, 32 (i.e. almost half) 
are judged ?vital? by more than 9 assessors.  
Furthermore, I found that the nuggets can be dis-
tinguished by how the question target (i.e. the 
TREC topic, referred to as T1) relates to the super-
lative target (T2): In the first case, T1 and T2 coin-
cide (referred to as class S1). In the second one, T2 
is part of or closely related to T1, or T2 is part of 
the comparison set (class S2). In the third case, T1 
is unrelated or only distantly related to T2 (S3). 
Table 1 shows examples of each class: 
 T1 nugget (T2 in bold) 
S1 Merck & Co. World's largest drug company 
S2 Florence 
Nightingale 
Nightingale Medal highest  
international nurses award 
S3 Kurds Irbil largest city controlled by 
Kurds 
Table 1. Examples of superlative nuggets. 
Of the 69 nuggets containing superlatives, 46 
fall into subclass S1, 15 into subclass S2 and 8 into 
subclass S3. While I noted earlier that 32/69 (46%) 
of superlative-containing nuggets were judged vital 
by more than 9 assessors, these judgements are not 
equally distributed over the subclasses: Table 2 
shows that 87% of S1 judgements are 'vital', while 
only 38% of S3 judgements are.  
 number of 
instances 
% of ?vital? 
judgements 
% of ?okay? 
judgements 
S1 46 87% 13% 
S2 15 59% 40% 
S3 8 38% 60% 
Table 2. Ratings of the classes S1, S2, and S3. 
These results strongly suggest that the presence 
of superlatives, and in particular S1 membership, is 
a good indicator of the importance of nuggets, and 
thus for answering definition questions. Some ex-
periments carried out in the framework of TREC 
2006 (Kaisser et al, 2006), however, showed that 
superlatives alone are not a winning indicator of 
nugget importance, but S1 membership may be. A 
similar simple technique was used by Ahn et al 
(2005) and by Razmara and Kosseim (2007). All 
just looked for the presence of a superlative and 
raised the score without further analysing the type 
of superlative or its role in the sentence. This calls 
for a more sophisticated approach, where class S1 
superlatives can be distinguished. 
6.2 Sentiment Detection/Opinion Extraction 
Like adjectives and adverbs, superlatives can be 
objective or subjective. Compare for example: 
[28] The Black Forest is the largest forest in 
Germany.                [objective] 
[29] The Black Forest is the most beautiful area 
in Germany.               [subjective] 
So far, none of the studies in sentiment detection 
(e.g. Wilson et al, 2005; Pang et al, 2002) or opin-
ion extraction (e.g. Hu and Liu, 2004; Popescu and 
Etzioni, 2005) have specifically looked at the role 
of superlatives in these areas. 
Like subjective adjectives, subjective superla-
tives can either express positive or negative opin-
ions. This polarity depends strongly on the adjec-
tive or adverb that the superlative is derived from.4 
As superlatives place the adjective or adverb at the 
highest or lowest point of the comparison scale (cf. 
Section 2), the question of interest is how this af-
fects the polarity of the adjective/adverb. If the 
intensity of the polarity increases in a likewise 
manner, then subjective superlatives are bound to 
express the strongest or weakest opinions possible. 
If this hypothesis holds true, an ?extreme opinion? 
extraction system could be created by combining 
the proposed superlative extraction system with a 
subjectivity recognition system that can identify 
subjective superlatives. This would clearly be of 
interest to many companies and market researchers. 
Initial searches in Hu and Liu?s annotated cor-
pus of customer reviews (2004) look promising. 
Sentences in this corpus are annotated with infor-
mation about positive and negative opinions, 
which are located on a six-point scale, where [+/-3] 
stand for the strongest positive/negative opinions, 
and [+/-1] stand for the weakest positive/negative 
opinions. A search for annotated sentences con-
taining superlatives shows that an overwhelming 
majority are marked with strongest opinion labels. 
7 Summary and Future Work 
This paper proposed the task of automatically ex-
tracting useful information from superlatives oc-
                                                 
4
 It may, however, also depend on whether the superla-
tive expresses the highest ('most') or the lowest ('least') 
point in the scale.  
71
curring in free text. It provided an overview of su-
perlative constructions and the main challenges 
that have to be faced, described previous computa-
tional approaches and their limitations, and dis-
cussed applications in two areas in NLP: QA and 
Sentiment Detection/Opinion Extraction.  
The proposed task can be seen as consisting of 
three subtasks:  
TASK 1: Decide whether a given sentence contains 
a superlative form  
TASK 2: Given a sentence containing a superlative 
form, identify what type of superlative it is (ini-
tially: IS-A superlative or not?) 
TASK 3: For set comparisons, identify the target 
and the comparison set, as well as the superlative 
relation 
Task 1 can be tackled by a simple approach rely-
ing on POS tags (e.g. JJS and RBS in the Penn 
Treebank tagset). For Task 2, I have carried out a 
thorough analysis of the different types of superla-
tive forms and postulated a new classification for 
them. My present efforts are on the creation of a 
gold standard data set for the extraction task. As 
superlatives are particularly frequent in encyclo-
paedic language (cf. Section 3), I am considering 
using the Wikipedia 5  as a knowledge base. The 
main challenge is to devise a suitable annotation 
scheme which can account for all syntactic struc-
tures in which IS-A superlatives occur and which 
incorporates their semantic properties in an ade-
quate way (semantic role labelling). Finally, for 
Task 3, I plan to use both manually created rules 
and machine learning techniques. 
Acknowledgements 
I would like to thank Bonnie Webber and Maria 
Milosavljevic for their helpful comments and sug-
gestions on this paper. Many thanks also go to 
Nitin Jindal and Bing Liu, Johan Bos and Malvina 
Nissim, and Jimmy Lin and Dina Demner-
Fushman for making their data available. 
References 
Kisuh Ahn, Johan Bos, James R. Curran, Dave Kor, 
Malvina Nissim and Bonnie Webber. 2005.  
Question Answering with QED. In Voorhees and 
Buckland (eds.): The 14th Text REtrieval  
Conference, TREC 2005. 
                                                 
5
 www.wikipedia.org 
Johan Bos and Malvina Nissim. 2006. An Empirical 
Approach to the Interpretation of Superlatives. In 
Proceedings of EMNLP 2006, pages 9-17, Sydney, 
Australia. 
Norbert Corver and Ora Matushansky. 2006. At our best 
when at our boldest. Handout. TIN-dag, Feb. 4, 2006.  
Irene Heim. 1999. Notes on superlatives. Ms., MIT. 
Minqing Hu and Bing Liu. 2004. Mining Opinion Fea-
tures in Customer Reviews. In Proceedings of AAAI, 
pages 755-760, San Jose, California, USA.  
Rodney Huddleston and Geoffrey K. Pullum (eds.). 
2002. The Cambridge grammar of the English lan-
guage. Cambridge: Cambridge University Press. 
Michael Kaisser, Silke Scheible and Bonnie Webber. 
2006. Experiments at the University of Edinburgh for 
the TREC 2006 QA track. In Proceedings of TREC 
2006, Gaithersburg, MD, USA.  
Nitin Jindal and Bing Liu. 2006. Mining Comparative 
Sentences and Relations. In Proceedings of AAAI, 
Boston, MA, USA. 
Jimmy Lin and Dina Demner-Fushman. 2006. Will 
pyramids built of nuggets topple over? In Proceed-
ings of the HLT/NAACL, pages 383-390, New York, 
NY, USA. 
Maria Milosavljevic. 1999. The Automatic Generation 
of Comparisons in Descriptions of Entities. PhD 
Thesis. Microsoft Research Institute, Macquarie Uni-
versity, Sydney, Australia. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP, pages 79-86, Philadelphia, PA, USA. 
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting 
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP-2005, pages 339-346, Van-
couver, British Columbia, Canada. 
Majid Razmara and Leila Kosseim. 2007. A little 
known fact is... Answering Other questions using in-
terest-markers. In Proceedings of CICLing-2007, 
Mexico City, Mexico. 
Anna Szabolcsi. 1986. Comparative superlatives. In 
MIT Working Papers in Linguistics (8). ed. by Naoki 
Fukui, Tova R. Rapoport and Elisabeth Sagey. 245-
265.  
Theresa Wilson, Janyce Wiebe and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of 
HLT/EMNLP 2005, pages 347-354, Vancouver, Brit-
ish Columbia, Canada. 
72
Proceedings of the Fifth Law Workshop (LAW V), pages 124?128,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Gold Standard Corpus of Early Modern German
Silke Scheible, Richard J. Whitt, Martin Durrell and Paul Bennett
School of Languages, Linguistics, and Cultures
University of Manchester
Silke.Scheible, Richard.Whitt@manchester.ac.uk
Martin.Durrell, Paul.Bennett@manchester.ac.uk
Abstract
This paper describes an annotated gold stan-
dard sample corpus of Early Modern German
containing over 50,000 tokens of text manu-
ally annotated with POS tags, lemmas, and
normalised spelling variants. The corpus is
the first resource of its kind for this variant of
German, and represents an ideal test bed for
evaluating and adapting existing NLP tools on
historical data. We describe the corpus for-
mat, annotation levels, and challenges, provid-
ing an example of the requirements and needs
of smaller humanities-based corpus projects.
1 Introduction
This paper describes work which is part of a larger
project whose goal is to develop a representative cor-
pus of Early Modern German from 1650-1800. The
GerManC corpus was born out of the need for a re-
source to facilitate comparative studies of the devel-
opment and standardisation of English and German
in the 17th and 18th centuries. One major goal is
to annotate GerManC with linguistic information in
terms of POS tags, lemmas, and normalised spelling
variants. However, due to the lexical, morpholog-
ical, syntactic, and graphemic peculiarities charac-
teristic of Early Modern German, automatic annota-
tion of the texts poses a major challenge. Most ex-
isting NLP tools are tuned to perform well on mod-
ern language data, but perform considerably worse
on historical, non-standardised data (Rayson et al,
2007). This paper describes a gold standard sub-
corpus of GerManC which has been manually anno-
tated by two human annotators for POS tags, lem-
mas, and normalised spelling variants. The corpus
will be used to test and adapt modern NLP tools on
historical data, and will be of interest to other current
corpus-based projects in historical linguistics (Jur-
ish, 2010; Fasshauer, 2011; Dipper, 2010).
2 Corpus design
2.1 GerManC
In order to enable corpus-linguistic investigations,
the GerManC corpus aims to be representative on
three different levels. First of all, the corpus includes
a range of text types: four orally-oriented genres
(dramas, newspapers, letters, and sermons), and four
print-oriented ones (narrative prose, and humanities,
scientific, and legal texts). Secondly, in order to en-
able historical developments to be traced, the pe-
riod has been divided into three fifty year sections
(1650-1700, 1700-1750, and 1750-1800). The com-
bination of historical and text-type coverage should
enable research on the evolution of style in differ-
ent genres (cf. Biber and Finegan, 1989). Finally,
the corpus also aims to be representative with re-
spect to region, including five broad dialect areas:
North German, West Central, East Central, West Up-
per (including Switzerland), and East Upper German
(including Austria). Per genre, period, and region,
three extracts of around 2000 words are selected,
yielding a corpus size of nearly a million words. The
structure of the GerManC corpus is summarised in
Table 1.
2.2 GerManC-GS
In order to facilitate a thorough linguistic inves-
tigation of the data, the final version of the Ger-
124
Periods Regions Genres
1650-1700 North Drama
1700-1750 West Central Newspaper
1750-1800 East Central Letter
West Upper Sermon
East Upper Narrative
Humanities
Scientific
Legal
Table 1: Structure of the GerManC corpus
ManC corpus aims to provide the following linguis-
tic annotations: 1.) Normalised spelling variants;
2.) Lemmas; 3.) POS tags. However, due to the
non-standard nature of written Early Modern Ger-
man, and the additional variation introduced by the
three variables of ?genre?, ?region?, and ?time?, au-
tomatic annotation of the corpus poses a major chal-
lenge. In order to assess the suitability of existing
NLP tools on historical data, and with a view to
adapting them to improve their performance, a man-
ually annotated gold standard subcorpus has been
developed, which aims to be as representative of
the main corpus as possible (GerManC-GS). To re-
main manageable in terms of annotation times and
cost, the subcorpus considers only two of the three
corpus variables, ?genre? and ?time?, as they alone
were found to display as much if not more varia-
tion than ?region?. GerManC-GS thus only includes
texts from the North German dialect region, with
one sample file per genre and time period. Table
2 provides an overview of GerManC-GS, showing
publication year, file name, and number of tokens for
each genre/period combination. It contains 57,845
tokens in total, which have been manually annotated
as described in the following sections.
2.3 Corpus format
As transcription of historical texts needs to be very
detailed with regard to document structure, glossing,
damaged or illegible passages, foreign language ma-
terial and special characters such as diacritics and
ligatures, the raw input texts have been annotated
according to the guidelines of the Text Encoding
Initiative (TEI)1 during manual transcription. The
TEI have published a set of XML-based encoding
conventions recommended for meta-textual markup
1http://www.tei-c.org
to minimise inconsistencies across projects and to
maximise mutual usability and data interchange.
The GerManC corpus has been marked up using
the TEI P5 Lite tagset, which serves as standard for
many humanities-based projects. Only the most rel-
evant tags have been selected to keep the document
structure as straightforward as possible. Figure 1
shows structural annotation of a drama excerpt, in-
cluding headers, stage directions, speakers, as well
as lines.
Figure 1: TEI annotation of raw corpus
3 Linguistic annotation
GerManC-GS has been annotated with linguistic in-
formation in terms of normalised word forms, lem-
mas, and POS tags. To reduce manual labour, a
semi-automatic approach was chosen whose output
was manually corrected by two trained annotators.
The following paragraphs provide an overview of
the annotation types and the main challenges en-
countered during annotation.
3.1 Tokenisation and sentence boundaries
As German orthography was not yet codified in the
Early Modern period, word boundaries were diffi-
cult to determine at times. Clitics and multi-word
tokens are particularly difficult issues: lack of stan-
dardisation means that clitics can occur in various
different forms, some of which are difficult to to-
kenise (e.g. wirstu instead of wirst du). Multi-word
tokens, on the other hand, represent a problem as the
same expression may be sometimes treated as com-
pound (e.g. obgleich), but written separately at other
times (ob gleich). Our tokenisation scheme takes cl-
itics into account, but does not yet deal with multi-
word tokens. This means that whitespace characters
usually act as token boundaries.
125
Genre P Year File name Tokens Genre P Year File name Tokens
DRAM
1 1673 Leonilda 2933
NARR
1 1659 Herkules 2345
2 1749 AlteJungfer 2835 2 1706 SatyrischerRoman 2379
3 1767 Minna 3037 3 1790 AntonReiser 2551
HUMA
1 1667 Ratseburg 2563
NEWS
1 1666 Berlin1 1132
2 1737 Ko?nigstein 2308 2 1735 Berlin 2273
3 1772 Ursprung 2760 3 1786 Wolfenbuettel1 1506
LEGA
1 1673 BergOrdnung 2534
SCIE
1 1672 Prognosticis 2323
2 1707 Reglement 2467 2 1734 Barometer 2438
3 1757 Rostock 2414 3 1775 Chemie 2303
LETT
1 1672 Guericke 2473
SERM
1 1677 LeichSermon 2585
2 1748 Borchward 2557 2 1730 JubelFeste 2523
3 1798 Arndt 2314 3 1770 Gottesdienst 2292
Total number of tokens 57,845
Table 2: GerManC-GS design
Annotation of sentence boundaries is also affected
by the non-standard nature of the data. Punctuation
is not standardised in Early Modern German and
varies considerably across the corpus. For example,
the virgule symbol ?/? was often used in place of
both comma and full-stop, which proves problem-
atic for sentence boundary detection.
3.2 Normalising spelling variants and
lemmatisation
One of the key challenges in working with histor-
ical texts is the large amount of spelling variation
they contain. As most existing NLP tools (such as
POS-taggers or parsers) are tuned to perform well
on modern language data, they are not usually able
to account for variable spelling, resulting in lower
overall performance (Rayson et al, 2007). Like-
wise, modern search engines do not take spelling
variation into account and are thus often unable to
retrieve all occurrences of a given historical search
word. Both issues have been addressed in previ-
ous work through the task of spelling normalisa-
tion. Ernst-Gerlach and Fuhr (2006) and Pilz and
Luther (2009) have created a tool that can gener-
ate variant spellings for historical German to retrieve
relevant instances of a given modern lemma, while
Baron and Rayson (2008) and Jurish (2010) have
implemented tools which normalise spelling vari-
ants in order to achieve better performance of NLP
tools such as POS taggers (by running the tools on
the normalised input). Our annotation of spelling
variants aims to compromise between these two ap-
proaches by allowing for historically accurate lin-
guistic searches, while also aiming to maximise the
performance of automatic annotation tools. We treat
the task of normalising spelling variation as a type
of pre-lemmatisation, where each word token occur-
ring in a text is labelled with a normalised head vari-
ant. As linguistic search requires a historically accu-
rate treatment of spelling variation, our scheme has a
preference for treating two seemingly similar tokens
as separate items on historical grounds (e.g. etwan
vs. etwa). However, the scheme normalises variants
to a modernised form even where the given lexical
item has since died out (e.g. obsolete verbs ending
in -iren are normalised to -ieren), in order to support
automatic tools using morphological strategies such
as suffix probabilities (Schmid, 1994).
Lemmatisation resolves the normalised variant to
a base lexeme in modern form, using Duden2 pre-
reform spelling. With obsolete words, the leading
form in Grimm?s Deutsches Wo?rterbuch3 is taken.
3.3 POS-Tagging
We introduce a modified version of the STTS tagset
(Schiller et al, 1999), the STTS-EMG tagset, to ac-
count for important differences between modern and
Early Modern German (EMG), and to facilitate more
accurate searches. The tagset merges two categories,
as the criteria for distinguishing them are not appli-
cable in EMG (1.), and provides a number of ad-
ditional ones to account for special EMG construc-
tions (2. to 6.):
2http://www.duden.de/
3http://www.dwb.uni-trier.de/
126
1. PIAT (merged with PIDAT): Indefinite deter-
miner (occurring on its own, or in conjunction
with another determiner), as in ?viele solche
Bemerkungen?
2. NA: Adjectives used as nouns, as in ?der
Gesandte?
3. PAVREL: Pronominal adverb used as relative,
as in ?die Puppe, damit sie spielt?
4. PTKREL: Indeclinable relative particle, as in
?die Fa?lle, so aus Schwachheit entstehen?
5. PWAVREL: Interrogative adverb used as
relative, as in ?der Zaun, woru?ber sie springt?
6. PWREL: Interrogative pronoun used as rela-
tive, as in ?etwas, was er sieht?
Around 2.0% (1132) of all tokens in the corpus
have been tagged with one of the above POS cate-
gories, of which the merged PIAT class contains the
majority (657 tokens). The remaining 475 cases oc-
cur as NA (291), or as one of the new relative mark-
ers PWAVREL (69), PWREL (57), PTKREL (38),
and PAVREL (20).
4 Annotation procedure and agreement
In order to produce the gold standard annotations in
GerManC-GS we used the GATE platform, which
facilitates automatic as well as manual annotation
(Cunningham et al 2002). Initially, GATE?s Ger-
man Language plugin4 was used to obtain word to-
kens and sentence boundaries. The output was man-
ually inspected and corrected by one annotator, who
manually added a layer of normalised spelling vari-
ants (NORM). This annotation layer was then used
as input for the TreeTagger (Schmid, 1994), obtain-
ing annotations in terms of lemmas (LEMMA) and
POS tags (POS). All annotations (NORM, LEMMA,
and POS) were subsequently corrected by two an-
notators, and all disagreements were reconciled to
produce the gold standard. Table 3 shows the over-
all agreement for the three annotation types across
GerManC-GS (measured in accuracy).
The agreement values demonstrate that nor-
malised word forms and lemmas are relatively easy
to determine for the annotators, with 96.9% and
95.5% agreement, respectively. POS tags, on the
other, represent more of a challenge with only 91.6%
4http://gate.ac.uk/sale/tao/splitch15.html
NORM LEMMA POS
Agreed tokens
(out of 57,845)
56,052 55,217 52,959
Accuracy (%) 96.9% 95.5% 91.6%
Table 3: Inter-annotator agreement
agreement between two annotators, which is consid-
erably lower than the agreement level reported for
annotating a corpus of modern German using STTS,
at 98.6% (Brants, 2000a). While a more detailed
analysis of the results remains to be carried out, an
initial study shows that POS agreement is lower in
earlier texts (89.3% in Period P1) compared to later
ones (93.1% in P3). It is likely that a substantial
amount of disagreements in the earlier texts are due
to the larger number of unfamiliar word forms and
variants on the one hand, and foreign word tokens
on the other. These represent a problem as from a
modern view point it is not always easy to decide
which words were ?foreign? to a language and which
ones ?native?.
5 Future work
The gold standard corpus described in this paper will
be used to test and adapt modern NLP tools on Early
Modern German data. Initial experiments focus on
utilising the layer of normalised spelling variants
to improve tagger performance, and investigating to
what extent normalisation can be reliably automated
(Jurish, 2010). We further plan to retrain state-of-
the-art POS taggers such as the TreeTagger and TnT
Tagger (Brants, 2000b) on our data.
Finally, we plan to investigate how linguistic an-
notations can be automatically integrated in the TEI-
annotated version of the corpus to produce TEI-
conformant output. Currently, both structural and
linguistic annotations are merged in GATE stand-off
XML format, which, as a consequence, is no longer
TEI-conformant. In the interest of interoperability
and comparative studies between corpora we aim to
contribute towards the development of clearer proce-
dures whereby structural and linguistic annotations
might be merged (Scheible et al, 2010).
127
References
Alistair Baron and Paul Rayson. 2008. VARD 2: A tool
for dealing with spelling variation in historical cor-
pora. Proceedings of the Postgraduate Conference in
Corpus Linguistics, Birmingham, UK.
Douglas Biber and Edward Finegan. 1989. Drift and the
evolution of English style: a history of three genres.
Language 65. 487-517.
Torsten Brants. 2000a. Inter-annotator agreement for
a German newspaper corpus. Second International
Conference on Language Resources and Evaluation
(LREC 2000), Athens, Greece.
Torsten Brants. 2000b. TnT ? a statistical part-of-speech
tagger. Proceedings of the 6th Applied NLP Confer-
ence, ANLP-2000, Seattle, WA.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. Proceedings of
the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Stefanie Dipper. 2010. POS-Tagging of historical lan-
guage data: First experiments in semantic approaches
in Natural Language Processing. Proceedings of
the 10th Conference on Natural Language Processing
(KONVENS-10). Saarbru?cken, Germany. 117-121.
Andrea Ernst-Gerlach and Norbert Fuhr. 2006. Gen-
erating search term variants for text collections with
historic spellings. Proceedings of the 28th European
Conference on Information Retrieval Research (ECIR
2006), London, UK.
Vera Fasshauer. 2011. http://www.indogermanistik.uni-
jena.de/index.php?auswahl=184
Accessed 30/03/2011.
Bryan Jurish. 2010. Comparing canonicalizations of his-
torical German text. Proceedings of the 11th Meeting
of the ACL Special Interest Group on Computational
Morphology and Phonology (SIGMORPHON), Upp-
sala, Sweden. 72-77.
Thomas Pilz and Wolfram Luther. 2009. Automated
support for evidence retrieval in documents with non-
standard orthography. The Fruits of Empirical Lin-
guistics. Sam Featherston and Susanne Winkler (eds.).
211?228.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan
Culpeper, and Nicholas Smith. 2007. Tagging the
Bard: Evaluating the accuracy of a modern POS tagger
on Early Modern English corpora. Proceedings of the
Corpus Linguistics Conference (CL2007), University
of Birmingham, UK.
Silke Scheible, Richard J. Whitt, Martin Durrell, and Paul
Bennett. 2010. Annotating a Historical Corpus of
German: A Case Study. Proceedings of the LREC
2010Workshop on Language Resources and Language
Technology Standards, Valletta, Malta.
Anne Schiller, Simone Teufel, Christine Sto?ckert, and
Christine Thielen. 1999. Guidelines fu?r das Tagging
deutscher Textcorpora mit STTS. Technical Report.
Institut fu?r maschinelle Sprachverarbeitung, Stuttgart.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. International Conference
on NewMethods in Language Processing, Manchester,
UK. 44?49.
128
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 19?23,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Evaluating an ?off-the-shelf? POS-tagger on Early Modern German text
Silke Scheible, Richard J. Whitt, Martin Durrell and Paul Bennett
School of Languages, Linguistics, and Cultures
University of Manchester
Silke.Scheible, Richard.Whitt@manchester.ac.uk
Martin.Durrell, Paul.Bennett@manchester.ac.uk
Abstract
The goal of this study is to evaluate an ?off-
the-shelf? POS-tagger for modern German on
historical data from the Early Modern period
(1650-1800). With no specialised tagger avail-
able for this particular stage of the language,
our findings will be of particular interest to
smaller, humanities-based projects wishing to
add POS annotations to their historical data
but which lack the means or resources to train
a POS tagger themselves. Our study assesses
the effects of spelling variation on the perfor-
mance of the tagger, and investigates to what
extent tagger performance can be improved by
using ?normalised? input, where spelling vari-
ants in the corpus are standardised to a mod-
ern form. Our findings show that adding such
a normalisation layer improves tagger perfor-
mance considerably.
1 Introduction
The work described in this paper is part of a larger
investigation whose goal is to create a representative
corpus of Early Modern German from 1650-1800.
The GerManC corpus, which is due to be completed
this summer, was developed to allow for compara-
tive studies of the development and standardisation
of English and German in the 17th and 18th cen-
turies. In order to facilitate corpus-linguistic inves-
tigations, one of the major goals of the project is
to annotate the corpus with POS tags. However,
no specialised tools are yet available for process-
ing data from this period. The goal of this study is
therefore to evaluate the performance of an ?off-the-
shelf? POS-tagger for modern German on data from
the Early Modern period, in order to assess if mod-
ern tools are suitable for a semi-automatic approach,
and how much manual post-processing work would
be necessary to obtain gold standard POS annota-
tions.
We report on our results of running the TreeTag-
ger (Schmid, 1994) on a subcorpus of GerManC
containing over 50,000 tokens of text annotated with
gold standard POS tags. This subcorpus is the first
resource of its kind for this variant of German, and
due to its complex structure it represents an ideal test
bed for evaluating and adapting existing NLP tools
on data from the Early Modern period. The study
described in this paper represents a first step towards
this goal. Furthermore, as spelling variants in our
corpus have been manually normalised to a modern
standard, this paper also aims to explore the extent
to which tagger performance is affected by spelling
variation, and to what degree performance can be
improved by using ?normalised? input. Our findings
promise to be of considerable interest to other cur-
rent corpus-based projects of earlier periods of Ger-
man (Jurish, 2010; Fasshauer, 2011; Dipper, 2010).
Before presenting the results in Section 4, we de-
scribe the corpus design (Section 2), and the prepro-
cessing steps necessary to create the gold standard
annotations, including adaptations to the POS tagset
(Section 3).
2 Corpus design
In order to be as representative of Early Modern Ger-
man as possible, the GerManC corpus design con-
siders three different levels. First, the corpus in-
cludes a range of text types: four orally-oriented
19
genres (dramas, newspapers, letters, and sermons),
and four print-oriented ones (narrative prose, and
humanities, scientific, and legal texts). Secondly, in
order to enable historical developments to be traced,
the period is divided into three fifty year sections
(1650-1700, 1700-1750, and 1750-1800). Finally,
the corpus also aims to be representative with re-
spect to region, including five broad areas: North
German, West Central, East Central, West Upper
(including Switzerland), and East Upper German
(including Austria). Three extracts of around 2000
words were selected per genre, period, and region,
yielding a corpus size of nearly a million words.
The experiments described in this paper were car-
ried out on a manually annotated gold standard sub-
corpus of GerManC, GerManC-GS. The subcorpus
was developed to enable an assessment of the suit-
ability of existing NLP tools on historical data, with
a view to adapting them to improve their perfor-
mance. For this reason, GerManC-GS aims to be as
representative of the main corpus as possible. How-
ever, to remain manageable in terms of annotation
times and cost, the subcorpus only considers two
of the three corpus variables, ?genre? and ?time?, as
they alone were found to display as much if not more
variation than ?region?. GerManC-GS thus includes
texts from the North German region, with one sam-
ple file per genre and time period. The corpus con-
tains 57,845 tokens in total, and was annotated with
gold standard POS tags, lemmas, and normalised
word forms (Scheible et al, to appear).
3 Creating the gold standard annotations
This section provides an overview of the preprocess-
ing work necessary to obtain the gold standard an-
notations in GerManC-GS. We used the GATE plat-
form to produce the initial annotations, which facil-
itates automatic as well as manual annotation (Cun-
ningham et al, 2002). First, GATE?s German Lan-
guage plugin1 was used to obtain word tokens and
sentence boundaries. The output was manually in-
spected and corrected by one annotator, who fur-
ther added a layer of normalised spelling variants.
This annotation layer was then used as input for the
TreeTagger (Schmid, 1994), obtaining annotations
in terms of POS tags and lemmas. All annotations
1http://gate.ac.uk/sale/tao/splitch15.html
were subsequently corrected by two annotators, and
disagreements were reconciled to produce the gold
standard.
3.1 Tokenisation
As German orthography was not yet codified in the
Early Modern period, a number of specific deci-
sions had to be made in respect of tokenisation. For
example, clitics can occur in various non-standard
forms. To allow for accurate POS tagging, clitics
should be tokenised as separate items, similar to the
negative particle n?t in can?t in English, which is
conventionally tokenised as ca|n?t. A case in point
is hastu, a clitic version of hast du (?have you?),
which we tokenise as has|tu. Furthermore, Ger-
man ?to-infinitive? verb forms are often directly ap-
pended to the infinitival marker zu without interven-
ing whitespace (e.g. zugehen instead of zu gehen,
?to go?). Such cases are tokenised as separate forms
(zu|gehen) to allow for their accurate tagging as
zu/PTKZU gehen/VVINF.
A further problem can be found in multi-word
tokens, where the same expression is sometimes
treated as a compound (e.g. obgleich), but at other
times written separately (ob gleich). Such cases rep-
resent a problem for POS-tagging as the variants
have to be treated differently even though their func-
tion in the sentence is the same. Our tokenisation
scheme deals with these in a similar way to nor-
mal conjunctions consisting of two words, where
the most suitable tags are assigned to each token
(e.g. als/KOKOM wenn/KOUS). Thus, the com-
pound obgleich is tagged KOUS, while the multi-
word variant ob gleich is tagged as ob/KOUS gle-
ich/ADV.
3.2 Normalising spelling variants
All spelling variants in GerManC-GS were nor-
malised to a modern standard. We view the task
of normalising spelling variation as a type of pre-
lemmatisation, where each word token occurring
in a text is labelled with a normalised head vari-
ant. As linguistic searches require a historically ac-
curate treatment of spelling variation, our scheme
has a preference for treating two seemingly simi-
lar tokens as separate items on historical grounds
(e.g. etwan vs. etwa). On the other hand, the
scheme normalises variants to a modernised form
20
even where the given lexical item has since died out
(e.g. obsolete verbs ending in -iren are normalised
to -ieren), in order to support automatic tools using
morphological strategies such as suffix probabilities
(Schmid, 1994). Inter-annotator agreement for an-
notating spelling variation was 96.9%, which indi-
cates that normalisation is a relatively easy task.
Figure 1 shows the proportion of normalised word
tokens in the individual corpus files plotted against
time. The graph clearly shows a decline of spelling
variants over time: while the earlier texts contain 35-
40% of normalised tokens, the proportion is lower
in later texts (11.3% in 1790, and 5.4% in 1798).
This suggests that by the end of the period (1800)
codification of the German language was already at
an advanced stage.
Figure 1: Proportion of normalised tokens (plotted
against time)
3.3 Adapting the POS tagset (STTS)
To account for important differences between mod-
ern and Early Modern German (EMG), and to facil-
itate more accurate searches, we adapted the STTS
tagset (Schiller et al, 1999). The STTS-EMG tagset
merges two categories, as the criteria for distinguish-
ing them are not applicable in EMG (1.), and pro-
vides a number of additional ones to account for spe-
cial EMG constructions (2. to 6.):
1. PIAT (merged with PIDAT): Indefinite de-
terminer, as in ?viele solche Bemerkungen?
(?many such remarks?)
2. NA: Adjectives used as nouns, as in ?der
Gesandte? (?the ambassador?)
3. PAVREL: Pronominal adverb used as relative,
as in ?die Puppe, damit sie spielt? (?the doll
with which she plays?)
4. PTKREL: Indeclinable relative particle, as in
?die Fa?lle, so aus Schwachheit entstehen? (?the
cases which arise from weakness?)
5. PWAVREL: Interrogative adverb used as
relative, as in ?der Zaun, woru?ber sie springt?
(?the fence over which she jumps?)
6. PWREL: Interrogative pronoun used as rel-
ative, as in ?etwas, was er sieht? (?something
which he sees?)
Around 2.0% (1132) of all tokens in the corpus
were tagged with one of the above POS categories.
Inter-annotator agreement for the POS tagging task
was 91.6%.
4 ?Off-the-shelf? tagger evaluation on
Early Modern German data
The evaluation described in this section aims to
complement the findings of Rayson et al (2007) for
Early Modern English, and a recent study by Dip-
per (2010), in which the TreeTagger is applied to a
corpus of texts from Middle High German (MHG)
- i.e. a period earlier than ours, from 1050-1350.
Both studies report considerable improvement of
POS-tagging accuracy on normalised data. How-
ever, unlike Dipper (2010), whose experiments in-
volve retraining the TreeTagger on a modified ver-
sion of STTS, our experiments assess the ?off-the-
shelf? performance of the modern tagger on histor-
ical data. We further explore the question of what
effect spelling variation has on the performance of a
tagger, and what improvement can be achieved when
running the tool on normalised data.
Table 1 shows the results of running the Tree-
Tagger on the original data vs. normalised data in
our corpus using the parameter file for modern Ger-
man supplied with the tagger2. The results show that
while overall accuracy for running the tagger on the
original input is relatively low at 69.6%, using the
normalised tokens as input results in an overall im-
provement of 10% (79.7%).
2http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
21
O N
Accuracy 69.6% 79.7%
Table 1: TreeTagger accuracy on original (O) vs. nor-
malised (N) input
However, improvement through normalisation is
not distributed evenly across the corpus. Figure 2
shows the performance curves of using TreeTagger
on original (O) and normalised (N) input plotted
against publication date. While both curves grad-
ually rise over time, the improvement curve (mea-
sured as difference in accuracy between N and O)
diminishes, a direct result of spelling variation be-
ing more prominent in earlier texts (cf. Figure 1).
Figure 2: Tagger performance plotted against publication
date
Compared with the performance of the TreeTag-
ger on modern data (ca. 97%; Schmid, (1995)), the
current results seem relatively low. However, two is-
sues should be taken into account when interpreting
these findings: First, the modern accuracy figures
result from an evaluation of the tagger on the text
type it was developed on (newspaper text), while
GerManC-GS includes a variety of genres, which
is bound to result in lower performance. Secondly,
inter-annotator agreement was also found to be con-
siderably lower in the present task (91.6%) than in
one reported for modern German (98.6%; Brants,
2000a). This is likely to be due to the large number
of unfamiliar word forms and variants in the corpus,
which represent a problem for human annotators.
Finally, Figure 3 provides a more detailed
overview of the effects of spelling variation on POS
tagger performance. Of 12,744 normalised tokens in
the corpus, almost half (5981; 47%) are only tagged
correctly when using the normalised variants as in-
put. Using the original word form as input results
in a false POS tag in these cases. Overall, this ac-
counts for an improvement of around 10.3% (5981
out of 57,845 tokens in the corpus). However, 32%
(4119) of normalised tokens are tagged correctly us-
ing both N and O input, while 18% (2339) of to-
kens are tagged incorrectly using both types of input.
This means that for 50% of all annotated spelling
variants, normalisation has no effect on POS tagger
performance. In a minority of cases (305; 3%) nor-
malisation has a negative effect on tagger accuracy.
Figure 3: Effect of using original (O)/normalised (N) in-
put on tagger accuracy for normalised tokens (+: cor-
rectly tagged; -: incorrectly tagged)
5 Conclusion and future work
The results of our study show that using an ?off-the
shelf? German POS tagger on data from the Early
Modern period achieves reasonable results (69.6%
on average), but requires a substantial amount of
manual post-editing. We further demonstrated that
adding a normalisation layer can improve results by
10%. However, using the current manual normalisa-
tion scheme only half of all annotations carried out
have a positive effect on tagger performance. In fu-
ture work we plan to investigate if the scheme can
be adapted to account for more cases, and to what
extent normalisation can be reliably automated (Jur-
ish, 2010). Finally, we plan to retrain state-of-the-art
POS taggers such as the TreeTagger and TnT Tagger
(Brants, 2000b) on our data and compare the results
to the findings of this study.
22
References
Torsten Brants. 2000a. Inter-annotator agreement for
a German newspaper corpus. Second International
Conference on Language Resources and Evaluation
(LREC 2000), Athens, Greece.
Torsten Brants. 2000b. TnT ? a statistical part-of-speech
tagger. Proceedings of the 6th Applied NLP Confer-
ence, ANLP-2000, Seattle, WA.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. Proceedings of
the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Stefanie Dipper. 2010. POS-Tagging of historical lan-
guage data: First experiments in semantic approaches
in Natural Language Processing. Proceedings of
the 10th Conference on Natural Language Processing
(KONVENS-10). Saarbru?cken, Germany. 117-121.
Vera Fasshauer. 2011. http://www.indogermanistik.uni-
jena.de/index.php?auswahl=184
Accessed 30/03/2011.
Bryan Jurish. 2010. Comparing canonicalizations of his-
torical German text. Proceedings of the 11th Meeting
of the ACL Special Interest Group on Computational
Morphology and Phonology (SIGMORPHON), Upp-
sala, Sweden. 72-77.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan
Culpeper, and Nicholas Smith. 2007. Tagging the
Bard: Evaluating the accuracy of a modern POS tagger
on Early Modern English corpora. Proceedings of the
Corpus Linguistics Conference (CL2007), University
of Birmingham, UK.
Silke Scheible, Richard J. Whitt, Martin Durrell, and Paul
Bennett. To appear. A Gold Standard Corpus of Early
Modern German. Proceedings of the Fifth Linguistic
Annotation Workshop (LAW V), Portland, Oregon.
Anne Schiller, Simone Teufel, Christine Sto?ckert, and
Christine Thielen. 1999. Guidelines fu?r das Tagging
deutscher Textcorpora mit STTS. Technical Report.
Institut fu?r maschinelle Sprachverarbeitung, Stuttgart.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. International Conference
on NewMethods in Language Processing, Manchester,
UK. 44?49.
Helmut Schmid. 1995. Improvements in Part-of-Speech
Tagging with an Application to German. Proceedings
of the ACL SIGDAT-Workshop. 47?50.
23
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 32?41,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
The (Un)expected Effects of Applying Standard Cleansing Models to
Human Ratings on Compositionality
Stephen Roller?? Sabine Schulte im Walde ? Silke Scheible ?
?Department of Computer Science ?Institut fu?r Maschinelle Sprachverarbeitung
The University of Texas at Austin Universita?t Stuttgart
roller@cs.utexas.edu {schulte,scheible}@ims.uni-stuttgart.de
Abstract
Human ratings are an important source for
evaluating computational models that predict
compositionality, but like many data sets of
human semantic judgements, are often fraught
with uncertainty and noise. However, despite
their importance, to our knowledge there has
been no extensive look at the effects of cleans-
ing methods on human rating data. This paper
assesses two standard cleansing approaches on
two sets of compositionality ratings for Ger-
man noun-noun compounds, in their ability
to produce compositionality ratings of higher
consistency, while reducing data quantity. We
find (i) that our ratings are highly robust
against aggressive filtering; (ii) Z-score filter-
ing fails to detect unreliable item ratings; and
(iii) Minimum Subject Agreement is highly
effective at detecting unreliable subjects.
1 Introduction
Compounds have long been a reoccurring focus of
attention within theoretical, cognitive, and compu-
tational linguistics. Recent manifestations of inter-
est in compounds include the Handbook of Com-
pounding (Lieber and Stekauer, 2009) on theoretical
perspectives, and a series of workshops1 and spe-
cial journal issues with respect to the computational
perspective (Journal of Computer Speech and Lan-
guage, 2005; Language Resources and Evaluation,
2010; ACM Transactions on Speech and Language
Processing, to appear). Some work has focused
on modeling meaning and compositionality for spe-
cific classes, such as particle verbs (McCarthy et al,
1www.multiword.sourceforge.net
2003; Bannard, 2005; Cook and Stevenson, 2006);
adjective-noun combinations (Baroni and Zampar-
elli, 2010; Boleda et al, 2013); and noun-noun com-
pounds (Reddy et al, 2011b; Reddy et al, 2011a).
Others have aimed at predicting the compositional-
ity of phrases and sentences of arbitrary type and
length, either by focusing on the learning approach
(Socher et al, 2011); by integrating symbolic mod-
els into distributional models (Coecke et al, 2011;
Grefenstette et al, 2013); or by exploring the arith-
metic operations to predict compositionality by the
meaning of the parts (Widdows, 2008; Mitchell and
Lapata, 2010).
An important resource in evaluating composition-
ality has been human compositionality ratings, in
which human subjects are asked to rate the degree to
which a compound is transparent or opaque. Trans-
parent compounds, such as raincoat, have a meaning
which is an obvious combination of its constituents,
e.g., a raincoat is a coat against the rain. Opaque
compounds, such as hot dog, have little or no rela-
tion to one or more of their constituents: a hot dog
need not be hot, nor is it (hopefully) made of dog.
Other words, such as ladybug, are transparent with
respect to just one constituent. As many words do
not fall clearly into one category or the other, sub-
jects are typically asked to rate the compositionality
of words or phrases on a scale, and the mean of sev-
eral judgements is taken as the gold standard.
Like many data sets of human judgements, com-
positionality ratings can be fraught with large quan-
tities of uncertainty and noise. For example, partici-
pants typically agree on items that are clearly trans-
parent or opaque, but will often disagree about the
32
gray areas in between. Such uncertainty represents
an inherent part of the semantic task and is the major
reason for using the mean ratings of many subjects.
Other types of noise, however, are undesirable,
and should be eliminated. In particular, we wish
to examine two types of potential noise in our data.
The first type of noise (Type I noise: uncertainty),
comes from when a subject is unfamiliar or un-
certain about particular words, resulting in sporad-
ically poor judgements. The second type of noise
(Type II noise: unreliability), occurs when a sub-
ject is consistently unreliable or uncooperative. This
may happen if the subject misunderstands the task,
or if a subject simply wishes to complete the task
as quickly as possible. Judgements collected via
crowdsourcing are especially prone to this second
kind of noise, when compared to traditional pen-
and-paper experiments, since participants aim to
maximize their hourly wage.2
In this paper, we apply two standard cleans-
ing methods (Ben-Gal, 2005; Maletic and Marcus,
2010), that have been used on similar rating data be-
fore (Reddy et al, 2011b), on two data sets of com-
positionality ratings of German noun-noun com-
pounds. We aim to address two main points. The
first is to assess the cleansing approaches in their
ability to produce compositionality ratings of higher
quality and consistency, while facing a reduction of
data mass in the cleansing process. In particular, we
look at the effects of removing outlier judgements
resulting from uncertainty (Type I noise) and drop-
ping unreliable subjects (Type II noise). The second
issue is to assess the overall reliability of our two
rating data sets: Are they clean enough to be used
as gold standard models in computational linguistics
approaches?
2 Compositionality Ratings
Our focus of interest is on German noun-noun com-
pounds (see Fleischer and Barz (2012) for a detailed
overview), such as Ahornblatt ?maple leaf? and
Feuerwerk ?fireworks?, and Obstkuchen ?fruit cake?
where both the head and the modifier are nouns.
We rely on a subset of 244 noun-noun compounds
2See Callison-Burch and Dredze (2010) for a collection of
papers on data collected with AMT. While the individual ap-
proaches deal with noise in individual ways, there is no general
approach to clean crowdsourcing data.
collected by von der Heide and Borgwaldt (2009),
who created a set of 450 concrete, depictable Ger-
man noun compounds according to four composi-
tionality classes (transparent+transparent, transpar-
ent+opaque, opaque+transparent, opaque+opaque).
We are interested in the degrees of composition-
ality of the German noun-noun compounds, i.e., the
relation between the meaning of the whole com-
pound (e.g., Feuerwerk) and the meaning of its con-
stituents (e.g., Feuer ?fire? and Werk ?opus?). We
work with two data sets of compositionality rat-
ings for the compounds. The first data set, the
individual compositionality ratings, consists of
participants rating the compositionality of a com-
pound with respect to each of the individual con-
stituents. These judgements were collected within
a traditional controlled, pen-and-paper setting. For
each compound-constituent pair, 30 native German
speakers rated the compositionality of the com-
pound with respect to its constituent on a scale
from 1 (opaque/non-compositional) to 7 (transpar-
ent/compositional). The subjects were allowed to
omit ratings for unfamiliar words, but very few did;
of the 14,640 possible ratings judgements, only 111
were left blank. Table 1 gives several examples of
such ratings. We can see that Fliegenpilz ?toadstool?
is an example of a very opaque (non-compositional)
word with respect to Fliege ?housefly/bow tie?; it has
little to do with either houseflies or bow ties. On
the other hand Teetasse ?teacup? is highly composi-
tional: it is a Tasse ?cup? intended for Tee ?tea?.
The second data set, the whole compositional-
ity ratings consists of participants giving a single
rating for the entire compound. These ratings, pre-
viously unpublished, reflect a very different view
of the same compounds. Rather than rating com-
pounds with respect to their constituents, subjects
were asked to give a single rating for the entire com-
pound using the same 1-7 scale as before. The rat-
ings were collected via Amazon Mechanical Turk
(AMT). The data was controlled for spammers by
removing subjects who failed to identify a number
of fake words. Subjects who rated less than 10 com-
pounds or had a low AMT reputation were also re-
moved. The resulting data represents 150 differ-
ent subjects with roughly 30 ratings per compound.
Most participants rated only a few dozen items. We
can see examples of these ratings in Table 2.
33
Compound W.R.T. Subject 1 Subject 2 Subject 3 Subject 4 Mean Comb.
Fliegenpilz ?toadstool? Fliege ?housefly/bow tie? 3 1 1 2 1.75
3.37
Fliegenpilz ?toadstool? Pilz ?mushroom? 5 7 7 7 6.50
Sonnenblume ?sunflower? Sonne ?sun? 4 3 1 2 2.50
4.11
Sonnenblume ?sunflower? Blume ?flower? 7 7 7 6 6.75
Teetasse ?teacup? Tee ?tea? 6 6 4 2 4.50
4.50
Teetasse ?teacup? Tasse ?cup? 7 6 4 1 4.50
Table 1: Sample compositionality ratings for three compounds with respect to their constituents. We list the mean rat-
ing for only these 4 subjects to facilitate examples. The Combined column is the geometric mean of both constituents.
Compound Subject 1 Subject 2 Subject 3 Subject 4 Mean
Fliegenpilz ?toadstool? - 2 1 2 2.67
Sonnenblume ?sunflower? 3 3 1 2 2.75
Teetasse ?teacup? 7 7 7 6 6.75
Table 2: Example whole compositionality ratings for three compounds. Note that Subject 1 chose not to rate Fliegen-
pilz, so the mean is computed using only the three available judgements.
3 Methodology
In order to check on the reliability of composition-
ality judgements in general terms as well as with re-
gard to our two specific collections, we applied two
standard cleansing approaches3 to our rating data: Z-
score filtering is a method for filtering Type I noise,
such as random guesses made by individuals when a
word is unfamiliar. Minimum Subject Agreement is
a method for filtering out Type II noise, such as sub-
jects who seem to misunderstand the rating task or
rarely agree with the rest of the population. We then
evaluated the original vs. cleaned data by one intrin-
sic and one extrinsic task. Section 3.1 presents the
two evaluations and the unadulterated, baseline mea-
sures for our experiments. Sections 3.2.1 and 3.2.2
describe the cleansing experiments and results.
3.1 Evaluations and Baselines
For evaluating the cleansing methods, we propose
two metrics, an intrinsic and an extrinsic measure.
3.1.1 Intrinsic Evaluation:
Consistency between Rating Data Sets
The intrinsic evaluation measures the consistency
between our two ratings sets individual and whole.
Assuming that the compositionality ratings for a
compound depend heavily on both constituents, we
expect a strong correlation between the two data
sets. For a compound to be rated transparent as a
3See Ben-Gal (2005) or Maletic and Marcus (2010) for
overviews of standard cleansing approaches.
whole, it should be transparent with respect to both
of its constituents. Compounds which are highly
transparent with respect to only one of their con-
stituents should be penalized appropriately.
In order to compute a correlation between the
whole ratings (which consist of one average rating
per compound) and the individual ratings (which
consist of two average ratings per compound, one for
each constituent), we need to combine the individual
ratings to arrive at a single value. We use the geo-
metric mean to combine the ratings, which is effec-
tively identical to the multiplicative methods in Wid-
dows (2008), Mitchell and Lapata (2010) and Reddy
et al (2011b). 4 For example, using our means listed
in Table 1, we may compute the combined rating for
Sonnenblume as
?
6.75 ? 2.50 ? 4.11. These com-
bined ratings are computed for all compounds, as
listed in the ?Comb.? column of Table 1. We then
compute our consistency measure as the Spearman?s
? rank correlation between these combined individ-
ual ratings with the whole ratings (?Mean? in Table
2). The original, unadulterated data sets have a con-
sistency measure of 0.786, indicating that, despite
the very different collection methodologies, the two
ratings sets largely agree.
3.1.2 Extrinsic Evaluation:
Correlation with Association Norms
The extrinsic evaluation compares the consistency
4We also tried the arithmetic mean, but the multiplicative
method always performs better.
34
Word Example Associations
Fliegenpilz ?toadstool? giftig ?poisonous?, rot ?red?, Wald ?forest?
Fliege ?housefly/bow tie? nervig ?annoying?, summen ?to buzz?, Insekt ?insect?
Pilz ?mushroom? Wald ?forest?, giftig ?poisonous?, sammeln ?to gather?
Sonnenblume ?sunflower? gelb ?yellow?, Sommer ?summer?, Kerne ?seeds?
Sonne ?sun? Sommer ?summer?, warm ?warm?, hell ?bright?
Blume ?flower? Wiese ?meadow?, Duft ?smell?, Rose ?rose?
Table 3: Example association norms for two German compounds and their constituents.
between our two rating sets individual and whole
with evidence from a large collection of associa-
tion norms. Association norms have a long tradition
in psycholinguistic research to investigate semantic
memory, making use of the implicit notion that asso-
ciates reflect meaning components of words (Deese,
1965; Miller, 1969; Clark, 1971; Nelson et al, 1998;
Nelson et al, 2000; McNamara, 2005; de Deyne and
Storms, 2008). They are collected by presenting a
stimulus word to a subject and collecting the first
words that come to mind.
We rely on association norms that were collected
for our compounds and constituents via both a large
scale web experiment and Amazon Mechanical Turk
(Schulte im Walde et al, 2012) (unpublished). The
resulting combined data set contains 85,049/34,560
stimulus-association tokens/types for the compound
and constituent stimuli. Table 3 gives examples of
associations from the data set for some stimuli.
The guiding intuition behind comparing our rat-
ing data sets with association norms is that a com-
pound which is compositional with respect to a con-
stituent should have similar associations as its con-
stituent (Schulte im Walde et al, 2012).
To measure the correlation of the rating data with
the association norms, we first compute the Jac-
card similarity that measures the overlap in two sets,
ranging from 0 (perfectly dissimilar) to 1 (perfectly
similar). The Jaccard is defined for two sets, A and
B, as
J(A,B) =
|A ?B|
|A ?B|
.
For example, we can use Table 3 to compute the
Jaccard similarity between Sonnenblume and Sonne:
|{Sommer}|
|{gelb, Sommer,Kerne,warm, hell}|
= 0.20.
After computing the Jaccard similarity between
all compounds and constituents across the associ-
ation norms, we correlate this association overlap
with the average individual ratings (i.e., column
?Mean? in Table 1) using Spearman?s ?. This cor-
relation ?Assoc Norm (Indiv)? reaches ? = 0.638
for our original data. We also compute a combined
Jaccard similarity using the geometric mean, e.g.
?
J(Fliegenpilz, F liege) ? J(Fliegenpilz, P ilz),
and calculate Spearman?s ? with the whole ratings
(i.e., column ?Mean? in Table 2). This correlation
?Assoc Norm (Whole)? reaches ? = 0.469 for our
original data.
3.2 Data Cleansing
We applied the two standard cleansing approaches,
Z-score Filtering and Minimum Subject Agreement,
to our rating data, and evaluated the results.
3.2.1 Z-score Filtering
Z-score filtering is a method to filter out Type I
noise, such as random guesses made by individu-
als when a word is unfamiliar. It makes the sim-
ple assumption that each item?s ratings should be
roughly normally distributed around the ?true? rat-
ing of the item, and throws out all outliers which
are more than z? standard deviations from the item?s
mean. With regard to our compositionality ratings,
for each item i (i.e., a compound in the whole data,
or a compound?constituent pair in the individual
data) we compute the mean x?i and standard devia-
tion ?i of the ratings for the given item. We then
remove all values from xi where
|xi ? x?i| > ?iz
?,
with the parameter z? indicating the maximum al-
lowed Z-score of the item?s ratings. For example, if
a particular item has ratings of xi = (1, 2, 1, 6, 1, 1),
then the mean x?i = 2 and the standard deviation
35
ll
lll
l
llllllll
lllll
l
llllll
l
l
l
ll
lll
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.80
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l l lCleaned Indiv Cleaned Whole Cleaned Indiv & Whole
(a) Intrinsic Evaluation of Z?score Filtering
l
ll
ll
lllllllll
l ll
lllllllllll
0.40
0.45
0.50
0.55
0.60
0.65
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Cor
rela
tion
 wit
h A
sso
ciat
ion
 No
rm 
Ove
rlap
(Spe
arm
an
's r
ho)
l lAssoc Norms (Indiv) Assoc Norms (Whole)
(b) Extrinsic Evaluation of Z?score Filtering
Figure 1: Intrinsic and Extrinsic evaluation of Z-score fil-
tering. We see that Z-score filtering makes a minimal dif-
ference when filtering is strict, and is slightly detrimental
with more aggressive filtering.
?i = 2. If we use a z? of 1, then we would filter rat-
ings outside of the range [2? 1 ? 2, 2 + 1 ? 2]. Thus,
the resulting new xi would be (1, 2, 1, 1, 1) and the
new mean x?i would be 1.2.
Filtering Outliers Figure 1a shows the results for
the intrinsic evaluation of Z-score filtering. The
solid black line represents the consistency of the fil-
tered individual ratings with the unadulterated whole
ratings. The dotted orange line shows the consis-
tency of the filtered whole ratings with the unadul-
terated individual ratings, and the dashed purple line
shows the consistency between the data sets when
both are filtered. In comparison, the consistency be-
tween the unadulterated data sets is provided by the
horizontal gray line. We see that Z-score filtering
overall has a minimal effect on the consistency of
l
l
l
ll
lllllllll
ll
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Fra
ctio
n D
ata
 Re
tain
ed
l l lIndiv Whole Both
Data Retention with Z?score Filtering
Figure 2: The data retention rate of Z-score filtering. Data
retention drops rapidly with aggressive filtering.
the two data sets. It provides very small improve-
ments with high Z-scores, but is slightly detrimental
at more aggressive levels.
Figure 1b shows the effects of Z-score filtering
with our extrinsic evaluation of correlation with as-
sociation norms. At all levels of filtering, we see that
correlation with association norms remains mostly
independent of the level of filtering.
An important factor to consider when evaluating
these results is the amount of data dropped at each
of the filtering levels. Figure 2 shows the data re-
tention rate for the different data sets and levels. As
expected, more aggressive filtering results in a sub-
stantially lower data retention rate. Comparing this
curve to the consistency ratings gives a clear picture:
the decrease in consistency is probably mostly due to
the decrease in available data but not due to filtering
outliers. As such, we believe that Z-score filtering
does not substantially improve data quality, but may
be safely applied with a conservative maximum al-
lowed Z-score.
Filtering Artificial Noise Z-score filtering has lit-
tle impact on the consistency of the data, but we
would like to determine whether this is due because
our data being very clean, so the filtering does not
apply, or Z-score filtering not being able to detect the
Type I noise. To test these two possibilities, we arti-
ficially introduce noise into our data sets: we create
100 variations of the original ratings matrices, where
with 0.25 probability, each entry in the matrix was
36
l l l l l l l ll ll ll l l ll l
l
0.65
0.70
0.75
0.80
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l lCleaned Indiv Noisy Indiv
(a) Removing Indiv Judgements with Uniform Noise
l ll ll ll ll ll ll ll l ll l
l ll ll
l
l
0.65
0.70
0.75
0.80
N/A 4.0 3.0 2.0 1.0Maximum Z?score of Judgements
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l lCleaned Whole Noisy Whole
(b) Removing Whole Judgements with Uniform Noise
Figure 3: Ability of Z-score filtering at removing artificial noise added in the (a) individual and (b) whole judgements.
The orange lines represent the consistency of the data with the noise, but no filtering, while the black lines indicate
the consistency after Z-score filtering. Z-score filtering appears to be unable to find uniform random noise in either
situation.
l l l l l l
l
l
l
l l
l l
l
l
l
l l l
l
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.80
0.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l l lCleaned Indiv Cleaned Whole Cleaned Indiv & Whole
(a) Intrinsic Evaluation of MSA Filtering
l l l l l l l l l
l
l
l l l l l l l l l l l
0.40
0.45
0.50
0.55
0.60
0.65
0.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)
Cor
rela
tion
 wit
h A
sso
ciat
ion
 No
rm 
Ove
rlap
(Spe
arm
an
's r
ho)
l lAssoc Norms (Indiv) Assoc Norms (Whole)
(b) Extrinsic Evaluation of MSA Filtering
Figure 4: Intrinsic and Extrinsic evaluation of Minimum Subject Agreement filtering. We see virtually no gains using
subject filtering, and the individual judgements are quite hindered by aggressive filtering.
37
replaced with a uniform random integer between 1
and 7. That is, roughly 1 in 4 of the entries in the
original matrix were replaced with random, uniform
noise. We then apply Z-score filtering on each of
these noisy matrices and report their average con-
sistency with its companion, unadulterated matrix.
That is, we add noise to the individual ratings ma-
trix, and then compare its consistency with the orig-
inal whole ratings matrix, and vice versa. Thus if we
are able to detect and remove the artificial noise, we
should see higher consistencies in the filtered matrix
over the noisy matrix.
Figure 3 shows the results of adding noise to the
original data sets. The lines indicate the averages
over all 100 matrix variations, while the shaded ar-
eas represent the 95% confidence intervals. Surpris-
ingly, even though 1/4 entries in the matrix were re-
placed with random values, the decrease in consis-
tency is relatively low in both settings. This likely
indicates our data already has high variance. Fur-
thermore, in both settings, we do not see any in-
crease in consistency from Z-score filtering. We
must conclude that Z-score appears ineffective at re-
moving Type I noise in compositionality ratings.
We also tried introducing artificial noise in a sec-
ond way, where judgements were not replaced with a
uniformly random value, but a fixed offset of either
+3 or -3, e.g., 4?s became either 1?s or 7?s. Again,
the values were changed with probability of 0.25.
The results were remarkably similar, so we do not
include them here.
3.2.2 Minimum Subject Agreement
Minimum Subject Agreement is a method for fil-
tering out subjects who seem to misunderstand the
rating task or rarely agree with the rest of the pop-
ulation. For each subject in our data, we compute
the average ratings for each item excluding the sub-
ject. The subject?s rank agreement with the exclu-
sive averages is computed using Spearman?s ?. We
can then remove subjects whose rank agreement is
below a threshold, or remove the n subjects with the
lowest rank agreement.
Filtering Unreliable Subjects Figure 4 shows the
effect of subject filtering on our intrinsic and extrin-
sic evaluations. We can see that mandating mini-
mum subject agreement has a strong, negative im-
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
l
lll
llllllll
lllll
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 5 10 15 20 25Number of Subjects Randomized/Removed
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l lCleaned Indiv Noisy Indiv
(a) Removing Indiv Subjects with Artificial Noise
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
llllllllll
lllllll
0.65
0.70
0.75
0.80
0 5 10 15 20 25Number of Subjects Randomized/Removed
Con
sist
enc
y b
etw
ee
n r
atin
gs
(Spe
arm
an
's r
ho)
l lCleaned Whole Noisy Whole
(b) Removing Whole Subjects with Artificial Noise
Figure 5: Ability of subject filtering at detecting highly
deviant subjects. We see that artificial noise strongly
hurts the quality of the individual judgements, while hav-
ing a much weaker effect on the whole judgements. The
process is effective at identifying deviants in both set-
tings.
pact on the individual ratings after a certain thresh-
old is reached, but virtually no effect on the whole
ratings. When we consider the corresponding data
retention curve in Figure 6, the result is not surpris-
ing: the dip in performance for the individual ratings
comes with a data retention rate of roughly 25%. In
this way, it?s actually surprising that it does so well:
with only 25% of the original data, consistency is
only 5 points lower. The effects are more dramatic
in the extrinsic evaluation.
On the other hand, subject filtering has almost no
effect on the whole ratings. This is not surprising, as
most subjects have only rated at most a few dozen
items, so removing subjects corresponds to a smaller
reduction in data, as seen in Figure 6. Furthermore,
the subjects with the highest deviations tend to be
38
l l l l l l l l
l
l
l
l l l l l l l l l l
l
l l l
l
l
l
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)
Fra
ctio
n D
ata
 Re
tain
ed
l l lIndiv Whole Both
Data Retention with MSA Filtering
Figure 6: Data retention rates for various levels of mini-
mum subject agreement. The whole ratings remain rela-
tively untouched by mandating high levels of agreement,
but individual ratings are aggressively filtered after a sin-
gle breaking point.
the subjects who rated the fewest items since their
agreement is more sensitive to small changes. As
such, the subjects removed tend to be the subjects
with the least influence on the data set.
Removing Artificial Subject-level Noise To test
the hypothesis that minimum subject agreement fil-
tering is effective at removing Type II noise, we in-
troduce artificial noise at the subject level. For these
experiments, we create 100 variations of our ma-
trices where n subjects have all of their ratings re-
placed with random, uniform ratings. We then apply
subject-level filtering where we remove the n sub-
jects who agree least with the overall averages.
Figure 5a shows the ability of detecting Type II
noise in the individual ratings. The results are un-
surprising, but encouraging. We see that increasing
the number of randomized subjects rapidly lowers
the consistency with the whole ratings. However, the
cleaned whole ratings matrix maintains a fairly high
consistency, indicating that we are doing a nearly
perfect job at identifying the noisy individuals.
Figure 5b shows the ability of detecting Type II
noise in the whole ratings. Again, we see that the
cleaned noisy ratings have a higher consistency than
the noisy ratings, indicating the efficacy of subject
agreement filtering at detecting unreliable subjects.
The effect is less pronounced in the whole ratings
than the individual ratings due to the lower propor-
tion of subjects being randomized.
Identification of Spammers Removing subjects
with the least agreement lends itself to another sort
of evaluation: predicting subjects rejected during
data collection. As discussed in Section 2, subjects
who failed to identify the fake words or had an over-
all low reputability were filtered from the data before
any analysis. To test the quality of minimum sub-
ject agreement, we reconstructed the data set where
these previously rejected users were included, rather
than removed. Subjects who rated fewer than 10
items were still excluded.
The resulting data set had a total of 242 users: 150
(62.0%) which were included in the original data,
and 92 (38.0%) which were originally rejected. Af-
ter constructing the modified data set, we sorted the
subjects by their agreement. Of the 92 subjects with
the lowest agreement, 75 of them were rejected in
the original data set (81.5%). Of the 150 subjects
with the highest agreement, only 17 of them were
rejected from the original data set (11.3%). The typ-
ical precision-recall tradeoff obviously applies.
Curiously, we note that the minimum subject
agreement at this 92nd subject was 0.457. Compar-
ing with the curves for the individual ratings in Fig-
ures 4a and 6, we see this is the point where intrinsic
consistency and data retention both begin dropping
rapidly. While this may be a happy coincidence, it
does seem to suggest that the ideal minimum sub-
ject agreement is roughly where the data retention
rate starts rapidly turning.
Regardless, we can definitely say that minimum
subject agreement is a highly effective way of root-
ing out spammers and unreliable participants.
4 Conclusion
In this paper, we have performed a thorough anal-
ysis of two sets of compositionality ratings to Ger-
man noun-noun compounds, and assessed their reli-
ability from several perspectives. We conclude that
asking for ratings of compositionality of compound
words is reasonable and that such judgements are
notably reliable and robust. Even when composi-
tionality ratings are collected in two very different
settings (laboratory vs. AMT) and with different dy-
namics, the produced ratings are highly consistent.
This is shown by the high initial correlation of the
two sets of compositionality ratings. We believe this
39
provides strong evidence that human judgements of
compositionality, or at least these particular data
sets, are reasonable as gold standards for other com-
putational linguistic tasks.
We also find that such ratings can be highly ro-
bust against large amounts of data loss, as in the
case of aggressive Z-score and minimum subject
agreement filtering: despite data retention rates of
10-70%, consistency between our data sets never
dropped more than 6 points. In addition, we find that
the correlation between compositionality ratings and
association norms is substantial, but generally much
lower and less sensitive than internal consistency.
We generally find Type I noise to be very diffi-
cult to detect, and Z-score filtering is mostly inef-
fective at eliminating unreliable item ratings. This
is confirmed by both our natural and artificial exper-
iments. At the same time, Z-score filtering seems
fairly harmless at conservative levels, and probably
can be safely applied in moderation with discretion.
On the other hand, we have confirmed that mini-
mum subject agreement is highly effective at filter-
ing out incompetent and unreliable subjects, as evi-
denced by both our artificial and spammer detection
experiments. We conclude that, as we have defined
it, Type II noise is easily detected, and removing this
noise produces much higher quality data. We recom-
mend using subject agreement as a first-pass identi-
fier of likely unreliable subjects in need of manual
review.
We would also like to explore other types of
compounds, such as adjective-noun compounds (e.g.
Gro?eltern ?grandparents?), and compounds with
more than two constituents (e.g. Bleistiftspitzma-
chine ?automatic pencil sharpener?).
Acknowledgments
We thank the SemRel group, Alexander Fraser, and
the reviewers for helpful comments and feedback.
The authors acknowledge the Texas Advanced Com-
puting Center (TACC) for providing grid resources
that have contributed to these results.5
5http://www.tacc.utexas.edu
References
Collin Bannard. 2005. Learning about the Meaning of
Verb?Particle Constructions from Corpora. Computer
Speech and Language, 19:467?478.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October.
Irad Ben-Gal. 2005. Outlier detection. In O. Maimon
and L. Rockach, editors, Data Mining and Knowledge
Discobery Handbook: A Complete Guide for Practi-
tioners and Researchers. Kluwer Academic Publish-
ers.
Gemma Boleda, Marco Baroni, Nghia The Pham, and
Louise McNally. 2013. On adjective-noun compo-
sition in distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics, Potsdam, Germany.
Chris Callison-Burch and Mark Dredze, editors. 2010.
Proceedings of the NAACL/HLT Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk, Los Angeles, California.
Herbert H. Clark. 1971. Word Associations and Lin-
guistic Theory. In John Lyons, editor, New Horizon in
Linguistics, chapter 15, pages 271?286. Penguin.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2011. Mathematical foundations for a compositional
distributional model of meaning. Linguistic Analysis,
36(1-4):345?384.
Paul Cook and Suzanne Stevenson. 2006. Classifying
Particle Semantics in English Verb-Particle Construc-
tions. In Proceedings of the ACL/COLING Workshop
on Multiword Expressions: Identifying and Exploiting
Underlying Properties, Sydney, Australia.
Simon de Deyne and Gert Storms. 2008. Word associ-
ations: Norms for 1,424 dutch words in a continuous
task. Behavior Research Methods, 40(1):198?205.
James Deese. 1965. The Structure of Associations in
Language and Thought. The John Hopkins Press, Bal-
timore, MD.
Wolfgang Fleischer and Irmhild Barz. 2012. Wortbil-
dung der deutschen Gegenwartssprache. de Gruyter.
Edward Grefenstette, G. Dinu, Y. Zhang, Meernoosh
Sadrzadeh, and Marco Baroni. 2013. Multi-step re-
gression learning for compositional distributional se-
mantics. In Proceedings of the 10th International
Conference on Computational Semantics, Potsdam,
Germany.
Rochelle Lieber and Pavol Stekauer, editors. 2009. The
Oxford Handbook of Compounding. Oxford Univer-
sity Press.
40
Jonathan I. Maletic and Adrian Marcus. 2010. Data
cleansing: A prelude to knowledge discovery. In
O. Maimon and L. Rokach, editors, Data Mining
and Knowledge Discovery Handbook. Springer Sci-
ence and Business Media, 2 edition.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan.
Timothy P. McNamara. 2005. Semantic Priming: Per-
spectives from Memory and Word Recognition. Psy-
chology Press, New York.
George Miller. 1969. The Organization of Lexical Mem-
ory: Are Word Associations sufficient? In George A.
Talland and Nancy C. Waugh, editors, The Pathol-
ogy of Memory, pages 223?237. Academic Press, New
York.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive Sci-
ence, 34:1388?1429.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 1998. The University of South Florida
Word Association, Rhyme, and Word Fragment
Norms.
Douglas L. Nelson, Cathy L. McEvoy, and Simon Den-
nis. 2000. What is Free Association and What does it
Measure? Memory and Cognition, 28:887?899.
Siva Reddy, Ioannis P. Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011a. Dynamic and Static Pro-
totype Vectors for Semantic Composition. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing, pages 705?713, Chiang
Mai, Thailand.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011b. An Empirical Study on Compositionality in
Compound Nouns. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 210?218, Chiang Mai, Thailand.
Sabine Schulte im Walde, Susanne Borgwaldt, and
Ronny Jauch. 2012. Association Norms of German
Noun Compounds. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 632?639, Istanbul, Turkey.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic Pooling and Unfolding Recursive Autoencoders
for Paraphrase Detection. In Advances in Neural In-
formation Processing Systems 24.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter-, Basis- und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages 51?
74.
Dominic Widdows. 2008. Semantic Vector Products:
Some Initial Investigations. In Proceedings of the 2nd
Conference on Quantum Interaction, Oxford, UK.
41
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 111?119,
Coling 2014, Dublin, Ireland, August 24 2014.
A Database of Paradigmatic Semantic Relation Pairs for
German Nouns, Verbs, and Adjectives
Silke Scheible and Sabine Schulte im Walde
Institut f?ur Maschinelle Sprachverarbeitung
Universit?at Stuttgart
{scheible,schulte}@ims.uni-stuttgart.de
Abstract
A new collection of semantically related word pairs in German is presented, which was compiled
via human judgement experiments and comprises (i) a representative selection of target lexi-
cal units balanced for semantic category, polysemy, and corpus frequency, (ii) a set of human-
generated semantically related word pairs based on the target units, and (iii) a subset of the
generated word pairs rated for their relation strength, including positive and negative relation
evidence. We address the three paradigmatic relations antonymy, hypernymy and synonymy, and
systematically work across the three word classes of adjectives, nouns, and verbs.
A series of quantitative and qualitative analyses demonstrates that (i) antonyms are more canon-
ical than hypernyms and synonyms, (ii) relations are more or less natural with regard to the spe-
cific word classes, (iii) antonymy is clearly distinguishable from hypernymy and synonymy, but
hypernymy and synonymy are often confused. We anticipate that our new collection of seman-
tic relation pairs will not only be of considerable use in computational areas in which semantic
relations play a role, but also in studies in theoretical linguistics and psycholinguistics.
1 Introduction
This paper describes the collection of a database of paradigmatically related word pairs in German which
was compiled via human judgement experiments hosted on Amazon Mechanical Turk. While paradig-
matic relations (such as synonymy, antonymy, hypernymy, and hyponymy) have been extensively re-
searched in theoretical linguistics and psycholinguistics, they are still notoriously difficult to identify
and distinguish computationally, because their distributions in text tend to be very similar. For example,
in The boy/girl/person loves/hates the cat, the nominal co-hyponyms boy, girl and their hypernym per-
son as well as the verbal antonyms love and hate occur in identical contexts, respectively. A dataset of
paradigmatic relation pairs would thus represent a valuable test-bed for research on semantic relatedness.
For the compilation of the relation dataset we aimed for a sufficiently large amount of human-labelled
data, which may both serve as seeds for a computational approach, and provide a gold-standard for evalu-
ating the resulting computational models. This paper describes our efforts to create such a paradigmatic
relation dataset in a two-step process, making use of two types of human-generated data: (1) human sug-
gestions of semantically related word pairs, and (2) human ratings of semantic relations between word
pairs. Furthermore, we are the first to explicitly work across word classes (covering adjective, noun
and verb targets), and to incorporate semantic classes, corpus frequency and polysemy as balancing
criteria into target selection. The resulting dataset
1
consists of three parts:
1. A representative selection of target lexical units drawn from GermaNet, a broad-coverage lexical-
semantic net for German, using a principled sampling technique and taking into account the three
major word classes adjectives, nouns, and verbs, which are balanced according to semantic category,
polysemy, and type frequency.
2. A set of human-generated semantically related word pairs, based on the target lexical units.
3. A subset of semantically related word pairs, rated for the strength of the relation between them.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The dataset is available from http://www.ims.uni-stuttgart.de/data/sem-rel-database.
111
We anticipate that our new collection of semantic relation pairs will not only be of considerable use in
computational areas in which semantic relations play a role (such as Distributional Semantics, Natural
Language Understanding/Generation, and Opinion Mining), but also in studies in theoretical linguistics
and psycholinguistics. In addition, our dataset may be of major interest for research groups working on
automatic measures of semantic relatedness, as it allows a principled evaluation of such tools. Finally,
since the target lexical units are drawn from the GermaNet database, our results will be directly relevant
for assessing, developing, and maintaining this resource.
2 Related work
Over the years a number of datasets have been made available for studying and evaluating semantic
relatedness. For English, Rubenstein and Goodenough (1965) obtained similarity judgements from 51
subjects on 65 noun pairs, a seminal study which was later replicated by Miller and Charles (1991), and
Resnik (1995). Finkelstein et al. (2002) created a set of 353 English noun-noun pairs rated by 16 subjects
according to their semantic relatedness on a scale from 0 to 10. For German, Gurevych (2005) replicated
Rubenstein and Goodenough?s experiments by translating the original 65 word pairs into German. In
later work, she used the same experimental setup to increase the number of word pairs to 350 (Gurevych,
2006).
The dataset most similar to ours is BLESS (Baroni and Lenci, 2011), a freely available dataset that
includes 200 distinct English concrete nouns as target concepts, equally divided between living and non-
living entities, and grouped into 17 broad classes such as bird, fruit. For each target concept, BLESS con-
tains several relata, connected to it through a semantic relation (hypernymy, co-hyponymy, meronymy,
attribute, event), or through a null-relation. BLESS thus includes two paradigmatic relations (hypernymy,
co-hyponymy) but does not focus on paradigmatic relations. Furthermore, it is restricted to concrete
nouns, rather than working across word classes.
3 Paradigmatic relations
The focus of this work is on semantic relatedness, and in particular on paradigmatic semantic relations.
This section discusses the theoretical background of the notion of paradigmatic semantic relations. The
term paradigmatic goes back to de Saussure (1916), who introduced a distinction between linguistic
elements based on their position relative to each other. This distinction derives from the linear nature of
linguistic elements, which is reflected in the fact that speech sounds follow each other in time. Saussure
refers to successive linguistic elements that combine with each other as ?syntagma?, and thus the relation
between these elements is called ?syntagmatic?. On the other hand, elements that can be found in the
same position in a syntagma, and which could be substituted for each other, are in a ?paradigmatic?
relationship. While syntagmatic and paradigmatic relations can hold between a variety of linguistic units
(such as morphemes, phonemes, clauses, or sentences), the focus of this research is on the relations
between words.
Many studies in computational linguistics work on the assumption that paradigmatic semantic relations
hold between words. As will become apparent in the course of this work, it is necessary to move beyond
these definitions for an appropriate investigation of paradigmatic semantic relations. According to Cruse
(1986), sense is defined as ?the meaning aspect of a lexical unit?, and he states that ?semantic relations?
hold between lexical units, not between lexemes.
The goal of this work is to create a database of semantic relations for German adjectives, nouns and
verbs, focussing on the three types of paradigmatic relations referred to as sense-relations by Lyons
(1968) and Lyons (1977): synonymy, antonymy, and hypernymy.
4 Experimental setup
Our aim was to collect semantically related word pairs for the paradigmatic relations antonymy, syn-
onymy, and hypernymy for the three word classes nouns, verbs, and adjectives. For this purpose we
implemented two experiments involving human participants. Starting with a set of target words, in the
first experiment participants were asked to propose suitable antonyms, synonyms and hypernyms for
112
each of the targets. For example, for the target verb befehlen (?to command?), participants proposed
antonyms such as gehorchen (?to obey?), synonyms such as anordnen (?to order?), and hypernyms such
as sagen (?to say?).
In the second experiment, participants were asked to rate the strength of a given semantic relation with
respect to a word pair on a 6-point scale. For example, workers would be presented with a pair ?wild ?
free? and asked to rate the strength of antonymy between the two words. All word pairs were assessed
with respect to all three relation types.
Both experiments will be described in further detail in Sections 5 and 6. The current section aims to
provide an overview of GermaNet, a lexical-semantic word net for German, from which the set of target
words was drawn (4.1). We then describe the selection of target words from GermaNet, which used a
stratified sampling approach (4.2). Finally, we introduce the platform used to implement the experiments,
Amazon Mechanical Turk (4.3).
4.1 Target source: GermaNet
GermaNet is a lexical-semantic word net that aims to relate German nouns, verbs, and adjectives seman-
tically. GermaNet has been modelled along the lines of the Princeton WordNet for English (Miller et al.,
1990; Fellbaum, 1998) and shares its general design principles (Hamp and Feldweg, 1997; Kunze and
Wagner, 1999; Lemnitzer and Kunze, 2007). For example, lexical units denoting the same concept are
grouped into synonym sets (?synsets?). These are in turn interlinked via conceptual-semantic relations
(such as hypernymy) and lexical relations (such as antonymy). For each of the major word classes, the
databases further take a number of semantic categories into consideration, expressed via top-level nodes
in the semantic network (such as ?Artefakt/artifact?, ?Geschehen/event?, ?Gef?uhl/feeling?). However, in
contrast to WordNet, GermaNet also includes so-called ?artificial concepts? to fill lexical gaps and thus
enhance network connectivity, and to avoid unsuitable co-hyponymy (e.g. by providing missing hyper-
nyms or hyponyms). GermaNet also differs from WordNet in the way in which it handles part of speech.
For example, while WordNet employs a clustering approach to structuring adjectives, GermaNet uses a
hierarchical structure similar to the one employed for the noun and verb hierarchies. Finally, the latest
releases of WordNet and GermaNet also differ in size: While WordNet 3.0 contains at total of 117,659
synsets and 155,287 lexical units, the respective numbers for GermaNet 6.0 are considerably lower, with
69,594 synsets and 93,407 lexical units.
Since GermaNet is the largest database of its kind for German, and as it encodes all types of relations
that are of interest for us (synonymy, antonymy, and hypernymy), it represents a suitable starting point
for our purposes.
4.2 Target selection
The purpose of collecting the set of targets was to acquire a broad range of lexical items which could
be used as input for generating semantically related word pairs (cf. Section 5). Relying on GermaNet
version 6.0 and the respective JAVA API, we used a stratified sampling technique to randomly select 99
nouns, 99 adjectives and 99 verbs from the GermaNet files. The random selection was balanced for
1. the size of the semantic classes,
2
accounting for the 16 semantic adjective classes and the 23 se-
mantic classes for both nouns and verbs, as represented by the file organisation;
2. three polysemy classes according to the number of GermaNet senses:
I) monosemous, II) two senses and III) more than two senses;
3. three frequency classes according to type frequency in the German web corpus SdeWaC (Faa? and
Eckart, 2013), which contains approx. 880 million words:
I) low (200?2,999), II) mid (3,000?9,999) and III) high (?10,000).
The total number of 99 targets per word class resulted from distinguishing 3 sense classes and 3 frequency
classes, 3?3 = 9 categories, and selecting 11 instances from each category, in proportion to the semantic
class sizes.
2
For example, if an adjective GermaNet class contained 996 word types, and the total number of adjectives over all semantic
classes was 8,582, and with 99 stimuli collected in total, we randomly selected 99?996/8, 582 = 11 adjectives from this class.
113
4.3 Experimental platform: Mechanical Turk
The experiments described below were implemented in Amazon Mechanical Turk (AMT)
3
, a web-based
crowdsourcing platform which allows simple tasks (so-called HITs) to be performed by a large number
of people in return for a small payment. In our first experiment, human associations were collected
for different semantic relation types, where AMT workers were asked to propose suitable synonyms,
antonyms, and hypernyms for each of the targets. The second experiment was based on a subset of the
generated synonym/antonym/hypernym pairs and asked the workers to rate each pair for the strength
of antonymy, synonymy, and hypernymy between them, on a scale between 1 (minimum strength) and
6 (maximum strength). To control for non-native speakers of German and spammers, each batch of
HITs included two examples of ?non-words? (invented words following German morphotactics such as
Blapselheit, gekortiert) in a random position. If participants did not recognise the invented words, we
excluded all their ratings from consideration. While we encouraged workers to complete all HITs in a
given batch, we also accepted a smaller number of submitted HITs, as long as the workers had a good
overall feedback score.
5 Generation experiment
5.1 Method
The goal of the generation experiment was to collect human associations for the semantic relation types
antonymy, hypernymy, and synonymy. For each of our 3?99 adjective, noun, and verb targets, we asked
10 participants to propose a suitable synonym, antonym, and hypernym. Targets were bundled randomly
in 9 batches per word class, each including 9 targets plus two invented words. The experiment consisted
of separate runs for each relation type to avoid confusion between them, with participants first generating
synonyms, then antonyms, and finally hypernyms for the targets, resulting in 3 word classes? 99 targets
? 3 relations ? 10 participants = 8, 910 target?response pairs.
5.2 Results and discussion
(a) Total number of responses:
Table 1 illustrates how the number of generated word pairs distributes across word classes and relations.
The total number per class and relation is 990 tokens (99 targets ? 10 participants). From the maximum
number of generated pairs, a total of 131 types (211 tokens) were discarded because the participants
provided no response. These cases had been accepted via AMT nevertheless because the participants
were approved workers and we assumed that the empty responses showed the difficulty of specific word?
relation constellations, see below. For example, six out of ten participants failed to provide a synonym
for the adjective bundesrepublikanisch ?federal republic?.
ANT HYP SYN all
types tokens types tokens types tokens types tokens
ADJ 524 990 676 990 597 990 1,797 2,970
NOUN 708 990 701 990 621 990 2,030 2,970
VERB 636 990 662 990 620 990 1,918 2,970
all 1,868 2,970 2,039 2,970 1,838 2,970 5,745 8,910
Table 1: Number of generated relation pairs across word classes.
(b) Number of ambiguous responses:
An interesting case is provided by pairs that were generated with regard to different relations but for the
same target word. Table 2 lists the number of types of such ambiguous pairs, and the intersection of
the tokens. For example, if five participants generated a pair with regard to a target and relation x, and
two participants generated the same pair with regard to relation y, the intersection is 2. The intersection
3
https://www.mturk.com
114
is more indicative of ambiguity here, because in most cases of ambiguity the intersection is only 1,
which might as well be the result of an erroneously generated pair (e.g., because the participant did
not pay attention to the task), rather than genuine ambiguity. Examples of ambiguous responses with an
intersection > 1 are Gegenargument?Argument ?counter argument ? argument?, which was provided five
times as an antonymy pair and twice as a hypernymy pair; freudlos?traurig ?joyless ? sad?, which was
provided four times as a synonymy pair and five times as a hypernymy pair; and beseitigen?entfernen
?eliminate ? remove?, which was provided five times as a synonymy pair and five times as a hypernymy
pair.
ANT+HYP ANT+SYN HYP+SYN ANT+HYP+SYN
types tokens types tokens types tokens types tokens
ADJ 6 6 4 4 195 342 2 2
NOUN 15 16 17 17 93 117 5 6
VERB 4 4 8 8 182 290 5 6
all 25 26 29 29 470 749 12 14
Table 2: Number of ambiguous relation pairs across word classes.
The ambiguities in Table 2 indicate that humans are quite clear about what distinguishes antonyms
from synonyms, and what distinguishes antonyms from hypernyms. On the other hand, the line dividing
hypernymy and synonymy is less clear, and the large amount of confusion between the two relations
lends support to theories claiming that hypernymy should be considered a type of synonymy, and that
real synonymy does not exist in natural languages for economical reasons. Furthermore, the confusion
is most obvious for adjectives and verbs, for which the relation is considered less natural than for nouns,
cf. Miller and Fellbaum (1991).
(c) Number of (different) responses across word classes and relations:
An analysis of the number of different antonyms, hypernyms and synonyms generated for a given target
shows no noticeable difference at first glance: on average, 6.04 different antonyms were generated for
the targets, while the number is minimally higher for synonyms with 6.08 different responses on average;
hypernyms received considerably more (6.78) different responses on average. However, the distribution
of the numbers of different antonym, hypernym, and synonym responses across the targets shows that the
antonymy generation task results in more targets with a small number of different responses compared
to the synonymy and the hypernym task (Figure 1): there are 10 targets for which all ten participants
generated the same antonym (x = number of different responses = 1), such as dunkel?hell ?dark ? light?
and verbieten?erlauben ?to forbid ? to allow?, while there are 17 targets where they generated exactly two
(x=2), and 29 targets where they suggested three different antonyms (x=3). In contrast, for hypernymy
and synonymy, there are 0/3 targets where all participants agreed on the same response, and there are
only 5/10 targets where they generated exactly two, and 8/21 targets where they generated only three
different hypernyms/synonyms.
These results are in line with previous findings for English and Swedish by Paradis and Willners (2006)
and Paradis et al. (2009), who argue against the strict contrast between ?direct? and ?indirect? antonyms
which has been assumed in the literature (see, for example, Gross et al. (1989)) in favour of a scale of
?canonicity? where some word pairs are perceived as more antonymic than others. In particular, they
propose that the weaker the degree of canonicity, the more different responses the target items will yield
in an elicitation experiment. Similar to the current findings for German, they found that for English
and Swedish there is a small core of highly opposable couplings which have been conventionalised as
antonym pairs in text and discourse, while all other couplings form a scale from more to less strongly
related. The ten targets for which all participants generated the same antonym response are thus likely
to represent highly ?canonical? pairings. The fact that the hypernymy and synonymy generation exper-
iments results in fewer targets with only one or two different responses suggests that hypernymy and
synonymy have a lower level of canonicity than antonymy.
115
Figure 1: Number of targets plotted against the number of different responses.
Figure 2 demonstrates that the overall distributions of the frequency of responses are, however, very
similar for antonyms, hypernyms and synonyms: between 72% and 77% of the responses were only
given once, with the curves following a clear downward trend. Note that a strength of 10 in Figure 2
refers to the case of one different response (x=1) in Figure 1.
Figure 2: Response magnitude.
Finally, Figure 3 compares the number of blank responses (types and tokens) regarding antonyms,
hypernyms and synonyms. Across word classes, 74/115 targets (types/tokens) received blank antonym
responses, while only 25/34 targets received blank hypernym responses and only 32/62 targets received
blank synonym responses. These numbers indicate that participants find it harder to come up with
antonyms than hypernyms or synonyms. Breaking the proportions down by word class, Figure 3 demon-
strates that in each case the number of missing antonyms (left panel: types; right panel: tokens) is larger
than those of missing hypernyms/synonyms. Figure 3 also shows that the difficulty to provide rela-
tion pairs varies across word classes. While antonyms are the most difficult relation in general, there
are more blank responses regarding adjectives and nouns, in comparison to verbs. Hypernymy seems
similarly difficult across classes, and synonymy is more difficult for nouns than for adjectives or verbs.
Figure 3: Blank responses (types and tokens).
116
(d) Comparison with GermaNet:
The results of the generation experiment can be used to extend and develop GermaNet, the resource the
targets were drawn from: a large proportion of responses are not covered in GermaNet. Table 3 below
shows for the three parts of speech and the three relation types how many responses were covered by both
the generation experiment (EXP) and GermaNet (GN) (column ?Both?), how many of them only appear
in the generation experiment (column ?EXP?), and how many are only listed in GermaNet (?GN?). Blank
and multi-word responses in the experimental results were excluded from consideration. The comparison
shows that the variety of semantic relation types in our experimental dataset is considerably larger than in
GermaNet, while the overlap is marginal. Especially for antonyms, the coverage in GermaNet seems to
be quite low, across word classes. For hypernymy and synonymy, the semantic relation types complement
each other to a large extent, with each resource containing relations that are not part of the other resource.
In sum, the tables confirm that extending GermaNet with our relation types should enrich the manual
resource.
ANT HYP SYN
Both EXP GN Both EXP GN Both EXP GN
ADJ 33 453 5 100 561 237 66 496 160
NOUN 3 633 2 108 561 393 59 516 150
VERB 10 542 2 132 507 260 40 554 109
Table 3: Relation coverage in Generation Experiment (EXP) and GermaNet (GN).
6 Rating experiment
6.1 Method
In the second experiment, Mechanical Turk workers were asked to rate the strength of a given semantic
relation with respect to a word pair on a 6-point scale. The main purpose of this experiment was to
identify and distinguish between ?strong? and ?weak? examples of a specific relation. The number
of times a specific response was given in the generation experiment does not necessarily indicate the
strength of the relation. This is especially true for responses that were suggested by only one or two
participants, where it is difficult to tell if the response is an error, or if it relates to a idiosyncratic sense
of the target word that the other participants did not think of in the first instance. Crucially, in the rating
experiment all word pairs were assessed with respect to all three relation types, thus asking not only for
positive but also negative evidence of semantic relation instances.
The set of word pairs used as input is a carefully selected subset of responses acquired in the generation
experiment.
4
For each of the 99 targets and each of the semantic relations antonymy, synonymy, and
hypernymy two responses were included (if available): the response with the highest frequency (random
choice if several available), and a response with a lower frequency (2, if available, otherwise 1; random
choice if several available). Multi-word responses and blanks were excluded from consideration. A
manual post-processing step aimed to address the issue of duplicate pairs in the randomly generated
dataset, where the same responses had been generated for two of the relations.
In theory, each target should have 6 associated pairs (2xANT, 2xHYP, 2xSYN). In practice, there are
sometimes fewer than 6 pairs per target in the dataset, because (i) for some targets, only one response
is available for a given relation (e.g., if all 10 participants provided the same response), or (ii) no valid
response of the required frequency type is available. The resulting dataset includes 1,684 target-response
pairs altogether, 546 of which are adjective pairs, 574 noun pairs, and 564 verb pairs. To avoid confusion,
the ratings were collected in separate experimental settings, i.e., for each word class and each relation
type, all generated pairs were first judged for their strength of one relation, and then for their strength of
another relation.
4
For time and money reasons, we could not collect the 8, 910? 3 ? 10 = 267, 300 ratings for all responses.
117
6.2 Results and discussion
In the following, we present the results of the rating experiment in terms of mean rating scores for each
word pair. The mean rating scores were calculated across all ten ratings per pair. The purpose of the
analysis was to verify that the responses generated in the generation experiment are in fact perceived as
examples of the given relation type by other raters. We thus looked at all responses for a given relation
type in the data set and calculated the average value of all mean ratings for this relation type. For example,
Figure 4 (left panel) shows that the responses generated as antonyms are clearly perceived as antonyms
in the case of adjectives, with an average rating score of 4.95. Verb antonyms are also identified as such
with a rating of 4.38. The situation for nouns, however, is less clear: an average rating of 3.70 is only
minimally higher than the middle point of the rating scale (3.50). These findings support the common
assumption that antonymy is a relation that applies well to adjectives and verbs, but less so to nouns.
Responses generated as synonyms (plot omitted for space reasons), on the other hand, are identified as
such for all three words classes, with average rating values of 4.78 for adjectives, 4.48 for nouns, and
4.66 for verbs.
Figure 4: Average ratings of antonym/hypernym responses as ANT or SYN, across word classes.
Finally, Figure 4 (right panel) shows the average ratings as synonyms/antonyms for responses gener-
ated as hypernyms. The findings corroborate our analysis of synonym/hypernym confusion in Section 5:
the distribution looks fairly similar to the one for synonyms, with low antonymy ratings, but an average
synonymy rating of 4.43 for adjectives, 3.08 for nouns, and 3.89 for verbs. The results suggest that
hypernymy is particularly difficult to distinguish from synonymy in the case of adjectives.
7 Conclusion
This article presented a new collection of semantically related word pairs in German which was compiled
via human judgement experiments. The database consists of three parts:
1. A representative selection of target lexical units drawn from GermaNet, using a principled sampling
technique and taking into account the three major word classes adjectives, nouns, and verbs, which
are balanced according to semantic category, polysemy, and type frequency.
2. A set of 8,910 human-generated semantically related word pairs, based on the target lexical units.
3. A subset of 1,684 semantically related word pairs, rated for the strengths of relations.
To our knowledge, our dataset is the first that (i) focuses on multiple paradigmatic relations, (ii) system-
atically works across word classes, (iii) explicitly balances the targets according to semantic category,
polysemy and type frequency, and (iv) explicitly provides positive and negative rating evidence. We de-
scribed the generation and the rating experiments, and presented a series of quantitative and qualitative
analyses. The analyses showed that (i) antonyms are more canonical than hypernyms and synonyms,
(ii) relations are more or less natural with regard to the specific word classes, (iii) antonymy is clearly
distinguishable from hypernymy and synonymy, and (iv) hypernymy and synonymy are often confused.
Acknowledgements
The research was funded by the DFG Sachbeihilfe SCHU-2580/2-1 (Silke Scheible) and the DFG
Heisenberg Fellowship SCHU-2580/1-1 (Sabine Schulte im Walde).
118
References
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed Distributional Semantic Evaluation. In Proceed-
ings of the EMNLP Workshop on Geometrical Models for Natural Language Semantics, pages 1?10, Edinburgh,
UK.
D. Allan Cruse. 1986. Lexical Semantics. Cambridge Textbooks in Linguistics. Cambridge University Press,
Cambridge, UK.
Ferdinand de Saussure. 1916. Cours de Linguistique G?en?erale. Payot.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC ? a Corpus of Parsable Sentences from the Web. In Proceedings
of the International Conference of the German Society for Computational Linguistics and Language Technology,
pages 61?68, Darmstadt, Germany.
Christiane Fellbaum, editor. 1998. WordNet ? An Electronic Lexical Database. Language, Speech, and Commu-
nication. MIT Press, Cambridge, MA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Rup-
pin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems,
20(1):116?131.
Derek Gross, Ute Fischer, and George A. Miller. 1989. Antonymy and the Representation of Adjectival Meanings.
Memory and Language, 28(1):92?106.
Iryna Gurevych. 2005. Using the Structure of a Conceptual Network in Computing Semantic Relatedness. In
Proceedings of the 2nd International Joint Conference on Natural Language Processing, pages 767?778, Jeju
Island, Korea.
Iryna Gurevych. 2006. Thinking beyond the Nouns - Computing Semantic Relatedness across Parts of Speech.
In Sprachdokumentation & Sprachbeschreibung, 28. Jahrestagung der Deutschen Gesellschaft f?ur Sprachwis-
senschaft, Bielefeld, Germany.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet ? a Lexical-Semantic Net for German. In Proceedings of
the ACL Workshop on Automatic Information Extraction and Building Lexical Semantic Resources for NLP
Applications, pages 9?15, Madrid, Spain.
Claudia Kunze and Andreas Wagner. 1999. Integrating GermaNet into EuroWordNet, a Multilingual Lexical-
Semantic Database. Sprache und Datenverarbeitung, 23(2):5?19.
Lothar Lemnitzer and Claudia Kunze. 2007. Computerlexikographie. Gunter Narr Verlag, T?ubingen, Germany.
John Lyons. 1968. Introduction to Theoretical Linguistics. Cambridge University Press.
John Lyons. 1977. Semantics. Cambridge University Press.
George A. Miller and Walter G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and
Cognitive Processes, 6(1):1?28.
George A. Miller and Christiane Fellbaum. 1991. Semantic Networks of English. Cognition, 41:197?229.
George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduc-
tion to Wordnet: An On-line Lexical Database. International Journal of Lexicography, 3(4):235?244.
Carita Paradis and Caroline Willners. 2006. Antonymy and Negation: The Boundedness Hypothesis. Journal of
Pragmatics, 38:1051?1080.
Carita Paradis, Caroline Willners, and Steven Jones. 2009. Good and Bad Opposites: Using Textual and Experi-
mental Techniques to Measure Antonym Canonicity. The Mental Lexicon, 4(3):380?429.
Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial Intelligence, pages 448?453, San Francisco, CA.
Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Communications of the
ACM, 8(10):627?633.
119
