Chinese Sketch Engine and 
the Extraction of Grammatical Collocations
Chu-Ren Huang  
Inst. of Linguistics  
Academia Sinica  
churen@sinica.edu.tw
Adam Kilgarriff 
Lexicography MasterClass 
Information Technology  
adam@lexmasterclass.com 
Yiching Wu 
Inst. of Linguistics 
Tsing Hua University 
d898702@oz.nthu.edu.tw 
Chih-Ming Chiu 
Inst. of Information Science 
Academia Sinica 
henning@hp.iis.sinica.edu.tw 
Simon Smith 
Dept. of Applied English 
Ming Chuan University 
ssmith@mcu.edu.tw 
Pavel Rychly 
Faculty of Informatics 
Masaryk University. 
pary@textforge.cz 
Ming-Hong Bai 
Inst. of Information Science 
Academia Sinica 
mhbai@sinica.edu.tw 
Keh-Jiann Chen 
Inst. of Information Science 
Academia Sinica 
kchen@iis.sinica.edu.tw
Abstract. This paper introduces a new 
technology for collocation extraction in Chinese. 
Sketch Engine (Kilgarriff et al, 2004) has 
proven to be a very effective tool for automatic 
description of lexical information, including 
collocation extraction, based on large-scale 
corpus. The original work of Sketch Engine was 
based on BNC. We extend Sketch Engine to 
Chinese based on Gigaword corpus from LDC. 
We discuss the available functions of the 
prototype Chinese Sketch Engine (CSE) as well 
as the robustness of language-independent 
adaptation of Sketch Engine. We conclude by 
discussing how Chinese-specific linguistic 
information can be incorporated to improve the 
CSE prototype.  
1. Introduction 
The accessibility to large scale corpora, at 
one billion words or above, has become both a 
blessing and a challenge for NLP research. How 
to efficiently use a gargantuan corpus is an 
urgent issue concerned by both users and corpora 
designers. Adam Kilgarriff et al (2004) 
developed the Sketch Engine to facilitate 
efficient use of corpora. Their claims are two 
folded: that genuine linguistic generalizations 
can be automatically extracted from a corpus 
with simple collocation information provided 
that the corpus is large enough; and that such a 
methodology is easily adaptable for a new 
language. The first claim was fully substantiated 
with their work on BNC. The current paper deals 
with the second claim by adapting the Sketch 
Engine to Chinese.  
2. Online Chinese Corpora: The State of 
the Arts 
2.1 Chinese Corpora 
The first online tagged Chinese corpus is 
Academia Sinica Balanced Corpus of Modern 
Chinese (Sinica Corpus), which has been 
web-accessible since November, 1996. The 
current version contains 5.2028 million words 
(7.8927 million characters). The corpus data was 
collected between 1990 and 1996 (CKIP, 
1995/1998). Two additional Chinese corpora 
were made available on line in 2003. The first is 
the Sinorama Chinese-English Parallel Text 
Corpus (Sinorama Corpus). The Sinorama 
Corpus is composed of 2,373 parallel texts in 
both Chinese and English that were published 
between 1976 and 2000. There are 103,252 pairs 
of sentences, composed of roughly 3.2 million 
48
English words and 5.3 million Chinese 
characters 1 . The second one is the modern 
Chinese corpus developed by the Center for 
Chinese Linguistics (CCL Corpus) at Peking 
University. It contains eighty-five million 
(85,398,433) simplified Chinese characters 
which were published after 1919 A.D. 
2.2 Extracting Linguistic Information from 
Online Chinese Corpora: Tools and Interfaces 
The Chinese corpora discussed above are 
all equipped with an online interface to allow 
users to extract linguistic generalizations. Both 
Sinica Corpus and CCL Corpus offer 
KWIC-based functions, while Sinorama Corpus 
gives sentence and paragraph aligned output. 
2.2.1 String Matching or Word Matching 
The basic unit of query that a corpus allows 
defines the set of information that can be 
extracted from that corpus. While there is no 
doubt that segmented corpus allows more precise 
linguistic generalizations, string-based 
collocation still afford a corpus of the robustness 
that is not restricted by an arbitrary word-list or 
segmentation algorithm. This robustness is of 
greatest value when extracting neologism or 
sub-lexical collocations. Since CCL Corpus is 
not segmented and tagged, string-based KWIC is 
its main tool for extracting generalizations. This 
comes with the familiar pitfall of word boundary 
ambiguity. For instance, a query of ci.yao ??
?secondary? may yield the intended result (la), as 
well as noise (1b). 
1a. ??????
dan zhe shi ci.yao de 
but this is secondary DE
1http://cio.nist.gov/esd/emaildir/lists/mt_list/msg0003
3.html 
?But this is secondary? 
 b. ????????!
ta ji ci yao.qiu ta da.fu 
he several time ask her answer 
?He had asked her to answer for several times? 
 Sinica Corpus, on the other hand, is fully 
segmented and allows word-based 
generalizations. In addition, Sinica Corpus also 
allows wildcards in its search. Users specify a 
wildcard of arbitrary length (*), or fixed length 
(?). This allows search of a class of words 
sharing some character strings.
2.2.2 Display of Extracted Data 
Formal restriction on the display of 
extracted data also constraints the type of 
information that can be obtained from that 
corpus. Sinica Corpus allows users to change 
window size from about 25 to 57 Chinese 
characters. However, since a Chinese sentence 
may be longer than 57 characters, Sinica Corpus 
cannot guarantee that a full sentence is displayed. 
CCL Corpus, on the other hand, is able to show a 
full output sentence, which may be up to 200 
Chinese characters. However, it does not display 
more than a full sentence. Thus it cannot show 
discourse information. Sinorama Corpus with 
TOTALrecall interface is most versatile in this 
respect. Aligned bilingual full sentences are 
shown with an easy link to the full text. 
In terms of size and completeness of 
extracted data, Sinica Corpus returns all matched 
examples. However, cut and paste must be 
performed for the user to build his/her dataset. 
CCL Corpus, on the other hand, limits data to 
500 lines per page, but allows easy download of 
output data. Lastly, Sinorama/TOTALrecall 
provides choices of 5 to 100 sentences per page. 
49
2.2.3 Refining Extracted Information: Filter 
and Sorter 
Both Sinica Corpus and CCL corpus allows 
users to process extracted information, using 
linguistic and contextual filter or sorter. The CCL 
corpus requires users to remember the rules, 
while Sinica Corpus allows users to fill in blanks 
and/or choose from pull-down menu. In 
particular, Sinica Corpus allows users to refine 
their generalization by quantitatively 
characterizing the left and right contexts. The 
quantitative sorting functions allowed include 
both word and POS frequency, as well as word 
mutual information.  
2.2.4 Extracting Grammatical Information 
Availability of grammatical information 
depends on corpus annotation. CCL and 
Sinorama Corpus do not have POS tags. Sinica 
Corpus is the only Chinese corpus allowing users 
to access an overview of a keyword?s syntactic 
behavior. Users can obtain a list of types and 
distribution of the keyword?s syntactic category. 
In addition, users can find possible collocations 
of the keyword from the output of Mutual 
Information (MI).  
The most salient grammatical information, 
such as grammatical functions (subject, object, 
adjunct etc.) is beyond the scope of the 
traditional corpus interface tools. Traditional 
corpora rely on the human users to arrive at these 
kinds of generalizations.
3. Sketch Engine: A New Corpus-based 
approach to Grammatical Information  
Several existing linguistically annotated 
corpus of Chinese, e.g. Penn Chinese Tree Bank 
(Xia et al, 2000), Sinica Treebank (Chen et al, 
2003), Proposition Bank (Xue and Palmer, 2003, 
2005) and Mandarin VerbNet (Wu and Liu, 
2003), suffer from the same problem. They are 
all extremely labor-intensive to build and 
typically have a narrow coverage. In addition, 
since structural assignment is theory-dependent 
and abstract, inter-annotator consistency is 
difficult to achieve. Since there is also no general 
consensus on the annotation scheme in Chinese 
NLP and linguistics, building an effective 
interface for public use is almost impossible. 
The Sketch Engine offers an answer to the 
above issues.
3.1 Initial Implementation and Design of the 
Sketch Engine 
The Sketch Engine is a corpus processing 
system developed in 2002 (Kilgarriff and 
Tugwell, 2002; Kilgarriff et al, 2004). The main 
components of the Sketch Engine are KWIC 
concordances, word sketches, grammatical 
relations, and a distributional thesaurus. In its 
first implementation, it takes as input basic BNC 
(British National Corpus, (Leech, 1992)) data: 
the annotated corpus, as well as list of lemmas 
with frequencies. In other words, the Sketch 
Engine has a relatively low threshold for the 
complexity of input corpus. 
The Sketch Engine has a versatile query 
system. Users can restrict their query in any 
sub-corpus of BNC. A query string may be a 
word (with or without POS specification), or a 
phrasal segment. A query can also be performed 
using Corpus Query Language (CQL). The 
output display format can be adjusted, and the 
displayed window of a specific item can be 
freely expanded left and right. Most of all, the 
Sketch Engine produces a Word Sketch 
(Kilgarriff and Tugwell, 2002) that is an 
automatically generated grammatical description 
of a lemma in terms of corpus collocations. All 
items in each collocation are linked back to the 
original corpus data. Hence it is similar to a 
50
Linguistic Knowledge Net anchored by a lexicon 
(Huang et al, 2001). 
 A Word Sketch is a one-page list of a 
keyword?s functional distribution and collocation 
in the corpus. The functional distribution 
includes: subject, object, prepositional object, 
and modifier. Its collocations are described by a 
list of linguistically significant patterns in the 
language. Word Sketch uses regular expressions 
over POS-tags to formalize rules of collocation 
patterns, e.g. (2) is used to retrieve the 
verb-object relation in English:
2. 1:?V? ?(DET|NUM|ADJ|ADV|N)?* 2:?N? 
The expression in (2) says: extract the data 
containing a verb followed by a noun regardless 
of how many determiners, numerals, adjectives, 
adverbs and nouns preceding the noun. It can 
extract data containing cook meals and cooking a 
five-course gala dinner, and cooked the/his/two 
surprisingly good meals etc.
The Sketch Engine also produces thesaurus 
lists, for an adjective, a noun or a verb, the other 
words most similar to it in their use in the 
language (Kilgarriff et al 2004). For instance, 
the top five synonym candidates for the verb kill
are shoot (0.249), murder (0.23), injure (0.229), 
attack (0.223), and die (0.212).2 It also provides 
direct links to the Sketch Difference which lists 
the similar and different patterns between a 
keyword and its similar word. For example, both 
kill and murder can occur with objects such as 
people and wife, but murder usually occurs with 
personal proper names and seldom selects animal 
nouns as complement whereas kill can take fox,
whale, dolphin, and guerrilla, etc. as its object. 
 The Sketch Engine adopts Mutual 
2 The similarity is measured and ranked adopting 
Lin?s (1998) mathematics. 
Information (MI) to measure the salience of a 
collocation. Salience data are shown against each 
collocation in Word Sketches and other Sketch 
Engine output. MI provides a measure of the 
degree of association of a given segment with 
others. Pointwise MI, calculated by Equation 3, 
is what is used in lexical processing to return the 
degree of association of two words x and y (a 
collocation).
3. 
)(
)|(log);(
xP
yxPyxI  
3.2 Application to Chinese Corpus 
In order to show the cross-lingual 
robustness of the Sketch Engine as well as to 
propose a powerful tool for collocation 
extraction based on a large scale corpus with 
minimal pre-processing; we constructed Chinese 
Sketch Engine (CSE) by loading the Chinese 
Gigaword to the Sketch Engine (Kilgarriff et al, 
2005). The Chinese Gigaword contains about 
1.12 billion Chinese characters, including 735 
million characters from Taiwan?s Central News 
Agency, and 380 million characters from China?s 
Xinhua News Agency3. Before loading Chinese 
Gigaword into Sketch Engine, all the simplified 
characters were converted into traditional 
characters, and the texts were segmented and 
POS tagged using the Academia Sinica 
segmentation and tagging system (Huang et al, 
1997). An array of machine was used to process 
the 1.12 million characters, which took over 3 
days to perform. All components of the Sketch 
Engine were implemented, including 
Concordance, Word Sketch, Thesaurus and 
Sketch Difference.  
 In our initial in-house testing of this 
prototype of the Chinese Sketch Engine, it does 
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T09 
51
produce the expected results with an easy to use 
interface. For instance, the Chinese Word Sketch 
correctly shows that the most common and 
salient object of dai.bu ??  ?to arrest? is
xian.fan ??  ?suspect?; the most common 
subject jing.fang ??!?police?; and the most 
common modifier dang.chang??.
 The output data of Thesaurus correctly 
verify the following set of synonyms from the 
Chinese VerbNet Project: that ren.wei ???to
think? behaves most like biao.shi ??  ?to 
express, to state? (salience 0.451), while yi.wei?
? ?to take somebody/something as? is more like 
jue.de?? ?to feel, think? (salience 0.488). The 
synonymous relation can be illustrated by (4) and 
(5).
4a. ????????????????????
?????????????????
ta ren.wei dao hai.wai tou.zi you yi ge guan.nian 
hen zhong.yao, jiu shi yao zhi.dao dang.di de 
you.xi gui.ze 
?He believes that for those investing overseas, 
there is a very important principle-one must know 
the local rules of the game, and accept them.? 
 b. ????????????????????
?????!
zhi.zheng.dang ye biao.shi, you.yu gong.shi 
zheng.yi tai da, kong.pa wu.fa quan.li zhi.chi 
?The KMT also commented that due to the many 
controversies surrounding PTV, it could not 
wholeheartedly support it either.? 
5a. ?????????????????????
????????
he.jia.ju jiu ren.wei??dian.shi you ji.ben yu.yan 
he wen.fa, yao jiang.jiu mai.dian he shi.chang??
?Ho Chia-chu says, "Television has its own 
fundamental language and grammar. You must 
consider selling points and the market."? 
b. ?????????????????????
???????????
ta biao.shi??wo xi.wang fuo.jiao.tu neng liao.jie, 
fu.quan she.hui yu jue.wu de she.hui shi bu 
xiang.he de??
?She says "I hope that followers of Buddhism can 
realize that a patriarchal society is incompatible 
with an enlightened society."? 
The above examples show that ren.wei and 
biao.shi can take both direct and indirect 
quotation. Yi.wei and jue.de, on the other hand, 
can only be used in reportage and cannot 
introduce direct quotation. 
Distinction between near synonymous pairs 
can be obtained from Sketch Difference. This 
function is verified with results from Tsai et al?s 
study on gao.xing?? ?glad? and kuai.le??!
?happy? (Tsai et al, 1998). Gao.xing ?glad? 
specific patterns include the negative imperative 
bie? ?don?t?. It also has a dominant collocation 
with the potentiality complement marker de?
(e.g. ta gao.xing de you jiao you tiao ????
???? ?she was so happy that she cried and 
danced?). In contrast, kuai.le ?happy? has the 
specific collocation with holiday nouns such as 
qiu.jie ??  ?Autumn Festival?. The Sketch 
Difference result is consistent with the account 
that gao.xing/kuai.le contrast is that inchoative 
state vs. homogeneous state. 
4. Evaluation and Future Developments 
An important feature of the prototype of the 
Chinese Sketch Engine is that, in order to test the 
robustness of the Sketch Engine design, the 
original regular expression patterns were adopted 
with minimal modification for Chinese. Even 
though both are SVO languages with similar 
surface word order, it is obvious that they differ 
substantially in terms of assignment of 
grammatical functions. In addition, the Sinica 
tagset is different from the BNC tagset and 
52
actually has much richer functional information. 
These are the two main directions that we will 
pursue in modification and improvement of the 
Chinese Sketch Engine. 
4.1 Word Boundary Representation 
Word breaks are not conventionalized in 
Chinese texts. This poses a challenge in Chinese 
language processing. The Chinese Sketch Engine 
inserted space after segmentation, which helps to 
visualize words. In the future, it will be trivial to 
allow the conventional alternative of no word 
boundary markups. However, it will not be trivial 
to implement fuzzy function to allow searches 
for non-canonical lemmas (i.e. lemmas that are 
segmented differently from the standard corpus). 
4.2 Sub-Corpora Comparison  
The Chinese Gigaword corpus is marked 
with two different genres, story and non-story. A 
still more salient sub-corpus demarcation is the 
one between Mainland China corpus and Taiwan 
corpus. Sketch Difference between lemmas form 
two sub-corpora is being planned. This would 
allow future comparative studies and would have 
wide applications in the localization adaptations 
of language related applications.  
4.3 Collating Frequency Information with 
POS
One of the convenient features of Sketch 
Engine that a frequency ranked word list is 
linked to all major components. This allows a 
very easy and informative reference. Since 
cross-categorical derivation with zero 
morphology is dominant in Chinese, it would 
help the processing greatly if POS information is 
added to the word list. Adding such information 
would also open the possibility of accessing the 
POS ranked frequency information. 
4.5 Fine-tuning Collocation Patterns  
The Sketch Engine relies on collocation 
patterns, such as (2) above, to extract 
collocations. The regular expression format 
allows fast processing of large scale corpora with 
good results. However, these patterns can be 
fine-tuned for better results. We give VN 
collocates with object function as example here. 
In (6), verbs are underlined with a single line, 
and the collocated nouns identified by English 
Word Sketch are underlined with double lines. 
Other nominal objects that the Sketch Engine 
misses are marked with a dotted line. 
6.a. In addition to encouraging kids to ask, think and 
do, parents need to be tolerant and appreciative to 
avoid killing a child's creative sense.
b. Children are taught to love their parents,
classmates, animals, nature . . . . in fact they are 
taught to love just about everything except to 
love China, their mother country. 
c. For example, the government deliberately chose 
not to teach Chinese history and culture, nor 
civics, in the schools. 
d. At the game there will be a lottery drawing for a 
motorcycle! And perhaps you'll catch a foul ball
or a home run.
The sentences in (6) show that the current Sketch 
Engine tend to only identify the first object when 
there are multiple objects. The resultant 
distributional information thus obtained will be 
valid given a sufficiently large corpus. However, 
if the collocation patterns are fine-tuned to allow 
treatment of coordination, richer and more 
precise information can be extracted. 
 A regular expression collocation pattern 
also runs the risk of mis-classification. For 
instance, speech act verbs often allow subject to 
occur in post-verbal positions, and intransitive 
53
verbs can often take temporal nouns in 
post-verbal positions too.  
7. a. ?you can say goodbye to your competitive 
career. 
b. `No,' said Scarlet, `but then I don't notice much.'
8. a. Where did you sleep last night?
  b. ?it arrived Thursday morning.
  c. From Arty's room came the sound of an 
accordion.
9. `I'll look forward to that.' `So will I.' 
Such non-canonical word orders are even more 
prevalent in Chinese. Chinese objects often occur 
in pre-verbal positions in various pre-posing 
constructions, such as topicalization. 
10. ???????????
quan.gu mian.bao, chi le hen jian.kang 
whole-grain bread, eat LE very healthy 
?Eating whole-grain bread is very healthy.? 
11a. ??????????????????
you ren chang.shi yao jiang zhe he.hua fen.lei,
que yue fen yue lei 
someone try to JIANG the lotus classify, but more 
classify more tired 
?People have tried to decide what category the 
lotus belongs in, but have found the effort 
taxing.? 
b. ??????????
wo yi.ding yao ba lao.da chu.diao
I must want BA the oldest (son) get rid of
?I really want to get rid of the older son.?
When objects are pre-posed, they tend to stay 
closer to the verb than the subject. Adding object 
marking information, such as ba?, jiang?, lian
?  would help correctly identify collocating 
pre-posed objects. However, for those unmarked 
pre-posed structures, closeness to the verb may 
not provide sufficient information. Several rules 
will need to be implemented jointly.  
 The above example underlines a critical 
issue. That is, whether relative position alone is 
enough to identify positional information. The 
Sketch Engine is in essence a powerful tool 
extracting generalizations from annotated corpus 
data. We have shown that it can extract useful 
grammatical information with POS tag alone. If 
the corpus is tagged with richer annotation, the 
Sketch Engine should be able to extract even 
richer information. 
 The Sinica Corpus tagset adapts to the fact 
that Chinese has a freer word order than English 
by incorporating semantic information with the 
grammatical category. For instance, locational 
and temporal nouns, proper nouns, and common 
nouns each are assigned a different tag. Verbs are 
sub-categorized according to activity and 
transitivity. Such information is not available in 
the BNC tagset and hence not used in the 
original Sketch Engine design. We will enrich the 
collocation patterns with the annotated linguistic 
information from the Sinica Corpus tagset. In 
particular, we are converting ICG lexical 
subcategorization frames (Chen and Huang 1990) 
to Sketch Engine collocation patters. These ICG 
frames, called Basic Patterns and Adjunct 
Patterns, have already been fully annotated 
lexically and tested on the Sinica Corpus. We 
expect their incorporation to improve Chinese 
Sketch Engine results markedly. 
6. Conclusion 
In this paper, we introduce a powerful tool 
for extraction of collocation information from 
large scale corpora. Our adaptation proved the 
cross-lingual robustness of the Sketch Engine. In 
particular, we show the robustness of the Sketch 
Engine by achieving better results through 
fine-tuning of the collocation patterns via 
integrating available grammatical knowledge. 
54
References 
Chen, Keh-Jiann and Huang, Chu-Ren. 1990.  
Information-based Case Grammar.  
Proceedings of the 13th COLING. Helsinki, 
Finland. 2:54-59. 
Chen, Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, 
Chi-Ching Luo, Ming-Chung Chang, and 
Chao-Jan Chen. 2003. Sinica Treebank: 
Design Criteria, Representational Issues and 
Implementation. In Anne Abeill?e, (ed.): 
Building and Using Parsed Corpora. Text, 
Speech and Language Technology,
20:231-248. Dordrecht: Kluwer.  
CKIP (Chinese Knowledge Information Processing 
Group). 1995/1998. The Content and 
Illustration of Academica Sinica Corpus.
(Technical Report no 95-02/98-04). Taipei: 
Academia Sinica  
Huang, Chu-Ren, Feng-Ju Lo, Hui-Jun Hsiao, 
Chiu-Jung Lu, and Ching-chun Hsieh. 2001. 
From Language Archives to Digital 
Museums: Synergizing Linguistic Databases. 
Presented at the IRCS workshop on linguistic 
Databases. University of Pennsylvania. 
Huang, Chu-Ren, Keh-Jiann Chen, and Lili Chang. 
1997. Segmentation Standard for Chinese 
Natural Language Processing. 
Computational Linguistics and Chinese 
Language Processing. 2(2):47-62.  
Kilgarriff, Adam and Tugwell, David. Sketching 
Words. 2002. In Marie-H?l?ne Corr?ard (ed.): 
Lexicography and Natural Language 
Processing. A Festschrift in Honour of B.T.S. 
Atkins. 125-137. Euralex.  
Kilgarriff, Adam, Chu-Ren Huang, Pavel Rychl?, 
Simon Smith, and David Tugwell. 2005. 
Chinese Word Sketches. ASIALEX 2005: 
Words in Asian Cultural Context. Singapore.  
Kilgarriff, Adam, Pavel Rychl?, Pavel Smrz and 
David Tugwell. 2004. The Sketch Engine. 
Proceedings of EURALEX, Lorient, France. 
(http://www.sketchengine.co.uk/) 
Leech, Geoffrey. 1992. 100 million words of 
English: the British National Corpus (BNC). 
Language Research 28(1):1-13 
Lin, Dekang. 1998. An Information-Theoretic 
Definition of Similarity. Proceedings of 
International Conference on Machine 
Learning. Madison, Wisconsin. 
(http://www.cs.umanitoba.ca/~lindek/publica
tion.htm) 
Tsai, Mei-Chih, Chu-Ren Huang, Keh-Jiann Chen, 
and Kathleen Ahrens. 1998. Towards a 
Representation of Verbal Semantics--An 
Approach Based on Near Synonyms. 
Computational Linguistics and Chinese 
Language Processing. 3(1): 61-74. 
Wu, Yiching and Liu, Mei-Chun. 2003. The 
Construction and Application of Mandarin 
Verbnet. Proceedings of the Third 
International Conference of Internet Chinese 
Education. 39-48. Taipei, Taiwan. 
Xia, Fei, Martha Palmer, Nianwen Xue, Mary Ellen 
Okurowski, John Kovarik, Fu-Dong Chiou, 
Shizhe Huang, Tony Kroch, and Mitch 
Marcus. 2000. Developing Guidelines and 
Ensuring Consistency for Chinese Text 
Annotation. Proceedings of the second 
International Conference on Language 
Resources and Evaluation (LREC 2000), 
Athens, Greece.  
    (http://www.cis.upenn.edu/~chinese/ctb.html)
Xue, Nianwen and Palmer, Martha. 2003. 
Annotating Propositions in the Penn Chinese 
Treebank. Proceedings of the Second Sighan 
Workshop. Sapporo, Japan. 
     (http://www.cis.upenn.edu/~xueniwen/) 
Xue, Nianwen and Palmer, Martha. 2005. 
Automatic Semantic Role Labeling for 
Chinese Verbs. Proceedings of the 19th 
International Joint Conference on Artificial 
Intelligence. Edinburgh, Scotland. 
     (http://www.cis.upenn.edu/~xueniwen/) 
Websites
Sinica Corpus.  
http://www.sinica.edu.tw/SinicaCorpus/  
British National Corpus (BNC). 
http://www.natcorp.ox.ac.uk/  
Center for Chinese Linguistics, PKU.  
http://ccl.pku.edu.cn/#  
Corpora And NLP (Natural Language Processing) 
for Digital Learning of English (CANDLE). 
http://candle.cs.nthu.edu.tw/candle/     
FrameNet.  
http://www.icsi.berkeley.edu/~framenet/  
Penn Chinese Treebank. 
http://www.cis.upenn.edu/~chinese/ctb.html  
Proposition Bank.  
http://www.cis.upenn.edu/~ace/  
Sinica Treebank.   
http://treebank.sinica.edu.tw/  
Sketch Engine (English).  
http://www.sketchengine.co.uk/     
Sketch Engine (Chinese).  
http://corpora.fi.muni.cz/chinese/  
Sou Wen Jie Zi-A Linguistic KnowledgeNet. 
http://words.sinica.edu.tw/ 
55
The Robustness of Domain Lexico-Taxonomy:  
Expanding Domain Lexicon with CiLin  
Chu-Ren Huang
Institute of Linguistics, 
Academia Sinica, Taipei 
churen@sinica.edu.tw
Xiang-Bing Li 
Institute of Information Science, 
Academia Sinica, Taipei  
dreamer@hp.iis.sinica.edu.tw 
Jia-Fei Hong
Institute of Linguistics, 
Academia Sinica, Taipei
jiafei@gate.sinica.edu.tw
Abstract.
This paper deals with the robust 
expansion of Domain Lexico- 
Taxonomy (DLT). DLT is a domain 
taxonomy enriched with domain lexica. 
DLT was proposed as an infrastructure 
for crossing domain barriers (Huang et 
al. 2004). The DLT proposal is based 
on the observation that domain lexica 
contain entries that are also part of a 
general lexicon. Hence, when entries of 
a general lexicon are marked with their 
associated domain attributes, this 
information can have two important 
applications. First, the DLT will serve 
as seeds for domain lexica. Second, the 
DLT offers the most reliable evidence 
for deciding the domain of a new text 
since these lexical clues belong to the 
general lexicon and do occur reliably in 
all texts. Hence general lexicon 
lemmas are extracted to populate 
domain lexica, which are situated in 
domain taxonomy. Based on this 
previous work, we show in this paper 
that the original DLT can be further 
expanded when a new language 
resource is introduced. We applied 
CiLin, a Chinese thesaurus, and added 
more than 1000 new entries for DLT 
and show with evaluation that the DLT 
approach is robust since the size and 
number of domain lexica increased 
effectively. 
1.  Introduction 
Domain-based language processing has an 
inherent research dilemma when the 
construction of domain lexicons is involved. 
The standard approach of building domain 
lexicon from domain corpora requires a very 
high threshold of existing domain resources and 
knowledge. Since only well-documented 
domains can provide enough quality corpora, it 
is likely these fields already have good manually 
constructed domain lexica. Hence this approach 
is can only deal with domains where only 
marginal benefit can be achieved, while it 
cannot deal with domains where it can make 
most contribution since there is not enough 
resources to work with. 
It was observed that the type of domain 
language processing that has the widest 
application and best potentials are cross-domain 
and multi-domain in nature. For instance, a 
typical web-search is a search for specific 
domain information from the www as an archive 
of mixed and heterogeneous domains. The 
contribution will be immediate and salient to be 
able to acquire resources and information for a 
new domain that is not well documented yet. 
A new approach towards domain language 
processing by constructing an infrastructure for 
multi-domain language processing called the 
Domain Lexico-Taxonomy (DLT) was proposed 
in Huang et al (2004). In the DLT approach, 
domain lexica are semi-automatically acquired 
to populate domain taxonomy. This lexically 
populated domain taxonomy serves two 
purposes: as the basis of stylo-statistical 
prediction of the domain of a new text, and as 
the core seed of complete domain lexica. For the 
first purpose, the DLT approach relies crucially 
on the ability to effectively identify words that 
are good indicators of specific domains. For the 
second purpose, the DLT needs to be robust 
enough to allow incremental expansion when 
new content resources are integrated. In this 
study, we integrate CiLin, a Chinese thesaurus, 
to show that the DLT architecture is indeed 
robust.
103
2.  Related Work
Typical studies on domain lexica focuses on 
assigning texts to specific classes, hence they
use a limited taxonomy augmented with a small 
set of features (e.g. Avancini et al 2003,
Sebastiani 2002, and Yand and Pederson 1997).
However, specialized lemmas cannot be useful 
in multi-domain processing. To achieve domain
versatility in processing, it is necessary to 
identify lemmas with wider distributions and yet
is associated with particular domain(s). We
follow the DLT architecture (Huang et al 2004), 
which was shown to be effective in predicting 
the domain of documents extracted from the 
web. We aim to elaborate that framework by 
proposing a domain lexica can be incrementally 
expanded with knowledge from a new resource. 
3.  Domain Taxonomy
A domain taxonomy containing 549 nodes was 
manually constructed. The main sources of
domain classification are from Chinese Library 
Classification system, Encyclopedia Britannica 
and the Global View English-Chinese dictionary.
Two important criteria were chosen: that the 
taxonomy is bilingual and that it is maintained
locally. First, the bilingual taxonomy is essential
for future cross-lingual processing but also 
allows us to access relevant resources in both 
languages. Second, since our emphasis was not 
on the correctness of a dogmatic taxonomy but
on the flexibility that allows monotonic
extensions, it is essential to be able to monitor
any changes in the taxonomy.
There are four layers in the constructed 
domain taxonomy. Fourteen (14) domains are in 
the upper layer, including Humanities, Social 
Science, Formal Science, Natural Science,
Medical Science, Engineering Science,
Agriculture and Industry, Fine Arts, Recreation,
Proper Name, Genre/Strata, Etymology, Country
Name, Country People. The Second layer has 
147 domains. The third layer has 279 domains.
Lastly the fourth layer has only 109 domains
since not all branches need to be expanded at 
this level. In sum, there are 549 possible domain
tags when the hierarchy is ignored. The domain
taxonomy is available online at the Sinica BOW
website (http://BOW.sinica.edu.tw/, Huang and
Chang 2004).
4. Detection of Domain Lexicon in DLT
The challenge in integrating heterogeneous 
language resources for domain information is 
that conceptual classification varies from one 
resource to another and hence cannot be directly
harvested. We propose to utilize the inheritance 
relations of these resources, instead of their 
hierarchy. In other words, lexical (and hence 
conceptual) identity is established first, 
following by expanding this matching with 
logical inheritance but without branching out on
the conceptual hierarchy.
DLT establish the correspondences 
between the taxonomic nodes of domains and 
the linguistic resources of sub-lexica. Note that 
a lexical knowledgebase, in a Wordnet fashion,
also contains hierarchical relations. The domain
taxonomy can be enriched by taking the
hierarchical information internal to the lexica. If 
these resources directly encodes the ?is-a?
relation by hyponymy, we assume that both the
node (lexicons) and their hyponym node 
(lexicons) belong to that domain. Using the
simple supposition, we can observe the domain
knowledge with various resources, and
strengthens the domain lexica for domain
taxonomy. The process of populating DLT is
shown in Fig. 1. 
Figure. 1. Populating DLT from Linguistic Resource
104
5.  Experiment 
5.1. The Original Study with Bilingual 
WordNet
The original DLT work was based on bilingual 
Wordnet (Huang et al 2004). This is because of 
the Wordnet lexical knowledgebase is highly 
enriched with lexical semantic relation 
information. In addition, the bilingual Wordnet 
adds an unparallel dimension of knowledge 
coverage. The bilingual Wordnet used is Sinica 
BOW (The Academia Sinica Bilingual 
Ontological WordNet, Huang and Chang 
(2004)). Sinica BOW is bilingual lexical 
knowledgebase connecting WordNet and SUMO 
and mapping both between English and Chinese. 
The study reported in Huang et al (2004) also 
contains a small domain identification 
experiment to show the application of DLT.  
5.1.1 Description of WordNet and Sinica 
BOW
WordNet is inspired by current psycholinguistic 
and computational theories of human lexical 
memory (Fellbaum (1998), Miller et al (1993)). 
English nouns, verbs, adjectives, and adverbs 
are organized into synonym sets, each 
representing one underlying lexicalized concept. 
Different semantic relations link the synonym 
sets (synsets). The version of WordNet that 
Sinica BOW implemented is version 1.6, with 
nearly 100,000 synsets. 
In Sinica BOW, ach English synset was 
given up to 3 most appropriate Chinese 
translation equivalents. And in cases where the 
translation pairs are not synonyms, their 
semantic relations are marked (Huang et al 
2003). The bilingual WordNet is further linked 
to the SUMO ontology. We use the semantic 
relations in bilingual resource to expand and 
predict domain classification when it cannot be 
judged directly from a lexical lemma. 
5.1.2 Experiment and Result with WordNet 
463 of the 549 nodes in the domain taxonomy 
were successfully mapped to a WordNet synset 
through an identical lemma. 452 or 463 
mappings were manually confirmed to be 
correct, a precision score of over 97%. These 
domains were expanded to cover a total of 
11,918 synsets corresponding to 15,160 Chinese 
lemmas. Note that both English and Chinese 
correspondences are used since our resources 
(WordNet and domain taxonomy) are both 
bilingual.
Due mostly to hyponymy expansion, each 
lemma is mapped to 1.38 domains in average. 
While each lemma is assigned to no more than 8 
domains, with the majority (6,464) assigned to 
only one. These mapped lemmas populate a set 
of domain lexica. The number of entries in these 
domain lexica ranges from 1 to 3762. The 
average size of these domain lexica is 32.8 
lemmas. Only 41 domains lexical contain 33 or 
more lemmas. Since we cannot know the 
effective of the lexicon of a domain a priori, we 
take those whose size are above average as the 
effective domain lexica. 
These domain lexica and their sizes are 
shown in Table 1. 
5.1.3 Evaluation: precision of domain lexica 
It is impossible to formally evaluate the recall 
rate of this domain lexica study since we do not 
know the total number of entries to be recalled. 
However, it is possible to evaluate the precision 
rate of the constructed domain lexica. First, the 
precision of all recalled lemmas is tested. 
Among the mapped lemmas, 8696 (out of 
15,160) lemmas are assigned to multiple 
domains, while 6,464 are assigned to single 
domain. The single domain mappings were 
spot-checked to be correct. On the other hand, 
the precision of all 8,696 multi-domain lemmas 
are carefully evaluated. Among these lemmas, 
only 4.81% (418) proves to be wrong; and an 
overwhelming majority of 95.19% turns out to 
be correct (8278). 
Second, a more meaningful test is to 
evaluate how well the domain lexica are defined. 
Five effective domain lexica with over 100 
entries were randomly chosen for evaluation: 
Insect (515 entries), Natural Science (262 
entries), Sports (180 entries), Dance (124 entries) 
and Religious Music (48 entries).  
The manually checked precision of these 
domain lexica is listed below the Table 2: 
105
Domain Domain Domain Domain
Vertebrates 
???3676 ? Food ?2968 ? Bird ??  1059 Fish ?? 729 
Language ??
699 
Recreation
???? 548 Insect ??  515 
Natural Science 
???? 262 
Country?? 250 contest?? 207 music?? 192 Indian???188 
Sports!?? 180 commerce?? 144 Business ?? 144 Dance?? 124 
Heraldic design 
???? 120 
Medical Science 
????  85 Medicine ?? 76 
Pathological 
medicine ???
? 76 
Clinical medicine 
???? 76 
Mathematics 
??  69 
Humanities 
???64 ? 
Social Science 
????  62 
physics??? 56 Religion?? 52 
Religious Music 
???? 48 
Plastic art 
???? 45 
Pure mathematics 
??? 44 
Anthropology 
??? 42 
Earth science 
???? 39 drawing?? 4:
Norse Mythology 
???? 39 Philosophy ?? 37
Telecommunication
???? 35 theater?? 34 
Fine Arts?? 33 
Table 1. Domain lexica containing 33 or more lemmas 
Domain Label # of entries Precision (%) 
Insect 515 99.03 
Natural Science 262 69.85 
Sports 180 86.11 
Dance 124 100.00 
Religious Music 48 93.75 
Table 2. Size and Precision of selected domain lexica 
Table 2 shows an overall precision of over 95%, 
while no other lexica has precision lower than 
86%, natural science is lowest at just below 70%. 
This is because ?Natural Science? is a higher 
level domain and hence open to more noises in 
the detection process. This study clearly showed 
that the WordNet helped to effectively build 
core domain lexica. 
We take the domain ?Dance? as an 
example to explain the process. First, we map 
?Dance? to the Wordnet synset??dance?, and 
we look for the hyponym synsets. Table3 will be 
shown the expanding lexica of one of hyponym 
synsets. These lexical entries are associated with 
domain ?Dance? and populate the domain 
lexicon.
Level synset 
1 social_dancing 
2 folk_dancing, folk_dance 
3 country-dance, country_dancing, 
4          square_dance, square_dancing 
5             quadrille 
5.2 For CiLin 
5.2.1 Description of CiLin 
CiLin, a short name for Tongyici CiLin, is a 
Chinese thesaurus published in 1984 (Mei et al 
1984). The terms in CiLin are organized in a 
conceptual hierarchy, with near-synonym terms 
forming a set. There are five levels in the 
taxonomy structure of CiLin. The CiLin terms 
between Level1 to Level4 are taxonomy 
categories. Level1 is the upper class, and it 
includes 12 categories, like as people, object, 
time and space, abstract etc. Level2 has 106 
categories. Level3 has 3,948 categories. Level4 
has 4,014 categories. There are 64,157 terms in 
Level5 since all branches need to be expanded 
at this level. These terms are classified to 12,193 
sets by the meaning. The average number of 
terms in each set is 5.34. Fig. 2 shows the 
structure of CiLin. 
Table 3. The expanding hyponym synsets of ?dance? 
106
Figure. 2. The structure of CiLin
5.2.2. Experiment and Result with CiLin
First, we map the 549 domains to CiLin?s
taxonomy. Unlike the previous study, only
Chinese terms were available on CiLin. The
result is given in Table 4.
# of
entries
# of
domains entries/domains
Leve1 1 146 1 146
Level 2 1,587 3 529
Level 4 1,222 32 38.19
Table 4. Number of expanding entries and 
mapping domains
Manual checking showed that mappings to 
Level 1 and Level 2 are both imprecise and 
small in number. Hence we take Level 4 as the 
lexical anchor for enriching domain lexica.
1,222 lexical items are expanded from 32 
domains, and these domain lexica and their sizes
are shown in Table 5. 
Domain Domain
Insect(??) -- 146 Sewing(??) -- 25
Country(??) -- 128 Movie(??) -- 25
Theater(??) -- 116 Game(??) -- 25
Painting(??) -- 88 Photography(??) -- 21
Capital(??) -- 54 Payment(??) -- 20
Cookery(??) -- 52 Printing(20 -- (??
Dance(??) -- 52 Literature(??) -- 18
Law(??) -- 50 Investment(??) -- 14
Education(??) -- 47 Swimming(??) -- 12
Martial_art(??) -- 45 Broadcasting(??) -- 11
Religion(??) -- 39 Ranching_and_animal_husbandry(??) -- 10
Architecture(??) -- 38 Textile_industry?? 10
Carving(?37 -- (? Boating(??) -- 8 
Language(??) -- 37 Trade(?7 -- (? 
Table 5. Domain lexica 
When all mappings are evaluated, 873(71.44%)
of them are correct, and 349 (28.56%) incorrect. 
Five effective domain lexica are evaluated, as
shown below in Table 6: 
Domain Label # of entries Precision (%)
Insect 146 58.9
Country 128 55.47
Theater 116 80.17
Painting 88 80.68
Dance 52 80.77
Table 6. Size and Precision of selected domain lexica
Compared with the work reported in (Huang et 
al. 2004), both the number of lemma (1,222 vs.
15,160) and precision (71.44% vs. nearly 95%)
are lower. This result is expected since CiLin
has a simple taxonomy without the rich lexical 
information of a Wordnet. The crucial fact
shown, however, is that DLT can be
incrementally enhanced with the new mappings. 
Of the 873 correct domain lexica entries, 79.5%
(694) are new entries that were not identified
previously. Even more impressive is the 
effectiveness of increase in lexica sizes for 
applicable domains, as shown below in Table 7. 
107
domain WN/old CiLin/new increase domain WN/old CiLin/new!increase!
?? 34 80 0.7018?? 12 15 0.5556
?? 515 65 0.1121?? 22 15 0.4054
?? 17 61 0.7821?? 28 14 0.3333
?? 250 44 0.1497?? 192 12 0.0588
?? 124 34 0.2152?? 20 11 0.3548
?? 7 33 0.8250?? 23 10 0.3030
?? 26 33 0.5593?? 15 9 0.3750
?? 14 32 0.6957?? 5 8 0.6154
?? 2 29 0.9355?? 9 7 0.4375
?? 2 27 0.9310?? 2 7 0.7778
?? 22 24 0.5217?? 0 5 1.0000
?? 26 23 0.4694?? 16 5 0.2381
?? 699 22 0.0305?? 27 4 0.1290
?? 52 21 0.2877?? 4 4 0.5000
?? 55 19 0.2568?? 6 2 0.2500
?? 21 17 0.4474?? 1 2 0.6667
Table 7. Increase in Domain Lexicon Size after CiLin Integration 
Table 7 shows that, even though adding CiLin 
only helped 32 domain lexica, 14 of them have 
their lexicon size more than doubled. One of 
them, ranching and animal husbandry is a new 
domain lexicon where no mapping was possible 
with WordNet. In other words, adding the CiLin 
resource substantially enhanced effective 
domain coverage of DLT. 
6.  Conclusion 
In this paper, test the robustness of the DLT 
architecture. We show both the coverage and the 
sizes of the domain lexica on DLT can be 
effectively expanded by integrating a new 
language resource. The robustness is convincing 
given that the coverage and quality of the new 
resource is actually not as good as the original 
reference resources. In other words, we showed 
the open architecture of DLT facilitates 
integration of new domain information without 
imposing any high threshold on the format and 
quality of new resources. We also verify partial 
results of previous work since 205 lemma 
mappings were repeated. For future work, we 
plan to continue to populate DLT, as well as to 
explore other possibilities for putting DLT to 
actual applications. 
References 
Chu-Ren Huang, Elanna I. J. Tseng, Dylan B. S. Tsai, 
Brian Murphy.  Cross-lingual Portability of 
Semantic relations: Bootstrapping Chinese 
WordNet with English WordNet Relations. 
Languages and Linguistics. 4.3. (2003)509-532 
Chu-Ren, Huang and Ru-Yng Chang. Sinica BOW 
(Bilingual Ontological Wordnet): Integration of 
Bilingual WordNet and SUMO?. Presented at the 
4th International Conference on Language 
Resources and Evaluation (LREC2004). Lisbon. 
Portugal. 26-28 May (2004) 
Chu-Ren Huang, Xiang-Bing Li, Jia-Fei Hong. 
"Domain Lexico-Taxonomy:An Approach 
Towards Multi-domain Language Processing",
Asian Symposium on Natural Language 
Processing to Overcome Language Barriers, The 
First International Joint Conference on Natural 
Language Processing (IJCNLP-04). Sanya City, 
Hainan Island, China. 22-24 March (2004) 
F. Sebastiani., ?Machine learning in automated text 
categorization?. ACM Computing Surveys, 34(1) 
(2002)1-47 
Fellbaum C.. WordNet: An Electronic Lexical 
Database. Cambridge: MIT Press (1998) 
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross 
and K. Miller. ?Introduction to WordNet: An 
On-line Lexical Database,? In Proceedings of the 
fifteenth International Joint Conference on 
108
Artificial Intelligence. Chamb?ry, France. 28 
August- 3 September (1993) 
Henri Avancini, Alberto Lavelli, Bernardo Magnini, 
Fabrizio Sebastiani, Roberto Zanoli. Expanding 
Domain-Specific Lexicons by Term 
Categorization. Proceedings of the 2003 ACM 
symposium on Applied computing. Melbourne, 
Florida, USA. 9-12 March (2003) 
Jia-ju Mei, Yi-Ming Zheng, Yun- Qi Gao and Hung- 
Xiang Yin. TongYiCi CiLin. Shanghai: the 
COMMERCIAL Press (1984) 
Y. Yang and J. O. Pedersen. A comparative study on 
feature selection in text categorization. In D. H. 
Fisher, editor, Proceedings of ICML-97, 14th 
International Conference on Machine Learning, 
pages 412 420. San Francisco: Morgan Kaufmann 
(1997) 
109
Cross-lingual Conversion of Lexical Semantic Relations: Building 
Parallel Wordnets 
Chu-Ren Huang1 , I-Li Su1, Jia-Fei Hong1, Xiang-Bing Li2
1. Institute of Linguistics 
2. Institute of Information Science 
Academia Sinica, 
No.128 Academic Sinica Road, SEC.2 Nankang, 
Taipei 115, Taiwan 
1.{ churen, isu, jiafei }@gate.sinica.edu.tw 
Abstract
Parallel wordnets built upon 
correspondences between different 
languages can play a crucial role in 
multilingual knowledge processing. Since 
there is no homomorphism between pairs of 
monolingual wordnets, we must rely on 
lexical semantic relation (LSR) mappings to 
ensure conceptual cohesion. In this paper, 
we propose and implement a model for 
bootstrapping parallel wordnets based on 
one monolingual wordnet and a set of 
cross-lingual lexical semantic relations. In 
particular, we propose a set of inference 
rules to predict Chinese wordnet structure 
based on English wordnet and 
English-Chinese translation relations. We 
show that this model of parallel wordnet 
building is effective and achieves higher 
precision in LSR prediction. 
1 Introduction 
A knowledgebase which systemizes 
lexical and conceptual information of 
human knowledge is a basic infrastructure 
for Natural Language Processing (NLP) 
applications. Wordnets, pioneered by the 
Princeton WordNet (WN, Fellbaum 1998), 
and greatly enriched by EuroWordnet (EWN, 
Vossen 1998), have become the standard for 
a lexical knowledgebase enriched with 
lexical semantic relations. In addition to the 
multilingual architecture of EWN, there are 
some proposals to construct the expansion 
for monolingual wordnets to parallel 
wordnet systems, such as Pianta and Girardi 
(2002). However, the construction of 
multilingual wordnets eventually faces the 
challenge of low-density languages, which 
is dealt with in Huang, et al (2002). 
Low-density languages, as opposed to 
high-density languages, usually refer to 
languages that are not spoken by a large 
number of people. However, there is neither 
a direct correspondence between language 
population and language technology, nor an 
objective population number that defines 
density level. In this work, we use the 
availability of language resources instead to 
define language density. That is, low-density 
languages are languages that do not have 
enough language resources to support fully 
automated language processing, such as 
machine translation. In our current line of 
work, we (Huang et al 2002) refer to 
low-density languages as those which do not 
have enough existing resources for 
semi-automatic construction of monolingual 
wordnet.
There are two alternative approaches to 
build parallel wordnets, as shown in Figure 
1. The first approach relies on two fully 
annotated monolingual wordnets with 
synsets and LSR?s. The second approach 
requires only one fully annotated WN in 
addition to LSR-based cross-lingual 
translation correspondences.  
48
Figure1. Two Approaches to Building Bilingual Wordnets
Approach I maps and pairs Language A
synsets with Language B synsets and 
annotates cross-lingual LSR?s. The result is 
a fully annotated parallel wordnet. Approach
II maps language A synsets to language B 
through translation equivalents. After 
language B synsets are thus established, 
language B LSR?s are predicted based on
corresponding LSR?s in language A. A new 
set of monolingual LSR?s is bootstrapped
and predicted basing on inference rules 
governed by translation LSR?s (T-LSR?s). In 
general, approach I applies to high-density
languages while approach II applies to
low-density languages. In this paper, we will 
focus on the application of approach II to
build a Chinese Wordnet with conceptual
cohesion.
The current model was first explored in
Huang et al (2003). This previous study
covered 210 lemmas, consisted of the top
ranked lemmas in each part-of-speech 
(POS). The translation LSR?s discussed in 
the previous model were antonymy,
hypernymy and hyponymy. In this current
work, we expand our study to all possible
LSR?s as well as to all the bilingual lexical 
pairs in our English-Chinese translation
equivalents databases. Moreover, the LSR?s
in Princeton WordNet are again used as the 
basis for bootstrapping. In addition, we 
establish a set of evaluation for the results.
The approach will be evaluated in term of 
both the precision of prediction and the
confidence of prediction. We aim to show 
that T-LSR?s bootstrapped approach does
provide an effective model for building
parallel wordnets for low-density languages. 
After the introduction, the main part of 
this paper consists of the following sections:
in section 2, we briefly introduce the 
existing resources required for this work. 
We discuss methodology of T-LSR
bootstrapping step by step in section 3. A 
series of LSR-predicting inference rules are
also given in this section. In section 4, we 
plan to evaluate the results of our 
experiment and demonstrate the feasibility
of maintaining conceptual cohesion in
cross-lingual LSR mapping.
2 Required Resources: ECTED and 
WN
As we mentioned above, the T-LSR
approach to parallel wordnet requires two
language resources: a fully annotated 
monolingual wordnet and a set of translation
LSR?s to map the wordnet information to 
the target language. In our current study, we 
use the English WN as the source of synset
and LSR information. The semantic relation 
between an English synset and its Chinese 
translation is based on The English-Chinese 
Translation Equivalents Database (ECTED, 
Huang et al 2002). 
2.1 The English-Chinese Translation
Equivalents Databases (ECTED)
The basic idea of ECTED is to provide 
the Chinese translation equivalents for each 
APPROACH I
Given fully annotated
monolingual wordents
with synsets and LSRs
Fully annotated
parallel wordnet
APPROACH II
Given fully annotated WN 
of language A; and
bilingual translation
equivalents annotated
with LSR
Map LSR-annotated
synsets in Language A to
Language B through
translation LSRs (T-LSR?s)
Grow LSR links among
Language B synsets by 
using language A LSR 
and cross-lingual LSR
inference rules
Map and pair Language A and
Language B synsets with 
cross-lingual LSRs
49
WN English synset. Our ECTED was 
bootstrapped with a combined lexical
knowledgebase integrating at least four
English-Chinese or Chinese-English 
bilingual resources. Based on this combined
LKB, a group of translators chose (or
created) up to three best translation
equivalents for each WN synset. In addition, 
for each English-Chinese translation
equivalent, a lexical semantic relation is
annotated. In addition to synonym, the 
semantic relations marked including
antonym, hypernym, hyponym, holonym,
meronym, and near-synonym. We use all 
semantic relations, with the exception of
antonymy, in this study.
2.2 Wordnet (WN)
The Cognitive Science Laboratory of
Princeton University created WN, a lexical
knowledgebase for English, in 1990
(Fellbaum, 1998). Synsets (a group of
form-meaning pairs sharing same sense) are
the main units used in WN to organize the 
lexicon conceptually. Each sense can be
expanded either by gloss or context. It is 
easy for users to distinguish each sense by 
simply checking the synonyms, the example
sentences or explanation. Nouns, verbs,
adjectives and adverbs are the main lexical
categories to classify all the lexicons. Such 
classification of lexicons is based on the 
principles in psycholinguistics. Besides, the 
semantic relations of each sense in WN are 
also expressed like a Word-network. In 
other words, WN resembles an ontology
system and links all the semantic relations
of words. Therefore, English WN is not just 
a lexical knowledgebase but also an 
ontological system that expresses the 
semantic relations and the concepts of
words.
The current version of WN is Wordnet
2.0, but Wordnet 1.6 is more widely used by
the most applications in NLP and linguistic 
research. Therefore, after considering the 
compatibility with other applications, we 
connected the ECTED with Wordnet 1.6.
However, we are still working on keeping 
updating our systems by using the content in 
the new version of WN. We believe this will 
keep the information updated and shorten
the gap caused by the different versions of 
WN.
3 Inferring Lexical Semantic 
Relations for WN and ECTED 
As we mentioned above, WN does not 
only express the knowledge of lexicons but 
also cover the semantic relations of lexicons.
Therefore, in order to present such semantic
relations clearly and logically, Huang (2002)
proposed to use cross-lingual Lexical
Semantic Relations (LSRs) to predict the
semantic relations in the target language. 
The proposed framework is shown in
Diagram 1. 
Diagram 1. Translation-mediated LSR (the complete model)
In Diagram1, EW1 and EW2 are head 
words for two different English synsets.
CW1 and CW2 are translation equivalents
in ECTED for these two head words. LSR i 
and ii are the T-LSRs stipulating the 
semantic relations between the head words 
and their Chinese TEs. In WN, each synset
is linked to a network of their synsets
through a number of LSR?s. Hence, we use 
LSR x to represent the semantic relation 
CW1 EW1(Synset number)
EW2(Synset number)CW2
y
i
x
ii
x = EW1-EW2
y = CW1-CW2
i = Translation LSR
ii = Translation LSR
The unknown LSR y = i+x+ii 
50
between EW1 and EW2. The four LSR?s
form a closed network that includes three 
know LSR?s: two T-LSRs, i and ii, and one
English LSR, x, from WN. The only
unknown LSR is y, the semantic relation
between CW1 and CW2. Huang et al(2002) 
claimed that LSR y can be inferred as a 
functional combination of the three LSRs - i, 
x and ii. 
Language translation does not only
involve the semantic correspondences but 
also the human decision in choosing
translation equivalents that are affected by
the social and cultural factors. Our main
priority in this paper is to infer the lexical 
semantic information across different
language rather than the translational 
idiosyncrasies, so the elements regarding 
translational idiosyncrasies are excluded 
here. In order to simplify the complexity of 
LSR combination and get a better prediction 
of LSR, here, we only take account of the
situations when LSR ii is exactly equivalent,
EW2=CW2 or ii=0. Therefore, we have a 
reduced model of the translation-mediated 
LSR Prediction as shown in diagram 2. 
Diagram 2. Translation-mediated LSR (the reduced model)
Synonym, hypernym, hyponym,
holonym, meronym and near-synonym are 
the main semantic relations that we will
discuss in the following sections. First of all, 
we would like to discuss the foundational
situation of LSR prediction, synonym, as 
shown in diagram 3. When translation LSR i 
is exactly equivalent, i.e. CW1=EW1, and 
LSR ii is also exactly equivalent, i.e. 
EW2=CW2, the LSR combination, LSR y,
is directly inherited the semantic relation of
LSR x. 
Diagram 3. Translation-mediated LSR (When TEs are synonymous)
CW1 EW1(Synset number)
EW2(Synset number)=CW2 (ii=0)
y
i
x
The unknown LSR y= i + x
CW1 EW1(Synset number)
EW2(Synset number)=CW2 (ii=0)
y
CW1=EW1(i=0)
x
The unknown LSR y= 0 + x = x 
51
Diagram 4. Examples of the LSR (When TEs are synonymous)
As shown in diagram 4 above,
according to the ECTED, the English head 
word ?thin? is exactly equivalent with
?shou4? in Chinese. The LSR x between 
EW1 and EW2 in WN is marked ?ANT?
which means ?fat? is the antonym of ?thin.?
Therefore, according to the prediction in 
diagram 3, we can infer that the CW2
(fei2pang4de5) is the antonym of CW1 
(shou4). The above inference can also be
applied to another example in diagram 4. 
The LSR prediction in WN plays a very
crucial role in determining the unknown 
LSR y. Even an English head word may
have more than one sense, it is still very
clear to infer the LSR between the TEs. 
However, there is a potential problem within
this inference. If a head word has more than 
one Chinese TEs which can all correspond 
to the head word, there might be a problem 
to consider whether those TEs are really
synonyms.
However, the situation is not always
that ideal as above. When the Chinese 
translation equivalents and the corresponded 
English synset have a non-identical
semantic relation, CW1?EW1, the 
prediction of LSR y needs to be considered
further and carefully.
fei2pang4d fat (00934421A)
chubby(00935062A) = feng1man3de5
y =NSYN
CW1=EW1(i=0)
x = NSYN 
shou4 thin (00936334A)
fat(00934421A) = fei2pang4de5
y = ANT
CW1=EW1(i=0)
x= ANT
52
Diagram 5. Predicting LSR (Hypernym) and its example
Diagram 6. Predicting LSR(Hyponym) and its example 
Logically, hypernym and hyponym are 
symmetric semantic relations. For instance,
if A is a hypernym of B, B is a hyponym of 
A. For instance, as shown in diagram 5, the
English word ?nick? is the hypernym of the 
Chinese term ?shang1kou3? and ?cut? is the 
hypernym of ?nick? in WN and the exact 
translation equivalent of ?cut? in Chinese is 
?jian3kai1.? According to the logicality,
?jian3kai1? is the hypernym of 
?shang1kou3.? The example of hyponym is 
shown in diagram 6. Due to the varied
semantic relations in WN, the inferences of 
LSRs , the unknown LSR y = i + x ,for
hypernym, hyponym, near-synonym,
holonym, and mernoym are listed as below: 
Hypernym(HYP)
(a) IF x=ANT 
LSR y =HYP +ANT =ANT (CW2 is the 
antonym of CW1.) 
(b) IF x=HYP 
LSR y = HYP+HYP =HYP (CW2 is the 
hypernym of CW1.) 
(c) IF x= NSYN 
LSR y = HYP+NSYN =HYP (CW2 is the
hypernym of CW1.) 
(d) IF x = HOL 
LSR y = HYP+HOL =HOL (CW2 is the 
holonym of CW1.)
(e) IF x = all other LSR 
LSR y = HYP +all other LSRs = ? 
(Undecided)
Hyponym(HPO)
(a) IF x=ANT 
LSR y =HPO +ANT =ANT (CW2 is the
antonym of CW1.) 
(b) IF x=HPO 
LSR y = HPO+HPO =HPO (CW2 is the
hyponym of CW1.)
(c) IF x= NSYN 
LSR y = HPO+NSYN =HPO (CW2 is the 
hyponym of CW1.)
(d) IF x = MER 
LSR y = HPO+MER =MER (CW2 is the 
meronym of CW1.) 
(e) IF x = all other LSR 
LSR y = HPO +all other LSRs = ? 
(Undecided)
gao1dian3 pastry(05670938N)
baklava(05674827N)=guo3ren2mi4tang2qian1ceng2bing3
y
i= HPO 
x= HPO 
The unknown LSR y
= i + x
=HPO +HPO =HPO
(?guo3ren2mi4tang2qian1ceng2bing3? is the
hyponym of ?gao1dian3?)
shang1kou3 nick(00248910N)
cut(00248688N)=jian3kai1
y
i= HYP (?nick? is the hypernym of ?shang1kou3? )
x= HYP (?cut? is the hypernym of ?nick?)
The unknown LSR y 
= i + x
=HYP +HYP =HYP
(?jian3kai1? is the hypernym of ?shang1kou3?)
53
Near-Synonym(NSYN) 
(a) IF x=ANT 
LSR y = NSYN+ANT =ANT (CW2 is the 
antonym of CW1.) 
(b) IF x=HYP 
LSR y = NSYN+HYP =HYP (CW2 is the 
hypernym of CW1.) 
(c) IF x=HPO 
LSR y = NSYN+HPO =HPO (CW2 is the 
hyponym of CW1.) 
(d) IF x= NSYN 
LSR y = NSYN+NSYN =NSYN (CW2 is 
the near-synonym of CW1.) 
(e) IF x = MER 
LSR y = NSYN+MER =MER (CW2 is the 
meronym of CW1.) 
(f) IF x = HOL 
LSR y = NSYN+HOL =HOL (CW2 is the 
holonym of CW1.) 
Holonym(HOL) 
(a) IF x=ANT 
LSR y = HOL+ANT =ANT (CW2 is the 
antonym of CW1.) 
(b) IF x=HYP 
LSR y = HOL+HYP =HYP (CW2 is the 
hypernym of CW1.) 
(c) IF x= NSYN 
LSR y = HOL+NSYN =HOL (CW2 is the 
holonym of CW1.) 
(d) IF x = HOL 
LSR y = HOL+HOL =HOL (CW2 is the 
holonym of CW1.) 
(e) IF x = all other LSR 
LSR y = HPO +all other LSRs = ? 
(Undecided)
Meronym(MER) 
(a) IF x=ANT 
LSR y = MER+ANT =ANT (CW2 is the 
antonym of CW1.) 
(b) IF x=HPO 
LSR y = MER+HPO =HPO (CW2 is the 
hyponym of CW1.) 
(c) IF x= NSYN 
LSR y = MER+NSYN =MER (CW2 is the 
meronym of CW1.) 
(d) IF x = MER 
LSR y = MER+MER =MER (CW2 is the 
meronym of CW1.) 
(e) IF x = all other LSR 
LSR y = HPO +all other LSRs = ? 
(Undecided)
4 Implementation and Evaluation 
WN 1.6 contains 99,642 English 
synsets and expands to 157,507 English 
lemma tokens. On the other hand, the total 
number of Chinese lemma types found in 
our ECTED is 108,533. Hence, each 
Chinese lemma type translates roughly 1.1 
English synsets in average.  
In comparing the two approaches to 
parallel wordnet building, we treat at 
baseline the cases where the translation LSR 
is synonymy. In others words, these are the 
cases where both approach I and approach II 
will make highly accurate predictions (e.g. 
Huang, et al 2003). However, if the T-LSR 
is other than synonymy, we expect the 
prediction based on source language LSR 
will be much lower.  
In our study, there are in total 372,927 
lexical semantic relations that can 
potentially be bootstrapped when the T-LSR 
is one of the five semantic relations in study. 
These are expanded from the following 
types of translations equivalence relations: 
11,396 translation near-synonyms, 2,782 
translation hypernyms, 2,106 translation 
hyponyms, 252 translation meronyms and 
145 translations holonyms. For evaluation, 
due to constraints on resources, we 
exhaustively check the types with less than 
300 lemmas, while randomly checked close 
to 300 lemmas for the other types. 
 We first introduce the baseline model 
where synonym is assumed. This is where 
source language LSR?s will be mapped 
directly to target languages. We have shown 
that if the T-LSR is really synonymy, the 
precision will be 62.7%. However, when the 
T-LSR?s are different, the baseline precision 
is much lower. In Table 1, such na?ve 
prediction is manually classes into three 
types: Correct, Incorrect, and Others. 
?Correct? means that the prediction is 
verified. ?Incorrect? means the assigned 
LSR is wrong. Two scenarios are possible. 
One is that there is a possible prediction and 
another one is the correct LSR is different 
from the predicted one. ?Others? refers to 
exceptional cases where these is no lexical 
translation, or the source language LSR is 
wrongly assigned and so on. Table 1 shows 
that the baseline for non-synonymous 
T-LSR is only 47% in average, and range 
from 30% to 65% for each semantic relation.
54
Correct Incorrect Others Total 
NSYN 400 51% 379 49% 0 0% 779 100% 
HYP 178 65% 72 27% 22 8% 272 100% 
HPO 402 40% 285 28% 330 32% 1017 100% 
HOL 48 30%  108 69% 2 1% 158 100% 
MER 52 56% 32 34% 9 10% 93 100% 
Total 1079 47% 877 37% 363 16% 2319 100% 
Table 1 Baseline Results (assuming synonym)
Table 2 shows the comparison between 
the T-LSR model and the baseline. It shows 
that there is improvement of 17.8% in 
average and that there is gain in precision 
for each LSR type. The improvement varies 
from just below 2% to 39%.  
Baseline T-LSR Difference Improvement 
NSYN 400 51% 556 71% 156 20 % 156/400 39 % 
HYP 178 65% 184 66%  6 2.2% 6/178  3.4% 
HPO 402 40% 409 40%  7 0.7% 7/402  1.7% 
HOL 48 30% 64 41% 16 10.1% 16/48 33.3% 
MER 52 56% 58 62% 6 6.5% 6/52 11.5% 
Total 1079 47% 1271 55% 191 8.2% 191/1080 17.7% 
Table 2 Precision of using the LSR inferences
5 Conclusion
It is interesting to note that the classes 
with least improvements are hypernymy and 
hyponymy. Since these are the classical 
IS-A relations, we hypothesize that their 
predictions are similar to the baseline 
relation of synonym. If we take these two 
relations out, the T-LSR model with 
inference rules has a precision difference of 
17.3% (178/1030), as well as an 
improvement of 35.6% (178/500). These are 
substantial improvements over the baseline 
model. The result will be reinforced when 
the evaluation is completed. We will also 
analyze the prediction based on each T-LSR 
to give a more explanatory account as well a 
measure confidence or prediction. The result 
offers strong support for T-LSR as a model 
for bootstrapping parallel wordnets with a 
low-density target language. 
References 
Fellbaum, C. (ed.) 1998. Wordent: An Electronic 
Lexical Database. Cambridge, MA: MIT Press. 
Huang, Chu-Ren, D.B. Tsai, J.Lin, S. Tseng, K.J. 
Chen and Y. Chuang. 2001 Definition and Test 
for Lexical Semantic Relation in Chinese. [in 
Chinese] Paper presented at the Second Chinese 
Lexical Semantics Workshop. May 2001,  
Beijing, China. 
Huang, Chu-Ren, I-Ju E. Tseng, Dylan B.S. Tsai. 
2002. Translating Lexical Semantic Relations: 
The first step towards Multilingual Wordnets.
Proceedings of the COLONG2002 Workshop 
?SemaNet:Building and Using Semantic 
Networks?, ed. By Grace Ngai, Pascale Fung, 
and Kenneth W. Church, 2-8. 
Huang, Chu-Ren, Elanna I. J. Tseng, Dylan B.S. 
Tsai, Brian Murphy. 2003 Cross-lingual 
Portability of Semantic Relations: Bootstrapping 
Chinese WordNet with English WordNet 
Relations. pp.509-531. 
Pianta, Emanuel, L. Benitivogli, C. Girardi. 
2002 MultiWordnet: Developing an aligned 
nultilingual database. Proceedings of the 1st
International WordNet Conference, Maysore, 
Inda, pp.293-302. 
Tsai, D.B.S., Chu-Ren Huang, J.Lin, K.J. Chen 
and Y. Chuang. 2002. Definition and Test for 
Lexical Semantic Relation in Chinese. [???
??????????!
?] Journal of Chinese Information Processing 
[??????]. 16.4.21-31. 
Vossen P. (ed.). 1998. EuroWordNet: A 
multilingual database with lexical semantic 
networks. Norwell, MA: Kluwer Academic 
Publisher 
55
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
Constructing Taxonomy of Numerative Classifiers for Asian Languages
Kiyoaki Shirai
JAIST
kshirai@jaist.ac.jp
Takenobu Tokunaga
Tokyo Inst. of Tech.
take@cl.cs.titech.ac.jp
Chu-Ren Huang
Academia Sinica
churenhuang@gmail.com
Shu-Kai Hsieh
National Taiwan Normal Univ.
shukai@gmail.com
Tzu-Yi Kuo
Academia Sinica
ivykuo@gate.sinica.edu.tw
Virach Sornlertlamvanich
TCL, NICT
virach@tcllab.org
Thatsanee Charoenporn
TCL, NICT
thatsanee@tcllab.org
Abstract
Numerative classifiers are ubiquitous in
many Asian languages. This paper pro-
poses a method to construct a taxonomy
of numerative classifiers based on a noun-
classifier agreement database. The taxon-
omy defines superordinate-subordinate rela-
tion among numerative classifiers and rep-
resents the relations in tree structures. The
experiments to construct taxonomies were
conducted for evaluation by using data from
three different languages: Chinese, Japanese
and Thai. We found that our method was
promising for Chinese and Japanese, but in-
appropriate for Thai. It confirms that there
really is no hierarchy among Thai classifiers.
1 Introduction
Many Asian languages do not mark grammatical
numbers (singular/plural) in noun form, but use nu-
merative classifiers together with numerals instead
when describing the number of nouns. Numerative
classifiers (hereafter ?classifiers?) are used with a
limited group of nouns, in particular material nouns.
In English, for example: ?three pieces of paper?. In
Asian languages these classifiers are ubiquitous and
used with common nouns. Therefore the number of
classifiers is much larger than in Western languages.
An agreement between nouns and classifiers is also
necessary, i.e., a certain noun specifies possible clas-
sifiers. The agreement is determined based on var-
ious aspects of a noun, such as its meaning, shape,
pragmatic aspect and so on.
This paper proposes a method to automati-
cally construct a taxonomy of numerative classi-
fiers for Asian languages. The taxonomy defines
superordinate-subordinate relations between classi-
fiers. For instance, the Japanese classifier ?? (to?)?
is used for counting big animals such as elephants
and tigers, while ?? (hiki)? is used for all animals.
Since ??? can be considered more general than ?
??, ??? is the superordinate classifier of ???, rep-
resented as ???  ??? in this paper. The taxon-
omy represents such superordinate-subordinate rela-
tions between classifiers in the form of a tree struc-
ture. A taxonomy of classifiers would be fundamen-
tal knowledge for natural language processing. In
addition, it will be useful for language learners, be-
cause learning usage of classifiers is rather difficult,
especially for Western language speakers.
We evaluate the proposed method by using the
data of three Asian languages: Chinese, Japanese
and Thai.
2 Noun-classifier agreement database
First, let us introduce usages of classifiers in Asian
languages. In the following examples, ?CL? stands
for classifier.
? Chinese: yi-ju
(CL)
dian-hua
(telephone)
? ? ? a telephone
? Japanese: inu
(dog)
2 hiki
(CL)
? ? ? 2 dogs
? Thai: nakrian
(student)
3 khon
(CL)
? ? ? 3 students
397
As mentioned earlier, the agreement between nouns
and classifiers is observed. For instance, the
Japanese classifier ?hiki? in the above example
agrees with only animals. The agreement is also
found in Chinese and Thai.
The proposed method to construct a classifier tax-
onomy is based on agreement between nouns and
classifiers. First we prepare a collection of pairs
(n, c) of a noun n and a classifier c which agrees
with n for a language. The statistics of our Chinese,
Japanese, and Thai database are summarized in Ta-
ble 1.
Table 1: Noun-classifier agreement database
Chinese Japanese Thai
No. of (n,c) pairs 28,202 9,582 9,618
No. of nouns (type) 10,250 4,624 8,224
No. of CLs (type) 205 331 608
The Japanese database was built by extracting
noun-classifier pairs from a dictionary (Iida, 2004)
which enumerates nouns and their corresponding
classifiers. The Chinese database was derived from
a dictionary (Huang et al, 1997). The Thai database
consists of a mixture of two kinds of noun-classifier
pairs: 8,024 nouns and their corresponding classi-
fiers from a dictionary of a machine translation sys-
tem (CICC, 1995) and 200 from a corpus. The pairs
from the corpus were manually checked for their va-
lidity.
3 Proposed Method
3.1 Extracting superordinate-subordinate
relations of classifiers
We extracted superordinate-subordinate classifier
pairs based on inclusive relations of sets of nouns
agreeing with those classifiers. Suppose that Nk is
a set of nouns that agrees with a classifier ck. If Ni
subsumes Nj (Ni ? Nj), we can estimate that ci
subsumes cj (ci  cj). For instance, in our Japanese
database, the classifier ?? (ten)? agrees with shops
such as ?drug store?, ?kiosk? and ?restaurant?, and
these nouns also agree with ?? (ken)?, since ??? is
a classifier which agrees with any kind of building.
Thus, we can estimate the relation ???  ???.
Given a certain classifier cj , ci satisfying the fol-
lowing two conditions (1) and (2) is considered as a
N
j
N
i
Figure 1: Relation of sets of nouns agreeing with
classifiers
superordinate classifier of cj .
|Ni| > |Nj | (1)
IR(ci, cj) ? Tir
where IR(ci, cj)
def
=
|N
i
?N
j
|
|N
j
|
(2)
Condition (1) requires that a superordinate classifier
agrees with more nouns than a subordinate classifier.
IR(ci, cj) is an inclusion ratio representing to what
extent nouns in Nj are also included in Ni (the ratio
of the light gray area to the area of the small circle
in Figure 1).
Condition (2) means that if IR(ci, cj) is greater
than a certain threshold T
ir
, we estimate a
superordinate-subordinate relation between ci and
cj . The basic idea is that superordinate-subordinate
relations are extracted when Nj is a proper subset
of Ni, i.e. IR(ci, cj) = 1, but this is too strict. In
order to extract more relations, we loosen this condi-
tion such that relations are extracted when IR(ci, cj)
is large enough. If we set Tir lower, more relations
can be acquired, but they may be less reliable.
Table 2: Extraction of superordinate-subordinate re-
lations
Chinese Japanese Thai
T
ir
0.7 0.6 0.6
No. of extracted relations 251 322 239
No. of CLs not in 36 76 395
the extracted relations (18%) (23%) (61%)
Table 2 shows the results of our experiments to
extract superordinate-subordinate relations of classi-
fiers. The threshold T
ir
was determined in an ad hoc
manner for each language. The numbers of extracted
superordinate-subordinate relations are shown in the
second row in the table. Manual inspection of the
sampled relations revealed that many reasonable re-
lations were extracted. The objective evaluation of
these extracted relations will be discussed in 4.2.
398
The third row in Table 2 indicates the numbers of
classifiers which were not included in the extracted
superordinate-subordinate relations with its ratio to
the total number of classifiers in the database in
parentheses. We found that no relation is extracted
for a large number of Thai classifiers.
3.2 Constructing structure
The structure of a taxonomy is constructed based
on a set of superordinate-subordinate relations be-
tween classifiers. Currently we adopt a very naive
approach to construct structures, i.e., starting from
the most superordinate classifiers as roots, we ex-
tend trees downward to less general classifiers by
using the extracted superordinate-subordinate rela-
tions. Note that since there is more than one classi-
fier that does not have any superordinate classifiers,
we will have a set of trees rather than a single tree.
When constructing structures, redundant relations
are ignored in order to make the structures as concise
as possible. A relation is considered redundant if the
relation can be inferred by using other relations and
transitivity of the relations. The formal definition of
redundant relations is given below:
ca  cb is redundant iff ?cm : ca  cm, cm  cb
Statistics of constructed structures for each lan-
guage are shown in Table 3. More than 50 iso-
lated structures (trees) were obtained for Chinese
and Japanese, while more than 100 for Thai. We ob-
tained several large structures, the largest containing
45, 85 and 23 classifiers for Chinese, Japanese and
Thai, respectively. As indicated in the fifth row in
Table 3, however, many structures consisting of only
2 classifiers were also constructed.
Table 3: Construction of structures
Chinese Japanese Thai
No. of structures 52 54 102
No. of CLs in a structure
Average 4.9 6.3 3.3
Maximum 45 85 23
Max. depth of structures 4 3 3
No. of structures with 2 CLs 18 24 54
4 Discussion
In this section, we will discuss the results of our
experiments. First 4.1 discusses appropriateness of
our method for the three languages. Then we eval-
uate our method in more detail. The evaluation of
extracted superordinate-subordinate relations is de-
scribed in 4.2, and the evaluation of structures in 4.3.
4.1 Comparison of different languages
According to the results of our experiments, the
proposed method seems promising for Chinese and
Japanese, but not for Thai. From the Thai data,
no relation was obtained for about 60% of classi-
fiers (Table 2), and many small fragmented struc-
tures were created (Table 3).
This is because of the characteristic that nouns
and classifiers are strongly coupled in Thai, i.e.,
many classifiers agree with only one noun. In our
Thai database, 252 (41.5%) classifiers agree with
only one noun. This means that the overlap between
two noun sets Ni and Nj can be quite small, making
the inclusion ratio IR(ci, cj) very small. Out basic
idea is that we can extract superordinate-subordinate
relations between two classifiers when the overlap of
their corresponding noun sets is large. However, this
assumption does not hold in Thai classifiers. The
above facts suggest that there seems to be no hierar-
chical taxonomy of classifiers in Thai.
4.2 Evaluation of extracted relations
4.2.1 Analysis of Nouns in Nj \ Ni
As explained in 3.1, our method extracts a relation
ci  cj even when Ni does not completely subsume
Nj . We analysed nouns in the relative complement
of Ni in Nj (Nj \Ni), i.e., the dark gray area in Fig-
ure 1. The relation ci  cj implies that all nouns
which are countable with a subordinate classifier cj
are also countable with its superordinate classifier ci,
but there is no guarantee of this for nouns in Nj \Ni,
since we loosened the condition as in (2) by intro-
ducing a threshold.
To see to what extent nouns in Nj \ Ni agree
with ci as well, we manually verified the agreement
of nouns in Nj \ Ni and ci for all extracted rela-
tions ci  cj . The verification was done by native
speakers of each language. Results of the valida-
tion are summarized in Table 4. For Japanese and
Chinese, multiple judges verified the results. When
judgments conflicted, we decided the final decision
by a discussion of two judges for Japanese, and by
majority voting for Chinese. The 4th and 5th rows
399
in Table 4 show the agreement of judgments. The
?Agreement ratio? is the ratio of cases that judg-
ments agree. Since three judges verified nouns for
Chinese, we show the average of the agreement ra-
tios for two judges out of the three. The agreement
ratio and Cohen?s ? is relatively high for Japanese,
but not for Chinese. We found many uncertain cases
for Chinese nouns. For example, ?? (wei)? is a clas-
sifier used when counting people with honorific per-
spective. However, judgement if ??? can modify
nouns such as ?political prisoner? or ?local villain?
is rather uncertain.
Table 4: Analysis of nouns in Nj \ Ni
Chinese Japanese Thai
No. of nouns in N
j
\N
i
1,650 579 43
No. of nouns countable 1,195 241 24
with c
i
as well 72% 42% 56%
No. of judges 3 2 1
Agreement ratio 0.677 0.936 ?
Cohen?s ? 0.484 0.868 ?
Table 4 reveals that a considerable number of
nouns in Nj \ Ni are actually countable with ci,
meaning that our databases do not include noun-
classifier agreement exhaustively.
4.2.2 Reliability of relations ??
Based on the analysis in 4.2.1, we evaluate ex-
tracted superordinate-subordinate relations. We de-
fine the reliability R of the relation ci  cj as
R(ci  cj) =
|Ni ? Nj |+ |NCj,i|
|Nj |
, (3)
where, NCj,i is a subset of Nj \ Ni consisting of
nouns which are manually judged to agree with ci.
We can consider that the more strictly this statement
holds, the more reliable the extracted relations will
be.
Figure 2 shows the relations between the thresh-
old T
ir
and both the number of extracted relations
and their reliability. The horizontal axis indicates
the threshold T
ir
in (2). The bar charts indicate the
number of extracted relations, while the line graphs
indicate the averages of reliability of all extracted re-
lations. Of course, if we set T
ir
lower, we can extract
more relations at the cost of their reliability. How-
ever, even when T
ir
is set to the lowest value, the
averages of reliability are relatively high, i.e. 0.98
(Chinese), 0.91 (Japanese) and 0.99 (Thai). Thus
we can conclude that the extracted superordinate-
subordinate relations are reliable enough.
4.3 Evaluation of structures
As in ordinary ontologies, we will assume that prop-
erties of superordinate classifiers can be inherited to
their subordinate classifiers. In other words, a clas-
sifier taxonomy suggests transitivity of agreement
with nouns over superordinate-subordinate relations
as
c
1
 c
2
? c
2
 c
3
? c
1
 c
3
.
In order to evaluate the structures of our taxonomy,
we verify the validity of transitivity.
First, we extracted all pairs of classifiers having
an ancestor-descendant relation from our classifier
taxonomy. Hereafter we denote ancestor-descendant
pairs of classifiers as (ca, cd), where ca is an ances-
tor and cd an descendant. The path from ca to cd on
the taxonomy can be represented as
c
0
(= ca)  c1  ...  cn(= cd). (4)
We denote a superordinate-subordinate relation de-
rived by transitivity as
?
, such as c
0
?
 cn. Among
all ancestor-descendant relations, we extracted ones
with a path length of more than one, or n > 1
in (4). Then we compare R(ca
?
 cd), the re-
liability of a relation derived by transitivity, with
R(ci  ci+1) (0 ? i < n), the reliability of di-
rect relations in the path from ca to cd. If these are
comparable, we can conclude that transitivity in the
taxonomy is valid.
Table 5 shows the results of the analysis of transi-
tivity. As indicated in the column ?all? in Table 5, 78
and 86 ancestor-descendant pairs (ca, cd) were ex-
tracted from the Chinese and Japanese classifier tax-
onomy, respectively. In contrast, only 6 pairs were
extracted from the Thai taxonomy, since each struc-
ture of the Thai taxonomy is rather small as we al-
ready discussed with Table 3. Thus we have omit-
ted further analysis of Thai. The extracted ancestor-
descendant pairs of classifiers are then classified into
three cases, (A), (B) and (C). Their numbers are
shown in the last three rows in Table 5, where mini
and maxi denote the minimum and maximum of re-
liability among all direct relations R(ci  ci+1) in
the path from ca to cd.
400
Chinese Japanese Thai
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
Figure 2: Reliability of extracted superordinate-subordinate relations
Table 5: Verification of transitivity
Chinese Japanese
all direct indirect all direct indirect
No. of (c
a
, c
d
) 78 58 20 86 55 31
Average of R(c
a
?
c
d
) 0.88 0.98 0.61 0.77 0.93 0.48
(A) min
i
> R(c
a
?
c
d
) 16 (21%) 4 (7%) 12 (60%) 24 (28%) 3 (5%) 21 (68%)
(B) min
i
? R(c
a
?
c
d
) < max
i
39 (50%) 34 (59%) 5 (25%) 27 (31%) 24 (44%) 3 (9%)
(C) max
i
? R(c
a
?
c
d
) 23 (29%) 20 (34%) 3 (15%) 35 (41%) 28 (51%) 7 (23%)
In case (A), reliability of a relation derived by
transitivity, R(ca
?
 cd), is less than that of any di-
rect relations, R(ci  ci+1). In case (B), reliability
of a transitive relation is comparable with that of di-
rect relations, i.e. R(ca
?
 cd) is greater or equal to
mini and less than maxi. In case (C), the transitive
relation is more reliable than direct relations.
The average of the reliability of ca
?
 cd is rela-
tively high, 0.88 for Chinese and 0.77 for Japanese.
We also found that more than 70% of derived rela-
tions (case (B) and case (C)) are comparable to or
greater than direct relations. The above facts indi-
cate transitivity on our structural taxonomy is valid
to some degree.
From a different point of view, we divided pairs
of (ca, cd) into two other cases, ?direct? and ?indi-
rect? as shown in the columns of Table 5. The ?di-
rect? case includes the relations which are also ex-
tracted by our method. Note that such relations are
discarded as redundant ones. On the other hand, the
?indirect? case includes the relations which can not
be extracted from the database but only inferred by
using transitivity on the taxonomy. That is, they are
truly new relations. In order to calculate reliability
of ?indirect? cases, we performed additional manual
validation of nouns in Nd\Na.
However, the average of R(ca
?
 cd) in ?in-
direct? cases is not so high for both Chinese and
Japanese, as a large amount of pairs are classi-
fied into case (A). Thus it is not effective to infer
new superordinate-subordinate relations by transi-
tivity. Since we currently only adopted a very naive
method to construct a classifier taxonomy, more so-
phisticated methods should be explored in order to
prevent inferring irrelevant relations.
5 Related Work
Bond (2000) proposed a method to choose an appro-
priate classifier for a noun by referring its seman-
tic class. This method is implemented in a sentence
generation module of a machine translation system.
Similar attempts to generate both Japanese and Ko-
rean classifiers were also reported (Paik and Bond,
2001). Bender and Siegel (2004) implemented a
HPSG that handles several intricate structures in-
cluding Japanese classifiers. Matsumoto (1993)
reported his close analysis of Japanese classi-
fiers based on prototype semantics. Sornlertlam-
vanich (1994) presented an algorithm for selecting
an adequate classifier for a noun by using a cor-
pus. Their research can be regarded as a method to
construct a noun-classifier agreement database au-
401
tomatically from corpora. We used databases de-
rived from dictionaries except for a small number
of noun-classifier pairs in Thai, because we believe
dictionaries provide more reliable and stable infor-
mation than corpora, and in addition they were avail-
able and on hand. Note that we are not concerned
with frequencies of noun-classifier coocurrence in
this study. Huang (1998) proposed a method to
construct a noun taxonomy based on noun-classifier
agreement that is very similar to ours, but aims at
developing a taxonomy for nouns rather than one for
classifiers. There has not been very much work on
building resources concerning noun-classifier agree-
ment. To our knowledge, this is the first attempt to
construct a classifier taxonomy.
6 Conclusion
This paper proposed a method to construct a tax-
onomy of numerative classifiers based on a noun-
classifier agreement database. First, superordinate-
subordinate relations of two classifiers are extracted
by measuring the overlap of two sets of nouns agree-
ing with each classifier. Then these relations are
used as building blocks to build a taxonomy of
tree structures. We conducted experiments to build
classifier taxonomies for three languages: Chinese,
Japanese and Thai. The effectiveness of our method
was evaluated by measuring reliability of extracted
relations, and verifying validity of transitivity in the
taxonomy. We found that extracted relations are re-
liable, and the transitivity in the taxonomy relatively
valid. Relations inferred by transitivity, however, are
less reliable than those directly derived from noun-
classifier agreement.
Future work includes investigating a way to en-
large classifier taxonomies. Currently, not all clas-
sifiers are included in our taxonomy, and it con-
sists of a set of fragmented structures. A more so-
phisticated method to build a large taxonomy in-
cluding more classifiers should be examined. Our
method should also be refined in order to make
superordinate-subordinate relations inferred by the
transitivity more reliable. We are now investigat-
ing a stepwise method to construct taxonomies that
prefers more reliable relations, i.e. an initial tax-
onomy is built with a small number of highly reli-
able relations, and is then expanded with less reli-
able ones.
Acknowledgment
This research was carried out through financial sup-
port provided under the NEDO International Joint
Research Grant Program (NEDO Grant).
References
Emily M. Bender and Melanie Siegel. 2004. Imple-
menting the syntax of Japanese numeral classifiers. In
Proceedings of the the First International Joint Con-
ference on Natural Language Processing, pages 398?
405.
Francis Bond and Kyonghee Paik. 2000. Reusing an on-
tology to generate numeral classifiers. In Proceedings
of the COLING, pages 90?96.
CICC. 1995. CICC Thai basic dictionary. (developed by
Center of the International Cooperation for Computer-
ization).
Chu-Ren Huang, Keh-Jian Chen, and Chin-Hsiung Lai,
editors. 1997. Mandarin Daily News Dictionary of
Measure Words. Mandarin Daily News Publisher.
Chu-Ren Huang, Keh-jiann Chen, and Zhao-ming Gao.
1998. Noun class extraction from a corpus-based col-
location dictionary: An integration of computational
and qualitative approaches. In Quantitative and Com-
putational Studies of Chinese Linguistics, pages 339?
352.
Asako Iida. 2004. Kazoekata no Ziten (Dictionary for
counting things). Sho?gakukan. (in Japanese).
Yo Matsumoto. 1993. The Japanese numeral classifiers:
A study of semantic categories and lexical organiza-
tion. Linguistics, 31:667?713.
Kyonghee Paik and Francis Bond. 2001. Multilin-
gual generation of numeral classifiers using a common
ontology. In Proceedings of the 19th International
Conference on Computer Processing of Oriental Lan-
guages (ICCPOL), pages 141?147.
Virach Sornlertlamvanich, Wantanee Pantachat, and
Surapant Meknavin. 1994. Classifier assignment by
corpus-based approach. In Proceedings of the COL-
ING, pages 556?561.
402
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 385?390,
Sydney, July 2006. c?2006 Association for Computational Linguistics
When Conset meets Synset: A Preliminary Survey of an Ontological
Lexical Resource based on Chinese Characters
Shu-Kai Hsieh
Institute of Linguistics
Academia Sinica
Taipei, Taiwan
shukai@gate.sinica.edu.tw
Chu-Ren Huang
Institute of Linguistics
Academia Sinica
Taipei, Taiwan
churen@gate.sinica.edu.tw
Abstract
This paper describes an on-going project
concerning with an ontological lexical re-
source based on the abundant conceptual
information grounded on Chinese charac-
ters. The ultimate goal of this project is set
to construct a cognitively sound and com-
putationally effective character-grounded
machine-understandable resource.
Philosophically, Chinese ideogram has its
ontological status, but its applicability to
the NLP task has not been expressed ex-
plicitly in terms of language resource. We
thus propose the first attempt to locate Chi-
nese characters within the context of on-
tology. Having the primary success in ap-
plying it to some NLP tasks, we believe
that the construction of this knowledge re-
source will shed new light on theoretical
setting as well as the construction of Chi-
nese lexical semantic resources.
1 Introduction
In the history of western linguistics, writing has
long been viewed as a surrogate or substitute for
speech, the latter being the primary vehicle for hu-
man communication. Such ?surrogational model?
which neglects the systematicity of writing in
its own right has also occupied the predominant
views in current computational linguistic studies.
This paper is set to provide a quite different per-
spective along with the Eastern philological tra-
dition of the study of scripts, especially the ideo-
graphic one i.e., Chinese characters (Hanzi). We
believe that the conceptual knowledge information
which has been grounded on Chinese characters
can be used as a cognitively sound and compu-
tationally effective ontological lexical resource in
performing some NLP tasks, and it will have con-
tribution to the development of Semantic Web as
well.
2 Background Issues of Chinese
Ideographic Writing
2.1 Ideographic Script and Conceptual
Knowledge
From the view of writing system and cognition,
human conceptual information has been regarded
as being wired in ideographic scripts. However, in
reviewing the contemporary linguistic literatures
concerning with the discussions of the essence of
Chinese writing system, we found that the main
theoretical dispute lies in the fact that, both struc-
tural descriptions and psycholinguistic modeling
seem to presume that the notions of ideography
and phonography are mutually exclusive.
To break the theoretical impasse?, we take a
pragmatic position in claiming the tripartite prop-
erties of Chinese characters: They are logographic
(morpho-syllabic) in essence, function phonologi-
cally at the same time, and can be interpreted ideo-
graphically and implemented as concept instances
by computers.
2.2 Chinese Wordhood
Roughly put, a Chinese character is regarded as
an ideographic symbol representing syllable and
meaning of a ?morpheme? in spoken Chinese.
But unlike most affixing languages, Chinese has
a large class ofmorphemes - which Packard (2000)
calls ?bound roots? - that possess certain affixal
properties (namely, they are bound and productive
in forming words), but encode lexical rather than
385
grammatical information. These may occur as ei-
ther the left- or right-hand component of a word.
For example, the morpheme ? (/shu/; ?transport?)
can be used as either the first morpheme (e.g., ??
(/yu`n-ru`/; transport-into ?import?), or the second
morpheme (e.g., ?? /yu`n-shu/; transit-transport
?conveyance?) of a dissyllabic word, but cannot
occur in isolation.
The fuzzy boundary between free and bound
morphemes is directly related to the notori-
ous controversial notion of Chinese Wordhood.
There are multiple studies showing that to a
large extent, (trained or untrained) native speak-
ers of Chinese disagree on what a (free) mor-
pheme/word/compound is.
Such difficulty could be traced back to its histor-
ical facts. In modern Mandarin Chinese, there is a
strong tendency toward dissyllabic words, while
the predominant monosyllabic words in ancient
Chinese remain more or less a closed set. But
the conceptual knowledge encoded in monosyl-
labic morphemes still have their influence even on
contemporary texts, and thus resulting the difficul-
ties of word-marking decision.
3 Theoretical Setting
Yu et al(1999) reported that a Morpheme Knowl-
edge Base of Modern Chinese according to all Chi-
nese characters in GB2312-80 code has been con-
structed by the institute of Computational Linguis-
tics of Peking University. This Morpheme Knowl-
edge Base has been later integrated into the project
called ?Grammatical Knowledge Base of Contem-
porary Chinese?.
It is noted that the ?morphemes? adopted in this
database are monosyllabic ?bound morphemes?.
As for ?free morphemes?, that is, characters which
can be independently used as words, are not in-
cluded in the Knowledge Base. For example,
the monosyllabic character ? (/shu/,?comb?) has
(at least) two senses. For the verbal sense (?to
comb?), it can be used as a word; for the nomi-
nal sense (?a comb?), it can only be used in com-
bining with other morphemes. Therefore, only the
nominal sense of ? is included in the Knowledge
Base. However, such morpheme-based approach
can hardly escape from facing with the difficult
decision of free/bound distinction in contemporary
Chinese.
3.1 Hanzi/Word Space Model
Based on the consideration mentioned above, in
this paper, we will propose a historical, conven-
tionalized, pre-theoretical perspective in viewing
the lexical and knowledge information within Chi-
nese characters. In Figure 1, (a) illustrates a naive
Hanzi space, while (d) shows a linguistic theory-
laden result of Hanzi/Word space, where green ar-
eas denote to words, consisting of 1 to 4 char-
acters. The decision of words (green) and non-
words (white) in the space is based on certain per-
spectives (be it psycholinguistic or computational
linguistic). Instead, we take the traditional philo-
logical construct of Hanzi into consideration. By
analyzing the conceptual relations between char-
acters (b) which scatter among diverse lexical re-
sources, we construct an top-level ontology with
Hanzi as its instances (c). Rather than (a) ? (d),
which is a predominant approach in contempo-
rary linguistic theoretical construction of Chinese
Wordhood, we believe that the proposed approach
(a) ? (b) ? (c) ? (d) could not only enclose
the implicit conceptual information evolutionarily
encoded in Chinese characters, but also provide a
more clear knowledge scenario for the interaction
of characters/words in modern linguistic theoreti-
cal setting.
3.2 Conset and Character Ontology
The new model that we propose here is called
HanziNet. It relies on a novel notion called con-
set and a coarsely grained upper-level ontology
of characters.
In comparison with synset, which has become
a core notion in the construction of Wordnet-like
lexical semantic resources, we will argue that there
is a crucial difference between Word-based lexi-
cal resource and character-based lexical resource,
in that they rest with finely-differentiated informa-
tion contents represented by the nodes of network.
A synset, or synonym set in WordNet contains a
group of words,1 and each of which is synony-
mous with the other words in the same synset.
In WordNet?s design, each synset can be viewed
as a concept in a taxonomy, While in HanziNet,
we are seeking to align Hanzi which share a given
putatively primitive meaning extracted from tradi-
tional philological resources, so a new term con-
set (concept set) is proposed. A conset contains
1To put it exactly, it contains a group of lexical units,
which can be words or collocations.
386
(a) (b) (c) (d)
Figure 1: Illustrations of Hanzi/Word Spaces
a group of Chinese characters similar in concept,
and each of which shares with similar conceptual
information with the other characters in the same
conset.2
The relations between consets constitute a char-
acter ontology. Formally, it is a tree-structured
conceptual taxonomy in terms of which only two
kinds of relations are allowed: the INSTANCE-OF
(i.e., characters are instances of consets) and IS-
A relations (i.e., consets are hypernyms/hyponyms
to other consets).
Currently, frequently used monosyllabic char-
acters are assigned to at least one of 309 consets.
Following are some examples:
conset 126 (SUBJECTIVE ? EXCITABILITY ? ABILITY ? ORGANIC
FUNCTION)
?? ???????????????,
conset 130 (SUBJECTIVE? EXCITABILITY? ABILITY? SKILLS)
?????????????,
conset 133 (SUBJECTIVE? EXCITABILITY? ABILITY? INTELLECT)
?????????????,
In fact, the core assumption behind the
synset/conset distinction is non-trivial. In this
project, we assume a hypothesis of the locality
of Concept Gestalt and the context-sensibility of
Word Sense concerning with Chinese characters.
That is, characters carry two meaning dimensions:
on the one hand, they are lexicalized concepts;
2At the time of writing, about 3,600 characters have been
finished in their information construction.
on the other hands, they can be observed lin-
guistically as bound root morphemes and mono-
morphemic words according to their independent
usage in modern Chinese texts.
Figure 2 shows a schematic diagram of our pro-
posed model. In Aitchison?s (2003) terms, for the
character level, we take an ?atomic globule? net-
work viewpoint, where the characters - realized as
instances of core concept Gestalt - which share
similar conceptual information, cluster together.
The relationships between these concept Gestalt
form a rooted tree structure. Characters are thus
assigned to the leaves of the tree in terms of an
assemblage of bits. For the word level, we take
the ?cobweb? viewpoint, as words -built up from
a pool of characters- are connected to each other
through lexical semantic relations. In such case,
the network does not form a tree structure but a
more complex, long-range highly-correlated ran-
dom acyclic graphic structure.
4 Hanzi-grounded Ontological
CharacterNet
In light of the previous consideration, this sec-
tion attempts to further clarify the building blocks
of the HanziNet system, ? a Hanzi-grounded on-
tological Character Net ? with the goal to ar-
rive at a working model which will serve as a
framework for ontological knowledge processing.
Briefly, HanziNet is consisted of two main parts:
387
Figure 2: The Schematic Representation of
character-triggered tree-like conceptual hierarchy
and word-based semantic network
a character-stored machine-readable lexicon and a
top-level character ontology.
4.1 Hanzi-grounded Lexicon and Ontology
The current lexicon contains over 5000 characters,
and 30,000 derived words in total.3
The building of the lexical specification of the
entries in HanziNet includes various aspects of
Hanzi:
1. Conset(s): The conceptual code is the core
part of the MRD lexicon in HanziNet. Con-
cepts in HanziNet are indicated by means
of a label (conset name) with a code form.
In order to increase the efficiency, an ideal
strategy is to adopt the Huffmann-coding-like
method, by encoding the conceptual structure
of Hanzi as a pattern of bits set within a bit
string.4 The coding thus refers to the assign-
ment of code sequences to an character. The
sequence of edges from the root to any char-
acter yields the code for that character, and
the number of bits varies from one character
to another. Currently, for each conset (309 in
total) there are 12 characters assigned on the
average; for each character, it is assigned to
3Since this lexicon aims at establishing an knowl-
edge resource for modern Chinese NLP, characters
and words are mostly extracted from the Academia
Sinica Balanced Corpus of Modern Chinese
(http://www.sinica.edu.tw/SinicaCorpus/), those charac-
ters and words which have probably only appeared in
classical literary works, (considered ghost words in the
lexicography), will be discarded.
4This is inspired by Chu (1999)?s works.
2-3 consets on the average.5
2. Character Semantic Head (CSH) and Char-
acter Semantic Modifier (CSM) division.6
3. Shallow parts of speech (mainly Nominal(N)
and Verbal(V) tags)
4. Gloss of prototypical meaning
5. List of combined words with statistics calcu-
lated from corpus, and
6. Further aspects such as character types and
cognates: According to ancient study, char-
acters can be compartmentalized into six
groups based on the six classical principles of
character construction. Character type here
means which group the character belongs to.
And the term cognate here is defined as char-
acters that share the same CSH or CSM. Fig-
ure 3 shows a snapshot of this lexicon.
Figure 3: The character-stored lexicon: a snapshot
The second core component of the proposed re-
source is a set of hierarchically related Top Con-
cepts called Top-level Ontology (or Upper ontol-
ogy). This is similar to EuroWordnet 1.2, which is
5The disputing point here is that, if some of the mono-
syllabic morphemes are taken as words, they should be very
ambiguous in the daily linguistic context, at least more am-
biguous than the dissyllabic words. However, as we argued
previously, HanziNet takes a different perspective in locating
theoretical roles of Hanzi.
6This distinction is made based on the glyphographical
consideration, which has been a crucial topic in the studies of
traditional Chinese scriptology. Due to the limited space, this
will not be discussed here.
388
also enriched with the Top Ontology and the set of
Base Concepts (Vossen 1998).
As mentioned, a tentative set of 309 conset,
a kind of ontological categories in contrast with
synset has been proposed 7, and over 5000 charac-
ters have been used as instances in populating the
character ontology.
Methodologically, following the basic line of
OntoClear approach (Guarino and Welty (2002)),
we use simple monotonic inheritance in our ontol-
ogy design, which means that each node inherits
properties only from a single ancestor, and the in-
herited value cannot be overwritten at any point of
the ontology. The decision to keep the relations
to one single parent was made in order to guaran-
tee that the structure would be able to grow indef-
initely and still be manageable, i.e. that the tran-
sitive quality of the relations between the nodes
would not degenerate with size. Figure 4 shows a
snapshot of the character ontology.
ROOT
OBJ
SUBJ
CONCRETE
ABSTRACT
EXISTENCE
ARTIFACT
EXCITABLE
COGNITIVE
SEMIOTIC
RELATIONA
L
SENSATION
STATE
INNATE
SOCIAL
conset 1
conset 309
conset 2
conset 3
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
conset 308
conset 307
{????????????}
{????????????}
{???????????}
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
------
{???}
{????????????????}
{??????????????}
Figure 4: The character ontology: a snapshot
4.2 Characters in a Small World
In addition, an experiment concerning the char-
acter network that was based on the meaning as-
pects of characters, was performed from a statisti-
cal point of view. It was found that this character
network, like many other linguistic semantic net-
works (such as WordNet), exhibits a small-world
property (Watt 1998), characterized by sparse con-
nectivity, small average shortest paths between
characters, and strong local clustering. Moreover,
due to its dynamic property, it appears to exhibit
an asymptotic scale-free (Barabasi 1999) feature
7It would be interesting to compare consets with the basic
400 nodes in the upper region proposed by Hovy(2005).
Table 1: Statistical characteristics of the char-
acter network: N is the total number of
nodes(characters), k is the average number of links
per node, C is the clustering coefficient, and L is
the average shortest-path length, and Lmax is the
maximum length of the shortest path between a
pair of characters in the network.
N k C L
Actual configuration 6493 350 0.64 2.0
Random configuration 6493 350 0.06 1.5
with the connectivity of power laws distribution,
which is found in many other network systems as
well.
Our first result is that our proposed conceptual
network is highly clustered and at the same time
and has a very small length, i.e., it is a small
world model in the static aspect. Specifically,
L & Lrandom but C  Crandom. Results for the
network of characters, and a comparison with a
corresponding random network with the same pa-
rameters are shown in Table 1. N is the total num-
ber of nodes (characters), k is the average number
of links per node, C is the clustering coefficient,
and L is the average shortest path.
4.3 HanziNet in the Global Wordnet Grid
In order to promote a semantic and ontological
interoperability, we have aligned conset with the
164 Base Concepts, a shared set of concepts from
EWN in terms of Wordnet synsets and SUMO
definitions, which has been currently proposed in
the international collaborative platform of Global
Wordnet Grid.
5 Applications and Future Development
5.1 Sense Prediction and Disambiguation
Based on the initial version of the proposed re-
sources, Hsieh (2005b) has proposed a semantic
class prediction model which aims to gain the pos-
sible semantic classes of unknown two-characters
words. The results obtained shows that, with this
knowledge resource, the system can achieve fairly
high level of performance. Meaning relevant NLP
Tasks such asWord Sense Disambiguation are also
in preparation.
389
5.2 Interfacing Hantology, HanziNet and
Chinese Wordnet
Interfacing ontologies and lexical resources has
been a research topic in the coming age of se-
mantic web. In the case of Chinese, three existing
lexical resources (??Radicals::Hantology (Chou
and Huang (2005))- ? Characters::HanziNet -
? Words::Chinese Wordnet) constitutes an inte-
grated 3-level knowledge scenario which would
provide important insights into the problems of
understanding the complexities and its interaction
with Chinese natural language.
6 Conclusion
In conclusion, the goal of this research is set
to survey the unique characteristics of Chinese
Ideographs.
Though it has been well understood and agreed
upon in cognitive linguistics that concepts can be
represented in many ways, using various construc-
tions at different syntactical levels, conceptual rep-
resentation at the script level has been unfortu-
nately both undervalued and under-represented in
computational linguistics. Therefore, the Hanzi-
driven conceptual approach in this thesis might re-
quire that we consider the Chinese writing system
from a perspective that is not normally found in
canonical treatments of writing systems in con-
temporary linguistics.
Against the deep-seated tradition in contempo-
rary Chinese linguistics, which views the use of
Chinese characters in scientific theories as a mani-
festation of mathematical immaturity and interpre-
tational subjectivity, we propose the first lexical
knowledge resource based on Chinese characters
in the field of linguistic as well as in the NLP.
It is noted that HanziNet, as a general knowl-
edge resource, should not claim to be a sufficient
knowledge resource in and of itself, but instead
seek to provide a groundwork for the incremen-
tal integration of other knowledge resources for
language processing tasks. In order to augment
HanziNet, additional information will needed to
be incorporated and mapped into HanziNet. This
leads us to several avenues of future research.
Acknowledgements
The authors would like to thank the anonymous
referees for constructive comments. Thanks also
go to the institute of linguistics of Academia
Sinica for their kindly data support.
References
Aitchison, Jean. 2003. Words in the mind: an introduc-
tion to the mental lexicon. Blackwell publishing.
Barabasi, Albert-Laszlo and Reka Albert. 1999. Emer-
gence of scaling in random networks. Science,
286:509-512.
Chou, Ya-Min and Chu-Ren Huang. 2005. Hantology:
An ontology based on conventionalized conceptual-
ization. OntoLex Workshop, Korea.
Chu, Bong-Foo. 1999-. http://www.cbflabs.com
Guarino, Nicola and Chris Welty. 2002. Evaluating on-
tological decisions with OntoClean. In: Communi-
cations of the ACM. 45(2):61-65
Hovy, E.H. 2005. Methodologies for the Reliable Con-
struction of Ontological Knowledge. In : F. Dau,
M.-L. Mugnier, and G. Stumme (eds), Conceptual
Structures: Common Semantics for Sharing Knowl-
edge. Proceedings of the 13th Annual International
Conference on Conceptual Structures (ICCS 2005).
Kassel, Germany.
Hsieh, Shu-Kai. 2005(a). HanziNet: An enriched
conceptual network of Chinese characters. The 5rd
workshop on Chinese lexical semantics, China: Xi-
amen.
Hsieh, Shu-Kai. 2005(b). Word Meaning Inducing via
Character Ontology. IJINLP, SIGHAN Workshop,
Jijeu Island, South Korea.
Packard, J. L. 2000. The morphology of Chinese. Cam-
bridge, UK: Cambridge University Press.
Steyvers, M. and Tenenbaum, J.B. 2002 The Large-
Scale Structure of Semantic Networks: Statistical
Analyses and a Model of Semantic Growth. Cog-
nitive Science.
Watts, D. J. and Strogatz, S. H. 1998. Collective dy-
namics of ?small-world? networks. Nature 393:440-
42.
Yu, Shiwen, Zhu Xuefeng and Li Feng. 1999. The de-
velopment and application of modern Chinese mor-
pheme knowledge base.[in Chinese]. In: ?????
?, No.2. pp38-45.
390
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 827?834,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Infrastructure for standardization of Asian language resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Chu-Ren Huang
Academia Sinica
Xia YingJu
Fujitsu R&D Center
Yu Hao
Fujitsu R&D Center
Laurent Prevot
Academia Sinica
Shirai Kiyoaki
JAIST
Abstract
As an area of great linguistic and cul-
tural diversity, Asian language resources
have received much less attention than
their western counterparts. Creating a
common standard for Asian language re-
sources that is compatible with an interna-
tional standard has at least three strong ad-
vantages: to increase the competitive edge
of Asian countries, to bring Asian coun-
tries to closer to their western counter-
parts, and to bring more cohesion among
Asian countries. To achieve this goal, we
have launched a two year project to create
a common standard for Asian language re-
sources. The project is comprised of four
research items, (1) building a description
framework of lexical entries, (2) building
sample lexicons, (3) building an upper-
layer ontology and (4) evaluating the pro-
posed framework through an application.
This paper outlines the project in terms of
its aim and approach.
1 Introduction
There is a long history of creating a standard
for western language resources. The human
language technology (HLT) society in Europe
has been particularly zealous for the standardiza-
tion, making a series of attempts such as EA-
GLES1, PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Calzolari et al, 2003) and LIRICS2.
These continuous efforts has been crystallized as
activities in ISO-TC37/SC4 which aims to make
an international standard for language resources.
1http://www.ilc.cnr.it/Eagles96/home.html
2lirics.loria.fr/documents.html
(1) Description 
framework of lexical 
entries
(2) Sample lexicons
(4) Evaluation 
through application
(3) Upper layer 
ontologyrefinement
description classification
refinement
evaluationevaluation
Figure 1: Relations among research items
On the other hand, since Asia has great lin-
guistic and cultural diversity, Asian language re-
sources have received much less attention than
their western counterparts. Creating a common
standard for Asian language resources that is com-
patible with an international standard has at least
three strong advantages: to increase the competi-
tive edge of Asian countries, to bring Asian coun-
tries to closer to their western counterparts, and to
bring more cohesion among Asian countries.
To achieve this goal, we have launched a two
year project to create a common standard for
Asian language resources. The project is com-
prised of the following four research items.
(1) building a description framework of lexical
entries
(2) building sample lexicons
(3) building an upper-layer ontology
(4) evaluating the proposed framework through
an application
Figure 1 illustrates the relations among these re-
search items.
Our main aim is the research item (1), building
a description framework of lexical entries which
827
fits with as many Asian languages as possible, and
contributing to the ISO-TC37/SC4 activities. As
a starting point, we employ an existing descrip-
tion framework, the MILE framework (Bertagna
et al, 2004a), to describe several lexical entries of
several Asian languages. Through building sam-
ple lexicons (research item (2)), we will find prob-
lems of the existing framework, and extend it so
as to fit with Asian languages. In this extension,
we need to be careful in keeping consistency with
the existing framework. We start with Chinese,
Japanese and Thai as target Asian languages and
plan to expand the coverage of languages. The re-
search items (2) and (3) also comprise the similar
feedback loop. Through building sample lexicons,
we refine an upper-layer ontology. An application
built in the research item (4) is dedicated to evalu-
ating the proposed framework. We plan to build an
information retrieval system using a lexicon built
by extending the sample lexicon.
In what follows, section 2 briefly reviews the
MILE framework which is a basis of our de-
scription framework. Since the MILE framework
is originally designed for European languages, it
does not always fit with Asian languages. We ex-
emplify some of the problems in section 3 and sug-
gest some directions to solve them. We expect
that further problems will come into clear view
through building sample lexicons. Section 4 de-
scribes a criteria to choose lexical entries in sam-
ple lexicons. Section 5 describes an approach
to build an upper-layer ontology which can be
sharable among languages. Section 6 describes
an application through which we evaluate the pro-
posed framework.
2 The MILE framework for
interoperability of lexicons
The ISLE (International Standards for Language
Engineering) Computational Lexicon Working
Group has consensually defined the MILE (Mul-
tilingual ISLE Lexical Entry) as a standardized
infrastructure to develop multilingual lexical re-
sources for HLT applications, with particular at-
tention toMachine Translation (MT) and Crosslin-
gual Information Retrieval (CLIR) application
systems.
The MILE is a general architecture devised
for the encoding of multilingual lexical informa-
tion, a meta-entry acting as a common representa-
tional layer for multilingual lexicons, by allowing
integration and interoperability between different
monolingual lexicons3.
This formal and standardized framework to en-
code MILE-conformant lexical entries is provided
to lexicon and application developers by the over-
all MILE Lexical Model (MLM). As concerns
the horizontal organization, the MLM consists of
two independent, but interlinked primary compo-
nents, the monolingual and the multilingual mod-
ules. The monolingual component, on the vertical
dimension, is organized over three different repre-
sentational layers which allow to describe differ-
ent dimensions of lexical entries, namely the mor-
phological, syntactic and semantic layers. More-
over, an intermediate module allows to define
mechanisms of linkage and mapping between the
syntactic and semantic layers. Within each layer, a
basic linguistic information unit is identified; basic
units are separated but still interlinked each other
across the different layers.
Within each of the MLM layers, different types
of lexical object are distinguished :
? the MILE Lexical Classes (MLC) represent
the main building blocks which formalize
the basic lexical notions. They can be seen
as a set of structural elements organized in
a layered fashion: they constitute an on-
tology of lexical objects as an abstraction
over different lexical models and architec-
tures. These elements are the backbone of
the structural model. In the MLM a defini-
tion of the classes is provided together with
their attributes and the way they relate to each
other. Classes represent notions like Inflec-
tionalParadigm, SyntacticFunction, Syntac-
ticPhrase, Predicate, Argument,
? the MILE Data Categories (MDC) which
constitute the attributes and values to adorn
the structural classes and allow concrete en-
tries to be instantiated. MDC can belong to
a shared repository or be user-defined. ?NP?
and ?VP? are data category instances of the
class SyntacticPhrase, whereas and ?subj?
and ?obj? are data category instances of the
class SyntacticFunction.
? lexical operations, which are special lexical
entities allowing the user to define multilin-
3MILE is based on the experience derived from exist-
ing computational lexicons (e.g. LE-PAROLE, SIMPLE, Eu-
roWordNet, etc.).
828
gual conditions and perform operations on
lexical entries.
Originally, in order to meet expectations placed
upon lexicons as critical resources for content pro-
cessing in the Semantic Web, the MILE syntactic
and semantic lexical objects have been formalized
in RDF(S), thus providing a web-based means to
implement the MILE architecture and allowing for
encoding individual lexical entries as instances of
the model (Ide et al, 2003; Bertagna et al, 2004b).
In the framework of our project, by situating our
work in the context of W3C standards and relying
on standardized technologies underlying this com-
munity, the original RDF schema for ISLE lexi-
cal entries has been made compliant to OWL. The
whole data model has been formalized in OWL by
using Prote?ge? 3.2 beta and has been extended to
cover the morphological component as well (see
Figure 2). Prote?ge? 3.2 beta has been also used as
a tool to instantiate the lexical entries of our sam-
ple monolingual lexicons, thus ensuring adherence
to the model, encoding coherence and inter- and
intra-lexicon consistency.
3 Existing problems with the MILE
framework for Asian languages
In this section, we will explain some problematic
phenomena of Asian languages and discuss pos-
sible extensions of the MILE framework to solve
them.
Inflection The MILE provides the powerful
framework to describe the information about in-
flection. InflectedForm class is devoted to de-
scribe inflected forms of a word, while Inflec-
tionalParadigm to define general inflection rules.
However, there is no inflection in several Asian
languages, such as Chinese and Thai. For these
languages, we do not use the Inflected Form and
Inflectional Paradigm.
Classifier Many Asian languages, such as
Japanese, Chinese, Thai and Korean, do not dis-
tinguish singularity and plurality of nouns, but use
classifiers to denote the number of objects. The
followings are examples of classifiers of Japanese.
? inu
(dog)
ni
(two)
hiki
(CL)
? ? ? two dogs
? hon
(book)
go
(five)
satsu
(CL)
? ? ? five books
?CL? stands for a classifier. They always follow
cardinal numbers in Japanese. Note that differ-
ent classifiers are used for different nouns. In the
above examples, classifier ?hiki? is used to count
noun ?inu (dog)?, while ?satsu? for ?hon (book)?.
The classifier is determined based on the semantic
type of the noun.
In the Thai language, classifiers are used in var-
ious situations (Sornlertlamvanich et al, 1994).
The classifier plays an important role in construc-
tion with noun to express ordinal, pronoun, for in-
stance. The classifier phrase is syntactically gener-
ated according to a specific pattern. Here are some
usages of classifiers and their syntactic patterns.
? Enumeration
(Noun/Verb)-(cardinal number)-(CL)
e.g. nakrian
(student)
3 khon
(CL)
? ? ? three students
? Ordinal
(Noun)-(CL)-/thi:/-(cardinal number)
e.g. kaew
(glass)
bai
(CL)
thi: 4
(4th)
? ? ? the 4th glass
? Determination
(Noun)-(CL)-(Determiner)
e.g. kruangkhidlek
(calculator)
kruang
(CL)
nii
(this)
? ? ? this calculator
Classifiers could be dealt as a class of the part-
of-speech. However, since classifiers depend on
the semantic type of nouns, we need to refer to
semantic features in the morphological layer, and
vice versa. Some mechanism to link between fea-
tures beyond layers needs to be introduced into the
current MILE framework.
Orthographic variants Many Chinese words
have orthographic variants. For instance, the con-
cept of rising can be represented by either char-
acter variants of sheng1: ? or ?. However,
the free variants become non-free in certain com-
pound forms. For instance, only? allowed for?
? ?liter?, and only? is allowed for?? ?to sub-
lime?. The interaction of lemmas and orthographic
variations is not yet represented in MILE.
Reduplication as a derivational process In
some Asian languages, reduplication of words de-
rives another word, and the derived word often has
a different part-of-speech. Here are some exam-
ples of reduplication in Chinese. Man4 ? ?to be
slow? is a state verb, while a reduplicated form
829
Inflectional
Paradigm
Lexical Entry SyntacticUnit
Form Lemmatized Form Stem
Inflected Form
Combiner
Calculator Mrophfeat
Operation Argument
Morph
DataCats
0..*
0..* 0..*
0..*
0..*
0..1
0..*
0..*
1..*
<LemmatizedForm rdf:ID="LFstar">
  <hasInflectedForm>
    <InflectedForm rdf:ID="stars">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="pl">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    plural
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
  <hasInflectedForm>
    <InflectedForm rdf:ID="star">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="sg">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    singular
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
</LemmatiedForm>
Figure 2: Formalization of the morphological layer and excerpt of a sample RDF instantiation
man4-man4 ?? is an adverb. Another example
of reduplication involves verbal aspect. Kan4 ?
?to look? is an activity verb, while the reduplica-
tive form kan4-kan4 ??, refers to the tentative
aspect, introducing either stage-like sub-division
or the event or tentativeness of the action of the
agent. This morphological process is not provided
for in the current MILE standard.
There are also various usages of reduplication in
Thai. Some words reduplicate themselves to add a
specific aspect to the original meaning. The redu-
plication can be grouped into 3 types according to
the tonal sound change of the original word.
? Word reduplication without sound change
e.g. /dek-dek/ ? ? ? (N) children, (ADV) child-
ishly, (ADJ) childish
/sa:w-sa:w/ ? ? ? (N) women
? Word reduplication with high tone on the first
word
e.g. /dam4-dam/ ? ? ? (ADJ) extremely black
/bo:i4-bo:i/ ? ? ? (ADV) really often
? Triple word reduplication with high tone on
the second word
e.g. /dern-dern4-dern/ ?? (V) intensively walk
/norn-norn4-norn/??(V) intensively sleep
In fact, only the reduplication of the same sound
is accepted in the written text, and a special sym-
bol, namely /mai-yamok/ is attached to the origi-
nal word to represent the reduplication. The redu-
plication occurs in many parts-of-speech, such as
noun, verb, adverb, classifier, adjective, preposi-
tion. Furthermore, various aspects can be added
to the original meaning of the word by reduplica-
tion, such as pluralization, emphasis, generaliza-
tion, and so on. These aspects should be instanti-
ated as features.
Change of parts-of-speech by affixes Af-
fixes change parts-of-speech of words in
Thai (Charoenporn et al, 1997). There are
three prefixes changing the part-of-speech of the
original word, namely /ka:n/, /khwa:m/, /ya:ng/.
They are used in the following cases.
? Nominalization
/ka:n/ is used to prefix an action verb and
/khwa:m/ is used to prefix a state verb
in nominalization such as /ka:n-tham-nga:n/
(working), /khwa:m-suk/ (happiness).
? Adverbialization
An adverb can be derived by using /ya:ng/ to
prefix a state verb such as /ya:ng-di:/ (well).
Note that these prefixes are also words, and form
multi-word expressions with the original word.
This phenomenon is similar to derivation which
is not handled in the current MILE framework.
Derivation is traditionally considered as a different
phenomenon from inflection, and current MILE
focuses on inflection. The MILE framework is al-
ready being extended to treat such linguistic phe-
nomenon, since it is important to European lan-
guages as well. It would be handled in either the
morphological layer or syntactic layer.
830
Function Type Function types of predicates
(verbs, adjectives etc.) might be handled in a
partially different way for Japanese. In the syn-
tactic layer of the MILE framework, Function-
Type class is prepared to denote subcategorization
frames of predicates, and they have function types
such as ?subj? and ?obj?. For example, the verb
?eat? has two FunctionType data categories of
?subj? and ?obj?. Function types basically stand
for positions of case filler nouns. In Japanese,
cases are usually marked by postpositions and case
filler positions themselves do not provide much in-
formation on case marking. For example, both of
the following sentences mean the same, ?She eats
a pizza.?
? kanojo
(she)
ga
(NOM)
piza
(pizza)
wo
(ACC)
taberu
(eat)
? piza
(pizza)
wo
(ACC)
kanojo
(she)
ga
(NOM)
taberu
(eat)
?Ga? and ?wo? are postpositions which mark
nominative and accusative cases respectively.
Note that two case filler nouns ?she? and ?pizza?
can be exchanged. That is, the number of slots is
important, but their order is not.
For Japanese, we might use the set of post-
positions as values of FunctionType instead of
conventional function types such as ?subj? and
?obj?. It might be an user defined data category or
language dependent data category. Furthermore,
it is preferable to prepare the mapping between
Japanese postpositions and conventional function
types. This is interesting because it seems more
a terminological difference, but the model can be
applied also to Japanese.
4 Building sample lexicons
4.1 Swadesh list and basic lexicon
The issue involved in defining a basic lexicon for a
given language is more complicated than one may
think (Zhang et al, 2004). The naive approach of
simply taking the most frequent words in a lan-
guage is flawed in many ways. First, all frequency
counts are corpus-based and hence inherit the bias
of corpus sampling. For instance, since it is eas-
ier to sample written formal texts, words used pre-
dominantly in informal contexts are usually under-
represented. Second, frequency of content words
is topic-dependent and may vary from corpus to
corpus. Last, and most crucially, frequency of a
word does not correlate to its conceptual necessity,
which should be an important, if not only, criteria
for core lexicon. The definition of a cross-lingual
basic lexicon is even more complicated. The first
issue involves determination of cross-lingual lexi-
cal equivalencies. That is, how to determine that
word a (and not a?) in language A really is word b
in language B. The second issue involves the deter-
mination of what is a basic word in a multilingual
context. In this case, not even the frequency of-
fers an easy answer since lexical frequency may
vary greatly among different languages. The third
issue involves lexical gaps. That is, if there is a
word that meets all criteria of being a basic word
in language A, yet it does not exist in language D
(though it may exist in languages B, and C). Is this
word still qualified to be included in the multilin-
gual basic lexicon?
It is clear not all the above issues can be un-
equivocally solved with the time frame of our
project. Fortunately, there is an empirical core lex-
icon that we can adopt as a starting point. The
Swadesh list was proposed by the historical lin-
guist Morris Swadesh (Swadesh, 1952), and has
been widely used by field and historical linguists
for languages over the world. The Swadesh list
was first proposed as lexico-statistical metrics.
That is, these are words that can be reliably ex-
pected to occur in all historical languages and can
be used as the metrics for quantifying language
variations and language distance. The Swadesh
list is also widely used by field linguists when
they encounter a new language, since almost all
of these terms can be expected to occur in any
language. Note that the Swadesh list consists of
terms that embody human direct experience, with
culture-specific terms avoided. Swadesh started
with a 215 items list, before cutting back to 200
items and then to 100 items. A standard list of
207 items is arrived at by unifying the 200 items
list and the 100 items list. We take the 207 terms
from the Swadesh list as the core of our basic lex-
icon. Inclusion of the Swadesh list also gives us
the possibility of covering many Asian languages
in which we do not have the resources to make a
full and fully annotated lexicon. For some of these
languages, a Swadesh lexicon for reference is pro-
vided by a collaborator.
4.2 Aligning multilingual lexical entries
Since our goal is to build a multilingual sample
lexicon, it is required to align words in several
831
Asian languages. In this subsection, we propose
a simple method to align words in different lan-
guages. The basic idea for multilingual alignment
is an intermediary by English. That is, first we
prepare word pairs between English and other lan-
guages, then combine them together to make cor-
respondence among words in several languages.
The multilingual alignment method currently we
consider is as follows:
1. Preparing the set of frequent words of each
language
Suppose that {Jw
i
}, {Cw
i
}, {Tw
i
} is the
set of frequent words of Japanese, Chinese
and Thai, respectively. Now we try to con-
struct a multilingual lexicon for these three
languages, however, our multilingual align-
ment method can be easily extended to han-
dle more languages.
2. Obtaining English translations
A word Xw
i
is translated into a set of En-
glish words EXw
ij
by referring to the bilin-
gual dictionary, where X denotes one of our
languages, J , C or T . We can obtain map-
pings as in (1).
Jw
1
: EJw
11
, EJw
12
, ? ? ?
Jw
2
: EJw
21
, EJw
22
, ? ? ?
...
Cw
1
: ECw
11
, ECw
12
, ? ? ?
Cw
2
: ECw
21
, ECw
22
, ? ? ?
...
Tw
1
: ETw
11
, ETw
12
, ? ? ?
Tw
2
: ETw
21
, ETw
22
, ? ? ?
...
(1)
Notice that this procedure is automatically
done and ambiguities would be left at this
stage.
3. Generating new mapping
From mappings in (1), a new mapping is gen-
erated by inverting the key. That is, in the
new mapping, a key is an English word Ew
i
and a correspondence for each key is sets
of translations XEw
ij
for 3 languages, as
shown in (2):
Ew
1
: (JEw
11
, JEw
12
, ? ? ?)
(CEw
11
, CEw
12
, ? ? ?)
(TEw
11
, TEw
12
, ? ? ?)
Ew
2
: (JEw
21
, JEw
22
, ? ? ?)
(CEw
21
, CEw
22
, ? ? ?)
(TEw
21
, TEw
22
, ? ? ?)
...
(2)
Notice that at this stage, correspondence be-
tween different languages is very loose, since
they are aligned on the basis of sharing only
a single English word.
4. Refinement of alignment
Groups of English words are constructed by
referring to the WordNet synset information.
For example, suppose that Ew
i
and Ew
j
be-
long to the same synset S
k
. We will make a
new alignment by making an intersection of
{XEw
i
} and {XEw
j
} as shown in (3).
Ew
i
: (JEw
i1
, ??) (CEw
i1
, ??) (TEw
i1
, ??)
Ew
j
: (JEw
j1
, ??)(CEw
j1
, ??)(TEw
j1
, ??)
? intersection
S
k
: (JEw?
k1
, ??)(CEw?
k1
, ??)(TEw?
k1
, ??)
(3)
In (3), the key is a synset S
k
, which is sup-
posed to be a conjunction of Ew
i
and Ew
j
,
and the counterpart is the intersection of set
of translations for each language. This oper-
ation would reduce the number of words of
each language. That means, we can expect
that the correspondence among words of dif-
ferent languages becomes more precise. This
new word alignment based on a synset is a
final result.
To evaluate the performance of this method,
we conducted a preliminary experiment using the
Swadesh list. Given the Swadesh list of Chi-
nese, Italian, Japanese and Thai as a gold stan-
dard, we tried to replicate these lists from the En-
glish Swadesh list and bilingual dictionaries be-
tween English and these languages. In this experi-
ment, we did not perform the refinement step with
WordNet. From 207 words in the Swadesh list,
we dropped 4 words (?at?, ?in?, ?with? and ?and?)
due to their too many ambiguities in translation.
As a result, we obtained 181 word groups
aligned across 5 languages (Chinese, English, Ital-
ian, Japanese and Thai) for 203 words. An
aligned word group was judged ?correct? when the
words of each language include only words in the
Swadesh list of that language. It was judged ?par-
tially correct? when the words of a language also
include the words which are not in the Swadesh
list. Based on the correct instances, we obtain
0.497 for precision and 0.443 for recall. These fig-
ures go up to 0.912 for precision and 0.813 for re-
call when based on the partially correct instances.
This is quite a promising result.
832
5 Upper-layer ontology
The empirical success of the Swadesh list poses
an interesting question that has not been explored
before. That is, does the Swadesh list instantiates a
shared, fundamental human conceptual structure?
And if there is such as a structure, can we discover
it?
In the project these fundamental issues are as-
sociated with our quest for cross-lingual interop-
erability. We must make sure that the items of
the basic lexicon are given the same interpreta-
tion. One measure taken to ensure this consists in
constructing an upper-ontology based on the ba-
sic lexicon. Our preliminary work of mapping the
Swadesh list items to SUMO (Suggested Upper
Merged Ontology) (Niles and Pease, 2001) has al-
ready been completed. We are in the process of
mapping the list to DOLCE (Descriptive Ontology
for Linguistic and Cognitive Engineering) (Ma-
solo et al, 2003). After the initial mapping, we
carry on the work to restructure the mapped nodes
to form a genuine conceptual ontology based on
the language universal basic lexical items. How-
ever one important observation that we have made
so far is that the success of the Swadesh list is
partly due to its underspecification and to the lib-
erty it gives to compilers of the list in a new lan-
guage. If this idea of underspecification is essen-
tial for basic lexicon for human languages, then we
must resolve this apparent dilemma of specifying
them in a formal ontology that requires fully spec-
ified categories. For the time being, genuine ambi-
guities resulted in the introduction of each disam-
biguated sense in the ontology. We are currently
investigating another solution that allows the in-
clusion of underspecified elements in the ontology
without threatening its coherence. More specifi-
cally we introduce a underspecified relation in the
structure for linking the underspecified meaning
to the different specified meaning. The specified
meanings are included in the taxonomic hierarchy
in a traditional manner, while a hierarchy of un-
derspecified meanings can be derived thanks to the
new relation. An underspecified node only inherits
from the most specific common mother of its fully
specified terms. Such distinction avoids the clas-
sical misuse of the subsumption relation for rep-
resenting multiple meanings. This method does
not reflect a dubious collapse of the linguistic and
conceptual levels but the treatment of such under-
specifications as truly conceptual. Moreover we
Internet
Query
Local 
DB
User interest
 model
Topic
Feedback
Search
engine
Crawler
Retrieval
results
Figure 3: The system architecture
hope this proposal will provide a knowledge rep-
resentation framework for the multilingual align-
ment method presented in the previous section.
Finally, our ontology will not only play the role
of a structured interlingual index. It will also serve
as a common conceptual base for lexical expan-
sion, as well as for comparative studies of the lex-
ical differences of different languages.
6 Evaluation through an application
To evaluate the proposed framework, we are build-
ing an information retrieval system. Figure 3
shows the system architecture.
A user can input a topic to retrieve the docu-
ments related to that topic. A topic can consist
of keywords, website URL?s and documents which
describe the topic. From the topic information, the
system builds a user interest model. The system
then uses a search engine and a crawler to search
for information related to this topic in WWW and
stores the results in the local database. Generally,
the search results include many noises. To filter
out these noises, we build a query from the user
interest model and then use this query to retrieve
documents in the local database. Those documents
similar to the query are considered as more related
to the topic and the user?s interest, and are returned
to the user. When the user obtains these retrieval
results, he can evaluate these documents and give
the feedback to the system, which is used for the
further refinement of the user interest model.
Language resources can contribute to improv-
ing the system performance in various ways.
Query expansion is a well-known technique which
expands user?s query terms into a set of similar and
related terms by referring to ontologies. Our sys-
tem is based on the vector space model (VSM) and
traditional query expansion can be applicable us-
ing the ontology.
There has been less research on using lexical in-
833
formation for information retrieval systems. One
possibility we are considering is query expansion
by using predicate-argument structures of terms.
Suppose a user inputs two keywords, ?hockey?
and ?ticket? as a query. The conventional query
expansion technique expands these keywords to
a set of similar words based on an ontology. By
referring to predicate-argument structures in the
lexicon, we can derive actions and events as well
which take these words as arguments. In the above
example, by referring to the predicate-argument
structure of ?buy? or ?sell?, and knowing that
these verbs can take ?ticket? in their object role,
we can add ?buy? and ?sell? to the user?s query.
This new type of expansion requires rich lexical
information such as predicate argument structures,
and the information retrieval system would be a
good touchstone of the lexical information.
7 Concluding remarks
This paper outlined a new project for creating a
common standard for Asian language resources
in cooperation with other initiatives. We start
with three Asian languages, Chinese, Japanese
and Thai, on top of the existing framework which
was designed mainly for European languages.
We plan to distribute our draft to HLT soci-
eties of other Asian languages, requesting for
their feedback through various networks, such
as the Asian language resource committee net-
work under Asian Federation of Natural Language
Processing (AFNLP)4, and Asian Language Re-
source Network project5. We believe our ef-
forts contribute to international activities like ISO-
TC37/SC46 (Francopoulo et al, 2006) and to the
revision of the ISO Data Category Registry (ISO
12620), making it possible to come close to the
ideal international standard of language resources.
Acknowledgment
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004a. Content interoperability of lexical re-
sources, open issues and ?MILE? perspectives. In
4http://www.afnlp.org/
5http://www.language-resource.net/
6http://www.tc37sc4.org/
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004),
pages 131?134.
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004b. The MILE lexical classes: Data cat-
egories for content interoperability among lexicons.
In A Registry of Linguistic Data Categories within
an Integrated Language Resources Repository Area
? LREC2004 Satellite Workshop, page 8.
N. Calzolari, F. Bertagna, A. Lenci, and M. Mona-
chini. 2003. Standards and best practice for mul-
tilingual computational lexicons. MILE (the mul-
tilingual ISLE lexical entry). ISLE Deliverable
D2.2&3.2.
T. Charoenporn, V. Sornlertlamvanich, and H. Isahara.
1997. Building a large Thai text corpus ? part-
of-speech tagged corpus: ORCHID?. In Proceed-
ings of the Natural LanguageProcessing Pacific Rim
Symposium.
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006 (forthcoming).
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
C. Masolo, A. Borgo, S.; Gangemi, N. Guarino, and
A. Oltramari. 2003. Wonderweb deliverable d18
?ontology library (final)?. Technical report, Labo-
ratory for Applied Ontology, ISTC-CNR.
I. Niles and A Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
V. Sornlertlamvanich, W. Pantachat, and S. Mek-
navin. 1994. Classifier assignment by corpus-
based approach. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING-94), pages 556?561.
M. Swadesh. 1952. Lexico-statistical dating of pre-
historic ethnic contacts: With special reference to
north American Indians and Eskimos. In Proceed-
ings of the American Philo-sophical Society, vol-
ume 96, pages 452?463.
H. Zhang, C. Huang, and S. Yu. 2004. Distributional
consistency: A general method for defining a core
lexicon. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC2004), pages 1119?1222.
834
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 69?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Rethinking Chinese Word Segmentation: Tokenization, Character
Classification, or Wordbreak Identification
Chu-Ren Huang
Institute of Linguistics
Academia Sinica,Taiwan
churen@gate.sinica.edu.tw
Petr S?imon
Institute of Linguistics
Academia Sinica,Taiwan
sim@klubko.net
Shu-Kai Hsieh
DoFLAL
NIU, Taiwan
shukai@gmail.com
Laurent Pre?vot
CLLE-ERSS, CNRS
Universite? de Toulouse, France
prevot@univ-tlse2.fr
Abstract
This paper addresses two remaining chal-
lenges in Chinese word segmentation. The
challenge in HLT is to find a robust seg-
mentation method that requires no prior lex-
ical knowledge and no extensive training to
adapt to new types of data. The challenge
in modelling human cognition and acqui-
sition it to segment words efficiently with-
out using knowledge of wordhood. We pro-
pose a radical method of word segmenta-
tion to meet both challenges. The most
critical concept that we introduce is that
Chinese word segmentation is the classifi-
cation of a string of character-boundaries
(CB?s) into either word-boundaries (WB?s)
and non-word-boundaries. In Chinese, CB?s
are delimited and distributed in between two
characters. Hence we can use the distri-
butional properties of CB among the back-
ground character strings to predict which
CB?s are WB?s.
1 Introduction: modeling and theoretical
challenges
The fact that word segmentation remains a main re-
search topic in the field of Chinese language pro-
cessing indicates that there maybe unresolved theo-
retical and processing issues. In terms of processing,
the fact is that none of exiting algorithms is robust
enough to reliably segment unfamiliar types of texts
before fine-tuning with massive training data. It is
true that performance of participating teams have
steadily improved since the first SigHAN Chinese
segmentation bakeoff (Sproat and Emerson, 2004).
Bakeoff 3 in 2006 produced best f-scores at 95%
and higher. However, these can only be achieved af-
ter training with the pre-segmented training dataset.
This is still very far away from real-world applica-
tion where any varieties of Chinese texts must be
successfully segmented without prior training for
HLT applications.
In terms of modeling, all exiting algorithms suffer
from the same dilemma. Word segmentation is sup-
posed to identify word boundaries in a running text,
and words defined by these boundaries are then com-
pared with the mental/electronic lexicon for POS
tagging and meaning assignments. All existing seg-
mentation algorithms, however, presuppose and/or
utilize a large lexical databases (e.g. (Chen and Liu,
1992) and many subsequent works), or uses the po-
sition of characters in a word as the basis for seg-
mentation (Xue, 2003).
In terms of processing model, this is a contradic-
tion since segmentation should be the pre-requisite
of dictionary lookup and should not presuppose lex-
ical information. In terms of cognitive modeling,
such as for acquisition, the model must be able to ac-
count for how words can be successfully segmented
and learned by a child/speaker without formal train-
ing or a priori knowledge of that word. All current
models assume comprehensive lexical knowledge.
2 Previous work
Tokenization model. The classical model, de-
scribed in (Chen and Liu, 1992) and still adopted in
many recent works, considers text segmentation as a
69
tokenization. Segmentation is typically divided into
two stages: dictionary lookup and out of vocabulary
(OOV) word identification. This approach requires
comparing and matching tens of thousands of dic-
tionary entries in addition to guessing thousands of
OOV words. That is, this is a 104x104 scale map-
ping problem with unavoidable data sparseness.
More precisely the task consist in finding
all sequences of characters Ci, . . . , Cn such that
[Ci, . . . Cn] either matches an entry in the lexicon
or is guessed to be so by an unknown word resolu-
tion algorithm. One typical kind of the complexity
this model faces is the overlapping ambiguity where
e.g. a string [Ci ? 1, Ci, Ci + 1] contains multiple
substrings, such as [Ci ? 1, Ci, ] and [Ci,Ci + 1],
which are entries in the dictionary. The degree of
such ambiguities is estimated to fall between 5% to
20% (Chiang et al, 1996; Meng and Ip, 1999).
2.1 Character classification model
A popular recent innovation addresses the scale
and sparseness problem by modeling segmentation
as character classification (Xue, 2003; Gao et al,
2004). This approach observes that by classifying
characters as word-initial, word-final, penultimate,
etc., word segmentation can be reduced to a simple
classification problem which involves about 6,000
characters and around 10 positional classes. Hence
the complexity is reduced and the data sparseness
problem resolved. It is not surprising then that the
character classification approach consistently yields
better results than the tokenization approach. This
approach, however, still leaves two fundamental
questions unanswered. In terms of modeling, us-
ing character classification to predict segmentation
not only increases the complexity but also necessar-
ily creates a lower ceiling of performance In terms
of language use, actual distribution of characters is
affected by various factors involving linguistic vari-
ation, such as topic, genre, region, etc. Hence the
robustness of the character classification approach
is restricted.
The character classification model typically clas-
sifies all characters present in a string into at least
three classes: word Initial, Middle or Final po-
sitions, with possible additional classification for
word-middle characters. Word boundaries are in-
ferred based on the character classes of ?Initial? or
?Final?.
This method typically yields better result than the
tokenization model. For instance, Huang and Zhao
(2006) claims to have a f-score of around 97% for
various SIGHAN bakeoff tasks.
3 A radical model
We propose a radical model that returns to the
core issue of word segmentation in Chinese. Cru-
cially, we no longer pre-suppose any lexical knowl-
edge. Any unsegmented text is viewed as a string
of character-breaks (CB?s) which are evenly dis-
tributed and delimited by characters. The characters
are not considered as components of words, instead,
they are contextual background providing informa-
tion about the likelihood of whether each CB is also
a wordbreak (WB). In other words, we model Chi-
nese word segmentation as wordbreak (WB) iden-
tification which takes all CB?s as candidates and
returns a subset which also serves as wordbreaks.
More crucially, this model can be trained efficiently
with a small corpus marked with wordbreaks and
does not require any lexical database.
3.1 General idea
Any Chinese text is envisioned as se-
quence of characters and character-boundaries
CB0C1CB1C2 . . . CBi?1CiCBi . . . CBn?1CnCBn The
segmentation task is reduced to finding all CBs
which are also wordbreaks WB.
3.2 Modeling character-based information
Since CBs are all the same and do not carry any
information, we have to rely on their distribution
among different characters to obtain useful infor-
mation for modeling. In a segmented corpus, each
WB can be differentiated from a non-WB CB by the
character string before and after it. We can assume
a reduced model where either one character imme-
diately before and after a CB is considered or two
characters (bigram). These options correspond to
consider (i) only word-initial and word-final posi-
tions (hereafter the 2-CB-model or 2CBM) or (ii) to
add second and penultimate positions (hereafter the
4-CB-model or 4CBM). All these positions are well-
attested as morphologically significant.
70
3.3 The nature of segmentation
It is important to note that in this approaches,
although characters are recognized, unlike (Xue,
2003) and Huang et al (2006), charactes simply
are in the background. That is, they are the neces-
sary delimiter, which allows us to look at the string
of CB?s and obtaining distributional information of
them.
4 Implementation and experiments
In this section we slightly change our notation to
allow for more precise explanation. As noted be-
fore, Chinese text can be formalized as a sequence
of characters and intervals as illustrated in we call
this representation an interval form.
c1I1c2I2 . . . cn?1In?1cn.
In such a representation, each interval Ik is either
classified as a plain character boundary (CB) or as
a word boundary (WB).
We represent the neighborhood of the character
ci as (ci?2, Ii?2, ci?1, Ii?1, ci, Ii, ci+1, Ii+1), which
we can be simplified as (I?2, I?1, ci, I+1, I+2) by
removing all the neighboring characters and retain-
ing only the intervals.
4.1 Data collection models
This section makes use of the notation introduced
above for presenting several models accounting for
character-interval class co-occurrence.
Word based model. In this model, statistical data
about word boundary frequencies for each character
is retrieved word-wise. For example, in the case of
a monosyllabic word only two word boundaries are
considered: one before and one after the character
that constitutes the monosyllabic word in question.
The method consists in mapping all the Chinese
characters available in the training corpus to a vector
of word boundary frequencies. These frequencies
are normalized by the total frequency of the char-
acter in a corpus and thus represent probability of a
word boundary occurring at a specified position with
regard to the character.
Let us consider for example, a tri-syllabic word
W = c1c2c3, that can be rewritten as the following
interval form as W I = IB?1c1I
N
1 c2I
N
2 c3I
B
3 .
In this interval form, each interval Ik is marked
as word boundary B or N for intervals within words.
When we consider a particular character c1 in W ,
there is a word boundary at index?1 and 3. We store
this information in a mapping c1 = {?1 : 1, 3 : 1}.
For each occurrence of this character in the corpus,
we modify the character vector accordingly, each
WB corresponding to an increment of the relevant
position in the vector. Every character in every word
of the corpus in processed in a similar way.
Obviously, each character yields only information
about positions of word boundaries of a word this
particular character belongs to. This means that the
index I?1 and I3 are not necessarily incremented
everytime (e.g. for monosyllabic and bi-syllabic
words)
Sliding window model. This model does not op-
erate on words, but within a window of a give size
(span) sliding through the corpus. We have exper-
imented this method with a window of size 4. Let
us consider a string, s = ?c1c2c3c4? which is not
necessarily a word and is rewritten into an interval
form as sI = ?c1I1c2I2c3I3c4I4?. We store the
co-occurrence character/word boundaries informa-
tion in a fixed size (span) vector.
For example, we collect the information for
character c3 and thus arrive at a vector c3 =
(I1, I2, I3, I4), where 1 is incremented at the respec-
tive position ifIk = WB, zero otherwise.
This model provides slightly different informa-
tion that the previous one. For example, if
a sequence of four characters is segmented as
c1IN1 c2I
B
2 c3I
B
3 c4I
B
4 (a sequence of one bi-syllabic
and two monosyllabic words), for c3 we would also
get probability of I4, i.e. an interval with index +2
. In other words, this model enables to learn WB
probability across words.
4.2 Training corpus
In the next step, we convert our training corpus into
a corpus of interval vectors of specified dimension.
Let?s assume we are using dimension span = 4.
Each value in such a vector represents the proba-
bility of this interval to be a word boundary. This
probability is assigned by character for each position
with regard to the interval. For example, we have
segmented corpus C = c1I1c2I2 . . . cn?1In?1cn,
where each Ik is labeled as B for word boundary
or N for non-boundary.
71
In the second step, we move our 4-sized window
through the corpus and for each interval we query
a character at the corresponding position from the
interval to retrieve the word boundary occurrence
probability. This procedure provides us with a vec-
tor of 4 probability values for each interval. Since
we are creating this training corpus from an already
segmented text, a class (B or N ) is assigned to each
interval.
The testing corpus (unsegmented) is encoded in a
similar way, but does not contain the class labels B
and N .
Finally, we automatically assign probability of 0.5
for unseen events.
4.3 Predicting word boundary with a classifier
The Sinica corpus contains 6820 types of characters
(including Chinese characters, numbers, punctua-
tion, Latin alphabet, etc.). When the Sinica corpus is
converted into our interval vector corpus, it provides
14.4 million labeled interval vectors. In this first
study we have implement a baseline model, without
any pre-processing of punctuation, numbers, names.
A decision tree classifier (Ruggieri, 2004) has
been adopted to overcome the non-linearity issue.
The classifier was trained on the whole Sinica cor-
pus, i.e. on 14.4 million interval vectors. Due to
space limit, actual bakeoff experiment result will be
reported in our poster presentation.
Our best results is based on the sliding window
model, which provides better results. It has to be
emphasized that the test corpora were not processed
in any way, i.e. our method is sufficiently robust to
account for a large number of ambiguities like nu-
merals, foreign words.
5 Conclusion
In this paper, we presented a radical and robust
model of Chinese segmentation which is supported
by initial experiment results. The model does not
pre-suppose any lexical information and it treats
character strings as context which provides infor-
mation on the possible classification of character-
breaks as word-breaks. We are confident that once
a standard model of pre-segmentation, using tex-
tual encoding information to identify WB?s which
involves non-Chinese characters, will enable us to
achieve even better results. In addition, we are look-
ing at other alternative formalisms and tools to im-
plement this model to achieve the optimal results.
Other possible extensions including experiments to
simulate acquisition of wordhood knowledge to pro-
vide support of cognitive modeling, similar to the
simulation work on categorization in Chinese by
(Redington et al, 1995). Last, but not the least,
we will explore the possibility of implementing a
sharable tool for robust segmentation for all Chinese
texts without training.
References
Academia Sinica Balanced Corpus of Modern Chinese.
http://www.sinica.edu.tw/SinicaCorpus/
Chen K.J and Liu S.H. 1992. Word Identification for
Mandarin Chinese sentences. Proceedings of the 14th
conference on Computational Linguistics, p.101-107,
France.
Chiang,T.-H., J.-S. Chang, M.-Y. Lin and K.-Y. Su. 1996.
Statistical Word Segmentation. In C.-R. Huang, K.-J.
Chen and B.K. T?sou (eds.): Journal of Chinese Lin-
guistics, Monograph Series, Number 9, Readings in
Chinese Natural Language Processing, pp. 147-173.
Gao, J. and A. Wu and Mu Li and C.-N.Huang and H. Li
and X. Xia and H. Qin. 2004. Adaptive Chinese Word
Segmentation. In Proceedings of ACL-2004.
Meng, H. and C. W. Ip. 1999. An Analytical Study of
Transformational Tagging for Chinese Text. In. Pro-
ceedings of ROCLING XII. 101-122. Taipei
Ruggieri S. 2004. YaDT: Yet another Decision Tree
builder. Proceedings of the 16th International Con-
ference on Tools with Artificial Intelligence (ICTAI
2004): 260-265. IEEE Press, November 2004.
Richard Sproat and Thomas Emerson. 2003. The
First International Chinese Word Segmentation Bake-
off. Proceedings of the Second SIGHAN Workshop on
Chinese Language Processing, Sapporo, Japan, July
2003.
Xue, N. 2003. Chinese Word Segmentation as Charac-
ter Tagging. Computational Linguistics and Chinese
Language Processing. 8(1): 29-48
Redington, M. and N. Chater and C. Huang and L. Chang
and K. Chen. 1995. The Universality of Simple Dis-
tributional Methods: Identifying Syntactic Categories
in Mandarin Chinese. Presented at the Proceedings of
the International Conference on Cognitive Science and
Natural Language Processing. Dublin City University.
72
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 153?156,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Discovery of Named Entity Variants
? Grammar-driven Approaches to Non-alphabetical Transliterations
Chu-Ren Huang
Institute of Linguistics
Academia Sinica, Taiwan
churenhuang@gmail.com
Petr S?imon
Institute of Linguistics
Academia Sinica, Taiwan
sim@klubko.net
Shu-Kai Hsieh
DoFLAL
NIU, Taiwan
shukai@gmail.com
Abstract
Identification of transliterated names is a
particularly difficult task of Named Entity
Recognition (NER), especially in the Chi-
nese context. Of all possible variations of
transliterated named entities, the difference
between PRC and Taiwan is the most preva-
lent and most challenging. In this paper, we
introduce a novel approach to the automatic
extraction of diverging transliterations of
foreign named entities by bootstrapping co-
occurrence statistics from tagged and seg-
mented Chinese corpus. Preliminary experi-
ment yields promising results and shows its
potential in NLP applications.
1 Introduction
Named Entity Recognition (NER) is one of the most
difficult problems in NLP and Document Under-
standing. In the field of Chinese NER, several
approaches have been proposed to recognize per-
sonal names, date/time expressions, monetary and
percentage expressions. However, the discovery of
transliteration variations has not been well-studied
in Chinese NER. This is perhaps due to the fact
that the transliteration forms in a non-alphabetic lan-
guage such as Chinese are opaque and not easy to
compare. On the hand, there is often more than
one way to transliterate a foreign name. On the
other hand, dialectal difference as well as differ-
ent transliteration strategies often lead to the same
named entity to be transliterated differently in dif-
ferent Chinese speaking communities.
Corpus Example (Clinton) Frequency
XIN ??? 24382
CNA ??? 150
XIN ??? 0
CNA ??? 120842
Table 1: Distribution of two transliteration variants
for ?Clinton? in two sub-corpora
Of all possible variations, the cross-strait differ-
ence between PRC and Taiwan is the most prevalent
and most challenging.1The main reason may lie in
the lack of suitable corpus.
Even given some subcorpora of PRC and Taiwan
variants of Chinese, a simple contrastive approach is
still not possible. It is because: (1) some variants
might overlap and (2) there are more variants used
in each corpus due to citations or borrowing cross-
strait. Table 1 illustrates this phenomenon, where
CNA stands for Central News Agency in Taiwan,
XIN stands for Xinhua News Agency in PRC, re-
spectively.
With the availability of Chinese Gigaword Cor-
pus (CGC) and Word Sketch Engine (WSE) Tools
(Kilgarriff, 2004). We propose a novel approach
towards discovery of transliteration variants by uti-
lizing a full range of grammatical information aug-
mented with phonological analysis.
Existing literatures on processing of translitera-
tion concentrate on the identification of either the
transliterated term or the original term, given knowl-
edge of the other (e.g. (Virga and Khudanpur,
1For instance, we found at least 14 transliteration variants
for Lewinsky,such as ????????????????????????
?????????????????????????????????????
?????? and so on.
153
2003)). These studies are typically either rule-based
or statistics-based, and specific to a language pair
with a fixed direction (e.g. (Wan and Verspoor,
1998; Jiang et al, 2007)). To the best of our knowl-
edge, ours is the first attempt to discover transliter-
ated NE?s without assuming prior knowledge of the
entities. In particular, we propose that transliteration
variants can be discovered by extracting and com-
paring terms from similar linguistic context based
on CGC and WSE tools. This proposal has great po-
tential of increasing robustness of future NER work
by enabling discovery of new and unknown translit-
erated NE?s.
Our study shows that resolution of transliterated
NE variations can be fully automated. This will have
strong and positive implications for cross-lingual
and multi-lingual informational retrieval.
2 Bootstrapping transliteration pairs
The current study is based on Chinese Gigaword
Corpus (CGC) (Graff el al., 2005), a large corpus
contains with 1.1 billion Chinese characters contain-
ing data from Central News Agency of Taiwan (ca.
700 million characters), Xinhua News Agency of
PRC (ca. 400 million characters). These two sub-
corpora represent news dispatches from roughly the
same period of time, i.e. 1990-2002. Hence the two
sub-corpora can be expected to have reasonably par-
allel contents for comparative studies.2
The premises of our proposal are that transliter-
ated NE?s are likely to collocate with other translit-
erated NE?s, and that collocates of a pair of translit-
eration variants may form contrasting pairs and are
potential variants. In particular, since the transliter-
ation variations that we are interested in are those
between PRC and Taiwan Mandarin, we will start
with known contrasting pairs of these two language
variants and mine potential variant pairs from their
collocates. These potential variant pairs are then
checked for their phonological similarity to deter-
mine whether they are true variants or not. In order
to effectively select collocates from specific gram-
matical constructions, the Chinese Word Sketch3 is
adopted. In particular, we use the Word Sketch dif-
2To facilitate processing, the complete CGC was segmented
and POS tagged using the Academia Sinica segmentation and
tagging system (Ma and Huang, 2006).
3http://wordsketch.ling.sinica.edu.tw
ference (WSDiff) function to pick the grammatical
contexts as well as contrasting pairs. It is important
to bear in mind that Chinese texts are composed of
Chinese characters, hence it is impossible to com-
pare a transliterated NE with the alphabetical form
in its original language. The following characteris-
tics of a transliterated NE?s in CGC are exploited to
allow discovery of transliteration variations without
referring to original NE.
? frequent co-occurrence of named entities
within certain syntagmatic relations ? named
entities frequently co-occur in relations such as
AND or OR and this fact can be used to collect
and score mutual predictability.
? foreign named entities are typically transliter-
ated phonetically ? transliterations of the same
name entity using different characters can be
matched by using simple heuristics to map their
phonological value.
? presence and co-occurrence of named entities
in a text is dependent on a text type ? journalis-
tic style cumulates many foreign named entities
in close relations.
? many entities will occur in different domains
? famous person can be mentioned together
with someone from politician, musician, artist
or athlete. Thus allows us to make leaps from
one domain to another.
There are, however, several problems with the
phonological representation of foreign named enti-
ties in Chinese. Due to the nature of Chinese script,
NE transliterations can be realized very differently.
The following is a summary of several problems that
have to be taken into account:
? word ending: ??? vs.???? ?Arafat? or ?
?? vs.???? ?Mubarak?. The final conso-
nant is not always transliterated. XIN translit-
erations tend to try to represent all phonemes
and often add vowels to a final consonant to
form a new syllable, whereas CNA transliter-
ation tends to be shorter and may simply leave
out a final consonant.
? gender dependent choice of characters: ???
?Leslie? vs.??? ?Chris? or ???? vs. ???
154
?. Some occidental names are gender neutral.
However, the choice of characters in a personal
name in Chinese is often gender sensitive. So
these names are likely to be transliterated dif-
ferently depending on the gender of its referent.
? divergent representations caused by scope of
transliteration, e.g. both given and surname
vs. only surname: ???? / ????? ?Venus
Williams?.
? difference in phonological interpretation: ??
? vs. ??? ?Rafter? or??? vs. ??? ?Connors?.
? native vs. non-native pronunciation: ??? ?
vs. ???? ?Escudero? or ??? vs. ???
?Federer?.
2.1 Data collection
All data were collected from Chinese Gigaword Cor-
pus using Chinese Sketch Engine with WSDiff
function, which provides side-by-side syntagmatic
comparison of Word Sketches for two different
words. WSDiff query for wi and wj returns pat-
terns that are common for both words and also pat-
terns that are particular for each of them. Three data
sets are thus provided. We neglect the common pat-
terns set and concentrate only on the wordlists spe-
cific for each word.
2.2 Pairs extraction
Transliteration pairs are extracted from the two sets,
A and B, collected with WSDiff using default set
of seed pairs :
- for each seed pair in seeds retrieve WSDiff for
and/or relation, thus have pairs of word lists,
< Ai, Bi >
- for each word wii ? Ai find best matching
counterpart(s) wij ? Bi. Comparison is done
using simple phonological rules, viz. 2.3
- use newly extracted pairs as new seeds (original
seeds are stored as good pairs and not queried
any more)
- loop until there are no new pairs
Notice that even though substantial proportion of
borrowing among different communities, there is no
mixing in the local context of collocation, which
means, local collocation could be the most reliable
way to detect language variants with known variants.
2.3 Phonological comparison
All word forms are converted from Chinese script
into a phonological representation4 during the pairs
extraction phase and then these representations are
compared and similarity scores are given to all pair
candidates.
A lot of Chinese characters have multiple pro-
nunciations and thus multiple representations are de-
rived. In case of multiple pronunciations for certain
syllable, this syllable is commpared to its counter-
part from the other set. E.g. (? has three pronunci-
ations: ye`, xie?, she`. When comparing syllables such
as ?[pei,fei] and ?[fei], ? will be represented as
[fei]. In case of pairs such as ??? [ye er qin] and
??? [ye er qin], which have syllables with multi-
ple pronunciations and this multiple representations.
However, since these two potential variants share
the first two characters (out of three), they are con-
sidered as variants without superfluous phonological
checking.
Phonological representations of whole words are
then compared by Levenstein algorithm, which is
widely used to measure the similarity between two
strings. First, each syllable is split into initial and
final components: gao:g+ao. In case of syllables
without initials like er, an ? is inserted before the
syllable, thus er:?+er.
Before we ran the Levenstein measure, we also
apply phonological corrections on each pair of can-
didate representations. Rules used for these cor-
rections are derived from phonological features of
Mandarin Chinese and extended with few rules
from observation of the data: (1) For Initials, (a):
voiced/voiceless stop contrasts are considered as
similar for initials: g:k, e.g. ? [gao] (??) vs. ?
[ke] (??),d:t, b:p, (b): r:l ? [rui] (????) ? [lie]
(????) is added to distinctive feature set based on
observation. (2). For Finals, (a): pair ei:ui is eval-
uated as equivalent.5 (b): oppositions of nasalised
final is evaluated as dissimilar.
4http://unicode.org/charts/unihan.html
5Pinyin representation of phonology of Mandarin Chinese
does not follow the phonological reality exactly: [ui] = [uei]
etc.
155
2.4 Extraction algorithm
Our algorithm will potentially exhaust the whole
corpus, i.e. find most of the named entities that oc-
cur with at least few other names entities, but only
if seeds are chosen wisely and cover different do-
mains6. However, some domains might not over-
lap at all, that is, members of those domains never
appear in the corpus in relation and/or. And con-
currence of members within some domains might be
sparser than in other, e.g. politicians tend to be men-
tioned together more often than novelists. Nature of
the corpus also plays important role. It is likely to
retrieve more and/or related names from journal-
istic style. This is one of the reasons why we chose
Chinese Gigaword Corpus for this task.
3 Experiment and evaluation
We have tested our method on the Chinese Giga-
word Second Edition corpus with 11 manually se-
lected seeds Apart from the selection of the starter
seeds, the whole process is fully automatic. For this
task we have collected data from syntagmatic rela-
tion and/or, which contains words co-occurring
frequently with our seed words. When we make a
query for peoples names, it is expected that most of
the retrieved items will also be names, perhaps also
names of locations, organizations etc.
The whole experiment took 505 iterations in
which 494 pairs were extracted.
Our complete experiment with 11 pre-selected
transliteration pairs as seed took 505 iterations to
end. The iterations identified 494 effective transliter-
ation variant pairs (i.e. those which were not among
the seeds or pairs identified by earlier iteration.) All
the 494 candidate pairs were manually evaluated 445
of them are found to be actual contrast pairs, a pre-
cision of 90.01%. In addition, the number of new
transliteration pairs yielded is 4,045%, a very pro-
ductive yield for NE discovery.
Preliminary results show that this approach is
competitive against other approaches reported in
previous studies. Performances of our algorithms is
calculated in terms of precision rate with 90.01%.
6The term domain refers to politics,music,sport, film etc.
4 Conclusion and Future work
In this paper, we have shown that it is possible to
identify NE?s without having prior knowledge of
them. We also showed that, applying WSE to re-
strict grammatical context and saliency of colloca-
tion, we are able to effectively extract transliteration
variants in a language where transliteration is not
explicitly represented. We also show that a small
set of seeds is all it needs for the proposed method
to identify hundreds of transliteration variants. This
proposed method has important applications in in-
formation retrieval and data mining in Chinese data.
In the future, we will be experimenting with a dif-
ferent set of seeds in a different domain to test the
robustness of this approach, as well as to discover
transliteration variants in our fields. We will also be
focusing on more refined phonological analysis. In
addition, we would like to explore the possibility of
extending this proposal to other language pairs.
References
Jiang, L. and M.Zhou and L.f. Chien. 2007. Named En-
tity Discovery based on Transliteration and WWW [In
Chinese]. Journal of the Chinese Information Process-
ing Society. 2007 no.1. pp.23-29.
Graff, David et al 2005. Chinese Gigaword Second Edi-
tion. Linguistic Data Consortium, Philadelphia.
Ma, Wei-Yun and Huang, Chu-Ren. 2006. Uniform and
Effective Tagging of a Heterogeneous Giga-word Cor-
pus. Presented at the 5th International Conference on
Language Resources and Evaluation (LREC2006), 24-
28 May. Genoa, Italy.
Kilgarriff, Adam et al 2004. The Sketch Engine. Pro-
ceedings of EURALEX 2004. Lorient, France.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. of the ACL Workshop on Multi-
lingual Named Entity Recognition, pp.57-64.
Wan, Stephen and Cornelia Verspoor. 1998. Auto-
matic English-Chinese Name Transliteration for De-
velopment of Multiple Resources. In Proc. of COL-
ING/ACL, pp.1352-1356.
156
Sinica Treebank: 
Design Criteria, Annotation Guidelines, and On-line Interface 
Chu-Ren Huang t, Feng-Yi Chen 2, Keh-Jiann Chen 2, Zhao-ming Gao s, & 
Kuang-Yu Chen 2 
churen(a3,sinica.edu.tw, aoole(~jis.sinica.edu.tw, kchen(~,iis.sinica.edu.tw, 
zmgao@ccms.ntu.edu.tw, sasami@iis.sinica.edu.tw 
=Institute ofLinguistics, Academia Siniea, Taipei, Taiwan 
2Institute of Information Science, Academia Sinica, Taipei, Taiwan 
3Dept. of Foreign Languages & Literatures, National Taiwan University, Taipei, Taiwan 
Abstract 
This paper describes the design 
criteria and annotation guidelines of 
Sinica Treebank. The three design 
criteria are: Maximal Resource Sharing, 
Minimal Structural Complexity, and 
Optimal Semantic Information. One of 
the important design decisions 
following these criteria is the encoding 
of thematic role information. An on-line 
interface facilitating empirical studies of 
Chinese phrase structure is also 
described. 
1. Introduction 
The Penn Treebank (Marcus et al 
1993) initiated a new paradigm in 
corpus-based research. The English. 
Penn Treebank has enabled and 
motivated corpus and computational 
linguistic research based on information 
extractable from structurally annotated 
corpora. Recently, the research has 
focused on the following two issues: 
first, when and how can a structurally 
annotated corpus of language X be 
built? 
Second, what information should or 
can be annotated? A good sample of 
issues in these two directions can be 
found in the papers collected in Abeille 
(1999). 
The construction of the Sinica 
Treebank deals with both issues. First, it 
is one of the first structurally annotated 
corpora in Mandarin Chinese. Second, 
as a design feature, the Sinica Treebank 
annotation includes thematic role 
information in addition to syntactic 
categories. In this paper, we will discuss 
the design criteria and annotation 
guidelines of the Sinica Treebank. We 
will also give a preliminary research 
result based on the Sinica Treebank. 
2. Design Cr i te r ia  
There are three important design 
criteria for the Sinica Treebank: 
maximal resource sharing, minimal 
structural complexity, and optimal 
semantic information. 
First, to achieve maximal resource 
sharing, the construction of the Sinica 
Treebank is bootstrapped from existing 
29 
Chinese computational linguistic 
resources. The textual material is 
extracted from the tagged Sinica Corpus 
(hRp:l/www.sinica.edu.tw/ftms-bin/ 
kiwi.sh, Chen et al 1996). In other 
words, the tasks and issues involving 
tokenization / word segmentation and 
category assignment are previously 
resolved. It is worth noting that the 
segmentation and tagging of Sinica 
Corpus have undergone vigorous 
post-editing. Hence the precision of 
category-assignment is much higher 
than with an automatically tagged 
corpora. In addition, since the same 
research team carried out the tagging of 
Sinica Corpus and annotation of Sinica 
Treebank, consistency of the 
interpretation of texts and tags are 
ensured. For structure-assigument, an 
automatic parser (Chen 1996) is applied 
before human post-editing. 
Second) the criterion of minimal 
structural complexity is motivated to 
ensure that the assigned structural 
information can be shared regardless of 
users' theoretical presupposition. It is 
observed that theory-internal 
motivations often require abstract 
intermediate phrasal levels (such as in 
various versions of the X-bar theory). 
Other theories may also call for an 
abstract covert phrasal category (such as 
INFL in the GB theory for Chinese). In 
either case, although the phrasal 
categories are well-motivated within the 
theory, their significance cannot be 
maintained in the context of other 
theoretical frameworks. Since a primary 
goal of annotated corpora is to serve as 
the empirical base of linguistic 
investigations, it is desirable to annotate 
structure divisions that are the most 
commonly shared among theories. We 
came to the conclusion that the minimal 
basic level structures are the ones that 
are shared by all theories. Thus our 
annotation is designed to achieve 
minimal structural complexity. All 
abstract phrasal levels are eliminated 
and only canonical phrasal categories 
are marked. 
Third) a critical issue involving 
Treebank construction as well as 
theories of NLP is how much semantic 
information, if any, should be 
incorporated. The original Penn 
Treebank took a fairly straightforward 
syntactic approach. A purely semantic 
approach, though tempting in terms of 
theoretical and practical considerations, 
has never been attempted yet. A third 
approach is to annotate partial semantic 
information, especially those pertaining 
to argument-relations. This is an 
approach shared by us and the Prague 
Dependency Treebank (e.g. Bohmova 
and Hajikova 1999). In this approach, 
the thematic relation between a predicate 
and an argument is marked in addition to 
grammatical category. Note that the 
predicate-argument relation is usually 
grammatically instantiated and generally 
considered to be the semantic relation 
that interacts most closely with syntactic 
behavior. This allows optimal semantic 
30 
information to be encoded without going 
too beyond the partially automatic 
process of argument identification. 
3. Annotation Guidelines I: 
Category and Hierarchy 
The basic structure of a tree in a 
treebank is a hierarchy of nodes with 
categorical denotation. As in any 
standard phrase structure grammar, the 
lexieal (i.e. terrninal) symbols are 
defined.by the lexicon (CKIP 1992). 
And following the recent lexicon-driven 
and information=based trends in 
linguistic theory, linguistic information 
will be projected from encoded lexical 
information. Please refer to CKIP (1993) 
for the definition of lexieal categories 
that we followed. We will give below 
the inventory of the restricted set of 
phrasal categories used and their 
interpretation. This set defines the 
domain of expressed syntactic 
information (instead of projected or 
inherited information). Readers can also 
consult Chen et al's (2000) general 
description of how the Siniea Treebank 
is constructed for a more complete list of 
tags as well as explanation i Chinese. 
3.1. Defining Phrasal Categories 
There are only 6 non-terminal 
phrasal categories annotated in the 
Sinica Treebank. 
(1) Phrasal Categories 
1. S: An S is a complete tree headed by a 
predicate (i.e. S is the start symbol). 
2.VP: A VP is a phrase headed by a 
predicate. However, it lacks a 
subject and cannot function alone. 
3. NP: An NP is beaded by an N. 
4.GP: A GP is a phrase headed by 
locational noun or locational adjunct. 
Since the thematic role is often 
determined by the governing 
predicate and not encoded locally; 
nominal phrases are given a tentative 
role of DUMMY so that it can 
inherit he correct role from the main 
predicate. 
5. PP: A PP is headed by a preposition. 
The thematic role of its argument is 
inherited from the mother, hence its 
argument is marked with a 
DUMMY. 
6. XP: A XP is a conjunctive phrase that 
is headed by a conjunction. Its 
syntactic head is the conjunction. 
However, since the actual category 
depends on the interactive 
inheritance from possibly 
non-identical conjoined elements, X 
in XP stands for an under-specified 
category. 
3.2. Defining Inheritance Relations 
Following unification-based 
grammatical theories, categorical 
assignments in Sinica Treebank are both 
lexicon-driven and head-driven. In 
principle, all grammatical information is
lexically encoded. Structurally heads 
indicate the direction of information 
inheritance and define possible 
predicate-argument relations. However, 
since the notion 'head' can have several 
31 
different linguistic definitions, we 
attempt to allow at least the discrepancy 
between syntactic and semantic heads. 
In Sinica Treebank, three different kinds 
of grammatical heads are annotated. 
(2) Heads 
1.Head: indicates a grammatical head in 
an endocentrie phrasal category. 
Unless a different semantic head is 
explicitly marked, a Head marks a 
category that serves simultaneously as
the syntactic and semantic heads of the 
construction. 
2.head: indicates a semantic head which 
does not simultaneously function as a 
syntactic head. For instance in 
constructions involving 
grammatiealized 'particles,' such as in 
the 'VP-de' construction, the 
grammatical head ('de' in this case) 
does not carry any semantic 
information. In these cases, the head 
marks the semantic head ('VP" in this 
case) to indicate the flow of content 
information. 
3. DUMMY: indicates the semantic 
head(s) whose categorical or thematic 
identity cannot be locally determined. 
The two most likely scenarios 
involving DUMMY are (a) in a 
coordination construction, where the 
head category depends on the sum of 
all conjuncts. And (b) in a non-NP 
argument phrase, such as PP, where 
the semantic head carries a thematic 
role assigned not by the immediate 
governing syntactic head ("P" in this 
case), but by a higher predicate. In 
these cases, DUMMY allows a parser 
to determine the correct categorical / 
thematic relation later, while 
maintaining identical local structures. 
3.3. Beyond Simple Inheritance 
When simple inheritance fails, the 
following principles derived from our 
design criteria serve to predict the 
structural assignments of a phrasal 
category: default inheritance, sisters only, 
and left most. 
3.3.1. Default Inheritance 
This principle deals primarily and 
most effectively with coordinations and 
conjunctions. The theoretical motivation 
of this account follows Sag et al's (1985) 
proposal. In essence, the category of a 
conjunctive construction must be 
inherited from its semantic heads. 
However, since conjunctions are not 
restricted to same categories, languages 
must have principled ways to determine 
the categorical identity when different 
semantic heads carry different 
information. 
First, in the trivial case when all 
head daughters are of the same category, 
the mother will inherit hat category. 
Second, when the different head 
daughters are an elaboration of the same 
basic category (e.g. both Nd and Ne are 
elaboration of N), then the basic 
category is the default inheritance 
category for the mother. This can be 
illustrated by (3). 
32 
(3) \[\[\[da4\]VH1 l\[er2\]Caa \[yuan2\]VH13\]\] 
VP 
big 
Third," 
mechanisms 
categorical 
and round 
when other inheritance 
fail to provide a clear 
choice, the default 
inheritance is activated. There are two 
default hierarchies. The first one deals 
with when the head daughters are all 
lexical categories (4a), and the second 
one deals with when they are all phrasal 
categories (4b). If there is a disparity 
between lexical and phrasal categories, 
then a lexical category will be expanded 
to a phrasal category first. 
(4)Default Inheritance Hierarchy for 
Categories 
a) Lexical Categories: V > N > P > Ng 
b) Phrasal Categories: S> VP> NP> 
PP> GP 
When phrasal conjuncts are involved, S 
is the privileged category since it is the 
start symbol of the grammar. VP comes 
next since its structural composition is
identical to that of S. If the structure 
involved is not a predicate (i.e. head of a 
sentence), then it must be a role. For 
argument roles, NP's are more 
privileged than PP's, and PP's are more 
privileged than GP's. (5) is an instance 
of the application of this default 
hierarchy. 
(5) \[\[da41iang4\]Neqa \[ r2\]Caa 
\[feng l sheng4\]VH11 \]V\]VP 
big-quantity and 
bountiful 
"bountiful and of big quantity" 
When lexical conjuncts are involved, the 
same principle is used. The priority is 
given to the predicate head of the 
sentence. Among possible argument 
roles, the nominal category is the default. 
An illustrative xample can be found in 
(6). 
(6) \[\[wei4lan2 de tianlkongl\]NP 
\[yu3\]Caa\[zhul qun2biao l han4\]S\]S 
aqua-blue DE sky 
and people ferocious 
'That the sky being aqua blue and 
that he people being ferocious...' 
3.3.2 Sisters Only 
Following most current linguistic 
theory, argument roles and adjunct 
complements must be sisters of a lexieal 
head. However, driven by our design 
criteria of minimal structural complexity, 
no same level iteration is allowed. Thus 
these arguments and adjuncts can be 
located by the straightforward definition 
of sisterhood: that they share the same 
mother-daughter r lation with the head. 
The result is a flat structure. 
33.3 Left First 
This principle is designed to 
account for possible internal structure 
when there are more than two sisters 
-without having to add on hierarchical 
complexity. Hence, the default 
interpretation of internal structure of 
multiple sisters is that the internal 
association starts from leR to right. 
33 
4. Annotation Guidelines II: 
Structural Annotation of 
Thematic Information 
A thematic relation contains a 
compact bundle of syntactic and 
semantic information. Although 
thematic relations are lexically encoded 
on a predicate, they can only be 
instantiated when that information is 
projected to phrasal arguments. In other 
words, the only empirical evidence for 
the existence of a thematic relation is a 
realized argument. However, a realized 
argument cannot by itself determine the 
thematic relation. The exact nature of 
the relation must be determined based 
on the lexical information fi'om the 
predicate as well as checking of the 
compatibility of that realized argument. 
Since structural information alone 
cannot determine thematic relations, 
prototypical structural annotation, such 
as in the original Penn Treebank, does 
not include thematic roles since they 
contain on-structural information. 
On the other hand, in theories 
where lexical heads drive the structural 
derivation / construction (e.g. ICG and 
HPSG and LFG), thematic relations are 
critical. Hence, we decided to encode 
realized thematic relations on each 
phrasal argument. The list of  thematic 
relations encoded on the head predicate 
is consulted whenever a phrasal 
argument is constructed, and a 
contextually appropriate relation 
sanctioned by the lexical information is 
encoded. It is worth noting that in our 
account., we not only mark the thematic 
relations of a verbal predicate, but we 
also mark the thematic relations 
governed by a deverbal noun, among 
others. Also note that an argument of a 
preposition is marked as a placeholder 
DUMMY. This is because a preposition 
only governs an argument syntactically, 
while its thematic relation is determined 
by a higher verb. 
(7) Thematic Roles: Classification and 
Inventory 
34 
THEMATIC ROLES 
I OUMMY I 
I 
I '~ '  I 
e~edeneer Ioe~on 
I ?*"~'*" t *o.v,~ 
\[ be.erect ~ *errnlemr~ 
\[ ?~mdi~m conjunction 
\[ e~eluaem negae~ 
\[ exrJudon incl~on 
\[ f l~*cy  -{ impera~ 
\[ quamiler quamiol 
\[ s~ndard 
I ~ deg~e 
I dei~$ ma.~0n 
re~.dz uncondJllon 
I hylxnl'~s: oondusion 
I wl '~u~f con~rdon 
I a'uDidanoe puq)ose 
I l- 
I I PR~?''~'?N I I I "?UN I 
o8,o~ I -L - - - t .oM,~T,o . I  I 
I 
I OtJMMY \[ 
5. Current  Status of  the Sinica 
Treebank and On- l ine 
Interface 
Following the above criteria and 
principles, we have already f inished 
Sinica Treebank 1.0. It contains 
annotations of 38,725 Chinese structural 
trees containing 239,532 words. It 
covers ubject areas that include politics, 
traveling, sports, finance, society, etc. 
This version of the Sinica Treebank will 
be released in the near future as soon as 
the licensing documents are cleared by 
the legal dep~,ent  of  Academia Sinica. 
A small subset of  it (1,000 sentences) is
already available for researchers to 
download from the website 
http ://godel.i is.sinica, edu. tw/CKIP/ 
treeslOOO.htm. A searchable interface is 
also being developed and tested for 
researchers o that they can directly 
access the complete treebank 
information. 
As an annotated corpus, one of the 
most important roles that a treebank can 
play is that it can serve as a shared 
35 
source of data for linguistic, especially 
syntactic studies. Following the example 
of the successful Sinica Corpus, we have 
developed an on-line interface for 
extraction of grammatical information 
from the Sinica Treebank. Although the 
users that we have in mind are 
theoretical linguists who do not 
necessarily have computational 
background; we hope that non-linguists 
can also benefit from the ready 
availability of such grammatical 
information. And of course, 
computational linguists should be able 
to use this interface for quick references 
before going into a more in-depth study 
of the annotated corpus. 
Currently, the beta site allows users 
specify a variety of conditions to search 
for structurally annotated sentences. 
Conditions can be specified in terms of 
keywords, grammatical tags (lexical or 
phrasal), thematic relations, or any 
boolean combination of the above 
elements. The search result can be 
presented aseither annotated structure or 
simply the example sentences. Simply 
statistics, based on either straightforward 
frequency count or mutual information, 
are also available. For linguistically 
interesting information, such as the 
heads of various phrasal constructions, a 
user can simply look up the explicitly 
syntactic Head or semantic head; as 
well as DUMMY when it serves as a 
head placeholder. The website of this 
interface, as well as the general release 
of the Sinica Treebank 1.0, is scheduled 
to be announced at the second ACL 
workshop on Chinese Language 
Processing inOctober 2000. 
6. Conclusion 
The construction of the Sinica 
Treebank is only a first step towards 
application of structurally annotated 
corpora. Continuing expansion and 
correction will make this database an 
invaluable resource for linguistic and 
computational studies of Chinese. 
References 
I.ABEILI..E, Anne. 1999. Ed. 
Proceedings ofATALA Workshop - 
Treebanks. Paris, June 18-19, 1999. 
Univ. de Paris VII. 
2.BOHMOVA, Alla and Eva Hajicova. 
1999. How Much of the Underlying 
Syntactic Structure Can be Tagged 
Automatically? In Abeille (Ed). 
1999.31-40. 
3.CHEN, Feng-Yi, Pi-Fang Tsai, 
Keh-Jiann Chen, and Chu-Ren Huang. 
2000. Sinica Treebank. \[in Chinese\] 
Computational Linguistics and 
Chinese Language Processing. 
4.2.87-103. 
4.CHEN, Keh-Jiarm. 1996. A Model for 
Robust Chinese Parser. Computational 
Linguistics and Chinese Language 
Processing. 1.1.183-204. 
5.CHEN, Keh-Jiann, Chu-Ren Huang. 
1996. Information-based Case 
Grammar: A Unification-based 
Formalism for Parsing Chinese. In 
Journal of Chinese Linguistics 
Monograph Series No. 9. Chu-Ren 
Huang, Keh-Jiaun Chen, and 
Benjamin K. T'sou Eds. Readings in 
Chinese Natural Language Processing. 
23-45. Berkeley: JCL. 
6.CHEN, Keh-Jiann, Chu-Ren Huang, 
Li-Ping Chang, Hui-Li Hsu. 1996. 
36 
Sinica Corpus: Design Methodology 
for Balanced Corpora. Proceedings of 
the 11th Pacific Asia Conference on 
Language, Information, and 
Computation (PA CLIC I1). Seoul 
Korea. 167-176. 
7.CHEN, Keh-Jiann and Shing-Huan 
Liu. 1992. Word Identification for 
Mandarin Chinese Sentences. 
Proceedings of COLING-92.101 - 105. 
8.CHEN, Keh- Jiann, Shing-Huan Liu, 
Li-Ping Chang, Yeh-Hao Chin. 1994. 
A Practical Tagger for Chinese 
Corpora." Proceedings of R OCLING 
V/I. 111-126. 
9.CHEN, Keh-Jiann, Chi-Ching Luo, 
Zhao-Ming Gao, Ming-Chung Chang, 
Feng-Yi Chen, and Chao-Ran Chert. 
1999. The CKIP Chinese Treebank: 
Guidelines for Annotation. In Abeille 
(Ed). 1999.85-96. 
I0. CKIP (Chinese Knowledge 
Information Processing). 1993. The 
Categorical Analysis of Chinese. CKIP 
Technical Report 93-05. Nankang: 
Academia Sinica. 
11. HUANG, Chu-Ren, Keh-Jiann 
Chen, Feng-Yi Chen, and Li-Li Chang. 
1997. Segmentation Standard for 
Chinese Natural Language Processing. 
Computational Linguistics and 
Chinese Language Processing. 
2.2.47-62 
12. Lin, Fu-Wen. 1992. Some 
Reflections on the Thematic System of 
Information-based Case Grammar 
(ICG). CKIP Technical Report 92-01. 
Nankang: Academia Sinica. 
13. Marcus, Miteh P., Beatrice 
Santorini, and M. A. Marcinkiewiicz. 
1993. Building a Large Annotated 
Corpus of English: The Peen Treebank. 
Computational Linguistics. 
19.2.313-330. 
14. SAG, Ivan, Gerald Gazdar, Thomas 
Wasow, and Steven Weisler. 1985. 
Coordination and How to Distinguish 
Categories. Natural Language and 
Linguistic Theories. 117-171. 
Appendix 
1. Lexical Categories 
(1) NON-PREDCITIVE ADJVECTIVE: A 
(2) CONJUNCTION: C 
(3) ADVERB: D 
(4) INTERJECTION: I 
(5) NOUN: N 
(6) DETERMINATIVES: Ne 
(7) MEASURE WORD / CLASSIFIER: 
Nf 
(8) POSTPOSITION WORD: Ng 
(9) PRONOUN: Nh 
(10) PREPOSITION: P 
(11) PARTICLES: T 
(12) VERB: V 
2. Sample Sentence and Tree 
nage wanfi de nyuren baifa zhihou 
bian buzai lihui 
? that hair-style DE woman white-hair after 
then never pay-attention 
ting qian tingting .FuR de 
qingcao 
courtyard front slender-ly standing-erect DE 
green-grass 
'After her hair had turned white, that 
coiffured woman ever paid any more 
attention tothe nicely standing green grass 
in the front courtyard." 
S(agent:NP(quantifier:DM:~l 
property:VP- ~j(head:VP(Head:VA4:~-) 
IHead:DE: ~)lHead:Nab:~A.)ltime:GP 
(DUMMY:VP(Head:VI-I 11:~1 ~)1 Head: 
Ng:-~-~.)\[firne:Dd:~l~ I time:Dd: ~"~'1 Head: 
VC2:J~ ~'\[goal:NP (property:VP ? ~(head: 
VP (location:NP(property:Neb:/~.l 
Head:Neda: ~,f)lHead:VH11: ;~ ,~ ~ 2Y_)\[ 
Head:DE: ~)I  Head:Nab: ff ~)) 
37 
Induction of Classification from Lexicon Expansion : 
Assigning Domain Tags to WordNet Entries 
 
Echa Chang*, Chu-Ren Huang**, Sue-Jin Ker***, Chang-Hua Yang*** 
*University of Waterloo, 200 University Ave. W., Waterloo, ON  N2L 3G1 Canada 
cecha@yahoo.com 
**Institute of Linguistics, Academia Sinica, Nankang, Taipei, 115, Taiwan 
churen@sinica.edu.tw 
***Soochow University 
ksj@sun.cis.scu.edu.tw, changhua@mail2000.com.tw 
 
Abstract 
We present in this paper a series of 
induced methods to assign domain tags to 
WordNet entries. Our prime objective is 
to enrich the contextual information in 
WordNet specific to each synset entry. By 
using the available lexical sources such as 
Far East Dictionary and the contextual 
information in WordNet itself, we can 
find a foundation upon which we can base 
our categorization. Next we further 
examine the similarity between common 
lexical taxonomy and the semantic 
hierarchy of WordNet. Based on this 
observation and the knowledge of other 
semantic relations we enlarge the 
coverage of our findings in a systematic 
way. Evaluation of the results shows that 
we achieved reasonable and satisfactory 
accuracy. We propose this as the first step 
of wordnet expansion into a bona fide 
semantic network linked to real-world 
knowledge. 
 
0. Introduction1 
WordNet is a lexicon comprising of nouns, 
verbs, adjectives and adverbs. Its basic 
                                                   
1
 This research is partially funded by  an IDLP project grant 
from the National Science Council of Taiwan, ROC. Work 
reported in this paper was carried out in summer 2001, 
during Chang's internship at Academia Sinica. We are 
indebted to two anonymous reviewers of SemaNet 2002, as 
well as from the First International WordNet Conference for 
their helpful comments. An earlier version of this paper was 
accepted by the first IWC but was not presented because of 
the authors' travelling difficulties at that time. We thank 
colleagues at Academia Sinica, especially Shu-Chuan Tseng, 
Keh-jiann Chen, and members of the WordNet group, for 
their input and help. 
organization is based on different semantic 
relations among the words.  Entries (or lemas) 
sharing the same meaning is grouped into a synset 
and assigned with a unique sense identification 
number for easy retrieval and tracking purposes.  
This unique offset number gives the information 
about the parts of speech and the hierarchy 
position to which a specific synset belongs.  For 
nouns and verbs the synsets are grouped into 
multiple lexical hierarchies; modifiers such as 
adjectives and adverbs are simply ?organized into 
clusters on the basis of binary opposition 
(antinomy).? [1]  This lexical hierarchy makes the 
lexical domain assigning task more 
straightforward because it coincides with a 
ontological taxonomy in many aspects.  The 
primary objective of our project is to enrich the 
WordNet knowledge content due to the fact that 
?WordNet lacks relations between related 
concepts.? [2] We adopt WordNet itself, together 
with other lexical resources to develop an 
integrated domain specific lexical resource. 
 
1. The Five Tagging Methods 
Starting with two lexical resources, we 
employed five steps to assign and expand domain 
tags. Basically, the explicit domain information 
from Far East Dictionary as well as WordNet's 
own hierarchy of semantic relation are used to 
extend the coverage of domain - assignment. 
 
1.1 Domain Data Lookup from Far East 
Dictionary 
The digital file of Far East Dictionary 
contains complete information for each word 
entry that can be found in an ordinary printed 
version. Most of all, it lists the domain 
information for each vocabulary wherever 
possible.  Thus we employ the available data from 
a text source file (each vocabulary entry is 
organized as one single row) and extract all the 
information by running a string manipulation 
program coded in Visual Basic.  During the 
extraction process we only take into account the 
part of speech of each word in Far East Dictionary. 
Next, we map the domains obtained from Far East 
Dictionary if the word and its part of speech 
coincides with the entries in our database which 
contains a complete list of synset.  Since  
WordNet collects only nouns, verbs, adjectives 
and adverbs, we only extract the domain data that 
falls into these four categories.  Later we group 
the information in a database table and extent the 
assigned domains of each word to its synset.  
Table 1 is an example of our database table which 
'contains all the adverbial uses of `aback.' 
 
id term domain 
00073303R aback aviation, 
00073386R aback aviation, 
Table 1 Example of The Far East Dictionary 
Domain Database Table 
 
In Table 1, it is shown that 'aback' has two 
adverbial senses. Since in Far East Dictionary 
?aback? is labeled with domain 'aviation,' extra 
work of expansion is necessary to further label all 
of its adverb synset with the same domain to 
maintain the integrity of the information. Because 
both the extraction and expansion method would 
produce ambiguities in domain assignment, 
manual verifications are required in the future. 
 
1.2 Extracting Domain Information from 
WordNet Sense Description 
Each WordNet entry (i.e. each synset) is 
followed by its sense. Although there is no 
specifically defined set of controlled vocabulary, 
the sense definition does specify the field the 
synset members are commonly used in that 
specific field of study, such as biology, physics or 
chemistry. This specification comes in a special 
format contained in a bracket for each WordNet 
entry so that extraction of data is possible and 
straightforward.  Due to the fact that each domain 
is directly extracted by its corresponding synset, 
there is simply no ambiguity in assigning the 
domain tags.  And if there is more than one lexical 
item in that synset, all will share the same domain 
tag. 
 
1.3 Establishment of a Common Domain 
Taxonomy for Nouns  
Each lexical resource uses a different domain 
taxonomy, which may be explicitly defined or 
implicitly assumed. Hence, when combining 
domain information from multiple sources, the 
establishment of a Common Domain Taxonomy 
(CDT) is crucial for both efficient representation 
as well as effective knowledge merging. Our 
survey of existing domain taxonomy, including 
LDOCE, HowNet, Tongyici Cilin, etc., show that 
there is quite a lot in common. Hence we decide to 
build a working CDT based on the two resources 
we have. Note that since our goal is to establish a 
domain taxonomy for wordnets (for English now 
and for Chinese in the future), the existing domain 
information in WordNet need to be assumed as 
defaults that can be over-ridden. Hence a model of 
CDT based on basic binary combination 
involving WordNet is necessary. 
After collecting all the domain tags from the 
two resources, we build our CDT. First, all 
common domain nodes are put in a hierarchy 
based on their relation. Second, inconsistent 
domain names are resolved. Last, when gaps 
appear after all domain tags are attached to the 
taxonomy, new domain categories are adopted to 
fill in the gaps and make a more complete CDT. 
Since top taxonomy presupposes a particular view 
on conceptual primacy and may differ in different 
lexical sources, we took a bottom-up approach to 
our CDT. That is, right now each taxonomy tree 
now stops at some broad-consensus level without 
being committed to a higher taxonomy. The 
following is a partial list of our current CDT. 
 
Humanity 
 
Linguistics   
 
Rhetorical Device   
 
Literature   
 
History   
 
Archeology   
? 
Social Science     
 
Sociology   
 
Statistics   
 
Economics   
 
Business   
 
Finance     
? 
Formal Science   
 
Mathematics   
 Geometry   
 
Algebra   
? 
Natural Science   
 
Physics   
 
Nuclear   
 
Chemistry   
 
Biology   
 
Palaeontology   
 
Botany   
 
Animal   
 
Fish   
 
Bird   
 ? 
Applied Science 
 
Medicine   
 
Anatomy   
 
Physiology   
 
Genetics   
 
Pharmacy   
 
Agriculture   
 ? 
Fine Arts 
 
Painting   
 
Sculpture   
 
Architecture   
 
Music   
 
Drama   
 ? 
Entertainment 
 
Sports   
 
Balls   
 
Track & Field   
 
Competition   
 
Game   
 
Board   
 
Card   
? 
Proper Noun 
 
Name   
 
Geographical Name   
 
Country   
 
Religion   
 
Trademark   
 ? 
Humanity 
 
Archaic   
 
Informal   
 
Slang   
 
Metaphor   
 
Formal   
 
Abbreviate   
 ? 
Lexical Sources 
 
Latin   
 
Greece   
 
Spanish   
 
French   
 
American   
 ? 
Please note that by induction and actual 
examples from the lexical organization in 
WordNet, it is found that a hyponym is very likely 
to belong to the same domain as its hypernym. 
Similar results are also found for wordnet based 
cross-lingual inference of lexical semantic 
relations [4]. For instance, under the term 
'mathematics,' all the hyponyms below are related 
to this field of study. To make us of this lexical 
semantic phenomenon, we make a table of all the 
domain terms and map them to their unique 
WordNet sense identification number.  Later we 
use the tree expansion method (discussed in more 
detail in Section 2.4) to trace down all the 
hyponyms.  For example, by using this method, 
the hyponyms of  Linguistics  are all labeled as 
'linguistics' and so forth. 
 
1.4 Lexical Hierarchy Expansion of 
Nominal Domain Assignment 
WordNet has is a lexical semantic hierarchy 
linking all synsets with lexical semantic relations. 
We convert all the relations to a database in a 
relational table, as shown in Table 2 [1]: 
 
Hypernym ID Hyponym ID Relation 
00001740A 04349777N = 
00001740A 00002062A ! 
00001740N 00002086N ~ 
? ? ? 
Table 2 Lexical Relation Table 
The relation symbols in Table 2 are adopted from 
the WordNet database files. These symbols are 
saved with each synset entry to indicate a specific 
semantic relation with other synsets. The 
implemented information allows us to trace and 
locate all the related synsets. 
 
 
 
 
WN Relation Symbol 
 
Antonym: ! 
Hyponym: ~ 
Hypernym: @ 
Meronym: # 
Holonym: % 
Attribute: = 
 
Table 3 Relations and Pointer Symbols 
 
  By manipulating Table 2 with SQL, all 
nouns can be traced to the eight unique beginners.  
 
Unique Beginners of Nouns In WordNet 
Entity,something 
Abstraction 
Act,human action,human activity 
State 
Event 
Group,grouping 
Phenomenon 
Possession 
Table 4. The Eight Unique Beginners for Nouns 
 
The general structure of tree expansion can be 
visualized as Figure1: 
 
 
1st 
 
 
 
 
? 
Figure 1. Example of Tree Expansion for Nouns 
 
This form of data presentation makes inspection 
and observation on the hierarchy among nouns 
more straightforward. After careful and 
systematic examination, domain assignment is 
trickled down to each synset level by level. The 
same task is performed up to the fifth level. A tree 
traversal program is executed to trace down the 
hyponyms and assign domain-tag based on its 
hypernyms. 
 
1.5  Relational Expansion of Other Parts 
of Speech 
The hierarchy expansion method based on 
taxonomy mainly applies to nouns.  For modifiers 
such as adjectives and adverbs this general 
observation does not produce a satisfactory result  
since ?[t]he semantic organization of modifiers in 
WordNet is unlike?the tree structures created by 
hyponymy for nouns or troponymy for verbs.? [1] 
However since adverbs/adjectives are often 
morphologically derived from other major 
categories, such information can be used to infer 
domain classification. For example, the adjective 
'stellar' is derived directly from the noun 'star.' 
The term, 'star' is mostly mentioned in an 
astronomical context.  Based on this relation, 
since 'star' is labeled with 'astronomy' based on 
Lexical Hierarchy, the adjective 'stellar' can be 
assigned with the same domain.   We combine  the 
tables on the left side and right side of Table 2 
Lexical Relation Table to obtain a table organized 
as follows: 
 
 
 
 
 
 
Figure 2. JOIN Method 
 
Later the recordsets that have the relation symbol 
as ?\?(denoted ?derived from,? refer to Table 2) 
are extracted and these derived adjectives and 
adverbs are further assigned with the same 
domain as the nouns they are derived from. 
 
Results 
There are 99,642 unique senses organized by 
WordNet. By expanding each specific vocabulary 
coupling with its specific senses, the number of 
these ?word & sense? unique pairs total up to  
173,941, which is the basis for all the results. 
 
Parts of Speech Percentage in Total 
Noun 66.87 % 
Adjective 17.18 % 
Verb 12.69 % 
Adverb     3.27 % 
Table 5. Percentage of Each Part of Speech in      
The 173,941 ?Word & Senses Pairs? Entries 
 
1.6 Far East Dictionary 
There are 20,126 senses that have been assigned 
with a domain tag with Far East Dictionary, which 
account for 20.20 % of the total senses (99,642 in 
total in WordNet).  However after expanding it to 
its synset the total 'word & sense' pairs, there are 
42,643 entries being tagged, which account for 
24.52 % of the 173,941 pairs in total. 
 
Entity 
 
cell 
object unit 
 
Domain 
Tagged 
Noun ID 
morpho-lo
gical 
Relation 
Un-tagged 
Adj/Adv 
ID 
2nd 
Parts of Speech Number Tagged Synset Coverage 
Noun 29,946 17.22 % 
Adjective   6,188  3.56 % 
Verb   6,160  3.54 % 
Adverb     349  0.20 % 
Table 6. Coverage by POS 
 
 	
Translating Lexical Semantic Relations: 
The First Step Towards Multilingual Wordnets* 
 
Chu-Ren Huang, I-Ju E. Tseng, Dylan B.S. Tsai 
Institute of Linguistics, Preparatory Office, Academia Sinica 
128 Sec.2 Academy Rd., Nangkang, Taipei, 115, Taiwan, R.O.C. 
churen@gate.sinica.edu.tw, {elanna, dylan}@hp.iis.sinica.edu.tw 
 
                                                 
*
 An earlier version of this paper was presented at the Third Chinese Lexical Semantics Workshop at Academia Sinica in 
May 2002. We are indebted to the participants as well as colleagues at CKIP for their comments. We would also like to thank 
the SemaNet 2002 reviewers for their helpful comments. It is our own responsibilities that, due to the short revision time, we 
were not able to incorporate all their suggestions, especially comparative studies with some relative GWA papers. We are 
also responsible for all remaining errors 
 
Abstract 
Establishing correspondences between 
wordnets of different languages is essential 
to both multilingual knowledge processing 
and for bootstrapping wordnets of 
low-density languages. We claim that such 
correspondences must be based on lexical 
semantic relations, rather than top ontology 
or word translations. In particular, we define 
a translation equivalence relation as a 
bilingual lexical semantic relation. Such 
relations can then be part of a logical 
entailment predicting whether source 
language semantic relations will hold in a 
target language or not. Our claim is tested 
with a study of 210 Chinese lexical lemmas 
and their possible semantic relations links 
bootstrapped from the Princeton WordNet. 
The results show that lexical semantic 
relation translations are indeed highly precise 
when they are logically inferable. 
 
1. Introduction 
A semantic network is critical to knowledge 
processing, including all NLP and Semantic Web 
applications. The construction of semantic 
networks, however, is notoriously difficult for 
?small? (or ?low-density?) languages. For these 
languages, the poverty of language resources, 
and the lack of prospect of material gains for 
infrastructure work conspire to create a vicious 
circle. This means that the construction of a 
semantic network for any small language must 
start from scratch and faces inhibitive financial 
and linguistic challenges. 
In addition, semantic networks serve as 
reliable ontolog(ies) for knowledge processing 
only if their conceptual bases are valid and 
logically inferable across different languages. 
Take wordnets (Fellbaum 1998), the de facto 
standard for linguistic ontology, for example. 
Wordnets express ontology via a network of 
words linked by lexical semantic relations. Since 
these words are by definition the lexicon of each 
language, the wordnet design feature ensures 
versatility in faithfully and comprehensively 
representing the semantic content of each 
language. Hence, on one hand, these conceptual 
atoms reflect linguistic idiosyncrasies; on the 
other hand, the lexical semantic relations (LSR?s) 
receive universal interpretation across different 
languages. For example, the definition of 
relations such as synonymy or hypernymy is 
universal. The universality of the LSR?s is the 
foundation that allows wordnet to serve as a 
potential common semantic network 
representation for all languages. The premise is 
tacit in Princeton WordNet (WN), EuroWordNet 
(EWN, Vossen 1998), and MultiWordNet (MWN, 
Pianta et al 2002). It is also spelled out explicitly 
in the adaptation of LSR tests for Chinese 
(Huang et al 2001).  
Given that LSR?s are semantic primitives 
applicable to all language wordnets, and that the 
solution to the low-density problem in building 
language wordnets must involve bootstrapping 
from another language, LSR?s seem to be the 
natural units for such bootstrapping operations. 
The rich and structured semantic information 
described in WN and EWN can be transported 
through accurate translation if the conceptual 
relations defined by LSRs remain constant in 
both languages. In practice, such an application 
would also serve the dual purpose of creating a 
bilingual wordnet in the process.  
In this paper, we will examine the validity 
of cross-lingual LSR inferences by bootstrapping 
a Chinese Wordnet with WN. In practice, this 
small-scale experiment shows how a wordnet for 
a low-density language can be built through 
bootstrapping from an available wordnet. In 
theoretical terms, we explore the logical 
conditions for the cross-lingual inference of 
LSR's. 
 
2. Translation Equivalents and Semantic 
Relations 
 Note that two translation equivalents (TE) 
in a pair of languages stand in a lexical semantic 
relation. The most desirable scenario is that when 
the two TE?s are synonymous, such as the 
English ?apple? to the Mandarin ?ping2guo3?. 
However, since the conceptual space is not 
segmented identically for all languages, TE?s 
may often stand in other relations to each other. 
For instance, the Mandarin ?zuo1zhi5? is a 
hypernym for both the English ?desk? and ?table?. 
Suppose we postulate that the LSR?s between 
TE?s are exactly identical in nature to the 
monolingual LSR?s described in wordnets. This 
means that the lexical semantic relation 
introduced by translation can be combined with 
monolingual LRS?s. Predicting LSR?s in a target 
language based on source language data become 
a simple logical operation of combining 
relational functions when the LSR of translation 
equivalency is defined. This framework is 
illustrated in Diagram 1.  
 
CW2      ii  

2 
 
     y           x 
 
CW1     i  

1 
 
 x = EW1 - EW2  LSR 
 y = CW1- CW2  LSR 
 i = CW1 - EW1  Translation LSR 
 ii = CW2 - EW2  Translation LSR 
The unknown LSR y = i + x + ii 
Diagram 1. Translation-mediated LSR Prediction 
(The complete model) 
 
CW1 represents our starting Chinese lemma 
which can be linked to EW1 through the 
translation LSR i. The linked EW1 can than 
provide a set of LSR predictions based on the 
English WN. Assume that we take the LSR x, 
which is linked to EW2. That LSR prediction is 
mapped back to Chinese when EW2 is translated 
to CW2 with a translation LSR ii. In this model, 
the relation y, between CW1 and CW2 is a 
functional combination of the three LSR?s i, x, 
and ii. 
However, it is well known that language 
translation involves more than semantic 
correspondences. Social and cultural factors also 
play a role in (human) choices of translation 
equivalents. It is not the aim of this paper to 
predict when or how these semantically 
non-identical translations arise. The aim is to see 
how much lexical semantic information is 
inferable across different languages, regardless of 
translational idiosyncrasies. In this model, the 
prediction relies crucially on the semantic 
information provided by the source language (e.g. 
English) lexical entry as well as the lexical 
semantic correspondence of a target language 
(e.g. Chinese) entry. The translation relations of 
the relational target pairs, although capable of 
introducing more idiosyncrasies, are not directly 
involved in the prediction. Hence we make the 
generalization that any discrepancy introduced at 
this level does not affect the logical relation of 
LSR prediction and adopt a working model 
described in Diagram 2. We only take into 
consideration those cases where the translation 
LSR ii is exactly equivalent, i.e., EW2 = CW2. 
This step also allows us to reduce the maximal 
number of LSR combination in each prediction 
to two. Thus we are able to better predict the 
contribution of each mono- or bi-lingual LSR. 
 
    

2 = CW2 (ii = 0) 
 
          y       x 
 

   i    1 
 The unknown LSR y = i + x 
Diagram 2. Translation-mediated LSR Prediction 
(Reduced Model, currently adopted) 
 
2.1 LRS Inference as Relational Combination 
 With the semantic contribution of the 
translation equivalency defined as a (bilingual) 
LSR, the inference of LSR in the target language 
wordnet is a simple combination of semantic 
relations. The default and ideal situation is where 
the two TE?s are synonymous.  
 
 
 
 
 
2 = EW2 
 
          y   x 
 
 i   
CW1 = EW1 (i = 0) 
 The unknown LSR y = x 
Diagram 3. Translation-mediated LSR Prediction 
(when TE?s are synonymous) 
 
In this case, the translation LSR is an identical 
relation; the LSR of the source language wordnet 
can be directly inherited. This is illustrated in 
Diagram 3. 
 However, when the translation has a 
non-identical semantic relation, such as 
antonyms and hypernyms, then the LSR 
predicted is the combination of the bilingual 
relation and the monolingual relation. In this 
paper, we will concentrate on Hypernyms and 
Hyponyms. The choice is made because these 
two LSR?s are transitive relations by definition 
and allows clear logical predications when 
combined. The same, with some qualifications, 
may apply to the Holonym relations. 
Combinations of other LSR?s may not yield clear 
logical entailments. The scenarios involving 
Hyponymy and Hypernymy will be discussed in 
section 3.3. 
 
3. Cross-lingual LSR Inference: A Study 
based on English-Chinese Correspondences 
 In this study, we start with a WN-based 
English-Chinese Translation Equivalents 
Database (TEDB)1. Each translation equivalents 
pair was based on a WN synset. For quality 
control, we mark each TE pair for its accuracy as 
well as the translation semantic relation. 
 For this study, the 200 most frequently used 
Chinese words plus 10 adjectives are chosen 
(since there is no adjective among the top 200 
words in Mandarin). Among the 210 input 
lemmas, 179 lemmas2 find translation 
equivalents in the TEDB and are mapped to 497 
                                                 
1
 The translation equivalence database was hand-crafted by 
the CKIP WordNet team. For each of the 99642 English 
synset head words, three appropriate translation equivalents 
were chosen whenever possible. At the time when this study 
was carried out, 42606 TE?s were proofed and available 
2
 The input lemmas for which TE?s were unable to find are 
demonstratives or pronouns for nouns, and aspect markers 
for adverbs 
English synsets. The occurring distribution is as 
follows: 84 N?s with 195 times; 41 V?s with 161 
times; 10 Adj?s with 47 times; and 47 Adv?s with 
94 times. 441 distinct English synsets are 
covered under this process, since some of the 
TE?s are for the same synset. This means that 
each input Chinese lemma linked to 2.4 English 
synsets in average. Based on the TEDB and 
English WN, the 179 mapped input Chinese 
lemmas expanded to 597 synonyms. And 
extending from the 441 English synsets, there are 
1056 semantically related synsets in WN, which 
yields 1743 Chinese words with our TEDB.   
 
3.1. Evaluation of the Semantics of Translation 
Six evaluative tags are assigned for the 
TEDB. Four of them are remarks for future 
processing. The LSR marked are 
 
Synonymous: TE?s that are semantically 
equivalent. 
 Other Relation: TE?s that hold other 
semantic relations 
 
The result of evaluation of TE?s involving the 
210 chosen lemma are given in Table 1. 
 
 
 Syn. Incorrect Other Relation Total 
148 32 15 195 
N 
75.90% 16.41% 7.69% 100% 
113 29 19 161 
V 
70.18% 18.01% 11.8% 100% 
39 8 0 47 
Adj 
82.98% 17.02% 0% 100% 
83 8 3 94 
Adv 
88.3% 8.51% 3.19% 100% 
382 78 36 496 
Total 
77.02% 15.73% 7.26% 100% 
Table 1. Input Lemmas (Total subject =496) 
 
Illustrative examples of our evaluation are given 
below: 
 
1a) Synonymous: 	

	 

 qi4ye4 (N) // enterprise: 
an organization created for business ventures 
1b) Incorrect: 


 biao3shi4 (V) // ?extend?, 
?offer?: make available; provide 
1c) Other Relation: 


 shi4chang3 (N) // 
?market, securities_industry?: the securities 
markets in the aggregate 
 
Table 2 indicates the relations between the 
synonyms of an input lemma and the same 
English synset. Recall that our TEDB gives more 
than one Chinese translation equivalent to one 
English WN entry. Hence we can hypothesize 
that the set of Chinese translation equivalents 
form a synset. It is natural, then, to examine the 
semantic relations between other synset members 
and the original WN entry. Table 1 and 2 show a 
rather marked difference in terms of the 
correctness of the synonymy relation. This will 
be further explained later. 
 
 Syn. Incor. 
Other 
Rel. Others Total 
114 51 25 19 209 N 
54.5% 24.4% 11.0% 9.1% 100% 
104 46 18 14 182 V 
57.1% 25.3% 9.99% 7.7% 100% 
37 8 2 10 57 Adj 
64.9% 14.0% 3.5% 17.5% 100% 
119 20 4 6 149 Adv 
79.9% 13.4% 2.7% 4.0% 100% 
374 125 49 49 597 Total 
62.6% 20.9% 8.2% 8.2% 100% 
Table 2. Synonyms of Input Lemma  
(Total Subject=597) 
 
From the data above, we observe two 
generalizations: First, polysemous lemmas have 
lower possibility of being synonymous to the 
corresponding English synset. In addition, we 
also observe that there is a tendency for some 
groups, i.e., groups with polysemy and with 
abstract meanings, to match synonymous English 
synsets. These findings are helpful in our further 
studies when constructing CWN, as well as in the 
application of TEDB. 
 
3.2 Cross-lingual LSR predictions with 
synonymous translations 
The next step is to take the set of English 
LSR?s stipulated on a WN synset and transport 
them to its Chinese translation equivalents. We 
evaluated the validity of the inferred semantic 
relations in Chinese. In this study, we 
concentrated on three better-defined (and more 
frequently used) semantic relations: antonyms 
(ANT); hypernyms (HYP); and hyponyms 
(HPO). Here, we limit our examination to the 
Chinese lemmas that are both translation 
equivalents of an English WN entry and are 
considered to have synonymous semantic 
relations to that entry. The nominal and verbal 
statistics are given in Table 3 and Table 4 
respectively. 
 
 Syn. Incor. 
Other 
Rel. Others Total 
7 3 0 2 12 ANT 
58.3% 25% 0% 16.7% 100% 
117 33 15 20 185 HYP 
63.2% 17.8% 8.1% 10.8% 100% 
284 119 66 256 725 HPO 
39.2% 16.4% 9.1% 35.3% 100% 
408 155 81 278 922 Total 
44.3% 16.8% 8.8% 30.2% 100% 
Table 3. Nouns (Total Number of Inferable 
Semantic Relations=922) 
 
 Syn. Incor. 
Other 
Rel. Others Total 
8 6 0 9 23 ANT 
34.8% 26.1% 0% 39.1% 100% 
61 18 6 2 87 HYP 
70.1% 20.7% 6.9% 2.3% 100% 
118 81 19 74 292 HPO 
40.4% 27.7% 6.5% 25.3% 100% 
187 105 25 85 402 Total 
46.5% 26.1% 6.2% 21.1% 100% 
Table 4. Verbs (Total Number of Inferable 
Semantic Relations=402) 
 
 From the 148 nouns where the English and 
Chinese translation equivalents are also 
synonymous, there are 357 pairs of semantic 
relations that are marked in English WN and are 
therefore candidates for inferred relations in 
Chinese. On average, each nominal RC 
translation equivalent yields 2.41 inferable 
semantic relations. The precision of the inferred 
semantic relation is tabulated below.  
 
 Correct Others Total 
ANT 8 100% 0 0% 8 100% 
HYP 70 79.5% 18 20.5% 88 100% 
HPO 238 91.2% 23 8.8% 261 100% 
Total 316 88.5% 41 11.5% 357 100% 
Table 5. Precision of English-to-Chinese SR 
Inference (Nouns) 
 
The study here shows that when no additional 
relational distance is introduced by translation 
(i.e. the 75.9% of nominal cases when TE?s are 
synonyms), up to 90% precision can be achieved 
for bilingual LSR inference. And among the 
semantic relations examined, antonymous 
relations are the most reliable when 
transportabled cross-linguistically. 
For the 112 verbs where the English and 
Chinese TE?s are synonymous, there are 155 
pairs of semantic relations that are marked in 
WN and are therefore candidates for inferred 
relations in Chinese. In contrast to nominal 
translation equivalents, each pair of verbal TE 
only yields 1.38 inferable semantic relations. The 
precision of the inferred semantic relation is 
tabulated in Table 6. 
 
 Correct Incorrect Total 
ANT 14 100% 0 0% 14 100% 
HYP 35 70% 15 30% 50 100% 
HPO 75 82.4% 16 17.6% 91 100% 
Total 124 80% 31 20% 155 100% 
Table 6. Precision of English-to-Chinese SR 
Inference (Verbs) 
 
Similar to the results of nouns, antonymous 
relations appear reliable in the behaviors of verbs 
as well. As to the other types of relations, the 
correct rates seem to be slightly lower than nouns. 
The precision for English-to-Chinese semantic 
relation inference is 80% for verbs.  
The observed discrepancy in terms of 
semantic relations inference between nouns and 
verbs deserves in-depth examination. Firstly, the 
precision of nominal inference is 8.52% higher 
than verbal inference. Secondly, the contrast may 
not be attributed to a specific semantic relation. 
Both nouns and verbs have the same precision 
pattern for the three semantic relations that we 
studied. Inference of antonymous relations is 
highly reliable in both categories (both 100%). 
Hyponymous inference is second, and about 12% 
higher than hypernymous inference in each 
category (the difference is 11.64% for nouns and 
12.42% for verbs). And, last but not least, the 
precision gaps between nouns and verbs, when 
applicable, are similar for different semantic 
relations (9.55% for hypernyms and 8.77% for 
hyponyms). All the above facts support the 
generalization that nominal semantic relations 
are more reliably inferred cross-linguistically 
than verbal semantic relations. A plausible 
explanation of this generalization is the 
difference in mutability of nominal and verbal 
meanings, as reported by Ahrens (1999). Ahrens 
demonstrated with off-line experiments that verb 
meanings are more mutable than noun meanings. 
She also reported that verb meanings have the 
tendency to change under coercive contexts. We 
may assume that making the cross-lingual 
transfer is a coercive context in terms of meaning 
identification. Taking the mutability into account, 
we can predict that since verb meanings are more 
likely than nouns to change under given coercive 
conditions, the changes will affect their semantic 
relations. Hence the precision for semantic 
relations inference is lower for verbs than for 
nouns. 
In the above discussion, we observed that 
the three semantic relations seem to offer clear 
generalizations with regard to the precision of the 
inferences, as shown in Table 7. 
 
 Correct Incorrect Total 
ANT 22 100% 0 0% 22 100% 
HYP 105 76.1% 33 13.9% 138 100% 
HPO 313 88.9% 39 11.1% 352 100% 
Total 440 85.9% 72 14.1% 512 100% 
Table 7. Combined Precision of 
English-to-Chinese SR Inference (Nouns+Verbs) 
 
Two generalizations emerge from the above data 
and call for explanation: First, inference of 
antonymous relations is highly reliable; second, 
inference of hypernymous relations is more 
reliable than inference of hyponymous relations. 
 The fact that inference of antonymous 
relations is highly precise may be due to either of 
the following facts. Since the number of 
antonymic relations encoded is relatively few 
(only 22 all together), they may all be the most 
prototypical case. In addition, a pair of antonyms 
by definition differs in only one semantic feature 
and has the shortest semantic distance between 
them. In other words, an antonym (of any word) 
is simply a privileged (near) synonym whose 
meaning offers contrast at one particular 
semantic dimension. Since antonymy 
presupposes synonymous relations, it preserves 
the premise of our current semantic relation 
inference.  
 The fact that hyponymous relations can be 
more reliably inferred cross-linguistically than 
hypernymous relations is somewhat surprising, 
since they are symmetric semantic relations. That 
is, if A is a hypernym of B, then B is a hyponym 
of A. Logically, there does not seem to be any 
reason for the two relations to have disjoint 
distributions when transported to another 
language. However, more careful study of the 
conceptual nature of the semantic relations yields 
a plausible explanation.  
 We should take note of the two following 
facts: First, a hyponym link defined on an 
English word Y presupposes a conceptual class 
denoted by Y, and stipulates that Z is a kind of Y 
(see Diagram 4).  
 
 
 
 
 
 
 
 
 
Diagram 4. class vs. member identity (HPO) 
 
Second, a hypernym link defined on Y 
presupposes an identity class X which is NOT 
explicitly denoted, and stipulates that Y is a kind 
of X (see Diagram 5). Hence, it is possible that 
there is another valid conceptual class W in the 
target language that Y is a member of. And yet 
W is not equivalent to X. 
 
 
 
 
 
 
 
 
 
 
Diagram 5. class vs. member identity (HYP) 
 
Since our inference is based on the synonymous 
relation of the Chinese TE to the English word Y, 
the conceptual foundation of the semantic 
relation is largely preserved, and the inference 
has a high precision. The failure of inference can 
in most cases be attributed to the fact that the 
intended HYP has no synonymous TE in Chinese. 
To infer a hyponymous relation, however, we 
need to presuppose the trans-lingual equivalence 
of the conceptual class defined by HPO. And 
since our inference only presupposes the 
synonymous relation of Y and its TE, and says 
nothing about HPO, the success of inference of 
the hyponymous relation is than dependent upon 
an additional semantic condition. Hence that it 
will have lower precision can be expected. 
 To sum up, our preliminary evaluation 
found that the precision of cross-lingual 
inference of semantic relation can be higher than 
90% if the inference does not require other 
conceptual/semantic relations other than the 
synonymy of the translation equivalents. On the 
other hand, an additional semantic relation, such 
as the equivalence of the hypernym node in both 
languages when inferring hyponym relations, 
seems to bring down the precision rate by about 
10%. 
 
3.3. When Translation Introduces an additional 
LSR 
 In this section, we study the cases where 
translation introduces a hypernymous/ 
hyponymous LSR. These cases offer the real test 
to our proposal that TE?s be treated as bilingual 
LSR?s. The LSR inference here refuses 
non-vacuous combinations of two LSR?s. For 37 
Chinese input lemmas that hold other relations 
with English synsets, 57 semantically related 
links were expanded. First, we investigated the 
situation when the English synset occurs as a 
hyponym of the Chinese input lemma (Diagram 
6). 
       

2 = EW2 (ii = 0) 
 
          y       x 
 
CW1      

1 
    
(a) IF x = HPO 
y = HPO + HPO = HPO (Hyponym is 
transitive.) 
(b) IF x = HYP 
y = HPO + HYP =   
Diagram 6. Predicting LSR, when English is the 
hyponym of Chinese translation 
 
33 inferable relations satisfied above description. 
Among them, 8 falls in the entailment of figure 
Y= class identity 
class 
Tclunknown 
LSR 
Z 
HPO Member set identity 
is entailed. 
Y=member identity 
X 
W 
Class identity is 
NOT entailed. 
HYP HYP 
  i = HPO 
6(a). Manual evaluation confirms the prediction. 
The other 25 cases are not logically inferable and 
do indeed show a range of different relations. 
The logically entailed HPO relation is 
exemplified below: 
  


chi1 HPO  	
 OLACMS: Comparisons and Applications in Chinese and 
Formosan Languages 
Ru-Yng Chang 
Institute of Linguistics, Academia Sinica 
130 Sec.2 Academy Rd. 
Nankang, Taipei, Taiwan, 115 
ruyng@gate.sinica.edu.tw 
Chu-Ren Huang 
Institute of Linguistics, Academia Sinica 
130 Sec.2 Academy Rd 
Nankang, Taipei, Taiwan, 115 
churen@gate.sinica.edu.tw 
Abstract  
OLACMS (stands for Open Language 
Archives Community Metadata Set) is a 
standard for describe language resources. 
This paper provides suggestion to OLACMS 
0.4 version by comparing it with other 
standards and applying it to Chinese and 
Formosan languages.  
1 Introduction1 
The Open Language Archives Community 
(OLAC, http://www.language-archives.org) is 
an international partnership of institutions and 
individuals who are creating a worldwide virtual 
library of language resources by: (1) developing 
consensus on best current practices for the 
digital archiving of language resources; (2) 
developing a network of interoperating 
repositories and services for housing and 
accessing such resources. 
Three primary standards are the foundational 
basis of the OLAC infrastructure that serve to 
bridge the multiple gaps which now lie in 
between language resources and users: 
(1)OLACMS: the OLAC Metadata Set 
(Qualified DC, Dublin Core), (2) OLAC MHP: 
refinements to the OAI (Open Archives 
Initiative, http://www.openarchives.org) protocol, 
and (3) OLAC Process: a procedure for 
identifying Best Common Practice 
Recommendations. 
                                                     
1 We are indebted to Steven Bird and reviewers of 
the 3rd Workshop on Asian Language Resources and 
International Standardization for their valuable 
comments and corrections. Colleagues of the 
Language Archives project at Academia Sinica 
provided data and suggestions. Any remaining errors 
are ours. 
It is crucial to note that the OLAC standards are 
not standards for the language resources 
community alone. They are based on two 
broadly accepted standards in the digital 
archives community. First, the Dublin Core 
Metadata Initiative (DCMI) is an open forum 
engaged in the development of interoperable 
online metadata standards that support a broad 
range of purposes and business models. There 
are fifteen Doblin Core Metadata Elements 
(DCMS) and their qualifiers. OLACMS extends 
the DC minimally to anwer the needs of the 
language archives community (Bird, Simons, 
and Huang 2001). 
Second, the Open Archives Initiative (OAI) was 
launched in October 1999 to provide a common 
framework across electronic preprint archives, 
and it has since been broadened to include 
digital repositories of scholarly materials 
regardless of their type. The OAI infrastructure 
requires compliance with two standards: the 
OAI Shared Metadata Set (i.e. DCMS), and the 
OAI Metadata Harvesting Protocol (MHP). The 
OAI MHP allows software services to query a 
repository using HTTP requests, also an 
important feature of the recently hyped Semantic 
Web (http://www.w3.org/2001/sw/). Using the 
OAI infrastructure, the community's archives 
can be federated and become a virtual 
meta-archive collecting all available information. 
The federeated structure allows end-users to 
query multiple archives simultaneously. 
Currently, the Linguistic Data Consortium has 
harvested the catalogs of over 20 participating 
archives on behalf of OLAC, and created a 
search interface which permits queries over all 
30,000+ records. A single search typically 
returns records from multiple archives. The 
prototype can be accessed via the OLAC 
website. 
In this paper, we trace the version changes of 
OLACMS, especially in comparison with other 
(often related) international standards. We will 
then concentrate on the application of OLACMS 
to Chinese language resources. In conclusion, 
we will make some suggestions for OLACMS to 
account for the characteristics of Chinese 
language archives. 
2 Mapping with other international 
standards 
2.1.Mapping with IMDI 
ISLE Meta Data Initiative (IMDI) is a cousin of 
OLACMS. IMDI proposes a metadata set for 
natural language processing under the broader 
International Standards for Language 
Engineering (ISLE) project. ISLE is 
co-sponsored by the European Commission of 
the EU and National Science Foundation of the 
USA. It aims to develop a set of internationally 
accepted standards for natural language 
processing base on the result of the earlier 
European standard building project (EAGLES, 
http://www.ilc.pi.cnr.it/EAGLES96/home.html). 
On one hand, IMDI is an elaboration of 
OLACMS since it deals specifically with 
recording sessions. They can also be considered 
a complimenting each other since they are both 
devised under the aegis of ISLE. 
IMDI Metadata Elements for Session 
Descriptions, Version 2.5 was completed in June 
2001. The elements evolved from the previous 
EAGLES metadata set described in Wittenburg 
et al (2000). Both metadata sets share the aim to 
improve the accessibility/availability of 
Language Resources (LR) on the Internet. To 
achieve this goal, they created a browsable and 
searchable universe of meta-descriptions similar 
to those devised by other communities on the 
Internet. 
The focus on Session Description was motivated 
in Broeder et al (2000). They observed that 
individual linguistic resource usually exists in 
clusters of related resources. For instance, a field 
video recording of an informant who describes a 
picture sequence involves several resources. By 
his definition, an (linguistic) event that called a 
session is the top element and there results a 
number of related linguistic resources: Video 
tape, Photographs, Digitised video file, Digitised 
photographs, Digitisations of the images used as 
stimuli, One electronic transcription file, One or 
more electronic analysis files, Field notes and 
experiment descriptions (in electronic form). 
However, since not all linguistic resources come 
to existence directly through sessions, hence not 
all linguistic resources can be described by 
IMDI.2  
In principle, IMDI metadata can be mapped to 
OLAC metadata, just as OLAC metadata can be 
mapped to DC. IMDI Team (August 2001) 
mapped IMDI Session Descriptions with OLAC 
0.3 Version. IMDI Team also use existing 
description formalisms used by institutions that 
deal with ?published corpora? such as [ELRA] 
and [LDC]. The set of metadata elements that 
describe ?published corpora? are called 
?catalogue? metadata elements. The IMDI Team 
(Gibbon, et al 2001) launched IMDI Metadata 
Elements for Catalogue Descriptions, Version 
2.1. It also includes Metadata Elements for 
Lexicon Descriptions. 
OLACMS has been updated since December 
2001. Hence we did an updated comparison and 
present the result in this section. Note that since 
IMDI is an elaboration of OLACMS, we 
concentrate on the IMDI elements that are not 
specified in OLACMS and are likely to find 
wider application. Please note that the section 
contains our own recommendations inspired by 
the IMDI/OLAC comparison. We try to add our 
motivation even for the items that are directly 
adopted from IMDI. In terms of OLAC scheme, 
these suggested revision/addition can be 
assigned the status of attributes (for use by 
sub-communities), and can be incorporated into 
the OLACMS later if the community find such 
addition necessary. 
2.1.1. Controlled Vocabulary 
Controlled vocabulary defines the basic concepts 
of the metadata set and any addition to the 
controlled vocabulary should be motivated by 
                                                     
2 It is possible to conceive language resources such 
as lexica and grammars as created through a very 
large set of (non-planned and non-documented) 
sessions. But this consideration is beyond the scope 
of this paper and will not be pursued further here. 
the essentiality of the concept. 
y Controlled Vocabulary for Logical 
Structure of linguistic resources: Language 
resources come in different forms and 
various units. A critical piece of information 
in cataloguing language resource is a 
description of  the composition of the 
resources. For instance, any English lexicon 
can be conventionally and naturally viewed 
as composed of 26 sections defined by 
shared initial alphabet. Having an element of 
Logical Structure: alphabetically ordered 
would give us vital information of how to 
manipulate the resource. Other vocabularies 
such as ?sequential chapter?, ?dialogue turns?, 
or ?sequential phonemes? would also offer 
crucial information. In addiiton, if sequential 
database is indeed the future of language 
resources, the description of the sequencing 
logic will play an essential role. 
y Add Annotator to [OLAC?Role]. By 
annotator, we do not refer to the natural 
person or an automatic program who puts 
the tags on. By annotator we refer to the 
institution that implemented the annotation. 
This information is crucial since this 
annotator 1) has at least partial IP right on 
the resource; 2) often set/defines the tagset 
standard adopted (e.g. Brown, LOB, Penn 
TreeBank). In other words, annotator can 
differentiate a new version of resource or 
even identify totally new resource. 
y Add values of archiving Quality to the refine 
controlled vocabulary of Format. 
2.1.2. Elements 
One existing elements may need further refining 
with existing mechanisms. 
y Refining the element Project: Many 
language resources are developed under or 
partially supported by a project grant. For 
now, a project can be the value of Creator or 
Contributor. But just like all other individual 
creators and contributors, a project needs to 
be described in fuller details. We need to use 
attributes to describe the Founder, PI?s, Host 
Institutes, etc. of a project. An umbrella 
project, such as EAGLES, ISLE, or at a even 
more complex level, ESPRIT, requires 
elaboration of contributors and funding 
timelines themselves. 
2.1.3. Updating and Revising the Attributes 
y Add sub type to the Space attribute : 
Coverage of the language resources often 
calls for geographical information. Hence 
we need to define the subtypes that include 
Continent, Country, Administrative division, 
longitude, latitude, address, etc. 
y Add subtype for non-standard Identifier : 
There are many sets of identifers are defined 
locally and do not follow URL. In this case, 
we can add the name of the identifier system 
(or cataloguer) under schme. For instance, 
each libary often has its own set of call 
numbers. Other well-known identifiers arre 
LCC Catalog No (<Identifier 
sceeme="LCC"> LCC Catalog 
No</Identifier>). This could also apply to 
well-established identifiers such as ISSN and 
ISBN. 
y Although OLAC:Format does not stipulate 
any refine attributes, however, it is already 
stipulated in DC:Format. The DC format 
refine has two control vocabulary entries: 
Medium specifies the material that the 
cataloguer uses; while extent records size 
and duration of the archive. We suggest that 
OLAC can simply adopt these two refine 
attributes. 
2.2.Mapping with Linguistic 
Documentation Archives 
In addition to IMDI Metadata, Gary Holton 
(2000) also proposes a system of metadata for 
the description of language documentation 
resources following OLACMS. While the 
system described here should be sufficient for 
any linguistic resource, it is motivated by the 
specific ongoing need to describe linguistic 
documentation materials contained in the Alaska 
Native Language Center (ANLC) Archive. 
Particular attention is paid to description of 
first-hand documentation materials such as field 
notes, grammatical notes, and phonological 
descriptions, many of which currently exist only 
in written form. Existing resources are in the 
process of being digitized, and new digital 
resources continue to be acquired. The ANLC 
collection presently contains more than ten 
thousand items. While much of the material 
consists of original manuscripts of archival 
quality, the collection also includes published 
materials and materials existing in other archival 
collections, duplicated in whole or in part. The 
ANLC Archive thus combines both archival and 
library functions. 
The unique need described in Holton (2000) is 
that he wants the Metadata set to be applied 
simultaneously to non-digital archives, such as 
manuscript, reel-to-reel cassettes, CD recordings 
etc. This can be done by adopting the 
DC:Format refine attribute of Medium. In order 
to descibe the archives more felicitously, we 
also need to add speaker, interviewer Holder, 
and Guardian to the value of controlled 
vocabulary of refine of Creator and Contributor. 
However, there does not seem to be any 
straightforward way to transfer Target Dialect. 
2.3.Summary 
Based on the two comparison of different 
metadata sets, we found that the DC qualifier 
can be applied effectively to solve the bridging 
and conversion problems between different 
DC-based extension metadata sets. This should 
be exactly what OLACMS design has in mind. 
The attributes that were not stipulated in 
OLACMS 0.4, if found in DC and motivated by 
actual need to describe language resources, can 
be easily adopted. One way to ensure the 
versatility is to keep all DC attribute in 
OLACMS, even though some of the attributes 
may be dormant and not actively applied now. 
Another issue worth noting is that any 
cataloguer may add sub-elements to achive more 
comprehensive description. However, such 
addition should, follow the extension and 
adaptability of the DC. 
3 Use Controlled Vocabulary for 
Temporal and Geographic Location 
Constable, and Simons (2000) listed all the 
causes for language changes, which basically 
involve the change in the temporal-spatial 
location of the poeple. Since China used a 
different calendar system until late in early 20th 
century, all inherent temporal description of 
inherited Chinese archives do not conform to the 
current DC standard. In order to identify 
Western and Chinese chronology, we may 
stipulate that the primary types of the scheme 
element to be Western (W_Calendar) or Chinese 
(C_Calendar). We may also add other 
chronological methods, such as lunar or solar 
calendar. The sub_type of Chinese calendar will 
then include time, dynasty name, state name, 
emperor?s reign, and the reign name of the 
emperor. Take the Academia Sinica Ancient 
Chinese Corpus for example. Its coverage is 
Early Mandarin Chinese, and will marked as 
such in the metadata: <Coverage 
scheme="C_calendar/phase">EarlyMandarin 
</Coverage>. The users will be able to refer to a 
historical linguistic calendar and find that the 
time equals to the dynasties of Yuan, Ming, and 
Ching. And will be able to convert the time to 
western calendar using the conversion table of 
[Sinica Calendar]. 
When Coverage has a spatial refinement, a 
location can have different names because of the 
unit used in cataloguing, as well as because of 
temporal or regional and linguistic variaions. 
Hence, the spatial value of Coverage must be 
defined by a scheme. A scheme must stipulate 
temporal reference as unit of catalogue. For 
instance, the Sinica Corpus covers the language 
of the Republica of China in Taiwan. Its 
metadata will have the following value 
<Coverage refine= "spatial" scheme= 
"ROC/Taiwan">. As mentioned above [Sinica 
Calendar] offers conversion table for the past 
2000 years between Chinese and Western 
calendars. As for the units for cataloguing of 
spatial location, OLAC 0.4 Version adopts 
[TGN]( Getty Thesaurus of Geographical 
Terms). And many other digital archives follow 
Alexandria Digital Library Feature Type 
Thesaurus [ADL]. The ADL type thesaurus have 
been adopted by the digital archives project in 
Taiwan and translated into Chinese by 
Academia Sinica Metadata Architecture and 
Application Team [Sinica MAAT]. 
4 Applying OLACMS to Language 
Archives in Taiwan 
Each text in Academia Sinica Balanced Corpus 
of Modern Chinese (Sinica Corpus) is marked 
up with five textual parameters: Mode, Genre, 
Style, Topic and Medium. These are important 
textual information that needs to be catalogued 
in metadata. The following shows how we 
transfer and represent these (legacy) textual 
information to OLACMS: 
4.1.Mode and Genre 
Table 1 The relation between Mode and Genre of 
Sinica Corpus(Ckip Technology Report 93-05) 
Mode Genre 
Written Reportages 
Commentary 
Advertisement 
Letters 
Announcement 
Fiction 
Prose 
Biography & Diary 
Poem 
Manual 
written-to-be-spoken Script 
Speech 
Spoken Conversation 
spoken-to-be-written analects 
Speech 
Meeting Minute 
We add a refine attribute under Type. Mode is 
added in the controlled vocabulary as Primary 
type, and Genre is added as sub type. For 
instance, a recorded and transcribed speech is 
catalogued as <Type code="Sound" 
refine="spoken-to-be-written/Speech"/>. 
4.2.Style  
There are four styles that are differentiated in 
Sinica Corpus: Narrative, Argumentative, 
Expository, and Descriptive. We add a new 
refine attirbute under Descriptio, with Style as a 
controlled vocabulary. For instance, a diary will 
be catalogued as: <Description refine="Style"> 
Narration </Description>. 
4.3.Medium 
Sinica Corpus specifies the media of the 
language reources as: Newspaper, General 
Magazine, Academic Journal, Textbook, 
Reference Book, Thesis, General Book, 
Audio/Visual Medium, Conversation/Interview. 
We may also add other audio-video media such 
as CD,V8?etc. As mentioned above, this can be 
easily described with DC: Format refine 
attribute of Medium. 
4.4.Topic 
The Topic parameter of Sinica Corpus has the 
same content as the element Subject. This can 
simply be transferred through a table. 
Table 2 Topic of Sinica Corpus(Ckip Technology 
Report 93-05) 
Primary Sub 
Philosophy Thoughts | Psychology | Religion | 
Natural 
Science 
Mathematics | Astronomy | Physics | 
Chemical | Mineral | Creature | 
Agriculture | Archeology | Geography | 
Environmental Protection | Earch 
Science | Engineering | 
Social 
Sciences 
Economy | Finance | Business & 
Management | Marketing | Politics | 
Political Party | Political Activities | 
National Policy | International 
Relations | Domestic Affairs | Military 
|Judicature | Education | 
Transportation | Culture | History | 
Race | Language | MassMedia | Public 
Welfare | Welfare | Personnel Matters | 
Statistical Survey | Crime | Calamity | 
Sociological Facts | 
Arts Music | Dance | Sculp | Painting | 
Photography | Drama | Artistry | 
Historical Relics | Architecture | 
General Arts | 
General 
/Leisure 
Travels | Sport | Foods | Medical 
Treatment | Hygine | Clothes | Movie 
and popular arts | People | Information 
| Cunsume | Family | 
Literature Literary Theory | Criticism | Other 
literary work | Indigenous Literature | 
Childern?s Literature | Martial Arts 
Literature | Romance | 
An example for the adoptation follows: for a 
Sinica Corpus text with a Topic of Arts and a 
sub-topic of Music, it will be catalgued as 
follows: <Subject>Arts/Music</Subject>. 
4.5.Additional Controlled Vocabulary  
y Proofreader: Since both manually and 
automatically digitized materials must be 
proofread to ensure quality, we suggest that 
[OLAC-Role] be enriched by a new value: 
Proofreader. For inherited texts with no IP 
restrictions, this may be the critical 
information piece of information to identify 
who is the rightful owner/creator of the 
electronic version. 
y There are many Medium values old 
(procelain, rubbing, bamboo engraving, silk 
scroll, etc.) and new (DVD, MO, ZIP...etc). 
Hence the controlled vocabulary of attributes 
such as Medium and SourceCode often has 
quick and drastic changes. In order to 
maintain versatility and comprehensive 
coverage, this set of controlled vocabulary 
must be open and allows each participant to 
register, subject to the approval by OLAC. 
5 Language Identification 
Constable and Simons (2000) noted that a 
computer, unlike human beings, cannot 
automatically identify the language of a text that 
it is reading yet. Hence metadata must play a 
central role in identifying the language that each 
resource uses. For instance, Malay and English 
uses the same 26 letters. And Archaic Chinese 
2000 years ago and Modern Mandarin can be 
expressed by pretty much the same set of 
Chinese characters. These are all different 
languages and need to be identified before a 
language resource can be used. SIL (Summer 
Institute of Linguistics, in its white-paper 
identified five major issues for language 
identification: Change, Categorization, 
Inadequate definition, Scale, and Documentation. 
SIL has produced an online searchable database: 
Ethnologue that provides a comprehensive 
system of language identification covering more 
than 6,800 languages. This is adopted by OLAC 
as an obvious improvement over the very small 
set covered in DC.  
Bird et al (2001), however, pointed out some 
problems of coverage if the Enthlogue system is 
adapted without further means of enrichment. 
The three broad categories of problem are: 
over-splitting, over-chunking and omission. 
Over-splitting occurs when a language variety is 
treated as a distinct language. For example, 
Nataoran is given its own language code (AIS) 
even though the scholars at Academia Sinica 
consider it to be a dialect of Amis (ALV). 
Over-chunking occurs when two distinct 
languages are treated as dialects of a single 
language (there does not appear to be an 
example of this in the Ethnologue's treatment of 
Formosan languages). Omission occurs when a 
language is not listed. For example, two extinct 
languages, Luilang and Quaquat, are not listed in 
the Ethnologue. Another kind of omission 
problem occurs when the language is actually 
listed, but the name by which the archivist 
knows it is not listed, whether as a primary name 
or an alternate name. In such a case the archivist 
cannot make the match to assign the proper code. 
For instance, the language listed as Taroko 
(TRV) in the Ethnologue is known as Seediq by 
Academia Sinica; several of the alternate names 
listed by the Ethnologue are similar, but none 
matches exactly.  
The above problems may prove to be a 
stumbling block for archives that attempt to 
integrate linguistic resources with GIS 
(Geographic Information System), such as the 
[Formosan Language Archive] at Academia 
Sinica. A GIS-based language atlas will most 
likely be very concerned with fine-grained 
changes and variations among languages and 
dialects within a geographic area. In other words, 
these kind of archives may either discover yet 
unrecorded language or sub-language 
differentiations or need even finer classification 
in Ethnologue or any language identification 
system. Hence the solution proposed in Bird et 
al. (2001) of allowing local language 
classification systems to register must be 
implemented under OLAC. 
6 Conclusion 
We looked at a couple of OLAC derived 
metadatasets, as well as applied OLAC version 
0.4. to three different language archives in 
Taiwan. We proposed some suggestions for 
enriching of OLACMS based on the study. 
There are two general directions to bear in mind. 
First, as the number and complexity of language 
resources becomes higher and higher, the need 
to have a uniform standard or to easy access to 
the owner of each resource becomes even 
greater. Therefore, we envision that the element 
of Creator, Contributor etc. needs further 
elaboration, which may include practical 
information such as email addresses etc. Second, 
as the language archives get richer, the need to 
note language variation grows even bigger. 
Simple language identification of allotting a 
resource a unique language code is not enough. 
There will be great need to infer linguistic 
relations from these codes. Since it is impossible 
to build a complete reportiore of resources for 
all languages, it is very often that a resources 
from the closest related language must be 
borrowed. The representation of linguistic 
relations will be the next challenge of language 
identification. 
References  
I. Bibliography 
Bird, S. 2000. ISLE: International Standards in Language 
Engineering Spoken Language Group, 
http://www.ldc.upenn.edu/sb/isle.html 
Bird, S., G. Simons, and C.-R. Huang 2001. The Open 
Language Archives Community and Asian Language 
Resources, 6th Natural Language Processing Pacific Rim 
Symposium Post-Conference Workshop, Tokyo, Japan. 
Broeder, D., P. Suihkonen, and P. Wittenburg. 2000. 
Developing a Standard for Meta-Descriptions of 
Multimedia Language Resources, Web-Based Language 
Documentation and Description workshop, Philadelphia, 
USA. 
CKIP. 1993. An Introduction to Sinica Corpus. CKIP 
Technology Report 93-05. IIS, Academia Sinica.  
Constable, P. and G. Simons. 2000. Language identification 
and IT: Addressing problems of linguistic diversity on a 
global scale, SIL Electronic Working Papers 
2000-001.http://www.sil.org/silewp/2000/001/ 
EAGLES/ISLE. ISLE Meta Data Initiative, 
http://www.mpi.nl/world/ISLE/ 
Gibbon, D., Peters, W., and Wittenburg, P., 2001. Metadata 
Elements for Lexicon Descriptions, Version 1.0, MPI 
Nijmegen, 
http://www.mpi.nl/ISLE/documents/draft/ISLE_Lexicon
_1.0.pdf 
Holton, G. 2000. Metadata for Linguistic Documentation 
Archives, Web-Based Language Documentation and 
Description workshop, Philadelphia, USA. 
IMDI Team. 2001. IMDI Metadata Elements for Session 
Descriptions, Version 2.5, MPI Nijmegen, 
http://www.mpi.nl/ISLE/documents/draft/ISLE_MetaDat
a_2.5.pdf. 
IMDI Team. 2001. Mapping IMDI Session Descriptions 
with OLAC, Version 1.04, MPI Nijmegen. 
http://www.mpi.nl/ISLE/documents/draft/IMDI%20to%
20OLAC%20Mapping%201.04.pdf 
IMDI Team. 2001. IMDI Metadata Elements for Catalogue 
Descriptions, Version 2.1, MPI Nijmegen, 
http://www.mpi.nl/ISLE/documents/draft/IMDI_Catalog
ue_2.1.pdf 
Palmer, M. 2000. ISLE: International Standards for 
Language Engineering: A European/US joint project,  
http://www.cis.upenn.edu/~mpalmer/isle.kickoff.ppt 
Wittenburg, P., D. Broeder, and B. Sloman. 2000. 
EAGLES/ISLE: A Proposal for a Meta Description 
Standard for Language Resources, White Paper. LREC 
2000 Workshop, Athens. 
 
II. Websites 
[OLAC] Open Language Archives Community, 
http://www.language-archives.org 
[OLACMS] OLAC Metadata Set, 
http://www.language-archives.org/OLAC/olacms-20011
022.html  
[DCMI] Dublin Core Metadata Initiative, 
http://dublincore.org/ 
[DCMS] Dublin Core Element Set, Version 1.1 - Reference 
Description, http://dublincore.org/documents/dces/. 
[DC-Q]  Dublin Core Qualifiers. 
http://dublincore.org/documents/2000/07/11/dcmes-quali
fiers/ 
[ISLE] International Standards for Language Engineering, 
http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE_Home_Pa
ge.htm 
[ELRA]  European Language Resources Association, 
http://www.icp.grenet.fr/ELRA/ 
[LDC] Linguistic Data Consortium, 
http://morph.ldc.upenn.edu/ 
[Sinica Calendar]  Western Calendar and Chinese 
Calendar Conversion Table of Academia Sinica 
Computing Centre. 
http://www.sinica.edu.tw/~tdbproj/sinocal/luso.html. 
[Academia Sinica Ancient Chinese Corpus]  Academia 
Sinica Tagged Corpus of Early Mandarin Chinese, 
http://www.sinica.edu.tw/Early_Mandarin/  
[TGN]  Getty Thesaurus of Geographical Terms, 
http://www.getty.edu/research/tools/vocabulary/tgn/inde
x.html 
[ADL]  Alexandria Digital Library Feature Type, 
http://alexandria.sdc.ucsb.edu/gazetteer/gaz_content_sta
ndard.html 
[Sinica MAAT]  Metadata Architecture and Application 
Team, 
http://www.sinica.edu.tw/~metadata/standard/place/ADL
-element.htm 
[Sinica Corpus]  Academia Sinica Balanced Corpus of 
ModernChinese, http://www.sinica.edu.tw/SinicaCorpus/ 
[Ethnologue] http://www.ethnologue.com 
[Formosan Language Archive]  Academia Sinica 
Formosan Language Archive, 
http://www.ling.sinica.edu.tw/Formosan/ 
Categorical Ambiguity and Information Content 
A Corpus-based Study of Chinese 
Chu-Ren Huang, Ru-Yng Chang 
Institute of Linguistics, Preparatory Office, Academia Sinica 
128 Sec.2 Academy Rd., Nangkang, Taipei, 115, Taiwan, R.O.C. 
churen@gate.sinica.edu.tw, ruyng@hp.iis.sinica.edu.tw 
 
 
1. Introduction 
Assignment of grammatical categories is 
the fundamental step in natural language 
processing. And ambiguity resolution is one of 
the most challenging NLP tasks that is currently 
still beyond the power of machines. When two 
questions are combined together, the problem of 
resolution of categorical ambiguity is what a 
computational linguistic system can do 
reasonably good, but yet still unable to mimic the 
excellence of human beings. This task is even 
more challenging in Chinese language processing 
because of the poverty of morphological 
information to mark categories and the lack of 
convention to mark word boundaries. In this 
paper, we try to investigate the nature of 
categorical ambiguity in Chinese based on Sinica 
Corpus. The study differs crucially from previous 
studies in that it directly measure information 
content as the degree of ambiguity. This method 
not only offers an alternative interpretation of 
ambiguity, it also allows a different measure of 
success of categorical disambiguation. Instead of 
precision or recall, we can also measure by how 
much the information load has been reduced. 
This approach also allows us to identify which 
are the most ambiguous words in terms of 
information content. The somewhat surprising 
result actually reinforces the Saussurian view 
that underlying the systemic linguistic structure, 
assignment of linguistic content for each 
linguistic symbol is arbitrary.  
 
2. Previous Work 
 Assignment of grammatical categories or 
tagging is a well-tested NLP task that can be 
reliably preformed with stochastic methodologies 
(e.g. Manning and Shutz 1999). Depending on 
the measurement method, over 95% precision 
can be achieved regularly. But the question 
remains as to why the last few percentages are so 
hard for machines and not a problem for humans. 
In addition, even though over 95% seems to be 
good scores intuitively, we still need to find out 
if they are indeed better than the na?ve baseline 
performance. Last but not the least, since natural 
languages are inherently and universally 
ambiguous, does this characteristic serve any 
communicative purpose and can a computational 
linguistic model take advantage of the same 
characteristics. 
 Since previous NLP work on categorical 
assignment and ambiguity resolution achieved 
very good results using only distributional 
information, it seems natural to try to capture the 
nature of categorical ambiguity in terms of 
distributional information. This is how the 
baseline model was set in Meng and Ip (1999), 
among others. Huang et al (2002), the most 
extensive study on categorical ambiguity in 
 Chinese so far, also uses only distributional 
information. 
 Huang et al (2002) confirmed some 
expected characteristics of ambiguity with 
convincing quantitative and qualitative data from 
the one million word Sinica Corpus 2.0. Their 
generalizations include that categorical 
ambiguity correlates with frequency; that verbs 
tend to be more ambiguous than nouns, and that 
certain categories (such as prepositions) are 
inherently more ambiguous. 
 What is not totally unexpected, and yet runs 
against certain long-held assumptions is the 
distribution of ambiguity. It is found that only a 
small fraction of all words (4.298%) are assigned 
more than one category. However, in terms of 
actual use, these words make up 54.59% of the 
whole corpus. These two facts are consistent 
with the frequency effect on ambiguity. An 
interesting fact is that of all the words that can 
have more than one category, 88.37% of the 
actual uses are in the default category. 
A significant fact regarding Chinese 
language processing can be derived from the 
above data. Presupposing lexical knowledge of 
the lexicon and the default category of each word, 
a na?ve baseline model for category assignment 
two simple steps: First, if a word has only one 
category in the lexicon, assign that category to 
the word. Second, if a word has more than one 
category in the lexicon, assign the default (i.e. 
most frequently used) category to that word. 
Since step 1) is always correct and the precision 
rate of step 2) depends on the percentage of use 
of the default category. Huang et al (2002) 
estimated the expected precision of such a na?ve 
model to be over 93.65%. 
Huang et al?s (2002) work, however, has its 
limitation. It takes categorical ambiguity as a 
lexical attribute. In other words, an attribute is 
either + or -, and a certain word is either 
categorically ambiguous or not. For Huang et al 
(2002), the degree of ambiguity is actually the 
distribution of the attribute of being ambiguous 
among a set of pre-defined (usually by frequency 
ranking) lexical items. Strictly speaking, this data 
only shows the tendency of being categorically 
ambiguous for the set members. In other words, 
what has been shown is actually: 
Words with higher frequency are more 
likely to be categorically ambiguous. 
The data has noting to say about whether a 
lexical item or a set of lexical items are more 
ambiguous than others or not. 
 A good example of the inadequacy of 
Huang et al?s (2002) approach is their 
measurement of the correlation between number 
of potential categories and the likelihood of 
default category to occur. 
 
No. of Categories Freq. (by type) Freq. (by token) 
 2 77.65%  91.21% 
 3 77.71%  88.39% 
 4  74.21%  89.50% 
 5  73.83%  92.43% 
 6  73.46%  86.09% 
 7  68.51%  86.09% 
Total           77.36% 
99/48& 
Table 1.  Frequency of Default Category 
 
In table one, the number seems to suggest that 
number of possible categories of a word form is 
not directly correlated with its degree of 
ambiguity, since its probability of being assigned 
the default category is not predictable and 
remains roughly the same in average. This is 
somewhat counter-intuitive in the sense that we 
expect the more complex the information 
structure (i.e. more possible categories), the less 
 likely that it will be assigned a simple default. 
Since the methodology is to take distributional 
information over a large corpus, it is most likely 
the number shown in table 1 is distorted by the 
dominance of the most frequent words.  
 Is there an alternative to pure distributional 
measurement? Recall that ambiguity is about 
information content. Hence if the quantity of 
information content is measured, there will be a 
more direct characterization of ambiguity.  
 
3. Towards an Informational Description 
of Categorical Ambiguity 
3.1. Degree of Categorical Ambiguity 
 In this paper, we will adopt Shannon?s 
Information Theory and measure categorical 
ambiguity by entropy. We define the information 
content of a sign as its entropy value.  
H = - (p0 log p0 + p1 log p1 + ?+pn log pn) 
 When measuring categorical ambiguity, for 
a word with n potential categories, the 
information content of that word in terms of 
grammatical categories is the sum all the entropy 
of all its possible categories. We will make the 
further assumption of that the degree of 
ambiguity of a word corresponds to the quantity 
of its information content. 
The above definition nicely reflects the intuition 
that the more predictable the category is, the less 
ambiguous it is. That is, a word that can be 90% 
predicted by default is less ambiguous than a 
word that can only be predicted in 70% of the 
context. And of course the least ambiguous 
words are those with only one possible category 
and can be predicted all the time (it information 
value is actually 0). 
 
3.2. Degree of Ambiguity and Number of 
Possible Categories Revisited 
 Armed with a new measurement of the 
degree of ambiguity for each lexical item, we can 
now take another look at the purported lack of 
correlation between number of possible 
categories and degree of ambiguity. Instead 
having to choose between type of token as units 
of frequency counting, we can now calculate the 
degree of categorical ambiguity for each lexical 
form in terms of entropy. The entropy of all 
lexical forms with the same numbers of possible 
categories can then be averaged. The results is 
diagramed below: 
Diagram 1. Degree. of Ambiguity vs. Number of Categories
?
???
?
???
?
???
?
???
?
???
?
???
?
???
? ? ? ? ? ? ? ? ?? ?? ??
Number of Possible Categories
Av
er
ag
e 
De
gr
ee
 o
f A
m
b.
(E
ntr
op
y)
?? tags
13 tags
  
In the above diagram, we can clearly see that 
whether a 47 tags system or 13 tags system is 
chosen, the number of potential categories 
correlates with the degree of ambiguity. The 
higher number of potential categories a word 
has, the more ambiguous it is. This correctly 
reflects previous observational and theoretical 
predictions. 
 
3.3. Frequency and Degree of Ambiguity 
 One of the important findings of Huang et 
al. (2002) was that the likelihood to be 
ambiguous indeed correlates with frequency. 
That is, a more frequently used word is more 
likely to be categorically ambiguous. However, 
we do not know that, of all the categorically 
ambiguous words, whether their degree of 
ambiguity corresponds to frequency or not. 
 In terms of the number of possible 
categories, more frequent words are more likely 
to have larger number of categories. Since we 
have just showed in last session that larger 
number of possible categories correlates with 
degree of ambiguity. This fact seems to favor 
the prediction that more frequent words are also 
more ambiguous (i.e. harder to predict their 
categories.) 
 Common sense of empirical models, 
however, suggests that it is easier to predict the 
behaviors of more familiar elements. 
Confidence of prediction corresponds to 
quantity of data. A different manifestation of 
this feature is that there is a data sparseness 
problem but never a data abundance problem. In 
addition, the high precision rate of categorical 
assignment requires that most frequent words, 
which take up the majority of the corpus, be 
assigned correct category at a reasonable 
precision rate. These two facts seem to suggest 
that the less frequent words may be harder to 
predict and hence more ambiguous.
. 
Diagram 2. Frequency and ambiguity
 ? ???????
??????
?
?
?
?
? ???? ????? ????? ????? ????? ????? ?????
????
A
m
bi
gu
ity
? ? ????
?A
G ?? ? ?????
 
Diagram 2 plots the degree of ambiguity of each 
ambiguous word in terms of its frequency in the 
Sinica Corpus (Chen et al 1996). Not only does 
the distribution of the degree of ambiguity vary 
widely, the medium tendency line (thick black 
line in the diagram) varies barely perceptibly 
across frequency ranges. As suggested by the 
two competing tendencies discussed above, our 
exhaustive study actually shows that there is no 
correlation between degree of ambiguity and 
frequency. This generalization can be shown 
with even more clarity in Diagram 3.
 Diagram 3. Degree of Ambiguity vs. Frequency Ranking
?
?
?
?
? ??? ??? ??? ??? ???? ????
Frequency Ranking
?






?
 
?
?
?

?


? ? ????
In Diagram 3, entropy value of each word form 
is plotted against its frequency ranking. When 
word forms share the same frequency, they are 
given the same ranking, and no ranking jumps 
were given after multiple elements sharing the 
same ranking. Due to the sharing of rankings, 
the highest rank only goes to 1,000. Diagram 3 
shows unequivocally that the range of degree of 
ambiguity remains the same across different 
frequency ranges. That is, degree of ambiguity 
does not correlate to word frequency. 
 
4. Conclusion 
 In this paper, we propose an 
information-based measure for ambiguity in 
Chinese. The measurement compliments the 
more familiar distributional data and allows us 
to investigate directly the categorical 
information content of each lexical word. We 
showed in this paper that degree of ambiguity 
indeed correlates with the number of possible 
categories of that word. However, degree of 
ambiguity of a word does not correlates with its 
frequency, although its tendency to be 
categorically ambiguous is dependent on 
frequency. 
 The above findings have very important 
implications for theories and applications in 
language processing. In terms of representation 
of linguistic knowledge, it underlines the 
arbitrariness of the encoding of lexical 
information, following Saussure. In terms of 
processing model and empirical prediction, it 
suggests a model not unlike the theory of 
unpredictability in physics. Each word is like an 
electron. While the behavior of a group of words 
can be accurately predicted by stochastic model, 
the behavior of any single word is not 
predictable. In terms of linguistic theory, this is 
because there are too many rules that may apply 
to each lexical item at different time and on 
different levels, hence we cannot predict exactly 
how these rules the results without knows 
exactly which ones applied and in what order. 
This view is compatible with the Lexical 
Diffusion (Wang 1969) view on application of 
linguistic rules.  
 In NLP, this clearly predicts the 
performance ceiling of stochastic approaches. 
As well as that the ceiling can be surpassed by 
hybriding with specific lexical heuristic rules 
covering the ?hard? cases for stochastic 
approaches, as suggested in Huang et al (2002). 
 
References: 
Chen, Keh-jiann, Chu-Ren Huang, Li-ping Chang, and 
Hui-Li Hsu. 1996. Sinica Corpus: Design 
Methodology for Balanced Corpora. In. B.-S. Park 
and J.B. Kim. Eds. Proceeding of the 11th Pacific Asia 
Conference on Language, Information and 
Computation. 167-176. 
http//:www.sinica.edu.tw/SinicaCorpus 
Chinese Knowledge Information Processing (CKIP) Group. 
1995. An Introduction to Academia Sinica Balanced 
Corpus for Modern Mandarin Chinese. CKIP Technical 
Report. 95-01. Nankang: Academia Sinic 
Huang, Chu-Ren, Chao-Ran Chen and Claude C.C. Shen. 
2002. Quantitative Criteria for Computational Chinese 
The Nature of Categorical Ambiguity and Its 
Implications for Language Processing: A 
Corpus-based Study of Mandarin Chinese. Mineharu 
Nakayama (Ed.) Sentence Processing in East Asian 
Languages. 53-83. Stanford: CSLI Publications 
Manning Christopher D. and Hinrich Shutze. 1999. 
Foundations of Statistical Natural Language 
Processing. Cambridge: MIT Press. 
Meng, Helen and Chun Wah Ip. 1999. An Analytical Study 
of Transformational Tagging for Chinese Text. In 
Proceedings of ROCLING XII. 101-122. Taipei: 
Association of Computational Linguistics and Chinese 
Language Processing. 
Schutz, Hinrich. 1997. Ambiguity Resolution in Language 
Learning: Computational and Cognitive Models. Stanford: 
CSLI Publications. 
Wang, S.-Y. W. 1969. Competing Changes as a Cause of 
Residue. Language. 45.9-25.
Conceptual Metaphors: Ontology-based representation and corpora
driven Mapping Principles
Kathleen Ahrens
National Taiwan University
kathleenahrens@yahoo.com
Siaw Fong Chung
National Taiwan University
claricefong6376@hotmail.com
Chu-Ren Huang
Academia Sinica
churen@sinica.edu.tw
Abstract
The goal of this paper is to integrate the
Conceptual Mapping Model with an on-
tology-based knowledge representation
(i.e. Suggested Upper Merged Ontology
(SUMO)) in order to demonstrate that
conceptual metaphor analysis can be re-
stricted and eventually, automated. In
particular, we will propose a corpora-
based operational definition for Mapping
Principles, which are explanations of why
a conventional conceptual metaphor has a
particular source-target domain pairing.
This paper will examine 2000 random ex-
amples of ?economy? (jingji) in Mandarin
Chinese and postulate Mapping Principles
based frequency and delimited with
SUMO.
1 Introduction
A theory of metaphor has been the focus of study
on lexical and figurative meaning for the past two
decades. Are conventional conceptual metaphors a
cognitive rather than a linguistic phenomenon?
Work within Cognitive Linguistics would seem to
say that this is the case. For example, Lakoff
(1993) writes with respect to the source-target do-
main mapping of the conventional conceptual
metaphor LOVE IS A JOURNEY:
Is there a general principle govern-
ing how these linguistic expressions
about journeys are used to charac-
terize love?. [Yes], but it is a gen-
eral principle that is neither part of
the grammar of English, nor the
English lexicon. Rather it is part of
the conceptual system underlying
English?. (page 306, italics added)
Thus, the onus of dealing with metaphorical
meaning in the lexicon is not necessary. Metaphor
must be treated at a different (i.e. higher) cognitive
level.
But is it really the case that there are no
general principles that can be extracted and pro-
posed at the lexical level? The Conceptual Map-
ping (CM) Model (Ahrens 2002) was proposed to
constrain the Contemporary Theory of Metaphor
(Lakoff 1993). This model analyzes the linguistic
correspondences between a source and target
(knowledge) domain in order to determine the un-
derlying reason for the source-target pairings. The
underlying reason is formulated in terms of a Map-
ping Principle. The theory also postulates a Map-
ping Principle Constraint, which says that a target
domain will select only source domains that in-
volve unique mapping principles. For example, the
target domain of IDEA uses the source domains of
BUILDING and FOOD, but it does so for different
reasons (as we will discuss in the next section).
With the addition of this constraint, the CM model
is able to explicate the polysemy inherent in a
given target domain. In addition, the CM Model
presupposes that Mapping Principles are conven-
tionalized linguistically but not conceptualized a
priori. This model is supported in psycholinguistic
experiments because it correctly predicted the
processing differences involved between conven-
tional and novel metaphors (Ahrens 2002). In this
paper, we propose a new approach to conceptual
metaphors that incorporates two computationally
trackable elements. First, the data analysis is cor-
pus-based, following the example of MetaBank
(Martin 1992). Second, the representation is ontol-
ogy-based. Both elements strengthen the empirical
basis of the account.
In this paper, we propose that the most
frequent mapping instance within a source domain
indicates the basis of the reason for the source-
target domain pairing, i.e. the mapping principle.
We test this empirical prototype (EP) hypothesis
by running extracting a dataset of 2000 examples
of jingji ?economy? from the Academia Sinica
Balanced Corpus. We hypothesize that each
source-target domain pairing will have a proto-
typical instance of mapping as evidenced by an
individual lexical item that is highly frequent as
compared with other mappings. In addition, we
propose using an ontological-based knowledge
representation, such as SUMO, to define and de-
limit the source domain knowledge in the CM
Model.  This has the advantage of using SUMO to
infer knowledge through automatic reasoning, and
as well as constraining the scope and falsifiablity
of the conceptual metaphor.
2 The Conceptual Mapping Model and
Ontology
Ahrens (2002) proposed that the question asked by
Lakoff above (?Is there a general principle gov-
erning how these linguistic expressions about jour-
neys are used to characterize love??) should be
answered by examining the lexical correspon-
dences that exist between a source and target do-
main. She proposes that the linguistic expressions
that are used metaphorically can be analyzed in
terms of the entities, qualities and functions that
can map between a source and a target domain.
When these conventionalized metaphorical expres-
sions have been analyzed, they are compared with
the real world knowledge that the source domain
entails, and an underlying reason for these map-
pings is then postulated.
For example, she points out that in the
conceptual metaphor IDEA IS BUILDING in
Mandarin, the linguistic expressions relating to the
concept of foundation, stability and construction
were mapped (i.e. are conventional linguistic ex-
amples) while concepts relating to position of the
building, internal wiring and plumbing, the exterior
of the building, windows and doors were not (and
these are the concepts that are in the real world
knowledge of the source domain). Thus she postu-
lated that the target domain of IDEA uses the
source domain of BUILDING in order to empha-
size the concept of structure. Thus, when someone
talks about ideas and want to express positive no-
tions concerning organization, they use the source
domain of BUILDING. The Mapping Principle
formulated in this case was therefore the follow-
ing:
(1) Mapping principle for IDEA IS BUILDING:
Idea is understood as building because buildings
involve a (physical) structure and ideas involve
an (abstract) structure. (Ahrens 2002)
When IDEA is talked about in terms of
FOOD, however, the expressions that are mapped
are ?ingredient?, ?spoil?, ?flavorless?, ?full?, ?taste?,
?chew?, ?digest? and ?absorb?. Mandarin Chinese,
in contrast with English, does not have conven-
tional expressions relating to ?cooking? or ?stew-
ing? of ideas. Thus, the postulated Mapping
Principle is: Idea is understood as food because
food involves being eaten and digested (by the
body) and ideas involved being taken in and
processed (by the mind) (Ahrens 2002).
Thus, IDEA uses the source domains of
BUILDING and FOOD for different reasons,
namely to convey information related to ?structure?
or ?processing? (i.e. ?understanding?) respectively.
Thus, it is similar to the Contemporary Theory of
metaphor in that it supposes that there are system-
atic mappings between a source and target domain,
but it goes a step further in postulating an under-
lying reason for that mapping. The CM Model pre-
dicts that conventional metaphors, novel metaphors
that follow the mapping principle and novel meta-
phors that don?t follow the mapping principle will
be rated differently on interpretability and accept-
ability scales when other factors, such as frequency
are controlled for. This was, in fact, found to be the
case (Ahrens 2002). Other theories of metaphor
processing such as Gentner?s Structure Mapping
Model (Gentner and Wolff 2000), or the Attribu-
tive Categorization Hypothesis (McGlone 1996) do
not distinguish between novel and conventional
metaphors, nor do they suppose that there might be
different types of novel metaphors.
The CM model of metaphor presupposed
structured shared source domain knowledge. For a
mapping to be conventionalized and understood by
speakers, the content and structure of the source
domain knowledge must be a priori knowledge
and should not have to be acquired. How to define
and verify such structured knowledge is a chal-
lenge to this theory. We attempt to meet this chal-
lenge in two ways: first, by assuming that source
domain knowledge representation is instantiated by
a shared upper ontology, such as SUMO. If the
source domain knowledge representation is indeed
ontology-based, we can adopt the null hypothesis
that the mapping principle is based on one of the
inference rules encoded on that particular concep-
tual node. In consequence, we can take the second
step by examining actual mappings of linguistic
expressions in corpora, and extract the most fre-
quent mappings to verify the null hypothesis. This
will also allow us to investigate if it is the case that
frequency of use in a newspaper corpora necessar-
ily reflects the underlying mapping principle, an
issue which is currently open to interpretation.
The integration of an upper ontology to the
CM model has the following theoretical implica-
tions:
First, the source domain knowledge repre-
sentation is now pre-defined and constrained. Sec-
ond, the validity of such hypothesis will in turn
support the robustness and universality of the pro-
posed upper ontology.
3 SUMO
SUMO (Suggested Upper Merged Ontology ?
http://ontology.teknowledge.com) is a shared upper
ontology developed by the IEEE sanctioned IEEE
Standard Upper Ontology Working Group. It is a
theory in first-order logic that consists of approxi-
mately one thousand concepts and 4000 axioms. Its
purpose is to be a shared and inter-operable upper
ontology (Niles and Pease 2001, Pease and Niles
2002, Sevcenko 2003)  Since ontologies are for-
malized descriptions of the structure of knowledge
bases, SUMO can also be viewed as a proposed
representation of shared human knowledge, and
thus a good candidate for mapping information
about the source domain to the target domain.
What we will look at below is whether the SUMO
conceptual terms and inferences are candidates for
knowledge representation in the source domain. In
order to analyze this, we first need to extract from
a corpora the linguistic terms that are used for
mappings between a source and a target domain.
The application of SUMO in NLP and in proc-
essing of lexical meaning is facilitated by its inter-
face with WordNet. The SUMO interface allows
users to search and map each English lexical
meaning defined in WordNet to a concept node on
the SUMO ontology. Similarly, one can also
search for a Chinese lexical meaning and map it to
a SUMO concept node through a Chinese-English
bilingual translation equivalents database
(http://ckip.iis.sinica.edu.tw/CKIP/ontology/).
4 Corpora Data
In order to test the feasibility of using SUMO to
aid the analysis of Mapping Principles within the
framework of the CM Model, we searched the
Academia Sinica Balanced Corpus, a tagged cor-
pus of over 5 million words of modern Mandarin
usage in Taiwan (available on the Internet:
http://www.sinica.edu.tw/SinicaCorpus/). The
maximum number of responses (i.e. 2000) was
obtained for the word ?jingji? (economy) in Man-
darin Chinese. Each of these 2000 was examined
and all metaphorical instances were marked. (A
metaphorical instance is defined as when an ab-
stract concept such as ?economy?  is discussed in
terms of a concrete concept, such as ?building? .)
All instances of concrete concepts were then
grouped into source domains. All source-target
domain pairings that had more than 20 instances
were then examined. In Tables 1-4 below we show
the source domains that were found for jingji
?economy?  and we give the total number of in-
stances and the number of tokens for each meta-
phor, as well as a proposed mapping principle
based. Also note that the following mappings were
manually analyzed and classified.
We first note that the EP (empirical proto-
type) hypothesis holds up since in all source-target
domain pairings except for in ECONOMY IS
WAR in Table 4. In the remaining three meta-
phors, there is one or two lexical items that is/are
obviously more frequent than the others.
Table 1: ECONOMY IS A PERSON (121 instances)
M.P.: Economy is person because people have a life
cycle and economy has growth cycle.
Metaphor Freq.
Entities Chen2zhang3 (growth) 67
Shuai1tui4 (regres-
sion/decay)
8
Chen2zhang3chi2 (growth
period)
2
Bing4zhuang4 (symptoms) 1
Ming4ma4i (lifeblood) 2
Quality Shuai1tui2 (weaken and de-
generate)
1
Functions Chen2zhang3 (grow) 21
Shuai1tui4 (to become
weaker)
5
Fu4shu1 (regain conscious-
ness)
9
E4hua4 (deteriorate) 4
Hui1fu4 (recover) 1
Thus, for ECONOMY IS A PERSON, the map-
ping principle is postulated to have to do with the
life cycle of a person (and not, for example, the
mental health of a person) because of the frequent
occurrence of the lexical item ?chengzhang?
(growth).
Table 2: ECONOMY IS A BUILDING (102 in-
stances)
M.P.: Economy is building because buildings involve a
(physical) structure and economy involves an (abstract)
structure.
Metaphors Frequency
Entities jianshe (construction) 39
jiegou (structure) 20
jiqu (foundation) 15
zhichu (pillar) 1
genji (foundation) 2
guimo (model) 5
chuxing (model) 1
Qualities wengu (firm) 2
wending (stable) 8
Functions chongjian (re-build) 9
In the case of ECONOMY IS A BUILDING the
mapping principle is postulated to having to do
with structure, and not for example, leaky plumb-
ing. This is an interesting case because, as men-
tioned above, Ahrens (2002) examined IDEA IS A
BUILDING and postulated that the mapping prin-
ciple also had to do with structure (i.e the structure
of a building and the structure of ideas). As Ahrens
(2002) points out, it is not always the case that dif-
ferent target domains use the same aspect of a
source domain. For example, the source domain of
FOOD is used differently for IDEAS (to express
the notion of digestion and processing) as com-
pared with LOVE which uses FOOD to compare
different tastes to different feelings.
Table 3: ECONOMY IS A COMPETITION (40 in-
stances)
M.P.: Economy is competition because a competition
involves physical and mental strength to defeat an op-
ponent and an economy requires financial strength in
order to prosper against other economies.
Metaphors Frequency
Entities shili (actual strength) 14
jingzheng (competition) 12
jingzhengyoushi (advantage
in competition)
3
ruozhe (the weak one) 2
jingzhengli (power of com-
petition)
3
ruoshi (a disadvantaged
situation)
1
qiangguo (a powerful nation) 1
douzheng  (a struggle) 2
tuishi (a declining tendency) 1
Function shuaibai (to lose) 1
Thus, for ECONOMY IS A COMPETITION, the
emphasis is on the strength of participant in order
to defeat the opponent.
Table 4: ECONOMY IS WAR (23 instances)
M.P.: Economy is war because war involves a violent
contest for territorial gain and the economy involves a
vigorous contest for financial gain.
Metaphors Frequency
Entities qinglue (invasion) 4
zhan (battle) 2
laobing (veteran) 1
gungfangzhan (defend and
attack battle)
1
chelue (tactics) 1
daquan (immense power) 4
Qualities qianchuangbaikong (one
thousand boils and a hundred
holes; holes all over)
1
Functions quanlichongchi (to dash with
full force)
1
guashuai (to take command) 5
(daquan) chaozai shoushang
(to grasp the power)
1
xisheng (sacrifice) 1
Xishengping (victims) 1
In ECONOMY IS WAR, there is no clear-cut in-
stance of a frequent mapping. We suggest that this
is because WAR is a subset of the source domain
of COMPETITION (i.e. a violent contest) in the
SUMO representation, as discussed below.
In short, the corpora data support the CM
model? s hypothesis that there is a subset of lin-
guistic expressions within a particular source do-
main that map to a target domain. It is not the case
that ?anything goes? . In fact, the corpora data pre-
sented above, suggest an even more restricted view
? that there are usually one or two linguistic ex-
pressions that frequently map between the source
and target domains and ?drive?  the motivating re-
lationship between them. In the next section, we
look at whether or not the source domain knowl-
edge can be defined a priori through an upper on-
tology such as SUMO.
5 Defining Source Domain Knowledge
with Shared Upper Ontology
The research on Shared Upper Ontology offers a
potential answer to the challenge of how to define
and verify the structured knowledge in a source
domain. A shared upper ontology is designed to
represent the shared knowledge structure of intelli-
gent agents and allows knowledge exchange
among them. In computational application, it is an
infrastructure for knowledge engineering. In cog-
nitive terms, we can view it as a candidate for he
description of shared human knowledge. In this
paper, we adopt SUMO.
In SUMO, conceptual terms are defined
and situated in a tree-taxonomy. In addition, a set
of first order inference rules can be attached to
each conceptual node to represent the knowledge
content encoded on that term. The conceptual
terms of SUMO are roughly equivalent to the
source domains in MP theory. Hence the well-
defined SUMO conceptual terms are candidates for
knowledge representation of the source domain in
the MP theory of metaphor. In other words, SUMO
provides a possible answer the question of how
source domain knowledge is represented and how
does this knowledge allows the mapping in con-
ceptual metaphors. We examine how this might be
possible by looking at two conceptual terms that
are represented in SUMO that related to our source
domains ? CONTEST and ORGANISM.
Economy is Contest
First, we found that what we intuitively termed as
?competition?  above has a corresponding ontologi-
cal node of Contest. The term Contest is docu-
mented as ?A SocialInteraction where the agent
and patient are CognitiveAgents who are trying to
defeat one another.?  Its only inference rule is
quoted here:
 (=> (instance ?CONTEST Contest) (exists
(?AGENT1 ?AGENT2 ?PURP1 ?PURP2) (and
(agent ?CONTEST ?AGENT1) (agent ?CONTEST
?AGENT2) (hasPurposeForAgent ?CONTEST
?PURP1 ?AGENT1) (hasPurposeForAgent
?CONTEST ?PURP2 ?AGENT2) (not (equal
?AGENT1 ?AGENT2)) (not (equal ?PURP1
?PURP2)))))
The knowledge inference rule stipulates that each
instance of Contest is carried out by two agents,
each has his own non-equal purpose. This is ex-
actly the source knowledge needed for the meta-
phor mapping. When the conceptual metaphor is
linguistically realized, lexical expressions are then
chosen to represent the conceptual terms of both
purposeful agents, and conflicting purposes for the
agents. Notice that in contest, as in economy, it is
not necessary to have only one winner. There may
be multiple winners and perhaps no winners. In
other words, the agents? purpose may not be con-
flicting. But the purposes-for-agent are definitely
different for each agent.
In addition to the 40 instances of economy
metaphors involving contest. There are also 23
instances of metaphors involving War. In these
cases, it is interesting to observe that the central
concept is still the conflicting purposes (one? s gain
is another? s loss) of the warring party. This is con-
firmed by the shared ontology. In SUMO, a War is
a kind of ViolentContest, which in term is a kind
of Contest.
Contest?ViolentContest?War
The term War is defined as ?A military confronta-
tion between two or more Nations or Organizations
whose members are Nations.?  And the term Vio-
lentContest is defined as ?Contest where one par-
ticipant attempts to physically injure another
participant.?  As can be seen from the definition and
the metaphoric uses involving War, the ontological
source domain knowledge is not involved.
In fact, when examined more closely, it is
clear that when the domain knowledge of War is
used, it either further specifies the conflicting pur-
poses by elaborating on the quality and manner of
the conflict, or elaborating on the agent partici-
pants as combatants. In other words, Economy is
War is not a different mapping. It is subsumed un-
der the mapping of Economy is Contest, and added
elaborations on the participants.
By carefully examining the mapping from
source domain knowledge based on SUMO, we
discovered that not only mapping is indeed based
on a priori source domain knowledge. We also dis-
covered that a metaphor can often involve addi-
tional and more specified terms within a domain.
In these cases, no additional mapping is required.
The same structured domain knowledge is used,
and the subsumed terms offers only elaborations
based on the same knowledge structure.
It is appropriate to note here that based on
WordNet to SUMO mapping, economy is a So-
cialInteraction, and Contest is a subclass of So-
cialInteraction. In other words, economy is a
related concept to Contest, although it does not
belong to that conceptual domain. That a metaphor
chooses a related domain is to be expected.
Economy is Organism
Among metaphors involving economies,
one source domain stands out as being far removed
conceptually. These are the metaphors involving
Organism. We arrived at this conclusion by re-
examining the examples that we generalized as
Economy is a Person in the previous section. After
closer examination with the help of SUMO knowl-
edge representation, we found that the linguistic
realizations of this mapping do not involve any
knowledge that is specific to Human. In fact, it
only involves the notion of a life cycle, which is
the defining knowledge involving an Organism.
Organism is defined in SUMO as ?a living
individual, including all Plants and Animals.?  And
the crucial knowledge encoded in of the attached
inference rules follows:
=> (and (instance ?ORGANISM Organism) (agent
?PROCESS ?ORGANISM)) (holdsDuring
(WhenFn ?PROCESS) (attribute ?ORGANISM
Living)))
The above inference rule encodes the knowledge
that ?An organism is the agent of a living process
that holds over a duration.?  In other words, having
a life cycle is the defining knowledge of an Or-
ganism. This turns out to be the source domain
knowledge that is involved in the mapping.
It is interesting to observe, though this is
not encoded by SUMO, that from a Darwinian per-
spective, the Purpose of an Organism as an Agent
is to prolong his own life cycle. We found that in
actual linguistic data, when the above two meta-
phors are used simultaneously, it is only when im-
proving the life cycle (Economy is Organism) is
incorporated as the PurposeForAgent (Economy is
Contest). In other words, the source domain
knowledge is robust in conceptual metaphor and
can be automatically mapped to and merged.
6 Conclusion
In this paper, we propose an ontology-based and
corpus-driven approach towards predicting lexical
meaning of conceptual metaphors. Our theory is
thus formally constrained. We also verified our
findings with examination of corpora data. In the
final version of this paper, we will demonstrate
how the process of establishing mapping principles
and deriving metaphorical meaning can be semi-
automaticized based on both the SUMO ontologi-
cal databases and corpora data. Such a process has
important implications both in cognitive explana-
tion of conceptual metaphors and in the application
of SUMO to predict figurative meaning in meta-
phorical uses.
Acknowledgments
This study is partially supported both by a NSC
project ?Sense and Sense-Ability?, as well as a
NDAP project ?Linguistic Anchoring.? We would
like to thank Adam Pease of Teknowledge, the
ACL workshop reviewers, as well as colleagues of
the two above projects, for their comments. Any
remaining errors are our own.
References
Ahrens, K. 2002. When Love is not Digested: Un-
derlying Reasons for Source to Target Domain
Pairing in the Contemporary Theory of Meta-
phor. In YuChau E. Hsiao (ed.) Proceeding of
the First Cognitive Linguistics Conference, pp
273-302. Taipei: Cheng-Chi University.
Farrar, S., Lewis, W., and Langendoen, T. 2002. A
Common Ontology for Linguistic Concepts. In
Proceedings of the Knowledge Technologies
Conference, Seattle, Washington, March 10-13,
2002. (available at
http://ontology.teknowledge.com/#pubs)
Gentner, D. and G. Wolff. 2000. ?Evidence for
Role-Neutral Initial Processing of Metaphors.?
Journal of Experimental Psychology, 26, 529-
541.
Lakoff, G. 1993. ?The Contemporary Theory of
Metaphor.? In Andrew Ortony (ed.) Metaphor
and Thought (2nd ed.). Cambridge: Cambridge
University Press. P 202-251.
Martin J., 1992. Metabank: a Knowledge Base of
Metaphoric Language Conventions. Computa-
tional Intelligence, 10, pg. 134-149.
McGlone, M. S. 1996. ?Conceptual Metaphors and
Figurative Language Interpretation: Food for
Thought?? Journal of Memory and Language,
35, 544-565.
Niles, I. 2003. Mapping WordNet to the SUMO
Ontology. Teknowledge Technical Report.
Niles, I., & Pease, A. 2001. Toward a Standard
Upper Ontology. Proceedings of the 2nd Interna-
tional Conference on Formal Ontology in In-
formation Systems (FOIS-2001).
Pease, A. & Niles, I. 2002. IEEE Standard Upper
Ontology: A Progress Report. Knowledge Engi-
neering Review, Special Issue on Ontology and
Agents, Volume 17.
Sevcenko, M. 2003. Online Presentation of an Up-
per Ontology. In Proceedings of Znalosti 2003,
Ostrava, Czech Republic, February 19-21, 2003.
On-line Resources
Academia Sinica Balanced Corpus
http://www.sinica.edu.tw/SinicaCorpus/
English-Chinese Ontology/WordNet Interface
(http://ckip.iis.sinica.edu.tw/CKIP/ontology/).
SUMO (Suggested Upper Merged Ontology)
http://ontology.teknowledge.com)
Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards Agent-based Cross-lingual Interoperability of Distributed    
Lexical Resources 
Claudia Soria* Maurizio Tesconi? Andrea Marchetti?
Francesca Bertagna* Monica Monachini*
Chu-Ren Huang?    Nicoletta Calzolari*
*CNR-ILC and ?CNR-IIT 
Via Moruzzi 1, 56024 Pisa 
Italy 
{firstname.lastname@ilc.cnr.it} 
{firstname.lastname@iit.cnr.it} 
?Academia Sinica  
Nankang, Taipei  
Taiwan 
churen@gate.sinica.edu.tw 
 
  
 
Abstract 
In this paper we present an application 
fostering the integration and interopera-
bility of computational lexicons, focusing 
on the particular case of mutual linking 
and cross-lingual enrichment of two wor-
dnets, the ItalWordNet and Sinica BOW 
lexicons. This is intended as a case-study 
investigating the needs and requirements 
of semi-automatic integration and inter-
operability of lexical resources. 
1 Introduction 
In this paper we present an application fostering 
the integration and interoperability of computa-
tional lexicons, focusing on the particular case of 
mutual linking and cross-lingual enrichment of 
two wordnets. The development of this applica-
tion is intended as a case-study and a test-bed for 
trying out needs and requirements posed by the 
challenge of semi-automatic integration and en-
richment of practical, large-scale multilingual 
lexicons for use in computer applications. While 
a number of lexicons already exist, few of them 
are practically useful, either since they are not 
sufficiently broad or because they don?t cover 
the necessary level of detailed information. 
Moreover, multilingual language resources are 
not as widely available and are very costly to 
construct: the work process for manual develop-
ment of new lexical resources or for tailoring 
existing ones is too expensive in terms of effort 
and time to be practically attractive.  
The need of ever growing lexical resources for 
effective multilingual content processing has 
urged the language resource community to call 
for a radical change in the perspective of lan-
guage resource creation and maintenance and the 
design of a ?new generation? of LRs: from static, 
closed and locally developed resources to shared 
and distributed language services, based on open 
content interoperability standards. This has often 
been called a ?change in paradigm? (in the sense 
of Kuhn, see Calzolari and Soria, 2005; Calzolari 
2006). Leaving aside the tantalizing task of 
building on-site resources, the new paradigm 
depicts a scenario where lexical resources are 
cooperatively built as the result of controlled co-
operation of different agents, adopting the para-
digm of accumulation of knowledge so success-
ful in more mature disciplines, such as biology 
and physics (Calzolari, 2006).  
According to this view (or, better, this vision), 
different lexical resources reside over distributed 
places and can not only be accessed but choreo-
graphed by agents presiding the actions that can 
be executed over them. This implies the ability to 
build on each other achievements, to merge re-
sults, and to have them accessible to various sys-
tems and applications. 
At the same time, there is another argument in 
favor of distributed lexical resources: language 
resources, lexicons included, are inherently dis-
tributed because of the diversity of languages 
distributed over the world. It is not only natural 
that language resources to be developed and 
maintained in their native environment. Since 
language evolves and changes over time, it is not 
possible to describe the current state of the lan-
17
guage away from where the language is spoken. 
Lastly, the vast range of diversity of languages 
also makes it impossible to have one single uni-
versal centralized resource, or even a centralized 
repository of resources. 
Although the paradigm of distributed and in-
teroperable lexical resources has largely been 
discussed and invoked, very little has been made 
in comparison for the development of new meth-
ods and techniques for its practical realization. 
Some initial steps are made to design frame-
works enabling inter-lexica access, search, inte-
gration and operability. An example is the Lexus 
tool (Kemps-Snijders et al, 2006), based on the 
Lexical Markup Framework (Romary et al, 
2006), that goes in the direction of managing the 
exchange of data among large-scale lexical re-
sources. A similar tool, but more tailored to the 
collaborative creation of lexicons for endangered 
language, is SHAWEL (Gulrajani and Harrison, 
2002). However, the general impression is that 
little has been made towards the development of 
new methods and techniques for attaining a con-
crete interoperability among lexical resources. 
Admittedly, this is a long-term scenario requiring 
the contribution of many different actors and ini-
tiatives (among which we only mention stan-
dardisation, distribution and international coop-
eration).  
Nevertheless, the intent of our project is to 
contribute to fill in this gap, by exploring in a 
controlled way the requirement and implications 
posed by new generation multilingual lexical 
resources. The paper is organized as follows: 
section 2 describes the general architectural de-
sign of our project; section 3 describes the mod-
ule taking care of cross-lingual integration of 
lexical resources, by also presenting a case-study 
involving an Italian and Chinese lexicons. Fi-
nally, section 4 presents our considerations and 
lessons learned on the basis of this exploratory 
testing. 
2 An Architecture for Integrating Lexi-
cal Resources 
 LeXFlow (Soria et al, 2006) was developed 
having in mind the long-term goal of lexical re-
source interoperability. In a sense, LeXFlow is 
intended as a proof of concept attempting to 
make the vision of an infrastructure for access 
and sharing of linguistic resources more tangible. 
LeXFlow is an adaptation to computational 
lexicons of XFlow, a cooperative web applica-
tion for the management of document workflows 
(DW, Marchetti et al, 2005). A DW can be seen 
as a process of cooperative authoring where a 
document can be the goal of the process or just a 
side effect of the cooperation. Through a DW, a 
document life-cycle is tracked and supervised, 
continually providing control over the actions 
leading to document compilation. In this envi-
ronment a document travels among agents who 
essentially carry out the pipeline receive-process-
send activity.  
There are two types of agents: external agents 
are human or software actors performing activi-
ties dependent from the particular Document 
Workflow Type; internal agents are software 
actors providing general-purpose activities useful 
for many DWTs and, for this reason, imple-
mented directly into the system. Internal agents 
perform general functionalities such as creat-
ing/converting a document belonging to a par-
ticular DW, populating it with some initial data, 
duplicating a document to be sent to multiple 
agents, splitting a document and sending portions 
of information to different agents, merging du-
plicated documents coming from multiple agents, 
aggregating fragments, and finally terminating 
operations over the document. External agents 
basically execute some processing using the 
document content and possibly other data; for 
instance, accessing an external database or 
launching an application.  
LeXFlow was born by tailoring XFlow to 
management of lexical entries; in doing so, we 
have assumed that each lexical entry can be 
modelled as a document instance, whose behav-
iour can be formally specified by means of a 
lexical workflow type (LWT). A LWT describes 
the life-cycle of a lexical entry, the agents al-
lowed to act over it, the actions to be performed 
by the agents, and the order in which the actions 
are to be executed. Embracing the view of coop-
erative workflows, agents can have different 
rights or views over the same entry: this nicely 
suits the needs of lexicographic work, where we 
can define different roles (such as encoder, anno-
tator, validator) that can be played by either hu-
man or software agents. Other software modules 
can be inserted in the flow, such as an automatic 
acquirer of information from corpora or from the 
web. Moreover, deriving from a tool designed 
for the cooperation of agents, LeXFlow allows to 
manage workflows where the different agents 
can reside over distributed places.  
LeXFlow thus inherits from XFlow the gen-
eral design and architecture, and can be consid-
ered as a specialized version of it through design 
18
of specific Lexical Workflow Types and plug-in 
of dedicated external software agents. In the next 
section we briefly illustrate a particular Lexical 
Workflow Type and the external software agents 
developed for the purpose of integrating different 
lexicons belonging to the same language. Since it 
allows the independent and coordinated sharing 
of actions over portions of lexicons, LeXFlow 
naturally lends itself as a tool for the manage-
ment of distributed lexical resources. 
Due to its versatility, LeXFlow is both a gen-
eral framework where ideas on automatic lexical 
resource integration can be tested and an infra-
structure for proving new methods for coopera-
tion among lexicon experts. 
2.1 Using LeXFlow for Lexicon Enrichment 
In previous work (Soria et al, 2006),  the LeX-
Flow framework has been tested for integration 
of lexicons with differently conceived lexical 
architectures and diverging formats. It was 
shown how interoperability is possible between 
two Italian lexicons from the SIMPLE and 
WordNet families, respectively, namely the 
SIMPLE/CLIPS (Ruimy et al, 2003) and Ital-
WordNet (Roventini et al, 2003) lexicons.  
In particular, a Lexical Workflow Type was 
designed where the two different monolingual 
semantic lexicons interact by reciprocally enrich-
ing themselves and moreover integrate informa-
tion coming from corpora. This LWT, called 
?lexicon augmentation?, explicitly addresses dy-
namic augmentation of semantic lexicons. In this 
scenario, an entry of a lexicon A becomes en-
riched via basically two steps. First, by virtue of 
being mapped onto a corresponding entry be-
longing to a lexicon B, the entryA inherits the 
semantic relations available in the mapped en-
tryB. Second, by resorting to an automatic appli-
cation that acquires information about semantic 
relations from corpora, the acquired relations are 
integrated into the entry and proposed to the hu-
man encoder. 
B
An overall picture of the flow is shown in 
Figure 1, illustrating the different agents partici-
pating in the flow. Rectangles represent human 
actors over the entries, while the other figures 
symbolize software agents: ovals are internal 
agents and octagons external ones. The two ex-
ternal agents involved in this flow are the ?rela-
tion calculator? and the ?corpora extractor?. The 
first is responsible for the mapping between the 
sets of semantic relations used by the different 
lexicons. The ?corpora extractor? module in-
vokes an application that acquires information 
about part-of relations by identifying syntactic 
constructions in a vast Italian corpus. It then 
takes care of creating the appropriate candidate 
semantic relations for each lemma that is pro-
posed by the application. 
Figure 1. Lexicons Augmentation Workflow 
Type. 
A prototype of LeXFlow has been imple-
mented with an extensive use of XML technolo-
gies (XML Schema, XSLT, XPath, XForms, 
SVG) and open-source tools (Cocoon, Tomcat, 
mySQL). It is a web-based application where 
human agents interact with the system through 
an XForms browser that displays the document 
to process as a web form whereas software 
agents interact with the system via web services. 
3 Multilingual WN Service 
In the Section above we have illustrated the gen-
eral architecture of LeXFlow and showed how a 
Lexical Workflow Type can be implemented in 
order to enrich already existing lexicons belong-
ing to the same language but realizing different 
models of lexicon encoding. In this section we 
move to a cross-lingual perspective of lexicon 
integration. We present a module that similarly 
addresses the issue of lexicon augmentation or 
enrichment focusing on mutual enrichment of 
two wordnets in different languages and residing 
at different sites. 
This module, named ?multilingual WN Ser-
vice? is responsible for the automatic cross-
lingual fertilization of lexicons having a Word-
19
Net-like structure. Put it very simply, the idea 
behind this module is that a monolingual word-
net can be enriched by accessing the semantic 
information encoded in corresponding entries of 
other monolingual wordnets.  
Since each entry in the monolingual lexicons 
is linked to the Interlingual Index (ILI, cf. Sec-
tion 3.1), a synset of a WN(A) is indirectly 
linked to another synset in another WN(B). On 
the basis of this correspondence, a synset(A) can 
be enriched by importing the relations that the 
corresponding synset(B) holds with other syn-
sets(B), and vice-versa. Moreover, the enrich-
ment of WN(A) will not only import the relations 
found in WN(B), but it will also propose target 
synsets in the language(A) on the basis of those 
found in language(B). 
The various WN lexicons reside over distrib-
uted servers and can be queried through web ser-
vice interfaces. The overall architecture for mul-
tilingual wordnet service is depicted in Figure 2. 
 
 
Figure 2. Multilingual Wordnet Service Archi-
tecture. 
 
Put in the framework of the general LeXFlow 
architecture, the Multilingual wordnet Service 
can be seen as an additional external software 
agent that can be added to the augmentation 
workflow or included in other types of lexical 
flows. For instance, it can be used not only to 
enrich a monolingual lexicon but to bootstrap a 
bilingual lexicon. 
3.1 Linking Lexicons through the ILI  
The entire mechanism of the Multilingual WN 
Service is based on the exploitation of Interlin-
gual Index (Peters et al, 1998), an unstructured 
version of WordNet used in EuroWordNet 
(Vossen et al, 1998) to link wordnets of different 
languages; each synset in the language-specific 
wordnet is linked to at least one record of the ILI 
by means of a set of equivalence relations 
(among which the most important is the 
EQ_SYNONYM, that expresses a total, perfect 
equivalence between two synsets).  
Figure 6 describes the schema of a WN lexical 
entry. Under the root ?synset? we find both in-
ternal relations (?synset relations?) and ILI Rela-
tions, which link to ILI synsets. 
Figure 3 shows the role played by the ILI as 
set of pivot nodes allowing the linkage between 
concepts belonging to different wordnets.  
 
 
Figure 3. Interlingual Linking of Language-
specific Synsets. 
 
In the Multilingual WN Service, only equiva-
lence relations of type EQ_SYNONYM and 
EQ_NEAR_SYNONYM have been taken into ac-
count, being them the ones used to represent a 
translation of concepts and also because they are 
the most exploited (for example, in IWN, they 
cover about the 60% of the encoded equivalence 
relations). The EQ_SYNONYM relation is used to 
realize the one-to-one mapping between the lan-
guage-specific synset and the ILI, while multiple 
EQ_NEAR_SYNONYM relations (because of their 
nature) might be encoded to link a single lan-
guage-specific synset to more than one ILI re-
cord. In Figure 4 we represented the possible 
relevant combinations of equivalence relations 
that can realize the mapping between synsets 
belonging to two languages. In all the four cases, 
a synset ?a? is linked via the ILI record to a syn-
set ?b? but a specific procedure has been fore-
seen in order to calculate different ?plausibility 
scores? to each situation. The procedure relies on 
different rates assigned to the two equivalence 
relations (rate ?1? to EQ_NEAR_SYNONYM rela-
tion and rate ?0? to the EQ_SYNONYM). In this 
way we can distinguish the four cases by assign-
ing respectively a weight of ?0?, ?1?, ?1? and 
?2?. 
20
  
Figure 4. Possible Combinations of Relations 
between two Lexicons A and B and the ILI. 
 
The ILI is a quite powerful yet simple method 
to link concepts across the many lexicons be-
longing to the WordNet-family. Unfortunately, 
no version of the ILI can be considered a stan-
dard and often the various lexicons exploit dif-
ferent version of WordNet as ILI 1 . This is a 
problem that is handled at web-service level, by 
incorporating the conversion tables provided by 
(Daud? et al, 2001). In this way, the use of dif-
ferent versions of WN does not have to be taken 
into consideration by the user who accesses the 
system but it is something that is resolved by the 
system itself2. This is why the version of the ILI 
is a parameter of the query to web service (see 
Section below). 
3.2 Description of the Procedure 
On the basis of ILI linking, a synset can be en-
riched by importing the relations contained in the 
corresponding synsets belonging to another 
wordnet. 
In the procedure adopted, the enrichment is 
performed on a synset-by-synset basis. In other 
words, a certain synset is selected from a word-
net resource, say WN(A). The cross-lingual mod-
ule identifies the corresponding ILI synset, on 
the basis of the information encoded in the syn-
set. It then sends a query to the WN(B) web ser-
vice providing the ID of ILI synset together with 
the ILI version of the starting WN. The WN(B) 
web service returns the synset(s) corresponding 
to the WN(A) synset, together with reliability 
scores. If WN(B) is based on a different ILI ver-
sion, it can carry out the mapping between ILI 
versions (for instance by querying the ILI map-
ping web service). The cross-lingual module then 
analyzes the synset relations encoded in the 
                                                 
1 For example, the Chinese and the Italian wordnets consid-
ered as our case-study use respectively versions 1.6 and 1.5. 
2 It should be noted, however, that the conversion between 
different WN versions could not be accurate so the mapping 
is always proposed with a probability score.
WN(B) synset and for each of them creates a 
new synset relation for the WN(A) synset. 
If the queried wordnets do not use the same set 
of synset relations, the module must take care of 
the mapping between different relation sets. In  
our case-study no mapping was needed, since the 
two sets were completely equivalent.   
Each new relation is obtained by substituting 
the target WN(B)  synset  with the corresponding 
synset WN(A), which again is found by querying 
back the WN(A) web service (all these steps 
through the ILI). The procedure is formally de-
fined by the following formula: 
 
 
 
 
 
Figure 5. Finding New Relations. 
 
Every local wordnet has to provide a web ser-
vice API  with the following methods: 
 
1. GetWeightedSynsetsByIli(ILIid, ILIversion) 
2. GetSynsetById(sysnsetID) 
3. GetSynsetsByLemma(lemma) 
 
21
The returned synsets of each method must be 
formatted in XML following the schema de-
picted in Figure 6: 
 
Figure 6. Schema of Wordnet Synsets Returned 
by WN Web Services. 
 
The scores returned by the method ?Get-
WeightedSynsetsByIli? are used by our module 
to calculate the reliability rating for each new 
proposed relation. 
3.3 A Case Study: Cross-fertilization be-
tween Italian and Chinese Wordnets. 
We explore this idea with a case-study involving 
the ItalianWordNet (Roventini et al, 2003) and 
the Academia Sinica Bilingual Ontological 
Wordnet (Sinica BOW, Huang et al, 2004).  
The BOW integrates three resources: Word-
Net, English-Chinese Translation Equivalents 
Database (ECTED), and SUMO (Suggested Up-
per Merged Ontology). With the integration of 
these three key resources, Sinica BOW functions 
both as an English-Chinese bilingual wordnet 
and a bilingual lexical access to SUMO. Sinica 
Bow currently has two bilingual versions, corre-
sponding to WordNet 1.6. and 1.7. Based on 
these bootstrapped versions, a Chinese Wordnet 
(CWN, Huang et al 2005) is under construction 
with handcrafted senses and lexical semantic re-
lations. For the current experiment, we have used 
the version linking to WordNet 1.6. 
ItalWordNet was realized as an extension of 
the Italian component of EuroWordNet. It com-
prises a general component consisting of about 
50,000 synsets and terminological wordnets 
linked to the generic wordnet by means of a spe-
cific set of relations. Each synset of ItalWordNet 
is linked to the Interlingual-Index (ILI). 
The two lexicons refer to different versions of 
the ILI (1.5 for IWN and 1.6 for BOW), thus 
making it necessary to provide a mapping be-
tween the two versions. On the other hand, no 
mapping is necessary for the set of synset rela-
tions used, since both of them adopt the same set. 
For the purposes of evaluating the cross-
lingual module, we have developed two web-
services for managing a subset of the two re-
sources.  
The following Figure shows a very simple ex-
ample where our procedure discovers and pro-
poses a new meronymy relation for the Italian 
synset {passaggio,strada,via}. This synset is 
equivalent to the ILI ?road,route? that is ILI-
connected with BOW synset ???,? ,?? (da-
o_lu, dao, lu) (Figure 7, A) . The Chinese synset 
has a meronymy relation with the synset ???
??? (wan) (B). This last  synset is equivalent 
to the ILI ?bend, crook, turn? that is ILI-
connected with Italian WordNet synset ?curva-
tura, svolta, curva? (C). Therefore the procedure 
will propose a new candidate meronymy relation 
between the two Italian WordNet synsets (D). 
 
 
Figure 7. Example of a New Proposed Mero-
nymy Relation for Italian. 
3.4 Considerations and Lessons Learned 
Given the diversity of the languages for which 
wordnets exist, we note that it is difficult to im-
plement an operational standard across all typo-
logically different languages. Work on enriching 
and merging multilingual resources presupposes 
that the resources involved are all encoded with 
the same standard. However, even with the best 
efforts of the NLP community, there are only a 
small number of language resources encoded in 
any given standard. In the current work, we pre-
suppose a de-facto standard, i.e. a shared and 
conventionalized architecture, the WordNet one. 
Since the WordNet framework is both conven-
tionalized and widely followed, our system is 
22
able to rely on it without resorting to a more sub-
stantial and comprehensive standard. In the case, 
for instance, of integration of lexicons with dif-
ferent underlying linguistic models, the availabil-
ity of the MILE (Calzolari et al, 2003) was an 
essential prerequisite of our work. Nevertheless, 
even from the perspective of the same model, a 
certain degree of standardization is required, at 
least at the format level. 
From a more general point of view, and even 
from the perspective of a limited experiment 
such as the one described in this paper, we must 
note that the realization of the new vision of dis-
tributed and interoperable language resources is 
strictly intertwined with at least two prerequi-
sites. On the one side, the language resources 
need to be available over the web; on the other, 
the language resource community will have to 
reconsider current distribution policies, and to 
investigate the possibility of developing an 
?Open Source? concept for LRs. 
4 Conclusion 
Our proposal to make distributed wordnets inter-
operable has the following applications in proc-
essing of lexical resources: 
 
? Enriching existing resources: informa-
tion is often not complete in any given 
wordnet: by making two wordnets inter-
operable, we can bootstrap semantic rela-
tions and other information from other 
wordnets. 
? Creation of new resources: multilingual 
lexicons can be bootstrapped by linking 
different language wordnets through ILI. 
? Validation of existing resources: seman-
tic relation information and other synset 
assignments can be validated when it is re-
inforced by data from a different wordnet. 
In particular, our work can be proposed as a 
prototype of a web application that would sup-
port the Global WordNet Grid initiative 
(www.globalwordnet.org/gwa/gwa_grid.htm).  
Any multilingual process, such as cross-
lingual information retrieval, must involve both 
resources and tools in a specific language and 
language pairs. For instance, a multilingual query 
given in Italian but intended for querying Eng-
lish, Chinese, French, German, and Russian 
texts, can be send to five different nodes on the 
Grid for query expansion, as well as performing 
the query itself. In this way, language specific 
query techniques can be applied in parallel to 
achieve best results that can be integrated in the 
future. As multilingualism clearly becomes one 
of the major challenges of the future of web-
based knowledge engineering, WordNet emerges 
as one leading candidate for a shared platform 
for representing a lexical knowledge model for 
different languages of the world. This is true 
even if it has to be recognized that the wordnet 
model is lacking in some important semantic in-
formation (like, for instance, a way to represent 
the semantic predicate). However, such knowl-
edge and resources are distributed. In order to 
create a shared multi-lingual knowledge base for 
cross-lingual processing based on these distrib-
uted resources, an initiative to create a grid-like 
structure has been recently proposed and pro-
moted by the Global WordNet Association, but 
until now has remained a wishful thinking. The 
success of this initiative will depend on whether 
there will be tools to access and manipulate the 
rich internal semantic structure of distributed 
multi-lingual WordNets. We believe that our 
work on LeXFlow offers such a tool to provide 
inter-operable web-services to access distributed 
multilingual WordNets on the grid. 
This allows us to exploit in a cross-lingual 
framework the wealth of monolingual lexical 
information built in the last decade. 
5 References 
Nicoletta Calzolari, Francesca Bertagna, Alessandro 
Lenci and Monica Monachini, editors. 2003. Stan-
dards and Best Practice for Multilingual Computa-
tional Lexicons. MILE (the Multilingual ISLE 
Lexical Entry). ISLE CLWG Deliverable D2.2 & 
3.2. Pisa. 
Nicoletta Calzolari and Claudia Soria. 2005. A New 
Paradigm for an Open Distributed Language Re-
source Infrastructure: the Case of Computational 
Lexicons. In Proceedings of the AAAI Spring Sym-
posium ?Knowledge Collection from Volunteer 
Contributors (KCVC05)?, pages 110-114, Stan-
ford, CA. 
Nicoletta Calzolari. 2006. Technical and Strategic 
issues on Language Resources for a Research In-
frastructure In Proceedings of the International 
Symposium on Large-scale Knowledge Resources 
(LKR2006), pages 53-58, Tokyo, Tokyo Institute 
of Technology. 
Jordi Daud?, Lluis Padr? and German Rigau. 2001. A 
Complete WN1.5 to WN1.6 Mapping. In Proceed-
ings of NAACL Workshop "WordNet and Other 
Lexical Resources: Applications, Extensions and 
23
Customizations", pages 83-88, Pittsburg, PA, USA, 
Association for Computational Linguistics.  
Greg Gulrajani and David Harrison. 2002. SHAWEL: 
Sharable and Interactive Web-Lexicons. In Pro-
ceedings of the LREC2002 Workshop on Tools and 
Resources in Field Linguistics, pages 1-4, Las 
Palmas, Canary Islands, Spain. 
Chu-Ren Huang, Ru-Yng Chang,  and Shiang-Bin 
Lee. 2004. Sinica BOW (Bilingual Ontological 
Wordnet): Integration of Bilingual WordNet and 
SUMO. In Proceedings of LREC2004, pages 1553-
1556, Lisbon, Portugal. 
Chu-Ren Huang, Chun-Ling Chen, Cui-Xia Weng, 
Hsiang-Ping Lee, Yong-Xiang Chen and Keh-jiann 
Chen. 2005. The Sinica Sense Management Sys-
tem: Design and Implementation. Computational 
Linguistics and Chinese Language Processing. 
10(4): 417-430. 
Marc Kemps-Snijders, Mark-Jan Nederhof, and Peter 
Wittenburg. 2006. LEXUS, a web-based tool for 
manipulating lexical resources. Accepted for publi-
cation in Proceedings of LREC2006, Genoa, Italy. 
Andrea Marchetti, Maurizio Tesconi, and Salvatore 
Minutoli. 2005. XFlow: An XML-Based Docu-
ment-Centric Workflow. In Proceedings of 
WISE?05, pages 290-303, New York, NY, USA. 
Wim Peters, Piek Vossen, Pedro Diez-Orzas, and 
Geert Adriaens. 1998. Cross-linguistic Alignment 
of Wordnets with an Inter-Lingual-Index. In Nancy 
Ide, Daniel Greenstein, and Piek Vossen, editors, 
Special Issue on EuroWordNet, Computers and the 
Humanities, 32(2-3): 221-251. 
Laurent Romary, Gil Francopoulo, Monica Monachi-
ni, and Susanne Salmon-Alt 2006. Lexical Markup 
Framework (LMF): working to reach a consensual 
ISO standard on lexicons. Accepted for publication 
in Proceedings of LREC2006, Genoa, Italy. 
Adriana Roventini, Antonietta Alonge, Francesca 
Bertagna, Nicoletta Calzolari, Christian Girardi, 
Bernardo Magnini, Rita Marinelli, and Antonio 
Zampolli. 2003. ItalWordNet: Building a Large 
Semantic Database for the Automatic Treatment of 
Italian. In Antonio Zampolli, Nicoletta Calzolari, 
and Laura Cignoni, editors, Computational Lingui-
stics in Pisa, IEPI, Pisa-Roma, pages 745-791. 
Nilda Ruimy, Monica Monachini, Elisabetta Gola, 
Nicoletta Calzolari, Cristina Del Fiorentino, Marisa 
Ulivieri, and Sergio Rossi. 2003. A Computational 
Semantic Lexicon of Italian: SIMPLE. In Antonio 
Zampolli, Nicoletta Calzolari, and Laura Cignoni, 
editors, Computational Linguistics in Pisa, IEPI, 
Pisa-Roma, pages 821-864. 
Claudia Soria, Maurizio Tesconi, Francesca Bertagna, 
Nicoletta Calzolari, Andrea Marchetti, and Monica 
Monachini. 2006. Moving to Dynamic Computa-
tional Lexicons with LeXFlow. Accepted for pu-
blication in Proceedings of LREC2006, Genova, I-
taly.  
Piek Vossen. 1998. Introduction to EuroWordNet. In 
Nancy Ide, Daniel Greenstein, and Piek Vossen, 
editors, Special Issue on EuroWordNet, Computers 
and the Humanities, 32(2-3): 73-89. 
 
 
 
 
24
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 692?700,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Framework of Feature Selection Methods for Text Categorization 
 
 
Shoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang1 
 
1
 Department of Chinese and Bilingual 
Studies 
The Hong Kong Polytechnic University 
{shoushan.li,churenhuang} 
@gmail.com 
 
2
 National Laboratory of Pattern 
Recognition 
 Institute of Automation 
 Chinese Academy of Sciences  
{rxia,cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
In text categorization, feature selection (FS) is 
a strategy that aims at making text classifiers 
more efficient and accurate. However, when 
dealing with a new task, it is still difficult to 
quickly select a suitable one from various FS 
methods provided by many previous studies. 
In this paper, we propose a theoretic 
framework of FS methods based on two basic 
measurements: frequency measurement and 
ratio measurement. Then six popular FS 
methods are in detail discussed under this 
framework. Moreover, with the guidance of 
our theoretical analysis, we propose a novel 
method called weighed frequency and odds 
(WFO) that combines the two measurements 
with trained weights. The experimental results 
on data sets from both topic-based and 
sentiment classification tasks show that this 
new method is robust across different tasks 
and numbers of selected features.  
1 Introduction 
With the rapid growth of online information, text 
classification, the task of assigning text 
documents to one or more predefined categories, 
has become one of the key tools for 
automatically handling and organizing text 
information. 
The problems of text classification normally 
involve the difficulty of extremely high 
dimensional feature space which sometimes 
makes learning algorithms intractable. A 
standard procedure to reduce the feature 
dimensionality is called feature selection (FS). 
Various FS methods, such as document 
frequency (DF), information gain (IG), mutual 
information (MI), 2? -test (CHI), Bi-Normal 
Separation (BNS), and weighted log-likelihood 
ratio (WLLR), have been proposed for the tasks 
(Yang and Pedersen, 1997; Nigam et al, 2000; 
Forman, 2003) and make text classification more 
efficient and accurate. 
However, comparing these FS methods 
appears to be difficult because they are usually 
based on different theories or measurements. For 
example, MI and IG are based on information 
theory, while CHI is mainly based on the 
measurements of statistic independence. 
Previous comparisons of these methods have 
mainly depended on empirical studies that are 
heavily affected by the experimental sets. As a 
result, conclusions from those studies are 
sometimes inconsistent. In order to better 
understand the relationship between these 
methods, building a general theoretical 
framework provides a fascinating perspective. 
Furthermore, in real applications, selecting an 
appropriate FS method remains hard for a new 
task because too many FS methods are available 
due to the long history of FS studies. For 
example, merely in an early survey paper 
(Sebastiani, 2002), eight methods are mentioned. 
These methods are provided by previous work 
for dealing with different text classification tasks 
but none of them is shown to be robust across 
different classification applications. 
In this paper, we propose a framework with 
two basic measurements for theoretical 
comparison of six FS methods which are widely 
used in text classification. Moreover, a novel 
method is set forth that combines the two 
measurements and tunes their influences 
considering different application domains and 
numbers of selected features. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work on 
692
feature selection for text classification. Section 3 
theoretically analyzes six FS methods and 
proposes a new FS approach. Experimental 
results are presented and analyzed in Section 4. 
Finally, Section 5 draws our conclusions and 
outlines the future work. 
2 Related Work 
FS is a basic problem in pattern recognition and 
has been a fertile field of research and 
development since the 1970s. It has been proven 
to be effective on removing irrelevant and 
redundant features, increasing efficiency in 
learning tasks, and improving learning 
performance. 
FS methods fall into two broad categories, the 
filter model and the wrapper model (John et al, 
1994). The wrapper model requires one 
predetermined learning algorithm in feature 
selection and uses its performance to evaluate 
and determine which features are selected. And 
the filter model relies on general characteristics 
of the training data to select some features 
without involving any specific learning 
algorithm. There is evidence that wrapper 
methods often perform better on small scale 
problems (John et al 1994), but on large scale 
problems, such as text classification, wrapper 
methods are shown to be impractical because of 
its high computational cost. Therefore, in text 
classification, filter methods using feature 
scoring metrics are popularly used. Below we 
review some recent studies of feature selection 
on both topic-based and sentiment classification. 
In the past decade, FS studies mainly focus on 
topic-based classification where the classification 
categories are related to the subject content, e.g., 
sport or education. Yang and Pedersen (1997) 
investigate five FS metrics and report that good 
FS methods improve the categorization accuracy 
with an aggressive feature removal using DF, IG, 
and CHI. More recently, Forman (2003) 
empirically compares twelve FS methods on 229 
text classification problem instances and 
proposes a new method called 'Bi-Normal 
Separation' (BNS). Their experimental results 
show that BNS can perform very well in the 
evaluation metrics of recall rate and F-measure. 
But for the metric of precision, it often loses to 
IG. Besides these two comparison studies, many 
others contribute to this topic (Yang and Liu, 
1999; Brank et al, 2002; Gabrilovich and 
Markovitch, 2004) and more and more new FS 
methods are generated, such as, Gini index 
(Shang et al, 2007), Distance to Transition Point 
(DTP) (Moyotl-Hernandez and Jimenez-Salazar, 
2005), Strong Class Information Words (SCIW) 
(Li and Zong, 2005) and parameter tuning based 
FS for Rocchio classifier (Moschitti, 2003). 
Recently, sentiment classification has become 
popular because of its wide applications (Pang et 
al., 2002). Its criterion of classification is the 
attitude expressed in the text (e.g., recommended 
or not recommended, positive or negative) rather 
than some facts (e.g., sport or education). To our 
best knowledge, yet no related work has focused 
on comparison studies of FS methods on this 
special task. There are only some scattered 
reports in their experimental studies. Riloff et al 
(2006) report that the traditional FS method 
(only using IG method) performs worse than the 
baseline in some cases. However, Cui et al 
(2006) present the experiments on the sentiment 
classification for large-scale online product 
reviews to show that using the FS method of CHI 
does not degrade the performance but can 
significantly reduce the dimension of the feature 
vector. 
Moreover, Ng et al (2006) examine the FS of 
the weighted log-likelihood ratio (WLLR) on the 
movie review dataset and achieves an accuracy 
of 87.1%, which is higher than the result reported 
by Pang and Lee (2004) with the same dataset. 
From the analysis above, we believe that the 
performance of the sentiment classification 
system is also dramatically affected by FS. 
3 Our Framework 
In the selection process, each feature (term, or 
single word) is assigned with a score according 
to a score-computing function. Then those with 
higher scores are selected. These mathematical 
definitions of the score-computing functions are 
often defined by some probabilities which are 
estimated by some statistic information in the 
documents across different categories. For the 
convenience of description, we give some 
notations of these probabilities below. 
( )P t : the probability that a document x  contains 
term t ; 
( )iP c : the probability that a document x  does 
not belong to category ic ; 
( , )iP t c : the joint probability that a document x  
contains term t  and also belongs to category ic ; 
( | )iP c t : the probability that a document x belongs 
to category ic ?under the condition that it contains  
term t. 
693
( | )iP t c : the probability that, a document x does 
not contain term t with the condition that x belongs to 
category ic ; 
Some other probabilities, such as ( )P t , ( )iP c , 
( | )iP t c , ( | )iP t c , ( | )iP c t ,  and ( | )iP c t , are 
similarly defined. 
In order to estimate these probabilities, 
statistical information from the training data is 
needed, and notations about the training data are 
given as follows: 
1{ }mi ic = : the set of categories; 
iA : the number of the documents that contain the 
term t  and also belong to category ic ; 
iB : the number of the documents that contain the 
term t  but do not belong to category ic ; 
iN : the total number of the documents that belong 
to category ic ; 
allN : the total number of all documents from the 
training data. 
iC : the number of the documents that do not 
contain the term t  but belong to category ic , i.e., 
i iN A?  
iD : the number of the documents that neither 
contain the term t  nor belong to category ic , i.e., 
all i iN N B? ? ; 
In this section, we would analyze theoretically 
six popular methods, namely DF, MI, IG, CHI, 
BNS, and WLLR. Although these six FS 
methods are defined differently with different 
scoring measurements, we believe that they are 
strongly related. In order to connect them, we 
define two basic measurements which are 
discussed as follows. 
The first measurement is to compute the 
document frequency in one category, i.e., iA .  
The second measurement is the ratio between 
the document frequencies in one category and 
the other categories, i.e., /i iA B . The terms with 
a high ratio are often referred to as the terms with 
high category information. 
These two measurements form the basis for all 
the measurements that are used by the FS 
methods throughout this paper. In particular, we 
show that DF and MI are using the first and 
second measurement respectively. Other 
complicated FS methods are combinations of 
these two measurements. Thus, we regard the 
two measurements as basic, which are referred to 
as the frequency measurement and ratio 
measurement. 
3.1 Document Frequency (DF) 
DF is the number of documents in which a term 
occurs. It is defined as 
1
( )m iiDF A==?  
The terms with low or high document 
frequency are often referred to as rare or 
common terms, respectively. It is easy to see that 
this FS method is based on the first basic 
measurement. It assumes that the terms with 
higher document frequency are more informative 
for classification. But sometimes this assumption 
does not make any sense, for example, the stop 
words (e.g., the, a, an) hold very high DF scores, 
but they seldom contribute to classification. In 
general, this simple method performs very well 
in some topic-based classification tasks (Yang 
and Pedersen, 1997). 
3.2 Mutual Information (MI) 
The mutual information between term t  and 
class ic  is defined as 
( | )( , ) log ( )
i
i
P t cI t c
P t
=  
And it is estimated as 
log ( )( )
i all
i i i i
A NMI
A C A B
?
=
+ +
 
Let us consider the following formula (using 
Bayes theorem) 
( | ) ( | )( , ) log log( ) ( )
i i
i
i
P t c P c tI t c
P t P c
= =  
Therefore, 
( , )= log ( | ) log ( )i i iI t c P c t P c?  
And it is estimated as 
log log
      log log
1
      log(1 ) log
/
i i
i i all
i i i
i all
i
i i all
A NMI
A B N
A B N
A N
N
A B N
= ?
+
+
= ? ?
= ? + ?
 
From this formula, we can see that the MI score 
is based on the second basic measurement. This 
method assumes that the term with higher 
category ratio is more effective for classification. 
It is reported that this method is biased 
towards low frequency terms and the bias 
becomes extreme when ( )P t  is near zero. It can 
be seen in the following formula (Yang and 
Pedersen, 1997)  
( , ) log( ( | )) log( ( ))i iI t c P t c P t= ?  
694
Therefore, this method might perform badly 
when common terms are informative for 
classification. 
Taking into account mutual information of all 
categories, two types of MI score are commonly 
used: the maximum score ( )
max
I t  and the 
average score ( )avgI t , i.e.,  
1( ) max { ( , )}mmax i iI t I t c== , 
1
( ) ( ) ( , )mavg i iiI t P c I t c== ?? .  
We choose the maximum score since it performs 
better than the average score (Yang and Pedersen, 
1997). It is worth noting that the same choice is 
made for other methods, including CHI, BNS, 
and WLLR in this paper. 
3.3 Information Gain (IG) 
IG measures the number of bits of information 
obtained for category prediction by recognizing 
the presence or absence of a term in a document 
(Yang and Pedersen, 1997). The function is 
1
1
1
( ) { ( ) log ( )}
            +{ ( )[ ( | ) log ( | )]
           ( )[ ( | ) log ( | )]}
m
i ii
m
i ii
m
i ii
G t P c P c
P t P c t P c t
P t P c t P c t
=
=
=
= ?
+
?
?
?
 
And it is estimated as 
1
1 1
1 1
{ log }
    +( / )[ log ]
  ( / )[ log ]
m i i
i
all all
m m i i
i alli i
i i i i
m m i i
i alli i
i i i i
N NIG
N N
A AA N
A B A B
C CC N
C D C D
=
= =
= =
= ?
+ +
+
+ +
?
? ?
? ?
From the definition, we know that the 
information gain is the weighted average of the 
mutual information ( , )iI t c and ( , )iI t c  where 
the weights are the joint probabilities ( , )iP t c and 
( , )iP t c : 
1 1
( ) ( , ) ( , ) ( , ) ( , )m mi i i ii iG t P t c I t c P t c I t c= == +? ?  
Since ( , )iP t c is closely related to the 
document frequency iA  and the mutual 
information ( , )iI t c  is shown to be based on the 
second measurement, we can say that the IG 
score is influenced by the two basic 
measurements. 
3.4 2?  Statistic (CHI) 
The CHI measurement (Yang and Pedersen, 
1997) is defined as 
2( )
( ) ( ) ( ) ( )
all i i i i
i i i i i i i i
N A D C BCHI
A C B D A B C D
? ?
=
+ ? + ? + ? +
 
In order to get the relationship between CHI 
and the two measurements, the above formula is 
rewritten as follows 
2[ ( ) ( ) ]
( ) ( ) [ ( )]
all i all i i i i i
i all i i i all i i
N A N N B N A BCHI
N N N A B N A B
? ? ? ? ?
=
? ? ? + ? ? +
  
For simplicity, we assume that there are two 
categories and the numbers of the training 
documents in the two categories are the same 
( 2
all iN N= ). The CHI score then can be written 
as
 
2
2
2 ( )
( ) [2 ( )]
2 ( / 1)
      2( / 1) [ / ( / 1)]
i i i
i i i i i
i i i
i
i i i i i i
i
N A BCHI
A B N A B
N A B
NA B A B A B
A
?
=
+ ? ? +
?
=
+ ? ? ? +
 
From the above formula, we see that the CHI 
score is related to both the frequency 
measurement iA
 
and ratio measurement 
/i iA B . Also, when keeping the same ratio value, 
the terms with higher document frequencies will 
yield higher CHI scores. 
3.5 Bi-Normal Separation (BNS) 
BNS method is originally proposed by Forman 
(2003) and it is defined as 
1 1( , ) ( ( | )) ( ( | )i i iBNS t c F P t c F P t c? ?= ?  
It is calculated using the following formula 
1 1( ) ( )i i
i all i
A B
BNS F F
N N N
? ?
= ?
?
 
where ( )F x  is the cumulative probability 
function of standard normal distribution. 
For simplicity, we assume that there are two 
categories and the numbers of the training 
documents in the two categories are the same, 
i.e., 2
all iN N=  and we also assume that i iA B> . 
It should be noted that this assumption is only to 
allow easier analysis but will not be applied in 
our experiment implementation. In addition, we 
only consider the case when / 0.5i iA N ? . In 
fact, most terms take the document frequency 
iA which is less than half of iN .  
Under these conditions, the BNS score can be 
shown in Figure 1 where the area of the shadow 
part represents ( / / )i i i iA N B N?  and the length 
of the projection to the x  axis represents the 
BNS score. 
695
From Figure 1, we can easily draw the two 
following conclusions: 
1) Given the same value of iA , the BNS score 
increases with the increase of i iA B? . 
2) Given the same value of i iA B? , BNS score 
increase with the decrease of iA . 
 
Figure 1. View of BNS using the normal probability 
distribution. Both the left and right graphs have 
shadowed areas of the same size. 
 
And the value of i iA B?  can be rewritten as 
the following 
1(1 )
/
i i
i i i i
i i i
A BA B A A
A A B
?
? = ? = ? ?  
The above analysis gives the following 
conclusions regarding the relationship between 
BNS and the two basic measurements: 
1) Given the same iA , the BNS score increases 
with the increase of /i iA B . 
2) Given the same /i iA B , when iA  increases, 
i iA B?  also increase. It seems that the BNS 
score does not show a clear relationship with 
iA . 
In summary, the BNS FS method is biased 
towards the terms with the high category ratio 
but cannot be said to be sensitive to document 
frequency. 
3.6 Weighted Log Likelihood Ratio 
(WLLR) 
WLLR method (Nigam et al, 2000) is defined as 
( | )( , ) ( | ) log ( | )
i
i i
i
P t cWLLR t c P t c
P t c
=  
And it is estimated as 
( )logi i all i
i i i
A A N NWLLR
N B N
? ?
=
?
 
The formula shows WLLR is proportional to 
the frequency measurement and the logarithm of 
the ratio measurement. Clearly, WLLR is biased 
towards the terms with both high category ratio 
and high document frequency and the frequency 
measurement seems to take a more important 
place than the ratio measurement. 
3.7 Weighed Frequency and Odds (WFO)  
So far in this section, we have shown that the 
two basic measurements constitute the six FS 
methods. The class prior probabilities, 
( ),  1,2,...,iP c i m= , are also related to the 
selection methods except for the two basic 
measurements. Since they are often estimated 
according to the distribution of the documents in 
the training data and are identical for all the 
terms in a class, we ignore the discussion of their 
influence on the selection measurements. In the 
experiment, we consider the case when training 
data have equal class prior probabilities. When 
training data are unbalanced, we need to change 
the forms of the two basic measurements to 
/i iA N  and ( ) / ( )i all i i iA N N B N? ? ? . 
Because some methods are expressed in 
complex forms, it is difficult to explain their 
relationship with the two basic measurements, 
for example, which one prefers the category ratio 
most. Instead, we will give the preference 
analysis in the experiment by analyzing the 
features in real applications. But the following 
two conclusions are drawn without doubt 
according to the theoretical analysis given above. 
1) Good features are features with high 
document frequency; 
2) Good features are features with high 
category ratio. 
These two conclusions are consistent with the 
original intuition. However, using any single one 
does not provide competence in selecting the 
best set of features. For example, stop words, 
such as ?a?, ?the? and ?as?, have very high 
document frequency but are useless for the 
classification. In real applications, we need to 
mix these two measurements to select good 
features. Because of different distribution of 
features in different domains, the importance of 
each measurement may differ a lot in different 
applications. Moreover, even in a given domain, 
when different numbers of features are to be 
selected, different combinations of the two 
measurements are required to provide the best 
performance. 
Although a great number of FS methods is 
available, none of them can appropriately change 
the preference of the two measurements. A better 
way is to tune the importance according to the 
application rather than to use a predetermined 
combination. Therefore, we propose a new FS 
method called Weighed Frequency and Odds 
(WFO), which is defined as 
696
 ( | ) / ( | ) 1i iwhen P t c P t c >  
1( | )( , ) ( | ) [log ]( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
                 ( , ) 0i
else
WFO t c =
 
And it is estimated as 
1( )( ) (log )i i all i
i i i
A A N NWFO
N B N
? ??? ?
=
?
 
where ?
 
is the parameter for tuning the weight 
between frequency and odds. The value of ?
 
varies from 0 to 1. By assigning different value 
to ?  we can adjust the preference of each 
measurement. Specially, when 0? = , the 
algorithm prefers the category ratio that is 
equivalent to the MI method; when 1? = , the 
algorithm is similar to DF; when 0.5? = , the 
algorithm is exactly the WLLR method. In real 
applications, a suitable parameter ?  needs to be 
learned by using training data. 
4 Experimental Studies  
4.1 Experimental Setup 
Data Set:  The experiments are carried out on 
both topic-based and sentiment text classification 
datasets. In topic-based text classification, we 
use two popular data sets: one subset of 
Reuters-21578 referred to as R2 and the 20 
Newsgroup dataset referred to as 20NG. In detail, 
R2 consist of about 2,000 2-category documents 
from standard corpus of Reuters-21578. And 
20NG is a collection of approximately 20,000 
20-category documents 1 . In sentiment text 
classification, we also use two data sets: one is 
the widely used Cornell movie-review dataset2 
(Pang and Lee, 2004) and one dataset from 
product reviews of domain DVD3 (Blitzer et al, 
2007). Both of them are 2-category tasks and 
each consists of 2,000 reviews. In our 
experiments, the document numbers of all data 
sets are (nearly) equally distributed cross all 
categories. 
Classification Algorithm: Many 
classification algorithms are available for text 
classification, such as Na?ve Bayes, Maximum 
Entropy, k-NN, and SVM. Among these methods, 
SVM is shown to perform better than other 
methods (Yang and Pedersen, 1997; Pang et al, 
                                                      
1
 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 
2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 
3
 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
 
2002). Hence we apply SVM algorithm with the 
help of the LIBSVM 4  tool. Almost all 
parameters are set to their default values except 
the kernel function which is changed from a 
polynomial kernel function to a linear one 
because the linear one usually performs better for 
text classification tasks. 
Experiment Implementation: In the 
experiments, each dataset is randomly and 
evenly split into two subsets: 90% documents as 
the training data and the remaining 10% as 
testing data. The training data are used for 
training SVM classifiers, learning parameters in 
WFO method and selecting "good" features for 
each FS method. The features are single words 
with a bool weight (0 or 1), representing the 
presence or absence of a feature. In addition to 
the ?principled? FS methods, terms occurring in 
less than three documents ( 3DF ? ) in the 
training set are removed. 
4.2 Relationship between FS Methods and 
the Two Basic Measurements 
To help understand the relationship between FS 
methods and the two basic measurements, the 
empirical study is presented as follows. 
Since the methods of DF and MI only utilize 
the document frequency and category 
information respectively, we use the DF scores 
and MI scores to represent the information of the 
two basic measurements. Thus we would select 
the top-2% terms with each method and then 
investigate the distribution of their DF and MI 
scores.  
First of all, for clear comparison, we 
normalize the scores coming from all the 
methods using Min-Max normalization method 
which is designed to map a score s  to 's  in 
the range [0, 1] by computing 
'
s Min
s
Max Min
?
=
?
 
where
 
Min
 
and Max
 
denote the minimum 
and maximum values respectively in all terms? 
scores using one FS method. 
Table 1 shows the mean values of all top-2% 
terms? MI scores and DF scores of all the six FS 
methods in each domain. From this table, we can 
apparently see the relationship between each 
method and the two basic measurements. For 
instance, BNS most distinctly prefers the terms 
with high MI scores and low DF scores. 
According to the degree of this preference, we 
                                                      
4
 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
697
FS 
Methods 
Domain 
20NG R2 Movie DVD 
DF score MI score DF score MI score DF score MI score DF score MI score 
MI 0.004 0.870 0.047 0.959 0.003 0.888 0.004 0.881 
BNS 0.005 0.864 0.117 0.922 0.008 0.881 0.006 0.880 
CHI 0.015 0.814 0.211 0.748 0.092 0.572 0.055 0.676 
IG 0.087 0.525 0.209 0.792 0.095 0.559 0.066 0.669 
WLLR 0.026 0.764 0.206 0.805 0.168 0.414 0.127 0.481 
DF 0.122 0.252 0.268 0.562 0.419 0.09 0.321 0.111 
 
Table 1. The mean values of all top-2% terms? MI and DF scores using six FS methods in each domain 
 
can rank these six methods as 
MI, BNS IG, CHI, WLLR DFf f , where x yf
 
means method x
 
prefers the terms with  
higher MI scores (higher category information) 
and lower DF scores (lower document frequency) 
than method y. This empirical discovery is in 
agreement with the finding that WLLR is biased 
towards the high frequency terms and also with 
the finding that BNS is biased towards high 
category information (cf. Section 3 theoretical 
analysis). Also, we can find that CHI and IG 
share a similar preference of these two 
measurements in 2-category domains, i.e., R2, 
movie, and DVD. This gives a good explanation 
that CHI and IG are two similar-performed 
methods for 2-category tasks, which have been 
found by Forman (2003) in their experimental 
studies. 
According to the preference, we roughly 
cluster FS methods into three groups. The first 
group includes the methods which dramatically 
prefer the category information, e.g., MI and 
BNS; the second one includes those which prefer 
both kinds of information, e.g., CHI, IG, and 
WLLR; and the third one includes those which 
strongly prefer frequency information, e.g., DF. 
4.3 Performances of Different FS Methods 
It is worth noting that learning parameters in 
WFO is very important for its good performance. 
We use 9-fold cross validation to help learning 
the parameter ?  so as to avoid over-fitting. 
Specifically, we run nine times by using every 8 
fold documents as a new training data set and the 
remaining one fold documents as a development 
data set. In each running with one fixed feature 
number m, we get the best 
,i m best? ? (i=1,..., 9) 
value through varying 
,i m?  from 0 to 1 with the 
step of 0.1 to get the best performance in the 
development data set. The average value 
m best? ? , 
i.e., 
9
,1
( ) / 9m best i m besti? ?? ?== ?  
is used for further testing. 
Figure 2 shows the experimental results when 
using all FS methods with different selected 
feature numbers. The red line with star tags 
represents the results of WFO. At the first glance, 
in R2 domain, the differences of performances 
across all are very noisy when the feature 
number is larger than 1,000, which makes the 
comparison meaningless. We think that this is 
because the performances themselves in this task 
are very high (nearly 98%) and the differences 
between two FS methods cannot be very large 
(less than one percent). Even this, WFO method 
do never get the worst performance and can also 
achieve the top performance in about half times, 
e.g., when feature numbers are 20, 50, 100, 500, 
3000. 
Let us pay more attention to the other three 
domains and discuss the results in the following 
two cases. 
In the first case when the feature number is 
low (about less than 1,000), the FS methods in 
the second group including IG, CHI, WLLR,  
always perform better than those in the other two 
groups. WFO can also perform well because its 
parameters 
m best? ?  are successfully learned to be 
around 0.5, which makes it consistently belong 
to the second group. Take 500 feature number 
for instance, the parameters 500 best? ?  are 0.42, 
0.50, and 0.34 in these three domains 
respectively. 
In the second case when the feature number is 
large, among the six traditional methods, MI and 
BNS take the leads in the domains of 20NG and 
Movie while IG and CHI seem to be better and 
more stable than others in the domain of DVD. 
As for WFO, its performances are excellent cross 
all these three domains and different feature 
numbers. In each domain, it performs similarly 
as or better than the top methods due to its 
well-learned parameters. For example, in 20NG, 
the parameters 
m best? ?  are 0.28, 0.20, 0.08, and 
0.01 when feature numbers are 10,000, 15,000, 
20,000, and 30,000. These values are close to 0 
698
(WFO equals MI when 0? = ) while MI is the 
top one in this domain. 
10 20 50 100 200 500 1000 2000 3000 4227
0.88
0.9
0.92
0.94
0.96
0.98
1
feature number
a
cc
u
ra
cy
Topic - R2
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
200 500 1000 2000 5000 10000 15000 20000 30000 32091
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
feature number
ac
cu
ra
cy
Topic - 20NG
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
50 200 500 1000 4000 7000 10000 13000 15176
0.55
0.6
0.65
0.7
0.75
0.8
0.85
feature number
ac
cu
ra
cy
Sentiment - Movie
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
20 50 100 500 1000 1500 2000 3000 4000 5824
0.5
0.55
0.6
0.65
0.7
0.75
0.8
feature number
ac
cu
ra
cy
Sentiment - DVD
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
Figure 2. The classification accuracies of the four domains 
using seven different FS methods while increasing the 
number of selected features. 
 
From Figure 2, we can also find that FS does 
help sentiment classification. At least, it can 
dramatically decrease the feature numbers 
without losing classification accuracies (see 
Movie domain, using only 500-4000 features is 
as good as using all 15176 features). 
5 Conclusion and Future Work 
In this paper, we propose a framework with two 
basic measurements and use it to theoretically 
analyze six FS methods. The differences among 
them mainly lie in how they use these two 
measurements. Moreover, with the guidance of 
the analysis, a novel method called WFO is 
proposed, which combine these two 
measurements with trained weights. The 
experimental results show that our framework 
helps us to better understand and compare 
different FS methods. Furthermore, the novel 
method WFO generated from the framework, can 
perform robustly across different domains and 
feature numbers. 
In our study, we use four data sets to test our 
new method. There are much more data sets on 
text categorization which can be used. In 
additional, we only focus on using balanced 
samples in each category to do the experiments. 
It is also necessary to compare the FS methods 
on some unbalanced data sets, which are 
common in real-life applications (Forman, 2003; 
Mladeni and Marko, 1999). These matters will 
be dealt with in the future work. 
Acknowledgments 
The research work described in this paper has 
been partially supported by Start-up Grant for 
Newly Appointed Professors, No. 1-BBZM in the 
Hong Kong Polytechnic University. 
References  
J. Blitzer, M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain adaptation for sentiment 
classification. In Proceedings of ACL-07, the 45th 
Meeting of the Association for Computational 
Linguistics. 
J. Brank, M. Grobelnik, N. Milic-Frayling, and D. 
Mladenic. 2002. Interaction of feature selection 
methods and linear classification models. In 
Workshop on Text Learning held at ICML. 
H. Cui, V. Mittal, and M. Datar. 2006. Comparative 
experiments on sentiment classification for online 
product reviews. In Proceedings of AAAI-06, the 
21st National Conference on Artificial Intelligence. 
G. Forman. 2003. An extensive empirical study of 
feature selection metrics for text classification. The 
Journal of Machine Learning Research, 3(1): 
1289-1305. 
699
E. Gabrilovich and S. Markovitch. 2004. Text 
categorization with many redundant features: using 
aggressive feature selection to make SVMs 
competitive with C4.5. In Proceedings of the ICML, 
the 21st International Conference on Machine 
Learning. 
G. John, K. Ron, and K. Pfleger. 1994. Irrelevant 
features and the subset selection problem. In 
Proceedings of ICML-94, the 11st International 
Conference on Machine Learning.  
S. Li and C. Zong. 2005. A new approach to feature 
selection for text categorization. In Proceedings of 
the IEEE International Conference on Natural 
Language Processing and Knowledge Engineering 
(NLP-KE). 
D. Mladeni and G. Marko. 1999. Feature selection for 
unbalanced class distribution and naive bayes. In 
Proceedings of ICML-99, the 16th International 
Conference on Machine Learning. 
A. Moschitti. 2003. A study on optimal parameter 
tuning for Rocchio text classifier. In Proceedings 
of ECIR, Lecture Notes in Computer Science, 
vol. 2633, pp. 420-435. 
E. Moyotl-Hernandez and H. Jimenez-Salazar. 2005. 
Enhancement of DTP feature selection method for 
text categorization. In Proceedings of CICLing, 
Lecture Notes in Computer Science, vol.3406, 
pp.719-722.  
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. 
Examining the role of linguistic knowledge sources 
in the automatic identification and classification of 
reviews. In Proceedings of the COLING/ACL Main 
Conference Poster Sessions. 
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 
2000. Text classification from labeled and 
unlabeled documents using EM. Machine Learning, 
39(2/3): 103-134. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine 
learning techniques. In Proceedings of EMNLP-02, 
the Conference on Empirical Methods in Natural 
Language Processing. 
B. Pang and L. Lee. 2004. A sentimental education: 
Sentiment analysis using subjectivity 
summarization based on minimum cuts. In 
Proceedings of ACL-04, the 42nd Meeting of the 
Association for Computational Linguistics. 
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature 
subsumption for opinion analysis. In Proceedings 
of EMNLP-06, the Conference on Empirical 
Methods in Natural Language Processing,. 
F. Sebastiani. 2002. Machine learning in automated 
text categorization. ACM Computing Surveys, 
34(1): 1-47. 
W. Shang, H. Huang, H. Zhu, Y. Lin, Y. Qu, and Z. 
Wang. 2007. A novel feature selection algorithm 
for text categorization. The Journal of Expert 
System with Applications, 33:1-5. 
Y. Yang and J. Pedersen. 1997. A comparative study 
on feature selection in text categorization. In 
Proceedings of ICML-97, the 14th International 
Conference on Machine Learning. 
Y. Yang and X. Liu. 1999. A re-examination of text 
categorization methods. In Proceedings of 
SIGIR-99, the 22nd annual international ACM 
Conference on Research and Development in 
Information Retrieval.  
700
Tutorial Abstracts of ACL-IJCNLP 2009, page 1,
Suntec, Singapore, 2 August 2009. c?2009 ACL and AFNLP
Fundamentals of Chinese Language Processing 
 
Chu-Ren Huang 
Dept. of Chinese and Bilingual Studies 
Hong Kong polytechnic University  
Churen.huang@inet.polyu.edu.hk
Qin Lu 
Department of Computing 
Hong Kong Polytechnic University 
csluqin@comp.polyu.edu.hk 
 
 
1 Introduction 
This tutorial gives an introduction to the funda-
mentals of Chinese language processing for text 
processing. Today, more and more Chinese in-
formation are available in electronic form and 
over the internet. Computer processing of Chi-
nese text requires the understanding of both the 
language itself and the technology to handle 
them. This tutorial is targeted for both Chinese 
linguists who are interested in computational 
linguistics and computer scientists who are inter-
ested in research on processing Chinese.  
2 Content Overview 
This tutorial consists of two parts. The first part 
overviews the grammar of the Chinese language 
from a language processing perspective based on 
naturally occurring data. The second part over-
views Chinese specific processing issues and 
corresponding computational technologies. 
The grammar introduced is a descriptive 
grammar of general-purpose, present-day stan-
dard Mandarin Chinese, which is fast becoming 
an internationally spoken language. Real exam-
ples of actual language use will be illustrated 
based on a data driven and corpus based ap-
proach so that its links to computational linguis-
tic approaches for computer processing are natu-
rally bridged in. A number of important Chinese 
NLP resources are also presented. On the tech-
nology side, the tutorial mainly covers Chinese 
word segmentation and Part-of-Speech tagging. 
Word segmentation problem has to deal with 
some Chinese language unique problems such as 
unknown word detection and named entity rec-
ognition which are the emphasis of this tutorial.  
3 Tutorial Outline  
Part 1: Highlights of Chinese Grammar for NLP 
1.1 Preliminaries: Orthography and writing 
conventions 
 
1.2 Basic unit of processing: word or character? 
a. Word-forms vs. character forms 
 b. Word-senses vs. character-senses 
1.3 Part-of-Speech: important issues in defin-
ing word classes 
1.4 Word formation: from affixation to com-
pounding  
1.5 Unique constructions and challenges 
a. Classifier-noun agreement 
b. Separable compounds (or ionization) 
 c. ?Verbless? Constructions 
1.6. Chinese NLP resources  
 
Part 2: Text Processing 
2.1 Lexical processing 
 a. Segmentation 
 b. Disambiguation 
 c. Unknown word detection 
 d. Named Entity Recognition 
2.2 Syntactic processing 
 a. Issues in PoS tagging 
 b. Hidden Markov Models 
2.3 NLP Applications 
References  
Academia Sinica Balance Corpus of Mandarin Chi-
nese.  http://www.sinica.edu.tw/SinicaCorpus/  
Chao, Y. R. 1968. A Grammar of Spoken Chinese. 
Berkeley: University of California Press. 
Huang, C.-R., K.-j. Chen and B. K. T'sou. 1996.  
Readings in Chinese Natural Language Processing. 
Journal of Chinese Linguistics Monograph Series 
No. 9.  Berkeley: POLA. 
T'sou, B. K. 2004. Chinese Language Processing at 
the Dawn of the 21st Century. In C.-R. Huang and 
W. Lenders. Eds. Computational Linguistics and 
Beyond. Pp. 189-206. Taipei: AcademiaSinica. 
Miao, S.Q., Wei, Z.H. 2007, Chinese Text Informa-
tion Processing Principles and Applications (In 
Chinese). Tsinghua University Press.  
 
1
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Cognitive-based Annotation System for Emotion Computing 
 
 
Ying Chen, Sophia Y. M. Lee and Chu-Ren Huang 
Department of Chinese & Bilingual Studies 
The Hong Kong Polytechnic University 
{chenying3176,sophiaym,churen.huang}@gmail.com 
 
  
 
Abstract 
Emotion computing is very important for 
expressive information extraction. In this 
paper, we provide a robust and versatile 
emotion annotation scheme based on cog-
nitive emotion theories, which not only 
can annotate both explicit and implicit 
emotion expressions, but also can encode 
different levels of emotion information for 
the given emotion content. In addition, 
motivated by a cognitive framework, an 
automatic emotion annotation system is 
developed, and large and comparatively 
high-quality emotion corpora are created 
for emotion computing, one in Chinese 
and the other in English. Such an annota-
tion system can be easily adapted for dif-
ferent kinds of emotion applications and 
be extended to other languages. 
1 Introduction 
Affective information is important for human 
language technology, and sentiment analysis, a 
coarse-grained affective computing (Shanahan et 
al., 2006), which is attitude assessment, has be-
come the most salient trend. The polarity-driven 
approach in sentiment analysis is, however, often 
criticized as too general to satisfy some applica-
tions, such as advertisement design and robot 
design, and one way to capture more fine-grained 
affective information is to detect emotion expres-
sions. Unlike sentiment, emotions are cognitive-
based, which consistently occur across domains 
because of its human psychological activities. 
We believe that emotion computing, which is a 
fine-grained and cognitive-based framework of 
affective computing, will provide a more robust 
and versatile model for human language technol-
ogy. 
Since the concept of emotion is very compli-
cated and subjective, comparing to some annota-
tions such as POS annotation and Chinese word 
segmentation annotation, emotion annotation is 
highly labor intensive as it requires careful hu-
man judgment. Both explicit and implicit emo-
tions must be recognized and tagged during emo-
tion annotation, therefore, emotion annotation is 
not a simple assignment exercise as in POS an-
notation. Technically, emotion annotation can be 
divided into two subtasks: emotion detection (i.e. 
differentiate emotional content from neutral con-
tent), which is a very important task for affective 
information extraction, and emotion classifica-
tion (i.e. assign emotion tags to emotional con-
tent.)  
Emotion computing often requires a large and 
high-quality annotated data, however, there is a 
lack of this kind of corpus. This is not only be-
cause of the enormous human involvement, but 
also because of the unavailability of emotion an-
notation scheme, which is robust and versatile 
for both emotion annotation and emotion com-
puting. Tokuhisa et al (2008) is the only work 
that explores the issue of emotion detection 
while most of the previous studies concentrate on 
the emotion classification given a known emo-
tion context (Mihalcea and Liu, 2006; Kozareva 
et al, 2007.) Even for emotion classification, 
some issues remain unresolved, such as the com-
plicated relationships among different emotion 
types, emotion type selection, and so on. Thus, it 
is still far from solving the emotion problem if 
emotion annotation is just considered as emo-
tion-tag assignment.  
In this paper, we first explore the relationships 
among different emotion types with the support 
of a proposed emotion taxonomy, which com-
bines some psychological theories and linguistic 
semantics. Based on the emotion taxonomy, a 
robust and versatile emotion annotation scheme 
is designed and used in both Chinese and English 
1
emotion corpora. Our emotion annotation 
scheme is very flexible, which is only a layer 
added to a sentence, although it can easily be 
extended to a higher level of a text. Our annota-
tion scheme not only can provide the emotion 
type information, but also can encode the infor-
mation regarding the relationship between emo-
tions. With this versatile annotated emotion in-
formation, different NLP users can extract dif-
ferent emotion information from a given anno-
tated corpus according to their applications.  
With such an emotion annotation scheme, a 
large and comparatively high-quality annotated 
emotion corpus is built for emotion computing 
through an unsupervised approach. Tokuhisa et 
al. (2008) pointed out that besides emotion cor-
pus, neutral corpus (i.e. sentences containing no 
emotion) is also very important for emotion 
computing. Therefore, a high-quality neutral 
corpus is also automatically collected using con-
textual information. These two corpora are com-
bined to form a complete emotion-driven corpus 
for emotion computing. Although the unsuper-
vised method cannot provide a perfectly-
annotated corpus, it can easily adapt for different 
emotion computing.  
The remainder of this paper is organized as 
follows. In Section 2, we give an overview of the 
previous work on emotion annotation and some 
related psychological and linguistic theories. In 
Section 3, we describe our emotion taxonomy 
and emotion annotation scheme. Section 4 dis-
cusses how the unsupervised corpus is created.  
Section 5 presents the pilot experiments for emo-
tion computing with our corpus, which suggests 
that the unsupervised approach of our corpus 
creation is effective. Finally, a conclusion is 
made in Section 5. 
2 Related work 
There is no clear consensus among many psy-
chological and linguistic theories on the concept 
of emotions. Here, we limit our work by the clas-
sic definition of ?emotions? (Cannon, 1927): 
Emotion is the felt awareness of bodily reactions 
to something perceived or thought. 
Emotion is a complicated concept, and there 
are complicated relationships among different 
emotions. For example, the relationship between 
?discouraged? and ?sad? is different with the one 
between ?remorse? and ?sad.? Hobbs & Gordon 
(2008) and Mathieu (2005) explore emotions 
mainly from a lexical semantics perspective, and 
Schr?der et al (2006) designed an annotation 
scheme, EARL, mainly for speech processing. 
Because of the disagreements in emotion theories, 
EARL did not explore the relationships among 
emotion types. In this paper, we focus on emo-
tions in written data, which is very different from 
that of in spoken data in terms of expressions. 
Here, we first adopt psychological theories 
(Plutchik, 1980; Turner, 2000) to create an emo-
tion taxonomy, and then design an emotion anno-
tation scheme based on the taxonomy. 
Since most of the previous emotion corpora 
are either too small (Xu et al, 2008) or compara-
tively ineffective in terms of accuracy (Tokuhisa 
et al, 2008), they cannot satisfy the requirements 
of emotion computing. In this paper, based on 
Natural Semantic Metalanguage (NSM), a cogni-
tive approach to human emotions (which will be 
discussed in the later section), we create an au-
tomatic emotion annotation system. While this 
annotation system needs only a little training da-
ta and does not require human supervision, the 
corpus still maintains a comparatively high qual-
ity. Another significant advantage of our auto-
matic annotation system is that it can easily adapt 
to different emotion applications by simply sup-
plying different training data. 
Most of the existing emotion theories study 
emotions from the biological and psychological 
perspectives, hence they cannot easily apply to 
NLP. Fortunately, NSM, one of the prominent 
cognitive models exploring human emotions, 
offers a comprehensive and practical approach to 
emotions (Wierbicka 1996.) NSM describes 
complex and abstract concepts, such as emotions, 
in terms of simpler and concrete ones. In such a 
way, emotions are decomposed as complex 
events involving a cause and a mental state, 
which can be further described with a set of uni-
versal, irreducible cores called semantic primi-
tives. This approach identifies the exact differ-
ences and connections between emotion concepts 
in terms of the causes, which provide an imme-
diate cue for emotion detection and classification. 
We believe that the NSM model offers a plausi-
ble framework to be implemented for automatic 
emotion computing.  
3 Emotion annotation scheme 
3.1 The emotion taxonomy 
Although there are many emotion theories devel-
oped in different fields, such as biology, psy-
chology, and linguistics, most of them agree that 
emotion can be divided into primary emotions 
and complex emotions (i.e. the combinations of 
2
primary emotions.) There is still controversy 
over the selection of primary emotions, nonethe-
less, ?happiness?, ?sadness?, ?anger?, and ?fear? 
are considered as primary emotions by most of 
emotion theories.  
Plutchik?s emotion taxonomy (Plutchik 1980), 
one of the classic emotion taxonomies, also fol-
lows the division of primary emotions and com-
plex emotions, and Turner's taxonomy (Turner 
2000), which is based on Plutchik?s work, allows  
more flexible combinations of primary emotions. 
In this paper, we adopt Turner?s taxonomy, with 
the two main points emphasized: 
1) For each primary emotion, it is divided into 
three levels according to its intensity: high, mod-
erate, and low. Besides ?happiness,? ?sadness,? 
?anger? and ?fear,? Turner also suggests that 
?disgust? and ?surprise? can be primary emo-
tions (Turner 1996; Turner 2007). In Chinese, 
the character ??? (?surprise?) has a strong abil-
ity to form many emotion words, such as ?? 
(surprise and happiness), and ?? (surprise and 
fear), which is consistent with the explanation of 
?surprise? emotion by Plutchik (1991): ?when 
the stimulus has been evaluated, the surprise may 
quickly change to any other emotion.? Therefore, 
in our annotation scheme, we consider ?happi-
ness,? ?sadness,? ?anger,? ?fear,? and ?surprise? 
as primary emotions. 
2) Complex emotion can be divided into first-
order complex emotions (consisting of two pri-
mary emotions), second-order complex emotions 
(consisting of three primary emotions), and so on, 
according to the number of primary emotions 
that involves in the complex emotion. For exam-
ple, ?pride? (happiness + fear) is a first-order 
complex emotion, which contains a greater 
amount of ?happiness? with a lesser amount of 
?fear.? 
Tables 1 and 2 show some keywords in Turn-
er?s taxonomy, and the symbol ?//? is to separate 
different emotion types. Table 1 lists the five 
most common English keywords and their cor-
responding primary emotions, and Table 2 lists 
the English keywords and their corresponding 
complex emotions. In Table 2, several emotion 
keywords, which express similar emotion 
meaning, are grouped into an emotion type. For 
example, the emotion keywords ?awe, reverence, 
veneration? are grouped into emotion type 
?awe.? For a complex emotion, the order of pri-
mary emotions indicates the importance of those 
primary emotions for that complex emotion. For 
examples, ?envy? is ?fear + anger,? which con-
tains a greater amount of ?fear? with a lesser 
amount of ?anger? whereas ?awe? is ?fear + 
happiness,? which contains a greater amount of 
?fear? with a lesser amount of ?happiness.?  
For English emotion keywords, as Turner?s 
taxonomy missed some common emotion key-
words, we add the emotion keywords from 
Plutchik's taxonomy. Besides, unlike Chinese, 
English words have morphological variations, for 
example, the emotion keyword ?pride? can occur 
in text with the various formats: ?pride,? 
?prides,? ?prided,? ?proud,? ?proudly.? As 
shown in Tables 1 and 2, there are 188 English 
lemmas in our taxonomy. In total, there are 720 
emotion keywords if morphology is taken into 
account.  
Since Turner?s emotion taxonomy is cogni-
tive-based, it is versatile for different languages 
although there is no one-to-one mapping. We 
also explore Chinese emotion taxonomy in our 
previous work (Chen at el., 2009). We first select 
emotion keywords from the cognitive-based feel-
ing words listed in Xu and Tao (2003), and then 
map those emotion keywords to Turner?s taxon-
omy with adaptation for some cases. Lastly, 
some polysemous emotion keywords are re-
moved to reduce ambiguity, and 226 Chinese 
emotion keywords remain. 
Moreover, Turner?s taxonomy is a compara-
tively flexible structure, and more extensions can 
be done for different applications. For example, 
for a complex emotion, not only its primary emo-
tions are listed, but also the intensity of the pri-
mary emotions can be given. For instance, three 
emotion types, which belong to ?anger + fear,? 
are extended as follows: 
Jealousy:      Anger (Moderate) + Fear (Moderate) 
Suspicion:    Anger (Low) + Fear (Low) 
Abhorrence: Anger (High) + Fear (Low) 
Finally, we should admit that the emotion tax-
onomy is still an on-going research topic and 
needs further exploration, such as the position of 
a given emotion keyword in the emotion taxon-
omy, whether and how to group similar emotion 
keywords, and how to decompose a complex 
emotion into primary emotions. 
3.2 The emotion annotation scheme 
Given Turner?s taxonomy, we design our annota-
tion scheme to encode this kind of emotion in-
formation. Our emotion annotation scheme is 
XML scheme, and conforms with the Text En-
coding Initiative (TEI) scheme with some modi-
fications. The emotion scheme is a layer just
3
Primary Emotions Keywords 
Happiness High: ecstatic, eager, joy, enthusiastic, happy//Moderate: cheerful, satisfy, pleased, enjoy, interest//Low: 
sanguine, serene, content, grateful 
Fear High: horror, terror//Moderate: misgivings, self-conscious, scare, panic, anxious//Low: bewilder, reluct, 
shy, puzzles, confuse 
Anger High: dislike, disgust, outrage, furious, hate//Moderate: contentious, offend, frustrate, hostile, an-
gry//Low: contemptuous, agitate, irritate, annoy, impatient 
Sadness High: deject, despondent, sorrow, anguish, despair//Moderate: gloomy, dismay, sad, unhappy, disap-
point//Low: dispirit, downcast, discourage 
Surprise High: astonish//Moderate: startled, amaze, surprise 
Table1: Primary emotions and some corresponding keywords 
Combinations Keywords 
Happiness + Fear Wonder: wonder, wondering, hopeful//Pride: pride, boastful 
Happiness + Anger Vengeance: vengeance, vengeful//Calm: appeased, calmed, calm, soothed//Bemused: bemused 
Happiness + Sadness Yearning: nostalgia, yearning 
Fear + Happiness Awe: awe, reverence, veneration 
Fear + Anger Antagonism: antagonism, revulsed//Envy: envy 
Fear + Sadness Worried: dread, wariness, pensive, helpless, apprehension, worried 
Anger +Happiness Unfriendly: snubbing, mollified, rudeness, placated, apathetic, unsympathetic, unfriendly, unaffection-
ate//Sarcastic: sarcastic 
Anger + Fear Jealousy: jealous//Suspicion: suspicion, distrustful//Abhorrence: abhorrence 
Anger + Sadness Depressed: bitter, depression//Intolerant: intolerant  
Sadness +Happiness Acceptance: acceptance, tolerant//Solace: moroseness, solace, melancholy 
Sadness+ Fear Hopeless: forlorn, lonely, hopeless, miserable//Remorseful: remorseful, ashamed, humiliated 
Sadness+ Anger Discontent: aggrieved, discontent, dissatisfied, unfulfilled//Boredom: boredom//Grief: grief, sullenness 
Surprise + Happiness Delight: delight 
Surprise + Sadness Embarrassed: embarrassed 
Table 2:  First-order complex emotions and some corresponding keywords 
 
beyond a sentence, and encodes emotion infor-
mation for a sentence. This annotation scheme 
can be compatible for any TEI-based annotated 
corpora as long as sentences are clearly marked. 
The emotion-related elements (tags) in our 
annotation scheme are described as follows. For 
easy demonstration, our elements are defined 
with the format of British National Corpus 
(BNC) annotation scheme1 , and our examples 
are also based on BNC annotated text. Figure 1 
gives the definition of each element, and Figure 
2 shows several examples using our annotation 
scheme. Note that <s> element is a tag for a sen-
tence-like division of a text, and its attribute ?n? 
gives the sentence index. In Figure 2, Sentence 1, 
which expresses emotions by emotion keywords, 
contains two types of emotions: ?surprise? (pri-
mary emotion) and ?jealousy? (complex emo-
tion); Sentence 2 is a neutral sentence. 
<emotion> element 
It is used only when the sentence expresses 
emotions. It contains a list of <emotionType> 
elements and a <s> element. As a sentence may 
                                                 
1 http://www.natcorp.ox.ac.uk/XMLedition/U
RG/ 
express several emotions, an <emotion> element 
can contain several <emotionType> elements, 
and each <emotionType> element describes an 
emotion occurring in that sentence separately. 
<neutral> element 
It is used only when the sentence does not 
contain any emotion expression. It contains only 
a <s> element. 
<emotionType> element 
It describes a type of emotion in that sentence.  
It contains an ordered sequence of <pri-
maryEmotion> elements. Attribute ?name? pro-
vides the name of the emotion type, such as 
?surprise?, ?jealousy,? and so on, and it is op-
tional. If the emotion type is a primary emotion, 
the <emotionType> element will have only one 
<primaryEmotion> element, which encodes the 
information of this primary emotion. If the emo-
tion is a complex emotion, the <emotionType> 
element will have several <primaryEmotion> 
elements (each of them describes the primary 
emotion involved in that complex emotion.) At-
tribute ?keyword? is an optional attribution if 
annotators want to provide the indicator of a text 
for that emotion. 
4
   
<primaryEmtion> element 
It describes the property of a primary emotion 
involved in the emotion type. There are three 
attributes: ?order,? ?name,? and ?intensity.?  
?Order? gives the weight of this primary emo-
tion in the emotion type, and the weight value 
decreases with the ascending ?order? value. 
?Name? and ?intensity? provide the name and 
intensity of a primary emotion. To encode the 
information in our emotion taxonomy, the value 
of ?order? is {1,2,3,4,5}, the value of ?name? is 
{?happiness,? ?sadness,? ?anger,?  ?fear?, ?sur-
prise? }, and  the value of ?intensity? is {?high?, 
?moderate?, ?low?.} 
The <primaryEmotion> element seems to be 
redundant because its encoded information can 
be obtained from the given emotion taxonomy if 
the name of the emotion type is available, but 
the presence of this element can make our anno-
tation scheme more robust. Sometimes emotion 
is so complicated (especially for those emotion 
expressions without any explicit emotion key-
word) that an annotator may not be able to find 
an exact emotion type to match this emotion, or 
to list all involved primary emotions. For those 
subtle cases, emotion annotation can be simpli-
fied to list the involved primary emotions as 
many as possible through <primaryEmotion> 
elements. For example, in Sentence 3 in Figure 2, 
although there is no emotion keyword occurring, 
the word ?hurt? indicates the presence of an 
emotion, which at least involves ?sadness.? 
However, because it is hard to explicitly list oth-
er primary emotions, therefore, we give only the 
annotation of ?sadness.?  
Our annotation scheme has the versatility to 
provide emotion data for different applications. 
For example, if textual information input anno-
tated with our scheme is provided for the Japa-
nese robot Saya (Hashimoto et al 2006) to con-
trol her facial emotion expression, a simple 
mapping from our 24 emotion types can be done 
automatically to Saya?s six emotion types, i.e. 
surprise, fear, disgust, anger, happiness, and 
sadness. As four of her emotion types are also 
unique primary emotions, using information en-
coded in <emotionType> element and <pri-
maryEmotion> element will ensure unique 
many-to-one mapping and the correct robotic 
expressions. A trickier case involves her ?anger? 
and ?disgust? emotions. The emotion type ?an-
ger? in our taxonomy includes emotion words 
?anger? and ?disgust?. However, with the ?key-
word? information provided in <emotionType> 
element, a small subset of ?anger? emotion in 
our taxonomy can be mapped to ?disgust? in 
Saya?s system. For example, we could map 
keywords ?dislike, disgust, hate? to ?disgust?, 
element emotion 
{ 
(emotionType)+, 
<s> 
} 
element emotionType 
{ 
attribute name (optional), 
attribute keyword (optional), 
(primaryEmotion)+ 
} 
element primaryEmotion 
{ 
attribute order (optional), 
attribute name (necessary), 
attribute intensity (optional) 
} 
element neutral 
{  
<s> 
} 
Figure 1: The definition of emotion-related elements 
<emotion> 
<emotionType name =  "surprise"  keyword ="surprised"> 
<primaryEmotion  order =  "1" name =  "surprise"  intensity = "moderate"></primaryEmotion> 
</emotionType>   
<emotionType name = "jealousy"  keyword = ?jealousy?> 
<primaryEmotion  order =  "1"  name = "anger" intensity =  "moderate"></primaryEmotion> 
<primaryEmotion  order =  "2"  name =  "fear"   intensity =  "moderate"></primaryEmotion> 
</emotionType> 
<s n = "1"> Hari was surprised at the rush of pure jealousy that swept over her at the mention of Emily Grenfell .</s> 
</emotion> 
<neutral> 
<s n = "2"> By law no attempts may be made to hasten death or prolong the life of the sufferer . </s> 
</neutral> 
<emotion> 
<emotionType> 
<primaryEmotion name =  "sadness"></primaryEmotion> 
</emotionType>    
<s n = "3">He looked hurt when she did n't join him , his emotions transparent as a child 's . </s> 
</emotion> 
Figure 2: The example of sentence annotation 
 
5
and all the remaining ones, such as ?outrage, 
furious,? to ?anger.? 
4 Emotion-driven corpus creation 
Similar to most corpora, our corpus creation is 
designed to satisfy the requirements of real emo-
tion computing. Emotions can be expressed with 
or without emotion vocabulary in the text. It 
seems to be intuitive that emotion computing for 
a context with emotion keywords can be satis-
factory when the collection of emotion vocabu-
lary is comprehensive, such as ?joyful? indicates 
the presence of ?happiness? emotion. However, 
this intuitive approach cannot work well because 
of the ambiguity of some emotion keywords and 
the emotion context shift as the sentiment shift 
(Polanyi and Zaenen, 2004). Moreover, the de-
tection of emotions in a context without emotion 
keywords is very challenging. To deal with these 
problems, we build the emotion corpus, which is 
motivated by the NSM theory. 
According to the NSM theory, an emotion is 
provoked by a stimulus. This indicates one pos-
sible way to detect emotions in text, i.e. the de-
tection of emotional stimulus, which is often 
provided in the text. In other words, emotion 
corpus is a collection of emotion stimuli. Since 
emotion is subjective, the stimulus-based ap-
proach works only when its context is provided. 
For example, the stimulus ? ?build a gym for 
this community? ? may cause different emotions, 
such as ?surprise?, ?happy? and so on, depend-
ing on its context. We also notice that the text 
containing an emotion keyword may contain 
emotional stimulus and its context. Thus, a natu-
ral corpus creation approach comes out. 
 In our system, a pattern-based approach is 
used to collect the emotion corpus, which is sim-
ilar to the one used in Tokuhisa et al (2008), but 
we do not limit to event-driven emotions 
(Kozareva et al, 2008), and adjust our rules to 
improve the quality of emotion annotation. 
There are five steps in our emotion sentence an-
notation as given below, and Steps (2) and (3) 
are to improve the annotation quality. 
1) Extract emotion sentences: sentences con-
taining emotion keywords are extracted by 
keyword matching.  
2) Delete ambiguous structures: some ambigu-
ous sentences, which contain structures such 
as negation and modal, are filtered out.  
3) Delete ambiguous emotion keywords: if an 
emotion keyword is very ambiguous, all sen-
tences containing this ambiguous emotion 
keyword are filtered out. 
4) Give emotion tags: each remaining sentence 
is marked with its emotion tag according to the 
emotion type which the focus emotion word 
belongs to (refer to Tables 1 and 2.) 
5) Ignore the focus emotion keywords: for 
emotion computing, the emotion word is re-
moved from each sentence.  
 Polanyi and Zaenen (2004) addressed the is-
sue of polarity-based sentiment context shift, 
and the similar phenomenon also exists in emo-
tion expressions. In our corpus creation, two 
kinds of contextual structures are handled with: 
the negation structure and the modal structure. 
In both English and Chinese, a negated emotion 
expression can be interpreted as one of the three 
possible meanings (as shown in Figure 3): oppo-
site to the target emotion (S1), deny the exis-
tence of the target emotion (S2), or confirm the 
existence of the target emotion (S3). The modal 
structure often indicates that the emotion expres-
sion is based on the counter-factual assumption, 
hence the emotion does not exist at all (S4 and 
S5 in Figure 3). Although Chinese and English 
have different interpretations about the modal 
structure, for emotion analysis, those sentences 
often do not express an emotion. Therefore, to 
ensure the quality of the emotion corpus, all sen-
tences containing a negation structure or a modal 
structure, which are detected by some rules plus 
a list of keywords (negation polarity words for 
the negation structure, and modal words for the 
modal structure), are removed. 
 
To overcome the high ambiguity of some 
emotion keywords, after Step (2), for each emo-
tion keyword, five sentences are randomly se-
lected and annotated by two annotators. If the 
accuracy of five sentences is lower than 40%, 
this emotion keyword is removed from our emo-
tion taxonomy. Finally, 191 Chinese keywords 
and 645 English keywords are remained.  
Tokuhisa et al found that a big challenge for 
emotion computing, especially for emotion de-
tection, is to collect neutral sentences. Since 
neutral sentences are unmarked and hard to de-
tect, we develop a na?ve yet effective algorithm 
S1  (Neg_Happiness): I am not happy about that. 
S2 (Netural): Though the palazzo is our family home, my 
father had never been very happy there. 
S3  (Pos_Happiness): I 've never been so happy. 
S4  (Netural): I can die happy if you will look after them when 
I have gone.  
S5  (Netural): Then you could move over there and we'd all be 
happy. 
Figure 3: Structures for emotion shift 
6
to create a neutral corpus. A sentence is consid-
ered as neutral only when the sentence itself and 
its context (i.e. the previous sentence and the 
following sentence) do not contain any of the 
given emotion keywords. 
We run our emotion sentence extraction and 
neutral sentence extraction on three corpora: the 
Sinica Corpus (Chinese), the Chinese Gigaword 
Corpus, and the British National Corpus (BNC, 
English), and create three emotion corpora and 
three neutral corpora separately. The Sinica 
Corpus is a balanced Chinese corpus, which in-
cludes documents in 15 kinds of genres; The 
Chinese Gigaword Corpus is a huge collection 
of news reports; The BNC is also a balanced 
corpus, which collects documents from different 
domains.  
To estimate the accuracy of our emotion sen-
tence extraction, we randomly select about 1000 
sentences from the three emotion corpora, and 
have two annotators to check it. Table 3 lists the 
accuracy of those emotions sentences (emotion 
corpus.) To test how good this straightforward 
neutral sentence extraction strategy is, about 
1000 sentences are randomly selected from each 
of the three neutral corpora and are checked by 
two annotators. Table 3 lists the accuracy of 
those neutral sentences (neutral corpus.)  
 Emotion corpus Neutral corpus 
Gigaword 82.17 98.61 
Sinica 77.56 98.39 
BNC 69.36 99.50 
Table 3: The accuracy of the emotion-driven corpora 
From Table 3, the high accuracy of neutral 
corpus proves that our approach is effective in 
extracting neutral sentences from the document-
based corpus which contains contextual informa-
tion. Although the accuracy of emotion corpus is 
lower, it is still much higher than the one re-
ported by Kozareva et al (2008), i.e. 49.4. The 
accuracy is significantly increased by deleting 
ambiguous emotion keywords in Step (3). For 
the 2,474 randomly selected Chinese sentences, 
the overall accuracy of the remaining 1,751 sen-
tence is increased by about 14% after Step (3). 
For the 803 randomly selected English sentences, 
the accuracy of the remaining 473 sentence is 
increased about 21% after Step (3). Whether or 
how the ambiguous emotion keywords in Step 3 
are removed is a tradeoff between the coverage 
and the accuracy of the emotion corpus.  
From Table 3, we also find that the accuracy 
of English emotion corpus is much lower than 
Chinese emotion corpus, which indicates Eng-
lish emotion sentences expressed by emotion 
keywords are more ambiguous than that of Chi-
nese. Moreover, during our emotion corpus 
building, 20.2% of Sinica sentences and 22.4% 
of Gigaword sentences are removed in Step (2) 
and (3), on the contrary, 41.2% of BNC sen-
tences are deleted. Although it is more difficult 
to develop the rules in Step (2) and (3) for Chi-
nese than for English, it also confirms the higher 
ambiguity of emotion expressions in English due 
to the ambiguity of emotion keyword. Finally, 
because of the comparatively-high percentage of 
the sentences removed in Step (2) and (3), more 
exploration about those sentences is needed, 
such as the emotion distribution, the expression 
patterns and so on, and how to re-incorporate 
them into the emotion corpus without hurting the 
whole quality is also our future work.  
We also explore emotions through the sen-
tences (no-emotion-keyword sentences) that do 
not contain any given emotion keyword, because 
our approach extracts only partial neutral sen-
tences and partial emotion sentences in reality. 
For each corpus, about 1000 no-emotion-
keyword sentences are randomly selected and 
checked by two annotators. It is surprising that 
only about 1% of those sentences express emo-
tions. This indicates that it is important for real 
emotion computing, which mainly works on 
formal written text, to deal with the emotion ex-
pressions which contain emotion keywords and 
however are ambiguous, such as the sentences 
deleted in Steps (2) and (3). More exploration is 
needed for the emotion and neutral sentence dis-
tribution on other kinds of written text, such as 
blogs, and on spoken text. 
The unsupervised corpus creation approach 
can easily be adapted for different languages and 
different emotion applications, provided that the 
keyword collection and patterns in Step (2) and 
(3) need some changes.  Moreover, another big 
advantage of our approach is that it can avoid 
the controversy during emotion annotation. 
Emotion is subjective, and therefore disagree-
ment for emotion types often arises if the emo-
tion is not expressed through an explicit emotion 
keyword.  
Overall, the annotated corpus created by the 
unsupervised approach has a comparatively high 
quality, and is suitable for the emotion comput-
ing. As the size of the neutral corpus is much 
bigger than its corresponding emotion corpus, to 
avoid model bias, we randomly select some neu-
tral sentences from the neutral corpus, combin-
7
ing with its corresponding emotion sentences to 
form a complete emotion-driven corpus. 
5 Emotion computing system 
In this paper, we present some pilot work to 
prove that our emotion-driven corpus is useful 
for emotion computing. With the inclusion of 
neutral sentences, emotion detection and classi-
fication is simplified into a general classification 
problem, and a supervised machine learning 
method can be directly applied if enough anno-
tated data are obtained. Here, we choose the 
MaxEnt learning in Mallet as a classifier. 
 Both the Sinica Corpus and the Chinese Gi-
gaword Corpus are segmented, and POS-tagged. 
This allows us to implement the bag-of-words 
approach in the focus sentences in both Chinese 
and English. However, emotions are mostly hu-
man attitudes or expectations arising from situa-
tions, where situations are often expressed in 
more than a single word. Such kind of situations 
tends to be more easily extracted by word bi-
grams (2-gram word) than by word unigram (1-
gram word.) To take this into account, besides 1-
gram words, we also extract word bi-grams from 
the focus sentences.  
There are too many emotion types in our cor-
pus, which can cause data sparse; therefore, we 
choose the most frequent emotions to do explo-
ration. Besides the five primary emotions, for 
Chinese, we select another nine complex emo-
tions, and for English, we select another four 
complex emotions. Other emotion types are re-
named as ?Other Emotions.? 
Since Chinese emotion-driven corpus is much 
larger than the English one, to fairly compare the 
performance, we reduce the size of Chinese cor-
pus in our experiments. Then, for each corpus, 
we reserve 80% as the training data, 10% as the 
development data, and 10% as the test data 
(there are two sets of test data as follows.) In the 
evaluation, for each emotion sentence, if our 
system detects one of its emotion tags, we con-
sider this sentence is correctly tagged. 
Test data set 1 (TDS 1): contains about 10% 
of the sentences from the complete emotion-
driven corpus, and emotion tags are automati-
cally given during the corpus creation.  
Test data set 2 (TDS 2): contains the sen-
tences used in Table 3, which is checked by two 
annotators. If more than one emotion tags co-
exist in a sentence, all of them are chosen to la-
bel the sentence. If there exists an emotion that 
does not belong to any of the emotion types, it is 
labeled as ?Other Emotions.? 
Table 4 shows the performance (accuracy) of 
our system for Test data set 1 and 2 for both 
Chinese and English. We notice that our corpus 
creation approach is effective for emotion com-
puting. As we expect, the 2-gram words can par-
tially catch the emotion stimulus, and improves 
the performances. However, the overall per-
formance is still very low, which indicates that 
emotion computing is a difficult task. From the 
error analysis, it is surprised that for Chinese, 
the mislabeling of emotion sentences as neutral 
sentences (?emotion? vs. ?neutral?) is a common 
error, and whereas, for English, two kinds of 
errors: ?emotion? vs. ?neutral? and ?focus emo-
tions? vs. ?Other emotions? (the mislabeling of a 
sentence with a focus emotion as ?Other emo-
tions,?) occupy at least 50%. The error distribu-
tion confirms the importance of emotion detec-
tion during emotion computing. The high fre-
quency of the error of ?focus emotions? vs. 
?Other Emotions? in English may be because 
there are fewer focus emotion types for English.  
 1-gram words  {1,2}-gram words 
Chinese TDS 1 53.92 58.75 
English TDS 1 44.02 48.20 
Chinese TDS 2 37.18 39.95 
English TDS 2 33.24 36.31 
Table 4: The performances of our system for the test data  
6 Conclusion 
Emotion, no matter its annotation or computing, 
is still a new and difficult topic. In this paper, we 
apply emotion theories to design a cognitive-
based emotion annotation scheme, which are 
robust and versatile so that it can encode differ-
ent levels of emotion information for different 
emotion computing. Moreover, motivated from 
NSM, we develop an unsupervised approach to 
create a large and comparatively high-quality 
corpus for emotion computing, which is proven 
in our pilot experiments to be useful. Moreover, 
this approach makes emotion computing for dif-
ferent applications possible through a little mod-
ification. 
Certainly, there are some issues remaining un-
solved. For corpus construction, we will explore 
emotion distribution in other kinds of corpora, 
such as blog and dialog, and make analysis of 
ambiguous emotion sentences, such as negation 
structure and modal structure. For emotion com-
puting, we did only pilot experiments and more 
work needs to be done, such as feature extrac-
tion. 
8
References  
W. B. Cannon. 1927. The James-Lange theory of 
emotions: A Critical Examination and an Alterna-
tive Theory. American Journal of Psychology, 39, 
106-124. 
Y. Chen, S. Y. M. Lee and C. R. Huang, 2009. Con-
struction of Chinese Emotion Corpus with an Un-
supervised Approach. In CNCCL-2009, 2009. (in 
Chinese) 
T. Hashimoto, S. Hiramatsu, T. Tsuji and H. Kobaya-
shi. 2006. Development of the Face Robot SAYA 
for Rich Facial Expressions. SICE-ICASE Inter-
national Joint Conference, Busan,Korea. 
J. Hobbs and A. Gordon. 2008. The Deep Lexical 
Semantics of Emotions. Workshop on Sentiment 
Analysis: Emotion, Metaphor, Ontology and 
Terminology (EMOT-08), 6th International con-
ference on Language Resources and Evaluation 
(LREC-08), Marrakech, Morocco, May 27, 2008. 
P. Livia, A. Zaenen. 2004. Contextual Valence Shift-
ers. In Shanahan, J. G., Y. Qu, and J. Wiebe 
(Eds.), Computing Attitude and Affect in Text: 
Theory and Applications, pp. 1-10. 
Z. Kozareva, Borja Navarro, Sonia Vazquez, and 
Andres Nibtoyo. 2007. UA-ZBSA: A Headline 
Emotion Classification through Web Information. 
In Proceedings of the 4th International Workshop 
on Semantic Evaluations.  
Y. Y. Mathieu. 2005.  Annotations of Emotions and 
Feelings in Texts. In Conference on Affective 
Computing and intelligent Interaction (ACII2005), 
Beijing, Springer Lecture Notes in Computer Sci-
ence, pp. 350-357. 
R. Mihalcefa, and Hugo Liu. 2006. A Corpus-based 
Approach to Finding Happiness. In Proceedings 
of AAAI.  
R. Plutchik. 1991. The Emotions. University Press of 
America, Inc. 
R. Plutchik. 1980. Emotions: A psychoevolutionary 
synthesis. New York:Harper & Row. 
M. Schr?der, H. Pirker and M. Lamolle. 2006. First 
suggestions for an emotion annotation and repre-
sentation language. In L. Deviller et al (Ed.), 
Proceedings of LREC'06 Workshop on Corpora 
for Research on Emotion and Affect (pp. 88-92). 
Genoa, Italy. 
J. G. Shanahan, Y. Qu and J. Wiebe. 2006. Comput-
ing attitude and affect in text: theory and applica-
tions, Springer. 
R. Tokuhisa, K. Inui, and Y. Matsumoto (Eds.) 2008. 
Emotion Classification Using Massive Examples 
Extracted from the Web. COLING.   
J. H. Turner. 2007. Human Emotions: A sociological 
theory. New York : Routledge, 2007. 
J. H. Turner. 2000. On the origins of human emotions: 
A sociological inquiry into the evolution of hu-
man affect. Stanford, CA: Stanford University 
Press.  
J. H. Turner. 1996. The Evolution of Emotions in 
Humans: A Darwinian?Durkheimian Analysis. 
Journal for the theory of social behaviour26:1-34 
L. Xu, H. Lin, J. ZHAO.2008. Construction and 
Analysis of Emotional Corpus. JOURNAL OF 
CHINESE INFORMA TION PROCESSIN. 
X. Y. Xu, and J. H. Tao. 2003. The study of affective 
categorization in Chinese. The 1st Chinese Con-
ference on Affective Computing and Intelligent 
Interaction. Beijing, China. 
A. Wierzbicka, 1996. Semantics: Primes and Univer-
sals. Oxford: Oxford University Press. 
 
9
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 19?27,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Wiktionary and NLP: Improving synonymy networks
Emmanuel Navarro
IRIT, CNRS &
Universit? de Toulouse
navarro@irit.fr
Franck Sajous
CLLE-ERSS, CNRS &
Universit? de Toulouse
sajous@univ-tlse2.fr
Bruno Gaume
CLLE-ERSS & IRIT, CNRS &
Universit? de Toulouse
gaume@univ-tlse2.fr
Laurent Pr?vot
LPL, CNRS &
Universit? de Provence
laurent.prevot@lpl-aix.fr
Hsieh ShuKai
English Department
NTNU, Taiwan
shukai@gmail.com
Kuo Tzu-Yi
Graduate Institute of Linguistics
NTU, Taiwan
tzuyikuo@ntu.edu.tw
Pierre Magistry
TIGP, CLCLP, Academia Sinica,
GIL, NTU, Taiwan
pmagistry@gmail.com
Huang Chu-Ren
Dept. of Chinese and Bilingual Studies
Hong Kong Poly U. , Hong Kong.
churenhuang@gmail.com
Abstract
Wiktionary, a satellite of the Wikipedia
initiative, can be seen as a potential re-
source for Natural Language Processing.
It requires however to be processed be-
fore being used efficiently as an NLP re-
source. After describing the relevant as-
pects of Wiktionary for our purposes, we
focus on its structural properties. Then,
we describe how we extracted synonymy
networks from this resource. We pro-
vide an in-depth study of these synonymy
networks and compare them to those ex-
tracted from traditional resources. Fi-
nally, we describe two methods for semi-
automatically improving this network by
adding missing relations: (i) using a kind
of semantic proximity measure; (ii) using
translation relations of Wiktionary itself.
Note: The experiments of this paper are based on Wik-
tionary?s dumps downloaded in year 2008. Differences may
be observed with the current versions available online.
1 Introduction
Reliable and comprehensive lexical resources con-
stitute a crucial prerequisite for various NLP tasks.
However their building cost keeps them rare. In
this context, the success of the Princeton Word-
Net (PWN) (Fellbaum, 1998) can be explained by
the quality of the resource but also by the lack of
serious competitors. Widening this observation to
more languages only makes this observation more
acute. In spite of various initiatives, costs make
resource development extremely slow or/and re-
sult in non freely accessible resources. Collabo-
rative resources might bring an attractive solution
to this difficult situation. Among them Wiktionary
seems to be the perfect resource for building com-
putational mono-lingual and multi-lingual lexica.
This paper focuses therefore on Wiktionary, how
to improve it, and on its exploitation for creating
resources.
In next section, we present some relevant infor-
mation about Wiktionary. Section 3 presents the
lexical graphs we are using and the way we build
them. Then we pay some attention to evaluation
(?4) before exploring some tracks of improvement
suggested by Wiktionary structure itself.
2 Wiktionary
As previously said, NLP suffers from a lack of
lexical resources, be it due to the low-quality or
non-existence of such resources, or to copyrights-
related problems. As an example, we consider
French language resources. Jacquin et al (2002)
highlighted the limitations and inconsistencies
from the French EuroWordnet. Later, Sagot and
Fi?er (2008) explained how they needed to re-
course to PWN, BalkaNet (Tufis, 2000) and other
resources (notably Wikipedia) to build WOLF, a
free French WordNet that is promising but still a
very preliminary resource. Some languages are
straight-off purely under-resourced.
The Web as Corpus initiative arose (Kilgarriff
and Grefenstette, 2003) as an attempt to design
tools and methodologies to use the web for over-
coming data sparseness (Keller and Lapata, 2002).
Nevertheless, this initiative raised non-trivial tech-
nical problems described in Baroni et al (2008).
Moreover, the web is not structured enough to eas-
ily and massively extract semantic relations.
In this context, Wiktionary could appear to be
a paradisiac playground for creating various lexi-
19
cal resources. We describe below the Wiktionary
resource and we explain the restrictions and prob-
lems we are facing when trying to exploit it. This
description may complete few earlier ones, for ex-
ample Zesch et al (2008a).
2.1 Collaborative editing
Wiktionary, the lexical companion to Wikipedia,
is a collaborative project to produce a free-content
multilingual dictionary.
1
As the other Wikipedia?s
satellite projects, the resource is not experts-led,
rather filled by any kind of users. The might-be
inaccuracy of the resulting resource has lengthily
been discussed and we will not debate it: see Giles
(2005) and Britannica (2006) for an illustration
of the controversy. Nevertheless, we think that
Wiktionary should be less subject (so far) than
Wikipedia to voluntary misleading content (be it
for ideological, commercial reasons, or alike).
2.2 Articles content
As one may expect, a Wiktionary article
2
may (not
systematically) give information on a word?s part
of speech, etymology, definitions, examples, pro-
nunciation, translations, synonyms/antonyms, hy-
pernyms/hyponyms, etc.
2.2.1 Multilingual aspects
Wiktionary?s multilingual organisation may be
surprising and not always meet one?s expectations
or intuitions. Wiktionaries exist in 172 languages,
but we can read on the English language main
page, ?1,248,097 entries with English definitions
from over 295 languages?. Indeed, a given wik-
tionary describes the words in its own language
but also foreign words. For example, the English
article moral includes the word in English (adjec-
tive and noun) and Spanish (adjective and noun)
but not in French. Another example, boucher,
which does not exist in English, is an article of the
English wiktionary, dedicated to the French noun
(a butcher) and French verb (to cork up).
A given wiktionary?s ?in other languages? left
menu?s links, point to articles in other wiktionar-
ies describing the word in the current language.
For example, the Fran?ais link in the dictionary
article of the English wiktionary points to an arti-
cle in the French one, describing the English word
dictionary.
1
http://en.wiktionary.org/
2
What article refers to is more fuzzy than classical entry
or acceptance means.
2.2.2 Layouts
In the following paragraph, we outline wik-
tionary?s general structure. We only consider
words in the wiktionary?s own language.
An entry consists of a graphical form and a cor-
responding article that is divided into the follow-
ing, possibly embedded, sections:
? etymology sections separate homonyms when
relevant;
? among an etymology section, different parts
of speech may occur;
? definitions and examples belong to a part of
speech section and may be subdivided into sub-
senses;
? translations, synonyms/antonyms and hy-
pernyms/hyponyms are linked to a given part of
speech, with or without subsenses distinctions.
In figure 1 is depicted an article?s layout example.
Figure 1: Layout of boot article (shortened)
About subsenses, they are identified with an in-
dex when first introduced but they may appear as
a plain text semantic feature (without index) when
used in relations (translations, synonyms, etc.). It
is therefore impossible to associate the relations
arguments to subsenses. Secondly, subsense index
appears only in the current word (the source of the
relation) and not in the target word?s article it is
linked to (see orange French N. and Adj., Jan. 10,
2008
3
).
A more serious issue appears when relations are
shared by several parts of speech sections. In Ital-
3
http://fr.wiktionary.org/w/index.php?
title=orange&oldid=2981313
20
ian, both synonyms and translations parts are com-
mon to all words categories (see for example car-
dinale N. and Adj., Apr. 26, 2009
4
).
2.3 Technical issues
As Wikipedia and the other Wikimedia Founda-
tion?s projects, the Wiktionary?s content manage-
ment system relies on the MediaWiki software
and on the wikitext. As stated in Wikipedia?s
MetaWiki article, ?no formal syntax has been de-
fined? for the MediaWiki and consequently it is
not possible to write a 100% reliable parser.
Unlike Wikipedia, no HTML dump is available
and one has to parse the Wikicode. Wikicode
is difficult to handle since wiki templates require
handwritten rules that need to be regularly up-
dated. Another difficulty is the language-specific
encoding of the information. Just to mention one,
the target language of a translation link is iden-
tified by a 2 or 3 letters ISO-639 code for most
languages. However in the Polish wiktionary the
complete name of the language name (angielski,
francuski, . . . ) is used.
2.4 Parsing and modeling
The (non-exhaustive) aforementioned list of diffi-
culties (see ?2.2.2 and ?2.3) leads to the following
consequences:
? Writing a parser for a given wiktionary is
possible only after an in-depth observation of its
source. Even an intensive work will not prevent
all errors as long as (i) no syntax-checking is made
when editing an article and (ii) flexibility with the
?tacitly agreed? layout conventions is preserved.
Better, flexibility is presented as a characteristic of
the framework:
?[. . . ] it is not a set of rigid rules. You may
experiment with deviations, but other editors
may find those deviations unacceptable, and
revert those changes. They have just as much
right to do that as you have to make them.
5
?
Moreover, a parser has to be updated every new
dump, as templates, layout conventions (and so
on) may change.
?Writing parsers for different languages is not a
simple adjustment, rather a complete overhaul.
? When extracting a network of semantic rela-
tions from a given wiktionary, some choices are
more driven by the wiktionary inner format than
scientific modelling choices. An illustration fol-
4
http://it.wiktionary.org/w/index.php?
title=cardinale&oldid=758205
5
http://en.wiktionary.org/wiki/WT:ELE
lows in ?3.2. When merging information extracted
from several languages, the homogenisation of the
data structure often leads to the choice of the poor-
est one, resulting in a loss of information.
2.5 The bigger the better?
Taking advantage of colleagues mastering various
languages, we studied the wiktionary of the fol-
lowing languages: French, English, German, Pol-
ish and Mandarin Chinese. A first remark con-
cerns the size of the resource. The official num-
ber of declared articles in a given wiktionary in-
cludes a great number of meta-articles which are
not word entries As of April 2009, the French wik-
tionary reaches the first rank
6
, before the English
one. This can be explained by the automated im-
port of public-domain dictionaries articles (Littr?
1863 and Dictionnaire de l?Acad?mie Fran?aise
1932-1935). Table 1 shows the ratio between the
total number of articles and the ?relevant? ones
(numbers based on year 2008 snapshots).
Total Meta
?
Other
??
Relevant
fr 728,266 25,244 369,948 337,074 46%
en 905,963 46,202 667,430 192,331 21%
de 88,912 7,235 49,672 32,005 36%
pl 110,369 4,975 95,241 10,153 9%
zh 131,752 8,195 112,520 1,037 0.7%
?
templates definitions, help pages, user talks, etc.
??
other languages, redirection links, etc.
Table 1: Ratio of ?relevant? articles in wiktionaries
By ?relevant?, we mean an article about a word
in the wiktionary?s own language (e.g. not an
article about a French word in the English Wik-
tionary). Among the ?relevant? articles, some
are empty and some do not contain any transla-
tion nor synonym link. Therefore, before deciding
to use Wiktionary, it is necessary to compare the
amount of extracted information contribution and
the amount of work required to obtain it .
3 Study of synonymy networks
In this section, we study synonymy networks built
from different resources. First, we introduce
some general properties of lexical networks (?3.1).
Then we explain how we build Wiktionary?s syn-
onymy network and how we analyse its proper-
ties. In ?3.3, we show how we build similar graphs
from traditional resources for evaluation purposes.
3.1 Structure of lexical networks
In the following sections, a graph G = (V,E)
is defined by a set V of n vertices and a set
E ? V
2
of m edges. In this paper, V is
6
http://meta.wikimedia.org/wiki/List_
of_Wiktionaries
21
a set of words and E is defined by a relation
E
R
7?? E : (w
1
, w
2
) ? E if and only if w
1
R
? w
2
.
Most of lexical networks, as networks extracted
from real world, are small worlds (SW) net-
works. Comparing structural characteristics of
wiktionary-based lexical networks to some stan-
dard resource should be done according to well-
known properties of SW networks (Watts and
Strogatz, 1998; Barabasi et al, 2000; Newman,
2003; Gaume et al, 2008). These properties are:
? Edge sparsity: SW are sparse in edges
m = O(n) or m = O(n log(n))
? Short paths: in SW, the average path length
(L)
7
is short. Generally there is at least one short
path between any two nodes.
? High clustering: in SW, the clustering coef-
ficient (C) that expresses the probability that two
distinct nodes adjacent to a given third one are ad-
jacent, is an order of magnitude higher than for
Erdos-Renyi (random) graphs: C
SW
 C
random
;
this indicates that the graph is locally dense, al-
though it is globally sparse.
?Heavy-tailed degree distribution: the distri-
bution of the vertices incidence degrees follows a
power law in a SW graph. The probability P (k)
that a given node has k neighbours decreases as a
power law, P (k) ? k
a
(a being a constant charac-
teristic of the graph). Random graphs conforms to
a Poisson Law.
3.2 Wiktionary?s network
Graph extraction Considering what said in
?2.2.2 and ?2.4, we made the following choices:
8
? Vertices: a vertex is built for each entry?s part
of speech.
? Parts of speech: when modeling the links
from X (X having for part of speech Pos
X
) to
one of its synonyms Y , we assume that Pos
Y
=
Pos
X
, thus building vertex Pos
Y
.Y.
? Subsenses: subsenses are flattened. First, the
subsenses are not always mentioned in the syn-
onyms section. Second, if we take into account
the subsenses, they only appear in the source of the
relation. For example, considering in figure 1 the
relation boot
syn
??? kick (both nouns), and given the
10 subsenses for boot and the 5 ones for kick, we
should build 15 vertices. And we should then add
7
Average length of the shortest path between any two
nodes.
8
These choices can clearly be discussed from a linguis-
tic point of view and judged to be biased. Nevertheless, we
adopted them as a first approximation to make the modelling
possible.
all the links between the mentioned boot?s sub-
senses and the 5 kick?s existing subsenses. This
would lead to a high number of edges, but the
graph would not be closer to the reality. The way
subsenses appear in Wiktionary are unpredictable.
"Subsenses" correspond sometimes to homonyms
or clear-cut senses of polysemous words, but can
also correspond to facets, word usage or regu-
lar polysemy. Moreover, some entries have no
subsenses distinction whereas it would be wor-
thy. More globally, the relevance of discrete word
senses has been seriously questioned, see (Victorri
and Fuchs, 1996) or (Kilgarriff, 1997) for very
convincing discussions. Two more practical rea-
sons led us to this choice. We want our method to
be reproducible for other languages and some wik-
tionaries do not include subsenses. At last, some
gold standard resources (eg. Dicosyn) have their
subsenses flattened too and we want to compare
the resources against each other.
? Edges: wiktionary?s synonymy links are ori-
ented but we made the graph symmetric. For ex-
ample, boot does not appear in kick?s synonyms.
Some words even appear as synonyms without be-
ing an entry of Wiktionary.
From the boot example (figure 1), we extract ver-
tices {N.boot, V.boot}, build {N.buskin,
N.kick, V.kick} and we add the follow-
ing (symmetrized) edges: N.boot?N.buskin,
N.boot?N.kick and V.boot?V.kick.
Graph properties By observing the table 2, we
can see that the graphs of synonyms extracted
from Wiktionary are all typical small worlds. In-
deed their l
lcc
remains short, their C
lcc
is always
greater or equal than 0.2 and their distribution
curves of the vertices incidence degree is very
close to a power law (a least-square method gives
always exponent a
lcc
? ?2.35 with a confidence
r
2
lcc
always greater than 0.89). It can also be seen
that the average incidence k
lcc
ranges from 2.32
to 3.32.
9
It means that no matter which language
9
It is noteworthy that the mean incidence of vertices is al-
most always the same (close to 2.8) no matter the graph size
is. If we assume that all wiktionary?s graphs grow in a similar
way but at different speed rates (after all it is the same frame-
work), graphs (at least their statistical properties) from differ-
ent languages can be seen as snapshots of the same graph at
different times. This would mean that the number of graphs
edges tends to grow proportionally with the number of ver-
tices. This fits with the dynamic properties of small worlds
(Steyvers and Tenenbaum, 2005). It means that for a wik-
tionary system, even with many contributions, graph density
is likely to remain constant and we will see that in compar-
ison to traditional lexical resources this density is quite low.
22
graph n m n
lcc
m
lcc
k
lcc
l
lcc
C
lcc
a
lcc
r
2
lcc
fr-N 18017 9650 3945 4690 2.38 10.18 0.2 -2.03 0.89
fr-A 5411 2516 1160 1499 2.58 8.86 0.23 -2.04 0.95
fr-V 3897 1792 886 1104 2.49 9.84 0.21 -1.65 0.91
en-N 22075 11545 3863 4817 2.49 9.7 0.24 -2.31 0.95
en-A 8437 4178 2486 3276 2.64 8.26 0.2 -2.35 0.95
en-V 6368 3274 2093 2665 2.55 8.33 0.2 -2.01 0.93
de-N 32824 26622 12955 18521 2.86 7.99 0.28 -2.16 0.93
de-A 5856 6591 3690 5911 3.2 6.78 0.24 -1.93 0.9
de-V 5469 7838 4574 7594 3.32 5.75 0.23 -1.92 0.9
pl-N 8941 4333 2575 3143 2.44 9.85 0.24 -2.31 0.95
pl-A 1449 731 449 523 2.33 7.79 0.21 -1.71 0.94
pl-V 1315 848 601 698 2.32 5.34 0.2 -1.61 0.92
n: number of vertices m: number of edges
k: avg. number of neighbours per vertex l: avg. path length between vertices
C: clustering rate a: power law exponent with r
2
confidence
_
lcc
: denotes on largest connected component
Table 2: Wiktionary synonymy graphs properties
or part of speech, m = O(n) as for most of SW
graphs (Newman, 2003; Gaume et al, 2008).
3.3 Building synonymy networks from
known standards
WordNet There are many possible ways for
building lexical networks from PWN. We tried
several methods but only two of them are worth
to be mentioned here. The graphs we built have
words as vertices, not synsets or senses. A first
straightforward method (method A) consists in
adding an edge between two vertices only if the
corresponding words appear as elements of the
same synset. This method produced many discon-
nected graphs of various sizes. Both the compu-
tational method we planned to use and our intu-
itions about such graphs were pointing towards a
bigger graph that would cover most of the lexical
network.
We therefore decided to exploit the hypernymy
relation. Traditional dictionaries indeed propose
hypernyms when one look for synonyms of very
specific terms, making hypernymy the closest re-
lation to synonymy at least from a lexicographic
viewpoint. However, adding all the hypernymy re-
lations resulted in a network extremely dense in
edges with some vertices having a high number of
neighbours. This was due to the tree-like organi-
sation of WordNet that gives a very special impor-
tance to higher nodes of the tree.
In the end we retained method B that consists in
adding edges in following cases:
? if two words belong to the same synset;
? if a word only appears in a synset that is a leaf
of the tree and contains only this word, then cre-
ate edges linking to words included in the hyper-
nym(s) synset.
We would like to study the evolution through time of wik-
tionaries, however this is outside the scope of this paper.
Therefore when a vertice w do not get any neigh-
bour according to method A, method B adds edges
linking w to words included in the hypernym(s)
synset of the synset {w}. We only added hyper-
nyms for the leaves of the tree in order to keep our
relations close to the synonymy idea. This idea has
already been exploited for some WordNet-based
semantic distances calculation taking into account
the depth of the relation in the tree (Leacock and
Chodorow, 1998).
Dicosyn graphs Dicosyn is a compilation of
synonym relations extracted from seven dictionar-
ies (Bailly, Benac, Du Chazaud, Guizot, Lafaye,
Larousse and Robert):
10
there is an edge r ? s if
and only if r and s have the same syntactic cate-
gory and at least one dictionary proposes s being
a synonym in the dictionary entry r. Then, each
of the three graphs (Nouns, Verbs, Adjectives) ob-
tained is made symmetric (dicosyn-fr-N, dicosyn-
fr-V and dicosyn-fr-A).
Properties of the graphs extracted Table 3
sums-up the structural properties of the synonyms
networks built from standard resources.
We can see that all the synonymy graphs ex-
tracted from PWN or Dicosyn are SW graphs.
Indeed their l
lcc
remains short, their C
lcc
is al-
ways greater or equal than 0.35 and their distri-
bution curves of the vertices incidence degree is
very close to a power law (a least-square method
gives always exponent a
lcc
near of ?2.30 with a
confidence r
2
lcc
always greater than 0.85). It can
also be observed that no matter the part of speech,
the average incidence of Dicosyn-based graphs is
always lower than WordNet ones.
10
Dicosyn has been first produced at ATILF, before being
corrected at CRISCO laboratory.
(http://elsap1.unicaen.fr/dicosyn.html)
23
graph n m n
lcc
m
lcc
k
lcc
l
lcc
C
lcc
a
lcc
r
2
lcc
pwn-en-N-A 117798 104929 12617 28608 4.53 9.89 0.76 -2.62 0.89
pwn-en-N-B 117798 168704 40359 95439 4.73 7.79 0.72 -2.41 0.91
pwn-en-A-A 21479 22164 4406 11276 5.12 9.08 0.75 -2.32 0.85
pwn-en-A-B 21479 46614 15945 43925 5.51 6.23 0.78 -2.09 0.9
pwn-en-V-A 11529 23019 6534 20806 6.37 5.93 0.7 -2.34 0.87
pwn-en-V-B 11529 40919 9674 39459 8.16 4.66 0.64 -2.06 0.91
dicosyn-fr-N 29372 100759 26143 98627 7.55 5.37 0.35 -2.17 0.92
dicosyn-fr-A 9452 42403 8451 41753 9.88 4.7 0.37 -1.92 0.92
dicosyn-fr-V 9147 51423 8993 51333 11.42 4.2 0.41 -1.88 0.91
Table 3: Gold standard?s synonymy graphs properties
4 Wiktionary graphs evaluation
Coverage and global SW analysis By compar-
ing tables 2 and 3, one can observe that:
? The lexical coverage of Wiktionary-based syn-
onyms graphs is always quantitatively lower than
those of standard resources although this may
change. For example, to horn (in PWN), absent
from Wiktionary in 2008, appeared in 2009. At
last, Wiktionary is more inclined to include some
class of words such as to poo (childish) or to
prefetch, to google (technical neologisms).
? The average number of synonyms for an en-
try of a Wiktionary-based resource is smaller than
those of standard resources. For example, com-
mon synonyms such as to act/to play appear in
PWN and not in Wiktionary. Nevertheless, some
other appear (rightly) in Wiktionary: to reduce/to
decrease, to cook/to microwave.
? The clustering rate of Wiktionary-based
graphs is always smaller than those of standard re-
sources. This is particularly the case for English.
However, this specificity might be due to differ-
ences between the resources themselves (Dicosyn
vs. PWN) rather than structural differences at the
linguistic level.
Evaluation of synonymy In order to evaluate
the quality of extracted synonymy graphs from
Wiktionary, we use recall and precision measure.
The objects we compare are not simple sets but
graphs (G = (V ;E)), thus we should compare
separately set of vertices (V ) and set of edges (E).
Vertices are words and edges are synonymy links.
Vertices evaluation leads to measure the resource
(a) English Wiktionary vs. Wordnet
Precision Recall
Nouns 14120/22075 = 0.64 14120/117798 = 0.12
Adj. 5874/8437 = 0.70 5874/21479 = 0.27
Verbs 5157/6368 = 0.81 5157/11529 = 0.45
(b) French Wiktionary vs. Dicosyn
Precision Recall
Nouns 10393/18017 = 0.58 10393/29372 = 0.35
Adj. 3076/5411 = 0.57 3076/9452 = 0.33
Verbs 2966/3897 = 0.76 2966/9147 = 0.32
Table 4: Wiktionary coverage
coverage whereas edges evaluation leads to mea-
sure the quality of the synonymy links in Wik-
tionary resource.
First of all, the global picture (table 4) shows
clearly that the lexical coverage is rather poor. A
lot of words included in standard resources are not
included yet in the corresponding wiktionary re-
sources. Overall the lexical coverage is always
lower than 50%. This has to be kept in mind while
looking at the evaluation of relations shown in ta-
ble 5. To compute the relations evaluation, each
resource has been first restricted to the links be-
tween words being present in each resource.
About PWN, since every link added with
method A will also be added with method B, the
precision of Wiktionary-based graphs synonyms
links will be always lower for "method A graphs"
than for "method B graphs". Precision is rather
good while recall is very low. That means that a
lot of synonymy links of the standard resources
are missing within Wiktionary. As for Dicosyn,
the picture is similar with even better precision but
very low recall.
5 Exploiting Wiktionary for improving
Wiktionary
As seen in section 4, Wiktionary-based resources
are very incomplete with regard to synonymy. We
propose two tasks for adding some of these links:
Task 1: Adding synonyms to Wiktionary by
taking into account its Small World characteristics
for proposing new synonyms.
(a) English wiktionary vs. Wordnet
Precision Recall
Nouns (A) 2503/6453 = 0.39 2503/11021 = 0.23
Nouns (B) 2763/6453 = 0.43 2763/18440 = 0.15
Adj. (A) 786/3139 = 0.25 786/5712 = 0.14
Adj. (B) 1314/3139 = 0.42 1314/12792 = 0.10
Verbs (A) 866/2667 = 0.32 866/10332 = 0.08
Verbs (B) 993/2667 = 0.37 993/18725 = 0.05
(b) French wiktionary vs. Dicosyn
Precision Recall
Nouns 3510/5075 = 0.69 3510/44501 = 0.08
Adj. 1300/1677 = 0.78 1300/17404 = 0.07
Verbs 899/1267 = 0.71 899/23968 = 0.04
Table 5: Wiktionary synonymy links precision & recall
24
Task 2: Adding synonyms to Wiktionary by
taking into account the translation relations.
We evaluate these two tasks against the bench-
marks presented in section 3.2.
5.1 Improving synonymy in Wiktionary by
exploiting its small world structure
We propose here to enrich synonymy links of Wik-
tionary by taking into account that lexical net-
works have a high clustering coefficient. Our hy-
pothesis is that missing links in Wiktionary should
be within clusters.
A high clustering coefficient means that two
words which are connected to a third one are likely
to be connected together. In other words neigh-
bours of my neighbours should also be in my
neighbourhood. We propose to reverse this prop-
erty to the following hypothesis: "neighbour of my
neighbours which are not in my neighbourhood
should be a good neighbour candidate". Thus the
first method we test consist simply in connecting
every vertex to neighbours of its neighbours. One
can repeat this operation until the expected num-
ber of edges is obtained.
11
Secondly we used the PROX approach pro-
posed by (Gaume et al, 2009). It is a stochastic
method designed for studying ?Hierarchical Small
Worlds?. Briefly put, for a given vertex u, one
computes for all other vertices v the probability
that a randomly wandering particle starting from
u stands in v after a fixed number of steps. Let
P (u, v) be this value. We propose to connect u
to the k first vertices ranked in descending order
with respect of P (u, v). We always choose k pro-
portionally to the original degree of u (number of
neighbours of u).
For a small number of steps (3 in our case) ran-
dom wanderings tend to be trapped into local clus-
ter structures. So a vertex v with a high P (u, v) is
likely to belong to the same cluster as u, which
means that a link u?v might be relevant.
Figure 2 shows precision, recall and f-score
evolution for French verbs graph when edges are
added using ?neighourhood? method (neigh), and
using ?Prox? method. Dashed line correspond to
the value theoretically obtained by choosing edges
at random. First, both methods are clearly more
efficient than a random addition, which is not sur-
prising but it seems to confirm our hypothesis that
missing edges are within clusters. Adding sharply
11
We repeat it only two times, otherwise the number of
added edges is too large.
0 2000 4000 6000 8000 10000 12000 140000.0
0.10.2
0.30.4
0.50.6
0.70.8
P
prox3neighrandom
0 2000 4000 6000 8000 10000 12000 140000.03
0.040.05
0.060.07
0.080.09
R
0 2000 4000 6000 8000 10000 12000 140000.05
0.060.07
0.080.09
0.100.11
0.120.13
F
fr.V
Figure 2: Precision, recall and F-score of French verbs
graph enlarged using only existing synonymy links
neighbours of neighbours seems to be as good as
adding edges ranked by Prox, anyway the rank
provided by Prox permits to add a given number
of edges. This ranking can also be useful to order
potential links if one think about a user validation
system. Synonyms added by Prox and absent from
gold standards are not necessarily false.
For example Prox proposes a relevant link ab-
solve/forgive, not included in PWN. Moreover,
many false positive are still interesting to consider
for improving the resource. For example, Prox
adds relations such as hypernyms (to uncover/to
peel) or inter-domain ?synonyms? (to skin/to peel).
This is due to high clustering (see ?3.1) and to
the fact that clusters in synonymy networks corre-
lates with language concepts (Gaume et al, 2008;
Duvignau and Gaume, 2008; Gaume et al, 2009;
Fellbaum, 1999).
Finally note that results are similar for other
parts of speech and other languages.
5.2 Using Wiktionary?s translation links to
improve its synonymy network
Assuming that two words sharing many transla-
tions in different languages are likely to be syn-
onymous, we propose to use Wiktionary?s transla-
tion links to enhance the synonymy network of a
given language.
In order to rank links to be potentially added,
we use a simple Jaccard measure: let T
w
be the set
of a word w?s translations, then for every couple
of words (w,w
?
) we have:
Jaccard(w,w
?
) =
|T
w
? T
w
?
|
|T
w
? T
w
?
|
We compute this measure for every possible pair
of words and then, starting from Wiktionary?s syn-
onymy graph, we incrementally add links accord-
ing to their Jaccard rank.
25
We notice first that most of synonymy links
added by this method were not initially included
in Wiktionary?s synonymy network. For exam-
ple, regarding English verbs, 95% of 2000 best
ranked proposed links are new. Hence this method
may be efficient to improve graph density. How-
ever one can wonder about the quality of the new
added links, so we discuss precision in the next
paragraph.
In figure 3 is depicted the evolution of precision,
recall and F-score for French verbs in the enlarged
graph in regard of the total number of edges. We
use Dicosyn graph as a gold standard. The dashed
line corresponds to theoretical scores one can ex-
pect by adding randomly chosen links.
First we notice that both precision and recall
are significantly higher than we can expect from
random addition. This confirms that words shar-
ing the same translations are good synonym candi-
dates. Added links seem to be particularly relevant
at the beginning for higher Jaccard scores. From
the first dot to the second one we add about 1000
edges (whereas the original graph contains 1792
edges) and the precision only decreases from 0.71
to 0.69.
The methods we proposed in this section are
quite simple and there is room for improvement.
First, both methods can be combined in order
to improve the resource using translation links
and then using clusters structure. One can also
think to the corollary task that would consists in
adding translation links between two languages
using synonymy links of others languages.
0 2000 4000 6000 8000 10000 120000.0
0.10.2
0.30.4
0.50.6
0.70.8
P
random
0 2000 4000 6000 8000 10000 120000.02
0.040.06
0.080.10
0.120.14
0.16
R
0 2000 4000 6000 8000 10000 120000.04
0.060.08
0.100.12
0.140.16
0.180.20
0.22
F
fr.V
Figure 3: Precision, recall and F-score of French verbs
graph enlarged using translation links
6 Conclusion and future work
This paper gave us the opportunity to share some
Wiktionary experience related lexical resources
building. We presented in addition two approaches
for improving these resources and their evaluation.
The first approach relies on the small world struc-
ture of synonymy networks. We postulated that
many missing links in Wiktionary should be added
among members of the same cluster. The second
approach assumes that two words sharing many
translations in different languages are likely to be
synonymous. The comparison with traditional re-
sources shows that our hypotheses are confirmed.
We now plan to combine both approaches.
The work presented in this paper combines a
NLP contribution involving data extraction and
rough processing of the data and a mathematical
contribution concerning graph-like resource. In
our viewpoint the second aspect of our work is
therefore complementary of other NLP contribu-
tions, like (Zesch et al, 2008b), involving more
sophisticated NLP processing of the resource.
Support for collaborative editing Our results
should be useful for setting up a more efficient
framework for Wiktionary collaborative editing.
We should be able to always propose a set of syn-
onymy relations that are likely to be. For exam-
ple, when a contributor creates or edits an arti-
cle, he may think about adding very few links but
might not bother providing an exhaustive list of
synonyms. Our tool can propose a list of potential
synonyms, ordered by relevancy. Each item of this
list would only need to be validated (or not).
Diachronic study An interesting topic for future
work is a "diachronic" study of the resource. It
is possible to access Wiktionary at several stages,
this can be used for studying how such resources
evolve. Grounded on this kind of study, one may
predict the evolution of newer wiktionaries and
foresee contributors? NLP needs. We would like
to set up a framework for everyone to test out new
methodologies for enriching and using Wiktionary
resources. Such observatory, would allow to fol-
low not only the evolution of Wiktionary but also
of Wiktionary-grounded resources, that will only
improve thanks to steady collaborative develop-
ment.
Invariants and variabality Wiktionary as a
massively mutiligual synonymy networks is an
extremely promising resource for studying the
(in)variability of semantic pairings such as
house/family, child/fruit, feel/know... (Sweetser,
1991; Gaume et al, 2009). A systematic study
within the semantic approximation framework
presented in the paper on Wiktionary data will be
carried on in the future.
26
References
A-L. Barabasi, R. Albert, H. Jeong, and G. Bianconi.
2000. Power-Law Distribution of the World Wide
Web. Science, 287. (in Technical Comments).
M. Baroni, F. Chantree, A. Kilgarriff, and S. Sharoff.
2008. Cleaneval: a Competition for Cleaning
Web Pages. In Proceedings of the Conference on
Language Resources and Evaluation (LREC), Mar-
rakech.
Encyclopaedia Britannica. 2006. Fatally flawed: re-
futing the recent study on encyclopedic accuracy by
the journal Nature.
K. Duvignau and B. Gaume. 2008. Between words
and world: Verbal "metaphor" as semantic or prag-
matic approximation? In Proceedings of Interna-
tional Conference "Language, Communication and
Cognition", Brighton.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
C. Fellbaum. 1999. La repr?sentation des verbes
dans le r?seau s?mantique Wordnet. Langages,
33(136):27?40.
B. Gaume, K. Duvignau, L. Pr?vot, and Y. Desalle.
2008. Toward a cognitive organization for electronic
dictionaries, the case for semantic proxemy. In Col-
ing 2008: Proceedings of the Workshop on Cogni-
tive Aspects of the Lexicon (COGALEX 2008), pages
86?93, Manchester.
B. Gaume, K. Duvignau, and M. Vanhove. 2009. Se-
mantic associations and confluences in paradigmatic
networks. In M. Vanhove, editor, From Polysemy to
Semantic Change: Towards a Typology of Lexical
Semantic Associations, pages 233?264. John Ben-
jamins Publishing.
J. Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438:900?901.
C. Jacquin, E. Desmontils, and L. Monceaux. 2002.
French EuroWordNet Lexical Database Improve-
ments. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLING 2002), Mexico
City.
F. Keller and M. Lapata. 2002. Using the web to over-
come data sparseness. In Proceedings of EMNLP-
02, pages 230?237.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29:333?347.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the humanities, 31(2):91?113.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet similarity for word sense iden-
tification. In C. Fellbaum, editor, WordNet: An elec-
tronic lexical database, pages 265?283. MIT Press.
M. Newman. 2003. The structure and function of com-
plex networks.
B. Sagot and D. Fi?er. 2008. Building a Free French
Wordnet from Multilingual Resources. In Proceed-
ings of OntoLex 2008, Marrackech.
M. Steyvers and J. B. Tenenbaum. 2005. The large-
scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cogni-
tive Science, 29:41?78.
E. Sweetser. 1991. From etymology to pragmatics.
Cambridge University Press.
D. Tufis. 2000. Balkanet design and development of a
multilingual balkan wordnet. Romanian Journal of
Information Science and Technology, 7(1-2).
B. Victorri and C. Fuchs. 1996. La polys?mie, con-
struction dynamique du sens. Herm?s.
D.J. Watts and S.H. Strogatz. 1998. Collective dynam-
ics of small-world networks. Nature, 393:440?442.
T. Zesch, C. M?ller, and I. Gurevych. 2008a. Extract-
ing Lexical Semantic Knowledge from Wikipedia
and Wiktionary. In Proceedings of the Conference
on Language Resources and Evaluation (LREC),
Marrakech.
T. Zesch, C. Muller, and I. Gurevych. 2008b. Using
wiktionary for computing semantic relatedness. In
Proceedings of 23rd AAAI Conference on Artificial
Intelligence.
27
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 145?152,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Query Expansion using LMF-Compliant Lexical Resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Dain Kaplan
Tokyo Inst. of Tech.
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Xia Yingju
Fujitsu R&D Center
Chu-Ren Huang
The Hong Kong Polytec. Univ.
Shu-Kai Hsieh
National Taiwan Normal Univ.
Shirai Kiyoaki
JAIST
Abstract
This paper reports prototype multilin-
gual query expansion system relying on
LMF compliant lexical resources. The
system is one of the deliverables of a
three-year project aiming at establish-
ing an international standard for language
resources which is applicable to Asian
languages. Our important contributions
to ISO 24613, standard Lexical Markup
Framework (LMF) include its robustness
to deal with Asian languages, and its ap-
plicability to cross-lingual query tasks, as
illustrated by the prototype introduced in
this paper.
1 Introduction
During the last two decades corpus-based ap-
proaches have come to the forefront of NLP re-
search. Since without corpora there can be no
corpus-based research, the creation of such lan-
guage resources has also necessarily advanced
as well, in a mutually beneficial synergetic re-
lationship. One of the advantages of corpus-
based approaches is that the techniques used
are less language specific than classical rule-
based approaches where a human analyses the
behaviour of target languages and constructs
rules manually. This naturally led the way
for international resource standardisation, and in-
deed there is a long standing precedent in the
West for it. The Human Language Technol-
ogy (HLT) society in Europe has been particu-
larly zealous in this regard, propelling the cre-
ation of resource interoperability through a se-
ries of initiatives, namely EAGLES (Sanfilippo et
al., 1999), PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Ide et al, 2003), and LIRICS1. These
1http://lirics.loria.fr/
continuous efforts have matured into activities in
ISO-TC37/SC42, which aims at making an inter-
national standard for language resources.
However, due to the great diversity of languages
themselves and the differing degree of technolog-
ical development for each, Asian languages, have
received less attention for creating resources than
their Western counterparts. Thus, it has yet to be
determined if corpus-based techniques developed
for well-computerised languages are applicable on
a broader scale to all languages. In order to effi-
ciently develop Asian language resources, utilis-
ing an international standard in this creation has
substantial merits.
We launched a three-year project to create an
international standard for language resources that
includes Asian languages. We took the following
approach in seeking this goal.
? Based on existing description frameworks,
each research member tries to describe sev-
eral lexical entries and find problems with
them.
? Through periodical meetings, we exchange
information about problems found and gen-
eralise them to propose solutions.
? Through an implementation of an application
system, we verify the effectiveness of the pro-
posed framework.
Below we summarise our significant contribution
to an International Standard (ISO24613; Lexical
Markup Framework: LMF).
1st year After considering many characteristics
of Asian languages, we elucidated the shortcom-
ings of the LMF draft (ISO24613 Rev.9). The
draft lacks the following devices for Asian lan-
guages.
2http://www.tc37sc4.org/
145
(1) A mapping mechanism between syntactic
and semantic arguments
(2) Derivation (including reduplication)
(3) Classifiers
(4) Orthography
(5) Honorifics
Among these, we proposed solutions for (1) and
(2) to the ISO-TC37 SC4 working group.
2nd year We proposed solutions for above the
(2), (3) and (4) in the comments of the Committee
Draft (ISO24613 Rev. 13) to the ISO-TC37 SC4
working group. Our proposal was included in DIS
(Draft International Standard).
(2?) a package for derivational morphology
(3?) the syntax-semantic interface resolving the
problem of classifiers
(4?) representational issues with the richness of
writing systems in Asian languages
3rd year Since ISO 24613 was in the FDIS stage
and fairly stable, we built sample lexicons in Chi-
nese, English, Italian, Japanese, and Thai based
on ISO24613. At the same time, we implemented
a query expansion system utilising rich linguis-
tic resources including lexicons described in the
ISO 24613 framework. We confirmed that a sys-
tem was feasible which worked on the tested lan-
guages (including both Western and Asian lan-
guages) when given lexicons compliant with the
framework. ISO 24613 (LMF) was approved by
the October 2008 ballot and published as ISO-
24613:2008 on 17th November 2008.
Since we have already reported our first 2 year
activities elsewhere (Tokunaga and others, 2006;
Tokunaga and others, 2008), we focus on the
above query expansion system in this paper.
2 Query expansion using
LMF-compliant lexical resources
We evaluated the effectiveness of LMF on a mul-
tilingual information retrieval system, particularly
the effectiveness for linguistically motivated query
expansion.
The linguistically motivated query expansion
system aims to refine a user?s query by exploiting
the richer information contained within a lexicon
described using the adapted LMF framework. Our
lexicons are completely complaint with this inter-
national standard. For example, a user inputs a
keyword ?ticket? as a query. Conventional query
expansion techniques expand this keyword to a
set of related words by using thesauri or ontolo-
gies (Baeza-Yates and Ribeiro-Neto, 1999). Using
the framework proposed by this project, expand-
ing the user?s query becomes a matter of following
links within the lexicon, from the source lexical
entry or entries through predicate-argument struc-
tures to all relevant entries (Figure 1). We focus
on expanding the user inputted list of nouns to rel-
evant verbs, but the reverse would also be possible
using the same technique and the same lexicon.
This link between entries is established through
the semantic type of a given sense within a lexical
entry. These semantic types are defined by higher-
level ontologies, such as MILO or SIMPLE (Lenci
et al, 2000) and are used in semantic predicates
that take such semantic types as a restriction ar-
gument. Since senses for verbs contain a link to
a semantic predicate, using this semantic type, the
system can then find any/all entries within the lexi-
con that have this semantic type as the value of the
restriction feature of a semantic predicate for any
of their senses. As a concrete example, let us con-
tinue using the ?ticket? scenario from above. The
lexical entry for ?ticket? might contain a semantic
type definition something like in Figure 2.
<LexicalEntry ...>
<feat att="POS" val="N"/>
<Lemma>
<feat att="writtenForm"
val="ticket"/>
</Lemma>
<Sense ...>
<feat att="semanticType"
val="ARTIFACT"/>
...
</Sense>
...
</LexicalEntry>
Figure 2: Lexical entry for ?ticket?
By referring to the lexicon, we can then derive
any actions and events that take the semantic type
?ARTIFACT? as an argument.
First all semantic predicates are searched for ar-
guments that have an appropriate restriction, in
this case ?ARTIFACT? as shown in Figure 3, and
then any lexical entries that refer to these predi-
cates are returned. An equally similar definition
would exist for ?buy?, ?find? and so on. Thus,
by referring to the predicate-argument structure of
related verbs, we know that these verbs can take
146
<LexicalEntry ...>
  <feat att="POS" val="Noun"/>
  <Lemma>
    <feat att="writtenForm" val="ticket"/>
  </Lemma>
  <Sense ...>
    <feat att="semanticType" val="ARTIFACT"/>
    ...
  </Sense>
  ...
</LexicalEntry>
User Inputs
ticket
<Sense>
<SemanticFeature>
Semantic Features of type 
"restriction" that take 
Sense's semanticType
All senses for 
matched nouns
<SemanticPredicate 
  id="pred-sell-1">
  <SemanticArgument>
    <feat att="label" val="X"/>
    <feat att="semanticRole" val="Agent"/>
    <feat att="restriction" val="Human"/>
  </SemanticArgument>
  ...
  <SemanticArgument>
    <feat att="label" val="Z"/>
    <feat att="semanticRole" val="Patient"/>
    <feat att="restriction" 
          val="ARTIFACT,LOCATION"/>
  </SemanticArgument>
</SemanticPredicate>
All Semantic Predicates 
that contain matched 
Semantic Features
<Sense>
Senses that use matched 
Semantic Predicates
<LexicalEntry ...>
  <feat att="POS" val="Verb"/>
  <Lemma>
    <feat att="writtenForm" val="sell"/>
  </Lemma>
  <Sense id="sell-1" ...>
    ...
    <PredicativeRepresentation
      predicate="pred-sell-1" ...>
  </Sense>
</LexicalEntry>
<LexicalEntry>
<SemanticPredicate>
<LexicalEntry>
System outputs
"sell", ...
For each <Sense> find all 
<SemanticArgument> that 
take this semanticType as 
a feature of type 
"restriction"
Find all verbs <LexicalEntry> 
that use these 
<SemanticPredicate>
All verbs that have 
matched Senses
Figure 1: QE Process Flow
147
<LexicalEntry ...>
<feat att="POS" val="V"/>
<Lemma>
<feat att="writtenForm"
val="sell"/>
</Lemma>
<Sense id="sell-1" ...>
<feat att="semanticType"
val="Transaction"/>
<PredicativeRepresentation
predicate="pred-sell-1"
correspondences="map-sell1">
</Sense>
</LexicalEntry>
<SemanticPredicate id="pred-sell-1">
<SemanticArgument ...>
...
<feat att="restriction"
val="ARTIFACT"/>
</SemanticArgument>
</SemanticPredicate>
Figure 3: Lexical entry for ?sell? with its semantic
predicate
?ticket? in the role of object. The system then re-
turns all relevant entries, here ?buy?, ?sell? and
?find?, in response to the user?s query. Figure 1
schematically shows this flow.
3 A prototype system in detail
3.1 Overview
To test the efficacy of the LMF-compliant lexi-
cal resources, we created a system implementing
the query expansion mechanism explained above.
The system was developed in Java for its ?com-
pile once, run anywhere? portability and its high-
availability of reusable off-the-shelf components.
On top of Java 5, the system was developed us-
ing JBoss Application Server 4.2.3, the latest stan-
dard, stable version of the product at the time of
development. To provide fast access times, and
easy traversal of relational data, a RDB was used.
The most popular free open-source database was
selected, MySQL, to store all lexicons imported
into the system, and the system was accessed, as a
web-application, via any web browser.
3.2 Database
The finalised database schema is shown in Fig-
ure 4. It describes the relationships between en-
tities, and more or less mirrors the classes found
within the adapted LMF framework, with mostly
only minor exceptions where it was efficacious for
querying the data. Due to space constraints, meta-
data fields, such as creation time-stamps have been
left out of this diagram. Since the system also al-
lows for multiple lexicons to co-exist, a lexicon id
resides in every table. This foreign key has been
highlighted in a different color, but not connected
via arrows to make the diagram easier to read. In
addition, though in actuality this foreign key is not
required for all tables, it has been inserted as a con-
venience for querying data more efficiently, even
within join tables (indicated in blue). Having mul-
tiple lexical resources co-existing within the same
database allows for several advantageous features,
and will be described later. Some tables also con-
tain a text id, which stores the original id attribute
for that element found within the XML. This is
not used in the system itself, and is stored only for
reference.
3.3 System design
As mentioned above, the application is deployed
to JBoss AS as an ear-file. The system it-
self is composed of java classes encapsulating
the data contained within the database, a Pars-
ing/Importing class for handling the LMF XML
files after they have been validated, and JSPs,
which contain HTML, for displaying the inter-
face to the user. There are three main sections
to the application: Search, Browse, and Config-
ure. Explaining last to first, the Configure section,
shown in Figure 5, allows users to create a new
lexicon within the system or append to an exist-
ing lexicon by uploading a LMF XML file from
their web browser, or delete existing lexicons that
are no longer needed/used. After import, the data
may be immediately queried upon with no other
changes to system configuration, from within both
the Browse and Search sections. Regardless of
language, the rich syntactic/semantic information
contained within the lexicon is sufficient for car-
rying out query expansion on its own.
The Browse section (Figure 6) allows the user to
select any available lexicon to see the relationships
contained within it, which contains tabs for view-
ing all noun to verb connections, a list of nouns, a
list of verbs, and a list of semantic types. Each has
appropriate links allowing the user to easily jump
to a different tab of the system. Clicking on a noun
takes them to the Search section (Figure 7). In this
section, the user may select many lexicons to per-
form query extraction on, as is visible in Figure 7.
148
semantic_link 
VARCHAR (64)
sense
sense_id
PRIMARY KEY
synset_id
FOREIGN KEY
syn_sem_correspondence_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_type
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
semantic_predicate_id
PRIMARY KEY
semantic_predicate
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
semantic_argument_id
PRIMARY KEY
semantic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
semantic_feature_id
PRIMARY KEY
semantic_feature
lexicon_id
FOREIGN KEY
semantic_argument_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_predicate_to_argument
lexicon_id
FOREIGN KEY
semantic_feature_id
FOREIGN KEY
semantic_argument_id 
FOREIGN KEY
semantic_argument_to_feature
description
TEXT
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
synset_id
PRIMARY KEY
synset
written_form
VARCHAR (64) NOT NULL
part_of_speech
ENUM( 'Verb', 'Noun' , 'Unknown')
lexical_entry
text_id
VARCHAR (64)
entry_id 
PRIMARY KEY
lexicon_id 
FOREIGN KEY
semantic_feature
FOREIGN KEY
syntactic_feature
FOREIGN KEY
lexicon_id
FOREIGN KEY
argument_map_id
PRIMARY KEY
syn_sem_argument_map
lexicon_id
FOREIGN KEY
argument_map_id
FOREIGN KEY
syn_sem_correspondence_id 
FOREIGN KEY
syn_sem_correspondence_to_map
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syn_sem_correspondence_id
PRIMARY KEY
syn_sem_correspondence
lexicon_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_sense
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
frame_id
PRIMARY KEY
subcat_frame
lexicon_id
FOREIGN KEY
frame_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_subcat_frame
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syntactic_argument_id
PRIMARY KEY
syntactic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
syntactic_feature_id
PRIMARY KEY
syntactic_feature
lexicon_id
FOREIGN KEY
syntactic_argument_id
FOREIGN KEY
frame_id
FOREIGN KEY
subcat_frame_to_argument
lexicon_id
FOREIGN KEY
syntactic_feature_id
FOREIGN KEY
syntactic_argument_id 
FOREIGN KEY
syntactic_argument_to_feature
description
VARCHAR(128)
language
VARCHAR(64)
lexicon_id
PRIMARY KEY
lexicon
relation_type 
VARCHAR (64)
lexicon_id
FOREIGN KEY
related_sense_id
FOREIGN KEY
sense_id
FOREIGN KEY
sense_relation
Figure 4: Database schema
Figure 5: QE System - Configure Figure 6: QE System - Browse
149
Figure 7: QE System - Search
3.4 Semantic information
This new type of query expansion requires rich
lexical information. We augmented our data using
the SIMPLE ontology for semantic types, using
the same data for different languages. This had
the added benefit of allowing cross-language ex-
pansion as a result. In steps two and three of Fig-
ure 1 when senses are retrieved that take specific
semantic types as arguments, this process can be
done across all (or as many as are selected) lex-
icons in the database. Thus, results such as are
shown in Figure 7 are possible. In this figure the
Japanese word for ?nail? is entered, and results for
both selected languages, Japanese and Italian, are
returned. This feature requires the unification of
the semantic type ontology strata.
3.5 Possible extension
Next steps for the QE platform are to explore the
use of other information already defined within the
adapted framework, specifically sense relations.
Given to the small size of our sample lexicon, data
sparsity is naturally an issue, but hopefully by ex-
ploring and exploiting these sense relations prop-
erly, the system may be able to further expand a
user?s query to include a broader range of selec-
tions using any additional semantic types belong-
ing to these related senses. The framework also
contains information about the order in which syn-
tactic arguments should be placed. This informa-
tion should be used to format the results from the
user?s query appropriately.
4 An Additional Evaluation
We conducted some additional query expansion
experiments using a corpus that was acquired from
Chinese LDC (No. ?2004-863-009?) as a base (see
below). This corpus marked an initial achievement
in building a multi-lingual parallel corpus for sup-
porting development of cross-lingual NLP appli-
cations catering to the Beijing 2008 Olympics.
The corpus contains parallel texts in Chinese,
English and Japanese and covers 5 domains that
are closely related to the Olympics: traveling, din-
ing, sports, traffic and business. The corpus con-
sists of example sentences, typical dialogues and
articles from the Internet, as well as other language
teaching materials. To deal with the different lan-
guages in a uniform manner, we converted the cor-
pus into our proposed LMF-compliant lexical re-
sources framework, which allowed the system to
expand the query between all the languages within
the converted resources without additional modifi-
cations.
As an example of how this IR system func-
tioned, suppose that Mr. Smith will be visiting
Beijing to see the Olympic games and wants to
know how to buy a newspaper. Using this system,
he would first enter the query ?newspaper?. For
this query, with the given corpus, the system re-
turns 31 documents, fragments of the first 5 shown
below.
(1) I?ll bring an English newspaper immediately.
(2) Would you please hand me the newspaper.
(3) There?s no use to go over the newspaper ads.
(4) Let?s consult the newspaper for such a film.
(5) I have little confidence in what the newspa-
pers say.
Yet it can be seen that the displayed results are not
yet useful enough to know how to buy a newspa-
per, though useful information may in fact be in-
cluded within some of the 31 documents. Using
the lexical resources, the query expansion module
suggests ?buy?, ?send?, ?get?, ?read?, and ?sell?
as candidates to add for a revised query.
Mr. Smith wants to buy a newspaper, so he se-
lects ?buy? as the expansion term. With this query
the system returns 11 documents, fragments of the
first 5 listed below.
(6) I?d like some newspapers, please.
150
(7) Oh, we have a barber shop, a laundry, a store,
telegram services, a newspaper stand, table
tennis, video games and so on.
(8) We can put an ad in the newspaper.
(9) Have you read about the Olympic Games of
Table Tennis in today?s newspaper, Miss?
(10) newspaper says we must be cautious about
tidal waves.
This list shows improvement, as information about
newspapers and shopping is present, but still ap-
pears to lack any documents directly related to
how to buy a newspaper.
Using co-occurrence indexes, the IR system
returns document (11) below, because the noun
?newspaper? and the verb ?buy? appear in the
same sentence.
(11) You can make change at some stores, just buy
a newspaper or something.
From this example it is apparent that this sort
of query expansion is still too naive to apply to
real IR systems. It should be noted, however, that
our current aim of evaluation was in confirming
the advantage of LMF in dealing with multiple
languages, for which we conducted a similar run
with Chinese and Japanese. Results of these tests
showed that in following the LMF framework in
describing lexical resources, it was possibile to
deal with all three languages without changing the
mechanics of the system at all.
5 Discussion
LMF is, admittedly, a ?high-level? specification,
that is, an abstract model that needs to be fur-
ther developed, adapted and specified by the lex-
icon encoder. LMF does not provide any off-the-
shelf representation for a lexical resource; instead,
it gives the basic structural components of a lexi-
con, leaving full freedom for modeling the partic-
ular features of a lexical resource. One drawback
is that LMF provides only a specification manual
with a few examples. Specifications are by no
means instructions, exactly as XML specifications
are by no means instructions on how to represent
a particular type of data.
Going from LMF specifications to a true instan-
tiation of an LMF-compliant lexicon is a long way,
and comprehensive, illustrative and detailed ex-
amples for doing this are needed. Our prototype
system provides a good starting example for this
direction. LMF is often taken as a prescriptive
description, and its examples taken as pre-defined
normative examples to be used as coding guide-
lines. Controlled and careful examples of conver-
sion to LMF-compliant formats are also needed to
avoid too subjective an interpretation of the stan-
dard.
We believe that LMF will be a major base
for various SemanticWeb applications because it
provides interoperability across languages and di-
rectly contributes to the applications themselves,
such as multilingual translation, machine aided
translation and terminology access in different lan-
guages.
From the viewpoint of LMF, our prototype
demonstrates the adaptability of LMF to a rep-
resentation of real-scale lexicons, thus promoting
its adoption to a wider community. This project
is one of the first test-beds for LMF (as one of
its drawbacks being that it has not been tested on
a wide variety of lexicons), particularly relevant
since it is related to both Western and Asian lan-
guage lexicons. This project is a concrete attempt
to specify an LMF-compliant XML format, tested
for representative and parsing efficiency, and to
provide guidelines for the implementation of an
LMF-compliant format, thus contributing to the
reduction of subjectivity in interpretation of stan-
dards.
From our viewpoint, LMF has provided a for-
mat for exchange of information across differently
conceived lexicons. Thus LMF provides a stan-
dardised format for relating them to other lexical
models, in a linguistically controlled way. This
seems an important and promising achievement in
order to move the sector forward.
6 Conclusion
This paper described the results of a three-year
project for creating an international standard for
language resources in cooperation with other ini-
tiatives. In particular, we focused on query expan-
sion using the standard.
Our main contribution can be summarised as
follows.
? We have contributed to ISO TC37/SC4 ac-
tivities, by testing and ensuring the portabil-
ity and applicability of LMF to the devel-
opment of a description framework for NLP
lexicons for Asian languages. Our contribu-
tion includes (1) a package for derivational
151
morphology, (2) the syntax-semantic inter-
face with the problem of classifiers, and (3)
representational issues with the richness of
writing systems in Asian languages. As of
October 2008, LMF including our contribu-
tions has been approved as the international
standard ISO 26413.
? We discussed Data Categories necessary
for Asian languages, and exemplified sev-
eral Data Categories including reduplication,
classifier, honorifics and orthography. We
will continue to harmonise our activity with
that of ISO TC37/SC4 TDG2 with respect to
Data Categories.
? We designed and implemented an evaluation
platform of our description framework. We
focused on linguistically motivated query ex-
pansion module. The system works with lexi-
cons compliant with LMF and ontologies. Its
most significant feature is that the system can
deal with any language as far as the those lex-
icons are described according to LMF. To our
knowledge, this is the first working system
adopting LMF.
In this project, we mainly worked on three
Asian languages, Chinese, Japanese and Thai, on
top of the existing framework which was designed
mainly for European languages. We plan to dis-
tribute our results to HLT societies of other Asian
languages, requesting for their feedback through
various networks, such as the Asian language re-
source committee network under Asian Federation
of Natural Language Processing (AFNLP)3, and
the Asian Language Resource Network project4.
We believe our efforts contribute to international
activities like ISO-TC37/SC45 (Francopoulo et al,
2006).
Acknowledgments
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison-Wesley.
3http://www.afnlp.org/
4http://www.language-resource.net/
5http://www.tc37sc4.org/
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006.
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
A. Sanfilippo, N. Calzolari, S. Ananiadou,
R. Gaizauskas, P. Saint-Dizier, and P. Vossen.
1999. EAGLES recommendations on semantic
encoding. EAGLES LE3-4244 Final Report.
T. Tokunaga et al 2006. Infrastructure for standard-
ization of Asian language resources. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 827?834.
T. Tokunaga et al 2008. Adapting international stan-
dard for asian language technologies. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08).
152
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 123?130,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
CWN-LMF: Chinese WordNet in the Lexical Markup Framework 
 
Lung-Hao Lee1, Shu-Kai Hsieh2, Chu-Ren Huang1,3 
 
1Institute of Linguistics, Academia Sinica 
2Department of English, National Taiwan Normal University 
3Department of Chinese & Bilingual Studies, The Hong Kong Polytechnic University 
1128 Academia Road, Section 2, Taipei 115, Taiwan 
2162 He-ping East Road, Section 1, Taipei 106, Taiwan 
3Hung Hom, Kowloon, Hong Kong 
1{lunghao,churen}@gate.sinica.edu.tw 
2shukai@ntnu.edu.tw 
3churen.huang@inet.polyu.edu.hk 
 
Abstract 
Lexical Markup Framework (LMF, ISO-
24613) is the ISO standard which provides 
a common standardized framework for the 
construction of natural language 
processing lexicons. LMF facilitates data 
exchange among computational linguistic 
resources, and also promises a convenient 
uniformity for future application. This 
study describes the design and implemen-
tation of the WordNet-LMF used to 
represent lexical semantics in Chinese 
WordNet. The compiled CWN-LMF will 
be released to the community for linguis-
tic researches.  
1 Introduction 
Princeton WordNet1 is an English lexical data-
base that groups nouns, verbs, adjectives and 
adverbs into sets of cognitive synonyms, which 
are named as synsets (Fellbaum, 1998; Miller, 
1995).  The Global WordNet Association 
(GWA)2 built on the results of Princeton Word-
Net and Euro WordNet (Vossen, 2004) is a free 
and public association that provides a platform to 
share and connect all languages in the world. For 
Mandarin Chinese in Taiwan, Huang et al (2004) 
constructed the Academia Sinica Bilingual Onto-
logical Wordnet (Sinica BOW) which integrates 
WordNet, English-Chinese Translation Equiva-
                                                 
n
1 Wordnet, available online 
at http://wordnetweb.princeton.edu/perl/webw  
2 Global WordNet Association (GWA), available on-
line at http://www.globalwordnet.org/ 
lents Database (ECTED) and SUMO for cross-
language linguistic studies. As a follow-up, Chi-
nese WordNet (CWN) has been built as a robust 
lexical knowledge system which also embodies a 
precise expression of sense relations (Huang et 
al., 2008). In recent years, WordNet-like re-
sources have become one of the most reliable 
and essential resources for linguistic studies for 
all languages (Magnini and Cavaglia, 2000; So-
ria et al 2009; Strapparava and Valitutti, 2004).  
Lexical Markup Framework (LMF, ISO-
24613) is the ISO standard which provides a 
common standardized framework for the con-
struction of natural language processing lexicons 
(Francopoulo et al, 2009).  One important pur-
pose of LMF is to define a standard for lexicons 
which covers multilingual lexical information 
(Francopoulo et al, 2006b). In this study, we 
describe the design and implementation of the 
Wordnet-LMF (Soria et al 2009) to represent 
lexical semantics in Chinese WordNet. 
The rest of this paper is organized as follows: 
Section 2 introduces Chinese WordNet and Lexi-
cal Markup Framework. Section 3 describes how 
we represent Chinese WordNet in the Lexical 
Markup Framework (CWN-LMF). Section 4 
presents an example on Chinese word sense dis-
tinction using CWN-LMF format. Quantitative 
analysis of compiled CWN-LMF is presented in 
Section 5. We also describe the application sce-
nario using CWN-LMF for information interope-
rability of lexical semantics in Section 6. Section 
7 discusses the experience and difficulties of en-
coding CWN into Wordnet-LMF.  Finally, Sec-
tion 8 concludes this study with future research.  
 
 
123
2 Related Work  
2.1 Chinese WordNet 
Creating a semantic relation-based language re-
source is a time consuming and labor intensive 
task, especially for Chinese due to the unobvious 
definition and distinction among characters, 
morphemes and words. Chinese WordNet 3  
(CWN) has been built by Academia Sinica and is 
successively extended its scope so far. Lemmas 
included in CWN mainly fall on the medium fre-
quency words. Each lexical entry is analyzed 
according to the guidelines of Chinese word 
sense distinctions (CKIP, 2003; Huang et al 
2003) which contain information including Part-
of-Speech, sense definition, example sentences, 
corresponding English synset(s) from Princeton 
WordNet, lexical semantic relations and so on. 
Unlike Princeton WordNet, CWN has not been 
constructed mainly on the synsets and semantic 
relations. Rather it focuses to provide precise 
expression for the Chinese sense division and the 
semantic relations needs to be based on the lin-
guistic theories, especially lexical semantics 
(Huang et al, 2008). Moreover, Huang et al 
(2005) designed and implemented the Sinica 
Sense Management System (SSMS) to store and 
manage word sense data generated in the analy-
sis stage. SSMS is meaning-driven. Each sense 
of a lemma is identified specifically using a 
unique identifier and given a separate entry. 
There are 8,646 lemmas / 25,961 senses until 
December 2008 have been analyzed and stored 
in SSMS. Figure 1 shows the result of sense dis-
tinction for ?? zu-ji ?footprint? as an example 
in Chinese WordNet. 
Huang et al (2004) proposed Domain Lexico-
Taxonomy (DLT) as a domain taxonomy popu-
lated with lexical entries. By using DLT with 
Chinese WordNet and Domain Taxonomy, there 
were 15,160 Chinese senses that linked and dis-
tributed in 463 domain nodes. In addition, Huang 
et al (2005) further applied DLT approach to a 
Chinese thesaurus called as CiLin and showed 
with evaluation that DLT approach is robust 
since the size and number of domain lexica in-
creased effectively.  
 
Figure1: The result of sense distinction for ?zu2 
ji1 (footprint)?. 
2.2 Lexical Markup Framework  
Lexical Markup Framework (LMF, ISO-24613) 
is the ISO standard for natural language 
processing lexicons and machine readable dic-
tionaries. The goals of LMF are to provide a 
common model for the creation and use of lexi-
cal resources, and to manage the exchange of 
data between them. Francopoulo et al (2006a; 
2009) offered a snapshot of how LMF represents 
multilingual lexicons. LMF facilitates data ex-
change among computational linguistic resources 
and also promises a convenient uniformity for 
future application. More updated information can 
be found online 
at http://www.lexicalmarkupframework.org . 
                                                
Soria et al (2009) proposed a Wordnet-LMF 
developed in the framework of the KYOTO 4  
project as a standardized interoperability format 
for the interchange of lexico-semantic informa-
tion. Wordnet-LMF is an LMF dialect tailored to 
encode lexical resources adhering to the Word-
                                                 
/
 
3 Chinese WordNet, available online 
at http://cwn.ling.sinica.edu.tw
4 KYOTO, available online at http://www.kyoto-
project.eu/  
124
Net model of lexical knowledge representation. 
Wordnet-LMF was designed by adhering to LMF 
principles yet taking into account on the one 
hand, the peculiarities of the Wordnet model, and 
on the other by trying to maximize the efficiency 
of the format.  
If we take Princeton WordNet 3.0 synset 
{footprint_1} for example, a Wordnet-LMF re-
presentation can be found in Figure 2. The de-
tails will be explained in Section 3. 
 
<Synset id=?eng-30-06645039-n? baseConcept=?1?>
<Definition gloss=?mark of a foot or shoe on a surface?>
<Statement example=?the police made casts of the 
footprints in the soft earth outside the window?/>
</Definition>
<SynsetRelations>
<SynsetRelation target=?eng-30-06798750-n?
relType=?has_hyperonym?>
</SynsetRelation>
<SynsetRelation target=?eng-30-06645266-n?
relType=?has_hyponym?>
</SynsetRelation>
</SynsetRelations>
<MonolingualExternalRefs>
<MonolingualExternalRef externalSystem=?Wordnet1.6?
externalReference=?eng-16-01234567-n?>
<MonolingualExternalRef externalSystem=?SUMO?
externalReference=?superficialPart? relType=?at?>
</MonolingualExternalRefs>
<Synset>
 
Figure 2: An example of Wordnet-LMF format. 
 
3 CWN in the Lexical Markup Frame-
xical se-
ation 
lInformation is used 
label=?Compile Chinese 
work (CWN-LMF) 
Wordnet-LMF is used to represent le
mantics in Chinese WordNet. As LexicalRe-
source is the root element in Wordnet-LMF, it 
has three children: one GlobalInformation ele-
ment, one or more Lexicon elements, zero or one 
SenseAxes element. This means the object Lexi-
calResource is the container for possibly more 
than one lexicon; inter-lingual correspondences 
are grouped in SenseAxes section. The details are 
presented as follows. 
3.1 Global Inform
The element named as Globa
to describe general information about the lexical 
resource. The attribute ?label? is a free text field. 
Example as follows: 
<GlobalInformation 
Wordnet entries using Wordnet-LMF?> 
3.2  Lexicon 
In CWN-LMF, only one element Lexicon is used 
to contain a monolingual resource as a set of 
LexicalEntry instances followed by a set of Syn-
set elements. The following attributes are speci-
fied: 
 
z languageCoding: It has ?ISO 639-3? as a 
fixed value. 
z language: The standardized 3-letter lan-
guage coding, e.g. zho, is used to spe-
cify the language represented by the 
lexical resource. It is a required 
attribute. 
z owner: It is a required attribute to speci-
fy the copyright holder 
z version: It is a required attribute to speci-
fy the resource version. 
z label: It is used to record additional in-
formation that may be needed. This 
attribute is optional. 
 
Example as follows:  
<Lexicon languageCoding=?ISO 639-3? la-
bel=?Chinese WordNet 1.6? language=?zho?, 
owner=?Academia Sinica?, version=?1.6?>. 
3.2.1 Lexical Entry 
A LexicalEntry element can contain one lemma 
and one sense and has an optional attribute ?id? 
which means a unique identifier.  
The element, Lemma, represents a word form 
chosen by convention to designate the lexical 
entry. It contains the following attributes: 
 
z partOfSpeech: It is a required attribute. 
This attribute takes as its value the 
part-of ?speech value that according 
to WordNet conventions is usually 
specified for a synset. There are four 
part-of-speech notations that are used 
in CWN-LMF. The notation ?n? is 
represented as a noun; the notation 
?v? is represented as a verb; the nota-
tion ?a? is represented as an adjective; 
the notation ?r? is represented as an 
adverb; and the other POS tags are 
represented as ?s?. 
z writtenForm: It is added in case that ?id? 
of LexicalEntry is numerical and it 
takes Unicode strings as values. This 
attribute is optional. 
 
 
125
The Sense element represents one meaning of 
a lexical entry. For WordNet representation, it 
represents the variant of a synset. Required 
attributes are:  
 
z id: It must be specified according to the 
convention used in Chinese WordNet, 
i.e. word_sense#nr.. For example, ??
?_1? means that the first sense of 
lemma ?? huan-jing ?environment?. 
z synset:  It takes as its value the ID of the 
synset to which the particular sense of 
the variant belongs. The ID of the 
synset will be described in the next 
subsection.  
 
Take the first sense of lemma ?? huan-jing 
?environment? for example, it will be represented 
as follows: 
<LexicalEntry> 
  <Lemma writtenForm="??" partOfS-
peech="n"></Lemma> 
     <Sense id="??_1" synset=" zho-16-
06640901-n"></Sense> 
</LexicalEntry> 
3.2.2 Synset 
This element encodes information about a Chi-
nese WordNet synset. Synset elements can con-
tain one Definition, optional SynsetRelations and 
MonolingualExternalRefs elements. Required 
attributes for Synset element are the following: 
 
z id: It is a unique identifier. The agreed 
syntax is ?languageCode-version-id-
POS?.  For example, ?zho-16-
06640901-n? is unique identifier of 
the first sense of lemma ?? huan-
jing ?environment?. 
z baseConcept: Values for the baseCon-
cept attribute will be numerical (1,2,3), 
which correspond to the BaseConcept 
sets. If the sense belongs to the first-
class basic words of NEDO project 
(Tokunaga et al 2006), we encode it 
as 1.  Similarly, if the sense belongs to 
second-class basic words, we encode 
it as 2. The other senses will be en-
coded as 3 if they are not basic words. 
 
The element Definition allows the representa-
tion of the gloss associated with each synset in 
attribute ?gloss?. The required attribute ?exam-
ple? of the element Statement contains the exam-
ples of use associated with the synset . 
SynsetRelations is a bracketing element for 
grouping all SynsetRelation elements. Relations 
between synsets are codified by means of Synse-
tRelation elements, one per relation. Required 
attributes are: 
 
z target: It contains the ID value of the 
synset that is the target of the relation. 
z relType: It means the particular type. 
There are nine semantic relations in 
Chinese WordNet, including 
?has_synonym?, ?has_nearsynonym?, 
?has_hypernym?, ?has_hyponym?, 
?has_holonym?, ?has_meronym?, 
?has_paranym?, ?has_antonym? and 
?has_variant?. Among them, the se-
mantic relation paranymy is used to 
refer to relation between any two lexi-
cal items belonging to the same se-
mantic classification (Huang et al 
2008). For example, the set of 
?spring/summer/fall/winter?   has pa-
ranymy relation of main concept of 
?seasons in a year?. 
 
MonolingualExternalRefs is a bracketing ele-
ment to group all MonolingualExternalRef ele-
ments. MonolingualExternalRef elements must 
be used to represent links between a Sense or 
Synset and other resources, such as an ontology, 
a database or other lexical resources. Attributes 
are: 
 
z  externalSystem: It is a required attribute 
to describe the name of the external 
resource. For instance, possible values 
are ?domain? (Magnini and Cavaglia, 
2000), ?SUMO? (Niles and Pease, 
2001), and ?Wordnet 3.0? for record-
ing SenseKey values.   
z externalReference: It means the particu-
lar identifier or node. This attribute is 
required. 
z relType: It is optional attribute. If the 
?externalSystem? is ?SUMO?. ?rel-
Type? is the type of relations with 
SUMO ontology nodes. Possible val-
ues are ?at?, ?plus?, and ?equal?. 
 
 
 
 
 
126
We use the first sense of lemma ?? huan-
jing ?environment? to illustrate as follows: 
 
<Synset id="zho-16-06640901-n" baseCon-
cept="2"> 
<Definition gloss="?????????? 
????????"> 
<Statement example="???????? 
????????????????? 
???"/> 
</Definition> 
<SynsetRelations> 
<SynsetRelation target="zho-16- 
07029502-n" relType="has_synonym"> 
</SynsetRelation> 
</SynsetRelations> 
<MonolingualExternalRefs> 
<MonolingualExternalRef externalSys 
tem="SUMO" externalRefe 
rence="GeographicArea" rel 
Type="plus"/> 
</MonolingualExternalRefs> 
</Synset> 
 
3.3 SenseAxes 
SenseAxes is a bracketing element that groups 
together SenseAxis elements used for inter-
lingual correspondences. The SenseAxis element 
is a means to group synsets belonging to differ-
ent monolingual wordnets and sharing the same 
equivalent relation to Princeton WordNet 3.0. 
Required attributes are: 
 
z id: It is a unique identifier. 
z relType: It specifies the particular type 
of correspondence among synsets be-
longing to different resources. We use 
?eq_synonym? to represent equal 
synonym relation between Chinese 
Wordnet and Princeton WordNet. 
 
For instance, Chinese synset zho-16-06640901-n 
maps onto English synset eng-30-08567235-n by 
means of an eq_synonym relation. This will be 
represented as follows: 
 
<SenseAxes> 
<SenseAxis id="sa_zho16-eng30_5709" rel 
Type="eq_synonym"> 
<Target ID="zho-16-06640901-n"/> 
<Target ID="eng-30-08567235-n"/> 
</SenseAxis> 
</SenseAxes> 
4 An Example of CWN-LMF Format 
Take ?? zi-ran ?nature? as an example shown 
in Figure 3. ?? has six senses (some of them 
are abridged in the figure). Id attribute of the first 
sense is ??_1 and its synset is called ?zho-16-
03059301-n?. This encoding of synset stands for
??_1 with the unique ID 03059301 in Chinese 
WordNet version 1.6 and its part-of-speech is 
noun. Moreover, one can also learn that??_1 
has a synonym, ???_1 (zho-16-06653601-n). 
Meanwhile, this sense is also corresponded to 
IEEE SUMO. Finally, this compiled CWN-LMF 
version is pointed to Princeton WordNet 3.0, i.e. 
Chinese synset ?zho-16-03059301-n? maps onto 
English synset ?eng-30-11408559-n? by means 
of an eq_synonym relation. 
 
<?xml version=?1.0? encoding=?UTF-8??>
<!DOCTYPE LexicalResource SYSTEM ?kyoto_wn.dtd?>
<LexicalResource>
<GlobalInformation label=?CWN-LMF? />
<Lexicon languageCoding=?ISO 693-3? label=?Chinese  
Wordnet 1.6? language=?zho? owner=?Academia Sinica?
version=?1.6? >
<LexicalEntry>
<Lemma writtenForm=???? partOfSpeech=?n?>
</Lemma>
<Sense id=???_1? synset=?zho-16-03059301-n?>
</Sense>
</LexicalEntry>
?????
<Synset id=?zho-16-03059301-n? baseConcept=?3?>
<Definition gloss=??????????????????>        
<Statement example=???????????????
?????? />
</Definition>
<SynsetRelations>
<SynsetRelation target=?zho-16-06653061-n?
relType=?has_synonym?>
</SynsetRelation>
<MonolingualExternalRefs>
<MonolingualExternalRef externalSystem=?SUMO?
externalReference=?(ComplementFn)InternationalProcess?
relType=?plus? />
</MonolingualExternalRefs>
</Synset>
?????
</Lexion>
<SenseAxes>
<SenseAxis id=?sa_zho16-eng30_17638? relType=?eq_synonym?>
<Target ID=?zho-16-03059301-n?>
<Target ID=?eng-30-11408559-n?>
</SenseAxis>
?????
</SenseAxes>
</LexicalResource>
 
Figure 3: The lemma ?? in  CWN-LMF format. 
5 Quantitative Analysis of CWN-LMF 
There are 8,646 lemmas / 25,961 senses until 
December 2008 have been analyzed in CWN 1.6. 
So far the work on Chinese word distinction is 
still ongoing. It is expected that there are more 
analyzed results in the next released version.  
127
Among analyzed 25,961 senses, there are 268 
senses and 1,217 senses that belong to the first-
class and the second ?class basic words, respec-
tively. When part-of-speech is concerned, we can 
find most of these senses belong to nouns or 
verbs. There are 12,106 nouns, 10,454 nouns, 
806 adjectives and 1,605 adverbs in CWN 1.6 
We further distinguish semantic relations of 
CWN 1.6 and found that there are 3,328 syn-
onyms, 213 near synonyms, 246 hypernyms, 38 
hyponyms, 3 holonyms, 240 paranyms, 369 an-
tonyms and 432 variants, respectively.  
The IEEE SUMO is the only external system 
for monolingual references in CWN-LMF. There 
are 21,925 senses that were pointed to SUMO so 
far. In addition, there are 17,952 senses which 
shared the same equivalent relation to Princeton 
WordNet 3.0 in CWN-LMF. 
 
6 Application Scenarios 
The EU-7 project, KYOTO (Knowledge Yield-
ing Ontologies for Transition-based Organiza-
tion), wants to make knowledge sharable be-
tween communities of people, culture, language 
and computers, by assigning meaning to text and 
giving text to meaning (Vossen et al, 2008a; 
2008b). The goal of KYOTO is a system that 
allows people in communities to define the 
meaning of their words and terms in a shared 
Wiki platform so that it becomes anchored across 
languages and cultures but also so that a comput-
er can use this knowledge to detect knowledge 
and facts in text. 
KYOTO is a generic system offering know-
ledge transition and information across different 
target groups, transgressing linguistic, cultural 
and geographic boundaries. Initially developed 
for the environmental domain, KYOTO will be 
usable in any knowledge domain for mining, or-
ganizing, and distributing information on a glob-
al scale in both European and non-European lan-
guages.   
Whereas the current Wikipedia uses free text 
to share knowledge, KYOTO will represent this 
knowledge so that a computer can understand it. 
For example, the notion of environmental foot-
print will become defined in the same way in all 
these languages but also in such a way that the 
computer knows what information is necessary 
to calculate a footprint. With these definitions it 
will be possible to find information on footprints 
in documents, websites and reports so that users 
can directly ask the computer for actual informa-
tion in their environment, for instance, what is 
the footprint of their town, their region or their 
company. 
KYOTO?s principal components are an ontol-
ogy linked to WordNets in seven different lan-
guages (Basque, Chinese, Dutch, English, Italian, 
Japanese and Spanish). Due to different natures 
of languages, the different designed architectures 
were used to develop WordNets in theses lan-
guages. A unified framework is needed for in-
formation exchange. LMF is hence adopted as 
the framework at lexical semantic level in this 
project. The WordNet in these languages are 
compiled with designed WordNet-LMF format. 
CWN-LMF will also be involved and benefit for 
cross-language interpretabilities in semantic 
search field.  
7 Discussion 
Due to characters of Chinese language, there are 
some difficulties of encoding Chinese WordNet 
into Wordnet-LMF. A brief description is pre-
sented as follows.   
Chinese WordNet was designed for Chinese 
word sense distinction and its lexical semantic 
relationships. The designed architecture belongs 
to word-driven, not synset-driven.  So in CWN-
LMF, we encoded a sense as an individual synset 
and marked up the ?has_synonym? relation when 
senses belong to the same WordNet synset.  
In addition, how to define the basic concept of 
Chinese language is difficult. So far the basic 
word lists of the NEDO project were used as pre-
liminary basis. We need a further method to dis-
tinguish baseConcept attribute of word senses. 
8 Conclusions 
This study describes the design and implementa-
tion of how the Wordnet-LMF used to represent 
lexical semantics in Chinese WordNet. CWN-
LMF is benefit for data exchange among compu-
tational linguistic resources, and also promises a 
convenient uniformity for domain-specific appli-
cations such as KYOTO in cross-language se-
mantic search field.   
Future work is investigated with several direc-
tions. We are planning to release Chinese Word-
Net 1.6 using CWN-LMF format in an xml file, 
including a XML DTD in the following days. In 
addition, the use of this lingual resource for fur-
ther linguistic research is also under investigation. 
 
 
 
128
Acknowledgements  
The authors would like to thank Prof. Claudia 
Soria for her constructive comments. This work 
was funded by National Science Council, Taiwan 
under Grants NSC 97-2923-I-001-001-MY3., 
and also cooperated with EU-FP7 KYOTO 
project. 
References  
CKIP. 2003. Sense and Sensibility Vol. I. Technical 
Report 03-01. Taipei: Academia Sinica. 
Fellbaum, C.. 1998. WordNet: an Electronic Lexical 
Database. The MIT Press. 
Francopoulo, G., Bel, N., George, M., Calzolari, N., 
Monachini, M., Pet, M. and Soria, C.. 2006a. Lexi-
cal Markup Framework (LMF) for NLP Multilin-
gual Resources. Proceedings of COLING-ACL 
Workshop on Multilingual Language Resources 
and Interoperability.  
Francopoulo, G., Bel, N., George, M., Calzolari, N., 
Monachini, M., Pet, M. and Soria, C.. 2006b. LMF 
for Multilingual, Specialized Lexicons. Proceed-
ings of the LREC Workshop on Acquiring and 
Representing Multilingual, Specialized Lexicons: 
the Case of Biomedicine.  
Francopoulo, G., Bel, N., George, M., Calzolari, N., 
Monachini, M., Pet, M. and Soria, C.. 2009. Multi-
lingual Resources for NLP in the Lexical Markup 
Framework (LMF). Language Resource and Eval-
uation. 43:57-70. 
Huang, C.-R., Chang, R.-Y. and Lee, H.-P.. 2004. 
Sinica BOW (Bilingual Ontological Wordnet): In-
tegration of Bilingual WordNet and SUMO. Pro-
ceedings of the 4th International Conference on 
Language Resources and Evaluation. 
Huang, C.-R., Chen, C.-L., Weng, C.-X., Lee, H.-P., 
Chen, Y.-X. and Chen, K.-J.. 2005. The Sinica 
Sense Management System: Design and Implemen-
tation. Computational Linguistics and Chinese 
Language Processing. 10(4): 417-430. 
Huang, C.-R., Hsieh, S.-K., Hong, J.-F., Chen, Y.-Z., 
Su, I.-L., Chen, Y.-X. and Huang, S.-W.. 2008. 
Chinese Wordnet: Design, Implementation, and 
Application of an Infrastructure for Cross-lingual 
Knowledge Processing. Proceedings of the 9th Chi-
nese Lexical Semantics Workshop. 
Huang, C.-R., Lee, H.-P. and Hong, J.-F.. 2004. Do-
main Lexico-Taxonomy: an Approach Towards 
Multi-domain Language Processing. Proceedings 
of the Asian Symposium on Natural Language 
Processing to Overcome Language Barriers. 
Huang, C.-R., Lee, H.-P. and Hong, J.-F.. 2005. The 
Robustness of Domain Lexico-Taxonomy: Ex-
panding Domain Lexicon with Cilin. Proceedings 
of the 4th ACL SIGHAN Workshop on Chinese 
Language Processing. 
Huang, C.-R., Su, I.-L., Hsiao, P.-Y. and Ke, X.-L.. 
2008. Paranymy: Enriching Ontological Know-
ledge in Wordnets. Proceedings of the 4th Global 
WordNet Conference. 
Huang, C.-R., Tsai, D. B.-S., Weng, C.-X., Chu, N.-
X., Ho, W.-R., Huang, L.-H. and Tsai, I.-N.. 2003. 
Sense and Meaning Facet: Criteria and Operational 
Guidelines for Chinese Sense Distinction. Proceed-
ings of the 4th Chinese Lexical Semantics Work-
shop. 
LMF. 2009. Lexical Markup Framework. ISO-24613. 
Geneva:ISO. 
Magnini, B. and Cavaglia, G.. 2000. Integrating Sub-
ject Field Codes into WordNet.  Proceedings of the 
2nd International Conference on Language Re-
sources and Evaluation. 
Miller, G. A.. 1995. WordNet: a Lexical Database for 
English. Communications of the ACM. 38(11): 39-
41. 
Niles, I. and Pease, A.. 2001. Toward a Standard Up-
per Ontology. Proceedings of the 2nd International 
Conference on Formal Ontology in Information 
Systems.  
Soria, C., Monachini, M. and Vossen, P.. 2009. 
Wordnet-LMF: Fleshing out a Standardized For-
mat for Wordnet Interoperability. Proceedings of 
ACM Workshop on Intercultural Collaboration. 
Soria, C., Monachini, M., Bertagna, F., Calzolari, N., 
Huang, C.-R., Hsieh, S.-K., Marchetti, A. and Tes-
coni, M.. 2009. Exploring Interoperability of Lan-
guage Resources: the Case of Cross-lingual Semi-
automatic Enrichment of Wordnets. Language Re-
source and Evaluation. 43:87-96. 
Strapparava, C. and Valitutti, A.. 2004. WordNet-
Affect: an Affective Extension of WordNet. Pro-
ceedings of the 4th International Conference on 
Language Resources and Evaluation. 
Tokuaga, T., Sornlertlamvanich, V., Charoenporn, T., 
Calzolari, N., Monachini, M., Soria, C., Huang, C.-
R., Yu, Y., Yu, H. and Prevot, L.. 2006. Infrastruc-
ture for Standardization of Asian Language Re-
sources. Proceedings of the COLING/ACL Main 
Conference Poster Sessions. 
Vossen, P.. 2004. EuroWordNet: a Multilingual Data-
base of Autonomous and Language-specific Word-
nets Connected via an Inter-Lingual-Index. Interna-
tional Journal of Linguistics. 17(2): 1-23. 
Vossen, P., Agirre, E., Calzolari, N., Fellbaum, C., 
Hsieh, S.-K., Huang, C.-R., Isahara, H., Kanzaki, 
K., Marchetti, A., Monachini, M., Neri, F., Raffael-
li, R., Rigau, G., Tescon, M. and VanGent, J.. 
2008a.   KYOTO: A System for Mining, Structur-
129
ing, and Distributing Knowledge Across Languag-
es and Cultures. Proceedings of 6th International 
Conference on Language Resource and Evaluation.  
Vossen, P., Agirre, E., Calzolari, N., Fellbaum, C., 
Hsieh, S.-K., Huang, C.-R., Isahara, H., Kanzaki, 
K., Marchetti, A., Monachini, M., Neri, F., Raffael-
li, R., Rigau, G., Tescon, M. and VanGent, J.. 
2008b.   KYOTO: A System for Mining, Structur-
ing, and Distributing Knowledge Across Languag-
es and Cultures. Proceedings of the 4th Internation-
al Global WordNet Conference.  
 
 
130
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 179?187,
Beijing, August 2010
Emotion Cause Detection with Linguistic Constructions 
 Ying Chen*?, Sophia Yat Mei Lee?, Shoushan Li?, Chu-Ren Huang?  
*
 Dep. of Computer Engineering 
China Agricultural University 
?Dep. of Chinese and Bilingual Studies 
The Hong Kong Polytechnic University 
{chenying3176,sophiaym,shoushan.li,churenhuang}@gmail.com 
 
 
Abstract 
This paper proposes a multi-label ap-
proach to detect emotion causes. The 
multi-label model not only detects mul-
ti-clause causes, but also captures the 
long-distance information to facilitate 
emotion cause detection. In addition, 
based on the linguistic analysis, we cre-
ate two sets of linguistic patterns during 
feature extraction. Both manually gener-
alized patterns and automatically gener-
alized patterns are designed to extract 
general cause expressions or specific 
constructions for emotion causes. Ex-
periments show that our system 
achieves a performance much higher 
than a baseline model.   
1 Introduction 
Text-based emotion processing has been a cen-
ter of attention in the NLP field in the past few 
years. Most previous researches have focused 
on detecting the surface information of emo-
tions, especially emotion classes, e.g., ?happi-
ness? and ?anger? (Mihalcea and Liu 2006, 
Strapparava and Mihalcea 2008, Abbasi et al 
2008, Tokuhisa et al 2008). Although most 
emotion theories recognize the important role of 
causes in emotion analysis (Descartes, 1649; 
James, 1884; Plutchik 1980, Wierzbicka 1999), 
very few studies explore the interactions be-
tween emotion and causes. Emotion-cause in-
teraction is the eventive relation which poten-
tially yields the most crucial information in 
terms of information extraction. For instance, 
knowing the existence of an emotion is often 
insufficient to predict future events or decide on 
the best reaction. However, if the emotion cause 
is known in addition to the type of emotion, 
prediction of future events or assessment of po-
tential implications can be done more reliably. 
In other words, when emotion is treated as an 
event, causal relation is the pivotal relation to 
discover. In this paper, we explore one of the 
crucial deep level types of information of emo-
tion, i.e. cause events.  
Our study focuses on explicit emotions in 
which emotions are often presented by emotion 
keywords such as ?shocked? in ?He was 
shocked after hearing the news?. Emotion caus-
es are the explicitly expressed propositions that 
evoke the presence of the corresponding emo-
tions. They can be expressed by verbs, nomi-
nalizations, and nominals. Lee et al (2010a) 
explore the causes of explicit emotions by con-
structing a Chinese emotion cause corpus. 
Based on this corpus, we formalize the emotion 
cause detection problem through extensive data 
analysis. We find that ~14% emotion causes are 
complicated events containing multi-clauses, to 
which previous cause detection systems can 
hardly be applied directly. Most previous cause 
detection systems focus on the causal relation 
between a pair of small-size text units, such as 
clauses or phrases. They are thus not able to 
detect emotion causes that are multi-clauses. In 
this paper, we formalize emotion cause detec-
tion as a multi-label classification task (i.e. each 
instance may contain more than one label), 
which allows us to capture long-distance infor-
mation for emotion cause detection. 
In term of feature extraction, as emotion 
cause detection is a case of cause detection, 
some typical patterns used in existing cause de-
tection systems, e.g., ?because? and ?thus?, can 
be adopted. In addition, various linguistic cues 
are examined which potentially indicate emo-
tion causes, such as causative verbs and epis-
temic markers (Lee at al. 2010a). Then some 
linguistic patterns of emotion causes are manu-
179
ally generalized by examining the linguistic 
context of the empirical data (Lee et al, 2010b). 
It is expected that these manually generalized 
patterns often yield a low-coverage problem. 
Thus, we extracted features which enable us to 
automatically capture more emotion-specific 
constructions. Experiments show that such an 
integrated system with various linguistic fea-
tures performs promisingly well. We believe 
that the present study should provide the foun-
dation for future research on emotion analysis, 
such as the detection of implicit emotion or 
cause.  
The paper is organized as follows. Section 2 
discusses the related work on cause-effect de-
tection. Section 3 briefly describes the emotion 
cause corpus, and then presents our data analy-
sis. Section 4 introduces the multi-label classifi-
cation system for emotion cause detection. Sec-
tion 5 describes the two kinds of features for our 
system, one is based on hand-coded patterns and 
the other is the generalized features. Section 6 
presents the evaluation and performance of our 
system. Section 7 highlights our main contribu-
tions and the possible future work. 
2 Related Work 
Most previous studies on textual emotion proc-
essing focus on emotion recognition or classifi-
cation given a known emotion context (Mihal-
cea and Liu 2006, Strapparava and Mihalcea 
2008, Abbasi et al 2008, Tokuhisa et al 2008). 
However, the performance is far from satisfac-
tory. One crucial problem in these works is that 
they limit the emotion analysis to a simple clas-
sification and do not explore the underlying in-
formation regarding emotions. Most theories 
conclude that emotions are often invoked by the 
perception of external events. An effective emo-
tion recognition model should thus take this into 
account.  
To the best of our knowledge, little research 
has been done with respect to emotion cause 
detection. Lee et al (2010a) first investigate the 
interactions between emotions and the corre-
sponding causes from a linguistic perspective. 
They annotate a small-scale emotion cause cor-
pus, and identify six groups of linguistic cues 
facilitating emotion cause detection. Based on 
these findings, they develop a rule-based system 
for automatic emotion cause detection (Lee et 
al., 2010b).  
Emotion cause detection can be considered as 
a kind of causal relation detection, which has 
been intensively studied for years. Most previ-
ous cause detection studies focus on a specific 
domain, such as aviation (Persing and Ng, 2009) 
and finance (Low, et al, 2001). Few works 
(Marcu and Echihabi, 2002; Girju, 2003; Chang 
and Choi, 2005) examine causal relation for 
open domains. 
In recognizing causal relations, most existing 
systems involve two steps: 1) cause candidate 
identification; 2) causal relation detection. To 
simplify the task, most systems omit the step of 
identifying cause candidates. Instead, they often 
predefine or filter out possible causes based on 
domain knowledge, e.g., 14 kinds of cause types 
are identified for aviation incidents (Persing and 
Ng, 2009). For events without specific domain 
information, open-domain systems choose to 
limit their cause candidate. For example, the 
cause-effect pairs are limited to two noun 
phrases (Chang and Choi, 2005; Girju, 2003), or 
two clauses connected with fixed conjunction 
words (Marcu and Echihabi, 2002). 
Given pairs of cause-effect candidates, causal 
relation detection is considered as a binary clas-
sification problem, i.e. ?causal? vs. ?non-
causal?. In general, there are two kinds of in-
formation extracted to identify the causal rela-
tion. One is patterns or constructions expressing 
a cause-effect relation (Chang and Choi, 2005; 
Girju, 2003), and the other is semantic informa-
tion underlying in a text (Marcu and Echihabi, 
2002; Persing and Ng, 2009), such as word pair 
probability. Undoubtedly, the two kinds of in-
formation usually interact with each other in a 
real cause detection system. 
In the literature, the three common classifica-
tion methods, i.e. unsupervised, semi-supervised, 
and supervised, have all been used for cause 
detection systems. Marcu and Echihabi (2002) 
first collected a cause corpus using an unsuper-
vised approach with the help of several conjunc-
tion words, such as ?because? and ?thus?, and 
determined the causal relation for a clause pair 
using the word pair probability. Chang and Choi 
(2005) used a semi-supervised method to recur-
sively learn lexical patterns for cause recogni-
tion based on syntactic trees. Bethard and Mar-
tin (2008) put various causal information in a 
180
supervised classifier, such as the temporal in-
formation and syntactic information.  
For our emotion cause detection, several 
practical issues need to be investigated and re-
solved. First, for the identification of cause can-
didates, we need to define a reasonable span of 
a cause. Based on our data analysis, we find that 
emotion causes often appear across phrases or 
even clauses. Second, although in emotion 
cause detection the effect is fixed, the cause is 
open-domain. We also notice that besides the 
common patterns, emotion causes have their 
own expression patterns. An effective emotion 
cause detection system should take them into 
account. 
3 Corpus Analysis  
In this section, we briefly introduce the Chinese 
emotion cause corpus (Lee et al, 2010a), and 
discuss emotion cause distribution. 
3.1 Emotion Cause corpus 
Lee at al. (2010a) made the first attempt to ex-
plore the correlation between emotions and 
causes, and annotate a Chinese emotion cause 
corpus. The emotion cause corpus focuses on 
five primary emotions, namely ?happiness?, 
?sadness?, ?fear?, ?anger?, and ?surprise?. The 
emotions are explicitly expressed by emotion 
keywords, e.g., gao1xing4 ?happy?, shang1xin1 
?sad?, etc. The corpus is created as follows. 
1. 6,058 entries of Chinese sentences are ex-
tracted from the Academia Sinica Balanced 
Corpus of Mandarin Chinese (Sinica Cor-
pus) with the pattern-match method as well 
as the list of 91 Chinese primary emotion 
keywords (Chen et al, 2009). Each entry 
contains the focus sentence with the emo-
tion keyword ?<FocusSentence>? plus the 
sentence before ?<PrefixSentence>? and 
after ?<SuffixSentence>? it. For each entry, 
the emotion keywords are indexed since 
more than one emotion may be presented in 
an entry;  
2. Some preprocessing, such as balancing the 
number of entry among emotions, is done 
to remove some entries. Finally, 5,629 en-
tries remain; 
3. Each emotion keyword is annotated with 
its corresponding causes if existing. An 
emotion keyword can sometimes be associ-
ated with more than one cause, in such a 
case, both causes are marked. Moreover, 
the cause type is also identified, which is 
either a nominal event or a verbal event (a 
verb or a nominalization).  
Lee at al. (2010a) notice that 72% of the ex-
tracted entries express emotions, and 80% of the 
emotional entries have a cause. 
3.2 The Analysis of Emotion Causes 
To have a deeper understanding of emotion 
cause detection, we take a closer look at the 
emotion cause distribution, including the distri-
bution of emotion cause occurrence and the dis-
tribution of emotion cause text. 
 
The occurrence of emotion causes: According 
to most emotion theories, an emotion is gener-
ally invoked by an external event. The corpus 
shows that, however, 20% of the emotional en-
tries have no cause. Entries without causes ex-
plicitly expressed are mainly due to the follow-
ing reasons: 
i) There is not enough contextual information, 
for instance the previous or the suffix sentence 
is interjections, e.g., en heng ?aha?;  
ii) When the focus sentence is the beginning 
or the ending of a paragraph, no prefix sentence 
or suffix sentence can be extracted as the con-
text. In this case, the cause may be beyond the 
context;  
iii) The cause is obscure, which can be very 
abstract or even unknown reasons.  
 
The emotion cause text: A cause is considered 
as a proposition. It is generally assumed that a 
proposition has a verb which optionally takes a 
noun occurring before it as the subject and a 
noun after it as the object. However, a cause can 
also be expressed as a nominal. In other words, 
both the predicate and the two arguments are 
optional provided that at least one of them is 
present. Thus, the fundamental issue in design-
ing a cause detection system is the definition of 
the span of a cause text. As mentioned, most 
previous studies on causal relations choose to 
ignore the identification of cause candidates. In 
this paper, we first analyze the distribution of 
cause text and then determine the cause candi-
dates for an emotion. 
Based on the emotion cause corpus, we find 
that emotion causes are more likely to be ex-
181
pressed by verbal events than nominal events 
(85% vs. 15%). Although a nominalization (a 
kind of verbal events) is usually a noun phrase, 
a proposition containing a verb plays a salient 
role in the expressions of emotion causes, and 
thus a cause candidate are more likely to be a 
clause-based unit. 
In addition, the actual cause can sometimes 
be too long and complicated, which involves 
several events. In order to explore the span of a 
cause text, we do the following analysis. 
 
Table 1: The clause distribution of cause texts 
Position Cause (%) Position Cause (%) 
Left_0 12.90 Right _0 15.54 
Left_1 31.37 Right _1  9.55 
Left_2 13.31 Right_n  
(n>1) 
9.18 
Left_n 
(n>2) 
10.15   
Total  67.73  32.27 
 
Table 2: The multi-clause distribution of cause 
text 
Same clause % Cross-clauses % 
Left_0 16.80 Left_2_1_0 0.25 
Left_1 31.82 Left_2_1 10.84 
Left_2 7.33 Left_1_0 0.62 
Right _0 18.97 Right_0_1 2.55 
Right _1  10.59   
Total 85.75  14.25 
 
Firstly, for each emotion keyword, an entry is 
segmented into clauses with four punctuations 
(i.e. commas, periods, question marks and ex-
clamation marks), and thus an entry becomes a 
list of cause candidates. For example, when an 
entry has four clauses, its corresponding list of 
cause candidates contains five text units, i.e. 
<left_2, left_1, left_0, right_0, right_1>. If we 
assume the clause where emotion keyword lo-
cates is a focus clause, ?left_2? and ?left_1? are 
previous two clauses, and ?right_1? is the fol-
lowing one. ?left_0? and ?right_0? are the partial 
texts of the focus clause, which locate in the left 
side of and the right side of the emotion key-
word, respectively. Moreover, a cause candidate 
must contain either a noun or a verb because a 
cause is either a verbal event or a nominal event; 
otherwise, it will be removed from the list. 
Secondly, we calculate whether a cause can-
didate overlaps with the real cause, as shown in 
Table 1. We find that emotion causes are more 
likely to occur in the left of emotion keyword. 
This observation is consistent with the fact that 
an emotion is often trigged by an external hap-
pened event. Thirdly, for all causes occurring 
between ?left_2? and ?right_1?, we calculate 
whether a cause occurs across clauses, as in Ta-
ble 2. We observe that most causes locate 
within the same clause of the representation of 
the emotion (85.57%). This suggests that a 
clause may be the most appropriate unit to de-
tect a cause. 
 
4 Emotion Cause Detection Based on 
Multi-label Classification 
A cause detection system is to identify the caus-
al relation between a pair of two text units. For 
emotion cause detection, one of the two text 
units is fixed (i.e. the emotion keyword), and 
therefore the remaining two unresolved issues 
are the identification of the other text unit and 
the causal relation. 
From the above data analysis, there are two 
observations. First, most emotion causes are 
verbal events, which are often expressed by a 
proposition (or a clause). Thus, we define an-
other text unit as a clause, namely a cause can-
didate. Second, as most emotion causes occur 
between ?left_2? and ?right_1? (~80%), we de-
fine the cause candidates for an emotion as 
<left_2, left_1, left_0, right_0, right_1>.  
Differing from the existing cause systems, we 
formalize emotion cause detection as a multi-
label problem. In other words, given an emotion 
keyword and its context, its label is the loca-
tions of its causes, such as ?left_1, left_0?. This 
multi-label-based formalization of the cause 
detection task has two advantages. First, it is an 
integrated system detecting causes for an emo-
tion from the contextual information. In most 
previous cause detection systems, a causal rela-
tion is identified based on the information be-
tween two small text units, i.e. a pair of clauses 
or noun phrases, and therefore it is often the 
case that long-distance information is missed. 
Second, the multi-label-based tagging is able to 
182
capture the relationship between two cause can-
didates. For example, ?left_2? and ?left_1? are 
often combined as a complicated event as a 
cause.   
As a multi-label classification task, every 
multi-label classifier is applicable. In this study, 
we use a simple strategy: we treat each possible 
combination of labels appearing in the training 
data as a unique label. Note that an emotion 
without causes is labeled as ?None?. This con-
verts multi-label classification to single-label 
classification, which is suitable for any multi-
class classification technologies. In particular, 
we choose a Max Entropy tool, Mallet1, to per-
form the classification.  
5 Linguistic Features  
As explained, there are basically two kinds of 
features for cause detection, namely pattern-
based features and semantic-based features. In 
this study, we develop two sets of patterns 
based on linguistic analysis: one is a set of ma-
nually generalized patterns, and the other con-
tains automatically generalized patterns. All of 
these patterns explore causal constructions ei-
ther for general causal relations or for specific 
emotion cause relations. 
5.1 Linguistic Cues  
Based on the linguistic analysis, Lee et al 
(2010a) identify six groups of linguistic cue 
words that are highly collocated with emotion 
causes, as shown in Table 3. Each group of the 
linguistic cues serves as an indicator marking 
the causes in different emotional constructions. 
In this paper, these groups of linguistic cues are 
reinterpreted from the computational perspec-
tive, and are used to develop pattern-based fea-
tures for the emotion cause detection system.  
 
Table 3:  Linguistic cue words for emotion 
cause detection (Lee et al 2010a) 
Group Cue Words 
I: 
Prepositions 
?for? as in ?I will do this for you?: wei4, 
wei4le 
?for? as in ?He is too old for the job?: 
dui4, dui4yu2 
?as?: yi3 
                                                 
1
 http://mallet.cs.umass.edu/ 
II: 
Conjunctions 
?because?: yin1, yin1wei4, you2yu2 
?so?: yu1shi4, suo3yi3, yin1er2 
?but?: ke3shi4 
III:  
Light Verbs ?to make?: rang4, ling4, shi3 
IV: 
Reported 
Verbs 
?to think about?: xiang3dao4, 
xiang3qi3, yi1xiang3, xiang3 lai2 
?to talk about?: shuo1dao4, shuo1qi3, 
yi1shuo1, jiang3dao4, jiang3qi3, 
yi1jiang3, tan2dao4, tan2qi3, yi1tan2, 
ti2dao4, ti2qi3, yi1ti2 
V: 
Epistemic 
Markers 
?to hear?: ting1, ting1dao4, ting1shuo1 
?to see?: kan4, kan4dao4, kan4jian4, 
jian4dao4, jian4, yan3kan4, qiao2jian4 
?to know?: zhi1dao4, de2zhi1, de2xi1, 
huo4zhi1, huo4xi1, fa1xian4, fa1jue2 
?to exist?: you3 
VI: 
Others 
?is?: deshi4 
?say?: deshuo1 
?at?: yu2 
?can?: neng2  
 
For emotion cause processing, Group I and II 
contain cues which are for general cause detec-
tion, and while Group III, IV and V include 
cues specifically for emotion cause detection. 
Group VI includes other linguistic cues that do 
not fall into any of the five groups.  
Group I covers some prepositions which all 
roughly mean ?for?, and Group II contains the 
conjunctions that explicitly mark the emotion 
cause. Group I is expected to capture the prepo-
sitions constructions in the focus clause where 
the emotion keyword locates. Group II tends to 
capture the rhetorical relation expressed by con-
junction words so as to infer causal relation 
among multi-clauses. These two groups are typ-
ical features for general cause detection. 
Group III includes three common light verbs 
which correspond to the English equivalents ?to 
make? or ?to cause?. Although these light verbs 
themselves do not convey any concrete meaning, 
they are often associated with several construc-
tions to express emotions and at the same time 
indicate the position of emotion causes. For ex-
ample, ?The birthday party made her happy?.  
One apparent difference between emotion 
causes and general causes is that emotions are 
often triggered by human activities or the per-
ception of such activities, e.g., ?glad to say? or 
?glad to hear?. Those human activities are often 
strong indicators for the location of emotion 
183
causes. Group IV and V are used to capture this 
kind of information. Group IV is a list of verbs 
of thinking and talking, and Group V includes 
four types of epistemic markers which are usu-
ally verbs marking the cognitive awareness of 
emotions in the complement position. The epis-
temic markers include verbs of seeing, hearing, 
knowing, and existing. 
  
5.2 Linguistic Patterns  
With the six groups of linguistic cues, we gen-
eralize 14 rules used in Lee et al (2010b) to 
locate the clause positions of an emotion cause, 
as shown in Table 4. The abbreviations used in 
the rules are given as follows:  
 
C = Cause 
K = Emotion keyword 
B = Clauses before the focus clause 
F = Focus clause/the clause containing the emotion 
verb 
A = Clauses after the focus clause 
 
Table 4: Linguistic rules for emotion cause de-
tection (Lee et al 2010b) 
No. Rules 
1 i) C(B/F) + III(F)  + K(F)  
ii) C = the nearest N/V before I in F/B 
2 i)  IV/V/I/II(B/F) + C(B/F) + K(F)  
ii) C = the nearest N/V before K in F 
3 i) I/II/IV/V (B) + C(B)  + K(F)  
ii) C = the nearest N/V after I/II/IV/V in B 
4 i) K(F) + V/VI(F) + C(F/A)  
ii) C = the nearest N/V after V/VI in F/A 
5 i) K(F)+II(A)+C(A)  
ii) C = the nearest N/V after II in A 
6 i) III(F) + K(F) + C(F/A)  
ii) C = the nearest N/V after K in F or A 
7 i) yue4 C yue4 K ?the more C the more K? (F)   
ii) C = the V in between the two yue4?s in F 
8 i) K(F) + C(F)  
ii) C = the nearest N/V after K in F 
9 i) V(F) + K(F)  
ii) C = V+(an aspectual marker) in F 
10 i) K(F)  + de ?possession?(F) + C(F)  
ii) C = the nearest N/V +?+N after de in F 
12 i) K(B) + IV (B) + C(F)   
ii) C = the nearest N/V after IV in F 
13 i) IV(B) + C(B) + K(F)  
ii) C = the nearest N/V after IV in B 
14 i) C(B) +  K(F)  
ii) C = the nearest N/V before K in B  
 
For illustration, an example of the rule descrip-
tion is given in Rule 1. 
Rule 1: 
i) C(B/F) + III(F) + K(F)  
ii) C = the nearest N/V before III in F/B  
 
Rule 1 indicates that the cause (C) comes before 
Group III cue words. Theoretically, in identify-
ing C, we look for the nearest verb/noun occur-
ring before Group III cue words in the focus 
clause (F) or the clauses before the focus clause 
(B), and consider the clause containing this 
verb/noun as a cause. Practically, for each cause 
candidate, i.e. ?left_1?, if it contains this 
verb/noun, we create a feature with 
?left_1_rule_1=1?. 
5.3 Generalized Patterns  
Rule-based patterns usually achieve a rather 
high accuracy, but suffer from low coverage. To 
avoid this shortcoming, we extract a generalized 
feature automatically according to the rules in 
Table 4. The features are able to detect two 
kinds of constructions, namely functional con-
structions, i.e. rhetorical constructions, and spe-
cific constructions for emotion causes.  
Local functional constructions: a cause occur-
ring in the focus clause is often expressed with 
certain functional words, such as ?because of?, 
?due to?. In order to capture the various expres-
sions of these functional constructions, we iden-
tify all functional words around the given emo-
tion keyword. For an emotion keyword, we 
search ?left_0? from the right until a noun or a 
verb is found. Next, all unigrams and bigrams 
between the noun or the verb and the emotion 
keyword are extracted. The same applies to 
?right_0?. 
Long-distance conjunction constructions: 
Group II enumerates only some typical conjunc-
tion words. To capture more general rhetorical 
relations, according to the given POS tags, the 
conjunction word is extracted for each cause 
candidate, if it occurs at the beginning of the 
candidate. 
Generalized action and epistemic verbs: 
Group IV and V cover only partial action and 
epistemic verbs. To capture possible related ex-
pressions, we take the advantage of Chinese 
characters. In Chinese, each character itself usu-
ally has a meaning and some characters have a 
strong capability to create words with extended 
meaning. For example, the character ?ting1-
listen? combines with other characters to create 
184
words expressing ?listening?, such as ting1jian4, 
ting1wen5. With the selected characters regard-
ing reported verbs and epistemic markers, each 
cause candidate is checked to see whether it 
contains the predefined characters.  
6 Experiments 
For the emotion cause corpus, we reserve 80% 
as the training data, 10% as the development 
data, and 10% as the test data. During evalua-
tion, we first convert the multi-label tag output-
ted from our system into a binary tag (?Y? 
means the presence of a causal relation; ?N? in-
dicates the absence of a causal relation) between 
the emotion keyword and each candidate in its 
corresponding cause candidates. Thus, the 
evaluation scores for binary classification based 
on three common measures, i.e. precision, recall 
and F-score, are chosen. 
6.1 Linguistic Feature Analysis 
According to the distribution in Table 1, we de-
sign a naive baseline to allow feature analysis. 
The baseline searches for the cause candidates 
in the order of <left_1, right_0, left_2, left_0, 
right_1>. If the candidate contains a noun or 
verb, consider this clause as a cause and stop. 
We run the multi-label system with different 
groups of features and the performances are 
shown in Table 5. The feature set begins with 
linguistic patterns (LP), and is then incorporated 
with local functional constructions (LFC), long-
distance conjunction constructions (LCC), and 
generalized action and epistemic verbs (GAE), 
one by one. Since the ?N? tag is overwhelming, 
we report only the Mac average scores for both 
?Y? and ?N? tags.  
In Table 5, we first notice that the perform-
ances achieve significant improvement from the 
baseline to the final system (~17%). This indi-
cates that our linguistic features are effective for 
emotion cause detection. In addition, we ob-
serve that LP and LFC are the best two effective 
features, whereas LCC and GAE have slight 
contributions. This shows that our feature ex-
traction has a strong capability to detect local 
causal constructions, and is yet unable to detect 
the long-distance or semantic causal informa-
tion. Here, ?local? refers to the information in 
the focus clause. We also find that incorporating 
LFC, which is a pure local feature, generally 
improves the performances of all cause candi-
dates, i.e. ~5% improvement for ?left_1?. This 
indicates that our multi-label integrated system 
is able to convey information among cause can-
didates.  
 
Table 5: The overall performance with different 
feature sets of the multi-label system 
 Precision Recall F-score 
Baseline 56.64 57.70 56.96 
LP 74.92 66.70 69.21 
+ LFC 72.80 71.94 72.35 
+ LCC 73.60 72.50 73.02 
+ GAE 73.90 72.70 73.26 
 
Table 6: The separate performances for ?Y? and 
?N? tags of the multi-label system 
 ?Y? ?N? 
Baseline 33.06 80.85 
LP 48.32 90.11 
+ LFC 55.45 89.24 
+ LCC 56.48 89.57 
+ GPE 56.84 89.68 
 
Table 6 shows the performances (F-scores) 
for ?Y? and ?N? tags separately. First, we notice 
that the performances of the ?N? tag are much 
better than the ones of ?Y? tag. Second, it is sur-
prising that incorporating the linguistic features 
significantly improves only the ?Y? tag (from 
33% to 56%), but does not affect ?N? tag. This 
suggests that our linguistic features are effective 
to detect the presence of causal relation, and yet 
do not hurt the detection of ?non_causal? rela-
tion. For the ?Y? tag, the features LP and LFC 
achieve ~15% and ~7% improvements respec-
tively. LCC and GPE, on the other hand, show 
slight improvements only. 
Finally, Table 7 shows the detailed perform-
ances of our multi-label system with all features. 
The last row shows the overall performances of 
?Y? and ?N? tags. For the ?Y? tag, the closer the 
cause candidates are to the emotion keyword, 
the better performances the system achieves. 
This proves that the features we propose effec-
tively detect local emotion causes, more effort, 
185
Table 7: The detailed performance for the multi-label system including all features 
?Y? tag Precision Recall F-score ?N? tag Precision Recall F-score 
Left_0 68.92 68.92 68.92 Left_0 93.72 93.72 93.72 
Left_1 57.63 63.35 60.36 Left_1 82.90 79.22 81.02 
Left_2 29.27 20.69 24.24 Left_2 89.23 92.93 91.04 
Right_0 67.78 64.89 66.30 Right_0 82.63 84.41 83.51 
Right_1 54.84 30.91 39.54 Right_1 92.00 96.90 94.38 
Total 58.84 54.98 56.84 Total 88.96 90.42 89.68 
 
Table 8: The detailed performance for the single-label system including all features 
?Y? tag Precision Recall F-score ?N? tag Precision Recall F-score 
Left_0 65.39  68.92 67.11 Left_0 93.65  92.62 93.13 
Left_1 61.19  50.93 55.59 Left_1 79.64   85.60 82.51 
Left_2 28.57   20.69 24.00 Left_2 89.20   92.68 90.91 
Right_0 70.13   57.45 63.16 Right_0 80.30  87.63 83.81 
Right_1 33.33   40.00 36.36 Right_1 92.50   90.24 91.36 
Total 55.67   50.00 52.68 Total 87.85  90.08 88.95 
 
however, should be put on the detection of 
long-distance causes. In addition, we find that 
the detection of long-distance causes usually 
relies on two kinds of information for inference: 
rhetorical relation and deep semantic informa-
tion. 
6.2 Modeling Analysis 
To compare our multi-label model with single-
label models, we create a single-label system as 
follows. The single-label model is a binary 
classification for a pair comprising the emotion 
keyword and a candidate in its corresponding 
cause candidates. For each pair, all linguistic 
features are extracted only from the focus 
clause and its corresponding cause candidate. 
Note that we only use the features in the focus 
clause for ?left_0? and ?right_0?. The perform-
ances are shown in Table 8. 
Comparing Tables 7 and 8, all F-scores of 
the ?Y? tag increase and the performances of 
the ?N? tag remain almost the same for both the 
single-label model and our multi-label model. 
We also find that the multi-label model takes 
more advantage of local information, and im-
proves the performances, particularly for 
?left_1?.  
To take an in-depth analysis of the cause de-
tection capability of the multi-label model, an 
evaluation is designed that the label is treated 
as a tag from the multi-label classifier. Due to 
the tag sparseness problem (as in Table 2), only 
the ?left_2, left_1? tag is detected in the test 
data, and its performance is 21% precision, 
26% recall and 23% F-score. Furthermore, we 
notice that ~18% of the ?left_1? tags are de-
tected through this combination tag. This 
shows that some causes need to take into ac-
count the mutual information between clauses. 
Although the scores are low, it still shows that 
our multi-label model provides an effective 
way of detecting some of the multi-clauses 
causes. 
7 Conclusion 
We treat emotion cause detection as a multi-
label task, and develop two sets of linguistic 
features for emotion cause detection based on 
linguistic cues. The experiments on the small-
scale corpus show that both the multi-label 
model and the linguistic features are able to 
effectively detect emotion causes. The auto-
matic detection of emotion cause will in turn 
allow us to extract directly relevant information 
for public opinion mining and event prediction. 
It can also be used to improve emotion detec-
tion and classification. In the future, we will 
attempt to improve our system from two as-
pects. On the one hand, we will explore more 
powerful multi-label classification models for 
our system. On the other hand, we will investi-
gate more linguistic patterns or semantic in-
formation to further help emotion cause detec-
tion. 
186
References 
Abbasi, A., H. Chen, S. Thoms, and T. Fu. 2008. 
Affect Analysis of Web Forums and Blogs using 
Correlation Ensembles?. In IEEE Tran. Knowl-
edge and Data Engineering, vol. 20(9), pp. 1168-
1180. 
Bethard, S. and J. Martin. 2008. Learning Semantic 
Links from a Corpus of Parallel Temporal and 
Causal Relations. In Proceedings of ACL. 
Descartes, R. 1649. The Passions of the Soul. In J. 
Cottingham et al (Eds), The Philosophical Writ-
ings of Descartes. Vol. 1: 325-404. 
Chang, D.-S. and K.-S. Choi. 2006. Incremental cue 
phrase learning and bootstrapping method for 
causality extraction using cue phrase and word 
pair probabilities. Information Processing and 
Management. 42(3): 662-678. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. Are 
Emotions Enumerable or Decomposable? And 
Its Implications for Emotion Processing. In Pro-
ceedings of the 23rd Pacific Asia Conference on 
Language, Information and Computation. 
Girju, R. 2003. Automatic Detection of Causal Re-
lations for Question Answering. In the 41st An-
nual Meeting of the Association for Computa-
tional Linguistics, Workshop on Multilingual 
Summarization and Question Answering - Ma-
chine Learning and Beyond, Sapporo, Japan. 
James, W. 1884. What is an Emotion? Mind, 
9(34):188?205. 
Lee, S. Y. M., Y. Chen and C.-R. Huang. 2010a. A 
Text-driven Rule-based System for Emotion 
Cause Detection. In Proceedings of NAACL-HLT 
2010 Workshop on Computational Approaches to 
Analysis and Generation of Emotion in Text. 
Lee, S. Y. M., Y. Chen, S. Li and C.-R. Huang. 
2010b. Emotion Cause Events: Corpus Construc-
tion and Analysis. In Proceedings of LREC 2010. 
Low, B. T., K. Chan , L. L. Choi , M. Y. Chin , S. L. 
Lay. 2001. Semantic Expectation-Based Causa-
tion Knowledge Extraction: A Study on Hong 
Kong Stock Movement Analysis, In Proceedings 
of the 5th Pacific-Asia Conference on Knowledge 
Discovery and Data Mining, p.114-123, April 
16-18.  
Marcu, D., and A. Echihabi. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of ACL. 
Mihalcea, R. and H. Liu. 2006. A Corpus-based 
Approach to Finding Happiness. In Proceedings 
of the AAAI Spring Symposium on Computational 
Approaches to Weblogs.  
Persing, I. and V. Ng. 2009. Semi-Supervised Cause 
Identification from Aviation Safety Reports. In 
Proceedings of ACL. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Strapparava, C. and R. Mihalcea. 2008. Learning to 
Identify Emotions in Text. In Proceedings of the 
ACM Conference on Applied Computing ACM-
SAC. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. 
Emotion recognition Using Massive Examples 
Extracted from the Web. In Proceedings of COL-
ING. 
Wierzbicka, A. 1999. Emotions across Languages 
and Cultures: Diversity and Universals. Cam-
bridge: Cambridge University Press. 
 
 
 
187
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 635?643,
Beijing, August 2010
Sentiment Classification and Polarity Shifting 
 
Shoushan Li??  Sophia Yat Mei Lee?  Ying Chen?  Chu-Ren Huang?  Guodong Zhou?  
 
?Department of CBS 
The Hong Kong Polytechnic University 
{shoushan.li, sophiaym, 
chenying3176, churenhuang} 
@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and 
Technology 
      Soochow University
gdzhou@suda.edu.cn 
 
  
Abstract 
Polarity shifting marked by various 
linguistic structures has been a challenge 
to automatic sentiment classification. In 
this paper, we propose a machine learning 
approach to incorporate polarity shifting 
information into a document-level 
sentiment classification system. First, a 
feature selection method is adopted to 
automatically generate the training data 
for a binary classifier on polarity shifting 
detection of sentences. Then, by using the 
obtained binary classifier, each document 
in the original polarity classification 
training data is split into two partitions, 
polarity-shifted and polarity-unshifted, 
which are used to train two base 
classifiers respectively for further 
classifier combination. The experimental 
results across four different domains 
demonstrate the effectiveness of our 
approach. 
1 Introduction 
Sentiment classification is a special task of text 
classification whose objective is to classify a text 
according to the sentimental polarities of 
opinions it contains (Pang et al, 2002), e.g., 
favorable or unfavorable, positive or negative. 
This task has received considerable interests in 
the computational linguistic community due to its 
potential applications.  
In the literature, machine learning approaches 
have dominated the research in sentiment 
classification and achieved the state-of-the-art 
performance (e.g., Kennedy and Inkpen, 2006; 
Pang et al, 2002). In a typical machine learning 
approach, a document (text) is modeled as a 
bag-of-words, i.e. a set of content words without 
any word order or syntactic relation information. 
In other words, the underlying assumption is that 
the sentimental orientation of the whole text 
depends on the sum of the sentimental polarities 
of content words. Although this assumption is 
reasonable and has led to initial success, it is 
linguistically unsound since many function 
words and constructions can shift the 
sentimental polarities of a text. For example, in 
the sentence ?The chair is not comfortable?, the 
polarity of the word ?comfortable? is positive 
while the polarity of the whole sentence is 
reversed because of the negation word ?not?. 
Therefore, the overall sentiment of a document is 
not necessarily the sum of the content parts 
(Turney, 2002). This phenomenon is one main 
reason why machine learning approaches fail 
under some circumstances. 
As a typical case of polarity shifting, negation 
has been paid close attention and widely studied 
in the literature (Na et al, 2004; Wilson et al, 
2009; Kennedy and Inkpen, 2006). Generally, 
there are two steps to incorporate negation 
information into a system: negation detection 
and negation classification. For negation 
detection, some negation trigger words, such as 
?no?, ?not?, and ?never?, are usually applied to 
recognize negation phrases or sentences. As for 
negation classification, one way to import 
negation information is to directly reverse the 
polarity of the words which contain negation 
trigger words as far as term-counting approaches 
are considered (Kennedy and Inkpen, 2006). An 
alternative way is to add some negation features 
(e.g., negation bigrams or negation phrases) into 
635
machine learning approaches (Na et al, 2004). 
Such approaches have achieved certain success.  
There are, however, some shortcomings with 
current approaches in incorporating negation 
information. In terms of negation detection, 
firstly, the negation trigger word dictionary is 
either manually constructed or relies on existing 
resources. This leads to certain limitations 
concerning the quality and coverage of the 
dictionary. Secondly, it is difficult to adapt 
negation detection to other languages due to its 
language dependence nature of negation 
constructions and words. Thirdly, apart from 
negation, many other phenomena, e.g., contrast 
transition with trigger words like ?but?, 
?however?, and ?nevertheless?, can shift the 
sentimental polarity of a phrase or sentence. 
Therefore, considering negation alone is 
inadequate to deal with the polarity shifting 
problem, especially for document-level 
sentiment classification. 
In terms of negation classification, although it 
is easy for term-counting approaches to integrate 
negation information, they rarely outperform a 
machine learning baseline (Kennedy and Inkpen, 
2006). Even for machine learning approaches, 
although negation information is sometimes 
effective for local cases (e.g., not good), it fails 
on long-distance cases (e.g., I don?t think it is 
good). 
In this paper, we first propose a feature 
selection method to automatically generate a 
large scale polarity shifting training data for 
polarity shifting detection of sentences. Then, a 
classifier combination method is presented for 
incorporating polarity shifting information. 
Compared with previous ones, our approach 
highlights the following advantages?First of all, 
we apply a binary classifier to detect polarity 
shifting rather than merely relying on trigger 
words or phrases. This enables our approach to 
handle different kinds of polarity shifting 
phenomena. More importantly, a feature 
selection method is presented to automatically 
generate the labeled training data for polarity 
shifting detection of sentences. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
approach in details. Experimental results are 
presented and analyzed in Section 4. Finally, 
Section 5 draws the conclusion and outlines the 
future work. 
2 Related Work 
Generally, sentiment classification can be 
performed at four different levels: word level 
(Wiebe, 2000), phrase level (Wilson et al, 2009), 
sentence level (Kim and Hovy, 2004; Liu et al, 
2005), and document level (Turney, 2002; Pang 
et al, 2002; Pang and Lee, 2004; Riloff et al, 
2006). This paper focuses on document-level 
sentiment classification. 
In the literature, there are mainly two kinds of 
approaches on document-level sentiment 
classification: term-counting approaches 
(lexicon-based) and machine learning 
approaches (corpus-based). Term-counting 
approaches usually involve deriving a sentiment 
measure by calculating the total number of 
negative and positive terms (Turney, 2002; Kim 
and Hovy, 2004; Kennedy and Inkpen, 2006). 
Machine learning approaches recast the 
sentiment classification problem as a statistical 
classification task (Pang and Lee, 2004). 
Compared to term-counting approaches, 
machine learning approaches usually achieve 
much better performance (Pang et al, 2002; 
Kennedy and Inkpen, 2006), and have been 
adopted to more complicated scenarios, such as 
domain adaptation (Blitzer et al, 2007), 
multi-domain learning (Li and Zong, 2008) and 
semi-supervised learning (Wan, 2009; Dasgupta 
and Ng, 2009) for sentiment classification. 
Polarity shifting plays a crucial role in 
phrase-level, sentence-level, and document-level 
sentiment classification. However, most of 
previous studies merely focus on negation 
shifting (polarity shifting caused by the negation 
structure). As one pioneer research on sentiment 
classification, Pang et al (2002) propose a 
machine learning approach to tackle negation 
shifting by adding the tag ?not? to every word 
between a negation trigger word/phrase (e.g., not, 
isn't, didn't, etc.) and the first punctuation mark 
following the negation trigger word/phrase. To 
their disappointment, considering negation 
shifting has a negligible effect and even slightly 
harms the overall performance. Kennedy and 
Inkpen (2006) explore negation shifting by 
incorporating negation bigrams as additional 
features into machine learning approaches. The 
636
experimental results show that considering 
sentiment shifting greatly improves the 
performance of term-counting approaches but 
only slightly improves the performance of 
machine learning approaches. Other studies such 
as Na et al (2004), Ding et al (2008), and Wilson 
et al (2009) also explore negation shifting and 
achieve some improvements1. Nonetheless, as far 
as machine learning approaches are concerned, 
the improvement is rather insignificant (normally 
less than 1%). More recently, Ikeda et al (2008) 
first propose a machine learning approach to 
detect polarity shifting for sentence-level 
sentiment classification, based on a 
manually-constructed dictionary containing 
thousands of positive and negative sentimental 
words, and then adopt a term-counting approach 
to incorporate polarity shifting information. 
3 Sentiment Classification with Polarity 
Shifting Detection 
 
 
Figure 1: General framework of our approach 
 
The motivation of our approach is to improve the 
performance of sentiment classification by robust 
treatment of sentiment polarity shifting between 
sentences. With the help of a binary classifier, the 
sentences in a document are divided into two 
parts: sentences which contain polarity shifting 
structures and sentences without any polarity 
shifting structure. Figure 1 illustrates the general 
framework of our approach. Note that this 
framework is a general one, that is, different 
polarity shifting detection methods can be applied 
to differentiate polarity-shifted sentences from 
those polarity-unshifted sentences and different 
                                                      
1
 Note that Ding et al (2006) also consider but-clause, another 
important structure for sentiment shifting. Wilson et al (2009) use 
conjunctive and dependency relations among polarity words. 
polarity classification methods can be adopted to 
incorporate sentiment shifting information. For 
clarification, the training data used for polarity 
shifting detection and polarity classification are 
referred to as the polarity shifting training data 
and the polarity classification training data, 
respectively. 
3.1 Polarity Shifting Detection 
In this paper, polarity shifting means that the 
polarity of a sentence is different from the 
polarity expressed by the sum of the content 
words in the sentence. For example, in the 
sentence ?I am not disappointed?, the negation 
structure makes the polarity of the word 
'disappointed' different from that of the whole 
sentence (negative vs. positive). Apart from the 
negation structure, many other linguistic 
structures allow polarity shifting, such as 
contrast transition, modals, and 
pre-suppositional items (Polanyi and Zaenen, 
2006). We refer these structures as polarity 
shifting structures. 
One of the great challenges in building a 
polarity shifting detector lies on the lack of 
relevant training data since manually creating a 
large scale corpus of polarity shifting sentences 
is time-consuming and labor-intensive. Ikeda et 
al. (2008) propose an automatic way for 
collecting the polarity shifting training data 
based on a manually-constructed large-scale 
dictionary. Instead, we adopt a feature selection 
method to build a large scale training corpus of 
polarity shifting sentences, given only the 
already available document-level polarity 
classification training data. With the help of the 
feature selection method, the top-ranked word 
features with strong sentimental polarity 
orientation, e.g., ?great?, ?love?, ?worst? are first 
chosen as the polarity trigger words. Then, those 
sentences with the top-ranked polarity trigger 
words in both categories of positive and negative 
documents are selected. Finally, those candidate 
sentences taking opposite-polarity compared to 
the containing trigger word are deemed as 
polarity-shifted. 
The basic idea of automatically generating the 
polarity shifting training data is based on the 
assumption that the real polarity of a word or 
phrase is decided by the major polarity category 
where the word or phrase appears more often. As 
a result, the sentences in the 
Polarity Shifting 
Detector 
Documents 
 
Polarity-shifted 
Sentences 
Polarity-unshifted 
Sentences 
Polarity Classifier Positive/Negative 
637
frequently-occurring category would be seen as 
polarity-unshifted while the sentences in the 
infrequently-occurring category would be seen 
as polarity-shifted. 
In the literature, various feature selection 
methods, such as Mutual Information (MI), 
Information Gain (IG) and Bi-Normal Separation 
(BNS) (Yang and Pedersen, 1997; Forman 2003), 
have been employed to cope with the problem of 
the high-dimensional feature space which is 
normal in sentiment classification.  
In this paper, we employ the theoretical 
framework, proposed by Li et al (2009), 
including two basic measurements, i.e. frequency 
measurement and ratio measurement, where the 
first measures, the document frequency of a term 
in one category, and the second measures, the 
ratio between the document frequency in one 
category and other categories. In particular, a 
novel method called Weighed Frequency and 
Odds (WFO) is proposed to incorporate both 
basic measurements: 
1( | )( , ) ( | ) {max(0, log )}( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
where ( | )iP t c  denotes the probability that a 
document x contains the term t with the 
condition that x belongs to category ic ; 
( | )iP t c  denotes the probability that a document 
x contains the term t with the condition that x 
does not belong to category ic . The left part of 
the formula ( | )iP t c  implies the first basic 
measurement and the right part 
log( ( | ) / ( | ))i iP t c P t c  implies the second one. 
The parameter ?  0 1?? ?? ?is thus to tune the 
weight between the two basic measurements. 
Especially, when ?  equals 0, the WFO method 
fades to the MI method which fully prefers the 
second basic measurement. 
Figure 2 illustrates our algorithm for 
automatically generating the polarity shifting 
training data where 1c and 2c denote the two 
sentimental orientation categories, i.e. negative 
and positive. Step A segments a document into 
sentences with punctuations. Besides, two 
special words, ?but? and ?and?, are used to 
further segment some contrast transition 
structures and compound sentences. Step B 
employs the WFO method to rank all features 
including the words. Step D extracts those 
polarity-shifted and polarity-unshifted sentences 
containing top it ?  where maxN denotes the 
upper-limit number of sentences in each 
category of the polarity shifting training data and 
#(x) denotes the total number of the elements in 
x. Apart from that, the first word in the following 
sentence is also included to capture a common 
kind of long-distance polarity shifting structure: 
contrast transition. Thus, important trigger words 
like ?however? and ?but? may be considered. 
Finally, Step E guarantees the balance between 
the two categories of the polarity shifting 
training data. 
Given the polarity shifting training data, we 
apply SVM classification algorithm to train a 
polarity-shifting detector with word unigram 
features. 
Input: 
The polarity classification training data: the negative 
sentimental document set 
1c
D and the positive sentimental 
document set
 2c
D . 
Output: 
    The polarity shifting training data: the 
polarity-unshifted sentence set unshiftS  and the polarity- 
shifted sentence set
 
shiftS . 
Procedure: 
A. Segment documents 
1c
D  and  
2c
D  to single 
sentences  
1c
S  and  
2c
S . 
B. Apply feature selection on the polarity classification  
training data and get the ranked features, 
1( ,..., ,..., )top top i top Nt t t? ? ?  
C. shiftS  = {}, unshiftS  = {} 
D. For  top it ?  in  1( ,..., ,..., )top top i top Nt t t? ? ? : 
D1) if #( shiftS )> maxN : break 
D2) Collect all sentences  
1,top i c
S
?
 and  
2,top i c
S
?
 
which contain  top it ?  from  1cS  and  2cS  
respectively 
D3)  if #(
1,top i c
S
?
)>#(
2,top i c
S
?
): 
put  
2,top i c
S
?
 into  shiftS  
put  
1,top i c
S
?
 into  unshiftS  
else: 
put  
1,top i c
S
?
 into  shiftS  
put  
2,top i c
S
?
 into  unshiftS  
E. Randomly select 
maxN sentences from unshiftS as the 
output of 
unshiftS  
 
Figure 2: The algorithm for automatically 
generating the polarity shifting training data 
 
638
3.2 Polarity Classification with Classifier 
Combination  
After polarity shifting detection, each document 
in the polarity classification training data is 
divided into two parts, one containing 
polarity-shifted sentences and the other 
containing polarity-unshifted sentences, which 
are used to form the polarity-shifted training data 
and the polarity-unshifted training data. In this 
way, two different polarity classifiers, If  and 
2f , can be trained on the polarity-shifted 
training data and the polarity-unshifted training 
data respectively. Along with classifier 3f , 
trained on all original polarity classification 
training data, we now have three base classifiers 
in hand for possible classifier combination via a 
multiple classifier system. 
The key issue in constructing a multiple 
classifier system (MCS) is to find a suitable way 
to combine the outputs of the base classifiers. In 
MCS literature, various methods are available 
for combining the outputs, such as fixed rules 
including the voting rule, the product rule and 
the sum rule (Kittler et al, 1998) and trained 
rules including the weighted sum rule (Fumera 
and Roli, 2005) and the meta-learning 
approaches (Vilalta and Drissi, 2002). In this 
study, we employ the product rule, a popular 
fixed rule, and stacking (D?eroski and ?enko, 
2004), a well-known trained rule, to combine the 
outputs. 
Formally, each base classifier provides some 
kind of confidence measurements, e.g., posterior 
probabilities of the test sample belonging to each 
class. Formally, each base classifier 
 ( 1,2,3)lf l =  assigns a test sample (denoted as 
lx ) a posterior probability vector ( )lP x

:  
1 2( ) ( | ), ( | ))tl l lP x p c x p c x= (

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging 1c . 
The product rule combines the base classifiers 
by multiplying the posterior possibilities and 
using the multiplied possibility for decision, i.e. 
3
1
      arg max ( | )j i l
i l
assign y c when j p c x
=
? = ?  
Stacking belongs to well-known 
meta-learning (Vilalta and Drissi, 2002). The 
key idea behind meta-learning is to train a 
meta-classifier with input attributes that are the 
outputs of the base classifiers. Hence, 
meta-learning usually needs some development 
data for generating the meta-training data. Let 
'x  denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( ( | ' ), ( | ' ))l l l lP x p c x p c x=

 
A meta-classifier can be trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ( ' ), ( ' ), ( ' ))meta l l lx P x P x P x= = ==
  
 
Stacking is a specific meta-learning rule, in 
which a leave-one-out or a cross-validation 
procedure on the training data is applied to 
generate the meta-training data instead of using 
extra development data. In our experiments, we 
perform stacking with 10-fold cross-validation to 
generate the meta-training data. 
4 Experimentation 
4.1 Experimental Setting 
The experiments are carried out on product 
reviews from four domains: books, DVDs, 
electronics, and kitchen appliances (Blitzer et al, 
2007)2. Each domain contains 1000 positive and 
1000 negative reviews. 
For sentiment classification, all classifiers 
including the polarity shifting detector, three 
base classifiers and the meta-classifier in 
stacking are trained by SVM using the 
SVM-light tool 3  with Logistic Regression 
method for probability measuring (Platt, 1999). 
In all the experiments, each dataset is 
randomly and evenly split into two subsets: 50% 
documents as the training data and the remaining 
50% as the test data. The features include word 
unigrams and bigrams with Boolean weights. 
4.2 Experimental Results on Polarity 
Shifting Data 
To better understand the polarity shifting 
phenomena in document-level sentiment 
classification, we randomly investigate 200 
                                                      
2
 This data set is collected by Blitzer et al (2007): 
http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 It is available at: http://svmlight.joachims.org/ 
639
polarity-shifted sentences, together with their 
contexts (i.e. the sentences before and after it), 
automatically generated by the WFO ( 0? = ) 
feature selection method. We find that nearly 
half of the automatically generated polarity- 
shifted sentences are actually polarity-unshifted 
sentences or difficult to decide. That is to say, 
the polarity shifting training data is noisy to 
some extent. One main reason is that some 
automatically selected trigger words do not 
really contain sentiment information, e.g., ?hear?, 
?information? etc. Another reason is that some 
reversed opinion is given in a review without 
any explicit polarity shifting structures.  
To gain more insights, we manually checked 
100 sentences which are explicitly 
polarity-shifted and can also be judged by 
human according to their contexts. Table 1 
presents some typical structures causing polarity 
shifting. It shows that the most common polarity 
shifting type is Explicit Negation (37%), usually 
expressed by trigger words such as ?not?, ?no?, or 
?without?, e.g., in the sentence ?I am not happy 
with this flashcard at all?. Another common type 
of polarity shifting is Contrast Transition (20%), 
expressed by trigger words such as ?however?, 
e.g., in the sentence ?It is large and stylish, 
however, I cannot recommend it because of the 
lid?. Other less common yet productive polarity 
shifting types include Exception and Until. 
Exception structure is usually expressed by the 
trigger phrase ?the only? to indicate the one and 
only advantage of the product, e.g., in the 
sentence ?The only thing that I like about it is 
that bamboo is a renewable resource?. Until 
structure is often expressed by the trigger word 
?until? to show the reversed polarity, e.g. in the 
sentence ?This unit was a great addition until the 
probe went bad after only a few months?. 
 
Polarity Shifting 
Structures 
Trigger 
Words/Phrases 
Distribution 
(%) 
Explicit Negation not, no, without 37 
Contrast Transition but, however, 
unfortunately 
20 
Implicit Negation avoid, hardly,  7 
False Impression look, seem 6 
Likelihood probably, perhaps 5 
Counter-factual should, would 5 
Exception the only 5 
Until until 3 
Table 1: Statistics on various polarity shifting 
structures 
4.3 Experimental Results on Polarity 
Classification 
For comparison, several classifiers with different 
classification methods are developed.  
1) Baseline classifier, which applies SVM with 
all unigrams and bigrams. Note that it also 
serves as a base classifier in the following 
combined classifiers. 
2) Base classifier 1, a base classifier for the 
classifier combination method. It works on the 
polarity-unshifted data.  
3) Base classifier 2, another base classifier for 
the classifier combination method. It works on 
the polarity-shifted data. 
4) Negation classifier, which applies SVM with 
all unigrams and bigrams plus negation bigrams. 
It is a natural extension of the baseline classifier 
with the consideration of negation bigrams. In 
this study, the negation bigrams are collected 
using some negation trigger words, such as ?not? 
and ?never?. If a negation trigger word is found 
in a sentence, each word in the sentence is 
attached with the word ?_not? to form a negation 
bigram. 
5) Product classifier, which combines the 
baseline classifier, the base classifier 1 and the 
base classifier 2 using the product rule. 
6) Stacking classifier, a combined classifier 
similar to the Product classifier. It uses the 
stacking classifier combination method instead 
of the product rule.  
Please note that we do not compare our approach 
with the one as proposed in Ikeda et al (2008) 
due to the absence of a manually-collected 
sentiment dictionary. Besides, it is well known 
that a combination strategy itself is capable of 
improving the classification performance. To 
justify whether the improvement is due to the 
combination strategy or our polarity shifting 
detection or both, we first randomly split the 
training data into two portions and train two base 
classifiers on each portion, then apply the 
stacking method to combine them along with the 
baseline classifier. The corresponding results are 
shown as ?Random+Stacking? in Table 2. Finally, 
in our experiments, t-test is performed to 
evaluate the significance of the performance 
improvement between two systems employing 
different methods (Yang and Liu, 1999). 
 
640
Domain Baseline Base  
Classifier 
1 
Base  
Classifier 
2 
Negation 
Classifier 
Random 
+ 
Stacking 
Shifting 
+ 
Product 
Shifting 
+ 
Stacking 
Book 0.755 0.756 0.670 0.759 0.764 0.772 0.785 
DVD 0.750 0.743 0.667 0.748 0.759 0.768 0.770 
Electronic 0.779 0.786 0.711 0.785 0.789 0.820 0.830 
Kitchen 0.818 0.814 0.683 0.826 0.835 0.840 0.849 
Table 2: Performance comparison of different classifiers with equally-splitting between training and test data 
 
Performance comparison of different 
classifiers 
Table 2 shows the accuracy results of different 
methods using 2000 polarity shifted sentences 
and 2000 polarity-unshifted sentences to train the 
polarity shifting detector (Nmax=2000). Compared 
to the baseline classifier, it shows that: 1) The 
base classifier 1, which only uses the 
polarity-unshifted sentences as the training data, 
achieves similar performance. 2)  The base 
classifier 2 achieves much lower performance 
due to much fewer sentences involved. 3) 
Including negation bigrams usually allows 
insignificant improvements (p-value>0.1), which 
is consistent with most of previous works (Pang 
et al, 2002; Kennedy and Inkpen, 2006). 4) Both 
the product and stacking classifiers with polarity 
shifting detection significantly improve the 
performance (p-value<0.05). Compared to the 
product rule, the stacking classifier is preferable, 
probably due to the performance unbalance 
among the individual classifiers, e.g., the 
performance of the base classifier 2 is much 
lower than the other two. Although stacking with 
two randomly generated base classifiers, i.e. 
?Random + Stacking?, also consistently 
outperforms the baseline classifier, the 
improvements are much lower than what has 
been achieved by our approach. This suggests 
that both the classifier combination strategy and 
polarity shifting detection contribute to the 
overall performance improvement. 
Effect of WFO feature selection method 
Figure 3 presents the accuracy curve of the 
stacking classifier when using different Lambda 
( ? ) values in the WFO feature selection method. 
It shows that those feature selection methods 
which prefer frequency information, e.g., MI and 
BNS, are better in automatically generating the 
polarity shifting training data. This is reasonable 
since high frequency terms, e.g., ?is?, ?it?, ?a?, 
etc., tend to obey our assumption that the real 
polarity of one top term should belong to the 
polarity category where the term appears 
frequently. 
Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Lambda=0 0.25 0.5 0.75 1
Ac
cu
ra
cy
Book DVD Electronic Kitchen
Figure 3: Performance of the stacking classifier using 
WFO with different Lambda ( ? ) values 
 Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
200 500 1000 1500 2000 3000 4000 6000 8000
Ac
cu
ra
cy
Book DVD Electronic Kitchen
 Figure 4: Performance of the stacking classifier over 
different sizes of the polarity shifting training data 
(with Nmax sentences in each category) 
Effect of a classifier over different sizes of the 
polarity shifting training data 
Another factor which might influence the 
overall performance is the size of the polarity 
shifting training data. Figure 4 presents the 
overall performance on different numbers of the 
polarity shifting sentences when using the 
stacking classifier. It shows that 1000 to 4000 
sentences are enough for the performance 
improvement. When the number is too large, the 
noisy training data may harm polarity shifting 
detection. When the number is too small, it is not 
enough for the automatically generated polarity 
shifting training data to capture various polarity 
shifting structures. 
641
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: Book
The traning data sizes
Ac
c
ur
ac
y
 
 
Baseline BaseClassifier 1 BaseClassifier 2 Stacking
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: DVD
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Electronic
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Kitchen
The traning data sizes
Ac
c
ur
ac
y
 
 
Figure 5: Performance of different classifiers over different sizes of the polarity classification training data 
 
Effect of different classifiers over different 
sizes of the polarity classification training data 
Figure 5 shows the classification results of 
different classifiers with varying sizes of the 
polarity classification training data. It shows that 
our approach is able to improve the overall 
performance robustly. We also notice the big 
difference between the performance of the 
baseline classifier and that of the base classifier 
1 when using 30% training data in Book domain 
and 90% training data in DVD domain. Detailed 
exploration of the polarity shifting sentences in 
the training data shows that this difference is 
mainly attributed to the poor performance of the 
polarity shifting detector. Even so, the stacking 
classifier guarantees no worse performance than 
the baseline classifier. 
5 Conclusion and Future Work 
In this paper, we propose a novel approach to 
incorporate polarity shifting information into 
document-level sentiment classification. In our 
approach, we first propose a 
machine-learning-based classifier to detect 
polarity shifting and then apply two classifier 
combination methods to perform polarity 
classification. Particularly, the polarity shifting 
training data is automatically generated through 
a feature selection method. As shown in our 
experimental results, our approach is able to 
consistently improve the overall performance 
across different domains and training data sizes, 
although the automatically generated polarity 
shifting training data is prone to noise. 
Furthermore, we conclude that those feature 
selection methods, which prefer frequency 
information, e.g., MI and BNS, are good choices 
for generating the polarity shifting training data. 
In our future work, we will explore better 
ways in generating less-noisy polarity shifting 
training data. In addition, since our approach is 
language-independent, it is readily applicable to 
sentiment classification tasks in other languages. 
For availability of the automatically generated 
polarity shifting training data, please contact the 
first author (for research purpose only). 
Acknowledgments 
This research work has been partially supported 
by Start-up Grant for Newly Appointed 
Professors, No. 1-BBZM in the Hong Kong 
Polytechnic University and two NSFC grants, 
No. 60873150 and No. 90920004. We also thank 
the three anonymous reviewers for their helpful 
comments. 
642
References 
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Ding X., B. Liu, and P. Yu. 2008. A Holistic 
Lexicon-based Approach to Opinion Mining. In 
Proceedings of the International Conference on 
Web Search and Web Data Mining, WSDM-08. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Forman G. 2003. An Extensive Empirical Study of 
Feature Selection Metrics for Text Classification. 
The Journal of Machine Learning Research, 3(1), 
pp.1289-1305. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Ikeda D., H. Takamura, L. Ratinov, and M. Okumura. 
2008. Learning to Shift the Polarity of Words for 
Sentiment Classification. In Proceedings of 
IJCNLP-08. 
Kennedy, A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Li S., R. Xia, C. Zong, and C. Huang. 2009. A 
Framework of Feature Selection Methods for Text 
Categorization. In Proceedings of 
ACL-IJCNLP-09. 
Li S. and C. Zong. 2008. Multi-domain Sentiment 
Classification. In Proceedings of ACL-08: HLT, 
short paper. 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
Na J., H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004. 
Effectiveness of Simple Linguistic Processing in 
Automatic Sentiment Classification of Product 
Reviews. In Conference of the International 
Society for Knowledge Organization (ISKO-04). 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and Comparisons to Regularized 
Likelihood Methods. In: A. Smola, P. Bartlett, B. 
Schoelkopf and D. Schuurmans (Eds.): Advances 
in Large Margin Classiers. MIT Press, Cambridge, 
61?74. 
Polanyi L. and A. Zaenen. 2006. Contextual Valence 
Shifters. Computing attitude and affect in text: 
Theory and application. Springer Verlag. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In 
Proceedings of EMNLP-06. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial 
Intelligence Review, 18(2), pp. 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wiebe J. 2000. Learning Subjective Adjectives from 
Corpora. In Proceedings of AAAI-2000. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu, X. 1999. A Re-Examination of 
Text Categorization methods. In Proceedings of 
SIGIR-99. 
Yang Y. and J. Pedersen. 1997. A Comparative Study 
on Feature Selection in Text Categorization. In 
Proceedings of ICML-97. 
643
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 414?423,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Employing Personal/Impersonal Views in Supervised and 
Semi-supervised Sentiment Classification 
 
Shoushan Li??  Chu-Ren Huang?  Guodong Zhou?  Sophia Yat Mei Lee? 
 
?Department of Chinese and Bilingual 
Studies 
The Hong Kong Polytechnic University 
{shoushan.li,churenhuang, 
sophiaym}@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and Technology 
Soochow University, China 
gdzhou@suda.edu.cn 
 
 
Abstract 
In this paper, we adopt two views, personal 
and impersonal views, and systematically 
employ them in both supervised and 
semi-supervised sentiment classification. Here, 
personal views consist of those sentences 
which directly express speaker?s feeling and 
preference towards a target object while 
impersonal views focus on statements towards 
a target object for evaluation. To obtain them, 
an unsupervised mining approach is proposed. 
On this basis, an ensemble method and a 
co-training algorithm are explored to employ 
the two views in supervised and 
semi-supervised sentiment classification 
respectively. Experimental results across eight 
domains demonstrate the effectiveness of our 
proposed approach. 
1 Introduction 
As a special task of text classification, sentiment 
classification aims to classify a text according to 
the expressed sentimental polarities of opinions 
such as ?thumb up? or ?thumb down? on the 
movies (Pang et al, 2002). This task has recently 
received considerable interests in the Natural 
Language Processing (NLP) community due to its 
wide applications. 
In general, the objective of sentiment 
classification can be represented as a kind of 
binary relation R, defined as an ordered triple (X, 
Y, G), where X is an object set including different 
kinds of people (e.g. writers, reviewers, or users), 
Y is another object set including the target 
objects (e.g. products, events, or even some 
people), and G is a subset of the Cartesian 
product X Y? . The concerned relation in 
sentiment classification is X ?s evaluation on Y, 
such as ?thumb up?, ?thumb down?, ?favorable?, 
and ?unfavorable?. Such relation is usually 
expressed in text by stating the information 
involving either a person (one element in X ) or a 
target object itself (one element in Y ). The first 
type of statement called personal view, e.g. ?I am 
so happy with this book?, contains X ?s 
?subjective? feeling and preference towards a 
target object, which directly expresses 
sentimental evaluation. This kind of information 
is normally domain-independent and serves as 
highly relevant clues to sentiment classification. 
The latter type of statement called impersonal 
view, e.g. ?it is too small?, contains Y ?s 
?objective? (i.e. or at least criteria-based) 
evaluation of the target object. This kind of 
information tends to contain much 
domain-specific classification knowledge. 
Although such information is sometimes not as 
explicit as personal views in classifying the 
sentiment of a text, speaker?s sentiment is 
usually implied by the evaluation result.  
It is well-known that sentiment classification 
is very domain-specific (Blitzer et al, 2007), so 
it is critical to eliminate its dependence on a 
large-scale labeled data for its wide applications. 
Since the unlabeled data is ample and easy to 
collect, a successful semi-supervised sentiment 
classification system would significantly 
minimize the involvement of labor and time. 
Therefore, given the two different views 
mentioned above, one promising application is to 
adopt them in co-training algorithms, which has 
been proven to be an effective semi-supervised 
learning strategy of incorporating unlabeled data 
to further improve the classification performance 
(Zhu, 2005). In addition, we would show that 
personal/impersonal views are linguistically 
marked and mining them in text can be easily 
performed without special annotation.  
414
In this paper, we systematically employ 
personal/impersonal views in supervised and 
semi-supervised sentiment classification. First, 
an unsupervised bootstrapping method is adopted 
to automatically separate one document into 
personal and impersonal views. Then, both views 
are employed in supervised sentiment 
classification via an ensemble of individual 
classifiers generated by each view. Finally, a 
co-training algorithm is proposed to incorporate 
unlabeled data for semi-supervised sentiment 
classification. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
unsupervised approach for mining personal and 
impersonal views. Section 4 and Section 5 
propose our supervised and semi-supervised 
methods on sentiment classification respectively. 
Experimental results are presented and analyzed 
in Section 6. Section 7 discusses on the 
differences between personal/impersonal and 
subjective/objective. Finally, Section 8 draws our 
conclusions and outlines the future work. 
2 Related Work 
Recently, a variety of studies have been reported 
on sentiment classification at different levels: 
word level (Esuli and Sebastiani, 2005), phrase 
level (Wilson et al, 2009), sentence level (Kim 
and Hovy, 2004; Liu et al, 2005), and document 
level (Turney, 2002; Pang et al, 2002). This 
paper focuses on the document-level sentiment 
classification. Generally, document-level 
sentiment classification methods can be 
categorized into three types: unsupervised, 
supervised, and semi-supervised. 
Unsupervised methods involve deriving a 
sentiment classifier without any labeled 
documents. Most of previous work use a set of 
labeled sentiment words called seed words to 
perform unsupervised classification. Turney 
(2002) determines the sentiment orientation of a 
document by calculating point-wise mutual 
information between the words in the document 
and the seed words of ?excellent? and ?poor?. 
Kennedy and Inkpen (2006) use a term-counting 
method with a set of seed words to determine the 
sentiment. Zagibalov and Carroll (2008) first 
propose a seed word selection approach and then 
apply the same term-counting method for Chinese 
sentiment classifications. These unsupervised 
approaches are believed to be 
domain-independent for sentiment classification. 
Supervised methods consider sentiment 
classification as a standard classification problem 
in which labeled data in a domain are used to 
train a domain-specific classifier. Pang et al 
(2002) are the first to apply supervised machine 
learning methods to sentiment classification. 
Subsequently, many other studies make efforts to 
improve the performance of machine 
learning-based classifiers by various means, such 
as using subjectivity summarization (Pang and 
Lee, 2004), seeking new superior textual features 
(Riloff et al, 2006), and employing document 
subcomponent information (McDonald et al, 
2007). As far as the challenge of 
domain-dependency is concerned, Blitzer et al 
(2007) present a domain adaptation approach for 
sentiment classification. 
Semi-supervised methods combine unlabeled 
data with labeled training data (often 
small-scaled) to improve the models. Compared 
to the supervised and unsupervised methods, 
semi-supervised methods for sentiment 
classification are relatively new and have much 
less related studies. Dasgupta and Ng (2009) 
integrate various methods in semi-supervised 
sentiment classification including spectral 
clustering, active learning, transductive learning, 
and ensemble learning. They achieve a very 
impressive improvement across five domains. 
Wan (2009) applies a co-training method to 
semi-supervised learning with labeled English 
corpus and unlabeled Chinese corpus for Chinese 
sentiment classification. 
3 Unsupervised Mining of Personal and 
Impersonal Views 
As mentioned in Section 1, the objective of 
sentiment classification is to classify a specific 
binary relation: X ?s evaluation on Y, where X is 
an object set including different kinds of persons 
and Y is another object set including the target 
objects to be evaluated. First of all, we focus on 
an analysis on sentences in product reviews 
regarding the two views: personal and 
impersonal views.  
The personal view consists of personal 
sentences (i.e. X ?s sentences) exemplified 
below: 
I. Personal preference: 
E1: I love this breadmaker! 
E2: I disliked it from the beginning. 
II. Personal emotion description: 
E3: Very disappointed! 
E4: I am happy with the product. 
III. Personal actions: 
415
E5: Do not waste your money. 
E6: I have recommended this machine to all my 
friends. 
The impersonal view consists of impersonal 
sentences (i.e.Y ?s sentences) exemplified below: 
I. Impersonal feature description: 
E7: They are too thin to start with. 
E8: This product is extremely quiet. 
II. Impersonal evaluation: 
E9: It's great. 
E10: The product is a waste of time and money. 
III. Impersonal actions: 
E11: This product not even worth a penny. 
E12: It broke down again and again. 
We find that the subject of a sentence presents 
important cues for personal/impersonal views, 
even though a formal and computable definition 
of this contrast cannot be found. Here, subject 
refers to one of the two main constituents in the 
traditional English grammar (the other 
constituent being the predicate) (Crystal, 2003)1. 
For example, the subjects in the above examples 
of E1, E7 and E11 are ?I?, ?they?, and ?this 
product? respectively. For automatic mining the 
two views, personal/impersonal sentences can be 
defined according to their subjects: 
Personal sentence: the sentence whose 
subject is (or represents) a person. 
Impersonal sentence: the sentence whose 
subject is not (does not represent) a person. 
In this study, we mainly focus on product 
review classification where the target object in 
the set Y  is not a person. The definitions need 
to be adjusted when the evaluation target itself is 
a person, e.g. the political sentiment 
classification by Durant and Smith (2007). 
Our unsupervised mining approach for mining 
personal and impersonal sentences consists of 
two main steps. First, we extract an initial set of 
personal and impersonal sentences with some 
heuristic rules: If the first word of one sentence 
is (or implies) a personal pronoun including ?I?, 
?we?, and ?do?, then the sentence is extracted as a 
personal sentence; If the first word of one 
sentence is an impersonal pronoun including 'it', 
'they', 'this', and 'these', then the sentence is 
extracted as an impersonal sentence. Second, we 
apply the classifier which is trained with the 
initial set of personal and impersonal sentences 
to classify the remaining sentences. This step 
aims to classify the sentences without pronouns 
                                                      
1
 The subject has the grammatical function in a sentence of 
relating its constituent (a noun phrase) by means of the verb to any 
other elements present in the sentence, i.e. objects, complements, 
and adverbials. 
(e.g. E3). Figure 1 shows the unsupervised 
mining algorithm. 
Input: 
The training data D
 
 
Output: 
    All personal and impersonal sentences, i.e. 
sentence sets personalS  and impersonalS . 
Procedure: 
(1). Segment all documents in D to sentences 
S using punctuations (such as periods and 
interrogation marks) 
(2). Apply the heuristic rules to classify the 
sentences S  with proper pronouns into, 1pS  
and  1iS  
(3). Train a binary classifier p if ?  with  1pS  and  
1iS  
(4). Use  p if ?  to classify the remaining sentences 
into  2pS  and  2iS  
(5). 1 2personal p pS S S= ? ,  1 2impersonal i iS S S= ?  
 
Figure 1: The algorithm for unsupervised mining 
personal and impersonal sentences from a training 
data 
4 Employing Personal/Impersonal 
Views in Supervised Sentiment 
Classification 
After unsupervised mining of personal and 
impersonal sentences, the training data is divided 
into two views: the personal view, which 
contains personal sentences, and the impersonal 
view, which contains impersonal sentences. 
Obviously, these two views can be used to train 
two different classifiers, 1f  and 2f , for 
sentiment classification respectively.  
Since our mining approach is unsupervised, 
there inevitably exist some noises. In addition, 
the sentences of different views may share the 
same information for sentiment classification. 
For example, consider the following two 
sentences: ?It is a waste of money.? and ?Do not 
waste your money.? Apparently, the first one 
belongs to the impersonal view while the second 
one belongs to personal view, according to our 
heuristic rules. However, these two sentences 
share the same word, ?waste?, which conveys 
strong negative sentiment information. This 
suggests that training a single-view classifier 3f  
with all sentences should help. Therefore, three 
base classifiers, 1f , 2f , and 3f , are eventually 
derived from the personal view, the impersonal 
416
view and the single view, respectively. Each base 
classifier provides not only the class label 
outputs but also some kinds of confidence 
measurements, e.g. posterior probabilities of the 
testing sample belonging to each class.  
Formally, each base classifier  ( 1,2,3)lf l =  
assigns a test sample (denoted as lx ) a posterior 
probability vector ( )lP x

:  
1 2( ) ( | ), ( | ) tl l lP x p c x p c x= < >

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging to 1c . 
In the ensemble learning literature, various 
methods have been presented for combining base 
classifiers. The combining methods are 
categorized into two groups (Duin, 2002): fixed 
rules such as voting rule, product rule, and sum 
rule (Kittler et al, 1998), and trained rules such 
as weighted sum rule (Fumera and Roli, 2005) 
and meta-learning approaches (Vilalta and Drissi, 
2002). In this study, we choose a fixed rule and a 
trained rule to combine the three base classifiers 
1f , 2f , and 3f .  
The chosen fixed rule is product rule which 
combine base classifiers by multiplying the 
posterior possibilities and using the multiplied 
possibility for decision, i.e. 
3
1
                 
  arg max ( | )
j
i l
i l
assign y c
where j p c x
=
?
= ?  
The chosen trained rule is stacking (Vilalta and 
Drissi, 2002; D?eroski and ?enko, 2004) where a 
meta-classifier is trained with the output of the 
base classifiers as the input. Formally, let 'x  
denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( | ' ), ( | ' )l l l lP x p c x p c x=< >

 
Then, a meta-classifier is trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ' ), ( ' ), ( ' )meta l l lx P x P x P x= = ==< >
  
 
In our experiments, we perform stacking with 
4-fold cross validation to generate meta-training 
data where each fold is used as the development 
data and the other three folds are used to train the 
base classifiers in the training phase. 
5 Employing Personal/Impersonal 
Views in Semi-Supervised Sentiment 
Classification 
Semi-supervised learning is a strategy which 
combines unlabeled data with labeled training 
data to improve the models. Given the two-view 
classifiers 1f  and 2f  along with the single-view 
classifier 3f , we perform a co-training algorithm 
for semi-supervised sentiment classification. The 
co-training algorithm is a specific 
semi-supervised learning approach which starts 
with a set of labeled data and increases the 
amount of labeled data using the unlabeled data 
by bootstrapping (Blum and Mitchell, 1998). 
Figure 2 shows the co-training algorithm in our 
semi-supervised sentiment classification. 
Input: 
The labeled data L
 
containing personal 
sentence set L personalS ?  and impersonal sentence set 
L impersonalS ?  
The unlabeled data U  containing personal 
sentence set
 
U personalS ?  and impersonal sentence set 
U impersonalS ?  
Output: 
    New labeled data L  
Procedure: 
Loop for N iterations untilU ?=  
(1). Learn the first classifier 1f  with L personalS ?  
(2). Use 1f  to label samples from U with 
U personalS ?  
(3). Choose 1n  positive and 1n negative most 
confidently predicted samples 1A  
(4). Learn the second classifier 2f  with L impersonalS ?  
(5). Use 2f to label samples from U with 
U impersonalS ?   
(6). Choose 2n  positive and 2n negative most 
confidently predicted samples 2A   
(7). Learn the third classifier 3f  with L  
(8). Use 3f  to label samples from U  
(9). Choose 3n  positive and 3n  negative most 
confidently predicted samples 3A  
(10). Add samples 1 2 3A A A? ?  with the 
corresponding labels into L  
(11). Update L personalS ?  and L impersonalS ?  
 
Figure 2: Our co-training algorithm for 
semi-supervised sentiment classification 
417
After obtaining the new labeled data, we can 
either adopt one classifier (i.e. 3f ) or a 
combined classifier (i.e. 1 2 3f f f+ + ) in further 
training and testing. In our experimentation, we 
explore both of them with the former referred to 
as co-training and single classifier and the latter 
referred to as co-training and combined 
classifier. 
6 Experimental Studies 
We have systematically explored our method on 
product reviews from eight domains: book, DVD, 
electronic appliances, kitchen appliances, health, 
network, pet and software. 
6.1 Experimental Setting 
The product reviews on the first four domains 
(book, DVD, electronic, and kitchen appliances) 
come from the multi-domain sentiment 
classification corpus, collected from 
http://www.amazon.com/ by Blitzer et al (2007)2. 
Besides, we also collect the product views from 
http://www.amazon.com/ on other four domains 
(health, network, pet and software)3. Each of the 
eight domains contains 1000 positive and 1000 
negative reviews. Figure 3 gives the distribution 
of personal and impersonal sentences in the 
training data (75% labeled data of all data). It 
shows that there are more impersonal sentences 
than personal ones in each domain, in particular 
in the DVD domain, where the number of 
impersonal sentences is at least twice as many as 
that of personal sentences. This unusual 
phenomenon is mainly attributed to the fact that 
many objective descriptions, e.g. the movie plot 
introductions, are expressed in the DVD domain 
which makes the extracted personal and 
impersonal sentences rather unbalanced. 
We apply both support vector machine (SVM) 
and Maximum Entropy (ME) algorithms with the 
help of the SVM-light4 and Mallet5 tools. All 
parameters are set to their default values. We 
find that ME performs slightly better than SVM 
on the average. Furthermore, ME offers posterior 
probability information which is required for 
                                                      
2
 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 Note that the second version of multi-domain sentiment 
classification corpus does contain data from many other domains. 
However, we find that the reviews in the other domains contain 
many duplicated samples. Therefore, we re-collect the reviews from 
http://www.amazon.com/ and filter those duplicated ones. The new 
collection is here:  
http://llt.cbs.polyu.edu.hk/~lss/ACL2010_Data_SSLi.zip 
4
 http://svmlight.joachims.org/  
5
 http://mallet.cs.umass.edu/  
combination methods. Thus we apply the ME 
classification algorithm for further combination 
and co-training. In particular, we only employ 
Boolean features, representing the presence or 
absence of a word in a document. Finally, we 
perform t-test to evaluate the significance of the 
performance difference between two systems 
with different methods (Yang and Liu, 1999). 
Sentence Number in the Training Data
16134
8477 8337 8843
13097
29290
1485214414
12691 11941
1381814265 16441
1475315573
27714
0
10000
20000
30000
40000
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Nu
mb
er
Number of personal sentences
Number of impersonal sentences
Figure 3: Distribution of personal and impersonal 
sentences in the training data of each domain 
6.2 Experimental Results on Supervised 
Sentiment Classification 
4-fold cross validation is performed for 
supervised sentiment classification. For 
comparison, we generate two random views by 
randomly splitting the whole feature space into 
two parts. Each part is seen as a view and used to 
train a classifier. The combination (two random 
view classifiers along with the single-view 
classifier 3f ) results are shown in the last column 
of Table 1. The comparison between random two 
views and our proposed two views will clarify 
whether the performance gain comes truly from 
our proposed two-view mining, or simply from 
using the classifier combination strategy. 
Table 1 shows the performances of different 
classifiers, where the single-view classifier 3f  
which uses all sentences for training and testing, 
is considered as our baseline. Note that the 
baseline performances of the first four domains 
are worse than the ones reported in Blitzer et al 
(2007). But their experiment is performed with 
only one split on the data with 80% as the 
training data and 20% as the testing data, which 
means the size of their training data is larger than 
ours. Also, we find that our performances are 
similar to the ones (described as fully supervised 
results) reported in Dasgupta and Ng (2009) 
where the same data in the four domains are used 
and 10-fold cross validation is performed.  
418
Domain Personal 
View 
Classifier 
1f  
Impersonal 
View 
Classifier 
2f  
Single View 
Classifier 
(baseline) 
3f
 
Combination  
(Stacking) 
1 2 3f f f+ +  
Combination 
(Product rule) 
1 2 3f f f+ +  
Combination 
with two 
random views 
(Product rule) 
Book 0.7004 0.7474 0.7654 0.7919 0.7949 0.7546 
DVD 0.6931 0.7663 0.7884 0.8079 0.8165 0.8054 
Electronic 0.7414 0.7844 0.8074 0.8304 0.8364 0.8210 
Kitchen 0.7430 0.8030 0.8290 0.8555 0.8565 0.8152 
Health 0.7000 0.7370 0.7559 0.7780 0.7815 0.7548 
Network 0.7655 0.7710 0.8265 0.8360 0.8435 0.8312 
Pet 0.6940 0.7145 0.7390 0.7565 0.7665 0.7423 
Software 0.7035 0.7205 0.7470 0.7730 0.7715 0.7615 
AVERAGE 0.7176 0.7555 0.7823 0.8037 0.8084 0.7858 
 
Table 1: Performance of supervised sentiment classification 
 
From Table 1, we can see that impersonal view 
classifier 1f  consistently performs better than 
personal view classifier 2f . Similar to the 
sentence distributions, the difference in the 
classification performances between these two 
views in the DVD domain is the largest (0.6931 
vs. 0.7663). 
Both the combination methods (stacking and 
product rule) significantly outperform the 
baseline in each domain (p-value<0.01) with a 
decent average performance improvement of 
2.61%. Although the performance difference 
between the product rule and stacking is not 
significant, the product rule is obviously a better 
choice as it involves much easier implementation. 
Therefore, in the semi-supervised learning 
process, we only use the product rule to combine 
the individual classifiers. Finally, it shows that 
random generation of two views with the 
combination method of the product rule only 
slightly outperforms the baseline on the average 
(0.7858 vs. 0.7823) but performs much worse 
than our unsupervised mining of personal and 
impersonal views.  
6.3 Experimental Results on 
Semi-supervised Sentiment 
Classification 
We systematically evaluate and compare our 
two-view learning method with various 
semi-supervised ones as follows: 
Self-training, which uses the unlabeled data 
in a bootstrapping way like co-training yet limits 
the number of classifiers and the number of 
views to one. Only the baseline classifier 3f  is 
used to select most confident unlabeled samples 
in each iteration. 
Transductive SVM, which seeks the largest 
separation between labeled and unlabeled data 
through regularization (Joachims, 1999). We 
implement it with the help of the SVM-light tool. 
Co-training with random two-view 
generation (briefly called co-training with 
random views), where two views are generated 
by randomly splitting the whole feature space 
into two parts.  
In semi-supervised sentiment classification, 
the data are randomly partitioned into labeled 
training data, unlabeled data, and testing data 
with the proportion of 10%, 70% and 20% 
respectively. Figure 4 reports the classification 
accuracies in all iterations, where baseline 
indicates the supervised classifier 3f  trained on 
the 10% data; both co-training and single 
classifier and co-training and combined 
classifier refer to co-training using our proposed 
personal and impersonal views. But the former 
merely applies the baseline classifier 3f  trained 
the new labeled data to test on the testing data 
while the latter applies the combined classifier 
1 2 3f f f+ + . In each iteration, two top-confident 
samples in each category are chosen, i.e. 
1 2 3 2n n n= = = . For clarity, results of other 
methods (e.g. self-training, transductive SVM) 
are not shown in Figure 4 but will be reported in 
Figure 5 later.  
Figure 4 shows that co-training and 
combined classifier always outperforms 
co-training and single classifier. This again 
justifies the effectiveness of our two-view 
learning on supervised sentiment classification.
419
25 50 75 100 125
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
Domain: Book
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.58
0.6
0.62
0.64
0.66
0.68
0.7
Domain: DVD
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.7
0.72
0.74
0.76
0.78
0.8
Domain: Electronic
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.72
0.74
0.76
0.78
0.8
0.82
Domain: Kitchen
Iteration Number
Ac
cu
ra
cy
 
 
 
25 50 75 100 125
0.54
0.56
0.58
0.6
0.62
0.64
0.66
Domain: Health
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Domain: Network
Iteration Number
Ac
cu
ra
cy
 
 
Baseline
Co-traning and single classifier
Co-traning and combined classifier
25 50 75 100 125
0.58
0.6
0.62
0.64
0.66
0.68
Domain: Pet
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.62
0.64
0.66
0.68
0.7
0.72
Domain: Software
Iteration Number
Ac
cu
ra
cy
 
 
 
 
Figure 4: Classification performance vs. iteration numbers (using 10% labeled data as training data) 
 
One open question is whether the unlabeled 
data improve the performance. Let us set aside 
the influence of the combination strategy and 
focus on the effectiveness of semi-supervised 
learning by comparing the baseline and 
co-training and single classifier. Figure 4 
shows different results on different domains. 
Semi-supervised learning fails on the DVD 
domain while on the three domains of book, 
electronic, and software, semi-supervised 
learning benefits slightly (p-value>0.05). In 
contrast, semi-supervised learning benefits much 
on the other four domains (health, kitchen, 
network, and pet) from using unlabeled data and 
the performance improvements are statistically 
significant (p-value<0.01). Overall speaking, we 
think that the unlabeled data are very helpful as 
they lead to about 4% accuracy improvement on 
the average except for the DVD domain. Along 
with the supervised combination strategy, our 
approach can significantly improve the 
performance more than 7% on the average 
compared to the baseline. 
Figure 5 shows the classification results of 
different methods with different sizes of the 
labeled data: 5%, 10%, and 15% of all data, 
where the testing data are kept the same (20% of 
all data). Specifically, the results of other 
methods including self-training, transductive 
SVM, and random views are presented when 
10% labeled data are used in training. It shows 
that self-training performs much worse than our 
approach and fails to improve the performance of 
five of the eight domains. Transductive SVM 
performs even worse and can only improve the 
performance of the ?software? domain. Although 
co-training with random views outperforms the 
baseline on four of the eight domains, it performs 
worse than co-training and single classifier. 
This suggests that the impressive improvements 
are mainly due to our unsupervised two-view 
mining rather than the combination strategy.
420
Using 10% labeled data as training data
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Book DVD Electronic Kitchen Health Network Pet Software
Ac
cu
rac
y
Baseline Transductive SVM Self-training
Co-training with random views Co-training and single classifier Co-training and combined classifier
 
Using 5% labeled data as training data
0.69
0.747
0.584
0.525
0.67 0.6530.626
0.55
0.564
0.683
0.495
0.615
0.8675
0.7855
0.7
0.601
0.45
0.55
0.65
0.75
0.85
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Ac
cu
ra
cy
Using 15% labeled data as training data
0.763
0.6925
0.765
0.5925
0.679
0.564
0.677
0.7375
0.6625
0.735
0.655
0.615
0.8625
0.8325
0.782
0.716
0.45
0.55
0.65
0.75
0.85
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Ac
cu
ra
cy
 
Figure 5: Performance of semi-supervised sentiment classification when 5%, 10%, and 15% labeled data are used 
 
Figure 5 also shows that our approach is rather 
robust and achieves excellent performances in 
different training data sizes, although our 
approach fails on two domains, i.e. book and 
DVD, when only 5% of the labeled data are used. 
This failure may be due to that some of the 
samples in these two domains are too ambiguous 
and hard to classify. Manual checking shows that 
quite a lot of samples on these two domains are 
even too difficult for professionals to give a 
high-confident label. Another possible reason is 
that there exist too many objective descriptions 
in these two domains, thus introducing too much 
noisy information for semi-supervised learning. 
The effectiveness of different sizes of chosen 
samples in each iteration is also evaluated like 
1 2 3 6n n n= = = and 1 2 33, 6n n n= = = (This 
assignment is considered because the personal 
view classifier performs worse than the other two 
classifiers). Our experimental results are still 
unsuccessful in the DVD domain and do not 
show much difference on other domains. We also 
test the co-training approach without the 
single-view classifier 3f . Experimental results 
show that the inclusion of the single-view 
classifier 3f  slightly helps the co-training 
approach. The detailed discussion of the results 
is omitted due to space limit. 
6.4 Why our approach is effective? 
One main reason for the effectiveness of our 
approach on supervised learning is the way how 
personal and impersonal views are dealt with. As 
personal and impersonal views have different 
ways of expressing opinions, splitting them into 
two separations can filter some classification 
noises. For example, in the sentence of ?I have 
seen amazing dancing, and good dancing. This 
was TERRIBLE dancing!?. The first sentence is 
classified as a personal sentence and the second 
one is an impersonal sentence. Although the 
words ?amazing? and ?good? convey strong 
positive sentiment information, the whole text is 
negative. If we get the bag-of-words from the 
whole text, the classification result will be wrong. 
Rather, splitting the text into two parts based on 
different views allows correct classification as 
the personal view rarely contains impersonal 
words such as ?amazing? and ?good?. The 
classification result will thus be influenced by 
the impersonal view.  
In addition, a document may contain both 
personal and impersonal sentences, and each of 
them, to a certain extent, , provides classification 
evidence. In fact, we randomly select 50 
documents in the domain of kitchen appliances 
and find that 80% of the documents take both 
personal and impersonal sentences in which both 
of them express explicit opinions. That is to say, 
the two views provide different, complementary 
information for classification. This qualifies the 
success requirement of co-training algorithm to 
some extend. This might be the reason for the 
effectiveness of our approach on semi-supervised 
learning. 
421
7 Discussion on Personal/Impersonal vs. 
Subjective/Objective 
As mentioned in Section 1, personal view 
contains X ?s ?subjective? feeling, and 
impersonal view containsY ?s ?objective? (i.e. or 
at least criteria-based) evaluation of the target 
object. However, our technically-defined 
concepts of personal/impersonal are definitely 
different from subjective/objective: Personal 
view can certainly contain many objective 
expressions, e.g. ?I bought this electric kettle? and 
impersonal view can contain many subjective 
expressions, e.g. ?It is disappointing?.  
Our technically-defined personal/impersonal 
views are two different ways to describe 
opinions. Personal sentences are often used to 
express opinions in a direct way and their target 
object should be one of X. Impersonal ones are 
often used to express opinions in an indirect way 
and their target object should be one of Y. The 
ideal definition of personal (or impersonal) view 
given in Section 1 is believed to be a subset of 
our technical definition of personal (or 
impersonal) view. Thus impersonal view may 
contain both Y ?s objective evaluation (more 
likely to be domain independent) and subjective 
Y?s description. 
In addition, simply splitting text into 
subjective/objective views is not particularly 
helpful. Since a piece of objective text provides 
rather limited implicit classification information, 
the classification abilities of the two views are 
very unbalanced. This makes the co-training 
process unfeasible. Therefore, we believe that 
our technically-defined personal/impersonal 
views are more suitable for two-view learning 
compared to subjective/objective views. 
8 Conclusion and Future Work 
In this paper, we propose a robust and effective 
two-view model for sentiment classification 
based on personal/impersonal views. Here, the 
personal view consists of subjective sentences 
whose subject is a person, whereas the 
impersonal view consists of objective sentences 
whose subject is not a person. Such views are 
lexically cued and can be obtained without 
pre-labeled data and thus we explore an 
unsupervised learning approach to mine them.  
Combination methods and a co-training 
algorithm are proposed to deal with supervised 
and semi-supervised sentiment classification 
respectively. Evaluation on product reviews from 
eight domains shows that our approach 
significantly improves the performance across all 
eight domains on supervised sentiment 
classification and greatly outperforms the 
baseline with more than 7% accuracy 
improvement on the average across seven of 
eight domains (except the DVD domain) on 
semi-supervised sentiment classification. 
In the future work, we will integrate the 
subjectivity summarization strategy (Pang and 
Lee, 2004) to help discard noisy objective 
sentences. Moreover, we need to consider the 
cases when both X and Y appear in a sentence. 
For example, the sentence ?I think they're poor? 
should be an impersonal view but wrongly 
classified as a personal one according to our 
technical rules. We believe that these will help 
improve our approach and hopefully are 
applicable to the DVD domain. Another 
interesting and practical idea is to integrate 
active learning (Settles, 2009), another popular 
but principally different kind of semi-supervised 
learning approach, with our two-view learning 
approach to build high-performance systems 
with the least labeled data. 
Acknowledgments 
The research work described in this paper has 
been partially supported by Start-up Grant for 
Newly Appointed Professors, No. 1-BBZM in 
the Hong Kong Polytechnic University and two 
NSFC grants, No. 60873150 and No. 90920004. 
We also thank the three anonymous reviewers 
for their invaluable comments. 
References  
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Blum A. and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In 
Proceedings of COLT-98. 
Crystal D. 2003. The Cambridge Encyclopedia of the 
English Language. Cambridge University Press. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Duin R. 2002. The Combining Classifier: To Train Or 
Not To Train? In Proceedings of 16th International 
Conference on Pattern Recognition (ICPR-02). 
Durant K. and M. Smith. 2007. Predicting the 
Political Sentiment of Web Log Posts using 
422
Supervised Machine Learning Techniques Coupled 
with Feature Selection. In Processing of Advances 
in Web Mining and Web Usage Analysis. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Esuli A. and F. Sebastiani. 2005. Determining the 
Semantic Orientation of Terms through Gloss 
Classification. In Proceedings of CIKM-05. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Joachims, T. 1999. Transductive Inference for Text 
Classification using Support Vector Machines. 
ICML1999. 
Kennedy A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
McDonald R., K. Hannan, T. Neylon, M. Wells, and J. 
Reynar. 2007. Structured Models for 
Fine-to-coarse Sentiment Analysis. In Proceedings 
of ACL-07. 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In Proceedings 
of EMNLP-06. 
Settles B. 2009. Active Learning Literature Survey. 
Technical Report 1648, Department of Computer 
Sciences, University of Wisconsin at Madison, 
Wisconsin. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial Intelligence 
Review, 18(2): 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu. 1999. A Re-Examination of Text 
Categorization methods. In Proceedings of 
SIGIR-99. 
Zagibalov T. and J. Carroll. 2008. Automatic Seed 
Word Selection for Unsupervised Sentiment 
Classification of Chinese Test. In Proceedings of 
COLING-08.  
Zhu X. 2005. Semi-supervised Learning Literature 
Survey. Technical Report Computer Sciences 1530, 
University of Wisconsin ? Madison. 
 
423
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 511?515,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Modeling of News Reader?s and Comment Writer?s Emotions?
 
 
Huanhuan Liu?  Shoushan Li??*  Guodong Zhou?  Chu-Ren Huang?  Peifeng Li? 
 
?Natural Language Processing Lab 
Soochow University, China 
{huanhuanliu.suda,shoushan.li, 
churenhuang}@gmail.com 
 
?Department of CBS 
the Hong Kong Polytechnic University 
{gdzhou,pfli}@suda.edu.cn 
 
 
Abstract 
Emotion classification can be generally done 
from both the writer?s and reader?s 
perspectives. In this study, we find that two 
foundational tasks in emotion classification, 
i.e., reader?s emotion classification on the 
news and writer?s emotion classification on 
the comments, are strongly related to each 
other in terms of coarse-grained emotion 
categories, i.e., negative and positive. On the 
basis, we propose a respective way to jointly 
model these two tasks. In particular, a co-
training algorithm is proposed to improve 
semi-supervised learning of the two tasks. 
Experimental evaluation shows the 
effectiveness of our joint modeling 
approach.* 
1 Introduction 
Emotion classification aims to predict the emo-
tion categories (e.g., happy, angry, or sad) of a 
given text (Quan and Ren, 2009; Das and Ban-
dyopadhyay, 2009). With the rapid growth of 
computer mediated communication applications, 
such as social websites and miro-blogs, the re-
search on emotion classification has been attract-
ing more and more attentions recently from the 
natural language processing (NLP) community 
(Chen et al, 2010; Purver and Battersby, 2012). 
In general, a single text may possess two kinds 
of emotions, writer?s emotion and reader?s emo-
tion, where the former concerns the emotion ex-
pressed by the writer when writing the text and 
the latter concerns the emotion expressed by a 
reader after reading the text. For example, con-
sider two short texts drawn from a news and cor-
responding comments, as shown in Figure 1. On 
                                                 
* *  Corresponding author 
one hand, for the news text, while its writer just 
objectively reports the news and thus does not 
express his emotion in the text, a reader could 
yield sad or worried emotion. On the other hand, 
for the comment text, its writer clearly expresses 
his sad emotion while the emotion of a reader 
after reading the comments is not clear (Some 
may feel sorry but others might feel careless). 
 
News:  
Today's Japan earthquake could be 
     2011 quake aftershock. ?? 
News Writer?s emotion: None 
News Reader?s emotion: sad, worried 
Comments: 
(1) I hope everything is ok, so sad. I still can 
not forget last year. 
(2) My father-in-law got to experience this 
quake... what a suffering. 
Comment Writer?s emotion: sad 
Comment Reader?s emotion: Unknown 
Figure 1: An example of writer?s and reader?s 
emotions on a news and its comments 
 
Accordingly, emotion classification can be 
grouped into two categories: reader?s emotion 
and writer?s emotion classifications. Although 
both emotion classification tasks have been 
widely studied in recent years, they are always 
considered independently and treated separately.  
However, news and their corresponding com-
ments often appear simultaneously. For example, 
in many news websites, it is popular to see a 
news followed by many comments. In this case, 
because the writers of the comments are a part of 
the readers of the news, the writer?s emotions on 
the comments are exactly certain reflection of the 
reader?s emotions on the news. That is, the 
comment writer?s emotions and the news read-
er?s emotions are strongly related. For example, 
511
in Figure 1, the comment writer?s emotion ?sad? 
is among the news reader?s emotions. 
Above observation motivates joint modeling 
of news reader?s and comment writer?s emotions. 
In this study, we systematically investigate the 
relationship between the news reader?s emotions 
and the comment writer?s emotions. Specifically, 
we manually analyze their agreement in a corpus 
collected from a news website. It is interesting to 
find that such agreement only applies to coarse-
grained emotion categories (i.e., positive and 
negative) with a high probability and does not 
apply to fine-grained emotion categories (e.g., 
happy, angry, and sad). This motivates our joint 
modeling in terms of the coarse-grained emotion 
categories. Specifically, we consider the news 
text and the comment text as two different views 
of expressing either the news reader?s or com-
ment writer?s emotions. Given the two views, a 
co-training algorithm is proposed to perform 
semi-supervised emotion classification so that 
the information in the unlabeled data can be ex-
ploited to improve the classification performance. 
2 Related Work  
2.1 Comment Writer?s Emotion Classifica-
tion 
Comment writer?s emotion classification has 
been a hot research topic in NLP during the last 
decade (Pang et al, 2002; Turney, 2002; Alm et 
al., 2005; Wilson et al, 2009) and previous stud-
ies can be mainly grouped into two categories: 
coarse-grained and fine-grained emotion classifi-
cation. 
Coarse-grained emotion classification, also 
called sentiment classification, concerns only 
two emotion categories, such as like or dislike 
and positive or negative (Pang and Lee, 2008; 
Liu, 2012). This kind of emotion classification 
has attracted much attention since the pioneer 
work by Pang et al (2002) in the NLP communi-
ty due to its wide applications (Cui et al, 2006; 
Riloff et al, 2006; Dasgupta and Ng, 2009; Li et 
al., 2010; Li et al, 2011). 
In comparison, fine-grained emotion classifi-
cation aims to classify a text into multiple emo-
tion categories, such as happy, angry, and sad. 
One main group of related studies on this task is 
about emotion resource construction, such as 
emotion lexicon building (Xu et al, 2010; 
Volkova et al, 2012) and sentence-level or doc-
ument-level corpus construction (Quan and Ren, 
2009; Das and Bandyopadhyay, 2009). Besides, 
all the related studies focus on supervised learn-
ing (Alm et al, 2005; Aman and Szpakowicz, 
2008; Chen et al, 2010; Purver and Battersby, 
2012; Moshfeghi et al, 2011), and so far, we 
have not seen any studies on semi-supervised 
learning on fine-grained emotion classification.  
2.2 News Reader?s Emotion Classification 
While comment writer?s emotion classification 
has been extensively studied, there are only a 
few studies on news reader?s emotion classifica-
tion from the NLP and related communities.  
Lin et al (2007) first describe the task of read-
er?s emotion classification on the news articles 
and then employ some standard machine learning 
approaches to train a classifier for determining 
the reader?s emotion towards a news. Their fur-
ther study, Lin et al (2008) exploit more features 
and achieve a higher performance. 
Unlike all the studies mentioned above, our 
study is the first attempt on exploring the rela-
tionship between comment writer?s emotion 
classification and news reader?s emotion classifi-
cation.  
3 Relationship between News Reader?s 
and Comment Writer?s Emotions 
To investigate the relationship between news 
reader?s and comment writer?s emotions, we col-
lect a corpus of Chinese news articles and their 
corresponding comments from Yahoo! Kimo 
News (http://tw.news.yahoo.com), where each 
news article is voted with emotion tags from 
eight categories: happy, sad, angry, meaningless, 
boring, heartwarming, worried, and useful. 
These emotion tags on each news are selected by 
the readers of the news. Note that because the 
categories of ?useful? and ?meaningless? are not 
real emotion categories, we ignore them in our 
study. Same as previous studies of Lin et al 
(2007) and Lin et al (2008), we consider the 
voted emotions as reader?s emotions on the news, 
i.e., the news reader?s emotions. We only select 
the news articles with a dominant emotion (pos-
sessing more than 50% votes) in our data. Be-
sides, as we attempt to consider the comment 
writer?s emotions, the news articles without any 
comments are filtered. 
As a result, we obtain a corpus of 3495 news 
articles together with their comments and the 
numbers of the articles of happy, sad, angry, 
boring, heartwarming, and worried are 1405, 
230, 1673, 75, 92 and 20 respectively. For 
coarse-grained categories, happy and heartwarm-
ing are merged into the positive category while 
512
sad, angry, boring and worried are merged into 
the negative category. 
Besides the tags of the reader?s emotions, each 
news article is followed by some comments, 
which can be seen as a reflection of the writer?s 
emotions (Averagely, each news is followed by 
15 comments). In order to know the exact rela-
tionship between these two kinds of emotions, 
we select 20 news from each category and ask 
two human annotators, named A and B, to manu-
ally annotate the writer?s emotion (single-label) 
according to the comments of each news. Table 1 
reports the agreement on annotators and emo-
tions, measured with Cohen?s kappa (?) value 
(Cohen, 1960). 
 ?  Value 
(Fine-grained 
emotions) 
? Value 
(Coarse-grained 
emotions) 
Annotators 0.566 0.742 
Emotions 0.504 0.756 
Table 1: Agreement on annotators and emotions 
 
Agreement between two annotators: The 
annotation agreement between the two annota-
tors is 0.566 on the fine-grained emotion catego-
ries and 0.742 on the coarse-grained emotion 
categories.  
Agreement between news reader?s and 
comment writer?s emotions: We compare the 
news reader?s emotion (automatically extracted 
from the web page) and the comment writer?s 
emotion (manually annotated by annotator A). 
The annotation agreement between the two kinds 
of emotions is 0.504 on the fine-grained emotion 
categories and 0.756 on the coarse-grained emo-
tion categories. From the results, we can see that 
the agreement on the fine-grained emotions is a 
bit low while the agreement between the coarse-
grained emotions, i.e., positive and negative, is 
very high. We find that although some fine-
grained emotions of the comments are not con-
sistent with the dominant emotion of the news, 
they belong to the same coarse-grained category.  
In a word, the agreement between news read-
er?s and comment writer?s emotions on the 
coarse-grained emotions is very high, even high-
er than the agreement between the two annota-
tors (0.754 vs. 0.742).  
In the following, we focus on the coarse-
grained emotions in emotion classification. 
4 Joint Modeling of News Reader?s and 
Comment Writer?s Emotions 
Given the importance of both news reader?s and 
comment writer?s emotion classification as de-
scribed in Introduction and the close relationship 
between news reader?s and comment writer?s 
emotions as described in last section, we system-
atically explore their joint modeling on the two 
kinds of emotion classification. 
In semi-supervised learning, the unlabeled da-
ta is exploited to improve the models with a 
small amount of the labeled data. In our ap-
proach, we consider the news text and the com-
ment text as two different views to express the 
news or comment emotion and build the two 
classifiers 
NC  and CC . Given the two-view clas-
sifiers, we perform co-training for semi-
supervised emotion classification, as shown in 
Figure 2, on both news reader?s and comment 
writer?s emotion classification. 
 
 
Input:   
NewsL  the labeled data on the news 
CommentL the labeled data  on the comments 
NewsU the unlabeled data  on the news  
CommentU  the labeled data  on the comments 
Output: 
NewsL New labeled data on the news 
CommentL  New labeled data on the comments 
 
Procedure: 
 
Loop for N iterations until
NewsU ??  or CommentU ??  
(1). Learn classifier 
NC  with NewsL  
(2). Use 
NC  to label the samples from NewsU   
(3). Choose 
1n  positive and 1n negative news 1N  
most confidently predicted by 
NC  
(4). Choose corresponding comments 
1M (the 
comments of the news in 
1N ) 
(5). Learn classifier 
CC  with CommentL  
(6). Use 
CC  to label the samples from CommentU   
(7). Choose 
2n  positive and 2n negative comments 
2M  most confidently predicted by CC  
(8). Choose corresponding comments 
2N (the news 
of the comments in 
2M ) 
(9). 
1 2News NewsL L N N? ? ?  
1 2Comment CommentL L M M? ? ? 
(10). 
1 2News NewsU U N N? ? ?
1 2Comment CommentU U M M? ? ? 
 
Figure 2: Co-training algorithm for semi-
supervised emotion classification 
513
5 Experimentation 
5.1 Experimental Settings 
Data Setting: The data set includes 3495 news 
articles (1572 positive and 1923 negative) and 
their comments as described in Section 3. Alt-
hough the emotions of the comments are not giv-
en in the website, we just set their coarse-grained 
emotion categories the same as the emotions of 
their source news due to their close relationship, 
as described in Section 3. To make the data bal-
anced, we randomly select 1500 positive and 
1500 negative news with their comments for the 
empirical study. Among them, we randomly se-
lect 400 news with their comments as the test 
data. 
Features: Each news or comment text is treat-
ed as a bag-of-words and transformed into a bi-
nary vector encoding the presence or absence of 
word unigrams. 
Classification algorithm: the maximum en-
tropy (ME) classifier implemented with the pub-
lic tool, Mallet Toolkits*. 
5.2 Experimental Results 
News reader?s emotion classifier: The classifier 
trained with the news text. 
Comment writer?s emotion classifier: The 
classifier trained with the comment text. 
Figure 3 demonstrates the performances of the 
news reader?s and comment writer?s emotion 
classifiers trained with the 10 and 50 initial la-
beled samples plus automatically labeled data 
from co-training. Here, in each iteration, we pick 
2 positive and 2 negative most confident samples, 
i.e, 
1 2 2n n? ? . From this figure, we can see that 
our co-training algorithm is very effective: using 
only 10 labeled samples in each category 
achieves a very promising performance on either 
news reader?s or comment writer?s emotion clas-
sification. Especially, the performance when us-
ing only 10 labeled samples is comparable to that 
when using more than 1200 labeled samples on 
supervised learning of comment writer?s emotion 
classification. 
   For comparison, we also implement a self-
training algorithm for the news reader?s and 
comment writer?s emotion classifiers, each of 
which automatically labels the samples from the 
unlabeled data independently. For news reader?s 
emotion classification, the performances of self-
training are 0.783 and 0.79 when 10 and 50 ini-
                                                 
* http://mallet.cs.umass.edu/ 
tial labeled samples are used. For comment writ-
er?s emotion classification, the performances of 
self-training are 0.505 and 0.508. These results 
are much lower than the performances of our co-
training approach, especially on the comment 
writer?s emotion classification i.e., 0.505 and 
0.508 vs. 0.783 and 0.805. 
 
10 Initial Labeled Samples
0.5
0.6
0.7
0.8
0 400 800 1200 1600 2000 2400
Size of the added unlabeled data
A
c
c
u
r
a
c
y
 
50 Initial Labeled Samples
0.65
0.7
0.75
0.8
0.85
0.9
0 400 800 1200 1600 2000 2400
Size of the added unlabeled data data
A
c
c
u
r
a
c
y
The news reader's emotion
classifier (Co-training)
The comment writer's emotion
classifier (Co-training)
 Figure 3: Performances of the news reader?s and 
comment writer?s emotion classifiers using the 
co-training algorithm 
6 Conclusion 
In this paper, we focus on two popular emotion 
classification tasks, i.e., reader?s emotion classi-
fication on the news and writer?s emotion classi-
fication on the comments. From the data analysis, 
we find that the news reader?s and comment 
writer?s emotions are highly consistent to each 
other in terms of the coarse-grained emotion cat-
egories, positive and negative. On the basis, we 
propose a co-training approach to perform semi-
supervised learning on the two tasks. Evaluation 
shows that the co-training approach is so effec-
tive that using only 10 labeled samples achieves 
nice performances on both news reader?s and 
comment writer?s emotion classification.  
514
Acknowledgments 
This research work has been partially supported 
by two NSFC grants, No.61003155, and 
No.61273320, one National High-tech Research 
and Development Program of China 
No.2012AA011102, one General Research Fund 
(GRF) sponsored by the Research Grants Coun-
cil of Hong Kong No.543810, the NSF grant of 
Zhejiang Province No.Z1110551, and one pro-
ject supported by Zhejiang Provin-cial Natural 
Science Foundation of China, No.Y13F020030.  
References  
Alm C., D. Roth and R. Sproat. 2005. Emotions from 
Text: Machine Learning for Text-based Emotion 
Prediction. In Proceedings of EMNLP-05, pp.579-
586. 
Aman S. and S. Szpakowicz. 2008. Using Roget?s 
Thesaurus for Fine-grained Emotion Recognition. 
In Proceedings of IJCNLP-08, pp.312-318. 
Chen Y., S. Lee, S. Li and C. Huang. 2010. Emotion 
Cause Detection with Linguistic Constructions. In 
Proceeding of COLING-10, pp.179-187. 
Cohen J. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Meas-
urement, 20(1):37?46. 
 Cui H., V. Mittal and M. Datar. 2006. Comparative 
Experiments on Sentiment Classification for 
Online Product Comments. In Proceedings of 
AAAI-06, pp.1265-1270. 
Das D. and S. Bandyopadhyay. 2009. Word to Sen-
tence Level Emotion Tagging for Bengali Blogs. In 
Proceedings of ACL-09, pp.149-152. 
Dasgupta S. and V. Ng. 2009. Mine the Easy, Classify 
the Hard: A Semi-Supervised Approach to Auto-
matic Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09,  pp.701-709, 2009. 
Duin R. 2002. The Combining Classifier: To Train Or 
Not To Train? In Proceedings of 16th International 
Conference on Pattern Recognition (ICPR-02). 
Fumera G. and F. Roli. 2005. A Theoretical and Ex-
perimental Analysis of Linear Combiners for Mul-
tiple Classifier Systems. IEEE Trans. PAMI, vol.27, 
pp.942?956, 2005. 
Li S., Z. Wang, G. Zhou and S. Lee. 2011. Semi-
supervised Learning for Imbalanced Sentiment 
Classification. In Proceeding of IJCAI-11,  pp.826-
1831. 
Li S., C. Huang, G. Zhou and S. Lee.  2010. Employ-
ing Personal/Impersonal Views in Supervised and 
Semi-supervised Sentiment Classification. In Pro-
ceedings of ACL-10,  pp.414-423. 
Lin K., C. Yang and H. Chen. 2007. What Emotions 
do News Articles Trigger in Their Readers? In 
Proceeding of SIGIR-07, poster, pp.733-734. 
Lin K., C. Yang and H. Chen. 2008. Emotion Classi-
fication of Online News Articles from the Reader?s 
Perspective. In Proceeding of the International 
Conference on Web Intelligence and Intelligent 
Agent Technology, pp.220-226. 
 Liu B. 2012. Sentiment Analysis and Opinion Mining 
(Introduction and Survey). Morgan & Claypool 
Publishers, May 2012. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Moshfeghi Y., B. Piwowarski and J. Jose. 2011. Han-
dling Data Sparsity in Collaborative Filtering using 
Emotion and Semantic Based Features. In Proceed-
ings of SIGIR-11, pp.625-634. 
Pang B. and L. Lee. 2008. Opinion Mining and 
Sentiment Analysis: Foundations and Trends. 
Information Retrieval, vol.2(12), 1-135. 
Pang B., L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of EMNLP-
02, pp.79-86. 
Purver M. and S. Battersby. 2012. Experimenting 
with Distant Supervision for Emotion Classifica-
tion. In Proceedings of EACL-12, pp.482-491. 
Quan C. and F. Ren. 2009. Construction of a Blog 
Emotion Corpus for Chinese Emotional Expression 
Analysis. In Proceedings of EMNLP-09, pp.1446-
1454. 
Riloff E., S. Patwardhan and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In Proceedings 
of EMNLP-06, pp.440-448. 
Turney P. 2002. Thumbs up or Thumbs down? 
Semantic Orientation Applied to Unsupervised 
Classification of comments. In Proceedings of 
ACL-02, pp.417-424.  
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial Intelligence 
Review, 18(2): 77?95. 
Volkova S., W. Dolan and T. Wilson. 2012. CLex: A 
Lexicon for Exploring Color, Concept and Emo-
tion Associations in Language. In Proceedings of 
EACL-12, pp.306-314. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433. 
Xu G., X. Meng and H. Wang. 2010. Build Chinese 
Emotion Lexicons Using A Graph-based 
Algorithm and Multiple Resources. In Proceeding 
of COLING-10, pp.1209-1217. 
515
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 47?54
Manchester, August 2008
Multilingual Conceptual Access to Lexicon based on Shared Ortho-
graphy: An ontology-driven study of Chinese and Japanese 
Chu-Ren Huang 
Institute of Linguistics, 
Academia Sinica  
Nanking, Taipei,  
Taiwan 115 
churen@sinica.edu.tw 
Ya-Min Chou 
Ming Chuan University  
250 Zhong Shan N. Rd., Sec. 5,  
Taipei 111, Taiwan  
milesymchou@yahoo.com.tw  
Chiyo Hotani 
Department of Linguistics 
University of Tuebingen 
Wilhelmstr. 19 
72074 T?bingen, Deutschland 
chiyo.hotani@student.uni-tuebingen.de  
 Sheng-Yi Chen 
Institute of Linguistics, 
Academia Sinica 
Nanking, Taipei,  
Taiwan 115 
eagles@gate.sinica.edu.tw  
Wan-Ying Lin 
Institute of Linguistics, 
Academia Sinica 
Nanking, Taipei,  
Taiwan 115 
waiin@gate.sinica.edu.tw 
                                                
 
Abstract 
In this paper we propose a model for 
conceptual access to multilingual lexicon 
based on shared orthography. Our propo-
sal relies crucially on two facts: That both 
Chinese and Japanese conventionally use 
Chinese orthography in their respective 
writing systems, and that the Chinese 
orthography is anchored on a system of 
radical parts which encodes basic 
concepts. Each orthographic unit, called 
hanzi and kanji respectively, contains a 
radical which indicates the broad se-
mantic class of the meaning of that unit. 
Our study utilizes the homomorphism 
between the Chinese hanzi and Japanese 
kanji systems to ide1ntify bilingual word 
correspondences. We use bilingual dictio-
naries, including WordNet, to verify 
semantic relation between the cross-
lingual pairs. These bilingual pairs are 
then mapped to an ontology constructed 
based on relations to the relation between 
the meaning of each character and the 
 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
basic concept of their radical parts. The 
conceptual structure of the radical 
ontology is proposed as a model for 
simultaneous conceptual access to both 
languages. A study based on words 
containing characters composed of the 
??(mouth)? radical is given to illustrate 
the proposal and the actual model. The 
fact that this model works for two 
typologically very different languages 
and that the model contains generative 
lexicon like coersive links suggests that 
this model has the conceptual robustness 
to be applied to other languages. 
1 Motivation  
Computational conceptual access to multilingual 
lexicon can be achieved through the use of ontol-
ogy or WordNet as interlingual links. Some lan-
guages do conventionally encode semantic classi-
fication information, such as the linguistic system 
of classifiers or the orthographic system of cha-
racters. We attempt to make use of these implicit-
ly encoded linguistic knowledge for conceptual 
access to lexical information. 
On the other hand, even though ontology seems 
to be a natural choice for conceptual framework 
to access multilingual lexical information, there 
is no large-scale implementation nor is there any 
47
direct evidence for psychological reality of the 
frameworks of ontology. Hence, we hope that 
using a conventionalized semantic classification 
system will mitigate some of the problems and 
provide the constructed ontology some motiva-
tion since they are the shared and implicit con-
ceptual systems.  
2 Background  
2.1. Hanzi and kanji: Shared Orthography of 
Two Typologically Different Languages 
 
Chinese and Japanese are two typologically dif-
ferent languages sharing the same orthography 
since they both use Chinese characters in written 
text. What makes this sharing of orthography 
unique among languages in the world is that Chi-
nese characters (kanji in Japanese and hanzi in 
Chinese) explicitly encode information of seman-
tic classification (Xyu 121, Chou and Huang 
2005). This partially explains the process of Jap-
anese adopting Chinese orthography even though 
the two languages are not related. The adaptation 
is supposed to be based on meaning and not on 
cognates sharing some linguistic forms. Howev-
er, this meaning-based view of kanji/hanzi ortho-
graphy faces a great challenge given the fact that 
Japanese and Chinese form-meaning pair do not 
have strict one-to-one mapping. There are mean-
ings instantiated with different forms, as well as 
same forms representing different meanings. The 
character ? is one of most famous faux amis. It 
stands for ?hot soup? in Chinese and ?hot spring? 
in Japanese. In sum, these are two languages 
where their forms are supposed to be organized 
according to meanings, but show inconsistencies. 
 It is important to note that WordNet and 
the Chinese character orthography are not so dif-
ferent as they appear. WordNet assumes that 
there are some generalizations in how concepts 
are clustered and lexically organized in languages 
and propose an explicit lexical level representa-
tion framework which can be applied to all lan-
guages in the world. Chinese character orthogra-
phy intuited that there are some conceptual bases 
for how meanings are lexicalized and organized, 
hence devised a sub-lexical level representation 
to represent semantic clusters. Based on this ob-
servation, the study of cross-lingual homo-forms 
between Japanese and Chinese in the context of 
WordNet offers an unique window for different 
approaches to lexical conceptualization. Since 
Japanese and Chinese use the same character set 
with the same semantic primitives (i.e. radicals), 
we can compare their conceptual systems with 
the same atoms when there are variations in 
meanings of the same word-forms. When this is 
overlaid over WordNet, we get to compare the 
ontology of the two represent systems. 
 
2.2. Hantology and the Ontologization of the 
Semantic Classification of the Radicals 
 
The design of Hantology differs from other 
word-based ontology. A typical word-based on-
tology is WordNet which describes the different 
relations among synonyms. All of the relations 
among synonyms are based on the senses of 
words. Therefore, WordNet only needs to take 
senses into consideration. Hantology is more 
complicated than WordNet because it describes 
orthographic forms, pronunciations, senses, va-
riants, lexicalization, the spread of Chinese cha-
racters and Japanese kanji.This approach can sys-
tematically illustrate the development of Chinese 
writing system (Chou et al 2007). 
Hantology also provides mapping with Sinica 
BOW(Academia Sinica Bilingual Ontological 
WordNet). Sinica BOW is a Chinese-English 
Ontology and have mapping with WordNet. 
Therefore, character-based and word-based 
ontologies are integrated to provide resources 
from character to word for Chinese language 
processing. 
 
Figure 1. The Mapping among Hantology,  
Sinica BOW and WordNet 
 
The structure of Hantology is divided into 
three parts: orthography, pronunciation, and 
lexicalization. 
The orthographic part of Hantology describes the 
structure of characters, the principles of 
formatting characters, the evolution of script, 
Hantology 
(Chinese-Japanese charac-
ter- based ontology) 
Sinica BOW 
(Chinese-English word- 
based ontology)  
WordNet 
48
glyph expression, the relation of variant and the 
spread of Chinese characters. 
(1) The structure of characters describes the 
components of each hanzi/kanji, including 
semantic and phonetic symbols.  
(2) The principles of formatting Chinese 
characters encode the classification of the 
relation used to compose the character from its 
components: The pictographic characters were 
formed by reformatting the pictures of concrete 
objects. The ideographic (zhi3shi4, refer-event) 
characters are formed by abstract representation 
of an concept. The compound ideographic 
characters are formed by combining two (ore 
more) semantic symbols. The semantic-
phonetic (xing2sheng1) characters, representing 
over 90 percent of Chinese character, are 
formed by combining a semantic symbol and a 
phonetic symbol.  
(3) The evolution of script illustrates the 
different scripts of Chinese characters. The 
script is a kind of writing style. Because 
Chinese characters have been used for 
thousands years, the scripts have changed.The 
orthographic forms do not change with different 
scripts. Hantology provides Bronze, Lesser 
Seal, Kaishu scripts to illustrate evolution of 
Chinese scripts used from 3000 years ago.  
(4) Variants are the characters with different 
orthographic forms with identical pronunciation 
and meaning. For example, Chinese chara-
cters?and ?are variants. Variants relations are 
an important feature in Hantology, similar to 
WordNet synset relations. 
 (5) The contrasts between kanji and hanzi 
glyphs are also encoded. The Japanese language 
continues to evolve and change after the 
adoption of Chinese characters. Hence the kanji 
system includes both historical changes and 
cross-lingual variations. The kanji system has 
its own variants which are not necessarily the 
same set of variants in the hanzi system. Most 
of Chinese characters adopted by simplified 
kanji are the variants already used in Chinese. 
For example, ??? is a simplified kanji of tradi-
tional kanji ???. In addition, Chinese character 
??? is also the variant of Chinese charac-
ter???. So, ???and??? both are variants in 
Chinese and Japanese. But, some simplified 
kanji are not variants used in Chinese. For 
example, new kanji ??? is the variant of old 
kanji ??? in Japan. However, ??? is not the 
variant of ??? in Chinese.  
The second reason of the kanji orthographic 
form to to be changed is that Japanese not only 
adopted Chinese characters but also have created 
hundreds kanji known as Kokuji (??). Most 
Kokuji characters have only Japanese pronuncia-
tions. Some of Kokuji have been adopted in Chi-
nese. For example, Kokuji ???is also borrowed 
by Chinese. The meaning of ??? is the same both 
in Japanese and Chinese.  
 
3. Preliminaries: Orthography based 
Mapping of Chinese and Japanese Words 
3.1 EDR Japanese-English Dictionary 
The Japanese-English dictionary of EDR Elec-
tronic Dictionary is a machine-tractable dictio-
nary that contains the lexical knowledge of Japa-
nese and English.1It contains list of 165,695 Jap-
anese words (jwd) and each of their related in-
formation. 
In this experiment, the English synset, definition 
and the Part-of-Speech category (POS) of each 
jwd are used to determine the semantic relations. 
We assume that the concept, synonyms, near-
synonyms, and paraphrases are the synset of each 
jwd.In the case when there is no English defini-
tion for the word, we assume that there is no 
equivalent term in English, therefore we use the 
concept definition of the jwd as its definition. 
3.2 SinicaBow 
In the previous experiment, the CWN, which 
contains a list of 8,624 Chinese word (cwd) en-
tries, was used as the cwd data, however since 
the number of cwds was too small, many jwds 
were not mapped, even when there is actually a 
corresponding J-C word pairs exists. 
This time we adopt SinicaBow, which contains 
9,9642 entries, hoping to find more valid corres-
ponding J-C word pairs.In SinicaBow, each entry 
is a definition and it contains one or more cwds 
corresponds to the definition. 
In this experiment, the English synset, definition 
and the POS of each cwd are used to determine 
the semantic relations. 
3.3List of Kanji Variants 
List of 125 pairs of manually matched Chinese 
and Japanese characters with variant glyph forms 
provided by Kyoto University. 
                                                 
1 http://www2.nict.go.jp/r/r312/EDR/index.html 
 
49
Some Japanese kanji and Chinese hanzi have 
identical property but have different font and Un-
icode.This resource contains list of Japanese kan-
ji and Chinese hanzi pairs that the kanji proper-
ties are exactly the same but the forms and the 
Unicode are different. 
During the mapping procedure, whenever a Japa-
nese kanji and a Chinese hanzi being compared 
are in the variant list and are the variants of each 
other, they are considered to be the identical han-
zi. 
3.4 Procedure 
3.4.1Kanji Mapping 
Each jwd is mapped to the corresponding cwd 
according to their kanji similarity.Such mapping 
pairs are divided in to the following three groups: 
(1) Identical Kanji Sequence Pairs, where the 
numbers of kanji in the jwd and cwd are identical 
and the nth characters in the two words are also 
identical.  
E.g. ?, ?? 
(2) Different Kanji Order Pairs, where the num-
bers of kanji in the jwd and cwd are identical, 
and the kanji appear in the two words are identic-
al, but the order is different.  
E.g.  Japanese  Chinese 
??   ?? 
??   ?? 
(3) Partially Identical Pairs, where at least half 
kanji in the shorter word matches with the part of 
the longer word.In the case when the shorter 
word has 4 or less kanji, 2 of the kanji have to be 
in the longer word.In the case when the shorter 
word is only 1 kanji, the pair is not consi-
dered.jwd matches with a kanji in the cwd.  
E.g.,  Japanese  Chinese 
???  ??? 
???? 
??? 
????? ?? 
    ??? 
etc? 
In the case no corresponding pair relation (one of 
the three groups explained above) is found for a 
jwd or a cwd, each word is classified to one of 
the following group 
(4) unmapped jwd is classified to an independent 
Japanese 
(5) unmapped cwd is classified to an independent 
Chinese 
J-C word pairs in such mapping groups are clas-
sified in the following manner: (1) A jwd and a 
cwd are compared.If the words are identical, then 
they are an identical kanji sequence pair.(2) If the 
pair is found to be not an identical kanji sequence 
pair, check if the pair has identical kanji in dif-
ferent order (equal length).If so, then they are a 
different kanji order pair.(3) If the pair is found 
to be not a different kanji order pair, then check 
the partial identity of the pair.Meanwhile, if they 
are partially identical (according to the characte-
ristics of partially identical pairs described 
above), the pair is classified to a partially identic-
al pair. 
 
After the mapping process, if the jwd is not 
mapped to any of the cwd, the jwd is classified to 
(4) independent Japanese group. If a cwd is not 
mapped by any of the jwd, it is classified to (5) 
independent Chinese group. 
 
The number of Japanese kanji- Chinese hanzi 
pairs? similarity distribution is shown in Table1.  
 
 Number of 
Words 
Number of 
J-C Word 
Pairs 
(1) Identical hanzi 
Sequence Pairs  
2815 jwds  20199  
(2) Different hanzi 
Order Pairs  
204 jwds  473  
(3) Partly Identical 
Pairs  
264917 jwds  8438099  
(4) Independent 
Japanese  
57518 jwds  -  
(5) Independent 
Chinese  
851 cwds  -  
Table1. J-C Hanzi Similarity Distribution (Huang et 
al. 2008). 
 
3.4.2Finding Synonymous Relation (Word Re-
lation) 
After the kanji mapping, each of (1) identical 
kanji sequence pairs, (2) different kanji order 
pairs and (3) partially identical pairs is divided 
into three subgroups;  
(1-1, 2-1, 3-1) Synonym pairs with identical 
POS: words in a pair are synonym with identical 
POS.  
E.g. (1-1) ??: singer (noun)  
(2-1) ??? (Japanese) and  
??? (Chinese):  
blue-violet color (noun) 
50
(3-1) ??? (Japanese) and  
??? (Chinese):  
brown sugar (noun) 
(1-2, 2-2, 3-2) Synonym pairs with unmatched 
POS: words in a pair are synonym with different 
POS or POS of at least one of the words in the 
pair is missing.  
E.g. (1-2) ?:  
(Japanese) action of wrapping (noun)  
(Chinese) to wrap (verb) 
(2-2) ?? (Japanese): a cough (noun) 
 ?? (Chinese): cough (verb) 
(1-3, 2-3, 3-3) Relation Unidentified: the relation 
is not determinable by machine processing with 
the given information at this point.  
E.g. Japanese  Chinese 
(1-3) ?: hot spring (noun) ?: soup (noun) 
(2-3) ??:    ??: flower 
arrangement (noun) peanut (noun) 
(3-3) ???:   ???:  
blue grapes (noun) Portugal (noun) 
In order to find the semantic relation of J-C word 
pairs by machine analysis, the jwd and the cwd in 
a pair are compared according to the following 
information: 
Jwd: English synset (jsyn), definition (jdef) and 
POS 
Cwd: English synset (csyn), definition (cdef) and 
POS 
The process of checking the synonymy of each 
pair is done in the following manner:  
If any of the following conditions meets, we as-
sume that the pair is a synonym pair:  
at least any one of the synonym from each of jsyn 
and csyn are identical 
at least one of the word definition contains a syn-
onym of the other word 
 
If any synonym pair was found, check if the POS 
are identical.If the POS are identical, the pair is 
classified to a synonym pair with identical 
POS.Otherwise the pair is classified to a syn-
onym pair with non-identical POS.If the pair is 
not a synonym pair then they are classified to a 
relation-unidentified pair. 
After the process, each of the subgroups is ma-
nually examined to check the actual semantic 
relations of each word pair. 
4. Result 
4.1 Word Family as Domain Ontology Headed 
by a Basic Concept 
Chinese radical (yi4fu2, ideographs; semantic 
symbols) system offers a unique opportunity for 
systematic and comprehensive comparison be-
tween formal and linguistic ontologies. Chou and 
Huang (2005) suggests that the family of Chinese 
characters sharing the same radical can be linked 
to a basic concept by Qualia relations. Based on 
Pustejovsky?s Quilia Structure [Pustejovsky, 
1995] and the original analysis of ?ShuoWen-
JieXi?[Xyu, 121], each radical group can be as 
domain ontology headed by one basic concept.  
 
Chou and Huang (2005) assume that 540 radicals 
in ?ShuoWenJieXi? can each represent a basic 
concept and that all derivative characters are 
conceptually dependent on that basic concept. 
Also, they hypothesis that a radical can be classi-
fied into six main types: formal, constitutive, tel-
ic, participating, descriptive (state, manner) and 
agentive. Modes of conceptual extension capture 
the generative nature of radical creativity. All 
derived characters are conceptually dependent on 
the basic concept. In their preliminary studies, 
word family could be headed by a basic concept 
and also could be represented ontologies in OWL 
format. 
 
4.2Data Analysis: Japanese and Chinese 
Words with Identical Orthography 
4.2.1 Kanji Mapping 
We present our study over Japanese and Chinese 
lexical semantic relation based on the kanji se-
quences and their semantic relations.We com-
pared Japanese-English dictionary of Electric 
Dictionary Research (EDR) with the SinicaBow 
in order to examine the nature of cross-lingual 
lexical semantic relations. 
 
 Identical Different Order 
Part Identic-
al 
Synonym 
(Identical 
POS) 
(1-1) 13610 
pairs 
(2-1) 567 
pairs 
(3-1) 37466 
pairs 
Synonym 
(Unmatched 
POS) 
(1-2) 2265 
pairs 
(2-2) 214 
pairs 
(3-2) 22734 
pairs 
Relation Un-
identified 
(1-3) 21154 
pairs 
(2-3) 2336 
pairs 
(3-3) 
1116141 
pairs 
Total 
(1) 37029 
pairs 
(2) 3117 
pairs 
 (3) 
1176341 
pairs 
16950 jwds 1497 jwds 39821 jwds
(4) Unmapped Japanese: 107427 jwds 
51
(5) Unmapped Chinese: 41417 entries 
Table 1.J-C Kanji Similarity Distribution 
 
The next step is to find Synonymous Relation. 
(Word Relation). 
Number of 1-to-1 Form-
Meaning Pairs Found by 
Machine Analysis 
% in (1)
(1-1) Synonym 
(Identical POS) 13610 36.8%
(1-2) Synonym 
(Unmatched POS) 2265 6.1%
(1-3) Relation 
Unidentified 21154 57.1%
Table 2. Identical Kanji Sequence Pairs (37029 pairs) 
Synonymous Relation Distribution 
 
Number of 1-to-1 Form-
Meaning Pairs Found by 
Machine Analysis 
% in (2)
(2-1) Synonym 
(Identical POS) 567 18.2%
(2-2) Synonym 
(Unmatched POS) 214 6.9%
(2-3) Relation 
Unidentified 2336 74.9%
Table 3.Identical Kanji But Different Order Pairs 
(3117 pairs) Synonymous Relation Distribution 
 
Number of 1-to-1 Form-
Meaning Pairs Found by 
Machine Processing  
% in (3)
(3-1) Synonym 
(Identical POS) 37466 3.2% 
(3-2) Synonym 
(Unmatched POS) 22734 1.9% 
(3-3) Relation 
Unidentified 1116141 94.9%
Table 4. Partially Identical Pairs (1176341 pairs) Syn-
onymous Relation Distribution 
 
The following tables are summarized tables 
showing the Japanese-Chinese form-meaning 
relation distribution examined in our preliminary 
study. 
 
Pairs Found 
to be Syn-
onym 
% in 
(1) 
Relation 
Unidentified % in (1)
Machine 
Analysis 15875 
42.9
% 21154 57.1%
Table 5. Identical kanji Sequence Pairs (37029 pairs) 
Lexical Semantic Relation 
 
 
Pairs 
Found to 
be Syn-
onym 
% in 
(2) 
Relation Un-
identified % in (2)
Machine 
Analysis 781 25.1% 2336 74.9%
Table 6. Identical kanji But Different Order Pairs 
(3117 pairs) Lexical Semantic Relation 
 
 
Pairs Found 
to be Syn-
onym 
% in 
(3) 
Relation Un-
identified % in (3)
Machine 
Analysis 60200 5.1% 1116141 94.9%
Table7. Partially Identical Pairs (1176341 pairs) Lexi-
cal Semantic Relation 
 
Since each entry in SinicaBow corresponds to a 
definition and each jwd has at least a definition 
or a concept definition, no pairs with insufficient 
information to check the semantic relation was 
found.The data shows that as the word forms of 
the two languages are closer, the more synonyms 
are found.In order to confirm this observation 
and to see the actual semantic relation of each 
pairs, we will continue with more detailed analy-
sis.In addition, in order to pursue the further de-
tails of the Japanese-Chinese words relation, we 
will also analyze the semantic relations (not only 
synonymous relation) of the relation-unidentified 
pairs. 
 
4.2.2 ??(mouth)?Analysis Procedure: 
 
In our experiment, we select the identical kanji 
Sequence Pairs (POS) as our main resources. 
Characters with the radical??(mouth)?are se-
lected. In addition, if any character of the words 
owns the radical ??(mouth)?, then it would be 
included here for anaylysing the detailed seman-
tic relation between jwd and cwd..  
 
Second, we would like to define the semantic 
relations of J-C word pairs in more details. We 
examined the actual semantic relation of J-C 
word pairs by by classifying into 8 semantic rela-
tions and marked the relation into [ ] remark.  
 
1.[SYN](Synonym) 
2.[NSN](Near-Synonym) 
3.[HYP](Hypernym) 
4.[HPO](Hyponym) 
5.[HOL](Holonym) 
6.[MER](Meronym) 
7.[/](No Corresponding Semantic Relation) 
8.[??](unable to decide) 
The pattern is as follows. 
[(JWD>jsyn>??>jdef>)-[Semantic Relation]-
(CWD)>csyn>??>cdef]] 
52
 
Sample: 
[(J)-[HYP]-(C)]@  
(J is the hypernym of C) 
 
The examples are shown here. In each pair, we 
define the semantic relation between the jwd and 
the cwd. The mapping process would be as fol-
lows.  
 
E.g  
1. [(?> JWD0028646> N> a condition of being in-
capable of speaking using the voice> )-[SYN]-(?> 
10137481N> N> paralysis of the vocal cords resulting 
in an inability to speak> alalia,)]@ 
2. [(?> JWD0378514> N> of a bird, a bill> bill)-
[SYN]-(?> 01278388N> N> horny projecting jaws 
of a bird> nib,neb,bill,beak,)]@ 
3. [(??> JWD0161758> N> part of an animal called 
a throat> )-[SYN]-(??> 04296952N> N> the pas-
sage to the stomach and lungs; in the front part of the 
neck below the chin and above the collarbone> pha-
rynx,throat,)]@ 
4. [(???> JWD0398785> N> a bird that is related 
to the picidae, called woodpecker> woodpecker)-
[SYN]-(???> 01355454N> N> bird with strong 
claws and a stiff tail adapted for climbing and a hard 
chisel-like bill for boring into wood for insects> 
woodpecker,)]@ 
5. [(?????> JWD0401642> N> a medical in-
strument with which a patient can breathe artificially> 
respirator)-[SYN]-(?????> 03233384N> N> a 
device for administering long-term artificial respira-
tion> inhalator,respirator,)]@ 
 
According to our observation, we notice that 
most of the Japanese kanji can get their syn-
onyms or near-synonyms in Chinese hanzi and 
the percentage for this relation is about 63 % in 
characters with the radical??(mouth) selected 
from Identical Synonym POS data. Please refer 
to table1. The distributions of Semantic Relations 
comparing jwd to cwd in characters with the rad-
ical??(mouth) chosen from Identical Syno PO-
Sare as follows. 
 
Semantic 
Relations 
between  
J-C word 
Distribution  
in Characters 
with the radi-
cal?(mouth) 
% in Characters 
with the Radical 
?(mouth), 486 
total pairs 
[SYN] 190 39% 
[NSN] 129 27% 
[HYP] 16 4% 
[HPO] 7 2% 
[HOL] 11 3% 
[MER] 12 3% 
[/] 118 25% 
[??] 1 1% 
Table8. Semantic Relation Distribution in Characters 
with the radical?? Mouth? 
 
4.3 Conceptual Access: A Preliminary Model 
 
In this part, we try to apply dimension of concep-
tual extension of ??(mouth)? radical into the data 
we have chosen from the Identical Synonym POS 
data comparing with Japanese kanji and Chinese 
hanzi.(Please refer to the Appendix A.) A study 
based on words containing characters composed 
of the ??(mouth) ? radical is given for illustration 
in this preliminary study. It shows that the con-
ceptual robustness can also be applied to other 
languages, such as Japanese kanji.  
 
Categories in ?? 
mouth conceptual 
extension? 
Exam-
ples  
in ?? 
mouth 
concep-
tual 
exten-
sion? 
Japanese kanji-
Chinese hanziEx-
ample 
Formal 
-Sense-Vision&Size
?  
Formal 
-Sense-Hearing 
?  
Constitutive ???
?? 
????????
????????
???? 
Descriptive-Active ??? ?? 
Descriptive-State ? ????????
??? 
Participating-Action ???
???
? 
????????
?????? 
Participating-others ???  
Participating- 
instrument 
? ????????
??? 
Metaphor ? ????????
???? 
TELIC- Subordi-
nate Concept1& 
Subordinate Con-
cept2
 
Subordinate Con-
cept1(Speaking) 
 
Formal-Property ?  
Formal-Sense-
Hearing 
?  
Constitutive ??? ????????
???????? 
Descriptive-Active ??? ????? 
Participator ??? ????????
53
Fellbaum Christiane.1998. WordNet: An Electronic 
Lexical Database. Cambridge : MIT Press. 
?? 
Participating-
Action- 
Way 
??? ?? 
Participating-others ??? ????????
????? 
Subordinate Con-
cept2 (Eating) 
 
Formal-Sense-Taste ??? ???? 
Descriptive-Active ?  
Participating-Action ?  
Participating-State ?  
Participator ? ???????? 
Hsieh, Ching-Chun and Lin, Shih. A Survey of Full- 
text Data Bases and Related Techniques for Chinese 
Ancient Documents in Academia Sinica, International 
Journal of Computational Linguistics and Chinese 
Language Processing, Vol. 2, No. 1, Feb. 1997. (in 
Chinese) 
Huang, Chu-Ren, Chiyo Hotani, Tzu-Yi Kuo, I-Li Su, 
and Shu-kai Hsieh. 2008. WordNet-anchored Com-
parison of Chinese-Japanese kanji Word. Proceed-
ings of the 4th Global WordNet Conference. Szeged, 
Hungary. January 22-25 
Table 9.Jwd Correspondence to??(mouth) Concep-
tual Extension? Graph (?(mouth), Basic Concept: the 
body part which used mainly in Language & Food ) 
Pustejovsky, James. 1995. The Generative Lexicon, 
The MIT Press. 
Xyu, Sheng. 121/2004. 'The Explanation of Words 
and the Parsing of Characters' ShuoWenJieZi. This 
edition. Beijing: ZhongHua. 
 
5. Conclusion 
 
Appendix A. The Dimension of ?? 
(mouth) Conceptual extension?. 
The result of the experiment comparing the Japa-
nese and Chinese words is to see their form-
meaning similarities.Since the Japanese and the 
Chinese writing system (kanji) and its semantic 
meanings are near-related, analyzing such rela-
tion may contribute to the future research related 
to Hantology.In this paper, we examine and ana-
lyze the form of kanji and the semantic relations 
between Japanese and Chinese.This paper de-
scribes the structure of Hantology which is a cha-
racter-based bilingual ontology for Chinese and 
Japanese. Hantology represents orthographic 
forms, pronunciations, senses, variants, lexicali-
zation, the spread and relation between Chinese 
characters and Japanese kanji. The results show 
Hantology has two implications. First, Hantology 
provides the resources needed by Chinese lan-
guage processing for computers.Second, Hantol-
ogy provides a platform to analyze the variation 
and comparison of Chinese characters and kanji 
use. 
 
References 
Chou, Ya-Min and Chu-Ren Huang. 2005. Hantology: 
An Ontology based on Conventionalized Conceptua-
lization. Proceedings of the Fourth OntoLex Work-
shop. A workshop held in conjunction with the 
second IJCNLP. October 15. Jeju, Korea. 
Chou,Ya-Min, Shu-Kai Hsieh and Chu-Ren Huang. 
2007. HanziGrid: Toward a knowledge infrastruc-
ture for Chinese characters-based cultures. In: Ishida, 
T., Fussell, S.R., Vossen, P.T.J.M. Eds.: Intercultural 
Collaboration I. Lecture Notes in Computer Science, 
State-of-the-Art Survey. Springer-Verlag 
 
54
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 45?53,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Text-driven Rule-based System for Emotion Cause Detection 
 
Sophia Yat Mei Lee? Ying Chen?* Chu-Ren Huang?? 
?Department of Chinese and Bilingual Studies  
The Hong Kong Polytechnic University 
*Department of Computer Engineering 
China Agriculture University 
?Institute of Linguistics  
Academia Sinica 
{sophiaym, chenying3176, churenhuang}@gmail.com 
 
 
 
  
Abstract 
Emotion cause detection is a new research area 
in emotion processing even though most theo-
ries of emotion treat recognition of a triggering 
cause event as an integral part of emotion. As a 
first step towards fully automatic inference of 
cause-emotion correlation, we propose a text-
driven, rule-based approach to emotion cause 
detection in this paper. First of all, a Chinese 
emotion cause annotated corpus is constructed 
based on our proposed annotation scheme. By 
analyzing the corpus data, we identify seven 
groups of linguistic cues and generalize two 
sets of linguistic rules for detection of emotion 
causes. With the linguistic rules, we then de-
velop a rule-based system for emotion cause 
detection. In addition, we propose an evaluation 
scheme with two phases for performance as-
sessment. Experiments show that our system 
achieves a promising performance for cause oc-
currence detection as well as cause event detec-
tion. The current study should lay the ground 
for future research on the inferences of implicit 
information and the discovery of new informa-
tion based on cause-event relation. 
1 Introduction 
Text-based emotion processing has attracted plenty 
of attention in NLP. Most research has focused on 
the emotion detection and classification by 
identifying the emotion types, for instances 
happiness and sadness, for a given sentence or 
document (Alm 2005, Mihalcea and Liu 2006, 
Tokuhisa et al 2008). However, on top of this 
surface level information, deeper level information 
regarding emotions, such as the experiencer, cause, 
and result of an emotion, needs to be extracted and 
analyzed for real world applications (Alm 2009). 
In this paper, we aim at mining one of the crucial 
deep level types of information, i.e. emotion cause, 
which provides useful information for applications 
ranging from economic forecasting, public opinion 
mining, to product design. Emotion cause detection 
is a new research area in emotion processing. In 
emotion processing, the cause event and emotion 
correlation is a fertile ground for extraction and 
entailment of new information. As a first step 
towards fully automatic inference of cause-
emotion correlation, we propose a text-driven, 
rule-based approach to emotion cause detection in 
this paper.  
 Most theories of emotion treat recognition of 
a triggering cause event as an integral part of 
emotional experience (Descartes 1649, James 1884, 
Plutchik 1962, Wierzbicka 1999). In this study, 
cause events refer to the explicitly expressed 
arguments or events that evoke the presence of the 
corresponding emotions. They are usually 
expressed by means of propositions, 
nominalizations, and nominals. For example, ?they 
like it? is the cause event of the emotion happiness 
in the sentence ?I was very happy that they like it?. 
Note that we only take into account emotions that 
are explicitly expressed, which are usually 
presented by emotion keywords, e.g. ?This 
surprises me?. Implicit emotions that require 
inference or connotation are not processed in this 
first study. In this study, we first build a Chinese 
emotion cause annotated corpus with five primary 
emotions, i.e. happiness, sadness, anger, fear, and 
surprise. We then examine various linguistic cues 
which help detect emotion cause events: the 
position of cause event and experiencer relative to 
the emotion keyword, causative verbs (e.g. rang4 
?to cause?), action verbs (e.g. xiang3dao4 ?to think 
about?), epistemic markers (e.g. kan4jian4 ?to 
see?), conjunctions (e.g. yin1wei4 ?because?), and 
prepositions (e.g. dui4yu2 ?for?). With the help of 
45
these cues, a list of linguistic rules is generalized. 
Based on the linguistic rules, we develop a rule-
based system for emotion cause detection. 
Experiments show that such a rule-based system 
performs promisingly well. We believe that the 
current study should lay the ground for future 
research on inferences and discovery of new 
information based on cause-event relation, such as 
detection of implicit emotion or cause, as well as 
prediction of public opinion based on cause events, 
etc.  
The paper is organized as follows. Section 2 
discusses the related work on various aspects of 
emotion analysis. Section 3 describes the construc-
tion of the emotion cause corpus. Section 4 
presents our rule-based system for emotion cause 
detection. Section 5 describes its evaluation and 
performance. Section 6 highlights our main contri-
butions. 
2 Previous Work 
We discuss previous studies on emotion analysis in 
this section, and underline fundamental yet unre-
solved issues. We survey the previous attempts on 
textual emotion processing and how the present 
study differs.  
2.1 Emotion Classes 
Various approaches to emotion classification were 
proposed in different fields, such as philosophy 
(Spinoza 1675, James 1884), biology (Darwin 
1859, linguistics (Wierzbicka 1999, K?vecses 
2000), neuropsychology (Plutchik 1962, Turner 
1996), and computer science (Ortony et al 1988, 
Picard 1995), as well as varying from language to 
language. Although there is lack of agreement 
among different theories on emotion classification, 
a small number of primary emotions are commonly 
assumed. Other emotions are secondary emotions 
which are the mixtures of the primary emotions.  
Researchers have attempted to propose the list 
of primary emotions, varying from two to ten basic 
emotions (Ekman 1984, Plutchik 1980, Turner 
2000). Fear and anger appear on every list, whe-
reas happiness and sadness appear on most of the 
lists. These four emotions, i.e. fear, anger, happi-
ness, and sadness, are the most common primary 
emotions. Other less common primary emotions 
are surprise, disgust, shame, distress, guilt, interest, 
pain, and acceptance.  
In this study, we adopt Turner?s emotion clas-
sification (2000), which identifies five primary 
emotions, namely happiness, sadness, fear, anger, 
and surprise. Turner?s list consists of primary emo-
tions agreed upon by most previous work. 
2.2 Emotion Processing in Text 
Textual emotion processing is still in its early stag-
es in NLP. Most of the previous works focus on 
emotion classification given a known emotion con-
text such as a sentence or a document using either 
rule-based (Masum et al 2007, Chaumartin 2007) 
or statistical approaches (Mihalcea and Liu 2005, 
Kozareva et al 2007). However, the performance 
is far from satisfactory. What is more, many basic 
issues remain unresolved, for instances, the rela-
tionships among emotions, emotion type selection, 
etc. Tokuhisa et al (2008) was the first to explore 
both the issues of emotion detection and classifica-
tion. It created a Japanese emotion-provoking 
event corpus for an emotion classification task us-
ing an unsupervised approach. However, only 
49.4% of cases were correctly labeled. Chen et al 
(2009) developed two cognitive-based Chinese 
emotion corpora using a semi-unsupervised ap-
proach, i.e. an emotion-sentence (sentences con-
taining emotions) corpus and a neutral-sentence 
(sentences containing no emotion) corpus. They 
showed that studies based on the emotion-sentence 
corpus (~70%) outperform previous corpora. 
Little research, if not none, has been done to 
examine the interactions between emotions and the 
corresponding cause events, which may make a 
great step towards an effective emotion classifica-
tion model. The lack of research on cause events 
restricted current emotion analysis to simple classi-
ficatory work without exploring the potentials of 
the rich applications of putting emotion ?in con-
text?. In fact, emotions can be invoked by percep-
tions of external events and in turn trigger 
reactions. The ability to detect implicit invoking 
causes as well as predict actual reactions will add 
rich dimensions to emotion analysis and lead to 
further research on event computing.  
3 Emotion Cause Corpus  
This section briefly describes how the emotion 
cause corpus is constructed. We first explain what 
46
an emotion cause is and discuss how emotion 
cause is linguistically expressed in Chinese. We 
then describe the corpus data and the annotation 
scheme. For more detailed discussion on the con-
struction of the emotion cause corpus, please refer 
to Lee et al (2010). 
3.1 Cause Events 
Following Talmy (2000), the cause of an emotion 
should be an event itself. In this work, it is called a 
cause event. By cause event, we do not necessarily 
mean the actual trigger of the emotion or what 
leads to the emotion. Rather, it refers to the imme-
diate cause of the emotion, which can be the actual 
trigger event or the perception of the trigger event. 
Adapting TimeML annotation scheme (Saur? et al 
2004), events refer to situations that happen or oc-
cur. In this study, cause events specifically refer to 
the explicitly expressed arguments or events that 
are highly linked with the presence of the corres-
ponding emotions. In Lee et al?s (2010) corpus, 
cause events are categorized into two types: verbal 
events and nominal events. Verbal events refer to 
events that involve verbs (i.e. propositions and 
nominalizations), whereas nominal events are 
simply nouns (i.e. nominals). Some examples of 
cause event types are given in bold face in (1)-(6). 
 
(1) Zhe4-DET tou2-CL niu2-cattle de-POSS zhu3ren2-owner, 
yan3kan4-see zi4ji3-oneself de-POSS niu2-cattle 
re3chu1-cause huo4-trouble lai2-come le-ASP, 
fei1chang2-very hai4pa4-frighten, jiu4-then ba3-PREP 
zhe4-DET tou2-CL niu2-cattle di1jia4-low price 
mai4chu1-sell.  
 ?The owner was frightened to see that his cattle 
caused troubles, so he sold it at a low price.? 
 
(2) Mei2-not  xiang3dao4-think  ta1-3.SG.F  shuo1-say  de-
POSS  dou1-all shi4-is  zhen1-true  hua4-word,  rang4-
lead  ta1-3.SG.M  zhen4jing1-shocked  bu4yi3-very. 
 ?He was shocked that what she said was the 
truth.? 
 
(3) Ta1-3.SG.M  dui4-for  zhe4-DET  ge4-CL  chong1man3-
full of  nong2hou4-dense  ai4yi4-love  de-DE xiang3fa3-
idea  gao1xing4-happy de-DE  shou3wu3zu2dao3-flourish. 
 ?He was very happy about this loving idea.? 
 
(4) Zhe4-DET ci4-CL yan3chu1-performance de-POSS 
jing1zhi4-exquisite dao4shi4-is ling4-cause wo3-1.SG 
shi2fen1-very jing1ya4-surprise.  
 ?I was very surprised by this exquisite perfor-
mance.?   
 
(5) Ni2ao4-Leo de-POSS hua4-word hen3-very ling4-make 
kai3luo4lin2-Caroline shang1xin1-sad. 
 ?Caroline was very saddened by Leo?s words.? 
 
(6) Dui4yu2-for wei4lai2-future, lao3shi2shuo1-frankly wo3-
1.SG hen3-very hai4pa4-scared.  
 ?Frankly, I am very scared about the future.? 
 
The causes in (1) and (2) are propositional causes, 
which indicate the actual events involved in caus-
ing the emotions. The ones in (3) and (4) are no-
minalized causes, whereas (5) and (6) involve 
nominal causes  
3.2 Corpus Data and Annotation Scheme 
Based on the list of 91 Chinese primary emotion 
keywords identified in Chen et al (2009), we ex-
tract 6,058 instances of sentences by keyword 
matching from the Sinica Corpus 1 , which is a 
tagged balanced corpus of Mandarin Chinese con-
taining a total of ten million words. Each instance 
contains the focus sentence with the emotion key-
word ?<FocusSentence>? plus the sentence before 
?<PrefixSentence>? and after ?<SuffixSentence>? 
it. The extracted instances include all primary emo-
tion keywords occurring in the Sinica Corpus ex-
cept for the emotion class happiness, as the 
keywords of happiness exceptionally outnumber 
other emotion classes. In order to balance the 
number of each emotion class, we set the upper 
limit at about 1,600 instances for each primary 
emotion.  
Note that the presence of emotion keywords 
does not necessarily convey emotional information 
due to different possible reasons such as negative 
polarity and sense ambiguity. Hence, by manual 
inspection, we remove instances that 1) are non-
emotional; 2) contain highly ambiguous emotion 
keywords, such as ru2yi4 ?wish-fulfilled?, hai4xiu1 
?to be shy?, wei2nan2 ?to feel awkward?, from the 
corpus. After the removal, the remaining instances 
in the emotion cause corpus is 5,629. Among the 
remaining instances, we also remove the emotion 
keywords in which the instances do not express 
that particular emotion and yet are emotional. The 
total emotion keywords in the corpus is 5,958. 
For each emotional instance, two annotators 
manually annotate cause events of each keyword. 
Since more than one emotion can be present in an 
                                                           
1
 http://dbo.sinica.edu.tw/SinicaCorpus/ 
47
instance, the emotion keywords are tagged as 
<emotionword id=0>, <emotionword id=1>, and 
so on.  
 
573 Y 0/shang1 xin1/Sadness  
<PrefixSentence> ma1ma ye3 wen4 le ling2 ju1, dan4 shi4 
mei2 you3 ren4 jian4 dao4 xiao3 bai2. </PrefixSentence> 
<FocusSentence>wei4 le [*01n] zhe4 jian4 shi4 [*02n] , wo3 
ceng2 <emotionword id=0>shang1 xin1</emotionword> le 
hou2 jiu3,dan4 ye3 wu2 ji3 yu4 shi4. </FocusSentence> <Suf-
fixSentence>mei3 dang1 zai4 kan4 dao4 bai2 se4 de qi4 gou3, 
bu4 jin4 hui4 xiang3 qi3 xiao3 bai2 </SuffixSentence> 
 
573 Y 0/to be sad/Sadness  
<PrefixSentence> Mom also asked the neighbors, but no one 
ever saw Little White. </PrefixSentence> <FocusSentence> 
Because of [*01n] this [*02n] , I have been feeling very <emo-
tionword id=0> sad </emotionword> for a long time, but this 
did not help.  </FocusSentence> <SuffixSentence> Whenever 
[I] see a white stray dog, [I] cannot help thinking of Little 
White. </SuffixSentence> 
Figure 1: An Example of Cause Event Annotation 
 
Figure 1 shows an example of annotated emotional 
sentences in corpus, presented as pinyin with tones, 
followed by an English translation. For an emotion 
keyword tagged as <id=0>, [*01n] marks the be-
ginning of its cause event while [*02n] marks the 
end. The ?0? shows which index of emotion key-
word it refers to, ?1? marks the beginning of the 
cause event, ?2? marks the end, and ?n? indicates 
that the cause is a nominal event. For an emotion 
keyword tagged as <id=1>, [*11e] marks the be-
ginning of the cause event while [*12e] marks the 
end, in which ?e? refers to a verbal event, i.e. ei-
ther a proposition or a nominalization. An emotion 
keyword can sometimes be associated with more 
than one cause, in which case both causes are 
marked. The emotional sentences containing no 
explicitly expressed cause event remain as they are. 
The actual number of extracted instances of 
each emotion class to be analyzed, the positive 
emotional instances, and the instances with cause 
events are presented in Table 1.  
 
Table 1: Summary of Corpus Data 
Emotions No. of Instances Extracted Emotional With Causes 
Happiness 1,644 1,327 1,132 (85%) 
Sadness 901 616 468 (76%) 
Fear 897 689 567 (82%) 
Anger 1,175 847 629 (74%) 
Surprise 1,341 781 664 (85%) 
Total 5,958 4,260 (72%) 3,460 (81%) 
We can see that 72% of the extracted instances ex-
press emotions, and 81% of the emotional in-
stances have a cause. The corpus contains 
happiness (1,327) instances the most and sadness 
(616) the least. For each emotion type, about 81% 
of the emotional sentences, on average, are consi-
dered as containing a cause event, with surprise 
the highest (85%) and anger the lowest (73%). 
This indicates that an emotion mostly occurs with 
the cause event explicitly expressed in the text, 
which confirms the prominent role of cause events 
in expressing an emotion. 
4 A Rule-based System for Cause Detec-
tion  
4.1 Linguistic Analysis of Emotion Causes 
By analyzing the corpus data, we examine the 
correlations between emotions and cause events in 
terms of various linguistic cues: the position of 
cause event, verbs, epistemic markers, 
conjunctions, and prepositions. We hypothesize 
that these cues will facilitate the detection of 
emotion cause events.  
 First, we calculate the distribution of cause 
event types of each emotion as well as the position 
of cause events relative to emotion keywords and 
experiencers. The total number of emotional 
instances regarding each emotion is given in Table 
2.  
 
Table 2: Cause Event Position of Each Emotion 
Emotions Cause Type (%) Cause Position (%) 
Event Nominal Left Right 
Happiness 76 6 74 29 
Sadness 67 8 80 20 
Fear 68 13 52 48 
Anger 55 18 71 26 
Surprise 73 12 59 41 
 
Table 2 suggests that emotion cause events tend to 
be expressed by verbal events than nominal events 
and that cause events tend to occur at the position 
to the left of the emotion keyword, with fear (52%) 
being no preference. This may be attributed to the 
fact that fear can be triggered by either factive or 
potential causes, which is rare for other primary 
emotions. For fear, factive causes tend to take the 
left position whereas potential causes tend to take 
the right position. 
48
 Second, we identify seven groups of 
linguistic cues that are highly collocated with 
cause events (Lee et al 2010), as shown in Table 3.  
 
Table 3: Seven Groups of Linguistic Cues 
Group Cue Words 
I ?to cause?: rang4, ling4, shi3  
II ?to think about?: e.g. xiang3 dao4, xiang3 qi3, yi1 
xiang3  
?to talk about?: e.g. shuo1dao4, jiang3dao4, tan2dao4  
III ?to say?: e.g. shuo1, dao4 
IV ?to see?: e.g. kan4dao4, kan4jian4, jian4dao4  
?to hear?: e.g. ting1dao4, ting1 shuo1 
?to know?: e.g. zhi1dao4, de2zhi1, fa1xian4 
?to exist?: you3 
V ?for? as in ?I will do this for you?: wei4, wei4le 
?for? as in ?He is too old for the job?: dui4, dui4yu2 
VI ?because?: yin1, yin1wei4, you2yu2 
VII ?is?: deshi4 
?at?: yu2 
?can?: neng2 
 
Group I includes three common causative verbs, 
and Group II a list of verbs of thinking and talking. 
Group III is a list of say verbs. Group IV includes 
four types of epistemic markers which are usually 
verbs marking the cognitive awareness of emotion 
in the complement position (Lee and Huang 2009). 
The epistemic markers include verbs of seeing, 
hearing, knowing, and existing. Group V covers 
some prepositions which all roughly mean ?for?. 
Group VI contains the conjunctions that explicitly 
mark the emotion cause. Group VII includes other 
linguistic cues that do not fall into any of the six 
groups. Each group of linguistic cues serves as an 
indicator marking the cause events in different 
structures of emotional constructions, in which 
Group I specifically marks the end of the cause 
events while the other six groups marks the 
beginning of the cause events. 
4.2 Linguistic Rules for Cause Detection 
We examine 100 emotional sentences of each emo-
tion keyword randomly extracted from the devel-
opment data, and generalize some rules for 
identifying the cause of the corresponding emotion 
verb (Lee 2010). The cause is considered as a 
proposition. It is generally assumed that a proposi-
tion has a verb which optionally takes a noun oc-
curring before it as the subject and a noun after it 
as the object. However, a cause can also be ex-
pressed as a nominal. In other words, both the pre-
dicate and the two arguments are optional provided 
that at least one of them is present.  
We also manually identify the position of the 
experiencer as well as the linguistic cues discussed 
in Section 4.1. All these components may occur in 
the clause containing the emotion verb (focus 
clause), the clause before the focus clause, or the 
clause after the focus clause. The abbreviations 
used in the rules are given as follows:  
 
C = Cause event 
E = Experiencer 
K = Keyword/emotion verb 
B = Clause before the focus clause 
F = Focus clause/the clause containing the emotion verb 
A = Clause after the focus clause 
 
For illustration, an example of the rule description 
is given in Rule 1. 
 
Rule 1: 
i) C(B/F) + I(F) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) before I in F/B  
 
Rule 1 indicates that the experiencer (E) appears to 
be the nearest Na (common noun)/ Nb (proper 
noun)/ Nc (place noun)/ Nh (pronoun) after Group 
I cue words in the focus clause (F), while, at the 
same time, it comes before the keyword (K). Be-
sides, the cause (C) comes before Group I cue 
words. We simplify the proposition as a structure 
of (N)+(V)+(N), which is very likely to contain the 
cause event. Theoretically, in identifying C, we 
should first look for the nearest verb occurring be-
fore Group I cue words in the focus sentence (F) or 
the clause before the focus clause (B), and consider 
this verb as an anchor. From this verb, we search to 
the left for the nearest noun, and consider it as the 
subject; we then search to the right for the nearest 
noun until the presence of the cue words, and con-
sider it as the object. The detected subject, verb, 
and object form the cause event. In most cases, the 
experiencer is covertly expressed. It is, however, 
difficult to detect such causes in practice as causes 
may contain no verbs, and the two arguments are 
optional. Therefore, we take the clause instead of 
the structure of (N)+(V)+(N) as the actual cause. 
Examples are given in (7) and (8). For both sen-
tences, the clause that comes before the cue word 
is taken as the cause event of the emotion in ques-
tion. 
 
49
(7) [C yi1 la1 ke4 xi4 jun1 wu3 qi4 de bao4 guang1], [I 
shi3] [E lian2 he2 guo2 da4 wei2][K zhen4 jing1] . 
?[C The revealing of Iraq?s secret bacteriological 
weapons] [K shocked] [E the United Nations].? 
 
(8) [C heng2 shan1 jin1 tian1 ti2 chu1 ci2 cheng2], [I 
ling4] [E da4 ban3] zhi4 wei2 [K fen4 nu4] ? 
?[C Yokoyama submitted his resignation today], [K 
angered] [E the people of Osaka].? 
 
Table 4 summarizes the generalized rules for de-
tecting the cause events of the five primary emo-
tions in Chinese. We identify two sets of rules: 1) 
the specific rules that apply to all emotional in-
stances (i.e. rules 1-13); 2) the general rules that 
apply to the emotional instances in which causes 
are not found after applying the specific set of 
rules (i.e. rules 14 and 15).  
 
Table 4: Linguistic Rules for Cause Detection 
No. Rules 
1 i) C(B/F) + I(F) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) before I in F/B 
2 i) E(B/F) + II/IV/V/VI(B/F) + C(B/F) + K(F) 
ii) E=the nearest Na/Nb/Nc/Nh before II/IV/V/VI in B/F 
iii) C = the nearest (N)+(V)+(N) before K in F 
3 i) II/IV/V/VI (B) + C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after II/IV/V/VI in B 
4 i) E(B/F) + K(F) + IV/VII(F) + C(F/A) 
ii) E = a: the nearest Na/Nb/Nc/Nh before K in F; b: the 
first Na/Nb/Nc/Nh in B 
iii) C = the nearest (N)+(V)+(N) after IV/VII in F/A 
5 i) E(F)+K(F)+VI(A)+C(A) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after VI in A 
6 i) I(F) + E(F) + K(F) + C(F/A) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) after K in F or A 
7 i) E(B/F) + yue4 C yue4 K ?the more C the more K? (F)  
ii) E = the nearest Na/Nb/Nc/Nh before the first yue4 in 
B/F 
iii) C = the V in between the two yue4?s in F 
8 i) E(F) + K(F) + C(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after K in F 
9 i) E(F) + IV(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before IV in F 
iii) C = IV+(an aspectual marker) in F 
10 i) K(F) + E(F) + de ?possession?(F) + C(F) 
ii) E = the nearest Na/Nb/Nc/Nh after K in F 
iii) C = the nearest (N)+V+(N)+?+N after de in F 
11 i) C(F) + K(F) + E(F) 
ii) E = the nearest Na/Nb/Nc/Nh after K in F 
iii) C = the nearest (N)+(V)+(N) before K in F 
12 i) E(B) + K(B) + III (B) + C(F)  
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after III in F 
13 i) III(B) + C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after III in B 
14 i) C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) before K in B  
15 i) E(B) +C(B) + K(F)  
ii) E = the first Na/Nb/Nc/Nh in B 
iii) C = the nearest (N)+(V)+(N) before K in B 
 
 
Constraints are set to each rule to filter out incor-
rect causes. For instances, in Rule 1, the emotion 
keyword cannot be followed by the words de ?pos-
session?/ deshi4 ?is that?/ shi4 ?is? since it is very 
likely to have the cause event occurring after such 
words; in Rule 2, the cue word in III yuo3 ?to ex-
ist? is excluded as it causes noises; whereas for 
Rule 4, it only applies to instances containing 
keywords of happiness, fear, and surprise. 
5 Experiment  
5.1 Evaluation Metrics 
An evaluation scheme is designed to assess the 
ability to extract the cause of an emotion in context. 
We specifically look into two phases of the per-
formance of such a cause recognition system. 
Phase 1 assesses the detection of an emotion co-
occurrence with a cause; Phrase 2 evaluates the 
recognition of the cause texts for an emotion. 
 
Overall Evaluation:  
The definitions of related metrics are presented in 
Figure 2. For each emotion in a sentence, if neither 
the gold-standard file nor the system file has a 
cause, both precision and recall score 1; otherwise, 
precision and recall are calculated by the scoring 
method ScoreForTwoListOfCauses. As an emotion 
may have more than one cause, ScoreForTwoLis-
tOfCauses calculates the overlap scores between 
two lists of cause texts. Since emotion cause rec-
ognition is rather complicated, two relaxed string 
match scoring methods are selected to compare 
two cause texts, ScoreForTwoStrings: Relaxed 
Match 1 uses the minimal overlap between the 
gold-standard cause and the system cause. The sys-
tem cause is considered as correct provided that 
there is at least one overlapping Chinese character; 
Relaxed Match 2 is more rigid which takes into 
account the overlap text length during scoring. 
 
50
Phase 1: The Detection of Cause Occurrence 
The detection of cause occurrence is considered a 
preliminary task for emotion cause recognition and 
is compounded by the fact that neutral sentences 
are difficult to detect, as observed in Tokuhisa et al 
(2008). For Phase 1, each emotion keyword in a 
sentence has a binary tag: Y (i.e. with a cause) or 
N (without a cause). Similar to other NLP tasks, 
we adopt the common evaluation metrics, i.e. accu-
racy, precision, recall, and F score. 
 
Phase 2: The Detection of Causes 
The evaluation in Phase 2 is limited to the emotion 
keywords with a cause either in the gold-standard 
file or in the system file. The performance is calcu-
lated as in Overall Evaluation scheme. 
 
 
 
5.2 Results and Discussion 
We use 80% sentences as the development data, 
and 20% as the test data. The baseline is designed 
as follows: find a verb to the left of the keyword in 
question, and consider the clause containing the 
verb as a cause.  
Table 5 shows the performances of the overall 
evaluation. We find that the overall performances 
of our system have significantly improved using 
Relaxed Match 1 and Relaxed Match 2 by 19% 
and 19% respectively. Although the overall per-
formance of our system (47.95% F-score for Re-
laxed Match 1 and 41.67% for Relaxed Match 2) is 
not yet very high, it marks a good start for emotion 
 
Overall evaluation formula: 
 Precision (GF, SF) =  
ScoreForTwoListOfCauses ( , ) 
1 
j j
j
j
i i
i i
S GF em S
S SF em S
SCList GCList
?
?
?
?
? ?
? ?
 
 Recall (GF, SF) =  
ScoreForTwoListOfCauses ( , ) 
1 
j j
j
j
i i
i i
S GF em S
S GF em S
SCList GCList
?
?
?
?
? ?
? ?  
Where GF and SF are the gold-standard cause file and system cause file respectively, and both files include 
the same sentences. Si is a sentence, and emj is an emotion keyword in Si. GCListj and SCListj are the lists 
of the gold-standard causes and system causes respectively for the emotion keyword emj.  
 
ScoreForTwoListOfCauses (GCList, SCList):  
 If there is no cause in either GCList or SCList: Precision = 1; Recall = 1 
     Else: 
        Precision =  
( , )
| |
i j
GCi GCListSCj SCList
Max ScoreTwoStrings GC SC
SCList
?
?
?
 
Recall     =  
( , )
| |
i j
SCj SCListGCi GCList
Max ScoreTwoStrings GC SC
GCList
?
?
?
 
 
ScoreForTwoStrings(GC, SC): GC is a gold-standard cause text, and SC is a system cause text. 
Relaxed Match 1:  If overlap existing, both precision and recall are 1; Else, both are 0. 
Relaxed Match 2:    Precision (GC, SC) = ( )
( )
Len overlapText
Len SC
  
Recall (GC, SC)   = ( )
( )
Len overlapText
Len GC
  
Figure 2: The Definitions of Metrics for Cause Detection 
 
51
 Relaxed Match 1 Relaxed Match 2 
 Precision Recall F-score Precision Recall F-score 
Baseline 25.94 31.99  28.65 17.77 29.62  22.21 
Our System 45.06  51.24 47.95 39.89 43.63 41.67 
Table 5: The Overall Performances 
 
 Baseline Rule-based System 
Emotions Precision Recall F-score Precision Recall F-score 
With causes 99.42 79.74 88.50 96.871 80.851 88.139 
Without causes 4.39 66.67 8.23 13.158 52.632 21.053 
Table 7: The Detailed Performances in Phase 1 
 
 Relaxed Match 1 Relaxed Match 2 
 Precision Recall F-score Precision Recall F-score 
Baseline 25.37 39.28 30.83 17.09 36.29  23.24 
Our System 44.64 61.30  51.66 39.18 51.68 44.57 
Table 8: The Detailed Performances in Phase 2 
 
 Baseline Rule-based System 
Accuracy 79.56 79.38 
Table 6: The Overall Accuracy in Phase 1 
 
cause detection and extraction. 
Table 6 and 7 show the performances of the 
baseline and our rule-based system in Phase 1. Ta-
ble 6 shows the overall accuracy, and Table 7 
shows the detailed performances. In Table 6, we 
find that our system and the baseline have similar 
high accuracy scores. Yet Table 7 shows that both 
systems achieve a high performance for emotions 
with a cause, but much worse for emotions without 
a cause. It is important to note that even though the 
naive baseline system has comparably high per-
formance with our rule-based system in judging 
whether there is a cause in context, this result is 
biased by two facts. First, as the corpus contains 
more than 80% of sentences with emotion, a sys-
tem which is biased toward detecting a cause, such 
as the baseline system, naturally performs well. In 
addition, once the actual cause is examined, we can 
see that the baseline actually detects a lot of false 
positives in the sense that the cause it identifies is 
only correct in 4.39%. Our rule-based system 
shows great promise in being able to deal with 
neutral sentences effectively and being able to 
detect the correct cause at least three times more 
often than the baseline.  
Table 8 shows the performances in Phase 2. 
Comparing to the baseline, we find that our rules 
improve the performance of cause recognition us-
ing Relaxed Match 1 and 2 scoring by 21% and 
21% respectively. On the one hand, the 7% gap in 
F-score between Relaxed Match 1 and 2 also indi-
cates that our rules can effectively locate the clause 
of a cause. On the other hand, the rather low per-
formances of the baseline show that most causes 
recognized by the baseline are wrong although the 
baseline effectively detects the cause occurrence, 
as indicated in Table 7. In addition, we check the 
accuracy (precision) and contribution (recall) of 
each rule. In descending order, the top four accu-
rate rules are: Rules 7, 10, 11, and 1; and the top 
four contributive rules are: Rules 2, 15, 14, and 3.  
6 Conclusion  
Emotion processing has been a great challenge in 
NLP. Given the fact that an emotion is often trig-
gered by cause events and that cause events are 
integral parts of emotion, we propose a linguistic-
driven rule-based system for emotion cause detec-
tion, which is proven to be effective. In particular, 
we construct a Chinese emotion cause corpus an-
notated with emotions and the corresponding cause 
events. Since manual detection of cause events is 
labor-intensive and time-consuming, we intend to 
use the emotion cause corpus to produce automatic 
extraction system for emotion cause events with 
machine learning methods. We believe that our 
rule-based system is useful for many real world 
applications. For instance, the information regard-
ing causal relations of emotions is important for 
product design, political evaluation, etc. Such a 
system also shed light on emotion processing as 
the detected emotion cause events can serve as 
clues for the identification of implicit emotions.  
52
References  
Alm, C. O., D. Roth and R. Sproat. 2005. Emotions 
from Text: Machine Learning for Text-based Emo-
tion Prediction. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, Canada, 6-8 October, 
pp. 579-586. 
Alm, C. O. 2009. Affect in Text and Speech. VDM 
Verlag: Saarbr?cken. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. A Cog-
nitive-based Annotation System for Emotion Com-
puting. In Proceedings of the Third Linguistic 
Annotation Workshop (The LAW III), ACL 2009. 
Chaumartin, F.-R. 2007. A Knowledgebased System for 
Headline Sentiment Tagging. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions. 
Darwin, C. 1859. On the Origin of Species by Means of 
Natural Selection. London: John Murray. 
Descartes, R. 1649. The Passions of the Soul. In J. Cot-
tingham et al (Eds), The Philosophical Writings of 
Descartes. Vol. 1, 325-404. 
Ekman, P. 1984. Expression and the Nature of Emotion. 
In Scherer, K. and P. Ekman (Eds.), Approaches to 
Emotion. Hillsdale, N.J.: Lawrence Erlbaum. 319-
343. 
James, W. 1884. What is an Emotion? Mind, 9(34):188?
205. 
Kozareva, Z., B. Navarro, S. Vazquez, and A. Nibtoyo. 
2007. UA-ZBSA: A Headline Emotion Classifica-
tion through Web Information. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions.  
K?vecses, Z. 2000. Metaphor and Emotion: Language, 
Culture and Body in Human Feeling. Cambridge: 
Cambridge University Press. 
Lee, S. Y. M. 2010. A Linguistic Approach towards 
Emotion Detection and Classification. Ph.D. Disser-
tation. Hong Kong. 
Lee, S. Y. M., C. Ying, and C.-R. Huang. 2010. Emo-
tion Cause Events: Corpus Construction and Analy-
sis. In Proceedings of The Seventh International 
Conference on Language Resources and Evaluation 
(LREC 2010). May 19-21. Malta. 
Lee, S. Y. M. and C.-R. Huang. 2009. Explicit Epistem-
ic Markup of Causes in Emotion Constructions. The 
Fifth International Conference on Contemporary 
Chinese Grammar. Hong Kong. November 27 - De-
cember 1. 
Masum, S. M., H. Prendinger, and M. Ishizuka. 2007. 
Emotion Sensitive News Agent: An Approach To-
wards User Centric Emotion Sensing from the News. 
In Proceedings of the IEEE/WIC/ACM International 
Conference on Web Intelligence. 
Mihalcea, R. and H. Liu. 2006. A Corpus-based Ap-
proach to Finding Happiness. In Proceedings of the 
AAAI Spring Symposium on Computational Ap-
proaches to Weblogs.  
Ortony A., G. L. Clone, and A. Collins. 1988. The Cog-
nitive Structure of Emotions. New York: Cambridge 
University Press. 
Picard, R.W. 1995. Affective Computing. Cambridge. 
MA: The MIT Press. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Saur?, R., J. Littman, R. Knippen, R. Gaizauskas, A. 
Setzer, and J. Pustejovsky. 2004. TimeML Annota-
tion Guidelines. http://www.timeml.org. 
Spinoza, B. 1985. Ethics. In E. Curley, The Collected 
Works of Spinoza. Princeton, N.J.: Princeton Univer-
sity Press. Vol 1. 
Talmy, L. 2000. Toward a Cognitive Semantics. Vol. 
1and 2. Cambridge: MIT Press. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. Emo-
tion Classification Using Massive Examples Ex-
tracted from the Web. In Proceedings of COLING.   
Turner, J. H. 1996. The Evolution of Emotions in Hu-
mans: A Darwinian-Durkheimian Analysis. Journal 
for the Theory of Social Behaviour, 26:1-34. 
Turner, J. H. 2000. On the Origins of Human Emotions: 
A Sociological Inquiry into the Evolution of Human 
Affect. California: Stanford University Press. 
Wierzbicka, A. 1999. Emotions Across Languages and 
Cultures: Diversity and Universals. Cambridge: 
Cambridge University Press. 
 
53
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 10?17,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Evidentiality for Text Trustworthiness Detection 
 
 
 
Qi Su1, 2, Chu-Ren Huang and Helen Kai-yun Chen 
1Depart of Chinese & Bilingual Studies, The Hong Kong Polytechnic University 
2Key Laboratory of Computational Linguistics, Peking University 
sukia@pku.edu.cn, {helenkychen, churen.huang}@gmail.com 
 
  
 
Abstract 
 
Evidentiality is the linguistic representation of 
the nature of evidence for a statement. In 
other words, it is the linguistically encoded 
evidence for the trustworthiness of a state-
ment. In this paper, we aim to explore how 
linguistically encoded information of eviden-
tiality can contribute to the prediction of 
trustworthiness in natural language processing 
(NLP). We propose to incorporate evidential-
ity into a framework of machine learning 
based text classification. We first construct a 
taxonomy of evidentials. Then experiments 
involving collaborative question answering 
(CQA) are designed and implemented using 
this taxonomy. The experimental results con-
firm that evidentiality is an important clue for 
text trustworthiness detection. With the bi-
narized vector setting, evidential based text 
representation model has considerably per-
formaned better than both the bag-of-word 
model and the content word based model. 
Most crucially, we show that the best trust-
worthiness detection result is achieved when 
evidentiality is incorporated in a linguistically 
sophisticated model where their meanings are 
interpreted in both semantic and pragmatic 
terms. 
1 Introduction 
With the exponential increase in web sites and 
documents, the amount of information is no 
longer a main concern for automatic knowledge 
acquisition. This trend raises, however, at least 
two new issues. The first is how to locate the 
information which exactly meets our needs 
among the vast web content. Efforts to address 
this issue can be exemplified by advanced re-
search in information retrieval, information ex-
traction, etc. The second is how to judge the va-
lidity of the acquired information, that is, the 
trustworthiness of information. This issue has 
attracted considerable interest in some related 
research areas recently. Taking the specific in-
formation retrieval task, question answering 
(QA) as an example, a QA system attempts to 
retrieve the most appropriate answers to ques-
tions from web resources. To determine the 
trustworthiness of the extracted candidate an-
swers, a common approach is to exploit the co-
occurrence frequency of questions and candidate 
answers. That is, if a candidate answer co-occurs 
more frequently with the question than other 
candidates, the QA system may judge it as the 
best answer (Magnini, 2002). This approach pre-
supposes and relies crucially on information re-
dundancy. Although this heuristic method is 
simple and straightforward, it is not applicable 
to all cases. For the applications which don?t 
involve much information redundancy, the heu-
ristic could cease to be effective. The task of 
collaborative question answering (CQA) which 
we will address in this paper is just one of such 
examples. For a user posted question, there are 
usually only few answers provided. So, the heu-
ristic is not useful in providing the best answer. 
In addition, since the spread of unsubstantiated 
rumors on the Internet is so pervasive, the high-
frequency information on the Web sometimes 
may mislead the judgment of trustworthiness. In 
terms of the above consideration, it is essential 
to look for other approaches which allow di-
rectly modeling of the trustworthiness of a text.  
Given that non-textual features (such as user's 
Web behavior) used in text trustworthiness de-
tection are often manipulated by information 
providers, as well as no directly related textual 
features for the task has been proposed up to 
10
date, we need a more felicitous model for detect-
ing the trustworthiness of statements. Noting 
that evidentiality is often linguistically encoded 
and hence provides inherent information on 
trustworthiness for a statement, we propose to 
incorporate the linguistic model of evidentiality 
in our study. Specifically, we incorporate evi-
dentiality into a machine learning based text 
classification framework, and attempt to verify 
the validity of evidentiality in trustworthiness 
prediction of text information in the context of 
collaborative question answering. The experi-
mental results show that evidentials are impor-
tant clues in predicting the trustworthiness of 
text. Since none of the task-specific heuristics 
has been incorporated, the current approach 
could also be easily adapted to fit other natural 
language processing applications. 
The paper proceeds as follows. In section 2 
we discuss related work on text trustworthiness 
detection. The section is divided into two parts: 
the current methodology and the textual features 
for analysis in the task. Section 3 introduces the 
linguistic researches on evidentiality and our 
taxonomy of evidentials based on the trustwor-
thiness indication. Section 4 presents the ex-
periment settings and results. Finally, in section 
5 we discuss the experiment results and con-
clude the current research. 
2 Related Work 
The research of text trustworthiness is very 
helpful for many other natural language process-
ing applications. For example, in their research 
on question answering, Banerjee and Han (2009) 
modulate answer grade by using a weighted 
combination of the original score and answer 
credibility evaluation. Also, Weerkamp and Ri-
jke (2008) incorporate textual credibility indica-
tors in the retrieval process to improve topical 
blog posts retrieval. Gyongyi et al(2004) pro-
pose a TrustRank algorithm for semi-
automatically separating reputable, good Web 
pages from spams. 
2.1 General Approaches for Text Trust-
worthiness Detection 
In past research, the judgment for the trustwor-
thiness or credibility of a given text content is 
usually tackled from two aspects: entity oriented 
and content oriented (Rubin and Liddy, 2005). 
The former approach takes into consideration 
the information providers? individual profiles, 
such as their identity, reputation, authority and 
past web behavior; whereas the latter approach 
considers the actual content of texts. Metzger 
(2007) reviews several cognitive models of 
credibility assessment and points out that credi-
bility is a multifaceted concept with two primary 
dimensions: expertise and trustworthiness. Fol-
lowing Matzger?s framework, Rubin and Liddy 
(2005) compile a list of factors that users may 
take into account in assessing credibility of blog 
sites. This list could also be summarized as the 
above mentioned two-folds: the bloggers? pro-
files and the information posted in the entries.  
Comparing these two aspects, most existing 
research on text trustworthiness focuses on the 
user oriented features. Lots of user oriented fea-
tures have been proposed in the research of 
credibility detection. To score the user oriented 
features such as user?s authority, a common ap-
proach is based on a graph-based ranking algo-
rithm such as HITS and PageRank (Zhang et al 
2007; Bouguessa et al 2008).  
In the research of text trustworthiness detec-
tion, the overwhelmingly adaption of non-
textual features such as entity profiles over text 
content based features reflect some researchers? 
belief that superficial textual features cannot 
meet the need of text credibility identification 
(Jeon et al 2006). In this paper, we examine the 
lexical semantic feature of evidential and argue 
that evidentiality, as a linguistically instantiated 
representation of quality of information content, 
offers a robust processing model for text trust-
worthiness detection. 
The detection of information trustworthiness 
also has promising application values. Google 
News 1  is just such an application that ranks 
search results according to the credibility of the 
news. Other online news aggregation service, 
such as NewTrust 2, also focuses on providing 
users with credible and high quality news and 
stories. The existed applications, however, rely 
on either the quality of web sites or user voting.  
So, it is anticipated that the improvement on the 
technology of text trustworthiness detection by 
incorporating lexical semantic cues such as evi-
dentiality may shed light on these applications. 
2.2 Textual Feature Based Text Trustwor-
thiness Detection 
Although non-textual features have been popular 
in text credibility detection, there has been a few 
research focusing on textual features so far. Gil 
                                               
1 http://news.google.com/ 
2 http://www.newstrust.net/ 
11
and Artz (2006) argue that the degree of trust in 
an entity is only one ingredient in deciding 
whether or not to trust the information it pro-
vides. They further point out that entity-centered 
issues are made with respect to publicly avail-
able data and services, and thus will not be pos-
sible in many cases. In their research of topical 
blog posts retrieval, Weerkamp and Rijke (2008) 
also consider only textual credibility indicators 
since they mentioned that additional resources 
(such as bloggers? profiles) is hard to obtain for 
technical or legal reasons.   
However, most research which utilizes textual 
features in text trustworthiness detection usually 
equates writing quality of document with its 
trustworthiness. Therefore, some secondary fea-
tures which may not directly related to trustwor-
thiness are proposed, including spelling errors, 
the lack of leading capitals, the large number of 
exclamation markers, personal pronouns and 
text length (Weerkamp and Rijke, 2008). There 
has not been attempted to directly evaluate in-
herent linguistic cues for trustworthiness of a 
statement. 
3 On Evidentiality in Text 
Evidentiality, as an explicit linguistic system to 
encode quality of information, offers obvious 
and straightforward evidence for text trustwor-
thiness detection. Yet it has not attracted the at-
tention which it deserves in most of the natural 
language processing studies. In this paper, we 
aim to explore how we can incorporate the lin-
guistic model of evidentiality into a robust and 
efficient machine learning based text classifica-
tion framework.   
Aikhenvald (2003) observes that every lan-
guage has some way of making reference to the 
source of information. Once the language is be-
ing used, it always imprinted with the subjective 
relationship from the speakers towards the in-
formation. Evidentiality is information provid-
ers? specifications for the information sources 
and their attitudes toward the information. As a 
common linguistic phenomenon to all the lan-
guages, it has attracted linguists? attention since 
the beginning of 20th century. In any language, 
evidentiality is a semantic category which could 
be expressed on both grammatical level (as in 
some American Indian language) and lexical 
level (as in English, Chinese and many other 
languages). The linguistic expressions of eviden-
tiality are named as evidentials or evidential 
markers.  
Mushin (2000) defines evidential as a marker 
which qualifies the reliability of information.  It 
is an explicit expression of the speaker?s atti-
tudes toward the trustworthiness of information 
source. For instance, 
a). It?s probably raining. 
b). It must be raining. 
c). It sounds like it?s raining. 
d). I think/guess/suppose it?s raining. 
e). I can hear/see/feel/smell it raining. 
It is obvious that the information provided in 
the above examples is subjective. The informa-
tion expresses the personal experience or atti-
tudes, while at the same time reflects the speak-
ers? estimation for the trustworthiness of the 
statement by information providers. 
3.1 The Definition of Evidentiality 
There are two dimensions of the linguistic defi-
nition for evidentiality. The term evidentiality is 
originally introduced by Jakobson (1957) as a 
label for the verbal category indicating the al-
leged source of information about the narrated 
events. In line with Jakobson?s definition, the 
narrow definition of evidentiality proposed by 
other researchers focuses mainly on the specifi-
cation of the information sources, that is, the 
evidence through which information is acquired 
(DeLancey, 2001). Comparing with the narrow 
definition, the board definition explains eviden-
tiality in a much wider sense, and characterizes 
evidentiality as expressions of speaker?s attitude 
toward information, typically expressed by mo-
dalities (Chafe, 1986; Mushin, 2000). 
Ifantidou (2001) also holds that evidential has 
two main functions: 1) indicating the source of 
knowledge; 2) indicating the speaker?s degree of 
certainty about the proposition expressed. He 
further divides them in details as follows. 
a) Information can be acquired in various 
ways, including observation (e.g. see), hearsay 
(e.g. hear, reportedly), inference (e.g. must, de-
duce), memory (e.g. recall). 
b) Evidentiality can indicate the speaker?s de-
gree of certainty, including certain propositional 
attitude (e.g. think, guess) and adverbials (e.g. 
certainly, surely), also epistemic models (e.g. 
may, ought to). 
3.2 The Taxonomy of Evidentials 
Evidentiality has its hierarchy which forms a 
continuum that marks from the highest to the 
least trustworthiness. Up to now, there are many 
hierarchical schemes proposed by researchers. 
12
 
Table 1. The Categorization and Inside Items of Evidentiality 
 
Oswalt (1986) suggests a priority hierarchy of 
evidentials as: 
Performative > Factual > Visual > Auditory > 
Inferential > Quotative 
In this evidential hierarchy, performative car-
ries the highest degree of trustworthiness since 
Oswalt considers that the speaker is speaking of 
the act he himself is performing. It is the most 
reliable source of evidence for the knowledge of 
that event. 
Whereas Barners (1984) proposes the follow-
ing hierarchy: 
Visual > Non-visual > Apparent > Second-
hand > Assumed 
He points out that visual evidence takes 
precedence over the auditory evidence and is 
more reliable.  
The above two hierarchies are based on the 
narrow definition of evidentiality mentioned 
above. There are also some hierarchies involving 
the board definition of evidentiality, such as 
Chafe (1986)?s categories of evidentiality.   
In this paper, we adopt a broad definition of 
evidentiality and focus on a trustworthiness 
categorization. This categorization follows the 
model of four-dimensional certainty categoriza-
tion by Rubin et al(2005). In this model, it is 
suggested that the division of the certainty level 
dimension into four categories - Absolute, High, 
Moderate and Low. With some revision, there 
are different items of evidential words and 
phrased that we extracted from the corpus. 
These items from each category to be adopted in 
our experiments are presented in Table 1. 
4 Incorporating Evidentiality into Ma-
chine Learning for Trustworthiness 
Detection 
In this section, we apply evidentiality in an ac-
tual implement of text trustworthiness detection. 
It is based on a specific web application service, 
collaborative question answering (CQA), in 
which the trustworthiness of text content is very 
helpful for finding the best answers in the ser-
vice.  
With the development of Web2.0, the services 
of CQA in community media have largely at-
tracted people?s attention. Comparing with the 
general ad hoc information searching, question 
answering could help in finding the most accu-
rate answers extracted from the vast web content. 
Whereas in the collaborative question answering, 
the CQA community media just provide a web 
space in which users can freely post their ques-
tions, and at the same time other users may an-
swer these questions based on their knowledge 
and interests. Due to the advantage of interactiv-
ity, CQA usually could settle some questions 
which cannot be dealt with by ad hoc informa-
tion retrieval. However, since the platform is 
open to anyone, the quality of the answers pro-
vided by users is hard to identify. People may 
present answers of various qualities due to the 
limitation of their knowledge, attitude and pur-
pose of answering the questions. As a result, the 
issue of how to identify the most trustworthy 
answers from the user-provided content turns 
out to be the most challenging part to the system. 
As mentioned previously, the trustworthiness 
of text content could be identified from two di-
mensions. The first one relies on the features 
related with information distributors. The second 
one relies on the content of a text. In current re-
search we focus on textual features, especially 
the feature of evidentiality in texts. The feature 
will be incorporated into a machine learning 
based text classification framework in order to 
identify the best answers for CQA questions.  
 Absolute High Moderate Low 
Attributive/modal 
adverb 
certainly, sure, of 
course, definitely, ab-
solutely, undoubtedly 
clearly, obviously, ap-
parently, really, always 
 Seemingly, 
probably  
maybe, personally, 
perhaps, possibly, 
presumably 
Lexical verb 
report, certain believe, see seem, think, sound doubt, wish, wonder, 
infer, assume, fore-
cast, fell, heard 
Auxiliary verb  must ought, should, would, could, can 
may, might 
Epistemic adjec-
tive 
definite  possible, likely, 
unlikely, probable, 
positive, potential 
not sure, doubtful 
13
4.1 The Dataset 
For the experiments, we use the snapshot of Ya-
hoo! Answers dataset which is crawled by Emo-
ry University3. Since our experiments only in-
volve text features, we use the answer parts from 
it without considering the question sets and user 
profiles. Such information could be incorporated 
to achieve a higher performance in the future.  
With regard to the text classification problems, 
there is typically a substantial class distribution 
skew (Forman, 2003). For the Yahoo! Answers 
dataset, a question only has one best answer and 
accordingly all the other answers will be marked 
as non-best answers. Thus the class of best an-
swer contains much fewer texts than the class of 
non-best answers. In our dataset (a proportion of 
the overall CQA dataset provided by Emory 
University), the number of best answers is 2,165, 
and the number of non-best answers is 17,654. 
The proportion of the size of the two answer sets 
is around 1:8.15, showing a significant skews. 
For a better comparison of experimental results, 
we use a balanced dataset which is generated 
from a normal distribution dataset.  
A 10-fold validation is used for the evaluation, 
where the datasets of best and non-best answers 
are divided into 10 subsets of approximately 
equal size respectively. In the normally distrib-
uted dataset, we use one of the ten subsets as the 
test set, while the other nine are combined to-
gether to from the training set. In the balanced 
dataset, for each subset of the non-best answers, 
we only use the first k answers, in which k is the 
size of each subset of best answers. The training 
data and test data used in the machine learning 
process are shown in Table 2. 
 
 Training 
/Test Set 
Best  
answer 
Non-best 
 answer 
training 19,490 158,889 normal distribution 
dataset test 2,165 17,654 
training 19,490 19,490 balanced 
dataset test 2,165 2,165 
 
Table 2. The Dataset Used for the Experiments  
4.2 Experiment Settings 
To conduct a machine learning based classifica-
tion for best answers and non-best answers, we 
first need to construct the feature vectors. The 
representation of text is the core issue in the ma-
                                               
3 http://ir.mathcs.emory.edu/shared 
chine learning model for text classification. In 
text domains, feature selection plays an essential 
role to make the learning task efficient and more 
accurate. As the baseline comparison, we use the 
following feature vector settings. 
?Baseline1 represents using all the words in 
the text as features (when the frequency of the 
word in the dataset is bigger than a predefined 
threshold j). 
? Baseline2 represents using all the content 
words (here we include the four main categories 
of content words - nouns, verbs, adjectives and 
adverbs identified by a POS tagger) in the data-
set as features. 
We use both the above two baselines. The 
bag-of-word model of Baseline1 is a conven-
tional method in text representation. However, 
since not all the words are linguistically signifi-
cant, in Baseline2, we consider only the content 
words in the dataset, since content words convey 
the core meaning of a sentence.  
For the evidentiality-based classification, we 
adopt the following feature vector settings.  
?Evidential represents using all the evidentials 
in text as features. 
?Evidential? represents using all the eviden-
tials except for those in the category of Moder-
ate as features. 
?Evid.cat4 represents using the four eviden-
tiality categories of Absolute, High, Moderate 
and Low from Table 1. 
?Evid.cat2 represents using the two categories 
of Absolute and High as the positive evidential 
and Moderate and Low as the negative evidential. 
?Evid.cat2? omits the evidential category of 
Moderate, and represents using the two catego-
ries of Absolute and High as the positive eviden-
tial and only the category of Low as the negative 
evidential feature. 
Some researchers have proved that usually a 
Boolean indicator of whether the feature item 
occurred in the document is sufficient for classi-
fication (Forman, 2003). Although there are also 
some other feature weighting schemes such as 
term frequency (TF), document frequency (DF), 
etc, comparison of these different weighting 
schemes is not the object of the current research. 
So in this paper, we only consider Boolean 
weighting. In the Boolean text representation 
model, each feature represents the Boolean oc-
currence of a word, evidential, or evidential cat-
egory according to the different feature settings. 
By the experimental settings, we want to verify 
the hypothesis that incorporating the knowledge 
14
of evidentiality into text representation can lead 
to improvement in classification performance. 
In our experiment, we perform text preproc-
essing including word segmentation and part-of-
speech (POS) tagging. The Stanford Log-linear 
Part-Of-Speech Tagger (http://nlp.stanford.edu/ 
software/tagger.shtml) is used for POS tagging. 
We adopt support vector machine (SVM) as the 
machine learning model to classify best answers 
from non-best ones, and use the SVMlight pack-
age (http://svmlight.joachims.org) as the classi-
fier with the default parameters and a linear ker-
nel. For the evaluation, we use the metrics of 
precision (Prec. as in table 3), recall (Rec. as in 
table 3), accuracy (Acc. as in table 3) and F1: 
F1-measure, the harmonic mean of the precision 
and recall. 
4.3 Evaluation 
Table 3 shows the experimental results using the 
balanced dataset with Boolean weighting. The 
focus of the experiment evaluation is on identi-
fying the best answers, so the evaluation metrics 
are all for the best answers collection. From the 
table, we see increases of the two feature vector 
setting of evidentials over both baseline results. 
The highest improvement is 14.85%, achieved 
by the feature set of Evidential?. However, there 
is no increase found in the settings of using evi-
dential categories. This means that although the 
category of evidentials in indicating text trust-
worthiness is obvious for human, it is not neces-
sary a preferred feature for machine learning. 
 
 Prec. Rec. Acc. F1 
Baseline1 45.62% 51.51% 45.15% 47.94% 
Baseline2 59.58% 39.20% 56.30% 47.28% 
Evidential 67.78% 44.18% 61.59% 53.49% 
Evidential? 47.40% 90.12% 45.06% 62.13% 
Evid.cat4 64.15% 25.85% 55.70% 36.85% 
Evid.cat2 60.86% 28.21% 55.03% 38.55% 
Evid.cat2? 40.35% 25.85% 43.81% 31.51% 
 
Table 3. Experimental Results Using the Bal-
anced Training/Test Dataset (with Boolean 
Weighting) 
 
To eliminate the potential effect of term 
weighting scheme on performance trend among 
different text representation models, we also 
conduct experiments using TF weighting. By the 
experiments, we aim to compare the relative per-
formances of different feature vectors con-
structed with evidentials, and the results are 
demonstrated in Table 4. 
 
 Prec. Rec. Acc. F1 
Evidential 66.78% 45.57% 61.45% 54.17% 
Evidential? 59.66% 20.82% 53.37% 30.87% 
Evid.cat4 50.00% 18.14% 50.00% 26.63% 
Evid.cat2 55.91% 16.39% 51.73% 25.35% 
 
Table 4. Experimental Results Using the Bal-
anced Training/Test Dataset (with TF Weighting) 
 
From the table, it can be observed that using 
evidentials as features shows better improve-
ment in the performance than the category of 
evidentials as a feature. A similar performance 
has been summarized in Table 3. 
Finally, but not the least, to better understand 
the effect of evidential category on the machine 
learning performance, we design additional ex-
periments as follows. 
?Evid_cat1 stands for combining the four evi-
dential categories into one, and uses only this 
one category of evidential as a feature. The ap-
proach of Boolean weighting is actually the 
same as a rule-based approach that classifies the 
test dataset according to whether evidential oc-
curs or not. 
 
BOOL Prec. Rec. Acc. F1 
Evid_cat1 59.42% 61.59% 59.76% 60.49% 
 
Table 5. Experimental Results Using the Bal-
anced Training/Test Dataset (with Boolean 
Weighting; Only One Evidential Category) 
 
Table 5 presents a set of interesting experi-
mental result. In the result, all the four evalua-
tion metrics show performance increases com-
paring to the baseline, and it even outperforms 
almost all the other results from both weighting 
schemes. Based on this result, it is suggested 
that evidentiality still may contribute to the task 
of text trustworthiness detection. Moreover, it 
can significantly reduce the dimensionality of 
feature space (e.g. for Baseline 1, the dimen-
sionality of feature dimension is 218,328 in one 
of our cases; while for the experiment of Evi-
dential, it reduced to only 51 as shown in Table 
15
1). However, we should address the question of 
why not all types of evidential features demon-
strate improvement of detection. We will further 
discuss the issue from a  pragmatic viewpoint in 
the next section. 
5 Conclusion and Discussion 
In this paper, we propose to incorporate the lin-
guistic knowledge of evidentiality in the NLP 
task of trustworthiness prediction. As evidential-
ity is an integral and inherent part of any state-
ment and explicitly expresses information about 
the trustworthiness of this statement, it should 
provide the most robust and direct model for 
trustworthiness detection. We first set up the 
taxonomy of lexical evidentials. By incorporat-
ing evidentiality into a machine learning based 
text classification framework, we conduct ex-
periments for a specific application, CQA.  The 
evidentials in the dataset are extracted to form 
different text representation schemes. Our ex-
perimental results using evidentials show im-
provements up to 14.85% over the baselines. 
However, not all types of evidential features 
contributed to the improvement of detection. We 
also compared the effect of different types of 
evidential based feature representation schemes 
on the classification performance.  
The way to model evidentiality for trustwor-
thiness detection which we adopted in our initial 
experiment design actually could also be ex-
plained by Grice?s Maxim of Quality: be truthful. 
As the Maxim of Quality requires one ?not to 
say that for which one lacks adequate evidence?, 
we hypothesize that evidential constructions 
mark the adequacy of evidence and should indi-
cate reliable answers. However, the results from 
our experiments only partially supported this 
hypothesis. The results showed a satisfactory 
performance was achieved when all evidential 
markers were treated as negative evidence for 
reliability. This result could then be accounted 
by invoking another Gricean maxim: Quantity. 
The Maxim of Quantity requires that ?one 
makes his/her contribution as informative as is 
required, and at the same time does not make the 
contribution more informative than is required.? 
As evidentiality is not grammaticalized in Eng-
lish, the use of evidentiality is not a required 
grammatical element. An answer marked by evi-
dentials would violate Maxim of Quantity if it is 
correct. The Maxim of Quantity predicts that 
good answers are plain statements without evi-
dential markers. On the frequent use of eviden-
tial markers for less reliable answers can be ac-
counted for by speakers? attempt to follow both 
Maxims of Quality and Quantity. The evidential 
marks are used to compensate for the fact that 
speakers are not very confident about the answer, 
yet would like to adhere to the Maxim of Quality. 
In other words, evidentials are not likely to be 
used in reliable answers because of the Maxim 
of Quality, but it is likely used in less reliable 
answers because the speakers may try to provide 
proof of adequate evidence by a grammatical 
device instead of providing true answer. 
Therefore, this model elaborated above takes 
into account not only the grammatical function 
of evidential constructions but also how this lin-
guistic structure is used as a pragmatic/discourse 
device. In other words, this study suggests that 
modeling linguistic theory in NLP needs to take 
a more comprehensive approach than the simple 
modular approach where only one module 
(based on evidentiality) is used. Linguistic mod-
eling needs to consider both how linguistic 
structure/knowledge is represented and proc-
essed, we also need to model how a particular 
linguistic device in use. 
In the further works, we plan to continue de-
veloping and elaborate on a multi-modular lin-
guistic model of evidentiality for knowledge 
acquisition. We will also explore the possibility 
of incorporating other features, both textual and 
non-textual, to further improve performances in 
the tasks of text trustworthiness detection. 
References  
Agichtein E, Castillo C, and etc. 2008. Finding high-
quality content in social media. In Proceedings of  
WSDM2008. 
Aikhenvald A and Dixon, ed. 2003. Studies in evi-
dentiality. Amsterdam/Philadelphia: John Benja-
mins Publishing Company 
Banerjee P, Han H. 2009. Credibility: A Language 
Modeling Approach to Answer Validation, In Pro-
ceedings of NAACL HLT 2009, Boulder, Bolorado, 
US 
Barners J. 1984. Evidentials in the Tuyuca Verb. IN 
International Journal of American Linguistics, 50 
Bouguessa M, Dumoulin B, Wang S. 2008. Identify-
ing Authoritative Actors in Question-Answering 
Forums - The Case of Yahoo! Answers, In Pro-
ceedings of KDD?08, Las Vegas, Nevada, USA 
Chafe W. 1986. Evidentiality: The Linguistic Coding 
of Epistemology, Evidentiality in English Conver-
sation and Academic Writing. In Chafe and Nich-
16
ols, (ed.). Evidentiality: The Linguistic Coding of 
Epistemology. Norwood, NJ: Ablex 
DeLancey S. 2001. The mirative and evidentiality. In 
Journal of Pragmatic, 33 
Forman G. 2003. An Extensive Empirical Study of 
Feature Selection Metrics for Text Classification, 
In Journal of Machine Learning Research, 3 
Gil Y, Artz D. 2006. Towards Content Trust of Web 
Resources, In Proceedings of the 15th International 
World Wide Web Conference, Edinburgh, Scotland 
Gyongyi Z, Molina H, Pedersen J. 2004. Combating 
Web Spam with TrustRank. In Proceedings of the 
30th VLDB Conference, Toronto, Canada 
Ifantidou E. 2001. Evidentials and Relevance. John 
Benjamins Publishing Company. 
Jeon J, Croft W, Lee J and Park S. 2006. A Frame-
work to Predict the Quality of Answers with Non-
textual Features, In Proceedings of SIGIR?06, Se-
attle, Washington, USA 
Leopold E, Kindermann J. 2002. Text Categorization 
with Support Vector Machines. How to Represent 
Texts in Input Space?, In Machine Learning, 46, 
423-444 
Oswalt R. 1986. The evidential system of Kashaya. 
IN Chafe W and Nichols (Eds.), Evidentiality: The 
linguistic coding of epistemology. Norwood, NJ: 
Ablex 
Rubin V, Liddy E, Kando N. 2005. Certainty Identi-
fication in Texts: Categorization Model and Man-
ual Tagging Results, In Shanahan J and et al(Eds.), 
Computing Attitude and Affect in Text: Theory and 
Applications (The Information Retrieval Series): 
Springer-Verlag New York, Inc. 
Rubin V, Liddy E. 2006. Assessing Credibility of 
Weblogs, In Proceedings of the AAAI Spring Sym-
posium: Computational Approaches to Analyzing 
Weblogs (CAAW) 
Magnini B, Negri M, Prevete R and Tanev H. 2002. 
Is It the Right Answer? Exploiting Web Redun-
dancy for Answer Validation, In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics, Philadelphia, PA 
Metzger M. 2007. Evaluating Online Information and 
Recommendations for Future Research, Journal of 
the American Society for Information Science and 
Technology, 58(13) 
Mushin  I. 2000. Evidentiality and Deixis in Retelling, 
In Journal of Pragmatics, 32 
Weerkamp W, Rijke M. 2008. Credibility Improves 
Topical Blog Post Retrieval. In Proceedings of 
ACL08: HLT 
Zhang J, Ackerman M, Adamic L. 2007. Expertise 
Networks in Online Communities: Structure and 
Algorithms. In Proceedings of the 16th ACM Inter-
national World Wide Web Conference (WWW?07) 
17
 Textual Emotion Processing From Event Analysis 
 
 
Chu-Ren Huang?, Ying Chen*?, Sophia Yat Mei Lee?  
?Department of Chinese and Bilingual Studies * Department of Computer Engineering 
The Hong Kong Polytechnic University China Agricultural University 
{churenhuang, chenying3176, sophiaym}@gmail.com 
  
 
 
 
 
 
 
Abstract 
Textual emotion recognition has gained a lot of 
attention recent years; it is however less devel-
oped due to the complexity nature of emotion. In 
this paper, we start with the discussion of a num-
ber of fundamental yet unresolved issues concern-
ing emotion, which includes its definition, 
representation and technology. We then propose 
an alternative solution for emotion recognition 
taking into account of emotion causes. Two pilot 
experiments are done to justify our proposal. The 
first experiment explores the impact of emotion 
recognition. It shows that the context contains rich 
and crucial information that effectively help emo-
tion recognition. The other experiment examines 
emotion cause events in the context. We find that 
most emotions are expressed with the presence of 
causes. The experiments prove that emotion cause 
serves as an important cue for emotion recognition. 
We suggest that the combination of both emotion 
study and event analysis would be a fruitful direc-
tion for deep emotion processing. 
1 Introduction 
The study of emotion attracts increasingly greater 
attention in the field of NLP due to its emerging 
wide applications, such as customer care (Gupta et 
al., 2010), and social information understanding 
(Lisa and Steyvers, 2010). In contrast to sentiment, 
which is the external subjective evaluation, emo-
tion mainly concentrates on the internal mental 
state of human (Ortony et al, 1987). Emotion is 
indeed a highly complicated concept that raises a 
lot of controversies in the theories of emotion re-
garding the fundamental issues such as emotion 
definition, emotion structure and so on. The com-
plexity nature of emotion concept makes auto-
matic emotion processing rather challenging. 
Most emotion studies put great effort on emo-
tion recognition, identifying emotion classes, such 
as happiness, sadness, and fear. On top of this 
surface level information, deeper level informa-
tion regarding emotions such as the experiencer, 
cause, and result of an emotion, needs to be ex-
tracted and analyzed for real world applications. 
In this paper, we discuss these two closely related 
emotion tasks, namely emotion recognition and 
emotion cause detection and how they contribute 
to emotion processing. 
For emotion recognition, we construct an emo-
tion corpus for explicit emotions with an unsuper-
vised method. Explicit emotions are emotions 
represented by emotion keywords such as e.g., 
?shocked? in ?He was shocked after hearing the 
news?.  In the course of emotion recognition, the 
keyword in an explicit emotion expression is de-
leted and only contextual information remains. In 
our pilot experiments, the context-based emotion 
identification works fairly well. This implies that 
plenty of information is provided in the context 
for emotion recognition. Moreover, with an in-
depth analysis of the data, we observe that it is 
often the case that emotions co-occur and interact 
in a sentence. In this paper, we deal with emotion 
recognition from a dependent view so as to cap-
ture complicated emotion expressions.   
Emotion is often invoked by an event, which in 
turn is very likely to elicit an event (Descartes 
1649, James 1884, Plutchik 1980, Wierzbicka 
1999). Despite the fact that most researches rec-
ognize the important role of events in emotion 
theories, little work, if not none, attempts to make 
explicit link between events and emotion. In this 
paper, we examine emotion constructions based 
on contextual information which often contains 
considerable relevant eventive information. In 
particular, the correlations between emotion and 
cause events will be explored based on empirical 
data. Emotion causes refer to explicitly expressed 
propositions that evoke the corresponding emo-
tions.  
To enhance emotion recognition, we examine 
emotion causes occurring in the context of an 
emotion. First, we manually annotate causes for 
emotions in our explicit emotion corpus. Since an 
emotion cause can be a complicated event, we 
model emotion cause detection as a multi-label 
problem to detect a cross-clause emotion cause. 
Furthermore, an in-depth linguistic analysis is 
done to capture the different constructions in ex-
pressing emotion causes.  
The paper is organized as follows. Section 2 
discusses some related work regarding emotion 
recognition and emotion cause detection. In Sec-
tion 3, we present our context-based emotion cor-
pus and provide some data analysis. Section 4 
describes our emotion recognition system, and 
discusses the experiments and results. In Section 5, 
we examine our emotion cause detection system, 
and discuss the performances. Finally, Section 6 
concludes our main findings for emotion process-
ing from the event perspective.   
2 Related Work  
Most current emotion studies focus on the task of 
emotion recognition, especially in affective lexi-
con construction. In comparison with emotion 
recognition, emotion cause detection is a rather 
new research area, which account for emotions 
based on the correlations between emotions and 
cause events. This section discusses the related 
research on emotion recognition and emotion 
cause detection. 
2.1 Emotion Recognition 
Although emotion recognition has been inten-
sively studied, some issues concerning emotion 
remain unresolved, such as emotion definition, 
emotion representation, and emotion classification 
technologies. 
For the emotion definition, emotion has been 
well-known for its abstract and uncertain defini-
tion which hinders emotion processing as a whole. 
Ortony et al, (1987) conducted an empirical study 
for a structure of affective lexicon based on the 
~500 words used in previous emotion studies. 
However, most of the emotion corpora in NLP try 
to avoid the emotion definition problem. Instead, 
they choose to rely on the intuition of annotators 
(Ren?s Blog Emotion Corpus, RBEC, Quan and 
Ren, 2009) or authors (Mishne?s blog emotion 
corpus, Mishne, 2005). Therefore, one of the cru-
cial drawbacks of emotion corpora is the problem 
of poor quality. In this paper, we explore emotion 
annotation from a different perspective. We con-
centrate on explicit emotions, and utilize their 
contextual information for emotion recognition.  
In terms of emotion representation, textual 
emotion corpora are basically annotated using ei-
ther the enumerative representation or the compo-
sitional representation (Chen et al, 2009). The 
enumerative representation assigns an emotion a 
unique label, such as pride and jealousy. The 
compositional representation represents an emo-
tion through a vector with a small set of fixed ba-
sic emotions with associated strength. For instance, 
pride is decomposed into ?happiness + fear? ac-
cording to Turner (2000).  
With regard to emotion recognition technolo-
gies, there are two kinds of classification models. 
One is based on an independent view (Mishne, 
2005; Mihalcea and Liu, 2006; Aman and Szpa-
kowicz, 2007; Tokuhisa et al, 2008; Strapparava 
and Mihalcea, 2008), and the other is a dependent 
view (Abbasi et al 2008; Keshtkar and Inkpen, 
2009). The independent view treats emotions sep-
arately, and often chooses a single-label classifica-
tion approach to identify emotions. In contrast, the 
dependent view takes into account complicated 
emotion expressions, such as emotion interaction 
and emotion co-occurrences, and thus requires 
more complicated models. Abbasi et al (2008) 
adopt an ensemble classifier to detect the co-
occurrences of different emotions; Keshtkar and 
Inkpen (2009) use iteratively single-label classifi-
ers in the top-down order of a given emotion hier-
archy. In this paper, we examine emotion 
recognition as a multi-label problem and investi-
gate several multi-label classification approaches.    
 2.2 Emotion Cause Detection 
Although most emotion theories recognize the 
important role of causes in emotion analysis (Des-
cartes, 1649; James, 1884; Plutchik, 1962; Wierz-
bicka 1996), yet very few studies in NLP explore 
the event composition and causal relation of emo-
tions. As a pilot study, the current study proposes 
an emotion cause detection system.  
Emotion cause detection can be considered as a 
kind of causal relation detection between two 
events. In other words, emotion is envisioned as 
an event type which triggers another event, i.e. 
cause event. We attempt to examine emotion 
cause relations for open domains. However, not 
much work (Marcu and Echihabi, 2002; Girju, 
2003; Chang and Choi, 2006) has been done on 
this kind of general causal relation for open do-
mains. 
Most existing causal relation detection systems 
contain two steps: 1) cause candidate identifica-
tion; 2) causal relation detection. However, Step 1) 
is often oversimplified in real systems. For exam-
ple, the cause-effect pairs are limited to two noun 
phrases (Chang and Choi, 2005; Girju, 2003), or 
two clauses connected with selected conjunction 
words (Marcu and Echihabi, 2002). Moreover, the 
task of Step 2) often is considered as a binary 
classification problem, i.e. ?causal? vs. ?non-
causal?.  
With regard to feature extraction, there are two 
kinds of information extracted to identify the 
causal relation in Step 2). One is constructions 
expressing a cause-effect relation (Chang and 
Choi, 2005; Girju, 2003), and the other is seman-
tic information in a text (Marcu and Echihabi, 
2002; Persing and Ng, 2009), such as word pair 
probability. Undoubtedly, the two kinds of infor-
mation often interact with each other in a real 
cause detection system. 
3 Emotion Annotated Sinica Corpus 
(EASC) 
EASC is an emotion annotated corpus comprising 
two kinds of sentences: emotional-sentence corpus 
and neutral-sentence corpus. It involves two com-
ponents: one for emotion recognition, which is 
created with an unsupervised method (Chen et al 
2009), and the other is for emotion cause detection, 
which is manually annotated (Chen et al 2010).  
3.1 The Corpus for Emotion Recognition 
With the help of a set of rules and a collection of 
high quality emotion keywords, a pattern-based 
approach is used to extract emotional sentences 
and neutral sentences from the Academia Sinica 
Balanced Corpus of Mandarin Chinese (Sinica 
Corpus). If an emotion keyword occurring in a 
sentence satisfies the given patterns, its corre-
sponding emotion type will be listed for that sen-
tence. As for emotion recognition, each detected 
keyword in a sentence is removed, in other words, 
the sentence provides only the context of that 
emotion. Due to the overwhelming of neutral sen-
tences, EASC only contains partial neutral sen-
tences besides emotional sentences. For 
experiments, 995 sentences are randomly selected 
for human annotation, which serve as the test data. 
The remaining 17,243 sentences are used as the 
training data.  
In addition, in the course of creating the emo-
tion corpus, Chen et al (2009) list the emotion 
labels in a sentence using the enumerative repre-
sentation. Besides, an emotion taxonomy is pro-
vided to re-annotate an emotion with the 
compositional representation. With the taxonomy, 
an emotion is decomposed into a combination of 
primary emotions (i.e. happiness, fear, anger, 
sadness, and surprise). 
From this corpus, we observe that ~54% emo-
tional sentences contain two emotions, yet only 
~2% sentences contain more than two emotions. 
This implies emotion recognition is a typical mul-
ti-label problem. Particularly, more effort should 
be put on the co-occurrences of two emotions. 
3.2 The Corpus for Emotion Cause De-
tection 
Most emotion theories agree that the five primary 
emotions (i.e. happiness, sadness, fear, anger, and 
surprise) are prototypical emotions. Therefore, for 
emotion cause detection, we only deal with the 
emotional sentences containing a keyword repre-
senting one of these primary emotions. Beyond a 
focus sentence, its context (the previous sentence 
and the following sentence) is also extracted, and 
those three sentences constitute an entry. After 
filtering non-emotional and ambiguous sentences, 
5,629 entries remain in the emotion cause corpus.  
Each emotion keyword is annotated with its 
corresponding causes if existing. An emotion 
keyword can sometimes be associated with more 
than one cause, in such a case, both causes are 
marked. Moreover, the cause type is also identi-
fied, which is either a nominal event or a verbal 
event (a verb or a nominalization).  
From the corpus, we notice that 72% of the ex-
tracted entries express emotions, and 80% of the 
emotional entries have a cause, which means that 
causal event is a strong indicator for emotion rec-
ognition.  
Furthermore, since the actual cause can some-
times be so complicated that it involves several 
events, we investigate the span of a cause text as 
follows. For each emotion keyword, an entry is 
segmented into clauses with some punctuations, 
and thus an entry becomes a list of cause candi-
dates. In terms of the cause distribution, we find 
~90% causes occurring between ?left_2? and 
?right_1?. Therefore, our cause search is limited to 
the list of cause candidates which contains five 
text units, i.e. <left_2, left_1, left_0, right_0, 
right_1>. If the clause where emotion keyword 
locates is assumed as a focus clause, ?left_2? and 
?left_1? are the two previous clauses, and ?right_1? 
is the following one. ?left_0? and ?right_0? are the 
partial texts of the focus clause, which locate in 
the left side of and the right side of the emotion 
keyword, respectively. Finally, we find that ~14% 
causes occur cross clauses. 
4 Emotion Processing with multi-label 
models   
4.1 Multi-label Classification for Emo-
tion recognition 
Based on our corpus, two critical issues for emo-
tion recognition need to be dealt with: emotion 
interaction and emotion co-occurrences. Co-
occurrence of multiple emotions in a sentence 
makes emotion recognition a multi-label problem. 
Furthermore, the interaction among different emo-
tions in a sentence requires a multi-label model to 
have a dependent view. In this paper, we explore 
two simple multi-label models for emotion recog-
nition. 
The Binary-based (BB) model: decompose the 
task into multiple independent binary classifiers 
(i.e., ?1? for the presence of one emotion; ?0? for 
the absence of one emotion), where each emotion 
is allocated a classifier. For each test instance, all 
labels (emotions) from the classifiers compose a 
vector. 
The label powset (LP) model: treat each possible 
combination of labels appearing in the training 
data as a unique label, and convert multi-label 
classification to single-label classification.  
Both the BB model and the LP model need a 
multi-class classifier. For our experiment, we 
choose a Max Entropy package, Mallet1. In this 
paper, we use only words in the focus sentence as 
features. 
4.2 Emotion Recognition Experiments 
To demonstrate the impact of our context-based 
emotion corpus to emotion recognition, we com-
pare EASC data to Ren?s Blog Emotion Corpus 
(RBEC). RBEC is a human-annotated emotion 
corpus for both explicit emotions and implicit 
emotions. It adopts the compositional representa-
tion with eight emotion dimensions (anger, anxi-
ety, expect, hate, joy, love, sorrow, and surprise). 
For each dimension, a numerical value ranging in 
{0.0, 0.1, 0.2... 1.0} indicates the intensity of the 
emotion in question. There are totally 35,096 sen-
tences in RBEC. To fairly compare with the 
EASC data, we convert a numerical value to a 
binary value. An emotion exists in a sentence only 
when its corresponding intensity value is greater 
than 0.  
For RBEC data, we use 80% of the corpus as 
the training data, 10% as the development data, 
and 10% as the test data. For EASC, apart from 
the test data, we divide its training data into two 
sets: 90% for our training data, and 10% for our 
development data. For evaluation of a multi-label 
task, three measures are used: accuracy (extract 
match ratio), Micro F1, and Macro F1. Accuracy 
is the extract match ratio of the whole assignments 
in data, and Micro F1 and Macro F1 are the aver- 
 
 
 
                                                           
1
 http://mallet.cs.umass.edu/ 
Table 1: The overall performances for the multi-label models   
 
 
 
 
 
 
 
 
age scores of F scores of all possible values for all 
variables. Micro F1 takes the emotion distribution 
into account, while Macro F1 is just the average 
of all F scores. Note that due to the overwhelming 
percentage of value 0 in the multi-label task, dur-
ing the calculating of Micro F1 and Macro F1, 
most previous multi-label systems take only value 
1 (indicating the existence of the emotion) into 
account. 
In Table 1, we notice that the emotion recogni-
tion system on our context-based corpus achieves 
similar performance as the one on human-
annotated corpus. This implies that there is rich 
contextual information with respect to emotion 
identification. 
5 Emotion Cause Detection 
Most emotion theories agree that there is a strong 
relationship between emotions and events (Des-
cartes 1649, James 1884, Plutchik 1980, Wierz-
bicka 1999). Among the rich information in the 
context of an emotion, cause event is the most 
crucial component of emotion. We therefore at-
tempt to explore emotion causes, and extract 
causes for emotion automatically.  
5.1 Emotion Cause Detection 
Based on the cause distribution analysis in Section 
3.2, in contrast to binary classification used in 
previous work, we formalize emotion cause detec-
tion as a multi-label problem as follows.  
Given an emotion keyword and its context, its 
label is the locations of its causes, such as ?left_1, 
left_0?. Then, we use the LP model to identify the 
cause for each sentence as well as an emotion 
keyword. With regard to emotion cause detection, 
the LP model is more suitable than the BB model 
because the LP model can easily capture the pos-
sible label combinations.  
   In terms of feature extraction, unlike emotion 
recognition, emotion cause detection relies more 
on linguistic constructions, such as ?The BP oil 
spill makes the country angry?, ?I am sad because 
of the oil spill problem? and so on. 
According to our linguistic analysis, we cre-
ate 14 patterns to extraction some common emo-
tion cause expressions. Some patterns are 
designed for general cause detection using linguis-
tic cues such as conjunctions and prepositions. 
Others are designed for some specific emotion 
cause expressions, such as epistemic markers and 
reported verbs. Furthermore, to avoid the low 
coverage problem of the rule-based patterns, we 
create another set of features, which is a group of 
generalized patterns. For details, please refer to 
Chen et al (2010).  
5.2 Experiments 
For EASC, we reserve 80% as the training data, 
10% as the development data, and 10% as the test 
data. For evaluation, we first convert a multi-label 
tag outputted from our system into a binary tag 
(?Y? means the presence of a causal relation; ?N? 
means the absence of a causal relation) between 
the emotion keyword and each candidate in its 
corresponding cause candidates. We then adopt 
three common measures, i.e. precision, recall and 
F-score, to evaluate the result. 
A naive baseline is designed as follows: The 
baseline searches for the cause candidates in the 
order of <left_1, right_0, left_2, left_0, right_1>. 
If the candidate contains a noun or a verb, this 
clause is considered as a cause and the search 
stops. 
Table 2 shows the overall performances of our 
emotion cause detection system. First, our system 
based on a multi-label approach as well as power-
ful linguistic features significantly outperforms 
the na?ve baseline. Moreover, the greatest im-
provement is attributed to the 14 linguistic pat-
terns (LP). This implies the importance of 
linguistic cues for cause detection. Moreover, the 
general patterns (GP) achieve much better per-
 EASC RBEC 
 BB LP BB LP 
Accuracy 21.30 28.07 22.99 28.33 
Micro F1 41.96 46.25 44.77 44.74  
Macro F1 34.78 35.52 36.48 38.88  
formance on the recall and yet slightly hurt on the 
precision. 
The performances (F-scores) for ?Y? and ?N? 
tags separately are shown in Table 3. First, we 
notice that the performances of the ?N? tag are 
much better than the ones of ?Y? tag. Second, it is 
surprising that incorporating the linguistic features 
significantly improves the ?Y? tag only (from 33% 
to 56%), but does not affect ?N? tag. This suggests 
that our linguistic features are effective to detect 
the presence of causal relation, and yet do not hurt 
the detections of ?non_causal? relation. Further-
more, the general feature achieves ~8% improve-
ments for the ?Y? tag. 
 
Table 2: The overall performance with different 
feature sets of the multi-label system 
 Precision Recall F-score 
Baseline 56.64 57.70 56.96 
LP 74.92 66.70 69.21 
+ GP 73.90 72.70 73.26 
 
Table 3: The separate performances for ?Y? and 
?N? tags of the multi-label system 
 ?Y? ?N? 
Baseline 33.06 80.85 
LP 48.32 90.11 
+ GP 56.84 89.68 
 
6 Discussions 
Many previous works on emotion recognition 
concentrated on emotion keyword detection. 
However, Ortony et al (1987) pointed out the dif-
ficulty of emotion keyword annotation, be it man-
ual or automatic annotation. Emotion keywords 
are rather ambiguous, and also contain other in-
formation besides affective information, such as 
behavior and cognition. Therefore, contextual in-
formation provides important cues for emotion 
recognition. Furthermore, we propose an alterna-
tive way to explore emotion recognition, which is 
based on emotion cause. Through two pilot ex-
periments, we justify the importance of emotion 
contextual information for emotion recognition, 
particularly emotion cause.  
We first examine emotion processing in terms 
of events. Context information is found to be very 
important for emotion recognition. Furthermore, 
most emotions are expressed with the presence of 
causes in context, which implies that emotion 
cause is the crucial information for emotion rec-
ognition. In addition, emotion cause detection also 
explores deep understanding of an emotion. Com-
pared to emotion recognition, emotion cause de-
tection requires more semantic and pragmatic 
information. In this paper, based on the in-depth 
linguistic analysis, we extract different kinds of 
constructs to identify cause events for an emotion.  
To conclude, emotion processing is a compli-
cated problem. In terms of emotion keywords, 
how to understand appropriately to enhance emo-
tion recognition needs more exploration. With 
respect to emotion causes, first, event processing 
itself is a challenging topic, such as event extrac-
tion and co-reference. Second, how to combine 
event and emotion in NLP is still unclear, but it is 
a direction for further emotion studies.  
References  
Abbasi, A., H. Chen, S. Thoms, and T. Fu. 2008. Af-
fect Analysis of Web Forums and Blogs using Cor-
relation Ensembles?. In IEEE Tran. Knowledge and 
Data Engineering, vol. 20(9), pp. 1168-1180. 
Aman, S. and S. Szpakowicz. 2007. Identifying Ex-
pressions of Emotion in Text. In Proceedings of 
10th International Conference on Text, Speech and 
Dialogue, Lecture Notes in Computer Science 4629, 
196--205.   
Chang, D.-S. and K.-S. Choi. 2006. Incremental cue 
phrase learning and bootstrapping method for cau-
sality extraction using cue phrase and word pair 
probabilities. Information Processing and Man-
agement. 42(3): 662-678. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. Are 
Emotions Enumerable or Decomposable? And Its 
Implications for Emotion Processing. In Proceed-
ings of the 23rd Pacific Asia Conference on Lan-
guage, Information and Computation.  
Chen, Y., Y. M. Lee, S. Li and C.-R. Huang. 2010. 
Emotion Cause Detection with Linguistic Construc-
tions. In Proceedings of the 23rd International 
Conference on Computational Linguistics. 
Descartes, R. 1649. The Passions of the Soul. USA: 
Hackett Publishing Company. 
Ghazi, D., D. Inkpen and S. Szpakowicz. 2010. Hierar-
chical versus Flat Classification of Emotions in Text. 
In Proceedings of NAACL-HLT 2010 Workshop on 
Computational Approaches to Analysis and Genera-
tion of Emotion in Text. Los Angeles, CA: NAACL. 
Girju, R. 2003. Automatic Detection of Causal Rela-
tions for Question Answering. In the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, Workshop on Multilingual Summarization 
and Question Answering - Machine Learning and 
Beyond, Sapporo, Japan. 
Gupta, N., M. Gilbert, and G. D. Fabbrizio. Emotion 
Detection in Email Customer Care. In Proceedings 
of NAACL-HLT 2010 Workshop on Computational 
Approaches to Analysis and Generation of Emotion 
in Text. 
James, W. 1884. What is an Emotion? Mind, 9(34): 
188?205. 
Keshtkar, F. and D. Inkpen. 2009. Using Sentiment 
Orientation Features for Mood Classification in 
Blog Corpus. In Proceedings of IEEE International 
Conference on Natural Language Processing and 
Knowledge Eng. (IEEE NLP-KE 2009), Sep. 24-27. 
Marcu, D. and A. Echihabi. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of ACL. 
Mihalcea, R., and H. Liu. 2006. A Corpus-based Ap-
proach to Finding Happiness. In Proceedings of 
AAAI. 
Mishne, G. 2005. Experiments with Mood Classifica-
tion in Blog Posts. In Proceedings of Style2005 ? the 
1st Workshop on Stylistic Analysis of Text for Infor-
mation Access, at SIGIR 2005. 
Ortony, A., G. L. Clore, and M. A. Foss. 1987. The 
Referential Structure of the Affective Lexicon. Cog-
nitive Science, 11: 341-364. 
Pang B., L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification Using Machine Learn-
ing Techniques. In Proceedings of EMNLP02, 79-86. 
Pearl, L. and M. Steyvers. 2010. Identifying Emotions, 
Intentions, and Attitudes in Text Using a Game with 
a Purpose. In Proceedings of NAACL-HLT 2010 
Workshop on Computational Approaches to Analy-
sis and Generation of Emotion in Text. Los Angeles, 
CA: NAACL. 
Persing, Isaac and Vincent Ng. 2009. Semi-Supervised 
Cause Identification from Aviation Safety Reports. 
In Proceedings of ACL. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Quan, C. and F. Ren. 2009. Construction of a Blog 
Emotion Corpus for Chinese Expression Analysis. 
In Proceedings of EMNLP. 
Strapparava, C. and R. Mihalcea. 2008. Learning to 
Identify Emotions in Text. In Proceedings of the 
ACM Conference on Applied Computing ACM-SAC. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. Emo-
tion Classification Using Massive Examples Ex-
tracted from the Web. In Proceedings of COLING.   
Turner, J. H. 2000. On the Origins of Human Emotions: 
A Sociological Inquiry into the Evolution of Human 
Affect. California: Stanford University Press. 
Wierzbicka, A. 1999. Emotions across Languages and 
Cultures: Diversity and Universals. Cambridge: 
Cambridge University Press. 
  
 The Chinese Persons Name Disambiguation Evaluation: Exploration of 
Personal Name Disambiguation in Chinese News 
 
 
Ying Chen*, Peng Jin?, Wenjie Li?,Chu-Ren Huang? 
* China Agricultural University ?Leshan Teachers? College ?The Hong Kong Polytechnic University 
chenying3176@gmail.com jandp@pku.edu.cn cswjli@comp.polyu.edu.hk 
  churenhuang@gmail.com 
   
 
 
 
 
 
 
Abstract 
Personal name disambiguation becomes hot as it 
provides a way to incorporate semantic under-
standing into information retrieval. In this cam-
paign, we explore Chinese personal name 
disambiguation in news. In order to examine how 
well disambiguation technologies work, we con-
centrate on news articles, which is well-formatted 
and whose genre is well-studied. We then design a 
diagnosis test to explore the impact of Chinese 
word segmentation to personal name disambigua-
tion. 
1 Introduction 
Incorporating semantic understanding technolo-
gies from the field of NLP becomes one of further 
directions for information retrieval. Among them, 
named entity disambiguation, which intends to 
use state-of-the-art named entity processing to 
enhance a search engine, is a hot research issue. 
Because of the popularity of personal names in 
queries, more efforts are put on personal name 
disambiguation. The personal name disambigua-
tion used both in Web Personal Search (WePS1) 
and our campaign is defined as follow. Given 
documents containing a personal name in interest, 
the task is to cluster them according to which en-
tity the name in a document refers to.  
WePS, which explores English personal name 
disambiguation, has been held twice (Artiles et al, 
                                                           
1
 http://nlp.uned.es/weps/ 
2007, 2009). Compared to the one in English, per-
sonal name disambiguation in Chinese has special 
issues, such as Chinese text processing and Chi-
nese personal naming system. Therefore, we hold 
Chinese personal name disambiguation (CPND) to 
explore those problems. In this campaign, we 
mainly examine the relationships between Chinese 
word segmentation and Chinese personal name 
disambiguation.  
Moreover, from our experiences in WePS 
(Chen et al, 2007, 2009), we notice that webpages 
are so noisy that text pre-processing that extracts 
useful text for disambiguation needs much effort. 
In fact, text pre-processing for webpages is rather 
complicated, such as deleting of HTML tags, the 
detection of JavaScript codes and so on. Therefore, 
the final system performance in the WePS cam-
paign sometimes does not reflect the disambigua-
tion power of the system, and instead it shows the 
comprehensive result of text pre-processing as 
well as disambiguation. In order to focus on per-
sonal name disambiguation, we choose news 
documents in CPND. 
The paper is organized as follows. Section 2 de-
scribes our formal test including datasets and 
evaluation. Section 3 introduces the diagnosis test, 
which explores the impact of Chinese word seg-
mentation to personal name disambiguation. Sec-
tion 4 describes our campaign, and Section 5 
presents the results of the participating systems. 
Finally, Section 6 concludes our main findings in 
this campaign. 
 
2 The Formal Test 
2.1 Datasets 
To avoid the difficulty to clean a webpage, we 
choose news articles in this campaign. Given a 
full name in Chinese, we search the character-
based personal name string in all documents of 
Chinese Gigaword Corpus, a large Chinese news 
collection. If a document contains the name, it is 
belonged to the dataset of this name. To ensure 
the popularity of a personal name, we keep only a 
personal name whose corresponding dataset com-
prises more than 100 documents. In addition, if 
there are more than 300 documents in that dataset, 
we randomly select 300 articles to annotate. Fi-
nally, there are totally 58 personal names and 
12,534 news articles used in our data, where 32 
names are in the development data and 26 names 
are in the test data, as shown Appendix Table 4 
and 5 separately.   
From Table 4 and 5, we can find that the ambi-
guity (the document number per cluster) distribu-
tion is much different between the development 
data and the test data.  In fact, the ambiguity var-
ies with a personal name in interest, such as the 
popularity of the name in the given corpus, the 
celebrity degree of the name, and so on.    
2.2 Evaluation  
In WePS, Artiles et al (2009) made an intensive 
study of clustering evaluation metrics, and found 
that B-Cubed metric is an appropriate evaluation 
approach. Moreover, in order to handle overlap-
ping clusters (i.e. a personal name in a document 
refers to more than one person entity in reality), 
we extend B-Cubed metric as Table 1, where S = 
{S1, S2, ?} is a system clustering  and R = {R1, 
R2, ?} is a gold-standard clustering. The final 
performance of a system clustering for a personal 
name is the F score (?= 0.5), and the final per-
formance of a system is the Mac F score, the aver-
age of the F scores of all personal names. 
Moreover, Artiles et al (2009) also discuss 
three cheat systems: one-in-one, all-in-one, and 
the hybrid cheat system. One-in-one assigns each 
document into a cluster, and in contrast, all-in-one 
put all documents into one cluster. The hybrid 
cheat system just incorporates all clusters both in 
one-in-one and all-in-one clustering. Although the 
hybrid cheat system can achieve fairly good per-
formance, it is not useful for real applications. In 
the formal test, these three systems serve as the 
baseline.  
 
 Formula 
 
Precision 
?
? ?
?
? ? ??
?
S
S d dR
 i
 i i j j
S
i
S S i
ji
R;R
|S|
|S|
|RS|
 
max
 
 
Recall 
?
? ?
?
? ? ??
?
R
R d dS
 i
 i i j j
R
i
R i
ji
R S;S
|R|
|R|
|SR|
 
max
 
Table 1: the formula of the modified B-cubed 
metrics 
3 The Diagnosis Test 
Because of no word delimiter, Chinese text proc-
essing often needs to do Chinese word segmenta-
tion first. In order to explore the relationship 
between personal name disambiguation and word 
segmentation, we provide a diagnosis data which 
attempts to examine the impact of word segmenta-
tion to disambiguation.  
Firstly, for each personal name, its correspond-
ing dataset will be manually divided into three 
groups as follows. The disambiguation system 
then runs for each group of documents. The three 
clustering outputs are merged into the final clus-
tering for that personal name.  
(1) Exactly matching: news articles contain-
ing personal names that exactly match 
the query personal name. 
(2) Partially matching: news articles contain-
ing personal names that are super-strings 
of the query personal name. For instance, 
an article that has a person named with 
????? (Gao Jun Tian)  is retrieved 
for the query personal name ???? (Gao 
Jun).  
(3) Discarded: news articles containing 
character sequences that match the query 
personal name string and however in fact 
are not a personal name. For instance, an 
article that has the string ??????
?? (Zui Gao Jun Shi Fa Yuan: supreme 
military court) is also retrieved for the  
personal name ???? (Gao Jun).  
 
This diagnosis test is designed to simulate the 
realistic scenario where Chinese word segmenta-
tion works before personal name disambiguation. 
If a Chinese word segmenter works perfectly, a 
word-based matching can be used to retrieve the 
documents containing a personal name, and arti-
cles in Groups (2) and (3) should not be returned. 
The personal name disambiguation task that is 
limited to the documents in Group (1) should be 
simpler. 
Moreover, in this diagnosis test, we propose a 
baseline based on the gold-standard word segmen-
tation as follows, namely the word-segment sys-
tem.  
1) All articles in the ?exactly matching? 
group are merged into a cluster, and all 
articles in the ?discarded? group are 
merged into a cluster. 
2) In the ?partially matching? group, enti-
ties exactly sharing the same personal 
name are merged into a cluster.  For ex-
ample, all articles containing ????? 
(Gao Jun Tian) are merged into a cluster, 
and all articles containing???? (Gao 
Jun Hua) are merged into another cluster. 
4 Campaign Design  
4.1 The Participants 
The task of Chinese personal name disambigua-
tion in news has attracted the participation of 10 
teams. As a team can submit at most 2 results, 
there are 17 submissions from the 10 teams in the 
formal test, and there are 11 submissions from 7 
teams in the diagnosis.  
4.2 System descriptions 
Regarding system architecture, all systems are 
based on clustering, and most of them comprise 
two components: feature extraction and clustering. 
However, NEU-1 and HITSZ_CITYU develop a 
different clustering, which in fact is a cascaded 
clustering. Taking the advantage of the properties 
of a news article, both systems first divide the 
dataset for a personal name into two groups ac-
cording to whether the person in question is a re-
porter of the news. They then choose a different 
strategy to make further clustering for each group.    
In terms of feature extraction, we find that all 
systems except SoochowHY use word segmenta-
tion as pre-processing. Moreover, most systems 
choose named entity detection to enhance their 
feature extraction. In addition, character-based 
bigrams are also used in some systems.  In Ap-
pendix Table 6, we give the summary of word 
segmentation and named entity detection used in 
the participating systems. 
Regarding clustering algorithms, agglomerative 
hierarchical clustering is popular in the submis-
sions. Moreover, we find that weight learning is 
very crucial for similarity matrix, which has a big 
impact to the final clustering performance. Be-
sides the popular Boolean and TFIDF weighting 
schemes, SoochowHY and NEU-2 use different 
weighting learning. NEU-2 manually assigns 
weights to different kinds of features. So-
ochowHY develops an algorithm that iteratively 
learns a weight for a character-based n-gram.  
5 Results  
We first provide the performances of the formal 
test, and make some analysis. We then present and 
discuss the performances of the diagnosis test.  
5.1 Results of the Formal test 
For the formal test, we show the performances of 
11 submissions from 10 teams in Table 2. For 
each team, we keep only the better result except 
the NEU team because they use different tech-
nologies in their two submissions (NEU_1 and 
NEU_2).  
From Table 2, we first observe that 7 submis-
sions perform better than the hybrid cheat system. 
In contrast, in Artiles et al (2009), only 3 teams 
can beat the hybrid system. From our analysis, 
this may attribute to the following facts.  
1) Personal name disambiguation on Chinese 
may be easier than the one on English. For 
example, one of key issues in personal name 
disambiguation is to capture the occurrences 
of a query name in text. However, various 
personal name expressions, such as the use of 
 Precision Recall Macro F  
NEU_1 95.76 88.37 91.47 
NEU_2 95.08 88.62 91.15 
HITSZ_CITYU 83.99 93.29 87.42 
ICL_1 83.68 92.23 86.94 
DLUT_1 82.69 91.33 86.36 
BUPT_1 80.33 94.52 85.79 
XMU 90.55 84.88 85.72 
Hybrid cheat system 73.48 100 82.37 
HIT_ITNLP_2 91.08 62.75 71.03 
BIT 80.2 68.75 68.4 
ALL_IN_ONE 52.54 100 61.74 
BUPT_pris02 72.39 58.35 57.68 
SoochowHY_2 84.51 44.17 51.42 
ONE_IN_ONE 94.42 14.41 21.07 
Table 2: The B-Cubed performances of the formal test  
  
 Precision Recall Macro F  
NEU_1 95.6 89.74 92.14 
NEU_2 94.53 89.99 91.66 
XMU 89.84 89.84 89.08 
ICL_1 84.53 93.42 87.96 
BUPT_1 80.43 95.41 86.18 
Word_segment system 71.11 100 80.92 
BUPT_pris01 77.91 75.09 74.25 
BIT 94.62 63.32 72.48 
SoochowHY 87.22 58.52 61.85 
Table 3: The B-Cubed performances of the diagnosis test   
 
middle names in English, cause many prob-
lems during recognizing of the occurrences 
of a personal name in interest. 
2) We works on news articles, which have less 
noisy information compared to webpages 
used in Artiles et al (2009). More efforts are 
put on the exploration directly on disam-
biguation, not on text pre-processing. Fur-
thermore, most of systems extract features 
based on some popular NLP techniques, such 
as Chinese word segmentation, named entity 
recognition and POS tagger. As those tools 
usually are developed based on news corpora, 
they should extract high-quality features for 
disambiguation in our task.  
 
We then notice that the NEU team achieves the 
best performance. From their system description, 
we find that they make some special processing 
just for this task. For example, they develop a per-
sonal name recognition system to detect the occur-
rences of a query name in a news article, and a 
cascaded clustering for different kinds of persons. 
5.2 Results of the Diagnosis test 
We present the performances of 8 submissions for 
the diagnosis test from 7 teams in Table 3 as the 
format of Table 2. Meanwhile, we use the word-
segment system as the baseline.  
  Comparing Table 2 and 3, we first find that the 
word-segment system has a lower performance 
than the hybrid cheat system although the word-
segment system is more useful for real applica-
tions. This implies the importance to develop an 
appropriate evaluation method for clustering. 
From Table 3, five submissions achieve better 
performances than the word-segment system.  
Given the gold-standard word segmentation on 
personal names in the diagnosis test, from Table 3, 
our total impression is that the top systems take 
less advantages, and the bottom systems take 
more. This indicates that bottom systems suffer 
from their low-quality word segmentation and 
named entity detection. For example, 
BUPT_pris01 increases ~22% F score (from 
52.81% to 74.25%). 
6 Conclusions 
This campaign follows the work of WePS, and 
explores Chinese personal name disambiguation 
on news. We examine two issues: one is for Chi-
nese word segmentation, and the other is noisy 
information. As Chinese word segmentation usu-
ally is a pre-processing for most NLP processing, 
we investigate the impact of word segmentation to 
disambiguation. To avoid noisy information for 
disambiguation, such as HTML tags in webpage 
used in WePS, we choose news article to work on 
so that we can capture how good the state-of-the-
art disambiguation technique is. 
References  
 Artiles, Javier, Julio Gonzalo and Satoshi Sekine.2007. 
The SemEval-2007 WePS Evaluation: Establishing 
a benchmark for the Web People Search Task. In 
Proceedings of Semeval 2007, Association for Com-
putational Linguistics. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 2009. 
WePS 2 Evaluation Campaign: overview of the Web 
People Search Clustering Task. In 2nd Web People 
Search Evaluation Workshop (WePS 2009), 18th 
WWW Conference. 
Bagga, Amit and Breck Baldwin.1998. Entity-based 
Cross-document Co-referencing Using the Vector 
Space Model. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics.  
Chen, Ying and James H. Martin. 2007. CU-COMSEM: 
Exploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation. In Proceedings of Se-
meval 2007, Association for Computational 
Linguistics.  
Chen, Ying, Sophia Yat Mei Lee and Chu-Ren Huang. 
2009. PolyUHK: A Robust Information Extraction 
System for Web Personal Names. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th 
WWW Conference.  
 
 
Appendix 
 
name document #  cluster # document #  per cluster 
?? 
155 37 4.19 
?? 
301 42 7.17 
?? 
300 5 60 
?? 
105 30 3.5 
?? 
156 42 3.71 
??? 
350 15 23.33 
?? 
269 70 3.84 
?? 
257 8 32.13 
?? 
211 109 1.94 
?? 
177 36 4.92 
?? 
358 165 2.17 
?? 
300 20 15 
?? 
140 57 2.46 
?? 
300 27 11.11 
?? 
296 73 4.05 
?? 
135 75 1.8 
?? 
297 14 21.21 
?? 
110 24 4.58 
?? 
207 68 3.04 
?? 
131 26 5.04 
?? 
145 22 6.59 
?? 
164 15 10.93 
??? 
247 20 12.35 
?? 
173 34 5.09 
??? 
171 21 8.14 
?? 
170 34 5 
?? 
195 32 6.09 
?? 
301 22 13.68 
?? 
318 76 4.18 
?? 
234 117 2 
?? 
134 9 14.89 
?? 
123 7 17.57 
 
6930 1352 5.13 
Table 4: The training data distribution 
 
 
name document #  cluster # document #  per cluster 
?? 
190 96 1.99 
?? 
191 5 38.2 
?? 
258 16 16.13 
??? 
224 32 7 
??? 
118 29 4.07 
?? 
239 21 11.38 
?? 
208 43 4.84 
??? 
201 17 11.82 
??? 
317 3 105.67 
??? 
151 6 25.17 
?? 
188 61 3.08 
??? 
200 2 100 
?? 
213 69 3.09 
??? 
182 5 36.4 
?? 
278 11 25.27 
?? 
180 4 45 
??? 
286 1 286 
?? 
206 38 5.42 
?? 
193 16 12.06 
??? 
172 9 19.11 
?? 
174 5 34.8 
?? 
299 39 7.67 
?? 
233 90 2.59 
?? 
300 13 23.08 
?? 
141 25 5.64 
??? 
262 13 20.15 
 
5604 669 8.38 
Table5: The test data distribution 
 
 
 Word segmentation Named Entity 
NEU Name: Neucsp 
Source: 1998 People's Daily   
Name: in-house 
HITSZ_CITYU   
ICL Name: LTP 
F score: 96.5% 
Source:  2nd SIGHAN 
Name: LTP   
 
DLUT   
BUPT Name: in-house 
F score: 96.5% 
Source: SIGHAN 2010 
 
XMU Name: in-house 
Source: 1998 People's Daily 
F score: 97.8% 
 
HIT_ITNLP Name: IRLAS 
Source: 1998 People's Daily 
F score: 97.4% 
Name: IRLAS 
 
BIT Name: ICTCLAS2010   
Precision: ~97% 
Source: 1998 People's Daily   
Name: ICTCLAS2010   
 
BUPT_pris Name: LTP 
 
Name: LTP 
SoochowHY None None 
Table 6: The summary of word segmentation and named entity detection used in the participants 
 
* LTP(Language Technology Platform) 
 
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 105?113,
Dublin, Ireland, August 23, 2014.
Exploring Mental Lexicon in an Efficient and Economic Way:
Crowdsourcing Method for Linguistic Experiments
Shichang Wang, Chu-Ren Huang, Yao Yao, Angel Chan
Department of Chinese and Bilingual Studies
The Hong Kong Polytechnic University
Hung Hom, Kowloon, Hong Kong
shi-chang.wang@connect.polyu.hk
{churen.huang, y.yao, angel.ws.chan}@polyu.edu.hk
Abstract
Mental lexicon plays a central role in human language competence and inspires the creation of
new lexical resources. The traditional linguistic experiment methodwhich is used to exploremen-
tal lexicon has some disadvantages. Crowdsourcing has become a promising method to conduct
linguistic experiments which enables us to explore mental lexicon in an efficient and economic
way. We focus on the feasibility and quality control issues of conducting Chinese linguistic ex-
periments to collect Chinese word segmentation and semantic transparency data on the interna-
tional crowdsourcing platforms Amazon Mechanical Turk and Crowdflower. Through this work,
a framework for crowdsourcing linguistic experiments is proposed.
1 Introduction
Mental lexicon as a theoretical construct has two important implications. For an individual, it is where all
the grammatical and world information is stored and organized to enable speech. For a group of speakers
of the same language, however, the mental lexicon is a shared knowledge structure allowing speakers
to process and understand what each other said. WordNets, for example the English WordNet (Miller,
1995) and the Chinese WordNet (CWN) (Huang et al., 2003), and ontologies, for example the Suggested
Upper Merged Ontology (SUMO) (Niles and Pease, 2001) and the Sinica BOW (Huang et al., 2010),
have been proposed as a representational framework for this shared mental lexicon; and psycho- and
neuro-linguistic experiments have been designed to explore how individuals access their mental lexicon.
However, the question of whether there is a shared principle or strategy of mental lexicon by all speakers
of the same language was never seriously studied as the cognitive experimental paradigm does not allow
manipulation of a large number of subjects simultaneously. In this paper, we explore the possibility of
conducting lexical access related experiments through crowdsourcing. With the crowdsourcing experi-
ments, we intend to ask specific question about the share strategy of determination of lexical units, as
well as determination of semantic transparencies, two issues that would have direct implications of how
individuals access their mental lexicon.
Many scholars discuss applying crowdsourcing method to language resource construction recent years
(Snow et al., 2008; Callison-Burch and Dredze, 2010; Munro et al., 2010; Gurevych and Zesch, 2013).
Crowdsourcing has been proved to be an efficient tool to build lexical resources, for example, Wiktionary,
whose goal is to become the free online dictionary for all the words in all languages; Biemann (2013)
presents another example which creates the Turk BootstrapWord Sense Inventory for 397 frequent nouns
from scratch using AmazonMechanical Turk. And there is more andmore literature focusing on conduct-
ing experiments on crowdsourcing platforms (Schnoebelen and Kuperman, 2010; Paolacci et al., 2010;
Berinsky et al., 2011; Rand, 2012; Mason and Suri, 2012; Crump et al., 2013). Using crowdsourcing
method, it is easier to access highly diverse and huge amount of participants, so it is possible to obtain
more representative language behavioral data. The anonymous nature of crowdsourcing makes the par-
ticipants more open to contribute sensitive data. And Crowdsourcing experiments are usually much faster
and cheaper than laboratory experiments which enables ? faster iteration between developing theory and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
105
executing experiments? (Mason and Suri, 2012). It can be a promising tool to explore mental lexicon in
an efficient and economic way.
MTurk and Crowdflower are perhaps the two most important MTurk-like crowdsourcing platforms.
MTurk is the platform appears most frequently in the literature, so popular that it represents a major genre
of crowdsourcing and its name has become the name of that genre. Crowdflower is a rapid developing
platform and is drawing more and more attention. Although they are both MTurk-like platforms, they
differ from each other. On the MTurk platform, invalid responses submitted can be manually rejected
which is a very convenient quality control method; however Crowdflower has a much larger worker pool
than MTurk. Since MTurk is one channel of Crowdflower and Crowdflower can access the worker pool
of MTurk1, besides MTurk, Crowdflower has several dozens of other channels to which it can distribute
tasks. More importantly, Crowdflower is more accessible to requesters outside the U.S. (MTurk does not
support requesters outside the U.S. by now). Crowdflower basically doesn?t support manual rejection
of invalid responses but it integrates an effective quality control method named Test Questions which
uses predefined gold standard questions to measure the quality of contributions of workers and screens
low quality workers automatically in order to produce high quality data. Unfortunately, it is not suitable
to our task, for it requires multiple submissions form a worker. Neither MTurk nor Crowdflower is a
native Chinese crowdsourcing platform, so we can suppose that native Chinese speakers can only occupy
a small proposition in their worker pools, in this case, a larger worker pool means higher possibility of
successful data collection.
We have two objectives in this study: (1) to check if it is feasible to conduct Chinese language experi-
ments to collect Chinese word segmentation and semantic transparency (Libben, 1998) data which can be
used to explore the mental lexicon of Chinese speakers on international crowdsourcing platforms, such as
Amazon Mechanical Turk (MTurk) and Crowdflower; (2) to identify and solve some quality control and
experimental design issues in order to obtain high quality data and to establish a preliminary framework
for crowdsourcing linguistic experiments.
2 Initial Calibration Tests
Before the experiment, we conducted initial calibration tests. The purpose is to lay a basic foundation
(e.g., general experimental parameters, quality control methods, etc.) for the experiment. There are four
tests. We employed a problem-driven bootstrapping strategy in the design and conduct of these tests in
order to accumulate knowledge effectively. The repeated procedure is like this: one test is started and
once a problem has been identified, the test will be paused or stopped; after a proper solution has been
found, a modified version of that test will be resumed or a new test will be designed and started.
2.1 Parameters
Both MTurk and Crowdflower will be tested, however we cannot access MTurk directly as a requester
since it doesn?t support requesters outside the U.S. by now. Because MTurk is a channel of Crowdflower,
Crowdflower can distribute jobs to it, we can access it indirectly through Crowdflower. So Crowdflower
is selected as our job publishing platform. Whenwe testMTurk, wewill require Crowdflower to distribute
our jobs to the MTurk channel only; and when we test Crowdflower, we will conduct Crowdflower to
distribute our jobs to all of its channels.
The jobs published on Crowdflower can be divided into two types according to the existence or absence
of data-sets to be processed. A job without a data-set to be processed is a survey. The survey type fits
our objectives best since we want to collect data from different individuals and each person can only
participate in any one of the tests/experiments once. In order to ensure this one-time participation, we
use the following constrains: (1) each worker account can only submit one response, and (2) each IP
address can only submit one response.
Crowdflower allows us to specify ?included countries? or ?excluded countries? as an access control
method. We only use ?included countries? in our tests/experiments, only the countries and regions we
1However, this has become history, in December 2013, Crowdflower announced that Amazon Mechanical Turk would no
longer be a partner channel, see http://www.crowdflower.com/blog/2014/01/crowdflower-drops-mechanical-turk-to-ensure-the-
best-results-for-its-customers (Retrieved May 24, 2014).
106
selected are allowed to access our jobs. According to the distribution of Chinese people, we select the
following countries and regions: Mainland China, Hong Kong, Macau, Taiwan, Singapore, Malaysia,
Japan, Korea, Australia, Canada, the United States, the United Kingdom, France, Germany, Russia, and
New Zealand.
2.2 Test 1
The objective is to evaluate the feasibility to collect Chinese language data from MTurk. We published a
survey on Crowdflower and it was distributed to the MTurk channel only. The questionnaire contains 3
questions: (1) what place of China do you come from, (2) what country or region are you in now, (3) what
dialect of Chinese do you speak; all the questions are in Chinese. There is a text-box for each question
which allows the participants to input their answers. All the questions must be answered or the data are
not allowed to be submitted. It only takes 10 to 15 seconds to fill up the questionnaire. The unit price
of this survey is one cent. This test only collected 2 responses in 21 hours. Judged from the speed, we
can preliminarily conclude that MTurk is not a feasible platform for Chinese language data collecting
tasks. Because of the properties of crowdsourcing environment, this result can be accidental, so we will
continue to open the MTurk channel to validate this result.
2.3 Test 2
We published a new test on Crowdflower in order to evaluate the feasibility of collecting Chinese lan-
guage data from Crowdflower. It is mostly the same as Test 1, the only difference is that all the channels
of Crowdflower are enabled so we can access a much larger worker pool. This time, we collected 23
responses in 2 hours. Nine of them (39.1%) are valid, 14 (60.9%) are invalid. The speed is good, but the
data quality is not acceptable. In this test, we didn?t use powerful quality control method, thus large num-
ber of invalid responses were submitted. Invalid responses deteriorated data quality. This demonstrates
that quality control is essential to crowdsourcing practices.
2.4 Test 3
It?s important to detect and identify invalid responses. ?Checkpoint questions? (see section 5) can be
used to distinguish valid responses from invalid responses. This test attempts to test the effectiveness of
checkpoint questions. We added a Chinese character identification question to the questionnaire of Test
2. The participants are required to identify a Chinese character in a picture and then input this character
into a text-box. The frequency of that character is very high, so it is easy for Chinese native speakers
to identify. Because this an open-ended question, so it is robust enough. This question satisfies the
conditions to be a checkpoint question (see section 5). Then the test was resumed. After 2 hours, 9 new
responses were received (23 responses had been received since Test 2). Four of them (44.4%) are valid
responses, 5 (55.6 %) are invalid. All of the responses with correct answers to the checkpoint question
were checked to be valid responses.
Logically, correctly answering checkpoint questions doesn?t definitely mean the other questions are
also carefully answered. But human behaviors have certain consistency to some extent. If they carefully
answered checkpoint questions, then they are likely to answer the other questions carefully. Although it
is not 100% reliable to identify invalid/valid responses by checkpoint questions, it is acceptable if there
is no better method.
2.5 Test 4
Checkpoint questions can identify invalid responses but they cannot block them. We can set some condi-
tions for the submission of responses. Only the responses which satisfy these conditions can be submitted.
We call these submission conditions ?validations? (see section 5). Since checkpoint questions can be used
to identify invalid responses, we can set validations on them in order to block the submission of invalid
responses. We set a validation on the Chinese character identification checkpoint question: the response
can be submitted only when the checkpoint question is correctly answered. Then the test was resumed
and 28 new responses were received (a total of 60 responses had been received since Test 2). 26 of them
(92.9%) are valid responses, 2 (7.1%) are invalid. Before the adoption of validation, the proportion of
107
valid response is only 40.6%, after the adoption, it?s 92.9%. This basically shows that it can effectively
block invalid responses to set validation on checkpoint questions.
2.6 Summary
We collected 60 responses in Tests 1 to 4; among them, there are only two responses from MTurk. This
verified the result of Test 1, i.e., it is not quite feasible to collect Chinese language data fromMTurk at least
by now. However Crowdflower is a feasible choice since it has a much larger worker pool. Because of
the nature of Crowdsourcing, noise is everywhere. It is practically unacceptable to collect data without
effective quality control methods; otherwise more invalid responses than valid ones will be received.
Checkpoint questions can be used to identify valid and invalid responses. Validations are effective to
block the submission of invalid responses. It is a good strategy to set validations on checkpoint questions
in order to block invalid responses.
3 Experiment
The experiment was divided into two stages, and there was a time interval of about two months between
them. Based on the initial calibration tests, the experiment was conducted to test the feasibility of col-
lecting Chinese language data on international crowdsourcing platforms and to identify and solve some
quality control and experimental design issues.
Our original plan was to conduct one experiment to collect a sample of 200 responses. But after we
had collected 135 responses, we found a serious spammer problem which must be properly solved oth-
erwise the data quality would be greatly threatened and the feasibility of our task would be questionable.
Meanwhile, we found the amounts of responses from the region of mainland China and the channel ?bit-
coinget? were unexpectedly large, we doubted that it might result from the frequent media reports on
bitcoin at that time in mainland China. When the media reports ebbed, would the experiment be replica-
ble? Thus we thought it?s necessary to pause the experiment to seek a solution for the spammer problem
and to evade the strong external factor of media report. Thus the experiment was divided into two stages.
We chose to pause the experiment instead of stopping it so that the participants who had already taken
part in the experiment (Stage 1) could not take part again when the experiment was resumed (Stage 2).
The experiment was resumed after two months, with a spammer monitor program based on the API of
Crowdflower which could detect and combat spammers automatically. Other aspects of the experiment
remained unchanged. The Stage 2 experiment could be used to check the experimental repeatability and
to solve the spammer problem found in Stage 1.
3.1 Experimental Design
Questionnaire
The experiment we ran was a self-paced online questionnaire, consisting of 46 questions divided into
three parts. The first part contained 10 screening questions designed to verify that the participants were
(1) human and (2) native Chinese speakers. The second part of the experiment was a task of Chinese word
segmentation. The participants were presented with 12 Chinese sentences and their task was to put a ?/?
sign at the word boundary that they perceived. The third part of the experiment was a semantic trans-
parency data collection task. Semantic similarity rating tasks were used to obtain semantic transparency
data. 12 di-morphemic Chinese compounds, (e.g., ?? bangzhu, help-assist, ?help?) were shown in
12 carrier sentences (one target compound per carrier sentence). The participant?s task was to rate, on
a 5-point scale, the degree of semantic similarity between the meaning of each character in the target
compound and the meaning when it is used alone. In view of the different character systems used in
different Chinese-speaking regions, we implemented two versions of the questionnaire: a simplified Chi-
nese character version for participants from Mainland China and a traditional Chinese character version
for participants from Hong Kong.
Experiment Control
Experiment control measures are used to ensure the validity of participants and their participations. Be-
cause we cannot access the real identities of the participants, we can only use some indirect methods
108
which are not completely reliable but can satisfy our demands at large. Firstly, all the participants must
be native Chinese speakers. The questionnaire was displayed in Chinese characters which can be a natural
barrier to non-native Chinese speakers. Ten screening questions are designed in the questionnaire to test
the language backgrounds of the participants. By the above measures, we can effectively discriminate
native Chinese speakers from non-native ones. Chinese learners are not a major threat due to their small
amount and low overall Chinese fluency. We invited two Chinese learners to test our questionnaire;
neither of them could finish it. Secondly, one participant can only submit one response. We used the
methods which are already explained in 2.1: one account can only submit one response; one IP address
can only submit one response.
Quality Control
In addition to experiment control measures, quality control measures are used to further prevent invalid
responses. We used checkpoint questions and other measures for data validation. Only those responses
that fulfill the following conditions were considered as valid responses: (1) the screening questions in Part
1 were correctly answered, (2) the answers in Part 2 followed the correct format, and (3) the completion
time was equal or greater than 5 minutes. Those that failed one or more conditions were considered as
invalid. The effectiveness of the validation measures is discussed in 5. After the Stage 1 experiment, we
found a serious spammer problem. After adopting the above quality control measures, spammers became
the biggest threat to data quality. It can be exhausted to combat spammers manually due to their high
speeds and randomness. Thus, based on the API of Crowdflower we wrote a spammer monitor program
to detect and combat spammers automatically.
Parameters
The experiment uses the parameters described in 2.1. Besides that, the unit price of our task is set to
US$0.25. Pricing strategy should be carefully chosen in crowdsourcing practices. High prices tend to
attract cheating, but low prices may fail to attract enough participations, see (Mason and Watts, 2010).
4 Results and Evaluation
Stage 1 of the experiment lasted for about two days, with multiple manual pauses in between to resist
spamming attempts. A total of 135 responses were received, out of which 88 (65.19%) were valid and 47
(34.81%) were invalid according to the criteria stated above. Among the valid responses, 81 (92.05%)
were contributed by participants who claimed to be from Mainland China and only 7 (7.95%) by partici-
pants from Hong Kong. 38 out of the 47 invalid responses (80.85%) were probably produced by spam-
mers because their completion times were very short and/or the validation measures were bypassed. The
3 largest source channels of valid responses were bitcoinget (n=52, 59.09%), prodege (n=11, 12.50%)
and getpaid (n=7, 7.95%), while the 3 largest source regions (based on the IP addresses) were Mainland
China (n=54, 61.36%), USA (n=14, 15.91%) and Canada (n=6, 6.82%).
Stage 2 of the experiment lasted for about 4 days also with several breaks. 65 responses were received
in Stage 2, among which 54 (83.08%) were valid and 11 (11.92%) were invalid. 46 (85.19%) of the
valid responses were contributed by participants from Mainland China and 8 (14.81%) by participants
from Hong Kong. 6 (54.55%) of the invalid responses were probably produced by spammers. The main
contributing source channels and regions of valid data in Stage 2 were slightly different from Stage 1. Top
3 source channels were prodege (n=25, 46.30%), bitcoinget (n=7, 12.96%) and instagc (n=5, 9.26 %);
top 3 source regions were Canada (n=22, 40.74%), USA (n=15, 27.78%) and Mainland China (n=11,
20.37%). Despite the different distributions of source channels and regions, the data obtained from Stage
1 and Stage 2 were highly similar, suggesting that the experiment was highly replicable.
In total, we obtained 200 responses in this experiment, among which 142 (71%) were valid. The valid
responses showed high consistency in their answers to the language tasks in Part 2 and Part 3. For exam-
ple, among the 127 valid responses fromMainland China, the answers to the word segmentation questions
in Part 2 had an average consistency2 of 74.30% (SD=12.94%), while the semantic similarity ratings in
2Consistency here means the percentages of the majority-voted answers; if we consider the second most frequent answers,
109
Part 3 had an average consistency of 58.46% (SD=21.97%). Majority-voted answers and ratings were
verified by a team of trained linguists as the most likely segmentations/ratings of the given linguistic
materials, while the less popular answers were also verified as possible or reasonable alternatives. These
results suggest that the language behavioral data acquired in this experiment, when pruned of invalid
responses, were largely consistent with expectations for native language users? judgment.
4.1 Chinese Word Segmentation Data Example
In the experiment, the participants were required to segment 12 short Chinese sentences; because of space
limitation, we will only present the results of one representative sentence here. The theoretical segmen-
tation result of the target Chinese sentence ?????????????? (lit., character by character:
only-have-rely on-depend on-crowd-mass-only-can-do-well-job-work, ?The job can only be done well
by relying on the messes? ) is ???/??/??/?/?/?/?/??? (lit. word by word: only/rely on/the
messes/only/can/do/well/job) in which the symbol ?/? indicates word boundaries. The segmentation re-
sults of this sentence obtained in the experiment are listed in Table 1. We can see that the consistency
is high, however the majority-voted result ???/??/??/??/??/??? is different from the the-
oretical segmentation result. Most participants treat the slice ???? as one word instead of two words
and the same thing happened to the slice ????. Speakers? intuition can be different from theoretical
analysis: this is an important clue to investigate the representation of Chinese words in the mental lexicon
of Chinese speakers.
Segmentation Result n %
??/??/??/??/??/?? 100 78.74
??/??/??/??/?/?/?? 11 8.66
??/??/??/?/?/??/?? 5 3.94
??/??/??/?/?/?/?/?? 4 3.15
??/??/??/?/???/?? 2 1.57
??/??/??/?????? 1 0.79
??/??/??/??/???? 1 0.79
??/????/??/???? 1 0.79
??/????/??/?/?/?? 1 0.79
?/?/??/??/?/?/?/?/?? 1 0.79
Total 127 100
Table 1: Chinese Word Segmentation Data Example
4.2 Semantic Similarity Rating Data Example
Semantic transparency affects the representation and processing of compounds (Libben, 1998; Han et al.,
2014). In the experiment, we use semantic similarity rating tasks to collect semantic transparency data
of 12 compounds which can be used in the studies of mental lexicon. Here we will only discuss two of
them in detail. In Chinese, ???? (dongxi, east-west, ?thing?) is a typical semantically opaque word,
because its literal meaning is ?east and west? but its actual meaning is ?thing?: we can hardly find any link
between the two. In contrast, ???? (bangzhu, help-assist, ?help?) is a typical semantically transparent
word, for its literal meaning equals its actual meaning. In our experiment, for each target word, we ask
the participants to rate to what extent the meaning of each character when it is used alone is similar to
its meaning in the target word. This kind of semantic similarity rating task enables us to estimate the
semantic transparency of the target words. The semantic similarity rating data of the above two words
are shown in Table 2, and for the results of all the words, see Table 3.
the consistency numbers can be much larger than the reported ones, especially the ones of semantic similarity rating results (see
Table 3).
110
?? dongxi, east-west, ?thing? ?? bangzhu, help-assist, ?help?
Rating Score ? dong, ?east? ? xi, ?west? ? bang, ?help? ? zhu, ?assist?
1 115 121 6 4
2 2 2 2 13
3 1 1 8 7
4 0 1 23 38
5 8 1 88 63
? 1 1 0 2
Total 127 127 127 127
Table 2: Semantic Similarity Rating Data Example
In the tables, the rating scores 1 to 5 and ??? mean ?not similar at all?, ?slightly similar?, ?moderately
similar?, ?very similar?, ?identical?, and ?unable to rate? respectively. The consistency of the semantic
similarity rating data is also very high. For example, most participants (115 out of 127) think the meaning
of ??? (dong, ?east?) when it is used alone is not similar at all to its meaning in the word ???? (dongxi,
east-west, ?thing?), and most participants (121 out of 127) think the meaning of ??? (xi, ?west?)when
it is used alone is not similar at all to its meaning in the word ???? (dongxi, east-west, ?thing?). The
consistency of the rating data of ???? (bangzhu, help-assist, ?help?) is not as high as ???? (dongxi,
east-west, ?thing?), but most participants choose 5 which is our expectation and it is also normal that
many participants choose 4, since it is next to 5. The semantic transparency estimation of the two words
based on these data is quite consistent with our expectation.
5 The Quality Control Issues
In order to obtain high quality data in crowdsourcing environments, it is fundamental to identify invalid
responses. Checkpoint questions can be used to identify them. Checkpoint questions should satisfy two
conditions. Firstly, a checkpoint question should be super easy, since making wrong judgments to super
easy questions is a clear signal of carelessness. Secondly, a checkpoint question should have a publicly
recognized correct answer or it cannot act as a standard. Checkpoint questions can be open-ended or
close-ended. Open-ended questions are usually more robust than close-ended ones, since their answers
are difficult to guess.
There are at least 3 basic measures to deal with invalid responses: (1) blocking the submission of in-
valid responses; (2) rejecting the invalid responses that have been submitted; (3) refining the data-set
received and filter out invalid responses before analysis. Adopting validations on checkpoint questions is
a good strategy. A validation is a submission condition and the submission of responses will be blocked
if the validations of them are failed. Since checkpoint questions can identify invalid responses, using val-
idations on checkpoint questions can block the submissions of invalid responses. Crowdflower supports
validation but it is implemented on the client end, so can be bypassed; but average participants usually
don?t have the required expertise to do that, so it is largely reliable.
After the adoption of the above quality control measures, spammers are the major threats to data qual-
ity. It can be exhausted to combat spammers manually, because of their high speed and randomness, so
automatic monitor programs should be used to combat them. Monitor programs use patterns to detect
spammers. Patterns may depend on the specifics of different crowdsourcing practices, but there are some
general patterns which are based on the typical behaviors of spammers and can be applied to almost all
crowdsourcing practices. One pattern is the ?temporal pattern?, abnormal high speed is an obvious feature
of spammers and can be used as a general pattern. There are two cases. One case is that the completion
time of a response is abnormally short. For instance, the normal completion time of a response is around
9 minutes, but the human spammers only needed an average of 138 seconds and the robot spammers
only needed an average of 20 seconds. The other case is that the time interval between 2 responses is
111
Rating Score
Word Character 1 2 3 4 5 ? Total
?? ? 115 2 1 0 8 1 127? 121 2 1 1 1 1 127
?? ? 94 12 8 3 9 1 127? 100 11 8 2 4 2 127
?? ? 79 15 10 9 11 3 127? 63 32 15 7 5 5 127
?? ? 109 8 3 0 7 0 127? 84 29 7 2 3 2 127
?? ? 97 13 4 3 8 2 127? 110 7 3 0 3 4 127
?? ? 80 15 15 3 9 5 127? 98 12 6 0 3 8 127
?? ? 6 2 8 23 88 0 127? 4 13 7 38 63 2 127
?? ? 2 8 12 27 78 0 127? 32 29 19 24 20 3 127
?? ? 20 23 24 26 32 2 127? 19 41 30 21 13 3 127
?? ? 4 22 20 43 36 2 127? 12 25 31 33 24 2 127
?? ? 3 13 13 44 54 0 127? 3 8 16 42 56 2 127
?? ? 3 5 16 41 62 0 127? 2 11 21 43 50 0 127
Table 3: The Complete List of Semantic Similarity Rating Data
abnormally short and several such events take place one after another. This temporal pattern can be used
to detect concurrent attacks. The other pattern is the ?violation of validations?. If the validations of a
response failed but it was still submitted, then the validations were bypassed and this is a typical behavior
of spammers. Once a spammer is detected, we can block it and reject all the responses it submitted if
the crowdsourcing platform supports these methods, otherwise we can just pause the task for a while in
order to avoid or reduce its attack.
The effect of any single quality control measures is limited; multiple measures should be used at the
same time to form a quality control system with much more control power. A reasonable quality control
system should notice two key points: (1) maximally block the submission of invalid responses, and (2)
maximally filter invalid responses out.
6 Conclusion
Our study showed that crowdsourcing is a very powerful experimental design for exploration cognitive
access to the shared Mental Lexicon of the speakers of the same language. We showed that Mandarin
speakers shared the same strategy in determination of lexical units. The strategy seems to be match more
closely with distributional information. This suggests an empirical approach to lexical unit determination
which is then subject to the influence of language use and can lead to changes in the mental lexicon.
Although our study is far from conclusive as a proof for the shared lexical access strategy, it does point
out to the great potential of pursuing this issue using crowdsourcing experiments.
112
Acknowledgements
The work described in this paper was supported by a grant from the Research Grants Council of the Hong
Kong Special Administrative Region, China (Project No. 544011).
References
Adam J Berinsky, Gregory A Huber, and Gabriel S Lenz. 2011. Using mechanical turk as a subject recruitment
tool for experimental research. Submitted for review.
Chris Biemann. 2013. Creating a system for lexical substitutions from scratch using crowdsourcing. Language
Resources and Evaluation, 47(1):97?122.
Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 1?12. Association for Computational Linguistics.
Matthew JC Crump, John V McDonnell, and Todd M Gureckis. 2013. Evaluating amazon?s mechanical turk as a
tool for experimental behavioral research. PloS one, 8(3):e57410.
Iryna Gurevych and Torsten Zesch. 2013. Collective intelligence and language resources: introduction to the
special issue on collaboratively constructed language resources. Language Resources and Evaluation, 47(1):1?
7.
Yi-Jhong Han, Shuo-chieh Huang, Chia-Ying Lee, Wen-Jui Kuo, and Shih-kuen Cheng. 2014. The modulation
of semantic transparency on the recognition memory for two-character chinese words. Memory & Cognition,
pages 1?10.
Chu-Ren Huang, Elanna I. J. Tseng, Dylan B. S. Tsai, and Brian Murphy. 2003. Cross-lingual Portability of Se-
mantic relations: Bootstrapping Chinese WordNet with English WordNet Relations. Language and Linguistics,
4.3:509?532.
Chu-Ren Huang, Ru-Yng Chang, and Shiang bin Li, 2010. Ontology and the Lexicon, chapter Sinica BOW:
Integration of Bilingual WordNet and SUMO, pages 201?211. Cambridge University Press, Cambridge.
Gary Libben. 1998. Semantic transparency in the processing of compounds: Consequences for representation,
processing, and impairment. Brain and Language, 61(1):30 ? 44.
Winter Mason and Siddharth Suri. 2012. Conducting behavioral research on amazon?s mechanical turk. Behavior
research methods, 44(1):1?23.
Winter Mason and Duncan J Watts. 2010. Financial incentives and the performance of crowds. ACM SigKDD
Explorations Newsletter, 11(2):100?108.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler
Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic data.
In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 122?130. Association for Computational Linguistics.
Ian Niles and Adam Pease. 2001. Towards a standard upper ontology. In Proceedings of the international confer-
ence on Formal Ontology in Information Systems-Volume 2001, pages 2?9. ACM.
Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis. 2010. Running experiments on amazon mechanical
turk. Judgment and Decision making, 5(5):411?419.
David G Rand. 2012. The promise of mechanical turk: How online labor markets can help theorists run behavioral
experiments. Journal of theoretical biology, 299:172?179.
Tyler Schnoebelen and Victor Kuperman. 2010. Using amazon mechanical turk for linguistic research. Psi-
hologija, 43(4):441?464.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?263. Association for Computational Linguistics.
113
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 1?10,
Dublin, Ireland, August 23 2014.
   Corpus-based Study and Identification of Mandarin Chinese Light 
Verb Variations 
Chu-Ren Huang1      Jingxia Lin2     Menghan Jiang1      Hongzhi Xu1 
1Department of CBS, The Hong Kong Polytechnic University 
2Nanyang Technological University 
churen.huang@polyu.edu.hk, jingxialin@ntu.edu.sg, 
menghan.jiang@connect.polyu.hk, hongz.xu@gmail.com 
   
Abstract 
When PRC was founded on mainland China and the KMT retreated to Taiwan in 1949, the 
relation between mainland China and Taiwan became a classical Cold War instance. Neither 
travel, visit, nor correspondences were allowed between the people until 1987, when 
government on both sides started to allow small number of Taiwan people with relatives in 
China to return to visit through a third location. Although the thawing eventually lead to 
frequent exchanges, direct travel links, and close commercial ties between Taiwan and 
mainland China today, 38 years of total isolation from each other did allow the language use to 
develop into different varieties, which have become a popular topic for mainly lexical studies 
(e.g., Xu, 1995; Zeng, 1995; Wang & Li, 1996). Grammatical difference of these two variants, 
however, was not well studied beyond anecdotal observation, partly because the near identity 
of their grammatical systems. This paper focuses on light verb variations in Mainland and 
Taiwan variants and finds that the light verbs of these two variants indeed show distributional 
tendencies. Light verbs are chosen for two reasons: first, they are semantically bleached hence 
more susceptible to changes and variations. Second, the classification of light verbs is a 
challenging topic in NLP. We hope our study will contribute to the study of light verbs in 
Chinese in general. The data adopted for this study was a comparable corpus extracted from 
Chinese Gigaword Corpus and manually annotated with contextual features that may 
contribute to light verb variations. A multivariate analysis was conducted to show that for each 
light verb there is at least one context where the two variants show differences in tendencies 
(usually the presence/absence of a tendency rather than contrasting tendencies) and can be 
differentiated. In addition, we carried out a K-Means clustering analysis for the variations and 
the results are consistent with the multivariate analysis, i.e. the light verbs in Mainland and 
Taiwan indeed have variations and the variations can be successfully differentiated. 
1 Introduction: Language Variations in the Chinese Context 
Commonly dichotomy of language and dialect is not easily maintained in the context of Chinese 
language(s). Cantonese, Min, Hakka, and Wu are traditionally referred to as dialects of Chinese but 
are mutually unintelligible. However, they do share a common writing system and literary and textual 
tradition, which allows speakers to have a shared linguistic identity. To overcome the mutual 
unintelligibility problem, a variant of Northern Mandarin Chinese, is designated as the common 
language about a hundred years ago (called ??? Putonghua ?common language? in Mainland 
China, and ??  Guoyu ?national language? in Taiwan). Referred to as Mandarin or Mandarin 
Chinese, or simply Chinese nowadays, this is the one of the most commonly learned first or second 
languages in the world now. However, not unlike English, with the fast globalization of the Chinese 
language, both the term ?World Chineses? and the recognition that there are different variants of 
Chinese emerged. In this paper, we studied two of the most important variants of Chinese, Mainland 
Mandarin and Taiwan Mandarin. 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
1
1.1 Variations between Mainland and Taiwan Mandarin: Previous studies 
The lexical differences between Mainland and Taiwan Mandarin have been the focus of research in 
Chinese Linguistics in the recent years. A number of studies were carried out on lexical variations 
between these two variants of Mandarin Chinese, including variations in the meanings of the same 
word or using different words to express the same meaning (e.g., Xu, 1995; Zeng, 1995; Wang,1996). 
Some dictionaries also list the lexical differences between Mainland Mandarin and Taiwan Mandarin 
(e.g., Qiu, 1990; Wei & Sheng, 2000).  
By contrast, only a few of such studies were corpus driven (e.g. Hong and Huang 2008, 2013; 
Huang and Lin, 2013), and even few studies have been done on the grammatical variations of 
Mainland and Taiwan Chinese. Huang et al. (2013), the only such study based on comparable corpora 
so far, suggested that the subtlety of the underlining grammatical variations of these two dialectal 
variants at early stage of divergence may have contributed to the challenge as well as scarcity of 
previous studies.  
1.2 Light Verbs in Light Verb Variations 
The study of English light verb constructions (LVCs) (e.g., take a look, make an offer) has been an 
important topic in linguistics (Jespersen, 1965; Butt and Geuder, 2003; among others) as well as in 
Computational Linguistics (Tu and Dan, 2011; Nagy et al., 2013; Hwang et al., 2010; among others). 
Identification of LVCs is a fundamental crucial task for Natural Language Processing (NLP) 
applications, such as information retrieval and machine translation. For example, Tu and Dan (2011) 
proposed a supervised learning system to automatically identify English LVCs by training with groups 
of contextual or statistical features. Nagy et al. (2013) introduced a system that enables the full 
coverage identification of English LVCs in running context by using a machine leaning approach.  
However, little work has been done to identify Chinese LVCs, especially between different 
variants of Chinese (cf. Hwang et al., 2010). Chinese LVCs are similar to English LVCs in the sense 
that the light verb itself is semantically bleached and does not contain any eventive or contentive 
information, so the predicative information of the construction mainly comes from the complement 
taken by the light verb (e.g., Zhu, 1985; Zhou, 1987; Cai, 1982). For instance, ?? jinxing originally 
meant ?move forward/proceed?, but in an LVC such as???? jinxing taolun proceed discuss ?to 
discuss?, ?? jinxing only contributes aspectual information whereas the core meaning of the LVC 
comes from the complement ?? taolun ?discuss?. Chinese also differs from English in that many of 
the Chinese light verbs have similar usages and thus are often interchangeable, e.g., all the five light 
verbs ?? congshi, ? gao, ?? jiayi, ?? jinxing, and ? zuo can take ?? yanjiu ?do research? 
as their complement and form a LVC. But Huang et al. (2013) also observed that differences in 
collocation constraints can sometimes be found between different variants of Mandarin Chinese. For 
instance, constructions like ???? jinxing tou-piao proceed cast-ticket ?to cast votes?, where the 
complement is in the V(erb)-O(bject) form, usually can only be found in Taiwan Mandarin. Hence, 
Chinese LVCs are challenging for both linguistic studies and computational applications in two 
aspects: (a) to identify collocation constraints of the different light verbs in order to automatically 
classify and predict their uses in context, and (b) to identify the collocation constraints of the same 
light verb in order to differentiate and predict the two Chinese variants based on the use of such light 
verbs. The first issue has been explored in Lin et al. (2014): by analyzing Mainland and Taiwan 
Mandarin data extracted from comparable corpora with statistical and machine learning approaches, 
the authors find the five light verbs?? congshi, ? gao, ?? jiayi, ?? jinxing, and ? zuo can 
be reliably differentiated from each other in each variety. But to the best of our knowledge, there has 
been no previous computational study on modeling the light verb variations, or other syntactic 
variations of Chinese dialects or variants of the same dialect. Therefore, this paper builds on the study 
of Lin et al. (2014) and will adopt a comparable corpus driven approach to model light verb variations 
in Mainland and Taiwan Mandarin. 
2 Data and annotation 
Our study focuses on five light verbs, ?? jiayi, ?? jinxing, ?? congshi, ? gao and ? zuo 
(these words literally meant ?proceed?, ?inflict?, ?engage?, ?do?, and ?do? respectively). These five are 
2
chosen for two reasons. First, they are the most frequently used light verbs in Mandarin Chinese (Diao, 
2004); second, although the definition of Chinese light verbs is still debatable, these five are 
considered the most typical light verbs in most previous studies.   
The data for this study was extracted from the Annotated Chinese Gigaword Corpus (Huang, 2009) 
maintained by LDC which contains over 1.1 billion Chinese words, consisting of 700 million 
characters from Taiwan Central News Agency (CNA) and 400 million characters from Mainland 
Xinhua News Agency (XNA). For each of the five light verbs, 400 sentences were randomly selected, 
half from the Mainland XNA corpus and the other half from the Taiwan CNA Corpus, which results in 
2,000 sentences in total.  
Previous studies (Zhu, 1985; Zhou, 1987; Cai, 1982; Huang et al., 2013; among others) have 
proposed several syntactic and semantic features to compare and identify the similarities and 
differences among light verbs. For example, while Taiwan ?? congshi can take informal or 
semantically negative event complements such as ??? xingjiaoyi ?sexual trade?, Mainland ?? 
congshi is rarely found with such complements (Huang et al. 2013). 
In our study, we selected 11 features covering both syntactic and semantic features which may help 
to identify light verb variations, as in Table 1. All 2,000 sentences with light verbs were manually 
annotated with the 11 features. The annotator is a trained expert on Chinese linguistics. All ambiguous 
cases were discussed with another two experts in order to reach an agreement (the features and 
annotation were the same with Lin et al. (2014)). 
 
3 Modelling and Predicting Two Variants 
We carried out both a multivariate analysis and machine learning algorithm to explore the possible 
differences existing between Mainland and Taiwan Mandarin light verbs. Our analysis shows that for 
each light verb, there is at least one context where the two variants of Mandarin show differences in 
usage tendencies and thus can be differentiated, although the differences more often lie in the 
presence/absence of a tendency rather than complementary distribution.   
3.1 Multivariate Analysis of Light Verb Variations 
As introduced in Section 1, the five or some of the five light verbs sometimes can be interchangeably 
used in both Mainland and Taiwan Mandarin. This indicates that the interchangeable light verbs share 
some features. In other words, it is unlikely that a particular feature is preferred by only one light verb 
and thus differentiates the verb from the others. This is also proved in Lin et al. (2014). For instance, 
their study finds both Mainland and Taiwan ?? congshi and ? gao significantly prefer nominal 
complements (POS.N). Therefore, to better explore the light verb differences in the two variants, we 
adopt a multivariate analysis for this study.  
The multivariate analysis we used is polytomous logistic regression (Arppe 2008, cf. Han et al. 
2013, Bresnan et al. 2007), and the tool we used is the Polytomous() function in the Polytoumous 
package in R (Arppe 2008). The polytomous logistic regression is an extension of standard logistic 
regression; it calculates the odds of the occurrence of a particular light verb when a particular feature 
is present, with all other features being equal (Arppe, 2008). In addition, it also allows for 
simultaneous estimation of the occurrence probability of all the five light verbs. 
Before we discuss the light verb variations based on multivariate analysis, we will show that the 
polytomous multivariate model adopted is reliable for our study. Table 2 presents the probability 
estimates of Mainland and Taiwan light verbs calculated by the model. The results indicate that the 
overall performance of the model is good: the most frequently predicted light verb (in each column) 
corresponds to the light verb that actually occurs in the data (in each row) (see the numbers in bold).  
In addition, the recall, precision, and F-measure of the estimates given in Table 3 show that each 
light verb in each variant can be successfully identified with a F-score better than chance (0.2), while 
the performance varies from light verb to light verb, which is thus consistent with the results in Lin et 
al. (2014). The only exception is ? gao in Mainland Mandarin, but the low F-score of ? gao (0.14) is 
consistent with the linguistic observation that this verb is rarely used as a light verb in Mainland 
Mandarin. More detailed information of the factors that can distinguish the five light verbs in each 
3
variant can also be found in Table 4. In the following of this section, we focus on the variations of 
each light verb in Mainland and Taiwan Mandarin.  
 
 
 
Feature ID Explanation Values (example) 
1. OTHERLV Whether a light verb co-
occurs with another light 
verbs 
Yes (?????? kaishi jinxing taolun Start proceed 
discuss ?start to discuss?) 
No (???? jinxing taolun proceed discuss ?to 
discuss?) 
2. ASP 
 
Whether a light verb is 
affixed with an aspectual 
marker (e.g., perfective ? 
le, durative ? zhe, 
experential ? guo) 
ASP.le (????? jinxing-le zhandou ?fighted?) 
ASP.zhe ( ? ? ? ? ?  jinxing-zhe zhandou ?is 
fighting?) 
ASP.guo (????? jinxing-guo zhandou ?fighted?) 
ASP.none (???? jinxing zhandou ?fight?) 
3. EVECOMP Event complement of a light 
verb is in subject position 
Yes (??????? bisai zai xuexiao jinxing game at 
school proceed ?The game was held at the school?) 
No (??????? zai xuexiao jinxing bisai at 
school proceed game ?the game was held at the school?)  
4. POS 
 
The part-of-speech of the 
complement taken by a light 
verb  
Noun (???? jinxing zhanzheng proceed fight ?to 
fight?) 
Verb (???? jinxing zhandou proceed fight ?to 
fight?) 
5. ARGSTR 
 
The argument structure of 
the complement of a light 
verb, i.e. the number of 
arguments (subject and/or 
objects) that can be taken by 
the complement  
One (???? jinxing zhandou proceed fight ?to fight?) 
Two (???? jinxing piping proceed criticize ?to 
criticize?)  
Zero (???? jinxing zhanzheng proceed fight ?to 
fight?) 
6. VOCOMP Whether the complement of 
a light verb is in the V(erb)-
O(bject) form  
Yes (???? jinxing tou-piao proceed cast-ticket ?to 
vote?) 
No (???? jinxing zhan-dou proceed fight-fight ?to 
fight?) 
7. DUREVT Whether the event denoted 
by the complement of a light 
verb is durative 
Yes (???? jinxing zhandou proceed fight-fight ?to 
fight?) 
No (???? jiayi jujue inflict reject ?to reject?)  
8. FOREVT Whether the event denoted 
by the complement of a light 
verb is formal or official 
Yes (?????? jinxing guoshi fangwen proceed 
state visit ?to pay a state visit?) 
No (???? zuo xiao maimai do small business ?run a 
small business?)  
9. PSYEVT Whether the event denoted 
by the complement of a light 
verb is mental or 
psychological activity 
Yes (???? jiayi fanxing inflict retrospect ?to 
retrospect?) 
No (???? jiayi diaocha inflict investigate ?to 
investigate?)  
10. INTEREVT Whether the event denoted 
by the complement of a light 
verb involves interaction 
among participants 
Yes (???? jinxing taolun proceed discuss ?to 
discuss?)  
No (???? jiayi piping inflict criticize ?to criticize?) 
11. ACCOMPEVT Whether the event denoted 
by the complement of a light 
verb is an accomplishment 
Yes (???? jinxing jiejue proceed solve ?to solve?) 
No (???? jinxing zhandou proceed fight-fight ?to 
fight?) 
Table 1: Features used to differentiate five Chinese light verbs. 
 
 
 
4
Predicted 
 
Observed   
congshi gao jiayi jinxing zuo 
ML TW ML TW ML TW ML TW ML TW 
congshi 131 64 1 87 62 39 1 10 5 0 
gao 69 8 16 139 86 36 16 16 13 1 
jiayi 1 0 1 0 192 190 6 6 0 4 
jinxing 31 18 9 34 47 80 62 67 51 1 
zuo 50 24 5 16 44 114 4 14 97 32 
Table 2: Probability estimates of Mainland (ML) and Taiwan (TW) light verbs.  
 
 Recall Precision  F-measure 
ML TW ML TW ML TW 
congshi 0.66 0.32 0.46 0.56 0.54 0.41 
gao 0.08 0.70 0.5 0.5 0.14 0.58 
jiayi 0.96 0.95 0.45 0.41 0.61 0.58 
jinxing 0.31 0.34 0.70 0.59 0.43 0.43 
zuo 0.49 0.16 0.58 0.84 0.53 0.27 
Table 3: Recall, precision, and F-measure of the polytomous multivariate estimates. 
 
 
congshi gao jiayi jinxing zuo 
ML TW ML TW ML TW ML TW ML TW 
(Intercept) (1/Inf) (1/Inf) 0.02271 (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) 
ACCOMPEVTyes (1/Inf) (0.3419) 0.09863 (1/Inf) 56.25 11.33 0.1849 (0.1607) (1/Inf) 0.2272 
ARGSTRtwo 0.2652 0.1283 2.895 (0.7613) 76.47 (Inf) (1.481) (0.7062) 0.2177 (1.217) 
ARGSTRzero (1.097) (0.6219) 3.584 7.228 (1/Inf) (4.396) (1.179) 0.5393 0.245 0.2068 
ASPle (0.7487) (1/Inf) (0.1767) (1/Inf) (0.8257) (0.3027) (0.9196) (Inf) (1.853) 32.98 
ASPno (Inf) (0.9273) (1.499) (0.6967) (Inf) (Inf) (0.2307) (Inf) (0.2389) (0.2385) 
ASPzhe (1.603) 
 
(1/Inf) 
 
(0.4571) 
 
(Inf) 
 
(1/Inf) 
 
DUREVTyes (Inf) (Inf) (2.958) (Inf) (1/Inf) (1/Inf) (Inf) (0.9575) (Inf) (Inf) 
EVECOMPyes (1/Inf) (1/Inf) (1.726) (0.8491) (1/Inf) (1/Inf) 3.975 8.113 (1.772) (0.5019) 
FOREVTyes (2.744) 0.0867 (1.227) (Inf) (Inf) (Inf) (0.7457) (1.437) 0.2679 (1.467) 
INTEREVTyes 0.03255 0.1896 (0.5281) (1/Inf) (0.5432) (0.951) 18.67 10.47 0.08902 (0.398) 
PSYEVTyes (1/Inf) (1/Inf) (1/Inf) (1/Inf) 19.87 (1.395) (1/Inf) (1/Inf) (0.9619) (3.323) 
VOCOMPyes (0.1346) 0.18 (3.043) (2.35) 23.54 (Inf) (1.086) 3.161 (0.5344) (0.5956) 
Table 4: Multivariate analysis of light verb variations in Mainland and Taiwan Mandarin. 
Table 4 summarizes the results estimated by the Polytomous multivaraite analysis. The numbers in 
the table are the odds for the features in favor of or against the occurrence of each light verb: odds 
larger than 1 indicate that the chance of the occurrence of a light verb is significantly increased by the 
feature, e.g., the chance of Mainland ?? jiayi occurring is significantly increased by ARGSTRtwo 
(76.47: 1), followed by ACCOMPEVTyes (56.25: 1), VOCOMPyes (23.54: 1), PSYEVTyes (19.87: 
1); odds smaller than 1 indicate that the chance of the occurrence of a light verb is significantly 
descreased by the feature, e.g., the chance of Mainland ??  jinxing occurring is significantly 
decreased by ACCOMPEVTyse (0.1849: 1); in addition, ?inf? and ?1/inf? refer to odds larger than 
10,000 and smaller than 1/10,000 respectively, and non-significant odds (p-value < 0.05) are given in 
parentheses, regardless of the odds value.  
Table 4 finds that Mainland and Taiwan Mandarin indeed show some variations in each light verb. 
Furthermore, the variations of each light verb mainly lie in non-complementary distributional patterns. 
That is, as highlighted in dark grey colour in Table 4, the odds differences are more often between 
non-significance (odds in parentheses) and significance (odds larger or smaller than 1), rather than 
between significant preference (odds larger than 1) and significant dis-preference (odds smaller than 
1). In other words, the difference of a light verb in the two variants is more comparative, rather than 
5
contrastive.  This explains why the variations are not easily found by traditional linguistic studies. 
The following summarizes the key variations of each light verb.  
 
?? congshi 
?? congshi in both Mainland and Taiwan Mandarin has no feature significantly in its favor and it 
is significantly disfavored by ARGSTRtwo (taking two-argument complements, e.g., ?? yanjiu ?to 
research?) and INTEREVTyes (taking complements denoting interactive activities, e.g., ?? 
shangliang ?to discuss?). However, Taiwan ??  congshi is differentiated from Mainland ?? 
congshi in that the former is also disfavored by FOREVTyes (taking complements denoting formal 
events, e.g., ?? yanjiu ?to research?) and VOCOMPyes (taking complements in the form of V(erb)-
O(bject), e.g., ?? toupiao ?cast a vote?), whereas the latter is not. The finding that Taiwan ?? 
congshi is less likely to take formal event as its complement is consistent with that in Huang et al. 
(2013).  
 
? gao 
Both Mainland and Taiwan ? gao are significantly favored by ARGSTRzero (taking zero-argument 
complements, i.e. noun complement in this study). However, compared with Taiwan Mandarin, 
Mainland ? gao is more likely to take two-argument complements (ARGSTRtwo), but less likely to 
take complements denoting accomplishment events (ACCOMPEVTyes, e.g., ?? jiejue ?to solve?), 
and it is also disfavored by the aggregate of default variable values (i.e. the intercept, 0.02: 1).  
 
??  jiayi  
Both Mainland and Taiwan ? ?  jiayi are favored by the feature ACCOMPEVTyes 
(accomplishment complement such as ??  jiejue ?to solve?), but the chance of occurrence of 
Mainland ??  jiayi increases with the presence of two-argument complements (ARGSTRtwo), 
complements in VO form (VOCOMPyes), and complements denoting mental or phychological 
activities (PSYEVTyes, e.g., ?? fanxing ?to introspect?).  
 
?? jinxing 
Both Mainland and Taiwan ?? jinxing have INTEREVTyes (taking complements denoting 
interactive activities) and EVECOMPyes (allowing event complements in subject position, e.g., ??
???? huiyi jinxing shunli meeting procced smoothly ?The meeting proceeded smoothly?) in their 
favor. However, ??  jinxing in Mainland Mandarin is less likely to take accomplishment 
complements (ACCOMPEVTyes); whereas ?? jinxing in Taiwan Mandarin is more disfavored by 
ARGSTRzero, but more likely to take complements in VO form, which is also consistent with the 
findings in Huang et al. (2013).  
 
? zuo  
The occurrence of ? zuo in Mainland Mandarin is decreased by factors such as ARGSTRtwo, 
FOREVTyes, and INTEREVTyes, whereas the occurrence of ? zuo in Taiwan Mandarin is decreased 
by ACCOMPTEVTyes, but significantly increased by ASPle. It is obvious to linguists that ? zuo in 
both Mainland and Taiwan Mandarin are frequently found with the perfective marker ? le, but our 
analysis reveals that the affixation ? le to Taiwan ? zuo is much more frequent than that in Mainland.  
3.2 Clustering Analysis of Light Verb Variations 
We adopted a vector space model (VSM) to represent the use of light verbs. The features in Table 1 
could be expanded to 17 binary features. For example, ASP could be expanded into four binary 
features: ASP.le, ASP.zhe, ASP.guo, ASP.none. Each instance of a light verb in the corpus was 
represented by a vector with 17 dimensions. Each dimension stores the value of one of the 17 binary 
features determined by the context where the light verb is used. 
6
 
Cluster ID 0 1 2 3 4 5 6 7 8 9 
congshi TW 39 43 1 84 2 21 4 4 1 1 
ML 62 48 0 83 1 4 1 1 0 0 
gao TW 38 141 0 0 9 10 2 0 4 0 
ML 88 64 3 8 11 5 10 4 6 4 
jiayi TW 152 0 6 28 11 2 0 4 0 0 
ML 117 3 6 62 18 2 5 14 1 1 
jinxing TW 26 79 7 2 38 30 0 3 15 1 
ML 23 80 16 0 55 22 5 2 1 0 
zuo TW 20 3 0 2 23 130 20 2 1 6 
ML 23 44 3 16 38 45 20 11 8 3 
Table 5: The distribution of data origin by the clustering result. 
Then we adopt a clustering algorithm K-Means to identify the variations of light verbs in Taiwan 
and Mainland Mandarin. The assumption is that the instances of a light verb will form different 
clusters in the hyperspace according to the distances among them. Each cluster reflects a special use of 
a light verb. For example, there could be one cluster, where all the instances take non-accomplishment 
event argument, e.g., ????/??/??  jiayi fenxi/ yanjiu/ pinglun inflict analyze/ research/ 
comment ?to analyze/ research/ comment?, etc.  
In this sense, if there are light verb variations between Mainland and Taiwan Mandarin, the light 
verbs will be distributed to two clusters, one with data mainly from Mainland Mandarin, whereas the 
other mainly from Taiwan Mandarin. Meanwhile, if a cluster contains much more data from one 
variant than the other, it indicates the usage of a light verb is mainly restricted to the variant with more 
data; or if a cluster contains data of similar amount from both Mainland and Taiwan Mandarin, it 
indicates that the two variants share common usages regarding the light verbs. Therefore, for each 
light verb, all 400 examples from both Mainland and Taiwan Mandarin are mixed together for the 
analysis.  
As the K-Means algorithm requires an input of the number N of the clusters, the selection of N is 
then an issue we need to consider. Remembering that the clusters reflect the use of a light verb rather 
than data origin, the selection of N should be based on the consideration of how many different uses a 
light verb may have. As there are 17 expanded binary features, the whole space of the values of the 
vectors is 217 = 128K. However, the number of different uses for a light verb should not be too large. 
There is no problem if N is set slightly larger than the real number of different uses of a light verb. For 
example, if there are 5 different uses for a light verb and we set N=6, then we can imagine that there 
may be two clusters that reflect the same use of the light verb. On the contrary, if N is set too small, all 
different uses will be mixed together. Then, the clustering result may not be able to show any 
interesting result we expected. In our experiments, we set N=10 for all the five light verbs. Especially, 
we use the WEKA (Hall et al., 2009) implementation of the simple K-Means for our experiments. The 
result is shown in Table 5. The key variations of each light verb are summarized as follows.  
 
?? congshi 
Cluster 5 shows that Mainland ?? congshi prefers to take complements denoting formal or 
official events in Mainland Mandarin. However, Taiwan ?? congshi does not show such preference 
as it can take both formal and informal events. Clusters 6 and 9 show that Taiwan ?? congshi can 
also take complements in VO form, e.g., ???? jinxing kaipiao proceed ballot counting ?to proceed 
with ballot counting?, but this is not preferred by Mainland ?? congshi. 
 
? gao 
Clusters 6 and 7 together show that the argument of Mainland ? gao can occur in the subject 
position in addition to the complement position, but such word order is rarely found in Taiwan data. 
Cluster 3 shows a possibility for Mainland ? gao to take arguments denoting events involving 
interactions of participants (e.g., ?? taolun ?to discuss?). In addition, Cluster 9 shows the possibility 
7
that Mainland? gao can take complements describing informal events, while the complements to 
Taiwan Mainland ? gao are more often formal events (especially political activities).  
 
?? jiayi 
Cluster 7 suggests Mainland ?? jiayi show a preference over complements denoting mental or 
psychological events. However, although Clusters 1 and 6 show some difference between Mainland 
and Taiwan ?? jiayi, our closer examination of the original data found that such differences actually 
do not reflect any variant-specific uses.  
 
?? jinxing 
Cluster 6 suggests that Mainland?? jinxing show a preference over the aspectual marker ? -le, 
but such preference is not seen in Taiwan ?? jinxing. Cluster 8 shows a preference by Taiwan??  
jinxing that it could take VO compound (e.g., ?? toupiao cast-ticket ?to vote?) as complements, 
while this rarely happens in Mainland. 
 
? zuo 
Clusters 1 and 3 show that in Mainland Mandarin, it is common for ? zuo to take the aspectual 
marker ? -le, but such use of ? zuo in Taiwan is not as common as in Mainland. 
 
To sum up, the results from the machine learning method are consistent with that from the 
multivariate statistical analysis in Section 3.1. Bringing together, we find that while the light verbs in 
Mainland and Taiwan Mandarin show similarities (as the speakers of these two regions can 
communicate without difficulty), there are indeed also variations in the two variants.   
4 Concluding Remarks 
Our study is the one of the first comparable corpus driven computational modeling studies on newly 
emergent language variants. The automatic identification of Mainland and Taiwan syntactic variations 
has very significant linguistic and computational implications. Linguistically, we showed that our 
comparable corpus driven statistical approach can identify comparative differences which are 
challenging for human analysis. The fact that newly emergent variants differ from each other 
comparatively rather than contrastively may also have important linguistics implications. In addition, 
by successfully differentiating these two variants based on their uses of light verbs, the result also 
suggests that variations among such newly emergent variants may arise from categories that are 
semantically highly bleached and tend to be/or have been grammaticalized. Computationally, the 
ability of machine learning approaches to differentiate Mainland and Taiwan variants of Mandarin 
Chinese potentially contributes to overcoming the challenge of automatic identification of subtle 
language/dialect variations among other light verbs, other lexical categories, as well as other 
languages/dialects.  
 
Acknowledgements 
The work is supported by a General Research Fund (GRF) sponsored by the Research Grants Council 
(Project no. 543512) and NTU Grant no. M4081117.100.500000. 
 
References 
Arppe, Antti. 2008. Univariate, bivariate and multivariate methods in corpus-based lexicography - a 
study of synonymy. Publications of the Department of General Linguistics, University of Helsinki, 
volume 44.  
Butt, Miriam and Wilhelm, Geuder. 2003. On the (semi) lexical status of light verbs. Semi-lexical 
Categories, Pages 323-370. 
8
Bresnan, Joan, Anna Cueni, Tatiana Nikitina, and R. Harald Baayen 2007. Predicting the dative 
alternation. In: Cognitive Foundations of Interpretation. Boume, G., I. Kraemer, and J. Zwarts. 
Amsterdam: Royal Netherlands Academy of Science, pp. 69-94.  
Cai, Wenlan. (1982). Issues on the complement of jinxing (????????). Chinese Language 
Learning (????) (3), 7-11. 
Diao, Yanbin. 2004. ?????????? (Research on Delexical Verb in Modern Chinese).  Dalian: 
Liaoning Normal University Press. 
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann and Ian H. Witten. 
2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10-18.  
Han, Weifeng, Antti Arppe, and John Newman. 2013. Topic marking in a Shanghainese corpus: from 
observation to prediction. Corpus Linguistics and Linguistic Theory (preprint).  
Hong, Jia-fei, and Chu-Ren Huang. 2013. ????????????????????? (Cross-strait 
lexical differences: A comparative study based on Chinese Gigaword Corpus). Computational 
Linguistics and Chinese Language Processing. 18(2):19-34. 
Hong, Jia-fei, and Chu-Ren Huang. 2008. ??????????????. (A corpus-based approach 
to the discovery of cross-strait lexical contrasts). Language and Linguistics. 9 (2):221-238. 
Huang, Chu-Ren. 2009. Tagged Chinese Gigaword Version 2.0. Philadelphia: Lexical Data 
Consortium, University of Pennsylvania. ISBN  1-58563-516-2 
Huang, Chu-Ren and Jingxia Lin. 2013. The ordering of Mandarin Chinese light verbs. In Proceedings 
of the 13th Chinese Lexical Semantics Workshop. D. Ji and G. Xiao (Eds.): CLSW 2012, LNAI 
7717, pages 728-735. Heidelberg: Springer. 
Huang, Chu-Ren, Jingxia Lin, and Huarui Zhang. 2013. World Chineses based on comparable corpus:  
The case of grammatical variations of jinxing. ??????????, pages  397-414. 
Hwang, Jena D., Archna Bhatia, Clare Bonial, Aous Mansouri, Ashwini Vaidya, Nianwen Xue, 
Martha Palmer. 2010. PropBank annotation of multilingual light verb constructions. Proceedings of 
the Fourth Linguistic Annotation Workshop, ACL 2010, 82?90. Jespersen, Otto. 1965. A Modern 
English Grammar on Historical Principles. Part VI, Morphology. London: George Allen and 
Unwin Ltd. 
Lin, Jingxia, Hongzhi Xu, Menghan Jiang and Chu-Ren Huang. 2014.  Annotation and classification 
of light verbs and light verb variations in Mandarin Chinese. COLING Workshop on Lexical and 
Grammatical Resources for Language Processing. Dublin, August 24.  
Nagy, Istv?n, Veronika Vincze, and Rich?rd Farkas. 2013. Full-coverage identification of English 
light verb constructions. In Proceedings of the International Joint Conference on Natural Language 
Processing, pages 329-337. 
Qiu, Zhipu, 1990. ????????? (Dictionary of Mainland and Taiwan Mandarin). Nanjing 
University press. 
Tu, Yuancheng and Dan Roth. 2011. Learning English light verb constructions: Contextual or 
statistical. In Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation 
to the Real World. Association for Computational Linguistics. 
Wang Tiekun and Li Xingjian, 1996. ?????????? (Research on lexical differences between 
Mainland and Taiwan Mandarin), World Chinese (??????), volume 81. 
Wei Li and Sheng Yuqi, 2000. ?????????????. (Comparative Dictionary of Lexical use 
in Mainland, Hong Kong, Macau and Taiwan), Beijing Industry University Press.  
Xu Danhui, 1995. ????????? (Lexical difference between Mainland and Taiwan Chinese). 
1st symposium on Cross-Strait Lexical and Character differences (???????????????
????).   
9
Zeng Rongfen, 1995. ???????????  (Opinion on cross-Strait language differences)1st 
symposium on Cross-Strait Lexical and Character differences (????????????????
???). 
Zhou, Gang. 1987. ???????? (Subdivision of dummy verbs). Chinese Language Learning (?
???), volume 1, pages 11-14. 
Zhu, Dexi. (1985). ???????????????? (Dummy verbs and NV in Modern Chinese). 
Journal of Peking University (Humanities and Social Sciences) (??????(???????)), 
volume 5, pages 1-6.  
 
 
 
 
10
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 75?82,
Coling 2014, Dublin, Ireland, August 24 2014.
Annotation and Classification of Light Verbs and Light Verb 
Variations in Mandarin Chinese
?
 
Jingxia Lin1      Hongzhi Xu2     Menghan Jiang2     Chu-Ren Huang2 
1Nanyang Technological University 
2Department of CBS, The Hong Kong Polytechnic University 
jingxialin@ntu.edu.sg, hongz.xu@gmail.com, 
menghan.jiang@connect.polyu.hk, churen.huang@polyu.edu.hk 
 
  
Abstract 
Light verbs pose an a challenge in linguistics because of its syntactic and semantic versatility and  its 
unique distribution different from regular verbs with higher semantic content and selectional resrictions. 
Due to its light grammatical content, earlier natural language processing studies typically put light verbs 
in a stop word list and ignore them. Recently, however, classification and identification of light verbs 
and light verb construction have become a focus of study in computational linguistics, especially in the 
context of multi-word expression, information retrieval, disambiguation, and parsing. Past linguistic and 
computational studies on light verbs had very different foci. Linguistic studies tend to focus on the sta-
tus of light verbs and its various selectional constraints. While NLP studies have focused on light verbs 
in the context of either a multi-word expression (MWE) or a construction to be identified, classified, or 
translated, trying to overcome the apparent poverty of semantic content of light verbs. There has been 
nearly no work attempting to bridge these two lines of research. This paper takes this challenge by pro-
posing a corpus-bases study which classifies and captures syntactic-semantic difference among all light 
verbs. In this study, we first incorporate results from past linguistic studies to create annotated light verb 
corpora with syntactic-semantics features. We next adopt a statistic method for automatic identification 
of light verbs based on this annotated corpora. Our results show that a language resource based method-
ology optimally incorporating linguistic information can resolve challenges posed by light verbs in NLP. 
 
1 Introduction 
Identification of Light Verb Construction (LVC) plays an important role and poses a special challenge 
in many Natural Language Processing (NLP) applications, e.g. information retrieval and machine 
translation. In addition to addressing issues related to LVC as a contributing factor to errors for vari-
ous applications, a few computational linguistics studies have targeted LVC in English specifically 
(e.g., Tu and Roth, 2011; Nagy et al., 2013). To the best of our knowledge, however, there has been no 
computational linguistic study dealing with LVCs in Chinese specifically. It is important to know that, 
due to their lack of semantic content, light verbs can behave rather idiosyncratically in each language. 
Chinese LVC, in particular, has the characteristic that allows many different light verbs to share simi-
lar usage and be interchangeable in some context. We should also note that light verbs in Chinese can 
take both verbs, deverabal nouns, and eventive nouns, while the morphological status of these catego-
ries are typically unmarked, Hence, it is often difficult to differentiate a light verb from its non-light 
verb uses without careful analysis of the data. 
      It has been observed that some Chinese light verbs can be used interchangeably but will have 
different selectional restrictions in some (and generally more limited) contexts. For example, the five 
light verbs congshi, gao, jiayi, jinxing, zuo (these words originally meant ?engage?, ?do?, ?inflict?, 
?proceed?, ?do? respectively) can all take yanjiu ?to do research? as their complement and form a LVC. 
However, only the light verbs gao and jinxing can take bisai ?to play games? as complements, where-
as the other light verbs congshi, jiayi, and zuo cannot. Since light verbs are often interchangeable yet 
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
75
each also has its own selectional restrictions, it makes the identification of light verbs themselves both 
a challenging and necessary task. It is also observed that this kind of selectional versatility actually led 
to variations among different variants of Mandarin Chinese, such as Mainland and Taiwan. The versa-
tility of Chinese light verbs makes the identification of LVCs more complicated than English. 
Therefore, to study the differences among different light verbs and different variants of Chinese is 
important but challenging in both linguistic studies and computational applications. With annotated 
data from comparable corpora of Mainland and Taiwan Mandarin Chinese, this paper proposes both 
statistical and machine learning approaches to differentiate five most frequently used light verbs in 
both variants based on their syntactic and semantic features. The experimental results of our approach 
show that we can reliably differentiate different light verbs from each other in each variety of Manda-
rin Chinese.  
There are several contributions in our work. Firstly, rather than focusing on only two light verbs 
jiayi and jinxing as in previous linguistic studies, we extended the study to more light verbs that are 
frequently used in Chinese. Actually, we will show that although jiayi and jinxing were often dis-
cussed in a pair in previous literature, the two are quite different from each other. Secondly, we show 
that statistical analysis and machine learning approaches are effective to identify the differences of 
light verbs and the variations demonstrated by the same light verb in different variants of Chinese. 
Thirdly, we provide a corpus that covers all typical uses of Chinese light verbs. Finally, the feature set 
we used in our study could be potentially used in the identification of Chinese LVCs in NLP applica-
tions. 
This paper is organized as follows. Section 2 describes the data and annotation of the data. In Sec-
tion 3, we conducted both statistical and machine learning methodologies to classify the five light 
verbs in both Mainland and Taiwan Mandarin. We discussed the implications and applications of our 
methodologies and the findings of our study in Section 4. Section 5 presents the conclusion and our 
future work. 
2 Corpus Annotation 
2.1 Data Collection 
The data for this study is extracted from Annotated Chinese Gigaword corpus (Huang, 2009) which 
was collected and available from LDC and contains over 1.1 billion Chinese words, with 700 million 
characters from Taiwan Central News Agency and 400 million characters from Mainland Xinhua 
News Agency.  
The light verbs to be studied are congshi, gao, jiayi, jinxing, zuo; these five are among the most fre-
quently used light verbs in Chinese (Diao, 2004). 400 sentences are randomly selected for each light 
verb, half from the Mainland Gigaword subcorpus and the other from the Taiwan Gigaword subcorpus, 
which resulted in 2,000 sentences in total. The selection follows the principle that it could cover the 
different uses of each light verb.  
2.2 Feature Annotation 
Previous studies (Zhu, 1985; Zhou, 1987; Cai, 1982; Huang et al., 1995; Huang et al., 2013, among 
others) have proposed several syntactic and semantic features to identify the similarities and differ-
ences among light verbs, especially between the two most typical ones, i.e. jinxing (originally ?pro-
ceed?) and jiayi (originally ?inflict?). For example, jinxing can take aspectual markers like zhe ?pro-
gressive marker?, le ?aspect marker?, and guo ?experiential aspect marker? while jiayi cannot (Zhou, 
1987);  congshi can take nominal phrases such as disan chanye?the tertiary industry? as its comple-
ment while jiayi cannot. A few features are also found to be variant-specific; for example, Huang and 
Lin (2013) find that only the congshi in Taiwan, but not in Mainland Mandarin, can take informal and 
negative event complements like xingjiaoyi ?sexual trade?. 
In our study, we selected 11 features which may help to differentiate different light verbs in each 
Mandarin variant as well as light verb variations among Mandarin variants, as in Table 1. All 2,000 
examples collected for analysis were manually annotated based on the 11 features. The annotator is a 
trained expert on Chinese linguistics. Any ambiguous cases were discussed with another two experts 
in order to reach an agreement. 
 
76
 
 
Feature ID Explanation Values (example) 
1. OTHERLV Whether a light verb co-occurs 
with another light verbs 
Yes (kaishi jinxing taolun Start proceed discuss 
?start to discuss?) 
No (jinxing taolun proceed discuss ?to discuss?) 
2. ASP 
 
Whether a light verb is affixed 
with an aspectual marker (e.g., 
perfective le, durative zhe, experi-
ential guo) 
ASP.le (jinxing-le zhandou ?fighted?) 
ASP.zhe (jinxing-zhe zhandou ?is fighting?) 
ASP.guo (jinxing-guo zhandou ?fighted?) 
ASP.none (jinxing zhandou ?fight?) 
3. EVECOMP Event complement of a light verb 
is in subject position 
Yes (bisai zai xuexiao jinxing game at school pro-
ceed ?The game was held at the school?) 
No (zai xuexiao jinxing bisai at school proceed 
game ?the game was held at the school?)  
4. POS 
 
The part-of-speech of the com-
plement taken by a light verb  
Noun (jinxing zhanzheng proceed fight ?to fight?) 
Verb (jinxing zhandou proceed fight ?to fight?) 
5. ARGSTR 
 
The argument structure of the 
complement of a light verb, i.e. 
the number of arguments (subject 
and/or objects) that can be taken 
by the complement  
One (jinxing zhandou proceed fight ?to fight?) 
Two (jinxing piping proceed criticize ?to criticize?)  
Zero (jinxing zhanzheng proceed fight ?to fight?) 
6. VOCOMP Whether the complement of a 
light verb is in the V(erb)-
O(bject) form  
Yes (jinxing tou-piao proceed cast-ticket ?to vote?) 
No (jinxing zhan-dou proceed fight-fight ?to fight?) 
7. DUREVT Whether the event denoted by the 
complement of a light verb is du-
rative 
Yes (jinxing zhandou proceed fight-fight ?to fight?) 
No (jiayi jujue inflict reject ?to reject?)  
8. FOREVT Whether the event denoted by the 
complement of a light verb is 
formal or official 
Yes (jinxing guoshi fangwen proceed state visit ?to 
pay a state visit?) 
No (zuo xiao maimai do small business ?run a 
small business?)  
9. PSYEVT Whether the event denoted by the 
complement of a light verb is 
mental or psychological activity 
Yes (jiayi fanxing inflict retrospect ?to retrospect?) 
No (jiayi diaocha inflict investigate ?to investi-
gate?)  
10. INTEREVT Whether the event denoted by the 
complement of a light verb in-
volves interaction among partici-
pants 
Yes (jinxing taolun proceed discuss ?to discuss?)  
No (jiayi piping inflict criticize ?to criticize?) 
11. ACCOMPEVT Whether the event denoted by the 
complement of a light verb is an 
accomplishment 
Yes (jinxing jiejue proceed solve ?to solve?) 
No (jinxing zhandou proceed fight-fight ?to fight?) 
Table 1: Features used to differentiate five Chinese light verbs. 
 
3 Identification of light verbs based on annotated corpora  
In this section, we adopted both statistical analysis and machine learning approaches to identify the 
five light verbs (jiayi, jinxing, congshi, gao and zuo) on the corpora with 2,000 annotated examples. 
The results of all approaches show that the five light verbs can be differentiated from each other in 
both Mainland and Taiwan Mandarin. 
3.1 Identifying light verbs by statistical analysis 
Both univariate analysis and multivariate analysis were used in our study for the identification. The 
tool we used is the Polytomous Package in R (Arppe, 2008).  
 
 
77
3.1.1 Univariate analysis  
Among the 11 independent features, one was found with only one level in both Mainland and Taiwan 
variants, i.e. all five light verbs in the two variants show the same preference over the features and 
thus excluded from the analysis. The feature is OTHERLV (all light verbs do not co-occur with another 
light verb in a sentence). Chi-squared tests were conducted for the significance of the co-occurrence of 
the remaining ten features with individual light verbs in both Mainland and Taiwan variants. The 
chisq.posthoc() function in the Polytoumous Package (Arppe, 2008) in R was used for the tests. The 
results are presented in Table 2, where the ?+? and ?-? signs indicate respectively a statistically signif-
icant overuse and underuse of a light verb with a feature, and ?0? refers to a lack of statistical signifi-
cance.  
 
  
Feature 
  
N 
Mainland Mandarin Taiwan Mandarin 
congshi  gao jiayi jinxing zuo congshi gao jiayi jinxing zuo 
POS.N 585 + + - 0 0 + + - - - 
POS.V 1415 - - + 0 0 - - + + + 
ARGSTR.one 376 0 - - 0 + + - - + 0 
ARGSTR.two 1039 - 0 + 0 - - - + - + 
ARGSTR.zero 585 + + - 0 0 + + - - - 
VOCOMP.no 1939 0 0 0 0 0 0 0 + - 0 
VOCOMP.yes 61 0 0 0 0 0 0 0 - + 0 
EVECOMP.no 1919 + - + - - + 0 + - 0 
EVECOMP.yes 81 - + - + + - 0 - + 0 
ASP.guo 9 0 0 0 0 0 0 0 0 0 0 
ASP.le 155 - - - + + - - - - + 
ASP.no 1835 + + + - - + + + + - 
ASP.zhe 1 0 0 0 + 0           
DUREVT.no 35 - 0 + - - 0 0 + 0 0 
DUREVT.yes 1965 + 0 - + + 0 0 - 0 0 
FOREVT.no 66 0 0 - 0 + + - - 0 0 
FOREVT.yes 1934 0 0 + 0 - - + + 0 0 
PSYEVT.no 1981 0 0 - 0 0 0 0 0 0 - 
PSYEVT.yes 19 0 0 + 0 0 0 0 0 0 + 
INTEREVT.no 1870 + 0 + - + + + 0 - 0 
INTEREVT.yes 130 - 0 - + - - - 0 + 0 
ACCOMPEVT.no 1904 + + - + + + + - + 0 
ACCOMPEVT.yes 96 - - + - - - - + - 0 
Table 2: Identifying light verbs in Mainland and Taiwan Mandarin via univariate analysis.  
 
Table 2 suggests that in both Mainland and Taiwan Mandarin, each light verb shows significant 
preference for certain features, and thus can be distinguished from each other. For example, in Main-
land Mandarin, although both congshi and gao show significant preference for the features POS.N and 
ACCOMPEVT.no, congshi differs from gao in that it also significantly prefers DUREVT.yes (taking 
complements denoting durative events, e.g., yanjiu ?to research?), EVECOMP.no (event complements 
do not occur in subject position), and INTEREVT.no (not taking complements denoting events involv-
ing interaction among participants, e.g., taolun ?to discuss?), whereas gao shows either a dis-
preference or no significant preference over these features. Take gao and zuo in Taiwan Mandarin as 
another example. While both light verbs literally means ?to do?, there is no single feature preferred by 
both: gao prefers POS.N, ARGSTR.zero, FOREVT.yes, INTEREVT.no, ACCOMPEVT.no, whereas zuo 
shows significant preferences for POS.V, ARGSTR.two, ASP.le, and PSYEVT.yes.  
 
3.1.2 Multivariate analysis  
As shown in Table 2, in both Mainland and Taiwan Mandarin, some of the five light verbs share some 
features, which thus explains why sometimes they can be interchangeably used. This also indicates (a) 
that a particular feature is unlikely to be preferred by only one light verb and thus differentiates the 
verb from the others; (b) a certain context may allow the occurrence of more than one light verb. In 
78
this sense, a multivariate analysis was adopted to better classify the five light verbs in each variant. 
The multivariate analysis used in the current study is polytomous logistic regression (Arppe, 2008), 
and the tool we used is the Polytomous() function in the Polytoumous Package (Arppe, 2008) in R.  
The results from the multivariate analysis were summarized in Table 3.  The numbers shown in the 
table are the odds for the features in favor of or against the occurrence of each light verb: when the 
estimated odd is larger than 1, the chance of the occurrence of a light verb is significantly increased by 
the feature, e.g., the chance of Mainland jiayi occurring is significantly increased by ARGSTRtwo 
(76.47:1), followed by ACCOMPEVTyes (56:1), VOCOMPyes (23.54: 1), and PSYEVTyes (19.87: 1). 
When the estimated odd is smaller than 1, the chance of the occurrence of a light verb is significantly 
decreased by the feature, e.g., the chance of Mainland jinxing occurring is significantly decreased by 
ACCOMPEVTyes (0.1849: 1); in addition, ?inf? and ?1/inf? refer to odds larger than 10,000 and 
smaller than 1/10,000 respectively, whereas non-significant odds (p-value < 0.05) are given in paren-
theses.  
 
 
Mainland Mandarin Taiwan Mandarin 
congshi gao jiayi jinxing zuo congshi gao jiayi jinxing zuo 
(Intercept) (1/Inf) 0.02271 (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) 
ACCOMPEVTyes (1/Inf) 0.09863 56.25 0.1849 (1/Inf) (0.3419) (1/Inf) 11.33 (0.1607) 0.2272 
ARGSTRtwo 0.2652 2.895 76.47 (1.481) 0.2177 0.1283 (0.7613) (Inf) (0.7062) (1.217) 
ARGSTRzero (1.097) 3.584 (1/Inf) (1.179) 0.245 (0.6219) 7.228 (4.396) 0.5393 0.2068 
ASPle (0.7487) (0.1767) (0.8257) (0.9196) (1.853) (1/Inf) (1/Inf) (0.3027) (Inf) 32.98 
ASPno (Inf) (1.499) (Inf) (0.2307) (0.2389) (0.9273) (0.6967) (Inf) (Inf) (0.2385) 
ASPzhe (1.603) (1/Inf) (0.4571) (Inf) (1/Inf) 
     
DUREVTyes (Inf) (2.958) (1/Inf) (Inf) (Inf) (Inf) (Inf) (1/Inf) (0.9575) (Inf) 
EVECOMPyes (1/Inf) (1.726) (1/Inf) 3.975 (1.772) (1/Inf) (0.8491) (1/Inf) 8.113 (0.5019) 
FOREVTyes (2.744) (1.227) (Inf) (0.7457) 0.2679 0.0867 (Inf) (Inf) (1.437) (1.467) 
INTEREVTyes 0.03255 (0.5281) (0.5432) 18.67 0.08902 0.1896 (1/Inf) (0.951) 10.47 (0.398) 
PSYEVTyes (1/Inf) (1/Inf) 19.87 (1/Inf) (0.9619) (1/Inf) (1/Inf) (1.395) (1/Inf) (3.323) 
VOCOMPyes (0.1346) (3.043) 23.54 (1.086) (0.5344) 0.18 (2.35) (Inf) 3.161 (0.5956) 
Table 3: identifying light verbs in Mainland and Taiwan Mandarin via multivariate analysis.  
 
   As shown in Table 3, each of the light verbs in each Mandarin variant shows its favor and disfavor 
of certain features. Take Mainland Mandarin for example: although congshi has no feature significant-
ly in its favor, but it is significantly disfavored by ARGSTRtwo (0.27:1) and ITEREVTyes (0.03:1); gao 
is disfavored by the aggregate of default variable values (0.02:1), and ACCOMPEVTyes (0.1:1), but is 
significantly favored by ARGSTRtwo and ARGSTRzero; the chance of jiayi?s ocucrrence is significant-
ly increased by ARGSTRtwo(76.47:1), ACCOMPEVTyes (56.25:1), VOCOMPyes (23.54:1), and 
PSYEVTyes (19:87:1); jinxing has INTEREVTyes and EVECOMPyes in its favor, but ACOMPEVTyes 
in its disfavor; no feature is significantly in the favor of zuo, but this light verb is significantly disfa-
vored by ARGSTRtwo, ARGSTRzero, FOREVTyes and INTEREVTyes.   
     The results in Table 3 also show that sometimes one key feature is able to identify two light verbs 
from each other, although not all five light verbs. Take Mainland Mandarin again for example. Most 
combinations of two light verbs from the five can be effectively differentiated by one feature. For in-
stance, the feature ARGSTRtwo can differentiate congshi/gao, congshi/jiayi, jiayi/zuo and gao/zuo; the 
feature INTEREVTyes can differentiate congshi/jinxing and jinxing/zuo; the feature ACCOMPEVTyes 
can differentiate the pairs gao/jiayi and jinxing/jiayi. 
3.2 Identifying light verbs by classification 
In this section, we resorted to machine learning technologies to study the same issue. Different classi-
fiers were adopted to discriminate the five light verbs with the annotated corpora: ID3, Logistic Re-
gression, Na?ve Bayesian and SVM that are implemented in WEKA (Hall et al., 2009) and 10-fold 
cross validations were performed separately on the Taiwan and Mainland corpora.  
79
The results were presented in Table 4. We can see that different classifiers provide similar results 
on both corpora, which means that the classification results are reliable and the features we annotated 
are effective in identifying the five light verbs. Overall, ID3 out-performs SVM slightly, with Logistic 
and NB not far behind. ID3 performs the best since the data is in low dimension. The detailed results 
including precision, recall and F-measure by ID3 on both corpora are shown in Table 5. The corre-
sponding confusion matrixes are presented in Table 6. The confusion matrixes suggest two very im-
portant generalizations: (a) all five verbs can be classified with good confidence, and (b) the overall 
classification patterns of the Mainland and Taiwan Mandarin are very similar, which is consistent with 
the fact that Mainland and Taiwan Mandarin are two variants. However, we also observe that the con-
fusion matrixes between various light verb pairs may differ between Mainland and Taiwan Chineses. 
This is the difference we would like to explore in the next section to propose a way to automatically 
predict these two variants. In addition, it is worth noting that all classifiers identify jiayi more effec-
tively than other light verbs, which thus shows a potential different usage of jiayi from the others.  
 
 ID3 Logistic NB SVM 
TW ML TW ML TW ML TW ML 
jingxing 0.365 0.494 0.372 0.455 0.411 0.444 0.422 0.485 
gao 0.612 0.391 0.609 0.364 0.598 0.377 0.575 0.354 
zuo 0.571 0.566 0.568 0.582 0.525 0.576 0.574 0.561 
jiayi 0.759 0.800 0.758 0.807 0.752 0.794 0.759 0.767 
congshi 0.552 0.646 0.526 0.643 0.486 0.648 0.523 0.633 
Average 0.574 0.585 0.567 0.576 0.555 0.573 0.571 0.565 
Table 4: Result in F1-score of 10-fold cross validation of the classification of the five light verbs with 
different classifiers on the Taiwan (TW) and Mainland (ML) Corpora. 
 
 Precision Recall F-Measure 
TW ML TW ML TW ML 
jingxing 0.442 0.593 0.311 0.423 0.365 0.494 
gao 0.681 0.449 0.557 0.347 0.612 0.391 
zuo 0.610 0.570 0.537 0.562 0.571 0.566 
jiayi 0.634 0.720 0.946 0.900 0.759 0.800 
congshi 0.528 0.583 0.579 0.724 0.552 0.646 
Average 0.580 0.586 0.588 0.599 0.574 0.585 
Table 5: 10-fold cross validation result of ID3 algorithm on both corpora. 
 
 jingxing gao zuo jiayi congshi 
 TW ML TW ML TW ML TW ML TW ML 
jingxing 61 83 15 27 36 40 38 11 46 35 
gao 20 16 113 70 13 23 24 39 33 54 
zuo 24 25 8 28 108 118 39 25 22 14 
jiayi 5 11 0 6 5 6 192 206 1 0 
congshi 28 5 30 25 15 20 10 5 114 144 
Table 6: Confusion matrix of the classification with ID3 algorithm on both corpora. 
 
3.3 Identifying light verbs by automatic clustering 
We further used the clustering algorithm to test the differentiability of the five light verbs in both 
Mainland and Taiwan Mandarin. The results using the simple K-Means clustering algorithm on Tai-
wan and Mainland corpora are shown in Table 7. The results show that the light verb jiayi behaves 
80
quite differently from the other four light verbs in both Mainland and Taiwan corpora, which is similar 
to the analysis based on statistical methods in Section 3.1 and classification methods in Section 3.2. In 
both corpora, jiayi has a narrower usage than the other light verbs. Meanwhile, we can also find a clus-
ter which is mainly formed by instances of jiayi from the Mainland corpus (i.e. cluster 0). After closer 
examination of the examples in this cluster, we found that it mainly includes sentences where jiayi 
takes complements denoting accomplishment events, e.g. gaizheng ?to correct? and jiejue ?to solve?. 
However, jiayi in Taiwan corpus mainly takes complements denoting activity events, and thus almost 
all instances of Taiwan jiayi are mixed with those of the other light verbs. Meanwhile, our results 
show a tendency that all other light verbs (jinxing, congshi, zuo, and gao) mostly take activity com-
plements but fewer accomplishment complements in both Taiwan and Mainland corpora. More dis-
cussion on the light verb variations between Mainland and Taiwan Mandarin can be found in (Huang 
et al., 2014).  
 
 
 
 Mainland  Taiwan 
0 1 2 3 4 0 1 2 3 4 
jinxing 2 32 110 23 37 30 10 77 20 64 
gao 2 33 116 41 11 120 23 30 0 31 
zuo 0 36 80 14 81 19 4 47 5 132 
jiayi 68 0 161 0 0 0 0 1 6 196 
congshi 0 67 66 21 46 90 20 68 0 22 
Table 7: Clustering results on Mainland and Taiwan corpora.  
 
4 Applications and Implications 
4.1 Implications for Future Studies 
In the study above, we were able to annotate a corpus with all the types of significant context and, 
based on this annotated corpus, we were able to use statistic model to differentiate the use of different 
light verbs in different contexts. Such a module of generic linguistic tools can have several potentially 
very useful applications. First, in translation, LVC is one of the most difficult constructions as there is 
less grammatical or contextual information to make the correct translation. Our approach is especially 
promising. As we encode contextual selection information for all light verbs, the same approach can 
be applied to the other languages in the target-source pair to produce optimal pair. Second, in infor-
mation extraction, selection of different light verbs often conveys subtle difference in meanings. Our 
ability to differentiate similar light verbs in the same context could have great potential in extracting 
the subtle information change/increase in the same context. Lastly, in second language learning as well 
as error detection, light verbs have been one of the most challenging ones. Our studies can be readily 
applied to either error detection or second language learning environment to provide the correct con-
text where a certain light very is preferred over another. 
4.2 From light verb variations to variants for the same language 
One of the biggest challenges in computational processing of languages is probably to identify newly 
emergent variants, such as the cross-strait variations of Mandarin Chinese. For these two variants, the 
most commonly cited ones were on lexical differences. Systematic grammatical differences were 
much more difficult to study and hence rarely reported (comp. Huang et al., 2009). As these are two 
newly divergent variants, their main grammars are almost all identical, except for some subtle differ-
ences, such as the selection between different light verbs and their complements. Our preliminary re-
sults of univariate and multivariate analysis can be found in Table 2 and 3. It shows not only the simi-
larities/differences among the light verbs in each variety (e.g., both ML and TW congshi and gao 
show preferences over POS.N, whereas both ML and TW jiayi show dispreference), but also the simi-
larities/differences of the corresponding light verbs in Mainland and Taiwan Mandarin. For instance, 
jinxing in TW tends to take VO compounds as its complements e.g., jinxing toupiao ?cast a vote?, 
81
which is consistent with the analysis in (Huang et al., 2013) (see more in Huang et al., 2014). But one 
thing should be pointed out is the difference is more between a significant and non-significant feature, 
rather than between a significant positive and significant negative feature.  
5 Conclusion 
In this paper, we addressed the issue of automatic classification of Chinese light verbs based on their 
usage distribution, based on an annotated corpus marking relevant contextual information for light 
verbs. We used both statistical methods and machine learning technologies to address this issue. It is 
found that our approaches are effective in identifying light verbs and their variations. The automatic 
generated semantic and syntactic features can also be used for future studies on other light verbs as 
well as other lexical categories. The result suggested that richly annotated language resources paired 
with appropriate tool can lead to effective general solution for some common issues faced by linguis-
tics and natural language processing. 
Acknowledgements 
The work is supported by a General Research Fund (GRF) sponsored by the Research Grants Council 
(Project no. 543512) and NTU Grant NO. M4081117.100.500000. 
Reference 
Antti Arppe. 2008. Univariate, bivariate and multivariate methods in corpus-based lexicography - a study of 
synonymy. Publications of the Department of General Linguistics, University of Helsinki, volume 44.  
Wenlan Cai. (1982). Issues on the complement of jinxing (????????). Chinese Language Learning (?
???) (3), 7-11. 
Yanbin Diao. 2004. Research on Delexical Verb in Modern Chinese (??????????).  Dalian: Liao-
ning Normal University Press. 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann and Ian H. Witten. 2009. The 
WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10-18.  
Chu-ren Huang, Meili Yeh, and Li-ping Chang. 1995. Two light verbs in Mandarin Chinese. A corpus-based 
study of nominalization and verbal semantics. Proceedings of NACCL6, 1: 100-112. 
Chu-Ren Huang. 2009. Tagged Chinese Gigaword Version 2.0. Philadelphia: Lexical Data Consortium, Univer-
sity of Pennsylvania. ISBN  1-58563-516-2 
Chu-Ren Huang and Jingxia Lin. 2013. The ordering of Mandarin Chinese light verbs. In Proceedings of the 
13th Chinese Lexical Semantics Workshop. D. Ji and G. Xiao (Eds.): CLSW 2012, LNAI 7717, pages 728-
735. Heidelberg: Springer. 
Chu-Ren Huang, Jingxia Lin, and Huarui Zhang. 2013. World Chineses based on comparable corpus:  The case 
of grammatical variations of jinxing. ??????????, pages  397-414. 
Chu-Ren Huang, Jingxia Lin, Menghan Jiang and Hongzhi Xu. 2014. Corpus-based Study and Identification of 
Mandarin Chinese Light Verb Variations. COLING Workshop on Applying NLP Tools to Similar Languages, 
Varieties and Dialects. Dublin, August 23.  
Istv?n Nagy, Veronika Vincze, and Rich?rd Farkas. 2013. Full-coverage Identification of English Light Verb 
Constructions. In Proceedings of the International Joint Conference on Natural Language Processing, pages 
329-337. 
Yuancheng Tu and Dan Roth. 2011. Learning English light verb constructions: Contextual or statistical. In Pro-
ceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World. Asso-
ciation for Computational Linguistics. 
Gang Zhou. 1987. Subdivision of Dummy Verbs (????????). Chinese Language Learning (????), 
volume 1, pages 11-14. 
Dexi Zhu. (1985). Dummy Verbs and NV in Modern Chinese (????????????????). Journal 
of Peking University (Humanities and Social Sciences) (??????(???????)), volume 5, pages 
1-6.  
82
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 147?156,
Coling 2014, Dublin, Ireland, August 24 2014.
Building a Semantic Transparency Dataset of Chinese Nominal
Compounds: A Practice of Crowdsourcing Methodology
Shichang Wang, Chu-Ren Huang, Yao Yao, Angel Chan
Department of Chinese and Bilingual Studies
The Hong Kong Polytechnic University
Hung Hom, Kowloon, Hong Kong
shi-chang.wang@connect.polyu.hk
{churen.huang, y.yao, angel.ws.chan}@polyu.edu.hk
Abstract
This paper describes the work which aimed to create a semantic transparency dataset of Chi-
nese nominal compounds (SemTransCNC 1.0) by crowdsourcing methodology. We firstly se-
lected about 1,200 Chinese nominal compounds from a lexicon of modern Chinese and the Sinica
Corpus. Then through a series of crowdsourcing experiments conducted on the Crowdflower
platform, we successfully collected both overall semantic transparency and constituent semantic
transparency data for each of them. According to our evaluation, the data quality is good. This
work filled a gap in Chinese language resources and also practiced and explored the crowdsourc-
ing methodology for linguistic experiment and language resource construction.
1 Introduction
The meaning of ???? (m?hu, horse-tiger, ?careless?) has nearly nothing to do with neither ??? (m?,
?horse?) nor ??? (h?, ?tiger?). However the meaning of ???? (d?ol?, road-way, ?road?) is basically
equal to ??? (d?o, ?road?) or ??? (l?, ?way?). And there are intermediate cases too, for instance, ??
?? (ji?ngh?, river-lake, ?all corners of the country?), its meaning is not equal to ??? (ji?ng, ?river?)
plus ??? (h?, ?lake?), but clear relatedness between them can be observed. This phenomenon is called
semantic transparency of compounds. We distinguish between overall semantic transparency (OST) and
constituent semantic transparency (CST). The semantic transparency of a compound, i.e., the overall se-
mantic transparency, is the extent to which the compound retains its literal meaning in its actual meaning.
The semantic transparency of a constituent of a compound, i.e., the constituent semantic transparency, is
the extent to which the constituent retains its meaning in the actual meaning of the compound. Semantic
similarity between the literal meaning and the actual meaning of a compound can be used to estimate the
overall semantic transparency of a compound, for the more the literal meaning is retained in the actual
meaning, the more similar they are. The same technique can be used to estimate constituent semantic
transparency. Semantic transparency can be quantified; if we assign 0 to ?fully opaque? and assign 1 to
?fully transparent?, then semantic transparency can be quantified as a closed interval [0, 1].
The quantitative analysis of semantic transparency must be supported by semantic transparency
datasets. In previous semantic transparency related studies on Chinese compounds, some researchers
created some datasets to support their own studies. But this kind of datasets are usually relatively small
and restrictive, so cannot be used widely, for example, (??? and??, 2001; Myers et al., 2004;?
??, 2008; Mok, 2009), etc. Some datasets, although large enough and can be used in other studies, are
not publicly accessible, for example, (??? and???, 1999;?? and???, 2005), etc. A large
and publicly accessible semantic transparency dataset of Chinese compounds is still a gap in Chinese
language resources.
Crowdsourcing, as an emergingmethod of data collection and resource construction (Snow et al., 2008;
Callison-Burch and Dredze, 2010; Munro et al., 2010; Schnoebelen and Kuperman, 2010; Gurevych and
Zesch, 2013; Wang et al., 2013) and an emerging method of behavioral experiment (Paolacci et al., 2010;
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
147
Berinsky et al., 2011; Mason and Suri, 2012; Rand, 2012; Crump et al., 2013), is attracting more andmore
attention from the field of language study and language computing. As a method of data collection and
resource construction, it has the advantages of high speed and low cost, etc. It can use redundancy to filter
out noise in order to improve data quality; if used properly, it can produce expert-level data. As a method
of experiment, besides the above advantages, it also has the following ones, (1) it is easier to obtain large
samples, because the amount of potential participants is huge; (2) the diversity of participants is good,
because the participants are from different places and have different backgrounds; (3) crowdsourcing
environments are usually anonymous, so it is easier to collect certain sensitive data.
2 Method
2.1 Compound Selection
We use the following criteria to select compounds, (1) they are disyllabic nominal compounds; (2) each
of them has the structure NN, AN, or VN; (3) they are composed of free morphemes; (4) they have
mid-range word frequencies; and (5) they are used in both Mainland China and Taiwan. And we select
compounds according to the following procedure:
(1) Extract monosyllabic nouns, adjectives and verbs mainly according to ?The Dictionary of Con-
temporary Chinese (the 6th edition)? (??????,? 6?), and thus we get three sets, a) the set of
monosyllabic nouns, N; b) the set of monosyllabic adjectives, A; and c) the set of monosyllabic verbs, V.
(2) Extract the words of the structure NN, AN, or VN 1 from the ?Lexicon of Common Words in
Contemporary Chinese? (????????). In this step, NN means both morphemes of the word
appear in the set N; AN means the first morpheme appears in the set A and the second appears in the set
N; VN means the first morpheme appears in the set V and the second appears in the set N. After this step,
we get ?word list 1?.
(3) Extract the words which have mid-range frequencies 2 from the Sinica Corpus 4.0 (Chen et al.,
1996). These words are represented in traditional Chinese characters. We convert them into simplified
Chinese characters and only reserve the words which also appear in ?word list 1?. After this step, we get
?word list 2?.
(4) Manually verify ?word list 2? to generate the final list. Things need to be verified include the
following aspects. (a) Because in ?word list 2? word structures are judged automatically, there are many
errors, so we have to verify the correctness of the word structure judgments. (b) We have to make sure
that the morphemes of each word are free morphemes. (c) We also need to delete some proper nouns.
The words we selected appear in both Sinica Corpus 4.0 and ?Lexicon of Common Words in Contem-
porary Chinese?. Since there is no completely reliable criterion to identify Chinese word, appearing in
two lexicons ensures their word identity. This also ensures that they are used in both Mainland China and
Taiwan, and further means they are quite possible to be shared in other Chinese language communities,
for example Hong Kong, Macau, and Singapore, etc.
According to above criteria and procedure, we selected a total of 1,176 words. 664 (56.46%) of them
have the structure NN; 322 (27.38%) have the structure AN; and 190 (16.16%) have the structure VN.
2.2 Experimental Design
Normally, a crowdsourcing experiment should be reasonably small in size. We randomly divide these
1,176 words into 21 groups, G
i
(i = 1, 2, 3, ..., 21); each group has 56 words.
1See??? and??? (1998), and Huang (1998) for relevant statistics.
2We use cumulative frequency feature to determine mid-range frequency. Sort the word frequency list of Sinica Corpus
4.0 descendingly; then calculate cumulative frequency word by word until each word corresponds with a cumulative frequency
value; finally, plot a curve on a coordinate plane whose x-axis represents the ranks of words in the sorted list, and the y-axis
represents cumulative frequency values. Very apparently, this curve can be divided into three successive phases; the words
within each phase have similar word frequency features. According to this, we identify three word frequency categories, 5,163
high-frequency words (frequency range: [182, 581823], cumulative frequency range: [0%, 80%]), 19,803 mid-range frequency
words (frequency range: [23, 181], cumulative frequency range: (80%, 93%]), and 177,496 low-frequency words (frequency
range: [1, 22], cumulative frequency range: (93%, 100%]). Sinica Corpus 4.0 contains about 11.2 million word tokens.
148
Questionnaires
We collect overall semantic transparency (OST) and constituent semantic transparency (CST) data of
these words. In order to avoid interaction, we designed two kinds of questionnaires to collect OST data
and CST data respectively. SoG
i
(i = 1, 2, 3, ..., 21) has two questionnaires, one OST questionnaire for
OST data collection and one CST questionnaire for CST data collection. Besides titles and instructions,
each questionnaire has 3 sections. Section 1 is used to collect identity information includes gender, age,
education and location. Section 2 contains four very simple questions about the Chinese language; the
first two questions are open-ended Chinese character identification questions, the third question is a close-
ended homophonic character identification question, and the fourth one is a close-ended antonymous
character identification question; different questionnaires use different questions. Section 3 contains the
questions for semantic transparency data collection. Suppose AB is a disyllabic nominal compound, we
use the following question to collect its OST rating scores: ?How is the sum of the meanings of A and
B similar to the meaning of AB?? And use the following two questions to collect its CST rating scores
of its two constituents: ?How is the meaning of A when it is used alone similar to its meaning in AB??
and ?How is the meaning of B when it is used alone similar to its meaning in AB??. 7-point scales are
used in section 3; 1 means ?not similar at all? and 7 means ?almost the same?.
In order to evaluate the data received in the experiments, we embedded some evaluation devices in the
questionnaires. We mainly evaluated intra-group and inter-group consistency; and if the data have good
intra-group and inter-group consistency, we can believe that the data quality is good. In each group we
choose two words and make them appear twice, we call them intra-group repeated words and we can use
them to evaluate the intra-group consistency. We insert into each group two same extra words, w
1
??
??, w
2
????, to evaluate the inter-group consistency.
Quality Control Measures
On a crowdsourcing platform like Crowdflower, the participants are anonymous, they may try to cheat
and submit invalid data, and they may come from different countries and speak different languages rather
than the required one. There may be spammers who continuously submit invalid data at very high speed
and they may even bypass the quality control measures to cheat for money. In order to ensure that the
participants are native Chinese speakers and to improve data quality, we use the following measures, (1)
a participant must correctly answer the first two Chinese character identification questions in the section
2s of the questionnaires, and he/she must correctly answer at least one of the last two questions in these
section 2s; (2) If a participant do not satisfy the above conditions, he/she will not see Section 3s; (3) each
word stimulus in section 3s has an option which allows the participants to skip it in case he/she does not
recognize that word; (4) all the questions in the questionnaires must be answered except the ones which
allow to be skipped and are explicitly claimed to be skipped; (5) we wrote a monitor program to detect
and resist spammers automatically; (6) after the experiment is finished, we will analyze the data and filter
out invalid data, and we will discuss this in detail in section 3.
2.3 Experimental Platform and Procedure
We choose Crowdflower as our experimental platform, because according to our previous experiments,
it is a feasible crowdsourcing platform to collect Chinese language data. We create one task for each
questionnaire on the platform; there are 21 groups of word and each group has one OST questionnaire
and one CST questionnaire, so there are a total of 42 tasksT ost
i
, T
cst
i
(i = 1, 2, 3, ..., 21). We publish these
42 tasks successively, and for each task we create a monitor program to detect and resist spammers. All
of these tasks use the following parameters: (1) each task will collect 90 responses; (2) we pay 0.15USD
for each response of OST questionnaire and pay 0.25USD for each response of CST questionnaire; (3)
each worker account of Crowdflower can only submit one response for each questionnaire and each IP
address can only submit one response for each questionnaire; (4) we only allow the workers from the
following regions (according to IP addresses) to submit data: Mainland China, Hong Kong, Macau,
Taiwan, Singapore, Malaysia, USA, UK, Canada, Australia, Germany, France, Italy, New Zealand, and
Indonesia; and we can dynamically disable or enable certain regions on demand in order to ensure both
data quality and quantity.
149
3 Data Refinement and Result Calculation
TheOST dataset produced by theOST taskT ost
i
(i = 1, 2, 3, ..., 21) isDost
i
. The CST dataset produced by
the CST task T cst
i
is Dcst
i
. Each dataset contains 90 responses. Because of the nature of crowdsourcing
environment, there are many invalid responses in each dataset; so firstly we need to filter them out in
order to refine the data. A response is invalid if (1) its completion time is less than 135 seconds (for
OST responses); its completion time is less than 250 seconds (for CST responses) 3; or (2) it failed to
correctly answer the first two questions of section 2s of the questionnaires; or (3) it wrongly answered
the last two questions of section 2s of the questionnaires; or (4) it skipped one or more words in section
3s of the questionnaires; or (5) it used less than two numbers on the 7-point scales in section 3s of the
questionnaires. The statistics of valid response are shown in Table 1.
The OST dataset Dost
i
(i = 1, 2, 3, ..., 21) contains n
i
valid responses; it means word w in the OST
dataset of the ith group has n
i
OST rating scores; the arithmetic mean of these n
i
OST rating scores is
the OST result of word w. The CST results of the two constituents of word w are calculated using the
same algorithm.
OST CST
G
i
n % n %
G
1
54 60 59 65.56
G
2
60 66.67 59 65.56
G
3
55 61.11 60 66.67
G
4
59 65.56 59 65.56
G
5
50 55.56 55 61.11
G
6
55 61.11 52 57.78
G
7
53 58.89 53 58.89
G
8
60 66.67 50 55.56
G
9
48 53.33 52 57.78
G
10
57 63.33 62 68.89
G
11
46 51.11 56 62.22
G
12
48 53.33 58 64.44
G
13
51 56.67 52 57.78
G
14
50 55.56 50 55.56
G
15
52 57.78 52 57.78
G
16
57 63.33 56 62.22
G
17
50 55.56 46 50.55
G
18
51 56.67 53 58.89
G
19
50 55.56 49 54.44
G
20
50 55.56 47 52.22
G
21
50 55.56 50 55.56
Max 60 66.67 62 68.89
Min 46 51.11 46 50.55
Median 51.5 57.22 53 58.89
Mean 52.67 58.52 53.81 59.76
SD 4.09 4.55 4.49 5.04
Table 1: The Amount of Valid Response in the OST and CST Datasets of Each Group
4 Evaluation
Three kinds of evaluation measures are used, (1) the intra-group consistency of the OST and CST results,
(2) the inter-group consistency of the OST and CST results, and (3) the correlation between the OST and
CST results.
3Each OST questionnaire has about 70 questions, and each CST questionnaire has about 130; in an OST or CST question-
naire, almost all the questions are the same except the stimuli words and can be instantly answered by intuition; note that a
participant can take part in as many as 42 tasks; according to our test, if a participant is familiar with the tasks, he/she can
answer each question in less than 2 seconds (less than 1 second to identify the stimulus word and another less than 1 second
to rate it) without difficulty. 70 ? 2 = 140 seconds, the expected time should be less than this, so we use 135 seconds as
the temporal threshold for valid OST responses. The calculation of the temporal threshold for valid CST responses is similar,
130? 2 = 260 seconds, the expected time should be less than this, so we use 250 seconds.
150
4.1 Intra-group Consistency
In each group G
i
(i = 1, 2, 3, ..., 21), we selected two words w
i,1
, w
i,2
(intra-group repeated words) and
made them appear twice between which there is enough distance; we can calculate the difference values
between the results of the two appearances of these words.
Intra-group Consistency of OST Results
There are 21 groups and in each group there are two intra-group repeated words, so there are a total of 42
such words. Each intra-group repeated word appears twice, so we can obtain two OST results r
1
, r
2
. The
difference value between the two results, d = |r
1
? r
2
|, of each intra-group repeated word is calculated,
so there are 42 difference values. Among them, the maximum value is 0.29; the minimum value is 0;
the median is 0.1; their mean is 0.11; and their standard deviation is 0.08; all of these values are low and
indicate that these OST datasets have good intra-group consistency (see Table 2).
Intra-group Consistency of CST Results
Each intra-group repeated word has two constituents, c
1
, c
2
, so each constituent gets two CST results, i.e.,
r
c1,1
, r
c1,2
and r
c2,1
, r
c2,2
. We calculate the difference values for the two constituents, d
1
= |r
c1,1
?r
c1,2
|
and d
2
= |r
c2,1
? r
c2,2
|, and get 42 difference values of the first constituents and 42 difference values
of the second constituents. Among the difference values of the first constituents, the maximum value
is 0.27; the minimum value is 0; the median is 0.09; their mean is 0.1, and their standard deviation is
0.07; all of these values are low, this indicates that the CST results of the first constituents in the CST
datasets of the 21 groups have good intra-group consistency. Among the difference values of the second
constituents, the maximum value is 0.36; the minimum value is 0; the median is 0.07; their mean is 0.09,
and their standard deviation is 0.09; all of these values are low; this indicates that the CST results of the
second constituents in the CST datasets of the 21 groups have good intra-group consistency (see Table
3). So these 21 CST datasets have good intra-group consistency.
4.2 Inter-group Consistency
We inserted two inter-group repeated words, w
1
????, w
2
????, into all of these 21 groups G
i
(i =
1, 2, 3, ..., 21); we can evaluate the inter-group consistency by comparing their semantic transparency
rating results in different groups. Since w
1
, w
2
appear in all OST and CST questionnaires of 21 groups,
we can obtain (1) 21 OST results of w
1
, (2) 21 OST results of w
2
, (3) 21 CST results of each of the two
constituents w
1,c1
, w
1,c2
of w
1
, and (4) 21 CST results of each of the two constituents w
2,c1
, w
2,c2
of w
2
.
Standard deviation can be used to measure difference, for example, the standard deviation of the 21 OST
results of w
1
is 0.2; this value is small and indicates high consistency; because these 21 results are from
the OST datasets of 21 groups respectively, so we can say that these 21 OST datasets have good inter-
group consistency. The standard deviation of the 21 OST results of w
2
is 0.14; the standard deviation of
21 CST results of the first constituent of w
1
is 0.2, and that of the second is 0.18; the standard deviation
of 21 CST results of the first constituent of w
2
is 0.15, and that of the second is 0.2; all of these values
are small and all of them indicate good inter-group consistency (see Table 4).
4.3 Correlation between OST and CST Results
Each compound in the datasets has two constituents; both constituents affect the OST of the compound,
but neither of them can solely determine the OST of the compound. So the mean of the two CST values
of a compound is a fairly good estimation of its OST value. Therefore, if the datasets are reliable, in each
group, we should observe strong correlation between the OST results and their corresponding means of
the CST results. For each group, we calculate three Pearson product-moment correlation coefficients (r);
r
1
is the r between the OST results and their corresponding CST results of the first constituents; r
2
is
the r between the OST results and their corresponding CST results of the second constituents; and r
3
is
the r between the OST results and their corresponding means of the CST results. The r
3
values of the 21
groups are all greater than 0.9 which indicates very strong correlation; among them, the maximum value
is 0.96; the minimum value is 0.91; and their mean is 0.94 (SD = 0.02); the r
1
and r
2
values are also
151
Gi
w
i,1/2
r
1
r
2
d
G
1
?? 5.26 5.26 0
?? 3.57 3.61 0.04
G
2
?? 5.63 5.75 0.12
?? 2.68 2.9 0.22
G
3
?? 5.67 5.58 0.09
?? 3.51 3.62 0.11
G
4
?? 5.31 5.32 0.02
?? 3.19 3.02 0.17
G
5
?? 5.36 5.32 0.04
?? 3.12 3.3 0.18
G
6
?? 5.53 5.4 0.13
?? 5.25 4.96 0.29
G
7
?? 5.25 5.23 0.02
?? 4.19 4.11 0.08
G
8
?? 5.48 5.33 0.15
?? 3.2 3.37 0.17
G
9
?? 5.19 5.19 0
?? 3.69 3.75 0.06
G
10
?? 5.49 5.63 0.14
?? 3.46 3.54 0.09
G
11
?? 5.48 5.39 0.09
?? 3.26 3.24 0.02
G
12
?? 5.19 5.4 0.21
?? 3.6 3.54 0.06
G
13
?? 5.47 5.39 0.08
?? 3.37 3.41 0.04
G
14
?? 5.54 5.52 0.02
?? 3.46 3.56 0.1
G
15
?? 5.54 5.37 0.17
?? 3.29 3.56 0.27
G
16
?? 5.49 5.53 0.04
?? 3.82 4.07 0.25
G
17
?? 5.2 5.38 0.18
?? 3.76 3.76 0
G
18
?? 5.31 5.18 0.14
?? 3.41 3.25 0.16
G
19
?? 5.22 5.28 0.06
?? 4.04 3.88 0.16
G
20
?? 5.28 5.18 0.1
?? 4.04 3.84 0.2
G
21
?? 5.06 5.02 0.04
?? 3.8 4 0.2
Max 0.29
Min 0
Median 0.1
Mean 0.11
SD 0.08
Table 2: The Intra-group Consistency of the OST Results of Each Group
reasonably high (see Table 5)4. The results support the reliability of these datasets.
5 Merging and Normalization
The evaluation results show that the collected data are generally reliable and have relatively high intra-
group and inter-group consistency which further indicate that these datasets share similar scale and are
basically comparable, so we can merge the 21 OST datasets into one big OST dataset D
ost
and merge
the 21 CST datasets into one big CST dataset D
cst
. When we merge these datasets, we delete all the
extra words which are used to evaluate the inter-group consistency; for the repeated words which are
4After merging and normalization (see Section 5), we calculated these three correlation coefficients betweenD
ost
andD
cst
,
the results are r
1
= 0.68, r
2
= 0.68, r
3
= 0.87.
152
c1
c
2
G
i
w
i,1/2
r
c1,1
r
c1,2
d
1
r
c2,1
r
c2,2
d
2
G
1
?? 3.83 4.05 0.22 5.49 5.42 0.07
?? 2.88 3.03 0.15 3.92 3.92 0
G
2
?? 5.12 5.22 0.1 5.24 5.1 0.14
?? 4.27 4.27 0 2.19 2.51 0.32
G
3
?? 5.12 5.08 0.03 5.35 5.4 0.05
?? 2.92 2.95 0.03 3.22 3.42 0.2
G
4
?? 4.51 4.34 0.17 5.56 5.27 0.29
?? 2.39 2.49 0.1 4.22 4.12 0.1
G
5
?? 4.75 4.64 0.11 5.09 5.15 0.05
?? 2.29 2.4 0.11 4.67 4.76 0.09
G
6
?? 5.4 5.23 0.17 5.35 5.4 0.06
?? 5.08 5.02 0.06 5.38 5.46 0.08
G
7
?? 4.7 4.83 0.13 5.13 5.13 0
?? 3.85 3.94 0.09 4.45 4.57 0.11
G
8
?? 5.06 4.88 0.18 5.28 5.3 0.02
?? 3.24 3.14 0.1 3.36 3.16 0.2
G
9
?? 5 4.98 0.02 5 4.98 0.02
?? 3.63 3.71 0.08 3.71 3.83 0.12
G
10
?? 4.53 4.6 0.06 5.37 5.39 0.02
?? 3.13 3.21 0.08 3.15 3.16 0.02
G
11
?? 4.45 4.55 0.11 5.36 5.55 0.2
?? 3.8 3.79 0.02 2.64 3 0.36
G
12
?? 4.69 4.52 0.17 4.97 4.9 0.07
?? 3.03 3.21 0.17 3.28 3.4 0.12
G
13
?? 4.15 4.19 0.04 5.15 5.27 0.12
?? 2.52 2.79 0.27 3.44 3.42 0.02
G
14
?? 4.42 4.36 0.06 5.14 5.12 0.02
?? 3.56 3.5 0.06 3.08 3.06 0.02
G
15
?? 5.08 5.02 0.06 5.06 5.13 0.08
?? 3.21 3 0.21 3.46 3.5 0.04
G
16
?? 4.34 4.34 0 5.11 5.09 0.02
?? 3.8 3.63 0.18 3.32 3.38 0.05
G
17
?? 4.76 4.72 0.04 4.74 4.87 0.13
?? 3.93 3.96 0.02 3.89 3.87 0.02
G
18
?? 4.26 4.32 0.06 4.77 4.7 0.08
?? 3.4 3.36 0.04 2.74 2.68 0.06
G
19
?? 4.63 4.61 0.02 4.57 4.49 0.08
?? 3.55 3.29 0.27 3.53 3.41 0.12
G
20
?? 4.98 4.91 0.06 5.15 5.17 0.02
?? 2.94 2.96 0.02 4.7 4.45 0.26
G
21
?? 4.68 4.56 0.12 5 4.98 0.02
?? 3.68 3.88 0.2 3.66 3.6 0.06
Max 0.27 0.36
Min 0 0
Median 0.09 0.07
Mean 0.1 0.09
SD 0.07 0.09
Table 3: The Intra-group Consistency of the CST Results of Each Group
used to evaluate the intra-group consistency, the final result of each of them is the mean of its two results.
According to our definition, the range of semantic transparency value is [0, 1], but the experimental results
are obtained using 7-point scales, so we need to normalize these results in order to map them to the range
[0, 1]. The normalized OST and CST results will be merged into D
ost
and D
cst
respectively. Assume
that, in the dataset D
ost
, the OST result of the ith (i = 1, 2, 3, ..., 1176) word is Sw
i
, and the normalized
result is S?w
i
, then,
S
?w
i
=
S
w
i
? 1
6
153
OST CST
G
i
w
1
w
2
w
1,c1
w
1,c2
w
2,c1
w
2,c2
G
1
2.94 5.52 2.85 2.97 4.56 5.56
G
2
3.6 5.55 3.15 3.2 4.92 5.75
G
3
3.51 5.64 3.17 3.23 4.75 5.58
G
4
3.81 5.68 3.53 3.59 4.58 5.42
G
5
3.74 5.46 3.38 3.56 4.64 5.55
G
6
3.65 5.55 3.63 3.56 4.85 5.65
G
7
3.58 5.51 3.47 3.58 4.75 5.23
G
8
3.22 5.53 3.4 3.36 4.8 5.48
G
9
3.31 5.15 3.48 3.52 4.69 5.42
G
10
3.58 5.53 3.42 3.34 4.69 5.27
G
11
3.7 5.67 3.46 3.32 4.52 5.36
G
12
3.33 5.71 3.19 3.28 4.41 5.14
G
13
3.47 5.78 3.58 3.56 4.73 5.38
G
14
3.48 5.58 2.94 2.94 4.42 5.3
G
15
3.4 5.42 3.42 3.27 4.62 5.1
G
16
3.47 5.56 3.34 3.25 4.59 5.16
G
17
3.6 5.56 3.3 3.26 4.5 5.17
G
18
3.67 5.67 3.36 3.34 4.47 5
G
19
3.28 5.56 3.2 3.29 4.37 5.18
G
20
3.56 5.48 3.21 3.36 4.72 5.34
G
21
3.62 5.32 3.2 3.28 4.5 5.24
Max 3.81 5.78 3.63 3.59 4.92 5.75
Min 2.94 5.15 2.85 2.94 4.37 5
Median 3.56 5.55 3.36 3.32 4.62 5.34
Mean 3.5 5.54 3.32 3.34 4.62 5.35
SD 0.2 0.14 0.2 0.18 0.15 0.2
Table 4: The Inter-group Consistency of the OST and CST Results
And assume that, in the datasetD
cst
, the CST result of the jth (j = 1, 2) constituent of the ith word is
S
c
i,j
, and the normalized result is S?c
i,j
, then,
S
?c
i,j
=
S
c
i,j
? 1
6
6 Distribution
Influenced by outliers and perhaps other factors, the OST and CST results cannot cover the whole range
of the scale [0, 1]; both ends shrink towards the central point 0.5, and the shrinkage of each end is about
0.2; nevertheless, the results can still assign proper ranks of semantic transparency to the compounds and
their constituents which are generally consistent with our intuitions. Among the normalized OST results,
the maximum is 0.81; the minimum is 0.28; the median is 0.63; and their mean is 0.62 (SD = 0.09).
Among the normalized CST results of the first constituents (C1.CST results), the maximum is 0.77; the
minimum is 0.19; the median is 0.57; and their mean is 0.56 (SD = 0.09). And among the normalized
CST results of the second constituents (C2.CST results), the maximum is 0.79; the minimum is 0.22; the
median is 0.6; and their mean is 0.58 (SD = 0.1). The distributions of OST, C1.CST, and C2.CST results
are similar; all of them are negatively skewed (see Figure 1), and their estimated skewnesses are ?0.66,
?0.77, and ?0.63 respectively. These distributions exhibit that more compounds and their constituents
in our datasets have relatively high semantic transparency values.
7 Conclusion
This work created a dataset of semantic transparency of Chinese nominal compounds (SemTransCNC
1.0), which filled a gap in Chinese language resources. It contains the overall and constituent semantic
transparency data of about 1,200 Chinese disyllabic nominal compounds and can support semantic trans-
parency related studies of Chinese compounds, for example, theoretical, statistical, psycholinguistic, and
154
Gi
r
1
r
2
r
3
G
1
0.68 0.68 0.91
G
2
0.72 0.72 0.93
G
3
0.76 0.78 0.96
G
4
0.76 0.77 0.96
G
5
0.75 0.56 0.95
G
6
0.63 0.72 0.91
G
7
0.83 0.78 0.94
G
8
0.76 0.77 0.96
G
9
0.68 0.81 0.95
G
10
0.84 0.83 0.95
G
11
0.78 0.71 0.91
G
12
0.72 0.77 0.95
G
13
0.85 0.86 0.96
G
14
0.69 0.85 0.95
G
15
0.68 0.82 0.95
G
16
0.82 0.85 0.95
G
17
0.79 0.83 0.94
G
18
0.81 0.86 0.96
G
19
0.76 0.8 0.95
G
20
0.76 0.75 0.94
G
21
0.73 0.86 0.96
Max 0.85 0.86 0.96
Min 0.63 0.56 0.91
Median 0.76 0.78 0.95
Mean 0.75 0.78 0.94
SD 0.06 0.07 0.02
Table 5: The Correlation Coefficients between the OST and CST Results
Normalized OST Results
Freq
uen
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
100
200
300
Normalized C1.CST Results
Freq
uen
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
100
200
300
Normalized C2.CST Results
Freq
uen
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
100
200
300
Figure 1: The Distributions of the Normalized OST and CST Results
computational studies, etc. And this work was also a successful practice of crowdsourcing method for lin-
guistic experiment and language resource construction. Large scale language data collection experiments
which require large amount of participants are usually very difficult to conduct in laboratories using the
traditional paradigm. Crowdsourcing method enabled us to finish the data collection task within rela-
tively short period of time and relatively low budget (1,000USD); during the process of the experiment,
we needed not to organize and communicate with the participants, it saved a lot of time and energy. The
participants are from all over the world, so it is better than traditional laboratory method in the aspect
of participant diversity. The data collected have very good intra-group and inter-group consistency, the
OST and CST data highly correlate with each other as expected, and the results are consistent with our
intuitions: all of these indicate good data quality. The methods of questionnaire design, quality control,
data refinement, evaluation, emerging, and normalization can be used in crowdsourcing practices of the
same kind.
155
Acknowledgements
The work described in this paper was supported by grants from the Research Grants Council of the Hong
Kong Special Administrative Region, China (Project No. 544011 & 543512).
References
Adam J Berinsky, Gregory A Huber, and Gabriel S Lenz. 2011. Using mechanical turk as a subject recruitment
tool for experimental research. Submitted for review.
Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 1?12. Association for Computational Linguistics.
Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and Hui-Li Hsu. 1996. Sinica Corpus: Design Methodology
for Balanced Corpora. In B.-S. Park and J.B. Kim, editors, Proceeding of the 11th Pacific Asia Conference on
Language, Information and Computation, pages 167?176. Seoul:Kyung Hee University.
Matthew JC Crump, John V McDonnell, and Todd M Gureckis. 2013. Evaluating amazon?s mechanical turk as a
tool for experimental behavioral research. PloS one, 8(3):e57410.
Iryna Gurevych and Torsten Zesch. 2013. Collective intelligence and language resources: introduction to the
special issue on collaboratively constructed language resources. Language Resources and Evaluation, 47(1):1?
7.
Shuanfan Huang. 1998. Chinese as a headless language in compounding morphology. New approaches to Chinese
word formation: Morphology, phonology and the lexicon in modern and ancient Chinese, pages 261?284.
Winter Mason and Siddharth Suri. 2012. Conducting behavioral research on amazon?s mechanical turk. Behavior
research methods, 44(1):1?23.
Leh Woon Mok. 2009. Word-superiority effect as a function of semantic transparency of chinese bimorphemic
compound words. Language and Cognitive Processes, 24(7-8):1039?1081.
Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler
Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic data.
In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 122?130. Association for Computational Linguistics.
James Myers, Bruce Derwing, and Gary Libben. 2004. The effect of priming direction on reading chinese com-
pounds. Mental Lexicon Working Papers, 1:69?86.
Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis. 2010. Running experiments on amazon mechanical
turk. Judgment and Decision making, 5(5):411?419.
David G Rand. 2012. The promise of mechanical turk: How online labor markets can help theorists run behavioral
experiments. Journal of theoretical biology, 299:172?179.
Tyler Schnoebelen and Victor Kuperman. 2010. Using amazon mechanical turk for linguistic research. Psi-
hologija, 43(4):441?464.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?263. Association for Computational Linguistics.
AoboWang, CongDuyVuHoang, andMin-YenKan. 2013. Perspectives on crowdsourcing annotations for natural
language processing. Language Resources and Evaluation, 47:9?31.
???. 2008. ????????????????????. ??????, 1:82?90.
??? and??. 2001. ??????????????????. ??????, 1:53?59.
??? and???. 1999. ?????????,??????????. ????, 31(3):266?273.
??? and???. 1998. ?????????????????. ??????, 2(1):13.
?? and???. 2005. ????????????????????. ????, 28(6):1358?1360.
156
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 157?166,
Coling 2014, Dublin, Ireland, August 24 2014.
Annotate and Identify Modalities, Speech Acts and Finer-Grained Event
Types in Chinese Text
Hongzhi Xu
Department of CBS
The Hong Kong Polytechnic University
hongz.xu@gmail.com
Chu-Ren Huang
Faculty of Humanities
The Hong Kong Polytechnic University
churenhuang@gmail.com
Abstract
Discriminating sentences that denote modalities and speech acts from the ones that describe or
report events is a fundamental task for accurate event processing. However, little attention has
been paid on this issue. No Chinese corpus is available by now with all different types of sen-
tences annotated with their main functionalities in terms of modality, speech act or event. This
paper describes a Chinese corpus with all the information annotated. Based on the five event
types that are usually adopted in previous studies of event classification, namely state, activi-
ty, achievement, accomplishment and semelfactive, we further provide finer-grained categories,
considering that each of the finer-grained event types has different semantic entailments. To d-
ifferentiate them is useful for deep semantic processing and will thus benefit NLP applications
such as question answering and machine translation, etc. We also provide experiments to show
that the different types of sentences are differentiable with a promising performance.
1 Introduction
Event classification is a fundamental task for NLP applications, such as question answering and ma-
chine translation, which need deep understanding of the text. Previous work (Siegel, 1999; Siegel and
McKeown, 2000; Palmer et al., 2007; Zarcone and Lenci, 2008; Cao et al., 2006; Zhu et al., 2000)
aims to classify events into four categories, namely state, activity, accomplishment and achievement, i.e.
Vendler?s framework adopted from linguistic studies (Vendler, 1967; Smith, 1991). High performance
was reported on the classification, however based on the assumption that all sentences describe an even-
t, which is not case in real text. Modalities and speech acts are not considered and no finer-grained
classification is proposed.
The aim for aspectual classification for a specific language is to build verb classes. In such framework,
viewpoint aspect in terms of perfective vs. imperfective is not considered. For example, he is eating a
sandwich and he ate a sandwich are all instances of accomplishment. However, we argue that this
framework is not enough for more accurate event processing. It is obvious that the two sentences have
different meanings and different consequences. The situation described by the first sentence is still going
on at the speech time, while the second sentence implies that the event has finished. So, in the perspective
of event processing, it is necessary and important to discriminate the two different aspects.
Another important issue is that not all sentences describe events. For example, Austin (1975) discrim-
inated two different types of sentences: constative and performative. Sentences that report or describe
events are in the first category. Sentences of the performative category mainly refer to speech (illocu-
tionary) acts, actions that are done by speech. For example, by uttering the sentence I declare that the
new policy will take effect from now on, the authorized speaker brings a new policy into effect. In this
case, uttering the sentence itself is an event. Discriminating speech acts are especially useful in speech
corpora, e.g. (Avila and Mello, 2013).
Modality is important due to its interaction with factuality and truth of the embedded propositions. For
example, he can eat two sandwiches describes a dynamic modality about the subject?s ability of eating.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
157
However, no eating event has actually happened. Modality has been considered in modeling speaker?s
opinions (Benamara et al., 2012), machine translation (Baker et al., 2012), etc.
Sauri et al. (2006; 2012) proposed a framework for modeling modalities. However, their definition of
modality is a little different from that used by linguists. The main motivation of their work is to predict
the factuality of a proposition. As a result, all factors that may affect the factuality of propositions
are regarded as modalities. In our framework, we will adopt the definition in linguistic studies that
modality expresses a speaker?s belief or attitude on an embedded proposition (Palmer, 2001). Factuality
is determined by many factors other than modalities. However, we don?t want to mix all the factors
together in linguistic perspective.
In this paper, we will describe a Chinese corpus in which different sentence types are discriminated.
Finer-grained event types are also incorporated with a theory proposed in (Xu and Huang, 2013). The
details of the framework will be discussed in the next section.
The remaining of the paper is organized as follows. Section 2 introduces the theoretical framework we
shall adopt for our annotation. Section 3 describes a Chinese corpus we annotated with some statistical
information. Section 4 describes a classification experiment based on the annotated corpus. Section 5 is
the conclusion and our future work.
2 The Annotation Framework
In this section, we will give an introduction to the theoretical framework from a linguistic perspective.
There are two main levels for the classification. Sentences are first discriminated according to their main
functions, e.g. constative and performative (Austin, 1975). Constative sentences are further divided into
modality which mainly expresses the addresser?s propositional attitude and event which is a description
or report of a real situation without the speaker?s attitude. One basic assumption is that one sentence only
has one main function in terms of expressing speaker?s modality, speech act or describing an event. So,
there is no overlap among the three types of sentences.
2.1 Modality
Sentences denoting modalities are different from the sentences reporting events in that the former only
refers to a proposition upon which the speaker expresses his attitude its truth value, while the later is a
fact without incorporating speakers? opinions but only speaker?s perception. It is possible that speakers
can make mistakes in their perceptions. However it is beyond the linguistic level and there is no way to
predict the correctness based on the surface of the sentence. Thus, it is another issue out of the discussion
of this paper. We adopt the modal theory by Palmer (2001). According to him, modality could be divided
into epistemic, deontic and dynamic.
Epistemic modality expressed the speaker?s opinion on the truth of the embedded proposition in terms
of necessity and possibility. Informally, epistemic modality expresses what may be in the world. For
example, ta1 ken3ding4 zai4 ban4gong1shi4 ?he must be in his office? describes an epistemic modality
of the speaker that he is sure about the truth of the embedded proposition.
Deontic modality expresses what should be in the world, according to speaker?s expectations, certain
rules, laws and so on. For example, ni3 bi4xu1 zun1shou3 gui1ze2 ?You must obey the rules?.
Dynamic modality describes the abilities of a subject, such as ta1 hui4 you2you3 ?he can swim?, wo3
de0 ban4gong1shi4 ke3yi3 kan4jian4 da4hai3 ?you can see the ocean from my office?.
Evaluation is also treated as a modality in our framework. Evaluation describes the speaker?s opinion
on a proposition. It is different from epistemic in that it suggests rather than makes judgment on the truth
of a proposition. For example, ta1 suan4shi4 shi4jie4shang4 zui4hao3 de0 ge1shou3 le0 ?he should be
the best singer in the world?. Evaluative sentences only refer to those that contain explicit markers, e.g.
suan4shi4 ?should be?. The sentence ta1 shi4 shi4jie4shang4 zui4hao3 de0 ge1shou3 ?he is the best
singer in the world? is not treated as evaluation. In this sense, evaluative is not equivalent to subjective.
158
Exclamation is treated as a subset of evaluation. Take nian2qing1 ren2 a0 ! ?Young people!? for
example, it mostly expresses an implicit evaluation, e.g. only young people could do crazy things of
some kind, based on which the exclamation is expressed by the speaker.
2.2 Speech act
For speech act (illocutionary act), we adopt the theory by Searle (1976), where five different categories
are proposed, namely assertive, expressive, directive, commissive and declaration. In addition, we also
put interrogative sentences under this category. Speech act sentences only refer to those sentences that
are explicit utterances, e.g. the sentences quoted in text.
Assertive is to commit the speaker (in varying degrees) to something?s being the case or the truth of
the expressed proposition. For example, wo3 zheng4ming2 ta1 shi4 xue2sheng1 ?I certify that he is a
student?.
Expressive expresses the psychological state specified in the sincerity condition about a state of af-
fairs specified in the propositional content. Verbs for expressive speech act includes xie4xie4 ?thank?,
bao4qian4 ?apologize?, huan1ying2 ?welcome?, dui4bu4qi3 ?sorry? etc. For example, xie4xie4
bang1mang2 ?Thanks for your help?.
Directive is usually a command or requirement of the speaker to get the hearer to do something. For
example, ni3 guo4lai2 yi1xia4 ?Come here please?.
Commissive is to commit the speaker (in varying degrees) to some future course of action. For exam-
ple, wo3 hui4 bang1 ni3 ?I shall help you?.
Declaration is to bring about the correspondence between the propositional content and reality. Suc-
cessful performance guarantees that the propositional content corresponds to the world. For example,
wo3 xuan1bu4 ben3 ci4 hui4yi4 zheng4shi4 kai1mu4 ?The conference now start?.
Interrogative is an illocutionary act of the speaker that requires the hearer to provided some infor-
mation. For example, ni3 jiao4 shen2me0 ming2zi4 ? ?What?s your name?? and ni3 qu4 ting1 na4 ge4
jiang3zuo4 ma0 ? ?Will you attend the speech?? Interrogative sentences are usually with a question mark
???. However, not all sentences with question mark are interrogative. For example, rhetorical questions
usually don?t need the answer from the hearer. Instead, it actually expresses the speaker?s evaluation on
a situation. For example, the sentence wo3 zen3me0 ke3yi3 bu4 jin4xin1 zhao4gu4 ? ?How could I not
take care of him carefully?? should be labelled as evaluative modality rather than interrogative speech
act.
2.3 Events
Here, we describe a new framework by incorporating finer-grained event categories as described in (X-
u and Huang, 2013). Each of the finer-grained categories corresponds to only one of the five coarse
categories. So, it is an extension of and is compatible with the Vendler?s framework.
2.3.1 Primitive Events
According to Xu and Huang (2013), there are three event primitives, namely static state (S), dynamic state
(D), and change of state. Static state is equivalent to the previous notion state, which is a homogeneous
process, where all subparts are of the same kind of event. Dynamic state refers to an ongoing dynamic
process, e.g. running, eating etc., that is perceived like a state. Change of state is then defined as a change
from one state, either static or dynamic, to another state.
Change of state actually refers to the previous notion achievement. Theoretically, there are four type-
s of changes: static-static change (SS), static-dynamic change (SD), dynamic-static change (DS) and
dynamic-dynamic change (DD). In detail, SD change is somewhat equivalent to inceptive achievement,
and DS change is somewhat equivalent to terminative or completive achievement.
159
Event Type Representation Example
Static State ?- ta1 hen3 gao1 he is tall
Dynamic State ???? ta1 zai4 pao3bu4 he is running
SS Change ?|? ta1 bing4 le0 he got ill
SD Change ?|??? ta1 kai1shi3 pao3bu4 le0 he started running
DS Change ???|? ta1 ting2zhi3 pao3bu4 le0 he stopped running
DD Change ???|??? dian4nao3 qi3dong4 hao3 le0 the computer finished startup
Table 1: Primitives of Events.
Table 1 shows the extended event primitives with some illustrative examples. We use ??? and ????? to
denote static state and dynamic state respectively. ?|? is used to denote a temporal boundary. In case of
change of state, the temporal boundary overlap with the logical boundary, i.e. the change.
Negations usually denote static state. In Chinese, there are two negation adverbs, bu4 ?not? and
mei2you3 ?not?. However, they are different in that the former negates a generic event meaning that
such event doesn?t happen, while the latter negates the existence of an event instance. For example, ta1
bu4 he1jiu3 ?he doesn?t drink? describes an attribute of the subject, which is intrinsically a static state.
ta1 mei2you3 he1jiu3 ?he didn?t drink? describes a fact that there is no event instance of his drinking,
which is also a static state. Negation of a modality is still a modality. For example, ta1 bu4 ke3neng2
zai4 ban4gong1shi4 ?he cannot be in his office? still describes an epistemic modality.
2.3.2 Complex Events
Based on the primitives, we can compose complex events. Delimitative describes a temporal bounded
static state that has a potential starting point and ending point, within which the static state holds, e.g. ta1
bing4 le0 yi1 ge4 xing1qi1 ?he was ill for one week?. Process describes a temporal bounded dynamic
state that has a potential starting point and ending point, within which the dynamic state holds, e.g.
ta1 pao3 le0 yi1 ge4 xiao3shi2 ?he ran for one hour?. Semelfactive is different from Process in that its
durations is quite short and is usually perceived as instantaneous. In other words, the temporal boundaries
of semelfactive is usually naturally determined. For example, ta1 qiao1 le0 yi1 xia4 men2 ?he knocked
the door once?. There is no way to length the duration of the knocking action. However, a series of
iterative semelfactives could form dynamic process. For example, ta1 qiao1 le0 yi1 ge4 xiao3shi2 de0
men2 ?he knocked the door for an hour? gives a reading of iterative knocks.
For static state and dynamic state, we can only refer to their holding at a certain time point. In other
words, delimitative and process describe the life cycle of a state. For example, ta1 bing4 zhe0 ne0 ?he is
ill? and ta1 wan3shang4 jiu3dian3 de0 shi2hou0 zai4 pao3bu4 ?He was running at 9:00pm?. It is also
possible to claim that in a certain period, which for some reason became the focus of a conversation, a
state holds. For example, ta1 na4 liang3 tian1 dou1 bing4 zhe0 ?he was ill in that two days? and ta1
wan3shang4 jiu3dian3 dao4 shi2dian3 de0 shi2hou0 zai4 pao3bu4 ?From 9:00pm to 10:00pm, he was
running?. In this case, they are also state rather than delimitative or process. The difference is that there
is no information about the starts and the ends, while delimitative and process do.
Accomplishment is composed by a process with a final state. For example, ta1 xie3 le0 yi1 feng1 xin4
?he wrote a letter? describes an accomplishment composed by a writing process with a final state, i.e.
the existence of the letter. The final state of an accomplishment could also be dynamic. For example, ta1
ba3 dian4nao3 qi3dong4 le0 ?he started up the computer? describe an accomplishment with a dynamic
final state, i.e. the normal working of computer.
Some Resultative Verb Compounds (RVCs) in Chinese can denote achievements. However, they are
easy to be confused with accomplishment. Based on the representation, the difference of them is that
accomplishment encodes the start of the dynamic process, while achievement doesn?t. For example, ta1
xie3 wan2 le0 na4 feng1 xin4 ?He (write-)finished the letter? describes a DS change. To differentiate
them, we can use the yi3qian2 ?before? test. As in this example, ta1 xie3 wan2 na4 feng1 xin4 yi3 qian2
?before he finished the letter? refers to the period that includes the writing process. This means that
160
RVCs only focus on the final culminating point and are thus achievements. On the other hand, ta1 xie3
na4 feng1 xin4 zhi1 qian2 ?before he wrote the letter? refers to the period before the writing process. So,
ta1 xie3 le0 yi1 feng1 xin4 ?he wrote a letter? is then an accomplishment.
There is a counterpart for accomplishment, which is composed by an instantaneous dynamic process
(semelfactive) with a final state. RVCs can also denote instantaneous accomplishment. For example, ta1
da3sui4 le0 yi1 ge4 bei1zi0 ?he hit and broke a cup? is an accomplishment composed by a semelfactive
hitting action with a final state, i.e. the broken of the cup. Similarly, the final state could also be
dynamic. For example, in ta1 tan2zhuan4 le0 yi1 ge4 shai3zi0 ?He flicked and putted a spin on the
dice?, the predicate tan2zhuan4 ?flick-spin? is a compound that combines the predicate tan2 ?flick? and
zhuan4 ?spin?. The whole event is composed by a semelfactive flicking and a final dynamic state of the
dice?s spin.
Table 2 shows the seven event types with examples. Theoretically, there could be unlimited number
of complex events. However, the notions listed here are important in that they are the lexicalized units
which reflect the human?s cognition of real world events. For the perspective of computational linguis-
tics, discriminating all these linguistic events will be a fundamental step for deeper natural language
understanding.
2.3.3 The Neutral Aspect
Some sentences don?t include an explicit viewpoint aspect, e.g. without any aspectual markers. For
example, ta1 kan4 xiao3shuo1 ?he read novel? can possibly denote different event types in different
contexts. yi3qian2, ta1 kan4 xiao3shuo1 ?he read novel before? denotes an attribute of the subject
that he reads novels, while da4jia1 dou1 hen3mang2, xiao3hai2er0 xie3 zuo4ye4, ta1 kan4 xiao3shuo1
?Everyone is busy, children are doing homework, he is reading novels? describes a dynamic state. The
aspects of these examples are given by the specified contexts. Such sentences are usually called with
NEUTRAL aspect (Smith, 1991). In our framework, such sentences are ignored for now, unless the
context can help the annotator to figure out the aspectual information.
Semelfactive |?| ta1 qiao1 le0 qiao1 men2 ?he knocked the door?
Delimitative |?-| ta1 bing4 le0 yi1 ge4 xing1qing1 ?he was ill for one week?
Process |????| ta1 pao3 le0 yi1 ge4 xiao3shi2 ?he ran for an hour?
Instantaneous |?|? ta1 da3sui4 le0 bei1zi0 ?he broke the cup?
Accomplishment |?|??? ta1 tan2zhuan4 le0 yi1 ge4 shai3zi0 ?He putted a spin on the dice?
Accomplishment |????|? ta1 xie3 le0 yi1 feng1 xin4 ?he wrote a letter?
|????|??? ta1 ba3 dian4nao3 qi3dong4 le0 ?he started up the computer?
Table 2: Complex event types that are composed by more than one primitives.
The overall hierarchy is shown in Figure 1. Some traditional notions are kept in use e.g. accomplish-
ment and achievement. However, they now refer to event types rather than verb classes.
3 Annotating a Chinese Corpus
3.1 Data Selection
For annotation, we choose Sinica Treebank 3.0 (Huang et al., 2000), which contains more than 60,000
trees. Sinica Treebank is a subset of Sinica Corpus (Chen et al., 1996), which is a balanced corpus that
contains different genres of materials, including news, novels and some transcripts of spoken Chinese.
Sinica Treebank is annotated based on the Information-based Case Grammar (Chen and Huang, 1990).
The annotated syntactic and semantic information is kept for further studies, e.g. feature evaluation and
selection.
For annotation, we only select the sentences that are labeled as S and end with punctuation of period
???, exclamation ???, semicolon ??? and question mark ???. After removing duplicate sentences, we
get 5612 sentences Table 3 shows the detailed information of the raw corpus. There are 45728 tokens
from 11681 types in the corpus. For the heads of the sentences, there are 2127 different verbs.
161
Figure 1: Sentence type hierarchy.
Sentences Different Verbs Different Words Tokens Characters
5612 2127 11681 45728 75960
Table 3: Distribution information of the corpus for annotation.
3.2 Annotation Result
Each sentence is labeled as one specific finer-grained category from the 23 categories described in Sec-
tion 2. Whenever an example could not be decided by the annotator, it is discussed with another two
linguistic experts to make the final decision. However, we also did agreement test, which will be dis-
cussed later.
Finally, we annotated 1044 instances in modality, 764 speech act instances and 3811 event instances.
The distribution information is shown in Table 4. We can see that some event types, although theoretically
exist, don?t encounter any examples, such as the instantaneous accomplishment with dynamic final
state: |?|???.
Static state contains more than 40% instances. We think that it reflects the real distribution of event
types as we don?t make any bias for selecting data. Static state can be further divided into several
subcategories, e.g. attributive, relational, habitual, etc., which will be our future work.
Type No. Type No. Type No. Type No. Type No.
Epistemic 303 Assertive 64 ? 2475 ?|? 471 |?|? 257
Deontic 219 Expressive 13 ??? 166 |?|??? 0
Dynamic 111 Directive 65 |?| 6 ?|??? 96 |????|? 163
Evaluation 411 Commissive 58 |???| 48 ???|? 79 |????|??? 40
Interrogative 559 Declarative 2 |?| 4 ???|??? 2
Table 4: Distribution of different event types in the annotated corpus.
Table 5 shows the number of the main verbs regarding howmany event types they can denote excluding
modality and speech act. We can see that more than 200 verbs correspond to more than one category.
This shows that the verbs alone sometimes could not determine the event type.
162
No. of Event Types 1 2 3 4 5 6 7
No. of Verbs 1395 155 44 9 7 1 1
Table 5: Number of verbs with regard to how many event types they can denote.
Accuracy F1-Measure Kappa
Annotator 1 0.862 0.762 0.837
Annotator 2 0.821 0.677 0.784
Annotator 1+2 0.842 0.716 0.811
Table 6: Annotation agreements between the main annotator and annotator1, annotator 2, annotator 1+2.
Annotator 1+2 means the combination result of the two annotators, i.e. all the 2000 examples.
3.3 Agreement Evaluation
In order to test the reliability of the annotation, we randomly select 2000 examples from the corpus and
let another two linguists annotate them. Each of the linguists annotate half of them. The annotation
results are then compared with the main annotator. The agreements between the main annotator and the
other two annotators in terms of accuracy, F1 measure and Kappa value are shown in Table 6. The F1
measures are calculated based on the assumption that the main annotator?s result is the gold standard. The
result shows a very high agreement which means that our new framework for event type classification is
reliable and easy for annotation.
4 Automatic Classification of Chinese Sentences and Event Types
In this section, we conduct two classification experiments. The first is to discriminate the three sentence
types regarding their main functions, speech act, modality and event. The second is the classification
with the finer-grained categories. Before the experiments, we will first discuss the features that may help
for the classification.
4.1 Features
As suggested in previous literatures (Siegel, 1999; Siegel and McKeown, 2000; Zhu et al., 2000; Cao et
al., 2006), the following features are considered as important for event type classification.
Main verbs and their complements including argument structure are the most important indicators to
an event type. Negation of the main verb is a strong indicator for static state, as discussed above.
Aspectual markers, ? zhe0 ?ZHE?, ? le0 ?LE?, ? guo4 ?GUO? and some aspectual light verbs,
e.g. ? zai4 ?be doing?, ?? kai1shi3 ?start?, ?? ji4xu4 ?continue?, ?? ting2zhi3 ?stop?, ??
wan2cheng2 ?finish?, are strong indicators for different event types.
Temporal adverbials are also important features, which could potentially disambiguate neutral sen-
tences, e.g., yi3qian2, ta1 kan4 xiao3shuo1 ?he read novel before? as discussed above.
Frequency adverbs, such as?? jing1chang2 ?often?,?? ou3er3 ?sometimes?, etc., are indicators
for habitual states. For example, ta1 jing1chang2 qu4 he1jiu3 ?he often goes for drinking? is a habitual
state rather than a specific event.
Modalities could be expressed by auxiliaries, adverbs, sentence final particles etc. in Chinese. Adverbs
that modify the main verb, such as ?? ke3neng2 ?possibly?, are important features for identifying
modalities. Sentence final particles (SFP) and punctuation marks are also good indicators to evaluative
modality.
Since we don?t maintain a dictionary for the above indicators, we use a general feature set including
the dependency structure and the combinations of the dependent constituents. We suggest that the above
linguistic rules could be reflected by the dependency structures, which could be captured by the classi-
fiers. Meanwhile, the experiment result here is only to serve as a baseline for future comparisons. In all,
the features are listed in Table 7 with some examples.
163
ID Feature Example
f
1
Head head:word:kan4, head:pos:verb,
head:subj:word:ta1, head:subj:pos:pron,
head:obj:xp:NP, head:obj:xp:noun-noun
f
2
Dependency dep:word:ta1, dep:pos:pron,
dep:word:bu4, dep:pos:adv,
dep:word:xiao3shuo1, dep:pos:noun,
dep:word:le0, dep:pos:particle,
f
3
COMB subj:word:ta1-head:word:kan4-obj:xp:noun-noun,
subj:pos:pron-head:pos:verb-obj:xp:NP,
Table 7: Feature template we use for our classification of event types. Feature examples are based on the
sentence ta1 (he) bu4 (not) kan4 (read) zhen1tan4 (detective) xiao3shuo1 (novel) le0 (LE) ?he doesn?t
read detective novels any more?.
f
1
+f
2
+f
3
Prec Rec F1 Prec Rec F1 Prec Rec F1
Event 0.709 0.939 0.807 0.853 0.969 0.908 0.833 0.974 0.898
Modality 0.395 0.124 0.189 0.731 0.473 0.574 0.744 0.431 0.545
SpeechAct 0.430 0.130 0.199 0.829 0.664 0.737 0.845 0.609 0.707
MacroAvg 0.511 0.398 0.399 0.804 0.702 0.740 0.807 0.671 0.717
Accuracy 0.679 0.836 0.824
Table 8: Coarse level classification result.
4.2 Experimental Result
To give a real performance, the annotated syntactic and semantic information are not used. Instead, we
use the Stanford word segmenter (Tseng et al., 2005) and Stanford parser (Chang et al., 2009) to get the
syntactic structure of the sentences. All the experiment are results of 5-fold cross validation with a SVM
classifier implemented in LibSVM (Chang and Lin, 2011).
The result of the coarse level classification for modality, event and speech act is shown in Table 8.
We can see that the overall performance is reasonable. The F-Measure for modality is not as good as
the others. This is due to the fact that the modal markers and operators are quite critical for identifying
modalities, which may be sparse in our corpus. We suggest that maintaining a comprehensive dictionary
of modal operators could benefit the identification of the modalities. We can also see that the feature set
f
3
harms the performance, which is also caused by the feature sparseness problem.
For finer-grained classification, we use two different ways. The first way is to use a hierarchical
classification scheme. An instance is first classified as event, modality or speech act. According to the
result of the first round classification, the instance is put into the corresponding finer-grained model for
further classification. The second way is to classify all instances all at once based on a model trained on
all finer-grained categories.
Considering that some categories contain only few examples, which will provide unreliable evaluation
of the performance, we combined accomplishments with static final state and dynamic state, so does for
instantaneous accomplishment. We use ?=? to denote a general state, which could be either static or dy-
namic. Static state and delimitative are combined together, while dynamic state, process and semelfactive
are combined. Expressive, declarative and DD change are ignored in the experiments. The classification
results with feature sets f
1
and f
2
are shown in Table 9. The hierarchical classification is slightly better
than the all-at-once classification. Meanwhile, the accuracy for hierarchical classification is 0.621, which
is much better than the predominant guess 0.443.
We should note that parsing accuracy will significantly affect the result of event type classification.
This is true in the sense that the semantic content of words and their syntactic relations are all critical
164
All-At-Once Hierarchical
Precision Recall F1 Precision Recall F1
? 0.609 0.952 0.743 0.627 0.938 0.751
??? 0.840 0.078 0.142 0.830 0.069 0.127
?|? 0.454 0.384 0.415 0.473 0.418 0.443
?|??? 0.583 0.083 0.142 0.537 0.104 0.173
???|? 0 0 0 0 0 0
|????|=== 0.438 0.084 0.140 0.394 0.108 0.168
|?|=== 0.496 0.159 0.239 0.516 0.210 0.295
Epistemic 0.710 0.419 0.524 0.638 0.442 0.520
Deontic 0.629 0.360 0.455 0.573 0.383 0.457
Dynamic 0.388 0.233 0.290 0.391 0.287 0.330
Evaluation 0.592 0.319 0.412 0.523 0.302 0.382
Interrogative 0.844 0.789 0.815 0.818 0.789 0.803
Directive 0.692 0.309 0.418 0.695 0.354 0.458
Assertive 0 0 0 0.1 0.031 0.047
Commissive 0.83 0.277 0.409 0.713 0.155 0.246
MacroAvg 0.540 0.296 0.343 0.522 0.306 0.347
Accuracy 0.620 0.621
Table 9: 5-fold cross validation result of finer-grained classification with f
1
and f
2
features.
for the classification. Besides the parsing problem, there are other linguistic issues behind. Many modal
operators could result in different modalities, such as?? ying1gai1 ?should?,? hui4 ?will/can/may?,
? yao4 ?want/will/should/must? etc. Sometimes, it is hard to decide which meaning is correct in a
context. There may be also other linguistic issues that we have not discovered yet. This corpus thus
could be used for both linguistic study and computational applications, e.g. event processing.
5 Conclusion
In this paper, we present a Chinese corpus annotated with modalities, speech acts and finer-grained even-
t types. We also provide experiments on classification in different levels of categories with a general
feature set. The experimental result is acceptable concerning the difficult linguistic issues behind. In fu-
ture, we would like to continue our research work on improving the corpus and exploring more semantic
information including lexical semantic structures and lexical relations such as WordNet to improve the
performance of the classification.
Acknowledgements
The work is supported by a General Research Fund (GRF) sponsored by the Research Grants Council
(Project no. 543810 and 543512).
References
John Langshaw Austin. 1975. How to do things with words: Second Edition. Harvard University Press, Cam-
bridge, MA.
Luciana Beatriz Avila and Heliana Mello. 2013. Challenges in modality annotation in a brazilian portuguese
spontaneous speech corpus. Proceedings of WAMM-IWCS2013.
Kathryn Baker, Michael Bloodgood, Bonnie Dorr, Chris Callison-Burch, Nathaniel Filardo, Christine Piatko, Lori
Levin, and Scott Miller. 2012. Use of modality and negation in semantically-informed syntactic mt. Language
in Society, 38(2).
165
Farah Benamara, Baptiste Chardon, Yannick Mathieu, Vladimir Popescu, and Nicholas Asher. 2012. How do
negation and modality impact on opinions? In Proceedings of the Workshop on Extra-Propositional Aspects of
Meaning in Computational Linguistics, pages 10?18.
Defang Cao, Wenjie Li, Chunfa Yuan, and Kam-Fai Wong. 2006. Automatic chinese aspectual classification using
linguistic indicators. International Journal of Information Technology, 12(4):99?109.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions
on Intelligent Systems and Technology, 2(3):1?27.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning. 2009. Discriminative reordering
with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in
Statistical Translation, pages 51?59.
Keh-Jiann Chen and Chu-Ren Huang. 1990. Information-based case grammar. In Proceedings of the 13th confer-
ence on Computational linguistics, pages 54?59.
Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and Hui-Li Hsu. 1996. Sinica corpus: Design methodology
for balanced corpora. In Proceedings of Pacific Asia Conference on Language, Information and Computing
(PACLIC), pages 167?176.
Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao ming Gao, and Kuang-Yu Chen. 2000. Sinica treebank:
design criteria, annotation guidelines, and on-line interface. In Proceedings of the second workshop on Chinese
language processing: held in conjunction with the 38th Annual Meeting of the Association for Computational
Linguistics, pages 29?37.
Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith. 2007. A sequencing model for situation entity
classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,
pages 896?903.
Frank Robert Palmer. 2001. Mood and Modality. Cambridge University Press, Cambridge.
Roser Sauri and James Pustejovsky. 2012. Are you sure that this happened? assessing the factuality degree of
events in text. Computational Linguistics, 38(2):261?299.
Roser Sauri, Marc Verhagen, and James Pustejovsky. 2006. Annotating and recognizing event modality in text. In
Proceedings of 19th International FLAIRS Conference, pages 333?338.
John R. Searle. 1976. A classification of illocutionary acts. Language in Society, 5(1):1?23.
Eric V. Siegel and Kathleen R. McKeown. 2000. Learning methods to combine linguistic indicators: Improving
aspectual classification and revealing linguistic insights. Computational Linguistics, 26(4):595?628.
Eric V. Siegel. 1999. Corpus-based linguistic indicators for aspectual classification. In Proceedings of the 37th
annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 112?119.
Carlotta Smith. 1991. The Parameter of Aspect. Kluwer Academic Publishers, Dordrecht.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional
random field word segmenter for sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on
Chinese Language Processing, volume 171.
Zeno Vendler, 1967. Linguistics in Philosophy, chapter Verbs and times, pages 97?121. Cornell University Press,
Ithaca.
Hongzhi Xu and Chu-Ren Huang. 2013. Primitives of events and the semantic representation. In Proceedings of
the 6th International Conference on Generative Approaches to the Lexicon, pages 54?61.
Alessandra Zarcone and Alessandro Lenci. 2008. Computational models for event type classification in context.
In Proceedings of the International Conference on Language Resource and Evaluation (LREC), pages 1232?
1238.
Xiaodan Zhu, Chunfa Yuan, Kam-Fai Wong, and Wenjie Li. 2000. An algorithm for situation classification of
chinese verbs. In Proceedings of the second workshop on Chinese language processing: held in conjunction
with the 38th Annual Meeting of the Association for Computational Linguistics, volume 12, pages 140?145.
166
